{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " # Notebook1B-LoRA â€” Full Fine-Tuning vs. LoRA\n",
    "\n",
    " In the previous notebook, we learned how to train only the top classification head of the GPT model to perform spam SMS classification. Now, we will further explore two more sophisticated fine-tuning strategies:\n",
    " - **Full Fine-Tuning**\n",
    " - **LoRA (Low-Rank Adaptation)**\n",
    "\n",
    " **Core Objectives:**\n",
    " - Compare the differences between the two fine-tuning strategies in terms of resource consumption and model performance.\n",
    " - Understand the unique advantages of LoRA in fine-tuning large models.\n",
    "\n",
    "\n",
    " **Contents:**\n",
    " 1. [Preparations](#1-Preparations)\n",
    "    - [1.1 Download and Balance SMS Spam Data](#11-Download-and-Balance-SMS-Spam-Data)\n",
    "    - [1.2 Build Dataset and DataLoader](#12-Build-Dataset-and-DataLoader)\n",
    "    - [1.3 Load Pretrained GPT2 (Replacing Custom Model)](#13-Load-Pretrained-GPT2-Replacing-Custom-Model)\n",
    " 2. [Implementation and Training of Two Strategies](#2-Implementation-and-Training-of-Two-Strategies)\n",
    "    - [Strategy A: Full Fine-Tuning](#Strategy-A-Full-Fine-Tuning)\n",
    "    - [Strategy B: LoRA](#Strategy-B-LoRA)\n",
    " 3. [Parallel Comparison Results](#3-Parallel-Comparison-Results)\n",
    " 4. [Summary](#4-Summary)\n",
    "\n",
    " **References**\n",
    " - Build a Large Language Model (from scratch), pp.322-336\n",
    " - LoRA paper: [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)\n",
    " - LoRA explanation video by Edward Hu: [https://www.youtube.com/watch?v=DhRoTONcyZE](https://www.youtube.com/watch?v=DhRoTONcyZE)\n",
    " - LoRA with diffusion models: [https://huggingface.co/blog/lora](https://huggingface.co/blog/lora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ## 1. Preparations\n",
    "\n",
    " Before diving into fine-tuning strategies, we need to:\n",
    " - Download the SMS Spam dataset\n",
    " - Balance it (ham vs. spam)\n",
    " - Set up a custom dataset and data loaders\n",
    " - Load a pretrained GPT-2 classification model to serve as our base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ### 1.1 Download and Balance SMS Spam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "DEBUG = True\n",
    "import os\n",
    "if DEBUG: \n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "data_path = \"content\"\n",
    "import torch\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Max Length is:  128\n",
      "Max Length is:  128\n",
      "Max Length:  128\n",
      "Number of training batches: 4209, Number of validation batches: 55\n"
     ]
    }
   ],
   "source": [
    "from src.dataset_loader import get_enc_dataset\n",
    "# Set model name\n",
    "from transformers import GPT2Tokenizer\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if DEBUG:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Load GPT-2 Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# Add a separate pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    # Use '<|PAD|>' as the padding token\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|PAD|>'})\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    print(\"Added new pad_token '<|PAD|>' with ID:\", pad_token_id)\n",
    "batch_size = 16\n",
    "max_length=128\n",
    "train_dataset, val_dataset,train_loader,val_loader, pad_token_id = get_enc_dataset(data_path,\n",
    "                                                                                   tokenizer,\n",
    "                                                                                   batch_size=batch_size,\n",
    "                                                                                   max_length = max_length\n",
    "                                                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: (5572, 2)\n",
      "Balanced dataset size: (1494, 2)\n",
      "Train size: 1045 | Validation size: 149 | Test size: 300\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extract_dir = \"sms_spam_collection\"\n",
    "data_file = Path(extract_dir) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_spam_data(url, zip_path, extract_dir, data_file):\n",
    "    if data_file.exists():\n",
    "        print(f\"{data_file} already exists. Skipping download.\")\n",
    "        return\n",
    "    print(\"Downloading dataset...\")\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    print(\"Extracting files...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(extract_dir)\n",
    "    raw_file = Path(extract_dir) / \"SMSSpamCollection\"\n",
    "    raw_file.rename(data_file)\n",
    "    print(\"Data downloaded and saved as:\", data_file)\n",
    "\n",
    "if not data_file.exists():\n",
    "    download_spam_data(url, zip_path, extract_dir, data_file)\n",
    "\n",
    "df = pd.read_csv(data_file, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "print(f\"Original dataset size: {df.shape}\")\n",
    "\n",
    "def balance_spam_dataset(df):\n",
    "    \"\"\"\n",
    "    Balance the dataset by ensuring an equal number of 'spam' and 'ham' samples.\n",
    "    \"\"\"\n",
    "    df_spam = df[df[\"Label\"] == \"spam\"]\n",
    "    df_ham = df[df[\"Label\"] == \"ham\"].sample(len(df_spam), random_state=42)\n",
    "    balanced_df = pd.concat([df_spam, df_ham], ignore_index=True)\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = balance_spam_dataset(df)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "print(f\"Balanced dataset size: {balanced_df.shape}\")\n",
    "\n",
    "def random_split(df, train_frac=0.7, val_frac=0.1):\n",
    "    \"\"\"\n",
    "    Split the dataset into training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    total = len(df)\n",
    "    train_end = int(total * train_frac)\n",
    "    val_end = train_end + int(total * val_frac)\n",
    "    return df[:train_end], df[train_end:val_end], df[val_end:]\n",
    "\n",
    "train_df, valid_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"train.csv\", index=False)\n",
    "valid_df.to_csv(\"validation.csv\", index=False)\n",
    "test_df.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "print(f\"Train size: {len(train_df)} | Validation size: {len(valid_df)} | Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Build Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader: 65 batches, Val loader: 10 batches, Test loader: 19 batches\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=128):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.encodings = self.tokenizer(\n",
    "            self.df[\"Text\"].tolist(),\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.encodings['input_ids'][idx]\n",
    "        attention_mask = self.encodings['attention_mask'][idx]\n",
    "        label = torch.tensor(self.df.iloc[idx][\"Label\"], dtype=torch.long)\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#train_ds = SpamDataset(data_path+\"/train.tsv\", tokenizer, max_length=128)\n",
    "#val_ds   = SpamDataset(data_path+\"/dev.tsv\", tokenizer, max_length=128)\n",
    "train_ds = SpamDataset(\"train.csv\", tokenizer, max_length=128)\n",
    "val_ds   = SpamDataset(\"validation.csv\", tokenizer, max_length=128)\n",
    "test_ds  = SpamDataset(\"test.csv\", tokenizer, max_length=128)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train loader: {len(train_loader)} batches, Val loader: {len(val_loader)} batches, Test loader: {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load Pretrained GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels', 'text'])\n",
      "[50256]\n",
      "50256\n",
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "for x in train_loader:\n",
    "    print(x.keys())\n",
    "    break\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Errror\")\n",
    "print(tokenizer.encode(tokenizer.pad_token))\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.encode(tokenizer.eos_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128])\n",
      "tensor(50256)\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "tensor(50256)\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "tensor(50256)\n",
      "torch.Size([16, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     48\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 50\u001b[0m init_train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m init_val_acc   \u001b[38;5;241m=\u001b[39m calc_accuracy(val_loader, model, device, max_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#init_test_acc  = calc_accuracy(test_loader, model, device, max_batches=10)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 34\u001b[0m, in \u001b[0;36mcalc_accuracy\u001b[0;34m(loader, model, device, max_batches)\u001b[0m\n\u001b[1;32m     31\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 34\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mforward_for_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m y_batch)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m, in \u001b[0;36mforward_for_classification\u001b[0;34m(model, input_ids, attention_mask, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_for_classification\u001b[39m(model, input_ids, attention_mask, device):\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1375\u001b[0m, in \u001b[0;36mGPT2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1375\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1388\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1389\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:922\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    910\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    911\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    912\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m         output_attentions,\n\u001b[1;32m    920\u001b[0m     )\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:441\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    439\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    440\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 441\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    443\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:370\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    368\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[1;32m    369\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[0;32m--> 370\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/pytorch_utils.py:118\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    117\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 118\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2ForSequenceClassification, GPT2Config\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "def forward_for_classification(model, input_ids, attention_mask, device):\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    return logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_accuracy(loader, model, device, max_batches=None):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    #for i, instance in enumerate(loader):\n",
    "        #print(instance)\n",
    "    #for i, (input_ids, attention_mask, y_batch,text) in enumerate(loader):\n",
    "    for i, instance in enumerate(loader):\n",
    "        input_ids = instance['input_ids']\n",
    "        attention_mask = instance['attention_mask']\n",
    "        y_batch = instance['labels']\n",
    "        print(input_ids.shape)\n",
    "        print(torch.max(input_ids))\n",
    "        print(attention_mask.shape)\n",
    "        text = instance['text']\n",
    "        if max_batches and (i+1) > max_batches:\n",
    "            break\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        #input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        logits = forward_for_classification(model, input_ids, attention_mask, device)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "num_labels = 2\n",
    "model_config = GPT2Config.from_pretrained(model_name, num_labels=num_labels)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name, config=model_config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id \n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "init_train_acc = calc_accuracy(train_loader, model, device, max_batches=10)\n",
    "init_val_acc   = calc_accuracy(val_loader, model, device, max_batches=10)\n",
    "#init_test_acc  = calc_accuracy(test_loader, model, device, max_batches=10)\n",
    "print(f\"Initial Accuracies -> Train: {init_train_acc*100:.2f}%, Val: {init_val_acc*100:.2f}%, Test: {init_test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ## 2. Implementation and Training of Two Strategies\n",
    "\n",
    " We now compare:\n",
    " 1. **Full Fine-Tuning**: Updating all GPT-2 parameters + classifier\n",
    " 2. **LoRA**: Introducing Low-Rank Adaptation layers to reduce parameter count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ### Strategy A: Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import copy\n",
    "import time\n",
    "\n",
    "def train_model_full_finetune(model, train_loader, val_loader, device, epochs=3, lr=5e-5):\n",
    "    \"\"\"\n",
    "    Fully fine-tune all parameters of GPT-2, including the classification head.\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_accs = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for input_ids, attention_mask, y_batch in train_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = forward_for_classification(model, input_ids, attention_mask, device)\n",
    "            loss = loss_fn(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        val_acc = calc_accuracy(val_loader, model, device)\n",
    "        train_losses.append(avg_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch={epoch+1}, Loss={avg_loss:.4f}, ValAcc={val_acc*100:.2f}%\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    return elapsed, train_losses, val_accs\n",
    "\n",
    "print(\"=== Strategy A: Full Fine-Tuning ===\")\n",
    "modelA = copy.deepcopy(model)\n",
    "full_tune_params = sum(p.numel() for p in modelA.parameters() if p.requires_grad)\n",
    "print(f\"[Full Fine-Tuning] Trainable Params: {full_tune_params}\")\n",
    "\n",
    "init_accA = calc_accuracy(train_loader, modelA, device, max_batches=10)\n",
    "print(f\"[Full Fine-Tuning] Initial Train Acc (first 10 batches): {init_accA*100:.2f}%\")\n",
    "\n",
    "elapsedA, ft_train_losses, ft_val_accs = train_model_full_finetune(\n",
    "    modelA, train_loader, val_loader, device, epochs=5, lr=5e-5\n",
    ")\n",
    "\n",
    "train_accA = calc_accuracy(train_loader, modelA, device)\n",
    "val_accA   = calc_accuracy(val_loader, modelA, device)\n",
    "test_accA  = calc_accuracy(test_loader, modelA, device)\n",
    "print(f\"[Full Fine-Tuning] Time: {elapsedA:.2f}s, TrainAcc={train_accA*100:.2f}%, ValAcc={val_accA*100:.2f}%, TestAcc={test_accA*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ### Strategy B: LoRA (Low-Rank Adaptation)\n",
    "\n",
    " **Key Idea**:\n",
    " Insert low-rank matrices \\(A\\) and \\(B\\) into selected GPT-2 submodules (e.g., `c_fc`, `c_proj`), then train only these matrices (plus the classifier).\n",
    "\n",
    " This approach drastically reduces the number of trainable parameters and can still achieve performance close to full fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " #### B.1: LoRA Implementation from Scratch\n",
    "\n",
    " Here we define:\n",
    " - `LoRALayer` to handle low-rank factors\n",
    " - `LinearWithLoRA` and `Conv1DWithLoRA` as wrappers\n",
    " - `replace_modules_with_lora` to insert LoRA\n",
    " - `freeze_original_parameters` so that only LoRA + classifier are trainable\n",
    "\n",
    " Notice the shape logs:\n",
    " For GPT-2 `in_dim` = `out_dim` = 768 in certain layers (`c_fc`, `c_proj`), so you might see `[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32`.\n",
    " \n",
    " **Explanation**: GPT-2's hidden dimensionality is 768 in the base model, and `rank=16, alpha=32` are our chosen hyperparameters.\n",
    " - **Rank** determines the low-rank subspace.\n",
    " - **Alpha** is a scalar factor amplifying the LoRA update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def advanced_metrics(loader, model, device):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, F1-score, and accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds_list, labels_list = [], []\n",
    "    for input_ids, attention_mask, y_batch in loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = forward_for_classification(model, input_ids, attention_mask, device)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        preds_list.extend(preds.cpu().numpy())\n",
    "        labels_list.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    accuracy  = np.mean(np.array(preds_list) == np.array(labels_list))\n",
    "    precision = precision_score(labels_list, preds_list)\n",
    "    recall    = recall_score(labels_list, preds_list)\n",
    "    f1        = f1_score(labels_list, preds_list)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import torch.cuda.amp as amp\n",
    "from transformers.models.gpt2.modeling_gpt2 import Conv1D\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Low-Rank Adaptation layer to inject trainable parameters A and B into original weight update.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Low-rank matrices\n",
    "        self.A = nn.Parameter(torch.empty(in_dim, rank))\n",
    "        self.B = nn.Parameter(torch.empty(rank, out_dim))\n",
    "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.B)\n",
    "\n",
    "        # Explanation log: GPT-2 base has hidden_dim=768; rank=16, alpha=32 by default.\n",
    "        print(f\"[LoRALayer] in_dim={in_dim}, out_dim={out_dim}, rank={rank}, alpha={alpha}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Decomposition: alpha * (x @ A @ B)\n",
    "        return self.alpha * (x @ self.A @ self.B)\n",
    "\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for nn.Linear that adds a LoRA output to the original linear output.\n",
    "    \"\"\"\n",
    "    def __init__(self, linear_module, rank, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.linear = linear_module\n",
    "        self.lora   = LoRALayer(linear_module.in_features, linear_module.out_features, rank, alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "class Conv1DWithLoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for Conv1D that adds a LoRA output to the original Conv1D output.\n",
    "    \"\"\"\n",
    "    def __init__(self, conv1d_module: Conv1D, rank, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.conv = conv1d_module\n",
    "        in_dim, out_dim = conv1d_module.weight.shape\n",
    "        self.lora = LoRALayer(in_dim, out_dim, rank, alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_normal = self.conv(x)\n",
    "        B, S, hidden_dim = x.shape\n",
    "        x_2d = x.view(B*S, hidden_dim)\n",
    "        out_lora_2d = self.lora(x_2d)\n",
    "        out_lora_3d = out_lora_2d.view(B, S, -1)\n",
    "        return out_normal + out_lora_3d\n",
    "\n",
    "def replace_modules_with_lora(module, rank=16, alpha=32):\n",
    "    \"\"\"\n",
    "    Recursively replace GPT-2 submodules (c_fc, c_proj) with LoRA wrappers.\n",
    "    \"\"\"\n",
    "    for name, child in list(module.named_children()):\n",
    "        if isinstance(child, nn.Linear) and name in [\"c_fc\", \"c_proj\"]:\n",
    "            new_module = LinearWithLoRA(child, rank, alpha)\n",
    "            setattr(module, name, new_module)\n",
    "        elif isinstance(child, Conv1D) and name in [\"c_fc\", \"c_proj\"]:\n",
    "            new_module = Conv1DWithLoRA(child, rank, alpha)\n",
    "            setattr(module, name, new_module)\n",
    "        else:\n",
    "            replace_modules_with_lora(child, rank, alpha)\n",
    "\n",
    "def freeze_original_parameters(model):\n",
    "    \"\"\"\n",
    "    Freeze all parameters except LoRA layers and classifier.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora\" not in name.lower() and \"classifier\" not in name.lower():\n",
    "            param.requires_grad = False\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Print the number of trainable parameters vs. total parameters.\n",
    "    \"\"\"\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total     = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable params: {trainable} / Total params: {total}\")\n",
    "    return trainable\n",
    "\n",
    "def show_gradient_norms(model):\n",
    "    \"\"\"\n",
    "    Print gradient norms for LoRA layers to confirm only LoRA + classifier receive gradients.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            print(f\"Gradient Norm for {name}: {param.grad.norm():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " #### B.2: LoRA Training Function\n",
    "\n",
    " Here, we only train the LoRA parameters and the classifier head. The rest of GPT-2 is frozen.\n",
    "\n",
    " **Optional**: We can log gradient norms (`log_grad_norms=True`) to confirm that only LoRA + classifier layers get updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def train_model_lora(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    device, \n",
    "    epochs=3, \n",
    "    lr=1e-4, \n",
    "    log_grad_norms=False\n",
    "):\n",
    "    freeze_original_parameters(model)\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    train_losses, val_accs = [], []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, (input_ids, attention_mask, y_batch) in enumerate(train_loader):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with amp.autocast():\n",
    "                logits = forward_for_classification(model, input_ids, attention_mask, device)\n",
    "                loss = loss_fn(logits, y_batch)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # If the user wants to log gradient norms for educational demonstration:\n",
    "            if log_grad_norms:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                #show_gradient_norms(model)\n",
    "            else:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        val_acc  = calc_accuracy(val_loader, model, device)\n",
    "        train_losses.append(avg_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        print(f\"Epoch={epoch+1}, Loss={avg_loss:.4f}, ValAcc={val_acc*100:.2f}%\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    return elapsed, train_losses, val_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " Now, let's create a copy of our GPT-2 model and apply LoRA to it with the chosen `rank` and `alpha` hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "print(\"=== Strategy B: LoRA ===\")\n",
    "\n",
    "modelB = copy.deepcopy(model)  # Duplicate the base model\n",
    "replace_modules_with_lora(modelB, rank=16, alpha=32)  # Replace layers\n",
    "modelB.to(device)\n",
    "\n",
    "elapsedB, lora_train_losses, lora_val_accs = train_model_lora(\n",
    "    modelB, train_loader, val_loader, device, epochs=5, lr=1e-4, log_grad_norms=True\n",
    ")\n",
    "\n",
    "def calc_accuracy_full(loader, model, device, max_batches=None):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for i, (input_ids, attention_mask, y_batch) in enumerate(loader):\n",
    "        if max_batches and (i+1) > max_batches:\n",
    "            break\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = forward_for_classification(model, input_ids, attention_mask, device)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total   += y_batch.size(0)\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "train_accB = calc_accuracy_full(train_loader, modelB, device)\n",
    "val_accB   = calc_accuracy_full(val_loader, modelB, device)\n",
    "test_accB  = calc_accuracy_full(test_loader, modelB, device)\n",
    "print(f\"[LoRA] Training Time: {elapsedB:.2f}s\")\n",
    "print(f\"[LoRA] TrainAcc={train_accB*100:.2f}%, ValAcc={val_accB*100:.2f}%, TestAcc={test_accB*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " #### B.3: Advanced Metrics (Precision, Recall, F1)\n",
    "\n",
    " We'll calculate a more comprehensive set of metrics on the LoRA model to evaluate performance beyond accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "accB, precB, recB, f1B = advanced_metrics(test_loader, modelB, device)\n",
    "print(f\"[LoRA Advanced Metrics on Test] Accuracy={accB*100:.2f}%, Precision={precB*100:.2f}%, Recall={recB*100:.2f}%, F1={f1B*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " #### B.4: Saving and Loading LoRA Parameters\n",
    "\n",
    " LoRA's key advantage is the ability to save only the adapter components without storing the entire large model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def save_lora_params(model, save_path=\"lora_params.pt\"):\n",
    "    \"\"\"\n",
    "    Save only LoRA-related parameters (and the classifier) for demonstration.\n",
    "    \"\"\"\n",
    "    lora_dict = {\n",
    "        k: v for k, v in model.state_dict().items()\n",
    "        if \"lora\" in k.lower() or \"classifier\" in k.lower()\n",
    "    }\n",
    "    torch.save(lora_dict, save_path)\n",
    "    print(f\"LoRA params saved to {save_path}\")\n",
    "\n",
    "def load_lora_params(model, load_path=\"lora_params.pt\"):\n",
    "    \"\"\"\n",
    "    Load LoRA parameters into a GPT-2 model that already has LoRA layers.\n",
    "    \"\"\"\n",
    "    loaded_dict = torch.load(load_path, map_location=device)\n",
    "    model.load_state_dict(loaded_dict, strict=False)\n",
    "    print(f\"LoRA params loaded from {load_path}\")\n",
    "\n",
    "# Example usage (commented out):\n",
    "# save_lora_params(modelB, \"my_lora_params.pt\")\n",
    "# new_modelB = copy.deepcopy(model)\n",
    "# replace_modules_with_lora(new_modelB, rank=16, alpha=32)\n",
    "# new_modelB.to(device)\n",
    "# load_lora_params(new_modelB, \"my_lora_params.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the convergence behavior of full fine-tuning and LoRA by visualizing the training loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Left: Training Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(ft_train_losses, label=\"Full Fine-Tuning - Loss\")\n",
    "plt.plot(lora_train_losses, label=\"LoRA - Loss\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Comparison of Training Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Right: Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([acc * 100 for acc in ft_val_accs], label=\"Full Fine-Tuning - Accuracy\")\n",
    "plt.plot([acc * 100 for acc in lora_val_accs], label=\"LoRA - Accuracy\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy (%)\")\n",
    "plt.title(\"Comparison of Validation Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ## 3. Parallel Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "print(\"=== Comparison of Two Fine-Tuning Strategies ===\")\n",
    "\n",
    "print(f\"Full Fine-Tuning:\")\n",
    "print(f\" Trainable Params={full_tune_params}, Time={elapsedA:.2f}s, TestAcc={test_accA*100:.2f}%\\n\")\n",
    "\n",
    "lora_params_count = sum(p.numel() for p in modelB.parameters() if p.requires_grad)\n",
    "print(f\"LoRA:\")\n",
    "print(f\" Trainable Params={lora_params_count}, Time={elapsedB:.2f}s, TestAcc={test_accB*100:.2f}%\")\n",
    "print(f\" Precision={precB*100:.2f}%, Recall={recB*100:.2f}%, F1={f1B*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ## 4. Summary\n",
    "\n",
    " 1. **Full Fine-Tuning**: Updates all parameters of GPT-2 + the classification head.\n",
    "    - **Pros**: Maximum adaptation, potentially highest performance.\n",
    "    - **Cons**: Very expensive computationally; less flexible if you need multiple task-specific versions.\n",
    "\n",
    " 2. **LoRA**: Freezes original GPT-2 parameters, adds small low-rank matrices for adaptation.\n",
    "    - **Pros**: Parameter-efficient, faster training, easy to save/load adapters.\n",
    "    - **Cons**: Might need careful hyperparameter tuning (rank, alpha) to match full fine-tuning performance.\n",
    "\n",
    " **Key Takeaways**:\n",
    " - LoRA can achieve near-full fine-tuning performance with a fraction of trainable parameters.\n",
    " - It's an excellent choice for large language models in resource-constrained environments.\n",
    "\n",
    " ### Hyperparameter Exploration\n",
    "\n",
    " - **Rank and Alpha**: Try `rank=[4,8,16,32]` and `alpha=[8,16,32,64]` to see changes in parameter count and performance.\n",
    " - **Learning Rate**: Adjust `lr` to see if smaller ranks need a different learning rate.\n",
    " - **Epochs**: Extend or reduce the number of epochs to observe convergence speed and final performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
