{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e2586f-e204-4df2-808d-137e56425598",
   "metadata": {},
   "source": [
    "# Lora \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0040344-9dfe-46d3-b1c3-a199b19348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0dc8e5a-cb27-4aff-a0a0-484cdb09f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "import os\n",
    "if DEBUG: \n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "data_path = \"content\"\n",
    "import torch\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be5cbfb-b3dc-4cb0-9c45-45ad340b54ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Index(['Text', 'Label'], dtype='object')\n",
      "Max Length is:  65\n",
      "Index(['Text', 'Label'], dtype='object')\n",
      "Max Length is:  65\n",
      "Index(['Text', 'Label'], dtype='object')\n",
      "Max Length is:  65\n",
      "Max Length:  65\n",
      "Number of training batches: 3573, Number of validation batches: 149\n"
     ]
    }
   ],
   "source": [
    "from src.dataset_loader import get_enc_dataset\n",
    "# Set model name\n",
    "from transformers import GPT2Tokenizer\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if DEBUG:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Load GPT-2 Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# Add a separate pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#if tokenizer.pad_token is None:\n",
    "#    # Use '<|PAD|>' as the padding token\n",
    "#    tokenizer.add_special_tokens({'pad_token': '<|PAD|>'})\n",
    "#    pad_token_id = tokenizer.pad_token_id\n",
    "#    print(\"Added new pad_token '<|PAD|>' with ID:\", pad_token_id)\n",
    "batch_size = 16\n",
    "max_length=None#128\n",
    "train_dataset, val_dataset,test_dataset,train_loader,val_loader,test_loader, pad_token_id = get_enc_dataset(data_path,\n",
    "                                                                                   tokenizer,\n",
    "                                                                                   batch_size=batch_size,\n",
    "                                                                                   max_length = max_length\n",
    "                                                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06413808-f53c-4b95-a801-3eb2056d5da5",
   "metadata": {},
   "source": [
    "## Load Petrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3bf22-83a0-49a5-9ca0-2e7c48700c2f",
   "metadata": {},
   "source": [
    "### Test bevor Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3efbf1-90fe-44b5-a2ca-49dfaf4511c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd756a8-c293-4d59-adf6-0ce138924fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 12:42:25.114915: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-17 12:42:25.131686: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737114145.153154   25162 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737114145.159738   25162 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-17 12:42:25.181038: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Accuracies -> Train: 42.50%, Val: 48.12%, Test: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2ForSequenceClassification, GPT2Config\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "from src.lora_helper import calc_accuracy,forward_for_classification\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "num_labels = 2\n",
    "model_config = GPT2Config.from_pretrained(model_name, num_labels=num_labels)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name, config=model_config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id \n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "init_train_acc = calc_accuracy(train_loader, model, device, max_batches=10)\n",
    "init_val_acc   = calc_accuracy(val_loader, model, device, max_batches=10)\n",
    "#init_test_acc  = calc_accuracy(test_loader, model, device, max_batches=10)\n",
    "init_test_acc = 0.0\n",
    "print(f\"Initial Accuracies -> Train: {init_train_acc*100:.2f}%, Val: {init_val_acc*100:.2f}%, Test: {init_test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bce414-0a2e-4ea2-8330-e3c5115e7de3",
   "metadata": {},
   "source": [
    "## Different Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46cdb4-15b3-433d-ba70-5c1c3284d723",
   "metadata": {},
   "source": [
    "### Strategy A: Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ebd87f1-7a3f-441b-a104-c10ad85fcf68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Strategy A: Full Fine-Tuning ===\n",
      "[Full Fine-Tuning] Trainable Params: 124441344\n",
      "[Full Fine-Tuning] Initial Train Acc (first 10 batches): 44.38%\n",
      "Epoch 1/5, step 500/3573, loss = 0.2763\n",
      "Epoch 1/5, step 1000/3573, loss = 0.4824\n",
      "Epoch 1/5, step 1500/3573, loss = 0.1989\n",
      "Epoch 1/5, step 2000/3573, loss = 0.2015\n",
      "Epoch 1/5, step 2500/3573, loss = 0.3645\n",
      "Epoch 1/5, step 3000/3573, loss = 0.1001\n",
      "Epoch 1/5, step 3500/3573, loss = 0.3682\n",
      "Epoch 1/5, step 3570/3573, loss = 0.1308Epoch=1, Loss=0.2701, ValAcc=92.49%\n",
      "Epoch 2/5, step 500/3573, loss = 0.0914\n",
      "Epoch 2/5, step 1000/3573, loss = 0.0514\n",
      "Epoch 2/5, step 1500/3573, loss = 0.0654\n",
      "Epoch 2/5, step 2000/3573, loss = 0.1347\n",
      "Epoch 2/5, step 2500/3573, loss = 0.0311\n",
      "Epoch 2/5, step 3000/3573, loss = 0.0437\n",
      "Epoch 2/5, step 3500/3573, loss = 0.1943\n",
      "Epoch 2/5, step 3570/3573, loss = 0.0254Epoch=2, Loss=0.1501, ValAcc=93.54%\n",
      "Epoch 3/5, step 500/3573, loss = 0.1641\n",
      "Epoch 3/5, step 1000/3573, loss = 0.1421\n",
      "Epoch 3/5, step 1500/3573, loss = 0.3240\n",
      "Epoch 3/5, step 2000/3573, loss = 0.0129\n",
      "Epoch 3/5, step 2500/3573, loss = 0.0439\n",
      "Epoch 3/5, step 3000/3573, loss = 0.0697\n",
      "Epoch 3/5, step 3500/3573, loss = 0.0622\n",
      "Epoch 4/5, step 1000/3573, loss = 0.0026\n",
      "Epoch 4/5, step 1500/3573, loss = 0.3146\n",
      "Epoch 4/5, step 2000/3573, loss = 0.0343\n",
      "Epoch 4/5, step 2500/3573, loss = 0.0871\n",
      "Epoch 4/5, step 3000/3573, loss = 0.0060\n",
      "Epoch 4/5, step 3500/3573, loss = 0.0286\n",
      "Epoch 4/5, step 3570/3573, loss = 0.0223Epoch=4, Loss=0.0752, ValAcc=94.04%\n",
      "Epoch 5/5, step 500/3573, loss = 0.0061\n",
      "Epoch 5/5, step 1000/3573, loss = 0.0309\n",
      "Epoch 5/5, step 1500/3573, loss = 0.0898\n",
      "Epoch 5/5, step 2000/3573, loss = 0.1062\n",
      "Epoch 5/5, step 2500/3573, loss = 0.0078\n",
      "Epoch 5/5, step 3000/3573, loss = 0.0024\n",
      "Epoch 5/5, step 3500/3573, loss = 0.0141\n",
      "Epoch 5/5, step 3570/3573, loss = 0.0307Epoch=5, Loss=0.0565, ValAcc=94.25%\n",
      "[Full Fine-Tuning] Time: 2557.03s, TrainAcc=99.20%, ValAcc=94.25%, TestAcc=90.14%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from src.lora_train import train_model_full_finetune\n",
    "import copy\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "print(\"=== Strategy A: Full Fine-Tuning ===\")\n",
    "modelA = copy.deepcopy(model)\n",
    "full_tune_params = sum(p.numel() for p in modelA.parameters() if p.requires_grad)\n",
    "print(f\"[Full Fine-Tuning] Trainable Params: {full_tune_params}\")\n",
    "\n",
    "init_accA = calc_accuracy(train_loader, modelA, device, max_batches=10)\n",
    "print(f\"[Full Fine-Tuning] Initial Train Acc (first 10 batches): {init_accA*100:.2f}%\")\n",
    "\n",
    "elapsedA, ft_train_losses, ft_val_accs = train_model_full_finetune(\n",
    "    modelA, train_loader, val_loader, device, epochs=5, lr=5e-5\n",
    ")\n",
    "\n",
    "train_accA = calc_accuracy(train_loader, modelA, device)\n",
    "val_accA   = calc_accuracy(val_loader, modelA, device)\n",
    "test_accA  = calc_accuracy(test_loader, modelA, device)\n",
    "print(f\"[Full Fine-Tuning] Time: {elapsedA:.2f}s, TrainAcc={train_accA*100:.2f}%, ValAcc={val_accA*100:.2f}%, TestAcc={test_accA*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f381679a-8775-434b-bf29-e3ca5b1a5bc7",
   "metadata": {},
   "source": [
    "## Strategy B LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36d4eb-82a4-4cab-96c7-9b00d36cd9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d815cb7b-433a-4911-a499-adec7b82a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lora_model import *\n",
    "from src.lora_train import train_model_lora\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33ba74-36e3-4ffe-aac2-18487d7f6643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2adfa923-2d0d-4eac-8014-9a226bd63aac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Strategy B: LoRA ===\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "Trainable params: 1769472 / Total params: 126210816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/vast-react/home/joris.hellwig/u13685/jupyterhub-gwdg/ex3/src/lora_train.py:75: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()\n",
      "/mnt/vast-react/home/joris.hellwig/u13685/jupyterhub-gwdg/ex3/src/lora_train.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, step 500/3573, loss = 0.6137\n",
      "Epoch 1/5, step 1000/3573, loss = 0.6961\n",
      "Epoch 1/5, step 1500/3573, loss = 0.4717\n",
      "Epoch 1/5, step 2000/3573, loss = 0.8589\n",
      "Epoch 1/5, step 2500/3573, loss = 0.5421\n",
      "Epoch 1/5, step 3000/3573, loss = 0.7865\n",
      "Epoch 1/5, step 3500/3573, loss = 0.4068\n",
      "Epoch 1/5, step 3570/3573, loss = 0.6336Epoch=1, Loss=0.5694, ValAcc=79.73%\n",
      "Epoch 2/5, step 500/3573, loss = 0.6780\n",
      "Epoch 2/5, step 1000/3573, loss = 0.5363\n",
      "Epoch 2/5, step 1500/3573, loss = 0.6228\n",
      "Epoch 2/5, step 2000/3573, loss = 0.2439\n",
      "Epoch 2/5, step 2500/3573, loss = 0.5157\n",
      "Epoch 2/5, step 3000/3573, loss = 0.6061\n",
      "Epoch 2/5, step 3500/3573, loss = 0.3295\n",
      "Epoch 2/5, step 3570/3573, loss = 0.4837Epoch=2, Loss=0.4618, ValAcc=82.58%\n",
      "Epoch 3/5, step 500/3573, loss = 0.3901\n",
      "Epoch 3/5, step 1000/3573, loss = 0.3308\n",
      "Epoch 3/5, step 1500/3573, loss = 0.3026\n",
      "Epoch 3/5, step 2000/3573, loss = 0.6323\n",
      "Epoch 3/5, step 2500/3573, loss = 0.4690\n",
      "Epoch 3/5, step 3000/3573, loss = 0.2564\n",
      "Epoch 3/5, step 3500/3573, loss = 0.4319\n",
      "Epoch 3/5, step 3570/3573, loss = 0.7620Epoch=3, Loss=0.4327, ValAcc=83.13%\n",
      "Epoch 4/5, step 500/3573, loss = 0.4588\n",
      "Epoch 4/5, step 1000/3573, loss = 0.5311\n",
      "Epoch 4/5, step 1500/3573, loss = 0.4291\n",
      "Epoch 4/5, step 2000/3573, loss = 0.2772\n",
      "Epoch 4/5, step 2500/3573, loss = 0.3174\n",
      "Epoch 4/5, step 3000/3573, loss = 0.4023\n",
      "Epoch 4/5, step 3500/3573, loss = 0.4217\n",
      "Epoch 4/5, step 3570/3573, loss = 0.2519Epoch=4, Loss=0.4148, ValAcc=83.42%\n",
      "Epoch 5/5, step 500/3573, loss = 0.3664\n",
      "Epoch 5/5, step 1000/3573, loss = 0.5122\n",
      "Epoch 5/5, step 1500/3573, loss = 0.3970\n",
      "Epoch 5/5, step 2000/3573, loss = 0.6766\n",
      "Epoch 5/5, step 2500/3573, loss = 0.2349\n",
      "Epoch 5/5, step 3000/3573, loss = 0.4453\n",
      "Epoch 5/5, step 3500/3573, loss = 0.4816\n",
      "Epoch 5/5, step 3570/3573, loss = 0.5760Epoch=5, Loss=0.4026, ValAcc=83.80%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m         total   \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m correct \u001b[38;5;241m/\u001b[39m total \u001b[38;5;28;01mif\u001b[39;00m total \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 30\u001b[0m train_accB \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_accuracy_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m val_accB   \u001b[38;5;241m=\u001b[39m calc_accuracy_full(val_loader, modelB, device)\n\u001b[1;32m     32\u001b[0m test_accB  \u001b[38;5;241m=\u001b[39m calc_accuracy_full(test_loader, modelB, device)\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36mcalc_accuracy_full\u001b[0;34m(loader, model, device, max_batches)\u001b[0m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     16\u001b[0m correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (input_ids, attention_mask, y_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_batches \u001b[38;5;129;01mand\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m max_batches:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "\n",
    "print(\"=== Strategy B: LoRA ===\")\n",
    "\n",
    "modelB = copy.deepcopy(model)  # Duplicate the base model\n",
    "replace_modules_with_lora(modelB, rank=16, alpha=32)  # Replace layers\n",
    "modelB.to(device)\n",
    "\n",
    "elapsedB, lora_train_losses, lora_val_accs = train_model_lora(\n",
    "    modelB, train_loader, val_loader, device, epochs=5, lr=1e-4, log_grad_norms=True\n",
    ")\n",
    "\n",
    "def calc_accuracy_full(loader, model, device, max_batches=None):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for i, (input_ids, attention_mask, y_batch) in enumerate(loader):\n",
    "        if max_batches and (i+1) > max_batches:\n",
    "            break\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = forward_for_classification(model, input_ids, attention_mask, device)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total   += y_batch.size(0)\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "train_accB = calc_accuracy_full(train_loader, modelB, device)\n",
    "val_accB   = calc_accuracy_full(val_loader, modelB, device)\n",
    "test_accB  = calc_accuracy_full(test_loader, modelB, device)\n",
    "print(f\"[LoRA] Training Time: {elapsedB:.2f}s\")\n",
    "print(f\"[LoRA] TrainAcc={train_accB*100:.2f}%, ValAcc={val_accB*100:.2f}%, TestAcc={test_accB*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473cff1-4fed-4d24-a37e-a5f337530b94",
   "metadata": {},
   "source": [
    " #### B.3: Advanced Metrics (Precision, Recall, F1)\n",
    "\n",
    " We'll calculate a more comprehensive set of metrics on the LoRA model to evaluate performance beyond accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b3ce5-5e89-4be5-b3b5-59b68e8f5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from src.lora_helper import advanced_metrics\n",
    "accB, precB, recB, f1B = advanced_metrics(test_loader, modelB, device)\n",
    "print(f\"[LoRA Advanced Metrics on Test] Accuracy={accB*100:.2f}%, Precision={precB*100:.2f}%, Recall={recB*100:.2f}%, F1={f1B*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
