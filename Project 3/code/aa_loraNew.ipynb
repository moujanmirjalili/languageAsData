{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e2586f-e204-4df2-808d-137e56425598",
   "metadata": {},
   "source": [
    "# Lora \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0040344-9dfe-46d3-b1c3-a199b19348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0dc8e5a-cb27-4aff-a0a0-484cdb09f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "import os\n",
    "if DEBUG: \n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "data_path = \"content\"\n",
    "import torch\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time, math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be5cbfb-b3dc-4cb0-9c45-45ad340b54ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Index(['Text', 'Label'], dtype='object')\n",
      "Max Length is:  65\n",
      "Index(['Text', 'Label'], dtype='object')\n",
      "Max Length is:  65\n",
      "Index(['Text', 'Label'], dtype='object')\n",
      "Max Length is:  65\n",
      "Max Length:  65\n",
      "Number of training batches: 3573, Number of validation batches: 149\n"
     ]
    }
   ],
   "source": [
    "from src.dataset_loader import get_enc_dataset\n",
    "# Set model name\n",
    "from transformers import GPT2Tokenizer\n",
    "model_name = \"gpt2\"\n",
    "path_to_save_folder = \"model\"\n",
    "path_to_lora = os.path.join(path_to_save_folder,\"lora\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if DEBUG:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Load GPT-2 Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# Add a separate pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#if tokenizer.pad_token is None:\n",
    "#    # Use '<|PAD|>' as the padding token\n",
    "#    tokenizer.add_special_tokens({'pad_token': '<|PAD|>'})\n",
    "#    pad_token_id = tokenizer.pad_token_id\n",
    "#    print(\"Added new pad_token '<|PAD|>' with ID:\", pad_token_id)\n",
    "batch_size = 16\n",
    "max_length=None#128\n",
    "train_dataset, val_dataset,test_dataset,train_loader,val_loader,test_loader, pad_token_id = get_enc_dataset(data_path,\n",
    "                                                                                   tokenizer,\n",
    "                                                                                   batch_size=batch_size,\n",
    "                                                                                   max_length = max_length\n",
    "                                                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06413808-f53c-4b95-a801-3eb2056d5da5",
   "metadata": {},
   "source": [
    "## Load Petrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3bf22-83a0-49a5-9ca0-2e7c48700c2f",
   "metadata": {},
   "source": [
    "### Test bevor Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3efbf1-90fe-44b5-a2ca-49dfaf4511c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd756a8-c293-4d59-adf6-0ce138924fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 01:06:53.248448: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-18 01:06:53.266401: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737158813.289198   87324 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737158813.296247   87324 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-18 01:06:53.318846: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Accuracies -> Train: 52.50%, Val: 48.12%, Test: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2ForSequenceClassification, GPT2Config\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "from src.lora_helper import calc_accuracy,forward_for_classification\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "num_labels = 2\n",
    "model_config = GPT2Config.from_pretrained(model_name, num_labels=num_labels)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name, config=model_config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id \n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "init_train_acc = calc_accuracy(train_loader, model, device, max_batches=10)\n",
    "init_val_acc   = calc_accuracy(val_loader, model, device, max_batches=10)\n",
    "#init_test_acc  = calc_accuracy(test_loader, model, device, max_batches=10)\n",
    "init_test_acc = 0.0\n",
    "print(f\"Initial Accuracies -> Train: {init_train_acc*100:.2f}%, Val: {init_val_acc*100:.2f}%, Test: {init_test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bce414-0a2e-4ea2-8330-e3c5115e7de3",
   "metadata": {},
   "source": [
    "## Different Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46cdb4-15b3-433d-ba70-5c1c3284d723",
   "metadata": {},
   "source": [
    "### Strategy A: Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd87f1-7a3f-441b-a104-c10ad85fcf68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Strategy A: Full Fine-Tuning ===\n",
      "[Full Fine-Tuning] Trainable Params: 124441344\n",
      "[Full Fine-Tuning] Initial Train Acc (first 10 batches): 43.75%\n",
      "Epoch 1/6, step 500/3573, loss = 0.1961\n",
      "Epoch 1/6, step 1000/3573, loss = 0.1994\n",
      "Epoch 1/6, step 1500/3573, loss = 0.2621\n",
      "Epoch 1/6, step 2000/3573, loss = 0.2521\n",
      "Epoch 1/6, step 2500/3573, loss = 0.5072\n",
      "Epoch 1/6, step 3000/3573, loss = 0.3664\n",
      "Epoch 1/6, step 3500/3573, loss = 0.2179\n",
      "Epoch 1/6, step 3570/3573, loss = 0.4453Epoch=1, Loss=0.2836, ValAcc=93.12% TrainAcc=88.22%\n",
      "Epoch 2/6, step 500/3573, loss = 0.2414\n",
      "Epoch 2/6, step 1000/3573, loss = 0.1089\n",
      "Epoch 2/6, step 1500/3573, loss = 0.0770\n",
      "Epoch 2/6, step 2000/3573, loss = 0.1568\n",
      "Epoch 2/6, step 2500/3573, loss = 0.1587\n",
      "Epoch 2/6, step 3000/3573, loss = 0.0165\n",
      "Epoch 2/6, step 3500/3573, loss = 0.1963\n",
      "Epoch 2/6, step 3570/3573, loss = 0.0704Epoch=2, Loss=0.1578, ValAcc=93.08% TrainAcc=94.16%\n",
      "Epoch 3/6, step 500/3573, loss = 0.0095\n",
      "Epoch 3/6, step 1000/3573, loss = 0.2402\n",
      "Epoch 3/6, step 1500/3573, loss = 0.0100\n",
      "Epoch 3/6, step 2000/3573, loss = 0.0204\n",
      "Epoch 3/6, step 2500/3573, loss = 0.0247\n",
      "Epoch 3/6, step 3000/3573, loss = 0.2620\n",
      "Epoch 3/6, step 3500/3573, loss = 0.0076\n",
      "Epoch 3/6, step 3570/3573, loss = 0.0413Epoch=3, Loss=0.1077, ValAcc=93.66% TrainAcc=96.17%\n",
      "Epoch 4/6, step 290/3573, loss = 0.1122"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from src.lora_train import train_model_full_finetune\n",
    "import copy\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "is_dry=False\n",
    "print(\"=== Strategy A: Full Fine-Tuning ===\")\n",
    "modelA = copy.deepcopy(model)\n",
    "full_tune_params = sum(p.numel() for p in modelA.parameters() if p.requires_grad)\n",
    "print(f\"[Full Fine-Tuning] Trainable Params: {full_tune_params}\")\n",
    "\n",
    "\n",
    "init_accA = calc_accuracy(train_loader, modelA, device, max_batches=1)\n",
    "print(f\"[Full Fine-Tuning] Initial Train Acc (first 10 batches): {init_accA*100:.2f}%\")\n",
    "\n",
    "#elapsedB, lora_train_losses, lora_val_accs,train_acc_l \n",
    "elapsedA, ft_train_losses, ft_val_accs, train_acc_l  = train_model_full_finetune(\n",
    "    modelA, train_loader, val_loader, device, epochs=6, lr=5e-5,is_dry=is_dry\n",
    ")\n",
    "\n",
    "train_accA = calc_accuracy(train_loader, modelA, device,is_dry=is_dry)\n",
    "val_accA   = calc_accuracy(val_loader, modelA, device,is_dry=is_dry)\n",
    "test_accA  = calc_accuracy(test_loader, modelA, device,is_dry=is_dry)\n",
    "print(f\"[Full Fine-Tuning] Time: {elapsedA:.2f}s, TrainAcc={train_accA*100:.2f}%, ValAcc={val_accA*100:.2f}%, TestAcc={test_accA*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c072e-ff38-4986-8285-bf046200dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval_helper import * \n",
    "#path_to_partial path_to_lora\n",
    "train_run_label = \"temp_full\"\n",
    "#elapsedA, ft_train_losses, ft_val_accs, train_acc_l \n",
    "#save_everything(path_to_full, train_run_label, elapsedA, \n",
    "#                 ft_train_losses,train_acc_l, ft_val_accs,train_accA,\n",
    "#                 val_accA,test_accA,modelA)\n",
    "\n",
    "save_everything(path_to_save_folder=path_to_full,\n",
    "                 train_run_label=train_run_label,\n",
    "                 elapsed=elapsedA,\n",
    "                 train_losses=ft_train_losses,\n",
    "                 train_acc=train_acc_l,\n",
    "                 val_accs=ft_val_accs,\n",
    "                 train_acc_complete=train_accA,\n",
    "                 val_acc_complete=val_accA,\n",
    "                 test_acc_complete=test_accA,\n",
    "                 model=modelA)\n",
    "\"\"\"\n",
    "save_everything(path_to_save_folder=,\n",
    "                 train_run_label=,\n",
    "                 elapsed=,\n",
    "                 train_losses=,\n",
    "                 train_acc=,\n",
    "                 val_accs=,\n",
    "                 train_acc_complete=,\n",
    "                 val_acc_complete=,\n",
    "                 test_acc_complete=,\n",
    "                 model=)\n",
    "                 \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f381679a-8775-434b-bf29-e3ca5b1a5bc7",
   "metadata": {},
   "source": [
    "## Strategy B LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36d4eb-82a4-4cab-96c7-9b00d36cd9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815cb7b-433a-4911-a499-adec7b82a9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33ba74-36e3-4ffe-aac2-18487d7f6643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adfa923-2d0d-4eac-8014-9a226bd63aac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "from src.lora_model import *\n",
    "from src.lora_train import train_model_lora\n",
    "import copy\n",
    "import time\n",
    "is_dry=False\n",
    "print(\"=== Strategy B: LoRA ===\")\n",
    "##6 Epochs\n",
    "modelB = copy.deepcopy(model)  # Duplicate the base model\n",
    "replace_modules_with_lora(modelB, rank=16, alpha=32)  # Replace layers\n",
    "modelB.to(device)\n",
    "elapsedB, lora_train_losses, lora_val_accs,train_acc_l = train_model_lora(\n",
    "    modelB, train_loader, val_loader, device, epochs=6, lr=1e-4, log_grad_norms=True,is_dry=is_dry\n",
    ")\n",
    "\n",
    "def calc_accuracy_full(loader, model, device, max_batches=None,is_dry=False):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    #for i, (input_ids, attention_mask, y_batch) in enumerate(loader):\n",
    "    for i, instance in enumerate(train_loader):\n",
    "        input_ids = instance['input_ids']\n",
    "        attention_mask = instance['attention_mask']\n",
    "        y_batch = instance['labels']\n",
    "        #-\n",
    "        if max_batches and (i+1) > max_batches:\n",
    "            break\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = forward_for_classification(model, input_ids, attention_mask, device)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total   += y_batch.size(0)\n",
    "        if is_dry:\n",
    "            break\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "train_accB = calc_accuracy_full(train_loader, modelB, device,is_dry=is_dry)\n",
    "val_accB   = calc_accuracy_full(val_loader, modelB, device,is_dry=is_dry)\n",
    "test_accB  = calc_accuracy_full(test_loader, modelB, device,is_dry=is_dry)\n",
    "print(f\"[LoRA] Training Time: {elapsedB:.2f}s\")\n",
    "print(f\"[LoRA] TrainAcc={train_accB*100:.2f}%, ValAcc={val_accB*100:.2f}%, TestAcc={test_accB*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51451edb-8124-491f-8b4f-2819eecc41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval_helper import * \n",
    "#path_to_partial path_to_lora\n",
    "train_run_label = \"lora_ep6_saveUpdate2\"\n",
    "\"\"\"\n",
    "save_everything(path_to_lora, train_run_label, elapsedB, lora_train_losses,\n",
    "                lora_val_accs,\n",
    "                train_accB,\n",
    "                val_accB,test_accB,\n",
    "                modelB\n",
    "               )\n",
    "\"\"\"\n",
    "save_everything(path_to_save_folder=path_to_lora,\n",
    "                 train_run_label=train_run_label,\n",
    "                 elapsed=elapsedB,\n",
    "                 train_losses=lora_train_losses,\n",
    "                 train_acc=train_acc_l,\n",
    "                 val_accs=lora_val_accs,\n",
    "                 train_acc_complete=train_accB,\n",
    "                 val_acc_complete=val_accB,\n",
    "                 test_acc_complete=test_accB,\n",
    "                 model=modelB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473cff1-4fed-4d24-a37e-a5f337530b94",
   "metadata": {},
   "source": [
    " #### B.3: Advanced Metrics (Precision, Recall, F1)\n",
    "\n",
    " We'll calculate a more comprehensive set of metrics on the LoRA model to evaluate performance beyond accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b3ce5-5e89-4be5-b3b5-59b68e8f5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from src.lora_helper import advanced_metrics\n",
    "accB, precB, recB, f1B = advanced_metrics(test_loader, modelB, device)\n",
    "print(f\"[LoRA Advanced Metrics on Test] Accuracy={accB*100:.2f}%, Precision={precB*100:.2f}%, Recall={recB*100:.2f}%, F1={f1B*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055bdcb-e316-4877-9e7a-82ea24421213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da75dd1-da1e-42f4-ab0c-851d117276f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c35b3a-d7af-4fda-a3dc-f91c9a2b4aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
