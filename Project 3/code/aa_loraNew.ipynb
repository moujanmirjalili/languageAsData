{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e2586f-e204-4df2-808d-137e56425598",
   "metadata": {},
   "source": [
    "# Lora \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0040344-9dfe-46d3-b1c3-a199b19348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0dc8e5a-cb27-4aff-a0a0-484cdb09f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "import os\n",
    "if DEBUG: \n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "data_path = \"content\"\n",
    "import torch\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time, math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be5cbfb-b3dc-4cb0-9c45-45ad340b54ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Index(['Text', 'Label'], dtype='object')\n",
      "Max Length is:  65\n",
      "Index(['Text', 'Label'], dtype='object')\n",
      "Max Length is:  65\n",
      "Index(['Text', 'Label'], dtype='object')\n",
      "Max Length is:  65\n",
      "Max Length:  65\n",
      "Number of training batches: 3573, Number of validation batches: 149\n"
     ]
    }
   ],
   "source": [
    "from src.dataset_loader import get_enc_dataset\n",
    "# Set model name\n",
    "from transformers import GPT2Tokenizer\n",
    "model_name = \"gpt2\"\n",
    "path_to_save_folder = \"model\"\n",
    "path_to_lora = os.path.join(path_to_save_folder,\"lora\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if DEBUG:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Load GPT-2 Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# Add a separate pad_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#if tokenizer.pad_token is None:\n",
    "#    # Use '<|PAD|>' as the padding token\n",
    "#    tokenizer.add_special_tokens({'pad_token': '<|PAD|>'})\n",
    "#    pad_token_id = tokenizer.pad_token_id\n",
    "#    print(\"Added new pad_token '<|PAD|>' with ID:\", pad_token_id)\n",
    "batch_size = 16\n",
    "max_length=None#128\n",
    "train_dataset, val_dataset,test_dataset,train_loader,val_loader,test_loader, pad_token_id = get_enc_dataset(data_path,\n",
    "                                                                                   tokenizer,\n",
    "                                                                                   batch_size=batch_size,\n",
    "                                                                                   max_length = max_length\n",
    "                                                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06413808-f53c-4b95-a801-3eb2056d5da5",
   "metadata": {},
   "source": [
    "## Load Petrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3bf22-83a0-49a5-9ca0-2e7c48700c2f",
   "metadata": {},
   "source": [
    "### Test bevor Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3efbf1-90fe-44b5-a2ca-49dfaf4511c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd756a8-c293-4d59-adf6-0ce138924fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 02:32:17.554539: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-18 02:32:17.571832: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737163937.593706   94657 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737163937.600419   94657 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-18 02:32:17.622355: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Accuracies -> Train: 44.38%, Val: 51.88%, Test: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2ForSequenceClassification, GPT2Config\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "from src.lora_helper import calc_accuracy,forward_for_classification\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "num_labels = 2\n",
    "model_config = GPT2Config.from_pretrained(model_name, num_labels=num_labels)\n",
    "model = GPT2ForSequenceClassification.from_pretrained(model_name, config=model_config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id \n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "init_train_acc = calc_accuracy(train_loader, model, device, max_batches=10)\n",
    "init_val_acc   = calc_accuracy(val_loader, model, device, max_batches=10)\n",
    "#init_test_acc  = calc_accuracy(test_loader, model, device, max_batches=10)\n",
    "init_test_acc = 0.0\n",
    "print(f\"Initial Accuracies -> Train: {init_train_acc*100:.2f}%, Val: {init_val_acc*100:.2f}%, Test: {init_test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bce414-0a2e-4ea2-8330-e3c5115e7de3",
   "metadata": {},
   "source": [
    "## Different Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46cdb4-15b3-433d-ba70-5c1c3284d723",
   "metadata": {},
   "source": [
    "### Strategy A: Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ebd87f1-7a3f-441b-a104-c10ad85fcf68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Strategy A: Full Fine-Tuning ===\n",
      "[Full Fine-Tuning] Trainable Params: 124441344\n",
      "[Full Fine-Tuning] Initial Train Acc (first 10 batches): 68.75%\n",
      "Epoch 1/6, step 500/3573, loss = 0.5011\n",
      "Epoch 1/6, step 1000/3573, loss = 0.2666\n",
      "Epoch 1/6, step 1500/3573, loss = 0.4148\n",
      "Epoch 1/6, step 2000/3573, loss = 0.4052\n",
      "Epoch 1/6, step 2500/3573, loss = 0.1949\n",
      "Epoch 1/6, step 3000/3573, loss = 0.1800\n",
      "Epoch 1/6, step 3500/3573, loss = 0.2745\n",
      "Epoch 1/6, step 3570/3573, loss = 0.2524Epoch=1, Loss=0.2711, ValAcc=93.75% TrainAcc=88.75%\n",
      "Epoch 2/6, step 500/3573, loss = 0.0714\n",
      "Epoch 2/6, step 1000/3573, loss = 0.0246\n",
      "Epoch 2/6, step 1500/3573, loss = 0.0512\n",
      "Epoch 2/6, step 2000/3573, loss = 0.0820\n",
      "Epoch 2/6, step 2500/3573, loss = 0.1165\n",
      "Epoch 2/6, step 3000/3573, loss = 0.0377\n",
      "Epoch 2/6, step 3500/3573, loss = 0.1046\n",
      "Epoch 2/6, step 3570/3573, loss = 0.3488Epoch=2, Loss=0.1535, ValAcc=93.87% TrainAcc=94.33%\n",
      "Epoch 3/6, step 500/3573, loss = 0.0888\n",
      "Epoch 3/6, step 1000/3573, loss = 0.1622\n",
      "Epoch 3/6, step 1500/3573, loss = 0.0302\n",
      "Epoch 3/6, step 2000/3573, loss = 0.1854\n",
      "Epoch 3/6, step 2500/3573, loss = 0.1694\n",
      "Epoch 3/6, step 3000/3573, loss = 0.0636\n",
      "Epoch 3/6, step 3500/3573, loss = 0.0197\n",
      "Epoch 3/6, step 3570/3573, loss = 0.0199Epoch=3, Loss=0.1046, ValAcc=93.92% TrainAcc=96.29%\n",
      "Epoch 4/6, step 500/3573, loss = 0.0232\n",
      "Epoch 4/6, step 1000/3573, loss = 0.0414\n",
      "Epoch 4/6, step 1500/3573, loss = 0.0067\n",
      "Epoch 4/6, step 2000/3573, loss = 0.0542\n",
      "Epoch 4/6, step 2500/3573, loss = 0.0051\n",
      "Epoch 4/6, step 3000/3573, loss = 0.1794\n",
      "Epoch 4/6, step 3500/3573, loss = 0.0182\n",
      "Epoch 4/6, step 3570/3573, loss = 0.0127Epoch=4, Loss=0.0762, ValAcc=93.33% TrainAcc=97.27%\n",
      "Epoch 5/6, step 500/3573, loss = 0.0063\n",
      "Epoch 5/6, step 1000/3573, loss = 0.0105\n",
      "Epoch 5/6, step 1500/3573, loss = 0.0054\n",
      "Epoch 5/6, step 2000/3573, loss = 0.0645\n",
      "Epoch 5/6, step 2500/3573, loss = 0.0132\n",
      "Epoch 5/6, step 3000/3573, loss = 0.0350\n",
      "Epoch 5/6, step 3500/3573, loss = 0.0108\n",
      "Epoch 5/6, step 3570/3573, loss = 0.0551Epoch=5, Loss=0.0587, ValAcc=93.75% TrainAcc=97.84%\n",
      "Epoch 6/6, step 500/3573, loss = 0.0214\n",
      "Epoch 6/6, step 1000/3573, loss = 0.0020\n",
      "Epoch 6/6, step 1500/3573, loss = 0.0019\n",
      "Epoch 6/6, step 2000/3573, loss = 0.0742\n",
      "Epoch 6/6, step 2500/3573, loss = 0.0680\n",
      "Epoch 6/6, step 3000/3573, loss = 0.0023\n",
      "Epoch 6/6, step 3500/3573, loss = 0.0843\n",
      "Epoch 6/6, step 3570/3573, loss = 0.0058Epoch=6, Loss=0.0464, ValAcc=94.00% TrainAcc=98.37%\n",
      "[Full Fine-Tuning] Time: 2741.62s, TrainAcc=99.34%, ValAcc=94.00%, TestAcc=89.79%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from src.lora_train import train_model_full_finetune\n",
    "import copy\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "is_dry=False\n",
    "print(\"=== Strategy A: Full Fine-Tuning ===\")\n",
    "modelA = copy.deepcopy(model)\n",
    "full_tune_params = sum(p.numel() for p in modelA.parameters() if p.requires_grad)\n",
    "print(f\"[Full Fine-Tuning] Trainable Params: {full_tune_params}\")\n",
    "\n",
    "\n",
    "init_accA = calc_accuracy(train_loader, modelA, device, max_batches=1)\n",
    "print(f\"[Full Fine-Tuning] Initial Train Acc (first 10 batches): {init_accA*100:.2f}%\")\n",
    "\n",
    "#elapsedB, lora_train_losses, lora_val_accs,train_acc_l \n",
    "elapsedA, ft_train_losses, ft_val_accs, train_acc_l  = train_model_full_finetune(\n",
    "    modelA, train_loader, val_loader, device, epochs=6, lr=5e-5,is_dry=is_dry\n",
    ")\n",
    "\n",
    "train_accA = calc_accuracy(train_loader, modelA, device,is_dry=is_dry)\n",
    "val_accA   = calc_accuracy(val_loader, modelA, device,is_dry=is_dry)\n",
    "test_accA  = calc_accuracy(test_loader, modelA, device,is_dry=is_dry)\n",
    "print(f\"[Full Fine-Tuning] Time: {elapsedA:.2f}s, TrainAcc={train_accA*100:.2f}%, ValAcc={val_accA*100:.2f}%, TestAcc={test_accA*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d2c072e-ff38-4986-8285-bf046200dfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything saved at\n",
      " 02:19:24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nsave_everything(path_to_save_folder=,\\n                 train_run_label=,\\n                 elapsed=,\\n                 train_losses=,\\n                 train_acc=,\\n                 val_accs=,\\n                 train_acc_complete=,\\n                 val_acc_complete=,\\n                 test_acc_complete=,\\n                 model=)\\n                 '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.eval_helper import * \n",
    "#path_to_partial path_to_lora\n",
    "train_run_label = \"full_second_run_6ep_saveUpdate\"\n",
    "#elapsedA, ft_train_losses, ft_val_accs, train_acc_l \n",
    "#save_everything(path_to_full, train_run_label, elapsedA, \n",
    "#                 ft_train_losses,train_acc_l, ft_val_accs,train_accA,\n",
    "#                 val_accA,test_accA,modelA)\n",
    "\n",
    "save_everything(path_to_save_folder=path_to_full,\n",
    "                 train_run_label=train_run_label,\n",
    "                 elapsed=elapsedA,\n",
    "                 train_losses=ft_train_losses,\n",
    "                 train_acc=train_acc_l,\n",
    "                 val_accs=ft_val_accs,\n",
    "                 train_acc_complete=train_accA,\n",
    "                 val_acc_complete=val_accA,\n",
    "                 test_acc_complete=test_accA,\n",
    "                 model=modelA)\n",
    "\"\"\"\n",
    "save_everything(path_to_save_folder=,\n",
    "                 train_run_label=,\n",
    "                 elapsed=,\n",
    "                 train_losses=,\n",
    "                 train_acc=,\n",
    "                 val_accs=,\n",
    "                 train_acc_complete=,\n",
    "                 val_acc_complete=,\n",
    "                 test_acc_complete=,\n",
    "                 model=)\n",
    "                 \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f381679a-8775-434b-bf29-e3ca5b1a5bc7",
   "metadata": {},
   "source": [
    "## Strategy B LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36d4eb-82a4-4cab-96c7-9b00d36cd9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815cb7b-433a-4911-a499-adec7b82a9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33ba74-36e3-4ffe-aac2-18487d7f6643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2adfa923-2d0d-4eac-8014-9a226bd63aac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Strategy B: LoRA ===\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=768, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=768, out_dim=3072, rank=16, alpha=32\n",
      "[LoRALayer] in_dim=3072, out_dim=768, rank=16, alpha=32\n",
      "Trainable params: 1769472 / Total params: 126210816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/vast-react/home/joris.hellwig/u13685/jupyterhub-gwdg/ex3/src/lora_train.py:96: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()\n",
      "/mnt/vast-react/home/joris.hellwig/u13685/jupyterhub-gwdg/ex3/src/lora_train.py:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6, step 500/3573, loss = 0.8793\n",
      "Epoch 1/6, step 1000/3573, loss = 0.6966\n",
      "Epoch 1/6, step 1500/3573, loss = 0.5955\n",
      "Epoch 1/6, step 2000/3573, loss = 0.5622\n",
      "Epoch 1/6, step 2500/3573, loss = 0.5112\n",
      "Epoch 1/6, step 3000/3573, loss = 0.5744\n",
      "Epoch 1/6, step 3500/3573, loss = 0.5940\n",
      "Epoch 1/6, step 3570/3573, loss = 0.4414\n",
      "Epoch=1, Loss=0.5215, TrainAcc=73.84%,ValAcc=81.07%\n",
      "\n",
      "Epoch 2/6, step 500/3573, loss = 0.3120\n",
      "Epoch 2/6, step 1000/3573, loss = 0.3897\n",
      "Epoch 2/6, step 1500/3573, loss = 0.4436\n",
      "Epoch 2/6, step 2000/3573, loss = 0.3591\n",
      "Epoch 2/6, step 2500/3573, loss = 0.5414\n",
      "Epoch 2/6, step 3000/3573, loss = 0.7699\n",
      "Epoch 2/6, step 3500/3573, loss = 0.3334\n",
      "Epoch 2/6, step 3570/3573, loss = 0.2813\n",
      "Epoch=2, Loss=0.4435, TrainAcc=79.27%,ValAcc=82.42%\n",
      "\n",
      "Epoch 3/6, step 500/3573, loss = 0.4431\n",
      "Epoch 3/6, step 1000/3573, loss = 0.3789\n",
      "Epoch 3/6, step 1500/3573, loss = 0.3518\n",
      "Epoch 3/6, step 2000/3573, loss = 0.2178\n",
      "Epoch 3/6, step 2500/3573, loss = 0.4486\n",
      "Epoch 3/6, step 3000/3573, loss = 0.3750\n",
      "Epoch 3/6, step 3500/3573, loss = 0.3241\n",
      "Epoch 3/6, step 3570/3573, loss = 0.5930\n",
      "Epoch=3, Loss=0.4172, TrainAcc=80.89%,ValAcc=83.84%\n",
      "\n",
      "Epoch 4/6, step 500/3573, loss = 0.5129\n",
      "Epoch 4/6, step 1000/3573, loss = 0.4550\n",
      "Epoch 4/6, step 1500/3573, loss = 0.4740\n",
      "Epoch 4/6, step 2000/3573, loss = 0.2707\n",
      "Epoch 4/6, step 2500/3573, loss = 0.3850\n",
      "Epoch 4/6, step 3000/3573, loss = 0.3755\n",
      "Epoch 4/6, step 3500/3573, loss = 0.7937\n",
      "Epoch 4/6, step 3570/3573, loss = 0.4742\n",
      "Epoch=4, Loss=0.4155, TrainAcc=80.97%,ValAcc=81.33%\n",
      "\n",
      "Epoch 5/6, step 500/3573, loss = 0.2897\n",
      "Epoch 5/6, step 1000/3573, loss = 0.3973\n",
      "Epoch 5/6, step 1500/3573, loss = 0.5628\n",
      "Epoch 5/6, step 2000/3573, loss = 0.6099\n",
      "Epoch 5/6, step 2500/3573, loss = 0.2644\n",
      "Epoch 5/6, step 3000/3573, loss = 0.2563\n",
      "Epoch 5/6, step 3500/3573, loss = 0.1598\n",
      "Epoch 5/6, step 3570/3573, loss = 0.3993\n",
      "Epoch=5, Loss=0.4206, TrainAcc=80.78%,ValAcc=84.26%\n",
      "\n",
      "Epoch 6/6, step 500/3573, loss = 0.3192\n",
      "Epoch 6/6, step 1000/3573, loss = 0.3776\n",
      "Epoch 6/6, step 1500/3573, loss = 0.3294\n",
      "Epoch 6/6, step 2000/3573, loss = 0.2256\n",
      "Epoch 6/6, step 2500/3573, loss = 0.6500\n",
      "Epoch 6/6, step 3000/3573, loss = 0.4978\n",
      "Epoch 6/6, step 3500/3573, loss = 0.2910\n",
      "Epoch 6/6, step 3570/3573, loss = 0.1900\n",
      "Epoch=6, Loss=0.3995, TrainAcc=81.90%,ValAcc=84.68%\n",
      "\n",
      "[LoRA] Training Time: 905.94s\n",
      "[LoRA] TrainAcc=84.83%, ValAcc=84.83%, TestAcc=84.82%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from src.lora_model import *\n",
    "from src.lora_train import train_model_lora\n",
    "import copy\n",
    "import time\n",
    "is_dry=False\n",
    "print(\"=== Strategy B: LoRA ===\")\n",
    "##6 Epochs\n",
    "modelB = copy.deepcopy(model)  # Duplicate the base model\n",
    "replace_modules_with_lora(modelB, rank=16, alpha=32)  # Replace layers\n",
    "modelB.to(device)\n",
    "elapsedB, lora_train_losses, lora_val_accs,train_acc_l = train_model_lora(\n",
    "    modelB, train_loader, val_loader, device, epochs=6, lr=1e-4, log_grad_norms=True,is_dry=is_dry\n",
    ")\n",
    "\n",
    "def calc_accuracy_full(loader, model, device, max_batches=None,is_dry=False):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    #for i, (input_ids, attention_mask, y_batch) in enumerate(loader):\n",
    "    for i, instance in enumerate(train_loader):\n",
    "        input_ids = instance['input_ids']\n",
    "        attention_mask = instance['attention_mask']\n",
    "        y_batch = instance['labels']\n",
    "        #-\n",
    "        if max_batches and (i+1) > max_batches:\n",
    "            break\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = forward_for_classification(model, input_ids, attention_mask, device)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total   += y_batch.size(0)\n",
    "        if is_dry:\n",
    "            break\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "train_accB = calc_accuracy_full(train_loader, modelB, device,is_dry=is_dry)\n",
    "val_accB   = calc_accuracy_full(val_loader, modelB, device,is_dry=is_dry)\n",
    "test_accB  = calc_accuracy_full(test_loader, modelB, device,is_dry=is_dry)\n",
    "print(f\"[LoRA] Training Time: {elapsedB:.2f}s\")\n",
    "print(f\"[LoRA] TrainAcc={train_accB*100:.2f}%, ValAcc={val_accB*100:.2f}%, TestAcc={test_accB*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51451edb-8124-491f-8b4f-2819eecc41d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything saved at\n",
      " 02:38:45\n"
     ]
    }
   ],
   "source": [
    "from src.eval_helper import * \n",
    "#path_to_partial path_to_lora\n",
    "train_run_label = \"lora_ep6_saveUpdate2_secondRUn\"\n",
    "\"\"\"\n",
    "save_everything(path_to_lora, train_run_label, elapsedB, lora_train_losses,\n",
    "                lora_val_accs,\n",
    "                train_accB,\n",
    "                val_accB,test_accB,\n",
    "                modelB\n",
    "               )\n",
    "\"\"\"\n",
    "save_everything(path_to_save_folder=path_to_lora,\n",
    "                 train_run_label=train_run_label,\n",
    "                 elapsed=elapsedB,\n",
    "                 train_losses=lora_train_losses,\n",
    "                 train_acc=train_acc_l,\n",
    "                 val_accs=lora_val_accs,\n",
    "                 train_acc_complete=train_accB,\n",
    "                 val_acc_complete=val_accB,\n",
    "                 test_acc_complete=test_accB,\n",
    "                 model=modelB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473cff1-4fed-4d24-a37e-a5f337530b94",
   "metadata": {},
   "source": [
    " #### B.3: Advanced Metrics (Precision, Recall, F1)\n",
    "\n",
    " We'll calculate a more comprehensive set of metrics on the LoRA model to evaluate performance beyond accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c7b3ce5-5e89-4be5-b3b5-59b68e8f5d9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlora_helper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m advanced_metrics\n\u001b[0;32m----> 3\u001b[0m accB, precB, recB, f1B \u001b[38;5;241m=\u001b[39m \u001b[43madvanced_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LoRA Advanced Metrics on Test] Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccB\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, Precision=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecB\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, Recall=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecB\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1B\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/vast-react/home/joris.hellwig/u13685/jupyterhub-gwdg/ex3/src/lora_helper.py:54\u001b[0m, in \u001b[0;36madvanced_metrics\u001b[0;34m(loader, model, device)\u001b[0m\n\u001b[1;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     53\u001b[0m preds_list, labels_list \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids, attention_mask, y_batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     55\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     56\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from src.lora_helper import advanced_metrics\n",
    "accB, precB, recB, f1B = advanced_metrics(test_loader, modelB, device)\n",
    "print(f\"[LoRA Advanced Metrics on Test] Accuracy={accB*100:.2f}%, Precision={precB*100:.2f}%, Recall={recB*100:.2f}%, F1={f1B*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055bdcb-e316-4877-9e7a-82ea24421213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da75dd1-da1e-42f4-ab0c-851d117276f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c35b3a-d7af-4fda-a3dc-f91c9a2b4aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
