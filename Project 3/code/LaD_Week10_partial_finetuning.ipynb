{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPT-2 for Text Classification\n",
    "\n",
    "For demonstration purposes and due to resource limitations, we have chosen GPT-2 as our model. You should be able to apply the same methods to larger models which will likely yield better performance.\n",
    "\n",
    "We build on the notebook by Sebastian Raschka [3] Sebastian Raschka's LLMs Course: [GitHub - rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Environment Setup\n",
    "\n",
    "First, we need to install and import the required libraries. Ensure that all necessary packages are installed; otherwise, please use `pip install` to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!$ pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org <package_name>\n",
    "#!pip install torch --user\n",
    "#!pip uninstall matplotlib -y\n",
    "#!pip uninstall pillow -y \n",
    "#!pip uninstall numpy -y \n",
    "#!pip uninstall datasets -y -v \n",
    "#!pip install matplotlib\n",
    "#!pip install pillow\n",
    "#!pip install numpy\n",
    "#!pip install datasets --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "#!pip install datasets --user\n",
    "#!pip install evaluate --user\n",
    "#!pip3 install Cython --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.0\n",
      "numpy version: 2.0.2\n",
      "torch version: 2.5.1\n",
      "transformers version: 4.48.0\n",
      "datasets version: 3.2.0\n",
      "pandas version: 2.2.3\n",
      "evaluate version: 0.4.3\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "from importlib.metadata import version \n",
    "import sys \n",
    "\n",
    "pkgs = [ \n",
    "    \"matplotlib\", \n",
    "    \"numpy\", \n",
    "    \"torch\", \n",
    "    \"transformers\",   \n",
    "    \"datasets\",        \n",
    "    \"pandas\",          \n",
    "    \"evaluate\",        \n",
    "] \n",
    "\n",
    "for p in pkgs: \n",
    "    try: \n",
    "        print(f\"{p} version: {version(p)}\") \n",
    "    except: \n",
    "        print(f\"{p} is not installed. Please use `pip install {p}` to install.\") \n",
    "        sys.exit(1) \n",
    "data_path = \"content\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Set model name\n",
    "from transformers import GPT2Tokenizer\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Load GPT-2 Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Loading\n",
    "\n",
    "We will use the SMS Spam Collection dataset. This dataset contains SMS messages labeled as \"spam\" or \"ham\". We will download, preprocess the data, and split it into training, validation, and test sets.\n",
    "\n",
    "If there is an error downloading, please try to download it manually.\n",
    "Download link: https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\n",
    "\n",
    "The code below creates balanced classes of ham and spam. **Reflect on the advantages and disadvantages of this step.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset label counts:\n",
      "Label\n",
      "1        37569\n",
      "0        29780\n",
      "label        1\n",
      "Name: count, dtype: int64\n",
      "(67350, 2)\n",
      "\n",
      "Balanced dataset label distribution:\n",
      "Label\n",
      "1    29780\n",
      "0    29780\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training set size: 67350, Validation set size: 873, Test set size: 1822\n"
     ]
    }
   ],
   "source": [
    "from src.fine_helper import load_complete_dataframe\n",
    "is_balanced = True\n",
    "train_df, validation_df, test_df = load_complete_dataframe(data_path,is_balanced=is_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                               Text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "Original dataset label counts:\n",
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balanced dataset label distribution:\n",
      "Label\n",
      "0    747\n",
      "1    747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training set size: 1045, Validation set size: 149, Test set size: 300\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import urllib.request \n",
    "import zipfile \n",
    "import os \n",
    "from pathlib import Path \n",
    "import pandas as pd \n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\" \n",
    "zip_path = \"sms_spam_collection.zip\" \n",
    "extracted_path = \"sms_spam_collection\" \n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\" \n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path): \n",
    "    if data_file_path.exists(): \n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\") \n",
    "        return \n",
    "\n",
    "    with urllib.request.urlopen(url) as response: \n",
    "        with open(zip_path, \"wb\") as out_file: \n",
    "            out_file.write(response.read()) \n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref: \n",
    "        zip_ref.extractall(extracted_path) \n",
    "\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\" \n",
    "    os.rename(original_file_path, data_file_path) \n",
    "    print(f\"File downloaded and saved as {data_file_path}\") \n",
    "\n",
    "#download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path) \n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"]) \n",
    "print(df.head()) \n",
    "print(\"Original dataset label counts:\") \n",
    "print(df[\"Label\"].value_counts()) \n",
    "\n",
    "def create_balanced_dataset(df): \n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0] \n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123) \n",
    "    return pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]], ignore_index=True) \n",
    "\n",
    "balanced_df = create_balanced_dataset(df) \n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1}) \n",
    "print(\"\\nBalanced dataset label distribution:\") \n",
    "print(balanced_df[\"Label\"].value_counts()) \n",
    "\n",
    "def random_split(df, train_frac, validation_frac): \n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True) \n",
    "    train_end = int(len(df) * train_frac) \n",
    "    validation_end = train_end + int(len(df) * validation_frac) \n",
    "    train_df = df[:train_end] \n",
    "    validation_df = df[train_end:validation_end] \n",
    "    test_df = df[validation_end:] \n",
    "    return train_df, validation_df, test_df \n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1) \n",
    "train_df.to_csv(\"train.csv\", index=None) \n",
    "validation_df.to_csv(\"validation.csv\", index=None) \n",
    "test_df.to_csv(\"test.csv\", index=None) \n",
    "\n",
    "print(f\"\\nTraining set size: {len(train_df)}, Validation set size: {len(validation_df)}, Test set size: {len(test_df)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Loaders\n",
    "\n",
    "Next, we need to encode and pad the SMS text to ensure consistent input length in each batch. Here, we use `<|PAD|>` as the padding token and build the attention_mask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new pad_token '<|PAD|>' with ID: 50257\n",
      "Index(['Text', 'Label'], dtype='object')\n",
      "Max Length is:  65\n",
      "Index(['Text', 'Label'], dtype='object')\n",
      "Max Length is:  65\n",
      "Index(['Text', 'Label'], dtype='object')\n",
      "Max Length is:  65\n",
      "Max Length:  65\n",
      "Number of training batches: 7147, Number of validation batches: 298\n"
     ]
    }
   ],
   "source": [
    "from src.dataset_loader import get_enc_dataset\n",
    "train_dataset, val_dataset,test_dataset,train_loader,val_loader,test_loader, pad_token_id = get_enc_dataset(data_path,tokenizer,is_log = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Old Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new pad_token '<|PAD|>' with ID: 50257\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mnum_workers, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m#test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of training batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Number of validation batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Number of test batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mtest_loader\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "import pandas as pd\n",
    "import os \n",
    "# Set model name\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Load GPT-2 Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add a separate pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    # Use '<|PAD|>' as the padding token\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|PAD|>'})\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    print(\"Added new pad_token '<|PAD|>' with ID:\", pad_token_id)\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=None):\n",
    "        self.data = pd.read_csv(csv_file,sep=\"\\t\")\n",
    "        \n",
    "        # Verify necessary columns in the CSV file\n",
    "        required_columns = [\"sentence\", \"label\"]\n",
    "        if not all(col in self.data.columns for col in required_columns):\n",
    "            raise ValueError(f\"CSV file must contain the following columns: {required_columns}\")\n",
    "        \n",
    "        # Ensure labels are of integer type\n",
    "        self.data[\"label\"] = self.data[\"label\"].astype(int)\n",
    "        \n",
    "        self.texts = self.data[\"sentence\"].tolist()\n",
    "        self.labels = self.data[\"label\"].tolist()\n",
    "        \n",
    "        # Set pad_token_id, if not specified, use tokenizer's pad_token_id\n",
    "        self.pad_token_id = pad_token_id if pad_token_id is not None else tokenizer.pad_token_id\n",
    "        \n",
    "        # Encode texts\n",
    "        self.encoded_texts = []\n",
    "        for text in self.texts:\n",
    "            try:\n",
    "                encoded = tokenizer.encode(text, add_special_tokens=True)\n",
    "                self.encoded_texts.append(encoded)\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error encoding text: {text[:50]}...\") from e\n",
    "        \n",
    "        # Dynamically calculate max_length, or use specified max_length\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # Truncate sequences longer than max_length\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length] for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "        \n",
    "        # Pad all sequences and generate attention_mask\n",
    "        self.padded_texts = []\n",
    "        self.attention_masks = []\n",
    "        for enc in self.encoded_texts:\n",
    "            enc = enc[:self.max_length]\n",
    "            attention_mask = [1] * len(enc)\n",
    "            \n",
    "            pad_len = self.max_length - len(enc)\n",
    "            if pad_len > 0:\n",
    "                enc += [self.pad_token_id] * pad_len\n",
    "                attention_mask += [0] * pad_len\n",
    "            \n",
    "            self.padded_texts.append(enc)\n",
    "            self.attention_masks.append(attention_mask)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.padded_texts[idx], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(self.attention_masks[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        text = self.texts[idx]\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": label,\n",
    "            \"text\": text\n",
    "        }\n",
    "    \n",
    "    def _longest_encoded_length(self):\n",
    "        return max(len(encoded_text) for encoded_text in self.encoded_texts)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SpamDataset(os.path.join(data_path,\"train.tsv\"), tokenizer, pad_token_id=pad_token_id)\n",
    "val_dataset = SpamDataset(os.path.join(data_path,\"dev.tsv\"), tokenizer, max_length=train_dataset.max_length, pad_token_id=pad_token_id)\n",
    "#test_dataset = SpamDataset(os.path.join(data_path,\"test.tsv\"), tokenizer, max_length=train_dataset.max_length, pad_token_id=pad_token_id)\n",
    "\n",
    "# Set DataLoader parameters\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}, Number of validation batches: {len(val_loader)}, Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Computational Device\n",
    "Before loading and training the model, we need to define the computational device (CPU or GPU). If a GPU is available, it will be preferred to accelerate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    device = torch.device(\"cuda\") \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") \n",
    "else: \n",
    "    device = torch.device(\"cpu\") \n",
    "print(f\"Using device: {device}\") \n",
    "#device = torch.device(\"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Model Structure ##\n",
    "Load the pre-trained GPT-2 model and make sure you understand its structure. **What do the abbreviations represent? Which of the layer types have we seen in the lecture?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 12:41:02.277006: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-17 12:41:02.294455: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737114062.316656   25498 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737114062.323453   25498 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-17 12:41:02.345140: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2ForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2ForSequenceClassification\n",
    "pretrained_gpt2_lm = GPT2LMHeadModel.from_pretrained(\"gpt2\") \n",
    "\n",
    "#print(pretrained_gpt2_lm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the structure to the GPT-2 model for classification. **What is the difference?**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "classification_model = GPT2ForSequenceClassification.from_pretrained( \n",
    "    model_name, \n",
    "    num_labels=2, \n",
    "    pad_token_id=tokenizer.pad_token_id \n",
    ").to(device) \n",
    "\n",
    "#print(classification_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Testing\n",
    "\n",
    "At this point, the classification head is untrained, and the results are usually poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Before fine-tuning) Initial prediction of the classification head:\n",
      "torch.Size([1, 65])\n",
      "Postive sample => Prediction: 0\n",
      "torch.Size([1, 65])\n",
      "Negative sample => Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def classify_text(text, model, tokenizer, device, max_length, pad_token_id=50256): \n",
    "    model.eval() \n",
    "    enc = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=max_length) \n",
    "    att_mask = [1]*len(enc) \n",
    "    pad_len = max_length - len(enc) \n",
    "    if pad_len > 0: \n",
    "        enc += [pad_token_id]*pad_len \n",
    "        att_mask += [0]*pad_len \n",
    "     \n",
    "    input_ids = torch.tensor([enc], dtype=torch.long).to(device) \n",
    "    attention_mask = torch.tensor([att_mask], dtype=torch.long).to(device) \n",
    "\n",
    "    with torch.no_grad(): \n",
    "        print(input_ids.shape)\n",
    "        \n",
    "        outputs = model(input_ids)#, attention_mask=attention_mask) \n",
    "        logits = outputs.logits \n",
    "        predicted = torch.argmax(logits, dim=-1).item() \n",
    "    return predicted\n",
    "\n",
    "sample_text_spam = \"Fine\" \n",
    "sample_text_ham = \"Bad\" \n",
    "\n",
    "print(\"(Before fine-tuning) Initial prediction of the classification head:\") \n",
    "print(f\"Postive sample => Prediction: {classify_text(sample_text_spam, classification_model, \n",
    "                                                  tokenizer, device, max_length=65,pad_token_id=pad_token_id-1\n",
    "                                                    )}\") \n",
    "print(f\"Negative sample => Prediction: {classify_text(sample_text_ham, classification_model,\n",
    "                                                 tokenizer, device,max_length=65\n",
    "                                                )}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tuning only the classification layer\n",
    "\n",
    "As a first step, we will freeze the parameters of the GPT2-model and only tune the classification layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of base parameters: 124440576\n",
      "\n",
      "Training only the classification head, trainable parameters: 1536\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We need to add the padding token to the embeddings\n",
    "classification_model.resize_token_embeddings(len(tokenizer)) \n",
    "classification_model.to(device) \n",
    "\n",
    "i =0\n",
    "k=0\n",
    "for param in classification_model.base_model.parameters(): \n",
    "    param.requires_grad = False \n",
    "    i+=param.numel()\n",
    "\n",
    "print(f\"Number of base parameters: {i}\")\n",
    "\n",
    "\n",
    "for param in classification_model.score.parameters(): \n",
    "    param.requires_grad = True \n",
    "    k+=param.numel()\n",
    "\n",
    "print(f\"\\nTraining only the classification head, trainable parameters: {k}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8, step 500/7147, loss = 0.7802\n",
      "Epoch 1/8, step 1000/7147, loss = 0.6003\n",
      "Epoch 1/8, step 1500/7147, loss = 0.6505\n",
      "Epoch 1/8, step 2000/7147, loss = 0.7296\n",
      "Epoch 1/8, step 2500/7147, loss = 0.5789\n",
      "Epoch 1/8, step 3000/7147, loss = 0.5842\n",
      "Epoch 1/8, step 3500/7147, loss = 0.6220\n",
      "Epoch 1/8, step 4000/7147, loss = 0.5829\n",
      "Epoch 1/8, step 4500/7147, loss = 0.5478\n",
      "Epoch 1/8, step 5000/7147, loss = 0.6448\n",
      "Epoch 1/8, step 5500/7147, loss = 0.4478\n",
      "Epoch 1/8, step 6000/7147, loss = 0.4949\n",
      "Epoch 1/8, step 6500/7147, loss = 0.5545\n",
      "Epoch 1/8, step 7000/7147, loss = 0.4498\n",
      "Epoch 1/8, step 7140/7147, loss = 0.5660\n",
      "Epoch 1/8, Average training loss: 0.6997\n",
      "Validation accuracy: 73.90%\n",
      "\n",
      "Epoch 2/8, step 500/7147, loss = 0.5973\n",
      "Epoch 2/8, step 1000/7147, loss = 0.6920\n",
      "Epoch 2/8, step 1500/7147, loss = 0.5373\n",
      "Epoch 2/8, step 2000/7147, loss = 0.6156\n",
      "Epoch 2/8, step 2500/7147, loss = 0.5983\n",
      "Epoch 2/8, step 3000/7147, loss = 0.7882\n",
      "Epoch 2/8, step 3500/7147, loss = 0.7298\n",
      "Epoch 2/8, step 4000/7147, loss = 0.4281\n",
      "Epoch 2/8, step 4500/7147, loss = 0.5111\n",
      "Epoch 2/8, step 5000/7147, loss = 0.6628\n",
      "Epoch 2/8, step 5500/7147, loss = 0.7586\n",
      "Epoch 2/8, step 6000/7147, loss = 0.5780\n",
      "Epoch 2/8, step 6500/7147, loss = 0.3941\n",
      "Epoch 2/8, step 7000/7147, loss = 0.4180\n",
      "Epoch 2/8, step 7140/7147, loss = 0.6163\n",
      "Epoch 2/8, Average training loss: 0.5746\n",
      "Validation accuracy: 77.51%\n",
      "\n",
      "Epoch 3/8, step 500/7147, loss = 0.4853\n",
      "Epoch 3/8, step 1000/7147, loss = 0.4487\n",
      "Epoch 3/8, step 1500/7147, loss = 0.7718\n",
      "Epoch 3/8, step 2000/7147, loss = 0.4852\n",
      "Epoch 3/8, step 2500/7147, loss = 0.4086\n",
      "Epoch 3/8, step 3000/7147, loss = 0.4373\n",
      "Epoch 3/8, step 3500/7147, loss = 0.5284\n",
      "Epoch 3/8, step 4000/7147, loss = 0.6004\n",
      "Epoch 3/8, step 4500/7147, loss = 0.3923\n",
      "Epoch 3/8, step 5000/7147, loss = 0.6118\n",
      "Epoch 3/8, step 5500/7147, loss = 0.6403\n",
      "Epoch 3/8, step 6000/7147, loss = 0.5071\n",
      "Epoch 3/8, step 6500/7147, loss = 0.5236\n",
      "Epoch 3/8, step 7000/7147, loss = 0.4485\n",
      "Epoch 3/8, step 7140/7147, loss = 0.4100\n",
      "Epoch 3/8, Average training loss: 0.5468\n",
      "Validation accuracy: 79.02%\n",
      "\n",
      "Epoch 4/8, step 500/7147, loss = 0.2772\n",
      "Epoch 4/8, step 1000/7147, loss = 0.4807\n",
      "Epoch 4/8, step 1500/7147, loss = 0.4157\n",
      "Epoch 4/8, step 2000/7147, loss = 0.5496\n",
      "Epoch 4/8, step 2500/7147, loss = 0.6396\n",
      "Epoch 4/8, step 3000/7147, loss = 0.6788\n",
      "Epoch 4/8, step 3500/7147, loss = 0.2923\n",
      "Epoch 4/8, step 4000/7147, loss = 0.4076\n",
      "Epoch 4/8, step 4500/7147, loss = 0.4399\n",
      "Epoch 4/8, step 5000/7147, loss = 0.4922\n",
      "Epoch 4/8, step 5500/7147, loss = 0.5011\n",
      "Epoch 4/8, step 6000/7147, loss = 0.3691\n",
      "Epoch 4/8, step 6500/7147, loss = 0.3504\n",
      "Epoch 4/8, step 7000/7147, loss = 0.4634\n",
      "Epoch 4/8, step 7140/7147, loss = 0.4706\n",
      "Epoch 4/8, Average training loss: 0.5309\n",
      "Validation accuracy: 79.56%\n",
      "\n",
      "Epoch 5/8, step 500/7147, loss = 0.5256\n",
      "Epoch 5/8, step 1000/7147, loss = 0.4813\n",
      "Epoch 5/8, step 1500/7147, loss = 0.3488\n",
      "Epoch 5/8, step 2000/7147, loss = 0.6862\n",
      "Epoch 5/8, step 2500/7147, loss = 0.7200\n",
      "Epoch 5/8, step 3000/7147, loss = 0.3896\n",
      "Epoch 5/8, step 3500/7147, loss = 0.4274\n",
      "Epoch 5/8, step 4000/7147, loss = 0.4250\n",
      "Epoch 5/8, step 4500/7147, loss = 0.3077\n",
      "Epoch 5/8, step 5000/7147, loss = 0.5547\n",
      "Epoch 5/8, step 5500/7147, loss = 0.4673\n",
      "Epoch 5/8, step 6000/7147, loss = 0.7604\n",
      "Epoch 5/8, step 6500/7147, loss = 0.5963\n",
      "Epoch 5/8, step 7000/7147, loss = 0.4466\n",
      "Epoch 5/8, step 7140/7147, loss = 0.3896\n",
      "Epoch 5/8, Average training loss: 0.5215\n",
      "Epoch 6/8, step 2000/7147, loss = 0.7436\n",
      "Epoch 6/8, step 2500/7147, loss = 0.4990\n",
      "Epoch 6/8, step 3000/7147, loss = 0.6194\n",
      "Epoch 6/8, step 3500/7147, loss = 0.2609\n",
      "Epoch 6/8, step 4000/7147, loss = 0.5607\n",
      "Epoch 6/8, step 4500/7147, loss = 0.6482\n",
      "Epoch 6/8, step 5000/7147, loss = 0.8585\n",
      "Epoch 6/8, step 5500/7147, loss = 0.5161\n",
      "Epoch 6/8, step 6000/7147, loss = 0.5287\n",
      "Epoch 6/8, step 6500/7147, loss = 0.5230\n",
      "Epoch 6/8, step 7000/7147, loss = 0.3394\n",
      "Epoch 6/8, step 7140/7147, loss = 0.5061\n",
      "Epoch 6/8, Average training loss: 0.5141\n",
      "Validation accuracy: 80.40%\n",
      "\n",
      "Epoch 7/8, step 500/7147, loss = 0.6896\n",
      "Epoch 7/8, step 1000/7147, loss = 0.3596\n",
      "Epoch 7/8, step 1500/7147, loss = 0.4946\n",
      "Epoch 7/8, step 2000/7147, loss = 0.4253\n",
      "Epoch 7/8, step 2500/7147, loss = 0.5561\n",
      "Epoch 7/8, step 3000/7147, loss = 0.4022\n",
      "Epoch 7/8, step 3500/7147, loss = 0.5151\n",
      "Epoch 7/8, step 4000/7147, loss = 0.4688\n",
      "Epoch 7/8, step 4500/7147, loss = 0.6069\n",
      "Epoch 7/8, step 5000/7147, loss = 0.4786\n",
      "Epoch 7/8, step 5500/7147, loss = 0.3027\n",
      "Epoch 7/8, step 6000/7147, loss = 0.6765\n",
      "Epoch 7/8, step 6500/7147, loss = 0.4554\n",
      "Epoch 7/8, step 7000/7147, loss = 0.4437\n",
      "Epoch 7/8, step 7140/7147, loss = 0.4544\n",
      "Epoch 7/8, Average training loss: 0.5093\n",
      "Validation accuracy: 80.65%\n",
      "\n",
      "Epoch 8/8, step 500/7147, loss = 0.7194\n",
      "Epoch 8/8, step 1000/7147, loss = 0.3790\n",
      "Epoch 8/8, step 1500/7147, loss = 0.4478\n",
      "Epoch 8/8, step 2000/7147, loss = 0.4779\n",
      "Epoch 8/8, step 2500/7147, loss = 0.4417\n",
      "Epoch 8/8, step 3000/7147, loss = 0.4091\n",
      "Epoch 8/8, step 3500/7147, loss = 0.5122\n",
      "Epoch 8/8, step 4000/7147, loss = 0.7264\n",
      "Epoch 8/8, step 4500/7147, loss = 0.8481\n",
      "Epoch 8/8, step 5000/7147, loss = 0.6915\n",
      "Epoch 8/8, step 5500/7147, loss = 0.6530\n",
      "Epoch 8/8, step 6000/7147, loss = 0.4348\n",
      "Epoch 8/8, step 6500/7147, loss = 0.5104\n",
      "Epoch 8/8, step 7000/7147, loss = 0.4114\n",
      "Epoch 8/8, step 7140/7147, loss = 0.5753\n",
      "Epoch 8/8, Average training loss: 0.5071\n",
      "Validation accuracy: 80.99%\n",
      "\n",
      "\n",
      "=== Fine-tuning only the classification head completed in 36.42 minutes ===\n",
      "Training accuracy: 80.17%\n",
      "Validation accuracy: 80.99%\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from ult import evaluate_accuracy\n",
    "from src.train import train_head_only\n",
    "\n",
    "\n",
    "start_time_head = time.time() \n",
    "classification_model = train_head_only(classification_model, train_loader, val_loader, device, epochs=8, lr=3e-5) \n",
    "end_time_head = time.time() \n",
    "\n",
    "train_accuracy_head = evaluate_accuracy(classification_model, train_loader, device) \n",
    "val_accuracy_head = evaluate_accuracy(classification_model, val_loader, device) \n",
    "#test_accuracy_head = evaluate_accuracy(classification_model, test_loader, device) \n",
    "finetune_head_time = (end_time_head - start_time_head) / 60 \n",
    "\n",
    "print(f\"\\n=== Fine-tuning only the classification head completed in {finetune_head_time:.2f} minutes ===\") \n",
    "print(f\"Training accuracy: {train_accuracy_head*100:.2f}%\") \n",
    "print(f\"Validation accuracy: {val_accuracy_head*100:.2f}%\") \n",
    "#print(f\"Test accuracy: {test_accuracy_head*100:.2f}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find that fine-tuning just a linear layer can achieve a significant improvement. Are you satisfied with the test accuracy results? Try different hyperparameters, such as increasing the learning rate, and try to get better accuracy. \n",
    "\n",
    "Next, we will try unlocking the Transformer block before the linear layer; Sebastian Raschka found that this can significantly improve the model's performance on specific downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfreeze Partial Model Layers and Further Fine-Tuning\n",
    "\n",
    "We unfreeze the last Transformer block (`transformer.h.11`) and the final LayerNorm of GPT-2, and train them along with the classification head (`score`) to further enhance performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 7090944\n",
      "\n",
      "Trainable parts after unfreezing the last Transformer block and LayerNorm:\n",
      "  transformer.h.11.ln_1.weight => shape=torch.Size([768])\n",
      "  transformer.h.11.ln_1.bias => shape=torch.Size([768])\n",
      "  transformer.h.11.attn.c_attn.weight => shape=torch.Size([768, 2304])\n",
      "  transformer.h.11.attn.c_attn.bias => shape=torch.Size([2304])\n",
      "  transformer.h.11.attn.c_proj.weight => shape=torch.Size([768, 768])\n",
      "  transformer.h.11.attn.c_proj.bias => shape=torch.Size([768])\n",
      "  transformer.h.11.ln_2.weight => shape=torch.Size([768])\n",
      "  transformer.h.11.ln_2.bias => shape=torch.Size([768])\n",
      "  transformer.h.11.mlp.c_fc.weight => shape=torch.Size([768, 3072])\n",
      "  transformer.h.11.mlp.c_fc.bias => shape=torch.Size([3072])\n",
      "  transformer.h.11.mlp.c_proj.weight => shape=torch.Size([3072, 768])\n",
      "  transformer.h.11.mlp.c_proj.bias => shape=torch.Size([768])\n",
      "  transformer.ln_f.weight => shape=torch.Size([768])\n",
      "  transformer.ln_f.bias => shape=torch.Size([768])\n",
      "  score.weight => shape=torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze the last transformer block and LayerNorm in base_model\n",
    "for param in classification_model.base_model.h[-1].parameters(): \n",
    "    param.requires_grad = True \n",
    "    k+=param.numel()\n",
    "for param in classification_model.base_model.ln_f.parameters(): \n",
    "    param.requires_grad = True\n",
    "    k += param.numel()\n",
    "\n",
    "print(f\"Total trainable parameters: {k}\") \n",
    "print(\"\\nTrainable parts after unfreezing the last Transformer block and LayerNorm:\") \n",
    "trainable_params_count = 0 \n",
    "for name, param in classification_model.named_parameters(): \n",
    "    if param.requires_grad: \n",
    "        print(f\"  {name} => shape={param.size()}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, start training this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification head initialized.\n",
      "Classification head reinitialized to eliminate sequential advantage impact.\n",
      "\n",
      "Epoch 1/8, step 500/7147, loss = 0.8376\n",
      "Epoch 1/8, step 1000/7147, loss = 1.0200\n",
      "Epoch 1/8, step 1500/7147, loss = 1.5388\n",
      "Epoch 1/8, step 2000/7147, loss = 1.1472\n",
      "Epoch 1/8, step 2500/7147, loss = 1.0757\n",
      "Epoch 1/8, step 3000/7147, loss = 0.5841\n",
      "Epoch 1/8, step 3500/7147, loss = 0.7648\n",
      "Epoch 1/8, step 4000/7147, loss = 0.9431\n",
      "Epoch 1/8, step 4500/7147, loss = 0.5554\n",
      "Epoch 1/8, step 5000/7147, loss = 0.6419\n",
      "Epoch 1/8, step 5500/7147, loss = 0.6214\n",
      "Epoch 1/8, step 6000/7147, loss = 1.0334\n",
      "Epoch 1/8, step 6500/7147, loss = 0.6036\n",
      "Epoch 1/8, step 7000/7147, loss = 1.0877\n",
      "Epoch 1/8, step 7140/7147, loss = 0.4104\n",
      "Epoch 1/8, Average training loss: 1.0636\n",
      "Validation accuracy: 68.95%\n",
      "\n",
      "Epoch 2/8, step 500/7147, loss = 0.5880\n",
      "Epoch 2/8, step 1000/7147, loss = 0.7671\n",
      "Epoch 2/8, step 1500/7147, loss = 0.9469\n",
      "Epoch 2/8, step 2000/7147, loss = 0.4601\n",
      "Epoch 2/8, step 2500/7147, loss = 0.7905\n",
      "Epoch 2/8, step 3000/7147, loss = 0.5836\n",
      "Epoch 2/8, step 3500/7147, loss = 0.5895\n",
      "Epoch 2/8, step 4000/7147, loss = 0.6944\n",
      "Epoch 2/8, step 4500/7147, loss = 0.5686\n",
      "Epoch 2/8, step 5000/7147, loss = 0.6379\n",
      "Epoch 2/8, step 5500/7147, loss = 0.5894\n",
      "Epoch 2/8, step 6000/7147, loss = 0.5645\n",
      "Epoch 2/8, step 6500/7147, loss = 0.7521\n",
      "Epoch 2/8, step 7000/7147, loss = 0.4328\n",
      "Epoch 2/8, step 7140/7147, loss = 0.6204\n",
      "Epoch 2/8, Average training loss: 0.6143\n",
      "Validation accuracy: 75.41%\n",
      "\n",
      "Epoch 3/8, step 500/7147, loss = 0.4963\n",
      "Epoch 3/8, step 1000/7147, loss = 0.5694\n",
      "Epoch 3/8, step 1500/7147, loss = 0.7023\n",
      "Epoch 3/8, step 2000/7147, loss = 0.4267\n",
      "Epoch 3/8, step 2500/7147, loss = 0.6363\n",
      "Epoch 3/8, step 3000/7147, loss = 0.4996\n",
      "Epoch 3/8, step 3500/7147, loss = 0.5352\n",
      "Epoch 3/8, step 4000/7147, loss = 0.6160\n",
      "Epoch 3/8, step 4500/7147, loss = 0.7473\n",
      "Epoch 3/8, step 5000/7147, loss = 0.6416\n",
      "Epoch 3/8, step 5500/7147, loss = 0.4551\n",
      "Epoch 3/8, step 6000/7147, loss = 0.5423\n",
      "Epoch 3/8, step 6500/7147, loss = 0.5598\n",
      "Epoch 3/8, step 7000/7147, loss = 0.3298\n",
      "Epoch 3/8, step 7140/7147, loss = 0.4682\n",
      "Epoch 3/8, Average training loss: 0.5642\n",
      "Validation accuracy: 77.93%\n",
      "\n",
      "Epoch 4/8, step 500/7147, loss = 0.4899\n",
      "Epoch 4/8, step 1000/7147, loss = 0.6584\n",
      "Epoch 4/8, step 1500/7147, loss = 0.7787\n",
      "Epoch 4/8, step 2000/7147, loss = 0.7230\n",
      "Epoch 4/8, step 2500/7147, loss = 0.5498\n",
      "Epoch 4/8, step 3000/7147, loss = 0.6016\n",
      "Epoch 4/8, step 3500/7147, loss = 0.4104\n",
      "Epoch 4/8, step 4000/7147, loss = 0.7904\n",
      "Epoch 4/8, step 4500/7147, loss = 0.4646\n",
      "Epoch 4/8, step 5000/7147, loss = 0.6259\n",
      "Epoch 4/8, step 5500/7147, loss = 0.8585\n",
      "Epoch 4/8, step 6000/7147, loss = 0.6853\n",
      "Epoch 4/8, step 6500/7147, loss = 0.6683\n",
      "Epoch 4/8, step 7000/7147, loss = 0.5830\n",
      "Epoch 4/8, step 7140/7147, loss = 0.4489\n",
      "Epoch 4/8, Average training loss: 0.5439\n",
      "Validation accuracy: 79.10%\n",
      "\n",
      "Epoch 5/8, step 500/7147, loss = 0.7966\n",
      "Epoch 5/8, step 1000/7147, loss = 0.6618\n",
      "Epoch 5/8, step 1500/7147, loss = 0.5242\n",
      "Epoch 5/8, step 2000/7147, loss = 0.5885\n",
      "Epoch 5/8, step 2500/7147, loss = 0.5012\n",
      "Epoch 5/8, step 3000/7147, loss = 0.4279\n",
      "Epoch 5/8, step 3500/7147, loss = 0.3967\n",
      "Epoch 5/8, step 4000/7147, loss = 0.5487\n",
      "Epoch 5/8, step 4500/7147, loss = 0.5280\n",
      "Epoch 5/8, step 5000/7147, loss = 0.6139\n",
      "Epoch 5/8, step 5500/7147, loss = 0.6239\n",
      "Epoch 5/8, step 6000/7147, loss = 0.3460\n",
      "Epoch 5/8, step 6500/7147, loss = 0.6049\n",
      "Epoch 5/8, step 7000/7147, loss = 0.4887\n",
      "Epoch 5/8, step 7140/7147, loss = 0.4372\n",
      "Epoch 5/8, Average training loss: 0.5299\n",
      "Validation accuracy: 79.31%\n",
      "\n",
      "Epoch 6/8, step 500/7147, loss = 0.6761\n",
      "Epoch 6/8, step 1000/7147, loss = 0.4407\n",
      "Epoch 6/8, step 1500/7147, loss = 0.7726\n",
      "Epoch 6/8, step 2000/7147, loss = 0.5289\n",
      "Epoch 6/8, step 2500/7147, loss = 0.5173\n",
      "Epoch 6/8, step 3000/7147, loss = 0.6437\n",
      "Epoch 6/8, step 3500/7147, loss = 0.4272\n",
      "Epoch 6/8, step 4000/7147, loss = 0.2754\n",
      "Epoch 6/8, step 4500/7147, loss = 0.6917\n",
      "Epoch 6/8, step 5000/7147, loss = 0.6342\n",
      "Epoch 6/8, step 5500/7147, loss = 0.5066\n",
      "Epoch 6/8, step 6000/7147, loss = 0.4151\n",
      "Epoch 6/8, step 6500/7147, loss = 0.5423\n",
      "Epoch 6/8, step 7000/7147, loss = 0.5887\n",
      "Epoch 6/8, step 7140/7147, loss = 0.3111\n",
      "Epoch 6/8, Average training loss: 0.5240\n",
      "Validation accuracy: 79.82%\n",
      "\n",
      "Epoch 7/8, step 500/7147, loss = 0.3648\n",
      "Epoch 7/8, step 1000/7147, loss = 0.6160\n",
      "Epoch 7/8, step 1500/7147, loss = 0.5693\n",
      "Epoch 7/8, step 2000/7147, loss = 0.4613\n",
      "Epoch 7/8, step 2500/7147, loss = 0.4586\n",
      "Epoch 7/8, step 3000/7147, loss = 0.4512\n",
      "Epoch 7/8, step 3500/7147, loss = 0.7247\n",
      "Epoch 7/8, step 4000/7147, loss = 0.4298\n",
      "Epoch 7/8, step 4500/7147, loss = 0.7031\n",
      "Epoch 7/8, step 5000/7147, loss = 0.4745\n",
      "Epoch 7/8, step 5500/7147, loss = 0.4409\n",
      "Epoch 7/8, step 6000/7147, loss = 0.4922\n",
      "Epoch 7/8, step 6500/7147, loss = 0.4248\n",
      "Epoch 7/8, step 7000/7147, loss = 0.9000\n",
      "Epoch 7/8, step 7140/7147, loss = 0.4322\n",
      "Epoch 7/8, Average training loss: 0.5168\n",
      "Validation accuracy: 80.61%\n",
      "\n",
      "Epoch 8/8, step 500/7147, loss = 0.2656\n",
      "Epoch 8/8, step 1000/7147, loss = 0.3221\n",
      "Epoch 8/8, step 1500/7147, loss = 0.4600\n",
      "Epoch 8/8, step 2000/7147, loss = 0.3159\n",
      "Epoch 8/8, step 2500/7147, loss = 0.6527\n",
      "Epoch 8/8, step 3000/7147, loss = 0.4040\n",
      "Epoch 8/8, step 3500/7147, loss = 0.5933\n",
      "Epoch 8/8, step 4000/7147, loss = 0.5256\n",
      "Epoch 8/8, step 4500/7147, loss = 0.6572\n",
      "Epoch 8/8, step 5000/7147, loss = 0.5976\n",
      "Epoch 8/8, step 5500/7147, loss = 0.7593\n",
      "Epoch 8/8, step 6000/7147, loss = 0.3610\n",
      "Epoch 8/8, step 6500/7147, loss = 0.3483\n",
      "Epoch 8/8, step 7000/7147, loss = 0.4812\n",
      "Epoch 8/8, step 7140/7147, loss = 0.5616\n",
      "Epoch 8/8, Average training loss: 0.5123\n",
      "Validation accuracy: 80.44%\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m end_time_further \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \n\u001b[1;32m     11\u001b[0m finetune_further_time \u001b[38;5;241m=\u001b[39m (end_time_further \u001b[38;5;241m-\u001b[39m start_time_further) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m \n\u001b[0;32m---> 13\u001b[0m train_accuracy_partial \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_accuracy\u001b[49m(classification_model, train_loader, device) \n\u001b[1;32m     14\u001b[0m val_accuracy_partial \u001b[38;5;241m=\u001b[39m evaluate_accuracy(classification_model, val_loader, device) \n\u001b[1;32m     15\u001b[0m test_accuracy_partial \u001b[38;5;241m=\u001b[39m evaluate_accuracy(classification_model, test_loader, device) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "from src.ult import initialize_classifier_head\n",
    "from src.train import  train_partial_unfreeze\n",
    "import time \n",
    "\n",
    "initialize_classifier_head(classification_model)\n",
    "print(\"Classification head reinitialized to eliminate sequential advantage impact.\\n\")\n",
    "\n",
    "start_time_further = time.time() \n",
    "classification_model = train_partial_unfreeze(classification_model, train_loader, val_loader, device, epochs=8, lr=3e-5) \n",
    "end_time_further = time.time() \n",
    "finetune_further_time = (end_time_further - start_time_further) / 60 \n",
    "\n",
    "train_accuracy_partial = evaluate_accuracy(classification_model, train_loader, device) \n",
    "val_accuracy_partial = evaluate_accuracy(classification_model, val_loader, device) \n",
    "test_accuracy_partial = evaluate_accuracy(classification_model, test_loader, device) \n",
    "\n",
    "print(f\"\\n=== Fine-tuning after unfreezing partial Transformer completed in {finetune_further_time:.2f} minutes ===\") \n",
    "print(f\"Training accuracy: {train_accuracy_partial*100:.2f}%\") \n",
    "print(f\"Validation accuracy: {val_accuracy_partial*100:.2f}%\") \n",
    "print(f\"Test accuracy: {test_accuracy_partial*100:.2f}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the model's accuracy on the test set has improved.\n",
    "\n",
    "Now we have obtained a GPT-2 model that has been trained on the last Transformer block + LayerNorm layer + classification head."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
