{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7628c6ab",
   "metadata": {},
   "source": [
    "# Using mGPT for Zero-Shot and Few-Shot Translation\n",
    "This notebook demonstrates how to use mGPT for Zero-Shot and Few-Shot translation tasks.\n",
    "We use the `ai-forever/mGPT` model from Hugging Face's Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93091872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cdaa6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(100000, 2048)\n",
       "    (wpe): Embedding(2048, 2048)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=6144, nx=2048)\n",
       "          (c_proj): Conv1D(nf=2048, nx=2048)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=8192, nx=2048)\n",
       "          (c_proj): Conv1D(nf=2048, nx=8192)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=100000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load mGPT model and tokenizer\n",
    "model_name = 'ai-forever/mGPT'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fef42982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"ai-forever/mGPT\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 5,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 2048,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 2048,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.48.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 100000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad319734",
   "metadata": {},
   "source": [
    "vocab_size = 100000, so this model is indeed mGPT(https://huggingface.co/ai-forever/mGPT) rather than GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c8a205",
   "metadata": {},
   "source": [
    "## Zero-Shot Translation\n",
    "We translate an English sentence to German without providing any examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faab02ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Zero-Shot Translation: You are a helpful translator. Please translate the following English sentence into German. Do not add any extra text or explanations. Only provide the translation. \n",
      "\n",
      "English: Hello, how are you?\n",
      "German: Hallo, wie sind Sie?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the Zero-Shot prompt\n",
    "sentence_en = 'Hello, how are you?'\n",
    "zero_shot_prompt = (\n",
    "    \"You are a helpful translator. \"\n",
    "    \"Please translate the following English sentence into German. \"\n",
    "    \"Do not add any extra text or explanations. \"\n",
    "    \"Only provide the translation. \"\n",
    "    \"\\n\\n\"\n",
    "    \"English: Hello, how are you?\\n\"\n",
    "    \"German:\"\n",
    ")\n",
    "# Tokenize input\n",
    "inputs = tokenizer(zero_shot_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        num_beams=5,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "gen_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# post-process output\n",
    "#if \"German:\" in gen_text:\n",
    " #   gen_text = gen_text.split(\"German:\")[-1].strip()\n",
    "\n",
    "print(\"Final Zero-Shot Translation:\", gen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a943f81",
   "metadata": {},
   "source": [
    "## Few-Shot Translation\n",
    "We provide a few examples to guide the model in translating a new sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32ed7a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Few-Shot Translation for 'The weather is nice today.'] \"Der Wetter ist sch√∂n heute.\"\n"
     ]
    }
   ],
   "source": [
    "def generate_few_shot_prompt(sentence):\n",
    "    return (\n",
    "        \"You are a helpful translator. \"\n",
    "        \"Please translate the following English sentences into German, \"\n",
    "        \"using the examples given. \"\n",
    "        \"Provide only the correct German translation, without any additional text or commentary.\\n\\n\"\n",
    "        \"Example 1:\\n[English]: \\\"Good morning.\\\"\\n[German]: \\\"Guten Morgen.\\\"\\n\"\n",
    "        \"Example 2:\\n[English]: \\\"I like to study languages.\\\"\\n[German]: \\\"Ich lerne gerne Sprachen.\\\"\\n\\n\"\n",
    "        f\"Now translate this sentence:\\n[English]: \\\"{sentence}\\\"\\n[German]:\"\n",
    "    )\n",
    "    \n",
    "sentence_to_translate = \"The weather is nice today.\"\n",
    "few_shot_prompt = generate_few_shot_prompt(sentence_to_translate)\n",
    "\n",
    "few_shot_inputs = tokenizer(few_shot_prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    few_shot_outputs = model.generate(\n",
    "        **few_shot_inputs,\n",
    "        max_new_tokens=50, \n",
    "        num_beams=5,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "gen_text = tokenizer.decode(few_shot_outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "start_marker = f\"[English]: \\\"{sentence_to_translate}\\\"\\n[German]:\"\n",
    "if start_marker in gen_text:\n",
    "    gen_text = gen_text.split(start_marker)[-1].split(\"\\n\")[0].strip()\n",
    "else:\n",
    "    gen_text = \"Translation not found.\"\n",
    "\n",
    "print(f\"[Few-Shot Translation for '{sentence_to_translate}']\", gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665a983-8b87-4f04-b5f4-35bf60c15526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
