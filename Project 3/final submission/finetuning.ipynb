{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPT-2 for Text Classification\n",
    "\n",
    "For demonstration purposes and due to resource limitations, we have chosen GPT-2 as our model. You should be able to apply the same methods to larger models which will likely yield better performance.\n",
    "\n",
    "We build on the notebook by Sebastian Raschka [3] Sebastian Raschka's LLMs Course: [GitHub - rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "## Environment Setup\n",
    "\n",
    "First, we need to install and import the required libraries. Ensure that all necessary packages are installed; otherwise, please use `pip install` to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!$ pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org <package_name>\n",
    "#!pip install torch --user\n",
    "#!pip uninstall matplotlib -y\n",
    "#!pip uninstall pillow -y \n",
    "#!pip uninstall numpy -y \n",
    "#!pip uninstall datasets -y -v \n",
    "#!pip install matplotlib\n",
    "#!pip install pillow\n",
    "#!pip install numpy\n",
    "#!pip install datasets --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "#!pip install datasets --user\n",
    "#!pip install evaluate --user\n",
    "#!pip3 install Cython --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.0\n",
      "numpy version: 2.0.2\n",
      "torch version: 2.5.1\n",
      "transformers version: 4.48.0\n",
      "datasets version: 3.2.0\n",
      "pandas version: 2.2.3\n",
      "evaluate version: 0.4.3\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "from importlib.metadata import version \n",
    "import sys \n",
    "import os \n",
    "path_to_save_folder = \"model\"\n",
    "path_to_lora = os.path.join(path_to_save_folder,\"lora\")\n",
    "path_to_partial = os.path.join(path_to_save_folder,\"partial\")\n",
    "pkgs = [ \n",
    "    \"matplotlib\", \n",
    "    \"numpy\", \n",
    "    \"torch\", \n",
    "    \"transformers\",   \n",
    "    \"datasets\",        \n",
    "    \"pandas\",          \n",
    "    \"evaluate\",        \n",
    "] \n",
    "\n",
    "for p in pkgs: \n",
    "    try: \n",
    "        print(f\"{p} version: {version(p)}\") \n",
    "    except: \n",
    "        print(f\"{p} is not installed. Please use `pip install {p}` to install.\") \n",
    "        sys.exit(1) \n",
    "data_path = \"content\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Set model name\n",
    "from transformers import GPT2Tokenizer\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Load GPT-2 Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Loading\n",
    "\n",
    "We will use the SMS Spam Collection dataset. This dataset contains SMS messages labeled as \"spam\" or \"ham\". We will download, preprocess the data, and split it into training, validation, and test sets.\n",
    "\n",
    "If there is an error downloading, please try to download it manually.\n",
    "Download link: https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\n",
    "\n",
    "The code below creates balanced classes of ham and spam. **Reflect on the advantages and disadvantages of this step.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset label counts:\n",
      "Label\n",
      "1        37569\n",
      "0        29780\n",
      "label        1\n",
      "Name: count, dtype: int64\n",
      "(67350, 2)\n",
      "\n",
      "Balanced dataset label distribution:\n",
      "Label\n",
      "1    29780\n",
      "0    29780\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training set size: 67350, Validation set size: 873, Test set size: 1822\n"
     ]
    }
   ],
   "source": [
    "from src.fine_helper import load_complete_dataframe\n",
    "is_balanced = True\n",
    "#\n",
    "#Just needs to be done ONCE ! \n",
    "#\n",
    "train_df, validation_df, test_df = load_complete_dataframe(data_path,is_balanced=is_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Loaders\n",
    "\n",
    "Next, we need to encode and pad the SMS text to ensure consistent input length in each batch. Here, we use `<|PAD|>` as the padding token and build the attention_mask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new pad_token '<|PAD|>' with ID: 50257\n",
      "Index(['Text', 'Label'], dtype='object')\n",
      "Max Length is:  65\n",
      "Index(['Text', 'Label'], dtype='object')\n",
      "Max Length is:  65\n",
      "Index(['Text', 'Label'], dtype='object')\n",
      "Max Length is:  65\n",
      "Max Length:  65\n",
      "Number of training batches: 7147, Number of validation batches: 298\n"
     ]
    }
   ],
   "source": [
    "from src.dataset_loader import get_enc_dataset\n",
    "train_dataset, val_dataset,test_dataset,train_loader,val_loader,test_loader, pad_token_id = get_enc_dataset(data_path,tokenizer,is_log = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Computational Device\n",
    "Before loading and training the model, we need to define the computational device (CPU or GPU). If a GPU is available, it will be preferred to accelerate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    device = torch.device(\"cuda\") \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") \n",
    "else: \n",
    "    device = torch.device(\"cpu\") \n",
    "print(f\"Using device: {device}\") \n",
    "#device = torch.device(\"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Model Structure ##\n",
    "Load the pre-trained GPT-2 model and make sure you understand its structure. **What do the abbreviations represent? Which of the layer types have we seen in the lecture?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 02:34:10.259725: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-18 02:34:10.276726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737164050.298258   94856 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737164050.304884   94856 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-18 02:34:10.326565: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2ForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2ForSequenceClassification\n",
    "pretrained_gpt2_lm = GPT2LMHeadModel.from_pretrained(\"gpt2\") \n",
    "\n",
    "#print(pretrained_gpt2_lm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the structure to the GPT-2 model for classification. **What is the difference?**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "classification_model = GPT2ForSequenceClassification.from_pretrained( \n",
    "    model_name, \n",
    "    num_labels=2, \n",
    "    pad_token_id=tokenizer.pad_token_id \n",
    ").to(device) \n",
    "\n",
    "#print(classification_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Testing\n",
    "\n",
    "At this point, the classification head is untrained, and the results are usually poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Before fine-tuning) Initial prediction of the classification head:\n",
      "torch.Size([1, 65])\n",
      "Postive sample => Prediction: 0\n",
      "torch.Size([1, 65])\n",
      "Negative sample => Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def classify_text(text, model, tokenizer, device, max_length, pad_token_id=50256): \n",
    "    model.eval() \n",
    "    enc = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=max_length) \n",
    "    att_mask = [1]*len(enc) \n",
    "    pad_len = max_length - len(enc) \n",
    "    if pad_len > 0: \n",
    "        enc += [pad_token_id]*pad_len \n",
    "        att_mask += [0]*pad_len \n",
    "     \n",
    "    input_ids = torch.tensor([enc], dtype=torch.long).to(device) \n",
    "    attention_mask = torch.tensor([att_mask], dtype=torch.long).to(device) \n",
    "\n",
    "    with torch.no_grad(): \n",
    "        print(input_ids.shape)\n",
    "        \n",
    "        outputs = model(input_ids)#, attention_mask=attention_mask) \n",
    "        logits = outputs.logits \n",
    "        predicted = torch.argmax(logits, dim=-1).item() \n",
    "    return predicted\n",
    "\n",
    "sample_text_spam = \"Fine\" \n",
    "sample_text_ham = \"Bad\" \n",
    "\n",
    "print(\"(Before fine-tuning) Initial prediction of the classification head:\") \n",
    "print(f\"Postive sample => Prediction: {classify_text(sample_text_spam, classification_model, \n",
    "                                                  tokenizer, device, max_length=65,pad_token_id=pad_token_id-1\n",
    "                                                    )}\") \n",
    "print(f\"Negative sample => Prediction: {classify_text(sample_text_ham, classification_model,\n",
    "                                                 tokenizer, device,max_length=65\n",
    "                                                )}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tuning only the classification layer\n",
    "\n",
    "As a first step, we will freeze the parameters of the GPT2-model and only tune the classification layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of base parameters: 124440576\n",
      "\n",
      "Training only the classification head, trainable parameters: 1536\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We need to add the padding token to the embeddings\n",
    "classification_model.resize_token_embeddings(len(tokenizer)) \n",
    "classification_model.to(device) \n",
    "\n",
    "i =0\n",
    "k=0\n",
    "for param in classification_model.base_model.parameters(): \n",
    "    param.requires_grad = False \n",
    "    i+=param.numel()\n",
    "\n",
    "print(f\"Number of base parameters: {i}\")\n",
    "\n",
    "\n",
    "for param in classification_model.score.parameters(): \n",
    "    param.requires_grad = True \n",
    "    k+=param.numel()\n",
    "\n",
    "print(f\"\\nTraining only the classification head, trainable parameters: {k}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11, step 500/7147, loss = 0.3942\n",
      "Epoch 1/11, step 1000/7147, loss = 0.7415\n",
      "Epoch 1/11, step 1500/7147, loss = 0.7541\n",
      "Epoch 1/11, step 2000/7147, loss = 0.6668\n",
      "Epoch 1/11, step 2500/7147, loss = 0.6145\n",
      "Epoch 1/11, step 3000/7147, loss = 0.6348\n",
      "Epoch 1/11, step 3500/7147, loss = 0.5880\n",
      "Epoch 1/11, step 4000/7147, loss = 0.6075\n",
      "Epoch 1/11, step 4500/7147, loss = 0.4708\n",
      "Epoch 1/11, step 5000/7147, loss = 0.5165\n",
      "Epoch 1/11, step 5500/7147, loss = 0.6139\n",
      "Epoch 1/11, step 6000/7147, loss = 0.6627\n",
      "Epoch 1/11, step 6500/7147, loss = 0.6722\n",
      "Epoch 1/11, step 7000/7147, loss = 0.5596\n",
      "Epoch 1/11, step 7140/7147, loss = 0.7131\n",
      "Epoch 1/11, Average training loss: 0.6824\n",
      "Validation accuracy: 74.53% train_acc=61.19%\n",
      "\n",
      "Epoch 2/11, step 500/7147, loss = 0.6114\n",
      "Epoch 2/11, step 1000/7147, loss = 0.6706\n",
      "Epoch 2/11, step 1500/7147, loss = 0.6326\n",
      "Epoch 2/11, step 2000/7147, loss = 0.6903\n",
      "Epoch 2/11, step 2500/7147, loss = 0.6296\n",
      "Epoch 2/11, step 3000/7147, loss = 0.6398\n",
      "Epoch 2/11, step 3500/7147, loss = 0.6904\n",
      "Epoch 2/11, step 4000/7147, loss = 0.5793\n",
      "Epoch 2/11, step 4500/7147, loss = 0.6249\n",
      "Epoch 2/11, step 5000/7147, loss = 0.5255\n",
      "Epoch 2/11, step 5500/7147, loss = 0.4686\n",
      "Epoch 2/11, step 6500/7147, loss = 0.5744\n",
      "Epoch 2/11, step 7000/7147, loss = 0.4901\n",
      "Epoch 2/11, step 7140/7147, loss = 0.4693\n",
      "Epoch 2/11, Average training loss: 0.5771\n",
      "Validation accuracy: 77.13% train_acc=71.36%\n",
      "\n",
      "Epoch 3/11, step 500/7147, loss = 0.5347\n",
      "Epoch 3/11, step 1000/7147, loss = 0.4042\n",
      "Epoch 3/11, step 1500/7147, loss = 0.4130\n",
      "Epoch 3/11, step 2000/7147, loss = 0.5511\n",
      "Epoch 3/11, step 2500/7147, loss = 0.6946\n",
      "Epoch 3/11, step 3000/7147, loss = 0.4318\n",
      "Epoch 3/11, step 3500/7147, loss = 0.6221\n",
      "Epoch 3/11, step 4000/7147, loss = 0.6026\n",
      "Epoch 3/11, step 4500/7147, loss = 0.3964\n",
      "Epoch 3/11, step 5000/7147, loss = 0.6145\n",
      "Epoch 3/11, step 5500/7147, loss = 0.4717\n",
      "Epoch 3/11, step 6000/7147, loss = 0.5063\n",
      "Epoch 3/11, step 6500/7147, loss = 0.6522\n",
      "Epoch 3/11, step 7000/7147, loss = 0.5041\n",
      "Epoch 3/11, step 7140/7147, loss = 0.5684\n",
      "Epoch 3/11, Average training loss: 0.5474\n",
      "Validation accuracy: 78.05% train_acc=73.36%\n",
      "\n",
      "Epoch 4/11, step 500/7147, loss = 0.5381\n",
      "Epoch 4/11, step 1000/7147, loss = 0.5933\n",
      "Epoch 4/11, step 1500/7147, loss = 0.6607\n",
      "Epoch 4/11, step 2000/7147, loss = 0.3834\n",
      "Epoch 4/11, step 2500/7147, loss = 0.5351\n",
      "Epoch 4/11, step 3000/7147, loss = 0.6673\n",
      "Epoch 4/11, step 3500/7147, loss = 0.3036\n",
      "Epoch 4/11, step 4000/7147, loss = 0.5535\n",
      "Epoch 4/11, step 4500/7147, loss = 0.3387\n",
      "Epoch 4/11, step 5000/7147, loss = 0.5442\n",
      "Epoch 4/11, step 5500/7147, loss = 0.4823\n",
      "Epoch 4/11, step 6000/7147, loss = 0.6530\n",
      "Epoch 4/11, step 6500/7147, loss = 0.3156\n",
      "Epoch 4/11, step 7000/7147, loss = 0.6113\n",
      "Epoch 4/11, step 7140/7147, loss = 0.5617\n",
      "Epoch 4/11, Average training loss: 0.5332\n",
      "Validation accuracy: 78.89% train_acc=74.16%\n",
      "\n",
      "Epoch 5/11, step 500/7147, loss = 0.3355\n",
      "Epoch 5/11, step 1000/7147, loss = 0.3865\n",
      "Epoch 5/11, step 1500/7147, loss = 0.3660\n",
      "Epoch 5/11, step 2000/7147, loss = 0.4238\n",
      "Epoch 5/11, step 2500/7147, loss = 0.6594\n",
      "Epoch 5/11, step 3000/7147, loss = 0.6622\n",
      "Epoch 5/11, step 3500/7147, loss = 0.2627\n",
      "Epoch 5/11, step 4000/7147, loss = 0.5531\n",
      "Epoch 5/11, step 4500/7147, loss = 0.4921\n",
      "Epoch 5/11, step 5000/7147, loss = 0.8324\n",
      "Epoch 5/11, step 5500/7147, loss = 0.3425\n",
      "Epoch 5/11, step 6000/7147, loss = 0.3865\n",
      "Epoch 5/11, step 6500/7147, loss = 0.5076\n",
      "Epoch 5/11, step 7000/7147, loss = 0.3979\n",
      "Epoch 5/11, step 7140/7147, loss = 0.6584\n",
      "Epoch 5/11, Average training loss: 0.5224\n",
      "Validation accuracy: 79.94% train_acc=74.63%\n",
      "\n",
      "Epoch 6/11, step 500/7147, loss = 0.6462\n",
      "Epoch 6/11, step 1000/7147, loss = 0.6091\n",
      "Epoch 6/11, step 1500/7147, loss = 0.3741\n",
      "Epoch 6/11, step 2000/7147, loss = 0.5982\n",
      "Epoch 6/11, step 2500/7147, loss = 0.6555\n",
      "Epoch 6/11, step 3000/7147, loss = 0.4706\n",
      "Epoch 6/11, step 3500/7147, loss = 0.4596\n",
      "Epoch 6/11, step 4000/7147, loss = 0.4595\n",
      "Epoch 6/11, step 4500/7147, loss = 0.4887\n",
      "Epoch 6/11, step 5000/7147, loss = 0.3800\n",
      "Epoch 6/11, step 5500/7147, loss = 0.4666\n",
      "Epoch 6/11, step 6000/7147, loss = 0.5405\n",
      "Epoch 6/11, step 6500/7147, loss = 0.5775\n",
      "Epoch 6/11, step 7000/7147, loss = 0.4426\n",
      "Epoch 6/11, step 7140/7147, loss = 0.5022\n",
      "Epoch 6/11, Average training loss: 0.5173\n",
      "Validation accuracy: 80.49% train_acc=74.91%\n",
      "\n",
      "Epoch 7/11, step 500/7147, loss = 0.4739\n",
      "Epoch 7/11, step 1000/7147, loss = 0.6390\n",
      "Epoch 7/11, step 1500/7147, loss = 0.5345\n",
      "Epoch 7/11, step 2000/7147, loss = 0.5201\n",
      "Epoch 7/11, step 2500/7147, loss = 0.2614\n",
      "Epoch 7/11, step 3000/7147, loss = 0.8117\n",
      "Epoch 7/11, step 3500/7147, loss = 0.6596\n",
      "Epoch 7/11, step 4000/7147, loss = 0.4381\n",
      "Epoch 7/11, step 4500/7147, loss = 0.3734\n",
      "Epoch 7/11, step 5000/7147, loss = 0.4200\n",
      "Epoch 7/11, step 5500/7147, loss = 0.7209\n",
      "Epoch 7/11, step 6000/7147, loss = 0.4125\n",
      "Epoch 7/11, step 6500/7147, loss = 0.6671\n",
      "Epoch 7/11, step 7000/7147, loss = 0.6518\n",
      "Epoch 7/11, step 7140/7147, loss = 0.3683\n",
      "Epoch 7/11, Average training loss: 0.5101\n",
      "Validation accuracy: 80.53% train_acc=75.35%\n",
      "\n",
      "Epoch 8/11, step 500/7147, loss = 0.3681\n",
      "Epoch 8/11, step 1000/7147, loss = 0.7125\n",
      "Epoch 8/11, step 1500/7147, loss = 0.4562\n",
      "Epoch 8/11, step 2000/7147, loss = 0.4802\n",
      "Epoch 8/11, step 2500/7147, loss = 0.4893\n",
      "Epoch 8/11, step 3000/7147, loss = 0.4616\n",
      "Epoch 8/11, step 3500/7147, loss = 0.5223\n",
      "Epoch 8/11, step 4000/7147, loss = 0.6205\n",
      "Epoch 8/11, step 4500/7147, loss = 0.4003\n",
      "Epoch 8/11, step 5000/7147, loss = 0.2768\n",
      "Epoch 8/11, step 5500/7147, loss = 0.4302\n",
      "Epoch 8/11, step 6000/7147, loss = 0.5662\n",
      "Epoch 8/11, step 6500/7147, loss = 0.4311\n",
      "Epoch 8/11, step 7000/7147, loss = 0.4572\n",
      "Epoch 8/11, step 7140/7147, loss = 0.4922\n",
      "Epoch 8/11, Average training loss: 0.5068\n",
      "Validation accuracy: 80.11% train_acc=75.55%\n",
      "\n",
      "Epoch 9/11, step 500/7147, loss = 0.8004\n",
      "Epoch 9/11, step 1000/7147, loss = 0.4468\n",
      "Epoch 9/11, step 1500/7147, loss = 0.2876\n",
      "Epoch 9/11, step 2000/7147, loss = 0.3828\n",
      "Epoch 9/11, step 2500/7147, loss = 0.4201\n",
      "Epoch 9/11, step 3000/7147, loss = 0.3443\n",
      "Epoch 9/11, step 3500/7147, loss = 0.2106\n",
      "Epoch 9/11, step 4000/7147, loss = 0.3771\n",
      "Epoch 9/11, step 4500/7147, loss = 0.6263\n",
      "Epoch 9/11, step 5000/7147, loss = 0.5239\n",
      "Epoch 9/11, step 5500/7147, loss = 0.5881\n",
      "Epoch 9/11, step 6000/7147, loss = 0.4139\n",
      "Epoch 9/11, step 6500/7147, loss = 0.7671\n",
      "Epoch 9/11, step 7000/7147, loss = 0.4122\n",
      "Epoch 9/11, step 7140/7147, loss = 0.4403\n",
      "Epoch 9/11, Average training loss: 0.5028\n",
      "Validation accuracy: 81.24% train_acc=75.89%\n",
      "\n",
      "Epoch 10/11, step 500/7147, loss = 0.4740\n",
      "Epoch 10/11, step 1000/7147, loss = 0.2875\n",
      "Epoch 10/11, step 1500/7147, loss = 0.6600\n",
      "Epoch 10/11, step 2000/7147, loss = 0.4418\n",
      "Epoch 10/11, step 2500/7147, loss = 0.7110\n",
      "Epoch 10/11, step 3000/7147, loss = 0.5199\n",
      "Epoch 10/11, step 3500/7147, loss = 0.7215\n",
      "Epoch 10/11, step 4000/7147, loss = 0.7052\n",
      "Epoch 10/11, step 4500/7147, loss = 0.6079\n",
      "Epoch 10/11, step 5000/7147, loss = 0.3197\n",
      "Epoch 10/11, step 5500/7147, loss = 0.5450\n",
      "Epoch 10/11, step 6000/7147, loss = 0.6322\n",
      "Epoch 10/11, step 6500/7147, loss = 0.4147\n",
      "Epoch 10/11, step 7000/7147, loss = 0.3957\n",
      "Epoch 10/11, step 7140/7147, loss = 0.5277\n",
      "Epoch 10/11, Average training loss: 0.4984\n",
      "Validation accuracy: 81.41% train_acc=76.09%\n",
      "\n",
      "Epoch 11/11, step 500/7147, loss = 0.3364\n",
      "Epoch 11/11, step 1000/7147, loss = 0.5471\n",
      "Epoch 11/11, step 1500/7147, loss = 0.4876\n",
      "Epoch 11/11, step 2000/7147, loss = 0.6811\n",
      "Epoch 11/11, step 2500/7147, loss = 0.4607\n",
      "Epoch 11/11, step 3000/7147, loss = 0.8678\n",
      "Epoch 11/11, step 3500/7147, loss = 0.4148\n",
      "Epoch 11/11, step 4000/7147, loss = 0.7348\n",
      "Epoch 11/11, step 4500/7147, loss = 0.3493\n",
      "Epoch 11/11, step 5000/7147, loss = 0.3149\n",
      "Epoch 11/11, step 5500/7147, loss = 0.4052\n",
      "Epoch 11/11, step 6000/7147, loss = 0.5734\n",
      "Epoch 11/11, step 6500/7147, loss = 0.3768\n",
      "Epoch 11/11, step 7000/7147, loss = 0.3276\n",
      "Epoch 11/11, step 7140/7147, loss = 0.2614\n",
      "Epoch 11/11, Average training loss: 0.4997\n",
      "Validation accuracy: 81.49% train_acc=75.83%\n",
      "\n",
      "\n",
      "=== Fine-tuning only the classification head completed in 41.98 minutes ===\n",
      "Training accuracy: 80.84%\n",
      "Validation accuracy: 81.49%\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from ult import evaluate_accuracy\n",
    "from src.train import train_head_only\n",
    "is_dry=False\n",
    "#Epochs 10\n",
    "start_time_head = time.time() \n",
    "classification_model, all_avg_loss,all_acc_train,all_acc_val = train_head_only(classification_model, train_loader, val_loader, device, epochs=11, lr=3e-5,is_dry=is_dry) \n",
    "end_time_head = time.time() \n",
    "\n",
    "train_accuracy_head = evaluate_accuracy(classification_model, train_loader, device,is_dry=is_dry) \n",
    "val_accuracy_head = evaluate_accuracy(classification_model, val_loader, device,is_dry=is_dry) \n",
    "test_accuracy_head = evaluate_accuracy(classification_model, test_loader, device,is_dry=is_dry) \n",
    "finetune_head_time = (end_time_head - start_time_head) / 60 \n",
    "\n",
    "print(f\"\\n=== Fine-tuning only the classification head completed in {finetune_head_time:.2f} minutes ===\") \n",
    "print(f\"Training accuracy: {train_accuracy_head*100:.2f}%\") \n",
    "print(f\"Validation accuracy: {val_accuracy_head*100:.2f}%\") \n",
    "#print(f\"Test accuracy: {test_accuracy_head*100:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything saved at\n",
      " 01:30:00\n"
     ]
    }
   ],
   "source": [
    "# Save model \n",
    "from src.eval_helper import save_everything\n",
    "from src.eval_helper import *\n",
    "#path_to_partial TODO\n",
    "train_run_label = \"headonly_ep11_saveUpdate_try2\"\n",
    "#save_everything(path_to_partial, train_run_label, elapsed, train_losses, val_accs)\n",
    "#classification_model, all_avg_loss,all_acc_train,all_acc_val\n",
    "\n",
    "\"\"\"\n",
    "save_everything(path_to_head_only, train_run_label, finetune_further_time,\n",
    "                 all_avg_loss,all_acc_val,all_acc_val,\n",
    "                train_accuracy_partial,val_accuracy_partial,test_accuracy_partial,classification_model)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "save_everything(path_to_save_folder=path_to_head_only,\n",
    "                 train_run_label=train_run_label,\n",
    "                 elapsed=finetune_head_time,\n",
    "                 train_losses=all_avg_loss,\n",
    "                 train_acc=all_acc_train,\n",
    "                 val_accs=all_acc_val,\n",
    "                 train_acc_complete=train_accuracy_head,\n",
    "                 val_acc_complete=val_accuracy_head,\n",
    "                 test_acc_complete=test_accuracy_head,\n",
    "                 model=classification_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find that fine-tuning just a linear layer can achieve a significant improvement. Are you satisfied with the test accuracy results? Try different hyperparameters, such as increasing the learning rate, and try to get better accuracy. \n",
    "\n",
    "Next, we will try unlocking the Transformer block before the linear layer; Sebastian Raschka found that this can significantly improve the model's performance on specific downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfreeze Partial Model Layers and Further Fine-Tuning\n",
    "\n",
    "We unfreeze the last Transformer block (`transformer.h.11`) and the final LayerNorm of GPT-2, and train them along with the classification head (`score`) to further enhance performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 7090944\n",
      "\n",
      "Trainable parts after unfreezing the last Transformer block and LayerNorm:\n",
      "  transformer.h.11.ln_1.weight => shape=torch.Size([768])\n",
      "  transformer.h.11.ln_1.bias => shape=torch.Size([768])\n",
      "  transformer.h.11.attn.c_attn.weight => shape=torch.Size([768, 2304])\n",
      "  transformer.h.11.attn.c_attn.bias => shape=torch.Size([2304])\n",
      "  transformer.h.11.attn.c_proj.weight => shape=torch.Size([768, 768])\n",
      "  transformer.h.11.attn.c_proj.bias => shape=torch.Size([768])\n",
      "  transformer.h.11.ln_2.weight => shape=torch.Size([768])\n",
      "  transformer.h.11.ln_2.bias => shape=torch.Size([768])\n",
      "  transformer.h.11.mlp.c_fc.weight => shape=torch.Size([768, 3072])\n",
      "  transformer.h.11.mlp.c_fc.bias => shape=torch.Size([3072])\n",
      "  transformer.h.11.mlp.c_proj.weight => shape=torch.Size([3072, 768])\n",
      "  transformer.h.11.mlp.c_proj.bias => shape=torch.Size([768])\n",
      "  transformer.ln_f.weight => shape=torch.Size([768])\n",
      "  transformer.ln_f.bias => shape=torch.Size([768])\n",
      "  score.weight => shape=torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze the last transformer block and LayerNorm in base_model\n",
    "for param in classification_model.base_model.h[-1].parameters(): \n",
    "    param.requires_grad = True \n",
    "    k+=param.numel()\n",
    "for param in classification_model.base_model.ln_f.parameters(): \n",
    "    param.requires_grad = True\n",
    "    k += param.numel()\n",
    "\n",
    "print(f\"Total trainable parameters: {k}\") \n",
    "print(\"\\nTrainable parts after unfreezing the last Transformer block and LayerNorm:\") \n",
    "trainable_params_count = 0 \n",
    "for name, param in classification_model.named_parameters(): \n",
    "    if param.requires_grad: \n",
    "        print(f\"  {name} => shape={param.size()}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, start training this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification head initialized.\n",
      "Classification head reinitialized to eliminate sequential advantage impact.\n",
      "\n",
      "Epoch 1/8, step 500/7147, loss = 1.35494\n",
      "Epoch 1/8, step 1000/7147, loss = 1.2663\n",
      "Epoch 1/8, step 1500/7147, loss = 0.4811\n",
      "Epoch 1/8, step 2000/7147, loss = 0.5282\n",
      "Epoch 1/8, step 2500/7147, loss = 0.7143\n",
      "Epoch 1/8, step 3000/7147, loss = 0.5997\n",
      "Epoch 1/8, step 3500/7147, loss = 1.0294\n",
      "Epoch 1/8, step 4000/7147, loss = 0.7298\n",
      "Epoch 1/8, step 4500/7147, loss = 0.8081\n",
      "Epoch 1/8, step 5000/7147, loss = 0.4371\n",
      "Epoch 1/8, step 5500/7147, loss = 0.5999\n",
      "Epoch 1/8, step 6000/7147, loss = 0.6458\n",
      "Epoch 1/8, step 6500/7147, loss = 0.5266\n",
      "Epoch 1/8, step 7000/7147, loss = 0.5490\n",
      "Epoch 1/8, step 7140/7147, loss = 0.7666\n",
      "Epoch 1/8, Average training loss: 1.1999\n",
      "Validation accuracy: 67.56% train_acc=55.05%\n",
      "\n",
      "Epoch 2/8, step 500/7147, loss = 0.6014\n",
      "Epoch 2/8, step 1000/7147, loss = 0.5937\n",
      "Epoch 2/8, step 1500/7147, loss = 0.6144\n",
      "Epoch 2/8, step 2000/7147, loss = 0.6461\n",
      "Epoch 2/8, step 2500/7147, loss = 0.5552\n",
      "Epoch 2/8, step 3000/7147, loss = 0.5085\n",
      "Epoch 2/8, step 3500/7147, loss = 0.6194\n",
      "Epoch 2/8, step 4000/7147, loss = 0.5691\n",
      "Epoch 2/8, step 4500/7147, loss = 0.6120\n",
      "Epoch 2/8, step 5000/7147, loss = 0.4219\n",
      "Epoch 2/8, step 5500/7147, loss = 0.6790\n",
      "Epoch 2/8, step 6000/7147, loss = 0.3706\n",
      "Epoch 2/8, step 6500/7147, loss = 0.6155\n",
      "Epoch 2/8, step 7000/7147, loss = 0.5943\n",
      "Epoch 2/8, step 7140/7147, loss = 0.4957\n",
      "Epoch 2/8, Average training loss: 0.6018\n",
      "Validation accuracy: 75.03% train_acc=67.53%\n",
      "\n",
      "Epoch 3/8, step 500/7147, loss = 0.6498\n",
      "Epoch 3/8, step 1000/7147, loss = 0.3679\n",
      "Epoch 3/8, step 1500/7147, loss = 0.4216\n",
      "Epoch 3/8, step 2000/7147, loss = 0.5362\n",
      "Epoch 3/8, step 2500/7147, loss = 0.4846\n",
      "Epoch 3/8, step 3000/7147, loss = 0.5285\n",
      "Epoch 3/8, step 3500/7147, loss = 0.7733\n",
      "Epoch 3/8, step 4000/7147, loss = 0.5494\n",
      "Epoch 3/8, step 4500/7147, loss = 0.4949\n",
      "Epoch 3/8, step 5000/7147, loss = 0.4986\n",
      "Epoch 3/8, step 5500/7147, loss = 0.7319\n",
      "Epoch 3/8, step 6000/7147, loss = 0.5474\n",
      "Epoch 3/8, step 6500/7147, loss = 0.3208\n",
      "Epoch 3/8, step 7000/7147, loss = 0.4601\n",
      "Epoch 3/8, step 7140/7147, loss = 0.8406\n",
      "Epoch 3/8, Average training loss: 0.5616\n",
      "Validation accuracy: 77.42% train_acc=71.56%\n",
      "\n",
      "Epoch 4/8, step 500/7147, loss = 0.6074\n",
      "Epoch 4/8, step 1000/7147, loss = 0.3717\n",
      "Epoch 4/8, step 1500/7147, loss = 0.4442\n",
      "Epoch 4/8, step 2000/7147, loss = 0.4894\n",
      "Epoch 4/8, step 2500/7147, loss = 0.5100\n",
      "Epoch 4/8, step 3000/7147, loss = 0.4477\n",
      "Epoch 4/8, step 3500/7147, loss = 0.6053\n",
      "Epoch 4/8, step 4000/7147, loss = 0.5994\n",
      "Epoch 4/8, step 4500/7147, loss = 0.3250\n",
      "Epoch 4/8, step 5000/7147, loss = 0.7006\n",
      "Epoch 4/8, step 5500/7147, loss = 0.5696\n",
      "Epoch 4/8, step 6000/7147, loss = 0.3811\n",
      "Epoch 4/8, step 6500/7147, loss = 0.4668\n",
      "Epoch 4/8, step 7000/7147, loss = 0.4197\n",
      "Epoch 4/8, step 7140/7147, loss = 0.4650\n",
      "Epoch 4/8, Average training loss: 0.5422\n",
      "Validation accuracy: 78.51% train_acc=73.06%\n",
      "\n",
      "Epoch 5/8, step 500/7147, loss = 0.5943\n",
      "Epoch 5/8, step 1000/7147, loss = 0.3206\n",
      "Epoch 5/8, step 1500/7147, loss = 0.7110\n",
      "Epoch 5/8, step 2000/7147, loss = 0.4591\n",
      "Epoch 5/8, step 2500/7147, loss = 0.4995\n",
      "Epoch 5/8, step 3000/7147, loss = 0.6099\n",
      "Epoch 5/8, step 3500/7147, loss = 0.3317\n",
      "Epoch 5/8, step 4000/7147, loss = 0.3788\n",
      "Epoch 5/8, step 4500/7147, loss = 0.4375\n",
      "Epoch 5/8, step 5000/7147, loss = 0.5009\n",
      "Epoch 5/8, step 5500/7147, loss = 0.8479\n",
      "Epoch 5/8, step 6000/7147, loss = 0.5597\n",
      "Epoch 5/8, step 6500/7147, loss = 0.4233\n",
      "Epoch 5/8, step 7000/7147, loss = 0.4960\n",
      "Epoch 5/8, step 7140/7147, loss = 0.5864\n",
      "Epoch 5/8, Average training loss: 0.5283\n",
      "Validation accuracy: 79.90% train_acc=74.17%\n",
      "\n",
      "Epoch 6/8, step 500/7147, loss = 0.2941\n",
      "Epoch 6/8, step 1000/7147, loss = 0.4017\n",
      "Epoch 6/8, step 1500/7147, loss = 0.4415\n",
      "Epoch 6/8, step 2000/7147, loss = 0.4878\n",
      "Epoch 6/8, step 2500/7147, loss = 0.7258\n",
      "Epoch 6/8, step 3000/7147, loss = 0.4749\n",
      "Epoch 6/8, step 3500/7147, loss = 0.5865\n",
      "Epoch 6/8, step 4000/7147, loss = 0.5100\n",
      "Epoch 6/8, step 4500/7147, loss = 0.6973\n",
      "Epoch 6/8, step 5000/7147, loss = 0.6788\n",
      "Epoch 6/8, step 5500/7147, loss = 0.6427\n",
      "Epoch 6/8, step 6000/7147, loss = 0.3396\n",
      "Epoch 6/8, step 6500/7147, loss = 0.6946\n",
      "Epoch 6/8, step 7000/7147, loss = 0.5110\n",
      "Epoch 6/8, step 7140/7147, loss = 0.3511\n",
      "Epoch 6/8, Average training loss: 0.5216\n",
      "Validation accuracy: 80.32% train_acc=74.48%\n",
      "\n",
      "Epoch 7/8, step 500/7147, loss = 0.5355\n",
      "Epoch 7/8, step 1000/7147, loss = 0.4391\n",
      "Epoch 7/8, step 1500/7147, loss = 0.7251\n",
      "Epoch 7/8, step 2000/7147, loss = 0.5987\n",
      "Epoch 7/8, step 2500/7147, loss = 0.6775\n",
      "Epoch 7/8, step 3000/7147, loss = 0.5329\n",
      "Epoch 7/8, step 3500/7147, loss = 0.4259\n",
      "Epoch 7/8, step 4000/7147, loss = 0.5718\n",
      "Epoch 7/8, step 4500/7147, loss = 0.5647\n",
      "Epoch 7/8, step 5000/7147, loss = 0.8863\n",
      "Epoch 7/8, step 5500/7147, loss = 0.8695\n",
      "Epoch 7/8, step 6000/7147, loss = 0.5152\n",
      "Epoch 7/8, step 6500/7147, loss = 0.4690\n",
      "Epoch 7/8, step 7000/7147, loss = 0.4899\n",
      "Epoch 7/8, step 7140/7147, loss = 0.5999\n",
      "Epoch 7/8, Average training loss: 0.5157\n",
      "Validation accuracy: 80.28% train_acc=74.76%\n",
      "\n",
      "Epoch 8/8, step 500/7147, loss = 0.4232\n",
      "Epoch 8/8, step 1000/7147, loss = 0.4657\n",
      "Epoch 8/8, step 1500/7147, loss = 0.3984\n",
      "Epoch 8/8, step 2000/7147, loss = 0.7471\n",
      "Epoch 8/8, step 2500/7147, loss = 0.4285\n",
      "Epoch 8/8, step 3000/7147, loss = 0.3482\n",
      "Epoch 8/8, step 3500/7147, loss = 0.5067\n",
      "Epoch 8/8, step 4000/7147, loss = 0.5348\n",
      "Epoch 8/8, step 4500/7147, loss = 0.4740\n",
      "Epoch 8/8, step 5000/7147, loss = 0.6055\n",
      "Epoch 8/8, step 5500/7147, loss = 0.4269\n",
      "Epoch 8/8, step 6000/7147, loss = 0.3906\n",
      "Epoch 8/8, step 6500/7147, loss = 0.6284\n",
      "Epoch 8/8, step 7000/7147, loss = 0.8134\n",
      "Epoch 8/8, step 7140/7147, loss = 0.7674\n",
      "Epoch 8/8, Average training loss: 0.5112\n",
      "Validation accuracy: 80.32% train_acc=75.19%\n",
      "\n",
      "\n",
      "=== Fine-tuning after unfreezing partial Transformer completed in 38.39 minutes ===\n",
      "Training accuracy: 80.08%\n",
      "Validation accuracy: 80.32%\n",
      "Test accuracy: 81.54%\n"
     ]
    }
   ],
   "source": [
    "from src.ult import initialize_classifier_head\n",
    "from src.train import  train_partial_unfreeze\n",
    "from ult import evaluate_accuracy\n",
    "import time \n",
    "is_dry=False\n",
    "\n",
    "initialize_classifier_head(classification_model)\n",
    "print(\"Classification head reinitialized to eliminate sequential advantage impact.\\n\")\n",
    "\n",
    "start_time_further = time.time() \n",
    "classification_model, all_avg_loss,all_acc_train,all_acc_val = train_partial_unfreeze(classification_model, train_loader, val_loader, device, epochs=8, lr=3e-5,is_dry=is_dry) \n",
    "end_time_further = time.time() \n",
    "finetune_further_time = (end_time_further - start_time_further) / 60 \n",
    "\n",
    "train_accuracy_partial = evaluate_accuracy(classification_model, train_loader, device,is_dry=is_dry) \n",
    "val_accuracy_partial = evaluate_accuracy(classification_model, val_loader, device,is_dry=is_dry) \n",
    "test_accuracy_partial = evaluate_accuracy(classification_model, test_loader, device,is_dry=is_dry) \n",
    "\n",
    "print(f\"\\n=== Fine-tuning after unfreezing partial Transformer completed in {finetune_further_time:.2f} minutes ===\") \n",
    "print(f\"Training accuracy: {train_accuracy_partial*100:.2f}%\") \n",
    "print(f\"Validation accuracy: {val_accuracy_partial*100:.2f}%\") \n",
    "print(f\"Test accuracy: {test_accuracy_partial*100:.2f}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the model's accuracy on the test set has improved.\n",
    "\n",
    "Now we have obtained a GPT-2 model that has been trained on the last Transformer block + LayerNorm layer + classification head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save\n",
      "Everything saved at\n",
      " 02:17:14\n"
     ]
    }
   ],
   "source": [
    "from src.eval_helper import save_everything\n",
    "from src.eval_helper import *\n",
    "#path_to_partial TODO\n",
    "train_run_label = \"partial_ep8_try2\"\n",
    "#save_everything(path_to_partial, train_run_label, elapsed, train_losses, val_accs)\n",
    "#classification_model, all_avg_loss,all_acc_train,all_acc_val\n",
    "\"\"\"\n",
    "save_everything(path_to_partial, train_run_label, \n",
    "                finetune_further_time, all_avg_loss, all_acc_val,\n",
    "                train_accuracy_partial,\n",
    "                val_accuracy_partial,test_accuracy_partial,classification_model)\n",
    "\"\"\"\n",
    "print(\"Save\")\n",
    "\n",
    "save_everything(path_to_save_folder=path_to_partial,\n",
    "                 train_run_label=train_run_label,\n",
    "                 elapsed=finetune_further_time,\n",
    "                 train_losses=all_avg_loss,\n",
    "                 train_acc=all_acc_train,\n",
    "                 val_accs=all_acc_val,\n",
    "                 train_acc_complete=train_accuracy_partial,\n",
    "                 val_acc_complete=val_accuracy_partial,\n",
    "                 test_acc_complete=test_accuracy_partial,\n",
    "                 model=classification_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo Welt\n"
     ]
    }
   ],
   "source": [
    "print(\"Hallo Welt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
