{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yxGyxj9kM1l"
   },
   "source": [
    "# Importing Libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2024-12-14T14:59:56.919679Z",
     "iopub.status.busy": "2024-12-14T14:59:56.918931Z",
     "iopub.status.idle": "2024-12-14T15:00:21.184950Z",
     "shell.execute_reply": "2024-12-14T15:00:21.183811Z",
     "shell.execute_reply.started": "2024-12-14T14:59:56.919647Z"
    },
    "id": "3sxyNjnykM1n",
    "outputId": "258bfcf2-7eeb-4467-d180-0d6644298511",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.20.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.2)\n",
      "Requirement already satisfied: hazm in /opt/conda/lib/python3.10/site-packages (0.10.0)\n",
      "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from hazm) (0.9.2)\n",
      "Requirement already satisfied: flashtext<3.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from hazm) (2.7)\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from hazm) (4.3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/conda/lib/python3.10/site-packages (from hazm) (3.9.1)\n",
      "Requirement already satisfied: numpy==1.24.3 in /opt/conda/lib/python3.10/site-packages (from hazm) (1.24.3)\n",
      "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /opt/conda/lib/python3.10/site-packages (from hazm) (0.9.11)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /opt/conda/lib/python3.10/site-packages (from hazm) (1.2.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.10/site-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.13.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (70.0.0)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.0.4)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.5.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.16.0)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "! pip install tokenizers scikit-learn\n",
    "! pip install hazm\n",
    "! pip install tiktoken\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import trainers\n",
    "from tokenizers.normalizers import StripAccents, Lowercase, Sequence\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer, UnigramTrainer\n",
    "from tokenizers.models import BPE, Unigram\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tokenizers\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from hazm import *\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk import word_tokenize\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk import ngrams\n",
    "\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from importlib.metadata import version\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6m7MZBxTkM1p"
   },
   "source": [
    "# Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-14T15:01:19.836213Z",
     "iopub.status.busy": "2024-12-14T15:01:19.835293Z",
     "iopub.status.idle": "2024-12-14T15:01:25.807542Z",
     "shell.execute_reply": "2024-12-14T15:01:25.806674Z",
     "shell.execute_reply.started": "2024-12-14T15:01:19.836176Z"
    },
    "id": "dhThuVKgkM1p",
    "outputId": "c8e567cc-13af-4a05-b53a-9df0629a92c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-14 15:01:20--  https://downloads.wortschatz-leipzig.de/corpora/fas_news_2020_100K.tar.gz\n",
      "Resolving downloads.wortschatz-leipzig.de (downloads.wortschatz-leipzig.de)... 139.18.2.68\n",
      "Connecting to downloads.wortschatz-leipzig.de (downloads.wortschatz-leipzig.de)|139.18.2.68|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31227186 (30M) [application/x-gzip]\n",
      "Saving to: 'fas_news_2020_100K.tar.gz'\n",
      "\n",
      "fas_news_2020_100K. 100%[===================>]  29.78M  12.0MB/s    in 2.5s    \n",
      "\n",
      "2024-12-14 15:01:24 (12.0 MB/s) - 'fas_news_2020_100K.tar.gz' saved [31227186/31227186]\n",
      "\n",
      "fas_news_2020_100K/\n",
      "fas_news_2020_100K/fas_news_2020_100K-inv_w.txt\n",
      "fas_news_2020_100K/fas_news_2020_100K-sources.txt\n",
      "fas_news_2020_100K/fas_news_2020_100K-co_n.txt\n",
      "fas_news_2020_100K/fas_news_2020_100K-import.sql\n",
      "fas_news_2020_100K/fas_news_2020_100K-sentences.txt\n",
      "fas_news_2020_100K/fas_news_2020_100K-co_s.txt\n",
      "fas_news_2020_100K/fas_news_2020_100K-words.txt\n",
      "fas_news_2020_100K/fas_news_2020_100K-inv_so.txt\n"
     ]
    }
   ],
   "source": [
    "!wget https://downloads.wortschatz-leipzig.de/corpora/fas_news_2020_100K.tar.gz\n",
    "!tar --gunzip --extract --verbose --file=fas_news_2020_100K.tar.gz\n",
    "persian_text_path = \"/kaggle/working/fas_news_2020_100K/fas_news_2020_100K-sentences.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFzk8uhnkM1p"
   },
   "source": [
    "# Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-14T15:01:32.633628Z",
     "iopub.status.busy": "2024-12-14T15:01:32.632745Z",
     "iopub.status.idle": "2024-12-14T15:02:15.382637Z",
     "shell.execute_reply": "2024-12-14T15:02:15.381709Z",
     "shell.execute_reply.started": "2024-12-14T15:01:32.633582Z"
    },
    "id": "3MtFh8l-kM1p",
    "outputId": "8614c760-f414-4805-c1f7-d9e7c43abeb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Example: ['گفتم', 'نه', 'حاجی', 'شما', 'هم', 'بروید', 'دیگر', 'کسی', 'نیست']\n",
      "Validation Data Example: ['در', 'این', 'خصوص', 'مدیرکل', 'دفتر', 'پیشگیری', 'از', 'قاچاق', 'کالا', 'و', 'ارز', 'در', 'واکنش', 'به', 'اعلام', 'آمار', 'قاچاق', 'سیگار', 'در', 'کشور', 'توسط', 'برخی', 'دستگاه', 'ها', 'گفت', 'اعلام', 'رسمی', 'آمار', 'قاچاق', 'سیگار', 'در', 'کشور', 'بر', 'عهده', 'ستاد', 'مبارزه', 'با', 'قاچاق', 'کالا', 'و', 'ارز', 'است']\n",
      "Test Data Example: ['نشان', 'به', 'آن', 'نشان', 'که', 'در', 'حدود', 'کمتر', 'از', '۳۰', 'سال', 'خدمتش', 'همه', 'کیفها', 'پولها', 'چکهای', 'حامل', 'و', 'گوشیهای', 'همراهی', 'که', 'پیدا', 'کرده', 'با', 'پیجویی', 'تمام', 'به', 'صاحبانش', 'برگردانده', 'است']\n"
     ]
    }
   ],
   "source": [
    "# Punctuations and Separators\n",
    "punc = '''()-[]{};،:'\"\\\\, <>./?@#$%^&*_~.'''\n",
    "seperator = ['\\xad', '\\u200e', '\\u200f', '\\u200d', '\\u200c', '\\n']\n",
    "\n",
    "# Hazm Normalizer and Stemmer\n",
    "hazm_normalizer = Normalizer()\n",
    "hazm_stemmer = Stemmer()\n",
    "\n",
    "# Read corpus\n",
    "with open(persian_text_path, \"r\") as f:\n",
    "    sentences = f.readlines()\n",
    "\n",
    "def preprocess_text_with_hazm(text):\n",
    "    # Normalize using Hazm e.g. اصلاح نويسه ها -> اصلاح نویسه‌ها\n",
    "    text = hazm_normalizer.normalize(text)\n",
    "\n",
    "    # Remove unwanted separators\n",
    "    for sep in seperator:\n",
    "        text = text.replace(sep, \" \")\n",
    "\n",
    "    # Remove all punctuation\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF0-9\\s]+', '', text)\n",
    "    text = text.replace(',', '')\n",
    "    text = text.replace('،', '')\n",
    "    text = re.sub(r\"^\\d+\\s*\", \"\", text)\n",
    "\n",
    "    # Tokenize using Hazm for more accurate Persian tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Add <s> and </s> tags to the sentence\n",
    "    # tokens = ['<s>'] + tokens + ['</s>']\n",
    "\n",
    "    # Stemming using hazm e.g  کتاب‌ها -> کتاب‌\n",
    "    # tokens = [hazm_stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Preprocess all sentences\n",
    "cleaned_sentences = [preprocess_text_with_hazm(sentence) for sentence in sentences]\n",
    "\n",
    "# Split into training, validation, and testing datasets (80%, 10%, 10%)\n",
    "train_corpus, temp_corpus = train_test_split(cleaned_sentences, test_size=0.2, random_state=42)\n",
    "val_corpus, test_corpus = train_test_split(temp_corpus, test_size=0.5, random_state=42)\n",
    "\n",
    "# Example\n",
    "print(\"Training Data Example:\", train_corpus[0])\n",
    "print(\"Validation Data Example:\", val_corpus[0])\n",
    "print(\"Test Data Example:\", test_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Wtl9sQzkM1q"
   },
   "source": [
    "# N-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T11:54:40.083657Z",
     "iopub.status.busy": "2024-12-14T11:54:40.083025Z",
     "iopub.status.idle": "2024-12-14T11:54:44.707927Z",
     "shell.execute_reply": "2024-12-14T11:54:44.707061Z",
     "shell.execute_reply.started": "2024-12-14T11:54:40.083625Z"
    },
    "id": "fYNrj5kzkM1q",
    "outputId": "de1ee898-b652-4be3-eaad-a8e115aeac51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common 3-grams:\n",
      "('گزارش', 'همشهری', 'آنلاین'): 1113\n",
      "('همشهری', 'آنلاین', 'نقل'): 770\n",
      "('وی', 'ادامه', 'داد'): 510\n",
      "('دانشگاه', 'علوم', 'پزشکی'): 397\n",
      "('مجلس', 'شورای', 'اسلامی'): 384\n",
      "('جمهوری', 'اسلامی', 'ایران'): 284\n",
      "('شیوع', 'ویروس', 'کرونا'): 271\n",
      "('۲۴', 'ساعت', 'گذشته'): 253\n",
      "('قرار', 'گرفته', 'است'): 190\n",
      "('جان', 'خود', 'دست'): 172\n",
      "None\n",
      "\n",
      "Most common 2-grams:\n",
      "('کرده', 'است'): 1520\n",
      "('ادامه', 'داد'): 1315\n",
      "('همشهری', 'آنلاین'): 1178\n",
      "('ویروس', 'کرونا'): 1176\n",
      "('گزارش', 'همشهری'): 1130\n",
      "('بر', 'اساس'): 1076\n",
      "('وی', 'افزود'): 955\n",
      "('اعلام', 'کرد'): 953\n",
      "('خبر', 'داد'): 897\n",
      "('بوده', 'است'): 897\n",
      "('وجود', 'دارد'): 788\n",
      "('تصریح', 'کرد'): 778\n",
      "('آنلاین', 'نقل'): 771\n",
      "('کووید', '۱۹'): 706\n",
      "('ممکن', 'است'): 674\n",
      "('بیان', 'کرد'): 666\n",
      "('تاکید', 'کرد'): 664\n",
      "('خواهد', 'بود'): 646\n",
      "('نشان', 'دهد'): 643\n",
      "('وی', 'ادامه'): 642\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def ngram_counts(corpus, n, i):\n",
    "    ngram_list = []\n",
    "\n",
    "    stopwords = ['به', 'از', 'را', 'و', 'برای', 'این', 'که', 'با', 'در', 'چون', 'اگر', 'ها', 'نه', 'اینکه', 'یا', 'هم', 'تا', 'که', 'آن', 'باید', 'شده', 'چرا', 'همچنین', 'کردن', 'شد', 'می']\n",
    "    filtered_corpus = [\n",
    "        [word for word in sentence if word not in stopwords]\n",
    "        for sentence in corpus\n",
    "    ]\n",
    "\n",
    "    for sentence in filtered_corpus:\n",
    "        sentence_ngrams = list(ngrams(sentence, n))\n",
    "        ngram_list.extend(sentence_ngrams)\n",
    "\n",
    "    ngram_counts = Counter(ngram_list)\n",
    "\n",
    "    most_common_ngrams = ngram_counts.most_common(i)\n",
    "\n",
    "    print(f\"\\nMost common {n}-grams:\")\n",
    "    for ngram, count in most_common_ngrams:\n",
    "        print(f\"{ngram}: {count}\")\n",
    "\n",
    "print(ngram_counts(train_corpus, 3, 10))\n",
    "print(ngram_counts(train_corpus, 2, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-14T11:54:53.020739Z",
     "iopub.status.busy": "2024-12-14T11:54:53.020009Z",
     "iopub.status.idle": "2024-12-14T11:54:53.027519Z",
     "shell.execute_reply": "2024-12-14T11:54:53.026636Z",
     "shell.execute_reply.started": "2024-12-14T11:54:53.020705Z"
    },
    "id": "x5SbPwFBkM1q",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e3adb57c-0074-4755-acaa-07c2059024cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADDING:\n",
      "['<s>', '<s>', 'گفتم', 'نه', 'حاجی', 'شما', 'هم', 'بروید', 'دیگر', 'کسی', 'نیست', '</s>', '</s>', '<s>', '<s>', 'در', 'این', 'غربال', 'گری', 'ها', 'کیس', 'های', 'مشکوکی', 'با', 'علامت', 'تب', 'داشتیم', 'که', 'خدا', 'را', 'شکر', 'ورزشکاری', 'نبود', 'که', 'با', 'کرونا', 'درگیر', 'باشد', 'اما', 'در', 'بین', 'همکاران', 'بودند', 'کسانی', 'که', 'مبتلا', 'شدند', '</s>', '</s>', '<s>', '<s>', 'حضرت', 'رسول', 'در', 'حلقه', 'اصحاب', 'نشسته', 'بودند', '</s>', '</s>', '<s>', '<s>', 'این', 'اتفاق', 'باعث', 'شد', 'تا', 'نقش', 'ایالات', 'متحده', 'به', 'عنوان', 'جایگزین', 'طالبان', 'در', 'افغانستان', 'حذف', 'و', 'جای', 'آن', 'را', 'دولتی', 'بگیرد', 'که', 'بومی', 'و', 'از', 'میان', 'سیاستمداران', 'افغان', 'انتخاب', 'شده', 'است', '</s>', '</s>', '<s>', '<s>', 'وی', 'اضافه', 'کرد', 'همه', 'بخشهای', 'سازمان', 'در', 'مقابله', 'با', 'کرونا', 'و', 'حفظ', 'آرامش', 'مردم', 'از', 'هیچ', 'چیز', 'دریغ', 'نمی', 'کنند', 'و', 'تلاش', 'دارند', 'تا', 'آخرین', 'لحظه', 'کار', 'کنند', 'و', 'حتی', 'از', 'جان', 'خود', 'هم', 'می', 'گذرند', '</s>', '</s>', '<s>', '<s>', 'در', 'روایات', 'ما', 'به', 'طور', 'مکرر', 'از', 'دو', 'غیبت', 'آن', 'حضرت', 'سخن', 'به', 'میان', 'آمده', 'و', 'از', 'سالها', 'پیش', 'از', 'تولد', 'امام', 'مهدی', 'ع', 'بر', 'این', 'موضوع', 'تصریح', 'شده', 'است', 'که', 'حضرتش', 'دو', 'غیبت', 'خواهند', 'داشت', 'که', 'هر', 'یک', 'با', 'دیگری', 'متفاوت', 'است', '</s>', '</s>', '<s>', '<s>', 'نکته', 'مهم', 'این', 'است', 'که', 'نباید', 'انتظار', 'کار', 'ویژهای', 'را', 'در', 'مدت', 'کوتاه', 'داشت', 'اما', 'این', 'ریلگذاری', 'ما', 'را', 'به', 'نقطه', 'خوبی', 'خواهد', 'رساند', '</s>', '</s>', '<s>', '<s>', 'ارتفاع', 'پست', 'ابراهیم', 'حاتمیکیا', 'هم', 'پسزمینه', 'جنگ', 'دارد', 'و', 'پیامدهای', 'جنگ', 'را', 'بر', 'زندگی', 'مردم', 'خوزستان', 'روایت', 'می', 'کند', 'و', 'سرانجام', 'بمب؛', 'یک', 'عاشقانه', 'پیمان', 'معادی', 'که', 'داستان', 'آن', 'در', 'دوران', 'موشکباران', 'شهرها', 'روایت', 'می', 'شود', '</s>', '</s>', '<s>', '<s>', 'وی', 'ادامه', 'داد', 'این', 'مسیر', 'به', 'علت', 'پایین', 'آمدن', 'چند', 'قطعه', 'سنگ', 'بزرگ', 'بر', 'اثر', 'برخورد', 'صاعقه', 'با', 'کوه', 'مسدود', 'شده', 'است', 'و', 'در', 'سایه', 'تلاش', 'راهدارای', 'و', 'جابهجایی', 'سنگهای', 'کوچک', 'این', 'راه', 'روستایی', 'به', 'صورت', 'موقت', 'و', 'کنارگذر', 'بازگشایی', 'شده', 'است', '</s>', '</s>', '<s>', '<s>', 'اساسا', 'حمایت', 'از', 'بخش', 'کشاورزی', 'و', 'غذا', 'به', 'لحاظ', 'مبانی', 'تئوریک', 'اجتناب', 'ناپذیر', 'است', '</s>', '</s>']\n",
      "\n",
      "\n",
      "NGRAMS:\n",
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'گفتم'), ('<s>',), ('<s>', 'گفتم'), ('<s>', 'گفتم', 'نه'), ('گفتم',), ('گفتم', 'نه'), ('گفتم', 'نه', 'حاجی'), ('نه',), ('نه', 'حاجی'), ('نه', 'حاجی', 'شما'), ('حاجی',), ('حاجی', 'شما'), ('حاجی', 'شما', 'هم'), ('شما',), ('شما', 'هم'), ('شما', 'هم', 'بروید'), ('هم',), ('هم', 'بروید'), ('هم', 'بروید', 'دیگر'), ('بروید',), ('بروید', 'دیگر'), ('بروید', 'دیگر', 'کسی'), ('دیگر',), ('دیگر', 'کسی'), ('دیگر', 'کسی', 'نیست'), ('کسی',), ('کسی', 'نیست'), ('کسی', 'نیست', '</s>'), ('نیست',), ('نیست', '</s>'), ('نیست', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
      "\n",
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'در'), ('<s>',), ('<s>', 'در'), ('<s>', 'در', 'این'), ('در',), ('در', 'این'), ('در', 'این', 'غربال'), ('این',), ('این', 'غربال'), ('این', 'غربال', 'گری'), ('غربال',), ('غربال', 'گری'), ('غربال', 'گری', 'ها'), ('گری',), ('گری', 'ها'), ('گری', 'ها', 'کیس'), ('ها',), ('ها', 'کیس'), ('ها', 'کیس', 'های'), ('کیس',), ('کیس', 'های'), ('کیس', 'های', 'مشکوکی'), ('های',), ('های', 'مشکوکی'), ('های', 'مشکوکی', 'با'), ('مشکوکی',), ('مشکوکی', 'با'), ('مشکوکی', 'با', 'علامت'), ('با',), ('با', 'علامت'), ('با', 'علامت', 'تب'), ('علامت',), ('علامت', 'تب'), ('علامت', 'تب', 'داشتیم'), ('تب',), ('تب', 'داشتیم'), ('تب', 'داشتیم', 'که'), ('داشتیم',), ('داشتیم', 'که'), ('داشتیم', 'که', 'خدا'), ('که',), ('که', 'خدا'), ('که', 'خدا', 'را'), ('خدا',), ('خدا', 'را'), ('خدا', 'را', 'شکر'), ('را',), ('را', 'شکر'), ('را', 'شکر', 'ورزشکاری'), ('شکر',), ('شکر', 'ورزشکاری'), ('شکر', 'ورزشکاری', 'نبود'), ('ورزشکاری',), ('ورزشکاری', 'نبود'), ('ورزشکاری', 'نبود', 'که'), ('نبود',), ('نبود', 'که'), ('نبود', 'که', 'با'), ('که',), ('که', 'با'), ('که', 'با', 'کرونا'), ('با',), ('با', 'کرونا'), ('با', 'کرونا', 'درگیر'), ('کرونا',), ('کرونا', 'درگیر'), ('کرونا', 'درگیر', 'باشد'), ('درگیر',), ('درگیر', 'باشد'), ('درگیر', 'باشد', 'اما'), ('باشد',), ('باشد', 'اما'), ('باشد', 'اما', 'در'), ('اما',), ('اما', 'در'), ('اما', 'در', 'بین'), ('در',), ('در', 'بین'), ('در', 'بین', 'همکاران'), ('بین',), ('بین', 'همکاران'), ('بین', 'همکاران', 'بودند'), ('همکاران',), ('همکاران', 'بودند'), ('همکاران', 'بودند', 'کسانی'), ('بودند',), ('بودند', 'کسانی'), ('بودند', 'کسانی', 'که'), ('کسانی',), ('کسانی', 'که'), ('کسانی', 'که', 'مبتلا'), ('که',), ('که', 'مبتلا'), ('که', 'مبتلا', 'شدند'), ('مبتلا',), ('مبتلا', 'شدند'), ('مبتلا', 'شدند', '</s>'), ('شدند',), ('شدند', '</s>'), ('شدند', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
      "\n",
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'حضرت'), ('<s>',), ('<s>', 'حضرت'), ('<s>', 'حضرت', 'رسول'), ('حضرت',), ('حضرت', 'رسول'), ('حضرت', 'رسول', 'در'), ('رسول',), ('رسول', 'در'), ('رسول', 'در', 'حلقه'), ('در',), ('در', 'حلقه'), ('در', 'حلقه', 'اصحاب'), ('حلقه',), ('حلقه', 'اصحاب'), ('حلقه', 'اصحاب', 'نشسته'), ('اصحاب',), ('اصحاب', 'نشسته'), ('اصحاب', 'نشسته', 'بودند'), ('نشسته',), ('نشسته', 'بودند'), ('نشسته', 'بودند', '</s>'), ('بودند',), ('بودند', '</s>'), ('بودند', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
      "\n",
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'این'), ('<s>',), ('<s>', 'این'), ('<s>', 'این', 'اتفاق'), ('این',), ('این', 'اتفاق'), ('این', 'اتفاق', 'باعث'), ('اتفاق',), ('اتفاق', 'باعث'), ('اتفاق', 'باعث', 'شد'), ('باعث',), ('باعث', 'شد'), ('باعث', 'شد', 'تا'), ('شد',), ('شد', 'تا'), ('شد', 'تا', 'نقش'), ('تا',), ('تا', 'نقش'), ('تا', 'نقش', 'ایالات'), ('نقش',), ('نقش', 'ایالات'), ('نقش', 'ایالات', 'متحده'), ('ایالات',), ('ایالات', 'متحده'), ('ایالات', 'متحده', 'به'), ('متحده',), ('متحده', 'به'), ('متحده', 'به', 'عنوان'), ('به',), ('به', 'عنوان'), ('به', 'عنوان', 'جایگزین'), ('عنوان',), ('عنوان', 'جایگزین'), ('عنوان', 'جایگزین', 'طالبان'), ('جایگزین',), ('جایگزین', 'طالبان'), ('جایگزین', 'طالبان', 'در'), ('طالبان',), ('طالبان', 'در'), ('طالبان', 'در', 'افغانستان'), ('در',), ('در', 'افغانستان'), ('در', 'افغانستان', 'حذف'), ('افغانستان',), ('افغانستان', 'حذف'), ('افغانستان', 'حذف', 'و'), ('حذف',), ('حذف', 'و'), ('حذف', 'و', 'جای'), ('و',), ('و', 'جای'), ('و', 'جای', 'آن'), ('جای',), ('جای', 'آن'), ('جای', 'آن', 'را'), ('آن',), ('آن', 'را'), ('آن', 'را', 'دولتی'), ('را',), ('را', 'دولتی'), ('را', 'دولتی', 'بگیرد'), ('دولتی',), ('دولتی', 'بگیرد'), ('دولتی', 'بگیرد', 'که'), ('بگیرد',), ('بگیرد', 'که'), ('بگیرد', 'که', 'بومی'), ('که',), ('که', 'بومی'), ('که', 'بومی', 'و'), ('بومی',), ('بومی', 'و'), ('بومی', 'و', 'از'), ('و',), ('و', 'از'), ('و', 'از', 'میان'), ('از',), ('از', 'میان'), ('از', 'میان', 'سیاستمداران'), ('میان',), ('میان', 'سیاستمداران'), ('میان', 'سیاستمداران', 'افغان'), ('سیاستمداران',), ('سیاستمداران', 'افغان'), ('سیاستمداران', 'افغان', 'انتخاب'), ('افغان',), ('افغان', 'انتخاب'), ('افغان', 'انتخاب', 'شده'), ('انتخاب',), ('انتخاب', 'شده'), ('انتخاب', 'شده', 'است'), ('شده',), ('شده', 'است'), ('شده', 'است', '</s>'), ('است',), ('است', '</s>'), ('است', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
      "\n",
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'وی'), ('<s>',), ('<s>', 'وی'), ('<s>', 'وی', 'اضافه'), ('وی',), ('وی', 'اضافه'), ('وی', 'اضافه', 'کرد'), ('اضافه',), ('اضافه', 'کرد'), ('اضافه', 'کرد', 'همه'), ('کرد',), ('کرد', 'همه'), ('کرد', 'همه', 'بخشهای'), ('همه',), ('همه', 'بخشهای'), ('همه', 'بخشهای', 'سازمان'), ('بخشهای',), ('بخشهای', 'سازمان'), ('بخشهای', 'سازمان', 'در'), ('سازمان',), ('سازمان', 'در'), ('سازمان', 'در', 'مقابله'), ('در',), ('در', 'مقابله'), ('در', 'مقابله', 'با'), ('مقابله',), ('مقابله', 'با'), ('مقابله', 'با', 'کرونا'), ('با',), ('با', 'کرونا'), ('با', 'کرونا', 'و'), ('کرونا',), ('کرونا', 'و'), ('کرونا', 'و', 'حفظ'), ('و',), ('و', 'حفظ'), ('و', 'حفظ', 'آرامش'), ('حفظ',), ('حفظ', 'آرامش'), ('حفظ', 'آرامش', 'مردم'), ('آرامش',), ('آرامش', 'مردم'), ('آرامش', 'مردم', 'از'), ('مردم',), ('مردم', 'از'), ('مردم', 'از', 'هیچ'), ('از',), ('از', 'هیچ'), ('از', 'هیچ', 'چیز'), ('هیچ',), ('هیچ', 'چیز'), ('هیچ', 'چیز', 'دریغ'), ('چیز',), ('چیز', 'دریغ'), ('چیز', 'دریغ', 'نمی'), ('دریغ',), ('دریغ', 'نمی'), ('دریغ', 'نمی', 'کنند'), ('نمی',), ('نمی', 'کنند'), ('نمی', 'کنند', 'و'), ('کنند',), ('کنند', 'و'), ('کنند', 'و', 'تلاش'), ('و',), ('و', 'تلاش'), ('و', 'تلاش', 'دارند'), ('تلاش',), ('تلاش', 'دارند'), ('تلاش', 'دارند', 'تا'), ('دارند',), ('دارند', 'تا'), ('دارند', 'تا', 'آخرین'), ('تا',), ('تا', 'آخرین'), ('تا', 'آخرین', 'لحظه'), ('آخرین',), ('آخرین', 'لحظه'), ('آخرین', 'لحظه', 'کار'), ('لحظه',), ('لحظه', 'کار'), ('لحظه', 'کار', 'کنند'), ('کار',), ('کار', 'کنند'), ('کار', 'کنند', 'و'), ('کنند',), ('کنند', 'و'), ('کنند', 'و', 'حتی'), ('و',), ('و', 'حتی'), ('و', 'حتی', 'از'), ('حتی',), ('حتی', 'از'), ('حتی', 'از', 'جان'), ('از',), ('از', 'جان'), ('از', 'جان', 'خود'), ('جان',), ('جان', 'خود'), ('جان', 'خود', 'هم'), ('خود',), ('خود', 'هم'), ('خود', 'هم', 'می'), ('هم',), ('هم', 'می'), ('هم', 'می', 'گذرند'), ('می',), ('می', 'گذرند'), ('می', 'گذرند', '</s>'), ('گذرند',), ('گذرند', '</s>'), ('گذرند', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
      "\n",
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'در'), ('<s>',), ('<s>', 'در'), ('<s>', 'در', 'روایات'), ('در',), ('در', 'روایات'), ('در', 'روایات', 'ما'), ('روایات',), ('روایات', 'ما'), ('روایات', 'ما', 'به'), ('ما',), ('ما', 'به'), ('ما', 'به', 'طور'), ('به',), ('به', 'طور'), ('به', 'طور', 'مکرر'), ('طور',), ('طور', 'مکرر'), ('طور', 'مکرر', 'از'), ('مکرر',), ('مکرر', 'از'), ('مکرر', 'از', 'دو'), ('از',), ('از', 'دو'), ('از', 'دو', 'غیبت'), ('دو',), ('دو', 'غیبت'), ('دو', 'غیبت', 'آن'), ('غیبت',), ('غیبت', 'آن'), ('غیبت', 'آن', 'حضرت'), ('آن',), ('آن', 'حضرت'), ('آن', 'حضرت', 'سخن'), ('حضرت',), ('حضرت', 'سخن'), ('حضرت', 'سخن', 'به'), ('سخن',), ('سخن', 'به'), ('سخن', 'به', 'میان'), ('به',), ('به', 'میان'), ('به', 'میان', 'آمده'), ('میان',), ('میان', 'آمده'), ('میان', 'آمده', 'و'), ('آمده',), ('آمده', 'و'), ('آمده', 'و', 'از'), ('و',), ('و', 'از'), ('و', 'از', 'سالها'), ('از',), ('از', 'سالها'), ('از', 'سالها', 'پیش'), ('سالها',), ('سالها', 'پیش'), ('سالها', 'پیش', 'از'), ('پیش',), ('پیش', 'از'), ('پیش', 'از', 'تولد'), ('از',), ('از', 'تولد'), ('از', 'تولد', 'امام'), ('تولد',), ('تولد', 'امام'), ('تولد', 'امام', 'مهدی'), ('امام',), ('امام', 'مهدی'), ('امام', 'مهدی', 'ع'), ('مهدی',), ('مهدی', 'ع'), ('مهدی', 'ع', 'بر'), ('ع',), ('ع', 'بر'), ('ع', 'بر', 'این'), ('بر',), ('بر', 'این'), ('بر', 'این', 'موضوع'), ('این',), ('این', 'موضوع'), ('این', 'موضوع', 'تصریح'), ('موضوع',), ('موضوع', 'تصریح'), ('موضوع', 'تصریح', 'شده'), ('تصریح',), ('تصریح', 'شده'), ('تصریح', 'شده', 'است'), ('شده',), ('شده', 'است'), ('شده', 'است', 'که'), ('است',), ('است', 'که'), ('است', 'که', 'حضرتش'), ('که',), ('که', 'حضرتش'), ('که', 'حضرتش', 'دو'), ('حضرتش',), ('حضرتش', 'دو'), ('حضرتش', 'دو', 'غیبت'), ('دو',), ('دو', 'غیبت'), ('دو', 'غیبت', 'خواهند'), ('غیبت',), ('غیبت', 'خواهند'), ('غیبت', 'خواهند', 'داشت'), ('خواهند',), ('خواهند', 'داشت'), ('خواهند', 'داشت', 'که'), ('داشت',), ('داشت', 'که'), ('داشت', 'که', 'هر'), ('که',), ('که', 'هر'), ('که', 'هر', 'یک'), ('هر',), ('هر', 'یک'), ('هر', 'یک', 'با'), ('یک',), ('یک', 'با'), ('یک', 'با', 'دیگری'), ('با',), ('با', 'دیگری'), ('با', 'دیگری', 'متفاوت'), ('دیگری',), ('دیگری', 'متفاوت'), ('دیگری', 'متفاوت', 'است'), ('متفاوت',), ('متفاوت', 'است'), ('متفاوت', 'است', '</s>'), ('است',), ('است', '</s>'), ('است', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
      "\n",
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'نکته'), ('<s>',), ('<s>', 'نکته'), ('<s>', 'نکته', 'مهم'), ('نکته',), ('نکته', 'مهم'), ('نکته', 'مهم', 'این'), ('مهم',), ('مهم', 'این'), ('مهم', 'این', 'است'), ('این',), ('این', 'است'), ('این', 'است', 'که'), ('است',), ('است', 'که'), ('است', 'که', 'نباید'), ('که',), ('که', 'نباید'), ('که', 'نباید', 'انتظار'), ('نباید',), ('نباید', 'انتظار'), ('نباید', 'انتظار', 'کار'), ('انتظار',), ('انتظار', 'کار'), ('انتظار', 'کار', 'ویژهای'), ('کار',), ('کار', 'ویژهای'), ('کار', 'ویژهای', 'را'), ('ویژهای',), ('ویژهای', 'را'), ('ویژهای', 'را', 'در'), ('را',), ('را', 'در'), ('را', 'در', 'مدت'), ('در',), ('در', 'مدت'), ('در', 'مدت', 'کوتاه'), ('مدت',), ('مدت', 'کوتاه'), ('مدت', 'کوتاه', 'داشت'), ('کوتاه',), ('کوتاه', 'داشت'), ('کوتاه', 'داشت', 'اما'), ('داشت',), ('داشت', 'اما'), ('داشت', 'اما', 'این'), ('اما',), ('اما', 'این'), ('اما', 'این', 'ریلگذاری'), ('این',), ('این', 'ریلگذاری'), ('این', 'ریلگذاری', 'ما'), ('ریلگذاری',), ('ریلگذاری', 'ما'), ('ریلگذاری', 'ما', 'را'), ('ما',), ('ما', 'را'), ('ما', 'را', 'به'), ('را',), ('را', 'به'), ('را', 'به', 'نقطه'), ('به',), ('به', 'نقطه'), ('به', 'نقطه', 'خوبی'), ('نقطه',), ('نقطه', 'خوبی'), ('نقطه', 'خوبی', 'خواهد'), ('خوبی',), ('خوبی', 'خواهد'), ('خوبی', 'خواهد', 'رساند'), ('خواهد',), ('خواهد', 'رساند'), ('خواهد', 'رساند', '</s>'), ('رساند',), ('رساند', '</s>'), ('رساند', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
      "\n",
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'ارتفاع'), ('<s>',), ('<s>', 'ارتفاع'), ('<s>', 'ارتفاع', 'پست'), ('ارتفاع',), ('ارتفاع', 'پست'), ('ارتفاع', 'پست', 'ابراهیم'), ('پست',), ('پست', 'ابراهیم'), ('پست', 'ابراهیم', 'حاتمیکیا'), ('ابراهیم',), ('ابراهیم', 'حاتمیکیا'), ('ابراهیم', 'حاتمیکیا', 'هم'), ('حاتمیکیا',), ('حاتمیکیا', 'هم'), ('حاتمیکیا', 'هم', 'پسزمینه'), ('هم',), ('هم', 'پسزمینه'), ('هم', 'پسزمینه', 'جنگ'), ('پسزمینه',), ('پسزمینه', 'جنگ'), ('پسزمینه', 'جنگ', 'دارد'), ('جنگ',), ('جنگ', 'دارد'), ('جنگ', 'دارد', 'و'), ('دارد',), ('دارد', 'و'), ('دارد', 'و', 'پیامدهای'), ('و',), ('و', 'پیامدهای'), ('و', 'پیامدهای', 'جنگ'), ('پیامدهای',), ('پیامدهای', 'جنگ'), ('پیامدهای', 'جنگ', 'را'), ('جنگ',), ('جنگ', 'را'), ('جنگ', 'را', 'بر'), ('را',), ('را', 'بر'), ('را', 'بر', 'زندگی'), ('بر',), ('بر', 'زندگی'), ('بر', 'زندگی', 'مردم'), ('زندگی',), ('زندگی', 'مردم'), ('زندگی', 'مردم', 'خوزستان'), ('مردم',), ('مردم', 'خوزستان'), ('مردم', 'خوزستان', 'روایت'), ('خوزستان',), ('خوزستان', 'روایت'), ('خوزستان', 'روایت', 'می'), ('روایت',), ('روایت', 'می'), ('روایت', 'می', 'کند'), ('می',), ('می', 'کند'), ('می', 'کند', 'و'), ('کند',), ('کند', 'و'), ('کند', 'و', 'سرانجام'), ('و',), ('و', 'سرانجام'), ('و', 'سرانجام', 'بمب؛'), ('سرانجام',), ('سرانجام', 'بمب؛'), ('سرانجام', 'بمب؛', 'یک'), ('بمب؛',), ('بمب؛', 'یک'), ('بمب؛', 'یک', 'عاشقانه'), ('یک',), ('یک', 'عاشقانه'), ('یک', 'عاشقانه', 'پیمان'), ('عاشقانه',), ('عاشقانه', 'پیمان'), ('عاشقانه', 'پیمان', 'معادی'), ('پیمان',), ('پیمان', 'معادی'), ('پیمان', 'معادی', 'که'), ('معادی',), ('معادی', 'که'), ('معادی', 'که', 'داستان'), ('که',), ('که', 'داستان'), ('که', 'داستان', 'آن'), ('داستان',), ('داستان', 'آن'), ('داستان', 'آن', 'در'), ('آن',), ('آن', 'در'), ('آن', 'در', 'دوران'), ('در',), ('در', 'دوران'), ('در', 'دوران', 'موشکباران'), ('دوران',), ('دوران', 'موشکباران'), ('دوران', 'موشکباران', 'شهرها'), ('موشکباران',), ('موشکباران', 'شهرها'), ('موشکباران', 'شهرها', 'روایت'), ('شهرها',), ('شهرها', 'روایت'), ('شهرها', 'روایت', 'می'), ('روایت',), ('روایت', 'می'), ('روایت', 'می', 'شود'), ('می',), ('می', 'شود'), ('می', 'شود', '</s>'), ('شود',), ('شود', '</s>'), ('شود', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
      "\n",
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'وی'), ('<s>',), ('<s>', 'وی'), ('<s>', 'وی', 'ادامه'), ('وی',), ('وی', 'ادامه'), ('وی', 'ادامه', 'داد'), ('ادامه',), ('ادامه', 'داد'), ('ادامه', 'داد', 'این'), ('داد',), ('داد', 'این'), ('داد', 'این', 'مسیر'), ('این',), ('این', 'مسیر'), ('این', 'مسیر', 'به'), ('مسیر',), ('مسیر', 'به'), ('مسیر', 'به', 'علت'), ('به',), ('به', 'علت'), ('به', 'علت', 'پایین'), ('علت',), ('علت', 'پایین'), ('علت', 'پایین', 'آمدن'), ('پایین',), ('پایین', 'آمدن'), ('پایین', 'آمدن', 'چند'), ('آمدن',), ('آمدن', 'چند'), ('آمدن', 'چند', 'قطعه'), ('چند',), ('چند', 'قطعه'), ('چند', 'قطعه', 'سنگ'), ('قطعه',), ('قطعه', 'سنگ'), ('قطعه', 'سنگ', 'بزرگ'), ('سنگ',), ('سنگ', 'بزرگ'), ('سنگ', 'بزرگ', 'بر'), ('بزرگ',), ('بزرگ', 'بر'), ('بزرگ', 'بر', 'اثر'), ('بر',), ('بر', 'اثر'), ('بر', 'اثر', 'برخورد'), ('اثر',), ('اثر', 'برخورد'), ('اثر', 'برخورد', 'صاعقه'), ('برخورد',), ('برخورد', 'صاعقه'), ('برخورد', 'صاعقه', 'با'), ('صاعقه',), ('صاعقه', 'با'), ('صاعقه', 'با', 'کوه'), ('با',), ('با', 'کوه'), ('با', 'کوه', 'مسدود'), ('کوه',), ('کوه', 'مسدود'), ('کوه', 'مسدود', 'شده'), ('مسدود',), ('مسدود', 'شده'), ('مسدود', 'شده', 'است'), ('شده',), ('شده', 'است'), ('شده', 'است', 'و'), ('است',), ('است', 'و'), ('است', 'و', 'در'), ('و',), ('و', 'در'), ('و', 'در', 'سایه'), ('در',), ('در', 'سایه'), ('در', 'سایه', 'تلاش'), ('سایه',), ('سایه', 'تلاش'), ('سایه', 'تلاش', 'راهدارای'), ('تلاش',), ('تلاش', 'راهدارای'), ('تلاش', 'راهدارای', 'و'), ('راهدارای',), ('راهدارای', 'و'), ('راهدارای', 'و', 'جابهجایی'), ('و',), ('و', 'جابهجایی'), ('و', 'جابهجایی', 'سنگهای'), ('جابهجایی',), ('جابهجایی', 'سنگهای'), ('جابهجایی', 'سنگهای', 'کوچک'), ('سنگهای',), ('سنگهای', 'کوچک'), ('سنگهای', 'کوچک', 'این'), ('کوچک',), ('کوچک', 'این'), ('کوچک', 'این', 'راه'), ('این',), ('این', 'راه'), ('این', 'راه', 'روستایی'), ('راه',), ('راه', 'روستایی'), ('راه', 'روستایی', 'به'), ('روستایی',), ('روستایی', 'به'), ('روستایی', 'به', 'صورت'), ('به',), ('به', 'صورت'), ('به', 'صورت', 'موقت'), ('صورت',), ('صورت', 'موقت'), ('صورت', 'موقت', 'و'), ('موقت',), ('موقت', 'و'), ('موقت', 'و', 'کنارگذر'), ('و',), ('و', 'کنارگذر'), ('و', 'کنارگذر', 'بازگشایی'), ('کنارگذر',), ('کنارگذر', 'بازگشایی'), ('کنارگذر', 'بازگشایی', 'شده'), ('بازگشایی',), ('بازگشایی', 'شده'), ('بازگشایی', 'شده', 'است'), ('شده',), ('شده', 'است'), ('شده', 'است', '</s>'), ('است',), ('است', '</s>'), ('است', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
      "\n",
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'اساسا'), ('<s>',), ('<s>', 'اساسا'), ('<s>', 'اساسا', 'حمایت'), ('اساسا',), ('اساسا', 'حمایت'), ('اساسا', 'حمایت', 'از'), ('حمایت',), ('حمایت', 'از'), ('حمایت', 'از', 'بخش'), ('از',), ('از', 'بخش'), ('از', 'بخش', 'کشاورزی'), ('بخش',), ('بخش', 'کشاورزی'), ('بخش', 'کشاورزی', 'و'), ('کشاورزی',), ('کشاورزی', 'و'), ('کشاورزی', 'و', 'غذا'), ('و',), ('و', 'غذا'), ('و', 'غذا', 'به'), ('غذا',), ('غذا', 'به'), ('غذا', 'به', 'لحاظ'), ('به',), ('به', 'لحاظ'), ('به', 'لحاظ', 'مبانی'), ('لحاظ',), ('لحاظ', 'مبانی'), ('لحاظ', 'مبانی', 'تئوریک'), ('مبانی',), ('مبانی', 'تئوریک'), ('مبانی', 'تئوریک', 'اجتناب'), ('تئوریک',), ('تئوریک', 'اجتناب'), ('تئوریک', 'اجتناب', 'ناپذیر'), ('اجتناب',), ('اجتناب', 'ناپذیر'), ('اجتناب', 'ناپذیر', 'است'), ('ناپذیر',), ('ناپذیر', 'است'), ('ناپذیر', 'است', '</s>'), ('است',), ('است', '</s>'), ('است', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The n-gram size\n",
    "n = 3\n",
    "\n",
    "ngram_data, padded = padded_everygram_pipeline(n, train_corpus[0:10])\n",
    "\n",
    "# Padding adds special tokens (start symbol <s>) and (end symbol </s>) to the text to signify sentence boundaries.\n",
    "# This ensures that n-grams near the edges of a sentence (e.g., start or end) still form complete n-grams\n",
    "\n",
    "print(\"PADDING:\")\n",
    "print(list(padded))\n",
    "\n",
    "# Unigrams: Individual words or tokens, e.g., ('<s>',), ('i',).\n",
    "# Bigrams: Pairs of tokens, e.g., ('<s>', '<s>'), ('<s>', 'i').\n",
    "# Trigrams: Groups of three tokens, e.g., ('<s>', '<s>', 'i'), ('<s>', 'i', 'j').\n",
    "# Padding ensures valid n-grams even at sentence start/end.\n",
    "\n",
    "print(\"\\n\\nNGRAMS:\")\n",
    "for ngrams in ngram_data:\n",
    "    print(list(ngrams))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T11:54:58.129371Z",
     "iopub.status.busy": "2024-12-14T11:54:58.128467Z",
     "iopub.status.idle": "2024-12-14T11:55:31.277440Z",
     "shell.execute_reply": "2024-12-14T11:55:31.276719Z",
     "shell.execute_reply.started": "2024-12-14T11:54:58.129336Z"
    },
    "id": "0HgY7n8rkM1r"
   },
   "outputs": [],
   "source": [
    "n=3\n",
    "\n",
    "# Train data is an iterator over the pre-processed input\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, train_corpus)\n",
    "\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T11:55:31.279283Z",
     "iopub.status.busy": "2024-12-14T11:55:31.279006Z",
     "iopub.status.idle": "2024-12-14T11:55:31.316615Z",
     "shell.execute_reply": "2024-12-14T11:55:31.315812Z",
     "shell.execute_reply.started": "2024-12-14T11:55:31.279256Z"
    },
    "id": "Ft6RCl4nkM1r",
    "outputId": "78bccee6-156b-412f-e6a7-4e7adad7c9ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:<Vocabulary with cutoff=1 unk_label='<UNK>' and 55403 items>\n",
      "\n",
      "\n",
      "Most Common Vocabs:[('<s>', 160000), ('</s>', 160000), ('و', 80746), ('در', 68572), ('به', 57237), ('از', 46721), ('این', 36094), ('که', 35742), ('می', 30567), ('را', 28858), ('است', 28668), ('با', 27397), ('برای', 13200), ('کرد', 10948), ('های', 9609), ('شود', 9249), ('شده', 8746), ('شد', 8683), ('یک', 8471), ('هم', 8272), ('آن', 7971), ('تا', 7306), ('بود', 7298), ('گفت', 7092), ('خود', 6457), ('بر', 6089), ('کند', 5706), ('وی', 5503), ('ایران', 5448), ('ما', 5310), ('سال', 5265), ('کشور', 5083), ('ها', 5016), ('نیز', 4921), ('باید', 4876), ('اما', 4795), ('کنند', 4612), ('کرونا', 4601), ('دارد', 4519), ('او', 4453), ('داد', 4269), ('مردم', 4114), ('کرده', 4098), ('استان', 3984), ('قرار', 3792), ('روز', 3527), ('یا', 3504), ('آنها', 3433), ('اینکه', 3430), ('نمی', 3360)]\n",
      "\n",
      "\n",
      "Least Common Vocabs:[('گوادالاخارا', 1), ('جالیسکو', 1), ('ورشوه', 1), ('چکارمیکنن', 1), ('روچه', 1), ('روو', 1), ('آمادگیاش', 1), ('بوردو', 1), ('۶۱۳', 1), ('نینوی', 1), ('سرطانزادیی', 1), ('چاپگرهای', 1), ('حقا', 1), ('جهانپوری', 1), ('مقابلت', 1), ('سیداحد', 1), ('یوزباشی', 1), ('آبرومندانهتر', 1), ('سرگیرند', 1), ('کوچولوهای', 1), ('بهانهگیر', 1), ('پایگاهایش', 1), ('حتیالامکان', 1), ('کنگیها', 1), ('گچپزان', 1), ('گزارشهایشان', 1), ('رسمیتر', 1), ('علنیتر', 1), ('دفترآیتالله', 1), ('دشنام', 1), ('نامتناسب', 1), ('غیرقابلتصور', 1), ('پایانناپذیر', 1), ('ادعاهایمان', 1), ('پران', 1), ('نگرشهای', 1), ('میخوان؟', 1), ('۰۲۵۳۷۸۴۱۱۳۰', 1), ('۰۲۵۳۷۸۴۱۱۳۱', 1), ('سوختکی', 1), ('پرسشگر', 1), ('ایولین', 1), ('دروازهبانان', 1), ('نفعمان', 1), ('کاردستیمان', 1), ('پرچمهایی', 1), ('تورات', 1), ('ونژاد', 1), ('علیفر', 1), ('نفروشد', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nVocabulary:{model.vocab}\\n\")\n",
    "print(f\"\\nMost Common Vocabs:{model.vocab.counts.most_common(50)}\\n\")\n",
    "print(f\"\\nLeast Common Vocabs:{model.vocab.counts.most_common()[-50:]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T11:55:36.035935Z",
     "iopub.status.busy": "2024-12-14T11:55:36.035196Z",
     "iopub.status.idle": "2024-12-14T11:55:36.364237Z",
     "shell.execute_reply": "2024-12-14T11:55:36.363321Z",
     "shell.execute_reply.started": "2024-12-14T11:55:36.035900Z"
    },
    "id": "nZQQ_nRmkM1r",
    "outputId": "0fdb485b-5a60-4104-ae58-8507a902aa70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 3 ngram orders and 6536241 ngrams>\n",
      "1032\n",
      "475\n",
      "397\n"
     ]
    }
   ],
   "source": [
    "print(model.counts)\n",
    "#('دانشگاه', 'علوم', 'پزشکی'): 397\n",
    "\n",
    "# counts for unigrams:\n",
    "print(model.counts['پزشکی']) # i.e. Count('not')\n",
    "\n",
    "# count for bigrams\n",
    "print(model.counts[['علوم']]['پزشکی']) # i.e. Count('not'|'was')\n",
    "\n",
    "# count for trigrams\n",
    "print(model.counts[['دانشگاه', 'علوم']]['پزشکی']) # i.e. Count('not'|'emma was')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T11:55:42.180893Z",
     "iopub.status.busy": "2024-12-14T11:55:42.180499Z",
     "iopub.status.idle": "2024-12-14T11:55:42.341306Z",
     "shell.execute_reply": "2024-12-14T11:55:42.340343Z",
     "shell.execute_reply.started": "2024-12-14T11:55:42.180863Z"
    },
    "id": "s89XTuzwkM1r",
    "outputId": "e9983099-44e4-4f57-b324-d8f399f7d983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probability of the word 'دانشگاه'\n",
      "0.00057\n",
      "0.00067\n",
      "\n",
      "Adjust for padding tokens\n",
      "1938747 320000\n",
      "0.00057\n",
      "\n",
      "Probabilities padding tokens\n",
      "0.07084\n",
      "0.07084\n"
     ]
    }
   ],
   "source": [
    "all_tokens = [tok for sent in train_corpus for tok in sent]\n",
    "num_tokens = len(all_tokens)\n",
    "num_sentences = len(train_corpus)\n",
    "\n",
    "model_score = model.score('دانشگاه')\n",
    "probability = model.counts['دانشگاه']/num_tokens\n",
    "\n",
    "\n",
    "print(\"\\nProbability of the word 'دانشگاه'\")\n",
    "print(\"{:.5f}\".format(model_score))\n",
    "print(\"{:.5f}\".format(probability))\n",
    "\n",
    "print(\"\\nAdjust for padding tokens\")\n",
    "all_padding_tokens = num_sentences * (n-1) * 2\n",
    "print(num_tokens, all_padding_tokens)\n",
    "\n",
    "adjusted_probability = model.counts['دانشگاه']/(num_tokens + all_padding_tokens)\n",
    "print(\"{:.5f}\".format(adjusted_probability))\n",
    "\n",
    "print(\"\\nProbabilities padding tokens\")\n",
    "print(\"{:.5f}\".format(model.score('<s>')))\n",
    "print(\"{:.5f}\".format(model.score('</s>')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T11:55:45.472411Z",
     "iopub.status.busy": "2024-12-14T11:55:45.472041Z",
     "iopub.status.idle": "2024-12-14T11:55:45.478257Z",
     "shell.execute_reply": "2024-12-14T11:55:45.477236Z",
     "shell.execute_reply.started": "2024-12-14T11:55:45.472380Z"
    },
    "id": "-y_dl5qUkM1s",
    "outputId": "a72b670e-2a93-4c74-d48e-1547bd78782d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7089552238805971\n",
      "0.9925\n"
     ]
    }
   ],
   "source": [
    "# bigram\n",
    "print(model.score('پزشکی', ['علوم']))  # P('not'|'is')\n",
    "\n",
    "# trigram\n",
    "print(model.score('پزشکی', ['دانشگاه', 'علوم']))  # P('not'|'emma is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T11:55:47.902853Z",
     "iopub.status.busy": "2024-12-14T11:55:47.902481Z",
     "iopub.status.idle": "2024-12-14T11:55:47.907722Z",
     "shell.execute_reply": "2024-12-14T11:55:47.906809Z",
     "shell.execute_reply.started": "2024-12-14T11:55:47.902823Z"
    },
    "id": "RPIyks-ikM1s",
    "outputId": "2dc4940a-4146-4dc1-f000-c6422da621cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09771689497716896\n",
      "-3.3552481680873885\n"
     ]
    }
   ],
   "source": [
    "# To avoid underflow when working with many small score values, we usually work with log probabilities instead.\n",
    "# This can be done with the `logscore` method.\n",
    "\n",
    "#('هزار', 'نفر'): 690\n",
    "print(model.score('نفر', ['هزار']))\n",
    "print(model.logscore('نفر', ['هزار']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T11:55:50.298665Z",
     "iopub.status.busy": "2024-12-14T11:55:50.297631Z",
     "iopub.status.idle": "2024-12-14T11:55:50.303493Z",
     "shell.execute_reply": "2024-12-14T11:55:50.302388Z",
     "shell.execute_reply.started": "2024-12-14T11:55:50.298617Z"
    },
    "id": "_pqzt0j1kM1s",
    "outputId": "145efc84-4409-43ad-8adc-23ad60c0697e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('در', 'دانشگاه', 'علوم', 'پزشکی', '<UNK>', 'قدم', 'می', 'زند')\n"
     ]
    }
   ],
   "source": [
    "# The vocabulary helps us handle words that have not occurred during training.\n",
    "# If we lookup the vocab on unseen sentences not from the training data,\n",
    "# it automatically replace words not in the vocabulary with `<UNK>`.\n",
    "\n",
    "print(model.vocab.lookup('در دانشگاه علوم پزشکی موژان قدم می زند'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T11:55:52.333898Z",
     "iopub.status.busy": "2024-12-14T11:55:52.333307Z",
     "iopub.status.idle": "2024-12-14T11:55:52.342482Z",
     "shell.execute_reply": "2024-12-14T11:55:52.341317Z",
     "shell.execute_reply.started": "2024-12-14T11:55:52.333842Z"
    },
    "id": "BZ543bGikM1s",
    "outputId": "e4e7bcd4-2990-41da-e250-873364277f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0.0 -inf\n",
      "0.0 -inf\n"
     ]
    }
   ],
   "source": [
    "# Items that are not seen during training are mapped to the vocabulary's \"unknown label\" token.  This is \"<UNK>\" by default.\n",
    "print(model.score(\"<UNK>\") == model.score(\"موژان\"))\n",
    "\n",
    "# The MLE model does not apply any smoothing, so the probability for UNK is 0\n",
    "print(model.score(\"<UNK>\"),model.logscore(\"<UNK>\") )\n",
    "\n",
    "# As a consequence, the probability for a phrase containing an unknown word is also 0.\n",
    "print(model.score('موژان', ['علوم', 'پزشکی']), model.logscore('موژان', ['علوم', 'پزشکی']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T11:56:12.433855Z",
     "iopub.status.busy": "2024-12-14T11:56:12.433204Z",
     "iopub.status.idle": "2024-12-14T11:57:32.999182Z",
     "shell.execute_reply": "2024-12-14T11:57:32.998310Z",
     "shell.execute_reply.started": "2024-12-14T11:56:12.433821Z"
    },
    "id": "E7vQX7YokM1s",
    "outputId": "35a2b58b-c350-4e66-c953-1ab1fd3c0f64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002547311276882486\n",
      "0.000392156862745098\n",
      "1.7896130856508822e-05\n",
      "\n",
      "-11.938737114411097\n",
      "-11.31628153174622\n",
      "-15.769992763902437\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import Laplace\n",
    "n = 5\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, train_corpus)\n",
    "smoothed_model_small =  Laplace(n)\n",
    "smoothed_model_small.fit(train_data, padded_sents)\n",
    "\n",
    "print(smoothed_model_small.score('علوم'))\n",
    "print(smoothed_model_small.score('پزشکی'))\n",
    "print(smoothed_model_small.score('موژان', ['علوم', 'پزشکی']))\n",
    "print()\n",
    "print(smoothed_model_small.logscore('علوم'))\n",
    "print(smoothed_model_small.logscore('پزشکی'))\n",
    "print(smoothed_model_small.logscore('موژان', ['علوم', 'پزشکی']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T11:57:33.001038Z",
     "iopub.status.busy": "2024-12-14T11:57:33.000762Z",
     "iopub.status.idle": "2024-12-14T11:57:33.009165Z",
     "shell.execute_reply": "2024-12-14T11:57:33.008390Z",
     "shell.execute_reply.started": "2024-12-14T11:57:33.001012Z"
    },
    "id": "W9nLawX7kM1t",
    "outputId": "801f7191-7c13-4170-a4fb-bf2472aeca81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['شیراز',\n",
       " 'در',\n",
       " 'پاسخ',\n",
       " 'به',\n",
       " 'پرسش',\n",
       " 'یکی',\n",
       " 'از',\n",
       " 'دانشجویان',\n",
       " 'مبنی',\n",
       " 'بر',\n",
       " 'آنکه',\n",
       " 'آیا',\n",
       " 'شما',\n",
       " 'هویدا',\n",
       " 'را',\n",
       " 'پیش',\n",
       " 'از',\n",
       " 'محاکمه',\n",
       " 'زدید',\n",
       " 'گفت',\n",
       " 'این',\n",
       " 'دروغ',\n",
       " 'است',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating text with n-gram model\n",
    "smoothed_model_small.generate(text_seed=[\"در\", \"دانشگاه\", \"علوم\", \"پزشکی\"], num_words=40, random_seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T11:58:01.044430Z",
     "iopub.status.busy": "2024-12-14T11:58:01.044069Z",
     "iopub.status.idle": "2024-12-14T11:58:11.754878Z",
     "shell.execute_reply": "2024-12-14T11:58:11.754015Z",
     "shell.execute_reply.started": "2024-12-14T11:58:01.044401Z"
    },
    "id": "du9zY6QhkM1t",
    "outputId": "b0556868-551d-49c0-b667-e52c26b5e3ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.82055196756583\n",
      "['۴', 'درصد', 'بوده', 'است']\n"
     ]
    }
   ],
   "source": [
    "# perplexity of n-gram model\n",
    "test_data, _ = padded_everygram_pipeline(n, val_corpus)\n",
    "\n",
    "perplexity = []\n",
    "for test in test_data:\n",
    "  perplexity.append(smoothed_model_small.perplexity(test))\n",
    "\n",
    "values = []\n",
    "for i in range(len(perplexity)):\n",
    "  if not np.isinf(perplexity[i]):\n",
    "    values.append(i)\n",
    "\n",
    "valid_perplexity = [perplexity[i] for i in values]\n",
    "idx = np.argpartition(valid_perplexity, 10)\n",
    "\n",
    "min = np.argmin(valid_perplexity)\n",
    "print(perplexity[min])\n",
    "print(val_corpus[min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T11:58:11.757282Z",
     "iopub.status.busy": "2024-12-14T11:58:11.756674Z",
     "iopub.status.idle": "2024-12-14T11:58:11.761908Z",
     "shell.execute_reply": "2024-12-14T11:58:11.761088Z",
     "shell.execute_reply.started": "2024-12-14T11:58:11.757241Z"
    },
    "id": "i3QVJSZUkM1t"
   },
   "outputs": [],
   "source": [
    "# Convert the list to a NumPy array\n",
    "values_array = np.array(perplexity)\n",
    "\n",
    "# Get the indices that would sort the array\n",
    "sorted_indices = np.argsort(values_array)\n",
    "\n",
    "# Get the first 10 indices of the smallest values\n",
    "top_20_indices = sorted_indices[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T18:54:30.876318Z",
     "iopub.status.busy": "2024-12-09T18:54:30.876066Z",
     "iopub.status.idle": "2024-12-09T18:54:30.889969Z",
     "shell.execute_reply": "2024-12-09T18:54:30.889229Z",
     "shell.execute_reply.started": "2024-12-09T18:54:30.876293Z"
    },
    "id": "TlWEZNG9kM1t",
    "outputId": "0abf1a58-dd5d-4ad9-e980-4fa2ea66931c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['بیشتر', 'استفاده', 'شود']):136.84879397221403\n",
      "(['۴', 'درصد', 'بوده_است']):137.37757536063077\n",
      "(['در', 'نظر', 'گرفتهایم']):155.21181914518752\n",
      "(['۲', 'درصد', 'اعلام', 'شد']):159.41539629904034\n",
      "(['۹', 'درصد', 'افزایش', 'است']):167.88098852565022\n",
      "(['۸', 'درصد', 'افزایش', 'داشته_است']):175.70886198671326\n",
      "(['۹', 'درصد', 'اعلام', 'شد']):183.26984989234003\n",
      "(['به', 'گزارش', 'همشهری', 'آنلاین', 'به', 'نقل', 'از', 'نشریه', 'آ']):188.5691340823012\n",
      "(['اینجا', 'چطور', 'است', '؟']):193.56459410705116\n",
      "(['آن', 'را', 'پیدا', 'کنید']):193.70443425101107\n",
      "(['۶', 'درصد', 'افزایش', 'داشته_است']):200.6723100682942\n",
      "(['این', 'سیاست', 'استکباری', 'است']):201.01668563197362\n",
      "(['۶', 'درصد', 'رشد', 'داشته_است']):204.0636579908888\n",
      "(['این', 'خسارت', 'همچنان', 'ادامه', 'دارد']):204.44801120950063\n",
      "(['در', 'همین', 'رابطه', 'بخوانید', '؛']):206.89433492071743\n",
      "(['۵', 'درصد', 'رشد', 'نشان', 'می', 'دهد']):212.98259222906185\n",
      "(['این', 'زمان', 'زیادی', 'نیست']):214.94726810950934\n",
      "(['در', 'جلسات', 'اطلاعاتی', 'مطرح', 'می', 'شود']):216.3339858482399\n",
      "(['۶', 'درصد', 'برآورد', 'شده_است']):218.54417523894674\n",
      "(['۵', 'متقاضی', 'وجود', 'دارد']):218.71649941201267\n"
     ]
    }
   ],
   "source": [
    "for i in top_20_indices:\n",
    "  print(\"({0}):{1}\".format(val_corpus[i], perplexity[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kb63rn4vkM1t"
   },
   "source": [
    "# NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-14T15:02:15.384864Z",
     "iopub.status.busy": "2024-12-14T15:02:15.384459Z",
     "iopub.status.idle": "2024-12-14T15:02:15.572612Z",
     "shell.execute_reply": "2024-12-14T15:02:15.571650Z",
     "shell.execute_reply.started": "2024-12-14T15:02:15.384825Z"
    },
    "id": "F4IYNq1hkM1u",
    "outputId": "98441051-9931-4046-d6a4-423fef175dd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 12669251\n",
      "<class 'str'>\n",
      "1\t۰۰۰ پرس غذا برگزار خواهد گردید که با توجه به شرایط کرونایی کشور همه این غذاها در سطح شهر و مناطق محروم توزیع خواهد گردید\n",
      "2\t۰۰۰ تن تجهیزات و متریال بیش از ۱۱۰ کیلومتر کابلکشی و ۵۰ کیلومتر لولهکشی در سایزهای مختلف برشمرد که نشاندهنده وسعت این طرح ملی است\n",
      "3\t۰۰۰ ریال بابت خرید ۱۲\n",
      "4\t۰۰۰ ریال تجاوز ننماید با رعایت مقررات بند ۱ ۴ ماده ۴۱ آئین نامه اجرایی موضوع ماده ۲۱۹ قانون مالیاتهای مستقیم نسبت به محاسبه و مطالبه مابه التفاوت مالیات متعلقه با رعایت مقررات اقدام خواهد شد\n",
      "5\t۰۰۰ ریال تسویه شده است\n",
      "6\t۰۰۰ ریال را میگفتن ۱۰۰\n",
      "7\t۰۰۰ ریال که در حال بازپرداخت اقساط باشند\n",
      "8\t۰۰۰ نفر استقبال کردند\n",
      "9\t۰۱۶ یوان چین اعلام شد\n",
      "10\t۰۱ درصد سهم سبد سوخت کشور کرده است که با توجه به تولید فعلی سالانه ۱۷ میلیون تن و شرایط تحریم و عدم صادرات کامل تولید مازاد بر مصرف چارهای جز مصرف در خودرو و مصرف در پتروشیمیهای نیست\n",
      "11\t۰۱ مجاز بوده و بخشنامه جدید جایگزین بخشنامه های متناظر قبلی کارمزدهای خدمات بانکی ریالی و الکترونیکی می شود\n",
      "12\t۰۲ درصد افزایش به ۱۰۵\n",
      "13\t۰۳ دلار کاهش ۱۸۹۲\n",
      "14\t۰۴ هزار دلار رسید تا در یکی از بالاترین س\n"
     ]
    }
   ],
   "source": [
    "# formatted text is the preprocessed text in the format of raw text\n",
    "# but it does not work (change it if you can :))\n",
    "formatted_text = \"\\n\".join([f\"{i+1}\\t{' '.join(sentence)}\" for i, sentence in enumerate(cleaned_sentences)])\n",
    "\n",
    "print(\"Total number of character:\", len(formatted_text))\n",
    "print(type(formatted_text))\n",
    "print(formatted_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-14T15:02:15.573962Z",
     "iopub.status.busy": "2024-12-14T15:02:15.573686Z",
     "iopub.status.idle": "2024-12-14T15:02:15.660990Z",
     "shell.execute_reply": "2024-12-14T15:02:15.660183Z",
     "shell.execute_reply.started": "2024-12-14T15:02:15.573937Z"
    },
    "id": "f9iQi-J_kM1u",
    "outputId": "0903cf0d-074b-4bcb-93f7-cd53a20f00bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t۰۰۰ پرس غذا برگزار خواهد گردید که با توجه به شرایط کرونایی کشور همه این غذاها در سطح شهر و مناطق محروم توزیع خواهد گردید.\n",
      "2\t۰۰۰ تن تجهیزات و متریال، بیش از ۱۱۰ کیلومتر کابلکشی و ۵۰ کیلومتر لولهکشی در سایزهای مختلف برشمرد که نشاندهنده وسعت این طرح ملی است.\n",
      "3\t۰۰۰ ریال بابت خرید ۱۲.\n",
      "4\t۰۰۰ ریال) تجاوز ننماید، با رعایت مقررات بند (۱-۴) ماده (۴۱) آئین نامه اجرایی موضوع ماده (۲۱۹) قانون مالیاتهای مستقیم نسبت به محاسبه و مطالبه مابه التفاوت مالیات متعلقه با رعایت مقررات اقدام خواهد شد.\n",
      "5\t۰۰۰ ریال تسویه شده است.\n",
      "6\t۰۰۰ریال را میگفتن ۱۰۰.\n",
      "7\t۰۰۰) ریال که در حال بازپرداخت اقساط باشند.\n",
      "8\t۰۰۰ نفر استقبال کردند.\"\n",
      "9\t۰۱۶ یوان چین اعلام شد.\n",
      "10\t۰۱ درصد سهم سبد سوخت کشور کرده است که با توجه به تولید فعلی سالانه ۱۷ میلیون تن LPG و شرایط تحریم و عدم صادرات کامل تولید مازاد بر مصرف، چارهای جز مصرف LPG در خودرو و مصرف در پتروشیمیهای PDH نیست.\n",
      "11\t۰۱) مجاز بوده و بخشنامه جدید جایگزین بخشنامه های متناظر قبلی کارمزدهای خدمات بانکی ریالی و الکترونیکی می شود.\n",
      "12\t۰۲ درصد افزایش به ۱۰۵.\n",
      "13\t۰۳ دلار کاهش ۱۸۹۲.\n",
      "14\t۰۴ ه\n"
     ]
    }
   ],
   "source": [
    "with open(persian_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print((raw_text[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333,
     "referenced_widgets": [
      "01bed83d6aa24f02b4e7db2bec55c4c4",
      "7cfe08cd17204325a054c2f91df50a71",
      "86d4c87a6dbc4b8c9d6e556a3f2a1cb6",
      "210269387ff0465abd4e0180e2f5aa9c",
      "27eda8e4981d405fa68d93b7e9253ecb",
      "89b9acbf76af403f981cedf3b4cfcf52",
      "83909cb2409b4fd098a047e13cb2f455",
      "e4ed39a07db24bed8f87a92f6f547ac9",
      "679bcc1607184edcba1541d44e83b086",
      "12849537afda475babb0f262a020a6b7",
      "d88feef5efa94b1aaea2cd93e14ae749",
      "8518d5962366481098a65e337956d3bc",
      "771bec1698054cd29c3249e29d0b763f",
      "22a6dcf3b5664d97afa9d4ec886f9d44",
      "d0a416701cbf48b0aef255b2bf66e053",
      "04b90e34c23748719c4e0701232c4768",
      "ae81cf31445e4b799f152e9d22c6cafc",
      "44159909c9454a89ae4f47d5108cc753",
      "25c99daad60a449ca3416381d78775dd",
      "1a34a113bff84f5bb2fd7a50f13f28a9",
      "a99954dd7ecb4fe3a00e4f0ab45ca91f",
      "a0e841ebaf2846398754e87c073c07d1",
      "dbaf318333ec413f88da4fdc8e2be402",
      "389fa9fd3f16427c9dc5352ee8d4de2d",
      "9c08d01af7d94448b15f6e8f58b930fc",
      "5c3568387cb64ea39f6cae2f9445d4d5",
      "8362237777d740e38e4b684bf8855ac5",
      "504430e2c5354bfa919ad804eeb19033",
      "2cf419a938734e58a2692aab2c3c8fbd",
      "d370637fd8dc404dbe9f820040347eea",
      "c99a918efed94df5a47365a354d63544",
      "dbded0f35c904710b6f1dc03077ffeb2",
      "db4c061e45b24ec2919d99942b1059c1",
      "29243de39619472b899ece2f3fd07fae",
      "ae3dcc70d836478baa062ec078d134f5",
      "56f5c75b40c148fe96e2da20f74e5a28",
      "5926d35075cf4f8ebce4f3e9a372e849",
      "69f2eb3543174ac19ffeeefe5404502e",
      "5e05c683a6b540b69e5b2e9a9bff86ce",
      "ef1fde69336d4dc7b6c899c62ad9db71",
      "11016f9bfda04e03a540dbf7dadf0e83",
      "a0deefa4eae84ce1b5c7dd494d0d2d82",
      "df2ff6d447e64ef1ba57a48d89c52a3b",
      "77f24dbcfa3d475b8cd2b207e922a3e3",
      "76afda07719549e4aee9317bc8fa3414",
      "19e205c527564c5893208d3042dbadfd",
      "7ddb0b8085014e67a5eac5721edd18f3",
      "dab3c6fdd5994f0ea90fb0925f81c4cd",
      "164252500b5c4fadb4a71e374eb59ccd",
      "adc332f5f7f9428988e874099f2aadb2",
      "4b36b320618b4a95a6f83d5fa0ddb38a",
      "3ac4e0a264ad408d80441b2a65efc2a7",
      "52a9618414cc4f9a99bfff752afa9b26",
      "22d8698acb254230b9ff4c19ba5fa2fb",
      "db612cd21e9a4ac385bc587c653ab51f"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-14T15:02:15.663878Z",
     "iopub.status.busy": "2024-12-14T15:02:15.663247Z",
     "iopub.status.idle": "2024-12-14T15:02:34.568271Z",
     "shell.execute_reply": "2024-12-14T15:02:34.567347Z",
     "shell.execute_reply.started": "2024-12-14T15:02:15.663833Z"
    },
    "id": "UOV_LS4_kM1u",
    "outputId": "a99d430a-b22b-429c-9049-05fd237043b5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d2be153a09446cbe17db6e0a1537ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a38e7549cb24d11a9538d2b2878daa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb0e4387eb6452e856117fc6d4e3088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/537k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e88b1ff8d049ed8eea72c0e26be752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.13M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49d2b8f9c8040ac8da92af5d74b6743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 3456103\n",
      "First 10 tokens: [5, 43, 24935, 17, 17, 17, 4961, 1626, 383, 137]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "# tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# bolbolzaban is a persian tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "\n",
    "# Tokenize the text\n",
    "token_ids = tokenizer.encode(raw_text)\n",
    "\n",
    "print(\"Total number of tokens:\", len(token_ids))\n",
    "print(\"First 10 tokens:\", token_ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T15:02:34.587920Z",
     "iopub.status.busy": "2024-12-14T15:02:34.587516Z",
     "iopub.status.idle": "2024-12-14T15:02:34.648028Z",
     "shell.execute_reply": "2024-12-14T15:02:34.647135Z",
     "shell.execute_reply.started": "2024-12-14T15:02:34.587893Z"
    },
    "id": "xjR7klwkkM1u"
   },
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, context_length):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of context_length\n",
    "        for i in range(0, len(token_ids) - context_length):\n",
    "            input_sequence = token_ids[i:i + context_length]\n",
    "\n",
    "            #shift to the right\n",
    "            target_sequence = token_ids[i + 1: i + context_length + 1]\n",
    "\n",
    "            # input and output are represented as tensors\n",
    "            self.input_ids.append(torch.tensor(input_sequence))\n",
    "            self.target_ids.append(torch.tensor(target_sequence))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader(txt, batch_size=8, context_length=4, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    # tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDataset(txt, tokenizer, context_length)\n",
    "    train, dev, test = torch.utils.data.random_split(dataset, [0.8,0.1,0.1])\n",
    "\n",
    "    # Create dataloader\n",
    "    train_dataloader = DataLoader(\n",
    "        train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    dev_dataloader = DataLoader(\n",
    "        dev,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_dataloader, dev_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvkgTQiQkM1v"
   },
   "source": [
    "## simple nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T11:59:04.156337Z",
     "iopub.status.busy": "2024-12-14T11:59:04.155487Z",
     "iopub.status.idle": "2024-12-14T11:59:04.161802Z",
     "shell.execute_reply": "2024-12-14T11:59:04.160952Z",
     "shell.execute_reply.started": "2024-12-14T11:59:04.156306Z"
    },
    "id": "P3-z5NufkM1v"
   },
   "outputs": [],
   "source": [
    "class SimpleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_length):\n",
    "        super(SimpleLanguageModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(context_length, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        token_embeds = self.token_embedding(x)\n",
    "        position_embeds = self.position_embedding(positions)\n",
    "\n",
    "        embeddings = token_embeds + position_embeds\n",
    "        logits = self.linear(embeddings)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmmPPYUakM1v"
   },
   "source": [
    "## regularized nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T15:02:34.650350Z",
     "iopub.status.busy": "2024-12-14T15:02:34.649614Z",
     "iopub.status.idle": "2024-12-14T15:02:34.665857Z",
     "shell.execute_reply": "2024-12-14T15:02:34.665142Z",
     "shell.execute_reply.started": "2024-12-14T15:02:34.650322Z"
    },
    "id": "1HMZryLukM1v"
   },
   "outputs": [],
   "source": [
    "class RegularizedLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_length, dropout=0.2):\n",
    "        super(RegularizedLanguageModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(context_length, embedding_dim)\n",
    "        # This is new!\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        token_embeds = self.token_embedding(x)\n",
    "        position_embeds = self.position_embedding(positions)\n",
    "\n",
    "        embeddings = token_embeds + position_embeds\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        logits = self.linear(embeddings)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fEmJjZbmZuU"
   },
   "source": [
    "## setting up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T11:59:27.667373Z",
     "iopub.status.busy": "2024-12-14T11:59:27.666664Z",
     "iopub.status.idle": "2024-12-14T12:00:44.509911Z",
     "shell.execute_reply": "2024-12-14T12:00:44.509159Z",
     "shell.execute_reply.started": "2024-12-14T11:59:27.667343Z"
    },
    "id": "zjYW3poLl8CJ",
    "outputId": "827bf868-e3fb-4f3e-88fd-a6c4a375c6fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 128\n",
    "context_length = 32  # Context size for training\n",
    "# vocab_size = tokenizer.n_vocab\n",
    "vocab_size = 30000\n",
    "embedding_dim = 128\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_dataloader(\n",
    "    raw_text[:9999297], batch_size=batch_size,\n",
    "    context_length=context_length, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvDOS8VUnz8G"
   },
   "source": [
    "## If you want to see the result for any of the models above. Select the cell (below) with the name of the model and run it, then run other cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOTzrMv4nP4M"
   },
   "source": [
    "## setting up and training the simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWcNbinYnKVl"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = SimpleLanguageModel(vocab_size, embedding_dim, context_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMt__AqvkM1w"
   },
   "source": [
    "## setting up and training the regularized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:17:07.011088Z",
     "iopub.status.busy": "2024-12-14T12:17:07.010686Z",
     "iopub.status.idle": "2024-12-14T12:17:07.085567Z",
     "shell.execute_reply": "2024-12-14T12:17:07.084908Z",
     "shell.execute_reply.started": "2024-12-14T12:17:07.011058Z"
    },
    "id": "9FZESFcdkM1w"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = RegularizedLanguageModel(vocab_size, embedding_dim, context_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZt7uzBBkM1w"
   },
   "source": [
    "## Plotting the Loss and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-14T12:17:08.889928Z",
     "iopub.status.busy": "2024-12-14T12:17:08.889131Z",
     "iopub.status.idle": "2024-12-14T12:31:32.178708Z",
     "shell.execute_reply": "2024-12-14T12:31:32.177667Z",
     "shell.execute_reply.started": "2024-12-14T12:17:08.889892Z"
    },
    "id": "3iRETgGJkM1w",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "9464875e-3085-498c-8c2f-566478ca5b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [0/16869], Loss: 10.7069\n",
      "Epoch [1/2], Step [20/16869], Loss: 10.0134\n",
      "Epoch [1/2], Step [40/16869], Loss: 9.3026\n",
      "Epoch [1/2], Step [60/16869], Loss: 8.6582\n",
      "Epoch [1/2], Step [80/16869], Loss: 7.9338\n",
      "Epoch [1/2], Step [100/16869], Loss: 7.4142\n",
      "Epoch [1/2], Step [120/16869], Loss: 6.9028\n",
      "Epoch [1/2], Step [140/16869], Loss: 6.7764\n",
      "Epoch [1/2], Step [160/16869], Loss: 6.5467\n",
      "Epoch [1/2], Step [180/16869], Loss: 6.4092\n",
      "Epoch [1/2], Step [200/16869], Loss: 6.3443\n",
      "Epoch [1/2], Step [220/16869], Loss: 6.3124\n",
      "Epoch [1/2], Step [240/16869], Loss: 6.2930\n",
      "Epoch [1/2], Step [260/16869], Loss: 6.1343\n",
      "Epoch [1/2], Step [280/16869], Loss: 6.1124\n",
      "Epoch [1/2], Step [300/16869], Loss: 6.1238\n",
      "Epoch [1/2], Step [320/16869], Loss: 5.9978\n",
      "Epoch [1/2], Step [340/16869], Loss: 5.9316\n",
      "Epoch [1/2], Step [360/16869], Loss: 6.0134\n",
      "Epoch [1/2], Step [380/16869], Loss: 5.9090\n",
      "Epoch [1/2], Step [400/16869], Loss: 5.9864\n",
      "Epoch [1/2], Step [420/16869], Loss: 5.8169\n",
      "Epoch [1/2], Step [440/16869], Loss: 5.9319\n",
      "Epoch [1/2], Step [460/16869], Loss: 5.8498\n",
      "Epoch [1/2], Step [480/16869], Loss: 5.8888\n",
      "Epoch [1/2], Step [500/16869], Loss: 5.8317\n",
      "Epoch [1/2], Step [520/16869], Loss: 5.8149\n",
      "Epoch [1/2], Step [540/16869], Loss: 5.7960\n",
      "Epoch [1/2], Step [560/16869], Loss: 5.8023\n",
      "Epoch [1/2], Step [580/16869], Loss: 5.8158\n",
      "Epoch [1/2], Step [600/16869], Loss: 5.8056\n",
      "Epoch [1/2], Step [620/16869], Loss: 5.7679\n",
      "Epoch [1/2], Step [640/16869], Loss: 5.8235\n",
      "Epoch [1/2], Step [660/16869], Loss: 5.7561\n",
      "Epoch [1/2], Step [680/16869], Loss: 5.7089\n",
      "Epoch [1/2], Step [700/16869], Loss: 5.6410\n",
      "Epoch [1/2], Step [720/16869], Loss: 5.6164\n",
      "Epoch [1/2], Step [740/16869], Loss: 5.7562\n",
      "Epoch [1/2], Step [760/16869], Loss: 5.6762\n",
      "Epoch [1/2], Step [780/16869], Loss: 5.6001\n",
      "Epoch [1/2], Step [800/16869], Loss: 5.6383\n",
      "Epoch [1/2], Step [820/16869], Loss: 5.5669\n",
      "Epoch [1/2], Step [840/16869], Loss: 5.6154\n",
      "Epoch [1/2], Step [860/16869], Loss: 5.7124\n",
      "Epoch [1/2], Step [880/16869], Loss: 5.7160\n",
      "Epoch [1/2], Step [900/16869], Loss: 5.4957\n",
      "Epoch [1/2], Step [920/16869], Loss: 5.7131\n",
      "Epoch [1/2], Step [940/16869], Loss: 5.4829\n",
      "Epoch [1/2], Step [960/16869], Loss: 5.5858\n",
      "Epoch [1/2], Step [980/16869], Loss: 5.4755\n",
      "Epoch [1/2], Step [1000/16869], Loss: 5.5238\n",
      "Epoch [1/2], Step [1020/16869], Loss: 5.6031\n",
      "Epoch [1/2], Step [1040/16869], Loss: 5.4611\n",
      "Epoch [1/2], Step [1060/16869], Loss: 5.4977\n",
      "Epoch [1/2], Step [1080/16869], Loss: 5.5393\n",
      "Epoch [1/2], Step [1100/16869], Loss: 5.5485\n",
      "Epoch [1/2], Step [1120/16869], Loss: 5.5266\n",
      "Epoch [1/2], Step [1140/16869], Loss: 5.4853\n",
      "Epoch [1/2], Step [1160/16869], Loss: 5.4813\n",
      "Epoch [1/2], Step [1180/16869], Loss: 5.5991\n",
      "Epoch [1/2], Step [1200/16869], Loss: 5.5124\n",
      "Epoch [1/2], Step [1220/16869], Loss: 5.6002\n",
      "Epoch [1/2], Step [1240/16869], Loss: 5.4972\n",
      "Epoch [1/2], Step [1260/16869], Loss: 5.4611\n",
      "Epoch [1/2], Step [1280/16869], Loss: 5.5176\n",
      "Epoch [1/2], Step [1300/16869], Loss: 5.3276\n",
      "Epoch [1/2], Step [1320/16869], Loss: 5.4344\n",
      "Epoch [1/2], Step [1340/16869], Loss: 5.5040\n",
      "Epoch [1/2], Step [1360/16869], Loss: 5.4669\n",
      "Epoch [1/2], Step [1380/16869], Loss: 5.2944\n",
      "Epoch [1/2], Step [1400/16869], Loss: 5.4643\n",
      "Epoch [1/2], Step [1420/16869], Loss: 5.4625\n",
      "Epoch [1/2], Step [1440/16869], Loss: 5.4837\n",
      "Epoch [1/2], Step [1460/16869], Loss: 5.5057\n",
      "Epoch [1/2], Step [1480/16869], Loss: 5.3871\n",
      "Epoch [1/2], Step [1500/16869], Loss: 5.4941\n",
      "Epoch [1/2], Step [1520/16869], Loss: 5.4401\n",
      "Epoch [1/2], Step [1540/16869], Loss: 5.4135\n",
      "Epoch [1/2], Step [1560/16869], Loss: 5.3945\n",
      "Epoch [1/2], Step [1580/16869], Loss: 5.4428\n",
      "Epoch [1/2], Step [1600/16869], Loss: 5.4978\n",
      "Epoch [1/2], Step [1620/16869], Loss: 5.3864\n",
      "Epoch [1/2], Step [1640/16869], Loss: 5.3228\n",
      "Epoch [1/2], Step [1660/16869], Loss: 5.4384\n",
      "Epoch [1/2], Step [1680/16869], Loss: 5.3486\n",
      "Epoch [1/2], Step [1700/16869], Loss: 5.3401\n",
      "Epoch [1/2], Step [1720/16869], Loss: 5.3851\n",
      "Epoch [1/2], Step [1740/16869], Loss: 5.3647\n",
      "Epoch [1/2], Step [1760/16869], Loss: 5.4670\n",
      "Epoch [1/2], Step [1780/16869], Loss: 5.4337\n",
      "Epoch [1/2], Step [1800/16869], Loss: 5.2929\n",
      "Epoch [1/2], Step [1820/16869], Loss: 5.3130\n",
      "Epoch [1/2], Step [1840/16869], Loss: 5.4494\n",
      "Epoch [1/2], Step [1860/16869], Loss: 5.4101\n",
      "Epoch [1/2], Step [1880/16869], Loss: 5.3195\n",
      "Epoch [1/2], Step [1900/16869], Loss: 5.3305\n",
      "Epoch [1/2], Step [1920/16869], Loss: 5.1881\n",
      "Epoch [1/2], Step [1940/16869], Loss: 5.3689\n",
      "Epoch [1/2], Step [1960/16869], Loss: 5.3224\n",
      "Epoch [1/2], Step [1980/16869], Loss: 5.3717\n",
      "Epoch [1/2], Step [2000/16869], Loss: 5.3839\n",
      "Epoch [1/2], Step [2020/16869], Loss: 5.3549\n",
      "Epoch [1/2], Step [2040/16869], Loss: 5.3092\n",
      "Epoch [1/2], Step [2060/16869], Loss: 5.3378\n",
      "Epoch [1/2], Step [2080/16869], Loss: 5.2351\n",
      "Epoch [1/2], Step [2100/16869], Loss: 5.3493\n",
      "Epoch [1/2], Step [2120/16869], Loss: 5.4043\n",
      "Epoch [1/2], Step [2140/16869], Loss: 5.4186\n",
      "Epoch [1/2], Step [2160/16869], Loss: 5.2830\n",
      "Epoch [1/2], Step [2180/16869], Loss: 5.1971\n",
      "Epoch [1/2], Step [2200/16869], Loss: 5.2400\n",
      "Epoch [1/2], Step [2220/16869], Loss: 5.1594\n",
      "Epoch [1/2], Step [2240/16869], Loss: 5.3646\n",
      "Epoch [1/2], Step [2260/16869], Loss: 5.1830\n",
      "Epoch [1/2], Step [2280/16869], Loss: 5.3471\n",
      "Epoch [1/2], Step [2300/16869], Loss: 5.1994\n",
      "Epoch [1/2], Step [2320/16869], Loss: 5.2886\n",
      "Epoch [1/2], Step [2340/16869], Loss: 5.3501\n",
      "Epoch [1/2], Step [2360/16869], Loss: 5.2920\n",
      "Epoch [1/2], Step [2380/16869], Loss: 5.3705\n",
      "Epoch [1/2], Step [2400/16869], Loss: 5.2218\n",
      "Epoch [1/2], Step [2420/16869], Loss: 5.2722\n",
      "Epoch [1/2], Step [2440/16869], Loss: 5.2565\n",
      "Epoch [1/2], Step [2460/16869], Loss: 5.2277\n",
      "Epoch [1/2], Step [2480/16869], Loss: 5.3043\n",
      "Epoch [1/2], Step [2500/16869], Loss: 5.2926\n",
      "Epoch [1/2], Step [2520/16869], Loss: 5.2507\n",
      "Epoch [1/2], Step [2540/16869], Loss: 5.2969\n",
      "Epoch [1/2], Step [2560/16869], Loss: 5.1970\n",
      "Epoch [1/2], Step [2580/16869], Loss: 5.3122\n",
      "Epoch [1/2], Step [2600/16869], Loss: 5.2286\n",
      "Epoch [1/2], Step [2620/16869], Loss: 5.2650\n",
      "Epoch [1/2], Step [2640/16869], Loss: 5.1696\n",
      "Epoch [1/2], Step [2660/16869], Loss: 5.1599\n",
      "Epoch [1/2], Step [2680/16869], Loss: 5.2081\n",
      "Epoch [1/2], Step [2700/16869], Loss: 5.0840\n",
      "Epoch [1/2], Step [2720/16869], Loss: 5.2713\n",
      "Epoch [1/2], Step [2740/16869], Loss: 5.2891\n",
      "Epoch [1/2], Step [2760/16869], Loss: 5.1503\n",
      "Epoch [1/2], Step [2780/16869], Loss: 5.2940\n",
      "Epoch [1/2], Step [2800/16869], Loss: 5.1695\n",
      "Epoch [1/2], Step [2820/16869], Loss: 5.2864\n",
      "Epoch [1/2], Step [2840/16869], Loss: 5.1380\n",
      "Epoch [1/2], Step [2860/16869], Loss: 5.1651\n",
      "Epoch [1/2], Step [2880/16869], Loss: 5.2482\n",
      "Epoch [1/2], Step [2900/16869], Loss: 5.2333\n",
      "Epoch [1/2], Step [2920/16869], Loss: 5.1917\n",
      "Epoch [1/2], Step [2940/16869], Loss: 5.2415\n",
      "Epoch [1/2], Step [2960/16869], Loss: 5.2299\n",
      "Epoch [1/2], Step [2980/16869], Loss: 5.2254\n",
      "Epoch [1/2], Step [3000/16869], Loss: 5.1631\n",
      "Epoch [1/2], Step [3020/16869], Loss: 5.1838\n",
      "Epoch [1/2], Step [3040/16869], Loss: 5.1637\n",
      "Epoch [1/2], Step [3060/16869], Loss: 5.2126\n",
      "Epoch [1/2], Step [3080/16869], Loss: 5.1772\n",
      "Epoch [1/2], Step [3100/16869], Loss: 5.1589\n",
      "Epoch [1/2], Step [3120/16869], Loss: 5.0932\n",
      "Epoch [1/2], Step [3140/16869], Loss: 5.1089\n",
      "Epoch [1/2], Step [3160/16869], Loss: 5.0866\n",
      "Epoch [1/2], Step [3180/16869], Loss: 5.1667\n",
      "Epoch [1/2], Step [3200/16869], Loss: 5.2466\n",
      "Epoch [1/2], Step [3220/16869], Loss: 5.2240\n",
      "Epoch [1/2], Step [3240/16869], Loss: 5.2044\n",
      "Epoch [1/2], Step [3260/16869], Loss: 5.0851\n",
      "Epoch [1/2], Step [3280/16869], Loss: 5.1081\n",
      "Epoch [1/2], Step [3300/16869], Loss: 5.0422\n",
      "Epoch [1/2], Step [3320/16869], Loss: 5.1173\n",
      "Epoch [1/2], Step [3340/16869], Loss: 5.0935\n",
      "Epoch [1/2], Step [3360/16869], Loss: 5.1502\n",
      "Epoch [1/2], Step [3380/16869], Loss: 5.0697\n",
      "Epoch [1/2], Step [3400/16869], Loss: 5.1783\n",
      "Epoch [1/2], Step [3420/16869], Loss: 5.2248\n",
      "Epoch [1/2], Step [3440/16869], Loss: 5.1009\n",
      "Epoch [1/2], Step [3460/16869], Loss: 5.1321\n",
      "Epoch [1/2], Step [3480/16869], Loss: 5.0702\n",
      "Epoch [1/2], Step [3500/16869], Loss: 5.1052\n",
      "Epoch [1/2], Step [3520/16869], Loss: 5.1530\n",
      "Epoch [1/2], Step [3540/16869], Loss: 5.2188\n",
      "Epoch [1/2], Step [3560/16869], Loss: 5.2018\n",
      "Epoch [1/2], Step [3580/16869], Loss: 5.0995\n",
      "Epoch [1/2], Step [3600/16869], Loss: 5.0951\n",
      "Epoch [1/2], Step [3620/16869], Loss: 5.0931\n",
      "Epoch [1/2], Step [3640/16869], Loss: 5.0773\n",
      "Epoch [1/2], Step [3660/16869], Loss: 5.1163\n",
      "Epoch [1/2], Step [3680/16869], Loss: 5.1404\n",
      "Epoch [1/2], Step [3700/16869], Loss: 5.0902\n",
      "Epoch [1/2], Step [3720/16869], Loss: 5.1749\n",
      "Epoch [1/2], Step [3740/16869], Loss: 5.1644\n",
      "Epoch [1/2], Step [3760/16869], Loss: 5.1613\n",
      "Epoch [1/2], Step [3780/16869], Loss: 5.1471\n",
      "Epoch [1/2], Step [3800/16869], Loss: 5.1136\n",
      "Epoch [1/2], Step [3820/16869], Loss: 5.2390\n",
      "Epoch [1/2], Step [3840/16869], Loss: 5.0259\n",
      "Epoch [1/2], Step [3860/16869], Loss: 4.9813\n",
      "Epoch [1/2], Step [3880/16869], Loss: 5.0715\n",
      "Epoch [1/2], Step [3900/16869], Loss: 5.1182\n",
      "Epoch [1/2], Step [3920/16869], Loss: 5.1217\n",
      "Epoch [1/2], Step [3940/16869], Loss: 5.1426\n",
      "Epoch [1/2], Step [3960/16869], Loss: 5.1434\n",
      "Epoch [1/2], Step [3980/16869], Loss: 4.9886\n",
      "Epoch [1/2], Step [4000/16869], Loss: 5.0769\n",
      "Epoch [1/2], Step [4020/16869], Loss: 5.0423\n",
      "Epoch [1/2], Step [4040/16869], Loss: 5.0769\n",
      "Epoch [1/2], Step [4060/16869], Loss: 5.1181\n",
      "Epoch [1/2], Step [4080/16869], Loss: 5.0345\n",
      "Epoch [1/2], Step [4100/16869], Loss: 5.0968\n",
      "Epoch [1/2], Step [4120/16869], Loss: 5.1379\n",
      "Epoch [1/2], Step [4140/16869], Loss: 5.1166\n",
      "Epoch [1/2], Step [4160/16869], Loss: 5.0295\n",
      "Epoch [1/2], Step [4180/16869], Loss: 5.0045\n",
      "Epoch [1/2], Step [4200/16869], Loss: 5.1411\n",
      "Epoch [1/2], Step [4220/16869], Loss: 4.9402\n",
      "Epoch [1/2], Step [4240/16869], Loss: 4.9714\n",
      "Epoch [1/2], Step [4260/16869], Loss: 5.0249\n",
      "Epoch [1/2], Step [4280/16869], Loss: 5.0437\n",
      "Epoch [1/2], Step [4300/16869], Loss: 5.1147\n",
      "Epoch [1/2], Step [4320/16869], Loss: 5.0854\n",
      "Epoch [1/2], Step [4340/16869], Loss: 4.9703\n",
      "Epoch [1/2], Step [4360/16869], Loss: 4.9675\n",
      "Epoch [1/2], Step [4380/16869], Loss: 5.0541\n",
      "Epoch [1/2], Step [4400/16869], Loss: 5.0918\n",
      "Epoch [1/2], Step [4420/16869], Loss: 4.9838\n",
      "Epoch [1/2], Step [4440/16869], Loss: 4.9650\n",
      "Epoch [1/2], Step [4460/16869], Loss: 4.9475\n",
      "Epoch [1/2], Step [4480/16869], Loss: 5.1070\n",
      "Epoch [1/2], Step [4500/16869], Loss: 5.0446\n",
      "Epoch [1/2], Step [4520/16869], Loss: 5.0703\n",
      "Epoch [1/2], Step [4540/16869], Loss: 5.1733\n",
      "Epoch [1/2], Step [4560/16869], Loss: 5.1323\n",
      "Epoch [1/2], Step [4580/16869], Loss: 5.0128\n",
      "Epoch [1/2], Step [4600/16869], Loss: 5.0773\n",
      "Epoch [1/2], Step [4620/16869], Loss: 5.0057\n",
      "Epoch [1/2], Step [4640/16869], Loss: 5.0055\n",
      "Epoch [1/2], Step [4660/16869], Loss: 4.9744\n",
      "Epoch [1/2], Step [4680/16869], Loss: 4.9936\n",
      "Epoch [1/2], Step [4700/16869], Loss: 5.0556\n",
      "Epoch [1/2], Step [4720/16869], Loss: 4.9693\n",
      "Epoch [1/2], Step [4740/16869], Loss: 5.1066\n",
      "Epoch [1/2], Step [4760/16869], Loss: 5.0055\n",
      "Epoch [1/2], Step [4780/16869], Loss: 4.9608\n",
      "Epoch [1/2], Step [4800/16869], Loss: 5.0455\n",
      "Epoch [1/2], Step [4820/16869], Loss: 5.0483\n",
      "Epoch [1/2], Step [4840/16869], Loss: 4.9601\n",
      "Epoch [1/2], Step [4860/16869], Loss: 4.9540\n",
      "Epoch [1/2], Step [4880/16869], Loss: 5.0407\n",
      "Epoch [1/2], Step [4900/16869], Loss: 5.0572\n",
      "Epoch [1/2], Step [4920/16869], Loss: 5.0886\n",
      "Epoch [1/2], Step [4940/16869], Loss: 5.0397\n",
      "Epoch [1/2], Step [4960/16869], Loss: 4.9582\n",
      "Epoch [1/2], Step [4980/16869], Loss: 5.0061\n",
      "Epoch [1/2], Step [5000/16869], Loss: 5.0390\n",
      "Epoch [1/2], Step [5020/16869], Loss: 5.0372\n",
      "Epoch [1/2], Step [5040/16869], Loss: 5.0419\n",
      "Epoch [1/2], Step [5060/16869], Loss: 5.0387\n",
      "Epoch [1/2], Step [5080/16869], Loss: 4.9668\n",
      "Epoch [1/2], Step [5100/16869], Loss: 5.0745\n",
      "Epoch [1/2], Step [5120/16869], Loss: 5.0744\n",
      "Epoch [1/2], Step [5140/16869], Loss: 4.8890\n",
      "Epoch [1/2], Step [5160/16869], Loss: 5.0269\n",
      "Epoch [1/2], Step [5180/16869], Loss: 5.0656\n",
      "Epoch [1/2], Step [5200/16869], Loss: 5.0302\n",
      "Epoch [1/2], Step [5220/16869], Loss: 4.9756\n",
      "Epoch [1/2], Step [5240/16869], Loss: 5.0374\n",
      "Epoch [1/2], Step [5260/16869], Loss: 4.9800\n",
      "Epoch [1/2], Step [5280/16869], Loss: 4.9853\n",
      "Epoch [1/2], Step [5300/16869], Loss: 4.9947\n",
      "Epoch [1/2], Step [5320/16869], Loss: 5.0277\n",
      "Epoch [1/2], Step [5340/16869], Loss: 4.9600\n",
      "Epoch [1/2], Step [5360/16869], Loss: 4.9695\n",
      "Epoch [1/2], Step [5380/16869], Loss: 5.0221\n",
      "Epoch [1/2], Step [5400/16869], Loss: 5.0922\n",
      "Epoch [1/2], Step [5420/16869], Loss: 4.9300\n",
      "Epoch [1/2], Step [5440/16869], Loss: 4.9634\n",
      "Epoch [1/2], Step [5460/16869], Loss: 5.0261\n",
      "Epoch [1/2], Step [5480/16869], Loss: 5.0223\n",
      "Epoch [1/2], Step [5500/16869], Loss: 4.9718\n",
      "Epoch [1/2], Step [5520/16869], Loss: 5.0055\n",
      "Epoch [1/2], Step [5540/16869], Loss: 4.8978\n",
      "Epoch [1/2], Step [5560/16869], Loss: 5.0503\n",
      "Epoch [1/2], Step [5580/16869], Loss: 4.9430\n",
      "Epoch [1/2], Step [5600/16869], Loss: 4.9978\n",
      "Epoch [1/2], Step [5620/16869], Loss: 4.9777\n",
      "Epoch [1/2], Step [5640/16869], Loss: 4.9813\n",
      "Epoch [1/2], Step [5660/16869], Loss: 5.0292\n",
      "Epoch [1/2], Step [5680/16869], Loss: 4.9380\n",
      "Epoch [1/2], Step [5700/16869], Loss: 4.9972\n",
      "Epoch [1/2], Step [5720/16869], Loss: 4.9849\n",
      "Epoch [1/2], Step [5740/16869], Loss: 4.9086\n",
      "Epoch [1/2], Step [5760/16869], Loss: 4.9829\n",
      "Epoch [1/2], Step [5780/16869], Loss: 4.9364\n",
      "Epoch [1/2], Step [5800/16869], Loss: 4.9946\n",
      "Epoch [1/2], Step [5820/16869], Loss: 5.0014\n",
      "Epoch [1/2], Step [5840/16869], Loss: 4.9888\n",
      "Epoch [1/2], Step [5860/16869], Loss: 4.9332\n",
      "Epoch [1/2], Step [5880/16869], Loss: 4.9007\n",
      "Epoch [1/2], Step [5900/16869], Loss: 4.9302\n",
      "Epoch [1/2], Step [5920/16869], Loss: 4.9708\n",
      "Epoch [1/2], Step [5940/16869], Loss: 4.9278\n",
      "Epoch [1/2], Step [5960/16869], Loss: 4.8950\n",
      "Epoch [1/2], Step [5980/16869], Loss: 4.9876\n",
      "Epoch [1/2], Step [6000/16869], Loss: 4.8162\n",
      "Epoch [1/2], Step [6020/16869], Loss: 4.9611\n",
      "Epoch [1/2], Step [6040/16869], Loss: 4.9278\n",
      "Epoch [1/2], Step [6060/16869], Loss: 4.9081\n",
      "Epoch [1/2], Step [6080/16869], Loss: 4.9464\n",
      "Epoch [1/2], Step [6100/16869], Loss: 4.9175\n",
      "Epoch [1/2], Step [6120/16869], Loss: 4.8955\n",
      "Epoch [1/2], Step [6140/16869], Loss: 5.0338\n",
      "Epoch [1/2], Step [6160/16869], Loss: 5.0273\n",
      "Epoch [1/2], Step [6180/16869], Loss: 4.9498\n",
      "Epoch [1/2], Step [6200/16869], Loss: 4.9353\n",
      "Epoch [1/2], Step [6220/16869], Loss: 4.9209\n",
      "Epoch [1/2], Step [6240/16869], Loss: 4.9328\n",
      "Epoch [1/2], Step [6260/16869], Loss: 5.0248\n",
      "Epoch [1/2], Step [6280/16869], Loss: 4.9248\n",
      "Epoch [1/2], Step [6300/16869], Loss: 5.0058\n",
      "Epoch [1/2], Step [6320/16869], Loss: 4.9977\n",
      "Epoch [1/2], Step [6340/16869], Loss: 4.9037\n",
      "Epoch [1/2], Step [6360/16869], Loss: 4.9816\n",
      "Epoch [1/2], Step [6380/16869], Loss: 4.9334\n",
      "Epoch [1/2], Step [6400/16869], Loss: 4.8655\n",
      "Epoch [1/2], Step [6420/16869], Loss: 5.0432\n",
      "Epoch [1/2], Step [6440/16869], Loss: 4.9197\n",
      "Epoch [1/2], Step [6460/16869], Loss: 4.8946\n",
      "Epoch [1/2], Step [6480/16869], Loss: 4.9110\n",
      "Epoch [1/2], Step [6500/16869], Loss: 4.8866\n",
      "Epoch [1/2], Step [6520/16869], Loss: 4.9669\n",
      "Epoch [1/2], Step [6540/16869], Loss: 4.9892\n",
      "Epoch [1/2], Step [6560/16869], Loss: 4.9341\n",
      "Epoch [1/2], Step [6580/16869], Loss: 4.9527\n",
      "Epoch [1/2], Step [6600/16869], Loss: 4.9536\n",
      "Epoch [1/2], Step [6620/16869], Loss: 4.7806\n",
      "Epoch [1/2], Step [6640/16869], Loss: 4.9655\n",
      "Epoch [1/2], Step [6660/16869], Loss: 4.9448\n",
      "Epoch [1/2], Step [6680/16869], Loss: 4.9715\n",
      "Epoch [1/2], Step [6700/16869], Loss: 4.9165\n",
      "Epoch [1/2], Step [6720/16869], Loss: 4.8804\n",
      "Epoch [1/2], Step [6740/16869], Loss: 4.9385\n",
      "Epoch [1/2], Step [6760/16869], Loss: 4.9618\n",
      "Epoch [1/2], Step [6780/16869], Loss: 4.9926\n",
      "Epoch [1/2], Step [6800/16869], Loss: 4.8378\n",
      "Epoch [1/2], Step [6820/16869], Loss: 4.9179\n",
      "Epoch [1/2], Step [6840/16869], Loss: 4.8648\n",
      "Epoch [1/2], Step [6860/16869], Loss: 4.9415\n",
      "Epoch [1/2], Step [6880/16869], Loss: 4.8776\n",
      "Epoch [1/2], Step [6900/16869], Loss: 5.0067\n",
      "Epoch [1/2], Step [6920/16869], Loss: 4.9667\n",
      "Epoch [1/2], Step [6940/16869], Loss: 4.8602\n",
      "Epoch [1/2], Step [6960/16869], Loss: 4.8527\n",
      "Epoch [1/2], Step [6980/16869], Loss: 4.9712\n",
      "Epoch [1/2], Step [7000/16869], Loss: 4.9160\n",
      "Epoch [1/2], Step [7020/16869], Loss: 4.9698\n",
      "Epoch [1/2], Step [7040/16869], Loss: 4.8925\n",
      "Epoch [1/2], Step [7060/16869], Loss: 4.9779\n",
      "Epoch [1/2], Step [7080/16869], Loss: 4.8406\n",
      "Epoch [1/2], Step [7100/16869], Loss: 4.9010\n",
      "Epoch [1/2], Step [7120/16869], Loss: 4.9125\n",
      "Epoch [1/2], Step [7140/16869], Loss: 4.9755\n",
      "Epoch [1/2], Step [7160/16869], Loss: 4.8975\n",
      "Epoch [1/2], Step [7180/16869], Loss: 4.7922\n",
      "Epoch [1/2], Step [7200/16869], Loss: 4.9021\n",
      "Epoch [1/2], Step [7220/16869], Loss: 4.9803\n",
      "Epoch [1/2], Step [7240/16869], Loss: 4.8298\n",
      "Epoch [1/2], Step [7260/16869], Loss: 4.8184\n",
      "Epoch [1/2], Step [7280/16869], Loss: 4.8733\n",
      "Epoch [1/2], Step [7300/16869], Loss: 4.8399\n",
      "Epoch [1/2], Step [7320/16869], Loss: 4.9847\n",
      "Epoch [1/2], Step [7340/16869], Loss: 4.9333\n",
      "Epoch [1/2], Step [7360/16869], Loss: 4.8946\n",
      "Epoch [1/2], Step [7380/16869], Loss: 4.8989\n",
      "Epoch [1/2], Step [7400/16869], Loss: 4.8374\n",
      "Epoch [1/2], Step [7420/16869], Loss: 4.9400\n",
      "Epoch [1/2], Step [7440/16869], Loss: 4.8662\n",
      "Epoch [1/2], Step [7460/16869], Loss: 4.9226\n",
      "Epoch [1/2], Step [7480/16869], Loss: 4.8850\n",
      "Epoch [1/2], Step [7500/16869], Loss: 4.9005\n",
      "Epoch [1/2], Step [7520/16869], Loss: 4.9290\n",
      "Epoch [1/2], Step [7540/16869], Loss: 4.9583\n",
      "Epoch [1/2], Step [7560/16869], Loss: 4.9015\n",
      "Epoch [1/2], Step [7580/16869], Loss: 4.8702\n",
      "Epoch [1/2], Step [7600/16869], Loss: 4.9487\n",
      "Epoch [1/2], Step [7620/16869], Loss: 4.8674\n",
      "Epoch [1/2], Step [7640/16869], Loss: 4.8489\n",
      "Epoch [1/2], Step [7660/16869], Loss: 4.9652\n",
      "Epoch [1/2], Step [7680/16869], Loss: 4.8192\n",
      "Epoch [1/2], Step [7700/16869], Loss: 4.9122\n",
      "Epoch [1/2], Step [7720/16869], Loss: 4.8440\n",
      "Epoch [1/2], Step [7740/16869], Loss: 4.8574\n",
      "Epoch [1/2], Step [7760/16869], Loss: 4.8929\n",
      "Epoch [1/2], Step [7780/16869], Loss: 4.8899\n",
      "Epoch [1/2], Step [7800/16869], Loss: 4.9503\n",
      "Epoch [1/2], Step [7820/16869], Loss: 4.8567\n",
      "Epoch [1/2], Step [7840/16869], Loss: 4.8114\n",
      "Epoch [1/2], Step [7860/16869], Loss: 4.8846\n",
      "Epoch [1/2], Step [7880/16869], Loss: 4.8638\n",
      "Epoch [1/2], Step [7900/16869], Loss: 4.8765\n",
      "Epoch [1/2], Step [7920/16869], Loss: 4.8875\n",
      "Epoch [1/2], Step [7940/16869], Loss: 4.9131\n",
      "Epoch [1/2], Step [7960/16869], Loss: 4.9173\n",
      "Epoch [1/2], Step [7980/16869], Loss: 4.8385\n",
      "Epoch [1/2], Step [8000/16869], Loss: 4.8635\n",
      "Epoch [1/2], Step [8020/16869], Loss: 4.9418\n",
      "Epoch [1/2], Step [8040/16869], Loss: 4.8003\n",
      "Epoch [1/2], Step [8060/16869], Loss: 4.9413\n",
      "Epoch [1/2], Step [8080/16869], Loss: 4.8907\n",
      "Epoch [1/2], Step [8100/16869], Loss: 4.8723\n",
      "Epoch [1/2], Step [8120/16869], Loss: 4.7679\n",
      "Epoch [1/2], Step [8140/16869], Loss: 4.9124\n",
      "Epoch [1/2], Step [8160/16869], Loss: 4.9237\n",
      "Epoch [1/2], Step [8180/16869], Loss: 4.8334\n",
      "Epoch [1/2], Step [8200/16869], Loss: 4.8625\n",
      "Epoch [1/2], Step [8220/16869], Loss: 4.7031\n",
      "Epoch [1/2], Step [8240/16869], Loss: 4.8412\n",
      "Epoch [1/2], Step [8260/16869], Loss: 4.8732\n",
      "Epoch [1/2], Step [8280/16869], Loss: 4.8572\n",
      "Epoch [1/2], Step [8300/16869], Loss: 4.9135\n",
      "Epoch [1/2], Step [8320/16869], Loss: 4.8190\n",
      "Epoch [1/2], Step [8340/16869], Loss: 4.8054\n",
      "Epoch [1/2], Step [8360/16869], Loss: 4.9536\n",
      "Epoch [1/2], Step [8380/16869], Loss: 4.9000\n",
      "Epoch [1/2], Step [8400/16869], Loss: 4.8765\n",
      "Epoch [1/2], Step [8420/16869], Loss: 4.8696\n",
      "Epoch [1/2], Step [8440/16869], Loss: 4.8752\n",
      "Epoch [1/2], Step [8460/16869], Loss: 4.9377\n",
      "Epoch [1/2], Step [8480/16869], Loss: 4.8958\n",
      "Epoch [1/2], Step [8500/16869], Loss: 4.9006\n",
      "Epoch [1/2], Step [8520/16869], Loss: 4.8938\n",
      "Epoch [1/2], Step [8540/16869], Loss: 4.8403\n",
      "Epoch [1/2], Step [8560/16869], Loss: 4.8664\n",
      "Epoch [1/2], Step [8580/16869], Loss: 4.9968\n",
      "Epoch [1/2], Step [8600/16869], Loss: 4.8710\n",
      "Epoch [1/2], Step [8620/16869], Loss: 4.8270\n",
      "Epoch [1/2], Step [8640/16869], Loss: 4.8811\n",
      "Epoch [1/2], Step [8660/16869], Loss: 4.9245\n",
      "Epoch [1/2], Step [8680/16869], Loss: 4.8763\n",
      "Epoch [1/2], Step [8700/16869], Loss: 4.9107\n",
      "Epoch [1/2], Step [8720/16869], Loss: 4.8455\n",
      "Epoch [1/2], Step [8740/16869], Loss: 4.9128\n",
      "Epoch [1/2], Step [8760/16869], Loss: 4.9582\n",
      "Epoch [1/2], Step [8780/16869], Loss: 4.7666\n",
      "Epoch [1/2], Step [8800/16869], Loss: 4.8780\n",
      "Epoch [1/2], Step [8820/16869], Loss: 4.8842\n",
      "Epoch [1/2], Step [8840/16869], Loss: 4.9931\n",
      "Epoch [1/2], Step [8860/16869], Loss: 4.8440\n",
      "Epoch [1/2], Step [8880/16869], Loss: 4.9020\n",
      "Epoch [1/2], Step [8900/16869], Loss: 4.7910\n",
      "Epoch [1/2], Step [8920/16869], Loss: 4.9225\n",
      "Epoch [1/2], Step [8940/16869], Loss: 4.7796\n",
      "Epoch [1/2], Step [8960/16869], Loss: 4.8681\n",
      "Epoch [1/2], Step [8980/16869], Loss: 4.8911\n",
      "Epoch [1/2], Step [9000/16869], Loss: 4.7826\n",
      "Epoch [1/2], Step [9020/16869], Loss: 4.8355\n",
      "Epoch [1/2], Step [9040/16869], Loss: 4.8332\n",
      "Epoch [1/2], Step [9060/16869], Loss: 4.8604\n",
      "Epoch [1/2], Step [9080/16869], Loss: 4.8686\n",
      "Epoch [1/2], Step [9100/16869], Loss: 4.8042\n",
      "Epoch [1/2], Step [9120/16869], Loss: 4.9006\n",
      "Epoch [1/2], Step [9140/16869], Loss: 4.8377\n",
      "Epoch [1/2], Step [9160/16869], Loss: 4.7973\n",
      "Epoch [1/2], Step [9180/16869], Loss: 4.8515\n",
      "Epoch [1/2], Step [9200/16869], Loss: 4.8063\n",
      "Epoch [1/2], Step [9220/16869], Loss: 4.8770\n",
      "Epoch [1/2], Step [9240/16869], Loss: 4.7801\n",
      "Epoch [1/2], Step [9260/16869], Loss: 4.8770\n",
      "Epoch [1/2], Step [9280/16869], Loss: 4.8948\n",
      "Epoch [1/2], Step [9300/16869], Loss: 4.8073\n",
      "Epoch [1/2], Step [9320/16869], Loss: 4.8406\n",
      "Epoch [1/2], Step [9340/16869], Loss: 4.8811\n",
      "Epoch [1/2], Step [9360/16869], Loss: 4.8607\n",
      "Epoch [1/2], Step [9380/16869], Loss: 4.9755\n",
      "Epoch [1/2], Step [9400/16869], Loss: 4.8806\n",
      "Epoch [1/2], Step [9420/16869], Loss: 4.9079\n",
      "Epoch [1/2], Step [9440/16869], Loss: 4.8969\n",
      "Epoch [1/2], Step [9460/16869], Loss: 4.8170\n",
      "Epoch [1/2], Step [9480/16869], Loss: 4.8003\n",
      "Epoch [1/2], Step [9500/16869], Loss: 4.7983\n",
      "Epoch [1/2], Step [9520/16869], Loss: 4.8148\n",
      "Epoch [1/2], Step [9540/16869], Loss: 4.8589\n",
      "Epoch [1/2], Step [9560/16869], Loss: 4.8321\n",
      "Epoch [1/2], Step [9580/16869], Loss: 4.7599\n",
      "Epoch [1/2], Step [9600/16869], Loss: 4.8197\n",
      "Epoch [1/2], Step [9620/16869], Loss: 4.8367\n",
      "Epoch [1/2], Step [9640/16869], Loss: 4.8811\n",
      "Epoch [1/2], Step [9660/16869], Loss: 4.8479\n",
      "Epoch [1/2], Step [9680/16869], Loss: 4.7920\n",
      "Epoch [1/2], Step [9700/16869], Loss: 4.8903\n",
      "Epoch [1/2], Step [9720/16869], Loss: 4.8203\n",
      "Epoch [1/2], Step [9740/16869], Loss: 4.8964\n",
      "Epoch [1/2], Step [9760/16869], Loss: 4.7723\n",
      "Epoch [1/2], Step [9780/16869], Loss: 4.8680\n",
      "Epoch [1/2], Step [9800/16869], Loss: 4.8421\n",
      "Epoch [1/2], Step [9820/16869], Loss: 4.8218\n",
      "Epoch [1/2], Step [9840/16869], Loss: 4.7716\n",
      "Epoch [1/2], Step [9860/16869], Loss: 4.8088\n",
      "Epoch [1/2], Step [9880/16869], Loss: 4.8567\n",
      "Epoch [1/2], Step [9900/16869], Loss: 4.8490\n",
      "Epoch [1/2], Step [9920/16869], Loss: 4.8641\n",
      "Epoch [1/2], Step [9940/16869], Loss: 4.7530\n",
      "Epoch [1/2], Step [9960/16869], Loss: 4.8186\n",
      "Epoch [1/2], Step [9980/16869], Loss: 4.7698\n",
      "Epoch [1/2], Step [10000/16869], Loss: 4.8406\n",
      "Epoch [1/2], Step [10020/16869], Loss: 4.8123\n",
      "Epoch [1/2], Step [10040/16869], Loss: 4.8020\n",
      "Epoch [1/2], Step [10060/16869], Loss: 4.8226\n",
      "Epoch [1/2], Step [10080/16869], Loss: 4.8085\n",
      "Epoch [1/2], Step [10100/16869], Loss: 4.7909\n",
      "Epoch [1/2], Step [10120/16869], Loss: 4.8542\n",
      "Epoch [1/2], Step [10140/16869], Loss: 4.7785\n",
      "Epoch [1/2], Step [10160/16869], Loss: 4.8378\n",
      "Epoch [1/2], Step [10180/16869], Loss: 4.7452\n",
      "Epoch [1/2], Step [10200/16869], Loss: 4.7369\n",
      "Epoch [1/2], Step [10220/16869], Loss: 4.7584\n",
      "Epoch [1/2], Step [10240/16869], Loss: 4.8630\n",
      "Epoch [1/2], Step [10260/16869], Loss: 4.7445\n",
      "Epoch [1/2], Step [10280/16869], Loss: 4.8795\n",
      "Epoch [1/2], Step [10300/16869], Loss: 4.8793\n",
      "Epoch [1/2], Step [10320/16869], Loss: 4.8294\n",
      "Epoch [1/2], Step [10340/16869], Loss: 4.8353\n",
      "Epoch [1/2], Step [10360/16869], Loss: 4.8493\n",
      "Epoch [1/2], Step [10380/16869], Loss: 4.8353\n",
      "Epoch [1/2], Step [10400/16869], Loss: 4.8712\n",
      "Epoch [1/2], Step [10420/16869], Loss: 4.9113\n",
      "Epoch [1/2], Step [10440/16869], Loss: 4.7618\n",
      "Epoch [1/2], Step [10460/16869], Loss: 4.8187\n",
      "Epoch [1/2], Step [10480/16869], Loss: 4.7996\n",
      "Epoch [1/2], Step [10500/16869], Loss: 4.8914\n",
      "Epoch [1/2], Step [10520/16869], Loss: 4.7204\n",
      "Epoch [1/2], Step [10540/16869], Loss: 4.9002\n",
      "Epoch [1/2], Step [10560/16869], Loss: 4.8437\n",
      "Epoch [1/2], Step [10580/16869], Loss: 4.7482\n",
      "Epoch [1/2], Step [10600/16869], Loss: 4.9098\n",
      "Epoch [1/2], Step [10620/16869], Loss: 4.8269\n",
      "Epoch [1/2], Step [10640/16869], Loss: 4.7731\n",
      "Epoch [1/2], Step [10660/16869], Loss: 4.7201\n",
      "Epoch [1/2], Step [10680/16869], Loss: 4.7779\n",
      "Epoch [1/2], Step [10700/16869], Loss: 4.8862\n",
      "Epoch [1/2], Step [10720/16869], Loss: 4.8359\n",
      "Epoch [1/2], Step [10740/16869], Loss: 4.7598\n",
      "Epoch [1/2], Step [10760/16869], Loss: 4.8510\n",
      "Epoch [1/2], Step [10780/16869], Loss: 4.7219\n",
      "Epoch [1/2], Step [10800/16869], Loss: 4.7979\n",
      "Epoch [1/2], Step [10820/16869], Loss: 4.7393\n",
      "Epoch [1/2], Step [10840/16869], Loss: 4.6945\n",
      "Epoch [1/2], Step [10860/16869], Loss: 4.8491\n",
      "Epoch [1/2], Step [10880/16869], Loss: 4.7910\n",
      "Epoch [1/2], Step [10900/16869], Loss: 4.8944\n",
      "Epoch [1/2], Step [10920/16869], Loss: 4.7681\n",
      "Epoch [1/2], Step [10940/16869], Loss: 4.8495\n",
      "Epoch [1/2], Step [10960/16869], Loss: 4.7918\n",
      "Epoch [1/2], Step [10980/16869], Loss: 4.8735\n",
      "Epoch [1/2], Step [11000/16869], Loss: 4.7158\n",
      "Epoch [1/2], Step [11020/16869], Loss: 4.7638\n",
      "Epoch [1/2], Step [11040/16869], Loss: 4.7730\n",
      "Epoch [1/2], Step [11060/16869], Loss: 4.7085\n",
      "Epoch [1/2], Step [11080/16869], Loss: 4.8958\n",
      "Epoch [1/2], Step [11100/16869], Loss: 4.7774\n",
      "Epoch [1/2], Step [11120/16869], Loss: 4.8110\n",
      "Epoch [1/2], Step [11140/16869], Loss: 4.7280\n",
      "Epoch [1/2], Step [11160/16869], Loss: 4.7825\n",
      "Epoch [1/2], Step [11180/16869], Loss: 4.8449\n",
      "Epoch [1/2], Step [11200/16869], Loss: 4.8429\n",
      "Epoch [1/2], Step [11220/16869], Loss: 4.7624\n",
      "Epoch [1/2], Step [11240/16869], Loss: 4.9022\n",
      "Epoch [1/2], Step [11260/16869], Loss: 4.8176\n",
      "Epoch [1/2], Step [11280/16869], Loss: 4.7556\n",
      "Epoch [1/2], Step [11300/16869], Loss: 4.8421\n",
      "Epoch [1/2], Step [11320/16869], Loss: 4.8158\n",
      "Epoch [1/2], Step [11340/16869], Loss: 4.8139\n",
      "Epoch [1/2], Step [11360/16869], Loss: 4.7520\n",
      "Epoch [1/2], Step [11380/16869], Loss: 4.8284\n",
      "Epoch [1/2], Step [11400/16869], Loss: 4.7554\n",
      "Epoch [1/2], Step [11420/16869], Loss: 4.7825\n",
      "Epoch [1/2], Step [11440/16869], Loss: 4.8231\n",
      "Epoch [1/2], Step [11460/16869], Loss: 4.7485\n",
      "Epoch [1/2], Step [11480/16869], Loss: 4.8610\n",
      "Epoch [1/2], Step [11500/16869], Loss: 4.7698\n",
      "Epoch [1/2], Step [11520/16869], Loss: 4.7126\n",
      "Epoch [1/2], Step [11540/16869], Loss: 4.7066\n",
      "Epoch [1/2], Step [11560/16869], Loss: 4.8236\n",
      "Epoch [1/2], Step [11580/16869], Loss: 4.7190\n",
      "Epoch [1/2], Step [11600/16869], Loss: 4.8237\n",
      "Epoch [1/2], Step [11620/16869], Loss: 4.8553\n",
      "Epoch [1/2], Step [11640/16869], Loss: 4.7507\n",
      "Epoch [1/2], Step [11660/16869], Loss: 4.8213\n",
      "Epoch [1/2], Step [11680/16869], Loss: 4.7889\n",
      "Epoch [1/2], Step [11700/16869], Loss: 4.7737\n",
      "Epoch [1/2], Step [11720/16869], Loss: 4.8005\n",
      "Epoch [1/2], Step [11740/16869], Loss: 4.6746\n",
      "Epoch [1/2], Step [11760/16869], Loss: 4.8432\n",
      "Epoch [1/2], Step [11780/16869], Loss: 4.8459\n",
      "Epoch [1/2], Step [11800/16869], Loss: 4.8590\n",
      "Epoch [1/2], Step [11820/16869], Loss: 4.8686\n",
      "Epoch [1/2], Step [11840/16869], Loss: 4.8258\n",
      "Epoch [1/2], Step [11860/16869], Loss: 4.8342\n",
      "Epoch [1/2], Step [11880/16869], Loss: 4.7966\n",
      "Epoch [1/2], Step [11900/16869], Loss: 4.8275\n",
      "Epoch [1/2], Step [11920/16869], Loss: 4.7726\n",
      "Epoch [1/2], Step [11940/16869], Loss: 4.7541\n",
      "Epoch [1/2], Step [11960/16869], Loss: 4.7623\n",
      "Epoch [1/2], Step [11980/16869], Loss: 4.7379\n",
      "Epoch [1/2], Step [12000/16869], Loss: 4.8534\n",
      "Epoch [1/2], Step [12020/16869], Loss: 4.7988\n",
      "Epoch [1/2], Step [12040/16869], Loss: 4.8068\n",
      "Epoch [1/2], Step [12060/16869], Loss: 4.8350\n",
      "Epoch [1/2], Step [12080/16869], Loss: 4.8595\n",
      "Epoch [1/2], Step [12100/16869], Loss: 4.8483\n",
      "Epoch [1/2], Step [12120/16869], Loss: 4.7520\n",
      "Epoch [1/2], Step [12140/16869], Loss: 4.8207\n",
      "Epoch [1/2], Step [12160/16869], Loss: 4.8192\n",
      "Epoch [1/2], Step [12180/16869], Loss: 4.8397\n",
      "Epoch [1/2], Step [12200/16869], Loss: 4.7116\n",
      "Epoch [1/2], Step [12220/16869], Loss: 4.7793\n",
      "Epoch [1/2], Step [12240/16869], Loss: 4.7537\n",
      "Epoch [1/2], Step [12260/16869], Loss: 4.8422\n",
      "Epoch [1/2], Step [12280/16869], Loss: 4.7961\n",
      "Epoch [1/2], Step [12300/16869], Loss: 4.8842\n",
      "Epoch [1/2], Step [12320/16869], Loss: 4.7887\n",
      "Epoch [1/2], Step [12340/16869], Loss: 4.7672\n",
      "Epoch [1/2], Step [12360/16869], Loss: 4.8238\n",
      "Epoch [1/2], Step [12380/16869], Loss: 4.7533\n",
      "Epoch [1/2], Step [12400/16869], Loss: 4.8405\n",
      "Epoch [1/2], Step [12420/16869], Loss: 4.7711\n",
      "Epoch [1/2], Step [12440/16869], Loss: 4.7437\n",
      "Epoch [1/2], Step [12460/16869], Loss: 4.7799\n",
      "Epoch [1/2], Step [12480/16869], Loss: 4.7257\n",
      "Epoch [1/2], Step [12500/16869], Loss: 4.7547\n",
      "Epoch [1/2], Step [12520/16869], Loss: 4.7545\n",
      "Epoch [1/2], Step [12540/16869], Loss: 4.7768\n",
      "Epoch [1/2], Step [12560/16869], Loss: 4.7701\n",
      "Epoch [1/2], Step [12580/16869], Loss: 4.7391\n",
      "Epoch [1/2], Step [12600/16869], Loss: 4.7409\n",
      "Epoch [1/2], Step [12620/16869], Loss: 4.7217\n",
      "Epoch [1/2], Step [12640/16869], Loss: 4.7040\n",
      "Epoch [1/2], Step [12660/16869], Loss: 4.7350\n",
      "Epoch [1/2], Step [12680/16869], Loss: 4.8246\n",
      "Epoch [1/2], Step [12700/16869], Loss: 4.7496\n",
      "Epoch [1/2], Step [12720/16869], Loss: 4.7561\n",
      "Epoch [1/2], Step [12740/16869], Loss: 4.7619\n",
      "Epoch [1/2], Step [12760/16869], Loss: 4.7142\n",
      "Epoch [1/2], Step [12780/16869], Loss: 4.8511\n",
      "Epoch [1/2], Step [12800/16869], Loss: 4.7607\n",
      "Epoch [1/2], Step [12820/16869], Loss: 4.8321\n",
      "Epoch [1/2], Step [12840/16869], Loss: 4.6603\n",
      "Epoch [1/2], Step [12860/16869], Loss: 4.7440\n",
      "Epoch [1/2], Step [12880/16869], Loss: 4.7354\n",
      "Epoch [1/2], Step [12900/16869], Loss: 4.7939\n",
      "Epoch [1/2], Step [12920/16869], Loss: 4.9283\n",
      "Epoch [1/2], Step [12940/16869], Loss: 4.8225\n",
      "Epoch [1/2], Step [12960/16869], Loss: 4.7243\n",
      "Epoch [1/2], Step [12980/16869], Loss: 4.7596\n",
      "Epoch [1/2], Step [13000/16869], Loss: 4.8314\n",
      "Epoch [1/2], Step [13020/16869], Loss: 4.7725\n",
      "Epoch [1/2], Step [13040/16869], Loss: 4.8239\n",
      "Epoch [1/2], Step [13060/16869], Loss: 4.7411\n",
      "Epoch [1/2], Step [13080/16869], Loss: 4.8934\n",
      "Epoch [1/2], Step [13100/16869], Loss: 4.7812\n",
      "Epoch [1/2], Step [13120/16869], Loss: 4.8382\n",
      "Epoch [1/2], Step [13140/16869], Loss: 4.7464\n",
      "Epoch [1/2], Step [13160/16869], Loss: 4.7595\n",
      "Epoch [1/2], Step [13180/16869], Loss: 4.7094\n",
      "Epoch [1/2], Step [13200/16869], Loss: 4.7785\n",
      "Epoch [1/2], Step [13220/16869], Loss: 4.7639\n",
      "Epoch [1/2], Step [13240/16869], Loss: 4.7421\n",
      "Epoch [1/2], Step [13260/16869], Loss: 4.9120\n",
      "Epoch [1/2], Step [13280/16869], Loss: 4.7969\n",
      "Epoch [1/2], Step [13300/16869], Loss: 4.8067\n",
      "Epoch [1/2], Step [13320/16869], Loss: 4.8209\n",
      "Epoch [1/2], Step [13340/16869], Loss: 4.6516\n",
      "Epoch [1/2], Step [13360/16869], Loss: 4.7910\n",
      "Epoch [1/2], Step [13380/16869], Loss: 4.6874\n",
      "Epoch [1/2], Step [13400/16869], Loss: 4.6923\n",
      "Epoch [1/2], Step [13420/16869], Loss: 4.7984\n",
      "Epoch [1/2], Step [13440/16869], Loss: 4.6583\n",
      "Epoch [1/2], Step [13460/16869], Loss: 4.8043\n",
      "Epoch [1/2], Step [13480/16869], Loss: 4.7546\n",
      "Epoch [1/2], Step [13500/16869], Loss: 4.8092\n",
      "Epoch [1/2], Step [13520/16869], Loss: 4.7925\n",
      "Epoch [1/2], Step [13540/16869], Loss: 4.7570\n",
      "Epoch [1/2], Step [13560/16869], Loss: 4.7484\n",
      "Epoch [1/2], Step [13580/16869], Loss: 4.7753\n",
      "Epoch [1/2], Step [13600/16869], Loss: 4.7371\n",
      "Epoch [1/2], Step [13620/16869], Loss: 4.6271\n",
      "Epoch [1/2], Step [13640/16869], Loss: 4.7771\n",
      "Epoch [1/2], Step [13660/16869], Loss: 4.8024\n",
      "Epoch [1/2], Step [13680/16869], Loss: 4.7222\n",
      "Epoch [1/2], Step [13700/16869], Loss: 4.6941\n",
      "Epoch [1/2], Step [13720/16869], Loss: 4.7991\n",
      "Epoch [1/2], Step [13740/16869], Loss: 4.6625\n",
      "Epoch [1/2], Step [13760/16869], Loss: 4.7870\n",
      "Epoch [1/2], Step [13780/16869], Loss: 4.7726\n",
      "Epoch [1/2], Step [13800/16869], Loss: 4.7513\n",
      "Epoch [1/2], Step [13820/16869], Loss: 4.7511\n",
      "Epoch [1/2], Step [13840/16869], Loss: 4.7582\n",
      "Epoch [1/2], Step [13860/16869], Loss: 4.8108\n",
      "Epoch [1/2], Step [13880/16869], Loss: 4.8667\n",
      "Epoch [1/2], Step [13900/16869], Loss: 4.7796\n",
      "Epoch [1/2], Step [13920/16869], Loss: 4.7436\n",
      "Epoch [1/2], Step [13940/16869], Loss: 4.7896\n",
      "Epoch [1/2], Step [13960/16869], Loss: 4.7070\n",
      "Epoch [1/2], Step [13980/16869], Loss: 4.8515\n",
      "Epoch [1/2], Step [14000/16869], Loss: 4.8511\n",
      "Epoch [1/2], Step [14020/16869], Loss: 4.6868\n",
      "Epoch [1/2], Step [14040/16869], Loss: 4.7182\n",
      "Epoch [1/2], Step [14060/16869], Loss: 4.7552\n",
      "Epoch [1/2], Step [14080/16869], Loss: 4.7126\n",
      "Epoch [1/2], Step [14100/16869], Loss: 4.7170\n",
      "Epoch [1/2], Step [14120/16869], Loss: 4.7646\n",
      "Epoch [1/2], Step [14140/16869], Loss: 4.8482\n",
      "Epoch [1/2], Step [14160/16869], Loss: 4.8292\n",
      "Epoch [1/2], Step [14180/16869], Loss: 4.7289\n",
      "Epoch [1/2], Step [14200/16869], Loss: 4.7441\n",
      "Epoch [1/2], Step [14220/16869], Loss: 4.7649\n",
      "Epoch [1/2], Step [14240/16869], Loss: 4.7468\n",
      "Epoch [1/2], Step [14260/16869], Loss: 4.6384\n",
      "Epoch [1/2], Step [14280/16869], Loss: 4.8270\n",
      "Epoch [1/2], Step [14300/16869], Loss: 4.7162\n",
      "Epoch [1/2], Step [14320/16869], Loss: 4.7803\n",
      "Epoch [1/2], Step [14340/16869], Loss: 4.8066\n",
      "Epoch [1/2], Step [14360/16869], Loss: 4.6909\n",
      "Epoch [1/2], Step [14380/16869], Loss: 4.6790\n",
      "Epoch [1/2], Step [14400/16869], Loss: 4.6956\n",
      "Epoch [1/2], Step [14420/16869], Loss: 4.7736\n",
      "Epoch [1/2], Step [14440/16869], Loss: 4.7300\n",
      "Epoch [1/2], Step [14460/16869], Loss: 4.7723\n",
      "Epoch [1/2], Step [14480/16869], Loss: 4.7681\n",
      "Epoch [1/2], Step [14500/16869], Loss: 4.7716\n",
      "Epoch [1/2], Step [14520/16869], Loss: 4.7955\n",
      "Epoch [1/2], Step [14540/16869], Loss: 4.7647\n",
      "Epoch [1/2], Step [14560/16869], Loss: 4.6829\n",
      "Epoch [1/2], Step [14580/16869], Loss: 4.6900\n",
      "Epoch [1/2], Step [14600/16869], Loss: 4.7642\n",
      "Epoch [1/2], Step [14620/16869], Loss: 4.6204\n",
      "Epoch [1/2], Step [14640/16869], Loss: 4.7194\n",
      "Epoch [1/2], Step [14660/16869], Loss: 4.7126\n",
      "Epoch [1/2], Step [14680/16869], Loss: 4.7375\n",
      "Epoch [1/2], Step [14700/16869], Loss: 4.7212\n",
      "Epoch [1/2], Step [14720/16869], Loss: 4.7325\n",
      "Epoch [1/2], Step [14740/16869], Loss: 4.8835\n",
      "Epoch [1/2], Step [14760/16869], Loss: 4.7708\n",
      "Epoch [1/2], Step [14780/16869], Loss: 4.7039\n",
      "Epoch [1/2], Step [14800/16869], Loss: 4.6829\n",
      "Epoch [1/2], Step [14820/16869], Loss: 4.7599\n",
      "Epoch [1/2], Step [14840/16869], Loss: 4.8286\n",
      "Epoch [1/2], Step [14860/16869], Loss: 4.7090\n",
      "Epoch [1/2], Step [14880/16869], Loss: 4.7490\n",
      "Epoch [1/2], Step [14900/16869], Loss: 4.6513\n",
      "Epoch [1/2], Step [14920/16869], Loss: 4.7087\n",
      "Epoch [1/2], Step [14940/16869], Loss: 4.8022\n",
      "Epoch [1/2], Step [14960/16869], Loss: 4.6701\n",
      "Epoch [1/2], Step [14980/16869], Loss: 4.7640\n",
      "Epoch [1/2], Step [15000/16869], Loss: 4.6672\n",
      "Epoch [1/2], Step [15020/16869], Loss: 4.7191\n",
      "Epoch [1/2], Step [15040/16869], Loss: 4.6666\n",
      "Epoch [1/2], Step [15060/16869], Loss: 4.7586\n",
      "Epoch [1/2], Step [15080/16869], Loss: 4.7403\n",
      "Epoch [1/2], Step [15100/16869], Loss: 4.7276\n",
      "Epoch [1/2], Step [15120/16869], Loss: 4.7531\n",
      "Epoch [1/2], Step [15140/16869], Loss: 4.6478\n",
      "Epoch [1/2], Step [15160/16869], Loss: 4.6651\n",
      "Epoch [1/2], Step [15180/16869], Loss: 4.7594\n",
      "Epoch [1/2], Step [15200/16869], Loss: 4.7964\n",
      "Epoch [1/2], Step [15220/16869], Loss: 4.7576\n",
      "Epoch [1/2], Step [15240/16869], Loss: 4.7439\n",
      "Epoch [1/2], Step [15260/16869], Loss: 4.7166\n",
      "Epoch [1/2], Step [15280/16869], Loss: 4.8049\n",
      "Epoch [1/2], Step [15300/16869], Loss: 4.7308\n",
      "Epoch [1/2], Step [15320/16869], Loss: 4.7604\n",
      "Epoch [1/2], Step [15340/16869], Loss: 4.6756\n",
      "Epoch [1/2], Step [15360/16869], Loss: 4.7472\n",
      "Epoch [1/2], Step [15380/16869], Loss: 4.8528\n",
      "Epoch [1/2], Step [15400/16869], Loss: 4.7240\n",
      "Epoch [1/2], Step [15420/16869], Loss: 4.7868\n",
      "Epoch [1/2], Step [15440/16869], Loss: 4.7102\n",
      "Epoch [1/2], Step [15460/16869], Loss: 4.7084\n",
      "Epoch [1/2], Step [15480/16869], Loss: 4.7492\n",
      "Epoch [1/2], Step [15500/16869], Loss: 4.7039\n",
      "Epoch [1/2], Step [15520/16869], Loss: 4.7414\n",
      "Epoch [1/2], Step [15540/16869], Loss: 4.7096\n",
      "Epoch [1/2], Step [15560/16869], Loss: 4.6722\n",
      "Epoch [1/2], Step [15580/16869], Loss: 4.8199\n",
      "Epoch [1/2], Step [15600/16869], Loss: 4.7289\n",
      "Epoch [1/2], Step [15620/16869], Loss: 4.8290\n",
      "Epoch [1/2], Step [15640/16869], Loss: 4.7108\n",
      "Epoch [1/2], Step [15660/16869], Loss: 4.7375\n",
      "Epoch [1/2], Step [15680/16869], Loss: 4.6473\n",
      "Epoch [1/2], Step [15700/16869], Loss: 4.9049\n",
      "Epoch [1/2], Step [15720/16869], Loss: 4.7460\n",
      "Epoch [1/2], Step [15740/16869], Loss: 4.7597\n",
      "Epoch [1/2], Step [15760/16869], Loss: 4.7118\n",
      "Epoch [1/2], Step [15780/16869], Loss: 4.8083\n",
      "Epoch [1/2], Step [15800/16869], Loss: 4.6531\n",
      "Epoch [1/2], Step [15820/16869], Loss: 4.7835\n",
      "Epoch [1/2], Step [15840/16869], Loss: 4.7515\n",
      "Epoch [1/2], Step [15860/16869], Loss: 4.7211\n",
      "Epoch [1/2], Step [15880/16869], Loss: 4.7567\n",
      "Epoch [1/2], Step [15900/16869], Loss: 4.8155\n",
      "Epoch [1/2], Step [15920/16869], Loss: 4.7047\n",
      "Epoch [1/2], Step [15940/16869], Loss: 4.8115\n",
      "Epoch [1/2], Step [15960/16869], Loss: 4.8526\n",
      "Epoch [1/2], Step [15980/16869], Loss: 4.7017\n",
      "Epoch [1/2], Step [16000/16869], Loss: 4.6882\n",
      "Epoch [1/2], Step [16020/16869], Loss: 4.6890\n",
      "Epoch [1/2], Step [16040/16869], Loss: 4.7166\n",
      "Epoch [1/2], Step [16060/16869], Loss: 4.7969\n",
      "Epoch [1/2], Step [16080/16869], Loss: 4.7241\n",
      "Epoch [1/2], Step [16100/16869], Loss: 4.6469\n",
      "Epoch [1/2], Step [16120/16869], Loss: 4.6721\n",
      "Epoch [1/2], Step [16140/16869], Loss: 4.7596\n",
      "Epoch [1/2], Step [16160/16869], Loss: 4.6786\n",
      "Epoch [1/2], Step [16180/16869], Loss: 4.7632\n",
      "Epoch [1/2], Step [16200/16869], Loss: 4.7261\n",
      "Epoch [1/2], Step [16220/16869], Loss: 4.7547\n",
      "Epoch [1/2], Step [16240/16869], Loss: 4.7904\n",
      "Epoch [1/2], Step [16260/16869], Loss: 4.7248\n",
      "Epoch [1/2], Step [16280/16869], Loss: 4.6440\n",
      "Epoch [1/2], Step [16300/16869], Loss: 4.7566\n",
      "Epoch [1/2], Step [16320/16869], Loss: 4.6868\n",
      "Epoch [1/2], Step [16340/16869], Loss: 4.7061\n",
      "Epoch [1/2], Step [16360/16869], Loss: 4.6312\n",
      "Epoch [1/2], Step [16380/16869], Loss: 4.7549\n",
      "Epoch [1/2], Step [16400/16869], Loss: 4.6813\n",
      "Epoch [1/2], Step [16420/16869], Loss: 4.6331\n",
      "Epoch [1/2], Step [16440/16869], Loss: 4.7621\n",
      "Epoch [1/2], Step [16460/16869], Loss: 4.7228\n",
      "Epoch [1/2], Step [16480/16869], Loss: 4.7578\n",
      "Epoch [1/2], Step [16500/16869], Loss: 4.7016\n",
      "Epoch [1/2], Step [16520/16869], Loss: 4.6899\n",
      "Epoch [1/2], Step [16540/16869], Loss: 4.6534\n",
      "Epoch [1/2], Step [16560/16869], Loss: 4.7447\n",
      "Epoch [1/2], Step [16580/16869], Loss: 4.7204\n",
      "Epoch [1/2], Step [16600/16869], Loss: 4.7226\n",
      "Epoch [1/2], Step [16620/16869], Loss: 4.8173\n",
      "Epoch [1/2], Step [16640/16869], Loss: 4.7049\n",
      "Epoch [1/2], Step [16660/16869], Loss: 4.6925\n",
      "Epoch [1/2], Step [16680/16869], Loss: 4.6212\n",
      "Epoch [1/2], Step [16700/16869], Loss: 4.8084\n",
      "Epoch [1/2], Step [16720/16869], Loss: 4.6635\n",
      "Epoch [1/2], Step [16740/16869], Loss: 4.7158\n",
      "Epoch [1/2], Step [16760/16869], Loss: 4.7005\n",
      "Epoch [1/2], Step [16780/16869], Loss: 4.7555\n",
      "Epoch [1/2], Step [16800/16869], Loss: 4.7022\n",
      "Epoch [1/2], Step [16820/16869], Loss: 4.7732\n",
      "Epoch [1/2], Step [16840/16869], Loss: 4.8344\n",
      "Epoch [1/2], Step [16860/16869], Loss: 4.8467\n",
      "Epoch [1/2] Average Loss: 5.0018, Perplexity: 148.67\n",
      "Epoch [2/2], Step [0/16869], Loss: 4.7068\n",
      "Epoch [2/2], Step [20/16869], Loss: 4.7218\n",
      "Epoch [2/2], Step [40/16869], Loss: 4.7291\n",
      "Epoch [2/2], Step [60/16869], Loss: 4.6303\n",
      "Epoch [2/2], Step [80/16869], Loss: 4.7291\n",
      "Epoch [2/2], Step [100/16869], Loss: 4.6985\n",
      "Epoch [2/2], Step [120/16869], Loss: 4.6510\n",
      "Epoch [2/2], Step [140/16869], Loss: 4.6577\n",
      "Epoch [2/2], Step [160/16869], Loss: 4.7074\n",
      "Epoch [2/2], Step [180/16869], Loss: 4.7144\n",
      "Epoch [2/2], Step [200/16869], Loss: 4.7727\n",
      "Epoch [2/2], Step [220/16869], Loss: 4.6897\n",
      "Epoch [2/2], Step [240/16869], Loss: 4.7922\n",
      "Epoch [2/2], Step [260/16869], Loss: 4.6696\n",
      "Epoch [2/2], Step [280/16869], Loss: 4.7115\n",
      "Epoch [2/2], Step [300/16869], Loss: 4.8155\n",
      "Epoch [2/2], Step [320/16869], Loss: 4.7015\n",
      "Epoch [2/2], Step [340/16869], Loss: 4.7812\n",
      "Epoch [2/2], Step [360/16869], Loss: 4.7822\n",
      "Epoch [2/2], Step [380/16869], Loss: 4.7865\n",
      "Epoch [2/2], Step [400/16869], Loss: 4.6513\n",
      "Epoch [2/2], Step [420/16869], Loss: 4.7307\n",
      "Epoch [2/2], Step [440/16869], Loss: 4.7003\n",
      "Epoch [2/2], Step [460/16869], Loss: 4.6831\n",
      "Epoch [2/2], Step [480/16869], Loss: 4.7055\n",
      "Epoch [2/2], Step [500/16869], Loss: 4.7292\n",
      "Epoch [2/2], Step [520/16869], Loss: 4.6665\n",
      "Epoch [2/2], Step [540/16869], Loss: 4.7027\n",
      "Epoch [2/2], Step [560/16869], Loss: 4.7854\n",
      "Epoch [2/2], Step [580/16869], Loss: 4.8217\n",
      "Epoch [2/2], Step [600/16869], Loss: 4.7587\n",
      "Epoch [2/2], Step [620/16869], Loss: 4.7576\n",
      "Epoch [2/2], Step [640/16869], Loss: 4.6972\n",
      "Epoch [2/2], Step [660/16869], Loss: 4.6605\n",
      "Epoch [2/2], Step [680/16869], Loss: 4.7975\n",
      "Epoch [2/2], Step [700/16869], Loss: 4.7118\n",
      "Epoch [2/2], Step [720/16869], Loss: 4.7466\n",
      "Epoch [2/2], Step [740/16869], Loss: 4.6875\n",
      "Epoch [2/2], Step [760/16869], Loss: 4.7232\n",
      "Epoch [2/2], Step [780/16869], Loss: 4.7388\n",
      "Epoch [2/2], Step [800/16869], Loss: 4.7036\n",
      "Epoch [2/2], Step [820/16869], Loss: 4.8160\n",
      "Epoch [2/2], Step [840/16869], Loss: 4.6618\n",
      "Epoch [2/2], Step [860/16869], Loss: 4.7475\n",
      "Epoch [2/2], Step [880/16869], Loss: 4.7413\n",
      "Epoch [2/2], Step [900/16869], Loss: 4.6658\n",
      "Epoch [2/2], Step [920/16869], Loss: 4.7389\n",
      "Epoch [2/2], Step [940/16869], Loss: 4.7707\n",
      "Epoch [2/2], Step [960/16869], Loss: 4.7001\n",
      "Epoch [2/2], Step [980/16869], Loss: 4.7833\n",
      "Epoch [2/2], Step [1000/16869], Loss: 4.6811\n",
      "Epoch [2/2], Step [1020/16869], Loss: 4.6465\n",
      "Epoch [2/2], Step [1040/16869], Loss: 4.7574\n",
      "Epoch [2/2], Step [1060/16869], Loss: 4.6981\n",
      "Epoch [2/2], Step [1080/16869], Loss: 4.6975\n",
      "Epoch [2/2], Step [1100/16869], Loss: 4.7025\n",
      "Epoch [2/2], Step [1120/16869], Loss: 4.5940\n",
      "Epoch [2/2], Step [1140/16869], Loss: 4.7612\n",
      "Epoch [2/2], Step [1160/16869], Loss: 4.7762\n",
      "Epoch [2/2], Step [1180/16869], Loss: 4.6764\n",
      "Epoch [2/2], Step [1200/16869], Loss: 4.7228\n",
      "Epoch [2/2], Step [1220/16869], Loss: 4.7446\n",
      "Epoch [2/2], Step [1240/16869], Loss: 4.7005\n",
      "Epoch [2/2], Step [1260/16869], Loss: 4.6963\n",
      "Epoch [2/2], Step [1280/16869], Loss: 4.7088\n",
      "Epoch [2/2], Step [1300/16869], Loss: 4.7267\n",
      "Epoch [2/2], Step [1320/16869], Loss: 4.7907\n",
      "Epoch [2/2], Step [1340/16869], Loss: 4.6405\n",
      "Epoch [2/2], Step [1360/16869], Loss: 4.6396\n",
      "Epoch [2/2], Step [1380/16869], Loss: 4.6745\n",
      "Epoch [2/2], Step [1400/16869], Loss: 4.6990\n",
      "Epoch [2/2], Step [1420/16869], Loss: 4.6761\n",
      "Epoch [2/2], Step [1440/16869], Loss: 4.7292\n",
      "Epoch [2/2], Step [1460/16869], Loss: 4.7543\n",
      "Epoch [2/2], Step [1480/16869], Loss: 4.6736\n",
      "Epoch [2/2], Step [1500/16869], Loss: 4.6628\n",
      "Epoch [2/2], Step [1520/16869], Loss: 4.7388\n",
      "Epoch [2/2], Step [1540/16869], Loss: 4.7144\n",
      "Epoch [2/2], Step [1560/16869], Loss: 4.6221\n",
      "Epoch [2/2], Step [1580/16869], Loss: 4.6654\n",
      "Epoch [2/2], Step [1600/16869], Loss: 4.6206\n",
      "Epoch [2/2], Step [1620/16869], Loss: 4.5966\n",
      "Epoch [2/2], Step [1640/16869], Loss: 4.7234\n",
      "Epoch [2/2], Step [1660/16869], Loss: 4.7320\n",
      "Epoch [2/2], Step [1680/16869], Loss: 4.6594\n",
      "Epoch [2/2], Step [1700/16869], Loss: 4.6993\n",
      "Epoch [2/2], Step [1720/16869], Loss: 4.8097\n",
      "Epoch [2/2], Step [1740/16869], Loss: 4.6884\n",
      "Epoch [2/2], Step [1760/16869], Loss: 4.8272\n",
      "Epoch [2/2], Step [1780/16869], Loss: 4.6415\n",
      "Epoch [2/2], Step [1800/16869], Loss: 4.6501\n",
      "Epoch [2/2], Step [1820/16869], Loss: 4.7133\n",
      "Epoch [2/2], Step [1840/16869], Loss: 4.7584\n",
      "Epoch [2/2], Step [1860/16869], Loss: 4.7141\n",
      "Epoch [2/2], Step [1880/16869], Loss: 4.6843\n",
      "Epoch [2/2], Step [1900/16869], Loss: 4.6583\n",
      "Epoch [2/2], Step [1920/16869], Loss: 4.7501\n",
      "Epoch [2/2], Step [1940/16869], Loss: 4.5882\n",
      "Epoch [2/2], Step [1960/16869], Loss: 4.7664\n",
      "Epoch [2/2], Step [1980/16869], Loss: 4.6438\n",
      "Epoch [2/2], Step [2000/16869], Loss: 4.6960\n",
      "Epoch [2/2], Step [2020/16869], Loss: 4.7325\n",
      "Epoch [2/2], Step [2040/16869], Loss: 4.6500\n",
      "Epoch [2/2], Step [2060/16869], Loss: 4.7300\n",
      "Epoch [2/2], Step [2080/16869], Loss: 4.6973\n",
      "Epoch [2/2], Step [2100/16869], Loss: 4.6786\n",
      "Epoch [2/2], Step [2120/16869], Loss: 4.6513\n",
      "Epoch [2/2], Step [2140/16869], Loss: 4.7703\n",
      "Epoch [2/2], Step [2160/16869], Loss: 4.6481\n",
      "Epoch [2/2], Step [2180/16869], Loss: 4.7768\n",
      "Epoch [2/2], Step [2200/16869], Loss: 4.8070\n",
      "Epoch [2/2], Step [2220/16869], Loss: 4.6933\n",
      "Epoch [2/2], Step [2240/16869], Loss: 4.8337\n",
      "Epoch [2/2], Step [2260/16869], Loss: 4.6466\n",
      "Epoch [2/2], Step [2280/16869], Loss: 4.6594\n",
      "Epoch [2/2], Step [2300/16869], Loss: 4.6558\n",
      "Epoch [2/2], Step [2320/16869], Loss: 4.7411\n",
      "Epoch [2/2], Step [2340/16869], Loss: 4.7559\n",
      "Epoch [2/2], Step [2360/16869], Loss: 4.7451\n",
      "Epoch [2/2], Step [2380/16869], Loss: 4.6859\n",
      "Epoch [2/2], Step [2400/16869], Loss: 4.7571\n",
      "Epoch [2/2], Step [2420/16869], Loss: 4.5508\n",
      "Epoch [2/2], Step [2440/16869], Loss: 4.7927\n",
      "Epoch [2/2], Step [2460/16869], Loss: 4.7249\n",
      "Epoch [2/2], Step [2480/16869], Loss: 4.7511\n",
      "Epoch [2/2], Step [2500/16869], Loss: 4.7217\n",
      "Epoch [2/2], Step [2520/16869], Loss: 4.7026\n",
      "Epoch [2/2], Step [2540/16869], Loss: 4.6936\n",
      "Epoch [2/2], Step [2560/16869], Loss: 4.6959\n",
      "Epoch [2/2], Step [2580/16869], Loss: 4.6335\n",
      "Epoch [2/2], Step [2600/16869], Loss: 4.7437\n",
      "Epoch [2/2], Step [2620/16869], Loss: 4.6149\n",
      "Epoch [2/2], Step [2640/16869], Loss: 4.6828\n",
      "Epoch [2/2], Step [2660/16869], Loss: 4.6374\n",
      "Epoch [2/2], Step [2680/16869], Loss: 4.6869\n",
      "Epoch [2/2], Step [2700/16869], Loss: 4.7027\n",
      "Epoch [2/2], Step [2720/16869], Loss: 4.7328\n",
      "Epoch [2/2], Step [2740/16869], Loss: 4.7256\n",
      "Epoch [2/2], Step [2760/16869], Loss: 4.7083\n",
      "Epoch [2/2], Step [2780/16869], Loss: 4.7369\n",
      "Epoch [2/2], Step [2800/16869], Loss: 4.7820\n",
      "Epoch [2/2], Step [2820/16869], Loss: 4.7093\n",
      "Epoch [2/2], Step [2840/16869], Loss: 4.7140\n",
      "Epoch [2/2], Step [2860/16869], Loss: 4.7663\n",
      "Epoch [2/2], Step [2880/16869], Loss: 4.6802\n",
      "Epoch [2/2], Step [2900/16869], Loss: 4.6770\n",
      "Epoch [2/2], Step [2920/16869], Loss: 4.6358\n",
      "Epoch [2/2], Step [2940/16869], Loss: 4.7361\n",
      "Epoch [2/2], Step [2960/16869], Loss: 4.6902\n",
      "Epoch [2/2], Step [2980/16869], Loss: 4.6893\n",
      "Epoch [2/2], Step [3000/16869], Loss: 4.6579\n",
      "Epoch [2/2], Step [3020/16869], Loss: 4.6651\n",
      "Epoch [2/2], Step [3040/16869], Loss: 4.6912\n",
      "Epoch [2/2], Step [3060/16869], Loss: 4.6674\n",
      "Epoch [2/2], Step [3080/16869], Loss: 4.6173\n",
      "Epoch [2/2], Step [3100/16869], Loss: 4.6797\n",
      "Epoch [2/2], Step [3120/16869], Loss: 4.6995\n",
      "Epoch [2/2], Step [3140/16869], Loss: 4.7312\n",
      "Epoch [2/2], Step [3160/16869], Loss: 4.7234\n",
      "Epoch [2/2], Step [3180/16869], Loss: 4.6999\n",
      "Epoch [2/2], Step [3200/16869], Loss: 4.7702\n",
      "Epoch [2/2], Step [3220/16869], Loss: 4.6405\n",
      "Epoch [2/2], Step [3240/16869], Loss: 4.8428\n",
      "Epoch [2/2], Step [3260/16869], Loss: 4.6785\n",
      "Epoch [2/2], Step [3280/16869], Loss: 4.7718\n",
      "Epoch [2/2], Step [3300/16869], Loss: 4.6540\n",
      "Epoch [2/2], Step [3320/16869], Loss: 4.7329\n",
      "Epoch [2/2], Step [3340/16869], Loss: 4.6543\n",
      "Epoch [2/2], Step [3360/16869], Loss: 4.6547\n",
      "Epoch [2/2], Step [3380/16869], Loss: 4.7819\n",
      "Epoch [2/2], Step [3400/16869], Loss: 4.6190\n",
      "Epoch [2/2], Step [3420/16869], Loss: 4.6437\n",
      "Epoch [2/2], Step [3440/16869], Loss: 4.6856\n",
      "Epoch [2/2], Step [3460/16869], Loss: 4.6389\n",
      "Epoch [2/2], Step [3480/16869], Loss: 4.5512\n",
      "Epoch [2/2], Step [3500/16869], Loss: 4.7506\n",
      "Epoch [2/2], Step [3520/16869], Loss: 4.7318\n",
      "Epoch [2/2], Step [3540/16869], Loss: 4.6781\n",
      "Epoch [2/2], Step [3560/16869], Loss: 4.6625\n",
      "Epoch [2/2], Step [3580/16869], Loss: 4.6752\n",
      "Epoch [2/2], Step [3600/16869], Loss: 4.6913\n",
      "Epoch [2/2], Step [3620/16869], Loss: 4.6763\n",
      "Epoch [2/2], Step [3640/16869], Loss: 4.7602\n",
      "Epoch [2/2], Step [3660/16869], Loss: 4.6196\n",
      "Epoch [2/2], Step [3680/16869], Loss: 4.8301\n",
      "Epoch [2/2], Step [3700/16869], Loss: 4.7072\n",
      "Epoch [2/2], Step [3720/16869], Loss: 4.6641\n",
      "Epoch [2/2], Step [3740/16869], Loss: 4.6737\n",
      "Epoch [2/2], Step [3760/16869], Loss: 4.7598\n",
      "Epoch [2/2], Step [3780/16869], Loss: 4.7253\n",
      "Epoch [2/2], Step [3800/16869], Loss: 4.6714\n",
      "Epoch [2/2], Step [3820/16869], Loss: 4.7308\n",
      "Epoch [2/2], Step [3840/16869], Loss: 4.7006\n",
      "Epoch [2/2], Step [3860/16869], Loss: 4.5855\n",
      "Epoch [2/2], Step [3880/16869], Loss: 4.6565\n",
      "Epoch [2/2], Step [3900/16869], Loss: 4.7167\n",
      "Epoch [2/2], Step [3920/16869], Loss: 4.7055\n",
      "Epoch [2/2], Step [3940/16869], Loss: 4.7079\n",
      "Epoch [2/2], Step [3960/16869], Loss: 4.6953\n",
      "Epoch [2/2], Step [3980/16869], Loss: 4.7355\n",
      "Epoch [2/2], Step [4000/16869], Loss: 4.6799\n",
      "Epoch [2/2], Step [4020/16869], Loss: 4.6154\n",
      "Epoch [2/2], Step [4040/16869], Loss: 4.6960\n",
      "Epoch [2/2], Step [4060/16869], Loss: 4.5999\n",
      "Epoch [2/2], Step [4080/16869], Loss: 4.7124\n",
      "Epoch [2/2], Step [4100/16869], Loss: 4.7834\n",
      "Epoch [2/2], Step [4120/16869], Loss: 4.6627\n",
      "Epoch [2/2], Step [4140/16869], Loss: 4.7821\n",
      "Epoch [2/2], Step [4160/16869], Loss: 4.7395\n",
      "Epoch [2/2], Step [4180/16869], Loss: 4.6398\n",
      "Epoch [2/2], Step [4200/16869], Loss: 4.6429\n",
      "Epoch [2/2], Step [4220/16869], Loss: 4.7329\n",
      "Epoch [2/2], Step [4240/16869], Loss: 4.6697\n",
      "Epoch [2/2], Step [4260/16869], Loss: 4.7146\n",
      "Epoch [2/2], Step [4280/16869], Loss: 4.6554\n",
      "Epoch [2/2], Step [4300/16869], Loss: 4.6919\n",
      "Epoch [2/2], Step [4320/16869], Loss: 4.7476\n",
      "Epoch [2/2], Step [4340/16869], Loss: 4.6244\n",
      "Epoch [2/2], Step [4360/16869], Loss: 4.7338\n",
      "Epoch [2/2], Step [4380/16869], Loss: 4.7106\n",
      "Epoch [2/2], Step [4400/16869], Loss: 4.5624\n",
      "Epoch [2/2], Step [4420/16869], Loss: 4.6131\n",
      "Epoch [2/2], Step [4440/16869], Loss: 4.6632\n",
      "Epoch [2/2], Step [4460/16869], Loss: 4.6772\n",
      "Epoch [2/2], Step [4480/16869], Loss: 4.7176\n",
      "Epoch [2/2], Step [4500/16869], Loss: 4.7635\n",
      "Epoch [2/2], Step [4520/16869], Loss: 4.7153\n",
      "Epoch [2/2], Step [4540/16869], Loss: 4.6379\n",
      "Epoch [2/2], Step [4560/16869], Loss: 4.6512\n",
      "Epoch [2/2], Step [4580/16869], Loss: 4.6654\n",
      "Epoch [2/2], Step [4600/16869], Loss: 4.6679\n",
      "Epoch [2/2], Step [4620/16869], Loss: 4.6603\n",
      "Epoch [2/2], Step [4640/16869], Loss: 4.6653\n",
      "Epoch [2/2], Step [4660/16869], Loss: 4.6837\n",
      "Epoch [2/2], Step [4680/16869], Loss: 4.6530\n",
      "Epoch [2/2], Step [4700/16869], Loss: 4.7183\n",
      "Epoch [2/2], Step [4720/16869], Loss: 4.7374\n",
      "Epoch [2/2], Step [4740/16869], Loss: 4.6701\n",
      "Epoch [2/2], Step [4760/16869], Loss: 4.7681\n",
      "Epoch [2/2], Step [4780/16869], Loss: 4.6980\n",
      "Epoch [2/2], Step [4800/16869], Loss: 4.6652\n",
      "Epoch [2/2], Step [4820/16869], Loss: 4.7119\n",
      "Epoch [2/2], Step [4840/16869], Loss: 4.6971\n",
      "Epoch [2/2], Step [4860/16869], Loss: 4.7524\n",
      "Epoch [2/2], Step [4880/16869], Loss: 4.7009\n",
      "Epoch [2/2], Step [4900/16869], Loss: 4.6321\n",
      "Epoch [2/2], Step [4920/16869], Loss: 4.7245\n",
      "Epoch [2/2], Step [4940/16869], Loss: 4.6941\n",
      "Epoch [2/2], Step [4960/16869], Loss: 4.6769\n",
      "Epoch [2/2], Step [4980/16869], Loss: 4.6874\n",
      "Epoch [2/2], Step [5000/16869], Loss: 4.7010\n",
      "Epoch [2/2], Step [5020/16869], Loss: 4.6448\n",
      "Epoch [2/2], Step [5040/16869], Loss: 4.7713\n",
      "Epoch [2/2], Step [5060/16869], Loss: 4.6544\n",
      "Epoch [2/2], Step [5080/16869], Loss: 4.6912\n",
      "Epoch [2/2], Step [5100/16869], Loss: 4.7506\n",
      "Epoch [2/2], Step [5120/16869], Loss: 4.7192\n",
      "Epoch [2/2], Step [5140/16869], Loss: 4.8228\n",
      "Epoch [2/2], Step [5160/16869], Loss: 4.5744\n",
      "Epoch [2/2], Step [5180/16869], Loss: 4.6112\n",
      "Epoch [2/2], Step [5200/16869], Loss: 4.7715\n",
      "Epoch [2/2], Step [5220/16869], Loss: 4.7192\n",
      "Epoch [2/2], Step [5240/16869], Loss: 4.6789\n",
      "Epoch [2/2], Step [5260/16869], Loss: 4.6524\n",
      "Epoch [2/2], Step [5280/16869], Loss: 4.6253\n",
      "Epoch [2/2], Step [5300/16869], Loss: 4.6326\n",
      "Epoch [2/2], Step [5320/16869], Loss: 4.6425\n",
      "Epoch [2/2], Step [5340/16869], Loss: 4.6430\n",
      "Epoch [2/2], Step [5360/16869], Loss: 4.7182\n",
      "Epoch [2/2], Step [5380/16869], Loss: 4.7198\n",
      "Epoch [2/2], Step [5400/16869], Loss: 4.7880\n",
      "Epoch [2/2], Step [5420/16869], Loss: 4.6790\n",
      "Epoch [2/2], Step [5440/16869], Loss: 4.6495\n",
      "Epoch [2/2], Step [5460/16869], Loss: 4.5695\n",
      "Epoch [2/2], Step [5480/16869], Loss: 4.6348\n",
      "Epoch [2/2], Step [5500/16869], Loss: 4.6810\n",
      "Epoch [2/2], Step [5520/16869], Loss: 4.7714\n",
      "Epoch [2/2], Step [5540/16869], Loss: 4.6605\n",
      "Epoch [2/2], Step [5560/16869], Loss: 4.5959\n",
      "Epoch [2/2], Step [5580/16869], Loss: 4.6792\n",
      "Epoch [2/2], Step [5600/16869], Loss: 4.6031\n",
      "Epoch [2/2], Step [5620/16869], Loss: 4.6177\n",
      "Epoch [2/2], Step [5640/16869], Loss: 4.5911\n",
      "Epoch [2/2], Step [5660/16869], Loss: 4.6482\n",
      "Epoch [2/2], Step [5680/16869], Loss: 4.6211\n",
      "Epoch [2/2], Step [5700/16869], Loss: 4.6697\n",
      "Epoch [2/2], Step [5720/16869], Loss: 4.6594\n",
      "Epoch [2/2], Step [5740/16869], Loss: 4.5823\n",
      "Epoch [2/2], Step [5760/16869], Loss: 4.7703\n",
      "Epoch [2/2], Step [5780/16869], Loss: 4.6575\n",
      "Epoch [2/2], Step [5800/16869], Loss: 4.7180\n",
      "Epoch [2/2], Step [5820/16869], Loss: 4.7275\n",
      "Epoch [2/2], Step [5840/16869], Loss: 4.6813\n",
      "Epoch [2/2], Step [5860/16869], Loss: 4.6523\n",
      "Epoch [2/2], Step [5880/16869], Loss: 4.6559\n",
      "Epoch [2/2], Step [5900/16869], Loss: 4.6815\n",
      "Epoch [2/2], Step [5920/16869], Loss: 4.6582\n",
      "Epoch [2/2], Step [5940/16869], Loss: 4.6505\n",
      "Epoch [2/2], Step [5960/16869], Loss: 4.7312\n",
      "Epoch [2/2], Step [5980/16869], Loss: 4.7089\n",
      "Epoch [2/2], Step [6000/16869], Loss: 4.6572\n",
      "Epoch [2/2], Step [6020/16869], Loss: 4.6281\n",
      "Epoch [2/2], Step [6040/16869], Loss: 4.6264\n",
      "Epoch [2/2], Step [6060/16869], Loss: 4.6300\n",
      "Epoch [2/2], Step [6080/16869], Loss: 4.6795\n",
      "Epoch [2/2], Step [6100/16869], Loss: 4.6287\n",
      "Epoch [2/2], Step [6120/16869], Loss: 4.6887\n",
      "Epoch [2/2], Step [6140/16869], Loss: 4.6441\n",
      "Epoch [2/2], Step [6160/16869], Loss: 4.7700\n",
      "Epoch [2/2], Step [6180/16869], Loss: 4.6801\n",
      "Epoch [2/2], Step [6200/16869], Loss: 4.7224\n",
      "Epoch [2/2], Step [6220/16869], Loss: 4.7787\n",
      "Epoch [2/2], Step [6240/16869], Loss: 4.7637\n",
      "Epoch [2/2], Step [6260/16869], Loss: 4.6875\n",
      "Epoch [2/2], Step [6280/16869], Loss: 4.7126\n",
      "Epoch [2/2], Step [6300/16869], Loss: 4.5774\n",
      "Epoch [2/2], Step [6320/16869], Loss: 4.6260\n",
      "Epoch [2/2], Step [6340/16869], Loss: 4.6792\n",
      "Epoch [2/2], Step [6360/16869], Loss: 4.4978\n",
      "Epoch [2/2], Step [6380/16869], Loss: 4.6724\n",
      "Epoch [2/2], Step [6400/16869], Loss: 4.6102\n",
      "Epoch [2/2], Step [6420/16869], Loss: 4.7003\n",
      "Epoch [2/2], Step [6440/16869], Loss: 4.7510\n",
      "Epoch [2/2], Step [6460/16869], Loss: 4.7423\n",
      "Epoch [2/2], Step [6480/16869], Loss: 4.6602\n",
      "Epoch [2/2], Step [6500/16869], Loss: 4.6099\n",
      "Epoch [2/2], Step [6520/16869], Loss: 4.6758\n",
      "Epoch [2/2], Step [6540/16869], Loss: 4.6566\n",
      "Epoch [2/2], Step [6560/16869], Loss: 4.6516\n",
      "Epoch [2/2], Step [6580/16869], Loss: 4.6950\n",
      "Epoch [2/2], Step [6600/16869], Loss: 4.7363\n",
      "Epoch [2/2], Step [6620/16869], Loss: 4.5518\n",
      "Epoch [2/2], Step [6640/16869], Loss: 4.6800\n",
      "Epoch [2/2], Step [6660/16869], Loss: 4.6487\n",
      "Epoch [2/2], Step [6680/16869], Loss: 4.7007\n",
      "Epoch [2/2], Step [6700/16869], Loss: 4.7050\n",
      "Epoch [2/2], Step [6720/16869], Loss: 4.7610\n",
      "Epoch [2/2], Step [6740/16869], Loss: 4.6888\n",
      "Epoch [2/2], Step [6760/16869], Loss: 4.6968\n",
      "Epoch [2/2], Step [6780/16869], Loss: 4.6488\n",
      "Epoch [2/2], Step [6800/16869], Loss: 4.5857\n",
      "Epoch [2/2], Step [6820/16869], Loss: 4.7004\n",
      "Epoch [2/2], Step [6840/16869], Loss: 4.5507\n",
      "Epoch [2/2], Step [6860/16869], Loss: 4.5783\n",
      "Epoch [2/2], Step [6880/16869], Loss: 4.6011\n",
      "Epoch [2/2], Step [6900/16869], Loss: 4.6516\n",
      "Epoch [2/2], Step [6920/16869], Loss: 4.6459\n",
      "Epoch [2/2], Step [6940/16869], Loss: 4.6954\n",
      "Epoch [2/2], Step [6960/16869], Loss: 4.6945\n",
      "Epoch [2/2], Step [6980/16869], Loss: 4.7359\n",
      "Epoch [2/2], Step [7000/16869], Loss: 4.7472\n",
      "Epoch [2/2], Step [7020/16869], Loss: 4.6767\n",
      "Epoch [2/2], Step [7040/16869], Loss: 4.6475\n",
      "Epoch [2/2], Step [7060/16869], Loss: 4.6852\n",
      "Epoch [2/2], Step [7080/16869], Loss: 4.6580\n",
      "Epoch [2/2], Step [7100/16869], Loss: 4.6767\n",
      "Epoch [2/2], Step [7120/16869], Loss: 4.7313\n",
      "Epoch [2/2], Step [7140/16869], Loss: 4.6529\n",
      "Epoch [2/2], Step [7160/16869], Loss: 4.6877\n",
      "Epoch [2/2], Step [7180/16869], Loss: 4.6129\n",
      "Epoch [2/2], Step [7200/16869], Loss: 4.7233\n",
      "Epoch [2/2], Step [7220/16869], Loss: 4.6330\n",
      "Epoch [2/2], Step [7240/16869], Loss: 4.6692\n",
      "Epoch [2/2], Step [7260/16869], Loss: 4.6647\n",
      "Epoch [2/2], Step [7280/16869], Loss: 4.7503\n",
      "Epoch [2/2], Step [7300/16869], Loss: 4.6832\n",
      "Epoch [2/2], Step [7320/16869], Loss: 4.6621\n",
      "Epoch [2/2], Step [7340/16869], Loss: 4.7224\n",
      "Epoch [2/2], Step [7360/16869], Loss: 4.6682\n",
      "Epoch [2/2], Step [7380/16869], Loss: 4.6783\n",
      "Epoch [2/2], Step [7400/16869], Loss: 4.6599\n",
      "Epoch [2/2], Step [7420/16869], Loss: 4.7345\n",
      "Epoch [2/2], Step [7440/16869], Loss: 4.5817\n",
      "Epoch [2/2], Step [7460/16869], Loss: 4.6332\n",
      "Epoch [2/2], Step [7480/16869], Loss: 4.6815\n",
      "Epoch [2/2], Step [7500/16869], Loss: 4.7905\n",
      "Epoch [2/2], Step [7520/16869], Loss: 4.6680\n",
      "Epoch [2/2], Step [7540/16869], Loss: 4.6268\n",
      "Epoch [2/2], Step [7560/16869], Loss: 4.6812\n",
      "Epoch [2/2], Step [7580/16869], Loss: 4.7129\n",
      "Epoch [2/2], Step [7600/16869], Loss: 4.7288\n",
      "Epoch [2/2], Step [7620/16869], Loss: 4.6641\n",
      "Epoch [2/2], Step [7640/16869], Loss: 4.6481\n",
      "Epoch [2/2], Step [7660/16869], Loss: 4.6046\n",
      "Epoch [2/2], Step [7680/16869], Loss: 4.6244\n",
      "Epoch [2/2], Step [7700/16869], Loss: 4.7011\n",
      "Epoch [2/2], Step [7720/16869], Loss: 4.6063\n",
      "Epoch [2/2], Step [7740/16869], Loss: 4.6457\n",
      "Epoch [2/2], Step [7760/16869], Loss: 4.7231\n",
      "Epoch [2/2], Step [7780/16869], Loss: 4.6939\n",
      "Epoch [2/2], Step [7800/16869], Loss: 4.5171\n",
      "Epoch [2/2], Step [7820/16869], Loss: 4.5820\n",
      "Epoch [2/2], Step [7840/16869], Loss: 4.6513\n",
      "Epoch [2/2], Step [7860/16869], Loss: 4.6807\n",
      "Epoch [2/2], Step [7880/16869], Loss: 4.6712\n",
      "Epoch [2/2], Step [7900/16869], Loss: 4.7471\n",
      "Epoch [2/2], Step [7920/16869], Loss: 4.6868\n",
      "Epoch [2/2], Step [7940/16869], Loss: 4.7690\n",
      "Epoch [2/2], Step [7960/16869], Loss: 4.6854\n",
      "Epoch [2/2], Step [7980/16869], Loss: 4.7270\n",
      "Epoch [2/2], Step [8000/16869], Loss: 4.6368\n",
      "Epoch [2/2], Step [8020/16869], Loss: 4.6358\n",
      "Epoch [2/2], Step [8040/16869], Loss: 4.7173\n",
      "Epoch [2/2], Step [8060/16869], Loss: 4.7474\n",
      "Epoch [2/2], Step [8080/16869], Loss: 4.6913\n",
      "Epoch [2/2], Step [8100/16869], Loss: 4.6529\n",
      "Epoch [2/2], Step [8120/16869], Loss: 4.5877\n",
      "Epoch [2/2], Step [8140/16869], Loss: 4.5944\n",
      "Epoch [2/2], Step [8160/16869], Loss: 4.6849\n",
      "Epoch [2/2], Step [8180/16869], Loss: 4.6926\n",
      "Epoch [2/2], Step [8200/16869], Loss: 4.7505\n",
      "Epoch [2/2], Step [8220/16869], Loss: 4.6031\n",
      "Epoch [2/2], Step [8240/16869], Loss: 4.6037\n",
      "Epoch [2/2], Step [8260/16869], Loss: 4.5886\n",
      "Epoch [2/2], Step [8280/16869], Loss: 4.5481\n",
      "Epoch [2/2], Step [8300/16869], Loss: 4.6745\n",
      "Epoch [2/2], Step [8320/16869], Loss: 4.5869\n",
      "Epoch [2/2], Step [8340/16869], Loss: 4.6569\n",
      "Epoch [2/2], Step [8360/16869], Loss: 4.6371\n",
      "Epoch [2/2], Step [8380/16869], Loss: 4.6312\n",
      "Epoch [2/2], Step [8400/16869], Loss: 4.5984\n",
      "Epoch [2/2], Step [8420/16869], Loss: 4.5187\n",
      "Epoch [2/2], Step [8440/16869], Loss: 4.7157\n",
      "Epoch [2/2], Step [8460/16869], Loss: 4.6551\n",
      "Epoch [2/2], Step [8480/16869], Loss: 4.7142\n",
      "Epoch [2/2], Step [8500/16869], Loss: 4.7205\n",
      "Epoch [2/2], Step [8520/16869], Loss: 4.6875\n",
      "Epoch [2/2], Step [8540/16869], Loss: 4.6608\n",
      "Epoch [2/2], Step [8560/16869], Loss: 4.7674\n",
      "Epoch [2/2], Step [8580/16869], Loss: 4.6204\n",
      "Epoch [2/2], Step [8600/16869], Loss: 4.7261\n",
      "Epoch [2/2], Step [8620/16869], Loss: 4.6384\n",
      "Epoch [2/2], Step [8640/16869], Loss: 4.7429\n",
      "Epoch [2/2], Step [8660/16869], Loss: 4.6219\n",
      "Epoch [2/2], Step [8680/16869], Loss: 4.6696\n",
      "Epoch [2/2], Step [8700/16869], Loss: 4.6953\n",
      "Epoch [2/2], Step [8720/16869], Loss: 4.6699\n",
      "Epoch [2/2], Step [8740/16869], Loss: 4.6911\n",
      "Epoch [2/2], Step [8760/16869], Loss: 4.7008\n",
      "Epoch [2/2], Step [8780/16869], Loss: 4.6335\n",
      "Epoch [2/2], Step [8800/16869], Loss: 4.5734\n",
      "Epoch [2/2], Step [8820/16869], Loss: 4.6219\n",
      "Epoch [2/2], Step [8840/16869], Loss: 4.7174\n",
      "Epoch [2/2], Step [8860/16869], Loss: 4.6597\n",
      "Epoch [2/2], Step [8880/16869], Loss: 4.6672\n",
      "Epoch [2/2], Step [8900/16869], Loss: 4.7802\n",
      "Epoch [2/2], Step [8920/16869], Loss: 4.6158\n",
      "Epoch [2/2], Step [8940/16869], Loss: 4.6780\n",
      "Epoch [2/2], Step [8960/16869], Loss: 4.7416\n",
      "Epoch [2/2], Step [8980/16869], Loss: 4.6831\n",
      "Epoch [2/2], Step [9000/16869], Loss: 4.6915\n",
      "Epoch [2/2], Step [9020/16869], Loss: 4.7192\n",
      "Epoch [2/2], Step [9040/16869], Loss: 4.7858\n",
      "Epoch [2/2], Step [9060/16869], Loss: 4.6354\n",
      "Epoch [2/2], Step [9080/16869], Loss: 4.6292\n",
      "Epoch [2/2], Step [9100/16869], Loss: 4.7019\n",
      "Epoch [2/2], Step [9120/16869], Loss: 4.6678\n",
      "Epoch [2/2], Step [9140/16869], Loss: 4.6295\n",
      "Epoch [2/2], Step [9160/16869], Loss: 4.6821\n",
      "Epoch [2/2], Step [9180/16869], Loss: 4.6466\n",
      "Epoch [2/2], Step [9200/16869], Loss: 4.7296\n",
      "Epoch [2/2], Step [9220/16869], Loss: 4.5582\n",
      "Epoch [2/2], Step [9240/16869], Loss: 4.7487\n",
      "Epoch [2/2], Step [9260/16869], Loss: 4.7038\n",
      "Epoch [2/2], Step [9280/16869], Loss: 4.7022\n",
      "Epoch [2/2], Step [9300/16869], Loss: 4.6131\n",
      "Epoch [2/2], Step [9320/16869], Loss: 4.6781\n",
      "Epoch [2/2], Step [9340/16869], Loss: 4.6753\n",
      "Epoch [2/2], Step [9360/16869], Loss: 4.7023\n",
      "Epoch [2/2], Step [9380/16869], Loss: 4.5698\n",
      "Epoch [2/2], Step [9400/16869], Loss: 4.6659\n",
      "Epoch [2/2], Step [9420/16869], Loss: 4.6584\n",
      "Epoch [2/2], Step [9440/16869], Loss: 4.5460\n",
      "Epoch [2/2], Step [9460/16869], Loss: 4.7581\n",
      "Epoch [2/2], Step [9480/16869], Loss: 4.6662\n",
      "Epoch [2/2], Step [9500/16869], Loss: 4.6705\n",
      "Epoch [2/2], Step [9520/16869], Loss: 4.6697\n",
      "Epoch [2/2], Step [9540/16869], Loss: 4.5947\n",
      "Epoch [2/2], Step [9560/16869], Loss: 4.6623\n",
      "Epoch [2/2], Step [9580/16869], Loss: 4.6815\n",
      "Epoch [2/2], Step [9600/16869], Loss: 4.6346\n",
      "Epoch [2/2], Step [9620/16869], Loss: 4.6144\n",
      "Epoch [2/2], Step [9640/16869], Loss: 4.6543\n",
      "Epoch [2/2], Step [9660/16869], Loss: 4.6265\n",
      "Epoch [2/2], Step [9680/16869], Loss: 4.6701\n",
      "Epoch [2/2], Step [9700/16869], Loss: 4.5986\n",
      "Epoch [2/2], Step [9720/16869], Loss: 4.6658\n",
      "Epoch [2/2], Step [9740/16869], Loss: 4.6053\n",
      "Epoch [2/2], Step [9760/16869], Loss: 4.6830\n",
      "Epoch [2/2], Step [9780/16869], Loss: 4.6394\n",
      "Epoch [2/2], Step [9800/16869], Loss: 4.6956\n",
      "Epoch [2/2], Step [9820/16869], Loss: 4.5594\n",
      "Epoch [2/2], Step [9840/16869], Loss: 4.6967\n",
      "Epoch [2/2], Step [9860/16869], Loss: 4.6665\n",
      "Epoch [2/2], Step [9880/16869], Loss: 4.5734\n",
      "Epoch [2/2], Step [9900/16869], Loss: 4.6453\n",
      "Epoch [2/2], Step [9920/16869], Loss: 4.6355\n",
      "Epoch [2/2], Step [9940/16869], Loss: 4.6380\n",
      "Epoch [2/2], Step [9960/16869], Loss: 4.5876\n",
      "Epoch [2/2], Step [9980/16869], Loss: 4.6432\n",
      "Epoch [2/2], Step [10000/16869], Loss: 4.6949\n",
      "Epoch [2/2], Step [10020/16869], Loss: 4.6969\n",
      "Epoch [2/2], Step [10040/16869], Loss: 4.6080\n",
      "Epoch [2/2], Step [10060/16869], Loss: 4.6452\n",
      "Epoch [2/2], Step [10080/16869], Loss: 4.6397\n",
      "Epoch [2/2], Step [10100/16869], Loss: 4.5642\n",
      "Epoch [2/2], Step [10120/16869], Loss: 4.6633\n",
      "Epoch [2/2], Step [10140/16869], Loss: 4.6216\n",
      "Epoch [2/2], Step [10160/16869], Loss: 4.5909\n",
      "Epoch [2/2], Step [10180/16869], Loss: 4.6152\n",
      "Epoch [2/2], Step [10200/16869], Loss: 4.6438\n",
      "Epoch [2/2], Step [10220/16869], Loss: 4.6746\n",
      "Epoch [2/2], Step [10240/16869], Loss: 4.7043\n",
      "Epoch [2/2], Step [10260/16869], Loss: 4.6922\n",
      "Epoch [2/2], Step [10280/16869], Loss: 4.6303\n",
      "Epoch [2/2], Step [10300/16869], Loss: 4.6209\n",
      "Epoch [2/2], Step [10320/16869], Loss: 4.5237\n",
      "Epoch [2/2], Step [10340/16869], Loss: 4.5903\n",
      "Epoch [2/2], Step [10360/16869], Loss: 4.7209\n",
      "Epoch [2/2], Step [10380/16869], Loss: 4.6263\n",
      "Epoch [2/2], Step [10400/16869], Loss: 4.5688\n",
      "Epoch [2/2], Step [10420/16869], Loss: 4.6447\n",
      "Epoch [2/2], Step [10440/16869], Loss: 4.6891\n",
      "Epoch [2/2], Step [10460/16869], Loss: 4.5843\n",
      "Epoch [2/2], Step [10480/16869], Loss: 4.6576\n",
      "Epoch [2/2], Step [10500/16869], Loss: 4.6894\n",
      "Epoch [2/2], Step [10520/16869], Loss: 4.6399\n",
      "Epoch [2/2], Step [10540/16869], Loss: 4.6544\n",
      "Epoch [2/2], Step [10560/16869], Loss: 4.7001\n",
      "Epoch [2/2], Step [10580/16869], Loss: 4.6174\n",
      "Epoch [2/2], Step [10600/16869], Loss: 4.6796\n",
      "Epoch [2/2], Step [10620/16869], Loss: 4.5750\n",
      "Epoch [2/2], Step [10640/16869], Loss: 4.6203\n",
      "Epoch [2/2], Step [10660/16869], Loss: 4.6161\n",
      "Epoch [2/2], Step [10680/16869], Loss: 4.7283\n",
      "Epoch [2/2], Step [10700/16869], Loss: 4.7199\n",
      "Epoch [2/2], Step [10720/16869], Loss: 4.6271\n",
      "Epoch [2/2], Step [10740/16869], Loss: 4.7776\n",
      "Epoch [2/2], Step [10760/16869], Loss: 4.6417\n",
      "Epoch [2/2], Step [10780/16869], Loss: 4.6956\n",
      "Epoch [2/2], Step [10800/16869], Loss: 4.5201\n",
      "Epoch [2/2], Step [10820/16869], Loss: 4.7148\n",
      "Epoch [2/2], Step [10840/16869], Loss: 4.6080\n",
      "Epoch [2/2], Step [10860/16869], Loss: 4.7026\n",
      "Epoch [2/2], Step [10880/16869], Loss: 4.5841\n",
      "Epoch [2/2], Step [10900/16869], Loss: 4.5503\n",
      "Epoch [2/2], Step [10920/16869], Loss: 4.6152\n",
      "Epoch [2/2], Step [10940/16869], Loss: 4.6482\n",
      "Epoch [2/2], Step [10960/16869], Loss: 4.5828\n",
      "Epoch [2/2], Step [10980/16869], Loss: 4.6600\n",
      "Epoch [2/2], Step [11000/16869], Loss: 4.6048\n",
      "Epoch [2/2], Step [11020/16869], Loss: 4.5342\n",
      "Epoch [2/2], Step [11040/16869], Loss: 4.6193\n",
      "Epoch [2/2], Step [11060/16869], Loss: 4.6915\n",
      "Epoch [2/2], Step [11080/16869], Loss: 4.6320\n",
      "Epoch [2/2], Step [11100/16869], Loss: 4.7115\n",
      "Epoch [2/2], Step [11120/16869], Loss: 4.6509\n",
      "Epoch [2/2], Step [11140/16869], Loss: 4.6508\n",
      "Epoch [2/2], Step [11160/16869], Loss: 4.6752\n",
      "Epoch [2/2], Step [11180/16869], Loss: 4.5896\n",
      "Epoch [2/2], Step [11200/16869], Loss: 4.6037\n",
      "Epoch [2/2], Step [11220/16869], Loss: 4.6577\n",
      "Epoch [2/2], Step [11240/16869], Loss: 4.5829\n",
      "Epoch [2/2], Step [11260/16869], Loss: 4.6862\n",
      "Epoch [2/2], Step [11280/16869], Loss: 4.6866\n",
      "Epoch [2/2], Step [11300/16869], Loss: 4.5785\n",
      "Epoch [2/2], Step [11320/16869], Loss: 4.6651\n",
      "Epoch [2/2], Step [11340/16869], Loss: 4.6565\n",
      "Epoch [2/2], Step [11360/16869], Loss: 4.5939\n",
      "Epoch [2/2], Step [11380/16869], Loss: 4.6420\n",
      "Epoch [2/2], Step [11400/16869], Loss: 4.6317\n",
      "Epoch [2/2], Step [11420/16869], Loss: 4.5902\n",
      "Epoch [2/2], Step [11440/16869], Loss: 4.6492\n",
      "Epoch [2/2], Step [11460/16869], Loss: 4.7498\n",
      "Epoch [2/2], Step [11480/16869], Loss: 4.6308\n",
      "Epoch [2/2], Step [11500/16869], Loss: 4.5859\n",
      "Epoch [2/2], Step [11520/16869], Loss: 4.5881\n",
      "Epoch [2/2], Step [11540/16869], Loss: 4.6594\n",
      "Epoch [2/2], Step [11560/16869], Loss: 4.6329\n",
      "Epoch [2/2], Step [11580/16869], Loss: 4.6725\n",
      "Epoch [2/2], Step [11600/16869], Loss: 4.6671\n",
      "Epoch [2/2], Step [11620/16869], Loss: 4.6153\n",
      "Epoch [2/2], Step [11640/16869], Loss: 4.6317\n",
      "Epoch [2/2], Step [11660/16869], Loss: 4.6750\n",
      "Epoch [2/2], Step [11680/16869], Loss: 4.6615\n",
      "Epoch [2/2], Step [11700/16869], Loss: 4.6086\n",
      "Epoch [2/2], Step [11720/16869], Loss: 4.7426\n",
      "Epoch [2/2], Step [11740/16869], Loss: 4.7070\n",
      "Epoch [2/2], Step [11760/16869], Loss: 4.6369\n",
      "Epoch [2/2], Step [11780/16869], Loss: 4.5964\n",
      "Epoch [2/2], Step [11800/16869], Loss: 4.6366\n",
      "Epoch [2/2], Step [11820/16869], Loss: 4.6213\n",
      "Epoch [2/2], Step [11840/16869], Loss: 4.5362\n",
      "Epoch [2/2], Step [11860/16869], Loss: 4.6101\n",
      "Epoch [2/2], Step [11880/16869], Loss: 4.6655\n",
      "Epoch [2/2], Step [11900/16869], Loss: 4.5305\n",
      "Epoch [2/2], Step [11920/16869], Loss: 4.6923\n",
      "Epoch [2/2], Step [11940/16869], Loss: 4.5982\n",
      "Epoch [2/2], Step [11960/16869], Loss: 4.5968\n",
      "Epoch [2/2], Step [11980/16869], Loss: 4.5598\n",
      "Epoch [2/2], Step [12000/16869], Loss: 4.6856\n",
      "Epoch [2/2], Step [12020/16869], Loss: 4.6968\n",
      "Epoch [2/2], Step [12040/16869], Loss: 4.6617\n",
      "Epoch [2/2], Step [12060/16869], Loss: 4.6224\n",
      "Epoch [2/2], Step [12080/16869], Loss: 4.6852\n",
      "Epoch [2/2], Step [12100/16869], Loss: 4.6680\n",
      "Epoch [2/2], Step [12120/16869], Loss: 4.5911\n",
      "Epoch [2/2], Step [12140/16869], Loss: 4.6647\n",
      "Epoch [2/2], Step [12160/16869], Loss: 4.6549\n",
      "Epoch [2/2], Step [12180/16869], Loss: 4.6439\n",
      "Epoch [2/2], Step [12200/16869], Loss: 4.6988\n",
      "Epoch [2/2], Step [12220/16869], Loss: 4.6446\n",
      "Epoch [2/2], Step [12240/16869], Loss: 4.6180\n",
      "Epoch [2/2], Step [12260/16869], Loss: 4.7388\n",
      "Epoch [2/2], Step [12280/16869], Loss: 4.5612\n",
      "Epoch [2/2], Step [12300/16869], Loss: 4.7351\n",
      "Epoch [2/2], Step [12320/16869], Loss: 4.6494\n",
      "Epoch [2/2], Step [12340/16869], Loss: 4.6828\n",
      "Epoch [2/2], Step [12360/16869], Loss: 4.6491\n",
      "Epoch [2/2], Step [12380/16869], Loss: 4.6936\n",
      "Epoch [2/2], Step [12400/16869], Loss: 4.7202\n",
      "Epoch [2/2], Step [12420/16869], Loss: 4.6674\n",
      "Epoch [2/2], Step [12440/16869], Loss: 4.6095\n",
      "Epoch [2/2], Step [12460/16869], Loss: 4.5694\n",
      "Epoch [2/2], Step [12480/16869], Loss: 4.6754\n",
      "Epoch [2/2], Step [12500/16869], Loss: 4.7294\n",
      "Epoch [2/2], Step [12520/16869], Loss: 4.5368\n",
      "Epoch [2/2], Step [12540/16869], Loss: 4.6914\n",
      "Epoch [2/2], Step [12560/16869], Loss: 4.6434\n",
      "Epoch [2/2], Step [12580/16869], Loss: 4.5519\n",
      "Epoch [2/2], Step [12600/16869], Loss: 4.6563\n",
      "Epoch [2/2], Step [12620/16869], Loss: 4.5785\n",
      "Epoch [2/2], Step [12640/16869], Loss: 4.6690\n",
      "Epoch [2/2], Step [12660/16869], Loss: 4.5906\n",
      "Epoch [2/2], Step [12680/16869], Loss: 4.6057\n",
      "Epoch [2/2], Step [12700/16869], Loss: 4.6274\n",
      "Epoch [2/2], Step [12720/16869], Loss: 4.7609\n",
      "Epoch [2/2], Step [12740/16869], Loss: 4.5965\n",
      "Epoch [2/2], Step [12760/16869], Loss: 4.6483\n",
      "Epoch [2/2], Step [12780/16869], Loss: 4.5879\n",
      "Epoch [2/2], Step [12800/16869], Loss: 4.5870\n",
      "Epoch [2/2], Step [12820/16869], Loss: 4.5271\n",
      "Epoch [2/2], Step [12840/16869], Loss: 4.6744\n",
      "Epoch [2/2], Step [12860/16869], Loss: 4.6359\n",
      "Epoch [2/2], Step [12880/16869], Loss: 4.6280\n",
      "Epoch [2/2], Step [12900/16869], Loss: 4.6022\n",
      "Epoch [2/2], Step [12920/16869], Loss: 4.5275\n",
      "Epoch [2/2], Step [12940/16869], Loss: 4.6263\n",
      "Epoch [2/2], Step [12960/16869], Loss: 4.5363\n",
      "Epoch [2/2], Step [12980/16869], Loss: 4.7316\n",
      "Epoch [2/2], Step [13000/16869], Loss: 4.5924\n",
      "Epoch [2/2], Step [13020/16869], Loss: 4.6452\n",
      "Epoch [2/2], Step [13040/16869], Loss: 4.6652\n",
      "Epoch [2/2], Step [13060/16869], Loss: 4.6220\n",
      "Epoch [2/2], Step [13080/16869], Loss: 4.6468\n",
      "Epoch [2/2], Step [13100/16869], Loss: 4.6095\n",
      "Epoch [2/2], Step [13120/16869], Loss: 4.7510\n",
      "Epoch [2/2], Step [13140/16869], Loss: 4.6365\n",
      "Epoch [2/2], Step [13160/16869], Loss: 4.5774\n",
      "Epoch [2/2], Step [13180/16869], Loss: 4.6421\n",
      "Epoch [2/2], Step [13200/16869], Loss: 4.6570\n",
      "Epoch [2/2], Step [13220/16869], Loss: 4.6462\n",
      "Epoch [2/2], Step [13240/16869], Loss: 4.6709\n",
      "Epoch [2/2], Step [13260/16869], Loss: 4.6139\n",
      "Epoch [2/2], Step [13280/16869], Loss: 4.6330\n",
      "Epoch [2/2], Step [13300/16869], Loss: 4.6249\n",
      "Epoch [2/2], Step [13320/16869], Loss: 4.7212\n",
      "Epoch [2/2], Step [13340/16869], Loss: 4.6473\n",
      "Epoch [2/2], Step [13360/16869], Loss: 4.6725\n",
      "Epoch [2/2], Step [13380/16869], Loss: 4.6510\n",
      "Epoch [2/2], Step [13400/16869], Loss: 4.5705\n",
      "Epoch [2/2], Step [13420/16869], Loss: 4.7301\n",
      "Epoch [2/2], Step [13440/16869], Loss: 4.6256\n",
      "Epoch [2/2], Step [13460/16869], Loss: 4.6920\n",
      "Epoch [2/2], Step [13480/16869], Loss: 4.6350\n",
      "Epoch [2/2], Step [13500/16869], Loss: 4.5753\n",
      "Epoch [2/2], Step [13520/16869], Loss: 4.6270\n",
      "Epoch [2/2], Step [13540/16869], Loss: 4.7172\n",
      "Epoch [2/2], Step [13560/16869], Loss: 4.6114\n",
      "Epoch [2/2], Step [13580/16869], Loss: 4.6139\n",
      "Epoch [2/2], Step [13600/16869], Loss: 4.5986\n",
      "Epoch [2/2], Step [13620/16869], Loss: 4.7035\n",
      "Epoch [2/2], Step [13640/16869], Loss: 4.6286\n",
      "Epoch [2/2], Step [13660/16869], Loss: 4.7078\n",
      "Epoch [2/2], Step [13680/16869], Loss: 4.6428\n",
      "Epoch [2/2], Step [13700/16869], Loss: 4.7081\n",
      "Epoch [2/2], Step [13720/16869], Loss: 4.6719\n",
      "Epoch [2/2], Step [13740/16869], Loss: 4.6349\n",
      "Epoch [2/2], Step [13760/16869], Loss: 4.6597\n",
      "Epoch [2/2], Step [13780/16869], Loss: 4.6689\n",
      "Epoch [2/2], Step [13800/16869], Loss: 4.6113\n",
      "Epoch [2/2], Step [13820/16869], Loss: 4.6757\n",
      "Epoch [2/2], Step [13840/16869], Loss: 4.5483\n",
      "Epoch [2/2], Step [13860/16869], Loss: 4.6194\n",
      "Epoch [2/2], Step [13880/16869], Loss: 4.6797\n",
      "Epoch [2/2], Step [13900/16869], Loss: 4.6979\n",
      "Epoch [2/2], Step [13920/16869], Loss: 4.7369\n",
      "Epoch [2/2], Step [13940/16869], Loss: 4.6305\n",
      "Epoch [2/2], Step [13960/16869], Loss: 4.6828\n",
      "Epoch [2/2], Step [13980/16869], Loss: 4.6734\n",
      "Epoch [2/2], Step [14000/16869], Loss: 4.5426\n",
      "Epoch [2/2], Step [14020/16869], Loss: 4.5840\n",
      "Epoch [2/2], Step [14040/16869], Loss: 4.7441\n",
      "Epoch [2/2], Step [14060/16869], Loss: 4.4680\n",
      "Epoch [2/2], Step [14080/16869], Loss: 4.6423\n",
      "Epoch [2/2], Step [14100/16869], Loss: 4.5936\n",
      "Epoch [2/2], Step [14120/16869], Loss: 4.6148\n",
      "Epoch [2/2], Step [14140/16869], Loss: 4.6145\n",
      "Epoch [2/2], Step [14160/16869], Loss: 4.6436\n",
      "Epoch [2/2], Step [14180/16869], Loss: 4.6100\n",
      "Epoch [2/2], Step [14200/16869], Loss: 4.6363\n",
      "Epoch [2/2], Step [14220/16869], Loss: 4.6179\n",
      "Epoch [2/2], Step [14240/16869], Loss: 4.6180\n",
      "Epoch [2/2], Step [14260/16869], Loss: 4.6390\n",
      "Epoch [2/2], Step [14280/16869], Loss: 4.7166\n",
      "Epoch [2/2], Step [14300/16869], Loss: 4.6502\n",
      "Epoch [2/2], Step [14320/16869], Loss: 4.6932\n",
      "Epoch [2/2], Step [14340/16869], Loss: 4.5633\n",
      "Epoch [2/2], Step [14360/16869], Loss: 4.6548\n",
      "Epoch [2/2], Step [14380/16869], Loss: 4.6747\n",
      "Epoch [2/2], Step [14400/16869], Loss: 4.6596\n",
      "Epoch [2/2], Step [14420/16869], Loss: 4.5975\n",
      "Epoch [2/2], Step [14440/16869], Loss: 4.6039\n",
      "Epoch [2/2], Step [14460/16869], Loss: 4.6175\n",
      "Epoch [2/2], Step [14480/16869], Loss: 4.6395\n",
      "Epoch [2/2], Step [14500/16869], Loss: 4.5862\n",
      "Epoch [2/2], Step [14520/16869], Loss: 4.6103\n",
      "Epoch [2/2], Step [14540/16869], Loss: 4.6056\n",
      "Epoch [2/2], Step [14560/16869], Loss: 4.5678\n",
      "Epoch [2/2], Step [14580/16869], Loss: 4.6467\n",
      "Epoch [2/2], Step [14600/16869], Loss: 4.6620\n",
      "Epoch [2/2], Step [14620/16869], Loss: 4.6472\n",
      "Epoch [2/2], Step [14640/16869], Loss: 4.6253\n",
      "Epoch [2/2], Step [14660/16869], Loss: 4.6060\n",
      "Epoch [2/2], Step [14680/16869], Loss: 4.6264\n",
      "Epoch [2/2], Step [14700/16869], Loss: 4.6019\n",
      "Epoch [2/2], Step [14720/16869], Loss: 4.6748\n",
      "Epoch [2/2], Step [14740/16869], Loss: 4.6463\n",
      "Epoch [2/2], Step [14760/16869], Loss: 4.5668\n",
      "Epoch [2/2], Step [14780/16869], Loss: 4.6344\n",
      "Epoch [2/2], Step [14800/16869], Loss: 4.6319\n",
      "Epoch [2/2], Step [14820/16869], Loss: 4.6284\n",
      "Epoch [2/2], Step [14840/16869], Loss: 4.5493\n",
      "Epoch [2/2], Step [14860/16869], Loss: 4.6145\n",
      "Epoch [2/2], Step [14880/16869], Loss: 4.6218\n",
      "Epoch [2/2], Step [14900/16869], Loss: 4.6846\n",
      "Epoch [2/2], Step [14920/16869], Loss: 4.6872\n",
      "Epoch [2/2], Step [14940/16869], Loss: 4.6243\n",
      "Epoch [2/2], Step [14960/16869], Loss: 4.5849\n",
      "Epoch [2/2], Step [14980/16869], Loss: 4.6056\n",
      "Epoch [2/2], Step [15000/16869], Loss: 4.5699\n",
      "Epoch [2/2], Step [15020/16869], Loss: 4.6436\n",
      "Epoch [2/2], Step [15040/16869], Loss: 4.6540\n",
      "Epoch [2/2], Step [15060/16869], Loss: 4.6352\n",
      "Epoch [2/2], Step [15080/16869], Loss: 4.5802\n",
      "Epoch [2/2], Step [15100/16869], Loss: 4.6677\n",
      "Epoch [2/2], Step [15120/16869], Loss: 4.5928\n",
      "Epoch [2/2], Step [15140/16869], Loss: 4.6922\n",
      "Epoch [2/2], Step [15160/16869], Loss: 4.7151\n",
      "Epoch [2/2], Step [15180/16869], Loss: 4.6377\n",
      "Epoch [2/2], Step [15200/16869], Loss: 4.6844\n",
      "Epoch [2/2], Step [15220/16869], Loss: 4.6619\n",
      "Epoch [2/2], Step [15240/16869], Loss: 4.5371\n",
      "Epoch [2/2], Step [15260/16869], Loss: 4.6130\n",
      "Epoch [2/2], Step [15280/16869], Loss: 4.5352\n",
      "Epoch [2/2], Step [15300/16869], Loss: 4.6714\n",
      "Epoch [2/2], Step [15320/16869], Loss: 4.6636\n",
      "Epoch [2/2], Step [15340/16869], Loss: 4.6122\n",
      "Epoch [2/2], Step [15360/16869], Loss: 4.6628\n",
      "Epoch [2/2], Step [15380/16869], Loss: 4.6838\n",
      "Epoch [2/2], Step [15400/16869], Loss: 4.6301\n",
      "Epoch [2/2], Step [15420/16869], Loss: 4.5677\n",
      "Epoch [2/2], Step [15440/16869], Loss: 4.6696\n",
      "Epoch [2/2], Step [15460/16869], Loss: 4.7196\n",
      "Epoch [2/2], Step [15480/16869], Loss: 4.6710\n",
      "Epoch [2/2], Step [15500/16869], Loss: 4.6363\n",
      "Epoch [2/2], Step [15520/16869], Loss: 4.5792\n",
      "Epoch [2/2], Step [15540/16869], Loss: 4.6819\n",
      "Epoch [2/2], Step [15560/16869], Loss: 4.6444\n",
      "Epoch [2/2], Step [15580/16869], Loss: 4.6530\n",
      "Epoch [2/2], Step [15600/16869], Loss: 4.6143\n",
      "Epoch [2/2], Step [15620/16869], Loss: 4.6484\n",
      "Epoch [2/2], Step [15640/16869], Loss: 4.6129\n",
      "Epoch [2/2], Step [15660/16869], Loss: 4.5953\n",
      "Epoch [2/2], Step [15680/16869], Loss: 4.6662\n",
      "Epoch [2/2], Step [15700/16869], Loss: 4.6307\n",
      "Epoch [2/2], Step [15720/16869], Loss: 4.6783\n",
      "Epoch [2/2], Step [15740/16869], Loss: 4.6245\n",
      "Epoch [2/2], Step [15760/16869], Loss: 4.6370\n",
      "Epoch [2/2], Step [15780/16869], Loss: 4.5720\n",
      "Epoch [2/2], Step [15800/16869], Loss: 4.6549\n",
      "Epoch [2/2], Step [15820/16869], Loss: 4.6360\n",
      "Epoch [2/2], Step [15840/16869], Loss: 4.6094\n",
      "Epoch [2/2], Step [15860/16869], Loss: 4.6865\n",
      "Epoch [2/2], Step [15880/16869], Loss: 4.6928\n",
      "Epoch [2/2], Step [15900/16869], Loss: 4.5188\n",
      "Epoch [2/2], Step [15920/16869], Loss: 4.6521\n",
      "Epoch [2/2], Step [15940/16869], Loss: 4.5950\n",
      "Epoch [2/2], Step [15960/16869], Loss: 4.7147\n",
      "Epoch [2/2], Step [15980/16869], Loss: 4.5437\n",
      "Epoch [2/2], Step [16000/16869], Loss: 4.7008\n",
      "Epoch [2/2], Step [16020/16869], Loss: 4.5615\n",
      "Epoch [2/2], Step [16040/16869], Loss: 4.5403\n",
      "Epoch [2/2], Step [16060/16869], Loss: 4.6103\n",
      "Epoch [2/2], Step [16080/16869], Loss: 4.6164\n",
      "Epoch [2/2], Step [16100/16869], Loss: 4.6245\n",
      "Epoch [2/2], Step [16120/16869], Loss: 4.5754\n",
      "Epoch [2/2], Step [16140/16869], Loss: 4.6047\n",
      "Epoch [2/2], Step [16160/16869], Loss: 4.6394\n",
      "Epoch [2/2], Step [16180/16869], Loss: 4.5764\n",
      "Epoch [2/2], Step [16200/16869], Loss: 4.5106\n",
      "Epoch [2/2], Step [16220/16869], Loss: 4.6070\n",
      "Epoch [2/2], Step [16240/16869], Loss: 4.6387\n",
      "Epoch [2/2], Step [16260/16869], Loss: 4.5963\n",
      "Epoch [2/2], Step [16280/16869], Loss: 4.6898\n",
      "Epoch [2/2], Step [16300/16869], Loss: 4.6205\n",
      "Epoch [2/2], Step [16320/16869], Loss: 4.6024\n",
      "Epoch [2/2], Step [16340/16869], Loss: 4.6535\n",
      "Epoch [2/2], Step [16360/16869], Loss: 4.5924\n",
      "Epoch [2/2], Step [16380/16869], Loss: 4.6234\n",
      "Epoch [2/2], Step [16400/16869], Loss: 4.5609\n",
      "Epoch [2/2], Step [16420/16869], Loss: 4.6680\n",
      "Epoch [2/2], Step [16440/16869], Loss: 4.5419\n",
      "Epoch [2/2], Step [16460/16869], Loss: 4.6121\n",
      "Epoch [2/2], Step [16480/16869], Loss: 4.5484\n",
      "Epoch [2/2], Step [16500/16869], Loss: 4.6681\n",
      "Epoch [2/2], Step [16520/16869], Loss: 4.5740\n",
      "Epoch [2/2], Step [16540/16869], Loss: 4.5763\n",
      "Epoch [2/2], Step [16560/16869], Loss: 4.5856\n",
      "Epoch [2/2], Step [16580/16869], Loss: 4.5713\n",
      "Epoch [2/2], Step [16600/16869], Loss: 4.6345\n",
      "Epoch [2/2], Step [16620/16869], Loss: 4.6068\n",
      "Epoch [2/2], Step [16640/16869], Loss: 4.6318\n",
      "Epoch [2/2], Step [16660/16869], Loss: 4.6410\n",
      "Epoch [2/2], Step [16680/16869], Loss: 4.6020\n",
      "Epoch [2/2], Step [16700/16869], Loss: 4.6619\n",
      "Epoch [2/2], Step [16720/16869], Loss: 4.6651\n",
      "Epoch [2/2], Step [16740/16869], Loss: 4.5439\n",
      "Epoch [2/2], Step [16760/16869], Loss: 4.5528\n",
      "Epoch [2/2], Step [16780/16869], Loss: 4.6493\n",
      "Epoch [2/2], Step [16800/16869], Loss: 4.6141\n",
      "Epoch [2/2], Step [16820/16869], Loss: 4.6205\n",
      "Epoch [2/2], Step [16840/16869], Loss: 4.6174\n",
      "Epoch [2/2], Step [16860/16869], Loss: 4.6018\n",
      "Epoch [2/2] Average Loss: 4.6600, Perplexity: 105.64\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "perplexities = []\n",
    "\n",
    "# Go through learning epochs\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # Read in data in batches\n",
    "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Apply the forward pass\n",
    "        logits = model(x)\n",
    "\n",
    "        # Reshape logits and labels\n",
    "        token_logits = logits.view(-1, vocab_size)\n",
    "        token_labels = y.view(-1)\n",
    "\n",
    "        # To understand what is happening during reshaping, print out logits.shape and token_logits.shape\n",
    "        # and the same for y\n",
    "        # print(logits.shape, token_logits.shape)\n",
    "        # print(y.shape, token_labels.shape)\n",
    "        # print(y[0])\n",
    "        # print(token_labels[0:10])\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(token_logits,token_labels)\n",
    "\n",
    "        # Apply the backward step (calculate the gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss over batches\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Monitor progress every twenty batches\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Calculate average cross-entropy loss and perplexity\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "\n",
    "    # Monitor developments over learning process\n",
    "    train_losses.append(avg_loss)\n",
    "    perplexities.append(perplexity)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:32:08.805663Z",
     "iopub.status.busy": "2024-12-14T12:32:08.804671Z",
     "iopub.status.idle": "2024-12-14T12:32:09.197747Z",
     "shell.execute_reply": "2024-12-14T12:32:09.196825Z",
     "shell.execute_reply.started": "2024-12-14T12:32:08.805519Z"
    },
    "id": "DArhdN7tkM1w",
    "outputId": "8d5dad40-462e-4f67-a27c-9c94bdefcf79"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(steps, train_losses, label='Training Loss', linestyle='dashed', marker=\"o\")\n",
    "plt.title('Simple Model - Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps, perplexities, label='Perplexity', linestyle='dashed', marker=\"o\")\n",
    "plt.title('Simple Model - Perplexity over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:32:53.170032Z",
     "iopub.status.busy": "2024-12-14T12:32:53.169698Z",
     "iopub.status.idle": "2024-12-14T12:33:13.704165Z",
     "shell.execute_reply": "2024-12-14T12:33:13.703165Z",
     "shell.execute_reply.started": "2024-12-14T12:32:53.170006Z"
    },
    "id": "a2KNCMYrkM1w",
    "outputId": "c0fe89d2-5212-4967-9c26-c2982d85793b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of base model: 89.29\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in dev_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(dev_dataloader)\n",
    "perplexity_simple = math.exp(avg_loss)\n",
    "print(f\"Perplexity of base model: {perplexity_simple:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:56:20.450529Z",
     "iopub.status.busy": "2024-12-14T12:56:20.449831Z",
     "iopub.status.idle": "2024-12-14T12:56:20.455078Z",
     "shell.execute_reply": "2024-12-14T12:56:20.454074Z",
     "shell.execute_reply.started": "2024-12-14T12:56:20.450490Z"
    },
    "id": "dTXcPjnqjySQ",
    "outputId": "cd6d0272-770d-4ef6-91d0-2ba0a3d23e31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14992265640758254\n",
      "2529.04529093951\n"
     ]
    }
   ],
   "source": [
    "print(avg_loss)\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vzM3L6LkUca"
   },
   "source": [
    "## generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:33:57.127084Z",
     "iopub.status.busy": "2024-12-14T12:33:57.126072Z",
     "iopub.status.idle": "2024-12-14T12:33:57.302014Z",
     "shell.execute_reply": "2024-12-14T12:33:57.301073Z",
     "shell.execute_reply.started": "2024-12-14T12:33:57.127050Z"
    },
    "id": "R5vpTD2kkM1x",
    "outputId": "9228cdb2-7d32-4c4d-f567-9ab51c80cb98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "\n",
      "[CLS] من در راه[SEP] نگهداری میشود و در حال حاضر بیش از اعلام بودن نام در نتیجه برسیم.\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, start_text, context_length=15, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = tokenizer.encode(start_text)\n",
    "    context = torch.tensor(generated, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(context_length):\n",
    "            if context.size(1) >= context_length:\n",
    "                break\n",
    "            logits = model(context)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "            context = torch.cat([context, next_token_id.unsqueeze(0)], dim=1)\n",
    "\n",
    "    generated_text = tokenizer.decode(context[0].tolist())\n",
    "    return generated_text\n",
    "\n",
    "start_text = \" من در راه\"\n",
    "generated_text = generate_text(model, tokenizer, start_text, context_length=20)\n",
    "print(\"Generated Text:\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Cq2FqR7jySR"
   },
   "source": [
    "؟؟؟ من در راه ؟؟؟ نگهداری میشود و در حال حاضر بیش از اعلام بودن نام در نتیجه برسیم"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYj8BlenkYSa"
   },
   "source": [
    "# cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:37:27.749352Z",
     "iopub.status.busy": "2024-12-14T12:37:27.748482Z",
     "iopub.status.idle": "2024-12-14T12:37:27.757970Z",
     "shell.execute_reply": "2024-12-14T12:37:27.757150Z",
     "shell.execute_reply.started": "2024-12-14T12:37:27.749318Z"
    },
    "id": "PFtj67LxkM1x"
   },
   "outputs": [],
   "source": [
    "class ConvolutionalLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_length, num_heads=4, num_layers=2, dropout=0.2):\n",
    "        super(ConvolutionalLanguageModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(context_length, embedding_dim)\n",
    "\n",
    "        # Convolutional block\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=embedding_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=embedding_dim, out_channels=embedding_dim, kernel_size=5, padding=2)\n",
    "        self.batch_norm = nn.BatchNorm1d(embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embedding_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        token_embeds = self.token_embedding(x)\n",
    "        position_embeds = self.position_embedding(positions)\n",
    "\n",
    "        embeddings = token_embeds + position_embeds\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        # Convolutional processing\n",
    "        embeddings = embeddings.transpose(1, 2)  # Convert to [batch_size, embedding_dim, seq_len]\n",
    "        embeddings = self.conv1(embeddings)\n",
    "        embeddings = self.relu(self.batch_norm(embeddings))\n",
    "        embeddings = self.conv2(embeddings)\n",
    "        embeddings = embeddings.transpose(1, 2)  # Convert back to [batch_size, seq_len, embedding_dim]\n",
    "\n",
    "        # Transformer processing\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        transformer_output = self.transformer(embeddings)\n",
    "        logits = self.linear(transformer_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDqj6WrnkwkG"
   },
   "source": [
    "## setting up and training cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:37:37.671055Z",
     "iopub.status.busy": "2024-12-14T12:37:37.670243Z",
     "iopub.status.idle": "2024-12-14T12:37:37.767329Z",
     "shell.execute_reply": "2024-12-14T12:37:37.766432Z",
     "shell.execute_reply.started": "2024-12-14T12:37:37.671017Z"
    },
    "id": "_dzY0H7vkM1x"
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = ConvolutionalLanguageModel(vocab_size, embedding_dim, context_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-14T12:37:43.315955Z",
     "iopub.status.busy": "2024-12-14T12:37:43.315139Z",
     "iopub.status.idle": "2024-12-14T12:56:20.448018Z",
     "shell.execute_reply": "2024-12-14T12:56:20.447004Z",
     "shell.execute_reply.started": "2024-12-14T12:37:43.315920Z"
    },
    "id": "-O0ivSr2kM1x",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a849700a-5a0e-43d0-cbf9-2f60d4876d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [0/16869], Loss: 10.4468\n",
      "Epoch [1/2], Step [20/16869], Loss: 7.0463\n",
      "Epoch [1/2], Step [40/16869], Loss: 5.3469\n",
      "Epoch [1/2], Step [60/16869], Loss: 4.4832\n",
      "Epoch [1/2], Step [80/16869], Loss: 3.9462\n",
      "Epoch [1/2], Step [100/16869], Loss: 3.5298\n",
      "Epoch [1/2], Step [120/16869], Loss: 3.1291\n",
      "Epoch [1/2], Step [140/16869], Loss: 2.9753\n",
      "Epoch [1/2], Step [160/16869], Loss: 2.5834\n",
      "Epoch [1/2], Step [180/16869], Loss: 2.3177\n",
      "Epoch [1/2], Step [200/16869], Loss: 1.9112\n",
      "Epoch [1/2], Step [220/16869], Loss: 1.8109\n",
      "Epoch [1/2], Step [240/16869], Loss: 1.6709\n",
      "Epoch [1/2], Step [260/16869], Loss: 1.4575\n",
      "Epoch [1/2], Step [280/16869], Loss: 1.4356\n",
      "Epoch [1/2], Step [300/16869], Loss: 1.3383\n",
      "Epoch [1/2], Step [320/16869], Loss: 1.2711\n",
      "Epoch [1/2], Step [340/16869], Loss: 1.0968\n",
      "Epoch [1/2], Step [360/16869], Loss: 1.0016\n",
      "Epoch [1/2], Step [380/16869], Loss: 1.0869\n",
      "Epoch [1/2], Step [400/16869], Loss: 0.9464\n",
      "Epoch [1/2], Step [420/16869], Loss: 0.9248\n",
      "Epoch [1/2], Step [440/16869], Loss: 0.8954\n",
      "Epoch [1/2], Step [460/16869], Loss: 0.7360\n",
      "Epoch [1/2], Step [480/16869], Loss: 0.7733\n",
      "Epoch [1/2], Step [500/16869], Loss: 0.7386\n",
      "Epoch [1/2], Step [520/16869], Loss: 0.6791\n",
      "Epoch [1/2], Step [540/16869], Loss: 0.6101\n",
      "Epoch [1/2], Step [560/16869], Loss: 0.6304\n",
      "Epoch [1/2], Step [580/16869], Loss: 0.6014\n",
      "Epoch [1/2], Step [600/16869], Loss: 0.5720\n",
      "Epoch [1/2], Step [620/16869], Loss: 0.5476\n",
      "Epoch [1/2], Step [640/16869], Loss: 0.5227\n",
      "Epoch [1/2], Step [660/16869], Loss: 0.5031\n",
      "Epoch [1/2], Step [680/16869], Loss: 0.5178\n",
      "Epoch [1/2], Step [700/16869], Loss: 0.4850\n",
      "Epoch [1/2], Step [720/16869], Loss: 0.4965\n",
      "Epoch [1/2], Step [740/16869], Loss: 0.4284\n",
      "Epoch [1/2], Step [760/16869], Loss: 0.4095\n",
      "Epoch [1/2], Step [780/16869], Loss: 0.4036\n",
      "Epoch [1/2], Step [800/16869], Loss: 0.4220\n",
      "Epoch [1/2], Step [820/16869], Loss: 0.4217\n",
      "Epoch [1/2], Step [840/16869], Loss: 0.4079\n",
      "Epoch [1/2], Step [860/16869], Loss: 0.3481\n",
      "Epoch [1/2], Step [880/16869], Loss: 0.3639\n",
      "Epoch [1/2], Step [900/16869], Loss: 0.4066\n",
      "Epoch [1/2], Step [920/16869], Loss: 0.3624\n",
      "Epoch [1/2], Step [940/16869], Loss: 0.3290\n",
      "Epoch [1/2], Step [960/16869], Loss: 0.3603\n",
      "Epoch [1/2], Step [980/16869], Loss: 0.3349\n",
      "Epoch [1/2], Step [1000/16869], Loss: 0.3226\n",
      "Epoch [1/2], Step [1020/16869], Loss: 0.3179\n",
      "Epoch [1/2], Step [1040/16869], Loss: 0.3347\n",
      "Epoch [1/2], Step [1060/16869], Loss: 0.2996\n",
      "Epoch [1/2], Step [1080/16869], Loss: 0.2954\n",
      "Epoch [1/2], Step [1100/16869], Loss: 0.3003\n",
      "Epoch [1/2], Step [1120/16869], Loss: 0.2708\n",
      "Epoch [1/2], Step [1140/16869], Loss: 0.2808\n",
      "Epoch [1/2], Step [1160/16869], Loss: 0.2914\n",
      "Epoch [1/2], Step [1180/16869], Loss: 0.2573\n",
      "Epoch [1/2], Step [1200/16869], Loss: 0.2803\n",
      "Epoch [1/2], Step [1220/16869], Loss: 0.2544\n",
      "Epoch [1/2], Step [1240/16869], Loss: 0.2855\n",
      "Epoch [1/2], Step [1260/16869], Loss: 0.2505\n",
      "Epoch [1/2], Step [1280/16869], Loss: 0.2937\n",
      "Epoch [1/2], Step [1300/16869], Loss: 0.2642\n",
      "Epoch [1/2], Step [1320/16869], Loss: 0.2419\n",
      "Epoch [1/2], Step [1340/16869], Loss: 0.2573\n",
      "Epoch [1/2], Step [1360/16869], Loss: 0.2695\n",
      "Epoch [1/2], Step [1380/16869], Loss: 0.2472\n",
      "Epoch [1/2], Step [1400/16869], Loss: 0.2333\n",
      "Epoch [1/2], Step [1420/16869], Loss: 0.2653\n",
      "Epoch [1/2], Step [1440/16869], Loss: 0.2510\n",
      "Epoch [1/2], Step [1460/16869], Loss: 0.2247\n",
      "Epoch [1/2], Step [1480/16869], Loss: 0.2566\n",
      "Epoch [1/2], Step [1500/16869], Loss: 0.2481\n",
      "Epoch [1/2], Step [1520/16869], Loss: 0.2520\n",
      "Epoch [1/2], Step [1540/16869], Loss: 0.2375\n",
      "Epoch [1/2], Step [1560/16869], Loss: 0.2181\n",
      "Epoch [1/2], Step [1580/16869], Loss: 0.2383\n",
      "Epoch [1/2], Step [1600/16869], Loss: 0.2332\n",
      "Epoch [1/2], Step [1620/16869], Loss: 0.2283\n",
      "Epoch [1/2], Step [1640/16869], Loss: 0.2187\n",
      "Epoch [1/2], Step [1660/16869], Loss: 0.2188\n",
      "Epoch [1/2], Step [1680/16869], Loss: 0.2391\n",
      "Epoch [1/2], Step [1700/16869], Loss: 0.2119\n",
      "Epoch [1/2], Step [1720/16869], Loss: 0.2227\n",
      "Epoch [1/2], Step [1740/16869], Loss: 0.2196\n",
      "Epoch [1/2], Step [1760/16869], Loss: 0.2180\n",
      "Epoch [1/2], Step [1780/16869], Loss: 0.2240\n",
      "Epoch [1/2], Step [1800/16869], Loss: 0.2191\n",
      "Epoch [1/2], Step [1820/16869], Loss: 0.2467\n",
      "Epoch [1/2], Step [1840/16869], Loss: 0.2154\n",
      "Epoch [1/2], Step [1860/16869], Loss: 0.2214\n",
      "Epoch [1/2], Step [1880/16869], Loss: 0.2134\n",
      "Epoch [1/2], Step [1900/16869], Loss: 0.2251\n",
      "Epoch [1/2], Step [1920/16869], Loss: 0.2180\n",
      "Epoch [1/2], Step [1940/16869], Loss: 0.2136\n",
      "Epoch [1/2], Step [1960/16869], Loss: 0.2205\n",
      "Epoch [1/2], Step [1980/16869], Loss: 0.2189\n",
      "Epoch [1/2], Step [2000/16869], Loss: 0.2053\n",
      "Epoch [1/2], Step [2020/16869], Loss: 0.1979\n",
      "Epoch [1/2], Step [2040/16869], Loss: 0.2162\n",
      "Epoch [1/2], Step [2060/16869], Loss: 0.2215\n",
      "Epoch [1/2], Step [2080/16869], Loss: 0.2161\n",
      "Epoch [1/2], Step [2100/16869], Loss: 0.2121\n",
      "Epoch [1/2], Step [2120/16869], Loss: 0.2030\n",
      "Epoch [1/2], Step [2140/16869], Loss: 0.1902\n",
      "Epoch [1/2], Step [2160/16869], Loss: 0.2125\n",
      "Epoch [1/2], Step [2180/16869], Loss: 0.2110\n",
      "Epoch [1/2], Step [2200/16869], Loss: 0.1952\n",
      "Epoch [1/2], Step [2220/16869], Loss: 0.2014\n",
      "Epoch [1/2], Step [2240/16869], Loss: 0.2099\n",
      "Epoch [1/2], Step [2260/16869], Loss: 0.1945\n",
      "Epoch [1/2], Step [2280/16869], Loss: 0.1906\n",
      "Epoch [1/2], Step [2300/16869], Loss: 0.1877\n",
      "Epoch [1/2], Step [2320/16869], Loss: 0.1871\n",
      "Epoch [1/2], Step [2340/16869], Loss: 0.1935\n",
      "Epoch [1/2], Step [2360/16869], Loss: 0.2136\n",
      "Epoch [1/2], Step [2380/16869], Loss: 0.1624\n",
      "Epoch [1/2], Step [2400/16869], Loss: 0.1915\n",
      "Epoch [1/2], Step [2420/16869], Loss: 0.2055\n",
      "Epoch [1/2], Step [2440/16869], Loss: 0.1935\n",
      "Epoch [1/2], Step [2460/16869], Loss: 0.2180\n",
      "Epoch [1/2], Step [2480/16869], Loss: 0.1774\n",
      "Epoch [1/2], Step [2500/16869], Loss: 0.1985\n",
      "Epoch [1/2], Step [2520/16869], Loss: 0.2035\n",
      "Epoch [1/2], Step [2540/16869], Loss: 0.1818\n",
      "Epoch [1/2], Step [2560/16869], Loss: 0.2007\n",
      "Epoch [1/2], Step [2580/16869], Loss: 0.1924\n",
      "Epoch [1/2], Step [2600/16869], Loss: 0.2070\n",
      "Epoch [1/2], Step [2620/16869], Loss: 0.1823\n",
      "Epoch [1/2], Step [2640/16869], Loss: 0.2065\n",
      "Epoch [1/2], Step [2660/16869], Loss: 0.2032\n",
      "Epoch [1/2], Step [2680/16869], Loss: 0.2051\n",
      "Epoch [1/2], Step [2700/16869], Loss: 0.1877\n",
      "Epoch [1/2], Step [2720/16869], Loss: 0.1851\n",
      "Epoch [1/2], Step [2740/16869], Loss: 0.1899\n",
      "Epoch [1/2], Step [2760/16869], Loss: 0.2021\n",
      "Epoch [1/2], Step [2780/16869], Loss: 0.1810\n",
      "Epoch [1/2], Step [2800/16869], Loss: 0.1693\n",
      "Epoch [1/2], Step [2820/16869], Loss: 0.1873\n",
      "Epoch [1/2], Step [2840/16869], Loss: 0.1889\n",
      "Epoch [1/2], Step [2860/16869], Loss: 0.1812\n",
      "Epoch [1/2], Step [2880/16869], Loss: 0.1742\n",
      "Epoch [1/2], Step [2900/16869], Loss: 0.1954\n",
      "Epoch [1/2], Step [2920/16869], Loss: 0.1967\n",
      "Epoch [1/2], Step [2940/16869], Loss: 0.1745\n",
      "Epoch [1/2], Step [2960/16869], Loss: 0.1805\n",
      "Epoch [1/2], Step [2980/16869], Loss: 0.1805\n",
      "Epoch [1/2], Step [3000/16869], Loss: 0.1927\n",
      "Epoch [1/2], Step [3020/16869], Loss: 0.1925\n",
      "Epoch [1/2], Step [3040/16869], Loss: 0.1878\n",
      "Epoch [1/2], Step [3060/16869], Loss: 0.1867\n",
      "Epoch [1/2], Step [3080/16869], Loss: 0.1595\n",
      "Epoch [1/2], Step [3100/16869], Loss: 0.1839\n",
      "Epoch [1/2], Step [3120/16869], Loss: 0.1747\n",
      "Epoch [1/2], Step [3140/16869], Loss: 0.1944\n",
      "Epoch [1/2], Step [3160/16869], Loss: 0.1802\n",
      "Epoch [1/2], Step [3180/16869], Loss: 0.1698\n",
      "Epoch [1/2], Step [3200/16869], Loss: 0.1835\n",
      "Epoch [1/2], Step [3220/16869], Loss: 0.1961\n",
      "Epoch [1/2], Step [3240/16869], Loss: 0.1884\n",
      "Epoch [1/2], Step [3260/16869], Loss: 0.1812\n",
      "Epoch [1/2], Step [3280/16869], Loss: 0.2051\n",
      "Epoch [1/2], Step [3300/16869], Loss: 0.1726\n",
      "Epoch [1/2], Step [3320/16869], Loss: 0.1767\n",
      "Epoch [1/2], Step [3340/16869], Loss: 0.1719\n",
      "Epoch [1/2], Step [3360/16869], Loss: 0.1702\n",
      "Epoch [1/2], Step [3380/16869], Loss: 0.1971\n",
      "Epoch [1/2], Step [3400/16869], Loss: 0.1835\n",
      "Epoch [1/2], Step [3420/16869], Loss: 0.1723\n",
      "Epoch [1/2], Step [3440/16869], Loss: 0.1927\n",
      "Epoch [1/2], Step [3460/16869], Loss: 0.1752\n",
      "Epoch [1/2], Step [3480/16869], Loss: 0.1848\n",
      "Epoch [1/2], Step [3500/16869], Loss: 0.1850\n",
      "Epoch [1/2], Step [3520/16869], Loss: 0.1825\n",
      "Epoch [1/2], Step [3540/16869], Loss: 0.1785\n",
      "Epoch [1/2], Step [3560/16869], Loss: 0.1741\n",
      "Epoch [1/2], Step [3580/16869], Loss: 0.1778\n",
      "Epoch [1/2], Step [3600/16869], Loss: 0.1834\n",
      "Epoch [1/2], Step [3620/16869], Loss: 0.1818\n",
      "Epoch [1/2], Step [3640/16869], Loss: 0.1739\n",
      "Epoch [1/2], Step [3660/16869], Loss: 0.1926\n",
      "Epoch [1/2], Step [3680/16869], Loss: 0.1820\n",
      "Epoch [1/2], Step [3700/16869], Loss: 0.1847\n",
      "Epoch [1/2], Step [3720/16869], Loss: 0.1880\n",
      "Epoch [1/2], Step [3740/16869], Loss: 0.1951\n",
      "Epoch [1/2], Step [3760/16869], Loss: 0.1901\n",
      "Epoch [1/2], Step [3780/16869], Loss: 0.1619\n",
      "Epoch [1/2], Step [3800/16869], Loss: 0.1725\n",
      "Epoch [1/2], Step [3820/16869], Loss: 0.1845\n",
      "Epoch [1/2], Step [3840/16869], Loss: 0.2053\n",
      "Epoch [1/2], Step [3860/16869], Loss: 0.1798\n",
      "Epoch [1/2], Step [3880/16869], Loss: 0.1771\n",
      "Epoch [1/2], Step [3900/16869], Loss: 0.1672\n",
      "Epoch [1/2], Step [3920/16869], Loss: 0.1812\n",
      "Epoch [1/2], Step [3940/16869], Loss: 0.1855\n",
      "Epoch [1/2], Step [3960/16869], Loss: 0.1897\n",
      "Epoch [1/2], Step [3980/16869], Loss: 0.1890\n",
      "Epoch [1/2], Step [4000/16869], Loss: 0.1778\n",
      "Epoch [1/2], Step [4020/16869], Loss: 0.1822\n",
      "Epoch [1/2], Step [4040/16869], Loss: 0.1723\n",
      "Epoch [1/2], Step [4060/16869], Loss: 0.1798\n",
      "Epoch [1/2], Step [4080/16869], Loss: 0.1843\n",
      "Epoch [1/2], Step [4100/16869], Loss: 0.1662\n",
      "Epoch [1/2], Step [4120/16869], Loss: 0.1874\n",
      "Epoch [1/2], Step [4140/16869], Loss: 0.1862\n",
      "Epoch [1/2], Step [4160/16869], Loss: 0.1885\n",
      "Epoch [1/2], Step [4180/16869], Loss: 0.1679\n",
      "Epoch [1/2], Step [4200/16869], Loss: 0.1671\n",
      "Epoch [1/2], Step [4220/16869], Loss: 0.1787\n",
      "Epoch [1/2], Step [4240/16869], Loss: 0.1865\n",
      "Epoch [1/2], Step [4260/16869], Loss: 0.1858\n",
      "Epoch [1/2], Step [4280/16869], Loss: 0.1842\n",
      "Epoch [1/2], Step [4300/16869], Loss: 0.1745\n",
      "Epoch [1/2], Step [4320/16869], Loss: 0.1905\n",
      "Epoch [1/2], Step [4340/16869], Loss: 0.1856\n",
      "Epoch [1/2], Step [4360/16869], Loss: 0.1763\n",
      "Epoch [1/2], Step [4380/16869], Loss: 0.1788\n",
      "Epoch [1/2], Step [4400/16869], Loss: 0.1800\n",
      "Epoch [1/2], Step [4420/16869], Loss: 0.1805\n",
      "Epoch [1/2], Step [4440/16869], Loss: 0.1651\n",
      "Epoch [1/2], Step [4460/16869], Loss: 0.1800\n",
      "Epoch [1/2], Step [4480/16869], Loss: 0.1870\n",
      "Epoch [1/2], Step [4500/16869], Loss: 0.1731\n",
      "Epoch [1/2], Step [4520/16869], Loss: 0.1800\n",
      "Epoch [1/2], Step [4540/16869], Loss: 0.1631\n",
      "Epoch [1/2], Step [4560/16869], Loss: 0.1782\n",
      "Epoch [1/2], Step [4580/16869], Loss: 0.1820\n",
      "Epoch [1/2], Step [4600/16869], Loss: 0.1722\n",
      "Epoch [1/2], Step [4620/16869], Loss: 0.1783\n",
      "Epoch [1/2], Step [4640/16869], Loss: 0.1880\n",
      "Epoch [1/2], Step [4660/16869], Loss: 0.1843\n",
      "Epoch [1/2], Step [4680/16869], Loss: 0.1807\n",
      "Epoch [1/2], Step [4700/16869], Loss: 0.1712\n",
      "Epoch [1/2], Step [4720/16869], Loss: 0.1747\n",
      "Epoch [1/2], Step [4740/16869], Loss: 0.1785\n",
      "Epoch [1/2], Step [4760/16869], Loss: 0.1904\n",
      "Epoch [1/2], Step [4780/16869], Loss: 0.1759\n",
      "Epoch [1/2], Step [4800/16869], Loss: 0.1602\n",
      "Epoch [1/2], Step [4820/16869], Loss: 0.1746\n",
      "Epoch [1/2], Step [4840/16869], Loss: 0.1631\n",
      "Epoch [1/2], Step [4860/16869], Loss: 0.1767\n",
      "Epoch [1/2], Step [4880/16869], Loss: 0.1772\n",
      "Epoch [1/2], Step [4900/16869], Loss: 0.1630\n",
      "Epoch [1/2], Step [4920/16869], Loss: 0.1678\n",
      "Epoch [1/2], Step [4940/16869], Loss: 0.1807\n",
      "Epoch [1/2], Step [4960/16869], Loss: 0.1839\n",
      "Epoch [1/2], Step [4980/16869], Loss: 0.1772\n",
      "Epoch [1/2], Step [5000/16869], Loss: 0.1752\n",
      "Epoch [1/2], Step [5020/16869], Loss: 0.1756\n",
      "Epoch [1/2], Step [5040/16869], Loss: 0.1785\n",
      "Epoch [1/2], Step [5060/16869], Loss: 0.1635\n",
      "Epoch [1/2], Step [5080/16869], Loss: 0.1772\n",
      "Epoch [1/2], Step [5100/16869], Loss: 0.1815\n",
      "Epoch [1/2], Step [5120/16869], Loss: 0.1856\n",
      "Epoch [1/2], Step [5140/16869], Loss: 0.1817\n",
      "Epoch [1/2], Step [5160/16869], Loss: 0.1944\n",
      "Epoch [1/2], Step [5180/16869], Loss: 0.1669\n",
      "Epoch [1/2], Step [5200/16869], Loss: 0.1814\n",
      "Epoch [1/2], Step [5220/16869], Loss: 0.1773\n",
      "Epoch [1/2], Step [5240/16869], Loss: 0.1713\n",
      "Epoch [1/2], Step [5260/16869], Loss: 0.1792\n",
      "Epoch [1/2], Step [5280/16869], Loss: 0.1734\n",
      "Epoch [1/2], Step [5300/16869], Loss: 0.1869\n",
      "Epoch [1/2], Step [5320/16869], Loss: 0.1711\n",
      "Epoch [1/2], Step [5340/16869], Loss: 0.1767\n",
      "Epoch [1/2], Step [5360/16869], Loss: 0.1748\n",
      "Epoch [1/2], Step [5380/16869], Loss: 0.1728\n",
      "Epoch [1/2], Step [5400/16869], Loss: 0.1919\n",
      "Epoch [1/2], Step [5420/16869], Loss: 0.1688\n",
      "Epoch [1/2], Step [5440/16869], Loss: 0.1700\n",
      "Epoch [1/2], Step [5460/16869], Loss: 0.1493\n",
      "Epoch [1/2], Step [5480/16869], Loss: 0.1754\n",
      "Epoch [1/2], Step [5500/16869], Loss: 0.1680\n",
      "Epoch [1/2], Step [5520/16869], Loss: 0.1598\n",
      "Epoch [1/2], Step [5540/16869], Loss: 0.1650\n",
      "Epoch [1/2], Step [5560/16869], Loss: 0.1689\n",
      "Epoch [1/2], Step [5580/16869], Loss: 0.1755\n",
      "Epoch [1/2], Step [5600/16869], Loss: 0.1767\n",
      "Epoch [1/2], Step [5620/16869], Loss: 0.1649\n",
      "Epoch [1/2], Step [5640/16869], Loss: 0.1583\n",
      "Epoch [1/2], Step [5660/16869], Loss: 0.1689\n",
      "Epoch [1/2], Step [5680/16869], Loss: 0.1728\n",
      "Epoch [1/2], Step [5700/16869], Loss: 0.1810\n",
      "Epoch [1/2], Step [5720/16869], Loss: 0.1531\n",
      "Epoch [1/2], Step [5740/16869], Loss: 0.1698\n",
      "Epoch [1/2], Step [5760/16869], Loss: 0.1640\n",
      "Epoch [1/2], Step [5780/16869], Loss: 0.2009\n",
      "Epoch [1/2], Step [5800/16869], Loss: 0.1662\n",
      "Epoch [1/2], Step [5820/16869], Loss: 0.1621\n",
      "Epoch [1/2], Step [5840/16869], Loss: 0.1756\n",
      "Epoch [1/2], Step [5860/16869], Loss: 0.1766\n",
      "Epoch [1/2], Step [5880/16869], Loss: 0.1924\n",
      "Epoch [1/2], Step [5900/16869], Loss: 0.1865\n",
      "Epoch [1/2], Step [5920/16869], Loss: 0.1696\n",
      "Epoch [1/2], Step [5940/16869], Loss: 0.1713\n",
      "Epoch [1/2], Step [5960/16869], Loss: 0.1583\n",
      "Epoch [1/2], Step [5980/16869], Loss: 0.1761\n",
      "Epoch [1/2], Step [6000/16869], Loss: 0.1687\n",
      "Epoch [1/2], Step [6020/16869], Loss: 0.1673\n",
      "Epoch [1/2], Step [6040/16869], Loss: 0.1827\n",
      "Epoch [1/2], Step [6060/16869], Loss: 0.1874\n",
      "Epoch [1/2], Step [6080/16869], Loss: 0.1605\n",
      "Epoch [1/2], Step [6100/16869], Loss: 0.1675\n",
      "Epoch [1/2], Step [6120/16869], Loss: 0.1660\n",
      "Epoch [1/2], Step [6140/16869], Loss: 0.1841\n",
      "Epoch [1/2], Step [6160/16869], Loss: 0.1562\n",
      "Epoch [1/2], Step [6180/16869], Loss: 0.1709\n",
      "Epoch [1/2], Step [6200/16869], Loss: 0.1908\n",
      "Epoch [1/2], Step [6220/16869], Loss: 0.1673\n",
      "Epoch [1/2], Step [6240/16869], Loss: 0.1792\n",
      "Epoch [1/2], Step [6260/16869], Loss: 0.1754\n",
      "Epoch [1/2], Step [6280/16869], Loss: 0.1596\n",
      "Epoch [1/2], Step [6300/16869], Loss: 0.1664\n",
      "Epoch [1/2], Step [6320/16869], Loss: 0.1637\n",
      "Epoch [1/2], Step [6340/16869], Loss: 0.1702\n",
      "Epoch [1/2], Step [6360/16869], Loss: 0.1496\n",
      "Epoch [1/2], Step [6380/16869], Loss: 0.1607\n",
      "Epoch [1/2], Step [6400/16869], Loss: 0.1681\n",
      "Epoch [1/2], Step [6420/16869], Loss: 0.1481\n",
      "Epoch [1/2], Step [6440/16869], Loss: 0.1818\n",
      "Epoch [1/2], Step [6460/16869], Loss: 0.1701\n",
      "Epoch [1/2], Step [6480/16869], Loss: 0.1847\n",
      "Epoch [1/2], Step [6500/16869], Loss: 0.1736\n",
      "Epoch [1/2], Step [6520/16869], Loss: 0.1662\n",
      "Epoch [1/2], Step [6540/16869], Loss: 0.1575\n",
      "Epoch [1/2], Step [6560/16869], Loss: 0.1661\n",
      "Epoch [1/2], Step [6580/16869], Loss: 0.1686\n",
      "Epoch [1/2], Step [6600/16869], Loss: 0.1766\n",
      "Epoch [1/2], Step [6620/16869], Loss: 0.1481\n",
      "Epoch [1/2], Step [6640/16869], Loss: 0.1920\n",
      "Epoch [1/2], Step [6660/16869], Loss: 0.1569\n",
      "Epoch [1/2], Step [6680/16869], Loss: 0.1618\n",
      "Epoch [1/2], Step [6700/16869], Loss: 0.1650\n",
      "Epoch [1/2], Step [6720/16869], Loss: 0.1707\n",
      "Epoch [1/2], Step [6740/16869], Loss: 0.1501\n",
      "Epoch [1/2], Step [6760/16869], Loss: 0.1854\n",
      "Epoch [1/2], Step [6780/16869], Loss: 0.1771\n",
      "Epoch [1/2], Step [6800/16869], Loss: 0.1759\n",
      "Epoch [1/2], Step [6820/16869], Loss: 0.1599\n",
      "Epoch [1/2], Step [6840/16869], Loss: 0.1607\n",
      "Epoch [1/2], Step [6860/16869], Loss: 0.1766\n",
      "Epoch [1/2], Step [6880/16869], Loss: 0.1626\n",
      "Epoch [1/2], Step [6900/16869], Loss: 0.1544\n",
      "Epoch [1/2], Step [6920/16869], Loss: 0.1627\n",
      "Epoch [1/2], Step [6940/16869], Loss: 0.1546\n",
      "Epoch [1/2], Step [6960/16869], Loss: 0.1801\n",
      "Epoch [1/2], Step [6980/16869], Loss: 0.1750\n",
      "Epoch [1/2], Step [7000/16869], Loss: 0.1737\n",
      "Epoch [1/2], Step [7020/16869], Loss: 0.1785\n",
      "Epoch [1/2], Step [7040/16869], Loss: 0.1680\n",
      "Epoch [1/2], Step [7060/16869], Loss: 0.1708\n",
      "Epoch [1/2], Step [7080/16869], Loss: 0.1525\n",
      "Epoch [1/2], Step [7100/16869], Loss: 0.1640\n",
      "Epoch [1/2], Step [7120/16869], Loss: 0.1546\n",
      "Epoch [1/2], Step [7140/16869], Loss: 0.1680\n",
      "Epoch [1/2], Step [7160/16869], Loss: 0.1652\n",
      "Epoch [1/2], Step [7180/16869], Loss: 0.1811\n",
      "Epoch [1/2], Step [7200/16869], Loss: 0.1720\n",
      "Epoch [1/2], Step [7220/16869], Loss: 0.1807\n",
      "Epoch [1/2], Step [7240/16869], Loss: 0.1481\n",
      "Epoch [1/2], Step [7260/16869], Loss: 0.1749\n",
      "Epoch [1/2], Step [7280/16869], Loss: 0.1518\n",
      "Epoch [1/2], Step [7300/16869], Loss: 0.1690\n",
      "Epoch [1/2], Step [7320/16869], Loss: 0.1695\n",
      "Epoch [1/2], Step [7340/16869], Loss: 0.1804\n",
      "Epoch [1/2], Step [7360/16869], Loss: 0.1739\n",
      "Epoch [1/2], Step [7380/16869], Loss: 0.1576\n",
      "Epoch [1/2], Step [7400/16869], Loss: 0.1836\n",
      "Epoch [1/2], Step [7420/16869], Loss: 0.1652\n",
      "Epoch [1/2], Step [7440/16869], Loss: 0.1678\n",
      "Epoch [1/2], Step [7460/16869], Loss: 0.1578\n",
      "Epoch [1/2], Step [7480/16869], Loss: 0.1644\n",
      "Epoch [1/2], Step [7500/16869], Loss: 0.1679\n",
      "Epoch [1/2], Step [7520/16869], Loss: 0.1678\n",
      "Epoch [1/2], Step [7540/16869], Loss: 0.1430\n",
      "Epoch [1/2], Step [7560/16869], Loss: 0.1556\n",
      "Epoch [1/2], Step [7580/16869], Loss: 0.1776\n",
      "Epoch [1/2], Step [7600/16869], Loss: 0.1595\n",
      "Epoch [1/2], Step [7620/16869], Loss: 0.1620\n",
      "Epoch [1/2], Step [7640/16869], Loss: 0.1705\n",
      "Epoch [1/2], Step [7660/16869], Loss: 0.1463\n",
      "Epoch [1/2], Step [7680/16869], Loss: 0.1601\n",
      "Epoch [1/2], Step [7700/16869], Loss: 0.1879\n",
      "Epoch [1/2], Step [7720/16869], Loss: 0.1612\n",
      "Epoch [1/2], Step [7740/16869], Loss: 0.1707\n",
      "Epoch [1/2], Step [7760/16869], Loss: 0.1630\n",
      "Epoch [1/2], Step [7780/16869], Loss: 0.1709\n",
      "Epoch [1/2], Step [7800/16869], Loss: 0.1609\n",
      "Epoch [1/2], Step [7820/16869], Loss: 0.1757\n",
      "Epoch [1/2], Step [7840/16869], Loss: 0.1693\n",
      "Epoch [1/2], Step [7860/16869], Loss: 0.1684\n",
      "Epoch [1/2], Step [7880/16869], Loss: 0.1758\n",
      "Epoch [1/2], Step [7900/16869], Loss: 0.1829\n",
      "Epoch [1/2], Step [7920/16869], Loss: 0.1615\n",
      "Epoch [1/2], Step [7940/16869], Loss: 0.1584\n",
      "Epoch [1/2], Step [7960/16869], Loss: 0.1706\n",
      "Epoch [1/2], Step [7980/16869], Loss: 0.1479\n",
      "Epoch [1/2], Step [8000/16869], Loss: 0.1701\n",
      "Epoch [1/2], Step [8020/16869], Loss: 0.1566\n",
      "Epoch [1/2], Step [8040/16869], Loss: 0.1730\n",
      "Epoch [1/2], Step [8060/16869], Loss: 0.1740\n",
      "Epoch [1/2], Step [8080/16869], Loss: 0.1782\n",
      "Epoch [1/2], Step [8100/16869], Loss: 0.1532\n",
      "Epoch [1/2], Step [8120/16869], Loss: 0.1862\n",
      "Epoch [1/2], Step [8140/16869], Loss: 0.1738\n",
      "Epoch [1/2], Step [8160/16869], Loss: 0.1754\n",
      "Epoch [1/2], Step [8180/16869], Loss: 0.1618\n",
      "Epoch [1/2], Step [8200/16869], Loss: 0.1581\n",
      "Epoch [1/2], Step [8220/16869], Loss: 0.1644\n",
      "Epoch [1/2], Step [8240/16869], Loss: 0.1808\n",
      "Epoch [1/2], Step [8260/16869], Loss: 0.1719\n",
      "Epoch [1/2], Step [8280/16869], Loss: 0.1591\n",
      "Epoch [1/2], Step [8300/16869], Loss: 0.1525\n",
      "Epoch [1/2], Step [8320/16869], Loss: 0.1746\n",
      "Epoch [1/2], Step [8340/16869], Loss: 0.1952\n",
      "Epoch [1/2], Step [8360/16869], Loss: 0.1636\n",
      "Epoch [1/2], Step [8380/16869], Loss: 0.1688\n",
      "Epoch [1/2], Step [8400/16869], Loss: 0.1763\n",
      "Epoch [1/2], Step [8420/16869], Loss: 0.1666\n",
      "Epoch [1/2], Step [8440/16869], Loss: 0.1633\n",
      "Epoch [1/2], Step [8460/16869], Loss: 0.1667\n",
      "Epoch [1/2], Step [8480/16869], Loss: 0.1602\n",
      "Epoch [1/2], Step [8500/16869], Loss: 0.1492\n",
      "Epoch [1/2], Step [8520/16869], Loss: 0.1716\n",
      "Epoch [1/2], Step [8540/16869], Loss: 0.1573\n",
      "Epoch [1/2], Step [8560/16869], Loss: 0.1559\n",
      "Epoch [1/2], Step [8580/16869], Loss: 0.1465\n",
      "Epoch [1/2], Step [8600/16869], Loss: 0.1702\n",
      "Epoch [1/2], Step [8620/16869], Loss: 0.1735\n",
      "Epoch [1/2], Step [8640/16869], Loss: 0.1641\n",
      "Epoch [1/2], Step [8660/16869], Loss: 0.1710\n",
      "Epoch [1/2], Step [8680/16869], Loss: 0.1636\n",
      "Epoch [1/2], Step [8700/16869], Loss: 0.1737\n",
      "Epoch [1/2], Step [8720/16869], Loss: 0.1692\n",
      "Epoch [1/2], Step [8740/16869], Loss: 0.1649\n",
      "Epoch [1/2], Step [8760/16869], Loss: 0.1486\n",
      "Epoch [1/2], Step [8780/16869], Loss: 0.1586\n",
      "Epoch [1/2], Step [8800/16869], Loss: 0.1809\n",
      "Epoch [1/2], Step [8820/16869], Loss: 0.1719\n",
      "Epoch [1/2], Step [8840/16869], Loss: 0.1772\n",
      "Epoch [1/2], Step [8860/16869], Loss: 0.1700\n",
      "Epoch [1/2], Step [8880/16869], Loss: 0.1551\n",
      "Epoch [1/2], Step [8900/16869], Loss: 0.1735\n",
      "Epoch [1/2], Step [8920/16869], Loss: 0.1430\n",
      "Epoch [1/2], Step [8940/16869], Loss: 0.1606\n",
      "Epoch [1/2], Step [8960/16869], Loss: 0.1695\n",
      "Epoch [1/2], Step [8980/16869], Loss: 0.1740\n",
      "Epoch [1/2], Step [9000/16869], Loss: 0.1741\n",
      "Epoch [1/2], Step [9020/16869], Loss: 0.1594\n",
      "Epoch [1/2], Step [9040/16869], Loss: 0.1446\n",
      "Epoch [1/2], Step [9060/16869], Loss: 0.1561\n",
      "Epoch [1/2], Step [9080/16869], Loss: 0.1698\n",
      "Epoch [1/2], Step [9100/16869], Loss: 0.1804\n",
      "Epoch [1/2], Step [9120/16869], Loss: 0.1548\n",
      "Epoch [1/2], Step [9140/16869], Loss: 0.1769\n",
      "Epoch [1/2], Step [9160/16869], Loss: 0.1665\n",
      "Epoch [1/2], Step [9180/16869], Loss: 0.1589\n",
      "Epoch [1/2], Step [9200/16869], Loss: 0.1635\n",
      "Epoch [1/2], Step [9220/16869], Loss: 0.1622\n",
      "Epoch [1/2], Step [9240/16869], Loss: 0.1662\n",
      "Epoch [1/2], Step [9260/16869], Loss: 0.1719\n",
      "Epoch [1/2], Step [9280/16869], Loss: 0.1734\n",
      "Epoch [1/2], Step [9300/16869], Loss: 0.1720\n",
      "Epoch [1/2], Step [9320/16869], Loss: 0.1630\n",
      "Epoch [1/2], Step [9340/16869], Loss: 0.1458\n",
      "Epoch [1/2], Step [9360/16869], Loss: 0.1670\n",
      "Epoch [1/2], Step [9380/16869], Loss: 0.1588\n",
      "Epoch [1/2], Step [9400/16869], Loss: 0.1697\n",
      "Epoch [1/2], Step [9420/16869], Loss: 0.1557\n",
      "Epoch [1/2], Step [9440/16869], Loss: 0.1677\n",
      "Epoch [1/2], Step [9460/16869], Loss: 0.1789\n",
      "Epoch [1/2], Step [9480/16869], Loss: 0.1708\n",
      "Epoch [1/2], Step [9500/16869], Loss: 0.1762\n",
      "Epoch [1/2], Step [9520/16869], Loss: 0.1513\n",
      "Epoch [1/2], Step [9540/16869], Loss: 0.1604\n",
      "Epoch [1/2], Step [9560/16869], Loss: 0.1789\n",
      "Epoch [1/2], Step [9580/16869], Loss: 0.1502\n",
      "Epoch [1/2], Step [9600/16869], Loss: 0.1637\n",
      "Epoch [1/2], Step [9620/16869], Loss: 0.1679\n",
      "Epoch [1/2], Step [9640/16869], Loss: 0.1866\n",
      "Epoch [1/2], Step [9660/16869], Loss: 0.1496\n",
      "Epoch [1/2], Step [9680/16869], Loss: 0.1646\n",
      "Epoch [1/2], Step [9700/16869], Loss: 0.1586\n",
      "Epoch [1/2], Step [9720/16869], Loss: 0.1583\n",
      "Epoch [1/2], Step [9740/16869], Loss: 0.1599\n",
      "Epoch [1/2], Step [9760/16869], Loss: 0.1552\n",
      "Epoch [1/2], Step [9780/16869], Loss: 0.1736\n",
      "Epoch [1/2], Step [9800/16869], Loss: 0.1607\n",
      "Epoch [1/2], Step [9820/16869], Loss: 0.1605\n",
      "Epoch [1/2], Step [9840/16869], Loss: 0.1682\n",
      "Epoch [1/2], Step [9860/16869], Loss: 0.1675\n",
      "Epoch [1/2], Step [9880/16869], Loss: 0.1421\n",
      "Epoch [1/2], Step [9900/16869], Loss: 0.1561\n",
      "Epoch [1/2], Step [9920/16869], Loss: 0.1525\n",
      "Epoch [1/2], Step [9940/16869], Loss: 0.1451\n",
      "Epoch [1/2], Step [9960/16869], Loss: 0.1709\n",
      "Epoch [1/2], Step [9980/16869], Loss: 0.1297\n",
      "Epoch [1/2], Step [10000/16869], Loss: 0.1545\n",
      "Epoch [1/2], Step [10020/16869], Loss: 0.1582\n",
      "Epoch [1/2], Step [10040/16869], Loss: 0.1690\n",
      "Epoch [1/2], Step [10060/16869], Loss: 0.1613\n",
      "Epoch [1/2], Step [10080/16869], Loss: 0.1533\n",
      "Epoch [1/2], Step [10100/16869], Loss: 0.1642\n",
      "Epoch [1/2], Step [10120/16869], Loss: 0.1636\n",
      "Epoch [1/2], Step [10140/16869], Loss: 0.1798\n",
      "Epoch [1/2], Step [10160/16869], Loss: 0.1689\n",
      "Epoch [1/2], Step [10180/16869], Loss: 0.1632\n",
      "Epoch [1/2], Step [10200/16869], Loss: 0.1646\n",
      "Epoch [1/2], Step [10220/16869], Loss: 0.1557\n",
      "Epoch [1/2], Step [10240/16869], Loss: 0.1574\n",
      "Epoch [1/2], Step [10260/16869], Loss: 0.1622\n",
      "Epoch [1/2], Step [10280/16869], Loss: 0.1594\n",
      "Epoch [1/2], Step [10300/16869], Loss: 0.1643\n",
      "Epoch [1/2], Step [10320/16869], Loss: 0.1819\n",
      "Epoch [1/2], Step [10340/16869], Loss: 0.1521\n",
      "Epoch [1/2], Step [10360/16869], Loss: 0.1597\n",
      "Epoch [1/2], Step [10380/16869], Loss: 0.1644\n",
      "Epoch [1/2], Step [10400/16869], Loss: 0.1735\n",
      "Epoch [1/2], Step [10420/16869], Loss: 0.1688\n",
      "Epoch [1/2], Step [10440/16869], Loss: 0.1670\n",
      "Epoch [1/2], Step [10460/16869], Loss: 0.1579\n",
      "Epoch [1/2], Step [10480/16869], Loss: 0.1509\n",
      "Epoch [1/2], Step [10500/16869], Loss: 0.1543\n",
      "Epoch [1/2], Step [10520/16869], Loss: 0.1652\n",
      "Epoch [1/2], Step [10540/16869], Loss: 0.1577\n",
      "Epoch [1/2], Step [10560/16869], Loss: 0.1535\n",
      "Epoch [1/2], Step [10580/16869], Loss: 0.1468\n",
      "Epoch [1/2], Step [10600/16869], Loss: 0.1634\n",
      "Epoch [1/2], Step [10620/16869], Loss: 0.1633\n",
      "Epoch [1/2], Step [10640/16869], Loss: 0.1571\n",
      "Epoch [1/2], Step [10660/16869], Loss: 0.1614\n",
      "Epoch [1/2], Step [10680/16869], Loss: 0.1662\n",
      "Epoch [1/2], Step [10700/16869], Loss: 0.1669\n",
      "Epoch [1/2], Step [10720/16869], Loss: 0.1540\n",
      "Epoch [1/2], Step [10740/16869], Loss: 0.1702\n",
      "Epoch [1/2], Step [10760/16869], Loss: 0.1636\n",
      "Epoch [1/2], Step [10780/16869], Loss: 0.1654\n",
      "Epoch [1/2], Step [10800/16869], Loss: 0.1614\n",
      "Epoch [1/2], Step [10820/16869], Loss: 0.1543\n",
      "Epoch [1/2], Step [10840/16869], Loss: 0.1616\n",
      "Epoch [1/2], Step [10860/16869], Loss: 0.1666\n",
      "Epoch [1/2], Step [10880/16869], Loss: 0.1543\n",
      "Epoch [1/2], Step [10900/16869], Loss: 0.1566\n",
      "Epoch [1/2], Step [10920/16869], Loss: 0.1564\n",
      "Epoch [1/2], Step [10940/16869], Loss: 0.1492\n",
      "Epoch [1/2], Step [10960/16869], Loss: 0.1677\n",
      "Epoch [1/2], Step [10980/16869], Loss: 0.1513\n",
      "Epoch [1/2], Step [11000/16869], Loss: 0.1443\n",
      "Epoch [1/2], Step [11020/16869], Loss: 0.1588\n",
      "Epoch [1/2], Step [11040/16869], Loss: 0.1436\n",
      "Epoch [1/2], Step [11060/16869], Loss: 0.1675\n",
      "Epoch [1/2], Step [11080/16869], Loss: 0.1820\n",
      "Epoch [1/2], Step [11100/16869], Loss: 0.1714\n",
      "Epoch [1/2], Step [11120/16869], Loss: 0.1733\n",
      "Epoch [1/2], Step [11140/16869], Loss: 0.1551\n",
      "Epoch [1/2], Step [11160/16869], Loss: 0.1726\n",
      "Epoch [1/2], Step [11180/16869], Loss: 0.1526\n",
      "Epoch [1/2], Step [11200/16869], Loss: 0.1601\n",
      "Epoch [1/2], Step [11220/16869], Loss: 0.1699\n",
      "Epoch [1/2], Step [11240/16869], Loss: 0.1525\n",
      "Epoch [1/2], Step [11260/16869], Loss: 0.1619\n",
      "Epoch [1/2], Step [11280/16869], Loss: 0.1627\n",
      "Epoch [1/2], Step [11300/16869], Loss: 0.1699\n",
      "Epoch [1/2], Step [11320/16869], Loss: 0.1583\n",
      "Epoch [1/2], Step [11340/16869], Loss: 0.1757\n",
      "Epoch [1/2], Step [11360/16869], Loss: 0.1631\n",
      "Epoch [1/2], Step [11380/16869], Loss: 0.1629\n",
      "Epoch [1/2], Step [11400/16869], Loss: 0.1592\n",
      "Epoch [1/2], Step [11420/16869], Loss: 0.1533\n",
      "Epoch [1/2], Step [11440/16869], Loss: 0.1636\n",
      "Epoch [1/2], Step [11460/16869], Loss: 0.1610\n",
      "Epoch [1/2], Step [11480/16869], Loss: 0.1684\n",
      "Epoch [1/2], Step [11500/16869], Loss: 0.1633\n",
      "Epoch [1/2], Step [11520/16869], Loss: 0.1553\n",
      "Epoch [1/2], Step [11540/16869], Loss: 0.1550\n",
      "Epoch [1/2], Step [11560/16869], Loss: 0.1557\n",
      "Epoch [1/2], Step [11580/16869], Loss: 0.1622\n",
      "Epoch [1/2], Step [11600/16869], Loss: 0.1553\n",
      "Epoch [1/2], Step [11620/16869], Loss: 0.1424\n",
      "Epoch [1/2], Step [11640/16869], Loss: 0.1613\n",
      "Epoch [1/2], Step [11660/16869], Loss: 0.1650\n",
      "Epoch [1/2], Step [11680/16869], Loss: 0.1669\n",
      "Epoch [1/2], Step [11700/16869], Loss: 0.1535\n",
      "Epoch [1/2], Step [11720/16869], Loss: 0.1510\n",
      "Epoch [1/2], Step [11740/16869], Loss: 0.1610\n",
      "Epoch [1/2], Step [11760/16869], Loss: 0.1593\n",
      "Epoch [1/2], Step [11780/16869], Loss: 0.1613\n",
      "Epoch [1/2], Step [11800/16869], Loss: 0.1513\n",
      "Epoch [1/2], Step [11820/16869], Loss: 0.1623\n",
      "Epoch [1/2], Step [11840/16869], Loss: 0.1681\n",
      "Epoch [1/2], Step [11860/16869], Loss: 0.1407\n",
      "Epoch [1/2], Step [11880/16869], Loss: 0.1529\n",
      "Epoch [1/2], Step [11900/16869], Loss: 0.1482\n",
      "Epoch [1/2], Step [11920/16869], Loss: 0.1574\n",
      "Epoch [1/2], Step [11940/16869], Loss: 0.1605\n",
      "Epoch [1/2], Step [11960/16869], Loss: 0.1531\n",
      "Epoch [1/2], Step [11980/16869], Loss: 0.1643\n",
      "Epoch [1/2], Step [12000/16869], Loss: 0.1551\n",
      "Epoch [1/2], Step [12020/16869], Loss: 0.1595\n",
      "Epoch [1/2], Step [12040/16869], Loss: 0.1781\n",
      "Epoch [1/2], Step [12060/16869], Loss: 0.1634\n",
      "Epoch [1/2], Step [12080/16869], Loss: 0.1801\n",
      "Epoch [1/2], Step [12100/16869], Loss: 0.1657\n",
      "Epoch [1/2], Step [12120/16869], Loss: 0.1633\n",
      "Epoch [1/2], Step [12140/16869], Loss: 0.1732\n",
      "Epoch [1/2], Step [12160/16869], Loss: 0.1523\n",
      "Epoch [1/2], Step [12180/16869], Loss: 0.1689\n",
      "Epoch [1/2], Step [12200/16869], Loss: 0.1860\n",
      "Epoch [1/2], Step [12220/16869], Loss: 0.1549\n",
      "Epoch [1/2], Step [12240/16869], Loss: 0.1619\n",
      "Epoch [1/2], Step [12260/16869], Loss: 0.1570\n",
      "Epoch [1/2], Step [12280/16869], Loss: 0.1576\n",
      "Epoch [1/2], Step [12300/16869], Loss: 0.1674\n",
      "Epoch [1/2], Step [12320/16869], Loss: 0.1644\n",
      "Epoch [1/2], Step [12340/16869], Loss: 0.1687\n",
      "Epoch [1/2], Step [12360/16869], Loss: 0.1487\n",
      "Epoch [1/2], Step [12380/16869], Loss: 0.1496\n",
      "Epoch [1/2], Step [12400/16869], Loss: 0.1622\n",
      "Epoch [1/2], Step [12420/16869], Loss: 0.1565\n",
      "Epoch [1/2], Step [12440/16869], Loss: 0.1670\n",
      "Epoch [1/2], Step [12460/16869], Loss: 0.1572\n",
      "Epoch [1/2], Step [12480/16869], Loss: 0.1635\n",
      "Epoch [1/2], Step [12500/16869], Loss: 0.1547\n",
      "Epoch [1/2], Step [12520/16869], Loss: 0.1545\n",
      "Epoch [1/2], Step [12540/16869], Loss: 0.1854\n",
      "Epoch [1/2], Step [12560/16869], Loss: 0.1394\n",
      "Epoch [1/2], Step [12580/16869], Loss: 0.1543\n",
      "Epoch [1/2], Step [12600/16869], Loss: 0.1557\n",
      "Epoch [1/2], Step [12620/16869], Loss: 0.1688\n",
      "Epoch [1/2], Step [12640/16869], Loss: 0.1470\n",
      "Epoch [1/2], Step [12660/16869], Loss: 0.1489\n",
      "Epoch [1/2], Step [12680/16869], Loss: 0.1523\n",
      "Epoch [1/2], Step [12700/16869], Loss: 0.1491\n",
      "Epoch [1/2], Step [12720/16869], Loss: 0.1632\n",
      "Epoch [1/2], Step [12740/16869], Loss: 0.1567\n",
      "Epoch [1/2], Step [12760/16869], Loss: 0.1584\n",
      "Epoch [1/2], Step [12780/16869], Loss: 0.1484\n",
      "Epoch [1/2], Step [12800/16869], Loss: 0.1646\n",
      "Epoch [1/2], Step [12820/16869], Loss: 0.1616\n",
      "Epoch [1/2], Step [12840/16869], Loss: 0.1523\n",
      "Epoch [1/2], Step [12860/16869], Loss: 0.1616\n",
      "Epoch [1/2], Step [12880/16869], Loss: 0.1553\n",
      "Epoch [1/2], Step [12900/16869], Loss: 0.1559\n",
      "Epoch [1/2], Step [12920/16869], Loss: 0.1634\n",
      "Epoch [1/2], Step [12940/16869], Loss: 0.1600\n",
      "Epoch [1/2], Step [12960/16869], Loss: 0.1463\n",
      "Epoch [1/2], Step [12980/16869], Loss: 0.1681\n",
      "Epoch [1/2], Step [13000/16869], Loss: 0.1647\n",
      "Epoch [1/2], Step [13020/16869], Loss: 0.1572\n",
      "Epoch [1/2], Step [13040/16869], Loss: 0.1531\n",
      "Epoch [1/2], Step [13060/16869], Loss: 0.1524\n",
      "Epoch [1/2], Step [13080/16869], Loss: 0.1744\n",
      "Epoch [1/2], Step [13100/16869], Loss: 0.1436\n",
      "Epoch [1/2], Step [13120/16869], Loss: 0.1537\n",
      "Epoch [1/2], Step [13140/16869], Loss: 0.1538\n",
      "Epoch [1/2], Step [13160/16869], Loss: 0.1525\n",
      "Epoch [1/2], Step [13180/16869], Loss: 0.1630\n",
      "Epoch [1/2], Step [13200/16869], Loss: 0.1732\n",
      "Epoch [1/2], Step [13220/16869], Loss: 0.1511\n",
      "Epoch [1/2], Step [13240/16869], Loss: 0.1630\n",
      "Epoch [1/2], Step [13260/16869], Loss: 0.1398\n",
      "Epoch [1/2], Step [13280/16869], Loss: 0.1648\n",
      "Epoch [1/2], Step [13300/16869], Loss: 0.1583\n",
      "Epoch [1/2], Step [13320/16869], Loss: 0.1608\n",
      "Epoch [1/2], Step [13340/16869], Loss: 0.1656\n",
      "Epoch [1/2], Step [13360/16869], Loss: 0.1563\n",
      "Epoch [1/2], Step [13380/16869], Loss: 0.1641\n",
      "Epoch [1/2], Step [13400/16869], Loss: 0.1608\n",
      "Epoch [1/2], Step [13420/16869], Loss: 0.1427\n",
      "Epoch [1/2], Step [13440/16869], Loss: 0.1502\n",
      "Epoch [1/2], Step [13460/16869], Loss: 0.1592\n",
      "Epoch [1/2], Step [13480/16869], Loss: 0.1568\n",
      "Epoch [1/2], Step [13500/16869], Loss: 0.1552\n",
      "Epoch [1/2], Step [13520/16869], Loss: 0.1522\n",
      "Epoch [1/2], Step [13540/16869], Loss: 0.1604\n",
      "Epoch [1/2], Step [13560/16869], Loss: 0.1532\n",
      "Epoch [1/2], Step [13580/16869], Loss: 0.1685\n",
      "Epoch [1/2], Step [13600/16869], Loss: 0.1563\n",
      "Epoch [1/2], Step [13620/16869], Loss: 0.1757\n",
      "Epoch [1/2], Step [13640/16869], Loss: 0.1611\n",
      "Epoch [1/2], Step [13660/16869], Loss: 0.1630\n",
      "Epoch [1/2], Step [13680/16869], Loss: 0.1462\n",
      "Epoch [1/2], Step [13700/16869], Loss: 0.1452\n",
      "Epoch [1/2], Step [13720/16869], Loss: 0.1718\n",
      "Epoch [1/2], Step [13740/16869], Loss: 0.1442\n",
      "Epoch [1/2], Step [13760/16869], Loss: 0.1602\n",
      "Epoch [1/2], Step [13780/16869], Loss: 0.1654\n",
      "Epoch [1/2], Step [13800/16869], Loss: 0.1496\n",
      "Epoch [1/2], Step [13820/16869], Loss: 0.1622\n",
      "Epoch [1/2], Step [13840/16869], Loss: 0.1594\n",
      "Epoch [1/2], Step [13860/16869], Loss: 0.1452\n",
      "Epoch [1/2], Step [13880/16869], Loss: 0.1653\n",
      "Epoch [1/2], Step [13900/16869], Loss: 0.1407\n",
      "Epoch [1/2], Step [13920/16869], Loss: 0.1538\n",
      "Epoch [1/2], Step [13940/16869], Loss: 0.1675\n",
      "Epoch [1/2], Step [13960/16869], Loss: 0.1708\n",
      "Epoch [1/2], Step [13980/16869], Loss: 0.1625\n",
      "Epoch [1/2], Step [14000/16869], Loss: 0.1513\n",
      "Epoch [1/2], Step [14020/16869], Loss: 0.1523\n",
      "Epoch [1/2], Step [14040/16869], Loss: 0.1627\n",
      "Epoch [1/2], Step [14060/16869], Loss: 0.1679\n",
      "Epoch [1/2], Step [14080/16869], Loss: 0.1514\n",
      "Epoch [1/2], Step [14100/16869], Loss: 0.1725\n",
      "Epoch [1/2], Step [14120/16869], Loss: 0.1607\n",
      "Epoch [1/2], Step [14140/16869], Loss: 0.1491\n",
      "Epoch [1/2], Step [14160/16869], Loss: 0.1540\n",
      "Epoch [1/2], Step [14180/16869], Loss: 0.1678\n",
      "Epoch [1/2], Step [14200/16869], Loss: 0.1467\n",
      "Epoch [1/2], Step [14220/16869], Loss: 0.1662\n",
      "Epoch [1/2], Step [14240/16869], Loss: 0.1605\n",
      "Epoch [1/2], Step [14260/16869], Loss: 0.1527\n",
      "Epoch [1/2], Step [14280/16869], Loss: 0.1508\n",
      "Epoch [1/2], Step [14300/16869], Loss: 0.1504\n",
      "Epoch [1/2], Step [14320/16869], Loss: 0.1600\n",
      "Epoch [1/2], Step [14340/16869], Loss: 0.1548\n",
      "Epoch [1/2], Step [14360/16869], Loss: 0.1661\n",
      "Epoch [1/2], Step [14380/16869], Loss: 0.1650\n",
      "Epoch [1/2], Step [14400/16869], Loss: 0.1685\n",
      "Epoch [1/2], Step [14420/16869], Loss: 0.1622\n",
      "Epoch [1/2], Step [14440/16869], Loss: 0.1546\n",
      "Epoch [1/2], Step [14460/16869], Loss: 0.1553\n",
      "Epoch [1/2], Step [14480/16869], Loss: 0.1582\n",
      "Epoch [1/2], Step [14500/16869], Loss: 0.1697\n",
      "Epoch [1/2], Step [14520/16869], Loss: 0.1418\n",
      "Epoch [1/2], Step [14540/16869], Loss: 0.1709\n",
      "Epoch [1/2], Step [14560/16869], Loss: 0.1510\n",
      "Epoch [1/2], Step [14580/16869], Loss: 0.1615\n",
      "Epoch [1/2], Step [14600/16869], Loss: 0.1485\n",
      "Epoch [1/2], Step [14620/16869], Loss: 0.1594\n",
      "Epoch [1/2], Step [14640/16869], Loss: 0.1348\n",
      "Epoch [1/2], Step [14660/16869], Loss: 0.1459\n",
      "Epoch [1/2], Step [14680/16869], Loss: 0.1613\n",
      "Epoch [1/2], Step [14700/16869], Loss: 0.1567\n",
      "Epoch [1/2], Step [14720/16869], Loss: 0.1472\n",
      "Epoch [1/2], Step [14740/16869], Loss: 0.1593\n",
      "Epoch [1/2], Step [14760/16869], Loss: 0.1528\n",
      "Epoch [1/2], Step [14780/16869], Loss: 0.1627\n",
      "Epoch [1/2], Step [14800/16869], Loss: 0.1534\n",
      "Epoch [1/2], Step [14820/16869], Loss: 0.1511\n",
      "Epoch [1/2], Step [14840/16869], Loss: 0.1521\n",
      "Epoch [1/2], Step [14860/16869], Loss: 0.1520\n",
      "Epoch [1/2], Step [14880/16869], Loss: 0.1568\n",
      "Epoch [1/2], Step [14900/16869], Loss: 0.1479\n",
      "Epoch [1/2], Step [14920/16869], Loss: 0.1460\n",
      "Epoch [1/2], Step [14940/16869], Loss: 0.1468\n",
      "Epoch [1/2], Step [14960/16869], Loss: 0.1581\n",
      "Epoch [1/2], Step [14980/16869], Loss: 0.1518\n",
      "Epoch [1/2], Step [15000/16869], Loss: 0.1708\n",
      "Epoch [1/2], Step [15020/16869], Loss: 0.1548\n",
      "Epoch [1/2], Step [15040/16869], Loss: 0.1426\n",
      "Epoch [1/2], Step [15060/16869], Loss: 0.1812\n",
      "Epoch [1/2], Step [15080/16869], Loss: 0.1564\n",
      "Epoch [1/2], Step [15100/16869], Loss: 0.1446\n",
      "Epoch [1/2], Step [15120/16869], Loss: 0.1688\n",
      "Epoch [1/2], Step [15140/16869], Loss: 0.1496\n",
      "Epoch [1/2], Step [15160/16869], Loss: 0.1646\n",
      "Epoch [1/2], Step [15180/16869], Loss: 0.1641\n",
      "Epoch [1/2], Step [15200/16869], Loss: 0.1473\n",
      "Epoch [1/2], Step [15220/16869], Loss: 0.1543\n",
      "Epoch [1/2], Step [15240/16869], Loss: 0.1624\n",
      "Epoch [1/2], Step [15260/16869], Loss: 0.1690\n",
      "Epoch [1/2], Step [15280/16869], Loss: 0.1498\n",
      "Epoch [1/2], Step [15300/16869], Loss: 0.1545\n",
      "Epoch [1/2], Step [15320/16869], Loss: 0.1716\n",
      "Epoch [1/2], Step [15340/16869], Loss: 0.1652\n",
      "Epoch [1/2], Step [15360/16869], Loss: 0.1622\n",
      "Epoch [1/2], Step [15380/16869], Loss: 0.1657\n",
      "Epoch [1/2], Step [15400/16869], Loss: 0.1485\n",
      "Epoch [1/2], Step [15420/16869], Loss: 0.1547\n",
      "Epoch [1/2], Step [15440/16869], Loss: 0.1553\n",
      "Epoch [1/2], Step [15460/16869], Loss: 0.1590\n",
      "Epoch [1/2], Step [15480/16869], Loss: 0.1536\n",
      "Epoch [1/2], Step [15500/16869], Loss: 0.1832\n",
      "Epoch [1/2], Step [15520/16869], Loss: 0.1489\n",
      "Epoch [1/2], Step [15540/16869], Loss: 0.1403\n",
      "Epoch [1/2], Step [15560/16869], Loss: 0.1424\n",
      "Epoch [1/2], Step [15580/16869], Loss: 0.1520\n",
      "Epoch [1/2], Step [15600/16869], Loss: 0.1579\n",
      "Epoch [1/2], Step [15620/16869], Loss: 0.1587\n",
      "Epoch [1/2], Step [15640/16869], Loss: 0.1474\n",
      "Epoch [1/2], Step [15660/16869], Loss: 0.1517\n",
      "Epoch [1/2], Step [15680/16869], Loss: 0.1559\n",
      "Epoch [1/2], Step [15700/16869], Loss: 0.1456\n",
      "Epoch [1/2], Step [15720/16869], Loss: 0.1500\n",
      "Epoch [1/2], Step [15740/16869], Loss: 0.1740\n",
      "Epoch [1/2], Step [15760/16869], Loss: 0.1558\n",
      "Epoch [1/2], Step [15780/16869], Loss: 0.1504\n",
      "Epoch [1/2], Step [15800/16869], Loss: 0.1497\n",
      "Epoch [1/2], Step [15820/16869], Loss: 0.1451\n",
      "Epoch [1/2], Step [15840/16869], Loss: 0.1548\n",
      "Epoch [1/2], Step [15860/16869], Loss: 0.1502\n",
      "Epoch [1/2], Step [15880/16869], Loss: 0.1645\n",
      "Epoch [1/2], Step [15900/16869], Loss: 0.1547\n",
      "Epoch [1/2], Step [15920/16869], Loss: 0.1741\n",
      "Epoch [1/2], Step [15940/16869], Loss: 0.1666\n",
      "Epoch [1/2], Step [15960/16869], Loss: 0.1511\n",
      "Epoch [1/2], Step [15980/16869], Loss: 0.1485\n",
      "Epoch [1/2], Step [16000/16869], Loss: 0.1836\n",
      "Epoch [1/2], Step [16020/16869], Loss: 0.1662\n",
      "Epoch [1/2], Step [16040/16869], Loss: 0.1474\n",
      "Epoch [1/2], Step [16060/16869], Loss: 0.1505\n",
      "Epoch [1/2], Step [16080/16869], Loss: 0.1773\n",
      "Epoch [1/2], Step [16100/16869], Loss: 0.1506\n",
      "Epoch [1/2], Step [16120/16869], Loss: 0.1623\n",
      "Epoch [1/2], Step [16140/16869], Loss: 0.1513\n",
      "Epoch [1/2], Step [16160/16869], Loss: 0.1528\n",
      "Epoch [1/2], Step [16180/16869], Loss: 0.1749\n",
      "Epoch [1/2], Step [16200/16869], Loss: 0.1687\n",
      "Epoch [1/2], Step [16220/16869], Loss: 0.1602\n",
      "Epoch [1/2], Step [16240/16869], Loss: 0.1521\n",
      "Epoch [1/2], Step [16260/16869], Loss: 0.1756\n",
      "Epoch [1/2], Step [16280/16869], Loss: 0.1632\n",
      "Epoch [1/2], Step [16300/16869], Loss: 0.1621\n",
      "Epoch [1/2], Step [16320/16869], Loss: 0.1559\n",
      "Epoch [1/2], Step [16340/16869], Loss: 0.1413\n",
      "Epoch [1/2], Step [16360/16869], Loss: 0.1634\n",
      "Epoch [1/2], Step [16380/16869], Loss: 0.1603\n",
      "Epoch [1/2], Step [16400/16869], Loss: 0.1536\n",
      "Epoch [1/2], Step [16420/16869], Loss: 0.1477\n",
      "Epoch [1/2], Step [16440/16869], Loss: 0.1510\n",
      "Epoch [1/2], Step [16460/16869], Loss: 0.1562\n",
      "Epoch [1/2], Step [16480/16869], Loss: 0.1564\n",
      "Epoch [1/2], Step [16500/16869], Loss: 0.1580\n",
      "Epoch [1/2], Step [16520/16869], Loss: 0.1586\n",
      "Epoch [1/2], Step [16540/16869], Loss: 0.1562\n",
      "Epoch [1/2], Step [16560/16869], Loss: 0.1431\n",
      "Epoch [1/2], Step [16580/16869], Loss: 0.1538\n",
      "Epoch [1/2], Step [16600/16869], Loss: 0.1503\n",
      "Epoch [1/2], Step [16620/16869], Loss: 0.1426\n",
      "Epoch [1/2], Step [16640/16869], Loss: 0.1407\n",
      "Epoch [1/2], Step [16660/16869], Loss: 0.1660\n",
      "Epoch [1/2], Step [16680/16869], Loss: 0.1558\n",
      "Epoch [1/2], Step [16700/16869], Loss: 0.1632\n",
      "Epoch [1/2], Step [16720/16869], Loss: 0.1595\n",
      "Epoch [1/2], Step [16740/16869], Loss: 0.1513\n",
      "Epoch [1/2], Step [16760/16869], Loss: 0.1449\n",
      "Epoch [1/2], Step [16780/16869], Loss: 0.1506\n",
      "Epoch [1/2], Step [16800/16869], Loss: 0.1421\n",
      "Epoch [1/2], Step [16820/16869], Loss: 0.1614\n",
      "Epoch [1/2], Step [16840/16869], Loss: 0.1354\n",
      "Epoch [1/2], Step [16860/16869], Loss: 0.1432\n",
      "Epoch [1/2] Average Loss: 0.2462, Perplexity: 1.28\n",
      "Epoch [2/2], Step [0/16869], Loss: 0.1549\n",
      "Epoch [2/2], Step [20/16869], Loss: 0.1585\n",
      "Epoch [2/2], Step [40/16869], Loss: 0.1547\n",
      "Epoch [2/2], Step [60/16869], Loss: 0.1601\n",
      "Epoch [2/2], Step [80/16869], Loss: 0.1699\n",
      "Epoch [2/2], Step [100/16869], Loss: 0.1663\n",
      "Epoch [2/2], Step [120/16869], Loss: 0.1479\n",
      "Epoch [2/2], Step [140/16869], Loss: 0.1598\n",
      "Epoch [2/2], Step [160/16869], Loss: 0.1559\n",
      "Epoch [2/2], Step [180/16869], Loss: 0.1501\n",
      "Epoch [2/2], Step [200/16869], Loss: 0.1558\n",
      "Epoch [2/2], Step [220/16869], Loss: 0.1764\n",
      "Epoch [2/2], Step [240/16869], Loss: 0.1584\n",
      "Epoch [2/2], Step [260/16869], Loss: 0.1438\n",
      "Epoch [2/2], Step [280/16869], Loss: 0.1423\n",
      "Epoch [2/2], Step [300/16869], Loss: 0.1514\n",
      "Epoch [2/2], Step [320/16869], Loss: 0.1392\n",
      "Epoch [2/2], Step [340/16869], Loss: 0.1587\n",
      "Epoch [2/2], Step [360/16869], Loss: 0.1477\n",
      "Epoch [2/2], Step [380/16869], Loss: 0.1644\n",
      "Epoch [2/2], Step [400/16869], Loss: 0.1529\n",
      "Epoch [2/2], Step [420/16869], Loss: 0.1469\n",
      "Epoch [2/2], Step [440/16869], Loss: 0.1638\n",
      "Epoch [2/2], Step [460/16869], Loss: 0.1611\n",
      "Epoch [2/2], Step [480/16869], Loss: 0.1619\n",
      "Epoch [2/2], Step [500/16869], Loss: 0.1601\n",
      "Epoch [2/2], Step [520/16869], Loss: 0.1423\n",
      "Epoch [2/2], Step [540/16869], Loss: 0.1499\n",
      "Epoch [2/2], Step [560/16869], Loss: 0.1593\n",
      "Epoch [2/2], Step [580/16869], Loss: 0.1400\n",
      "Epoch [2/2], Step [600/16869], Loss: 0.1634\n",
      "Epoch [2/2], Step [620/16869], Loss: 0.1557\n",
      "Epoch [2/2], Step [640/16869], Loss: 0.1510\n",
      "Epoch [2/2], Step [660/16869], Loss: 0.1556\n",
      "Epoch [2/2], Step [680/16869], Loss: 0.1285\n",
      "Epoch [2/2], Step [700/16869], Loss: 0.1460\n",
      "Epoch [2/2], Step [720/16869], Loss: 0.1717\n",
      "Epoch [2/2], Step [740/16869], Loss: 0.1585\n",
      "Epoch [2/2], Step [760/16869], Loss: 0.1487\n",
      "Epoch [2/2], Step [780/16869], Loss: 0.1556\n",
      "Epoch [2/2], Step [800/16869], Loss: 0.1490\n",
      "Epoch [2/2], Step [820/16869], Loss: 0.1582\n",
      "Epoch [2/2], Step [840/16869], Loss: 0.1759\n",
      "Epoch [2/2], Step [860/16869], Loss: 0.1310\n",
      "Epoch [2/2], Step [880/16869], Loss: 0.1511\n",
      "Epoch [2/2], Step [900/16869], Loss: 0.1560\n",
      "Epoch [2/2], Step [920/16869], Loss: 0.1592\n",
      "Epoch [2/2], Step [940/16869], Loss: 0.1522\n",
      "Epoch [2/2], Step [960/16869], Loss: 0.1668\n",
      "Epoch [2/2], Step [980/16869], Loss: 0.1390\n",
      "Epoch [2/2], Step [1000/16869], Loss: 0.1593\n",
      "Epoch [2/2], Step [1020/16869], Loss: 0.1456\n",
      "Epoch [2/2], Step [1040/16869], Loss: 0.1439\n",
      "Epoch [2/2], Step [1060/16869], Loss: 0.1465\n",
      "Epoch [2/2], Step [1080/16869], Loss: 0.1346\n",
      "Epoch [2/2], Step [1100/16869], Loss: 0.1622\n",
      "Epoch [2/2], Step [1120/16869], Loss: 0.1647\n",
      "Epoch [2/2], Step [1140/16869], Loss: 0.1447\n",
      "Epoch [2/2], Step [1160/16869], Loss: 0.1637\n",
      "Epoch [2/2], Step [1180/16869], Loss: 0.1380\n",
      "Epoch [2/2], Step [1200/16869], Loss: 0.1504\n",
      "Epoch [2/2], Step [1220/16869], Loss: 0.1395\n",
      "Epoch [2/2], Step [1240/16869], Loss: 0.1528\n",
      "Epoch [2/2], Step [1260/16869], Loss: 0.1387\n",
      "Epoch [2/2], Step [1280/16869], Loss: 0.1365\n",
      "Epoch [2/2], Step [1300/16869], Loss: 0.1488\n",
      "Epoch [2/2], Step [1320/16869], Loss: 0.1446\n",
      "Epoch [2/2], Step [1340/16869], Loss: 0.1347\n",
      "Epoch [2/2], Step [1360/16869], Loss: 0.1567\n",
      "Epoch [2/2], Step [1380/16869], Loss: 0.1613\n",
      "Epoch [2/2], Step [1400/16869], Loss: 0.1635\n",
      "Epoch [2/2], Step [1420/16869], Loss: 0.1576\n",
      "Epoch [2/2], Step [1440/16869], Loss: 0.1465\n",
      "Epoch [2/2], Step [1460/16869], Loss: 0.1503\n",
      "Epoch [2/2], Step [1480/16869], Loss: 0.1542\n",
      "Epoch [2/2], Step [1500/16869], Loss: 0.1593\n",
      "Epoch [2/2], Step [1520/16869], Loss: 0.1403\n",
      "Epoch [2/2], Step [1540/16869], Loss: 0.1420\n",
      "Epoch [2/2], Step [1560/16869], Loss: 0.1564\n",
      "Epoch [2/2], Step [1580/16869], Loss: 0.1610\n",
      "Epoch [2/2], Step [1600/16869], Loss: 0.1364\n",
      "Epoch [2/2], Step [1620/16869], Loss: 0.1567\n",
      "Epoch [2/2], Step [1640/16869], Loss: 0.1468\n",
      "Epoch [2/2], Step [1660/16869], Loss: 0.1633\n",
      "Epoch [2/2], Step [1680/16869], Loss: 0.1538\n",
      "Epoch [2/2], Step [1700/16869], Loss: 0.1356\n",
      "Epoch [2/2], Step [1720/16869], Loss: 0.1627\n",
      "Epoch [2/2], Step [1740/16869], Loss: 0.1550\n",
      "Epoch [2/2], Step [1760/16869], Loss: 0.1567\n",
      "Epoch [2/2], Step [1780/16869], Loss: 0.1598\n",
      "Epoch [2/2], Step [1800/16869], Loss: 0.1372\n",
      "Epoch [2/2], Step [1820/16869], Loss: 0.1377\n",
      "Epoch [2/2], Step [1840/16869], Loss: 0.1482\n",
      "Epoch [2/2], Step [1860/16869], Loss: 0.1534\n",
      "Epoch [2/2], Step [1880/16869], Loss: 0.1420\n",
      "Epoch [2/2], Step [1900/16869], Loss: 0.1623\n",
      "Epoch [2/2], Step [1920/16869], Loss: 0.1645\n",
      "Epoch [2/2], Step [1940/16869], Loss: 0.1583\n",
      "Epoch [2/2], Step [1960/16869], Loss: 0.1624\n",
      "Epoch [2/2], Step [1980/16869], Loss: 0.1414\n",
      "Epoch [2/2], Step [2000/16869], Loss: 0.1510\n",
      "Epoch [2/2], Step [2020/16869], Loss: 0.1468\n",
      "Epoch [2/2], Step [2040/16869], Loss: 0.1424\n",
      "Epoch [2/2], Step [2060/16869], Loss: 0.1475\n",
      "Epoch [2/2], Step [2080/16869], Loss: 0.1543\n",
      "Epoch [2/2], Step [2100/16869], Loss: 0.1564\n",
      "Epoch [2/2], Step [2120/16869], Loss: 0.1504\n",
      "Epoch [2/2], Step [2140/16869], Loss: 0.1665\n",
      "Epoch [2/2], Step [2160/16869], Loss: 0.1386\n",
      "Epoch [2/2], Step [2180/16869], Loss: 0.1537\n",
      "Epoch [2/2], Step [2200/16869], Loss: 0.1495\n",
      "Epoch [2/2], Step [2220/16869], Loss: 0.1415\n",
      "Epoch [2/2], Step [2240/16869], Loss: 0.1447\n",
      "Epoch [2/2], Step [2260/16869], Loss: 0.1480\n",
      "Epoch [2/2], Step [2280/16869], Loss: 0.1562\n",
      "Epoch [2/2], Step [2300/16869], Loss: 0.1515\n",
      "Epoch [2/2], Step [2320/16869], Loss: 0.1396\n",
      "Epoch [2/2], Step [2340/16869], Loss: 0.1655\n",
      "Epoch [2/2], Step [2360/16869], Loss: 0.1549\n",
      "Epoch [2/2], Step [2380/16869], Loss: 0.1543\n",
      "Epoch [2/2], Step [2400/16869], Loss: 0.1438\n",
      "Epoch [2/2], Step [2420/16869], Loss: 0.1476\n",
      "Epoch [2/2], Step [2440/16869], Loss: 0.1730\n",
      "Epoch [2/2], Step [2460/16869], Loss: 0.1444\n",
      "Epoch [2/2], Step [2480/16869], Loss: 0.1588\n",
      "Epoch [2/2], Step [2500/16869], Loss: 0.1467\n",
      "Epoch [2/2], Step [2520/16869], Loss: 0.1507\n",
      "Epoch [2/2], Step [2540/16869], Loss: 0.1510\n",
      "Epoch [2/2], Step [2560/16869], Loss: 0.1633\n",
      "Epoch [2/2], Step [2580/16869], Loss: 0.1665\n",
      "Epoch [2/2], Step [2600/16869], Loss: 0.1482\n",
      "Epoch [2/2], Step [2620/16869], Loss: 0.1615\n",
      "Epoch [2/2], Step [2640/16869], Loss: 0.1704\n",
      "Epoch [2/2], Step [2660/16869], Loss: 0.1579\n",
      "Epoch [2/2], Step [2680/16869], Loss: 0.1356\n",
      "Epoch [2/2], Step [2700/16869], Loss: 0.1503\n",
      "Epoch [2/2], Step [2720/16869], Loss: 0.1618\n",
      "Epoch [2/2], Step [2740/16869], Loss: 0.1710\n",
      "Epoch [2/2], Step [2760/16869], Loss: 0.1491\n",
      "Epoch [2/2], Step [2780/16869], Loss: 0.1642\n",
      "Epoch [2/2], Step [2800/16869], Loss: 0.1492\n",
      "Epoch [2/2], Step [2820/16869], Loss: 0.1469\n",
      "Epoch [2/2], Step [2840/16869], Loss: 0.1444\n",
      "Epoch [2/2], Step [2860/16869], Loss: 0.1593\n",
      "Epoch [2/2], Step [2880/16869], Loss: 0.1489\n",
      "Epoch [2/2], Step [2900/16869], Loss: 0.1555\n",
      "Epoch [2/2], Step [2920/16869], Loss: 0.1523\n",
      "Epoch [2/2], Step [2940/16869], Loss: 0.1546\n",
      "Epoch [2/2], Step [2960/16869], Loss: 0.1452\n",
      "Epoch [2/2], Step [2980/16869], Loss: 0.1552\n",
      "Epoch [2/2], Step [3000/16869], Loss: 0.1564\n",
      "Epoch [2/2], Step [3020/16869], Loss: 0.1561\n",
      "Epoch [2/2], Step [3040/16869], Loss: 0.1432\n",
      "Epoch [2/2], Step [3060/16869], Loss: 0.1525\n",
      "Epoch [2/2], Step [3080/16869], Loss: 0.1389\n",
      "Epoch [2/2], Step [3100/16869], Loss: 0.1453\n",
      "Epoch [2/2], Step [3120/16869], Loss: 0.1326\n",
      "Epoch [2/2], Step [3140/16869], Loss: 0.1514\n",
      "Epoch [2/2], Step [3160/16869], Loss: 0.1663\n",
      "Epoch [2/2], Step [3180/16869], Loss: 0.1580\n",
      "Epoch [2/2], Step [3200/16869], Loss: 0.1673\n",
      "Epoch [2/2], Step [3220/16869], Loss: 0.1375\n",
      "Epoch [2/2], Step [3240/16869], Loss: 0.1575\n",
      "Epoch [2/2], Step [3260/16869], Loss: 0.1548\n",
      "Epoch [2/2], Step [3280/16869], Loss: 0.1604\n",
      "Epoch [2/2], Step [3300/16869], Loss: 0.1465\n",
      "Epoch [2/2], Step [3320/16869], Loss: 0.1678\n",
      "Epoch [2/2], Step [3340/16869], Loss: 0.1495\n",
      "Epoch [2/2], Step [3360/16869], Loss: 0.1653\n",
      "Epoch [2/2], Step [3380/16869], Loss: 0.1589\n",
      "Epoch [2/2], Step [3400/16869], Loss: 0.1533\n",
      "Epoch [2/2], Step [3420/16869], Loss: 0.1580\n",
      "Epoch [2/2], Step [3440/16869], Loss: 0.1450\n",
      "Epoch [2/2], Step [3460/16869], Loss: 0.1402\n",
      "Epoch [2/2], Step [3480/16869], Loss: 0.1534\n",
      "Epoch [2/2], Step [3500/16869], Loss: 0.1563\n",
      "Epoch [2/2], Step [3520/16869], Loss: 0.1434\n",
      "Epoch [2/2], Step [3540/16869], Loss: 0.1545\n",
      "Epoch [2/2], Step [3560/16869], Loss: 0.1527\n",
      "Epoch [2/2], Step [3580/16869], Loss: 0.1526\n",
      "Epoch [2/2], Step [3600/16869], Loss: 0.1441\n",
      "Epoch [2/2], Step [3620/16869], Loss: 0.1511\n",
      "Epoch [2/2], Step [3640/16869], Loss: 0.1507\n",
      "Epoch [2/2], Step [3660/16869], Loss: 0.1493\n",
      "Epoch [2/2], Step [3680/16869], Loss: 0.1474\n",
      "Epoch [2/2], Step [3700/16869], Loss: 0.1648\n",
      "Epoch [2/2], Step [3720/16869], Loss: 0.1400\n",
      "Epoch [2/2], Step [3740/16869], Loss: 0.1402\n",
      "Epoch [2/2], Step [3760/16869], Loss: 0.1559\n",
      "Epoch [2/2], Step [3780/16869], Loss: 0.1619\n",
      "Epoch [2/2], Step [3800/16869], Loss: 0.1600\n",
      "Epoch [2/2], Step [3820/16869], Loss: 0.1538\n",
      "Epoch [2/2], Step [3840/16869], Loss: 0.1435\n",
      "Epoch [2/2], Step [3860/16869], Loss: 0.1471\n",
      "Epoch [2/2], Step [3880/16869], Loss: 0.1613\n",
      "Epoch [2/2], Step [3900/16869], Loss: 0.1520\n",
      "Epoch [2/2], Step [3920/16869], Loss: 0.1481\n",
      "Epoch [2/2], Step [3940/16869], Loss: 0.1453\n",
      "Epoch [2/2], Step [3960/16869], Loss: 0.1451\n",
      "Epoch [2/2], Step [3980/16869], Loss: 0.1444\n",
      "Epoch [2/2], Step [4000/16869], Loss: 0.1570\n",
      "Epoch [2/2], Step [4020/16869], Loss: 0.1565\n",
      "Epoch [2/2], Step [4040/16869], Loss: 0.1488\n",
      "Epoch [2/2], Step [4060/16869], Loss: 0.1427\n",
      "Epoch [2/2], Step [4080/16869], Loss: 0.1480\n",
      "Epoch [2/2], Step [4100/16869], Loss: 0.1626\n",
      "Epoch [2/2], Step [4120/16869], Loss: 0.1571\n",
      "Epoch [2/2], Step [4140/16869], Loss: 0.1726\n",
      "Epoch [2/2], Step [4160/16869], Loss: 0.1509\n",
      "Epoch [2/2], Step [4180/16869], Loss: 0.1383\n",
      "Epoch [2/2], Step [4200/16869], Loss: 0.1557\n",
      "Epoch [2/2], Step [4220/16869], Loss: 0.1610\n",
      "Epoch [2/2], Step [4240/16869], Loss: 0.1472\n",
      "Epoch [2/2], Step [4260/16869], Loss: 0.1430\n",
      "Epoch [2/2], Step [4280/16869], Loss: 0.1492\n",
      "Epoch [2/2], Step [4300/16869], Loss: 0.1528\n",
      "Epoch [2/2], Step [4320/16869], Loss: 0.1672\n",
      "Epoch [2/2], Step [4340/16869], Loss: 0.1380\n",
      "Epoch [2/2], Step [4360/16869], Loss: 0.1472\n",
      "Epoch [2/2], Step [4380/16869], Loss: 0.1587\n",
      "Epoch [2/2], Step [4400/16869], Loss: 0.1765\n",
      "Epoch [2/2], Step [4420/16869], Loss: 0.1350\n",
      "Epoch [2/2], Step [4440/16869], Loss: 0.1449\n",
      "Epoch [2/2], Step [4460/16869], Loss: 0.1551\n",
      "Epoch [2/2], Step [4480/16869], Loss: 0.1413\n",
      "Epoch [2/2], Step [4500/16869], Loss: 0.1547\n",
      "Epoch [2/2], Step [4520/16869], Loss: 0.1597\n",
      "Epoch [2/2], Step [4540/16869], Loss: 0.1651\n",
      "Epoch [2/2], Step [4560/16869], Loss: 0.1511\n",
      "Epoch [2/2], Step [4580/16869], Loss: 0.1484\n",
      "Epoch [2/2], Step [4600/16869], Loss: 0.1627\n",
      "Epoch [2/2], Step [4620/16869], Loss: 0.1523\n",
      "Epoch [2/2], Step [4640/16869], Loss: 0.1744\n",
      "Epoch [2/2], Step [4660/16869], Loss: 0.1557\n",
      "Epoch [2/2], Step [4680/16869], Loss: 0.1590\n",
      "Epoch [2/2], Step [4700/16869], Loss: 0.1465\n",
      "Epoch [2/2], Step [4720/16869], Loss: 0.1637\n",
      "Epoch [2/2], Step [4740/16869], Loss: 0.1456\n",
      "Epoch [2/2], Step [4760/16869], Loss: 0.1714\n",
      "Epoch [2/2], Step [4780/16869], Loss: 0.1550\n",
      "Epoch [2/2], Step [4800/16869], Loss: 0.1657\n",
      "Epoch [2/2], Step [4820/16869], Loss: 0.1513\n",
      "Epoch [2/2], Step [4840/16869], Loss: 0.1388\n",
      "Epoch [2/2], Step [4860/16869], Loss: 0.1477\n",
      "Epoch [2/2], Step [4880/16869], Loss: 0.1429\n",
      "Epoch [2/2], Step [4900/16869], Loss: 0.1414\n",
      "Epoch [2/2], Step [4920/16869], Loss: 0.1553\n",
      "Epoch [2/2], Step [4940/16869], Loss: 0.1386\n",
      "Epoch [2/2], Step [4960/16869], Loss: 0.1706\n",
      "Epoch [2/2], Step [4980/16869], Loss: 0.1533\n",
      "Epoch [2/2], Step [5000/16869], Loss: 0.1407\n",
      "Epoch [2/2], Step [5020/16869], Loss: 0.1512\n",
      "Epoch [2/2], Step [5040/16869], Loss: 0.1457\n",
      "Epoch [2/2], Step [5060/16869], Loss: 0.1505\n",
      "Epoch [2/2], Step [5080/16869], Loss: 0.1604\n",
      "Epoch [2/2], Step [5100/16869], Loss: 0.1524\n",
      "Epoch [2/2], Step [5120/16869], Loss: 0.1615\n",
      "Epoch [2/2], Step [5140/16869], Loss: 0.1458\n",
      "Epoch [2/2], Step [5160/16869], Loss: 0.1770\n",
      "Epoch [2/2], Step [5180/16869], Loss: 0.1477\n",
      "Epoch [2/2], Step [5200/16869], Loss: 0.1377\n",
      "Epoch [2/2], Step [5220/16869], Loss: 0.1456\n",
      "Epoch [2/2], Step [5240/16869], Loss: 0.1490\n",
      "Epoch [2/2], Step [5260/16869], Loss: 0.1461\n",
      "Epoch [2/2], Step [5280/16869], Loss: 0.1389\n",
      "Epoch [2/2], Step [5300/16869], Loss: 0.1520\n",
      "Epoch [2/2], Step [5320/16869], Loss: 0.1662\n",
      "Epoch [2/2], Step [5340/16869], Loss: 0.1547\n",
      "Epoch [2/2], Step [5360/16869], Loss: 0.1505\n",
      "Epoch [2/2], Step [5380/16869], Loss: 0.1367\n",
      "Epoch [2/2], Step [5400/16869], Loss: 0.1536\n",
      "Epoch [2/2], Step [5420/16869], Loss: 0.1462\n",
      "Epoch [2/2], Step [5440/16869], Loss: 0.1512\n",
      "Epoch [2/2], Step [5460/16869], Loss: 0.1447\n",
      "Epoch [2/2], Step [5480/16869], Loss: 0.1386\n",
      "Epoch [2/2], Step [5500/16869], Loss: 0.1465\n",
      "Epoch [2/2], Step [5520/16869], Loss: 0.1448\n",
      "Epoch [2/2], Step [5540/16869], Loss: 0.1366\n",
      "Epoch [2/2], Step [5560/16869], Loss: 0.1443\n",
      "Epoch [2/2], Step [5580/16869], Loss: 0.1432\n",
      "Epoch [2/2], Step [5600/16869], Loss: 0.1388\n",
      "Epoch [2/2], Step [5620/16869], Loss: 0.1623\n",
      "Epoch [2/2], Step [5640/16869], Loss: 0.1571\n",
      "Epoch [2/2], Step [5660/16869], Loss: 0.1333\n",
      "Epoch [2/2], Step [5680/16869], Loss: 0.1533\n",
      "Epoch [2/2], Step [5700/16869], Loss: 0.1533\n",
      "Epoch [2/2], Step [5720/16869], Loss: 0.1433\n",
      "Epoch [2/2], Step [5740/16869], Loss: 0.1459\n",
      "Epoch [2/2], Step [5760/16869], Loss: 0.1461\n",
      "Epoch [2/2], Step [5780/16869], Loss: 0.1621\n",
      "Epoch [2/2], Step [5800/16869], Loss: 0.1526\n",
      "Epoch [2/2], Step [5820/16869], Loss: 0.1388\n",
      "Epoch [2/2], Step [5840/16869], Loss: 0.1597\n",
      "Epoch [2/2], Step [5860/16869], Loss: 0.1499\n",
      "Epoch [2/2], Step [5880/16869], Loss: 0.1417\n",
      "Epoch [2/2], Step [5900/16869], Loss: 0.1461\n",
      "Epoch [2/2], Step [5920/16869], Loss: 0.1505\n",
      "Epoch [2/2], Step [5940/16869], Loss: 0.1603\n",
      "Epoch [2/2], Step [5960/16869], Loss: 0.1689\n",
      "Epoch [2/2], Step [5980/16869], Loss: 0.1626\n",
      "Epoch [2/2], Step [6000/16869], Loss: 0.1531\n",
      "Epoch [2/2], Step [6020/16869], Loss: 0.1491\n",
      "Epoch [2/2], Step [6040/16869], Loss: 0.1599\n",
      "Epoch [2/2], Step [6060/16869], Loss: 0.1570\n",
      "Epoch [2/2], Step [6080/16869], Loss: 0.1361\n",
      "Epoch [2/2], Step [6100/16869], Loss: 0.1525\n",
      "Epoch [2/2], Step [6120/16869], Loss: 0.1528\n",
      "Epoch [2/2], Step [6140/16869], Loss: 0.1506\n",
      "Epoch [2/2], Step [6160/16869], Loss: 0.1552\n",
      "Epoch [2/2], Step [6180/16869], Loss: 0.1530\n",
      "Epoch [2/2], Step [6200/16869], Loss: 0.1386\n",
      "Epoch [2/2], Step [6220/16869], Loss: 0.1403\n",
      "Epoch [2/2], Step [6240/16869], Loss: 0.1390\n",
      "Epoch [2/2], Step [6260/16869], Loss: 0.1655\n",
      "Epoch [2/2], Step [6280/16869], Loss: 0.1398\n",
      "Epoch [2/2], Step [6300/16869], Loss: 0.1623\n",
      "Epoch [2/2], Step [6320/16869], Loss: 0.1360\n",
      "Epoch [2/2], Step [6340/16869], Loss: 0.1345\n",
      "Epoch [2/2], Step [6360/16869], Loss: 0.1514\n",
      "Epoch [2/2], Step [6380/16869], Loss: 0.1581\n",
      "Epoch [2/2], Step [6400/16869], Loss: 0.1469\n",
      "Epoch [2/2], Step [6420/16869], Loss: 0.1511\n",
      "Epoch [2/2], Step [6440/16869], Loss: 0.1555\n",
      "Epoch [2/2], Step [6460/16869], Loss: 0.1479\n",
      "Epoch [2/2], Step [6480/16869], Loss: 0.1482\n",
      "Epoch [2/2], Step [6500/16869], Loss: 0.1419\n",
      "Epoch [2/2], Step [6520/16869], Loss: 0.1470\n",
      "Epoch [2/2], Step [6540/16869], Loss: 0.1453\n",
      "Epoch [2/2], Step [6560/16869], Loss: 0.1469\n",
      "Epoch [2/2], Step [6580/16869], Loss: 0.1638\n",
      "Epoch [2/2], Step [6600/16869], Loss: 0.1392\n",
      "Epoch [2/2], Step [6620/16869], Loss: 0.1541\n",
      "Epoch [2/2], Step [6640/16869], Loss: 0.1570\n",
      "Epoch [2/2], Step [6660/16869], Loss: 0.1396\n",
      "Epoch [2/2], Step [6680/16869], Loss: 0.1427\n",
      "Epoch [2/2], Step [6700/16869], Loss: 0.1600\n",
      "Epoch [2/2], Step [6720/16869], Loss: 0.1496\n",
      "Epoch [2/2], Step [6740/16869], Loss: 0.1413\n",
      "Epoch [2/2], Step [6760/16869], Loss: 0.1480\n",
      "Epoch [2/2], Step [6780/16869], Loss: 0.1563\n",
      "Epoch [2/2], Step [6800/16869], Loss: 0.1580\n",
      "Epoch [2/2], Step [6820/16869], Loss: 0.1391\n",
      "Epoch [2/2], Step [6840/16869], Loss: 0.1503\n",
      "Epoch [2/2], Step [6860/16869], Loss: 0.1649\n",
      "Epoch [2/2], Step [6880/16869], Loss: 0.1525\n",
      "Epoch [2/2], Step [6900/16869], Loss: 0.1587\n",
      "Epoch [2/2], Step [6920/16869], Loss: 0.1593\n",
      "Epoch [2/2], Step [6940/16869], Loss: 0.1424\n",
      "Epoch [2/2], Step [6960/16869], Loss: 0.1732\n",
      "Epoch [2/2], Step [6980/16869], Loss: 0.1483\n",
      "Epoch [2/2], Step [7000/16869], Loss: 0.1516\n",
      "Epoch [2/2], Step [7020/16869], Loss: 0.1572\n",
      "Epoch [2/2], Step [7040/16869], Loss: 0.1527\n",
      "Epoch [2/2], Step [7060/16869], Loss: 0.1709\n",
      "Epoch [2/2], Step [7080/16869], Loss: 0.1443\n",
      "Epoch [2/2], Step [7100/16869], Loss: 0.1517\n",
      "Epoch [2/2], Step [7120/16869], Loss: 0.1570\n",
      "Epoch [2/2], Step [7140/16869], Loss: 0.1563\n",
      "Epoch [2/2], Step [7160/16869], Loss: 0.1642\n",
      "Epoch [2/2], Step [7180/16869], Loss: 0.1501\n",
      "Epoch [2/2], Step [7200/16869], Loss: 0.1626\n",
      "Epoch [2/2], Step [7220/16869], Loss: 0.1420\n",
      "Epoch [2/2], Step [7240/16869], Loss: 0.1509\n",
      "Epoch [2/2], Step [7260/16869], Loss: 0.1457\n",
      "Epoch [2/2], Step [7280/16869], Loss: 0.1463\n",
      "Epoch [2/2], Step [7300/16869], Loss: 0.1426\n",
      "Epoch [2/2], Step [7320/16869], Loss: 0.1445\n",
      "Epoch [2/2], Step [7340/16869], Loss: 0.1386\n",
      "Epoch [2/2], Step [7360/16869], Loss: 0.1494\n",
      "Epoch [2/2], Step [7380/16869], Loss: 0.1636\n",
      "Epoch [2/2], Step [7400/16869], Loss: 0.1624\n",
      "Epoch [2/2], Step [7420/16869], Loss: 0.1614\n",
      "Epoch [2/2], Step [7440/16869], Loss: 0.1337\n",
      "Epoch [2/2], Step [7460/16869], Loss: 0.1468\n",
      "Epoch [2/2], Step [7480/16869], Loss: 0.1476\n",
      "Epoch [2/2], Step [7500/16869], Loss: 0.1538\n",
      "Epoch [2/2], Step [7520/16869], Loss: 0.1337\n",
      "Epoch [2/2], Step [7540/16869], Loss: 0.1465\n",
      "Epoch [2/2], Step [7560/16869], Loss: 0.1536\n",
      "Epoch [2/2], Step [7580/16869], Loss: 0.1410\n",
      "Epoch [2/2], Step [7600/16869], Loss: 0.1496\n",
      "Epoch [2/2], Step [7620/16869], Loss: 0.1464\n",
      "Epoch [2/2], Step [7640/16869], Loss: 0.1502\n",
      "Epoch [2/2], Step [7660/16869], Loss: 0.1615\n",
      "Epoch [2/2], Step [7680/16869], Loss: 0.1454\n",
      "Epoch [2/2], Step [7700/16869], Loss: 0.1440\n",
      "Epoch [2/2], Step [7720/16869], Loss: 0.1471\n",
      "Epoch [2/2], Step [7740/16869], Loss: 0.1347\n",
      "Epoch [2/2], Step [7760/16869], Loss: 0.1502\n",
      "Epoch [2/2], Step [7780/16869], Loss: 0.1573\n",
      "Epoch [2/2], Step [7800/16869], Loss: 0.1480\n",
      "Epoch [2/2], Step [7820/16869], Loss: 0.1464\n",
      "Epoch [2/2], Step [7840/16869], Loss: 0.1548\n",
      "Epoch [2/2], Step [7860/16869], Loss: 0.1494\n",
      "Epoch [2/2], Step [7880/16869], Loss: 0.1499\n",
      "Epoch [2/2], Step [7900/16869], Loss: 0.1410\n",
      "Epoch [2/2], Step [7920/16869], Loss: 0.1317\n",
      "Epoch [2/2], Step [7940/16869], Loss: 0.1524\n",
      "Epoch [2/2], Step [7960/16869], Loss: 0.1532\n",
      "Epoch [2/2], Step [7980/16869], Loss: 0.1544\n",
      "Epoch [2/2], Step [8000/16869], Loss: 0.1599\n",
      "Epoch [2/2], Step [8020/16869], Loss: 0.1425\n",
      "Epoch [2/2], Step [8040/16869], Loss: 0.1399\n",
      "Epoch [2/2], Step [8060/16869], Loss: 0.1439\n",
      "Epoch [2/2], Step [8080/16869], Loss: 0.1504\n",
      "Epoch [2/2], Step [8100/16869], Loss: 0.1343\n",
      "Epoch [2/2], Step [8120/16869], Loss: 0.1477\n",
      "Epoch [2/2], Step [8140/16869], Loss: 0.1501\n",
      "Epoch [2/2], Step [8160/16869], Loss: 0.1491\n",
      "Epoch [2/2], Step [8180/16869], Loss: 0.1535\n",
      "Epoch [2/2], Step [8200/16869], Loss: 0.1429\n",
      "Epoch [2/2], Step [8220/16869], Loss: 0.1382\n",
      "Epoch [2/2], Step [8240/16869], Loss: 0.1554\n",
      "Epoch [2/2], Step [8260/16869], Loss: 0.1603\n",
      "Epoch [2/2], Step [8280/16869], Loss: 0.1684\n",
      "Epoch [2/2], Step [8300/16869], Loss: 0.1596\n",
      "Epoch [2/2], Step [8320/16869], Loss: 0.1535\n",
      "Epoch [2/2], Step [8340/16869], Loss: 0.1437\n",
      "Epoch [2/2], Step [8360/16869], Loss: 0.1446\n",
      "Epoch [2/2], Step [8380/16869], Loss: 0.1404\n",
      "Epoch [2/2], Step [8400/16869], Loss: 0.1458\n",
      "Epoch [2/2], Step [8420/16869], Loss: 0.1479\n",
      "Epoch [2/2], Step [8440/16869], Loss: 0.1591\n",
      "Epoch [2/2], Step [8460/16869], Loss: 0.1596\n",
      "Epoch [2/2], Step [8480/16869], Loss: 0.1366\n",
      "Epoch [2/2], Step [8500/16869], Loss: 0.1602\n",
      "Epoch [2/2], Step [8520/16869], Loss: 0.1284\n",
      "Epoch [2/2], Step [8540/16869], Loss: 0.1493\n",
      "Epoch [2/2], Step [8560/16869], Loss: 0.1422\n",
      "Epoch [2/2], Step [8580/16869], Loss: 0.1533\n",
      "Epoch [2/2], Step [8600/16869], Loss: 0.1524\n",
      "Epoch [2/2], Step [8620/16869], Loss: 0.1603\n",
      "Epoch [2/2], Step [8640/16869], Loss: 0.1371\n",
      "Epoch [2/2], Step [8660/16869], Loss: 0.1567\n",
      "Epoch [2/2], Step [8680/16869], Loss: 0.1399\n",
      "Epoch [2/2], Step [8700/16869], Loss: 0.1341\n",
      "Epoch [2/2], Step [8720/16869], Loss: 0.1482\n",
      "Epoch [2/2], Step [8740/16869], Loss: 0.1450\n",
      "Epoch [2/2], Step [8760/16869], Loss: 0.1548\n",
      "Epoch [2/2], Step [8780/16869], Loss: 0.1427\n",
      "Epoch [2/2], Step [8800/16869], Loss: 0.1600\n",
      "Epoch [2/2], Step [8820/16869], Loss: 0.1499\n",
      "Epoch [2/2], Step [8840/16869], Loss: 0.1451\n",
      "Epoch [2/2], Step [8860/16869], Loss: 0.1435\n",
      "Epoch [2/2], Step [8880/16869], Loss: 0.1361\n",
      "Epoch [2/2], Step [8900/16869], Loss: 0.1416\n",
      "Epoch [2/2], Step [8920/16869], Loss: 0.1585\n",
      "Epoch [2/2], Step [8940/16869], Loss: 0.1394\n",
      "Epoch [2/2], Step [8960/16869], Loss: 0.1474\n",
      "Epoch [2/2], Step [8980/16869], Loss: 0.1371\n",
      "Epoch [2/2], Step [9000/16869], Loss: 0.1413\n",
      "Epoch [2/2], Step [9020/16869], Loss: 0.1634\n",
      "Epoch [2/2], Step [9040/16869], Loss: 0.1410\n",
      "Epoch [2/2], Step [9060/16869], Loss: 0.1537\n",
      "Epoch [2/2], Step [9080/16869], Loss: 0.1419\n",
      "Epoch [2/2], Step [9100/16869], Loss: 0.1482\n",
      "Epoch [2/2], Step [9120/16869], Loss: 0.1507\n",
      "Epoch [2/2], Step [9140/16869], Loss: 0.1642\n",
      "Epoch [2/2], Step [9160/16869], Loss: 0.1454\n",
      "Epoch [2/2], Step [9180/16869], Loss: 0.1567\n",
      "Epoch [2/2], Step [9200/16869], Loss: 0.1608\n",
      "Epoch [2/2], Step [9220/16869], Loss: 0.1620\n",
      "Epoch [2/2], Step [9240/16869], Loss: 0.1674\n",
      "Epoch [2/2], Step [9260/16869], Loss: 0.1647\n",
      "Epoch [2/2], Step [9280/16869], Loss: 0.1520\n",
      "Epoch [2/2], Step [9300/16869], Loss: 0.1546\n",
      "Epoch [2/2], Step [9320/16869], Loss: 0.1579\n",
      "Epoch [2/2], Step [9340/16869], Loss: 0.1709\n",
      "Epoch [2/2], Step [9360/16869], Loss: 0.1480\n",
      "Epoch [2/2], Step [9380/16869], Loss: 0.1378\n",
      "Epoch [2/2], Step [9400/16869], Loss: 0.1566\n",
      "Epoch [2/2], Step [9420/16869], Loss: 0.1485\n",
      "Epoch [2/2], Step [9440/16869], Loss: 0.1605\n",
      "Epoch [2/2], Step [9460/16869], Loss: 0.1603\n",
      "Epoch [2/2], Step [9480/16869], Loss: 0.1481\n",
      "Epoch [2/2], Step [9500/16869], Loss: 0.1591\n",
      "Epoch [2/2], Step [9520/16869], Loss: 0.1514\n",
      "Epoch [2/2], Step [9540/16869], Loss: 0.1353\n",
      "Epoch [2/2], Step [9560/16869], Loss: 0.1531\n",
      "Epoch [2/2], Step [9580/16869], Loss: 0.1524\n",
      "Epoch [2/2], Step [9600/16869], Loss: 0.1437\n",
      "Epoch [2/2], Step [9620/16869], Loss: 0.1588\n",
      "Epoch [2/2], Step [9640/16869], Loss: 0.1623\n",
      "Epoch [2/2], Step [9660/16869], Loss: 0.1668\n",
      "Epoch [2/2], Step [9680/16869], Loss: 0.1551\n",
      "Epoch [2/2], Step [9700/16869], Loss: 0.1453\n",
      "Epoch [2/2], Step [9720/16869], Loss: 0.1468\n",
      "Epoch [2/2], Step [9740/16869], Loss: 0.1475\n",
      "Epoch [2/2], Step [9760/16869], Loss: 0.1496\n",
      "Epoch [2/2], Step [9780/16869], Loss: 0.1450\n",
      "Epoch [2/2], Step [9800/16869], Loss: 0.1469\n",
      "Epoch [2/2], Step [9820/16869], Loss: 0.1503\n",
      "Epoch [2/2], Step [9840/16869], Loss: 0.1481\n",
      "Epoch [2/2], Step [9860/16869], Loss: 0.1543\n",
      "Epoch [2/2], Step [9880/16869], Loss: 0.1533\n",
      "Epoch [2/2], Step [9900/16869], Loss: 0.1362\n",
      "Epoch [2/2], Step [9920/16869], Loss: 0.1506\n",
      "Epoch [2/2], Step [9940/16869], Loss: 0.1595\n",
      "Epoch [2/2], Step [9960/16869], Loss: 0.1333\n",
      "Epoch [2/2], Step [9980/16869], Loss: 0.1703\n",
      "Epoch [2/2], Step [10000/16869], Loss: 0.1350\n",
      "Epoch [2/2], Step [10020/16869], Loss: 0.1308\n",
      "Epoch [2/2], Step [10040/16869], Loss: 0.1599\n",
      "Epoch [2/2], Step [10060/16869], Loss: 0.1507\n",
      "Epoch [2/2], Step [10080/16869], Loss: 0.1529\n",
      "Epoch [2/2], Step [10100/16869], Loss: 0.1648\n",
      "Epoch [2/2], Step [10120/16869], Loss: 0.1421\n",
      "Epoch [2/2], Step [10140/16869], Loss: 0.1476\n",
      "Epoch [2/2], Step [10160/16869], Loss: 0.1589\n",
      "Epoch [2/2], Step [10180/16869], Loss: 0.1392\n",
      "Epoch [2/2], Step [10200/16869], Loss: 0.1709\n",
      "Epoch [2/2], Step [10220/16869], Loss: 0.1515\n",
      "Epoch [2/2], Step [10240/16869], Loss: 0.1484\n",
      "Epoch [2/2], Step [10260/16869], Loss: 0.1321\n",
      "Epoch [2/2], Step [10280/16869], Loss: 0.1400\n",
      "Epoch [2/2], Step [10300/16869], Loss: 0.1479\n",
      "Epoch [2/2], Step [10320/16869], Loss: 0.1706\n",
      "Epoch [2/2], Step [10340/16869], Loss: 0.1488\n",
      "Epoch [2/2], Step [10360/16869], Loss: 0.1624\n",
      "Epoch [2/2], Step [10380/16869], Loss: 0.1511\n",
      "Epoch [2/2], Step [10400/16869], Loss: 0.1374\n",
      "Epoch [2/2], Step [10420/16869], Loss: 0.1540\n",
      "Epoch [2/2], Step [10440/16869], Loss: 0.1460\n",
      "Epoch [2/2], Step [10460/16869], Loss: 0.1520\n",
      "Epoch [2/2], Step [10480/16869], Loss: 0.1574\n",
      "Epoch [2/2], Step [10500/16869], Loss: 0.1455\n",
      "Epoch [2/2], Step [10520/16869], Loss: 0.1575\n",
      "Epoch [2/2], Step [10540/16869], Loss: 0.1543\n",
      "Epoch [2/2], Step [10560/16869], Loss: 0.1354\n",
      "Epoch [2/2], Step [10580/16869], Loss: 0.1523\n",
      "Epoch [2/2], Step [10600/16869], Loss: 0.1482\n",
      "Epoch [2/2], Step [10620/16869], Loss: 0.1501\n",
      "Epoch [2/2], Step [10640/16869], Loss: 0.1478\n",
      "Epoch [2/2], Step [10660/16869], Loss: 0.1511\n",
      "Epoch [2/2], Step [10680/16869], Loss: 0.1481\n",
      "Epoch [2/2], Step [10700/16869], Loss: 0.1215\n",
      "Epoch [2/2], Step [10720/16869], Loss: 0.1638\n",
      "Epoch [2/2], Step [10740/16869], Loss: 0.1576\n",
      "Epoch [2/2], Step [10760/16869], Loss: 0.1653\n",
      "Epoch [2/2], Step [10780/16869], Loss: 0.1382\n",
      "Epoch [2/2], Step [10800/16869], Loss: 0.1459\n",
      "Epoch [2/2], Step [10820/16869], Loss: 0.1438\n",
      "Epoch [2/2], Step [10840/16869], Loss: 0.1511\n",
      "Epoch [2/2], Step [10860/16869], Loss: 0.1452\n",
      "Epoch [2/2], Step [10880/16869], Loss: 0.1342\n",
      "Epoch [2/2], Step [10900/16869], Loss: 0.1440\n",
      "Epoch [2/2], Step [10920/16869], Loss: 0.1581\n",
      "Epoch [2/2], Step [10940/16869], Loss: 0.1330\n",
      "Epoch [2/2], Step [10960/16869], Loss: 0.1498\n",
      "Epoch [2/2], Step [10980/16869], Loss: 0.1393\n",
      "Epoch [2/2], Step [11000/16869], Loss: 0.1473\n",
      "Epoch [2/2], Step [11020/16869], Loss: 0.1624\n",
      "Epoch [2/2], Step [11040/16869], Loss: 0.1532\n",
      "Epoch [2/2], Step [11060/16869], Loss: 0.1443\n",
      "Epoch [2/2], Step [11080/16869], Loss: 0.1591\n",
      "Epoch [2/2], Step [11100/16869], Loss: 0.1445\n",
      "Epoch [2/2], Step [11120/16869], Loss: 0.1544\n",
      "Epoch [2/2], Step [11140/16869], Loss: 0.1626\n",
      "Epoch [2/2], Step [11160/16869], Loss: 0.1396\n",
      "Epoch [2/2], Step [11180/16869], Loss: 0.1696\n",
      "Epoch [2/2], Step [11200/16869], Loss: 0.1412\n",
      "Epoch [2/2], Step [11220/16869], Loss: 0.1429\n",
      "Epoch [2/2], Step [11240/16869], Loss: 0.1302\n",
      "Epoch [2/2], Step [11260/16869], Loss: 0.1456\n",
      "Epoch [2/2], Step [11280/16869], Loss: 0.1612\n",
      "Epoch [2/2], Step [11300/16869], Loss: 0.1402\n",
      "Epoch [2/2], Step [11320/16869], Loss: 0.1440\n",
      "Epoch [2/2], Step [11340/16869], Loss: 0.1628\n",
      "Epoch [2/2], Step [11360/16869], Loss: 0.1434\n",
      "Epoch [2/2], Step [11380/16869], Loss: 0.1552\n",
      "Epoch [2/2], Step [11400/16869], Loss: 0.1416\n",
      "Epoch [2/2], Step [11420/16869], Loss: 0.1465\n",
      "Epoch [2/2], Step [11440/16869], Loss: 0.1384\n",
      "Epoch [2/2], Step [11460/16869], Loss: 0.1444\n",
      "Epoch [2/2], Step [11480/16869], Loss: 0.1564\n",
      "Epoch [2/2], Step [11500/16869], Loss: 0.1663\n",
      "Epoch [2/2], Step [11520/16869], Loss: 0.1281\n",
      "Epoch [2/2], Step [11540/16869], Loss: 0.1572\n",
      "Epoch [2/2], Step [11560/16869], Loss: 0.1327\n",
      "Epoch [2/2], Step [11580/16869], Loss: 0.1328\n",
      "Epoch [2/2], Step [11600/16869], Loss: 0.1633\n",
      "Epoch [2/2], Step [11620/16869], Loss: 0.1493\n",
      "Epoch [2/2], Step [11640/16869], Loss: 0.1423\n",
      "Epoch [2/2], Step [11660/16869], Loss: 0.1653\n",
      "Epoch [2/2], Step [11680/16869], Loss: 0.1366\n",
      "Epoch [2/2], Step [11700/16869], Loss: 0.1579\n",
      "Epoch [2/2], Step [11720/16869], Loss: 0.1484\n",
      "Epoch [2/2], Step [11740/16869], Loss: 0.1319\n",
      "Epoch [2/2], Step [11760/16869], Loss: 0.1340\n",
      "Epoch [2/2], Step [11780/16869], Loss: 0.1506\n",
      "Epoch [2/2], Step [11800/16869], Loss: 0.1468\n",
      "Epoch [2/2], Step [11820/16869], Loss: 0.1593\n",
      "Epoch [2/2], Step [11840/16869], Loss: 0.1520\n",
      "Epoch [2/2], Step [11860/16869], Loss: 0.1509\n",
      "Epoch [2/2], Step [11880/16869], Loss: 0.1420\n",
      "Epoch [2/2], Step [11900/16869], Loss: 0.1402\n",
      "Epoch [2/2], Step [11920/16869], Loss: 0.1423\n",
      "Epoch [2/2], Step [11940/16869], Loss: 0.1645\n",
      "Epoch [2/2], Step [11960/16869], Loss: 0.1390\n",
      "Epoch [2/2], Step [11980/16869], Loss: 0.1410\n",
      "Epoch [2/2], Step [12000/16869], Loss: 0.1405\n",
      "Epoch [2/2], Step [12020/16869], Loss: 0.1471\n",
      "Epoch [2/2], Step [12040/16869], Loss: 0.1641\n",
      "Epoch [2/2], Step [12060/16869], Loss: 0.1414\n",
      "Epoch [2/2], Step [12080/16869], Loss: 0.1510\n",
      "Epoch [2/2], Step [12100/16869], Loss: 0.1617\n",
      "Epoch [2/2], Step [12120/16869], Loss: 0.1488\n",
      "Epoch [2/2], Step [12140/16869], Loss: 0.1367\n",
      "Epoch [2/2], Step [12160/16869], Loss: 0.1332\n",
      "Epoch [2/2], Step [12180/16869], Loss: 0.1564\n",
      "Epoch [2/2], Step [12200/16869], Loss: 0.1372\n",
      "Epoch [2/2], Step [12220/16869], Loss: 0.1626\n",
      "Epoch [2/2], Step [12240/16869], Loss: 0.1537\n",
      "Epoch [2/2], Step [12260/16869], Loss: 0.1336\n",
      "Epoch [2/2], Step [12280/16869], Loss: 0.1578\n",
      "Epoch [2/2], Step [12300/16869], Loss: 0.1469\n",
      "Epoch [2/2], Step [12320/16869], Loss: 0.1615\n",
      "Epoch [2/2], Step [12340/16869], Loss: 0.1627\n",
      "Epoch [2/2], Step [12360/16869], Loss: 0.1443\n",
      "Epoch [2/2], Step [12380/16869], Loss: 0.1294\n",
      "Epoch [2/2], Step [12400/16869], Loss: 0.1539\n",
      "Epoch [2/2], Step [12420/16869], Loss: 0.1490\n",
      "Epoch [2/2], Step [12440/16869], Loss: 0.1427\n",
      "Epoch [2/2], Step [12460/16869], Loss: 0.1515\n",
      "Epoch [2/2], Step [12480/16869], Loss: 0.1579\n",
      "Epoch [2/2], Step [12500/16869], Loss: 0.1451\n",
      "Epoch [2/2], Step [12520/16869], Loss: 0.1488\n",
      "Epoch [2/2], Step [12540/16869], Loss: 0.1442\n",
      "Epoch [2/2], Step [12560/16869], Loss: 0.1514\n",
      "Epoch [2/2], Step [12580/16869], Loss: 0.1491\n",
      "Epoch [2/2], Step [12600/16869], Loss: 0.1573\n",
      "Epoch [2/2], Step [12620/16869], Loss: 0.1509\n",
      "Epoch [2/2], Step [12640/16869], Loss: 0.1563\n",
      "Epoch [2/2], Step [12660/16869], Loss: 0.1394\n",
      "Epoch [2/2], Step [12680/16869], Loss: 0.1579\n",
      "Epoch [2/2], Step [12700/16869], Loss: 0.1562\n",
      "Epoch [2/2], Step [12720/16869], Loss: 0.1396\n",
      "Epoch [2/2], Step [12740/16869], Loss: 0.1407\n",
      "Epoch [2/2], Step [12760/16869], Loss: 0.1407\n",
      "Epoch [2/2], Step [12780/16869], Loss: 0.1598\n",
      "Epoch [2/2], Step [12800/16869], Loss: 0.1534\n",
      "Epoch [2/2], Step [12820/16869], Loss: 0.1380\n",
      "Epoch [2/2], Step [12840/16869], Loss: 0.1543\n",
      "Epoch [2/2], Step [12860/16869], Loss: 0.1632\n",
      "Epoch [2/2], Step [12880/16869], Loss: 0.1599\n",
      "Epoch [2/2], Step [12900/16869], Loss: 0.1417\n",
      "Epoch [2/2], Step [12920/16869], Loss: 0.1581\n",
      "Epoch [2/2], Step [12940/16869], Loss: 0.1447\n",
      "Epoch [2/2], Step [12960/16869], Loss: 0.1455\n",
      "Epoch [2/2], Step [12980/16869], Loss: 0.1464\n",
      "Epoch [2/2], Step [13000/16869], Loss: 0.1278\n",
      "Epoch [2/2], Step [13020/16869], Loss: 0.1437\n",
      "Epoch [2/2], Step [13040/16869], Loss: 0.1588\n",
      "Epoch [2/2], Step [13060/16869], Loss: 0.1517\n",
      "Epoch [2/2], Step [13080/16869], Loss: 0.1489\n",
      "Epoch [2/2], Step [13100/16869], Loss: 0.1642\n",
      "Epoch [2/2], Step [13120/16869], Loss: 0.1572\n",
      "Epoch [2/2], Step [13140/16869], Loss: 0.1460\n",
      "Epoch [2/2], Step [13160/16869], Loss: 0.1551\n",
      "Epoch [2/2], Step [13180/16869], Loss: 0.1270\n",
      "Epoch [2/2], Step [13200/16869], Loss: 0.1372\n",
      "Epoch [2/2], Step [13220/16869], Loss: 0.1500\n",
      "Epoch [2/2], Step [13240/16869], Loss: 0.1499\n",
      "Epoch [2/2], Step [13260/16869], Loss: 0.1641\n",
      "Epoch [2/2], Step [13280/16869], Loss: 0.1334\n",
      "Epoch [2/2], Step [13300/16869], Loss: 0.1546\n",
      "Epoch [2/2], Step [13320/16869], Loss: 0.1532\n",
      "Epoch [2/2], Step [13340/16869], Loss: 0.1549\n",
      "Epoch [2/2], Step [13360/16869], Loss: 0.1599\n",
      "Epoch [2/2], Step [13380/16869], Loss: 0.1440\n",
      "Epoch [2/2], Step [13400/16869], Loss: 0.1439\n",
      "Epoch [2/2], Step [13420/16869], Loss: 0.1458\n",
      "Epoch [2/2], Step [13440/16869], Loss: 0.1511\n",
      "Epoch [2/2], Step [13460/16869], Loss: 0.1543\n",
      "Epoch [2/2], Step [13480/16869], Loss: 0.1310\n",
      "Epoch [2/2], Step [13500/16869], Loss: 0.1394\n",
      "Epoch [2/2], Step [13520/16869], Loss: 0.1438\n",
      "Epoch [2/2], Step [13540/16869], Loss: 0.1460\n",
      "Epoch [2/2], Step [13560/16869], Loss: 0.1485\n",
      "Epoch [2/2], Step [13580/16869], Loss: 0.1498\n",
      "Epoch [2/2], Step [13600/16869], Loss: 0.1420\n",
      "Epoch [2/2], Step [13620/16869], Loss: 0.1374\n",
      "Epoch [2/2], Step [13640/16869], Loss: 0.1392\n",
      "Epoch [2/2], Step [13660/16869], Loss: 0.1515\n",
      "Epoch [2/2], Step [13680/16869], Loss: 0.1519\n",
      "Epoch [2/2], Step [13700/16869], Loss: 0.1238\n",
      "Epoch [2/2], Step [13720/16869], Loss: 0.1502\n",
      "Epoch [2/2], Step [13740/16869], Loss: 0.1455\n",
      "Epoch [2/2], Step [13760/16869], Loss: 0.1400\n",
      "Epoch [2/2], Step [13780/16869], Loss: 0.1526\n",
      "Epoch [2/2], Step [13800/16869], Loss: 0.1348\n",
      "Epoch [2/2], Step [13820/16869], Loss: 0.1545\n",
      "Epoch [2/2], Step [13840/16869], Loss: 0.1431\n",
      "Epoch [2/2], Step [13860/16869], Loss: 0.1524\n",
      "Epoch [2/2], Step [13880/16869], Loss: 0.1553\n",
      "Epoch [2/2], Step [13900/16869], Loss: 0.1426\n",
      "Epoch [2/2], Step [13920/16869], Loss: 0.1494\n",
      "Epoch [2/2], Step [13940/16869], Loss: 0.1474\n",
      "Epoch [2/2], Step [13960/16869], Loss: 0.1344\n",
      "Epoch [2/2], Step [13980/16869], Loss: 0.1442\n",
      "Epoch [2/2], Step [14000/16869], Loss: 0.1575\n",
      "Epoch [2/2], Step [14020/16869], Loss: 0.1519\n",
      "Epoch [2/2], Step [14040/16869], Loss: 0.1610\n",
      "Epoch [2/2], Step [14060/16869], Loss: 0.1561\n",
      "Epoch [2/2], Step [14080/16869], Loss: 0.1656\n",
      "Epoch [2/2], Step [14100/16869], Loss: 0.1264\n",
      "Epoch [2/2], Step [14120/16869], Loss: 0.1552\n",
      "Epoch [2/2], Step [14140/16869], Loss: 0.1515\n",
      "Epoch [2/2], Step [14160/16869], Loss: 0.1596\n",
      "Epoch [2/2], Step [14180/16869], Loss: 0.1610\n",
      "Epoch [2/2], Step [14200/16869], Loss: 0.1419\n",
      "Epoch [2/2], Step [14220/16869], Loss: 0.1494\n",
      "Epoch [2/2], Step [14240/16869], Loss: 0.1446\n",
      "Epoch [2/2], Step [14260/16869], Loss: 0.1331\n",
      "Epoch [2/2], Step [14280/16869], Loss: 0.1499\n",
      "Epoch [2/2], Step [14300/16869], Loss: 0.1460\n",
      "Epoch [2/2], Step [14320/16869], Loss: 0.1399\n",
      "Epoch [2/2], Step [14340/16869], Loss: 0.1347\n",
      "Epoch [2/2], Step [14360/16869], Loss: 0.1448\n",
      "Epoch [2/2], Step [14380/16869], Loss: 0.1446\n",
      "Epoch [2/2], Step [14400/16869], Loss: 0.1412\n",
      "Epoch [2/2], Step [14420/16869], Loss: 0.1450\n",
      "Epoch [2/2], Step [14440/16869], Loss: 0.1393\n",
      "Epoch [2/2], Step [14460/16869], Loss: 0.1594\n",
      "Epoch [2/2], Step [14480/16869], Loss: 0.1616\n",
      "Epoch [2/2], Step [14500/16869], Loss: 0.1511\n",
      "Epoch [2/2], Step [14520/16869], Loss: 0.1581\n",
      "Epoch [2/2], Step [14540/16869], Loss: 0.1597\n",
      "Epoch [2/2], Step [14560/16869], Loss: 0.1545\n",
      "Epoch [2/2], Step [14580/16869], Loss: 0.1315\n",
      "Epoch [2/2], Step [14600/16869], Loss: 0.1338\n",
      "Epoch [2/2], Step [14620/16869], Loss: 0.1447\n",
      "Epoch [2/2], Step [14640/16869], Loss: 0.1383\n",
      "Epoch [2/2], Step [14660/16869], Loss: 0.1532\n",
      "Epoch [2/2], Step [14680/16869], Loss: 0.1431\n",
      "Epoch [2/2], Step [14700/16869], Loss: 0.1393\n",
      "Epoch [2/2], Step [14720/16869], Loss: 0.1539\n",
      "Epoch [2/2], Step [14740/16869], Loss: 0.1432\n",
      "Epoch [2/2], Step [14760/16869], Loss: 0.1384\n",
      "Epoch [2/2], Step [14780/16869], Loss: 0.1428\n",
      "Epoch [2/2], Step [14800/16869], Loss: 0.1379\n",
      "Epoch [2/2], Step [14820/16869], Loss: 0.1423\n",
      "Epoch [2/2], Step [14840/16869], Loss: 0.1617\n",
      "Epoch [2/2], Step [14860/16869], Loss: 0.1521\n",
      "Epoch [2/2], Step [14880/16869], Loss: 0.1364\n",
      "Epoch [2/2], Step [14900/16869], Loss: 0.1304\n",
      "Epoch [2/2], Step [14920/16869], Loss: 0.1369\n",
      "Epoch [2/2], Step [14940/16869], Loss: 0.1583\n",
      "Epoch [2/2], Step [14960/16869], Loss: 0.1586\n",
      "Epoch [2/2], Step [14980/16869], Loss: 0.1658\n",
      "Epoch [2/2], Step [15000/16869], Loss: 0.1589\n",
      "Epoch [2/2], Step [15020/16869], Loss: 0.1528\n",
      "Epoch [2/2], Step [15040/16869], Loss: 0.1514\n",
      "Epoch [2/2], Step [15060/16869], Loss: 0.1391\n",
      "Epoch [2/2], Step [15080/16869], Loss: 0.1450\n",
      "Epoch [2/2], Step [15100/16869], Loss: 0.1503\n",
      "Epoch [2/2], Step [15120/16869], Loss: 0.1576\n",
      "Epoch [2/2], Step [15140/16869], Loss: 0.1419\n",
      "Epoch [2/2], Step [15160/16869], Loss: 0.1388\n",
      "Epoch [2/2], Step [15180/16869], Loss: 0.1385\n",
      "Epoch [2/2], Step [15200/16869], Loss: 0.1591\n",
      "Epoch [2/2], Step [15220/16869], Loss: 0.1492\n",
      "Epoch [2/2], Step [15240/16869], Loss: 0.1357\n",
      "Epoch [2/2], Step [15260/16869], Loss: 0.1429\n",
      "Epoch [2/2], Step [15280/16869], Loss: 0.1494\n",
      "Epoch [2/2], Step [15300/16869], Loss: 0.1491\n",
      "Epoch [2/2], Step [15320/16869], Loss: 0.1387\n",
      "Epoch [2/2], Step [15340/16869], Loss: 0.1472\n",
      "Epoch [2/2], Step [15360/16869], Loss: 0.1725\n",
      "Epoch [2/2], Step [15380/16869], Loss: 0.1486\n",
      "Epoch [2/2], Step [15400/16869], Loss: 0.1421\n",
      "Epoch [2/2], Step [15420/16869], Loss: 0.1584\n",
      "Epoch [2/2], Step [15440/16869], Loss: 0.1500\n",
      "Epoch [2/2], Step [15460/16869], Loss: 0.1545\n",
      "Epoch [2/2], Step [15480/16869], Loss: 0.1417\n",
      "Epoch [2/2], Step [15500/16869], Loss: 0.1449\n",
      "Epoch [2/2], Step [15520/16869], Loss: 0.1535\n",
      "Epoch [2/2], Step [15540/16869], Loss: 0.1526\n",
      "Epoch [2/2], Step [15560/16869], Loss: 0.1504\n",
      "Epoch [2/2], Step [15580/16869], Loss: 0.1465\n",
      "Epoch [2/2], Step [15600/16869], Loss: 0.1426\n",
      "Epoch [2/2], Step [15620/16869], Loss: 0.1677\n",
      "Epoch [2/2], Step [15640/16869], Loss: 0.1392\n",
      "Epoch [2/2], Step [15660/16869], Loss: 0.1438\n",
      "Epoch [2/2], Step [15680/16869], Loss: 0.1523\n",
      "Epoch [2/2], Step [15700/16869], Loss: 0.1394\n",
      "Epoch [2/2], Step [15720/16869], Loss: 0.1175\n",
      "Epoch [2/2], Step [15740/16869], Loss: 0.1368\n",
      "Epoch [2/2], Step [15760/16869], Loss: 0.1526\n",
      "Epoch [2/2], Step [15780/16869], Loss: 0.1414\n",
      "Epoch [2/2], Step [15800/16869], Loss: 0.1385\n",
      "Epoch [2/2], Step [15820/16869], Loss: 0.1356\n",
      "Epoch [2/2], Step [15840/16869], Loss: 0.1398\n",
      "Epoch [2/2], Step [15860/16869], Loss: 0.1516\n",
      "Epoch [2/2], Step [15880/16869], Loss: 0.1282\n",
      "Epoch [2/2], Step [15900/16869], Loss: 0.1568\n",
      "Epoch [2/2], Step [15920/16869], Loss: 0.1640\n",
      "Epoch [2/2], Step [15940/16869], Loss: 0.1312\n",
      "Epoch [2/2], Step [15960/16869], Loss: 0.1425\n",
      "Epoch [2/2], Step [15980/16869], Loss: 0.1483\n",
      "Epoch [2/2], Step [16000/16869], Loss: 0.1471\n",
      "Epoch [2/2], Step [16020/16869], Loss: 0.1467\n",
      "Epoch [2/2], Step [16040/16869], Loss: 0.1343\n",
      "Epoch [2/2], Step [16060/16869], Loss: 0.1344\n",
      "Epoch [2/2], Step [16080/16869], Loss: 0.1454\n",
      "Epoch [2/2], Step [16100/16869], Loss: 0.1543\n",
      "Epoch [2/2], Step [16120/16869], Loss: 0.1381\n",
      "Epoch [2/2], Step [16140/16869], Loss: 0.1538\n",
      "Epoch [2/2], Step [16160/16869], Loss: 0.1548\n",
      "Epoch [2/2], Step [16180/16869], Loss: 0.1331\n",
      "Epoch [2/2], Step [16200/16869], Loss: 0.1460\n",
      "Epoch [2/2], Step [16220/16869], Loss: 0.1466\n",
      "Epoch [2/2], Step [16240/16869], Loss: 0.1267\n",
      "Epoch [2/2], Step [16260/16869], Loss: 0.1480\n",
      "Epoch [2/2], Step [16280/16869], Loss: 0.1578\n",
      "Epoch [2/2], Step [16300/16869], Loss: 0.1555\n",
      "Epoch [2/2], Step [16320/16869], Loss: 0.1559\n",
      "Epoch [2/2], Step [16340/16869], Loss: 0.1474\n",
      "Epoch [2/2], Step [16360/16869], Loss: 0.1346\n",
      "Epoch [2/2], Step [16380/16869], Loss: 0.1475\n",
      "Epoch [2/2], Step [16400/16869], Loss: 0.1327\n",
      "Epoch [2/2], Step [16420/16869], Loss: 0.1516\n",
      "Epoch [2/2], Step [16440/16869], Loss: 0.1416\n",
      "Epoch [2/2], Step [16460/16869], Loss: 0.1389\n",
      "Epoch [2/2], Step [16480/16869], Loss: 0.1546\n",
      "Epoch [2/2], Step [16500/16869], Loss: 0.1600\n",
      "Epoch [2/2], Step [16520/16869], Loss: 0.1332\n",
      "Epoch [2/2], Step [16540/16869], Loss: 0.1427\n",
      "Epoch [2/2], Step [16560/16869], Loss: 0.1533\n",
      "Epoch [2/2], Step [16580/16869], Loss: 0.1410\n",
      "Epoch [2/2], Step [16600/16869], Loss: 0.1403\n",
      "Epoch [2/2], Step [16620/16869], Loss: 0.1527\n",
      "Epoch [2/2], Step [16640/16869], Loss: 0.1489\n",
      "Epoch [2/2], Step [16660/16869], Loss: 0.1413\n",
      "Epoch [2/2], Step [16680/16869], Loss: 0.1297\n",
      "Epoch [2/2], Step [16700/16869], Loss: 0.1502\n",
      "Epoch [2/2], Step [16720/16869], Loss: 0.1542\n",
      "Epoch [2/2], Step [16740/16869], Loss: 0.1532\n",
      "Epoch [2/2], Step [16760/16869], Loss: 0.1482\n",
      "Epoch [2/2], Step [16780/16869], Loss: 0.1434\n",
      "Epoch [2/2], Step [16800/16869], Loss: 0.1603\n",
      "Epoch [2/2], Step [16820/16869], Loss: 0.1320\n",
      "Epoch [2/2], Step [16840/16869], Loss: 0.1509\n",
      "Epoch [2/2], Step [16860/16869], Loss: 0.1445\n",
      "Epoch [2/2] Average Loss: 0.1499, Perplexity: 1.16\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "perplexities = []\n",
    "\n",
    "# Go through learning epochs\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # Read in data in batches\n",
    "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Apply the forward pass\n",
    "        logits = model(x)\n",
    "\n",
    "        # Reshape logits and labels\n",
    "        token_logits = logits.view(-1, vocab_size)\n",
    "        token_labels = y.view(-1)\n",
    "\n",
    "        # To understand what is happening during reshaping, print out logits.shape and token_logits.shape\n",
    "        # and the same for y\n",
    "        # print(logits.shape, token_logits.shape)\n",
    "        # print(y.shape, token_labels.shape)\n",
    "        # print(y[0])\n",
    "        # print(token_labels[0:10])\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(token_logits,token_labels)\n",
    "\n",
    "        # Apply the backward step (calculate the gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss over batches\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Monitor progress every twenty batches\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Calculate average cross-entropy loss and perplexity\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "\n",
    "    # Monitor developments over learning process\n",
    "    train_losses.append(avg_loss)\n",
    "    perplexities.append(perplexity)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppOBpYMIlQHE"
   },
   "source": [
    "## plot + loss and perplexity (cnn model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:57:55.651007Z",
     "iopub.status.busy": "2024-12-14T12:57:55.650661Z",
     "iopub.status.idle": "2024-12-14T12:58:20.396085Z",
     "shell.execute_reply": "2024-12-14T12:58:20.395149Z",
     "shell.execute_reply.started": "2024-12-14T12:57:55.650982Z"
    },
    "id": "MJO7_RK0kM1x",
    "outputId": "b075d0a9-c0e6-47e1-9054-46b57b926bb4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/IAAAHWCAYAAADUwLIxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACrdklEQVR4nOzdd1QU198G8Gd3KUtvghRB7CgqoAJCYosYNcbeNQF7jSVqov4Su7EnMUZjjaBR7DXGbiyJIKiIvRdApIiFXnfn/cOXjSuggItDeT7n7NGdnR2eLXDnO3PnXokgCAKIiIiIiIiIqEyQih2AiIiIiIiIiAqPhTwRERERERFRGcJCnoiIiIiIiKgMYSFPREREREREVIawkCciIiIiIiIqQ1jIExEREREREZUhLOSJiIiIiIiIyhAW8kRERERERERlCAt5IiIiIiIiojKEhXwF4ejoiAEDBojys2fOnAmJRCLKz9Y0iUSCmTNnFvl5jx49gkQiQUBAgMYzlaSAgABIJBI8evSoyM89deoUJBIJTp06pfFcRK/L/f1asmSJ2FGIyhzuH2hGRds/KI7ivkeFVZ6+TxWFo6MjPv/8c7FjlFks5Mu4q1evokePHqhatSrkcjns7OzQpk0b/Prrr2JHKzG5xaVEIsG///6b53FBEGBvbw+JRFJu/zi0bNlS9R687VaSDWZplvsduXDhgthRyoXcHc2CbgsWLBA7IhG9gfsHFXP/AMj7N1smk8HBwQFdu3ZFeHi42PE+qHnz5mHv3r1ixxCNo6NjgW13u3btxI5H70lL7ABUfEFBQWjVqhUcHBwwdOhQWFtbIyoqCufOncMvv/yCMWPGqNa9ffs2pNLyddxGLpcjMDAQH3/8sdry06dP4/Hjx9DV1RUpWcn77rvvMGTIENX98+fPY9myZfjf//6HunXrqpY3bNjwvX7Ol19+iT59+hTrvWzevDnS09Oho6PzXhmo9Ojbty8+++yzPMvd3NxESENEBeH+QcXdP3hd7t9shUKBmzdvYuXKlTh06BDOnTsHV1dXseNp3Pfff48pU6aoLZs3bx569OiBLl26iBOqFHB1dcXEiRPzLLe1tRUhDWkSC/ky7IcffoCJiQnOnz8PU1NTtcfi4+PV7pfHRuuzzz7Djh07sGzZMmhp/fdVDgwMROPGjZGQkCBiupLVpk0btftyuRzLli1DmzZt0LJlywKfl5qaCgMDg0L/HJlMBplMVqyMUqkUcrm8WM+lD68w341GjRrhiy+++ECJiKi4uH9QcfcPXvfm3+yPPvoInTp1wsqVK7F69er32nZR9yc+BC0tLbXPuyLIycmBUql860kTOzs7tt3lVPk6BFvB3L9/H87OznkaaQCwsrJSu//mNXC53c/+/fdfjB07FpaWljA1NcXw4cORlZWFly9fwtfXF2ZmZjAzM8O3334LQRBUz3/9mtSff/4ZVatWhZ6eHlq0aIFr164VKv+mTZvQuHFj6OnpwdzcHH369EFUVFShX3/fvn3x7NkzHDt2TLUsKysLO3fuRL9+/fJ9TmpqKiZOnAh7e3vo6uqiTp06WLJkidprA4DMzEx8/fXXsLS0hJGRETp16oTHjx/nu83o6GgMGjQIlStXhq6uLpydnbF+/fpCv46Sknut2I0bN9CvXz+YmZmpzk5cuXIFAwYMQPXq1SGXy2FtbY1Bgwbh2bNnatvI7xr53OuZ/v33X3h4eEAul6N69erYuHGj2nPzu0a+ZcuWqF+/Pm7cuIFWrVpBX18fdnZ2WLRoUZ78ERER6NSpEwwMDGBlZYWvv/4aR44c0eh195cuXUL79u1hbGwMQ0NDtG7dGufOnVNbJzs7G7NmzUKtWrUgl8thYWGBjz/+WO17Fxsbi4EDB6JKlSrQ1dWFjY0NOnfuXKixBf7++280a9YMBgYGMDU1RefOnXHz5k3V4zt37oREIsHp06fzPHf16tWQSCRqv3O3bt1Cjx49YG5uDrlcjiZNmmD//v1qz8v9XE+fPo1Ro0bBysoKVapUKezb9la534+jR4/C1dUVcrkc9erVw+7du/Os++DBA/Ts2RPm5ubQ19dH06ZN8ddff+VZLyMjAzNnzkTt2rUhl8thY2ODbt264f79+3nWXbNmDWrUqAFdXV24u7vj/Pnzao+/z2dFVFZw/4D7B/n55JNPAAAPHz5ULQsJCUG7du1gYmICfX19tGjRAmfPnlV73tv2JwYMGABDQ0M8ePAAbdu2hYGBAWxtbTF79uw8711+3vUepaenw8nJCU5OTkhPT1ctf/78OWxsbODt7Q2FQqGWM5dEIkFqaio2bNig6k4+YMAAnDx5EhKJBHv27MmTJzAwEBKJBMHBwW/N/a72Ky4uDlpaWpg1a1ae596+fRsSiQTLly9XLXv58iXGjx+v+v7VrFkTCxcuhFKpVK3z+u/W0qVLVW3djRs33pq1MIryORb2dwV49bvs4eEBfX19mJmZoXnz5jh69Gie9d61T1mYfbGKqGIdtipnqlatiuDgYFy7dg3169cv1jbGjBkDa2trzJo1C+fOncOaNWtgamqKoKAgODg4YN68eTh48CAWL16M+vXrw9fXV+35GzduRHJyMkaPHo2MjAz88ssv+OSTT3D16lVUrly5wJ/7ww8/YNq0aejVqxeGDBmCp0+f4tdff0Xz5s1x6dKlfHc+3uTo6AgvLy9s2bIF7du3BwAcOnQIiYmJ6NOnD5YtW6a2viAI6NSpE06ePInBgwfD1dUVR44cwTfffIPo6Gj8/PPPqnWHDBmCTZs2oV+/fvD29sbff/+NDh065MkQFxeHpk2bQiKR4KuvvoKlpSUOHTqEwYMHIykpCePHj3/n6yhpPXv2RK1atTBv3jzVH9ljx47hwYMHGDhwIKytrXH9+nWsWbMG169fx7lz5945WMy9e/fQo0cPDB48GH5+fli/fj0GDBiAxo0bw9nZ+a3PffHiBdq1a4du3bqhV69e2LlzJyZPnowGDRqoPsfU1FR88skniImJwbhx42BtbY3AwECcPHlSM28KgOvXr6NZs2YwNjbGt99+C21tbaxevRotW7bE6dOn4enpCeDVjsH8+fMxZMgQeHh4ICkpCRcuXEBYWJiqZ0T37t1x/fp1jBkzBo6OjoiPj8exY8cQGRkJR0fHAjMcP34c7du3R/Xq1TFz5kykp6fj119/xUcffYSwsDA4OjqiQ4cOMDQ0xPbt29GiRQu152/btg3Ozs6q3//r16/jo48+gp2dHaZMmQIDAwNs374dXbp0wa5du9C1a1e1548aNQqWlpaYPn06UlNT3/mepaWl5Xsmy9TUVO0syN27d9G7d2+MGDECfn5+8Pf3R8+ePXH48GHVexYXFwdvb2+kpaVh7NixsLCwwIYNG9CpUyfs3LlTlVWhUODzzz/HiRMn0KdPH4wbNw7Jyck4duwYrl27hho1aqh+bmBgIJKTkzF8+HBIJBIsWrQI3bp1w4MHD6Ctrf1enxVRWcL9A+4f5Cf34KeFhQWAVweS27dvj8aNG2PGjBmQSqXw9/fHJ598gn/++QceHh5qz89vfwJ49Xe6Xbt2aNq0KRYtWoTDhw9jxowZyMnJwezZswvMU5j3SE9PDxs2bMBHH32E7777Dj/99BMAYPTo0UhMTERAQECBPQf/+OMPVds9bNgwAECNGjXQtGlT2NvbY/PmzXnaxc2bN6NGjRrw8vJ6a+53tV+VK1dGixYtsH37dsyYMUPt+du2bYNMJkPPnj0BvGpbW7RogejoaAwfPhwODg4ICgrC1KlTERMTg6VLl6o939/fHxkZGRg2bBh0dXVhbm5eYFbgVRGcX9ttYGAAPT091f3CfI5F+V2ZNWsWZs6cCW9vb8yePRs6OjoICQnB33//jU8//VS1XmH2KQuzL1YhCVRmHT16VJDJZIJMJhO8vLyEb7/9Vjhy5IiQlZWVZ92qVasKfn5+qvv+/v4CAKFt27aCUqlULffy8hIkEokwYsQI1bKcnByhSpUqQosWLVTLHj58KAAQ9PT0hMePH6uWh4SECACEr7/+WrVsxowZwutftUePHgkymUz44Ycf1DJevXpV0NLSyrP8TbnZz58/LyxfvlwwMjIS0tLSBEEQhJ49ewqtWrVSveYOHTqonrd3714BgDB37ly17fXo0UOQSCTCvXv3BEEQhPDwcAGAMGrUKLX1+vXrJwAQZsyYoVo2ePBgwcbGRkhISFBbt0+fPoKJiYkqV+775e/v/9bXVlw7duwQAAgnT55ULct93/v27Ztn/dxcr9uyZYsAQDhz5oxqWe57/fDhQ9WyqlWr5lkvPj5e0NXVFSZOnKhadvLkyTyZWrRoIQAQNm7cqFqWmZkpWFtbC927d1ct+/HHHwUAwt69e1XL0tPTBScnpzzbzM/r35GCdOnSRdDR0RHu37+vWvbkyRPByMhIaN68uWqZi4uL2vfoTS9evBAACIsXL35rpvy4uroKVlZWwrNnz1TLLl++LEilUsHX11e1rG/fvoKVlZWQk5OjWhYTEyNIpVJh9uzZqmWtW7cWGjRoIGRkZKiWKZVKwdvbW6hVq5ZqWe778/HHH6ttsyC539+CbsHBwap1c78fu3btUi1LTEwUbGxsBDc3N9Wy8ePHCwCEf/75R7UsOTlZqFatmuDo6CgoFApBEARh/fr1AgDhp59+ypMr929Xbj4LCwvh+fPnqsf37dsnABD+/PNPQRDe77MiKku4f1Cx9w9ytzlr1izh6dOnQmxsrHDq1CnBzc1N9fdZqVQKtWrVyvM5p6WlCdWqVRPatGmjWva2/Qk/Pz8BgDBmzBjVMqVSKXTo0EHQ0dERnj59qlpe3PdIEARh6tSpglQqFc6cOaPa51m6dKna8978PgmCIBgYGKh9v1/fnq6urvDy5UvVsvj4eEFLS0stY34K236tXr1aACBcvXpV7fn16tUTPvnkE9X9OXPmCAYGBsKdO3fU1psyZYogk8mEyMhIQRD++1yNjY2F+Pj4t2bMldsm53ebP3++ar3Cfo6F/V25e/euIJVKha5du6rej9e3+2a+d+1TvmtfrKJi1/oyrE2bNggODkanTp1w+fJlLFq0CG3btoWdnV2errQFGTx4sNrZV09PTwiCgMGDB6uWyWQyNGnSBA8ePMjz/C5dusDOzk5138PDA56enjh48GCBP3P37t1QKpXo1asXEhISVDdra2vUqlWrSGdde/XqhfT0dBw4cADJyck4cOBAgd3mDh48CJlMhrFjx6otnzhxIgRBwKFDh1TrAciz3ptHzwVBwK5du9CxY0cIgqD2Wtq2bYvExESEhYUV+rWUlBEjRuRZ9voR2IyMDCQkJKBp06YAUKjM9erVQ7NmzVT3LS0tUadOnXy/I28yNDRUu1ZLR0cHHh4eas89fPgw7Ozs0KlTJ9UyuVyOoUOHvnP7haFQKHD06FF06dIF1atXVy23sbFBv3798O+//yIpKQnAq7PN169fx927d/Pdlp6eHnR0dHDq1Cm8ePGi0BliYmIQHh6OAQMGqB1Nb9iwIdq0aaP2O9S7d2/Ex8erXVKwc+dOKJVK9O7dG8CrboZ///03evXqheTkZNV38dmzZ2jbti3u3r2L6OhotQxDhw4t0hgIw4YNw7Fjx/Lc6tWrp7aera2t2lkOY2Nj+Pr64tKlS4iNjQXw6vfMw8NDbTAqQ0NDDBs2DI8ePVJ1Fdy1axcqVaqkNjhXrjd7jvTu3RtmZmaq+7nf0dzvVnE/K6KyhvsH3D8AgBkzZsDS0hLW1tZo2bIl7t+/j4ULF6Jbt24IDw/H3bt30a9fPzx79kyVLzU1Fa1bt8aZM2fUunUD+e9P5Prqq69U/889w56VlYXjx4/nu35R36OZM2fC2dkZfn5+GDVqFFq0aJHncygKX19fZGZmYufOnapl27ZtQ05OzjuvJy9s+9WtWzdoaWlh27ZtqvWuXbuGGzduqNpuANixYweaNWsGMzMztffBx8cHCoUCZ86cUfv53bt3h6WlZaFfq6enZ75td9++ffOs+67PsbC/K3v37oVSqcT06dPzDKb5ZttdmH3Kd+2LVVTsWl/Gubu7Y/fu3cjKysLly5exZ88e/Pzzz+jRowfCw8Pz7GC/ycHBQe2+iYkJAMDe3j7P8vx2fGvVqpVnWe3atbF9+/YCf+bdu3chCEK+zwWg6gJbGJaWlvDx8UFgYCDS0tKgUCjQo0ePfNeNiIiAra0tjIyM1JbnjvIeERGh+lcqlap12QWAOnXqqN1/+vQpXr58iTVr1mDNmjX5/sw3BxV6G4VCgadPn6otMzc3f+9R36tVq5Zn2fPnzzFr1ixs3bo1T8bExMR3bvPN7w0AmJmZFao4qlKlSp4/4mZmZrhy5YrqfkREBGrUqJFnvZo1a75z+4Xx9OlTpKWl5flMgVffB6VSiaioKDg7O2P27Nno3Lkzateujfr166Ndu3b48ssvVTMC6OrqYuHChZg4cSIqV66Mpk2b4vPPP4evry+sra0LzJD7fSsow5EjR1SDCeVev7ht2za0bt0awKsdDldXV9SuXRvAq65pgiBg2rRpmDZtWr4/Mz4+Xm3HOr/vxtvUqlULPj4+71yvZs2aeT673JyPHj2CtbU1IiIiVJcvvO7138f69evj/v37qFOnTqEGMHrze5lb1Od+L4v7WRGVRdw/4P7BsGHD0LNnT0ilUpiamsLZ2Vk1uGFuQeTn51fg8xMTE9UOjhbUZkilUrWD4oD63/z8FPU90tHRwfr16+Hu7g65XA5/f//3mjPeyckJ7u7u2Lx5s+rg1ObNm9G0adN37msUtv2qVKkSWrduje3bt2POnDkAXrXdWlpa6Natm+p5d+/exZUrVwoszt/8rhS17a5UqVKh2u7CfI6F/V25f/8+pFLpO//OAIXbp3zXvlhFxUK+nNDR0YG7uzvc3d1Ru3ZtDBw4EDt27MhzXc6bCjobl99yoRCDlhSGUqmERCLBoUOH8v05hoaGRdpev379MHToUMTGxqJ9+/aFun5OE3KPVH/xxRcFNoRF+QMTFRWV54/zyZMn3zoKfWG8fvY9V69evRAUFIRvvvkGrq6uMDQ0hFKpRLt27fIcgc9PQd+bwnxH3ue5YmjevDnu37+Pffv24ejRo1i3bh1+/vlnrFq1SjUF4Pjx49GxY0fs3bsXR44cwbRp0zB//nz8/fffGpmaTVdXF126dMGePXvw22+/IS4uDmfPnsW8efNU6+R+bpMmTULbtm3z3c6bOyf5fTfKssJ8t0r6syIqbbh/UHH3D9528DU34+LFiwuciu7N91uTbUZx3qMjR44AeNWT8O7du0UuaN/k6+uLcePG4fHjx8jMzMS5c+fUBqDThD59+mDgwIEIDw+Hq6srtm/fjtatW6NSpUqqdZRKJdq0aYNvv/02323kFtO5KmLbXZh9sYqIhXw51KRJEwCvuu6WtPy6uNy5c+etg0bVqFEDgiCgWrVqef44FUfXrl0xfPhwnDt3Tq370puqVq2K48ePIzk5We1I4q1bt1SP5/6rVCpVZwJz3b59W217uSPWKhSKQh3pfBdra+s8o2+6uLi893bf9OLFC5w4cQKzZs3C9OnTVctLU3elqlWr4saNGxAEQe2I+7179zSyfUtLS+jr6+f5TIFX3wepVKp21snc3BwDBw7EwIEDkZKSgubNm2PmzJlqjUeNGjUwceJETJw4EXfv3oWrqyt+/PFHbNq0qcDXCOT9XuVmqFSpktrUPr1798aGDRtw4sQJ3Lx5E4IgqHXNyz2Krq2trZHv4/vI7R3w+md3584dAFD9bahatWqBrz33ceDV+xoSEoLs7OwinY17m6J+VkTlBfcP8lcR9w9yexUYGxu/d0alUokHDx6ofWZv/s1/U1HfoytXrmD27NmqonjIkCG4evWqqqdIQd521r5Pnz6YMGECtmzZgvT0dGhra6u1qwUpbPsFvLrEZPjw4arv3507dzB16lS159WoUQMpKSmit92F+RwL+7tSo0YNKJVK3Lhxo8ADRUVVmH2xiobXyJdhJ0+ezPcoeO41XPl12dW0vXv3ql13GxoaipCQENUosfnp1q0bZDIZZs2alSe/IAh5pkB7F0NDQ6xcuRIzZ85Ex44dC1zvs88+g0KhyHO09eeff4ZEIlFlzv33zVFt3xw1VCaToXv37ti1a1e+U+q82Q3uXeRyOXx8fNRur3dp05TcI59vvvdvvj4xtW3bFtHR0WrXcmZkZGDt2rUa2b5MJsOnn36Kffv2qXX7i4uLQ2BgID7++GMYGxsDQJ7vo6GhIWrWrInMzEwAr0abzcjIUFunRo0aMDIyUq2THxsbG7i6umLDhg14+fKlavm1a9dw9OhRfPbZZ2rr+/j4wNzcHNu2bcO2bdvg4eGhdjbCysoKLVu2xOrVq/PdSS/q9/F9PHnyRG1an6SkJGzcuBGurq6qLuyfffYZQkND1ab4SU1NxZo1a+Do6Kjqjte9e3ckJCTke5akqGcBi/tZEZU13D94hfsHBWvcuDFq1KiBJUuWICUl5b0zvv7eCYKA5cuXQ1tbW3U52JuK8h5lZ2djwIABsLW1xS+//IKAgADExcXh66+/fmcuAwMDtTb2dZUqVUL79u2xadMmbN68Ge3atVM7U16QwrZfwKtru9u2bYvt27dj69at0NHRQZcuXdS216tXLwQHB6t6HLzu5cuXyMnJeWcmTXnX51jY35UuXbpAKpVi9uzZeXp6FqcHz7v2xSoqnpEvw8aMGYO0tDR07doVTk5OyMrKQlBQELZt2wZHR0cMHDiwxDPUrFkTH3/8MUaOHInMzEwsXboUFhYWBXYPAl7tOM+dOxdTp07Fo0eP0KVLFxgZGeHhw4fYs2cPhg0bhkmTJhUpx9uu8crVsWNHtGrVCt999x0ePXoEFxcXHD16FPv27cP48eNVR6ddXV3Rt29f/Pbbb0hMTIS3tzdOnDiR79ngBQsW4OTJk/D09MTQoUNRr149PH/+HGFhYTh+/DieP39epNfxIRgbG6N58+ZYtGgRsrOzYWdnh6NHj6rNKyu24cOHY/ny5ejbty/GjRsHGxsbbN68GXK5HMDbj7C/bv369Th8+HCe5ePGjcPcuXNx7NgxfPzxxxg1ahS0tLSwevVqZGZmqs1rX69ePbRs2RKNGzeGubk5Lly4gJ07d6oGhLlz5w5at26NXr16oV69etDS0sKePXsQFxeHPn36vDXf4sWL0b59e3h5eWHw4MGq6edMTEwwc+ZMtXW1tbXRrVs3bN26FampqViyZEme7a1YsQIff/wxGjRogKFDh6J69eqIi4tDcHAwHj9+jMuXLxfqfStIWFhYvmet35yqp3bt2hg8eDDOnz+PypUrY/369YiLi4O/v79qnSlTpqimhho7dizMzc2xYcMGPHz4ELt27VINjuPr64uNGzdiwoQJCA0NRbNmzZCamorjx49j1KhR6Ny5c6Hzv89nRVSWcP/gP9w/yJ9UKsW6devQvn17ODs7Y+DAgbCzs0N0dDROnjwJY2Nj/Pnnn4Xallwux+HDh+Hn5wdPT08cOnQIf/31F/73v/+9dVC2wr5Hc+fORXh4OE6cOAEjIyM0bNgQ06dPx/fff48ePXrkOfD9usaNG+P48eP46aefYGtri2rVqqld3+7r66saOyH3OvZ3KWz7lat379744osv8Ntvv6Ft27Z5LvH45ptvsH//fnz++eeqaddSU1Nx9epV7Ny5E48ePSrUAYaCREdH59t2Gxoaqh1UKMznWNjflZo1a+K7777DnDlz0KxZM3Tr1g26uro4f/48bG1tMX/+/CK9hnfti1VYJTsoPpWkQ4cOCYMGDRKcnJwEQ0NDQUdHR6hZs6YwZswYIS4uTm3dgqaXeXN6rtypO16fLkQQXk1LYWBgoLqfOwXG4sWLhR9//FGwt7cXdHV1hWbNmgmXL1/Od5tv2rVrl/Dxxx8LBgYGgoGBgeDk5CSMHj1auH379ltfd2GmFst9zW9OVZGcnCx8/fXXgq2traCtrS3UqlVLWLx4sdpUGILwaqqzsWPHChYWFoKBgYHQsWNHISoqKs/UKYIgCHFxccLo0aMFe3t7QVtbW7C2thZat24trFmzJs/7Jcb0c29+loIgCI8fPxa6du0qmJqaCiYmJkLPnj2FJ0+e5Hl9BU0/l98UIC1atFCbgqig6eecnZ3zPNfPz0+oWrWq2rIHDx4IHTp0EPT09ARLS0th4sSJwq5duwQAwrlz5976fuTmLugWFRUlCIIghIWFCW3bthUMDQ0FfX19oVWrVkJQUJDatubOnSt4eHgIpqamgp6enuDk5CT88MMPqmmcEhIShNGjRwtOTk6CgYGBYGJiInh6egrbt29/a8Zcx48fFz766CNBT09PMDY2Fjp27CjcuHEj33WPHTsmABAkEonqNbzp/v37gq+vr2BtbS1oa2sLdnZ2wueffy7s3Lkzz/vzrt+hXO+afu71vy25348jR44IDRs2FHR1dQUnJydhx44d+Wbt0aOHYGpqKsjlcsHDw0M4cOBAnvXS0tKE7777TqhWrZrqd6xHjx6qqQNf/3v0pte/0+/7WRGVFdw/qNj7B2/7m/imS5cuCd26dRMsLCwEXV1doWrVqkKvXr2EEydOqNZ52/5E7ud///594dNPPxX09fWFypUrCzNmzMgz7Vhx3qOLFy8KWlpaatOiCcKrqQ/d3d0FW1tb4cWLF2o5X3fr1i2hefPmgp6eXp72ShBeTYFrZmYmmJiYCOnp6e98v3IVtv0SBEFISkpS/fxNmzblu05ycrIwdepUoWbNmoKOjo5QqVIlwdvbW1iyZIlqf6Mon2uut00/9/p+V1E+x8L+rgjCqylk3dzcBF1dXcHMzExo0aKFcOzYMbV8hdmnfNe+WEUlEYRSOsIUlWqPHj1CtWrVsHjx4iIfHScqrqVLl+Lrr7/G48eP1UZfp9LD0dER9evXx4EDB8SOQkQi4P5BxTJgwADs3Lkz3+75ZUFOTg5sbW3RsWNH/P7772LHEU1Z/xwrKl4jT0SlUnp6utr9jIwMrF69GrVq1WIRT0RERO9t7969ePr0KXx9fcWOQlRkvEaeiEqlbt26wcHBAa6urkhMTMSmTZtw69YtbN68WexoREREVIaFhITgypUrmDNnDtzc3NCiRQuxIxEVGQt5IiqV2rZti3Xr1mHz5s1QKBSoV68etm7dWqipYYiIiIgKsnLlSmzatAmurq4ICAgQOw5RsfAaeSIiIiIiIqIyhNfIExEREREREZUhLOSJiIiIiIiIyhBeI58PpVKJJ0+ewMjICBKJROw4REREEAQBycnJsLW1hVTK4/Dvi209ERGVNkVp61nI5+PJkyewt7cXOwYREVEeUVFRqFKlitgxyjy29UREVFoVpq1nIZ8PIyMjAK/eQGNjY5HTEBERAUlJSbC3t1e1UfR+2NYTEVFpU5S2noV8PnK72BkbG7NxJyKiUoXdwDWDbT0REZVWhWnreZEdERERERERURnCQp6IiIiIiIioDGEhT0RERERERFSG8Bp5IqIPQBAE5OTkQKFQiB2FSimZTAYtLS1eA09ERCrcfyhfNNnWs5AnIiphWVlZiImJQVpamthRqJTT19eHjY0NdHR0xI5CREQi4/5D+aSptp6FPBFRCVIqlXj48CFkMhlsbW2ho6PDM66UhyAIyMrKwtOnT/Hw4UPUqlULUimvfiMiqqi4/1D+aLqtZyFPRFSCsrKyoFQqYW9vD319fbHjUCmmp6cHbW1tREREICsrC3K5XOxIREQkEu4/lE+abOt5uJ+I6APg2VUqDH5PiIjodWwXyh9Nfab8ZhAREVGJOXPmDDp27AhbW1tIJBLs3bv3revv3r0bbdq0gaWlJYyNjeHl5YUjR46oraNQKDBt2jRUq1YNenp6qFGjBubMmQNBEErwlRAREZUeLORLkEIpIPj+M+wLj0bw/WdQKLmDQUREFUtqaipcXFywYsWKQq1/5swZtGnTBgcPHsTFixfRqlUrdOzYEZcuXVKts3DhQqxcuRLLly/HzZs3sXDhQixatAi//vprSb2Mt2J7T0REHxqvkS8hh6/FYNafNxCTmKFaZmMix4yO9dCuvo2IyYiorFIoBYQ+fI745AxYGcnhUc0cMmnZGvjG0dER48ePx/jx4wu1/qlTp9CqVSu8ePECpqamJZqNSkb79u3Rvn37Qq+/dOlStfvz5s3Dvn378Oeff8LNzQ0AEBQUhM6dO6NDhw4AXn2vtmzZgtDQUI3lLiy290RU2pWH/YfCaNmyJVxdXfO0I8UVEBCA8ePH4+XLlxrZnqbxjHwJOHwtBiM3hak16gAQm5iBkZvCcPhajEjJiKisOnwtBh8v/Bt9157DuK3h6Lv2HD5e+HeJ/T2RSCRvvc2cObNY2z1//jyGDRtW6PW9vb0RExMDExOTYv28wjp16hQkEkmpbawrMqVSieTkZJibm6uWeXt748SJE7hz5w4A4PLly/j333/fesAgMzMTSUlJarf3xfaeiEq7D73/AAADBgxQ7S/o6OigZs2amD17NnJyckrsZ5aE3r17q9oZAJg5cyZcXV3FC/QGFvIaplAKmPXnDeTXqS532aw/b7DbHREVmhjFQkxMjOq2dOlSGBsbqy2bNGmSal1BEArdOFtaWhZp9F0dHR1YW1tzyp0KbMmSJUhJSUGvXr1Uy6ZMmYI+ffrAyckJ2tracHNzw/jx49G/f/8CtzN//nyYmJiobvb29u+Vi+09EZV2Yh5sbNeuHWJiYnD37l1MnDgRM2fOxOLFi4u8HYVCAaVSWQIJ301PTw9WVlai/OzCYCGvYaEPn+f5ZXmdACAmMQOhD59/uFBEVCqlZeUUeMvIVgAoXLEw841ioaBtFoW1tbXqZmJiAolEorp/69YtGBkZ4dChQ2jcuDF0dXXx77//4v79++jcuTMqV64MQ0NDuLu74/jx42rbdXR0VOvyJpFIsG7dOnTt2hX6+vqoVasW9u/fr3r8zTPlAQEBMDU1xZEjR1C3bl0YGhqqdhZy5eTkYOzYsTA1NYWFhQUmT54MPz8/dOnSpUjvwetevHgBX19fmJmZQV9fH+3bt8fdu3dVj0dERKBjx44wMzODgYEBnJ2dcfDgQdVz+/fvD0tLS+jp6aFWrVrw9/cvdpaKJDAwELNmzcL27dvVdqa2b9+OzZs3IzAwEGFhYdiwYQOWLFmCDRs2FLitqVOnIjExUXWLiop6r2xs74lILKV5/yGXrq4urK2tUbVqVYwcORI+Pj7Yv38/MjMzMWnSJNjZ2cHAwACenp44deqU6nm57fz+/ftRr1496OrqIjIyEgMGDECXLl0wa9Ys1WCoI0aMQFZWVoEZ3vazMjIy4OzsrNZL8P79+zAyMsL69evVsuT+f9asWbh8+bKqt0FAQAAGDRqEzz//XO3nZmdnw8rKCr///nux3rvC4jXyGhafXHCjXpz1iKj8qjf9SIGPtapjCf+BHoUqFmL/v1jwqmEBAPh44Uk8T83bsD1a0OG9M79uypQpWLJkCapXrw4zMzNERUXhs88+ww8//ABdXV1s3LgRHTt2xO3bt+Hg4FDgdmbNmoVFixZh8eLF+PXXX9G/f39ERESodaV+XVpaGpYsWYI//vgDUqkUX3zxBSZNmoTNmzcDeDUQ2ubNm+Hv74+6devil19+wd69e9GqVativ9YBAwbg7t272L9/P4yNjTF58mR89tlnuHHjBrS1tTF69GhkZWXhzJkzMDAwwI0bN2BoaAgAmDZtGm7cuIFDhw6hUqVKuHfvHtLT04udpaLYunUrhgwZgh07dsDHx0ftsW+++UZ1Vh4AGjRogIiICMyfPx9+fn75bk9XVxe6uroay8f2nojEUhb3H/T09PDs2TN89dVXuHHjBrZu3QpbW1vs2bMH7dq1w9WrV1GrVi0Ar9r5hQsXYt26dbCwsFAdyD1x4gTkcjlOnTqFR48eYeDAgbCwsMAPP/yQ789818/avHkzPD090aFDB3z++ef44osv0KZNGwwaNCjPtnr37o1r167h8OHDqpMUJiYmqF27Npo3b46YmBjY2LwaF+XAgQNIS0tD79693/t9exsW8hpmZSTX6HpEVLGV5mJh9uzZaNOmjeq+ubk5XFxcVPfnzJmDPXv2YP/+/fjqq68K3M6AAQPQt29fAK8GNlu2bBlCQ0PRrl27fNfPzs7GqlWrUKNGDQCvGurZs2erHv/1118xdepUdO3aFQCwfPly1dnx4sgt4M+ePQtvb28AwObNm2Fvb4+9e/eiZ8+eiIyMRPfu3dGgQQMAQPXq1VXPj4yMhJubG5o0aQLgVa8EerstW7Zg0KBB2Lp1q2pAu9elpaXlmYdXJpN90O6XbO+JqDQrLfsPgiDgxIkTOHLkCPr27Qt/f39ERkbC1tYWADBp0iQcPnwY/v7+mDdvHoBX7fxvv/2mtk8BvLrcbv369dDX14ezszNmz56Nb775BnPmzMnTJkRGRr7zZ7m6umLu3LkYMmQI+vTpg4iICBw4cCDf16GnpwdDQ0NoaWnB2tpatdzb2xt16tTBH3/8gW+//RYA4O/vj549e6oO6JcUFvIa5lHNHDYmcsQmZuTblQUAjOVa8KiW/5kmIqo4bsxuW+Bj0v+/Jrw4xcK/k4t/5rkocgvTXCkpKZg5cyb++usvxMTEICcnB+np6YiMjHzrdho2bKj6v4GBAYyNjREfH1/g+vr6+qoiHgBsbGxU6ycmJiIuLg4eHh6qx2UyGRo3blzsIu/mzZvQ0tKCp6enapmFhQXq1KmDmzdvAgDGjh2LkSNH4ujRo/Dx8UH37t1Vr2vkyJHo3r07wsLC8Omnn6JLly6qAwIVQUpKCu7du6e6//DhQ4SHh8Pc3BwODg6YOnUqoqOjsXHjRgCvutP7+fnhl19+gaenJ2JjYwG82onKHfSwY8eO+OGHH+Dg4ABnZ2dcunQJP/30U75nUUpKYdp7axM523si0riysP9w4MABGBoaIjs7G0qlEv369UOPHj0QEBCA2rVrq62bmZkJCwsL1X0dHR21fYNcLi4uauPseHl5ISUlBVFRUahataraulevXoVCoXjnz5o4cSL27t2L5cuX49ChQ2qPFdaQIUOwZs0afPvtt4iLi8OhQ4fw999/F3k7RcVr5DVMJpVgRsd6AICChmZKysjBz8fuQBA4AA5RRaavo1XgTa4tA/BfsVDQ3xMJXk119XqxUNA2Nc3AwEDt/qRJk7Bnzx7MmzcP//zzD8LDw9GgQYO3Xr8GANra2mr3JRLJW4vu/NYX++/pkCFD8ODBA3z55Ze4evUqmjRpoprTvH379oiIiMDXX3+NJ0+eoHXr1mqDBZZ3Fy5cgJubm2rquAkTJsDNzQ3Tp08H8GpgxdcP9qxZswY5OTkYPXo0bGxsVLdx48ap1vn111/Ro0cPjBo1CnXr1sWkSZMwfPhwzJkz54O9rsK096Z6Wkj//+tViYg0pSzsP7Rq1Qrh4eG4e/cu0tPTsWHDBqSkpEAmk+HixYsIDw9X3W7evIlffvlF9Vw9Pb33HuS2sD8rPj4ed+7cgUwmUxv7pih8fX3x4MEDBAcHY9OmTahWrRqaNWv2XvkLg4V8CWhX3wYrv2gEaxP1I2E2JnJ0avjq2onlJ+9h+d/38ns6EZHK24qF3PszOtYrFfPBnj17FgMGDEDXrl3RoEEDWFtb49GjRx80g4mJCSpXrozz58+rlikUCoSFhRV7m3Xr1kVOTg5CQkJUy549e4bbt2+jXr16qmX29vYYMWIEdu/ejYkTJ2Lt2rWqxywtLeHn54dNmzZh6dKlWLNmTbHzlDUtW7aEIAh5bgEBAQBeDSD0+kBHp06deuv6AGBkZISlS5ciIiIC6enpuH//PubOnQsdHZ0P+toKau/NDXSgqyXFrdgUDAo4L/qBJiKqeMTefzAwMEDNmjXh4OAALa1XBwPc3NygUCgQHx+PmjVrqt1e765ekMuXL6uNMXPu3DkYGhrmOwtJYX/WoEGD0KBBA2zYsAGTJ09W9bTLj46ODhSKvAdnLSws0KVLF/j7+yMgIAADBw5852vRBHatLyHt6tugTT1rhD58jvjkDFgZvTriJZNK0LRGJFaevoceTaqIHZOIyoDcYmHWnzfUBq6xNpFjRsd6aFffRsR0/6lVqxZ2796Njh07QiKRYNq0aaJMGTNmzBjMnz8fNWvWhJOTE3799Ve8ePGiUEf3r169CiMjI9V9iUQCFxcXdO7cGUOHDsXq1athZGSEKVOmwM7ODp07dwYAjB8/Hu3bt0ft2rXx4sULnDx5EnXr1gUATJ8+HY0bN4azszMyMzNx4MAB1WNU9hXU3l9/koihGy9gdKuanD6RiERR2vYfateujf79+8PX1xc//vgj3Nzc8PTpU5w4cQINGzbMd0yU12VlZWHw4MH4/vvv8ejRI8yYMQNfffVVnuvjC/uzVqxYgeDgYFy5cgX29vb466+/0L9/f5w7dy7fA8OOjo6qy8OqVKkCIyMj1SCqQ4YMweeffw6FQlHgoKuaxkK+BMmkEtUokK/r5+mAbo3sVF1fACBHoYSWjB0kiCh/bzs4WFrkXqPs7e2NSpUqYfLkyUhKSvrgOSZPnozY2Fj4+vpCJpNh2LBhaNu2LWQy2Tuf27x5c7X7MpkMOTk58Pf3x7hx4/D5558jKysLzZs3x8GDB1Xd/BUKBUaPHo3Hjx/D2NgY7dq1w88//wzg1RH8qVOn4tGjR9DT00OzZs2wdetWzb9wEk1+7X3DKqY4/U0rtvVEJKrStv/g7++PuXPnYuLEiYiOjkalSpXQtGnTPFO45ad169aoVasWmjdvjszMTPTt2xczZ84s1s+6desWvvnmG/z++++qM/q//fYbGjZsiGnTpmHhwoV5tte9e3fs3r0brVq1wsuXL+Hv748BAwYAAHx8fGBjYwNnZ2fV4HolTSKwv1ceSUlJMDExQWJiIoyNjUv85/15+QlWnb4P/wHusDLm6LZE5UlGRgYePnyIatWqQS7n77cYlEol6tati169en3Qa6iL423flw/dNpV3H/r9fJiQikEB57GgWwN4Vi/6YEpEVLFw/0HdgAED8PLlS+zdu1fsKPlKSUmBnZ0d/P390a1bt7euq6m2noeFRZaRrcD8gzdx/UkSuq8KwqOEVLEjERGVaREREVi7di3u3LmDq1evYuTIkXj48CH69esndjSqwJaduIuHCan4cn0ojl6PFTsOERFpgFKpRHx8PObMmQNTU1N06tTpg/1sFvIik2vLsGVYU1S10EfU83T0WBWEa9GJYsciIiqzpFIpAgIC4O7ujo8++ghXr17F8ePHeV06iWp+twZoU68ysnKUGLHpIradf/u0jEREVPpFRkaicuXKCAwMxPr161UD+30IvEa+FKhqYYCdI7zhtz4UN2KS0GfNOaz5sjG8a1YSOxoRUZljb2+Ps2fPih2DSI1cW4aV/Rvhf3uuYvuFx5i86yoSUrIwqmUNDoZHRPQOr89cUpo4OjqKNjMJz8iXEpZGutg6vCmaVjdHSmYOBvifx8GrMWLHIiIiIg3RkkmxsHtDjGpZAwCw+MhtzPrzBpRKDldERERFw0K+FDGWayNgoAfaOVsjS6HE5aiXYkciIg3huKJUGPyelH8SiQTftnPC9M9fze98+fFLZCk+/DSNRFQ2sF0ofzT1mbJrfSkj15ZhRf9G2HspGl3d7MSOQ0TvKXd6srS0NOjp6Ymchkq7tLQ0AP99b6j8GvRxNdiZ6cHD0VxtijoiIoD7D+WZptp6FvKlkEwqQffGVVT3M7IV2BIaCV8vx1I1ZzQRvZtMJoOpqSni4+MBAPr6+rwelvIQBAFpaWmIj4+Hqalpoea8p7KvrbO12v2toZFoU68yLAx1RUpERKUF9x/KH0239SzkSzlBEDBx+2X8dTUG5x89x8+9XaGrxR08orLE2vrVznpuY0xUEFNTU9X3hSqWraGRmLL7KtaceYCNgz1QxUxf7EhEJDLuP5RPmmrrWciXchKJBO0bWOPojVgcvBqLl2nnsfrLxjCSs9slUVkhkUhgY2MDKysrZGdnix2HSiltbW2eia/Amjiaw9ZEjgcJqei+MggbB3mijrWR2LGISETcfyh/NNnWSwSOoJBHUlISTExMkJiYCGNjY7HjAAD+vZuA4X9cQGqWAvXtjOE/wAOWRux6R0RUUZTGtqksK43vZ0xiOnx/D8Xd+BQYy7WwfoA7mjiaix2LiIg+kKK0TRy1voz4uFYlbBnWFBYGOrgWnYSeq4IQ9TxN7FhERESkITYmetgxwguNq5ohKSMH/deF4MTNOLFjERFRKVQqCvkVK1bA0dERcrkcnp6eCA0NLXDdtWvXolmzZjAzM4OZmRl8fHzeuv6IESMgkUiwdOnSEkj+YTWsYoqdI71RxUwPj56lYWDAeSg49ywREVG5Yaqvg02DPfGJkxUyc5QY/sdFRDxLFTsWERGVMqIX8tu2bcOECRMwY8YMhIWFwcXFBW3bti1wUIdTp06hb9++OHnyJIKDg2Fvb49PP/0U0dHRedbds2cPzp07B1tb25J+GR9MtUoG2DXSG672pljQrQFHsSciIipn9HRkWP1lY3RvVAUTP62DqhYGYkciIqJSRvRr5D09PeHu7o7ly5cDAJRKJezt7TFmzBhMmTLlnc9XKBQwMzPD8uXL4evrq1oeHR0NT09PHDlyBB06dMD48eMxfvz4QmUqjdfNvUkQBLUpKBLTs2GixwHwiIjKq7LQNpUlZeH9zN1Fy23vkzKyYaijBSkP4hMRlUtl5hr5rKwsXLx4ET4+PqplUqkUPj4+CA4OLtQ20tLSkJ2dDXPz/waDUSqV+PLLL/HNN9/A2dn5ndvIzMxEUlKS2q20e72Iv/EkCS0Wn8TW0EgRExEREZEmSSQSVXufkpmD/mtDMGnHZWQrlCInIyIisYlayCckJEChUKBy5cpqyytXrozY2NhCbWPy5MmwtbVVOxiwcOFCaGlpYezYsYXaxvz582FiYqK62dvbF/5FlAL7LkfjZVo2puy+ihUn74ETERAREZUvFx49x42YJOy+FI1hGy8gLStH7EhERCQi0a+Rfx8LFizA1q1bsWfPHsjlcgDAxYsX8csvvyAgIEDtrPXbTJ06FYmJiapbVFRUScbWuCntnDCqZQ0AwOIjtzHrzxtQchA8IiKicqNlHSus9W0MubYUJ28/Rf91IXiZliV2LCIiEomohXylSpUgk8kQF6c+tUpcXBysra3f+twlS5ZgwYIFOHr0KBo2bKha/s8//yA+Ph4ODg7Q0tKClpYWIiIiMHHiRDg6Oua7LV1dXRgbG6vdyhKJRIJv2zlh+uf1AAABQY8wfls4snLY9Y6IiKi8+MSpMjYP8YSJnjYuRb5Ej1XBePIyXexYREQkAlELeR0dHTRu3BgnTpxQLVMqlThx4gS8vLwKfN6iRYswZ84cHD58GE2aNFF77Msvv8SVK1cQHh6uutna2uKbb77BkSNHSuy1lAaDPq6GX/q4Qksqwf7LTzB4w3mkZrLrHRERUXnRuKo5dozwgrWxHPfiU9B9ZRDuxSeLHYuIiD4wLbEDTJgwAX5+fmjSpAk8PDywdOlSpKamYuDAgQAAX19f2NnZYf78+QBeXf8+ffp0BAYGwtHRUXUtvaGhIQwNDWFhYQELCwu1n6GtrQ1ra2vUqVPnw744EXR2tYOJnjZGbgqDIABaMo5sS0REVJ7UrmyEXaO88eXvIUjOyIGulkzsSERE9IGJXsj37t0bT58+xfTp0xEbGwtXV1ccPnxYNQBeZGQkpNL/Og6sXLkSWVlZ6NGjh9p2ZsyYgZkzZ37I6KVWyzpW2DHCC46VDNi4ExERlUN2pnrYOcIbz1IyYW+uL3YcIiL6wESfR740KgtzyxaFIAhYduIe2tW3Rh1rI7HjEBFRMZS3tkls5fH9PHYjDimZ2ejqVkXsKEREVAxFaZtEPyNPJW9LaBR+Pn4Hv//7AOsHuKOJo7nYkYiIiEiD7sYl46vAMGTmKPEsJQtDmlUXOxIREZWgMj39HBXOZw2s0biqGZIyctB/XQhO3Ix795OIiIiozKhhaYj+nlUBAHP/uokFh26BnS6JiMovFvIVgKm+DjYN9sQnTlbIzFFi2B8XseNClNixiIiISEOkUgmmfV4X37Z7NbDvqtP3MXnXFeQoOBUtEVF5xEK+gtDTkWH1l43RvVEVKJQCvtl5BatP3xc7FhEREWmIRCLBqJY1sbB7A0glwPYLjzFiUxgyshViRyMiIg1jIV+BaMukWNKzIYY3f3Xd3ILDt3AzJknkVERERKRJvd0dsOqLxtDVkuL4zThsOhchdiQiItIwDnZXwUgkEkz9rC4sDHWgr6OFujblY6ReIiIi+s+nztb4Y7Andl18jIEfVRM7DhERaRgL+QpqWPMaavfjkjJgJNeCvg6/EkREROWBRzVzeFT7b6aabIUSsYkZnHeeiKgcYNd6wovULPRbew7914XgRWqW2HGIiIhIw5RKAd/uvILOK87ictRLseMQEdF7YiFPiH6ZjoSULFyKfImeq4Px5GW62JGIiIhIg1KzcnAvPgXPU7PQd+05/HP3qdiRiIjoPbCQJ9S3M8GOEV6wNpbjXnwKuq8Mwr34ZLFjERERkYYYybWxZVhTfFTTAmlZCgwKOI/9l5+IHYuIiIqJhTwBAGpXNsKuUd6oYWmAmMQM9FgVjLDIF2LHIiIiIg0x1NXC+gHu6NDQBtkKAeO2XkLA2YdixyIiomJgIU8qdqZ62DHCG672pniZlo3+a0MQfP+Z2LGIiIhIQ3S1ZFjWxw2+XlUhCMDMP29g+d93xY5FRERFxEKe1Jgb6CBwqCda1LZEJSMd1LA0EDsSERERaZBMKsGsTs6Y0KY2dGRSuDmYiR2JiIiKiHONUR76OlpY59cECSmZsDKWix2HiIiINEwikWBs61ro6mbH6eiIiMognpGnfGnLpLAx0VPd33spGgsP34IgCCKmIiIiIk16vYi/F5+MEX9cRFJGtoiJiIioMHhGnt4p6nkavtl5GdkKAc9SMjGvawNoyXgMiIiIqLxQKgWM2hyGO3EpiHiehg2D3GFlxF55RESlFasxeid7c3380KUBpBJg+4XHGLHpItKzFGLHIiIiIg2RSiX4qZcrKhnq4mZMEnqsDEbEs1SxYxERUQFYyFOh9HK3x+ovm0BXS4rjN+Px5e8hSExj1zsiIqLyor6dCXaN9IKDuT4in6eh+8ogXItOFDsWERHlg4U8FVqbepXxx2BPGMm1cCHiBXqtDkZsYobYsYiIiEhDqloYYOdIL9SzMUZCShb6rDmHoPsJYsciIqI3sJCnIvGoZo4dI7xgZaSL23HJ2BseLXYkIiIi0iArIzm2Dm+KptXNkZKZg99O3udgt0REpQwHu6Mic7I2xq6R3thx8TGGN68udhwiIiLSMGO5NgIGeuDnY3cwqlVNSCQSsSMREdFreEaeisXeXB8T2tRWNewZ2QpcjHghcioiIiLSFLm2DFM/qwsTPW3VsqB7CTw7T0RUCrCQp/eWo1Diq8BL6LMmGPsvPxE7DhEREZWA9f8+RL91IZix/zoUShbzRERiYiFP700pAHJtKbIVAsZtvYSAsw/FjkREREQapiWTQCIBNgZHYOzWS8jM4VS0RERiYSFP701HS4plfdzg51UVggDM/PMGfjx6m13viIgIZ86cQceOHWFrawuJRIK9e/e+df3du3ejTZs2sLS0hLGxMby8vHDkyJE860VHR+OLL76AhYUF9PT00KBBA1y4cKGEXgUBgK+XI37t6wZtmQR/XYnBoIDzSMnMETsWEVGFxEKeNEIqlWBmJ2dMaFMbAPDr3/fwvz3X2PWOiKiCS01NhYuLC1asWFGo9c+cOYM2bdrg4MGDuHjxIlq1aoWOHTvi0qVLqnVevHiBjz76CNra2jh06BBu3LiBH3/8EWZmZiX1Muj/fd7QFv4DPGCgI8PZe8/Qd805JKRkih2LiKjCkQg8bZpHUlISTExMkJiYCGNjY7HjlDmbQyIwbe81KAWgj7s9FnRvKHYkIqIyrzy0TRKJBHv27EGXLl2K9DxnZ2f07t0b06dPBwBMmTIFZ8+exT///FPobWRmZiIz87+CMykpCfb29mX6/RTTlccvMdD/PJ6lZqF6JQMcHNcMcm2Z2LGIiMq0orT1PCNPGtffsyp+698Ipvra6O1uL3YcIiIqw5RKJZKTk2Fubq5atn//fjRp0gQ9e/aElZUV3NzcsHbt2rduZ/78+TAxMVHd7O3ZPr2PhlVMsWOEF+xM9dDP04FFPBHRB8ZCnkpEu/o2+OfbVnBz+K+bo5Ld7ImIqIiWLFmClJQU9OrVS7XswYMHWLlyJWrVqoUjR45g5MiRGDt2LDZs2FDgdqZOnYrExETVLSoq6kPEL9eqWxri8PhmGNKsumoZ23oiog+DhTyVGCP5f/POXnn8Eh1+/RcRz1JFTERERGVJYGAgZs2ahe3bt8PKykq1XKlUolGjRpg3bx7c3NwwbNgwDB06FKtWrSpwW7q6ujA2Nla70ft7va1PTM9Gt5VBOHo9VsREREQVAwt5KnGCIGDm/uu4GZOE7iuDcC06UexIRERUym3duhVDhgzB9u3b4ePjo/aYjY0N6tWrp7asbt26iIyM/JAR6Q2///sQ4VEvMWLTRWw7z8+CiKgksZCnEieRSLDqy8aoZ2OMhJQs9FlzDkH3E8SORUREpdSWLVswcOBAbNmyBR06dMjz+EcffYTbt2+rLbtz5w6qVq36oSJSPsZ+UhO9mlSBUgAm77qKFSfvcSpaIqISwkKePggrIzm2Dm+KptXNkZKZgwHrz+PQ1RixYxERUQlLSUlBeHg4wsPDAQAPHz5EeHi46uz51KlT4evrq1o/MDAQvr6++PHHH+Hp6YnY2FjExsYiMfG/3lxff/01zp07h3nz5uHevXsIDAzEmjVrMHr06A/62kidlkyKhd0bYmTLGgCAxUduY/aBG7xunoioBLCQpw/GWK6NgIEeaOdsjSyFEqMCw7A5JELsWEREVIIuXLgANzc3uLm5AQAmTJgANzc31VRyMTExal3i16xZg5ycHIwePRo2Njaq27hx41TruLu7Y8+ePdiyZQvq16+POXPmYOnSpejfv/+HfXGUh0QiweR2Tpj2+atLH/zPPsLX28ORlaMUORkRUfnCeeTzUR7m6i3NFEoB0/ZdQ2BIJFrVscTvfu6QSiVixyIiKtXYNmkW38+Sty88GhO3X4alkS72jv4IlY3lYkciIirVitI2aX2gTEQqMqkEP3SpjwZ2JujsassinoiIqBzq7GoHU30d2JrIWcQTEWkYu9aTKCQSCfp6OEBf59WxJEEQsP1CFDJzFCInIyIiIk1pUdsStSobqe6fvBWP6JfpIiYiIiofWMhTqfDr3/fw7c4rGBRwHimZOWLHISIiIg079+AZhv9xEd1/C8KduGSx4xARlWks5KlUaORgBgMdGc7ee4Y+a4KRkJIpdiQiIiLSoKoW+qhqoY/YpAz0XBWMixHPxY5ERFRmsZCnUuHjWpWwZVhTWBjo4Fp0EnqsDELU8zSxYxEREZGG2JjoYccILzRyMEViejb6rwvB37fixI5FRFQmsZCnUqNhFVPsGOEFO1M9PHqWhm4rg3AzJknsWERERKQhpvo62DykKVrVsURGthJDN17EzouPxY5FRFTmsJCnUqW6pSF2j/KGk7URniZnot/ac0jKyBY7FhEREWmIno4Ma3yboFsjOyiUAibtuIygewlixyIiKlM4/RyVOpWN5dg23AtDN1xAj8ZVYCzXFjsSERERaZC2TIolPVxQyVAXcUkZaFrdQuxIRERlCgt5KpVM9LSxZVhTyF6bYz4tK0c1XR0RERGVbVKpBP/7rC4USgHS/2/vs3KUkEheFfpERFQw/pWkUuv1Ij4hJRMdlv2LFSfvQRAEEVMRERGRJuW290qlgAnbwzFs4wWkZXEqWiKit2EhT2XCoWuxeJiQisVHbmPWnzegVLKYJyIiKk/uxCfj+M04nLz9FP3XheBlWpbYkYiISi0W8lQmfNm0KqZ/Xg8AEBD0COO3hSMrRylyKiIiItIUJ2tjbB7iCRM9bVyKfImeq4IRk5gudiwiolKJhTyVGYM+roZf+rhCSyrB/stPMHjDeaRmsusdERFRedG4qjl2jPCCtbEcd+NT0P23INyLTxY7FhFRqcNCnsqUzq52WOfXBHraMvxzNwH91oXgeSq73hEREZUXtSsbYdcob1S3NMCTxAz0WBWMS5EvxI5FRFSqsJCnMqdlHSsEDvWEmb42EtOyoOTgd0REROWKnakedo7whou9KdKyFMjk5XRERGo4lxeVSW4OZtgxwhu6WlJUMtQVOw4RERFpmLmBDgKHeOJadCI8Oc88EZEanpGnMqumlSHszfVV93eHPcaFR89FTERERESaZKCrpVbE345NxoagR+IFIiIqJXhGnsqFoPsJmLTjMrRlUvzWvxFa160sdiQiIiLSoJdpWfBdH4K4pEzEJGZgcrs6kEgkYsciIhIFz8hTueBmb4YWtS2RmaPEsD8uYseFKLEjERERkQaZ6GnD18sRALDq9H1M3nUFOQpeO09EFRMLeSoX9HRkWOPbBN0a2UGhFPDNzitYffq+2LGIiIhIQyQSCUa3qokF3RpAKgG2X3iMEZvCkJGtEDsaEdEHx0Keyg1tmRQ/9nTB8ObVAQDzD93CD3/dgFLJUe2JiIjKiz4eDlj5RWPoaElx/GYcvvw9BIlp2WLHIiL6oFjIU7kikUgw9bO6+N9nTgCAtf88xNEbcSKnIiIiIk1q62yNPwZ5wEiuhfOPXmDRkVtiRyIi+qA42B2VS8Oa14CFgS6uRieirTMHviMiIipvPKtbYPtwLyw+chtT2juJHYeI6INiIU/lVvfGVdC9cRXV/bSsHGTlKGGqryNiKiIiItKUujbGWD/AXW1ZTGI6bEz0REpERPRhsGs9VQjZCiVGbQ5Dz1XBiElMFzsOERERlYDVp+/D58fT+PdugthRiIhKFAt5qhDikjJwKyYZd+NT0P23INyLTxY7EhEREWmQQingn7sJSM1SYGBAKP68/ETsSEREJYaFPFUIVcz0sWuUN6pbGuBJYgZ6rArGpcgXYsciIiIiDZFJJfh9QBN0aGCDbIWAsVsvYUPQI7FjERGViFJRyK9YsQKOjo6Qy+Xw9PREaGhogeuuXbsWzZo1g5mZGczMzODj46O2fnZ2NiZPnowGDRrAwMAAtra28PX1xZMnPCpb0dmZ6mHnCG+42JviZVo2+q0Nwcnb8WLHIiIiIg3R1ZJhWV83fNm0KgQBmLH/On46ehuCwKloiah8Eb2Q37ZtGyZMmIAZM2YgLCwMLi4uaNu2LeLj8y+wTp06hb59++LkyZMIDg6Gvb09Pv30U0RHRwMA0tLSEBYWhmnTpiEsLAy7d+/G7du30alTpw/5sqiUMjfQQeAQTzSvbYn0bAWGbrjArndERETliEwqwezOzhjvUwsAsOzve5i275rIqYiINEsiiHyI0tPTE+7u7li+fDkAQKlUwt7eHmPGjMGUKVPe+XyFQgEzMzMsX74cvr6++a5z/vx5eHh4ICIiAg4ODu/cZlJSEkxMTJCYmAhjY+OivSAqE7JylJi04zJO3IzD1mFeaFDFROxIRERvxbZJs/h+VgybzkVg2r5rmNXJGb5ejmLHISJ6q6K0TaJOP5eVlYWLFy9i6tSpqmVSqRQ+Pj4IDg4u1DbS0tKQnZ0Nc3PzAtdJTEyERCKBqalpvo9nZmYiMzNTdT8pKalwL4DKLB0tKZb2dsWDhBTUtDISOw4RERGVgC+aVoVnNXPUqsy2nojKF1G71ickJEChUKBy5cpqyytXrozY2NhCbWPy5MmwtbWFj49Pvo9nZGRg8uTJ6Nu3b4FHNebPnw8TExPVzd7evmgvhMokqVSiVsRfinyBaXuvIUehFDEVERERadLrRfzLtCyM3XIJ8ckZIiYiInp/ol8j/z4WLFiArVu3Ys+ePZDL5Xkez87ORq9evSAIAlauXFngdqZOnYrExETVLSoqqiRjUymUmpmDoRsv4I9zERixKQwZ2QqxIxEREZGGfbvzCvZffoIeK4MR8SxV7DhERMUmaiFfqVIlyGQyxMXFqS2Pi4uDtbX1W5+7ZMkSLFiwAEePHkXDhg3zPJ5bxEdERODYsWNvvcZAV1cXxsbGajeqWAx0tfBD1wbQ0ZLi+M04fPl7CBLTs8WORURERBr0v8/qwsFcH5HP09B9ZTCuRSeKHYmIqFhELeR1dHTQuHFjnDhxQrVMqVTixIkT8PLyKvB5ixYtwpw5c3D48GE0adIkz+O5Rfzdu3dx/PhxWFhYlEh+Kl/aOlvjj0EeMJJr4fyjF+i9OhhxSex6R0REVF44VjLAzpFeqGtjjISUTPRZcw7B95+JHYuIqMhE71o/YcIErF27Fhs2bMDNmzcxcuRIpKamYuDAgQAAX19ftcHwFi5ciGnTpmH9+vVwdHREbGwsYmNjkZKSAuBVEd+jRw9cuHABmzdvhkKhUK2TlZUlymukssOzugW2D/eCpZEubsUmo/vKIDx4miJ2LCIiItIQKyM5tg1vCs9q5kjJzIHf+lAcvhYjdiwioiIRvZDv3bs3lixZgunTp8PV1RXh4eE4fPiwagC8yMhIxMT898d15cqVyMrKQo8ePWBjY6O6LVmyBAAQHR2N/fv34/Hjx3B1dVVbJygoSJTXSGVLXRtj7B7pjWqVDPD4RTpWnrovdiQiIiLSIGO5NjYM8kA7Z2tkKZSYd/AWMnM4Pg4RlR2izyNfGnFuWQKAhJRMLD1+B993qAe5tkzsOERUwbFt0iy+nwQACqWAhYdvoY+7PapbGoodh4gquKK0TaKfkScqrSoZ6mJulwaqIl4QBFyKfCFyKiIiItIUmVSC/31WV62Ivxz1Egolz3MRUenGQp6okJYcvY1uK4OwIeiR2FGIiIioBJy58xQ9VgVh7NZL7GpPRKUaC3miQhAEAckZORAEYMb+6/jp6G3wqhQiIqLyJSnj1dSzf12JwaCA80jJzBE5ERFR/ljIExWCRCLBrE7O+NqnNgBg2d/38L8919j1joiIqBz5vKEt1g9wh76ODGfvPUPfNeeQkJIpdiwiojxYyBMVkkQiwTifWpjbpT4kEmBLaCRGbw5DRja73hEREZUXzWpZYuuwpjA30MHV6ET0XBWMqOdpYsciIlLDQp6oiL5oWhW/9WsEHZkUh6/HYujGC+xmT0REVI40rGKKnSO8YGeqh4cJqei+MgixiRlixyIiUmEhT1QM7RvYIGCQO4zkWujWyA4SiUTsSERERKRB1S0NsXuUN5ysjdC8tiUqG+uKHYmISEVL7ABEZZV3jUo4800rmBnoqJYJgsCinoiIqJyobCzH9hFe0NeWqdp3tvVEVBrwjDzRe3i9iI9PykD3lUG4Fp0oYiIiIiLSJGO5NrRkr3aZFUoBXwVewvbzUSKnIqKKjoU8kYbMP3QLYZEv0WfNOQTdTxA7DhEREWnYvvBo/HU1Bt/uuoIVJ+9xjBwiEg0LeSINmdXZGU2rmyMlMwcD1p/HoasxYkciIioVzpw5g44dO8LW1hYSiQR79+596/q7d+9GmzZtYGlpCWNjY3h5eeHIkSMFrr9gwQJIJBKMHz9es8GJ3tDVzQ4jW9YAACw+chuzD9yAklPREpEIWMgTaYixXBsBAz3QztkaWQolRgWGYXNIhNixiIhEl5qaChcXF6xYsaJQ6585cwZt2rTBwYMHcfHiRbRq1QodO3bEpUuX8qx7/vx5rF69Gg0bNtR0bKI8JBIJJrdzwrTP6wEA/M8+wtfbw5GVoxQ5GRFVNBzsjkiD5NoyrOjfCN/vvYYtoZH4bs81JCRnYWzrmhwYh4gqrPbt26N9+/aFXn/p0qVq9+fNm4d9+/bhzz//hJubm2p5SkoK+vfvj7Vr12Lu3Lmaikv0ToM/rgYLAx1M2nEZ+8Kf4EVaNlb2bwQDXe5aE9GHwTPyRBomk0owr2t9jP2kJgBg/+VopGYpRE5FRFR2KZVKJCcnw9zcXG356NGj0aFDB/j4+LxzG5mZmUhKSlK7Eb2PLm52WOfXBHraMoQ+fIYHT1PFjkREFQgPGxKVAIlEggmf1oGdmR4+rmUJQx6hJyIqtiVLliAlJQW9evVSLdu6dSvCwsJw/vz5Qm1j/vz5mDVrVklFpAqqZR0rBA71xMu0bDSoYiJ2HCKqQHhGnqgE9XZ3gJ2pnur+8RtxSMnMETEREVHZEhgYiFmzZmH79u2wsrICAERFRWHcuHHYvHkz5HJ5obYzdepUJCYmqm5RUZw+jDTDzcEMrZysVPdvPEnCnbhkERMRUUXA04REH8jhazEYuTkM9W1N4D/QHZUMdcWORERUqm3duhVDhgzBjh071LrPX7x4EfHx8WjUqJFqmUKhwJkzZ7B8+XJkZmZCJpOpbUtXVxe6uvy7SyUr8lkafNeHIluhxPoBTdC4qvm7n0REVAw8I0/0gdia6sFcXwdXoxPRc1Uwop6niR2JiKjU2rJlCwYOHIgtW7agQ4cOao+1bt0aV69eRXh4uOrWpEkT9O/fH+Hh4XmKeKIPxVhPCw7mekhMz0b/dSH4+1ac2JGIqJxiIU/0gTSsYoodI7xgZ6qHhwmp6L4yCDdjONgSEZV/KSkpqoIbAB4+fIjw8HBERkYCeNXt3dfXV7V+YGAgfH198eOPP8LT0xOxsbGIjY1FYmIiAMDIyAj169dXuxkYGMDCwgL169f/4K+PKJepvg42D2mKVnUskZGtxNCNF7Hr4mOxYxFROcRCnugDqm5piN2jvOFkbYT45Ez0Wh2M0IfPxY5FRFSiLly4ADc3N9XUcRMmTICbmxumT58OAIiJiVEV9QCwZs0a5OTkYPTo0bCxsVHdxo0bJ0p+oqLQ05FhjW8TdGtkB4VSwMQdl7HmzH2xYxFROSMRBEEQO0Rpk5SUBBMTEyQmJsLY2FjsOFQOJaZnY8iG8zj/6AV0taQ4NK4Zqlsaih2LiEoxtk2axfeTSppSKWDB4VtYc+YBAGBh9wbo7e4gcioiKs2K0jZxsDsiEZjoaeOPwZ74KvAS7M31UK2SgdiRiIiISIOkUgn+91ldWBjo4OC1WHRoaCt2JCIqR1jIE4lEri3Dqi8aQSqRQCKRAAAycxTQkUlV94mIiKhsG96iBgZ+VA06Wq+uaBUEAVkKJXS1OCgjERUfr5EnEpGWTAqp9L8ifnDABcw+cANKJa94ISIiKi9yi3gAWHHyHvquOYeXaVkiJiKiso6FPFEpEXTvGf69lwD/s4/w9fZwZOUoxY5EREREGvQsJRNr/3mIsMiX6LkqGDGJ6WJHIqIyioU8USnRyskKS3u7Qksqwb7wJxiy8QJSM3PEjkVEREQaYmGoi+3DvVDZWBd341PQ/bcg3ItPFjsWEZVBLOSJSpEubnZY59cEetoynLnzFP3WheB5KrveERERlRd1rI2wa6Q3qlsa4EliBnqsCsalyBdixyKiMoaFPFEp07KOFQKHesJMXxuXo16ix6ogPH6RJnYsIiIi0pAqZvrYOcIbLvameJmWjX5rQ3DqdrzYsYioDGEhT1QKuTmYYccIb9iayPE0KRNJ6exiT0REVJ6YG+ggcIgnmte2RHq2Ak9eZogdiYjKEE4/R1RK1bQyxK5R3njyMgP1bI3FjkNEREQaZqCrhXW+TXDydjzaOluLHYeIyhCekScqxWxM9NC4qpnq/oVHz/H3rTgRExEREZEm6WhJ1Yr456lZWPfPAwgCp6IlooLxjDxRGRHxLBWDAs4jNUuBhd0bokfjKmJHIiIiIg3KUSgxMOA8Lke9xJ24ZMzr2gBaMp53I6K8+JeBqIywNdWDT73KUCgFTNpxGatP3xc7EhEREWmQlkyKvu72kEqA7RceY8SmMGRkK8SORUSlEAt5ojJCWybFkh4uGNa8OgBg/qFbmHfwJpRKdr0jIiIqL/p4OGDlF42hoyXF8Ztx8P09FInp2WLHIqJShoU8URkilUrwv8/qYmp7JwDAmjMPMGnnZWQrlCInIyIiIk1p62yNjYM8YKSrhdBHz9F7dTDikjiqPRH9h4U8URk0vEUNLOnpAplUgt1h0Vj3z0OxIxEREZEGNa1ugW3DvWBppItbscmYsD1c7EhEVIpwsDuiMqpH4yow09fG5pBIDPzIUew4REREpGH1bI2xa4Q3Ju4Ix7yuDcSOQ0SlCAt5ojKsdd3K+MTJChKJBACgVAp4mZ4NcwMdkZMRERGRJjhY6GP7cC9VWw8Az1IyYWGoK2IqIhIbu9YTlXGvN+wLDt/C58v+wb34FBETERERkSa93tb/fSsOHy88iQNXnoiYiIjExkKeqJxIzsjG8ZtxeJKYgZ6rgnAp8oXYkYiIiEjD/roSi/RsBcZsuYSNwY/EjkNEImEhT1ROGMm1sXOEN1yqmOBFWjb6rQ3B6TtPxY5FREREGrSoR0N82bQqBAGYvu86fjp2B4LAqWiJKhoW8kTliLmBDgKHNkWzWpWQnq3A4IDz2BceLXYsIiqj/P39kZaWJnYMInqNTCrB7M7OGO9TCwCw7MRdfLf3GhRKFvNEFQkLeaJyxkBXC7/7uaOTiy1ylALGbQ1HwFlOT0dERTdlyhRYW1tj8ODBCAoKEjsOEf0/iUSC8T61MbdLfUgkQGBIJL4KDEO2Qil2NCL6QFjIE5VDOlpSLO3tigHejpBIgEpGHNmWiIouOjoaGzZsQEJCAlq2bAknJycsXLgQsbGxYkcjIgBfNK2KFf0aQUcmham+NrSkknc/iYjKBYnAi2rySEpKgomJCRITE2FsbCx2HKJiEwQB16KT0KCKidhRiOg9id02xcXFYdOmTdiwYQNu3bqFdu3aYfDgwejYsSOk0rJ3XkDs95NIk64/SYSTtTFkLOSJyrSitE1lr+UlokKTSCRqRXxMYjq+23MVGdkKEVMRUVlUuXJlfPzxx/Dy8oJUKsXVq1fh5+eHGjVq4NSpU2LHI6rQnG1NVEV8tkKJaXuvIeJZqsipiKgksZAnqiCUSgHDNl7E5pBI+P4eisT0bLEjEVEZEBcXhyVLlsDZ2RktW7ZEUlISDhw4gIcPHyI6Ohq9evWCn5+f2DGJ6P/9dOwO/jgXge4rg3EtOlHsOERUQljIE1UQUqkE33eoCyO5FkIfPUfv1cGIS8oQOxYRlWIdO3aEvb09AgICMHToUERHR2PLli3w8fEBABgYGGDixImIiooSOSkR5Rr4kSPq2hgjISUTfdacQ/D9Z2JHIqISwEKeqALxrG6B7cO9YGmki1uxyei+MggPnqaIHYuISikrKyucPn0a165dw/jx42Fubp5nHUtLSzx8yJkxiEoLKyM5tg1vCs9q5kjJzIHf+lAcvhYjdiwi0jAW8kQVTF0bY+we6Q1HC308fpGOnquCcfUxu94RUV4tWrRAo0aN8izPysrCxo0bAbwai6Nq1aofOhoRvYWxXBsbBnmgrXNlZCmUGLU5DIEhkWLHIiINYiFPVAHZm+tj50hv1LczxrPULMzYfw2cwIKI3jRw4EAkJuY90JecnIyBAweKkIiICkuuLcNv/Rujr4c9lAIw968biOcldUTlhpbYAYhIHJUMdbFlaFPM3H8D37StA4mEU9YQkTpBEPL92/D48WOYmHBaS6LSTiaVYF7XBrA0kqNJVTNYGcvFjkREGsJCnqgCM5Jr48deLmrLbsUmwcmacyoTVWRubm6QSCSQSCRo3bo1tLT+211QKBR4+PAh2rVrJ2JCIiosiUSCCW1qqy27/zQFVcz0oKslEykVEb0vFvJEpLL/8hOM23oJYz6pha99avEsPVEF1aVLFwBAeHg42rZtC0NDQ9VjOjo6cHR0RPfu3UVKR0Tv48HTFPRcFYx6NsZY9WVjGOqyHCAqi/ibS0Qqkc9SIQjAshN3kZCSiTmd60MmZTFPVNHMmDEDAODo6IjevXtDLmd3XKLyIjYpAxnZCvx7LwH91p6D/wB3WBjqih2LiIqIg90RkcpXn9TC3C71IZEAgSGRGL05DBnZCrFjEZFI/Pz8WMQTlTPeNSphy9CmMDfQwZXHieixKhhRz9PEjkVERcRCnojUfNG0Klb0awQdmRSHr8digH8okjKyxY5FRB+Iubk5EhISAABmZmYwNzcv8EZEZZOLvSl2jPCCnakeHiakovvKINyMSRI7FhEVAbvWE1EenzWwgameNob9cRHnHjxHn9XnsHOkF/R1+CeDqLz7+eefYWRkpPo/x8ogKp9qWBpi10hv+K0Pxe24ZPRaHYytw5rC2ZYzUhCVBdwrJ6J8edeshK3DmmKAfyia1arEIp6ogvDz81P9f8CAAeIFIaISZ20ix/bhXhiy8TwUSgHVKxm++0lEVCqwaz0RFai+nQkOjm2GKe2dxI5CRCIICAjId3lOTg6mTp36YcMQUYkw0dfGH4M94T/AA3o6nI6OqKxgIU9Eb2VlLFd1rc3IVmDIhgsIvv9M5FRE9CGMHTsWPXv2xIsXL1TLbt++DU9PT2zZskXEZESkSXJtGUz0tVX3fz52B7+dugdBEERMRURvU6xCPioqCo8fP1bdDw0Nxfjx47FmzZpihVixYgUcHR0hl8vh6emJ0NDQAtddu3YtmjVrBjMzM5iZmcHHxyfP+oIgYPr06bCxsYGenh58fHxw9+7dYmUjov+sPv0Ax2/GwW99KA5fixE7DhGVsEuXLuHx48do0KABjh07hhUrVqBRo0ZwcnLC5cuXxY5HRCUgLPIFfjlxF4sO38acAzehVLKYJyqNilXI9+vXDydPngQAxMbGok2bNggNDcV3332H2bNnF2lb27Ztw4QJEzBjxgyEhYXBxcUFbdu2RXx8fL7rnzp1Cn379sXJkycRHBwMe3t7fPrpp4iOjlats2jRIixbtgyrVq1CSEgIDAwM0LZtW2RkZBTn5RLR/xveojraOVsjS6HEqM1hCAyJFDsSEZWgGjVq4OzZs+jWrRvatWuHr7/+GuvWrcPmzZthYsIBsYjKo0YOZpj2eT0AwPqzDzFheziycpQipyKiN0mEYvSZMTMzw7lz51CnTh0sW7YM27Ztw9mzZ3H06FGMGDECDx48KPS2PD094e7ujuXLlwMAlEol7O3tMWbMGEyZMuWdz1coFDAzM8Py5cvh6+sLQRBga2uLiRMnYtKkSQCAxMREVK5cGQEBAejTp887t5mUlAQTExMkJibC2Ni40K+FqCJQKAV8v/catoS+KuIntKmNMZ/U5MjWRCVMrLbpzz//xODBg1G7dm3cuXMHDRs2xMaNG2Fra/vBMpQEtvVEb7f3UjQm7biMHKWA5rUtseqLRhz4lqiEFaVtKtYZ+ezsbOjq6gIAjh8/jk6dOgEAnJycEBNT+O62WVlZuHjxInx8fP4LJJXCx8cHwcHBhdpGWloasrOzVfPZPnz4ELGxsWrbNDExgaenZ4HbzMzMRFJSktqNiPInk0owr2t9jP2kJgDgp2N3MGP/dXa9IyqHhg8fjp49e2Ly5Mn4559/cOXKFejo6KBBgwbYvn272PGIqAR1cbPDOr8m0NOW4cydp+i3NgQvUrPEjkVE/69YhbyzszNWrVqFf/75B8eOHUO7du0AAE+ePIGFhUWht5OQkACFQoHKlSurLa9cuTJiY2MLtY3JkyfD1tZWVbjnPq8o25w/fz5MTExUN3t7+0K/BqKKSCKRYMKndTCrkzMkEmBf+BNEv0wXOxYRadjZs2cREhKCiRMnQiKRwNraGgcPHsTs2bMxaNAgseMRUQlrWccKgUM9YaqvjfColwjiYLdEpUax+scsXLgQXbt2xeLFi+Hn5wcXFxcAwP79++Hh4aHRgG+zYMECbN26FadOnYJcLi/2dqZOnYoJEyao7iclJbGYJyoEP29HmBvowNZUDntzfbHjEJGGXbx4UdUD73WjR49W6/lGROWXm4MZdo7wwrkHz9GhoY3YcYjo/xWrkG/ZsiUSEhKQlJQEMzMz1fJhw4ZBX7/wO/OVKlWCTCZDXFyc2vK4uDhYW1u/9blLlizBggULcPz4cTRs2FC1PPd5cXFxsLH5749NXFwcXF1d892Wrq5uvjsqRPRuHV3Ur5MNi3wBB3N9VDLk7xRRWaerq4v79+/D398f9+/fxy+//AIrKyscOnQIDg4OYscjog+kppURaloZqe4/Tc7E4xdpcHMwe8uziKgkFatrfXp6OjIzM1VFfEREBJYuXYrbt2/Dysqq0NvR0dFB48aNceLECdUypVKJEydOwMvLq8DnLVq0CHPmzMHhw4fRpEkTtceqVasGa2trtW0mJSUhJCTkrdskovd3/Uki/H4PRc9VwYh6niZ2HCJ6T6dPn0aDBg0QEhKC3bt3IyUlBQBw+fJlzJgxQ+R0RCSG5IxsDPAPRd+15/D3rbh3P4GISkSxCvnOnTtj48aNAICXL1/C09MTP/74I7p06YKVK1cWaVsTJkzA2rVrsWHDBty8eRMjR45EamoqBg4cCADw9fXF1KlTVesvXLgQ06ZNw/r16+Ho6IjY2FjExsaqdi4kEgnGjx+PuXPnYv/+/bh69Sp8fX1ha2uLLl26FOflElEh6WnLYKynjYcJqei+Mgg3YzhwJFFZNmXKFMydOxfHjh2Djo6Oavknn3yCc+fOiZiMiMSiJZXCykgXGdlKDN14EbsuPhY7ElGFVKxCPiwsDM2aNQMA7Ny5E5UrV0ZERAQ2btyIZcuWFWlbvXv3xpIlSzB9+nS4uroiPDwchw8fVg1WFxkZqTYS/sqVK5GVlYUePXrAxsZGdVuyZIlqnW+//RZjxozBsGHD4O7ujpSUFBw+fPi9rqMnonerbmmI3aO84WRthPjkTPRaHYzQh8/FjkVExXT16lV07do1z3IrKyskJCQUejtnzpxBx44dYWtrC4lEgr179751/d27d6NNmzawtLSEsbExvLy8cOTIEbV15s+fD3d3dxgZGcHKygpdunTB7du3C52JiIpHT0eGNb5N0K2RHRRKARN3XMaaM/fFjkVU4RSrkE9LS4OR0avrZI4ePYpu3bpBKpWiadOmiIiIKPL2vvrqK0RERCAzMxMhISHw9PRUPXbq1CkEBASo7j969AiCIOS5zZw5U7WORCLB7NmzERsbi4yMDBw/fhy1a9cuzksloiKqbCzHtuFecHc0Q3JGDr78PQTHbrDrHVFZZGpqmu+0spcuXYKdnV2ht5OamgoXFxesWLGiUOufOXMGbdq0wcGDB3Hx4kW0atUKHTt2xKVLl1TrnD59GqNHj8a5c+dw7NgxZGdn49NPP0VqamqhcxFR8WjLpFjSwwXDmlcHAMw7eAvzD96EIHAqWqIPRSIU4zeuYcOGGDJkCLp27Yr69evj8OHD8PLywsWLF9GhQ4dCTx1XWiUlJcHExASJiYkwNjYWOw5RmZSRrcBXgZdw/GYcpBJg02BPeNesJHYsojJLjLZp0qRJCAkJwY4dO1C7dm2EhYUhLi4Ovr6+8PX1LdZ18hKJBHv27Cny5W7Ozs7o3bs3pk+fnu/jT58+hZWVFU6fPo3mzZu/c3ts64k0Y/Xp+5h/6BYA4KtWNTGpbR2RExGVXUVpm4p1Rn769OmYNGkSHB0d4eHhoRpE7ujRo3BzcyvOJomonJFry7Dqi0bo1aQKPqpZCU0czcWORERFNG/ePDg5OcHe3h4pKSmoV68emjdvDm9vb3z//fcfLIdSqURycjLMzQv+O5KYmAgABa6TmZmJpKQktRsRvb/hLWpgSU8X2Jnqobc7p28m+lCKdUYeAGJjYxETEwMXFxdIpa+OB4SGhsLY2BhOTk4aDfmh8Sg9keYIgoDMHCXk2jIAgFL56k+OVCoRMxZRmSNm2xQZGYlr164hJSUFbm5uqFWrVrG3VZwz8osWLcKCBQtw69atfGfHUSqV6NSpE16+fIl///03323MnDkTs2bNyrOcbT2RZmRkK1RtPQDkKJTQkhXrnCFRhVWUtr5Y88gDr+Zrt7a2xuPHr0aqrFKlCjw8PIq7OSIqpyQSiaphFwQBsw/cwIu0LCzu4QIdLTbwRGWBg4ODaPPGBwYGYtasWdi3b1+BU9yOHj0a165dK7CIB4CpU6diwoQJqvtJSUmwt+fZQyJNeb2IP3I9Fj8fuwP/ge6wMdETMRVR+VWsQl6pVGLu3Ln48ccfVdO+GRkZYeLEifjuu+9UZ+iJiF53Jy4Fm85FIEcp4EVaNlb2bwQD3WIfTySiEvB6sfsuP/30UwkmAbZu3YohQ4Zgx44d8PHxyXedr776CgcOHMCZM2dQpUqVArelq6sLXV3dkopKRP8vK0eJeQdvIuJZGrr/FoSNgz1R08pQ7FhE5U6x9qC/++47/P7771iwYAE++ugjAMC///6LmTNnIiMjAz/88INGQxJR+VDH2gjr/Jpg5KYwnLnzFP3WhcB/gDvMDXTe/WQi+iBeHxn+bSSSkr08ZsuWLRg0aBC2bt2KDh065HlcEASMGTMGe/bswalTp1CtWrUSzUNEhaOjJcXmIZ7w/T0UDxJS0XNVENYPcIebg5nY0YjKlWJdI29ra4tVq1ahU6dOasv37duHUaNGITo6WmMBxcBr5IlK1qXIFxgYcB4v07JR3dIAGwd5oIqZvtixiEq1stw2paSk4N69ewAANzc3/PTTT2jVqhXMzc3h4OCAqVOnIjo6Ghs3bgTwqju9n58ffvnlF3Tr1k21HT09PZiYmAAARo0ahcDAQOzbtw916vw3SraJiQn09N7dlbcsv59EZcGzlEwMCjiPy48Toactw6ovG6NFbUuxYxGVaiU+av3z58/zHdDOyckJz58/L84miagCcXMww84RXrA1kePB01T0WBmMO3HJYscioreIiopCVFRUsZ574cIFuLm5qWa2mTBhAtzc3FRTycXExCAyMlK1/po1a5CTk4PRo0fDxsZGdRs3bpxqnZUrVyIxMREtW7ZUW2fbtm3v8SqJSFMsDHUROLQpmtWqhPRsBQYHnMe+8LJ9so+oNClWIe/i4oLly5fnWb58+XI0bNjwvUMRUflX08oIO0d6o6aVIeKTM/DgaarYkYjoDTk5OZg2bRpMTEzg6OgIR0dHmJiY4Pvvv0d2dnaht9OyZUsIgpDnFhAQAAAICAjAqVOnVOufOnXqresDyPdxQRAwYMAAzbx4InpvBrpa+N3PHR1dbJGjFBAW8ULsSETlRrGukV+0aBE6dOiA48ePq+aQDw4ORlRUFA4ePKjRgERUftma6mHnCC+ce/Ac7epbix2HiN4wZswY7N69G4sWLVJr72fOnIlnz55h5cqVIickotJOR0uKX3q7onmtSujWqOABKYmoaIo9j/yTJ0+wYsUK3Lp1CwBQt25dDBs2DHPnzsWaNWs0GvJD43VzROJ5/CINYZEv0cnFVuwoRKWKGG2TiYkJtm7divbt26stP3jwIPr27YvExMQPkqMksK0nEk9WjhKbQyLwZdOqnGue6DUfZB55W1vbPKPTX758Gb///nuZL+SJSByJ6dnwXR+KB09TEZuYjmHNa4gdiahC09XVhaOjY57l1apVg44OZ5sgouKZsvsKdodFI/j+Myzr66Y2Bz0RFQ4PgRFRqWGkq4XWTlYAgHkHb2H+wZsoZqchItKAr776CnPmzEFmZqZqWWZmJn744Qd89dVXIiYjorKsrbM1dLSkOHojDr7rQ5GYXvgxN4jolWKfkSci0jSpVILvOtRDJUNdzD90C6vPPEBCShYWdG8AbXa9I/rgLl26hBMnTqBKlSpwcXEB8Kr3XVZWFlq3bq02Ndzu3bvFiklEZUxbZ2tsHOSBoRsuIPThc/ReHYyNgzxgZSwXOxpRmcFCnohKneEtasDcQAdTdl/FrrDHeJGWhRX9GkFPh13viD4kU1NTdO/eXW2Zvb29SGmIqDxpWt0C24Z7wc8/FLdik9FtZRD+GOyJapUMxI5GVCYUabC714+85+fly5c4ffo0FArFewcTEwfAISodjt+Iw+jAMGTmKNHXwx7zu3F6S6q4PnTbJAgCoqKiYGlpCT09vRL/eR8a23qi0iHyWRp814fg0bM0VLXQx/EJLdgLjyqsorRNRfotMTExeeutatWq8PX1fa/wRES5fOpVxuYhnmhYxQRft6ktdhyiCkUQBNSsWROPHz8WOwoRlWMOFvrYMcIbjRxMMb8rL6UjKqwida339/cvqRxERPlq4miOfaM/gkQiUS1LTM+GiZ62iKmIyj+pVIpatWrh2bNnqFWrlthxiKgcszTSxa6R3mzriYqAh7yIqNR7vWHfdfExWi05hUuRL0RMRFQxLFiwAN988w2uXbsmdhQiKudeb+vvxiWj1ZJT2Bj8SLxARKUcC3kiKjOUSgGBoZF4npqFfmtDcPrOU7EjEZVrvr6+CA0NhYuLC/T09GBubq52IyIqCX9eicHz1CxM33cdPx27w6loifLBUeuJqMyQSiXYOMgDIzZdxD93EzA44Dx+7OWCzq52YkcjKpeWLl0qdgQiqoC+9qkFqQRYevwulp24i4SUTMzpXB8yqeTdTyaqIIo0an1FwZFsiUq3rBwlJu24jP2XnwAApn9eD4M+riZyKqKSxbZJs/h+EpV+m85FYNq+axAEoH19a/zc2xVybU5FS+VXiY1aT0RUGuhoSbG0tysGeDsCAGYfuIFFh2+x6x1RCbh//z6+//579O3bF/Hx8QCAQ4cO4fr16yInI6Ly7oumVbGiXyPoyKQ4dC0WA/xDkZyRLXYsolKBhTwRlUlSqQQzOtbDN23rAABkUonaQDlE9P5Onz6NBg0aICQkBLt370ZKSgoA4PLly5gxY4bI6YioIvisgQ0CBrrDUFcLSgGcno7o//EaeSIqsyQSCUa3qgl3R3O4O5qJHYeo3JkyZQrmzp2LCRMmwMjISLX8k08+wfLly0VMRkQViXfNStgxwgu2pnrsWk/0/3hIi4jKPI9q5qqz8elZCsz68zoS09n1juh9Xb16FV27ds2z3MrKCgkJCSIkIqKKqq6Nsdq88itO3sO16EQRExGJi4U8EZUrU3dfgf/ZR+i9OhhxSRlixyEq00xNTRETE5Nn+aVLl2Bnx9kiiEgcuy4+xuIjt9FnzTkE338mdhwiUbCQJ6JyZVjzGrA00sWt2GR0XxmEB09TxI5EVGb16dMHkydPRmxsLCQSCZRKJc6ePYtJkybB19dX7HhEVEG1ca4Mj2rmSMnMgd/6UBy+lveAI1F5x0KeiMqVerbG2DXCG44W+nj8Ih09VwXj6mN2vSMqjnnz5qFu3bpwcHBASkoK6tWrh+bNm8Pb2xvff/+92PGIqIIylmtj4yAPtHWujCyFEqM2hyEwJFLsWEQfFAt5Iip3HCz0sXOkN+rbGeNZahb6rAnGv3d5PS9RYSmVSixcuBCtWrXCpUuX8OWXX+LAgQPYtGkTbt26hT/++AMyGQecIiLxyLVl+K1/Y/T1sIdSAP635yqWnbjLqWipwmAhT0TlUiVDXWwZ2hTeNSyQmqXAxB3hyMhWiB2LqEz44Ycf8L///Q+Ghoaws7NDYGAgdu7ciV69eqFWrVpixyMiAvBq6tl5XRtgzCc1AQA/HbuDK+yFRxUEp58jonLLSK4N/4Hu+G7PNXzRtCqnrCEqpI0bN+K3337D8OHDAQDHjx9Hhw4dsG7dOkilPAdARKWHRCLBxE/rwMJAB0oBcLE3FTsS0QfBQp6IyjVdLRmW9HRRW/YoIRVVLfRVU9YRkbrIyEh89tlnqvs+Pj6QSCR48uQJqlSpImIyIqL8Dfiomtr9+OQM6OtowVCX5Q6VTzysTkQVypXHL/HZsn/w3d5rUCh5HR1RfnJyciCXy9WWaWtrIzs7W6RERESFl5ieDd/fQ9Fv7Tk8S8kUOw5RieAhKiKqUG7FJiM9W4HAkEi8SM3Cz71d2eWe6A2CIGDAgAHQ1dVVLcvIyMCIESNgYGCgWrZ7924x4hERvVVMYjrikzPxPDULPVYFY+MgD9ib64sdi0ijeEaeiCqUXk3ssaJfI+jIpDh0LRYD/EORlMGzjESv8/Pzg5WVFUxMTFS3L774Ara2tmrLiIhKIydrY+wY4QU7Uz08TEhF95VBuBmTJHYsIo2SCJyjIY+kpCSYmJggMTERxsbGYschohIQdC8Bw/64iJTMHNSzMUbAIHdYGcnf/UQikbBt0iy+n0TlX2xiBvzWh+J2XDKM5Fr43c8dHtXMxY5FVKCitE08I09EFZJ3zUrYOqwpKhnq4EZMEnqsDEZcUobYsYiIiEhDrE3k2D7cC02qmiE5Iwdf/h6CM3eeih2LSCNYyBNRhVXfzgQ7R3jDwVwfNa0MYW6gI3YkIiIi0iATfW38MdgTPnWtYG6gg5pWhmJHItIIDnZHRBWaYyUD7BzpBSNdbWjLeGyTiIiovNHTkWHVF40Rl5wJW1M9seMQaQT3WomowrMykkNP59XI9YIgYOb+6zh8LUbkVERERKQpWjIp7F4r4g9ejcHcAzeg5FS0VEaxkCcies2fV2IQEPQIozaHITAkUuw4REREpGGxiRkYvy0c6/59iAnbw5GVoxQ7ElGRsZAnInpNhwY26OvhAKUA/G/PVSw7cRec3IOIiKj8sDaRY2H3BtCSSrA3/AmGbLyAtKwcsWMRFQkLeSKi18ikEszrWh9jP6kJAPjp2B3M2H+dXe+IiIjKka5uVbDOrwn0tGU4c+cp+q0NwYvULLFjERUaC3kiojdIJBJM+LQOZnVyhkQCbAyOwNitl5CZoxA7GhEREWlIyzpWCBzqCVN9bYRHvUSPVUGIfpkudiyiQmEhT0RUAD9vRyzr4wZtmQSHrsXiyuNEsSMRERGRBrk5mGHnCC/Ymshx/2kqdl54LHYkokLh9HNERG/R0cUWpvraiE/KhLujudhxiIiISMNqWhlh50hvBIZEYsz/X1pHVNqxkCcieodmtSzV7kc+S4NEAtib64uUiIiIiDTJ1lQPk9rWUd3PzFHg6uNENOFBfCql2LWeiKgIniZn4sv1Iei+Mgg3Y5LEjkNEREQaplAKmLDtMnqvOYddF9nVnkonFvJEREWgFATItWSIT85Er9XBCH34XOxIREREpEFKQYCOlhQKpYCJOy5jzZn7YkciyoOFPBFREVQ2lmP7cC+4O5ohOSMHX/4egmM34sSORURERBqiLZPix54uGPJxNQDAvIO3MP/gTQgCp6Kl0oOFPBFREZnoa+OPwZ7wqWuFzBwlhv9xAdvPR4kdi4iIiDREKpXguw51MaW9EwBg9ZkHmLTjCnIUSpGTEb3CQp6IqBjk2jKs+qIxejWpAqUAfLvrCnZcYDFPRERUXkgkEoxoUQOLejSETCrBrrDH+GbnFbFjEQHgqPVERMWmJZNiYfeGsDDUxZFrsfjEyUrsSERERKRhvZrYw1xfB5N2XkZfDwex4xABACQCL/bIIykpCSYmJkhMTISxsbHYcYioDEjOyIaRXFt1X6kUIJVKRExE5Q3bJs3i+0lERcW2nkpaUdomdq0nItKA1xv2raGRGBhwHmlZOSImIiIiIk16va2/GZOEDr/+i/tPU0RMRBUZC3kiIg16kZqFuX/dxOk7T9FvbQhepGaJHYmIiIg0bPafN3AzJgk9VgYhPOql2HGoAmIhT0SkQWYGOtg42AOm+toIj3qJHquCEP0yXexYRKI6c+YMOnbsCFtbW0gkEuzdu/et6+/evRtt2rSBpaUljI2N4eXlhSNHjuRZb8WKFXB0dIRcLoenpydCQ0NL6BUQEalb3s8NLlVM8CItG/3WnsOZO0/FjkQVDAt5IiINa+Rghp0jvGBjIsf9p6nosTIId+OSxY5FJJrU1FS4uLhgxYoVhVr/zJkzaNOmDQ4ePIiLFy+iVatW6NixIy5duqRaZ9u2bZgwYQJmzJiBsLAwuLi4oG3btoiPjy+pl0FEpGJhqIvAoU3RrFYlpGUpMCjgPPaFR4sdiyoQDnaXDw6AQ0Sa8ORlOnzXh+JefApM9LSxfoA7Glc1EzsWlVHlpW2SSCTYs2cPunTpUqTnOTs7o3fv3pg+fToAwNPTE+7u7li+fDkAQKlUwt7eHmPGjMGUKVPeub3y8n4SkbiycpSYuOMy/rz8BAAwo2M9DPyomsipqKziYHdERKWArakedgz3gpuDKRLTs3HuwTOxIxGVSUqlEsnJyTA3NwcAZGVl4eLFi/Dx8VGtI5VK4ePjg+Dg4Hy3kZmZiaSkJLUbEdH70tGS4pferhjg7QgAOH4zDgolz5NSyeM88kREJcjMQAebh3hiz6Vo9OPcs0TFsmTJEqSkpKBXr14AgISEBCgUClSuXFltvcqVK+PWrVv5bmP+/PmYNWtWiWcloopHKpVgRsd6cLI2QoeGNpBxSjr6AEQ/I1+UgWquX7+O7t27w9HRERKJBEuXLs2zjkKhwLRp01CtWjXo6emhRo0amDNnDngFARGJRV9HC/09q0IiedWwp2bmYM+lxyKnIiobAgMDMWvWLGzfvh1WVlbF3s7UqVORmJioukVFRWkwJRFVdBKJBH08HFRT1AmCgB0XopCRrRA5GZVXohbyRR2oJi0tDdWrV8eCBQtgbW2d7zoLFy7EypUrsXz5cty8eRMLFy7EokWL8Ouvv5bkSyEiKhSFUsDIzWH4ettlzD94kwcZid5i69atGDJkCLZv367Wjb5SpUqQyWSIi4tTWz8uLq7A/QNdXV0YGxur3YiISsqaMw/wzc4r8F0fisT0bLHjUDkkaiH/008/YejQoRg4cCDq1auHVatWQV9fH+vXr893fXd3dyxevBh9+vSBrq5uvusEBQWhc+fO6NChAxwdHdGjRw98+umnnJKGiEoFmVSCj2pYAABW/38jn6NQipyKqPTZsmULBg4ciC1btqBDhw5qj+no6KBx48Y4ceKEaplSqcSJEyfg5eX1oaMSEeXhYm8KI10thD58jt6rgxGflCF2JCpnRCvkizNQTWF4e3vjxIkTuHPnDgDg8uXL+Pfff9G+ffsCn8MBcIjoQxreogYW92gImVSCnRcfY/gfF5Gexa53VH6lpKQgPDwc4eHhAICHDx8iPDwckZGRAF51e/f19VWtHxgYCF9fX/z444/w9PREbGwsYmNjkZiYqFpnwoQJWLt2LTZs2ICbN29i5MiRSE1NxcCBAz/oayMiyk/T6hbYNtwLlka6uBWbjO6rgvAwIVXsWFSOiFbIv22gmtjY2GJvd8qUKejTpw+cnJygra0NNzc3jB8/Hv379y/wOfPnz4eJiYnqZm9vX+yfT0RUGD2b2GP1F42hqyXFiVvx+PL3ELxMyxI7FlGJuHDhAtzc3ODm5gbgVRHu5uammkouJiZGVdQDwJo1a5CTk4PRo0fDxsZGdRs3bpxqnd69e2PJkiWYPn06XF1dER4ejsOHD+fZryAiEks9W2PsGuENRwt9RD1PR4+VQbj6OPHdTyQqBNEHu9O07du3Y/PmzQgMDERYWBg2bNiAJUuWYMOGDQU+hwPgEJEYfOpVxuYhnjCWa+FCxAuM2hwmdiSiEtGyZUsIgpDnFhAQAAAICAjAqVOnVOufOnXqrevn+uqrrxAREYHMzEyEhITA09Pzw70oIqJCcLDQx44R3nC2Ncaz1Cz0W3cOL1J54J7en2jTzxVnoJrC+Oabb1Rn5QGgQYMGiIiIwPz58+Hn55fvc3R1dQu85p6IqCQ1cTTHjhHeGLX5Iv73WV2x4xAREZGGWRrpYuuwphj+x0W0b2ADMwMdsSNROSDaGfmSGqgmLS0NUqn6y5LJZFAqOZgUEZVOdayNcPTrFqhvZ6JalpaVI2IiIiIi0iQjuTb+GOyJL5tWVS1jW0/vQ9Su9e8aqMbX1xdTp05VrZ+VlaUaLCcrKwvR0dEIDw/HvXv3VOt07NgRP/zwA/766y88evQIe/bswU8//YSuXbt+8NdHRFRYMqlE9f+LES/QbOFJnL7zVMREREREpEmvt/UvUrPQaflZ/HTsDqeipWIRrWs98GqgmqdPn2L69OmIjY2Fq6ur2kA1kZGRamfXnzx5ohooBwCWLFmCJUuWoEWLFqpr63799VdMmzYNo0aNQnx8PGxtbTF8+HDVgDpERKWd/9mHeJaahcEB5/FjLxd0drUTOxIRERFp0LEbcbgXn4JlJ+4iISUTczrXVyv0id5FIvAQUB5JSUkwMTFBYmIijI2NxY5DRBVMVo4SE3dcxp+XnwAApn9eD4M+riZyKhIb2ybN4vtJRGL741wEpu+7BkEA2te3xs+9XSHXlokdi0RUlLap3I1aT0RU1uloSfFLb1cM8HYEAMw+cAOLDt9i1zsiIqJy5MumVbGiXyPoyKQ4dC0WA/xDkZyRLXYsKiNYyBMRlUJSqQQzOtbDN23rAAB+O3UfU3ZdRY6CA3cSERGVF581sEHAQHcY6mrh3IPn6L36HJ4mZ4odi8oAFvJERKWURCLB6FY1saBbA0glwIs0zjtLRERU3njXrIStw5qikqEOkjKyoWQPPCoEUQe7IyKid+vj4QAHC300cjCDlozHX4mIiMqb+nYm2DnCGwKAysZyseNQGcA9QiKiMsC7RiXVADiCIODXE3cRn5QhcioiIiLSFMdKBqhWyUB1/8/LTxB8/5mIiag0YyFPRFTG/HbqPn48dgfdVgbhYUKq2HGIiIhIwy5GPMfX28Lh5x+Kw9dixI5DpRALeSKiMqZjQ1s4Wujj8Yt09FgZhKuPE8WORERERBrkbGuCT5yskJWjxKjNYdgSGil2JCplWMgTEZUxDhb62DHCG862xniWmoU+a4Jx9l6C2LGIiIhIQ+TaMvzWvxH6uNtDKQBTd1/FryfucipaUmEhT0RUBlka6WLrsKbwrmGB1CwFBvqfx4ErT8SORURERBqiJZNifrcG+KpVTQDAj8fuYOb+61AqWcwTC3kiojLLSK4N/4Hu+KyBNbIUSkzYdhkxielixyIiIiINkUgkmNS2DmZ2rAcA2BAcgT954J7A6eeIiMo0XS0Zfu3bCJUMr6ORgxlsTPTEjkREREQaNuCjajA31MW5B8/QycVW7DhUCrCQJyIq42RSCWZ3rq+2LC4pA5UMdSGTSkRKRURERJrUycVWrYjPyFYgLUsBcwMdEVORWNi1noionIlLykD3lUH4KjAMGdkKseMQERGRhuUolBi75RK6rwxC1PM0seOQCFjIExGVMzdjkhCflIlD12IxwD8UyRnZYkci+r/27j08ivpu//i9u0k2B0k45QREAkIQEMI5htRSFFSweR4rJwUhUE5BsKBVC1UbuTzweyq1aEsDKKAVBIGKYkEoRRElwUBK5CiInCEhQCBHkpDs/P6gpqYEZMMms5u8X9e1F5nZmeWeLySffHZm5wsAcKFzBaXaezpPR84ValByir7JyjM7EmoZjTwA1DE/axeit8f01C12L207nKOHF2zT2fwSs2MBAAAXCQvy1d8m9Va70AbKzi/R0Hmp2n40x+xYqEU08gBQB/Vu01TLJ9ypprf4aO/pPA2el6Lj57n0DgCAuiIsyFcrJsaqR8tGyisu06NvfaWN+86YHQu1hEYeAOqoO5oHaVVib0U09tOx80V6KDlFB8/kmx0LAAC4SJC/t94dG6N7bg9RSZlDiUvS9cG/TpodC7WARh4A6rDIpgH6W2JvtQ8PVEN/bwXfYjc7EgAAcCE/H5vmj+yuwd1byNtmUcsmAWZHQi1g+jkAqONCAn31/sQ7VVRSrkZMUQMAQJ3jZbPq1cGdldintdqENDA7DmoBZ+QBoB4I9PVWWJBvxfKSbce09KtjJiYCAACuZLFYKjXxe07lasYHu1Va5jAxFWoKZ+QBoJ7ZdfKinv9ojwxDOl9QqsfvbiOLxWJ2LAAA4CLFl8s17p0dysor1qmLlzTv0W7y96H1q0s4Iw8A9Uyn5kGa0reNJOm1jQeVtGavHA7D5FQAAMBVfL1tmvVQJ/l6W7Xl4FkNf/MrXSgsNTsWXIhGHgDqGYvFol/f204vxHeQxSL9NfWYfrV8p0rKys2OBgAAXKTv7SFaOu5OBfl5K+PERQ2Zn6rTFy+ZHQsuQiMPAPXU6LhWev3hrvK2WfT3XZka+/YOFZSUmR0LAAC4SPeWjbQqMVbhQb46lF2gQckp+papaOsEGnkAqMf+J7qZFo3uKX8fm748dE7rdmeaHQkAALhQ29AG+tuk3rotOECZucX606eHzI4EF+COBwBQz93VNljLxt+pT7/J1pDuLcyOAwAAXKxZQz+tSuytP2w8oN8ObG92HLgAjTwAQNERDRUd0bBiuaCkTJkXL6ltKHPRAgBQFzQK8NFLD3aqWDYMQ1+fzFWXH9R/eA4urQcAVFJSVq6J7+7QoOQUbT+aY3YcAABQA/706SE9OHer3vrisNlRUA008gCASkrKHCq57FBecZkefesrbdx3xuxIAADAhQzDUN6ly5Kkl9bu16xP9sswmIrWk9DIAwAqCfT11rtjY3TP7SEqKXMocUm6Vuw4YXYsAADgIhaLRc8+0F6/uf92SdL8zw/r6VW7VFbuMDkZbhSNPADgKn4+Ns0f2V2Du7dQucPQM6t2KXnzd7xbDwBAHWGxWDTpZ7fp94M7y2a1aFX6SU18N12XSsvNjoYbQCMPAKiSl82qVwd31sQ+rSVJ/7f+G6asAQCgjhnaI0LzH+0uu5dVm77JVsLiNDkcvHHv7mjkAQDXZLFYNGNAez33QHsF+Xmrf4dQsyMBAAAX69chVEvGxSjQ10sPdW0uq9VidiT8CKafAwD8qHF3tdZD3VqocYBPxTrDMGSxUOgBAKgLekY21uan+1LrPQRn5AEAN+SHhT3tSI6Gv/mVLhSWmpgIAAC40g9r/fmCEg2Zl6qvT1w0LxCuiUYeAOCUy+UOPbXya6UePq8h81N1+uIlsyMBAAAXe3XDAe04dkGPvLlNWw6eNTsO/guNPADAKd42qxaN7qHwIF8dyi7QoOQUfXsm3+xYAADAhZ7/eQfd1bapikrLNfad7foo45TZkfADNPIAAKe1CWmgv03qrTYhtygzt1hD5qcq/dgFs2MBAAAXCbB7aWFCT8VHN9PlckNTl2do8dYjZsfCv9HIAwCqpVlDP62cGKuutzbUxaLLGvHWNn12INvsWAAAwEV8vKx6fVgXje4dKUma+fE+zd5wQIbB9HRmo5EHAFRbowAfLR0Xo5+1C1bxZYc+3MlldwAA1CVWq0VJ8R301L1RkqSPd51WXnGZyanA9HMAgJvi7+OlN0f10KIvj2h0XKTZcQAAgItZLBZNubutwoL81CuysYL8vM2OVO9xRh4AcNO8bVZN7HOb7F42SZLDYWjN16e59A4AgDpkcPcWurWJf8Xyp9+cUe6lyyYmqr9o5AEALvfKuv361bKdenrVLpWVO8yOAwAAXOzTb85o/F/TNWx+qrLzis2OU+/QyAMAXC4qrIFsVotWpZ/UxHfTdam03OxIAADAhUIDfdXI30ffZOVr0LwUHTlXaHakeoVGHgDgckN7RGj+o91l97Jq0zfZGrnwK+UWceldfbRlyxbFx8erWbNmslgs+vDDD6+7fWZmpoYPH66oqChZrVZNmzatyu3mzJmjdu3ayc/PTxEREXriiSdUXMwZIQCoLR2bBemDSb3Vsom/TuRc0uDkFO05lWt2rHqDRh4AUCP6dQjVknExCvT10o5jFzRkfoqycmm06pvCwkJFR0dr7ty5N7R9SUmJgoOD9dxzzyk6OrrKbd577z1Nnz5dSUlJ2r9/vxYuXKj3339fv/3tb10ZHQDwI25t4q9Vib3VsVmgzheW6uEF25Ry6JzZseoFGnkAQI3pGdlYKxJjFRpo18EzBXrkzW0qLeMz8/XJgAED9NJLL+kXv/jFDW0fGRmp119/XaNGjVJQUFCV26SkpCguLk7Dhw9XZGSk7r33Xj3yyCNKS0tzZXQAwA0IbmDX8gl3qvdtTVRQUqbRi7frQFa+2bHqPBp5AECNuj0sUH+b1FttQm7RU/e2k48XpQc3p3fv3kpPT69o3A8fPqx169Zp4MCB19ynpKREeXl5lR4AANdo4OutxWN6amCnMA3u0UJRobeYHanOYx55AECNa9HIX+t+dVelJr60zEFTj2oZPny4zp07p5/85CcyDENlZWVKTEy87qX1s2bN0syZM2sxJQDUL3Yvm/70SDdJV+adl67Uem+bpWIZrsNvUACAWvHDpv30xUvq/8fP9VHGKRMTwVNt3rxZr7zyiv7yl7/oX//6lz744AOtXbtWL7744jX3mTFjhnJzcyseJ06cqMXEAFA/2KwW2axXmvbL5Q4lLknX8x/tUbnDMDlZ3cMZeQBArVuy7ZiOnS/S1OUZyiks1Zi4VmZHggd5/vnnNXLkSI0bN06S1KlTJxUWFmrChAl69tlnZbVefZ7CbrfLbrfXdlQAqLfSjuToswPZMgwpp7BUfxzWRXYvm9mx6gzOyAMAat1T97bT6N6RkqSZH+/Tqxu+kWHwbj1uTFFR0VXNus125ZdD/h8BgHuIa9NUf36km3xsVq3bnaXRi7Yrv5ipaF2FRh4AUOusVouS4jvoqXujJElzP/tOMz7YrbJy7mhf1xQUFCgjI0MZGRmSpCNHjigjI0PHjx+XdOWS91GjRlXa5/vtCwoKdPbsWWVkZGjfvn0Vz8fHxys5OVnLly/XkSNHtHHjRj3//POKj4+vaOgBAOZ7oHO43h7TUwE+NqUePq+HF2zT2fwSs2PVCRaDt66vkpeXp6CgIOXm5iowMNDsOABQpy1LO65nV++Ww5Du7RCqNx7pKl9vmrH/5qm1afPmzerbt+9V6xMSEvT2229r9OjROnr0qDZv3lzxXFU3RWrZsqWOHj0qSSorK9PLL7+sd999V6dOnVJwcLDi4+P18ssvq2HDhjeUy1PHEwA80e6TuRq9OE3nC0vVsom/3v1ljG5t4m92LLfjTG2ika8CxR0Aatf6PVn61fKdat00QO9PjFWQn7fZkdwOtcm1GE8AqF1HzhVq5MKvlFNYquUT7lTnFg3NjuR2nKlN3OwOAGC6++8I05KxMWrZxJ8mHgCAOqhV0wB9MKm3jpwrpIl3AT4jDwBwC71aNVZooG/F8oodJ3TkXKGJiQAAgCuFBPoqpnWTiuWvT1zUhr1ZJibyXDTyAAC3s35Ppp5ZtUuDk1O0+2Su2XEAAICLnbxQpNGL0zRpSbqWpR03O47HoZEHALid7i0bq2OzQJ0vLNXDC1K19dA5syMBAAAXCgv01X0dw+QwpBkf7NafNn3LFKJOoJEHALid4AZ2LZ9wp3rf1kSFpeUas3i71u7KNDsWAABwES+bVbMe6qQpfdtIkv6w8aBeWLNXDgfN/I2gkQcAuKUGvt5aPKanBnYKU2m5Q1OW/Uvvph41OxYAAHARi8Wip+5rpxfiO0iS3kk9pqnvZ6i0zGFyMvdHIw8AcFt2L5v+9Eg3PXrnrTIM6fmP9ir92AWzYwEAABcaHddKrz/cRd42iz7++rSSN39ndiS3Z3ojP3fuXEVGRsrX11cxMTFKS0u75rZ79+7VoEGDFBkZKYvFojlz5lS53alTp/Too4+qSZMm8vPzU6dOnbRjx44aOgIAQE2yWS168X/v0LR+bZXY5zZ1b9nI7EgAAMDF/rdLcy1M6KmfRgVrwk9bmx3H7Zk6j/z777+vJ598UvPmzVNMTIzmzJmj++67TwcOHFBISMhV2xcVFal169YaMmSInnjiiSpf88KFC4qLi1Pfvn31ySefKDg4WN9++60aNeIXPwDwVBaLRdP6RVW6CU5+8WV526zy9baZmAwAALjKT6OCdVfbprJYLJIkwzCUU1iqJrfYTU7mfiyGibcGjImJUc+ePfXnP/9ZkuRwOBQREaHHH39c06dPv+6+kZGRmjZtmqZNm1Zp/fTp07V161Z98cUX1c6Vl5enoKAg5ebmKjAwsNqvAwCoGcWXyzVqUZqsFunNUT3UwNfb7Eg1jtrkWownALi/P/zjgN7ffkJ/HdtLt4fV/Z/VztQm0y6tLy0tVXp6uvr16/efMFar+vXrp9TU1Gq/7po1a9SjRw8NGTJEISEh6tq1q958883r7lNSUqK8vLxKDwCA+zqUXaC9p3K17XCOHl6wTWfzS8yOBAAAXKiotEwb9mYpO79EQ+elavvRHLMjuRXTGvlz586pvLxcoaGhldaHhoYqKyur2q97+PBhJScnq23bttqwYYMmTZqkX/3qV3rnnXeuuc+sWbMUFBRU8YiIiKj23w8AqHl3NA/S8gmxahLgo72n8zR4XoqOny8yOxYAAHARfx8vrZgYq+4tGymvuEyPvvWV/rnvjNmx3IbpN7tzNYfDoW7duumVV15R165dNWHCBI0fP17z5s275j4zZsxQbm5uxePEiRO1mBgAUB2dWgRp1aTeimjsp2Pni/RQcor2ns41OxYAAHCRhv4+WjI2RnffHqKSMocmLknXih30apKJjXzTpk1ls9l05kzld1XOnDmjsLCwar9ueHi4OnToUGld+/btdfz48WvuY7fbFRgYWOkBAHB/rZoG6G+JvdU+PFDnCkr08PxtSjvCpXcAANQVfj42zR/ZXYO6tVC5w9Azq3ZpwRampzOtkffx8VH37t21adOminUOh0ObNm1SbGxstV83Li5OBw4cqLTu4MGDatmyZbVfEwDgvkICffX+xDvVq1VjWa0WNfKv+ze+AwCgPvG2WTV7SGdN7HNlWromAdzF3tTp55588kklJCSoR48e6tWrl+bMmaPCwkKNGTNGkjRq1Cg1b95cs2bNknTlBnn79u2r+PrUqVPKyMjQLbfcojZt2kiSnnjiCfXu3VuvvPKKhg4dqrS0NC1YsEALFiww5yABADUu0Ndbf/1lLx3PKVLb0AZmxwEAAC5msVg0Y0B7/bxTM3VqEWR2HNOZ+hn5YcOGafbs2frd736nLl26KCMjQ+vXr6+4Ad7x48eVmZlZsf3p06fVtWtXde3aVZmZmZo9e7a6du2qcePGVWzTs2dPrV69WsuWLdMdd9yhF198UXPmzNGIESNq/fgAALXH19umqB808SnfndPczw7JxFlWAQCAi/2wic/OL9ZvV+9WUWmZiYnMYeo88u6KuWUBwLNl5xXr7j98roKSMiXEtlRSfEdZrRazY90UapNrMZ4A4NkMw9CQeanaceyCut7aUIsSeqpRgI/ZsW6KR8wjDwBATQkJ9NWv742SJL2TekxT389QaZnD5FQAAMBVLBaLZgxsryA/b+08flFD5qfq9MVLZseqNTTyAIA6aUxcK73+cBd5WS36+OvT+uXb21VQUv8uvQMAoK7q3rKRVibGKizQV4eyCzQoOUWHsvPNjlUraOQBAHXW/3ZproWje8rfx6YvD53T8De36XxBidmxAACAi0SFNtDfHuut24IDlJlbrMHzUpV+7ILZsWocjTwAoE7rExWs98bfqUb+3tp1MlcLvjhsdiQAAOBCzRv6aWVib3WJaKiLRZeVtGaPHI66fSs4GnkAQJ3XJaKhVib21rAeEfp1/3ZmxwEAAC7WOMBH742P0eDuLTTv0e4ef5PbH0MjDwCoF9qE3KL/G9xZPl5XSl+5w9DBM/Xjc3QAANQH/j5emj0kWi0a+Ves+yYrz8RENYdGHgBQ7xiGoZkf71X8n77UP/edMTsOAACoAev3ZGrg619o1if7VddmXaeRBwDUO5fLDZ26cEklZQ5NXJKuFTtOmB0JAAC42ImcS3IY0vzPD+uZVbtUVl53pqKlkQcA1Ds+XlbNG9ldg7q1ULnD0DOrdmne59/VuXfrAQCoz8b/tLV+P6izrBZpZfpJJS5J16XScrNjuQSNPACgXvK2WTV7SGdN7NNakvT/PvlGL6/dX+fvcgsAQH0ytGeE5o/sIbuXVf/cn62RC79SbtFls2PdNBp5AEC9ZbFYNGNAez07sL0k6a0vj+i3q3ebnAoAALhS/w6hendsjAJ9vbTj2AUNnZ+q/GLPbuZp5AEA9d74n7bWa0Oj5eNlVd/bQ8yOAwAAXKxXq8ZakRir0EC7Ym9rolvsXmZHuimenR4AABd5qFsLxbVpqtBAX7OjAACAGnB7WKA+fvwnahpgl8Xi2fPMc0YeAIB/+2ETfyKnSCPe2qbTFy+ZmAgAALhSSANfWa1XmvjSMocS303XloNnTU7lPBp5AACqMP2DXdp66LwGJafo2zP5ZscBAAAutmjrEa3fm6Wx72zXRxmnzI7jFBp5AACq8OrgaN0WHKDM3GINmZ+q9GMXzI4EAABc6JdxrfTzzuG6XG5o6vIMLd56xOxIN4xGHgCAKjRr6KdVib3VJaKhLhZd1oi3tumzA9lmxwIAAC7i42XVGw93VUJsS0nSzI/3afaGAzIM95+KlkYeAIBraBTgo/fGx6hPVLCKLzs0/p0dWr3zpNmxAACAi1itFr3wPx316/5RkqQ/f3ZIv129W2XlDpOTXR+NPAAA1+Hv46W3EnroF12bq8xhaNGXR92+uAMAgBtnsVj0+D1tNeuhTrJapL9/nakTF9z7ZrdMPwcAwI/wtln1hyHRahNyi4b2iJCXjffBAQCoax7pdasa+fsoyM9brZoGmB3nuvhNBACAG2C1WjS5bxsFN7BXrPvsQDZn5wEAqEPuvyNMsbc1qVjedfKisvOKTUxUNRp5AACqYVX6SY1ZvF2JS9J1qbTc7DgAAMDFDp7J18iFaRo0L0VHzxWaHacSGnkAAKqhoZ+37F5W/XN/tkYu/Eq5RZfNjgQAAFzI18umhv7eOpFzSYPnpWjPqVyzI1WgkQcAoBr6dQjVknExCvT10o5jFzRkfoqyct3v0jsAAFA9tzbx16rE3urYLFDnCkr18IJtSjl0zuxYkmjkAQCotp6RjbUiMVahgXYdPFOgQckp+u5sgSSp3GEo9bvz+ijjlFK/O69yh/vPSQsAACoLbmDX8gl3KrZ1ExWUlGn04u1atztTkrm13mJ4wmz3tSwvL09BQUHKzc1VYGCg2XEAAG7uRE6REhal6fC5QjUO8NGMAbfrtY0HlfmDM/ThQb5Kiu+g++8Ir9bfQW1yLcYTAOCM4svleuL9DH2yJ0sWi/TYz27TB/86ZVqtp5GvAsUdAOCs8wUlGvP2drVo6KdP9mTpv4ur5d9/Jj/arVoFntrkWownAMBZ5Q5Dv/toj9KO5OhQdoGptZ5L6wEAcIEmt9i1bPyd+teJi1cVdkkV62Z+vI/L7AEA8EA2q0Uz/6ej8osvm17raeQBAHCRXSdzr3vDO0NSZm6x0o7k1F4oAADgMtuPXlBWXsk1n6+tWk8jDwCAi2Tn39hd6290OwAA4F7cpdbTyAMA4CIhDXxduh0AAHAv7lLraeQBAHCRXq0aKzzIt+JmN//Noit3tO3VqnFtxjLVli1bFB8fr2bNmslisejDDz+87vaZmZkaPny4oqKiZLVaNW3atCq3u3jxoiZPnqzw8HDZ7XZFRUVp3bp1rj8AAAB+wF1qPY08AAAuYrNalBTfQZKuKvDfLyfFd5DNeq3yX/cUFhYqOjpac+fOvaHtS0pKFBwcrOeee07R0dFVblNaWqr+/fvr6NGjWrVqlQ4cOKA333xTzZs3d2V0AACu4i613qtGXx0AgHrm/jvClfxoN838eF+luWXDbnJuWU81YMAADRgw4Ia3j4yM1Ouvvy5JWrRoUZXbLFq0SDk5OUpJSZG3t3fFftdTUlKikpL/3JwoLy/vhjMBAPBD7lDraeQBAHCx++8IV/8OYUo7kqPs/GKFNLhyiV19OhNfk9asWaPY2FhNnjxZH330kYKDgzV8+HD95je/kc1mq3KfWbNmaebMmbWcFABQV5ld62nkAQCoATarRbG3NTE7Rp10+PBhffrppxoxYoTWrVunQ4cO6bHHHtPly5eVlJRU5T4zZszQk08+WbGcl5eniIiI2ooMAKiDzKz1NPIAAMCjOBwOhYSEaMGCBbLZbOrevbtOnTqlV1999ZqNvN1ul91ur+WkAADUDBp5AADgUcLDw+Xt7V3pMvr27dsrKytLpaWl8vHxMTEdAAA1j7vWAwAAjxIXF6dDhw7J4XBUrDt48KDCw8Np4gEA9QKNPAAAqDEFBQXKyMhQRkaGJOnIkSPKyMjQ8ePHJV357PqoUaMq7fP99gUFBTp79qwyMjK0b9++iucnTZqknJwcTZ06VQcPHtTatWv1yiuvaPLkybV2XAAAmIlL6wEAQI3ZsWOH+vbtW7H8/Q3nEhIS9PbbbyszM7Oiqf9e165dK75OT0/Xe++9p5YtW+ro0aOSpIiICG3YsEFPPPGEOnfurObNm2vq1Kn6zW9+U/MHBACAG7AYhmGYHcLd5OXlKSgoSLm5uQoMDDQ7DgAA1CYXYzwBAO7GmdrEpfUAAAAAAHgQGnkAAAAAADwIjTwAAAAAAB6ERh4AAAAAAA/CXeur8P39//Ly8kxOAgDAFd/XJO5R6xrUegCAu3Gm1tPIVyE/P1/SleltAABwJ/n5+QoKCjI7hsej1gMA3NWN1Hqmn6uCw+HQ6dOn1aBBA1kslpt6rby8PEVEROjEiRNMb3ODGDPnMWbOY8ycx5g5z5VjZhiG8vPz1axZM1mtfDLuZrmy1kt8fziL8XIeY+Y8xsx5jJnzzKr1nJGvgtVqVYsWLVz6moGBgXwzOIkxcx5j5jzGzHmMmfNcNWaciXedmqj1Et8fzmK8nMeYOY8xcx5j5rzarvW8pQ8AAAAAgAehkQcAAAAAwIPQyNcwu92upKQk2e12s6N4DMbMeYyZ8xgz5zFmzmPM6g/+rZ3DeDmPMXMeY+Y8xsx5Zo0ZN7sDAAAAAMCDcEYeAAAAAAAPQiMPAAAAAIAHoZEHAAAAAMCD0MgDAAAAAOBBaORv0pYtWxQfH69mzZrJYrHoww8//NF9Nm/erG7duslut6tNmzZ6++23azynO3F2zD744AP1799fwcHBCgwMVGxsrDZs2FA7Yd1Edf6ffW/r1q3y8vJSly5daiyfu6nOeJWUlOjZZ59Vy5YtZbfbFRkZqUWLFtV8WDdRnTFbunSpoqOj5e/vr/DwcP3yl7/U+fPnaz6sm5g1a5Z69uypBg0aKCQkRA8++KAOHDjwo/utXLlSt99+u3x9fdWpUyetW7euFtLiZlDrnUetdx613nnUe+dR753jzrWeRv4mFRYWKjo6WnPnzr2h7Y8cOaIHHnhAffv2VUZGhqZNm6Zx48bVq2Ll7Jht2bJF/fv317p165Senq6+ffsqPj5eO3furOGk7sPZMfvexYsXNWrUKN1zzz01lMw9VWe8hg4dqk2bNmnhwoU6cOCAli1bpnbt2tVgSvfi7Jht3bpVo0aN0tixY7V3716tXLlSaWlpGj9+fA0ndR+ff/65Jk+erG3btmnjxo26fPmy7r33XhUWFl5zn5SUFD3yyCMaO3asdu7cqQcffFAPPvig9uzZU4vJ4SxqvfOo9c6j1juPeu886r1z3LrWG3AZScbq1auvu80zzzxjdOzYsdK6YcOGGffdd18NJnNfNzJmVenQoYMxc+ZM1wfyAM6M2bBhw4znnnvOSEpKMqKjo2s0l7u6kfH65JNPjKCgIOP8+fO1E8rN3ciYvfrqq0br1q0rrXvjjTeM5s2b12Ay95adnW1IMj7//PNrbjN06FDjgQceqLQuJibGmDhxYk3Hg4tQ651HrXcetd551HvnUe+d5061njPytSw1NVX9+vWrtO6+++5TamqqSYk8j8PhUH5+vho3bmx2FLe2ePFiHT58WElJSWZHcXtr1qxRjx499Pvf/17NmzdXVFSUnnrqKV26dMnsaG4rNjZWJ06c0Lp162QYhs6cOaNVq1Zp4MCBZkczTW5uriRd92cTNaB+4N/55lHrbwy13jnUe+dR7ytzp1rv5dJXw4/KyspSaGhopXWhoaHKy8vTpUuX5OfnZ1IyzzF79mwVFBRo6NChZkdxW99++62mT5+uL774Ql5efJv/mMOHD+vLL7+Ur6+vVq9erXPnzumxxx7T+fPntXjxYrPjuaW4uDgtXbpUw4YNU3FxscrKyhQfH+/0JaF1hcPh0LRp0xQXF6c77rjjmttdqwZkZWXVdETUImr9zaPW/zhqvfOo986j3v+Hu9V6zsjDo7z33nuaOXOmVqxYoZCQELPjuKXy8nINHz5cM2fOVFRUlNlxPILD4ZDFYtHSpUvVq1cvDRw4UK+99preeecd3qW/hn379mnq1Kn63e9+p/T0dK1fv15Hjx5VYmKi2dFMMXnyZO3Zs0fLly83Owrg8aj1P45aXz3Ue+dR7//D3Wo9b9/VsrCwMJ05c6bSujNnzigwMJB36H/E8uXLNW7cOK1cufKqy1XwH/n5+dqxY4d27typKVOmSLpSuAzDkJeXl/7xj3/o7rvvNjmlewkPD1fz5s0VFBRUsa59+/YyDEMnT55U27ZtTUznnmbNmqW4uDg9/fTTkqTOnTsrICBAd911l1566SWFh4ebnLD2TJkyRX//+9+1ZcsWtWjR4rrbXqsGhIWF1WRE1DJqffVR628Mtb56qPfOo95f4Y61njPytSw2NlabNm2qtG7jxo2KjY01KZFnWLZsmcaMGaNly5bpgQceMDuOWwsMDNTu3buVkZFR8UhMTFS7du2UkZGhmJgYsyO6nbi4OJ0+fVoFBQUV6w4ePCir1fqjP6zrq6KiIlmtlUuIzWaTJBmGYUakWmcYhqZMmaLVq1fr008/VatWrX50H2pA/cC/c/VQ628ctb56qPfOq+/13q1rvUtvnVcP5efnGzt37jR27txpSDJee+01Y+fOncaxY8cMwzCM6dOnGyNHjqzY/vDhw4a/v7/x9NNPG/v37zfmzp1r2Gw2Y/369WYdQq1zdsyWLl1qeHl5GXPnzjUyMzMrHhcvXjTrEGqds2P23+rbnWydHa/8/HyjRYsWxuDBg429e/can3/+udG2bVtj3LhxZh1CrXN2zBYvXmx4eXkZf/nLX4zvvvvO+PLLL40ePXoYvXr1MusQat2kSZOMoKAgY/PmzZV+NhUVFVVsM3LkSGP69OkVy1u3bjW8vLyM2bNnG/v37zeSkpIMb29vY/fu3WYcAm4Qtd551HrnUeudR713HvXeOe5c62nkb9Jnn31mSLrqkZCQYBiGYSQkJBh9+vS5ap8uXboYPj4+RuvWrY3FixfXem4zOTtmffr0ue729UF1/p/9UH0r7tUZr/379xv9+vUz/Pz8jBYtWhhPPvlkpR/SdV11xuyNN94wOnToYPj5+Rnh4eHGiBEjjJMnT9Z+eJNUNV6SKv1M79Onz1U/q1asWGFERUUZPj4+RseOHY21a9fWbnA4jVrvPGq986j1zqPeO4967xx3rvWWfwcEAAAAAAAegM/IAwAAAADgQWjkAQAAAADwIDTyAAAAAAB4EBp5AAAAAAA8CI08AAAAAAAehEYeAAAAAAAPQiMPAAAAAIAHoZEHAAAAAMCD0MgDcEsWi0Uffvih2TEAAEANodYD1UcjD+Aqo0ePlsViuepx//33mx0NAAC4ALUe8GxeZgcA4J7uv/9+LV68uNI6u91uUhoAAOBq1HrAc3FGHkCV7Ha7wsLCKj0aNWok6cqlcMnJyRowYID8/PzUunVrrVq1qtL+u3fv1t133y0/Pz81adJEEyZMUEFBQaVtFi1apI4dO8putys8PFxTpkyp9Py5c+f0i1/8Qv7+/mrbtq3WrFlTswcNAEA9Qq0HPBeNPIBqef755zVo0CB9/fXXGjFihB5++GHt379fklRYWKj77rtPjRo10vbt27Vy5Ur985//rFS8k5OTNXnyZE2YMEG7d+/WmjVr1KZNm0p/x8yZMzV06FDt2rVLAwcO1IgRI5STk1OrxwkAQH1FrQfcmAEA/yUhIcGw2WxGQEBApcfLL79sGIZhSDISExMr7RMTE2NMmjTJMAzDWLBggdGoUSOjoKCg4vm1a9caVqvVyMrKMgzDMJo1a2Y8++yz18wgyXjuuecqlgsKCgxJxieffOKy4wQAoL6i1gOejc/IA6hS3759lZycXGld48aNK76OjY2t9FxsbKwyMjIkSfv371d0dLQCAgIqno+Li5PD4dCBAwdksVh0+vRp3XPPPdfN0Llz54qvAwICFBgYqOzs7OoeEgAA+AFqPeC5aOQBVCkgIOCqy99cxc/P74a28/b2rrRssVjkcDhqIhIAAPUOtR7wXHxGHkC1bNu27arl9u3bS5Lat2+vr7/+WoWFhRXPb926VVarVe3atVODBg0UGRmpTZs21WpmAABw46j1gPvijDyAKpWUlCgrK6vSOi8vLzVt2lSStHLlSvXo0UM/+clPtHTpUqWlpWnhwoWSpBEjRigpKUkJCQl64YUXdPbsWT3++OMaOXKkQkNDJUkvvPCCEhMTFRISogEDBig/P19bt27V448/XrsHCgBAPUWtBzwXjTyAKq1fv17h4eGV1rVr107ffPONpCt3mV2+fLkee+wxhYeHa9myZerQoYMkyd/fXxs2bNDUqVPVs2dP+fv7a9CgQXrttdcqXishIUHFxcX64x//qKeeekpNmzbV4MGDa+8AAQCo56j1gOeyGIZhmB0CgGexWCxavXq1HnzwQbOjAACAGkCtB9wbn5EHAAAAAMCD0MgDAAAAAOBBuLQeAAAAAAAPwhl5AAAAAAA8CI08AAAAAAAehEYeAAAAAAAPQiMPAAAAAIAHoZEHAAAAAMCD0MgDAAAAAOBBaOQBAAAAAPAgNPIAAAAAAHiQ/w9byj9LTxfsFwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of base model: 1.16\n",
      "0.14410588694506624\n",
      "303.7752096801996\n"
     ]
    }
   ],
   "source": [
    "steps = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(steps, train_losses, label='Training Loss', linestyle='dashed', marker=\"o\")\n",
    "plt.title('Simple Model - Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps, perplexities, label='Perplexity', linestyle='dashed', marker=\"o\")\n",
    "plt.title('Simple Model - Perplexity over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in dev_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(dev_dataloader)\n",
    "perplexity_simple = math.exp(avg_loss)\n",
    "print(f\"Perplexity of base model: {perplexity_simple:.2f}\")\n",
    "print(avg_loss)\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9BEnG1FlZCH"
   },
   "source": [
    "## generating text (cnn model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:58:29.333430Z",
     "iopub.status.busy": "2024-12-14T12:58:29.332965Z",
     "iopub.status.idle": "2024-12-14T12:58:29.422744Z",
     "shell.execute_reply": "2024-12-14T12:58:29.421941Z",
     "shell.execute_reply.started": "2024-12-14T12:58:29.333395Z"
    },
    "id": "LI90850kkM1y",
    "outputId": "edea5b57-aec2-4ad1-bf40-8b6a1be201eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "\n",
      "[CLS] من در راه[SEP] برگزار شد. 60136 شایان ذکر است، اما از\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, start_text, context_length=15, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = tokenizer.encode(start_text)\n",
    "    context = torch.tensor(generated, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(context_length):\n",
    "            if context.size(1) >= context_length:\n",
    "                break\n",
    "            logits = model(context)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "            context = torch.cat([context, next_token_id.unsqueeze(0)], dim=1)\n",
    "            # print(f\"next_token_logits: {next_token_logits}, probabilities: {probabilities}, next_token_id: {next_token_id}, context: {context}\")\n",
    "\n",
    "\n",
    "    generated_text = tokenizer.decode(context[0].tolist())\n",
    "    return generated_text\n",
    "\n",
    "start_text = \" من در راه\"\n",
    "generated_text = generate_text(model, tokenizer, start_text, context_length=20)\n",
    "print(\"Generated Text:\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXfCfeaCjySR"
   },
   "source": [
    "؟؟؟ من در راه ؟؟؟ برگزار شد. 60136 شایان ذکر است، اما از"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AV6285XflpZC"
   },
   "source": [
    "# a pretrained model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "65152dd91add4b289dfda81a315d6a80"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-12-14T13:00:21.963182Z",
     "iopub.status.busy": "2024-12-14T13:00:21.962307Z",
     "iopub.status.idle": "2024-12-14T13:00:25.133035Z",
     "shell.execute_reply": "2024-12-14T13:00:25.132126Z",
     "shell.execute_reply.started": "2024-12-14T13:00:21.963146Z"
    },
    "id": "vR82OMPfloYQ",
    "outputId": "2cfdcbc9-7bf2-4338-bc78-5800aafb8b88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'در یک اتفاق شگفت انگیز، پژوهشگران به گزارش گروه استان\\u200cهای باشگاه خبرنگاران جوان از آبادان ؛ علی رضا داهیم بازیکن و مربی سابق تیم فوتبال وردربرمن با توجه به اینکه از کشور آلمان به کشور ما منتقل شده است'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65152dd91add4b289dfda81a315d6a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, GPT2LMHeadModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "model = GPT2LMHeadModel.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "generator = pipeline('text-generation', model, tokenizer=tokenizer, config={'max_length':512}, device='cuda')\n",
    "sample = generator('در یک اتفاق شگفت انگیز، پژوهشگران')\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a68Er363jySS"
   },
   "source": [
    "# running the reg model on pre-processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-14T16:29:37.238797Z",
     "iopub.status.busy": "2024-12-14T16:29:37.238021Z",
     "iopub.status.idle": "2024-12-14T16:44:47.229582Z",
     "shell.execute_reply": "2024-12-14T16:44:47.228548Z",
     "shell.execute_reply.started": "2024-12-14T16:29:37.238761Z"
    },
    "id": "64-meTZijySS",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "52436376-9fb1-423e-dfea-0306173adbb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/2], Step [0/16076], Loss: 10.6828\n",
      "Epoch [1/2], Step [20/16076], Loss: 10.0013\n",
      "Epoch [1/2], Step [40/16076], Loss: 9.3403\n",
      "Epoch [1/2], Step [60/16076], Loss: 8.7035\n",
      "Epoch [1/2], Step [80/16076], Loss: 8.1264\n",
      "Epoch [1/2], Step [100/16076], Loss: 7.5136\n",
      "Epoch [1/2], Step [120/16076], Loss: 7.0860\n",
      "Epoch [1/2], Step [140/16076], Loss: 6.8481\n",
      "Epoch [1/2], Step [160/16076], Loss: 6.7364\n",
      "Epoch [1/2], Step [180/16076], Loss: 6.5502\n",
      "Epoch [1/2], Step [200/16076], Loss: 6.3960\n",
      "Epoch [1/2], Step [220/16076], Loss: 6.3340\n",
      "Epoch [1/2], Step [240/16076], Loss: 6.3052\n",
      "Epoch [1/2], Step [260/16076], Loss: 6.2252\n",
      "Epoch [1/2], Step [280/16076], Loss: 6.1702\n",
      "Epoch [1/2], Step [300/16076], Loss: 6.1685\n",
      "Epoch [1/2], Step [320/16076], Loss: 6.0959\n",
      "Epoch [1/2], Step [340/16076], Loss: 6.0323\n",
      "Epoch [1/2], Step [360/16076], Loss: 6.0098\n",
      "Epoch [1/2], Step [380/16076], Loss: 6.0717\n",
      "Epoch [1/2], Step [400/16076], Loss: 6.0274\n",
      "Epoch [1/2], Step [420/16076], Loss: 5.9745\n",
      "Epoch [1/2], Step [440/16076], Loss: 5.9250\n",
      "Epoch [1/2], Step [460/16076], Loss: 5.9456\n",
      "Epoch [1/2], Step [480/16076], Loss: 6.0260\n",
      "Epoch [1/2], Step [500/16076], Loss: 5.9880\n",
      "Epoch [1/2], Step [520/16076], Loss: 5.9469\n",
      "Epoch [1/2], Step [540/16076], Loss: 5.8775\n",
      "Epoch [1/2], Step [560/16076], Loss: 5.7917\n",
      "Epoch [1/2], Step [580/16076], Loss: 5.8428\n",
      "Epoch [1/2], Step [600/16076], Loss: 5.9120\n",
      "Epoch [1/2], Step [620/16076], Loss: 5.6930\n",
      "Epoch [1/2], Step [640/16076], Loss: 5.8245\n",
      "Epoch [1/2], Step [660/16076], Loss: 5.8679\n",
      "Epoch [1/2], Step [680/16076], Loss: 5.7930\n",
      "Epoch [1/2], Step [700/16076], Loss: 5.8147\n",
      "Epoch [1/2], Step [720/16076], Loss: 5.8452\n",
      "Epoch [1/2], Step [740/16076], Loss: 5.7771\n",
      "Epoch [1/2], Step [760/16076], Loss: 5.8619\n",
      "Epoch [1/2], Step [780/16076], Loss: 5.7912\n",
      "Epoch [1/2], Step [800/16076], Loss: 5.7602\n",
      "Epoch [1/2], Step [820/16076], Loss: 5.7592\n",
      "Epoch [1/2], Step [840/16076], Loss: 5.8015\n",
      "Epoch [1/2], Step [860/16076], Loss: 5.7483\n",
      "Epoch [1/2], Step [880/16076], Loss: 5.7291\n",
      "Epoch [1/2], Step [900/16076], Loss: 5.7508\n",
      "Epoch [1/2], Step [920/16076], Loss: 5.6980\n",
      "Epoch [1/2], Step [940/16076], Loss: 5.7815\n",
      "Epoch [1/2], Step [960/16076], Loss: 5.7169\n",
      "Epoch [1/2], Step [980/16076], Loss: 5.6818\n",
      "Epoch [1/2], Step [1000/16076], Loss: 5.6901\n",
      "Epoch [1/2], Step [1020/16076], Loss: 5.6329\n",
      "Epoch [1/2], Step [1040/16076], Loss: 5.7478\n",
      "Epoch [1/2], Step [1060/16076], Loss: 5.7647\n",
      "Epoch [1/2], Step [1080/16076], Loss: 5.6677\n",
      "Epoch [1/2], Step [1100/16076], Loss: 5.6174\n",
      "Epoch [1/2], Step [1120/16076], Loss: 5.6655\n",
      "Epoch [1/2], Step [1140/16076], Loss: 5.5848\n",
      "Epoch [1/2], Step [1160/16076], Loss: 5.5722\n",
      "Epoch [1/2], Step [1180/16076], Loss: 5.6127\n",
      "Epoch [1/2], Step [1200/16076], Loss: 5.6529\n",
      "Epoch [1/2], Step [1220/16076], Loss: 5.5701\n",
      "Epoch [1/2], Step [1240/16076], Loss: 5.6345\n",
      "Epoch [1/2], Step [1260/16076], Loss: 5.6576\n",
      "Epoch [1/2], Step [1280/16076], Loss: 5.4641\n",
      "Epoch [1/2], Step [1300/16076], Loss: 5.6278\n",
      "Epoch [1/2], Step [1320/16076], Loss: 5.5294\n",
      "Epoch [1/2], Step [1340/16076], Loss: 5.6001\n",
      "Epoch [1/2], Step [1360/16076], Loss: 5.6046\n",
      "Epoch [1/2], Step [1380/16076], Loss: 5.5765\n",
      "Epoch [1/2], Step [1400/16076], Loss: 5.6145\n",
      "Epoch [1/2], Step [1420/16076], Loss: 5.5717\n",
      "Epoch [1/2], Step [1440/16076], Loss: 5.5763\n",
      "Epoch [1/2], Step [1460/16076], Loss: 5.5271\n",
      "Epoch [1/2], Step [1480/16076], Loss: 5.5853\n",
      "Epoch [1/2], Step [1500/16076], Loss: 5.5659\n",
      "Epoch [1/2], Step [1520/16076], Loss: 5.5907\n",
      "Epoch [1/2], Step [1540/16076], Loss: 5.4699\n",
      "Epoch [1/2], Step [1560/16076], Loss: 5.5314\n",
      "Epoch [1/2], Step [1580/16076], Loss: 5.5852\n",
      "Epoch [1/2], Step [1600/16076], Loss: 5.5626\n",
      "Epoch [1/2], Step [1620/16076], Loss: 5.4611\n",
      "Epoch [1/2], Step [1640/16076], Loss: 5.3943\n",
      "Epoch [1/2], Step [1660/16076], Loss: 5.4491\n",
      "Epoch [1/2], Step [1680/16076], Loss: 5.4620\n",
      "Epoch [1/2], Step [1700/16076], Loss: 5.4802\n",
      "Epoch [1/2], Step [1720/16076], Loss: 5.5706\n",
      "Epoch [1/2], Step [1740/16076], Loss: 5.4967\n",
      "Epoch [1/2], Step [1760/16076], Loss: 5.4871\n",
      "Epoch [1/2], Step [1780/16076], Loss: 5.5164\n",
      "Epoch [1/2], Step [1800/16076], Loss: 5.5437\n",
      "Epoch [1/2], Step [1820/16076], Loss: 5.5073\n",
      "Epoch [1/2], Step [1840/16076], Loss: 5.4706\n",
      "Epoch [1/2], Step [1860/16076], Loss: 5.5353\n",
      "Epoch [1/2], Step [1880/16076], Loss: 5.5232\n",
      "Epoch [1/2], Step [1900/16076], Loss: 5.5485\n",
      "Epoch [1/2], Step [1920/16076], Loss: 5.4101\n",
      "Epoch [1/2], Step [1940/16076], Loss: 5.4370\n",
      "Epoch [1/2], Step [1960/16076], Loss: 5.5223\n",
      "Epoch [1/2], Step [1980/16076], Loss: 5.4220\n",
      "Epoch [1/2], Step [2000/16076], Loss: 5.5128\n",
      "Epoch [1/2], Step [2020/16076], Loss: 5.4222\n",
      "Epoch [1/2], Step [2040/16076], Loss: 5.4075\n",
      "Epoch [1/2], Step [2060/16076], Loss: 5.3861\n",
      "Epoch [1/2], Step [2080/16076], Loss: 5.3691\n",
      "Epoch [1/2], Step [2100/16076], Loss: 5.5656\n",
      "Epoch [1/2], Step [2120/16076], Loss: 5.3649\n",
      "Epoch [1/2], Step [2140/16076], Loss: 5.3824\n",
      "Epoch [1/2], Step [2160/16076], Loss: 5.5220\n",
      "Epoch [1/2], Step [2180/16076], Loss: 5.4872\n",
      "Epoch [1/2], Step [2200/16076], Loss: 5.4771\n",
      "Epoch [1/2], Step [2220/16076], Loss: 5.4797\n",
      "Epoch [1/2], Step [2240/16076], Loss: 5.3959\n",
      "Epoch [1/2], Step [2260/16076], Loss: 5.3333\n",
      "Epoch [1/2], Step [2280/16076], Loss: 5.4019\n",
      "Epoch [1/2], Step [2300/16076], Loss: 5.3656\n",
      "Epoch [1/2], Step [2320/16076], Loss: 5.3757\n",
      "Epoch [1/2], Step [2340/16076], Loss: 5.4557\n",
      "Epoch [1/2], Step [2360/16076], Loss: 5.3165\n",
      "Epoch [1/2], Step [2380/16076], Loss: 5.3175\n",
      "Epoch [1/2], Step [2400/16076], Loss: 5.4272\n",
      "Epoch [1/2], Step [2420/16076], Loss: 5.4662\n",
      "Epoch [1/2], Step [2440/16076], Loss: 5.4226\n",
      "Epoch [1/2], Step [2460/16076], Loss: 5.3459\n",
      "Epoch [1/2], Step [2480/16076], Loss: 5.3898\n",
      "Epoch [1/2], Step [2500/16076], Loss: 5.2451\n",
      "Epoch [1/2], Step [2520/16076], Loss: 5.2360\n",
      "Epoch [1/2], Step [2540/16076], Loss: 5.3499\n",
      "Epoch [1/2], Step [2560/16076], Loss: 5.4191\n",
      "Epoch [1/2], Step [2580/16076], Loss: 5.3942\n",
      "Epoch [1/2], Step [2600/16076], Loss: 5.4301\n",
      "Epoch [1/2], Step [2620/16076], Loss: 5.2704\n",
      "Epoch [1/2], Step [2640/16076], Loss: 5.2925\n",
      "Epoch [1/2], Step [2660/16076], Loss: 5.1806\n",
      "Epoch [1/2], Step [2680/16076], Loss: 5.2978\n",
      "Epoch [1/2], Step [2700/16076], Loss: 5.3122\n",
      "Epoch [1/2], Step [2720/16076], Loss: 5.3440\n",
      "Epoch [1/2], Step [2740/16076], Loss: 5.3955\n",
      "Epoch [1/2], Step [2760/16076], Loss: 5.3702\n",
      "Epoch [1/2], Step [2780/16076], Loss: 5.3242\n",
      "Epoch [1/2], Step [2800/16076], Loss: 5.1929\n",
      "Epoch [1/2], Step [2820/16076], Loss: 5.2938\n",
      "Epoch [1/2], Step [2840/16076], Loss: 5.3648\n",
      "Epoch [1/2], Step [2860/16076], Loss: 5.3663\n",
      "Epoch [1/2], Step [2880/16076], Loss: 5.3100\n",
      "Epoch [1/2], Step [2900/16076], Loss: 5.2797\n",
      "Epoch [1/2], Step [2920/16076], Loss: 5.2912\n",
      "Epoch [1/2], Step [2940/16076], Loss: 5.3101\n",
      "Epoch [1/2], Step [2960/16076], Loss: 5.2959\n",
      "Epoch [1/2], Step [2980/16076], Loss: 5.2745\n",
      "Epoch [1/2], Step [3000/16076], Loss: 5.2780\n",
      "Epoch [1/2], Step [3020/16076], Loss: 5.3015\n",
      "Epoch [1/2], Step [3040/16076], Loss: 5.3998\n",
      "Epoch [1/2], Step [3060/16076], Loss: 5.3026\n",
      "Epoch [1/2], Step [3080/16076], Loss: 5.3067\n",
      "Epoch [1/2], Step [3100/16076], Loss: 5.3410\n",
      "Epoch [1/2], Step [3120/16076], Loss: 5.3359\n",
      "Epoch [1/2], Step [3140/16076], Loss: 5.4138\n",
      "Epoch [1/2], Step [3160/16076], Loss: 5.2922\n",
      "Epoch [1/2], Step [3180/16076], Loss: 5.2697\n",
      "Epoch [1/2], Step [3200/16076], Loss: 5.2523\n",
      "Epoch [1/2], Step [3220/16076], Loss: 5.3688\n",
      "Epoch [1/2], Step [3240/16076], Loss: 5.1892\n",
      "Epoch [1/2], Step [3260/16076], Loss: 5.2619\n",
      "Epoch [1/2], Step [3280/16076], Loss: 5.3212\n",
      "Epoch [1/2], Step [3300/16076], Loss: 5.2062\n",
      "Epoch [1/2], Step [3320/16076], Loss: 5.2375\n",
      "Epoch [1/2], Step [3340/16076], Loss: 5.2419\n",
      "Epoch [1/2], Step [3360/16076], Loss: 5.3203\n",
      "Epoch [1/2], Step [3380/16076], Loss: 5.2428\n",
      "Epoch [1/2], Step [3400/16076], Loss: 5.2745\n",
      "Epoch [1/2], Step [3420/16076], Loss: 5.2737\n",
      "Epoch [1/2], Step [3440/16076], Loss: 5.2236\n",
      "Epoch [1/2], Step [3460/16076], Loss: 5.1222\n",
      "Epoch [1/2], Step [3480/16076], Loss: 5.2771\n",
      "Epoch [1/2], Step [3500/16076], Loss: 5.2700\n",
      "Epoch [1/2], Step [3520/16076], Loss: 5.2588\n",
      "Epoch [1/2], Step [3540/16076], Loss: 5.2870\n",
      "Epoch [1/2], Step [3560/16076], Loss: 5.2981\n",
      "Epoch [1/2], Step [3580/16076], Loss: 5.1901\n",
      "Epoch [1/2], Step [3600/16076], Loss: 5.2011\n",
      "Epoch [1/2], Step [3620/16076], Loss: 5.2205\n",
      "Epoch [1/2], Step [3640/16076], Loss: 5.2596\n",
      "Epoch [1/2], Step [3660/16076], Loss: 5.2467\n",
      "Epoch [1/2], Step [3680/16076], Loss: 5.1577\n",
      "Epoch [1/2], Step [3700/16076], Loss: 5.2882\n",
      "Epoch [1/2], Step [3720/16076], Loss: 5.1477\n",
      "Epoch [1/2], Step [3740/16076], Loss: 5.1873\n",
      "Epoch [1/2], Step [3760/16076], Loss: 5.1572\n",
      "Epoch [1/2], Step [3780/16076], Loss: 5.2766\n",
      "Epoch [1/2], Step [3800/16076], Loss: 5.1492\n",
      "Epoch [1/2], Step [3820/16076], Loss: 5.2038\n",
      "Epoch [1/2], Step [3840/16076], Loss: 5.2709\n",
      "Epoch [1/2], Step [3860/16076], Loss: 5.2629\n",
      "Epoch [1/2], Step [3880/16076], Loss: 5.2240\n",
      "Epoch [1/2], Step [3900/16076], Loss: 5.1375\n",
      "Epoch [1/2], Step [3920/16076], Loss: 5.2208\n",
      "Epoch [1/2], Step [3940/16076], Loss: 5.2287\n",
      "Epoch [1/2], Step [3960/16076], Loss: 5.2163\n",
      "Epoch [1/2], Step [3980/16076], Loss: 5.1945\n",
      "Epoch [1/2], Step [4000/16076], Loss: 5.1065\n",
      "Epoch [1/2], Step [4020/16076], Loss: 5.1112\n",
      "Epoch [1/2], Step [4040/16076], Loss: 5.2404\n",
      "Epoch [1/2], Step [4060/16076], Loss: 5.1233\n",
      "Epoch [1/2], Step [4080/16076], Loss: 5.2337\n",
      "Epoch [1/2], Step [4100/16076], Loss: 5.1025\n",
      "Epoch [1/2], Step [4120/16076], Loss: 5.1722\n",
      "Epoch [1/2], Step [4140/16076], Loss: 5.2521\n",
      "Epoch [1/2], Step [4160/16076], Loss: 5.0621\n",
      "Epoch [1/2], Step [4180/16076], Loss: 5.2356\n",
      "Epoch [1/2], Step [4200/16076], Loss: 5.1845\n",
      "Epoch [1/2], Step [4220/16076], Loss: 5.3180\n",
      "Epoch [1/2], Step [4240/16076], Loss: 5.1990\n",
      "Epoch [1/2], Step [4260/16076], Loss: 5.1292\n",
      "Epoch [1/2], Step [4280/16076], Loss: 5.2370\n",
      "Epoch [1/2], Step [4300/16076], Loss: 5.1989\n",
      "Epoch [1/2], Step [4320/16076], Loss: 5.1588\n",
      "Epoch [1/2], Step [4340/16076], Loss: 5.0715\n",
      "Epoch [1/2], Step [4360/16076], Loss: 5.2275\n",
      "Epoch [1/2], Step [4380/16076], Loss: 5.1824\n",
      "Epoch [1/2], Step [4400/16076], Loss: 5.0828\n",
      "Epoch [1/2], Step [4420/16076], Loss: 5.2442\n",
      "Epoch [1/2], Step [4440/16076], Loss: 5.2052\n",
      "Epoch [1/2], Step [4460/16076], Loss: 5.1646\n",
      "Epoch [1/2], Step [4480/16076], Loss: 5.2199\n",
      "Epoch [1/2], Step [4500/16076], Loss: 5.1641\n",
      "Epoch [1/2], Step [4520/16076], Loss: 5.2800\n",
      "Epoch [1/2], Step [4540/16076], Loss: 5.1241\n",
      "Epoch [1/2], Step [4560/16076], Loss: 5.0975\n",
      "Epoch [1/2], Step [4580/16076], Loss: 5.2014\n",
      "Epoch [1/2], Step [4600/16076], Loss: 5.2270\n",
      "Epoch [1/2], Step [4620/16076], Loss: 5.1500\n",
      "Epoch [1/2], Step [4640/16076], Loss: 5.1432\n",
      "Epoch [1/2], Step [4660/16076], Loss: 5.2217\n",
      "Epoch [1/2], Step [4680/16076], Loss: 5.2273\n",
      "Epoch [1/2], Step [4700/16076], Loss: 5.1385\n",
      "Epoch [1/2], Step [4720/16076], Loss: 5.1817\n",
      "Epoch [1/2], Step [4740/16076], Loss: 5.1627\n",
      "Epoch [1/2], Step [4760/16076], Loss: 5.1241\n",
      "Epoch [1/2], Step [4780/16076], Loss: 5.1185\n",
      "Epoch [1/2], Step [4800/16076], Loss: 5.1252\n",
      "Epoch [1/2], Step [4820/16076], Loss: 5.1578\n",
      "Epoch [1/2], Step [4840/16076], Loss: 5.1023\n",
      "Epoch [1/2], Step [4860/16076], Loss: 5.2005\n",
      "Epoch [1/2], Step [4880/16076], Loss: 5.1552\n",
      "Epoch [1/2], Step [4900/16076], Loss: 5.2110\n",
      "Epoch [1/2], Step [4920/16076], Loss: 5.0632\n",
      "Epoch [1/2], Step [4940/16076], Loss: 5.1010\n",
      "Epoch [1/2], Step [4960/16076], Loss: 5.1015\n",
      "Epoch [1/2], Step [4980/16076], Loss: 5.0573\n",
      "Epoch [1/2], Step [5000/16076], Loss: 5.2129\n",
      "Epoch [1/2], Step [5020/16076], Loss: 5.0511\n",
      "Epoch [1/2], Step [5040/16076], Loss: 5.1086\n",
      "Epoch [1/2], Step [5060/16076], Loss: 5.0736\n",
      "Epoch [1/2], Step [5080/16076], Loss: 5.0927\n",
      "Epoch [1/2], Step [5100/16076], Loss: 5.1759\n",
      "Epoch [1/2], Step [5120/16076], Loss: 5.0510\n",
      "Epoch [1/2], Step [5140/16076], Loss: 5.1305\n",
      "Epoch [1/2], Step [5160/16076], Loss: 5.1425\n",
      "Epoch [1/2], Step [5180/16076], Loss: 5.1349\n",
      "Epoch [1/2], Step [5200/16076], Loss: 5.1069\n",
      "Epoch [1/2], Step [5220/16076], Loss: 5.1437\n",
      "Epoch [1/2], Step [5240/16076], Loss: 4.9932\n",
      "Epoch [1/2], Step [5260/16076], Loss: 5.0194\n",
      "Epoch [1/2], Step [5280/16076], Loss: 5.1398\n",
      "Epoch [1/2], Step [5300/16076], Loss: 5.2210\n",
      "Epoch [1/2], Step [5320/16076], Loss: 5.1527\n",
      "Epoch [1/2], Step [5340/16076], Loss: 5.1214\n",
      "Epoch [1/2], Step [5360/16076], Loss: 5.1102\n",
      "Epoch [1/2], Step [5380/16076], Loss: 5.0478\n",
      "Epoch [1/2], Step [5400/16076], Loss: 5.1577\n",
      "Epoch [1/2], Step [5420/16076], Loss: 5.1214\n",
      "Epoch [1/2], Step [5440/16076], Loss: 5.0699\n",
      "Epoch [1/2], Step [5460/16076], Loss: 5.1658\n",
      "Epoch [1/2], Step [5480/16076], Loss: 5.1008\n",
      "Epoch [1/2], Step [5500/16076], Loss: 4.9963\n",
      "Epoch [1/2], Step [5520/16076], Loss: 5.1178\n",
      "Epoch [1/2], Step [5540/16076], Loss: 4.9391\n",
      "Epoch [1/2], Step [5560/16076], Loss: 5.2003\n",
      "Epoch [1/2], Step [5580/16076], Loss: 5.1050\n",
      "Epoch [1/2], Step [5600/16076], Loss: 5.0607\n",
      "Epoch [1/2], Step [5620/16076], Loss: 5.0572\n",
      "Epoch [1/2], Step [5640/16076], Loss: 5.1022\n",
      "Epoch [1/2], Step [5660/16076], Loss: 5.0495\n",
      "Epoch [1/2], Step [5680/16076], Loss: 5.0792\n",
      "Epoch [1/2], Step [5700/16076], Loss: 5.1218\n",
      "Epoch [1/2], Step [5720/16076], Loss: 5.1271\n",
      "Epoch [1/2], Step [5740/16076], Loss: 5.0846\n",
      "Epoch [1/2], Step [5760/16076], Loss: 5.0322\n",
      "Epoch [1/2], Step [5780/16076], Loss: 5.1016\n",
      "Epoch [1/2], Step [5800/16076], Loss: 5.0948\n",
      "Epoch [1/2], Step [5820/16076], Loss: 5.0710\n",
      "Epoch [1/2], Step [5840/16076], Loss: 5.0962\n",
      "Epoch [1/2], Step [5860/16076], Loss: 5.0524\n",
      "Epoch [1/2], Step [5880/16076], Loss: 5.1518\n",
      "Epoch [1/2], Step [5900/16076], Loss: 5.0664\n",
      "Epoch [1/2], Step [5920/16076], Loss: 5.0707\n",
      "Epoch [1/2], Step [5940/16076], Loss: 5.0791\n",
      "Epoch [1/2], Step [5960/16076], Loss: 5.0507\n",
      "Epoch [1/2], Step [5980/16076], Loss: 4.9888\n",
      "Epoch [1/2], Step [6000/16076], Loss: 5.0754\n",
      "Epoch [1/2], Step [6020/16076], Loss: 5.0460\n",
      "Epoch [1/2], Step [6040/16076], Loss: 5.1498\n",
      "Epoch [1/2], Step [6060/16076], Loss: 5.0561\n",
      "Epoch [1/2], Step [6080/16076], Loss: 5.1453\n",
      "Epoch [1/2], Step [6100/16076], Loss: 5.0802\n",
      "Epoch [1/2], Step [6120/16076], Loss: 5.0829\n",
      "Epoch [1/2], Step [6140/16076], Loss: 5.0591\n",
      "Epoch [1/2], Step [6160/16076], Loss: 5.0533\n",
      "Epoch [1/2], Step [6180/16076], Loss: 5.0186\n",
      "Epoch [1/2], Step [6200/16076], Loss: 5.0770\n",
      "Epoch [1/2], Step [6220/16076], Loss: 5.0282\n",
      "Epoch [1/2], Step [6240/16076], Loss: 5.0573\n",
      "Epoch [1/2], Step [6260/16076], Loss: 4.9744\n",
      "Epoch [1/2], Step [6280/16076], Loss: 5.0203\n",
      "Epoch [1/2], Step [6300/16076], Loss: 5.1098\n",
      "Epoch [1/2], Step [6320/16076], Loss: 5.0756\n",
      "Epoch [1/2], Step [6340/16076], Loss: 5.0724\n",
      "Epoch [1/2], Step [6360/16076], Loss: 5.1033\n",
      "Epoch [1/2], Step [6380/16076], Loss: 5.0223\n",
      "Epoch [1/2], Step [6400/16076], Loss: 5.0219\n",
      "Epoch [1/2], Step [6420/16076], Loss: 4.9326\n",
      "Epoch [1/2], Step [6440/16076], Loss: 5.0499\n",
      "Epoch [1/2], Step [6460/16076], Loss: 4.9108\n",
      "Epoch [1/2], Step [6480/16076], Loss: 5.0554\n",
      "Epoch [1/2], Step [6500/16076], Loss: 5.0448\n",
      "Epoch [1/2], Step [6520/16076], Loss: 5.0292\n",
      "Epoch [1/2], Step [6540/16076], Loss: 5.0765\n",
      "Epoch [1/2], Step [6560/16076], Loss: 5.0793\n",
      "Epoch [1/2], Step [6580/16076], Loss: 5.0028\n",
      "Epoch [1/2], Step [6600/16076], Loss: 5.0307\n",
      "Epoch [1/2], Step [6620/16076], Loss: 5.0285\n",
      "Epoch [1/2], Step [6640/16076], Loss: 5.0318\n",
      "Epoch [1/2], Step [6660/16076], Loss: 5.0612\n",
      "Epoch [1/2], Step [6680/16076], Loss: 4.9417\n",
      "Epoch [1/2], Step [6700/16076], Loss: 4.9538\n",
      "Epoch [1/2], Step [6720/16076], Loss: 4.9464\n",
      "Epoch [1/2], Step [6740/16076], Loss: 5.0234\n",
      "Epoch [1/2], Step [6760/16076], Loss: 4.9457\n",
      "Epoch [1/2], Step [6780/16076], Loss: 5.0937\n",
      "Epoch [1/2], Step [6800/16076], Loss: 5.0678\n",
      "Epoch [1/2], Step [6820/16076], Loss: 5.0137\n",
      "Epoch [1/2], Step [6840/16076], Loss: 5.0669\n",
      "Epoch [1/2], Step [6860/16076], Loss: 5.0688\n",
      "Epoch [1/2], Step [6880/16076], Loss: 4.9532\n",
      "Epoch [1/2], Step [6900/16076], Loss: 5.0901\n",
      "Epoch [1/2], Step [6920/16076], Loss: 5.0624\n",
      "Epoch [1/2], Step [6940/16076], Loss: 4.9984\n",
      "Epoch [1/2], Step [6960/16076], Loss: 5.0031\n",
      "Epoch [1/2], Step [6980/16076], Loss: 5.0325\n",
      "Epoch [1/2], Step [7000/16076], Loss: 5.0577\n",
      "Epoch [1/2], Step [7020/16076], Loss: 4.9361\n",
      "Epoch [1/2], Step [7040/16076], Loss: 4.9921\n",
      "Epoch [1/2], Step [7060/16076], Loss: 4.9948\n",
      "Epoch [1/2], Step [7080/16076], Loss: 5.0094\n",
      "Epoch [1/2], Step [7100/16076], Loss: 5.0039\n",
      "Epoch [1/2], Step [7120/16076], Loss: 5.0188\n",
      "Epoch [1/2], Step [7140/16076], Loss: 4.9860\n",
      "Epoch [1/2], Step [7160/16076], Loss: 5.0375\n",
      "Epoch [1/2], Step [7180/16076], Loss: 5.0499\n",
      "Epoch [1/2], Step [7200/16076], Loss: 4.9506\n",
      "Epoch [1/2], Step [7220/16076], Loss: 5.0534\n",
      "Epoch [1/2], Step [7240/16076], Loss: 5.0372\n",
      "Epoch [1/2], Step [7260/16076], Loss: 5.0519\n",
      "Epoch [1/2], Step [7280/16076], Loss: 4.8629\n",
      "Epoch [1/2], Step [7300/16076], Loss: 5.0351\n",
      "Epoch [1/2], Step [7320/16076], Loss: 4.9520\n",
      "Epoch [1/2], Step [7340/16076], Loss: 4.9603\n",
      "Epoch [1/2], Step [7360/16076], Loss: 4.9997\n",
      "Epoch [1/2], Step [7380/16076], Loss: 5.0409\n",
      "Epoch [1/2], Step [7400/16076], Loss: 4.9560\n",
      "Epoch [1/2], Step [7420/16076], Loss: 5.1135\n",
      "Epoch [1/2], Step [7440/16076], Loss: 5.0200\n",
      "Epoch [1/2], Step [7460/16076], Loss: 5.0212\n",
      "Epoch [1/2], Step [7480/16076], Loss: 4.9762\n",
      "Epoch [1/2], Step [7500/16076], Loss: 5.0807\n",
      "Epoch [1/2], Step [7520/16076], Loss: 5.0819\n",
      "Epoch [1/2], Step [7540/16076], Loss: 4.9975\n",
      "Epoch [1/2], Step [7560/16076], Loss: 5.0224\n",
      "Epoch [1/2], Step [7580/16076], Loss: 5.1027\n",
      "Epoch [1/2], Step [7600/16076], Loss: 4.9792\n",
      "Epoch [1/2], Step [7620/16076], Loss: 5.0268\n",
      "Epoch [1/2], Step [7640/16076], Loss: 5.1172\n",
      "Epoch [1/2], Step [7660/16076], Loss: 4.9278\n",
      "Epoch [1/2], Step [7680/16076], Loss: 4.9915\n",
      "Epoch [1/2], Step [7700/16076], Loss: 5.0643\n",
      "Epoch [1/2], Step [7720/16076], Loss: 4.9502\n",
      "Epoch [1/2], Step [7740/16076], Loss: 4.9667\n",
      "Epoch [1/2], Step [7760/16076], Loss: 5.0763\n",
      "Epoch [1/2], Step [7780/16076], Loss: 5.0132\n",
      "Epoch [1/2], Step [7800/16076], Loss: 4.9331\n",
      "Epoch [1/2], Step [7820/16076], Loss: 5.0897\n",
      "Epoch [1/2], Step [7840/16076], Loss: 5.1120\n",
      "Epoch [1/2], Step [7860/16076], Loss: 5.0094\n",
      "Epoch [1/2], Step [7880/16076], Loss: 5.0384\n",
      "Epoch [1/2], Step [7900/16076], Loss: 4.9898\n",
      "Epoch [1/2], Step [7920/16076], Loss: 5.0531\n",
      "Epoch [1/2], Step [7940/16076], Loss: 4.9946\n",
      "Epoch [1/2], Step [7960/16076], Loss: 5.0250\n",
      "Epoch [1/2], Step [7980/16076], Loss: 5.0463\n",
      "Epoch [1/2], Step [8000/16076], Loss: 5.0516\n",
      "Epoch [1/2], Step [8020/16076], Loss: 4.9325\n",
      "Epoch [1/2], Step [8040/16076], Loss: 5.0075\n",
      "Epoch [1/2], Step [8060/16076], Loss: 5.0094\n",
      "Epoch [1/2], Step [8080/16076], Loss: 5.0317\n",
      "Epoch [1/2], Step [8100/16076], Loss: 4.9565\n",
      "Epoch [1/2], Step [8120/16076], Loss: 5.0563\n",
      "Epoch [1/2], Step [8140/16076], Loss: 5.0318\n",
      "Epoch [1/2], Step [8160/16076], Loss: 4.8599\n",
      "Epoch [1/2], Step [8180/16076], Loss: 4.9162\n",
      "Epoch [1/2], Step [8200/16076], Loss: 5.0018\n",
      "Epoch [1/2], Step [8220/16076], Loss: 4.9756\n",
      "Epoch [1/2], Step [8240/16076], Loss: 5.0054\n",
      "Epoch [1/2], Step [8260/16076], Loss: 5.0818\n",
      "Epoch [1/2], Step [8280/16076], Loss: 4.9380\n",
      "Epoch [1/2], Step [8300/16076], Loss: 4.8936\n",
      "Epoch [1/2], Step [8320/16076], Loss: 5.0162\n",
      "Epoch [1/2], Step [8340/16076], Loss: 4.9998\n",
      "Epoch [1/2], Step [8360/16076], Loss: 5.0219\n",
      "Epoch [1/2], Step [8380/16076], Loss: 4.9936\n",
      "Epoch [1/2], Step [8400/16076], Loss: 5.0295\n",
      "Epoch [1/2], Step [8420/16076], Loss: 4.9402\n",
      "Epoch [1/2], Step [8440/16076], Loss: 5.0244\n",
      "Epoch [1/2], Step [8460/16076], Loss: 4.9290\n",
      "Epoch [1/2], Step [8480/16076], Loss: 4.9537\n",
      "Epoch [1/2], Step [8500/16076], Loss: 4.9611\n",
      "Epoch [1/2], Step [8520/16076], Loss: 4.9850\n",
      "Epoch [1/2], Step [8540/16076], Loss: 4.8663\n",
      "Epoch [1/2], Step [8560/16076], Loss: 4.9725\n",
      "Epoch [1/2], Step [8580/16076], Loss: 4.9155\n",
      "Epoch [1/2], Step [8600/16076], Loss: 5.0183\n",
      "Epoch [1/2], Step [8620/16076], Loss: 5.0011\n",
      "Epoch [1/2], Step [8640/16076], Loss: 4.9691\n",
      "Epoch [1/2], Step [8660/16076], Loss: 4.9633\n",
      "Epoch [1/2], Step [8680/16076], Loss: 4.9211\n",
      "Epoch [1/2], Step [8700/16076], Loss: 4.9343\n",
      "Epoch [1/2], Step [8720/16076], Loss: 4.8448\n",
      "Epoch [1/2], Step [8740/16076], Loss: 4.9954\n",
      "Epoch [1/2], Step [8760/16076], Loss: 4.8322\n",
      "Epoch [1/2], Step [8780/16076], Loss: 4.9579\n",
      "Epoch [1/2], Step [8800/16076], Loss: 4.9416\n",
      "Epoch [1/2], Step [8820/16076], Loss: 4.9708\n",
      "Epoch [1/2], Step [8840/16076], Loss: 4.9823\n",
      "Epoch [1/2], Step [8860/16076], Loss: 5.0273\n",
      "Epoch [1/2], Step [8880/16076], Loss: 4.9728\n",
      "Epoch [1/2], Step [8900/16076], Loss: 4.9382\n",
      "Epoch [1/2], Step [8920/16076], Loss: 4.9058\n",
      "Epoch [1/2], Step [8940/16076], Loss: 4.9310\n",
      "Epoch [1/2], Step [8960/16076], Loss: 4.9876\n",
      "Epoch [1/2], Step [8980/16076], Loss: 4.9398\n",
      "Epoch [1/2], Step [9000/16076], Loss: 4.9427\n",
      "Epoch [1/2], Step [9020/16076], Loss: 4.9583\n",
      "Epoch [1/2], Step [9040/16076], Loss: 4.9699\n",
      "Epoch [1/2], Step [9060/16076], Loss: 4.9699\n",
      "Epoch [1/2], Step [9080/16076], Loss: 4.9492\n",
      "Epoch [1/2], Step [9100/16076], Loss: 4.9636\n",
      "Epoch [1/2], Step [9120/16076], Loss: 4.9669\n",
      "Epoch [1/2], Step [9140/16076], Loss: 4.9468\n",
      "Epoch [1/2], Step [9160/16076], Loss: 4.9536\n",
      "Epoch [1/2], Step [9180/16076], Loss: 4.9633\n",
      "Epoch [1/2], Step [9200/16076], Loss: 5.0376\n",
      "Epoch [1/2], Step [9220/16076], Loss: 4.9569\n",
      "Epoch [1/2], Step [9240/16076], Loss: 4.9493\n",
      "Epoch [1/2], Step [9260/16076], Loss: 4.9463\n",
      "Epoch [1/2], Step [9280/16076], Loss: 4.8755\n",
      "Epoch [1/2], Step [9300/16076], Loss: 4.9507\n",
      "Epoch [1/2], Step [9320/16076], Loss: 4.9666\n",
      "Epoch [1/2], Step [9340/16076], Loss: 4.8788\n",
      "Epoch [1/2], Step [9360/16076], Loss: 4.9748\n",
      "Epoch [1/2], Step [9380/16076], Loss: 4.8986\n",
      "Epoch [1/2], Step [9400/16076], Loss: 4.9134\n",
      "Epoch [1/2], Step [9420/16076], Loss: 4.9691\n",
      "Epoch [1/2], Step [9440/16076], Loss: 4.9952\n",
      "Epoch [1/2], Step [9460/16076], Loss: 4.9517\n",
      "Epoch [1/2], Step [9480/16076], Loss: 4.9810\n",
      "Epoch [1/2], Step [9500/16076], Loss: 5.0144\n",
      "Epoch [1/2], Step [9520/16076], Loss: 4.9733\n",
      "Epoch [1/2], Step [9540/16076], Loss: 4.9015\n",
      "Epoch [1/2], Step [9560/16076], Loss: 4.9623\n",
      "Epoch [1/2], Step [9580/16076], Loss: 5.0011\n",
      "Epoch [1/2], Step [9600/16076], Loss: 4.9605\n",
      "Epoch [1/2], Step [9620/16076], Loss: 4.9387\n",
      "Epoch [1/2], Step [9640/16076], Loss: 4.8643\n",
      "Epoch [1/2], Step [9660/16076], Loss: 5.0354\n",
      "Epoch [1/2], Step [9680/16076], Loss: 5.0081\n",
      "Epoch [1/2], Step [9700/16076], Loss: 4.8818\n",
      "Epoch [1/2], Step [9720/16076], Loss: 5.0128\n",
      "Epoch [1/2], Step [9740/16076], Loss: 4.9481\n",
      "Epoch [1/2], Step [9760/16076], Loss: 5.0039\n",
      "Epoch [1/2], Step [9780/16076], Loss: 4.8856\n",
      "Epoch [1/2], Step [9800/16076], Loss: 4.9749\n",
      "Epoch [1/2], Step [9820/16076], Loss: 5.0253\n",
      "Epoch [1/2], Step [9840/16076], Loss: 4.8878\n",
      "Epoch [1/2], Step [9860/16076], Loss: 4.9292\n",
      "Epoch [1/2], Step [9880/16076], Loss: 4.8266\n",
      "Epoch [1/2], Step [9900/16076], Loss: 4.9110\n",
      "Epoch [1/2], Step [9920/16076], Loss: 5.0364\n",
      "Epoch [1/2], Step [9940/16076], Loss: 4.8799\n",
      "Epoch [1/2], Step [9960/16076], Loss: 5.0288\n",
      "Epoch [1/2], Step [9980/16076], Loss: 4.9201\n",
      "Epoch [1/2], Step [10000/16076], Loss: 4.9434\n",
      "Epoch [1/2], Step [10020/16076], Loss: 4.8307\n",
      "Epoch [1/2], Step [10040/16076], Loss: 5.0210\n",
      "Epoch [1/2], Step [10060/16076], Loss: 4.9369\n",
      "Epoch [1/2], Step [10080/16076], Loss: 4.9256\n",
      "Epoch [1/2], Step [10100/16076], Loss: 4.9501\n",
      "Epoch [1/2], Step [10120/16076], Loss: 4.8725\n",
      "Epoch [1/2], Step [10140/16076], Loss: 4.8998\n",
      "Epoch [1/2], Step [10160/16076], Loss: 4.9411\n",
      "Epoch [1/2], Step [10180/16076], Loss: 4.9217\n",
      "Epoch [1/2], Step [10200/16076], Loss: 4.9578\n",
      "Epoch [1/2], Step [10220/16076], Loss: 4.9588\n",
      "Epoch [1/2], Step [10240/16076], Loss: 4.9551\n",
      "Epoch [1/2], Step [10260/16076], Loss: 4.9423\n",
      "Epoch [1/2], Step [10280/16076], Loss: 4.9505\n",
      "Epoch [1/2], Step [10300/16076], Loss: 5.0305\n",
      "Epoch [1/2], Step [10320/16076], Loss: 4.8699\n",
      "Epoch [1/2], Step [10340/16076], Loss: 5.0393\n",
      "Epoch [1/2], Step [10360/16076], Loss: 4.9374\n",
      "Epoch [1/2], Step [10380/16076], Loss: 4.9214\n",
      "Epoch [1/2], Step [10400/16076], Loss: 4.9530\n",
      "Epoch [1/2], Step [10420/16076], Loss: 4.9675\n",
      "Epoch [1/2], Step [10440/16076], Loss: 4.9037\n",
      "Epoch [1/2], Step [10460/16076], Loss: 4.8769\n",
      "Epoch [1/2], Step [10480/16076], Loss: 4.9507\n",
      "Epoch [1/2], Step [10500/16076], Loss: 4.8755\n",
      "Epoch [1/2], Step [10520/16076], Loss: 4.9180\n",
      "Epoch [1/2], Step [10540/16076], Loss: 4.8916\n",
      "Epoch [1/2], Step [10560/16076], Loss: 5.0537\n",
      "Epoch [1/2], Step [10580/16076], Loss: 4.9784\n",
      "Epoch [1/2], Step [10600/16076], Loss: 4.8941\n",
      "Epoch [1/2], Step [10620/16076], Loss: 4.9576\n",
      "Epoch [1/2], Step [10640/16076], Loss: 4.9130\n",
      "Epoch [1/2], Step [10660/16076], Loss: 4.8471\n",
      "Epoch [1/2], Step [10680/16076], Loss: 4.9874\n",
      "Epoch [1/2], Step [10700/16076], Loss: 4.8994\n",
      "Epoch [1/2], Step [10720/16076], Loss: 4.9272\n",
      "Epoch [1/2], Step [10740/16076], Loss: 4.8864\n",
      "Epoch [1/2], Step [10760/16076], Loss: 4.9379\n",
      "Epoch [1/2], Step [10780/16076], Loss: 4.9537\n",
      "Epoch [1/2], Step [10800/16076], Loss: 4.8714\n",
      "Epoch [1/2], Step [10820/16076], Loss: 4.9555\n",
      "Epoch [1/2], Step [10840/16076], Loss: 4.8798\n",
      "Epoch [1/2], Step [10860/16076], Loss: 4.8109\n",
      "Epoch [1/2], Step [10880/16076], Loss: 4.9182\n",
      "Epoch [1/2], Step [10900/16076], Loss: 4.9061\n",
      "Epoch [1/2], Step [10920/16076], Loss: 4.9215\n",
      "Epoch [1/2], Step [10940/16076], Loss: 4.9535\n",
      "Epoch [1/2], Step [10960/16076], Loss: 4.9528\n",
      "Epoch [1/2], Step [10980/16076], Loss: 4.8936\n",
      "Epoch [1/2], Step [11000/16076], Loss: 4.9532\n",
      "Epoch [1/2], Step [11020/16076], Loss: 4.9715\n",
      "Epoch [1/2], Step [11040/16076], Loss: 4.8495\n",
      "Epoch [1/2], Step [11060/16076], Loss: 4.9316\n",
      "Epoch [1/2], Step [11080/16076], Loss: 4.8629\n",
      "Epoch [1/2], Step [11100/16076], Loss: 4.9177\n",
      "Epoch [1/2], Step [11120/16076], Loss: 4.9352\n",
      "Epoch [1/2], Step [11140/16076], Loss: 4.9132\n",
      "Epoch [1/2], Step [11160/16076], Loss: 4.9838\n",
      "Epoch [1/2], Step [11180/16076], Loss: 4.9198\n",
      "Epoch [1/2], Step [11200/16076], Loss: 4.8548\n",
      "Epoch [1/2], Step [11220/16076], Loss: 4.9726\n",
      "Epoch [1/2], Step [11240/16076], Loss: 4.9573\n",
      "Epoch [1/2], Step [11260/16076], Loss: 4.8718\n",
      "Epoch [1/2], Step [11280/16076], Loss: 4.9475\n",
      "Epoch [1/2], Step [11300/16076], Loss: 4.8088\n",
      "Epoch [1/2], Step [11320/16076], Loss: 4.9424\n",
      "Epoch [1/2], Step [11340/16076], Loss: 4.8790\n",
      "Epoch [1/2], Step [11360/16076], Loss: 5.0112\n",
      "Epoch [1/2], Step [11380/16076], Loss: 4.9624\n",
      "Epoch [1/2], Step [11400/16076], Loss: 4.8733\n",
      "Epoch [1/2], Step [11420/16076], Loss: 5.0226\n",
      "Epoch [1/2], Step [11440/16076], Loss: 4.9088\n",
      "Epoch [1/2], Step [11460/16076], Loss: 4.9454\n",
      "Epoch [1/2], Step [11480/16076], Loss: 4.8571\n",
      "Epoch [1/2], Step [11500/16076], Loss: 4.8625\n",
      "Epoch [1/2], Step [11520/16076], Loss: 4.9603\n",
      "Epoch [1/2], Step [11540/16076], Loss: 4.9571\n",
      "Epoch [1/2], Step [11560/16076], Loss: 4.9706\n",
      "Epoch [1/2], Step [11580/16076], Loss: 4.9393\n",
      "Epoch [1/2], Step [11600/16076], Loss: 4.9755\n",
      "Epoch [1/2], Step [11620/16076], Loss: 4.9249\n",
      "Epoch [1/2], Step [11640/16076], Loss: 4.8990\n",
      "Epoch [1/2], Step [11660/16076], Loss: 5.0238\n",
      "Epoch [1/2], Step [11680/16076], Loss: 4.9405\n",
      "Epoch [1/2], Step [11700/16076], Loss: 4.8394\n",
      "Epoch [1/2], Step [11720/16076], Loss: 4.9082\n",
      "Epoch [1/2], Step [11740/16076], Loss: 4.9448\n",
      "Epoch [1/2], Step [11760/16076], Loss: 4.8929\n",
      "Epoch [1/2], Step [11780/16076], Loss: 4.9474\n",
      "Epoch [1/2], Step [11800/16076], Loss: 4.9772\n",
      "Epoch [1/2], Step [11820/16076], Loss: 4.9715\n",
      "Epoch [1/2], Step [11840/16076], Loss: 4.9635\n",
      "Epoch [1/2], Step [11860/16076], Loss: 4.9084\n",
      "Epoch [1/2], Step [11880/16076], Loss: 4.8713\n",
      "Epoch [1/2], Step [11900/16076], Loss: 4.8800\n",
      "Epoch [1/2], Step [11920/16076], Loss: 4.7833\n",
      "Epoch [1/2], Step [11940/16076], Loss: 4.8550\n",
      "Epoch [1/2], Step [11960/16076], Loss: 4.9403\n",
      "Epoch [1/2], Step [11980/16076], Loss: 4.8859\n",
      "Epoch [1/2], Step [12000/16076], Loss: 4.8077\n",
      "Epoch [1/2], Step [12020/16076], Loss: 4.9171\n",
      "Epoch [1/2], Step [12040/16076], Loss: 4.9255\n",
      "Epoch [1/2], Step [12060/16076], Loss: 4.8672\n",
      "Epoch [1/2], Step [12080/16076], Loss: 4.8670\n",
      "Epoch [1/2], Step [12100/16076], Loss: 4.8108\n",
      "Epoch [1/2], Step [12120/16076], Loss: 4.8901\n",
      "Epoch [1/2], Step [12140/16076], Loss: 4.9417\n",
      "Epoch [1/2], Step [12160/16076], Loss: 4.8798\n",
      "Epoch [1/2], Step [12180/16076], Loss: 4.8358\n",
      "Epoch [1/2], Step [12200/16076], Loss: 4.9106\n",
      "Epoch [1/2], Step [12220/16076], Loss: 4.9172\n",
      "Epoch [1/2], Step [12240/16076], Loss: 4.9643\n",
      "Epoch [1/2], Step [12260/16076], Loss: 4.8621\n",
      "Epoch [1/2], Step [12280/16076], Loss: 4.9341\n",
      "Epoch [1/2], Step [12300/16076], Loss: 4.8879\n",
      "Epoch [1/2], Step [12320/16076], Loss: 4.9170\n",
      "Epoch [1/2], Step [12340/16076], Loss: 4.8626\n",
      "Epoch [1/2], Step [12360/16076], Loss: 4.8933\n",
      "Epoch [1/2], Step [12380/16076], Loss: 4.8638\n",
      "Epoch [1/2], Step [12400/16076], Loss: 4.8878\n",
      "Epoch [1/2], Step [12420/16076], Loss: 4.9547\n",
      "Epoch [1/2], Step [12440/16076], Loss: 4.8844\n",
      "Epoch [1/2], Step [12460/16076], Loss: 4.8303\n",
      "Epoch [1/2], Step [12480/16076], Loss: 4.8803\n",
      "Epoch [1/2], Step [12500/16076], Loss: 4.9060\n",
      "Epoch [1/2], Step [12520/16076], Loss: 4.8491\n",
      "Epoch [1/2], Step [12540/16076], Loss: 4.8494\n",
      "Epoch [1/2], Step [12560/16076], Loss: 4.8358\n",
      "Epoch [1/2], Step [12580/16076], Loss: 4.8849\n",
      "Epoch [1/2], Step [12600/16076], Loss: 4.9492\n",
      "Epoch [1/2], Step [12620/16076], Loss: 4.8551\n",
      "Epoch [1/2], Step [12640/16076], Loss: 4.9845\n",
      "Epoch [1/2], Step [12660/16076], Loss: 4.8277\n",
      "Epoch [1/2], Step [12680/16076], Loss: 4.9203\n",
      "Epoch [1/2], Step [12700/16076], Loss: 4.8313\n",
      "Epoch [1/2], Step [12720/16076], Loss: 4.9325\n",
      "Epoch [1/2], Step [12740/16076], Loss: 4.8109\n",
      "Epoch [1/2], Step [12760/16076], Loss: 4.8638\n",
      "Epoch [1/2], Step [12780/16076], Loss: 4.8371\n",
      "Epoch [1/2], Step [12800/16076], Loss: 4.9728\n",
      "Epoch [1/2], Step [12820/16076], Loss: 4.9069\n",
      "Epoch [1/2], Step [12840/16076], Loss: 4.8996\n",
      "Epoch [1/2], Step [12860/16076], Loss: 4.8413\n",
      "Epoch [1/2], Step [12880/16076], Loss: 4.8603\n",
      "Epoch [1/2], Step [12900/16076], Loss: 4.8490\n",
      "Epoch [1/2], Step [12920/16076], Loss: 4.8764\n",
      "Epoch [1/2], Step [12940/16076], Loss: 4.8272\n",
      "Epoch [1/2], Step [12960/16076], Loss: 4.8050\n",
      "Epoch [1/2], Step [12980/16076], Loss: 4.9629\n",
      "Epoch [1/2], Step [13000/16076], Loss: 4.9087\n",
      "Epoch [1/2], Step [13020/16076], Loss: 4.8988\n",
      "Epoch [1/2], Step [13040/16076], Loss: 4.8708\n",
      "Epoch [1/2], Step [13060/16076], Loss: 4.8335\n",
      "Epoch [1/2], Step [13080/16076], Loss: 4.9647\n",
      "Epoch [1/2], Step [13100/16076], Loss: 4.8433\n",
      "Epoch [1/2], Step [13120/16076], Loss: 4.8914\n",
      "Epoch [1/2], Step [13140/16076], Loss: 4.9595\n",
      "Epoch [1/2], Step [13160/16076], Loss: 4.8907\n",
      "Epoch [1/2], Step [13180/16076], Loss: 4.8852\n",
      "Epoch [1/2], Step [13200/16076], Loss: 4.8078\n",
      "Epoch [1/2], Step [13220/16076], Loss: 4.9632\n",
      "Epoch [1/2], Step [13240/16076], Loss: 4.8849\n",
      "Epoch [1/2], Step [13260/16076], Loss: 4.8757\n",
      "Epoch [1/2], Step [13280/16076], Loss: 4.8257\n",
      "Epoch [1/2], Step [13300/16076], Loss: 4.8831\n",
      "Epoch [1/2], Step [13320/16076], Loss: 4.8561\n",
      "Epoch [1/2], Step [13340/16076], Loss: 4.8071\n",
      "Epoch [1/2], Step [13360/16076], Loss: 4.8321\n",
      "Epoch [1/2], Step [13380/16076], Loss: 4.8801\n",
      "Epoch [1/2], Step [13400/16076], Loss: 4.8308\n",
      "Epoch [1/2], Step [13420/16076], Loss: 4.9183\n",
      "Epoch [1/2], Step [13440/16076], Loss: 4.9585\n",
      "Epoch [1/2], Step [13460/16076], Loss: 4.7931\n",
      "Epoch [1/2], Step [13480/16076], Loss: 4.9102\n",
      "Epoch [1/2], Step [13500/16076], Loss: 4.9571\n",
      "Epoch [1/2], Step [13520/16076], Loss: 4.9353\n",
      "Epoch [1/2], Step [13540/16076], Loss: 4.8208\n",
      "Epoch [1/2], Step [13560/16076], Loss: 4.8969\n",
      "Epoch [1/2], Step [13580/16076], Loss: 4.8195\n",
      "Epoch [1/2], Step [13600/16076], Loss: 4.9436\n",
      "Epoch [1/2], Step [13620/16076], Loss: 4.9430\n",
      "Epoch [1/2], Step [13640/16076], Loss: 4.8239\n",
      "Epoch [1/2], Step [13660/16076], Loss: 4.8957\n",
      "Epoch [1/2], Step [13680/16076], Loss: 4.9183\n",
      "Epoch [1/2], Step [13700/16076], Loss: 4.8900\n",
      "Epoch [1/2], Step [13720/16076], Loss: 4.8875\n",
      "Epoch [1/2], Step [13740/16076], Loss: 4.8038\n",
      "Epoch [1/2], Step [13760/16076], Loss: 4.8732\n",
      "Epoch [1/2], Step [13780/16076], Loss: 4.8062\n",
      "Epoch [1/2], Step [13800/16076], Loss: 4.8413\n",
      "Epoch [1/2], Step [13820/16076], Loss: 4.8110\n",
      "Epoch [1/2], Step [13840/16076], Loss: 4.9249\n",
      "Epoch [1/2], Step [13860/16076], Loss: 4.8515\n",
      "Epoch [1/2], Step [13880/16076], Loss: 4.8644\n",
      "Epoch [1/2], Step [13900/16076], Loss: 4.8937\n",
      "Epoch [1/2], Step [13920/16076], Loss: 4.8714\n",
      "Epoch [1/2], Step [13940/16076], Loss: 4.8226\n",
      "Epoch [1/2], Step [13960/16076], Loss: 4.9349\n",
      "Epoch [1/2], Step [13980/16076], Loss: 4.9054\n",
      "Epoch [1/2], Step [14000/16076], Loss: 4.9056\n",
      "Epoch [1/2], Step [14020/16076], Loss: 4.8517\n",
      "Epoch [1/2], Step [14040/16076], Loss: 4.8991\n",
      "Epoch [1/2], Step [14060/16076], Loss: 4.8449\n",
      "Epoch [1/2], Step [14080/16076], Loss: 4.8800\n",
      "Epoch [1/2], Step [14100/16076], Loss: 4.8199\n",
      "Epoch [1/2], Step [14120/16076], Loss: 4.9295\n",
      "Epoch [1/2], Step [14140/16076], Loss: 4.8507\n",
      "Epoch [1/2], Step [14160/16076], Loss: 4.8847\n",
      "Epoch [1/2], Step [14180/16076], Loss: 4.8343\n",
      "Epoch [1/2], Step [14200/16076], Loss: 4.8716\n",
      "Epoch [1/2], Step [14220/16076], Loss: 4.8757\n",
      "Epoch [1/2], Step [14240/16076], Loss: 4.8989\n",
      "Epoch [1/2], Step [14260/16076], Loss: 4.8431\n",
      "Epoch [1/2], Step [14280/16076], Loss: 4.8903\n",
      "Epoch [1/2], Step [14300/16076], Loss: 4.7812\n",
      "Epoch [1/2], Step [14320/16076], Loss: 4.8784\n",
      "Epoch [1/2], Step [14340/16076], Loss: 4.9404\n",
      "Epoch [1/2], Step [14360/16076], Loss: 4.8983\n",
      "Epoch [1/2], Step [14380/16076], Loss: 4.8664\n",
      "Epoch [1/2], Step [14400/16076], Loss: 4.8191\n",
      "Epoch [1/2], Step [14420/16076], Loss: 4.9613\n",
      "Epoch [1/2], Step [14440/16076], Loss: 4.8483\n",
      "Epoch [1/2], Step [14460/16076], Loss: 4.9274\n",
      "Epoch [1/2], Step [14480/16076], Loss: 4.8593\n",
      "Epoch [1/2], Step [14500/16076], Loss: 4.8716\n",
      "Epoch [1/2], Step [14520/16076], Loss: 4.8379\n",
      "Epoch [1/2], Step [14540/16076], Loss: 4.8687\n",
      "Epoch [1/2], Step [14560/16076], Loss: 4.8672\n",
      "Epoch [1/2], Step [14580/16076], Loss: 4.8395\n",
      "Epoch [1/2], Step [14600/16076], Loss: 4.8262\n",
      "Epoch [1/2], Step [14620/16076], Loss: 4.9038\n",
      "Epoch [1/2], Step [14640/16076], Loss: 4.7426\n",
      "Epoch [1/2], Step [14660/16076], Loss: 4.9185\n",
      "Epoch [1/2], Step [14680/16076], Loss: 4.8727\n",
      "Epoch [1/2], Step [14700/16076], Loss: 4.8306\n",
      "Epoch [1/2], Step [14720/16076], Loss: 4.7674\n",
      "Epoch [1/2], Step [14740/16076], Loss: 4.8451\n",
      "Epoch [1/2], Step [14760/16076], Loss: 4.8909\n",
      "Epoch [1/2], Step [14780/16076], Loss: 4.7952\n",
      "Epoch [1/2], Step [14800/16076], Loss: 4.8928\n",
      "Epoch [1/2], Step [14820/16076], Loss: 4.9478\n",
      "Epoch [1/2], Step [14840/16076], Loss: 4.9222\n",
      "Epoch [1/2], Step [14860/16076], Loss: 4.8514\n",
      "Epoch [1/2], Step [14880/16076], Loss: 4.9935\n",
      "Epoch [1/2], Step [14900/16076], Loss: 4.8032\n",
      "Epoch [1/2], Step [14920/16076], Loss: 4.8695\n",
      "Epoch [1/2], Step [14940/16076], Loss: 4.8795\n",
      "Epoch [1/2], Step [14960/16076], Loss: 4.8216\n",
      "Epoch [1/2], Step [14980/16076], Loss: 4.8445\n",
      "Epoch [1/2], Step [15000/16076], Loss: 4.8725\n",
      "Epoch [1/2], Step [15020/16076], Loss: 4.7652\n",
      "Epoch [1/2], Step [15040/16076], Loss: 4.9067\n",
      "Epoch [1/2], Step [15060/16076], Loss: 4.8660\n",
      "Epoch [1/2], Step [15080/16076], Loss: 4.7941\n",
      "Epoch [1/2], Step [15100/16076], Loss: 4.7919\n",
      "Epoch [1/2], Step [15120/16076], Loss: 4.8704\n",
      "Epoch [1/2], Step [15140/16076], Loss: 4.9328\n",
      "Epoch [1/2], Step [15160/16076], Loss: 4.8612\n",
      "Epoch [1/2], Step [15180/16076], Loss: 4.8822\n",
      "Epoch [1/2], Step [15200/16076], Loss: 4.9332\n",
      "Epoch [1/2], Step [15220/16076], Loss: 4.9030\n",
      "Epoch [1/2], Step [15240/16076], Loss: 4.8868\n",
      "Epoch [1/2], Step [15260/16076], Loss: 4.8874\n",
      "Epoch [1/2], Step [15280/16076], Loss: 4.9251\n",
      "Epoch [1/2], Step [15300/16076], Loss: 4.7924\n",
      "Epoch [1/2], Step [15320/16076], Loss: 4.8434\n",
      "Epoch [1/2], Step [15340/16076], Loss: 4.8280\n",
      "Epoch [1/2], Step [15360/16076], Loss: 4.8489\n",
      "Epoch [1/2], Step [15380/16076], Loss: 4.9073\n",
      "Epoch [1/2], Step [15400/16076], Loss: 4.8275\n",
      "Epoch [1/2], Step [15420/16076], Loss: 4.8493\n",
      "Epoch [1/2], Step [15440/16076], Loss: 4.8427\n",
      "Epoch [1/2], Step [15460/16076], Loss: 4.8885\n",
      "Epoch [1/2], Step [15480/16076], Loss: 4.8431\n",
      "Epoch [1/2], Step [15500/16076], Loss: 4.7693\n",
      "Epoch [1/2], Step [15520/16076], Loss: 4.9301\n",
      "Epoch [1/2], Step [15540/16076], Loss: 4.8757\n",
      "Epoch [1/2], Step [15560/16076], Loss: 4.7257\n",
      "Epoch [1/2], Step [15580/16076], Loss: 4.8502\n",
      "Epoch [1/2], Step [15600/16076], Loss: 4.8255\n",
      "Epoch [1/2], Step [15620/16076], Loss: 4.8850\n",
      "Epoch [1/2], Step [15640/16076], Loss: 4.7986\n",
      "Epoch [1/2], Step [15660/16076], Loss: 4.8897\n",
      "Epoch [1/2], Step [15680/16076], Loss: 4.8251\n",
      "Epoch [1/2], Step [15700/16076], Loss: 4.8192\n",
      "Epoch [1/2], Step [15720/16076], Loss: 4.9613\n",
      "Epoch [1/2], Step [15740/16076], Loss: 4.8706\n",
      "Epoch [1/2], Step [15760/16076], Loss: 4.8893\n",
      "Epoch [1/2], Step [15780/16076], Loss: 4.8536\n",
      "Epoch [1/2], Step [15800/16076], Loss: 4.8541\n",
      "Epoch [1/2], Step [15820/16076], Loss: 4.8187\n",
      "Epoch [1/2], Step [15840/16076], Loss: 4.8814\n",
      "Epoch [1/2], Step [15860/16076], Loss: 4.8971\n",
      "Epoch [1/2], Step [15880/16076], Loss: 4.7833\n",
      "Epoch [1/2], Step [15900/16076], Loss: 4.8157\n",
      "Epoch [1/2], Step [15920/16076], Loss: 4.8139\n",
      "Epoch [1/2], Step [15940/16076], Loss: 4.8272\n",
      "Epoch [1/2], Step [15960/16076], Loss: 4.8331\n",
      "Epoch [1/2], Step [15980/16076], Loss: 4.8469\n",
      "Epoch [1/2], Step [16000/16076], Loss: 4.8488\n",
      "Epoch [1/2], Step [16020/16076], Loss: 4.8841\n",
      "Epoch [1/2], Step [16040/16076], Loss: 4.8329\n",
      "Epoch [1/2], Step [16060/16076], Loss: 4.9414\n",
      "Epoch [1/2] Average Loss: 5.1297, Perplexity: 168.97\n",
      "Epoch [2/2], Step [0/16076], Loss: 4.8438\n",
      "Epoch [2/2], Step [20/16076], Loss: 4.8329\n",
      "Epoch [2/2], Step [40/16076], Loss: 4.7928\n",
      "Epoch [2/2], Step [60/16076], Loss: 4.8035\n",
      "Epoch [2/2], Step [80/16076], Loss: 4.7599\n",
      "Epoch [2/2], Step [100/16076], Loss: 4.8230\n",
      "Epoch [2/2], Step [120/16076], Loss: 4.7919\n",
      "Epoch [2/2], Step [140/16076], Loss: 4.9275\n",
      "Epoch [2/2], Step [160/16076], Loss: 4.8709\n",
      "Epoch [2/2], Step [180/16076], Loss: 4.8395\n",
      "Epoch [2/2], Step [200/16076], Loss: 4.8417\n",
      "Epoch [2/2], Step [220/16076], Loss: 4.9028\n",
      "Epoch [2/2], Step [240/16076], Loss: 4.9062\n",
      "Epoch [2/2], Step [260/16076], Loss: 4.7942\n",
      "Epoch [2/2], Step [280/16076], Loss: 4.7865\n",
      "Epoch [2/2], Step [300/16076], Loss: 4.8693\n",
      "Epoch [2/2], Step [320/16076], Loss: 4.8862\n",
      "Epoch [2/2], Step [340/16076], Loss: 4.7926\n",
      "Epoch [2/2], Step [360/16076], Loss: 4.8618\n",
      "Epoch [2/2], Step [380/16076], Loss: 4.8691\n",
      "Epoch [2/2], Step [400/16076], Loss: 4.8635\n",
      "Epoch [2/2], Step [420/16076], Loss: 4.8665\n",
      "Epoch [2/2], Step [440/16076], Loss: 4.7961\n",
      "Epoch [2/2], Step [460/16076], Loss: 4.8892\n",
      "Epoch [2/2], Step [480/16076], Loss: 4.8353\n",
      "Epoch [2/2], Step [500/16076], Loss: 4.8100\n",
      "Epoch [2/2], Step [520/16076], Loss: 4.8232\n",
      "Epoch [2/2], Step [540/16076], Loss: 4.8197\n",
      "Epoch [2/2], Step [560/16076], Loss: 4.7996\n",
      "Epoch [2/2], Step [580/16076], Loss: 4.9017\n",
      "Epoch [2/2], Step [600/16076], Loss: 4.7455\n",
      "Epoch [2/2], Step [620/16076], Loss: 4.8912\n",
      "Epoch [2/2], Step [640/16076], Loss: 4.8120\n",
      "Epoch [2/2], Step [660/16076], Loss: 4.8195\n",
      "Epoch [2/2], Step [680/16076], Loss: 4.7810\n",
      "Epoch [2/2], Step [700/16076], Loss: 4.7871\n",
      "Epoch [2/2], Step [720/16076], Loss: 4.7895\n",
      "Epoch [2/2], Step [740/16076], Loss: 4.9289\n",
      "Epoch [2/2], Step [760/16076], Loss: 4.8821\n",
      "Epoch [2/2], Step [780/16076], Loss: 4.8624\n",
      "Epoch [2/2], Step [800/16076], Loss: 4.8245\n",
      "Epoch [2/2], Step [820/16076], Loss: 4.7952\n",
      "Epoch [2/2], Step [840/16076], Loss: 4.8838\n",
      "Epoch [2/2], Step [860/16076], Loss: 4.8280\n",
      "Epoch [2/2], Step [880/16076], Loss: 4.8493\n",
      "Epoch [2/2], Step [900/16076], Loss: 4.8426\n",
      "Epoch [2/2], Step [920/16076], Loss: 4.8020\n",
      "Epoch [2/2], Step [940/16076], Loss: 4.8455\n",
      "Epoch [2/2], Step [960/16076], Loss: 4.8266\n",
      "Epoch [2/2], Step [980/16076], Loss: 4.8572\n",
      "Epoch [2/2], Step [1000/16076], Loss: 4.7521\n",
      "Epoch [2/2], Step [1020/16076], Loss: 4.8387\n",
      "Epoch [2/2], Step [1040/16076], Loss: 4.8425\n",
      "Epoch [2/2], Step [1060/16076], Loss: 4.9043\n",
      "Epoch [2/2], Step [1080/16076], Loss: 4.8787\n",
      "Epoch [2/2], Step [1100/16076], Loss: 4.8179\n",
      "Epoch [2/2], Step [1120/16076], Loss: 4.8547\n",
      "Epoch [2/2], Step [1140/16076], Loss: 4.7956\n",
      "Epoch [2/2], Step [1160/16076], Loss: 4.8369\n",
      "Epoch [2/2], Step [1180/16076], Loss: 4.8900\n",
      "Epoch [2/2], Step [1200/16076], Loss: 4.8499\n",
      "Epoch [2/2], Step [1220/16076], Loss: 4.8100\n",
      "Epoch [2/2], Step [1240/16076], Loss: 4.8188\n",
      "Epoch [2/2], Step [1260/16076], Loss: 4.8280\n",
      "Epoch [2/2], Step [1280/16076], Loss: 4.8236\n",
      "Epoch [2/2], Step [1300/16076], Loss: 4.8121\n",
      "Epoch [2/2], Step [1320/16076], Loss: 4.7941\n",
      "Epoch [2/2], Step [1340/16076], Loss: 4.8091\n",
      "Epoch [2/2], Step [1360/16076], Loss: 4.8005\n",
      "Epoch [2/2], Step [1380/16076], Loss: 4.8220\n",
      "Epoch [2/2], Step [1400/16076], Loss: 4.8464\n",
      "Epoch [2/2], Step [1420/16076], Loss: 4.7928\n",
      "Epoch [2/2], Step [1440/16076], Loss: 4.7952\n",
      "Epoch [2/2], Step [1460/16076], Loss: 4.6976\n",
      "Epoch [2/2], Step [1480/16076], Loss: 4.7743\n",
      "Epoch [2/2], Step [1500/16076], Loss: 4.7876\n",
      "Epoch [2/2], Step [1520/16076], Loss: 4.8092\n",
      "Epoch [2/2], Step [1540/16076], Loss: 4.7561\n",
      "Epoch [2/2], Step [1560/16076], Loss: 4.7868\n",
      "Epoch [2/2], Step [1580/16076], Loss: 4.8113\n",
      "Epoch [2/2], Step [1600/16076], Loss: 4.8778\n",
      "Epoch [2/2], Step [1620/16076], Loss: 4.9143\n",
      "Epoch [2/2], Step [1640/16076], Loss: 4.8398\n",
      "Epoch [2/2], Step [1660/16076], Loss: 4.9185\n",
      "Epoch [2/2], Step [1680/16076], Loss: 4.7976\n",
      "Epoch [2/2], Step [1700/16076], Loss: 4.8635\n",
      "Epoch [2/2], Step [1720/16076], Loss: 4.8434\n",
      "Epoch [2/2], Step [1740/16076], Loss: 4.8456\n",
      "Epoch [2/2], Step [1760/16076], Loss: 4.8177\n",
      "Epoch [2/2], Step [1780/16076], Loss: 4.7932\n",
      "Epoch [2/2], Step [1800/16076], Loss: 4.7984\n",
      "Epoch [2/2], Step [1820/16076], Loss: 4.8293\n",
      "Epoch [2/2], Step [1840/16076], Loss: 4.7995\n",
      "Epoch [2/2], Step [1860/16076], Loss: 4.8624\n",
      "Epoch [2/2], Step [1880/16076], Loss: 4.8017\n",
      "Epoch [2/2], Step [1900/16076], Loss: 4.8458\n",
      "Epoch [2/2], Step [1920/16076], Loss: 4.7743\n",
      "Epoch [2/2], Step [1940/16076], Loss: 4.8467\n",
      "Epoch [2/2], Step [1960/16076], Loss: 4.8420\n",
      "Epoch [2/2], Step [1980/16076], Loss: 4.7669\n",
      "Epoch [2/2], Step [2000/16076], Loss: 4.8646\n",
      "Epoch [2/2], Step [2020/16076], Loss: 4.9010\n",
      "Epoch [2/2], Step [2040/16076], Loss: 4.7556\n",
      "Epoch [2/2], Step [2060/16076], Loss: 4.8085\n",
      "Epoch [2/2], Step [2080/16076], Loss: 4.8054\n",
      "Epoch [2/2], Step [2100/16076], Loss: 4.7737\n",
      "Epoch [2/2], Step [2120/16076], Loss: 4.7854\n",
      "Epoch [2/2], Step [2140/16076], Loss: 4.8512\n",
      "Epoch [2/2], Step [2160/16076], Loss: 4.8351\n",
      "Epoch [2/2], Step [2180/16076], Loss: 4.8046\n",
      "Epoch [2/2], Step [2200/16076], Loss: 4.7847\n",
      "Epoch [2/2], Step [2220/16076], Loss: 4.8503\n",
      "Epoch [2/2], Step [2240/16076], Loss: 4.9030\n",
      "Epoch [2/2], Step [2260/16076], Loss: 4.8136\n",
      "Epoch [2/2], Step [2280/16076], Loss: 4.8682\n",
      "Epoch [2/2], Step [2300/16076], Loss: 4.7927\n",
      "Epoch [2/2], Step [2320/16076], Loss: 4.8760\n",
      "Epoch [2/2], Step [2340/16076], Loss: 4.7243\n",
      "Epoch [2/2], Step [2360/16076], Loss: 4.8985\n",
      "Epoch [2/2], Step [2380/16076], Loss: 4.8108\n",
      "Epoch [2/2], Step [2400/16076], Loss: 4.8174\n",
      "Epoch [2/2], Step [2420/16076], Loss: 4.7980\n",
      "Epoch [2/2], Step [2440/16076], Loss: 4.7870\n",
      "Epoch [2/2], Step [2460/16076], Loss: 4.7941\n",
      "Epoch [2/2], Step [2480/16076], Loss: 4.8972\n",
      "Epoch [2/2], Step [2500/16076], Loss: 4.7732\n",
      "Epoch [2/2], Step [2520/16076], Loss: 4.7993\n",
      "Epoch [2/2], Step [2540/16076], Loss: 4.7533\n",
      "Epoch [2/2], Step [2560/16076], Loss: 4.7794\n",
      "Epoch [2/2], Step [2580/16076], Loss: 4.8139\n",
      "Epoch [2/2], Step [2600/16076], Loss: 4.8820\n",
      "Epoch [2/2], Step [2620/16076], Loss: 4.7831\n",
      "Epoch [2/2], Step [2640/16076], Loss: 4.7940\n",
      "Epoch [2/2], Step [2660/16076], Loss: 4.8086\n",
      "Epoch [2/2], Step [2680/16076], Loss: 4.7681\n",
      "Epoch [2/2], Step [2700/16076], Loss: 4.9028\n",
      "Epoch [2/2], Step [2720/16076], Loss: 4.7548\n",
      "Epoch [2/2], Step [2740/16076], Loss: 4.7739\n",
      "Epoch [2/2], Step [2760/16076], Loss: 4.7774\n",
      "Epoch [2/2], Step [2780/16076], Loss: 4.7877\n",
      "Epoch [2/2], Step [2800/16076], Loss: 4.7666\n",
      "Epoch [2/2], Step [2820/16076], Loss: 4.8233\n",
      "Epoch [2/2], Step [2840/16076], Loss: 4.7548\n",
      "Epoch [2/2], Step [2860/16076], Loss: 4.7632\n",
      "Epoch [2/2], Step [2880/16076], Loss: 4.7565\n",
      "Epoch [2/2], Step [2900/16076], Loss: 4.8157\n",
      "Epoch [2/2], Step [2920/16076], Loss: 4.7731\n",
      "Epoch [2/2], Step [2940/16076], Loss: 4.8281\n",
      "Epoch [2/2], Step [2960/16076], Loss: 4.7776\n",
      "Epoch [2/2], Step [2980/16076], Loss: 4.8249\n",
      "Epoch [2/2], Step [3000/16076], Loss: 4.8107\n",
      "Epoch [2/2], Step [3020/16076], Loss: 4.7799\n",
      "Epoch [2/2], Step [3040/16076], Loss: 4.7629\n",
      "Epoch [2/2], Step [3060/16076], Loss: 4.7711\n",
      "Epoch [2/2], Step [3080/16076], Loss: 4.8811\n",
      "Epoch [2/2], Step [3100/16076], Loss: 4.8204\n",
      "Epoch [2/2], Step [3120/16076], Loss: 4.8028\n",
      "Epoch [2/2], Step [3140/16076], Loss: 4.7642\n",
      "Epoch [2/2], Step [3160/16076], Loss: 4.7430\n",
      "Epoch [2/2], Step [3180/16076], Loss: 4.7710\n",
      "Epoch [2/2], Step [3200/16076], Loss: 4.8411\n",
      "Epoch [2/2], Step [3220/16076], Loss: 4.8217\n",
      "Epoch [2/2], Step [3240/16076], Loss: 4.8400\n",
      "Epoch [2/2], Step [3260/16076], Loss: 4.8621\n",
      "Epoch [2/2], Step [3280/16076], Loss: 4.8260\n",
      "Epoch [2/2], Step [3300/16076], Loss: 4.8207\n",
      "Epoch [2/2], Step [3320/16076], Loss: 4.8067\n",
      "Epoch [2/2], Step [3340/16076], Loss: 4.8757\n",
      "Epoch [2/2], Step [3360/16076], Loss: 4.7474\n",
      "Epoch [2/2], Step [3380/16076], Loss: 4.8375\n",
      "Epoch [2/2], Step [3400/16076], Loss: 4.8504\n",
      "Epoch [2/2], Step [3420/16076], Loss: 4.8187\n",
      "Epoch [2/2], Step [3440/16076], Loss: 4.7797\n",
      "Epoch [2/2], Step [3460/16076], Loss: 4.7988\n",
      "Epoch [2/2], Step [3480/16076], Loss: 4.7915\n",
      "Epoch [2/2], Step [3500/16076], Loss: 4.8065\n",
      "Epoch [2/2], Step [3520/16076], Loss: 4.8204\n",
      "Epoch [2/2], Step [3540/16076], Loss: 4.8044\n",
      "Epoch [2/2], Step [3560/16076], Loss: 4.7842\n",
      "Epoch [2/2], Step [3580/16076], Loss: 4.8658\n",
      "Epoch [2/2], Step [3600/16076], Loss: 4.7815\n",
      "Epoch [2/2], Step [3620/16076], Loss: 4.7703\n",
      "Epoch [2/2], Step [3640/16076], Loss: 4.7425\n",
      "Epoch [2/2], Step [3660/16076], Loss: 4.7755\n",
      "Epoch [2/2], Step [3680/16076], Loss: 4.7642\n",
      "Epoch [2/2], Step [3700/16076], Loss: 4.7212\n",
      "Epoch [2/2], Step [3720/16076], Loss: 4.7517\n",
      "Epoch [2/2], Step [3740/16076], Loss: 4.9459\n",
      "Epoch [2/2], Step [3760/16076], Loss: 4.8069\n",
      "Epoch [2/2], Step [3780/16076], Loss: 4.7931\n",
      "Epoch [2/2], Step [3800/16076], Loss: 4.7980\n",
      "Epoch [2/2], Step [3820/16076], Loss: 4.7456\n",
      "Epoch [2/2], Step [3840/16076], Loss: 4.7957\n",
      "Epoch [2/2], Step [3860/16076], Loss: 4.7679\n",
      "Epoch [2/2], Step [3880/16076], Loss: 4.6959\n",
      "Epoch [2/2], Step [3900/16076], Loss: 4.8652\n",
      "Epoch [2/2], Step [3920/16076], Loss: 4.7916\n",
      "Epoch [2/2], Step [3940/16076], Loss: 4.7995\n",
      "Epoch [2/2], Step [3960/16076], Loss: 4.7792\n",
      "Epoch [2/2], Step [3980/16076], Loss: 4.7752\n",
      "Epoch [2/2], Step [4000/16076], Loss: 4.7977\n",
      "Epoch [2/2], Step [4020/16076], Loss: 4.7260\n",
      "Epoch [2/2], Step [4040/16076], Loss: 4.7845\n",
      "Epoch [2/2], Step [4060/16076], Loss: 4.8402\n",
      "Epoch [2/2], Step [4080/16076], Loss: 4.7836\n",
      "Epoch [2/2], Step [4100/16076], Loss: 4.8108\n",
      "Epoch [2/2], Step [4120/16076], Loss: 4.8158\n",
      "Epoch [2/2], Step [4140/16076], Loss: 4.8403\n",
      "Epoch [2/2], Step [4160/16076], Loss: 4.6695\n",
      "Epoch [2/2], Step [4180/16076], Loss: 4.8516\n",
      "Epoch [2/2], Step [4200/16076], Loss: 4.7969\n",
      "Epoch [2/2], Step [4220/16076], Loss: 4.7933\n",
      "Epoch [2/2], Step [4240/16076], Loss: 4.8192\n",
      "Epoch [2/2], Step [4260/16076], Loss: 4.8445\n",
      "Epoch [2/2], Step [4280/16076], Loss: 4.8496\n",
      "Epoch [2/2], Step [4300/16076], Loss: 4.8661\n",
      "Epoch [2/2], Step [4320/16076], Loss: 4.8468\n",
      "Epoch [2/2], Step [4340/16076], Loss: 4.7056\n",
      "Epoch [2/2], Step [4360/16076], Loss: 4.8287\n",
      "Epoch [2/2], Step [4380/16076], Loss: 4.8356\n",
      "Epoch [2/2], Step [4400/16076], Loss: 4.7965\n",
      "Epoch [2/2], Step [4420/16076], Loss: 4.8202\n",
      "Epoch [2/2], Step [4440/16076], Loss: 4.7569\n",
      "Epoch [2/2], Step [4460/16076], Loss: 4.8361\n",
      "Epoch [2/2], Step [4480/16076], Loss: 4.7510\n",
      "Epoch [2/2], Step [4500/16076], Loss: 4.8335\n",
      "Epoch [2/2], Step [4520/16076], Loss: 4.8146\n",
      "Epoch [2/2], Step [4540/16076], Loss: 4.7492\n",
      "Epoch [2/2], Step [4560/16076], Loss: 4.8119\n",
      "Epoch [2/2], Step [4580/16076], Loss: 4.7805\n",
      "Epoch [2/2], Step [4600/16076], Loss: 4.8025\n",
      "Epoch [2/2], Step [4620/16076], Loss: 4.8842\n",
      "Epoch [2/2], Step [4640/16076], Loss: 4.7698\n",
      "Epoch [2/2], Step [4660/16076], Loss: 4.7731\n",
      "Epoch [2/2], Step [4680/16076], Loss: 4.7841\n",
      "Epoch [2/2], Step [4700/16076], Loss: 4.7899\n",
      "Epoch [2/2], Step [4720/16076], Loss: 4.7689\n",
      "Epoch [2/2], Step [4740/16076], Loss: 4.7669\n",
      "Epoch [2/2], Step [4760/16076], Loss: 4.8362\n",
      "Epoch [2/2], Step [4780/16076], Loss: 4.7363\n",
      "Epoch [2/2], Step [4800/16076], Loss: 4.7475\n",
      "Epoch [2/2], Step [4820/16076], Loss: 4.8244\n",
      "Epoch [2/2], Step [4840/16076], Loss: 4.7907\n",
      "Epoch [2/2], Step [4860/16076], Loss: 4.7851\n",
      "Epoch [2/2], Step [4880/16076], Loss: 4.8013\n",
      "Epoch [2/2], Step [4900/16076], Loss: 4.7575\n",
      "Epoch [2/2], Step [4920/16076], Loss: 4.8103\n",
      "Epoch [2/2], Step [4940/16076], Loss: 4.9325\n",
      "Epoch [2/2], Step [4960/16076], Loss: 4.8473\n",
      "Epoch [2/2], Step [4980/16076], Loss: 4.7964\n",
      "Epoch [2/2], Step [5000/16076], Loss: 4.7853\n",
      "Epoch [2/2], Step [5020/16076], Loss: 4.7443\n",
      "Epoch [2/2], Step [5040/16076], Loss: 4.7734\n",
      "Epoch [2/2], Step [5060/16076], Loss: 4.7545\n",
      "Epoch [2/2], Step [5080/16076], Loss: 4.7664\n",
      "Epoch [2/2], Step [5100/16076], Loss: 4.7637\n",
      "Epoch [2/2], Step [5120/16076], Loss: 4.8310\n",
      "Epoch [2/2], Step [5140/16076], Loss: 4.7127\n",
      "Epoch [2/2], Step [5160/16076], Loss: 4.7397\n",
      "Epoch [2/2], Step [5180/16076], Loss: 4.6924\n",
      "Epoch [2/2], Step [5200/16076], Loss: 4.7459\n",
      "Epoch [2/2], Step [5220/16076], Loss: 4.8689\n",
      "Epoch [2/2], Step [5240/16076], Loss: 4.7528\n",
      "Epoch [2/2], Step [5260/16076], Loss: 4.7978\n",
      "Epoch [2/2], Step [5280/16076], Loss: 4.8567\n",
      "Epoch [2/2], Step [5300/16076], Loss: 4.7174\n",
      "Epoch [2/2], Step [5320/16076], Loss: 4.7839\n",
      "Epoch [2/2], Step [5340/16076], Loss: 4.9141\n",
      "Epoch [2/2], Step [5360/16076], Loss: 4.8168\n",
      "Epoch [2/2], Step [5380/16076], Loss: 4.7485\n",
      "Epoch [2/2], Step [5400/16076], Loss: 4.8032\n",
      "Epoch [2/2], Step [5420/16076], Loss: 4.7360\n",
      "Epoch [2/2], Step [5440/16076], Loss: 4.7416\n",
      "Epoch [2/2], Step [5460/16076], Loss: 4.8634\n",
      "Epoch [2/2], Step [5480/16076], Loss: 4.8099\n",
      "Epoch [2/2], Step [5500/16076], Loss: 4.7029\n",
      "Epoch [2/2], Step [5520/16076], Loss: 4.8312\n",
      "Epoch [2/2], Step [5540/16076], Loss: 4.7926\n",
      "Epoch [2/2], Step [5560/16076], Loss: 4.7709\n",
      "Epoch [2/2], Step [5580/16076], Loss: 4.7865\n",
      "Epoch [2/2], Step [5600/16076], Loss: 4.7059\n",
      "Epoch [2/2], Step [5620/16076], Loss: 4.7577\n",
      "Epoch [2/2], Step [5640/16076], Loss: 4.7402\n",
      "Epoch [2/2], Step [5660/16076], Loss: 4.7977\n",
      "Epoch [2/2], Step [5680/16076], Loss: 4.8563\n",
      "Epoch [2/2], Step [5700/16076], Loss: 4.8004\n",
      "Epoch [2/2], Step [5720/16076], Loss: 4.8080\n",
      "Epoch [2/2], Step [5740/16076], Loss: 4.8301\n",
      "Epoch [2/2], Step [5760/16076], Loss: 4.7645\n",
      "Epoch [2/2], Step [5780/16076], Loss: 4.7507\n",
      "Epoch [2/2], Step [5800/16076], Loss: 4.8370\n",
      "Epoch [2/2], Step [5820/16076], Loss: 4.8707\n",
      "Epoch [2/2], Step [5840/16076], Loss: 4.8118\n",
      "Epoch [2/2], Step [5860/16076], Loss: 4.6906\n",
      "Epoch [2/2], Step [5880/16076], Loss: 4.7788\n",
      "Epoch [2/2], Step [5900/16076], Loss: 4.8180\n",
      "Epoch [2/2], Step [5920/16076], Loss: 4.7928\n",
      "Epoch [2/2], Step [5940/16076], Loss: 4.7847\n",
      "Epoch [2/2], Step [5960/16076], Loss: 4.7332\n",
      "Epoch [2/2], Step [5980/16076], Loss: 4.7877\n",
      "Epoch [2/2], Step [6000/16076], Loss: 4.8373\n",
      "Epoch [2/2], Step [6020/16076], Loss: 4.7663\n",
      "Epoch [2/2], Step [6040/16076], Loss: 4.7425\n",
      "Epoch [2/2], Step [6060/16076], Loss: 4.8683\n",
      "Epoch [2/2], Step [6080/16076], Loss: 4.7880\n",
      "Epoch [2/2], Step [6100/16076], Loss: 4.7733\n",
      "Epoch [2/2], Step [6120/16076], Loss: 4.6828\n",
      "Epoch [2/2], Step [6140/16076], Loss: 4.7921\n",
      "Epoch [2/2], Step [6160/16076], Loss: 4.7639\n",
      "Epoch [2/2], Step [6180/16076], Loss: 4.7707\n",
      "Epoch [2/2], Step [6200/16076], Loss: 4.8066\n",
      "Epoch [2/2], Step [6220/16076], Loss: 4.8456\n",
      "Epoch [2/2], Step [6240/16076], Loss: 4.7577\n",
      "Epoch [2/2], Step [6260/16076], Loss: 4.8213\n",
      "Epoch [2/2], Step [6280/16076], Loss: 4.8559\n",
      "Epoch [2/2], Step [6300/16076], Loss: 4.7882\n",
      "Epoch [2/2], Step [6320/16076], Loss: 4.7896\n",
      "Epoch [2/2], Step [6340/16076], Loss: 4.8280\n",
      "Epoch [2/2], Step [6360/16076], Loss: 4.7783\n",
      "Epoch [2/2], Step [6380/16076], Loss: 4.8297\n",
      "Epoch [2/2], Step [6400/16076], Loss: 4.8025\n",
      "Epoch [2/2], Step [6420/16076], Loss: 4.8383\n",
      "Epoch [2/2], Step [6440/16076], Loss: 4.8645\n",
      "Epoch [2/2], Step [6460/16076], Loss: 4.7604\n",
      "Epoch [2/2], Step [6480/16076], Loss: 4.7697\n",
      "Epoch [2/2], Step [6500/16076], Loss: 4.8142\n",
      "Epoch [2/2], Step [6520/16076], Loss: 4.7721\n",
      "Epoch [2/2], Step [6540/16076], Loss: 4.7610\n",
      "Epoch [2/2], Step [6560/16076], Loss: 4.8278\n",
      "Epoch [2/2], Step [6580/16076], Loss: 4.7628\n",
      "Epoch [2/2], Step [6600/16076], Loss: 4.7581\n",
      "Epoch [2/2], Step [6620/16076], Loss: 4.7644\n",
      "Epoch [2/2], Step [6640/16076], Loss: 4.7894\n",
      "Epoch [2/2], Step [6660/16076], Loss: 4.7445\n",
      "Epoch [2/2], Step [6680/16076], Loss: 4.7890\n",
      "Epoch [2/2], Step [6700/16076], Loss: 4.8054\n",
      "Epoch [2/2], Step [6720/16076], Loss: 4.8104\n",
      "Epoch [2/2], Step [6740/16076], Loss: 4.7689\n",
      "Epoch [2/2], Step [6760/16076], Loss: 4.7346\n",
      "Epoch [2/2], Step [6780/16076], Loss: 4.8154\n",
      "Epoch [2/2], Step [6800/16076], Loss: 4.8051\n",
      "Epoch [2/2], Step [6820/16076], Loss: 4.8295\n",
      "Epoch [2/2], Step [6840/16076], Loss: 4.8312\n",
      "Epoch [2/2], Step [6860/16076], Loss: 4.8310\n",
      "Epoch [2/2], Step [6880/16076], Loss: 4.7530\n",
      "Epoch [2/2], Step [6900/16076], Loss: 4.6974\n",
      "Epoch [2/2], Step [6920/16076], Loss: 4.7964\n",
      "Epoch [2/2], Step [6940/16076], Loss: 4.8178\n",
      "Epoch [2/2], Step [6960/16076], Loss: 4.8667\n",
      "Epoch [2/2], Step [6980/16076], Loss: 4.7855\n",
      "Epoch [2/2], Step [7000/16076], Loss: 4.7229\n",
      "Epoch [2/2], Step [7020/16076], Loss: 4.8026\n",
      "Epoch [2/2], Step [7040/16076], Loss: 4.8042\n",
      "Epoch [2/2], Step [7060/16076], Loss: 4.7371\n",
      "Epoch [2/2], Step [7080/16076], Loss: 4.7943\n",
      "Epoch [2/2], Step [7100/16076], Loss: 4.8122\n",
      "Epoch [2/2], Step [7120/16076], Loss: 4.7651\n",
      "Epoch [2/2], Step [7140/16076], Loss: 4.9039\n",
      "Epoch [2/2], Step [7160/16076], Loss: 4.7510\n",
      "Epoch [2/2], Step [7180/16076], Loss: 4.7132\n",
      "Epoch [2/2], Step [7200/16076], Loss: 4.8516\n",
      "Epoch [2/2], Step [7220/16076], Loss: 4.7634\n",
      "Epoch [2/2], Step [7240/16076], Loss: 4.7609\n",
      "Epoch [2/2], Step [7260/16076], Loss: 4.8259\n",
      "Epoch [2/2], Step [7280/16076], Loss: 4.8037\n",
      "Epoch [2/2], Step [7300/16076], Loss: 4.7884\n",
      "Epoch [2/2], Step [7320/16076], Loss: 4.7015\n",
      "Epoch [2/2], Step [7340/16076], Loss: 4.7821\n",
      "Epoch [2/2], Step [7360/16076], Loss: 4.8180\n",
      "Epoch [2/2], Step [7380/16076], Loss: 4.7653\n",
      "Epoch [2/2], Step [7400/16076], Loss: 4.8571\n",
      "Epoch [2/2], Step [7420/16076], Loss: 4.7774\n",
      "Epoch [2/2], Step [7440/16076], Loss: 4.7341\n",
      "Epoch [2/2], Step [7460/16076], Loss: 4.7050\n",
      "Epoch [2/2], Step [7480/16076], Loss: 4.7810\n",
      "Epoch [2/2], Step [7500/16076], Loss: 4.8912\n",
      "Epoch [2/2], Step [7520/16076], Loss: 4.7463\n",
      "Epoch [2/2], Step [7540/16076], Loss: 4.8335\n",
      "Epoch [2/2], Step [7560/16076], Loss: 4.7667\n",
      "Epoch [2/2], Step [7580/16076], Loss: 4.7954\n",
      "Epoch [2/2], Step [7600/16076], Loss: 4.7515\n",
      "Epoch [2/2], Step [7620/16076], Loss: 4.7078\n",
      "Epoch [2/2], Step [7640/16076], Loss: 4.8017\n",
      "Epoch [2/2], Step [7660/16076], Loss: 4.7943\n",
      "Epoch [2/2], Step [7680/16076], Loss: 4.8327\n",
      "Epoch [2/2], Step [7700/16076], Loss: 4.7873\n",
      "Epoch [2/2], Step [7720/16076], Loss: 4.7136\n",
      "Epoch [2/2], Step [7740/16076], Loss: 4.7876\n",
      "Epoch [2/2], Step [7760/16076], Loss: 4.7811\n",
      "Epoch [2/2], Step [7780/16076], Loss: 4.8114\n",
      "Epoch [2/2], Step [7800/16076], Loss: 4.7196\n",
      "Epoch [2/2], Step [7820/16076], Loss: 4.7593\n",
      "Epoch [2/2], Step [7840/16076], Loss: 4.7753\n",
      "Epoch [2/2], Step [7860/16076], Loss: 4.8427\n",
      "Epoch [2/2], Step [7880/16076], Loss: 4.7248\n",
      "Epoch [2/2], Step [7900/16076], Loss: 4.7017\n",
      "Epoch [2/2], Step [7920/16076], Loss: 4.7684\n",
      "Epoch [2/2], Step [7940/16076], Loss: 4.7606\n",
      "Epoch [2/2], Step [7960/16076], Loss: 4.7498\n",
      "Epoch [2/2], Step [7980/16076], Loss: 4.7898\n",
      "Epoch [2/2], Step [8000/16076], Loss: 4.8180\n",
      "Epoch [2/2], Step [8020/16076], Loss: 4.7517\n",
      "Epoch [2/2], Step [8040/16076], Loss: 4.7239\n",
      "Epoch [2/2], Step [8060/16076], Loss: 4.7567\n",
      "Epoch [2/2], Step [8080/16076], Loss: 4.7936\n",
      "Epoch [2/2], Step [8100/16076], Loss: 4.7233\n",
      "Epoch [2/2], Step [8120/16076], Loss: 4.7714\n",
      "Epoch [2/2], Step [8140/16076], Loss: 4.8261\n",
      "Epoch [2/2], Step [8160/16076], Loss: 4.8263\n",
      "Epoch [2/2], Step [8180/16076], Loss: 4.7502\n",
      "Epoch [2/2], Step [8200/16076], Loss: 4.7662\n",
      "Epoch [2/2], Step [8220/16076], Loss: 4.8132\n",
      "Epoch [2/2], Step [8240/16076], Loss: 4.7843\n",
      "Epoch [2/2], Step [8260/16076], Loss: 4.7430\n",
      "Epoch [2/2], Step [8280/16076], Loss: 4.7159\n",
      "Epoch [2/2], Step [8300/16076], Loss: 4.7666\n",
      "Epoch [2/2], Step [8320/16076], Loss: 4.7955\n",
      "Epoch [2/2], Step [8340/16076], Loss: 4.8562\n",
      "Epoch [2/2], Step [8360/16076], Loss: 4.7530\n",
      "Epoch [2/2], Step [8380/16076], Loss: 4.7651\n",
      "Epoch [2/2], Step [8400/16076], Loss: 4.8146\n",
      "Epoch [2/2], Step [8420/16076], Loss: 4.6927\n",
      "Epoch [2/2], Step [8440/16076], Loss: 4.7855\n",
      "Epoch [2/2], Step [8460/16076], Loss: 4.8189\n",
      "Epoch [2/2], Step [8480/16076], Loss: 4.7298\n",
      "Epoch [2/2], Step [8500/16076], Loss: 4.7968\n",
      "Epoch [2/2], Step [8520/16076], Loss: 4.7481\n",
      "Epoch [2/2], Step [8540/16076], Loss: 4.7375\n",
      "Epoch [2/2], Step [8560/16076], Loss: 4.7701\n",
      "Epoch [2/2], Step [8580/16076], Loss: 4.7736\n",
      "Epoch [2/2], Step [8600/16076], Loss: 4.8223\n",
      "Epoch [2/2], Step [8620/16076], Loss: 4.7382\n",
      "Epoch [2/2], Step [8640/16076], Loss: 4.7183\n",
      "Epoch [2/2], Step [8660/16076], Loss: 4.8232\n",
      "Epoch [2/2], Step [8680/16076], Loss: 4.8183\n",
      "Epoch [2/2], Step [8700/16076], Loss: 4.6626\n",
      "Epoch [2/2], Step [8720/16076], Loss: 4.8180\n",
      "Epoch [2/2], Step [8740/16076], Loss: 4.7903\n",
      "Epoch [2/2], Step [8760/16076], Loss: 4.8503\n",
      "Epoch [2/2], Step [8780/16076], Loss: 4.8148\n",
      "Epoch [2/2], Step [8800/16076], Loss: 4.7780\n",
      "Epoch [2/2], Step [8820/16076], Loss: 4.7233\n",
      "Epoch [2/2], Step [8840/16076], Loss: 4.7205\n",
      "Epoch [2/2], Step [8860/16076], Loss: 4.8251\n",
      "Epoch [2/2], Step [8880/16076], Loss: 4.6593\n",
      "Epoch [2/2], Step [8900/16076], Loss: 4.8170\n",
      "Epoch [2/2], Step [8920/16076], Loss: 4.8597\n",
      "Epoch [2/2], Step [8940/16076], Loss: 4.7101\n",
      "Epoch [2/2], Step [8960/16076], Loss: 4.7745\n",
      "Epoch [2/2], Step [8980/16076], Loss: 4.7908\n",
      "Epoch [2/2], Step [9000/16076], Loss: 4.7258\n",
      "Epoch [2/2], Step [9020/16076], Loss: 4.8072\n",
      "Epoch [2/2], Step [9040/16076], Loss: 4.7376\n",
      "Epoch [2/2], Step [9060/16076], Loss: 4.7810\n",
      "Epoch [2/2], Step [9080/16076], Loss: 4.7926\n",
      "Epoch [2/2], Step [9100/16076], Loss: 4.8020\n",
      "Epoch [2/2], Step [9120/16076], Loss: 4.7374\n",
      "Epoch [2/2], Step [9140/16076], Loss: 4.7912\n",
      "Epoch [2/2], Step [9160/16076], Loss: 4.7668\n",
      "Epoch [2/2], Step [9180/16076], Loss: 4.8039\n",
      "Epoch [2/2], Step [9200/16076], Loss: 4.7765\n",
      "Epoch [2/2], Step [9220/16076], Loss: 4.7352\n",
      "Epoch [2/2], Step [9240/16076], Loss: 4.7663\n",
      "Epoch [2/2], Step [9260/16076], Loss: 4.7319\n",
      "Epoch [2/2], Step [9280/16076], Loss: 4.7492\n",
      "Epoch [2/2], Step [9300/16076], Loss: 4.7437\n",
      "Epoch [2/2], Step [9320/16076], Loss: 4.7876\n",
      "Epoch [2/2], Step [9340/16076], Loss: 4.6798\n",
      "Epoch [2/2], Step [9360/16076], Loss: 4.7367\n",
      "Epoch [2/2], Step [9380/16076], Loss: 4.7214\n",
      "Epoch [2/2], Step [9400/16076], Loss: 4.8200\n",
      "Epoch [2/2], Step [9420/16076], Loss: 4.7969\n",
      "Epoch [2/2], Step [9440/16076], Loss: 4.7243\n",
      "Epoch [2/2], Step [9460/16076], Loss: 4.7399\n",
      "Epoch [2/2], Step [9480/16076], Loss: 4.8186\n",
      "Epoch [2/2], Step [9500/16076], Loss: 4.7520\n",
      "Epoch [2/2], Step [9520/16076], Loss: 4.7772\n",
      "Epoch [2/2], Step [9540/16076], Loss: 4.7034\n",
      "Epoch [2/2], Step [9560/16076], Loss: 4.8014\n",
      "Epoch [2/2], Step [9580/16076], Loss: 4.8150\n",
      "Epoch [2/2], Step [9600/16076], Loss: 4.7363\n",
      "Epoch [2/2], Step [9620/16076], Loss: 4.7252\n",
      "Epoch [2/2], Step [9640/16076], Loss: 4.7599\n",
      "Epoch [2/2], Step [9660/16076], Loss: 4.6857\n",
      "Epoch [2/2], Step [9680/16076], Loss: 4.7238\n",
      "Epoch [2/2], Step [9700/16076], Loss: 4.7454\n",
      "Epoch [2/2], Step [9720/16076], Loss: 4.6390\n",
      "Epoch [2/2], Step [9740/16076], Loss: 4.7415\n",
      "Epoch [2/2], Step [9760/16076], Loss: 4.7741\n",
      "Epoch [2/2], Step [9780/16076], Loss: 4.6898\n",
      "Epoch [2/2], Step [9800/16076], Loss: 4.7241\n",
      "Epoch [2/2], Step [9820/16076], Loss: 4.7892\n",
      "Epoch [2/2], Step [9840/16076], Loss: 4.7105\n",
      "Epoch [2/2], Step [9860/16076], Loss: 4.7156\n",
      "Epoch [2/2], Step [9880/16076], Loss: 4.7571\n",
      "Epoch [2/2], Step [9900/16076], Loss: 4.7898\n",
      "Epoch [2/2], Step [9920/16076], Loss: 4.7862\n",
      "Epoch [2/2], Step [9940/16076], Loss: 4.7688\n",
      "Epoch [2/2], Step [9960/16076], Loss: 4.7768\n",
      "Epoch [2/2], Step [9980/16076], Loss: 4.7005\n",
      "Epoch [2/2], Step [10000/16076], Loss: 4.7416\n",
      "Epoch [2/2], Step [10020/16076], Loss: 4.7313\n",
      "Epoch [2/2], Step [10040/16076], Loss: 4.7269\n",
      "Epoch [2/2], Step [10060/16076], Loss: 4.8089\n",
      "Epoch [2/2], Step [10080/16076], Loss: 4.7436\n",
      "Epoch [2/2], Step [10100/16076], Loss: 4.7537\n",
      "Epoch [2/2], Step [10120/16076], Loss: 4.7317\n",
      "Epoch [2/2], Step [10140/16076], Loss: 4.7565\n",
      "Epoch [2/2], Step [10160/16076], Loss: 4.8000\n",
      "Epoch [2/2], Step [10180/16076], Loss: 4.8091\n",
      "Epoch [2/2], Step [10200/16076], Loss: 4.7972\n",
      "Epoch [2/2], Step [10220/16076], Loss: 4.7888\n",
      "Epoch [2/2], Step [10240/16076], Loss: 4.8060\n",
      "Epoch [2/2], Step [10260/16076], Loss: 4.8200\n",
      "Epoch [2/2], Step [10280/16076], Loss: 4.8563\n",
      "Epoch [2/2], Step [10300/16076], Loss: 4.7441\n",
      "Epoch [2/2], Step [10320/16076], Loss: 4.6701\n",
      "Epoch [2/2], Step [10340/16076], Loss: 4.7504\n",
      "Epoch [2/2], Step [10360/16076], Loss: 4.7816\n",
      "Epoch [2/2], Step [10380/16076], Loss: 4.7506\n",
      "Epoch [2/2], Step [10400/16076], Loss: 4.7552\n",
      "Epoch [2/2], Step [10420/16076], Loss: 4.6826\n",
      "Epoch [2/2], Step [10440/16076], Loss: 4.7802\n",
      "Epoch [2/2], Step [10460/16076], Loss: 4.7270\n",
      "Epoch [2/2], Step [10480/16076], Loss: 4.7390\n",
      "Epoch [2/2], Step [10500/16076], Loss: 4.7341\n",
      "Epoch [2/2], Step [10520/16076], Loss: 4.7121\n",
      "Epoch [2/2], Step [10540/16076], Loss: 4.7605\n",
      "Epoch [2/2], Step [10560/16076], Loss: 4.7867\n",
      "Epoch [2/2], Step [10580/16076], Loss: 4.8109\n",
      "Epoch [2/2], Step [10600/16076], Loss: 4.7658\n",
      "Epoch [2/2], Step [10620/16076], Loss: 4.7631\n",
      "Epoch [2/2], Step [10640/16076], Loss: 4.7244\n",
      "Epoch [2/2], Step [10660/16076], Loss: 4.7953\n",
      "Epoch [2/2], Step [10680/16076], Loss: 4.8177\n",
      "Epoch [2/2], Step [10700/16076], Loss: 4.7080\n",
      "Epoch [2/2], Step [10720/16076], Loss: 4.7876\n",
      "Epoch [2/2], Step [10740/16076], Loss: 4.7287\n",
      "Epoch [2/2], Step [10760/16076], Loss: 4.6781\n",
      "Epoch [2/2], Step [10780/16076], Loss: 4.7785\n",
      "Epoch [2/2], Step [10800/16076], Loss: 4.8549\n",
      "Epoch [2/2], Step [10820/16076], Loss: 4.7595\n",
      "Epoch [2/2], Step [10840/16076], Loss: 4.7862\n",
      "Epoch [2/2], Step [10860/16076], Loss: 4.8486\n",
      "Epoch [2/2], Step [10880/16076], Loss: 4.6804\n",
      "Epoch [2/2], Step [10900/16076], Loss: 4.7643\n",
      "Epoch [2/2], Step [10920/16076], Loss: 4.6971\n",
      "Epoch [2/2], Step [10940/16076], Loss: 4.7710\n",
      "Epoch [2/2], Step [10960/16076], Loss: 4.7878\n",
      "Epoch [2/2], Step [10980/16076], Loss: 4.7503\n",
      "Epoch [2/2], Step [11000/16076], Loss: 4.8190\n",
      "Epoch [2/2], Step [11020/16076], Loss: 4.8530\n",
      "Epoch [2/2], Step [11040/16076], Loss: 4.7939\n",
      "Epoch [2/2], Step [11060/16076], Loss: 4.7693\n",
      "Epoch [2/2], Step [11080/16076], Loss: 4.7293\n",
      "Epoch [2/2], Step [11100/16076], Loss: 4.6519\n",
      "Epoch [2/2], Step [11120/16076], Loss: 4.7410\n",
      "Epoch [2/2], Step [11140/16076], Loss: 4.6985\n",
      "Epoch [2/2], Step [11160/16076], Loss: 4.7006\n",
      "Epoch [2/2], Step [11180/16076], Loss: 4.7827\n",
      "Epoch [2/2], Step [11200/16076], Loss: 4.7065\n",
      "Epoch [2/2], Step [11220/16076], Loss: 4.6775\n",
      "Epoch [2/2], Step [11240/16076], Loss: 4.7902\n",
      "Epoch [2/2], Step [11260/16076], Loss: 4.7445\n",
      "Epoch [2/2], Step [11280/16076], Loss: 4.8554\n",
      "Epoch [2/2], Step [11300/16076], Loss: 4.7750\n",
      "Epoch [2/2], Step [11320/16076], Loss: 4.7264\n",
      "Epoch [2/2], Step [11340/16076], Loss: 4.8496\n",
      "Epoch [2/2], Step [11360/16076], Loss: 4.7724\n",
      "Epoch [2/2], Step [11380/16076], Loss: 4.8299\n",
      "Epoch [2/2], Step [11400/16076], Loss: 4.8007\n",
      "Epoch [2/2], Step [11420/16076], Loss: 4.7831\n",
      "Epoch [2/2], Step [11440/16076], Loss: 4.7925\n",
      "Epoch [2/2], Step [11460/16076], Loss: 4.7559\n",
      "Epoch [2/2], Step [11480/16076], Loss: 4.8173\n",
      "Epoch [2/2], Step [11500/16076], Loss: 4.8299\n",
      "Epoch [2/2], Step [11520/16076], Loss: 4.7152\n",
      "Epoch [2/2], Step [11540/16076], Loss: 4.8196\n",
      "Epoch [2/2], Step [11560/16076], Loss: 4.7595\n",
      "Epoch [2/2], Step [11580/16076], Loss: 4.7937\n",
      "Epoch [2/2], Step [11600/16076], Loss: 4.6688\n",
      "Epoch [2/2], Step [11620/16076], Loss: 4.6630\n",
      "Epoch [2/2], Step [11640/16076], Loss: 4.7394\n",
      "Epoch [2/2], Step [11660/16076], Loss: 4.6620\n",
      "Epoch [2/2], Step [11680/16076], Loss: 4.7200\n",
      "Epoch [2/2], Step [11700/16076], Loss: 4.8235\n",
      "Epoch [2/2], Step [11720/16076], Loss: 4.6933\n",
      "Epoch [2/2], Step [11740/16076], Loss: 4.6151\n",
      "Epoch [2/2], Step [11760/16076], Loss: 4.7900\n",
      "Epoch [2/2], Step [11780/16076], Loss: 4.6374\n",
      "Epoch [2/2], Step [11800/16076], Loss: 4.7717\n",
      "Epoch [2/2], Step [11820/16076], Loss: 4.7472\n",
      "Epoch [2/2], Step [11840/16076], Loss: 4.7542\n",
      "Epoch [2/2], Step [11860/16076], Loss: 4.7435\n",
      "Epoch [2/2], Step [11880/16076], Loss: 4.6742\n",
      "Epoch [2/2], Step [11900/16076], Loss: 4.7097\n",
      "Epoch [2/2], Step [11920/16076], Loss: 4.8617\n",
      "Epoch [2/2], Step [11940/16076], Loss: 4.8023\n",
      "Epoch [2/2], Step [11960/16076], Loss: 4.7733\n",
      "Epoch [2/2], Step [11980/16076], Loss: 4.6641\n",
      "Epoch [2/2], Step [12000/16076], Loss: 4.7220\n",
      "Epoch [2/2], Step [12020/16076], Loss: 4.8226\n",
      "Epoch [2/2], Step [12040/16076], Loss: 4.7648\n",
      "Epoch [2/2], Step [12060/16076], Loss: 4.7321\n",
      "Epoch [2/2], Step [12080/16076], Loss: 4.8047\n",
      "Epoch [2/2], Step [12100/16076], Loss: 4.7850\n",
      "Epoch [2/2], Step [12120/16076], Loss: 4.7149\n",
      "Epoch [2/2], Step [12140/16076], Loss: 4.7824\n",
      "Epoch [2/2], Step [12160/16076], Loss: 4.7242\n",
      "Epoch [2/2], Step [12180/16076], Loss: 4.7977\n",
      "Epoch [2/2], Step [12200/16076], Loss: 4.6616\n",
      "Epoch [2/2], Step [12220/16076], Loss: 4.7440\n",
      "Epoch [2/2], Step [12240/16076], Loss: 4.7408\n",
      "Epoch [2/2], Step [12260/16076], Loss: 4.7951\n",
      "Epoch [2/2], Step [12280/16076], Loss: 4.6144\n",
      "Epoch [2/2], Step [12300/16076], Loss: 4.6246\n",
      "Epoch [2/2], Step [12320/16076], Loss: 4.8567\n",
      "Epoch [2/2], Step [12340/16076], Loss: 4.7173\n",
      "Epoch [2/2], Step [12360/16076], Loss: 4.6621\n",
      "Epoch [2/2], Step [12380/16076], Loss: 4.7136\n",
      "Epoch [2/2], Step [12400/16076], Loss: 4.7247\n",
      "Epoch [2/2], Step [12420/16076], Loss: 4.8148\n",
      "Epoch [2/2], Step [12440/16076], Loss: 4.7683\n",
      "Epoch [2/2], Step [12460/16076], Loss: 4.7784\n",
      "Epoch [2/2], Step [12480/16076], Loss: 4.8194\n",
      "Epoch [2/2], Step [12500/16076], Loss: 4.6706\n",
      "Epoch [2/2], Step [12520/16076], Loss: 4.6882\n",
      "Epoch [2/2], Step [12540/16076], Loss: 4.7027\n",
      "Epoch [2/2], Step [12560/16076], Loss: 4.6888\n",
      "Epoch [2/2], Step [12580/16076], Loss: 4.7799\n",
      "Epoch [2/2], Step [12600/16076], Loss: 4.7497\n",
      "Epoch [2/2], Step [12620/16076], Loss: 4.7448\n",
      "Epoch [2/2], Step [12640/16076], Loss: 4.8177\n",
      "Epoch [2/2], Step [12660/16076], Loss: 4.7517\n",
      "Epoch [2/2], Step [12680/16076], Loss: 4.7134\n",
      "Epoch [2/2], Step [12700/16076], Loss: 4.7613\n",
      "Epoch [2/2], Step [12720/16076], Loss: 4.7030\n",
      "Epoch [2/2], Step [12740/16076], Loss: 4.7165\n",
      "Epoch [2/2], Step [12760/16076], Loss: 4.6992\n",
      "Epoch [2/2], Step [12780/16076], Loss: 4.7169\n",
      "Epoch [2/2], Step [12800/16076], Loss: 4.7650\n",
      "Epoch [2/2], Step [12820/16076], Loss: 4.7013\n",
      "Epoch [2/2], Step [12840/16076], Loss: 4.7497\n",
      "Epoch [2/2], Step [12860/16076], Loss: 4.7796\n",
      "Epoch [2/2], Step [12880/16076], Loss: 4.7670\n",
      "Epoch [2/2], Step [12900/16076], Loss: 4.7729\n",
      "Epoch [2/2], Step [12920/16076], Loss: 4.6668\n",
      "Epoch [2/2], Step [12940/16076], Loss: 4.8177\n",
      "Epoch [2/2], Step [12960/16076], Loss: 4.7744\n",
      "Epoch [2/2], Step [12980/16076], Loss: 4.7737\n",
      "Epoch [2/2], Step [13000/16076], Loss: 4.6793\n",
      "Epoch [2/2], Step [13020/16076], Loss: 4.7699\n",
      "Epoch [2/2], Step [13040/16076], Loss: 4.6922\n",
      "Epoch [2/2], Step [13060/16076], Loss: 4.7368\n",
      "Epoch [2/2], Step [13080/16076], Loss: 4.6570\n",
      "Epoch [2/2], Step [13100/16076], Loss: 4.7819\n",
      "Epoch [2/2], Step [13120/16076], Loss: 4.7618\n",
      "Epoch [2/2], Step [13140/16076], Loss: 4.7688\n",
      "Epoch [2/2], Step [13160/16076], Loss: 4.6965\n",
      "Epoch [2/2], Step [13180/16076], Loss: 4.8206\n",
      "Epoch [2/2], Step [13200/16076], Loss: 4.7519\n",
      "Epoch [2/2], Step [13220/16076], Loss: 4.7715\n",
      "Epoch [2/2], Step [13240/16076], Loss: 4.6752\n",
      "Epoch [2/2], Step [13260/16076], Loss: 4.8075\n",
      "Epoch [2/2], Step [13280/16076], Loss: 4.6834\n",
      "Epoch [2/2], Step [13300/16076], Loss: 4.7393\n",
      "Epoch [2/2], Step [13320/16076], Loss: 4.7416\n",
      "Epoch [2/2], Step [13340/16076], Loss: 4.7145\n",
      "Epoch [2/2], Step [13360/16076], Loss: 4.7887\n",
      "Epoch [2/2], Step [13380/16076], Loss: 4.7087\n",
      "Epoch [2/2], Step [13400/16076], Loss: 4.7097\n",
      "Epoch [2/2], Step [13420/16076], Loss: 4.6934\n",
      "Epoch [2/2], Step [13440/16076], Loss: 4.6951\n",
      "Epoch [2/2], Step [13460/16076], Loss: 4.7361\n",
      "Epoch [2/2], Step [13480/16076], Loss: 4.7745\n",
      "Epoch [2/2], Step [13500/16076], Loss: 4.8299\n",
      "Epoch [2/2], Step [13520/16076], Loss: 4.8248\n",
      "Epoch [2/2], Step [13540/16076], Loss: 4.7942\n",
      "Epoch [2/2], Step [13560/16076], Loss: 4.8008\n",
      "Epoch [2/2], Step [13580/16076], Loss: 4.7778\n",
      "Epoch [2/2], Step [13600/16076], Loss: 4.7725\n",
      "Epoch [2/2], Step [13620/16076], Loss: 4.6619\n",
      "Epoch [2/2], Step [13640/16076], Loss: 4.7770\n",
      "Epoch [2/2], Step [13660/16076], Loss: 4.7662\n",
      "Epoch [2/2], Step [13680/16076], Loss: 4.7344\n",
      "Epoch [2/2], Step [13700/16076], Loss: 4.6806\n",
      "Epoch [2/2], Step [13720/16076], Loss: 4.7009\n",
      "Epoch [2/2], Step [13740/16076], Loss: 4.7255\n",
      "Epoch [2/2], Step [13760/16076], Loss: 4.7192\n",
      "Epoch [2/2], Step [13780/16076], Loss: 4.7597\n",
      "Epoch [2/2], Step [13800/16076], Loss: 4.7299\n",
      "Epoch [2/2], Step [13820/16076], Loss: 4.8474\n",
      "Epoch [2/2], Step [13840/16076], Loss: 4.6707\n",
      "Epoch [2/2], Step [13860/16076], Loss: 4.7593\n",
      "Epoch [2/2], Step [13880/16076], Loss: 4.7450\n",
      "Epoch [2/2], Step [13900/16076], Loss: 4.7410\n",
      "Epoch [2/2], Step [13920/16076], Loss: 4.7639\n",
      "Epoch [2/2], Step [13940/16076], Loss: 4.7070\n",
      "Epoch [2/2], Step [13960/16076], Loss: 4.7272\n",
      "Epoch [2/2], Step [13980/16076], Loss: 4.8404\n",
      "Epoch [2/2], Step [14000/16076], Loss: 4.7698\n",
      "Epoch [2/2], Step [14020/16076], Loss: 4.6711\n",
      "Epoch [2/2], Step [14040/16076], Loss: 4.7504\n",
      "Epoch [2/2], Step [14060/16076], Loss: 4.7841\n",
      "Epoch [2/2], Step [14080/16076], Loss: 4.7115\n",
      "Epoch [2/2], Step [14100/16076], Loss: 4.7319\n",
      "Epoch [2/2], Step [14120/16076], Loss: 4.7657\n",
      "Epoch [2/2], Step [14140/16076], Loss: 4.6980\n",
      "Epoch [2/2], Step [14160/16076], Loss: 4.7071\n",
      "Epoch [2/2], Step [14180/16076], Loss: 4.7079\n",
      "Epoch [2/2], Step [14200/16076], Loss: 4.7059\n",
      "Epoch [2/2], Step [14220/16076], Loss: 4.7512\n",
      "Epoch [2/2], Step [14240/16076], Loss: 4.7033\n",
      "Epoch [2/2], Step [14260/16076], Loss: 4.7128\n",
      "Epoch [2/2], Step [14280/16076], Loss: 4.8185\n",
      "Epoch [2/2], Step [14300/16076], Loss: 4.7902\n",
      "Epoch [2/2], Step [14320/16076], Loss: 4.7163\n",
      "Epoch [2/2], Step [14340/16076], Loss: 4.7447\n",
      "Epoch [2/2], Step [14360/16076], Loss: 4.7120\n",
      "Epoch [2/2], Step [14380/16076], Loss: 4.6709\n",
      "Epoch [2/2], Step [14400/16076], Loss: 4.6924\n",
      "Epoch [2/2], Step [14420/16076], Loss: 4.7782\n",
      "Epoch [2/2], Step [14440/16076], Loss: 4.7688\n",
      "Epoch [2/2], Step [14460/16076], Loss: 4.6885\n",
      "Epoch [2/2], Step [14480/16076], Loss: 4.7817\n",
      "Epoch [2/2], Step [14500/16076], Loss: 4.6913\n",
      "Epoch [2/2], Step [14520/16076], Loss: 4.7223\n",
      "Epoch [2/2], Step [14540/16076], Loss: 4.8560\n",
      "Epoch [2/2], Step [14560/16076], Loss: 4.7283\n",
      "Epoch [2/2], Step [14580/16076], Loss: 4.7330\n",
      "Epoch [2/2], Step [14600/16076], Loss: 4.7114\n",
      "Epoch [2/2], Step [14620/16076], Loss: 4.6995\n",
      "Epoch [2/2], Step [14640/16076], Loss: 4.6787\n",
      "Epoch [2/2], Step [14660/16076], Loss: 4.7297\n",
      "Epoch [2/2], Step [14680/16076], Loss: 4.6814\n",
      "Epoch [2/2], Step [14700/16076], Loss: 4.7692\n",
      "Epoch [2/2], Step [14720/16076], Loss: 4.7572\n",
      "Epoch [2/2], Step [14740/16076], Loss: 4.7321\n",
      "Epoch [2/2], Step [14760/16076], Loss: 4.7253\n",
      "Epoch [2/2], Step [14780/16076], Loss: 4.7003\n",
      "Epoch [2/2], Step [14800/16076], Loss: 4.7275\n",
      "Epoch [2/2], Step [14820/16076], Loss: 4.7488\n",
      "Epoch [2/2], Step [14840/16076], Loss: 4.7135\n",
      "Epoch [2/2], Step [14860/16076], Loss: 4.7320\n",
      "Epoch [2/2], Step [14880/16076], Loss: 4.7527\n",
      "Epoch [2/2], Step [14900/16076], Loss: 4.7050\n",
      "Epoch [2/2], Step [14920/16076], Loss: 4.7324\n",
      "Epoch [2/2], Step [14940/16076], Loss: 4.6985\n",
      "Epoch [2/2], Step [14960/16076], Loss: 4.7091\n",
      "Epoch [2/2], Step [14980/16076], Loss: 4.7401\n",
      "Epoch [2/2], Step [15000/16076], Loss: 4.7600\n",
      "Epoch [2/2], Step [15020/16076], Loss: 4.7344\n",
      "Epoch [2/2], Step [15040/16076], Loss: 4.7578\n",
      "Epoch [2/2], Step [15060/16076], Loss: 4.6907\n",
      "Epoch [2/2], Step [15080/16076], Loss: 4.7886\n",
      "Epoch [2/2], Step [15100/16076], Loss: 4.7471\n",
      "Epoch [2/2], Step [15120/16076], Loss: 4.6680\n",
      "Epoch [2/2], Step [15140/16076], Loss: 4.6982\n",
      "Epoch [2/2], Step [15160/16076], Loss: 4.6832\n",
      "Epoch [2/2], Step [15180/16076], Loss: 4.7077\n",
      "Epoch [2/2], Step [15200/16076], Loss: 4.8067\n",
      "Epoch [2/2], Step [15220/16076], Loss: 4.8098\n",
      "Epoch [2/2], Step [15240/16076], Loss: 4.7577\n",
      "Epoch [2/2], Step [15260/16076], Loss: 4.7287\n",
      "Epoch [2/2], Step [15280/16076], Loss: 4.7678\n",
      "Epoch [2/2], Step [15300/16076], Loss: 4.7385\n",
      "Epoch [2/2], Step [15320/16076], Loss: 4.7349\n",
      "Epoch [2/2], Step [15340/16076], Loss: 4.7033\n",
      "Epoch [2/2], Step [15360/16076], Loss: 4.7086\n",
      "Epoch [2/2], Step [15380/16076], Loss: 4.7188\n",
      "Epoch [2/2], Step [15400/16076], Loss: 4.6729\n",
      "Epoch [2/2], Step [15420/16076], Loss: 4.7650\n",
      "Epoch [2/2], Step [15440/16076], Loss: 4.6880\n",
      "Epoch [2/2], Step [15460/16076], Loss: 4.7168\n",
      "Epoch [2/2], Step [15480/16076], Loss: 4.7530\n",
      "Epoch [2/2], Step [15500/16076], Loss: 4.7566\n",
      "Epoch [2/2], Step [15520/16076], Loss: 4.6110\n",
      "Epoch [2/2], Step [15540/16076], Loss: 4.6992\n",
      "Epoch [2/2], Step [15560/16076], Loss: 4.7212\n",
      "Epoch [2/2], Step [15580/16076], Loss: 4.8205\n",
      "Epoch [2/2], Step [15600/16076], Loss: 4.6976\n",
      "Epoch [2/2], Step [15620/16076], Loss: 4.6691\n",
      "Epoch [2/2], Step [15640/16076], Loss: 4.7547\n",
      "Epoch [2/2], Step [15660/16076], Loss: 4.6710\n",
      "Epoch [2/2], Step [15680/16076], Loss: 4.7941\n",
      "Epoch [2/2], Step [15700/16076], Loss: 4.7328\n",
      "Epoch [2/2], Step [15720/16076], Loss: 4.8191\n",
      "Epoch [2/2], Step [15740/16076], Loss: 4.7560\n",
      "Epoch [2/2], Step [15760/16076], Loss: 4.6243\n",
      "Epoch [2/2], Step [15780/16076], Loss: 4.7183\n",
      "Epoch [2/2], Step [15800/16076], Loss: 4.7640\n",
      "Epoch [2/2], Step [15820/16076], Loss: 4.7143\n",
      "Epoch [2/2], Step [15840/16076], Loss: 4.7610\n",
      "Epoch [2/2], Step [15860/16076], Loss: 4.7295\n",
      "Epoch [2/2], Step [15880/16076], Loss: 4.6982\n",
      "Epoch [2/2], Step [15900/16076], Loss: 4.6504\n",
      "Epoch [2/2], Step [15920/16076], Loss: 4.7698\n",
      "Epoch [2/2], Step [15940/16076], Loss: 4.7520\n",
      "Epoch [2/2], Step [15960/16076], Loss: 4.7550\n",
      "Epoch [2/2], Step [15980/16076], Loss: 4.6531\n",
      "Epoch [2/2], Step [16000/16076], Loss: 4.7084\n",
      "Epoch [2/2], Step [16020/16076], Loss: 4.8004\n",
      "Epoch [2/2], Step [16040/16076], Loss: 4.6664\n",
      "Epoch [2/2], Step [16060/16076], Loss: 4.7223\n",
      "Epoch [2/2] Average Loss: 4.7763, Perplexity: 118.67\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 128\n",
    "context_length = 32  # Context size for training\n",
    "# vocab_size = tokenizer.n_vocab\n",
    "vocab_size = 30000\n",
    "embedding_dim = 128\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_dataloader(\n",
    "    formatted_text[:9999297], batch_size=batch_size,\n",
    "    context_length=context_length, shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = RegularizedLanguageModel(vocab_size, embedding_dim, context_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses = []\n",
    "perplexities = []\n",
    "\n",
    "# Go through learning epochs\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # Read in data in batches\n",
    "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Apply the forward pass\n",
    "        logits = model(x)\n",
    "\n",
    "        # Reshape logits and labels\n",
    "        token_logits = logits.view(-1, vocab_size)\n",
    "        token_labels = y.view(-1)\n",
    "\n",
    "        # To understand what is happening during reshaping, print out logits.shape and token_logits.shape\n",
    "        # and the same for y\n",
    "        # print(logits.shape, token_logits.shape)\n",
    "        # print(y.shape, token_labels.shape)\n",
    "        # print(y[0])\n",
    "        # print(token_labels[0:10])\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(token_logits,token_labels)\n",
    "\n",
    "        # Apply the backward step (calculate the gradients)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss over batches\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Monitor progress every twenty batches\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Calculate average cross-entropy loss and perplexity\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "\n",
    "    # Monitor developments over learning process\n",
    "    train_losses.append(avg_loss)\n",
    "    perplexities.append(perplexity)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T16:47:28.395182Z",
     "iopub.status.busy": "2024-12-14T16:47:28.394340Z",
     "iopub.status.idle": "2024-12-14T16:47:48.585048Z",
     "shell.execute_reply": "2024-12-14T16:47:48.584113Z",
     "shell.execute_reply.started": "2024-12-14T16:47:28.395147Z"
    },
    "id": "PC4iXCThjySS",
    "outputId": "3f9144d8-3395-4455-9799-ddb315632e86"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of base model: 99.54\n",
      "4.600590808169648\n",
      "9242.586933612823\n"
     ]
    }
   ],
   "source": [
    "steps = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(steps, train_losses, label='Training Loss', linestyle='dashed', marker=\"o\")\n",
    "plt.title('Simple Model - Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps, perplexities, label='Perplexity', linestyle='dashed', marker=\"o\")\n",
    "plt.title('Simple Model - Perplexity over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in dev_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(dev_dataloader)\n",
    "perplexity_simple = math.exp(avg_loss)\n",
    "print(f\"Perplexity of base model: {perplexity_simple:.2f}\")\n",
    "print(avg_loss)\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T16:47:48.586993Z",
     "iopub.status.busy": "2024-12-14T16:47:48.586704Z",
     "iopub.status.idle": "2024-12-14T16:48:12.481869Z",
     "shell.execute_reply": "2024-12-14T16:48:12.480994Z",
     "shell.execute_reply.started": "2024-12-14T16:47:48.586965Z"
    },
    "id": "_27VbE5IjySS",
    "outputId": "76614586-f290-4e2f-94d4-722615c80d14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "\n",
      "[CLS] من در راه[SEP] هستند که می کردند 54 این بیمار کووید۱۶ روستا تو\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, start_text, context_length=15, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = tokenizer.encode(start_text)\n",
    "    context = torch.tensor(generated, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(context_length):\n",
    "            if context.size(1) >= context_length:\n",
    "                break\n",
    "            logits = model(context)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "            context = torch.cat([context, next_token_id.unsqueeze(0)], dim=1)\n",
    "            # print(f\"next_token_logits: {next_token_logits}, probabilities: {probabilities}, next_token_id: {next_token_id}, context: {context}\")\n",
    "\n",
    "\n",
    "    generated_text = tokenizer.decode(context[0].tolist())\n",
    "    return generated_text\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "token_ids = tokenizer.encode(formatted_text)\n",
    "\n",
    "start_text = \" من در راه\"\n",
    "generated_text = generate_text(model, tokenizer, start_text, context_length=20)\n",
    "print(\"Generated Text:\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb1RASo8jySS"
   },
   "source": [
    "؟؟؟  من در راه ؟؟؟  هستند که می کردند 54 این بیمار کووید۱۶ روستا تو"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUamOEyDjySS"
   },
   "source": [
    "# transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T15:04:57.052949Z",
     "iopub.status.busy": "2024-12-14T15:04:57.052594Z",
     "iopub.status.idle": "2024-12-14T15:04:57.060175Z",
     "shell.execute_reply": "2024-12-14T15:04:57.059226Z",
     "shell.execute_reply.started": "2024-12-14T15:04:57.052922Z"
    },
    "id": "SjzKNOZqjySS"
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_length, hidden_dim=256, dropout=0.2):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(context_length, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Adding feedforward layers and non-linear activation functions\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        token_embeds = self.token_embedding(x)\n",
    "        position_embeds = self.position_embedding(positions)\n",
    "\n",
    "        embeddings = token_embeds + position_embeds\n",
    "        dropout_layer = self.dropout(embeddings)\n",
    "\n",
    "        # Apply the feedforward layers\n",
    "        first_layer = self.fc1(dropout_layer)\n",
    "        non_linearity = self.relu(first_layer)\n",
    "        # Second layer\n",
    "        logits = self.fc2(non_linearity)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T15:05:05.526914Z",
     "iopub.status.busy": "2024-12-14T15:05:05.526239Z",
     "iopub.status.idle": "2024-12-14T15:05:05.534154Z",
     "shell.execute_reply": "2024-12-14T15:05:05.533156Z",
     "shell.execute_reply.started": "2024-12-14T15:05:05.526883Z"
    },
    "id": "WeAxjWBTjySS"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, attention_dim, context_length):\n",
    "        super(CausalSelfAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.W_q = nn.Linear(embedding_dim, attention_dim)\n",
    "        self.W_k = nn.Linear(embedding_dim, attention_dim)\n",
    "        self.W_v = nn.Linear(embedding_dim, attention_dim)\n",
    "\n",
    "\n",
    "        # Register a buffer for the causal mask\n",
    "        self.register_buffer(  'mask',torch.tril(torch.ones(context_length, context_length)).unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.attention_dim)\n",
    "\n",
    "        # Apply the causal mask\n",
    "        mask = self.mask[:, :seq_length, :seq_length]\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T15:05:09.043932Z",
     "iopub.status.busy": "2024-12-14T15:05:09.043210Z",
     "iopub.status.idle": "2024-12-14T15:05:09.053540Z",
     "shell.execute_reply": "2024-12-14T15:05:09.052544Z",
     "shell.execute_reply.started": "2024-12-14T15:05:09.043898Z"
    },
    "id": "bSfsQ49KjyST"
   },
   "outputs": [],
   "source": [
    "class MultiHeadCausalAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, attention_dim, num_heads, context_length, dropout=0.1):\n",
    "        super(MultiHeadCausalAttention, self).__init__()\n",
    "        assert attention_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = attention_dim // num_heads\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.W_q = nn.Linear(embedding_dim, attention_dim)\n",
    "        self.W_k = nn.Linear(embedding_dim, attention_dim)\n",
    "        self.W_v = nn.Linear(embedding_dim, attention_dim)\n",
    "        self.fc_out = nn.Linear(attention_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer( 'mask',torch.tril(torch.ones(context_length, context_length)).unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # Split the embedding into self.num_heads different pieces\n",
    "        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "        #print(\"Q,K,V: \", Q.shape, K.shape, V.shape)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        #print(\"scores: \", scores.shape)\n",
    "        # Apply the causal mask\n",
    "        mask = self.mask[:, :seq_length, :seq_length]\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        #print(\"attention weights, values: \", attention_weights.shape, V.shape)\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        #print(\"context: \", context.shape)\n",
    "        context = context.transpose(1, 2).contiguous()\n",
    "        #print(\"context after transpose: \", context.shape)\n",
    "        context = context.view(batch_size, seq_length, self.attention_dim)\n",
    "\n",
    "\n",
    "        out = self.fc_out(context)\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T15:05:13.068415Z",
     "iopub.status.busy": "2024-12-14T15:05:13.067757Z",
     "iopub.status.idle": "2024-12-14T15:05:13.075403Z",
     "shell.execute_reply": "2024-12-14T15:05:13.074446Z",
     "shell.execute_reply.started": "2024-12-14T15:05:13.068385Z"
    },
    "id": "hHRtUAf0jyST"
   },
   "outputs": [],
   "source": [
    "class LanguageModelWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, attention_dim, context_length, hidden_dim=256, num_heads=8, dropout=0.2):\n",
    "        super(LanguageModelWithAttention, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(context_length, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attention = MultiHeadCausalAttention(embedding_dim, attention_dim, num_heads, context_length, dropout)\n",
    "\n",
    "        # Adding more layers and non-linear activation functions\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        token_embeds = self.token_embedding(x)\n",
    "        position_embeds = self.position_embedding(positions)\n",
    "\n",
    "        embeddings = token_embeds + position_embeds\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        # Apply the attention layer\n",
    "        context, attention_weights = self.attention(embeddings)\n",
    "\n",
    "        # Apply the feedforward layers\n",
    "        out = self.fc1(context)\n",
    "        out = self.relu(out)\n",
    "        logits = self.fc2(out)\n",
    "        return logits, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-14T15:05:18.663943Z",
     "iopub.status.busy": "2024-12-14T15:05:18.663344Z",
     "iopub.status.idle": "2024-12-14T15:06:41.658643Z",
     "shell.execute_reply": "2024-12-14T15:06:41.657557Z",
     "shell.execute_reply.started": "2024-12-14T15:05:18.663908Z"
    },
    "id": "So3ZdROKjyST",
    "outputId": "e82e47cb-cf5a-4fc4-b1e2-0a5d9330dd2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "context_length = 32  # Increased context size\n",
    "# vocab_size = tokenizer.n_vocab\n",
    "vocab_size = len(tokenizer)\n",
    "embedding_dim = 128\n",
    "attention_dim = 64\n",
    "hidden_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_dataloader(\n",
    "    raw_text[:9999297], batch_size=batch_size,\n",
    "    context_length=context_length, shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = LanguageModelWithAttention(\n",
    "    vocab_size, embedding_dim, attention_dim, context_length, hidden_dim, num_heads, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters'\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-14T15:25:37.488032Z",
     "iopub.status.busy": "2024-12-14T15:25:37.487644Z",
     "iopub.status.idle": "2024-12-14T15:41:28.733605Z",
     "shell.execute_reply": "2024-12-14T15:41:28.732655Z",
     "shell.execute_reply.started": "2024-12-14T15:25:37.488003Z"
    },
    "id": "z236JdYWjyST",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4f7beac3-dc78-4d9e-a138-867010c0b7f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [0/67476], Loss: 4.8641\n",
      "Epoch [1/2], Step [10/67476], Loss: 4.4229\n",
      "Epoch [1/2], Step [20/67476], Loss: 4.7749\n",
      "Epoch [1/2], Step [30/67476], Loss: 4.6650\n",
      "Epoch [1/2], Step [40/67476], Loss: 4.7652\n",
      "Epoch [1/2], Step [50/67476], Loss: 4.4543\n",
      "Epoch [1/2], Step [60/67476], Loss: 4.7768\n",
      "Epoch [1/2], Step [70/67476], Loss: 4.7679\n",
      "Epoch [1/2], Step [80/67476], Loss: 4.7035\n",
      "Epoch [1/2], Step [90/67476], Loss: 4.3886\n",
      "Epoch [1/2], Step [100/67476], Loss: 4.7196\n",
      "Epoch [1/2], Step [110/67476], Loss: 4.7399\n",
      "Epoch [1/2], Step [120/67476], Loss: 4.5634\n",
      "Epoch [1/2], Step [130/67476], Loss: 4.7028\n",
      "Epoch [1/2], Step [140/67476], Loss: 4.5428\n",
      "Epoch [1/2], Step [150/67476], Loss: 4.5316\n",
      "Epoch [1/2], Step [160/67476], Loss: 4.7211\n",
      "Epoch [1/2], Step [170/67476], Loss: 4.6018\n",
      "Epoch [1/2], Step [180/67476], Loss: 4.8427\n",
      "Epoch [1/2], Step [190/67476], Loss: 4.6155\n",
      "Epoch [1/2], Step [200/67476], Loss: 4.6325\n",
      "Epoch [1/2], Step [210/67476], Loss: 4.8043\n",
      "Epoch [1/2], Step [220/67476], Loss: 4.4829\n",
      "Epoch [1/2], Step [230/67476], Loss: 4.6235\n",
      "Epoch [1/2], Step [240/67476], Loss: 4.7684\n",
      "Epoch [1/2], Step [250/67476], Loss: 4.6777\n",
      "Epoch [1/2], Step [260/67476], Loss: 4.5999\n",
      "Epoch [1/2], Step [270/67476], Loss: 4.5134\n",
      "Epoch [1/2], Step [280/67476], Loss: 4.6754\n",
      "Epoch [1/2], Step [290/67476], Loss: 4.7107\n",
      "Epoch [1/2], Step [300/67476], Loss: 4.6641\n",
      "Epoch [1/2], Step [310/67476], Loss: 4.5839\n",
      "Epoch [1/2], Step [320/67476], Loss: 4.5289\n",
      "Epoch [1/2], Step [330/67476], Loss: 4.4622\n",
      "Epoch [1/2], Step [340/67476], Loss: 4.5644\n",
      "Epoch [1/2], Step [350/67476], Loss: 4.6586\n",
      "Epoch [1/2], Step [360/67476], Loss: 4.5484\n",
      "Epoch [1/2], Step [370/67476], Loss: 4.5027\n",
      "Epoch [1/2], Step [380/67476], Loss: 4.5970\n",
      "Epoch [1/2], Step [390/67476], Loss: 4.5374\n",
      "Epoch [1/2], Step [400/67476], Loss: 4.7493\n",
      "Epoch [1/2], Step [410/67476], Loss: 4.7283\n",
      "Epoch [1/2], Step [420/67476], Loss: 4.5621\n",
      "Epoch [1/2], Step [430/67476], Loss: 4.4781\n",
      "Epoch [1/2], Step [440/67476], Loss: 4.5378\n",
      "Epoch [1/2], Step [450/67476], Loss: 4.6561\n",
      "Epoch [1/2], Step [460/67476], Loss: 4.5691\n",
      "Epoch [1/2], Step [470/67476], Loss: 4.6118\n",
      "Epoch [1/2], Step [480/67476], Loss: 4.7680\n",
      "Epoch [1/2], Step [490/67476], Loss: 4.4483\n",
      "Epoch [1/2], Step [500/67476], Loss: 4.5738\n",
      "Epoch [1/2], Step [510/67476], Loss: 4.6343\n",
      "Epoch [1/2], Step [520/67476], Loss: 4.6570\n",
      "Epoch [1/2], Step [530/67476], Loss: 4.3522\n",
      "Epoch [1/2], Step [540/67476], Loss: 4.5942\n",
      "Epoch [1/2], Step [550/67476], Loss: 4.5090\n",
      "Epoch [1/2], Step [560/67476], Loss: 4.5041\n",
      "Epoch [1/2], Step [570/67476], Loss: 4.5515\n",
      "Epoch [1/2], Step [580/67476], Loss: 4.5445\n",
      "Epoch [1/2], Step [590/67476], Loss: 4.6709\n",
      "Epoch [1/2], Step [600/67476], Loss: 4.8820\n",
      "Epoch [1/2], Step [610/67476], Loss: 4.4667\n",
      "Epoch [1/2], Step [620/67476], Loss: 4.7128\n",
      "Epoch [1/2], Step [630/67476], Loss: 4.5164\n",
      "Epoch [1/2], Step [640/67476], Loss: 4.4911\n",
      "Epoch [1/2], Step [650/67476], Loss: 4.5262\n",
      "Epoch [1/2], Step [660/67476], Loss: 4.7142\n",
      "Epoch [1/2], Step [670/67476], Loss: 4.7305\n",
      "Epoch [1/2], Step [680/67476], Loss: 4.6671\n",
      "Epoch [1/2], Step [690/67476], Loss: 4.5677\n",
      "Epoch [1/2], Step [700/67476], Loss: 4.8634\n",
      "Epoch [1/2], Step [710/67476], Loss: 4.5080\n",
      "Epoch [1/2], Step [720/67476], Loss: 4.5890\n",
      "Epoch [1/2], Step [730/67476], Loss: 4.6845\n",
      "Epoch [1/2], Step [740/67476], Loss: 4.6521\n",
      "Epoch [1/2], Step [750/67476], Loss: 4.5376\n",
      "Epoch [1/2], Step [760/67476], Loss: 4.7228\n",
      "Epoch [1/2], Step [770/67476], Loss: 4.6327\n",
      "Epoch [1/2], Step [780/67476], Loss: 4.5889\n",
      "Epoch [1/2], Step [790/67476], Loss: 4.6578\n",
      "Epoch [1/2], Step [800/67476], Loss: 4.5000\n",
      "Epoch [1/2], Step [810/67476], Loss: 4.5711\n",
      "Epoch [1/2], Step [820/67476], Loss: 4.5783\n",
      "Epoch [1/2], Step [830/67476], Loss: 4.5538\n",
      "Epoch [1/2], Step [840/67476], Loss: 4.4617\n",
      "Epoch [1/2], Step [850/67476], Loss: 4.4980\n",
      "Epoch [1/2], Step [860/67476], Loss: 4.3912\n",
      "Epoch [1/2], Step [870/67476], Loss: 4.7959\n",
      "Epoch [1/2], Step [880/67476], Loss: 4.6198\n",
      "Epoch [1/2], Step [890/67476], Loss: 4.6546\n",
      "Epoch [1/2], Step [900/67476], Loss: 4.8715\n",
      "Epoch [1/2], Step [910/67476], Loss: 4.5362\n",
      "Epoch [1/2], Step [920/67476], Loss: 4.5557\n",
      "Epoch [1/2], Step [930/67476], Loss: 4.6077\n",
      "Epoch [1/2], Step [940/67476], Loss: 4.3901\n",
      "Epoch [1/2], Step [950/67476], Loss: 4.8938\n",
      "Epoch [1/2], Step [960/67476], Loss: 4.7050\n",
      "Epoch [1/2], Step [970/67476], Loss: 4.6057\n",
      "Epoch [1/2], Step [980/67476], Loss: 4.6261\n",
      "Epoch [1/2], Step [990/67476], Loss: 4.3973\n",
      "Epoch [1/2], Step [1000/67476], Loss: 4.5168\n",
      "Epoch [1/2], Step [1010/67476], Loss: 4.6865\n",
      "Epoch [1/2], Step [1020/67476], Loss: 4.5826\n",
      "Epoch [1/2], Step [1030/67476], Loss: 4.6600\n",
      "Epoch [1/2], Step [1040/67476], Loss: 4.7394\n",
      "Epoch [1/2], Step [1050/67476], Loss: 4.6043\n",
      "Epoch [1/2], Step [1060/67476], Loss: 4.6828\n",
      "Epoch [1/2], Step [1070/67476], Loss: 4.7857\n",
      "Epoch [1/2], Step [1080/67476], Loss: 4.5863\n",
      "Epoch [1/2], Step [1090/67476], Loss: 4.6926\n",
      "Epoch [1/2], Step [1100/67476], Loss: 4.6930\n",
      "Epoch [1/2], Step [1110/67476], Loss: 4.5273\n",
      "Epoch [1/2], Step [1120/67476], Loss: 4.6195\n",
      "Epoch [1/2], Step [1130/67476], Loss: 4.5781\n",
      "Epoch [1/2], Step [1140/67476], Loss: 4.5947\n",
      "Epoch [1/2], Step [1150/67476], Loss: 4.3401\n",
      "Epoch [1/2], Step [1160/67476], Loss: 4.5409\n",
      "Epoch [1/2], Step [1170/67476], Loss: 4.5461\n",
      "Epoch [1/2], Step [1180/67476], Loss: 4.6113\n",
      "Epoch [1/2], Step [1190/67476], Loss: 4.5593\n",
      "Epoch [1/2], Step [1200/67476], Loss: 4.5472\n",
      "Epoch [1/2], Step [1210/67476], Loss: 4.5891\n",
      "Epoch [1/2], Step [1220/67476], Loss: 4.4982\n",
      "Epoch [1/2], Step [1230/67476], Loss: 4.6205\n",
      "Epoch [1/2], Step [1240/67476], Loss: 4.7344\n",
      "Epoch [1/2], Step [1250/67476], Loss: 4.6125\n",
      "Epoch [1/2], Step [1260/67476], Loss: 4.7411\n",
      "Epoch [1/2], Step [1270/67476], Loss: 4.4822\n",
      "Epoch [1/2], Step [1280/67476], Loss: 4.7791\n",
      "Epoch [1/2], Step [1290/67476], Loss: 4.4278\n",
      "Epoch [1/2], Step [1300/67476], Loss: 4.1483\n",
      "Epoch [1/2], Step [1310/67476], Loss: 4.7402\n",
      "Epoch [1/2], Step [1320/67476], Loss: 4.5906\n",
      "Epoch [1/2], Step [1330/67476], Loss: 4.4838\n",
      "Epoch [1/2], Step [1340/67476], Loss: 4.5607\n",
      "Epoch [1/2], Step [1350/67476], Loss: 4.4449\n",
      "Epoch [1/2], Step [1360/67476], Loss: 4.5735\n",
      "Epoch [1/2], Step [1370/67476], Loss: 4.5322\n",
      "Epoch [1/2], Step [1380/67476], Loss: 4.6776\n",
      "Epoch [1/2], Step [1390/67476], Loss: 4.3915\n",
      "Epoch [1/2], Step [1400/67476], Loss: 4.7663\n",
      "Epoch [1/2], Step [1410/67476], Loss: 4.6551\n",
      "Epoch [1/2], Step [1420/67476], Loss: 4.3758\n",
      "Epoch [1/2], Step [1430/67476], Loss: 4.5156\n",
      "Epoch [1/2], Step [1440/67476], Loss: 4.5557\n",
      "Epoch [1/2], Step [1450/67476], Loss: 4.5860\n",
      "Epoch [1/2], Step [1460/67476], Loss: 4.6955\n",
      "Epoch [1/2], Step [1470/67476], Loss: 4.7103\n",
      "Epoch [1/2], Step [1480/67476], Loss: 4.3839\n",
      "Epoch [1/2], Step [1490/67476], Loss: 4.4673\n",
      "Epoch [1/2], Step [1500/67476], Loss: 4.5487\n",
      "Epoch [1/2], Step [1510/67476], Loss: 4.4288\n",
      "Epoch [1/2], Step [1520/67476], Loss: 4.5181\n",
      "Epoch [1/2], Step [1530/67476], Loss: 4.5228\n",
      "Epoch [1/2], Step [1540/67476], Loss: 4.4099\n",
      "Epoch [1/2], Step [1550/67476], Loss: 4.6198\n",
      "Epoch [1/2], Step [1560/67476], Loss: 4.6541\n",
      "Epoch [1/2], Step [1570/67476], Loss: 4.5249\n",
      "Epoch [1/2], Step [1580/67476], Loss: 4.4782\n",
      "Epoch [1/2], Step [1590/67476], Loss: 4.6902\n",
      "Epoch [1/2], Step [1600/67476], Loss: 4.5209\n",
      "Epoch [1/2], Step [1610/67476], Loss: 4.5058\n",
      "Epoch [1/2], Step [1620/67476], Loss: 4.3911\n",
      "Epoch [1/2], Step [1630/67476], Loss: 4.5037\n",
      "Epoch [1/2], Step [1640/67476], Loss: 4.5235\n",
      "Epoch [1/2], Step [1650/67476], Loss: 4.4929\n",
      "Epoch [1/2], Step [1660/67476], Loss: 4.6954\n",
      "Epoch [1/2], Step [1670/67476], Loss: 4.5953\n",
      "Epoch [1/2], Step [1680/67476], Loss: 4.4896\n",
      "Epoch [1/2], Step [1690/67476], Loss: 4.6438\n",
      "Epoch [1/2], Step [1700/67476], Loss: 4.6090\n",
      "Epoch [1/2], Step [1710/67476], Loss: 4.5009\n",
      "Epoch [1/2], Step [1720/67476], Loss: 4.7289\n",
      "Epoch [1/2], Step [1730/67476], Loss: 4.7347\n",
      "Epoch [1/2], Step [1740/67476], Loss: 4.6101\n",
      "Epoch [1/2], Step [1750/67476], Loss: 4.4806\n",
      "Epoch [1/2], Step [1760/67476], Loss: 4.6646\n",
      "Epoch [1/2], Step [1770/67476], Loss: 4.5416\n",
      "Epoch [1/2], Step [1780/67476], Loss: 4.7023\n",
      "Epoch [1/2], Step [1790/67476], Loss: 4.6436\n",
      "Epoch [1/2], Step [1800/67476], Loss: 4.4085\n",
      "Epoch [1/2], Step [1810/67476], Loss: 4.7730\n",
      "Epoch [1/2], Step [1820/67476], Loss: 4.5937\n",
      "Epoch [1/2], Step [1830/67476], Loss: 4.5243\n",
      "Epoch [1/2], Step [1840/67476], Loss: 4.5339\n",
      "Epoch [1/2], Step [1850/67476], Loss: 4.5596\n",
      "Epoch [1/2], Step [1860/67476], Loss: 4.5430\n",
      "Epoch [1/2], Step [1870/67476], Loss: 4.5813\n",
      "Epoch [1/2], Step [1880/67476], Loss: 4.3661\n",
      "Epoch [1/2], Step [1890/67476], Loss: 4.4378\n",
      "Epoch [1/2], Step [1900/67476], Loss: 4.4927\n",
      "Epoch [1/2], Step [1910/67476], Loss: 4.7184\n",
      "Epoch [1/2], Step [1920/67476], Loss: 4.6780\n",
      "Epoch [1/2], Step [1930/67476], Loss: 4.6959\n",
      "Epoch [1/2], Step [1940/67476], Loss: 4.8023\n",
      "Epoch [1/2], Step [1950/67476], Loss: 4.5968\n",
      "Epoch [1/2], Step [1960/67476], Loss: 4.8454\n",
      "Epoch [1/2], Step [1970/67476], Loss: 4.7295\n",
      "Epoch [1/2], Step [1980/67476], Loss: 4.7107\n",
      "Epoch [1/2], Step [1990/67476], Loss: 4.5428\n",
      "Epoch [1/2], Step [2000/67476], Loss: 4.4579\n",
      "Epoch [1/2], Step [2010/67476], Loss: 4.3910\n",
      "Epoch [1/2], Step [2020/67476], Loss: 4.6595\n",
      "Epoch [1/2], Step [2030/67476], Loss: 4.4652\n",
      "Epoch [1/2], Step [2040/67476], Loss: 4.4550\n",
      "Epoch [1/2], Step [2050/67476], Loss: 4.6689\n",
      "Epoch [1/2], Step [2060/67476], Loss: 4.5114\n",
      "Epoch [1/2], Step [2070/67476], Loss: 4.5509\n",
      "Epoch [1/2], Step [2080/67476], Loss: 4.4458\n",
      "Epoch [1/2], Step [2090/67476], Loss: 4.6000\n",
      "Epoch [1/2], Step [2100/67476], Loss: 4.7599\n",
      "Epoch [1/2], Step [2110/67476], Loss: 4.5605\n",
      "Epoch [1/2], Step [2120/67476], Loss: 4.6057\n",
      "Epoch [1/2], Step [2130/67476], Loss: 4.3893\n",
      "Epoch [1/2], Step [2140/67476], Loss: 4.5449\n",
      "Epoch [1/2], Step [2150/67476], Loss: 4.6682\n",
      "Epoch [1/2], Step [2160/67476], Loss: 4.6176\n",
      "Epoch [1/2], Step [2170/67476], Loss: 4.6677\n",
      "Epoch [1/2], Step [2180/67476], Loss: 4.4112\n",
      "Epoch [1/2], Step [2190/67476], Loss: 4.7200\n",
      "Epoch [1/2], Step [2200/67476], Loss: 4.5800\n",
      "Epoch [1/2], Step [2210/67476], Loss: 4.6172\n",
      "Epoch [1/2], Step [2220/67476], Loss: 4.7502\n",
      "Epoch [1/2], Step [2230/67476], Loss: 4.5937\n",
      "Epoch [1/2], Step [2240/67476], Loss: 4.4794\n",
      "Epoch [1/2], Step [2250/67476], Loss: 4.6263\n",
      "Epoch [1/2], Step [2260/67476], Loss: 4.7731\n",
      "Epoch [1/2], Step [2270/67476], Loss: 4.4500\n",
      "Epoch [1/2], Step [2280/67476], Loss: 4.4680\n",
      "Epoch [1/2], Step [2290/67476], Loss: 4.7316\n",
      "Epoch [1/2], Step [2300/67476], Loss: 4.6599\n",
      "Epoch [1/2], Step [2310/67476], Loss: 4.5985\n",
      "Epoch [1/2], Step [2320/67476], Loss: 4.4813\n",
      "Epoch [1/2], Step [2330/67476], Loss: 4.6623\n",
      "Epoch [1/2], Step [2340/67476], Loss: 4.4724\n",
      "Epoch [1/2], Step [2350/67476], Loss: 4.7059\n",
      "Epoch [1/2], Step [2360/67476], Loss: 4.7929\n",
      "Epoch [1/2], Step [2370/67476], Loss: 4.6690\n",
      "Epoch [1/2], Step [2380/67476], Loss: 4.5380\n",
      "Epoch [1/2], Step [2390/67476], Loss: 4.5630\n",
      "Epoch [1/2], Step [2400/67476], Loss: 4.6780\n",
      "Epoch [1/2], Step [2410/67476], Loss: 4.7252\n",
      "Epoch [1/2], Step [2420/67476], Loss: 4.3583\n",
      "Epoch [1/2], Step [2430/67476], Loss: 4.5418\n",
      "Epoch [1/2], Step [2440/67476], Loss: 4.4292\n",
      "Epoch [1/2], Step [2450/67476], Loss: 4.5082\n",
      "Epoch [1/2], Step [2460/67476], Loss: 4.5416\n",
      "Epoch [1/2], Step [2470/67476], Loss: 4.5513\n",
      "Epoch [1/2], Step [2480/67476], Loss: 4.5949\n",
      "Epoch [1/2], Step [2490/67476], Loss: 4.4100\n",
      "Epoch [1/2], Step [2500/67476], Loss: 4.4721\n",
      "Epoch [1/2], Step [2510/67476], Loss: 4.7118\n",
      "Epoch [1/2], Step [2520/67476], Loss: 4.5604\n",
      "Epoch [1/2], Step [2530/67476], Loss: 4.4454\n",
      "Epoch [1/2], Step [2540/67476], Loss: 4.7231\n",
      "Epoch [1/2], Step [2550/67476], Loss: 4.6364\n",
      "Epoch [1/2], Step [2560/67476], Loss: 4.8744\n",
      "Epoch [1/2], Step [2570/67476], Loss: 4.5299\n",
      "Epoch [1/2], Step [2580/67476], Loss: 4.6097\n",
      "Epoch [1/2], Step [2590/67476], Loss: 4.4872\n",
      "Epoch [1/2], Step [2600/67476], Loss: 4.6356\n",
      "Epoch [1/2], Step [2610/67476], Loss: 4.5831\n",
      "Epoch [1/2], Step [2620/67476], Loss: 4.7951\n",
      "Epoch [1/2], Step [2630/67476], Loss: 4.6375\n",
      "Epoch [1/2], Step [2640/67476], Loss: 4.7646\n",
      "Epoch [1/2], Step [2650/67476], Loss: 4.4431\n",
      "Epoch [1/2], Step [2660/67476], Loss: 4.6260\n",
      "Epoch [1/2], Step [2670/67476], Loss: 4.5162\n",
      "Epoch [1/2], Step [2680/67476], Loss: 4.5159\n",
      "Epoch [1/2], Step [2690/67476], Loss: 4.5354\n",
      "Epoch [1/2], Step [2700/67476], Loss: 4.5535\n",
      "Epoch [1/2], Step [2710/67476], Loss: 4.6105\n",
      "Epoch [1/2], Step [2720/67476], Loss: 4.6875\n",
      "Epoch [1/2], Step [2730/67476], Loss: 4.6220\n",
      "Epoch [1/2], Step [2740/67476], Loss: 4.5566\n",
      "Epoch [1/2], Step [2750/67476], Loss: 4.6039\n",
      "Epoch [1/2], Step [2760/67476], Loss: 4.4183\n",
      "Epoch [1/2], Step [2770/67476], Loss: 4.5053\n",
      "Epoch [1/2], Step [2780/67476], Loss: 4.6825\n",
      "Epoch [1/2], Step [2790/67476], Loss: 4.5646\n",
      "Epoch [1/2], Step [2800/67476], Loss: 4.4711\n",
      "Epoch [1/2], Step [2810/67476], Loss: 4.6353\n",
      "Epoch [1/2], Step [2820/67476], Loss: 4.7900\n",
      "Epoch [1/2], Step [2830/67476], Loss: 4.5535\n",
      "Epoch [1/2], Step [2840/67476], Loss: 4.7477\n",
      "Epoch [1/2], Step [2850/67476], Loss: 4.5137\n",
      "Epoch [1/2], Step [2860/67476], Loss: 4.7488\n",
      "Epoch [1/2], Step [2870/67476], Loss: 4.6369\n",
      "Epoch [1/2], Step [2880/67476], Loss: 4.6550\n",
      "Epoch [1/2], Step [2890/67476], Loss: 4.4235\n",
      "Epoch [1/2], Step [2900/67476], Loss: 4.6250\n",
      "Epoch [1/2], Step [2910/67476], Loss: 4.6829\n",
      "Epoch [1/2], Step [2920/67476], Loss: 4.6051\n",
      "Epoch [1/2], Step [2930/67476], Loss: 4.5685\n",
      "Epoch [1/2], Step [2940/67476], Loss: 4.6103\n",
      "Epoch [1/2], Step [2950/67476], Loss: 4.6097\n",
      "Epoch [1/2], Step [2960/67476], Loss: 4.7387\n",
      "Epoch [1/2], Step [2970/67476], Loss: 4.6857\n",
      "Epoch [1/2], Step [2980/67476], Loss: 4.3900\n",
      "Epoch [1/2], Step [2990/67476], Loss: 4.5864\n",
      "Epoch [1/2], Step [3000/67476], Loss: 4.5218\n",
      "Epoch [1/2], Step [3010/67476], Loss: 4.4309\n",
      "Epoch [1/2], Step [3020/67476], Loss: 4.6891\n",
      "Epoch [1/2], Step [3030/67476], Loss: 4.5529\n",
      "Epoch [1/2], Step [3040/67476], Loss: 4.4144\n",
      "Epoch [1/2], Step [3050/67476], Loss: 4.4317\n",
      "Epoch [1/2], Step [3060/67476], Loss: 4.5558\n",
      "Epoch [1/2], Step [3070/67476], Loss: 4.5266\n",
      "Epoch [1/2], Step [3080/67476], Loss: 4.6111\n",
      "Epoch [1/2], Step [3090/67476], Loss: 4.8174\n",
      "Epoch [1/2], Step [3100/67476], Loss: 4.4634\n",
      "Epoch [1/2], Step [3110/67476], Loss: 4.5797\n",
      "Epoch [1/2], Step [3120/67476], Loss: 4.5774\n",
      "Epoch [1/2], Step [3130/67476], Loss: 4.6557\n",
      "Epoch [1/2], Step [3140/67476], Loss: 4.5819\n",
      "Epoch [1/2], Step [3150/67476], Loss: 4.7245\n",
      "Epoch [1/2], Step [3160/67476], Loss: 4.8197\n",
      "Epoch [1/2], Step [3170/67476], Loss: 4.7704\n",
      "Epoch [1/2], Step [3180/67476], Loss: 4.6368\n",
      "Epoch [1/2], Step [3190/67476], Loss: 4.7415\n",
      "Epoch [1/2], Step [3200/67476], Loss: 4.5897\n",
      "Epoch [1/2], Step [3210/67476], Loss: 4.5748\n",
      "Epoch [1/2], Step [3220/67476], Loss: 4.6626\n",
      "Epoch [1/2], Step [3230/67476], Loss: 4.3328\n",
      "Epoch [1/2], Step [3240/67476], Loss: 4.7614\n",
      "Epoch [1/2], Step [3250/67476], Loss: 4.6527\n",
      "Epoch [1/2], Step [3260/67476], Loss: 4.7303\n",
      "Epoch [1/2], Step [3270/67476], Loss: 4.7721\n",
      "Epoch [1/2], Step [3280/67476], Loss: 4.6222\n",
      "Epoch [1/2], Step [3290/67476], Loss: 4.5528\n",
      "Epoch [1/2], Step [3300/67476], Loss: 4.7351\n",
      "Epoch [1/2], Step [3310/67476], Loss: 4.6609\n",
      "Epoch [1/2], Step [3320/67476], Loss: 4.6596\n",
      "Epoch [1/2], Step [3330/67476], Loss: 4.6339\n",
      "Epoch [1/2], Step [3340/67476], Loss: 4.6480\n",
      "Epoch [1/2], Step [3350/67476], Loss: 4.6150\n",
      "Epoch [1/2], Step [3360/67476], Loss: 4.4973\n",
      "Epoch [1/2], Step [3370/67476], Loss: 4.5158\n",
      "Epoch [1/2], Step [3380/67476], Loss: 4.6309\n",
      "Epoch [1/2], Step [3390/67476], Loss: 4.6227\n",
      "Epoch [1/2], Step [3400/67476], Loss: 4.3824\n",
      "Epoch [1/2], Step [3410/67476], Loss: 4.8097\n",
      "Epoch [1/2], Step [3420/67476], Loss: 4.5316\n",
      "Epoch [1/2], Step [3430/67476], Loss: 4.8281\n",
      "Epoch [1/2], Step [3440/67476], Loss: 4.5911\n",
      "Epoch [1/2], Step [3450/67476], Loss: 4.6955\n",
      "Epoch [1/2], Step [3460/67476], Loss: 4.6437\n",
      "Epoch [1/2], Step [3470/67476], Loss: 4.5205\n",
      "Epoch [1/2], Step [3480/67476], Loss: 4.6172\n",
      "Epoch [1/2], Step [3490/67476], Loss: 4.9066\n",
      "Epoch [1/2], Step [3500/67476], Loss: 4.5258\n",
      "Epoch [1/2], Step [3510/67476], Loss: 4.8089\n",
      "Epoch [1/2], Step [3520/67476], Loss: 4.7808\n",
      "Epoch [1/2], Step [3530/67476], Loss: 4.4491\n",
      "Epoch [1/2], Step [3540/67476], Loss: 4.5638\n",
      "Epoch [1/2], Step [3550/67476], Loss: 4.6931\n",
      "Epoch [1/2], Step [3560/67476], Loss: 4.5232\n",
      "Epoch [1/2], Step [3570/67476], Loss: 4.5458\n",
      "Epoch [1/2], Step [3580/67476], Loss: 4.6004\n",
      "Epoch [1/2], Step [3590/67476], Loss: 4.6748\n",
      "Epoch [1/2], Step [3600/67476], Loss: 4.6177\n",
      "Epoch [1/2], Step [3610/67476], Loss: 4.5123\n",
      "Epoch [1/2], Step [3620/67476], Loss: 4.6678\n",
      "Epoch [1/2], Step [3630/67476], Loss: 4.6722\n",
      "Epoch [1/2], Step [3640/67476], Loss: 4.8026\n",
      "Epoch [1/2], Step [3650/67476], Loss: 4.4895\n",
      "Epoch [1/2], Step [3660/67476], Loss: 4.4670\n",
      "Epoch [1/2], Step [3670/67476], Loss: 4.7497\n",
      "Epoch [1/2], Step [3680/67476], Loss: 4.6432\n",
      "Epoch [1/2], Step [3690/67476], Loss: 4.6031\n",
      "Epoch [1/2], Step [3700/67476], Loss: 4.4498\n",
      "Epoch [1/2], Step [3710/67476], Loss: 4.6226\n",
      "Epoch [1/2], Step [3720/67476], Loss: 4.6556\n",
      "Epoch [1/2], Step [3730/67476], Loss: 4.6461\n",
      "Epoch [1/2], Step [3740/67476], Loss: 4.6069\n",
      "Epoch [1/2], Step [3750/67476], Loss: 4.5999\n",
      "Epoch [1/2], Step [3760/67476], Loss: 4.6203\n",
      "Epoch [1/2], Step [3770/67476], Loss: 4.6115\n",
      "Epoch [1/2], Step [3780/67476], Loss: 4.5799\n",
      "Epoch [1/2], Step [3790/67476], Loss: 4.4657\n",
      "Epoch [1/2], Step [3800/67476], Loss: 4.6865\n",
      "Epoch [1/2], Step [3810/67476], Loss: 4.5444\n",
      "Epoch [1/2], Step [3820/67476], Loss: 4.7014\n",
      "Epoch [1/2], Step [3830/67476], Loss: 4.5709\n",
      "Epoch [1/2], Step [3840/67476], Loss: 4.6369\n",
      "Epoch [1/2], Step [3850/67476], Loss: 4.7690\n",
      "Epoch [1/2], Step [3860/67476], Loss: 4.6245\n",
      "Epoch [1/2], Step [3870/67476], Loss: 4.5543\n",
      "Epoch [1/2], Step [3880/67476], Loss: 4.6872\n",
      "Epoch [1/2], Step [3890/67476], Loss: 4.7626\n",
      "Epoch [1/2], Step [3900/67476], Loss: 4.5031\n",
      "Epoch [1/2], Step [3910/67476], Loss: 4.5984\n",
      "Epoch [1/2], Step [3920/67476], Loss: 4.4209\n",
      "Epoch [1/2], Step [3930/67476], Loss: 4.5212\n",
      "Epoch [1/2], Step [3940/67476], Loss: 4.7410\n",
      "Epoch [1/2], Step [3950/67476], Loss: 4.4582\n",
      "Epoch [1/2], Step [3960/67476], Loss: 4.6040\n",
      "Epoch [1/2], Step [3970/67476], Loss: 4.6663\n",
      "Epoch [1/2], Step [3980/67476], Loss: 4.4843\n",
      "Epoch [1/2], Step [3990/67476], Loss: 4.4653\n",
      "Epoch [1/2], Step [4000/67476], Loss: 4.6225\n",
      "Epoch [1/2], Step [4010/67476], Loss: 4.5362\n",
      "Epoch [1/2], Step [4020/67476], Loss: 4.6295\n",
      "Epoch [1/2], Step [4030/67476], Loss: 4.5797\n",
      "Epoch [1/2], Step [4040/67476], Loss: 4.5706\n",
      "Epoch [1/2], Step [4050/67476], Loss: 4.5685\n",
      "Epoch [1/2], Step [4060/67476], Loss: 4.5374\n",
      "Epoch [1/2], Step [4070/67476], Loss: 4.5255\n",
      "Epoch [1/2], Step [4080/67476], Loss: 4.7941\n",
      "Epoch [1/2], Step [4090/67476], Loss: 4.4684\n",
      "Epoch [1/2], Step [4100/67476], Loss: 4.6640\n",
      "Epoch [1/2], Step [4110/67476], Loss: 4.6357\n",
      "Epoch [1/2], Step [4120/67476], Loss: 4.7054\n",
      "Epoch [1/2], Step [4130/67476], Loss: 4.5883\n",
      "Epoch [1/2], Step [4140/67476], Loss: 4.6712\n",
      "Epoch [1/2], Step [4150/67476], Loss: 4.7394\n",
      "Epoch [1/2], Step [4160/67476], Loss: 4.5808\n",
      "Epoch [1/2], Step [4170/67476], Loss: 4.6023\n",
      "Epoch [1/2], Step [4180/67476], Loss: 4.6257\n",
      "Epoch [1/2], Step [4190/67476], Loss: 4.7663\n",
      "Epoch [1/2], Step [4200/67476], Loss: 4.5929\n",
      "Epoch [1/2], Step [4210/67476], Loss: 4.5621\n",
      "Epoch [1/2], Step [4220/67476], Loss: 4.6440\n",
      "Epoch [1/2], Step [4230/67476], Loss: 4.4245\n",
      "Epoch [1/2], Step [4240/67476], Loss: 4.5729\n",
      "Epoch [1/2], Step [4250/67476], Loss: 4.8082\n",
      "Epoch [1/2], Step [4260/67476], Loss: 4.8920\n",
      "Epoch [1/2], Step [4270/67476], Loss: 4.5800\n",
      "Epoch [1/2], Step [4280/67476], Loss: 4.4357\n",
      "Epoch [1/2], Step [4290/67476], Loss: 4.5281\n",
      "Epoch [1/2], Step [4300/67476], Loss: 4.4182\n",
      "Epoch [1/2], Step [4310/67476], Loss: 4.6508\n",
      "Epoch [1/2], Step [4320/67476], Loss: 4.3405\n",
      "Epoch [1/2], Step [4330/67476], Loss: 4.5932\n",
      "Epoch [1/2], Step [4340/67476], Loss: 4.5747\n",
      "Epoch [1/2], Step [4350/67476], Loss: 4.6769\n",
      "Epoch [1/2], Step [4360/67476], Loss: 4.5240\n",
      "Epoch [1/2], Step [4370/67476], Loss: 4.6614\n",
      "Epoch [1/2], Step [4380/67476], Loss: 4.6310\n",
      "Epoch [1/2], Step [4390/67476], Loss: 4.7377\n",
      "Epoch [1/2], Step [4400/67476], Loss: 4.5193\n",
      "Epoch [1/2], Step [4410/67476], Loss: 4.5887\n",
      "Epoch [1/2], Step [4420/67476], Loss: 4.6766\n",
      "Epoch [1/2], Step [4430/67476], Loss: 4.6987\n",
      "Epoch [1/2], Step [4440/67476], Loss: 4.5579\n",
      "Epoch [1/2], Step [4450/67476], Loss: 4.5653\n",
      "Epoch [1/2], Step [4460/67476], Loss: 4.4881\n",
      "Epoch [1/2], Step [4470/67476], Loss: 4.6164\n",
      "Epoch [1/2], Step [4480/67476], Loss: 4.7391\n",
      "Epoch [1/2], Step [4490/67476], Loss: 4.4977\n",
      "Epoch [1/2], Step [4500/67476], Loss: 4.6641\n",
      "Epoch [1/2], Step [4510/67476], Loss: 4.6257\n",
      "Epoch [1/2], Step [4520/67476], Loss: 4.5538\n",
      "Epoch [1/2], Step [4530/67476], Loss: 4.6299\n",
      "Epoch [1/2], Step [4540/67476], Loss: 4.5508\n",
      "Epoch [1/2], Step [4550/67476], Loss: 4.9532\n",
      "Epoch [1/2], Step [4560/67476], Loss: 4.7850\n",
      "Epoch [1/2], Step [4570/67476], Loss: 4.5968\n",
      "Epoch [1/2], Step [4580/67476], Loss: 4.6226\n",
      "Epoch [1/2], Step [4590/67476], Loss: 4.6130\n",
      "Epoch [1/2], Step [4600/67476], Loss: 4.5537\n",
      "Epoch [1/2], Step [4610/67476], Loss: 4.5904\n",
      "Epoch [1/2], Step [4620/67476], Loss: 4.5066\n",
      "Epoch [1/2], Step [4630/67476], Loss: 4.6184\n",
      "Epoch [1/2], Step [4640/67476], Loss: 4.3786\n",
      "Epoch [1/2], Step [4650/67476], Loss: 4.8291\n",
      "Epoch [1/2], Step [4660/67476], Loss: 4.5774\n",
      "Epoch [1/2], Step [4670/67476], Loss: 4.5519\n",
      "Epoch [1/2], Step [4680/67476], Loss: 4.6104\n",
      "Epoch [1/2], Step [4690/67476], Loss: 4.7486\n",
      "Epoch [1/2], Step [4700/67476], Loss: 4.6685\n",
      "Epoch [1/2], Step [4710/67476], Loss: 4.6765\n",
      "Epoch [1/2], Step [4720/67476], Loss: 4.8363\n",
      "Epoch [1/2], Step [4730/67476], Loss: 4.6892\n",
      "Epoch [1/2], Step [4740/67476], Loss: 4.8348\n",
      "Epoch [1/2], Step [4750/67476], Loss: 4.6298\n",
      "Epoch [1/2], Step [4760/67476], Loss: 4.6596\n",
      "Epoch [1/2], Step [4770/67476], Loss: 4.5238\n",
      "Epoch [1/2], Step [4780/67476], Loss: 4.5234\n",
      "Epoch [1/2], Step [4790/67476], Loss: 4.6784\n",
      "Epoch [1/2], Step [4800/67476], Loss: 4.5051\n",
      "Epoch [1/2], Step [4810/67476], Loss: 4.6448\n",
      "Epoch [1/2], Step [4820/67476], Loss: 4.5851\n",
      "Epoch [1/2], Step [4830/67476], Loss: 4.5699\n",
      "Epoch [1/2], Step [4840/67476], Loss: 4.4667\n",
      "Epoch [1/2], Step [4850/67476], Loss: 4.4609\n",
      "Epoch [1/2], Step [4860/67476], Loss: 4.7488\n",
      "Epoch [1/2], Step [4870/67476], Loss: 4.4793\n",
      "Epoch [1/2], Step [4880/67476], Loss: 4.7022\n",
      "Epoch [1/2], Step [4890/67476], Loss: 4.7092\n",
      "Epoch [1/2], Step [4900/67476], Loss: 4.5788\n",
      "Epoch [1/2], Step [4910/67476], Loss: 4.8546\n",
      "Epoch [1/2], Step [4920/67476], Loss: 4.4299\n",
      "Epoch [1/2], Step [4930/67476], Loss: 4.7527\n",
      "Epoch [1/2], Step [4940/67476], Loss: 4.5330\n",
      "Epoch [1/2], Step [4950/67476], Loss: 4.5149\n",
      "Epoch [1/2], Step [4960/67476], Loss: 4.5049\n",
      "Epoch [1/2], Step [4970/67476], Loss: 4.4323\n",
      "Epoch [1/2], Step [4980/67476], Loss: 4.6274\n",
      "Epoch [1/2], Step [4990/67476], Loss: 4.7732\n",
      "Epoch [1/2], Step [5000/67476], Loss: 4.7870\n",
      "Epoch [1/2], Step [5010/67476], Loss: 4.5516\n",
      "Epoch [1/2], Step [5020/67476], Loss: 4.5966\n",
      "Epoch [1/2], Step [5030/67476], Loss: 4.8416\n",
      "Epoch [1/2], Step [5040/67476], Loss: 4.5563\n",
      "Epoch [1/2], Step [5050/67476], Loss: 4.5356\n",
      "Epoch [1/2], Step [5060/67476], Loss: 4.7710\n",
      "Epoch [1/2], Step [5070/67476], Loss: 4.5301\n",
      "Epoch [1/2], Step [5080/67476], Loss: 4.5615\n",
      "Epoch [1/2], Step [5090/67476], Loss: 4.4730\n",
      "Epoch [1/2], Step [5100/67476], Loss: 4.6431\n",
      "Epoch [1/2], Step [5110/67476], Loss: 4.5396\n",
      "Epoch [1/2], Step [5120/67476], Loss: 4.6217\n",
      "Epoch [1/2], Step [5130/67476], Loss: 4.4532\n",
      "Epoch [1/2], Step [5140/67476], Loss: 4.4846\n",
      "Epoch [1/2], Step [5150/67476], Loss: 4.6155\n",
      "Epoch [1/2], Step [5160/67476], Loss: 4.5664\n",
      "Epoch [1/2], Step [5170/67476], Loss: 4.5464\n",
      "Epoch [1/2], Step [5180/67476], Loss: 4.6068\n",
      "Epoch [1/2], Step [5190/67476], Loss: 4.5882\n",
      "Epoch [1/2], Step [5200/67476], Loss: 4.6169\n",
      "Epoch [1/2], Step [5210/67476], Loss: 4.4271\n",
      "Epoch [1/2], Step [5220/67476], Loss: 4.4305\n",
      "Epoch [1/2], Step [5230/67476], Loss: 4.6815\n",
      "Epoch [1/2], Step [5240/67476], Loss: 4.8311\n",
      "Epoch [1/2], Step [5250/67476], Loss: 4.6874\n",
      "Epoch [1/2], Step [5260/67476], Loss: 4.4562\n",
      "Epoch [1/2], Step [5270/67476], Loss: 4.6888\n",
      "Epoch [1/2], Step [5280/67476], Loss: 4.5051\n",
      "Epoch [1/2], Step [5290/67476], Loss: 4.5555\n",
      "Epoch [1/2], Step [5300/67476], Loss: 4.3623\n",
      "Epoch [1/2], Step [5310/67476], Loss: 4.4482\n",
      "Epoch [1/2], Step [5320/67476], Loss: 4.6256\n",
      "Epoch [1/2], Step [5330/67476], Loss: 4.5385\n",
      "Epoch [1/2], Step [5340/67476], Loss: 4.4885\n",
      "Epoch [1/2], Step [5350/67476], Loss: 4.7283\n",
      "Epoch [1/2], Step [5360/67476], Loss: 4.5431\n",
      "Epoch [1/2], Step [5370/67476], Loss: 4.5630\n",
      "Epoch [1/2], Step [5380/67476], Loss: 4.4977\n",
      "Epoch [1/2], Step [5390/67476], Loss: 4.6210\n",
      "Epoch [1/2], Step [5400/67476], Loss: 4.6737\n",
      "Epoch [1/2], Step [5410/67476], Loss: 4.4879\n",
      "Epoch [1/2], Step [5420/67476], Loss: 4.5235\n",
      "Epoch [1/2], Step [5430/67476], Loss: 4.5487\n",
      "Epoch [1/2], Step [5440/67476], Loss: 4.6241\n",
      "Epoch [1/2], Step [5450/67476], Loss: 4.6737\n",
      "Epoch [1/2], Step [5460/67476], Loss: 4.4532\n",
      "Epoch [1/2], Step [5470/67476], Loss: 4.5663\n",
      "Epoch [1/2], Step [5480/67476], Loss: 4.7453\n",
      "Epoch [1/2], Step [5490/67476], Loss: 4.7555\n",
      "Epoch [1/2], Step [5500/67476], Loss: 4.5201\n",
      "Epoch [1/2], Step [5510/67476], Loss: 4.6272\n",
      "Epoch [1/2], Step [5520/67476], Loss: 4.6223\n",
      "Epoch [1/2], Step [5530/67476], Loss: 4.8041\n",
      "Epoch [1/2], Step [5540/67476], Loss: 4.7740\n",
      "Epoch [1/2], Step [5550/67476], Loss: 4.6445\n",
      "Epoch [1/2], Step [5560/67476], Loss: 4.5186\n",
      "Epoch [1/2], Step [5570/67476], Loss: 4.5208\n",
      "Epoch [1/2], Step [5580/67476], Loss: 4.6527\n",
      "Epoch [1/2], Step [5590/67476], Loss: 4.4256\n",
      "Epoch [1/2], Step [5600/67476], Loss: 4.5715\n",
      "Epoch [1/2], Step [5610/67476], Loss: 4.5791\n",
      "Epoch [1/2], Step [5620/67476], Loss: 4.6803\n",
      "Epoch [1/2], Step [5630/67476], Loss: 4.3871\n",
      "Epoch [1/2], Step [5640/67476], Loss: 4.8011\n",
      "Epoch [1/2], Step [5650/67476], Loss: 4.5174\n",
      "Epoch [1/2], Step [5660/67476], Loss: 4.2883\n",
      "Epoch [1/2], Step [5670/67476], Loss: 4.4463\n",
      "Epoch [1/2], Step [5680/67476], Loss: 4.4605\n",
      "Epoch [1/2], Step [5690/67476], Loss: 4.8083\n",
      "Epoch [1/2], Step [5700/67476], Loss: 4.7211\n",
      "Epoch [1/2], Step [5710/67476], Loss: 4.5576\n",
      "Epoch [1/2], Step [5720/67476], Loss: 4.5183\n",
      "Epoch [1/2], Step [5730/67476], Loss: 4.6976\n",
      "Epoch [1/2], Step [5740/67476], Loss: 4.4497\n",
      "Epoch [1/2], Step [5750/67476], Loss: 4.6961\n",
      "Epoch [1/2], Step [5760/67476], Loss: 4.6706\n",
      "Epoch [1/2], Step [5770/67476], Loss: 4.4931\n",
      "Epoch [1/2], Step [5780/67476], Loss: 4.4457\n",
      "Epoch [1/2], Step [5790/67476], Loss: 4.5349\n",
      "Epoch [1/2], Step [5800/67476], Loss: 4.5761\n",
      "Epoch [1/2], Step [5810/67476], Loss: 4.6968\n",
      "Epoch [1/2], Step [5820/67476], Loss: 4.5741\n",
      "Epoch [1/2], Step [5830/67476], Loss: 4.5320\n",
      "Epoch [1/2], Step [5840/67476], Loss: 4.6958\n",
      "Epoch [1/2], Step [5850/67476], Loss: 4.5467\n",
      "Epoch [1/2], Step [5860/67476], Loss: 4.3822\n",
      "Epoch [1/2], Step [5870/67476], Loss: 4.6646\n",
      "Epoch [1/2], Step [5880/67476], Loss: 4.5792\n",
      "Epoch [1/2], Step [5890/67476], Loss: 4.6306\n",
      "Epoch [1/2], Step [5900/67476], Loss: 4.4908\n",
      "Epoch [1/2], Step [5910/67476], Loss: 4.5950\n",
      "Epoch [1/2], Step [5920/67476], Loss: 4.6497\n",
      "Epoch [1/2], Step [5930/67476], Loss: 4.3294\n",
      "Epoch [1/2], Step [5940/67476], Loss: 4.5578\n",
      "Epoch [1/2], Step [5950/67476], Loss: 4.8232\n",
      "Epoch [1/2], Step [5960/67476], Loss: 4.7444\n",
      "Epoch [1/2], Step [5970/67476], Loss: 4.5474\n",
      "Epoch [1/2], Step [5980/67476], Loss: 4.6072\n",
      "Epoch [1/2], Step [5990/67476], Loss: 4.6136\n",
      "Epoch [1/2], Step [6000/67476], Loss: 4.5974\n",
      "Epoch [1/2], Step [6010/67476], Loss: 4.5277\n",
      "Epoch [1/2], Step [6020/67476], Loss: 4.6340\n",
      "Epoch [1/2], Step [6030/67476], Loss: 4.6640\n",
      "Epoch [1/2], Step [6040/67476], Loss: 4.6107\n",
      "Epoch [1/2], Step [6050/67476], Loss: 4.6498\n",
      "Epoch [1/2], Step [6060/67476], Loss: 4.7040\n",
      "Epoch [1/2], Step [6070/67476], Loss: 4.4715\n",
      "Epoch [1/2], Step [6080/67476], Loss: 4.6365\n",
      "Epoch [1/2], Step [6090/67476], Loss: 4.5221\n",
      "Epoch [1/2], Step [6100/67476], Loss: 4.4458\n",
      "Epoch [1/2], Step [6110/67476], Loss: 4.4996\n",
      "Epoch [1/2], Step [6120/67476], Loss: 4.6464\n",
      "Epoch [1/2], Step [6130/67476], Loss: 4.6651\n",
      "Epoch [1/2], Step [6140/67476], Loss: 4.4612\n",
      "Epoch [1/2], Step [6150/67476], Loss: 4.5277\n",
      "Epoch [1/2], Step [6160/67476], Loss: 4.4774\n",
      "Epoch [1/2], Step [6170/67476], Loss: 4.6953\n",
      "Epoch [1/2], Step [6180/67476], Loss: 4.5111\n",
      "Epoch [1/2], Step [6190/67476], Loss: 4.4873\n",
      "Epoch [1/2], Step [6200/67476], Loss: 4.5619\n",
      "Epoch [1/2], Step [6210/67476], Loss: 4.4980\n",
      "Epoch [1/2], Step [6220/67476], Loss: 4.6985\n",
      "Epoch [1/2], Step [6230/67476], Loss: 4.6331\n",
      "Epoch [1/2], Step [6240/67476], Loss: 4.6562\n",
      "Epoch [1/2], Step [6250/67476], Loss: 4.5646\n",
      "Epoch [1/2], Step [6260/67476], Loss: 4.3705\n",
      "Epoch [1/2], Step [6270/67476], Loss: 4.6941\n",
      "Epoch [1/2], Step [6280/67476], Loss: 4.4974\n",
      "Epoch [1/2], Step [6290/67476], Loss: 4.6401\n",
      "Epoch [1/2], Step [6300/67476], Loss: 4.5554\n",
      "Epoch [1/2], Step [6310/67476], Loss: 4.6638\n",
      "Epoch [1/2], Step [6320/67476], Loss: 4.5024\n",
      "Epoch [1/2], Step [6330/67476], Loss: 4.6768\n",
      "Epoch [1/2], Step [6340/67476], Loss: 4.6658\n",
      "Epoch [1/2], Step [6350/67476], Loss: 4.5572\n",
      "Epoch [1/2], Step [6360/67476], Loss: 4.3926\n",
      "Epoch [1/2], Step [6370/67476], Loss: 4.6149\n",
      "Epoch [1/2], Step [6380/67476], Loss: 4.5815\n",
      "Epoch [1/2], Step [6390/67476], Loss: 4.6896\n",
      "Epoch [1/2], Step [6400/67476], Loss: 4.6905\n",
      "Epoch [1/2], Step [6410/67476], Loss: 4.6079\n",
      "Epoch [1/2], Step [6420/67476], Loss: 4.6376\n",
      "Epoch [1/2], Step [6430/67476], Loss: 4.6612\n",
      "Epoch [1/2], Step [6440/67476], Loss: 4.4884\n",
      "Epoch [1/2], Step [6450/67476], Loss: 4.6765\n",
      "Epoch [1/2], Step [6460/67476], Loss: 4.8512\n",
      "Epoch [1/2], Step [6470/67476], Loss: 4.5415\n",
      "Epoch [1/2], Step [6480/67476], Loss: 4.6990\n",
      "Epoch [1/2], Step [6490/67476], Loss: 4.6902\n",
      "Epoch [1/2], Step [6500/67476], Loss: 4.4637\n",
      "Epoch [1/2], Step [6510/67476], Loss: 4.6430\n",
      "Epoch [1/2], Step [6520/67476], Loss: 4.6127\n",
      "Epoch [1/2], Step [6530/67476], Loss: 4.6603\n",
      "Epoch [1/2], Step [6540/67476], Loss: 4.6254\n",
      "Epoch [1/2], Step [6550/67476], Loss: 4.7218\n",
      "Epoch [1/2], Step [6560/67476], Loss: 4.6051\n",
      "Epoch [1/2], Step [6570/67476], Loss: 4.6635\n",
      "Epoch [1/2], Step [6580/67476], Loss: 4.6645\n",
      "Epoch [1/2], Step [6590/67476], Loss: 4.6078\n",
      "Epoch [1/2], Step [6600/67476], Loss: 4.5925\n",
      "Epoch [1/2], Step [6610/67476], Loss: 4.7601\n",
      "Epoch [1/2], Step [6620/67476], Loss: 4.4337\n",
      "Epoch [1/2], Step [6630/67476], Loss: 4.5607\n",
      "Epoch [1/2], Step [6640/67476], Loss: 4.6875\n",
      "Epoch [1/2], Step [6650/67476], Loss: 4.6964\n",
      "Epoch [1/2], Step [6660/67476], Loss: 4.4246\n",
      "Epoch [1/2], Step [6670/67476], Loss: 4.4226\n",
      "Epoch [1/2], Step [6680/67476], Loss: 4.6228\n",
      "Epoch [1/2], Step [6690/67476], Loss: 4.4685\n",
      "Epoch [1/2], Step [6700/67476], Loss: 4.7074\n",
      "Epoch [1/2], Step [6710/67476], Loss: 4.5283\n",
      "Epoch [1/2], Step [6720/67476], Loss: 4.8415\n",
      "Epoch [1/2], Step [6730/67476], Loss: 4.5598\n",
      "Epoch [1/2], Step [6740/67476], Loss: 4.4182\n",
      "Epoch [1/2], Step [6750/67476], Loss: 4.7262\n",
      "Epoch [1/2], Step [6760/67476], Loss: 4.6726\n",
      "Epoch [1/2], Step [6770/67476], Loss: 4.6072\n",
      "Epoch [1/2], Step [6780/67476], Loss: 4.6678\n",
      "Epoch [1/2], Step [6790/67476], Loss: 4.5818\n",
      "Epoch [1/2], Step [6800/67476], Loss: 4.5481\n",
      "Epoch [1/2], Step [6810/67476], Loss: 4.7861\n",
      "Epoch [1/2], Step [6820/67476], Loss: 4.5152\n",
      "Epoch [1/2], Step [6830/67476], Loss: 4.6342\n",
      "Epoch [1/2], Step [6840/67476], Loss: 4.4756\n",
      "Epoch [1/2], Step [6850/67476], Loss: 4.5944\n",
      "Epoch [1/2], Step [6860/67476], Loss: 4.5580\n",
      "Epoch [1/2], Step [6870/67476], Loss: 4.7601\n",
      "Epoch [1/2], Step [6880/67476], Loss: 4.5176\n",
      "Epoch [1/2], Step [6890/67476], Loss: 4.6464\n",
      "Epoch [1/2], Step [6900/67476], Loss: 4.3891\n",
      "Epoch [1/2], Step [6910/67476], Loss: 4.4499\n",
      "Epoch [1/2], Step [6920/67476], Loss: 4.6855\n",
      "Epoch [1/2], Step [6930/67476], Loss: 4.6757\n",
      "Epoch [1/2], Step [6940/67476], Loss: 4.5056\n",
      "Epoch [1/2], Step [6950/67476], Loss: 4.6262\n",
      "Epoch [1/2], Step [6960/67476], Loss: 4.6765\n",
      "Epoch [1/2], Step [6970/67476], Loss: 4.6456\n",
      "Epoch [1/2], Step [6980/67476], Loss: 4.6021\n",
      "Epoch [1/2], Step [6990/67476], Loss: 4.5861\n",
      "Epoch [1/2], Step [7000/67476], Loss: 4.5512\n",
      "Epoch [1/2], Step [7010/67476], Loss: 4.5351\n",
      "Epoch [1/2], Step [7020/67476], Loss: 4.6099\n",
      "Epoch [1/2], Step [7030/67476], Loss: 4.7231\n",
      "Epoch [1/2], Step [7040/67476], Loss: 4.6953\n",
      "Epoch [1/2], Step [7050/67476], Loss: 4.7168\n",
      "Epoch [1/2], Step [7060/67476], Loss: 4.4777\n",
      "Epoch [1/2], Step [7070/67476], Loss: 4.7854\n",
      "Epoch [1/2], Step [7080/67476], Loss: 4.5334\n",
      "Epoch [1/2], Step [7090/67476], Loss: 4.4179\n",
      "Epoch [1/2], Step [7100/67476], Loss: 4.7128\n",
      "Epoch [1/2], Step [7110/67476], Loss: 4.4770\n",
      "Epoch [1/2], Step [7120/67476], Loss: 4.6166\n",
      "Epoch [1/2], Step [7130/67476], Loss: 4.4416\n",
      "Epoch [1/2], Step [7140/67476], Loss: 4.6879\n",
      "Epoch [1/2], Step [7150/67476], Loss: 4.6311\n",
      "Epoch [1/2], Step [7160/67476], Loss: 4.6927\n",
      "Epoch [1/2], Step [7170/67476], Loss: 4.5758\n",
      "Epoch [1/2], Step [7180/67476], Loss: 4.6757\n",
      "Epoch [1/2], Step [7190/67476], Loss: 4.8022\n",
      "Epoch [1/2], Step [7200/67476], Loss: 4.3765\n",
      "Epoch [1/2], Step [7210/67476], Loss: 4.6291\n",
      "Epoch [1/2], Step [7220/67476], Loss: 4.5464\n",
      "Epoch [1/2], Step [7230/67476], Loss: 4.3377\n",
      "Epoch [1/2], Step [7240/67476], Loss: 4.4609\n",
      "Epoch [1/2], Step [7250/67476], Loss: 4.6175\n",
      "Epoch [1/2], Step [7260/67476], Loss: 4.6945\n",
      "Epoch [1/2], Step [7270/67476], Loss: 4.4495\n",
      "Epoch [1/2], Step [7280/67476], Loss: 4.5777\n",
      "Epoch [1/2], Step [7290/67476], Loss: 4.6370\n",
      "Epoch [1/2], Step [7300/67476], Loss: 4.6703\n",
      "Epoch [1/2], Step [7310/67476], Loss: 4.5677\n",
      "Epoch [1/2], Step [7320/67476], Loss: 4.7112\n",
      "Epoch [1/2], Step [7330/67476], Loss: 4.6812\n",
      "Epoch [1/2], Step [7340/67476], Loss: 4.7139\n",
      "Epoch [1/2], Step [7350/67476], Loss: 4.6877\n",
      "Epoch [1/2], Step [7360/67476], Loss: 4.8001\n",
      "Epoch [1/2], Step [7370/67476], Loss: 4.4439\n",
      "Epoch [1/2], Step [7380/67476], Loss: 4.6968\n",
      "Epoch [1/2], Step [7390/67476], Loss: 4.4526\n",
      "Epoch [1/2], Step [7400/67476], Loss: 4.3576\n",
      "Epoch [1/2], Step [7410/67476], Loss: 4.7448\n",
      "Epoch [1/2], Step [7420/67476], Loss: 4.4766\n",
      "Epoch [1/2], Step [7430/67476], Loss: 4.7258\n",
      "Epoch [1/2], Step [7440/67476], Loss: 4.6168\n",
      "Epoch [1/2], Step [7450/67476], Loss: 4.5968\n",
      "Epoch [1/2], Step [7460/67476], Loss: 4.8606\n",
      "Epoch [1/2], Step [7470/67476], Loss: 4.4224\n",
      "Epoch [1/2], Step [7480/67476], Loss: 4.5182\n",
      "Epoch [1/2], Step [7490/67476], Loss: 4.5995\n",
      "Epoch [1/2], Step [7500/67476], Loss: 4.7347\n",
      "Epoch [1/2], Step [7510/67476], Loss: 4.6644\n",
      "Epoch [1/2], Step [7520/67476], Loss: 4.5276\n",
      "Epoch [1/2], Step [7530/67476], Loss: 4.5485\n",
      "Epoch [1/2], Step [7540/67476], Loss: 4.5732\n",
      "Epoch [1/2], Step [7550/67476], Loss: 4.6069\n",
      "Epoch [1/2], Step [7560/67476], Loss: 4.7372\n",
      "Epoch [1/2], Step [7570/67476], Loss: 4.7250\n",
      "Epoch [1/2], Step [7580/67476], Loss: 4.4946\n",
      "Epoch [1/2], Step [7590/67476], Loss: 4.6105\n",
      "Epoch [1/2], Step [7600/67476], Loss: 4.5469\n",
      "Epoch [1/2], Step [7610/67476], Loss: 4.7840\n",
      "Epoch [1/2], Step [7620/67476], Loss: 4.6368\n",
      "Epoch [1/2], Step [7630/67476], Loss: 4.6227\n",
      "Epoch [1/2], Step [7640/67476], Loss: 4.6395\n",
      "Epoch [1/2], Step [7650/67476], Loss: 4.5301\n",
      "Epoch [1/2], Step [7660/67476], Loss: 4.7781\n",
      "Epoch [1/2], Step [7670/67476], Loss: 4.5653\n",
      "Epoch [1/2], Step [7680/67476], Loss: 4.6039\n",
      "Epoch [1/2], Step [7690/67476], Loss: 4.4918\n",
      "Epoch [1/2], Step [7700/67476], Loss: 4.4521\n",
      "Epoch [1/2], Step [7710/67476], Loss: 4.5658\n",
      "Epoch [1/2], Step [7720/67476], Loss: 4.7110\n",
      "Epoch [1/2], Step [7730/67476], Loss: 4.6374\n",
      "Epoch [1/2], Step [7740/67476], Loss: 4.7299\n",
      "Epoch [1/2], Step [7750/67476], Loss: 4.6335\n",
      "Epoch [1/2], Step [7760/67476], Loss: 4.4125\n",
      "Epoch [1/2], Step [7770/67476], Loss: 4.7399\n",
      "Epoch [1/2], Step [7780/67476], Loss: 4.5064\n",
      "Epoch [1/2], Step [7790/67476], Loss: 4.6307\n",
      "Epoch [1/2], Step [7800/67476], Loss: 4.5781\n",
      "Epoch [1/2], Step [7810/67476], Loss: 4.5520\n",
      "Epoch [1/2], Step [7820/67476], Loss: 4.4275\n",
      "Epoch [1/2], Step [7830/67476], Loss: 4.4134\n",
      "Epoch [1/2], Step [7840/67476], Loss: 4.5012\n",
      "Epoch [1/2], Step [7850/67476], Loss: 4.4994\n",
      "Epoch [1/2], Step [7860/67476], Loss: 4.4366\n",
      "Epoch [1/2], Step [7870/67476], Loss: 4.6205\n",
      "Epoch [1/2], Step [7880/67476], Loss: 4.5151\n",
      "Epoch [1/2], Step [7890/67476], Loss: 4.6780\n",
      "Epoch [1/2], Step [7900/67476], Loss: 4.6845\n",
      "Epoch [1/2], Step [7910/67476], Loss: 4.5647\n",
      "Epoch [1/2], Step [7920/67476], Loss: 4.6700\n",
      "Epoch [1/2], Step [7930/67476], Loss: 4.5218\n",
      "Epoch [1/2], Step [7940/67476], Loss: 4.5688\n",
      "Epoch [1/2], Step [7950/67476], Loss: 4.3900\n",
      "Epoch [1/2], Step [7960/67476], Loss: 4.4975\n",
      "Epoch [1/2], Step [7970/67476], Loss: 4.6526\n",
      "Epoch [1/2], Step [7980/67476], Loss: 4.6714\n",
      "Epoch [1/2], Step [7990/67476], Loss: 4.8711\n",
      "Epoch [1/2], Step [8000/67476], Loss: 4.2644\n",
      "Epoch [1/2], Step [8010/67476], Loss: 4.3996\n",
      "Epoch [1/2], Step [8020/67476], Loss: 4.5448\n",
      "Epoch [1/2], Step [8030/67476], Loss: 4.7171\n",
      "Epoch [1/2], Step [8040/67476], Loss: 4.5491\n",
      "Epoch [1/2], Step [8050/67476], Loss: 4.5795\n",
      "Epoch [1/2], Step [8060/67476], Loss: 4.5656\n",
      "Epoch [1/2], Step [8070/67476], Loss: 4.4205\n",
      "Epoch [1/2], Step [8080/67476], Loss: 4.6375\n",
      "Epoch [1/2], Step [8090/67476], Loss: 4.5391\n",
      "Epoch [1/2], Step [8100/67476], Loss: 4.5597\n",
      "Epoch [1/2], Step [8110/67476], Loss: 4.5105\n",
      "Epoch [1/2], Step [8120/67476], Loss: 4.4702\n",
      "Epoch [1/2], Step [8130/67476], Loss: 4.5107\n",
      "Epoch [1/2], Step [8140/67476], Loss: 4.5785\n",
      "Epoch [1/2], Step [8150/67476], Loss: 4.3687\n",
      "Epoch [1/2], Step [8160/67476], Loss: 4.6632\n",
      "Epoch [1/2], Step [8170/67476], Loss: 4.4467\n",
      "Epoch [1/2], Step [8180/67476], Loss: 4.5969\n",
      "Epoch [1/2], Step [8190/67476], Loss: 4.6162\n",
      "Epoch [1/2], Step [8200/67476], Loss: 4.4184\n",
      "Epoch [1/2], Step [8210/67476], Loss: 4.5187\n",
      "Epoch [1/2], Step [8220/67476], Loss: 4.5866\n",
      "Epoch [1/2], Step [8230/67476], Loss: 4.4485\n",
      "Epoch [1/2], Step [8240/67476], Loss: 4.5232\n",
      "Epoch [1/2], Step [8250/67476], Loss: 4.8292\n",
      "Epoch [1/2], Step [8260/67476], Loss: 4.5380\n",
      "Epoch [1/2], Step [8270/67476], Loss: 4.5042\n",
      "Epoch [1/2], Step [8280/67476], Loss: 4.5648\n",
      "Epoch [1/2], Step [8290/67476], Loss: 4.7142\n",
      "Epoch [1/2], Step [8300/67476], Loss: 4.3841\n",
      "Epoch [1/2], Step [8310/67476], Loss: 4.5957\n",
      "Epoch [1/2], Step [8320/67476], Loss: 4.6264\n",
      "Epoch [1/2], Step [8330/67476], Loss: 4.3075\n",
      "Epoch [1/2], Step [8340/67476], Loss: 4.6060\n",
      "Epoch [1/2], Step [8350/67476], Loss: 4.6896\n",
      "Epoch [1/2], Step [8360/67476], Loss: 4.4825\n",
      "Epoch [1/2], Step [8370/67476], Loss: 4.5538\n",
      "Epoch [1/2], Step [8380/67476], Loss: 4.7614\n",
      "Epoch [1/2], Step [8390/67476], Loss: 4.4762\n",
      "Epoch [1/2], Step [8400/67476], Loss: 4.7791\n",
      "Epoch [1/2], Step [8410/67476], Loss: 4.5152\n",
      "Epoch [1/2], Step [8420/67476], Loss: 4.7042\n",
      "Epoch [1/2], Step [8430/67476], Loss: 4.5482\n",
      "Epoch [1/2], Step [8440/67476], Loss: 4.7243\n",
      "Epoch [1/2], Step [8450/67476], Loss: 4.7753\n",
      "Epoch [1/2], Step [8460/67476], Loss: 4.5025\n",
      "Epoch [1/2], Step [8470/67476], Loss: 4.6480\n",
      "Epoch [1/2], Step [8480/67476], Loss: 4.6575\n",
      "Epoch [1/2], Step [8490/67476], Loss: 4.6858\n",
      "Epoch [1/2], Step [8500/67476], Loss: 4.5802\n",
      "Epoch [1/2], Step [8510/67476], Loss: 4.5320\n",
      "Epoch [1/2], Step [8520/67476], Loss: 4.7119\n",
      "Epoch [1/2], Step [8530/67476], Loss: 4.5599\n",
      "Epoch [1/2], Step [8540/67476], Loss: 4.4108\n",
      "Epoch [1/2], Step [8550/67476], Loss: 4.4687\n",
      "Epoch [1/2], Step [8560/67476], Loss: 4.4627\n",
      "Epoch [1/2], Step [8570/67476], Loss: 4.5915\n",
      "Epoch [1/2], Step [8580/67476], Loss: 4.5824\n",
      "Epoch [1/2], Step [8590/67476], Loss: 4.6952\n",
      "Epoch [1/2], Step [8600/67476], Loss: 4.6248\n",
      "Epoch [1/2], Step [8610/67476], Loss: 4.7325\n",
      "Epoch [1/2], Step [8620/67476], Loss: 4.6822\n",
      "Epoch [1/2], Step [8630/67476], Loss: 4.6314\n",
      "Epoch [1/2], Step [8640/67476], Loss: 4.4259\n",
      "Epoch [1/2], Step [8650/67476], Loss: 4.8488\n",
      "Epoch [1/2], Step [8660/67476], Loss: 4.6553\n",
      "Epoch [1/2], Step [8670/67476], Loss: 4.5277\n",
      "Epoch [1/2], Step [8680/67476], Loss: 4.7223\n",
      "Epoch [1/2], Step [8690/67476], Loss: 4.4470\n",
      "Epoch [1/2], Step [8700/67476], Loss: 4.6337\n",
      "Epoch [1/2], Step [8710/67476], Loss: 4.6488\n",
      "Epoch [1/2], Step [8720/67476], Loss: 4.6273\n",
      "Epoch [1/2], Step [8730/67476], Loss: 4.4615\n",
      "Epoch [1/2], Step [8740/67476], Loss: 4.5983\n",
      "Epoch [1/2], Step [8750/67476], Loss: 4.6717\n",
      "Epoch [1/2], Step [8760/67476], Loss: 4.5939\n",
      "Epoch [1/2], Step [8770/67476], Loss: 4.6289\n",
      "Epoch [1/2], Step [8780/67476], Loss: 4.5229\n",
      "Epoch [1/2], Step [8790/67476], Loss: 4.6803\n",
      "Epoch [1/2], Step [8800/67476], Loss: 4.7391\n",
      "Epoch [1/2], Step [8810/67476], Loss: 4.3285\n",
      "Epoch [1/2], Step [8820/67476], Loss: 4.7550\n",
      "Epoch [1/2], Step [8830/67476], Loss: 4.7938\n",
      "Epoch [1/2], Step [8840/67476], Loss: 4.7016\n",
      "Epoch [1/2], Step [8850/67476], Loss: 4.4385\n",
      "Epoch [1/2], Step [8860/67476], Loss: 4.3799\n",
      "Epoch [1/2], Step [8870/67476], Loss: 4.8165\n",
      "Epoch [1/2], Step [8880/67476], Loss: 4.5191\n",
      "Epoch [1/2], Step [8890/67476], Loss: 4.7908\n",
      "Epoch [1/2], Step [8900/67476], Loss: 4.4158\n",
      "Epoch [1/2], Step [8910/67476], Loss: 4.4981\n",
      "Epoch [1/2], Step [8920/67476], Loss: 4.5243\n",
      "Epoch [1/2], Step [8930/67476], Loss: 4.4254\n",
      "Epoch [1/2], Step [8940/67476], Loss: 4.5075\n",
      "Epoch [1/2], Step [8950/67476], Loss: 4.5048\n",
      "Epoch [1/2], Step [8960/67476], Loss: 4.5553\n",
      "Epoch [1/2], Step [8970/67476], Loss: 4.6476\n",
      "Epoch [1/2], Step [8980/67476], Loss: 4.6457\n",
      "Epoch [1/2], Step [8990/67476], Loss: 4.6766\n",
      "Epoch [1/2], Step [9000/67476], Loss: 4.7077\n",
      "Epoch [1/2], Step [9010/67476], Loss: 4.5569\n",
      "Epoch [1/2], Step [9020/67476], Loss: 4.6284\n",
      "Epoch [1/2], Step [9030/67476], Loss: 4.5177\n",
      "Epoch [1/2], Step [9040/67476], Loss: 4.6083\n",
      "Epoch [1/2], Step [9050/67476], Loss: 4.6704\n",
      "Epoch [1/2], Step [9060/67476], Loss: 4.5735\n",
      "Epoch [1/2], Step [9070/67476], Loss: 4.6955\n",
      "Epoch [1/2], Step [9080/67476], Loss: 4.5530\n",
      "Epoch [1/2], Step [9090/67476], Loss: 4.6968\n",
      "Epoch [1/2], Step [9100/67476], Loss: 4.5809\n",
      "Epoch [1/2], Step [9110/67476], Loss: 4.5536\n",
      "Epoch [1/2], Step [9120/67476], Loss: 4.7139\n",
      "Epoch [1/2], Step [9130/67476], Loss: 4.6466\n",
      "Epoch [1/2], Step [9140/67476], Loss: 4.7641\n",
      "Epoch [1/2], Step [9150/67476], Loss: 4.3899\n",
      "Epoch [1/2], Step [9160/67476], Loss: 4.5354\n",
      "Epoch [1/2], Step [9170/67476], Loss: 4.5374\n",
      "Epoch [1/2], Step [9180/67476], Loss: 4.5941\n",
      "Epoch [1/2], Step [9190/67476], Loss: 4.6956\n",
      "Epoch [1/2], Step [9200/67476], Loss: 4.4722\n",
      "Epoch [1/2], Step [9210/67476], Loss: 4.5357\n",
      "Epoch [1/2], Step [9220/67476], Loss: 4.6628\n",
      "Epoch [1/2], Step [9230/67476], Loss: 4.7068\n",
      "Epoch [1/2], Step [9240/67476], Loss: 4.6121\n",
      "Epoch [1/2], Step [9250/67476], Loss: 4.6773\n",
      "Epoch [1/2], Step [9260/67476], Loss: 4.5550\n",
      "Epoch [1/2], Step [9270/67476], Loss: 4.5878\n",
      "Epoch [1/2], Step [9280/67476], Loss: 4.7834\n",
      "Epoch [1/2], Step [9290/67476], Loss: 4.8371\n",
      "Epoch [1/2], Step [9300/67476], Loss: 4.6867\n",
      "Epoch [1/2], Step [9310/67476], Loss: 4.6676\n",
      "Epoch [1/2], Step [9320/67476], Loss: 4.5966\n",
      "Epoch [1/2], Step [9330/67476], Loss: 4.5296\n",
      "Epoch [1/2], Step [9340/67476], Loss: 4.6305\n",
      "Epoch [1/2], Step [9350/67476], Loss: 4.4218\n",
      "Epoch [1/2], Step [9360/67476], Loss: 4.4142\n",
      "Epoch [1/2], Step [9370/67476], Loss: 4.6946\n",
      "Epoch [1/2], Step [9380/67476], Loss: 4.6741\n",
      "Epoch [1/2], Step [9390/67476], Loss: 4.4481\n",
      "Epoch [1/2], Step [9400/67476], Loss: 4.5002\n",
      "Epoch [1/2], Step [9410/67476], Loss: 4.6861\n",
      "Epoch [1/2], Step [9420/67476], Loss: 4.5233\n",
      "Epoch [1/2], Step [9430/67476], Loss: 4.5323\n",
      "Epoch [1/2], Step [9440/67476], Loss: 4.6346\n",
      "Epoch [1/2], Step [9450/67476], Loss: 4.8318\n",
      "Epoch [1/2], Step [9460/67476], Loss: 4.6166\n",
      "Epoch [1/2], Step [9470/67476], Loss: 4.7513\n",
      "Epoch [1/2], Step [9480/67476], Loss: 4.6346\n",
      "Epoch [1/2], Step [9490/67476], Loss: 4.6835\n",
      "Epoch [1/2], Step [9500/67476], Loss: 4.7006\n",
      "Epoch [1/2], Step [9510/67476], Loss: 4.6322\n",
      "Epoch [1/2], Step [9520/67476], Loss: 4.4045\n",
      "Epoch [1/2], Step [9530/67476], Loss: 4.7823\n",
      "Epoch [1/2], Step [9540/67476], Loss: 4.4913\n",
      "Epoch [1/2], Step [9550/67476], Loss: 4.5486\n",
      "Epoch [1/2], Step [9560/67476], Loss: 4.7229\n",
      "Epoch [1/2], Step [9570/67476], Loss: 4.6342\n",
      "Epoch [1/2], Step [9580/67476], Loss: 4.8264\n",
      "Epoch [1/2], Step [9590/67476], Loss: 4.6315\n",
      "Epoch [1/2], Step [9600/67476], Loss: 4.7259\n",
      "Epoch [1/2], Step [9610/67476], Loss: 4.6351\n",
      "Epoch [1/2], Step [9620/67476], Loss: 4.4246\n",
      "Epoch [1/2], Step [9630/67476], Loss: 4.5540\n",
      "Epoch [1/2], Step [9640/67476], Loss: 4.4398\n",
      "Epoch [1/2], Step [9650/67476], Loss: 4.6630\n",
      "Epoch [1/2], Step [9660/67476], Loss: 4.4737\n",
      "Epoch [1/2], Step [9670/67476], Loss: 4.6124\n",
      "Epoch [1/2], Step [9680/67476], Loss: 4.6005\n",
      "Epoch [1/2], Step [9690/67476], Loss: 4.6243\n",
      "Epoch [1/2], Step [9700/67476], Loss: 4.5263\n",
      "Epoch [1/2], Step [9710/67476], Loss: 4.6945\n",
      "Epoch [1/2], Step [9720/67476], Loss: 4.5092\n",
      "Epoch [1/2], Step [9730/67476], Loss: 4.6074\n",
      "Epoch [1/2], Step [9740/67476], Loss: 4.4771\n",
      "Epoch [1/2], Step [9750/67476], Loss: 4.5278\n",
      "Epoch [1/2], Step [9760/67476], Loss: 4.6744\n",
      "Epoch [1/2], Step [9770/67476], Loss: 5.0015\n",
      "Epoch [1/2], Step [9780/67476], Loss: 4.7806\n",
      "Epoch [1/2], Step [9790/67476], Loss: 4.6256\n",
      "Epoch [1/2], Step [9800/67476], Loss: 4.5098\n",
      "Epoch [1/2], Step [9810/67476], Loss: 4.5023\n",
      "Epoch [1/2], Step [9820/67476], Loss: 4.7341\n",
      "Epoch [1/2], Step [9830/67476], Loss: 4.5264\n",
      "Epoch [1/2], Step [9840/67476], Loss: 4.4289\n",
      "Epoch [1/2], Step [9850/67476], Loss: 4.4408\n",
      "Epoch [1/2], Step [9860/67476], Loss: 4.5849\n",
      "Epoch [1/2], Step [9870/67476], Loss: 4.4974\n",
      "Epoch [1/2], Step [9880/67476], Loss: 4.5215\n",
      "Epoch [1/2], Step [9890/67476], Loss: 4.8948\n",
      "Epoch [1/2], Step [9900/67476], Loss: 4.3976\n",
      "Epoch [1/2], Step [9910/67476], Loss: 4.4361\n",
      "Epoch [1/2], Step [9920/67476], Loss: 4.6427\n",
      "Epoch [1/2], Step [9930/67476], Loss: 4.5035\n",
      "Epoch [1/2], Step [9940/67476], Loss: 4.8252\n",
      "Epoch [1/2], Step [9950/67476], Loss: 4.6410\n",
      "Epoch [1/2], Step [9960/67476], Loss: 4.5939\n",
      "Epoch [1/2], Step [9970/67476], Loss: 4.8245\n",
      "Epoch [1/2], Step [9980/67476], Loss: 4.5272\n",
      "Epoch [1/2], Step [9990/67476], Loss: 4.5938\n",
      "Epoch [1/2], Step [10000/67476], Loss: 4.5607\n",
      "Epoch [1/2], Step [10010/67476], Loss: 4.6815\n",
      "Epoch [1/2], Step [10020/67476], Loss: 4.5874\n",
      "Epoch [1/2], Step [10030/67476], Loss: 4.7377\n",
      "Epoch [1/2], Step [10040/67476], Loss: 4.9593\n",
      "Epoch [1/2], Step [10050/67476], Loss: 4.5629\n",
      "Epoch [1/2], Step [10060/67476], Loss: 4.5551\n",
      "Epoch [1/2], Step [10070/67476], Loss: 4.8805\n",
      "Epoch [1/2], Step [10080/67476], Loss: 4.7164\n",
      "Epoch [1/2], Step [10090/67476], Loss: 4.7264\n",
      "Epoch [1/2], Step [10100/67476], Loss: 4.5274\n",
      "Epoch [1/2], Step [10110/67476], Loss: 4.6545\n",
      "Epoch [1/2], Step [10120/67476], Loss: 4.4267\n",
      "Epoch [1/2], Step [10130/67476], Loss: 4.6414\n",
      "Epoch [1/2], Step [10140/67476], Loss: 4.6266\n",
      "Epoch [1/2], Step [10150/67476], Loss: 4.5310\n",
      "Epoch [1/2], Step [10160/67476], Loss: 4.4830\n",
      "Epoch [1/2], Step [10170/67476], Loss: 4.5459\n",
      "Epoch [1/2], Step [10180/67476], Loss: 4.7912\n",
      "Epoch [1/2], Step [10190/67476], Loss: 4.7834\n",
      "Epoch [1/2], Step [10200/67476], Loss: 4.7354\n",
      "Epoch [1/2], Step [10210/67476], Loss: 4.5971\n",
      "Epoch [1/2], Step [10220/67476], Loss: 4.4691\n",
      "Epoch [1/2], Step [10230/67476], Loss: 4.5829\n",
      "Epoch [1/2], Step [10240/67476], Loss: 4.8405\n",
      "Epoch [1/2], Step [10250/67476], Loss: 4.5952\n",
      "Epoch [1/2], Step [10260/67476], Loss: 4.6885\n",
      "Epoch [1/2], Step [10270/67476], Loss: 4.5723\n",
      "Epoch [1/2], Step [10280/67476], Loss: 4.6549\n",
      "Epoch [1/2], Step [10290/67476], Loss: 4.5500\n",
      "Epoch [1/2], Step [10300/67476], Loss: 4.4860\n",
      "Epoch [1/2], Step [10310/67476], Loss: 4.6193\n",
      "Epoch [1/2], Step [10320/67476], Loss: 4.4892\n",
      "Epoch [1/2], Step [10330/67476], Loss: 4.6366\n",
      "Epoch [1/2], Step [10340/67476], Loss: 4.6934\n",
      "Epoch [1/2], Step [10350/67476], Loss: 4.9045\n",
      "Epoch [1/2], Step [10360/67476], Loss: 4.5426\n",
      "Epoch [1/2], Step [10370/67476], Loss: 4.6720\n",
      "Epoch [1/2], Step [10380/67476], Loss: 4.5440\n",
      "Epoch [1/2], Step [10390/67476], Loss: 4.5434\n",
      "Epoch [1/2], Step [10400/67476], Loss: 4.6368\n",
      "Epoch [1/2], Step [10410/67476], Loss: 4.5837\n",
      "Epoch [1/2], Step [10420/67476], Loss: 4.5242\n",
      "Epoch [1/2], Step [10430/67476], Loss: 4.5977\n",
      "Epoch [1/2], Step [10440/67476], Loss: 4.6616\n",
      "Epoch [1/2], Step [10450/67476], Loss: 4.6141\n",
      "Epoch [1/2], Step [10460/67476], Loss: 4.6959\n",
      "Epoch [1/2], Step [10470/67476], Loss: 4.5875\n",
      "Epoch [1/2], Step [10480/67476], Loss: 4.6995\n",
      "Epoch [1/2], Step [10490/67476], Loss: 4.5080\n",
      "Epoch [1/2], Step [10500/67476], Loss: 4.4786\n",
      "Epoch [1/2], Step [10510/67476], Loss: 4.4403\n",
      "Epoch [1/2], Step [10520/67476], Loss: 4.4849\n",
      "Epoch [1/2], Step [10530/67476], Loss: 4.7742\n",
      "Epoch [1/2], Step [10540/67476], Loss: 4.4881\n",
      "Epoch [1/2], Step [10550/67476], Loss: 4.4917\n",
      "Epoch [1/2], Step [10560/67476], Loss: 4.7173\n",
      "Epoch [1/2], Step [10570/67476], Loss: 4.4844\n",
      "Epoch [1/2], Step [10580/67476], Loss: 4.7506\n",
      "Epoch [1/2], Step [10590/67476], Loss: 4.5824\n",
      "Epoch [1/2], Step [10600/67476], Loss: 4.7875\n",
      "Epoch [1/2], Step [10610/67476], Loss: 4.5627\n",
      "Epoch [1/2], Step [10620/67476], Loss: 4.6913\n",
      "Epoch [1/2], Step [10630/67476], Loss: 4.7934\n",
      "Epoch [1/2], Step [10640/67476], Loss: 4.5223\n",
      "Epoch [1/2], Step [10650/67476], Loss: 4.4941\n",
      "Epoch [1/2], Step [10660/67476], Loss: 4.3893\n",
      "Epoch [1/2], Step [10670/67476], Loss: 4.6331\n",
      "Epoch [1/2], Step [10680/67476], Loss: 4.6409\n",
      "Epoch [1/2], Step [10690/67476], Loss: 4.4248\n",
      "Epoch [1/2], Step [10700/67476], Loss: 4.5403\n",
      "Epoch [1/2], Step [10710/67476], Loss: 4.7673\n",
      "Epoch [1/2], Step [10720/67476], Loss: 4.5751\n",
      "Epoch [1/2], Step [10730/67476], Loss: 4.6903\n",
      "Epoch [1/2], Step [10740/67476], Loss: 4.3671\n",
      "Epoch [1/2], Step [10750/67476], Loss: 4.5429\n",
      "Epoch [1/2], Step [10760/67476], Loss: 4.6274\n",
      "Epoch [1/2], Step [10770/67476], Loss: 4.6627\n",
      "Epoch [1/2], Step [10780/67476], Loss: 4.3898\n",
      "Epoch [1/2], Step [10790/67476], Loss: 4.6264\n",
      "Epoch [1/2], Step [10800/67476], Loss: 4.6102\n",
      "Epoch [1/2], Step [10810/67476], Loss: 4.7039\n",
      "Epoch [1/2], Step [10820/67476], Loss: 4.6952\n",
      "Epoch [1/2], Step [10830/67476], Loss: 4.5243\n",
      "Epoch [1/2], Step [10840/67476], Loss: 4.7023\n",
      "Epoch [1/2], Step [10850/67476], Loss: 4.6660\n",
      "Epoch [1/2], Step [10860/67476], Loss: 4.6931\n",
      "Epoch [1/2], Step [10870/67476], Loss: 4.6414\n",
      "Epoch [1/2], Step [10880/67476], Loss: 4.6011\n",
      "Epoch [1/2], Step [10890/67476], Loss: 4.6113\n",
      "Epoch [1/2], Step [10900/67476], Loss: 4.5815\n",
      "Epoch [1/2], Step [10910/67476], Loss: 4.8779\n",
      "Epoch [1/2], Step [10920/67476], Loss: 4.6524\n",
      "Epoch [1/2], Step [10930/67476], Loss: 4.5156\n",
      "Epoch [1/2], Step [10940/67476], Loss: 4.3968\n",
      "Epoch [1/2], Step [10950/67476], Loss: 4.7491\n",
      "Epoch [1/2], Step [10960/67476], Loss: 4.7938\n",
      "Epoch [1/2], Step [10970/67476], Loss: 4.4805\n",
      "Epoch [1/2], Step [10980/67476], Loss: 4.3787\n",
      "Epoch [1/2], Step [10990/67476], Loss: 4.7032\n",
      "Epoch [1/2], Step [11000/67476], Loss: 4.6721\n",
      "Epoch [1/2], Step [11010/67476], Loss: 4.5018\n",
      "Epoch [1/2], Step [11020/67476], Loss: 4.6125\n",
      "Epoch [1/2], Step [11030/67476], Loss: 4.4413\n",
      "Epoch [1/2], Step [11040/67476], Loss: 4.6718\n",
      "Epoch [1/2], Step [11050/67476], Loss: 4.5655\n",
      "Epoch [1/2], Step [11060/67476], Loss: 4.7309\n",
      "Epoch [1/2], Step [11070/67476], Loss: 4.5210\n",
      "Epoch [1/2], Step [11080/67476], Loss: 4.6559\n",
      "Epoch [1/2], Step [11090/67476], Loss: 4.5601\n",
      "Epoch [1/2], Step [11100/67476], Loss: 4.6119\n",
      "Epoch [1/2], Step [11110/67476], Loss: 4.5171\n",
      "Epoch [1/2], Step [11120/67476], Loss: 4.5265\n",
      "Epoch [1/2], Step [11130/67476], Loss: 4.5787\n",
      "Epoch [1/2], Step [11140/67476], Loss: 4.6255\n",
      "Epoch [1/2], Step [11150/67476], Loss: 4.5608\n",
      "Epoch [1/2], Step [11160/67476], Loss: 4.6188\n",
      "Epoch [1/2], Step [11170/67476], Loss: 4.5400\n",
      "Epoch [1/2], Step [11180/67476], Loss: 4.6443\n",
      "Epoch [1/2], Step [11190/67476], Loss: 4.4875\n",
      "Epoch [1/2], Step [11200/67476], Loss: 4.6692\n",
      "Epoch [1/2], Step [11210/67476], Loss: 4.4396\n",
      "Epoch [1/2], Step [11220/67476], Loss: 4.6078\n",
      "Epoch [1/2], Step [11230/67476], Loss: 4.6002\n",
      "Epoch [1/2], Step [11240/67476], Loss: 4.6553\n",
      "Epoch [1/2], Step [11250/67476], Loss: 4.3916\n",
      "Epoch [1/2], Step [11260/67476], Loss: 4.6285\n",
      "Epoch [1/2], Step [11270/67476], Loss: 4.5247\n",
      "Epoch [1/2], Step [11280/67476], Loss: 4.6206\n",
      "Epoch [1/2], Step [11290/67476], Loss: 4.6340\n",
      "Epoch [1/2], Step [11300/67476], Loss: 4.5732\n",
      "Epoch [1/2], Step [11310/67476], Loss: 4.5357\n",
      "Epoch [1/2], Step [11320/67476], Loss: 4.6357\n",
      "Epoch [1/2], Step [11330/67476], Loss: 4.6737\n",
      "Epoch [1/2], Step [11340/67476], Loss: 4.5255\n",
      "Epoch [1/2], Step [11350/67476], Loss: 4.4776\n",
      "Epoch [1/2], Step [11360/67476], Loss: 4.6663\n",
      "Epoch [1/2], Step [11370/67476], Loss: 4.5795\n",
      "Epoch [1/2], Step [11380/67476], Loss: 4.7508\n",
      "Epoch [1/2], Step [11390/67476], Loss: 4.6594\n",
      "Epoch [1/2], Step [11400/67476], Loss: 4.4058\n",
      "Epoch [1/2], Step [11410/67476], Loss: 4.5465\n",
      "Epoch [1/2], Step [11420/67476], Loss: 4.4388\n",
      "Epoch [1/2], Step [11430/67476], Loss: 4.5509\n",
      "Epoch [1/2], Step [11440/67476], Loss: 4.5719\n",
      "Epoch [1/2], Step [11450/67476], Loss: 4.6457\n",
      "Epoch [1/2], Step [11460/67476], Loss: 4.8238\n",
      "Epoch [1/2], Step [11470/67476], Loss: 4.5782\n",
      "Epoch [1/2], Step [11480/67476], Loss: 4.3893\n",
      "Epoch [1/2], Step [11490/67476], Loss: 4.3871\n",
      "Epoch [1/2], Step [11500/67476], Loss: 4.5860\n",
      "Epoch [1/2], Step [11510/67476], Loss: 4.8708\n",
      "Epoch [1/2], Step [11520/67476], Loss: 4.6390\n",
      "Epoch [1/2], Step [11530/67476], Loss: 4.4698\n",
      "Epoch [1/2], Step [11540/67476], Loss: 4.4521\n",
      "Epoch [1/2], Step [11550/67476], Loss: 4.6009\n",
      "Epoch [1/2], Step [11560/67476], Loss: 4.5570\n",
      "Epoch [1/2], Step [11570/67476], Loss: 4.6083\n",
      "Epoch [1/2], Step [11580/67476], Loss: 4.5450\n",
      "Epoch [1/2], Step [11590/67476], Loss: 4.5066\n",
      "Epoch [1/2], Step [11600/67476], Loss: 4.4514\n",
      "Epoch [1/2], Step [11610/67476], Loss: 4.4841\n",
      "Epoch [1/2], Step [11620/67476], Loss: 4.6676\n",
      "Epoch [1/2], Step [11630/67476], Loss: 4.4440\n",
      "Epoch [1/2], Step [11640/67476], Loss: 4.6253\n",
      "Epoch [1/2], Step [11650/67476], Loss: 4.4981\n",
      "Epoch [1/2], Step [11660/67476], Loss: 4.4681\n",
      "Epoch [1/2], Step [11670/67476], Loss: 4.4246\n",
      "Epoch [1/2], Step [11680/67476], Loss: 4.7933\n",
      "Epoch [1/2], Step [11690/67476], Loss: 4.7500\n",
      "Epoch [1/2], Step [11700/67476], Loss: 4.5559\n",
      "Epoch [1/2], Step [11710/67476], Loss: 4.6611\n",
      "Epoch [1/2], Step [11720/67476], Loss: 4.6149\n",
      "Epoch [1/2], Step [11730/67476], Loss: 4.4612\n",
      "Epoch [1/2], Step [11740/67476], Loss: 4.6727\n",
      "Epoch [1/2], Step [11750/67476], Loss: 4.6995\n",
      "Epoch [1/2], Step [11760/67476], Loss: 4.4570\n",
      "Epoch [1/2], Step [11770/67476], Loss: 4.3580\n",
      "Epoch [1/2], Step [11780/67476], Loss: 4.5578\n",
      "Epoch [1/2], Step [11790/67476], Loss: 4.5132\n",
      "Epoch [1/2], Step [11800/67476], Loss: 4.5084\n",
      "Epoch [1/2], Step [11810/67476], Loss: 4.6199\n",
      "Epoch [1/2], Step [11820/67476], Loss: 4.5606\n",
      "Epoch [1/2], Step [11830/67476], Loss: 4.4914\n",
      "Epoch [1/2], Step [11840/67476], Loss: 4.6274\n",
      "Epoch [1/2], Step [11850/67476], Loss: 4.4694\n",
      "Epoch [1/2], Step [11860/67476], Loss: 4.7349\n",
      "Epoch [1/2], Step [11870/67476], Loss: 4.5265\n",
      "Epoch [1/2], Step [11880/67476], Loss: 4.6993\n",
      "Epoch [1/2], Step [11890/67476], Loss: 4.7849\n",
      "Epoch [1/2], Step [11900/67476], Loss: 4.7062\n",
      "Epoch [1/2], Step [11910/67476], Loss: 4.7089\n",
      "Epoch [1/2], Step [11920/67476], Loss: 4.5764\n",
      "Epoch [1/2], Step [11930/67476], Loss: 4.7941\n",
      "Epoch [1/2], Step [11940/67476], Loss: 4.7852\n",
      "Epoch [1/2], Step [11950/67476], Loss: 4.3413\n",
      "Epoch [1/2], Step [11960/67476], Loss: 4.4806\n",
      "Epoch [1/2], Step [11970/67476], Loss: 4.4622\n",
      "Epoch [1/2], Step [11980/67476], Loss: 4.5616\n",
      "Epoch [1/2], Step [11990/67476], Loss: 4.4528\n",
      "Epoch [1/2], Step [12000/67476], Loss: 4.6185\n",
      "Epoch [1/2], Step [12010/67476], Loss: 4.5963\n",
      "Epoch [1/2], Step [12020/67476], Loss: 4.5006\n",
      "Epoch [1/2], Step [12030/67476], Loss: 4.6202\n",
      "Epoch [1/2], Step [12040/67476], Loss: 4.4638\n",
      "Epoch [1/2], Step [12050/67476], Loss: 4.5363\n",
      "Epoch [1/2], Step [12060/67476], Loss: 4.6659\n",
      "Epoch [1/2], Step [12070/67476], Loss: 4.5299\n",
      "Epoch [1/2], Step [12080/67476], Loss: 4.5602\n",
      "Epoch [1/2], Step [12090/67476], Loss: 4.6320\n",
      "Epoch [1/2], Step [12100/67476], Loss: 4.5434\n",
      "Epoch [1/2], Step [12110/67476], Loss: 4.5868\n",
      "Epoch [1/2], Step [12120/67476], Loss: 4.5611\n",
      "Epoch [1/2], Step [12130/67476], Loss: 4.4987\n",
      "Epoch [1/2], Step [12140/67476], Loss: 4.7641\n",
      "Epoch [1/2], Step [12150/67476], Loss: 4.5461\n",
      "Epoch [1/2], Step [12160/67476], Loss: 4.6524\n",
      "Epoch [1/2], Step [12170/67476], Loss: 4.6352\n",
      "Epoch [1/2], Step [12180/67476], Loss: 4.5832\n",
      "Epoch [1/2], Step [12190/67476], Loss: 4.5138\n",
      "Epoch [1/2], Step [12200/67476], Loss: 4.6043\n",
      "Epoch [1/2], Step [12210/67476], Loss: 4.5534\n",
      "Epoch [1/2], Step [12220/67476], Loss: 4.7252\n",
      "Epoch [1/2], Step [12230/67476], Loss: 4.6251\n",
      "Epoch [1/2], Step [12240/67476], Loss: 4.5278\n",
      "Epoch [1/2], Step [12250/67476], Loss: 4.6113\n",
      "Epoch [1/2], Step [12260/67476], Loss: 4.6826\n",
      "Epoch [1/2], Step [12270/67476], Loss: 4.4472\n",
      "Epoch [1/2], Step [12280/67476], Loss: 4.5903\n",
      "Epoch [1/2], Step [12290/67476], Loss: 4.4857\n",
      "Epoch [1/2], Step [12300/67476], Loss: 4.4193\n",
      "Epoch [1/2], Step [12310/67476], Loss: 4.5694\n",
      "Epoch [1/2], Step [12320/67476], Loss: 4.4843\n",
      "Epoch [1/2], Step [12330/67476], Loss: 4.5733\n",
      "Epoch [1/2], Step [12340/67476], Loss: 4.8388\n",
      "Epoch [1/2], Step [12350/67476], Loss: 4.6931\n",
      "Epoch [1/2], Step [12360/67476], Loss: 4.4306\n",
      "Epoch [1/2], Step [12370/67476], Loss: 4.5508\n",
      "Epoch [1/2], Step [12380/67476], Loss: 4.6524\n",
      "Epoch [1/2], Step [12390/67476], Loss: 4.4098\n",
      "Epoch [1/2], Step [12400/67476], Loss: 4.6259\n",
      "Epoch [1/2], Step [12410/67476], Loss: 4.4994\n",
      "Epoch [1/2], Step [12420/67476], Loss: 4.6715\n",
      "Epoch [1/2], Step [12430/67476], Loss: 4.4043\n",
      "Epoch [1/2], Step [12440/67476], Loss: 4.6189\n",
      "Epoch [1/2], Step [12450/67476], Loss: 4.3933\n",
      "Epoch [1/2], Step [12460/67476], Loss: 4.5630\n",
      "Epoch [1/2], Step [12470/67476], Loss: 4.7186\n",
      "Epoch [1/2], Step [12480/67476], Loss: 4.5040\n",
      "Epoch [1/2], Step [12490/67476], Loss: 4.5621\n",
      "Epoch [1/2], Step [12500/67476], Loss: 4.6600\n",
      "Epoch [1/2], Step [12510/67476], Loss: 4.6448\n",
      "Epoch [1/2], Step [12520/67476], Loss: 4.6912\n",
      "Epoch [1/2], Step [12530/67476], Loss: 4.5640\n",
      "Epoch [1/2], Step [12540/67476], Loss: 4.5140\n",
      "Epoch [1/2], Step [12550/67476], Loss: 4.5489\n",
      "Epoch [1/2], Step [12560/67476], Loss: 4.5826\n",
      "Epoch [1/2], Step [12570/67476], Loss: 4.6648\n",
      "Epoch [1/2], Step [12580/67476], Loss: 4.5929\n",
      "Epoch [1/2], Step [12590/67476], Loss: 4.5393\n",
      "Epoch [1/2], Step [12600/67476], Loss: 4.6568\n",
      "Epoch [1/2], Step [12610/67476], Loss: 4.5447\n",
      "Epoch [1/2], Step [12620/67476], Loss: 4.5857\n",
      "Epoch [1/2], Step [12630/67476], Loss: 4.5863\n",
      "Epoch [1/2], Step [12640/67476], Loss: 4.5753\n",
      "Epoch [1/2], Step [12650/67476], Loss: 4.6716\n",
      "Epoch [1/2], Step [12660/67476], Loss: 4.6331\n",
      "Epoch [1/2], Step [12670/67476], Loss: 4.7161\n",
      "Epoch [1/2], Step [12680/67476], Loss: 4.6388\n",
      "Epoch [1/2], Step [12690/67476], Loss: 4.5835\n",
      "Epoch [1/2], Step [12700/67476], Loss: 4.6913\n",
      "Epoch [1/2], Step [12710/67476], Loss: 4.7805\n",
      "Epoch [1/2], Step [12720/67476], Loss: 4.6641\n",
      "Epoch [1/2], Step [12730/67476], Loss: 4.5958\n",
      "Epoch [1/2], Step [12740/67476], Loss: 4.5451\n",
      "Epoch [1/2], Step [12750/67476], Loss: 4.5975\n",
      "Epoch [1/2], Step [12760/67476], Loss: 4.7303\n",
      "Epoch [1/2], Step [12770/67476], Loss: 4.5737\n",
      "Epoch [1/2], Step [12780/67476], Loss: 4.8475\n",
      "Epoch [1/2], Step [12790/67476], Loss: 4.5823\n",
      "Epoch [1/2], Step [12800/67476], Loss: 4.4620\n",
      "Epoch [1/2], Step [12810/67476], Loss: 4.5982\n",
      "Epoch [1/2], Step [12820/67476], Loss: 4.4579\n",
      "Epoch [1/2], Step [12830/67476], Loss: 4.4173\n",
      "Epoch [1/2], Step [12840/67476], Loss: 4.5563\n",
      "Epoch [1/2], Step [12850/67476], Loss: 4.7277\n",
      "Epoch [1/2], Step [12860/67476], Loss: 4.7212\n",
      "Epoch [1/2], Step [12870/67476], Loss: 4.6540\n",
      "Epoch [1/2], Step [12880/67476], Loss: 4.4004\n",
      "Epoch [1/2], Step [12890/67476], Loss: 4.6672\n",
      "Epoch [1/2], Step [12900/67476], Loss: 4.6352\n",
      "Epoch [1/2], Step [12910/67476], Loss: 4.7030\n",
      "Epoch [1/2], Step [12920/67476], Loss: 4.6727\n",
      "Epoch [1/2], Step [12930/67476], Loss: 4.6206\n",
      "Epoch [1/2], Step [12940/67476], Loss: 4.8424\n",
      "Epoch [1/2], Step [12950/67476], Loss: 4.7687\n",
      "Epoch [1/2], Step [12960/67476], Loss: 4.4626\n",
      "Epoch [1/2], Step [12970/67476], Loss: 4.6331\n",
      "Epoch [1/2], Step [12980/67476], Loss: 4.6687\n",
      "Epoch [1/2], Step [12990/67476], Loss: 4.6883\n",
      "Epoch [1/2], Step [13000/67476], Loss: 4.6570\n",
      "Epoch [1/2], Step [13010/67476], Loss: 4.5070\n",
      "Epoch [1/2], Step [13020/67476], Loss: 4.6020\n",
      "Epoch [1/2], Step [13030/67476], Loss: 4.4915\n",
      "Epoch [1/2], Step [13040/67476], Loss: 4.6122\n",
      "Epoch [1/2], Step [13050/67476], Loss: 4.5977\n",
      "Epoch [1/2], Step [13060/67476], Loss: 4.5380\n",
      "Epoch [1/2], Step [13070/67476], Loss: 4.6965\n",
      "Epoch [1/2], Step [13080/67476], Loss: 4.7314\n",
      "Epoch [1/2], Step [13090/67476], Loss: 4.3627\n",
      "Epoch [1/2], Step [13100/67476], Loss: 4.7163\n",
      "Epoch [1/2], Step [13110/67476], Loss: 4.6614\n",
      "Epoch [1/2], Step [13120/67476], Loss: 4.4297\n",
      "Epoch [1/2], Step [13130/67476], Loss: 4.6182\n",
      "Epoch [1/2], Step [13140/67476], Loss: 4.5528\n",
      "Epoch [1/2], Step [13150/67476], Loss: 4.7518\n",
      "Epoch [1/2], Step [13160/67476], Loss: 4.6570\n",
      "Epoch [1/2], Step [13170/67476], Loss: 4.5667\n",
      "Epoch [1/2], Step [13180/67476], Loss: 4.5914\n",
      "Epoch [1/2], Step [13190/67476], Loss: 4.5155\n",
      "Epoch [1/2], Step [13200/67476], Loss: 4.5373\n",
      "Epoch [1/2], Step [13210/67476], Loss: 4.6203\n",
      "Epoch [1/2], Step [13220/67476], Loss: 4.6044\n",
      "Epoch [1/2], Step [13230/67476], Loss: 4.7147\n",
      "Epoch [1/2], Step [13240/67476], Loss: 4.5038\n",
      "Epoch [1/2], Step [13250/67476], Loss: 4.4224\n",
      "Epoch [1/2], Step [13260/67476], Loss: 4.4601\n",
      "Epoch [1/2], Step [13270/67476], Loss: 4.6956\n",
      "Epoch [1/2], Step [13280/67476], Loss: 4.7197\n",
      "Epoch [1/2], Step [13290/67476], Loss: 4.5029\n",
      "Epoch [1/2], Step [13300/67476], Loss: 4.6756\n",
      "Epoch [1/2], Step [13310/67476], Loss: 4.4339\n",
      "Epoch [1/2], Step [13320/67476], Loss: 4.4891\n",
      "Epoch [1/2], Step [13330/67476], Loss: 4.6837\n",
      "Epoch [1/2], Step [13340/67476], Loss: 4.5425\n",
      "Epoch [1/2], Step [13350/67476], Loss: 4.6986\n",
      "Epoch [1/2], Step [13360/67476], Loss: 4.4462\n",
      "Epoch [1/2], Step [13370/67476], Loss: 4.5543\n",
      "Epoch [1/2], Step [13380/67476], Loss: 4.6843\n",
      "Epoch [1/2], Step [13390/67476], Loss: 4.5701\n",
      "Epoch [1/2], Step [13400/67476], Loss: 4.7735\n",
      "Epoch [1/2], Step [13410/67476], Loss: 4.6085\n",
      "Epoch [1/2], Step [13420/67476], Loss: 4.6027\n",
      "Epoch [1/2], Step [13430/67476], Loss: 4.6114\n",
      "Epoch [1/2], Step [13440/67476], Loss: 4.5179\n",
      "Epoch [1/2], Step [13450/67476], Loss: 4.5944\n",
      "Epoch [1/2], Step [13460/67476], Loss: 4.7714\n",
      "Epoch [1/2], Step [13470/67476], Loss: 4.7472\n",
      "Epoch [1/2], Step [13480/67476], Loss: 4.6113\n",
      "Epoch [1/2], Step [13490/67476], Loss: 4.5231\n",
      "Epoch [1/2], Step [13500/67476], Loss: 4.5139\n",
      "Epoch [1/2], Step [13510/67476], Loss: 4.4426\n",
      "Epoch [1/2], Step [13520/67476], Loss: 4.5386\n",
      "Epoch [1/2], Step [13530/67476], Loss: 4.6448\n",
      "Epoch [1/2], Step [13540/67476], Loss: 4.4876\n",
      "Epoch [1/2], Step [13550/67476], Loss: 4.5311\n",
      "Epoch [1/2], Step [13560/67476], Loss: 4.5728\n",
      "Epoch [1/2], Step [13570/67476], Loss: 4.4684\n",
      "Epoch [1/2], Step [13580/67476], Loss: 4.6342\n",
      "Epoch [1/2], Step [13590/67476], Loss: 4.4538\n",
      "Epoch [1/2], Step [13600/67476], Loss: 4.4565\n",
      "Epoch [1/2], Step [13610/67476], Loss: 4.6662\n",
      "Epoch [1/2], Step [13620/67476], Loss: 4.4511\n",
      "Epoch [1/2], Step [13630/67476], Loss: 4.6111\n",
      "Epoch [1/2], Step [13640/67476], Loss: 4.6348\n",
      "Epoch [1/2], Step [13650/67476], Loss: 4.7091\n",
      "Epoch [1/2], Step [13660/67476], Loss: 4.4104\n",
      "Epoch [1/2], Step [13670/67476], Loss: 4.5674\n",
      "Epoch [1/2], Step [13680/67476], Loss: 4.5277\n",
      "Epoch [1/2], Step [13690/67476], Loss: 4.3962\n",
      "Epoch [1/2], Step [13700/67476], Loss: 4.6208\n",
      "Epoch [1/2], Step [13710/67476], Loss: 4.5888\n",
      "Epoch [1/2], Step [13720/67476], Loss: 4.7274\n",
      "Epoch [1/2], Step [13730/67476], Loss: 4.6343\n",
      "Epoch [1/2], Step [13740/67476], Loss: 4.5468\n",
      "Epoch [1/2], Step [13750/67476], Loss: 4.6743\n",
      "Epoch [1/2], Step [13760/67476], Loss: 4.4901\n",
      "Epoch [1/2], Step [13770/67476], Loss: 4.5790\n",
      "Epoch [1/2], Step [13780/67476], Loss: 4.3551\n",
      "Epoch [1/2], Step [13790/67476], Loss: 4.6379\n",
      "Epoch [1/2], Step [13800/67476], Loss: 4.5358\n",
      "Epoch [1/2], Step [13810/67476], Loss: 4.5555\n",
      "Epoch [1/2], Step [13820/67476], Loss: 4.7146\n",
      "Epoch [1/2], Step [13830/67476], Loss: 4.6240\n",
      "Epoch [1/2], Step [13840/67476], Loss: 4.3989\n",
      "Epoch [1/2], Step [13850/67476], Loss: 4.6252\n",
      "Epoch [1/2], Step [13860/67476], Loss: 4.6407\n",
      "Epoch [1/2], Step [13870/67476], Loss: 4.6331\n",
      "Epoch [1/2], Step [13880/67476], Loss: 4.7022\n",
      "Epoch [1/2], Step [13890/67476], Loss: 4.6366\n",
      "Epoch [1/2], Step [13900/67476], Loss: 4.6593\n",
      "Epoch [1/2], Step [13910/67476], Loss: 4.5754\n",
      "Epoch [1/2], Step [13920/67476], Loss: 4.5940\n",
      "Epoch [1/2], Step [13930/67476], Loss: 4.5011\n",
      "Epoch [1/2], Step [13940/67476], Loss: 4.4564\n",
      "Epoch [1/2], Step [13950/67476], Loss: 4.4384\n",
      "Epoch [1/2], Step [13960/67476], Loss: 4.4841\n",
      "Epoch [1/2], Step [13970/67476], Loss: 4.6802\n",
      "Epoch [1/2], Step [13980/67476], Loss: 4.6139\n",
      "Epoch [1/2], Step [13990/67476], Loss: 4.4282\n",
      "Epoch [1/2], Step [14000/67476], Loss: 4.6810\n",
      "Epoch [1/2], Step [14010/67476], Loss: 4.6174\n",
      "Epoch [1/2], Step [14020/67476], Loss: 4.5765\n",
      "Epoch [1/2], Step [14030/67476], Loss: 4.4303\n",
      "Epoch [1/2], Step [14040/67476], Loss: 4.6147\n",
      "Epoch [1/2], Step [14050/67476], Loss: 4.8990\n",
      "Epoch [1/2], Step [14060/67476], Loss: 4.5025\n",
      "Epoch [1/2], Step [14070/67476], Loss: 4.4565\n",
      "Epoch [1/2], Step [14080/67476], Loss: 4.5026\n",
      "Epoch [1/2], Step [14090/67476], Loss: 4.8058\n",
      "Epoch [1/2], Step [14100/67476], Loss: 4.6468\n",
      "Epoch [1/2], Step [14110/67476], Loss: 4.5173\n",
      "Epoch [1/2], Step [14120/67476], Loss: 4.4916\n",
      "Epoch [1/2], Step [14130/67476], Loss: 4.6773\n",
      "Epoch [1/2], Step [14140/67476], Loss: 4.4733\n",
      "Epoch [1/2], Step [14150/67476], Loss: 4.6212\n",
      "Epoch [1/2], Step [14160/67476], Loss: 4.4257\n",
      "Epoch [1/2], Step [14170/67476], Loss: 4.6517\n",
      "Epoch [1/2], Step [14180/67476], Loss: 4.7572\n",
      "Epoch [1/2], Step [14190/67476], Loss: 4.6462\n",
      "Epoch [1/2], Step [14200/67476], Loss: 4.3593\n",
      "Epoch [1/2], Step [14210/67476], Loss: 4.5096\n",
      "Epoch [1/2], Step [14220/67476], Loss: 4.5349\n",
      "Epoch [1/2], Step [14230/67476], Loss: 4.4730\n",
      "Epoch [1/2], Step [14240/67476], Loss: 4.5195\n",
      "Epoch [1/2], Step [14250/67476], Loss: 4.6202\n",
      "Epoch [1/2], Step [14260/67476], Loss: 4.5715\n",
      "Epoch [1/2], Step [14270/67476], Loss: 4.5827\n",
      "Epoch [1/2], Step [14280/67476], Loss: 4.5408\n",
      "Epoch [1/2], Step [14290/67476], Loss: 4.2561\n",
      "Epoch [1/2], Step [14300/67476], Loss: 4.6707\n",
      "Epoch [1/2], Step [14310/67476], Loss: 4.6365\n",
      "Epoch [1/2], Step [14320/67476], Loss: 4.4025\n",
      "Epoch [1/2], Step [14330/67476], Loss: 4.4252\n",
      "Epoch [1/2], Step [14340/67476], Loss: 4.5898\n",
      "Epoch [1/2], Step [14350/67476], Loss: 4.5306\n",
      "Epoch [1/2], Step [14360/67476], Loss: 4.4886\n",
      "Epoch [1/2], Step [14370/67476], Loss: 4.6445\n",
      "Epoch [1/2], Step [14380/67476], Loss: 4.7038\n",
      "Epoch [1/2], Step [14390/67476], Loss: 4.8478\n",
      "Epoch [1/2], Step [14400/67476], Loss: 4.3627\n",
      "Epoch [1/2], Step [14410/67476], Loss: 4.4873\n",
      "Epoch [1/2], Step [14420/67476], Loss: 4.4268\n",
      "Epoch [1/2], Step [14430/67476], Loss: 4.6532\n",
      "Epoch [1/2], Step [14440/67476], Loss: 4.4432\n",
      "Epoch [1/2], Step [14450/67476], Loss: 4.7186\n",
      "Epoch [1/2], Step [14460/67476], Loss: 4.6322\n",
      "Epoch [1/2], Step [14470/67476], Loss: 4.5995\n",
      "Epoch [1/2], Step [14480/67476], Loss: 4.6916\n",
      "Epoch [1/2], Step [14490/67476], Loss: 4.6069\n",
      "Epoch [1/2], Step [14500/67476], Loss: 4.5024\n",
      "Epoch [1/2], Step [14510/67476], Loss: 5.0088\n",
      "Epoch [1/2], Step [14520/67476], Loss: 4.6734\n",
      "Epoch [1/2], Step [14530/67476], Loss: 4.5072\n",
      "Epoch [1/2], Step [14540/67476], Loss: 4.5998\n",
      "Epoch [1/2], Step [14550/67476], Loss: 4.5826\n",
      "Epoch [1/2], Step [14560/67476], Loss: 4.6796\n",
      "Epoch [1/2], Step [14570/67476], Loss: 4.5234\n",
      "Epoch [1/2], Step [14580/67476], Loss: 4.7068\n",
      "Epoch [1/2], Step [14590/67476], Loss: 4.6149\n",
      "Epoch [1/2], Step [14600/67476], Loss: 4.5963\n",
      "Epoch [1/2], Step [14610/67476], Loss: 4.5465\n",
      "Epoch [1/2], Step [14620/67476], Loss: 4.8144\n",
      "Epoch [1/2], Step [14630/67476], Loss: 4.5408\n",
      "Epoch [1/2], Step [14640/67476], Loss: 4.5524\n",
      "Epoch [1/2], Step [14650/67476], Loss: 4.6443\n",
      "Epoch [1/2], Step [14660/67476], Loss: 4.5236\n",
      "Epoch [1/2], Step [14670/67476], Loss: 4.6658\n",
      "Epoch [1/2], Step [14680/67476], Loss: 4.6180\n",
      "Epoch [1/2], Step [14690/67476], Loss: 4.7618\n",
      "Epoch [1/2], Step [14700/67476], Loss: 4.3945\n",
      "Epoch [1/2], Step [14710/67476], Loss: 4.5597\n",
      "Epoch [1/2], Step [14720/67476], Loss: 4.6912\n",
      "Epoch [1/2], Step [14730/67476], Loss: 4.6978\n",
      "Epoch [1/2], Step [14740/67476], Loss: 4.6453\n",
      "Epoch [1/2], Step [14750/67476], Loss: 4.4888\n",
      "Epoch [1/2], Step [14760/67476], Loss: 4.8217\n",
      "Epoch [1/2], Step [14770/67476], Loss: 4.6748\n",
      "Epoch [1/2], Step [14780/67476], Loss: 4.8126\n",
      "Epoch [1/2], Step [14790/67476], Loss: 4.5836\n",
      "Epoch [1/2], Step [14800/67476], Loss: 4.5994\n",
      "Epoch [1/2], Step [14810/67476], Loss: 4.5540\n",
      "Epoch [1/2], Step [14820/67476], Loss: 4.5505\n",
      "Epoch [1/2], Step [14830/67476], Loss: 4.6734\n",
      "Epoch [1/2], Step [14840/67476], Loss: 4.8537\n",
      "Epoch [1/2], Step [14850/67476], Loss: 4.8954\n",
      "Epoch [1/2], Step [14860/67476], Loss: 4.6452\n",
      "Epoch [1/2], Step [14870/67476], Loss: 4.6228\n",
      "Epoch [1/2], Step [14880/67476], Loss: 4.6664\n",
      "Epoch [1/2], Step [14890/67476], Loss: 4.5259\n",
      "Epoch [1/2], Step [14900/67476], Loss: 4.7122\n",
      "Epoch [1/2], Step [14910/67476], Loss: 4.6272\n",
      "Epoch [1/2], Step [14920/67476], Loss: 4.6491\n",
      "Epoch [1/2], Step [14930/67476], Loss: 4.5400\n",
      "Epoch [1/2], Step [14940/67476], Loss: 4.5044\n",
      "Epoch [1/2], Step [14950/67476], Loss: 4.4521\n",
      "Epoch [1/2], Step [14960/67476], Loss: 4.7159\n",
      "Epoch [1/2], Step [14970/67476], Loss: 4.3918\n",
      "Epoch [1/2], Step [14980/67476], Loss: 4.8201\n",
      "Epoch [1/2], Step [14990/67476], Loss: 4.4309\n",
      "Epoch [1/2], Step [15000/67476], Loss: 4.5143\n",
      "Epoch [1/2], Step [15010/67476], Loss: 4.6086\n",
      "Epoch [1/2], Step [15020/67476], Loss: 4.4709\n",
      "Epoch [1/2], Step [15030/67476], Loss: 4.5959\n",
      "Epoch [1/2], Step [15040/67476], Loss: 4.5148\n",
      "Epoch [1/2], Step [15050/67476], Loss: 4.5075\n",
      "Epoch [1/2], Step [15060/67476], Loss: 4.6344\n",
      "Epoch [1/2], Step [15070/67476], Loss: 4.5632\n",
      "Epoch [1/2], Step [15080/67476], Loss: 4.5604\n",
      "Epoch [1/2], Step [15090/67476], Loss: 4.4528\n",
      "Epoch [1/2], Step [15100/67476], Loss: 4.5755\n",
      "Epoch [1/2], Step [15110/67476], Loss: 4.6926\n",
      "Epoch [1/2], Step [15120/67476], Loss: 4.7873\n",
      "Epoch [1/2], Step [15130/67476], Loss: 4.7079\n",
      "Epoch [1/2], Step [15140/67476], Loss: 4.6675\n",
      "Epoch [1/2], Step [15150/67476], Loss: 4.5427\n",
      "Epoch [1/2], Step [15160/67476], Loss: 4.5990\n",
      "Epoch [1/2], Step [15170/67476], Loss: 4.6650\n",
      "Epoch [1/2], Step [15180/67476], Loss: 4.7742\n",
      "Epoch [1/2], Step [15190/67476], Loss: 4.8256\n",
      "Epoch [1/2], Step [15200/67476], Loss: 4.6941\n",
      "Epoch [1/2], Step [15210/67476], Loss: 4.7373\n",
      "Epoch [1/2], Step [15220/67476], Loss: 4.4591\n",
      "Epoch [1/2], Step [15230/67476], Loss: 4.4909\n",
      "Epoch [1/2], Step [15240/67476], Loss: 4.7613\n",
      "Epoch [1/2], Step [15250/67476], Loss: 4.4802\n",
      "Epoch [1/2], Step [15260/67476], Loss: 4.6202\n",
      "Epoch [1/2], Step [15270/67476], Loss: 4.5707\n",
      "Epoch [1/2], Step [15280/67476], Loss: 4.5515\n",
      "Epoch [1/2], Step [15290/67476], Loss: 4.5781\n",
      "Epoch [1/2], Step [15300/67476], Loss: 4.4725\n",
      "Epoch [1/2], Step [15310/67476], Loss: 4.6519\n",
      "Epoch [1/2], Step [15320/67476], Loss: 4.6656\n",
      "Epoch [1/2], Step [15330/67476], Loss: 4.5157\n",
      "Epoch [1/2], Step [15340/67476], Loss: 4.6478\n",
      "Epoch [1/2], Step [15350/67476], Loss: 4.6937\n",
      "Epoch [1/2], Step [15360/67476], Loss: 4.7977\n",
      "Epoch [1/2], Step [15370/67476], Loss: 4.6127\n",
      "Epoch [1/2], Step [15380/67476], Loss: 4.6876\n",
      "Epoch [1/2], Step [15390/67476], Loss: 4.5683\n",
      "Epoch [1/2], Step [15400/67476], Loss: 4.5730\n",
      "Epoch [1/2], Step [15410/67476], Loss: 4.6521\n",
      "Epoch [1/2], Step [15420/67476], Loss: 4.4477\n",
      "Epoch [1/2], Step [15430/67476], Loss: 4.6925\n",
      "Epoch [1/2], Step [15440/67476], Loss: 4.8211\n",
      "Epoch [1/2], Step [15450/67476], Loss: 4.6258\n",
      "Epoch [1/2], Step [15460/67476], Loss: 4.4629\n",
      "Epoch [1/2], Step [15470/67476], Loss: 4.6135\n",
      "Epoch [1/2], Step [15480/67476], Loss: 4.5323\n",
      "Epoch [1/2], Step [15490/67476], Loss: 4.6406\n",
      "Epoch [1/2], Step [15500/67476], Loss: 4.5212\n",
      "Epoch [1/2], Step [15510/67476], Loss: 4.8299\n",
      "Epoch [1/2], Step [15520/67476], Loss: 4.2513\n",
      "Epoch [1/2], Step [15530/67476], Loss: 4.4934\n",
      "Epoch [1/2], Step [15540/67476], Loss: 4.5481\n",
      "Epoch [1/2], Step [15550/67476], Loss: 4.2776\n",
      "Epoch [1/2], Step [15560/67476], Loss: 4.4849\n",
      "Epoch [1/2], Step [15570/67476], Loss: 4.6226\n",
      "Epoch [1/2], Step [15580/67476], Loss: 4.6117\n",
      "Epoch [1/2], Step [15590/67476], Loss: 4.5808\n",
      "Epoch [1/2], Step [15600/67476], Loss: 4.3902\n",
      "Epoch [1/2], Step [15610/67476], Loss: 4.8439\n",
      "Epoch [1/2], Step [15620/67476], Loss: 4.4642\n",
      "Epoch [1/2], Step [15630/67476], Loss: 4.8493\n",
      "Epoch [1/2], Step [15640/67476], Loss: 4.7344\n",
      "Epoch [1/2], Step [15650/67476], Loss: 4.5971\n",
      "Epoch [1/2], Step [15660/67476], Loss: 4.2859\n",
      "Epoch [1/2], Step [15670/67476], Loss: 4.5746\n",
      "Epoch [1/2], Step [15680/67476], Loss: 4.9345\n",
      "Epoch [1/2], Step [15690/67476], Loss: 4.6750\n",
      "Epoch [1/2], Step [15700/67476], Loss: 4.6311\n",
      "Epoch [1/2], Step [15710/67476], Loss: 4.6792\n",
      "Epoch [1/2], Step [15720/67476], Loss: 4.6189\n",
      "Epoch [1/2], Step [15730/67476], Loss: 4.4589\n",
      "Epoch [1/2], Step [15740/67476], Loss: 4.7259\n",
      "Epoch [1/2], Step [15750/67476], Loss: 4.6260\n",
      "Epoch [1/2], Step [15760/67476], Loss: 4.5154\n",
      "Epoch [1/2], Step [15770/67476], Loss: 4.7096\n",
      "Epoch [1/2], Step [15780/67476], Loss: 4.5001\n",
      "Epoch [1/2], Step [15790/67476], Loss: 4.4446\n",
      "Epoch [1/2], Step [15800/67476], Loss: 4.5472\n",
      "Epoch [1/2], Step [15810/67476], Loss: 4.7110\n",
      "Epoch [1/2], Step [15820/67476], Loss: 4.7505\n",
      "Epoch [1/2], Step [15830/67476], Loss: 4.4691\n",
      "Epoch [1/2], Step [15840/67476], Loss: 4.4026\n",
      "Epoch [1/2], Step [15850/67476], Loss: 4.6648\n",
      "Epoch [1/2], Step [15860/67476], Loss: 4.5222\n",
      "Epoch [1/2], Step [15870/67476], Loss: 4.6570\n",
      "Epoch [1/2], Step [15880/67476], Loss: 4.5570\n",
      "Epoch [1/2], Step [15890/67476], Loss: 4.5354\n",
      "Epoch [1/2], Step [15900/67476], Loss: 4.5813\n",
      "Epoch [1/2], Step [15910/67476], Loss: 4.5701\n",
      "Epoch [1/2], Step [15920/67476], Loss: 4.5880\n",
      "Epoch [1/2], Step [15930/67476], Loss: 4.3525\n",
      "Epoch [1/2], Step [15940/67476], Loss: 4.7403\n",
      "Epoch [1/2], Step [15950/67476], Loss: 4.6512\n",
      "Epoch [1/2], Step [15960/67476], Loss: 4.4863\n",
      "Epoch [1/2], Step [15970/67476], Loss: 4.5818\n",
      "Epoch [1/2], Step [15980/67476], Loss: 4.7787\n",
      "Epoch [1/2], Step [15990/67476], Loss: 4.5672\n",
      "Epoch [1/2], Step [16000/67476], Loss: 4.3919\n",
      "Epoch [1/2], Step [16010/67476], Loss: 4.6430\n",
      "Epoch [1/2], Step [16020/67476], Loss: 4.8525\n",
      "Epoch [1/2], Step [16030/67476], Loss: 4.5600\n",
      "Epoch [1/2], Step [16040/67476], Loss: 4.3037\n",
      "Epoch [1/2], Step [16050/67476], Loss: 4.5491\n",
      "Epoch [1/2], Step [16060/67476], Loss: 4.8459\n",
      "Epoch [1/2], Step [16070/67476], Loss: 4.4925\n",
      "Epoch [1/2], Step [16080/67476], Loss: 4.4592\n",
      "Epoch [1/2], Step [16090/67476], Loss: 4.5525\n",
      "Epoch [1/2], Step [16100/67476], Loss: 4.7137\n",
      "Epoch [1/2], Step [16110/67476], Loss: 4.5810\n",
      "Epoch [1/2], Step [16120/67476], Loss: 4.5235\n",
      "Epoch [1/2], Step [16130/67476], Loss: 4.4594\n",
      "Epoch [1/2], Step [16140/67476], Loss: 4.4056\n",
      "Epoch [1/2], Step [16150/67476], Loss: 4.6627\n",
      "Epoch [1/2], Step [16160/67476], Loss: 4.6510\n",
      "Epoch [1/2], Step [16170/67476], Loss: 4.9027\n",
      "Epoch [1/2], Step [16180/67476], Loss: 4.6448\n",
      "Epoch [1/2], Step [16190/67476], Loss: 4.7522\n",
      "Epoch [1/2], Step [16200/67476], Loss: 4.6343\n",
      "Epoch [1/2], Step [16210/67476], Loss: 4.4150\n",
      "Epoch [1/2], Step [16220/67476], Loss: 4.3719\n",
      "Epoch [1/2], Step [16230/67476], Loss: 4.5610\n",
      "Epoch [1/2], Step [16240/67476], Loss: 4.5573\n",
      "Epoch [1/2], Step [16250/67476], Loss: 4.7160\n",
      "Epoch [1/2], Step [16260/67476], Loss: 4.4249\n",
      "Epoch [1/2], Step [16270/67476], Loss: 4.6123\n",
      "Epoch [1/2], Step [16280/67476], Loss: 4.4000\n",
      "Epoch [1/2], Step [16290/67476], Loss: 4.5928\n",
      "Epoch [1/2], Step [16300/67476], Loss: 4.6630\n",
      "Epoch [1/2], Step [16310/67476], Loss: 4.5085\n",
      "Epoch [1/2], Step [16320/67476], Loss: 4.6875\n",
      "Epoch [1/2], Step [16330/67476], Loss: 4.5194\n",
      "Epoch [1/2], Step [16340/67476], Loss: 4.4712\n",
      "Epoch [1/2], Step [16350/67476], Loss: 4.6673\n",
      "Epoch [1/2], Step [16360/67476], Loss: 4.6999\n",
      "Epoch [1/2], Step [16370/67476], Loss: 4.4308\n",
      "Epoch [1/2], Step [16380/67476], Loss: 4.6195\n",
      "Epoch [1/2], Step [16390/67476], Loss: 4.7711\n",
      "Epoch [1/2], Step [16400/67476], Loss: 4.4879\n",
      "Epoch [1/2], Step [16410/67476], Loss: 4.6391\n",
      "Epoch [1/2], Step [16420/67476], Loss: 4.6922\n",
      "Epoch [1/2], Step [16430/67476], Loss: 4.5810\n",
      "Epoch [1/2], Step [16440/67476], Loss: 4.6687\n",
      "Epoch [1/2], Step [16450/67476], Loss: 4.6212\n",
      "Epoch [1/2], Step [16460/67476], Loss: 4.6512\n",
      "Epoch [1/2], Step [16470/67476], Loss: 4.5698\n",
      "Epoch [1/2], Step [16480/67476], Loss: 4.6212\n",
      "Epoch [1/2], Step [16490/67476], Loss: 4.7432\n",
      "Epoch [1/2], Step [16500/67476], Loss: 4.5454\n",
      "Epoch [1/2], Step [16510/67476], Loss: 4.5499\n",
      "Epoch [1/2], Step [16520/67476], Loss: 4.6347\n",
      "Epoch [1/2], Step [16530/67476], Loss: 4.5744\n",
      "Epoch [1/2], Step [16540/67476], Loss: 4.7595\n",
      "Epoch [1/2], Step [16550/67476], Loss: 4.3347\n",
      "Epoch [1/2], Step [16560/67476], Loss: 4.7319\n",
      "Epoch [1/2], Step [16570/67476], Loss: 4.6302\n",
      "Epoch [1/2], Step [16580/67476], Loss: 4.5966\n",
      "Epoch [1/2], Step [16590/67476], Loss: 4.5668\n",
      "Epoch [1/2], Step [16600/67476], Loss: 4.5797\n",
      "Epoch [1/2], Step [16610/67476], Loss: 4.3459\n",
      "Epoch [1/2], Step [16620/67476], Loss: 4.5552\n",
      "Epoch [1/2], Step [16630/67476], Loss: 4.7426\n",
      "Epoch [1/2], Step [16640/67476], Loss: 4.4721\n",
      "Epoch [1/2], Step [16650/67476], Loss: 4.5232\n",
      "Epoch [1/2], Step [16660/67476], Loss: 4.6467\n",
      "Epoch [1/2], Step [16670/67476], Loss: 4.4394\n",
      "Epoch [1/2], Step [16680/67476], Loss: 4.5167\n",
      "Epoch [1/2], Step [16690/67476], Loss: 4.5949\n",
      "Epoch [1/2], Step [16700/67476], Loss: 4.4747\n",
      "Epoch [1/2], Step [16710/67476], Loss: 4.5356\n",
      "Epoch [1/2], Step [16720/67476], Loss: 4.4584\n",
      "Epoch [1/2], Step [16730/67476], Loss: 4.7357\n",
      "Epoch [1/2], Step [16740/67476], Loss: 4.5336\n",
      "Epoch [1/2], Step [16750/67476], Loss: 4.5007\n",
      "Epoch [1/2], Step [16760/67476], Loss: 4.4985\n",
      "Epoch [1/2], Step [16770/67476], Loss: 4.5474\n",
      "Epoch [1/2], Step [16780/67476], Loss: 4.4960\n",
      "Epoch [1/2], Step [16790/67476], Loss: 4.4913\n",
      "Epoch [1/2], Step [16800/67476], Loss: 4.2953\n",
      "Epoch [1/2], Step [16810/67476], Loss: 4.6653\n",
      "Epoch [1/2], Step [16820/67476], Loss: 4.4693\n",
      "Epoch [1/2], Step [16830/67476], Loss: 4.5716\n",
      "Epoch [1/2], Step [16840/67476], Loss: 4.6994\n",
      "Epoch [1/2], Step [16850/67476], Loss: 4.5807\n",
      "Epoch [1/2], Step [16860/67476], Loss: 4.6069\n",
      "Epoch [1/2], Step [16870/67476], Loss: 4.4561\n",
      "Epoch [1/2], Step [16880/67476], Loss: 4.6358\n",
      "Epoch [1/2], Step [16890/67476], Loss: 4.5133\n",
      "Epoch [1/2], Step [16900/67476], Loss: 4.6510\n",
      "Epoch [1/2], Step [16910/67476], Loss: 4.6122\n",
      "Epoch [1/2], Step [16920/67476], Loss: 4.5967\n",
      "Epoch [1/2], Step [16930/67476], Loss: 4.6006\n",
      "Epoch [1/2], Step [16940/67476], Loss: 4.6143\n",
      "Epoch [1/2], Step [16950/67476], Loss: 4.5053\n",
      "Epoch [1/2], Step [16960/67476], Loss: 4.7312\n",
      "Epoch [1/2], Step [16970/67476], Loss: 4.6082\n",
      "Epoch [1/2], Step [16980/67476], Loss: 4.7562\n",
      "Epoch [1/2], Step [16990/67476], Loss: 4.4320\n",
      "Epoch [1/2], Step [17000/67476], Loss: 4.5731\n",
      "Epoch [1/2], Step [17010/67476], Loss: 4.5484\n",
      "Epoch [1/2], Step [17020/67476], Loss: 4.5706\n",
      "Epoch [1/2], Step [17030/67476], Loss: 4.5995\n",
      "Epoch [1/2], Step [17040/67476], Loss: 4.5543\n",
      "Epoch [1/2], Step [17050/67476], Loss: 4.6026\n",
      "Epoch [1/2], Step [17060/67476], Loss: 4.4332\n",
      "Epoch [1/2], Step [17070/67476], Loss: 4.5640\n",
      "Epoch [1/2], Step [17080/67476], Loss: 4.5839\n",
      "Epoch [1/2], Step [17090/67476], Loss: 4.5796\n",
      "Epoch [1/2], Step [17100/67476], Loss: 4.5680\n",
      "Epoch [1/2], Step [17110/67476], Loss: 4.6736\n",
      "Epoch [1/2], Step [17120/67476], Loss: 4.6701\n",
      "Epoch [1/2], Step [17130/67476], Loss: 4.5785\n",
      "Epoch [1/2], Step [17140/67476], Loss: 4.3704\n",
      "Epoch [1/2], Step [17150/67476], Loss: 4.8000\n",
      "Epoch [1/2], Step [17160/67476], Loss: 4.7006\n",
      "Epoch [1/2], Step [17170/67476], Loss: 4.6909\n",
      "Epoch [1/2], Step [17180/67476], Loss: 4.6577\n",
      "Epoch [1/2], Step [17190/67476], Loss: 4.7168\n",
      "Epoch [1/2], Step [17200/67476], Loss: 4.4824\n",
      "Epoch [1/2], Step [17210/67476], Loss: 4.5306\n",
      "Epoch [1/2], Step [17220/67476], Loss: 4.7816\n",
      "Epoch [1/2], Step [17230/67476], Loss: 4.6181\n",
      "Epoch [1/2], Step [17240/67476], Loss: 4.5159\n",
      "Epoch [1/2], Step [17250/67476], Loss: 4.4947\n",
      "Epoch [1/2], Step [17260/67476], Loss: 4.6686\n",
      "Epoch [1/2], Step [17270/67476], Loss: 4.4603\n",
      "Epoch [1/2], Step [17280/67476], Loss: 4.5324\n",
      "Epoch [1/2], Step [17290/67476], Loss: 4.6210\n",
      "Epoch [1/2], Step [17300/67476], Loss: 4.8098\n",
      "Epoch [1/2], Step [17310/67476], Loss: 4.5796\n",
      "Epoch [1/2], Step [17320/67476], Loss: 4.6661\n",
      "Epoch [1/2], Step [17330/67476], Loss: 4.5264\n",
      "Epoch [1/2], Step [17340/67476], Loss: 4.6376\n",
      "Epoch [1/2], Step [17350/67476], Loss: 4.6078\n",
      "Epoch [1/2], Step [17360/67476], Loss: 4.5866\n",
      "Epoch [1/2], Step [17370/67476], Loss: 4.7986\n",
      "Epoch [1/2], Step [17380/67476], Loss: 4.3247\n",
      "Epoch [1/2], Step [17390/67476], Loss: 4.5476\n",
      "Epoch [1/2], Step [17400/67476], Loss: 4.4800\n",
      "Epoch [1/2], Step [17410/67476], Loss: 4.4287\n",
      "Epoch [1/2], Step [17420/67476], Loss: 4.3258\n",
      "Epoch [1/2], Step [17430/67476], Loss: 4.4850\n",
      "Epoch [1/2], Step [17440/67476], Loss: 4.3326\n",
      "Epoch [1/2], Step [17450/67476], Loss: 4.5586\n",
      "Epoch [1/2], Step [17460/67476], Loss: 4.5403\n",
      "Epoch [1/2], Step [17470/67476], Loss: 4.5973\n",
      "Epoch [1/2], Step [17480/67476], Loss: 4.5028\n",
      "Epoch [1/2], Step [17490/67476], Loss: 4.5624\n",
      "Epoch [1/2], Step [17500/67476], Loss: 4.7813\n",
      "Epoch [1/2], Step [17510/67476], Loss: 4.5567\n",
      "Epoch [1/2], Step [17520/67476], Loss: 4.4576\n",
      "Epoch [1/2], Step [17530/67476], Loss: 4.6805\n",
      "Epoch [1/2], Step [17540/67476], Loss: 4.3195\n",
      "Epoch [1/2], Step [17550/67476], Loss: 4.4865\n",
      "Epoch [1/2], Step [17560/67476], Loss: 4.5419\n",
      "Epoch [1/2], Step [17570/67476], Loss: 4.7978\n",
      "Epoch [1/2], Step [17580/67476], Loss: 4.4362\n",
      "Epoch [1/2], Step [17590/67476], Loss: 4.7997\n",
      "Epoch [1/2], Step [17600/67476], Loss: 4.5726\n",
      "Epoch [1/2], Step [17610/67476], Loss: 4.3844\n",
      "Epoch [1/2], Step [17620/67476], Loss: 4.6252\n",
      "Epoch [1/2], Step [17630/67476], Loss: 4.6871\n",
      "Epoch [1/2], Step [17640/67476], Loss: 4.8769\n",
      "Epoch [1/2], Step [17650/67476], Loss: 4.5383\n",
      "Epoch [1/2], Step [17660/67476], Loss: 4.4457\n",
      "Epoch [1/2], Step [17670/67476], Loss: 4.5734\n",
      "Epoch [1/2], Step [17680/67476], Loss: 4.4642\n",
      "Epoch [1/2], Step [17690/67476], Loss: 4.4856\n",
      "Epoch [1/2], Step [17700/67476], Loss: 4.5766\n",
      "Epoch [1/2], Step [17710/67476], Loss: 4.5360\n",
      "Epoch [1/2], Step [17720/67476], Loss: 4.5859\n",
      "Epoch [1/2], Step [17730/67476], Loss: 4.6707\n",
      "Epoch [1/2], Step [17740/67476], Loss: 4.4617\n",
      "Epoch [1/2], Step [17750/67476], Loss: 4.5315\n",
      "Epoch [1/2], Step [17760/67476], Loss: 4.5436\n",
      "Epoch [1/2], Step [17770/67476], Loss: 4.4168\n",
      "Epoch [1/2], Step [17780/67476], Loss: 4.4063\n",
      "Epoch [1/2], Step [17790/67476], Loss: 4.5815\n",
      "Epoch [1/2], Step [17800/67476], Loss: 4.4942\n",
      "Epoch [1/2], Step [17810/67476], Loss: 4.5448\n",
      "Epoch [1/2], Step [17820/67476], Loss: 4.5563\n",
      "Epoch [1/2], Step [17830/67476], Loss: 4.6821\n",
      "Epoch [1/2], Step [17840/67476], Loss: 4.5147\n",
      "Epoch [1/2], Step [17850/67476], Loss: 4.6770\n",
      "Epoch [1/2], Step [17860/67476], Loss: 4.7799\n",
      "Epoch [1/2], Step [17870/67476], Loss: 4.7457\n",
      "Epoch [1/2], Step [17880/67476], Loss: 4.5799\n",
      "Epoch [1/2], Step [17890/67476], Loss: 4.6035\n",
      "Epoch [1/2], Step [17900/67476], Loss: 4.4064\n",
      "Epoch [1/2], Step [17910/67476], Loss: 4.5161\n",
      "Epoch [1/2], Step [17920/67476], Loss: 4.4629\n",
      "Epoch [1/2], Step [17930/67476], Loss: 4.6631\n",
      "Epoch [1/2], Step [17940/67476], Loss: 4.6149\n",
      "Epoch [1/2], Step [17950/67476], Loss: 4.6225\n",
      "Epoch [1/2], Step [17960/67476], Loss: 4.5617\n",
      "Epoch [1/2], Step [17970/67476], Loss: 4.7170\n",
      "Epoch [1/2], Step [17980/67476], Loss: 4.5934\n",
      "Epoch [1/2], Step [17990/67476], Loss: 4.4541\n",
      "Epoch [1/2], Step [18000/67476], Loss: 4.4349\n",
      "Epoch [1/2], Step [18010/67476], Loss: 4.9007\n",
      "Epoch [1/2], Step [18020/67476], Loss: 4.5173\n",
      "Epoch [1/2], Step [18030/67476], Loss: 4.6984\n",
      "Epoch [1/2], Step [18040/67476], Loss: 4.5290\n",
      "Epoch [1/2], Step [18050/67476], Loss: 4.5968\n",
      "Epoch [1/2], Step [18060/67476], Loss: 4.2960\n",
      "Epoch [1/2], Step [18070/67476], Loss: 4.5545\n",
      "Epoch [1/2], Step [18080/67476], Loss: 4.5835\n",
      "Epoch [1/2], Step [18090/67476], Loss: 4.6998\n",
      "Epoch [1/2], Step [18100/67476], Loss: 4.5944\n",
      "Epoch [1/2], Step [18110/67476], Loss: 4.4432\n",
      "Epoch [1/2], Step [18120/67476], Loss: 4.4876\n",
      "Epoch [1/2], Step [18130/67476], Loss: 4.5367\n",
      "Epoch [1/2], Step [18140/67476], Loss: 4.6717\n",
      "Epoch [1/2], Step [18150/67476], Loss: 4.3312\n",
      "Epoch [1/2], Step [18160/67476], Loss: 4.5727\n",
      "Epoch [1/2], Step [18170/67476], Loss: 4.7482\n",
      "Epoch [1/2], Step [18180/67476], Loss: 4.6041\n",
      "Epoch [1/2], Step [18190/67476], Loss: 4.4963\n",
      "Epoch [1/2], Step [18200/67476], Loss: 4.4034\n",
      "Epoch [1/2], Step [18210/67476], Loss: 4.3487\n",
      "Epoch [1/2], Step [18220/67476], Loss: 4.5763\n",
      "Epoch [1/2], Step [18230/67476], Loss: 4.7180\n",
      "Epoch [1/2], Step [18240/67476], Loss: 4.7245\n",
      "Epoch [1/2], Step [18250/67476], Loss: 4.6215\n",
      "Epoch [1/2], Step [18260/67476], Loss: 4.5834\n",
      "Epoch [1/2], Step [18270/67476], Loss: 4.5659\n",
      "Epoch [1/2], Step [18280/67476], Loss: 4.7010\n",
      "Epoch [1/2], Step [18290/67476], Loss: 4.3267\n",
      "Epoch [1/2], Step [18300/67476], Loss: 4.6985\n",
      "Epoch [1/2], Step [18310/67476], Loss: 4.6352\n",
      "Epoch [1/2], Step [18320/67476], Loss: 4.5928\n",
      "Epoch [1/2], Step [18330/67476], Loss: 4.7521\n",
      "Epoch [1/2], Step [18340/67476], Loss: 4.5590\n",
      "Epoch [1/2], Step [18350/67476], Loss: 4.5316\n",
      "Epoch [1/2], Step [18360/67476], Loss: 4.5395\n",
      "Epoch [1/2], Step [18370/67476], Loss: 4.5582\n",
      "Epoch [1/2], Step [18380/67476], Loss: 4.5781\n",
      "Epoch [1/2], Step [18390/67476], Loss: 4.5866\n",
      "Epoch [1/2], Step [18400/67476], Loss: 4.5511\n",
      "Epoch [1/2], Step [18410/67476], Loss: 4.4354\n",
      "Epoch [1/2], Step [18420/67476], Loss: 4.7044\n",
      "Epoch [1/2], Step [18430/67476], Loss: 4.5378\n",
      "Epoch [1/2], Step [18440/67476], Loss: 4.5943\n",
      "Epoch [1/2], Step [18450/67476], Loss: 4.6483\n",
      "Epoch [1/2], Step [18460/67476], Loss: 4.5353\n",
      "Epoch [1/2], Step [18470/67476], Loss: 4.4600\n",
      "Epoch [1/2], Step [18480/67476], Loss: 4.7369\n",
      "Epoch [1/2], Step [18490/67476], Loss: 4.4156\n",
      "Epoch [1/2], Step [18500/67476], Loss: 4.6005\n",
      "Epoch [1/2], Step [18510/67476], Loss: 4.6632\n",
      "Epoch [1/2], Step [18520/67476], Loss: 4.8252\n",
      "Epoch [1/2], Step [18530/67476], Loss: 4.5162\n",
      "Epoch [1/2], Step [18540/67476], Loss: 4.3720\n",
      "Epoch [1/2], Step [18550/67476], Loss: 4.6027\n",
      "Epoch [1/2], Step [18560/67476], Loss: 4.6628\n",
      "Epoch [1/2], Step [18570/67476], Loss: 4.5837\n",
      "Epoch [1/2], Step [18580/67476], Loss: 4.5417\n",
      "Epoch [1/2], Step [18590/67476], Loss: 4.5606\n",
      "Epoch [1/2], Step [18600/67476], Loss: 4.5169\n",
      "Epoch [1/2], Step [18610/67476], Loss: 4.5494\n",
      "Epoch [1/2], Step [18620/67476], Loss: 4.5532\n",
      "Epoch [1/2], Step [18630/67476], Loss: 4.6024\n",
      "Epoch [1/2], Step [18640/67476], Loss: 4.6295\n",
      "Epoch [1/2], Step [18650/67476], Loss: 4.5225\n",
      "Epoch [1/2], Step [18660/67476], Loss: 4.7140\n",
      "Epoch [1/2], Step [18670/67476], Loss: 4.6410\n",
      "Epoch [1/2], Step [18680/67476], Loss: 4.7246\n",
      "Epoch [1/2], Step [18690/67476], Loss: 4.4843\n",
      "Epoch [1/2], Step [18700/67476], Loss: 4.5549\n",
      "Epoch [1/2], Step [18710/67476], Loss: 4.6046\n",
      "Epoch [1/2], Step [18720/67476], Loss: 4.6440\n",
      "Epoch [1/2], Step [18730/67476], Loss: 4.5507\n",
      "Epoch [1/2], Step [18740/67476], Loss: 4.8043\n",
      "Epoch [1/2], Step [18750/67476], Loss: 4.5232\n",
      "Epoch [1/2], Step [18760/67476], Loss: 4.3995\n",
      "Epoch [1/2], Step [18770/67476], Loss: 4.4640\n",
      "Epoch [1/2], Step [18780/67476], Loss: 4.5879\n",
      "Epoch [1/2], Step [18790/67476], Loss: 4.6752\n",
      "Epoch [1/2], Step [18800/67476], Loss: 4.6797\n",
      "Epoch [1/2], Step [18810/67476], Loss: 4.6336\n",
      "Epoch [1/2], Step [18820/67476], Loss: 4.6761\n",
      "Epoch [1/2], Step [18830/67476], Loss: 4.6926\n",
      "Epoch [1/2], Step [18840/67476], Loss: 4.4755\n",
      "Epoch [1/2], Step [18850/67476], Loss: 4.4256\n",
      "Epoch [1/2], Step [18860/67476], Loss: 4.5237\n",
      "Epoch [1/2], Step [18870/67476], Loss: 4.3548\n",
      "Epoch [1/2], Step [18880/67476], Loss: 4.6768\n",
      "Epoch [1/2], Step [18890/67476], Loss: 4.8338\n",
      "Epoch [1/2], Step [18900/67476], Loss: 4.3250\n",
      "Epoch [1/2], Step [18910/67476], Loss: 4.6094\n",
      "Epoch [1/2], Step [18920/67476], Loss: 4.3811\n",
      "Epoch [1/2], Step [18930/67476], Loss: 4.6870\n",
      "Epoch [1/2], Step [18940/67476], Loss: 4.5374\n",
      "Epoch [1/2], Step [18950/67476], Loss: 4.5399\n",
      "Epoch [1/2], Step [18960/67476], Loss: 4.5123\n",
      "Epoch [1/2], Step [18970/67476], Loss: 4.6078\n",
      "Epoch [1/2], Step [18980/67476], Loss: 4.5085\n",
      "Epoch [1/2], Step [18990/67476], Loss: 4.6191\n",
      "Epoch [1/2], Step [19000/67476], Loss: 4.4088\n",
      "Epoch [1/2], Step [19010/67476], Loss: 4.6451\n",
      "Epoch [1/2], Step [19020/67476], Loss: 4.5832\n",
      "Epoch [1/2], Step [19030/67476], Loss: 4.7895\n",
      "Epoch [1/2], Step [19040/67476], Loss: 4.9514\n",
      "Epoch [1/2], Step [19050/67476], Loss: 4.3990\n",
      "Epoch [1/2], Step [19060/67476], Loss: 4.4610\n",
      "Epoch [1/2], Step [19070/67476], Loss: 4.5758\n",
      "Epoch [1/2], Step [19080/67476], Loss: 4.5830\n",
      "Epoch [1/2], Step [19090/67476], Loss: 4.5658\n",
      "Epoch [1/2], Step [19100/67476], Loss: 4.4053\n",
      "Epoch [1/2], Step [19110/67476], Loss: 4.6163\n",
      "Epoch [1/2], Step [19120/67476], Loss: 4.8254\n",
      "Epoch [1/2], Step [19130/67476], Loss: 4.6823\n",
      "Epoch [1/2], Step [19140/67476], Loss: 4.5842\n",
      "Epoch [1/2], Step [19150/67476], Loss: 4.6337\n",
      "Epoch [1/2], Step [19160/67476], Loss: 4.4166\n",
      "Epoch [1/2], Step [19170/67476], Loss: 4.4128\n",
      "Epoch [1/2], Step [19180/67476], Loss: 4.5392\n",
      "Epoch [1/2], Step [19190/67476], Loss: 4.6620\n",
      "Epoch [1/2], Step [19200/67476], Loss: 4.4387\n",
      "Epoch [1/2], Step [19210/67476], Loss: 4.5644\n",
      "Epoch [1/2], Step [19220/67476], Loss: 4.8073\n",
      "Epoch [1/2], Step [19230/67476], Loss: 4.6504\n",
      "Epoch [1/2], Step [19240/67476], Loss: 4.5166\n",
      "Epoch [1/2], Step [19250/67476], Loss: 4.6811\n",
      "Epoch [1/2], Step [19260/67476], Loss: 4.8575\n",
      "Epoch [1/2], Step [19270/67476], Loss: 4.4855\n",
      "Epoch [1/2], Step [19280/67476], Loss: 4.6631\n",
      "Epoch [1/2], Step [19290/67476], Loss: 4.4973\n",
      "Epoch [1/2], Step [19300/67476], Loss: 4.7081\n",
      "Epoch [1/2], Step [19310/67476], Loss: 4.6310\n",
      "Epoch [1/2], Step [19320/67476], Loss: 4.5797\n",
      "Epoch [1/2], Step [19330/67476], Loss: 4.5453\n",
      "Epoch [1/2], Step [19340/67476], Loss: 4.4633\n",
      "Epoch [1/2], Step [19350/67476], Loss: 4.6378\n",
      "Epoch [1/2], Step [19360/67476], Loss: 4.5051\n",
      "Epoch [1/2], Step [19370/67476], Loss: 4.5096\n",
      "Epoch [1/2], Step [19380/67476], Loss: 4.5127\n",
      "Epoch [1/2], Step [19390/67476], Loss: 4.4661\n",
      "Epoch [1/2], Step [19400/67476], Loss: 4.4051\n",
      "Epoch [1/2], Step [19410/67476], Loss: 4.6396\n",
      "Epoch [1/2], Step [19420/67476], Loss: 4.6417\n",
      "Epoch [1/2], Step [19430/67476], Loss: 4.5439\n",
      "Epoch [1/2], Step [19440/67476], Loss: 4.4652\n",
      "Epoch [1/2], Step [19450/67476], Loss: 4.7090\n",
      "Epoch [1/2], Step [19460/67476], Loss: 4.5205\n",
      "Epoch [1/2], Step [19470/67476], Loss: 4.5331\n",
      "Epoch [1/2], Step [19480/67476], Loss: 4.6293\n",
      "Epoch [1/2], Step [19490/67476], Loss: 4.7262\n",
      "Epoch [1/2], Step [19500/67476], Loss: 4.3587\n",
      "Epoch [1/2], Step [19510/67476], Loss: 4.7293\n",
      "Epoch [1/2], Step [19520/67476], Loss: 4.7607\n",
      "Epoch [1/2], Step [19530/67476], Loss: 4.8000\n",
      "Epoch [1/2], Step [19540/67476], Loss: 4.6026\n",
      "Epoch [1/2], Step [19550/67476], Loss: 4.4163\n",
      "Epoch [1/2], Step [19560/67476], Loss: 4.6762\n",
      "Epoch [1/2], Step [19570/67476], Loss: 4.5250\n",
      "Epoch [1/2], Step [19580/67476], Loss: 4.4185\n",
      "Epoch [1/2], Step [19590/67476], Loss: 4.6171\n",
      "Epoch [1/2], Step [19600/67476], Loss: 4.5166\n",
      "Epoch [1/2], Step [19610/67476], Loss: 4.7611\n",
      "Epoch [1/2], Step [19620/67476], Loss: 4.6785\n",
      "Epoch [1/2], Step [19630/67476], Loss: 4.4674\n",
      "Epoch [1/2], Step [19640/67476], Loss: 4.4726\n",
      "Epoch [1/2], Step [19650/67476], Loss: 4.5831\n",
      "Epoch [1/2], Step [19660/67476], Loss: 4.5750\n",
      "Epoch [1/2], Step [19670/67476], Loss: 4.6041\n",
      "Epoch [1/2], Step [19680/67476], Loss: 4.4249\n",
      "Epoch [1/2], Step [19690/67476], Loss: 4.6035\n",
      "Epoch [1/2], Step [19700/67476], Loss: 4.5475\n",
      "Epoch [1/2], Step [19710/67476], Loss: 4.5587\n",
      "Epoch [1/2], Step [19720/67476], Loss: 4.6866\n",
      "Epoch [1/2], Step [19730/67476], Loss: 4.4522\n",
      "Epoch [1/2], Step [19740/67476], Loss: 4.8103\n",
      "Epoch [1/2], Step [19750/67476], Loss: 4.7133\n",
      "Epoch [1/2], Step [19760/67476], Loss: 4.6404\n",
      "Epoch [1/2], Step [19770/67476], Loss: 4.3688\n",
      "Epoch [1/2], Step [19780/67476], Loss: 4.3948\n",
      "Epoch [1/2], Step [19790/67476], Loss: 4.7835\n",
      "Epoch [1/2], Step [19800/67476], Loss: 4.4940\n",
      "Epoch [1/2], Step [19810/67476], Loss: 4.5982\n",
      "Epoch [1/2], Step [19820/67476], Loss: 4.7974\n",
      "Epoch [1/2], Step [19830/67476], Loss: 4.7089\n",
      "Epoch [1/2], Step [19840/67476], Loss: 4.6532\n",
      "Epoch [1/2], Step [19850/67476], Loss: 4.5593\n",
      "Epoch [1/2], Step [19860/67476], Loss: 4.4159\n",
      "Epoch [1/2], Step [19870/67476], Loss: 4.5828\n",
      "Epoch [1/2], Step [19880/67476], Loss: 4.5736\n",
      "Epoch [1/2], Step [19890/67476], Loss: 4.5300\n",
      "Epoch [1/2], Step [19900/67476], Loss: 4.7231\n",
      "Epoch [1/2], Step [19910/67476], Loss: 4.5610\n",
      "Epoch [1/2], Step [19920/67476], Loss: 4.5046\n",
      "Epoch [1/2], Step [19930/67476], Loss: 4.6742\n",
      "Epoch [1/2], Step [19940/67476], Loss: 4.5665\n",
      "Epoch [1/2], Step [19950/67476], Loss: 4.5644\n",
      "Epoch [1/2], Step [19960/67476], Loss: 4.6524\n",
      "Epoch [1/2], Step [19970/67476], Loss: 4.5124\n",
      "Epoch [1/2], Step [19980/67476], Loss: 4.6366\n",
      "Epoch [1/2], Step [19990/67476], Loss: 4.4929\n",
      "Epoch [1/2], Step [20000/67476], Loss: 4.6867\n",
      "Epoch [1/2], Step [20010/67476], Loss: 4.5488\n",
      "Epoch [1/2], Step [20020/67476], Loss: 4.5733\n",
      "Epoch [1/2], Step [20030/67476], Loss: 4.6180\n",
      "Epoch [1/2], Step [20040/67476], Loss: 4.6816\n",
      "Epoch [1/2], Step [20050/67476], Loss: 4.7717\n",
      "Epoch [1/2], Step [20060/67476], Loss: 4.5646\n",
      "Epoch [1/2], Step [20070/67476], Loss: 4.4179\n",
      "Epoch [1/2], Step [20080/67476], Loss: 4.6244\n",
      "Epoch [1/2], Step [20090/67476], Loss: 4.5478\n",
      "Epoch [1/2], Step [20100/67476], Loss: 4.6793\n",
      "Epoch [1/2], Step [20110/67476], Loss: 4.6129\n",
      "Epoch [1/2], Step [20120/67476], Loss: 4.7704\n",
      "Epoch [1/2], Step [20130/67476], Loss: 4.7481\n",
      "Epoch [1/2], Step [20140/67476], Loss: 4.7082\n",
      "Epoch [1/2], Step [20150/67476], Loss: 4.5514\n",
      "Epoch [1/2], Step [20160/67476], Loss: 4.4781\n",
      "Epoch [1/2], Step [20170/67476], Loss: 4.5447\n",
      "Epoch [1/2], Step [20180/67476], Loss: 4.3591\n",
      "Epoch [1/2], Step [20190/67476], Loss: 4.6300\n",
      "Epoch [1/2], Step [20200/67476], Loss: 4.5092\n",
      "Epoch [1/2], Step [20210/67476], Loss: 4.7566\n",
      "Epoch [1/2], Step [20220/67476], Loss: 4.6018\n",
      "Epoch [1/2], Step [20230/67476], Loss: 4.7714\n",
      "Epoch [1/2], Step [20240/67476], Loss: 4.4745\n",
      "Epoch [1/2], Step [20250/67476], Loss: 4.7783\n",
      "Epoch [1/2], Step [20260/67476], Loss: 4.4904\n",
      "Epoch [1/2], Step [20270/67476], Loss: 4.5950\n",
      "Epoch [1/2], Step [20280/67476], Loss: 4.6271\n",
      "Epoch [1/2], Step [20290/67476], Loss: 4.8991\n",
      "Epoch [1/2], Step [20300/67476], Loss: 4.6526\n",
      "Epoch [1/2], Step [20310/67476], Loss: 4.4168\n",
      "Epoch [1/2], Step [20320/67476], Loss: 4.5486\n",
      "Epoch [1/2], Step [20330/67476], Loss: 4.5499\n",
      "Epoch [1/2], Step [20340/67476], Loss: 4.4422\n",
      "Epoch [1/2], Step [20350/67476], Loss: 4.5103\n",
      "Epoch [1/2], Step [20360/67476], Loss: 4.5254\n",
      "Epoch [1/2], Step [20370/67476], Loss: 4.5307\n",
      "Epoch [1/2], Step [20380/67476], Loss: 4.8001\n",
      "Epoch [1/2], Step [20390/67476], Loss: 4.7779\n",
      "Epoch [1/2], Step [20400/67476], Loss: 4.6606\n",
      "Epoch [1/2], Step [20410/67476], Loss: 4.5274\n",
      "Epoch [1/2], Step [20420/67476], Loss: 4.7243\n",
      "Epoch [1/2], Step [20430/67476], Loss: 4.5698\n",
      "Epoch [1/2], Step [20440/67476], Loss: 4.5605\n",
      "Epoch [1/2], Step [20450/67476], Loss: 4.5254\n",
      "Epoch [1/2], Step [20460/67476], Loss: 4.6024\n",
      "Epoch [1/2], Step [20470/67476], Loss: 4.3891\n",
      "Epoch [1/2], Step [20480/67476], Loss: 4.6101\n",
      "Epoch [1/2], Step [20490/67476], Loss: 4.4004\n",
      "Epoch [1/2], Step [20500/67476], Loss: 4.7241\n",
      "Epoch [1/2], Step [20510/67476], Loss: 4.5392\n",
      "Epoch [1/2], Step [20520/67476], Loss: 4.5445\n",
      "Epoch [1/2], Step [20530/67476], Loss: 4.5807\n",
      "Epoch [1/2], Step [20540/67476], Loss: 4.6304\n",
      "Epoch [1/2], Step [20550/67476], Loss: 4.5185\n",
      "Epoch [1/2], Step [20560/67476], Loss: 4.6889\n",
      "Epoch [1/2], Step [20570/67476], Loss: 4.5390\n",
      "Epoch [1/2], Step [20580/67476], Loss: 4.6329\n",
      "Epoch [1/2], Step [20590/67476], Loss: 4.4253\n",
      "Epoch [1/2], Step [20600/67476], Loss: 4.6121\n",
      "Epoch [1/2], Step [20610/67476], Loss: 4.4967\n",
      "Epoch [1/2], Step [20620/67476], Loss: 4.5514\n",
      "Epoch [1/2], Step [20630/67476], Loss: 4.8207\n",
      "Epoch [1/2], Step [20640/67476], Loss: 4.7384\n",
      "Epoch [1/2], Step [20650/67476], Loss: 4.7500\n",
      "Epoch [1/2], Step [20660/67476], Loss: 4.7476\n",
      "Epoch [1/2], Step [20670/67476], Loss: 4.5435\n",
      "Epoch [1/2], Step [20680/67476], Loss: 4.5526\n",
      "Epoch [1/2], Step [20690/67476], Loss: 4.4866\n",
      "Epoch [1/2], Step [20700/67476], Loss: 4.7014\n",
      "Epoch [1/2], Step [20710/67476], Loss: 4.4659\n",
      "Epoch [1/2], Step [20720/67476], Loss: 4.5848\n",
      "Epoch [1/2], Step [20730/67476], Loss: 4.7878\n",
      "Epoch [1/2], Step [20740/67476], Loss: 4.4812\n",
      "Epoch [1/2], Step [20750/67476], Loss: 4.7975\n",
      "Epoch [1/2], Step [20760/67476], Loss: 4.5503\n",
      "Epoch [1/2], Step [20770/67476], Loss: 4.9559\n",
      "Epoch [1/2], Step [20780/67476], Loss: 4.6313\n",
      "Epoch [1/2], Step [20790/67476], Loss: 4.7016\n",
      "Epoch [1/2], Step [20800/67476], Loss: 4.5915\n",
      "Epoch [1/2], Step [20810/67476], Loss: 4.5226\n",
      "Epoch [1/2], Step [20820/67476], Loss: 4.5679\n",
      "Epoch [1/2], Step [20830/67476], Loss: 4.5261\n",
      "Epoch [1/2], Step [20840/67476], Loss: 4.4987\n",
      "Epoch [1/2], Step [20850/67476], Loss: 4.5979\n",
      "Epoch [1/2], Step [20860/67476], Loss: 4.5113\n",
      "Epoch [1/2], Step [20870/67476], Loss: 4.6389\n",
      "Epoch [1/2], Step [20880/67476], Loss: 4.6682\n",
      "Epoch [1/2], Step [20890/67476], Loss: 4.6333\n",
      "Epoch [1/2], Step [20900/67476], Loss: 4.6987\n",
      "Epoch [1/2], Step [20910/67476], Loss: 4.4561\n",
      "Epoch [1/2], Step [20920/67476], Loss: 4.5983\n",
      "Epoch [1/2], Step [20930/67476], Loss: 4.4198\n",
      "Epoch [1/2], Step [20940/67476], Loss: 4.7351\n",
      "Epoch [1/2], Step [20950/67476], Loss: 4.7014\n",
      "Epoch [1/2], Step [20960/67476], Loss: 4.6993\n",
      "Epoch [1/2], Step [20970/67476], Loss: 4.7749\n",
      "Epoch [1/2], Step [20980/67476], Loss: 4.6025\n",
      "Epoch [1/2], Step [20990/67476], Loss: 4.6897\n",
      "Epoch [1/2], Step [21000/67476], Loss: 4.7040\n",
      "Epoch [1/2], Step [21010/67476], Loss: 4.5868\n",
      "Epoch [1/2], Step [21020/67476], Loss: 4.4808\n",
      "Epoch [1/2], Step [21030/67476], Loss: 4.5421\n",
      "Epoch [1/2], Step [21040/67476], Loss: 4.7901\n",
      "Epoch [1/2], Step [21050/67476], Loss: 4.4955\n",
      "Epoch [1/2], Step [21060/67476], Loss: 4.7689\n",
      "Epoch [1/2], Step [21070/67476], Loss: 4.6848\n",
      "Epoch [1/2], Step [21080/67476], Loss: 4.3273\n",
      "Epoch [1/2], Step [21090/67476], Loss: 4.7322\n",
      "Epoch [1/2], Step [21100/67476], Loss: 4.3395\n",
      "Epoch [1/2], Step [21110/67476], Loss: 4.4661\n",
      "Epoch [1/2], Step [21120/67476], Loss: 4.3882\n",
      "Epoch [1/2], Step [21130/67476], Loss: 4.7133\n",
      "Epoch [1/2], Step [21140/67476], Loss: 4.5448\n",
      "Epoch [1/2], Step [21150/67476], Loss: 4.6012\n",
      "Epoch [1/2], Step [21160/67476], Loss: 4.5249\n",
      "Epoch [1/2], Step [21170/67476], Loss: 4.7017\n",
      "Epoch [1/2], Step [21180/67476], Loss: 4.6273\n",
      "Epoch [1/2], Step [21190/67476], Loss: 4.5707\n",
      "Epoch [1/2], Step [21200/67476], Loss: 4.7999\n",
      "Epoch [1/2], Step [21210/67476], Loss: 4.5119\n",
      "Epoch [1/2], Step [21220/67476], Loss: 4.7591\n",
      "Epoch [1/2], Step [21230/67476], Loss: 4.5345\n",
      "Epoch [1/2], Step [21240/67476], Loss: 4.4957\n",
      "Epoch [1/2], Step [21250/67476], Loss: 4.6202\n",
      "Epoch [1/2], Step [21260/67476], Loss: 4.5965\n",
      "Epoch [1/2], Step [21270/67476], Loss: 4.5190\n",
      "Epoch [1/2], Step [21280/67476], Loss: 4.6127\n",
      "Epoch [1/2], Step [21290/67476], Loss: 4.6150\n",
      "Epoch [1/2], Step [21300/67476], Loss: 4.6037\n",
      "Epoch [1/2], Step [21310/67476], Loss: 4.6068\n",
      "Epoch [1/2], Step [21320/67476], Loss: 4.5697\n",
      "Epoch [1/2], Step [21330/67476], Loss: 4.4993\n",
      "Epoch [1/2], Step [21340/67476], Loss: 4.5288\n",
      "Epoch [1/2], Step [21350/67476], Loss: 4.5916\n",
      "Epoch [1/2], Step [21360/67476], Loss: 4.6424\n",
      "Epoch [1/2], Step [21370/67476], Loss: 4.3675\n",
      "Epoch [1/2], Step [21380/67476], Loss: 4.6620\n",
      "Epoch [1/2], Step [21390/67476], Loss: 4.6012\n",
      "Epoch [1/2], Step [21400/67476], Loss: 4.7731\n",
      "Epoch [1/2], Step [21410/67476], Loss: 4.4906\n",
      "Epoch [1/2], Step [21420/67476], Loss: 4.6626\n",
      "Epoch [1/2], Step [21430/67476], Loss: 4.6291\n",
      "Epoch [1/2], Step [21440/67476], Loss: 4.5645\n",
      "Epoch [1/2], Step [21450/67476], Loss: 4.6500\n",
      "Epoch [1/2], Step [21460/67476], Loss: 4.6252\n",
      "Epoch [1/2], Step [21470/67476], Loss: 4.4586\n",
      "Epoch [1/2], Step [21480/67476], Loss: 4.7729\n",
      "Epoch [1/2], Step [21490/67476], Loss: 4.4471\n",
      "Epoch [1/2], Step [21500/67476], Loss: 4.5340\n",
      "Epoch [1/2], Step [21510/67476], Loss: 4.4994\n",
      "Epoch [1/2], Step [21520/67476], Loss: 4.7616\n",
      "Epoch [1/2], Step [21530/67476], Loss: 4.4026\n",
      "Epoch [1/2], Step [21540/67476], Loss: 4.7619\n",
      "Epoch [1/2], Step [21550/67476], Loss: 4.6229\n",
      "Epoch [1/2], Step [21560/67476], Loss: 4.3283\n",
      "Epoch [1/2], Step [21570/67476], Loss: 4.4064\n",
      "Epoch [1/2], Step [21580/67476], Loss: 4.6535\n",
      "Epoch [1/2], Step [21590/67476], Loss: 4.6636\n",
      "Epoch [1/2], Step [21600/67476], Loss: 4.5453\n",
      "Epoch [1/2], Step [21610/67476], Loss: 4.4373\n",
      "Epoch [1/2], Step [21620/67476], Loss: 4.3680\n",
      "Epoch [1/2], Step [21630/67476], Loss: 4.6466\n",
      "Epoch [1/2], Step [21640/67476], Loss: 4.5153\n",
      "Epoch [1/2], Step [21650/67476], Loss: 4.5227\n",
      "Epoch [1/2], Step [21660/67476], Loss: 4.5398\n",
      "Epoch [1/2], Step [21670/67476], Loss: 4.4626\n",
      "Epoch [1/2], Step [21680/67476], Loss: 4.5724\n",
      "Epoch [1/2], Step [21690/67476], Loss: 4.4970\n",
      "Epoch [1/2], Step [21700/67476], Loss: 4.4551\n",
      "Epoch [1/2], Step [21710/67476], Loss: 4.7904\n",
      "Epoch [1/2], Step [21720/67476], Loss: 4.4950\n",
      "Epoch [1/2], Step [21730/67476], Loss: 4.6407\n",
      "Epoch [1/2], Step [21740/67476], Loss: 4.7087\n",
      "Epoch [1/2], Step [21750/67476], Loss: 4.3849\n",
      "Epoch [1/2], Step [21760/67476], Loss: 4.4232\n",
      "Epoch [1/2], Step [21770/67476], Loss: 4.6448\n",
      "Epoch [1/2], Step [21780/67476], Loss: 4.7149\n",
      "Epoch [1/2], Step [21790/67476], Loss: 4.4460\n",
      "Epoch [1/2], Step [21800/67476], Loss: 4.6580\n",
      "Epoch [1/2], Step [21810/67476], Loss: 4.6585\n",
      "Epoch [1/2], Step [21820/67476], Loss: 4.7239\n",
      "Epoch [1/2], Step [21830/67476], Loss: 4.6100\n",
      "Epoch [1/2], Step [21840/67476], Loss: 4.4863\n",
      "Epoch [1/2], Step [21850/67476], Loss: 4.6213\n",
      "Epoch [1/2], Step [21860/67476], Loss: 4.4986\n",
      "Epoch [1/2], Step [21870/67476], Loss: 4.5554\n",
      "Epoch [1/2], Step [21880/67476], Loss: 4.7290\n",
      "Epoch [1/2], Step [21890/67476], Loss: 4.4897\n",
      "Epoch [1/2], Step [21900/67476], Loss: 4.7322\n",
      "Epoch [1/2], Step [21910/67476], Loss: 4.6436\n",
      "Epoch [1/2], Step [21920/67476], Loss: 4.6299\n",
      "Epoch [1/2], Step [21930/67476], Loss: 4.5966\n",
      "Epoch [1/2], Step [21940/67476], Loss: 4.7138\n",
      "Epoch [1/2], Step [21950/67476], Loss: 4.5574\n",
      "Epoch [1/2], Step [21960/67476], Loss: 4.5870\n",
      "Epoch [1/2], Step [21970/67476], Loss: 4.6890\n",
      "Epoch [1/2], Step [21980/67476], Loss: 4.4533\n",
      "Epoch [1/2], Step [21990/67476], Loss: 4.4717\n",
      "Epoch [1/2], Step [22000/67476], Loss: 4.6195\n",
      "Epoch [1/2], Step [22010/67476], Loss: 4.4869\n",
      "Epoch [1/2], Step [22020/67476], Loss: 4.7592\n",
      "Epoch [1/2], Step [22030/67476], Loss: 4.5753\n",
      "Epoch [1/2], Step [22040/67476], Loss: 4.6886\n",
      "Epoch [1/2], Step [22050/67476], Loss: 4.6789\n",
      "Epoch [1/2], Step [22060/67476], Loss: 4.4665\n",
      "Epoch [1/2], Step [22070/67476], Loss: 4.6589\n",
      "Epoch [1/2], Step [22080/67476], Loss: 4.6267\n",
      "Epoch [1/2], Step [22090/67476], Loss: 4.4921\n",
      "Epoch [1/2], Step [22100/67476], Loss: 4.6231\n",
      "Epoch [1/2], Step [22110/67476], Loss: 4.6887\n",
      "Epoch [1/2], Step [22120/67476], Loss: 4.7726\n",
      "Epoch [1/2], Step [22130/67476], Loss: 4.6369\n",
      "Epoch [1/2], Step [22140/67476], Loss: 4.5386\n",
      "Epoch [1/2], Step [22150/67476], Loss: 4.5677\n",
      "Epoch [1/2], Step [22160/67476], Loss: 4.6225\n",
      "Epoch [1/2], Step [22170/67476], Loss: 4.5217\n",
      "Epoch [1/2], Step [22180/67476], Loss: 4.4072\n",
      "Epoch [1/2], Step [22190/67476], Loss: 4.4491\n",
      "Epoch [1/2], Step [22200/67476], Loss: 4.6838\n",
      "Epoch [1/2], Step [22210/67476], Loss: 4.4594\n",
      "Epoch [1/2], Step [22220/67476], Loss: 4.7988\n",
      "Epoch [1/2], Step [22230/67476], Loss: 4.8687\n",
      "Epoch [1/2], Step [22240/67476], Loss: 4.6345\n",
      "Epoch [1/2], Step [22250/67476], Loss: 4.5164\n",
      "Epoch [1/2], Step [22260/67476], Loss: 4.4254\n",
      "Epoch [1/2], Step [22270/67476], Loss: 4.5760\n",
      "Epoch [1/2], Step [22280/67476], Loss: 4.4342\n",
      "Epoch [1/2], Step [22290/67476], Loss: 4.5321\n",
      "Epoch [1/2], Step [22300/67476], Loss: 4.2191\n",
      "Epoch [1/2], Step [22310/67476], Loss: 4.6485\n",
      "Epoch [1/2], Step [22320/67476], Loss: 4.3940\n",
      "Epoch [1/2], Step [22330/67476], Loss: 4.7734\n",
      "Epoch [1/2], Step [22340/67476], Loss: 4.5297\n",
      "Epoch [1/2], Step [22350/67476], Loss: 4.6533\n",
      "Epoch [1/2], Step [22360/67476], Loss: 4.6699\n",
      "Epoch [1/2], Step [22370/67476], Loss: 4.5537\n",
      "Epoch [1/2], Step [22380/67476], Loss: 4.4016\n",
      "Epoch [1/2], Step [22390/67476], Loss: 4.5337\n",
      "Epoch [1/2], Step [22400/67476], Loss: 4.5477\n",
      "Epoch [1/2], Step [22410/67476], Loss: 4.7911\n",
      "Epoch [1/2], Step [22420/67476], Loss: 4.2641\n",
      "Epoch [1/2], Step [22430/67476], Loss: 4.3917\n",
      "Epoch [1/2], Step [22440/67476], Loss: 4.7335\n",
      "Epoch [1/2], Step [22450/67476], Loss: 4.4857\n",
      "Epoch [1/2], Step [22460/67476], Loss: 4.5896\n",
      "Epoch [1/2], Step [22470/67476], Loss: 4.5432\n",
      "Epoch [1/2], Step [22480/67476], Loss: 4.6055\n",
      "Epoch [1/2], Step [22490/67476], Loss: 4.5487\n",
      "Epoch [1/2], Step [22500/67476], Loss: 4.6815\n",
      "Epoch [1/2], Step [22510/67476], Loss: 4.7375\n",
      "Epoch [1/2], Step [22520/67476], Loss: 4.4380\n",
      "Epoch [1/2], Step [22530/67476], Loss: 4.8231\n",
      "Epoch [1/2], Step [22540/67476], Loss: 4.3978\n",
      "Epoch [1/2], Step [22550/67476], Loss: 4.7355\n",
      "Epoch [1/2], Step [22560/67476], Loss: 4.5633\n",
      "Epoch [1/2], Step [22570/67476], Loss: 4.4942\n",
      "Epoch [1/2], Step [22580/67476], Loss: 4.5180\n",
      "Epoch [1/2], Step [22590/67476], Loss: 4.5016\n",
      "Epoch [1/2], Step [22600/67476], Loss: 4.5221\n",
      "Epoch [1/2], Step [22610/67476], Loss: 4.5360\n",
      "Epoch [1/2], Step [22620/67476], Loss: 4.6660\n",
      "Epoch [1/2], Step [22630/67476], Loss: 4.6595\n",
      "Epoch [1/2], Step [22640/67476], Loss: 4.3319\n",
      "Epoch [1/2], Step [22650/67476], Loss: 4.6901\n",
      "Epoch [1/2], Step [22660/67476], Loss: 4.7409\n",
      "Epoch [1/2], Step [22670/67476], Loss: 4.6207\n",
      "Epoch [1/2], Step [22680/67476], Loss: 4.7932\n",
      "Epoch [1/2], Step [22690/67476], Loss: 4.6732\n",
      "Epoch [1/2], Step [22700/67476], Loss: 4.5477\n",
      "Epoch [1/2], Step [22710/67476], Loss: 4.5100\n",
      "Epoch [1/2], Step [22720/67476], Loss: 4.5322\n",
      "Epoch [1/2], Step [22730/67476], Loss: 4.7246\n",
      "Epoch [1/2], Step [22740/67476], Loss: 4.8170\n",
      "Epoch [1/2], Step [22750/67476], Loss: 4.8526\n",
      "Epoch [1/2], Step [22760/67476], Loss: 4.6253\n",
      "Epoch [1/2], Step [22770/67476], Loss: 4.7520\n",
      "Epoch [1/2], Step [22780/67476], Loss: 4.6835\n",
      "Epoch [1/2], Step [22790/67476], Loss: 4.5875\n",
      "Epoch [1/2], Step [22800/67476], Loss: 4.5653\n",
      "Epoch [1/2], Step [22810/67476], Loss: 4.5772\n",
      "Epoch [1/2], Step [22820/67476], Loss: 4.7235\n",
      "Epoch [1/2], Step [22830/67476], Loss: 4.5314\n",
      "Epoch [1/2], Step [22840/67476], Loss: 4.4356\n",
      "Epoch [1/2], Step [22850/67476], Loss: 4.6181\n",
      "Epoch [1/2], Step [22860/67476], Loss: 4.5061\n",
      "Epoch [1/2], Step [22870/67476], Loss: 4.4635\n",
      "Epoch [1/2], Step [22880/67476], Loss: 4.5512\n",
      "Epoch [1/2], Step [22890/67476], Loss: 4.6556\n",
      "Epoch [1/2], Step [22900/67476], Loss: 4.5425\n",
      "Epoch [1/2], Step [22910/67476], Loss: 4.5900\n",
      "Epoch [1/2], Step [22920/67476], Loss: 4.7089\n",
      "Epoch [1/2], Step [22930/67476], Loss: 4.4441\n",
      "Epoch [1/2], Step [22940/67476], Loss: 4.8297\n",
      "Epoch [1/2], Step [22950/67476], Loss: 4.8661\n",
      "Epoch [1/2], Step [22960/67476], Loss: 4.5477\n",
      "Epoch [1/2], Step [22970/67476], Loss: 4.6851\n",
      "Epoch [1/2], Step [22980/67476], Loss: 4.5075\n",
      "Epoch [1/2], Step [22990/67476], Loss: 4.4911\n",
      "Epoch [1/2], Step [23000/67476], Loss: 4.7733\n",
      "Epoch [1/2], Step [23010/67476], Loss: 4.6177\n",
      "Epoch [1/2], Step [23020/67476], Loss: 4.7511\n",
      "Epoch [1/2], Step [23030/67476], Loss: 4.6591\n",
      "Epoch [1/2], Step [23040/67476], Loss: 4.5713\n",
      "Epoch [1/2], Step [23050/67476], Loss: 4.6545\n",
      "Epoch [1/2], Step [23060/67476], Loss: 4.4857\n",
      "Epoch [1/2], Step [23070/67476], Loss: 4.4158\n",
      "Epoch [1/2], Step [23080/67476], Loss: 4.5988\n",
      "Epoch [1/2], Step [23090/67476], Loss: 4.5223\n",
      "Epoch [1/2], Step [23100/67476], Loss: 4.6444\n",
      "Epoch [1/2], Step [23110/67476], Loss: 4.6623\n",
      "Epoch [1/2], Step [23120/67476], Loss: 4.3739\n",
      "Epoch [1/2], Step [23130/67476], Loss: 4.6749\n",
      "Epoch [1/2], Step [23140/67476], Loss: 4.2867\n",
      "Epoch [1/2], Step [23150/67476], Loss: 4.9165\n",
      "Epoch [1/2], Step [23160/67476], Loss: 4.4456\n",
      "Epoch [1/2], Step [23170/67476], Loss: 4.7045\n",
      "Epoch [1/2], Step [23180/67476], Loss: 4.4477\n",
      "Epoch [1/2], Step [23190/67476], Loss: 4.3206\n",
      "Epoch [1/2], Step [23200/67476], Loss: 4.7697\n",
      "Epoch [1/2], Step [23210/67476], Loss: 4.7355\n",
      "Epoch [1/2], Step [23220/67476], Loss: 4.3217\n",
      "Epoch [1/2], Step [23230/67476], Loss: 4.6771\n",
      "Epoch [1/2], Step [23240/67476], Loss: 4.6344\n",
      "Epoch [1/2], Step [23250/67476], Loss: 4.8280\n",
      "Epoch [1/2], Step [23260/67476], Loss: 4.5148\n",
      "Epoch [1/2], Step [23270/67476], Loss: 4.6377\n",
      "Epoch [1/2], Step [23280/67476], Loss: 4.5909\n",
      "Epoch [1/2], Step [23290/67476], Loss: 4.5730\n",
      "Epoch [1/2], Step [23300/67476], Loss: 4.5273\n",
      "Epoch [1/2], Step [23310/67476], Loss: 4.7365\n",
      "Epoch [1/2], Step [23320/67476], Loss: 4.7010\n",
      "Epoch [1/2], Step [23330/67476], Loss: 4.4931\n",
      "Epoch [1/2], Step [23340/67476], Loss: 4.6795\n",
      "Epoch [1/2], Step [23350/67476], Loss: 4.6492\n",
      "Epoch [1/2], Step [23360/67476], Loss: 4.6979\n",
      "Epoch [1/2], Step [23370/67476], Loss: 4.6571\n",
      "Epoch [1/2], Step [23380/67476], Loss: 4.5724\n",
      "Epoch [1/2], Step [23390/67476], Loss: 4.5513\n",
      "Epoch [1/2], Step [23400/67476], Loss: 4.5658\n",
      "Epoch [1/2], Step [23410/67476], Loss: 4.6961\n",
      "Epoch [1/2], Step [23420/67476], Loss: 4.4892\n",
      "Epoch [1/2], Step [23430/67476], Loss: 4.3834\n",
      "Epoch [1/2], Step [23440/67476], Loss: 4.6105\n",
      "Epoch [1/2], Step [23450/67476], Loss: 4.5689\n",
      "Epoch [1/2], Step [23460/67476], Loss: 4.5906\n",
      "Epoch [1/2], Step [23470/67476], Loss: 4.5420\n",
      "Epoch [1/2], Step [23480/67476], Loss: 4.7320\n",
      "Epoch [1/2], Step [23490/67476], Loss: 4.8646\n",
      "Epoch [1/2], Step [23500/67476], Loss: 4.6288\n",
      "Epoch [1/2], Step [23510/67476], Loss: 4.5992\n",
      "Epoch [1/2], Step [23520/67476], Loss: 4.5134\n",
      "Epoch [1/2], Step [23530/67476], Loss: 4.8024\n",
      "Epoch [1/2], Step [23540/67476], Loss: 4.5729\n",
      "Epoch [1/2], Step [23550/67476], Loss: 4.5926\n",
      "Epoch [1/2], Step [23560/67476], Loss: 4.4577\n",
      "Epoch [1/2], Step [23570/67476], Loss: 4.5637\n",
      "Epoch [1/2], Step [23580/67476], Loss: 4.6481\n",
      "Epoch [1/2], Step [23590/67476], Loss: 4.4746\n",
      "Epoch [1/2], Step [23600/67476], Loss: 4.6493\n",
      "Epoch [1/2], Step [23610/67476], Loss: 4.7069\n",
      "Epoch [1/2], Step [23620/67476], Loss: 4.7336\n",
      "Epoch [1/2], Step [23630/67476], Loss: 4.8034\n",
      "Epoch [1/2], Step [23640/67476], Loss: 4.5375\n",
      "Epoch [1/2], Step [23650/67476], Loss: 4.4674\n",
      "Epoch [1/2], Step [23660/67476], Loss: 4.6716\n",
      "Epoch [1/2], Step [23670/67476], Loss: 4.4993\n",
      "Epoch [1/2], Step [23680/67476], Loss: 4.5437\n",
      "Epoch [1/2], Step [23690/67476], Loss: 4.5588\n",
      "Epoch [1/2], Step [23700/67476], Loss: 4.5682\n",
      "Epoch [1/2], Step [23710/67476], Loss: 4.6820\n",
      "Epoch [1/2], Step [23720/67476], Loss: 4.4812\n",
      "Epoch [1/2], Step [23730/67476], Loss: 4.8274\n",
      "Epoch [1/2], Step [23740/67476], Loss: 4.7564\n",
      "Epoch [1/2], Step [23750/67476], Loss: 4.3791\n",
      "Epoch [1/2], Step [23760/67476], Loss: 4.5255\n",
      "Epoch [1/2], Step [23770/67476], Loss: 4.8644\n",
      "Epoch [1/2], Step [23780/67476], Loss: 4.8655\n",
      "Epoch [1/2], Step [23790/67476], Loss: 4.4899\n",
      "Epoch [1/2], Step [23800/67476], Loss: 4.8558\n",
      "Epoch [1/2], Step [23810/67476], Loss: 4.5678\n",
      "Epoch [1/2], Step [23820/67476], Loss: 4.5632\n",
      "Epoch [1/2], Step [23830/67476], Loss: 4.6006\n",
      "Epoch [1/2], Step [23840/67476], Loss: 4.6562\n",
      "Epoch [1/2], Step [23850/67476], Loss: 4.4980\n",
      "Epoch [1/2], Step [23860/67476], Loss: 4.6595\n",
      "Epoch [1/2], Step [23870/67476], Loss: 4.7481\n",
      "Epoch [1/2], Step [23880/67476], Loss: 4.6171\n",
      "Epoch [1/2], Step [23890/67476], Loss: 4.2779\n",
      "Epoch [1/2], Step [23900/67476], Loss: 4.8209\n",
      "Epoch [1/2], Step [23910/67476], Loss: 4.4882\n",
      "Epoch [1/2], Step [23920/67476], Loss: 4.6066\n",
      "Epoch [1/2], Step [23930/67476], Loss: 4.4674\n",
      "Epoch [1/2], Step [23940/67476], Loss: 4.6457\n",
      "Epoch [1/2], Step [23950/67476], Loss: 4.3642\n",
      "Epoch [1/2], Step [23960/67476], Loss: 4.5941\n",
      "Epoch [1/2], Step [23970/67476], Loss: 4.6104\n",
      "Epoch [1/2], Step [23980/67476], Loss: 4.6249\n",
      "Epoch [1/2], Step [23990/67476], Loss: 4.5818\n",
      "Epoch [1/2], Step [24000/67476], Loss: 4.6663\n",
      "Epoch [1/2], Step [24010/67476], Loss: 4.4694\n",
      "Epoch [1/2], Step [24020/67476], Loss: 4.5794\n",
      "Epoch [1/2], Step [24030/67476], Loss: 4.6258\n",
      "Epoch [1/2], Step [24040/67476], Loss: 4.7051\n",
      "Epoch [1/2], Step [24050/67476], Loss: 4.6655\n",
      "Epoch [1/2], Step [24060/67476], Loss: 4.6016\n",
      "Epoch [1/2], Step [24070/67476], Loss: 4.5052\n",
      "Epoch [1/2], Step [24080/67476], Loss: 4.5606\n",
      "Epoch [1/2], Step [24090/67476], Loss: 4.6169\n",
      "Epoch [1/2], Step [24100/67476], Loss: 4.7020\n",
      "Epoch [1/2], Step [24110/67476], Loss: 4.6462\n",
      "Epoch [1/2], Step [24120/67476], Loss: 4.6188\n",
      "Epoch [1/2], Step [24130/67476], Loss: 4.7635\n",
      "Epoch [1/2], Step [24140/67476], Loss: 4.5567\n",
      "Epoch [1/2], Step [24150/67476], Loss: 4.5274\n",
      "Epoch [1/2], Step [24160/67476], Loss: 4.4155\n",
      "Epoch [1/2], Step [24170/67476], Loss: 4.4878\n",
      "Epoch [1/2], Step [24180/67476], Loss: 4.7805\n",
      "Epoch [1/2], Step [24190/67476], Loss: 4.5408\n",
      "Epoch [1/2], Step [24200/67476], Loss: 4.7855\n",
      "Epoch [1/2], Step [24210/67476], Loss: 4.5927\n",
      "Epoch [1/2], Step [24220/67476], Loss: 4.6318\n",
      "Epoch [1/2], Step [24230/67476], Loss: 4.6425\n",
      "Epoch [1/2], Step [24240/67476], Loss: 4.6115\n",
      "Epoch [1/2], Step [24250/67476], Loss: 4.7267\n",
      "Epoch [1/2], Step [24260/67476], Loss: 4.5786\n",
      "Epoch [1/2], Step [24270/67476], Loss: 4.7214\n",
      "Epoch [1/2], Step [24280/67476], Loss: 4.6700\n",
      "Epoch [1/2], Step [24290/67476], Loss: 4.5998\n",
      "Epoch [1/2], Step [24300/67476], Loss: 4.5593\n",
      "Epoch [1/2], Step [24310/67476], Loss: 4.5538\n",
      "Epoch [1/2], Step [24320/67476], Loss: 4.5597\n",
      "Epoch [1/2], Step [24330/67476], Loss: 4.5930\n",
      "Epoch [1/2], Step [24340/67476], Loss: 4.6143\n",
      "Epoch [1/2], Step [24350/67476], Loss: 4.6385\n",
      "Epoch [1/2], Step [24360/67476], Loss: 4.6191\n",
      "Epoch [1/2], Step [24370/67476], Loss: 4.5328\n",
      "Epoch [1/2], Step [24380/67476], Loss: 4.8046\n",
      "Epoch [1/2], Step [24390/67476], Loss: 4.5898\n",
      "Epoch [1/2], Step [24400/67476], Loss: 4.5968\n",
      "Epoch [1/2], Step [24410/67476], Loss: 4.4184\n",
      "Epoch [1/2], Step [24420/67476], Loss: 4.8623\n",
      "Epoch [1/2], Step [24430/67476], Loss: 4.6389\n",
      "Epoch [1/2], Step [24440/67476], Loss: 4.5271\n",
      "Epoch [1/2], Step [24450/67476], Loss: 4.5829\n",
      "Epoch [1/2], Step [24460/67476], Loss: 4.5207\n",
      "Epoch [1/2], Step [24470/67476], Loss: 4.5453\n",
      "Epoch [1/2], Step [24480/67476], Loss: 4.5405\n",
      "Epoch [1/2], Step [24490/67476], Loss: 4.5747\n",
      "Epoch [1/2], Step [24500/67476], Loss: 4.4080\n",
      "Epoch [1/2], Step [24510/67476], Loss: 4.6163\n",
      "Epoch [1/2], Step [24520/67476], Loss: 4.7776\n",
      "Epoch [1/2], Step [24530/67476], Loss: 4.6873\n",
      "Epoch [1/2], Step [24540/67476], Loss: 4.5842\n",
      "Epoch [1/2], Step [24550/67476], Loss: 4.4144\n",
      "Epoch [1/2], Step [24560/67476], Loss: 4.6595\n",
      "Epoch [1/2], Step [24570/67476], Loss: 4.7194\n",
      "Epoch [1/2], Step [24580/67476], Loss: 4.5915\n",
      "Epoch [1/2], Step [24590/67476], Loss: 4.2384\n",
      "Epoch [1/2], Step [24600/67476], Loss: 4.6444\n",
      "Epoch [1/2], Step [24610/67476], Loss: 4.6537\n",
      "Epoch [1/2], Step [24620/67476], Loss: 4.5723\n",
      "Epoch [1/2], Step [24630/67476], Loss: 4.4729\n",
      "Epoch [1/2], Step [24640/67476], Loss: 4.6786\n",
      "Epoch [1/2], Step [24650/67476], Loss: 4.5536\n",
      "Epoch [1/2], Step [24660/67476], Loss: 4.3450\n",
      "Epoch [1/2], Step [24670/67476], Loss: 4.8036\n",
      "Epoch [1/2], Step [24680/67476], Loss: 4.4985\n",
      "Epoch [1/2], Step [24690/67476], Loss: 4.5569\n",
      "Epoch [1/2], Step [24700/67476], Loss: 4.8272\n",
      "Epoch [1/2], Step [24710/67476], Loss: 4.5794\n",
      "Epoch [1/2], Step [24720/67476], Loss: 4.6314\n",
      "Epoch [1/2], Step [24730/67476], Loss: 4.7071\n",
      "Epoch [1/2], Step [24740/67476], Loss: 4.4105\n",
      "Epoch [1/2], Step [24750/67476], Loss: 4.4509\n",
      "Epoch [1/2], Step [24760/67476], Loss: 4.6034\n",
      "Epoch [1/2], Step [24770/67476], Loss: 4.6775\n",
      "Epoch [1/2], Step [24780/67476], Loss: 4.4449\n",
      "Epoch [1/2], Step [24790/67476], Loss: 4.6685\n",
      "Epoch [1/2], Step [24800/67476], Loss: 4.7008\n",
      "Epoch [1/2], Step [24810/67476], Loss: 4.5791\n",
      "Epoch [1/2], Step [24820/67476], Loss: 4.4927\n",
      "Epoch [1/2], Step [24830/67476], Loss: 4.7462\n",
      "Epoch [1/2], Step [24840/67476], Loss: 4.6830\n",
      "Epoch [1/2], Step [24850/67476], Loss: 4.4920\n",
      "Epoch [1/2], Step [24860/67476], Loss: 4.5790\n",
      "Epoch [1/2], Step [24870/67476], Loss: 4.8678\n",
      "Epoch [1/2], Step [24880/67476], Loss: 4.6322\n",
      "Epoch [1/2], Step [24890/67476], Loss: 4.7775\n",
      "Epoch [1/2], Step [24900/67476], Loss: 4.6614\n",
      "Epoch [1/2], Step [24910/67476], Loss: 4.5953\n",
      "Epoch [1/2], Step [24920/67476], Loss: 4.6859\n",
      "Epoch [1/2], Step [24930/67476], Loss: 4.5397\n",
      "Epoch [1/2], Step [24940/67476], Loss: 4.4545\n",
      "Epoch [1/2], Step [24950/67476], Loss: 4.7707\n",
      "Epoch [1/2], Step [24960/67476], Loss: 4.5737\n",
      "Epoch [1/2], Step [24970/67476], Loss: 4.6328\n",
      "Epoch [1/2], Step [24980/67476], Loss: 4.7173\n",
      "Epoch [1/2], Step [24990/67476], Loss: 4.6446\n",
      "Epoch [1/2], Step [25000/67476], Loss: 4.4884\n",
      "Epoch [1/2], Step [25010/67476], Loss: 4.4398\n",
      "Epoch [1/2], Step [25020/67476], Loss: 4.5948\n",
      "Epoch [1/2], Step [25030/67476], Loss: 4.4944\n",
      "Epoch [1/2], Step [25040/67476], Loss: 4.5139\n",
      "Epoch [1/2], Step [25050/67476], Loss: 4.4435\n",
      "Epoch [1/2], Step [25060/67476], Loss: 4.3644\n",
      "Epoch [1/2], Step [25070/67476], Loss: 4.6632\n",
      "Epoch [1/2], Step [25080/67476], Loss: 4.5140\n",
      "Epoch [1/2], Step [25090/67476], Loss: 4.5967\n",
      "Epoch [1/2], Step [25100/67476], Loss: 4.6727\n",
      "Epoch [1/2], Step [25110/67476], Loss: 4.5802\n",
      "Epoch [1/2], Step [25120/67476], Loss: 4.5710\n",
      "Epoch [1/2], Step [25130/67476], Loss: 4.8265\n",
      "Epoch [1/2], Step [25140/67476], Loss: 4.7246\n",
      "Epoch [1/2], Step [25150/67476], Loss: 4.5734\n",
      "Epoch [1/2], Step [25160/67476], Loss: 4.5583\n",
      "Epoch [1/2], Step [25170/67476], Loss: 4.7735\n",
      "Epoch [1/2], Step [25180/67476], Loss: 4.8089\n",
      "Epoch [1/2], Step [25190/67476], Loss: 4.7270\n",
      "Epoch [1/2], Step [25200/67476], Loss: 4.5315\n",
      "Epoch [1/2], Step [25210/67476], Loss: 4.5561\n",
      "Epoch [1/2], Step [25220/67476], Loss: 4.5790\n",
      "Epoch [1/2], Step [25230/67476], Loss: 4.5989\n",
      "Epoch [1/2], Step [25240/67476], Loss: 4.4711\n",
      "Epoch [1/2], Step [25250/67476], Loss: 4.4567\n",
      "Epoch [1/2], Step [25260/67476], Loss: 4.7082\n",
      "Epoch [1/2], Step [25270/67476], Loss: 4.5700\n",
      "Epoch [1/2], Step [25280/67476], Loss: 4.4885\n",
      "Epoch [1/2], Step [25290/67476], Loss: 4.5431\n",
      "Epoch [1/2], Step [25300/67476], Loss: 4.4771\n",
      "Epoch [1/2], Step [25310/67476], Loss: 4.8170\n",
      "Epoch [1/2], Step [25320/67476], Loss: 4.5878\n",
      "Epoch [1/2], Step [25330/67476], Loss: 4.4227\n",
      "Epoch [1/2], Step [25340/67476], Loss: 4.7053\n",
      "Epoch [1/2], Step [25350/67476], Loss: 4.6611\n",
      "Epoch [1/2], Step [25360/67476], Loss: 4.7186\n",
      "Epoch [1/2], Step [25370/67476], Loss: 4.4396\n",
      "Epoch [1/2], Step [25380/67476], Loss: 4.5610\n",
      "Epoch [1/2], Step [25390/67476], Loss: 4.6757\n",
      "Epoch [1/2], Step [25400/67476], Loss: 4.5655\n",
      "Epoch [1/2], Step [25410/67476], Loss: 4.6701\n",
      "Epoch [1/2], Step [25420/67476], Loss: 4.5462\n",
      "Epoch [1/2], Step [25430/67476], Loss: 4.4152\n",
      "Epoch [1/2], Step [25440/67476], Loss: 4.5635\n",
      "Epoch [1/2], Step [25450/67476], Loss: 4.5805\n",
      "Epoch [1/2], Step [25460/67476], Loss: 4.5316\n",
      "Epoch [1/2], Step [25470/67476], Loss: 4.2852\n",
      "Epoch [1/2], Step [25480/67476], Loss: 4.7167\n",
      "Epoch [1/2], Step [25490/67476], Loss: 4.5355\n",
      "Epoch [1/2], Step [25500/67476], Loss: 4.5248\n",
      "Epoch [1/2], Step [25510/67476], Loss: 4.5102\n",
      "Epoch [1/2], Step [25520/67476], Loss: 4.3915\n",
      "Epoch [1/2], Step [25530/67476], Loss: 4.2899\n",
      "Epoch [1/2], Step [25540/67476], Loss: 4.7192\n",
      "Epoch [1/2], Step [25550/67476], Loss: 4.5625\n",
      "Epoch [1/2], Step [25560/67476], Loss: 4.5060\n",
      "Epoch [1/2], Step [25570/67476], Loss: 4.6572\n",
      "Epoch [1/2], Step [25580/67476], Loss: 4.4476\n",
      "Epoch [1/2], Step [25590/67476], Loss: 4.6053\n",
      "Epoch [1/2], Step [25600/67476], Loss: 4.5365\n",
      "Epoch [1/2], Step [25610/67476], Loss: 4.5683\n",
      "Epoch [1/2], Step [25620/67476], Loss: 4.7204\n",
      "Epoch [1/2], Step [25630/67476], Loss: 4.6459\n",
      "Epoch [1/2], Step [25640/67476], Loss: 4.4542\n",
      "Epoch [1/2], Step [25650/67476], Loss: 4.5853\n",
      "Epoch [1/2], Step [25660/67476], Loss: 4.5910\n",
      "Epoch [1/2], Step [25670/67476], Loss: 4.7902\n",
      "Epoch [1/2], Step [25680/67476], Loss: 4.6037\n",
      "Epoch [1/2], Step [25690/67476], Loss: 4.6962\n",
      "Epoch [1/2], Step [25700/67476], Loss: 4.6185\n",
      "Epoch [1/2], Step [25710/67476], Loss: 4.5886\n",
      "Epoch [1/2], Step [25720/67476], Loss: 4.6243\n",
      "Epoch [1/2], Step [25730/67476], Loss: 4.5494\n",
      "Epoch [1/2], Step [25740/67476], Loss: 4.5566\n",
      "Epoch [1/2], Step [25750/67476], Loss: 4.7105\n",
      "Epoch [1/2], Step [25760/67476], Loss: 4.4276\n",
      "Epoch [1/2], Step [25770/67476], Loss: 4.7565\n",
      "Epoch [1/2], Step [25780/67476], Loss: 4.5190\n",
      "Epoch [1/2], Step [25790/67476], Loss: 4.7073\n",
      "Epoch [1/2], Step [25800/67476], Loss: 4.4547\n",
      "Epoch [1/2], Step [25810/67476], Loss: 4.4723\n",
      "Epoch [1/2], Step [25820/67476], Loss: 4.6958\n",
      "Epoch [1/2], Step [25830/67476], Loss: 4.5202\n",
      "Epoch [1/2], Step [25840/67476], Loss: 4.5719\n",
      "Epoch [1/2], Step [25850/67476], Loss: 4.4561\n",
      "Epoch [1/2], Step [25860/67476], Loss: 4.5048\n",
      "Epoch [1/2], Step [25870/67476], Loss: 4.5825\n",
      "Epoch [1/2], Step [25880/67476], Loss: 4.7463\n",
      "Epoch [1/2], Step [25890/67476], Loss: 4.7411\n",
      "Epoch [1/2], Step [25900/67476], Loss: 4.4530\n",
      "Epoch [1/2], Step [25910/67476], Loss: 4.4689\n",
      "Epoch [1/2], Step [25920/67476], Loss: 4.3948\n",
      "Epoch [1/2], Step [25930/67476], Loss: 4.3867\n",
      "Epoch [1/2], Step [25940/67476], Loss: 4.5748\n",
      "Epoch [1/2], Step [25950/67476], Loss: 4.5057\n",
      "Epoch [1/2], Step [25960/67476], Loss: 4.4421\n",
      "Epoch [1/2], Step [25970/67476], Loss: 4.5058\n",
      "Epoch [1/2], Step [25980/67476], Loss: 4.6173\n",
      "Epoch [1/2], Step [25990/67476], Loss: 4.6400\n",
      "Epoch [1/2], Step [26000/67476], Loss: 4.7028\n",
      "Epoch [1/2], Step [26010/67476], Loss: 4.4907\n",
      "Epoch [1/2], Step [26020/67476], Loss: 4.5847\n",
      "Epoch [1/2], Step [26030/67476], Loss: 4.6655\n",
      "Epoch [1/2], Step [26040/67476], Loss: 4.5343\n",
      "Epoch [1/2], Step [26050/67476], Loss: 4.7372\n",
      "Epoch [1/2], Step [26060/67476], Loss: 4.4952\n",
      "Epoch [1/2], Step [26070/67476], Loss: 4.7047\n",
      "Epoch [1/2], Step [26080/67476], Loss: 4.8345\n",
      "Epoch [1/2], Step [26090/67476], Loss: 4.8146\n",
      "Epoch [1/2], Step [26100/67476], Loss: 4.6746\n",
      "Epoch [1/2], Step [26110/67476], Loss: 4.6637\n",
      "Epoch [1/2], Step [26120/67476], Loss: 4.5660\n",
      "Epoch [1/2], Step [26130/67476], Loss: 4.7577\n",
      "Epoch [1/2], Step [26140/67476], Loss: 4.3928\n",
      "Epoch [1/2], Step [26150/67476], Loss: 4.4533\n",
      "Epoch [1/2], Step [26160/67476], Loss: 4.7210\n",
      "Epoch [1/2], Step [26170/67476], Loss: 4.5051\n",
      "Epoch [1/2], Step [26180/67476], Loss: 4.6805\n",
      "Epoch [1/2], Step [26190/67476], Loss: 4.6332\n",
      "Epoch [1/2], Step [26200/67476], Loss: 4.7491\n",
      "Epoch [1/2], Step [26210/67476], Loss: 4.6569\n",
      "Epoch [1/2], Step [26220/67476], Loss: 4.6499\n",
      "Epoch [1/2], Step [26230/67476], Loss: 4.5736\n",
      "Epoch [1/2], Step [26240/67476], Loss: 4.6835\n",
      "Epoch [1/2], Step [26250/67476], Loss: 4.5013\n",
      "Epoch [1/2], Step [26260/67476], Loss: 4.4535\n",
      "Epoch [1/2], Step [26270/67476], Loss: 4.6896\n",
      "Epoch [1/2], Step [26280/67476], Loss: 4.7255\n",
      "Epoch [1/2], Step [26290/67476], Loss: 4.3803\n",
      "Epoch [1/2], Step [26300/67476], Loss: 4.6588\n",
      "Epoch [1/2], Step [26310/67476], Loss: 4.6093\n",
      "Epoch [1/2], Step [26320/67476], Loss: 4.4776\n",
      "Epoch [1/2], Step [26330/67476], Loss: 4.3626\n",
      "Epoch [1/2], Step [26340/67476], Loss: 4.5893\n",
      "Epoch [1/2], Step [26350/67476], Loss: 4.6955\n",
      "Epoch [1/2], Step [26360/67476], Loss: 4.4440\n",
      "Epoch [1/2], Step [26370/67476], Loss: 4.7469\n",
      "Epoch [1/2], Step [26380/67476], Loss: 4.4154\n",
      "Epoch [1/2], Step [26390/67476], Loss: 4.6375\n",
      "Epoch [1/2], Step [26400/67476], Loss: 4.5174\n",
      "Epoch [1/2], Step [26410/67476], Loss: 4.6109\n",
      "Epoch [1/2], Step [26420/67476], Loss: 4.7364\n",
      "Epoch [1/2], Step [26430/67476], Loss: 4.4089\n",
      "Epoch [1/2], Step [26440/67476], Loss: 4.6679\n",
      "Epoch [1/2], Step [26450/67476], Loss: 4.4233\n",
      "Epoch [1/2], Step [26460/67476], Loss: 4.7352\n",
      "Epoch [1/2], Step [26470/67476], Loss: 4.3532\n",
      "Epoch [1/2], Step [26480/67476], Loss: 4.3179\n",
      "Epoch [1/2], Step [26490/67476], Loss: 4.3694\n",
      "Epoch [1/2], Step [26500/67476], Loss: 4.3540\n",
      "Epoch [1/2], Step [26510/67476], Loss: 4.6455\n",
      "Epoch [1/2], Step [26520/67476], Loss: 4.5492\n",
      "Epoch [1/2], Step [26530/67476], Loss: 4.4987\n",
      "Epoch [1/2], Step [26540/67476], Loss: 4.5635\n",
      "Epoch [1/2], Step [26550/67476], Loss: 4.4798\n",
      "Epoch [1/2], Step [26560/67476], Loss: 4.8417\n",
      "Epoch [1/2], Step [26570/67476], Loss: 4.8118\n",
      "Epoch [1/2], Step [26580/67476], Loss: 4.6529\n",
      "Epoch [1/2], Step [26590/67476], Loss: 4.5289\n",
      "Epoch [1/2], Step [26600/67476], Loss: 4.4953\n",
      "Epoch [1/2], Step [26610/67476], Loss: 4.8373\n",
      "Epoch [1/2], Step [26620/67476], Loss: 4.6967\n",
      "Epoch [1/2], Step [26630/67476], Loss: 4.3305\n",
      "Epoch [1/2], Step [26640/67476], Loss: 4.5015\n",
      "Epoch [1/2], Step [26650/67476], Loss: 4.5318\n",
      "Epoch [1/2], Step [26660/67476], Loss: 4.6548\n",
      "Epoch [1/2], Step [26670/67476], Loss: 4.4547\n",
      "Epoch [1/2], Step [26680/67476], Loss: 4.4193\n",
      "Epoch [1/2], Step [26690/67476], Loss: 4.6718\n",
      "Epoch [1/2], Step [26700/67476], Loss: 4.6746\n",
      "Epoch [1/2], Step [26710/67476], Loss: 4.6914\n",
      "Epoch [1/2], Step [26720/67476], Loss: 4.5358\n",
      "Epoch [1/2], Step [26730/67476], Loss: 4.6649\n",
      "Epoch [1/2], Step [26740/67476], Loss: 4.7557\n",
      "Epoch [1/2], Step [26750/67476], Loss: 4.4389\n",
      "Epoch [1/2], Step [26760/67476], Loss: 4.7563\n",
      "Epoch [1/2], Step [26770/67476], Loss: 4.6537\n",
      "Epoch [1/2], Step [26780/67476], Loss: 4.4465\n",
      "Epoch [1/2], Step [26790/67476], Loss: 4.7473\n",
      "Epoch [1/2], Step [26800/67476], Loss: 4.6213\n",
      "Epoch [1/2], Step [26810/67476], Loss: 4.2646\n",
      "Epoch [1/2], Step [26820/67476], Loss: 4.7675\n",
      "Epoch [1/2], Step [26830/67476], Loss: 4.6794\n",
      "Epoch [1/2], Step [26840/67476], Loss: 4.5095\n",
      "Epoch [1/2], Step [26850/67476], Loss: 4.5034\n",
      "Epoch [1/2], Step [26860/67476], Loss: 4.5120\n",
      "Epoch [1/2], Step [26870/67476], Loss: 4.5911\n",
      "Epoch [1/2], Step [26880/67476], Loss: 4.5693\n",
      "Epoch [1/2], Step [26890/67476], Loss: 4.3695\n",
      "Epoch [1/2], Step [26900/67476], Loss: 4.6361\n",
      "Epoch [1/2], Step [26910/67476], Loss: 4.6754\n",
      "Epoch [1/2], Step [26920/67476], Loss: 4.5835\n",
      "Epoch [1/2], Step [26930/67476], Loss: 4.5083\n",
      "Epoch [1/2], Step [26940/67476], Loss: 4.4960\n",
      "Epoch [1/2], Step [26950/67476], Loss: 4.6311\n",
      "Epoch [1/2], Step [26960/67476], Loss: 4.6029\n",
      "Epoch [1/2], Step [26970/67476], Loss: 4.5605\n",
      "Epoch [1/2], Step [26980/67476], Loss: 4.7158\n",
      "Epoch [1/2], Step [26990/67476], Loss: 4.3594\n",
      "Epoch [1/2], Step [27000/67476], Loss: 4.4463\n",
      "Epoch [1/2], Step [27010/67476], Loss: 4.5223\n",
      "Epoch [1/2], Step [27020/67476], Loss: 4.5628\n",
      "Epoch [1/2], Step [27030/67476], Loss: 4.2912\n",
      "Epoch [1/2], Step [27040/67476], Loss: 4.7866\n",
      "Epoch [1/2], Step [27050/67476], Loss: 4.4924\n",
      "Epoch [1/2], Step [27060/67476], Loss: 4.5942\n",
      "Epoch [1/2], Step [27070/67476], Loss: 4.5557\n",
      "Epoch [1/2], Step [27080/67476], Loss: 4.6165\n",
      "Epoch [1/2], Step [27090/67476], Loss: 4.6542\n",
      "Epoch [1/2], Step [27100/67476], Loss: 4.6584\n",
      "Epoch [1/2], Step [27110/67476], Loss: 4.6368\n",
      "Epoch [1/2], Step [27120/67476], Loss: 4.4843\n",
      "Epoch [1/2], Step [27130/67476], Loss: 4.4610\n",
      "Epoch [1/2], Step [27140/67476], Loss: 4.5094\n",
      "Epoch [1/2], Step [27150/67476], Loss: 4.5550\n",
      "Epoch [1/2], Step [27160/67476], Loss: 4.4943\n",
      "Epoch [1/2], Step [27170/67476], Loss: 4.4192\n",
      "Epoch [1/2], Step [27180/67476], Loss: 4.3992\n",
      "Epoch [1/2], Step [27190/67476], Loss: 4.5758\n",
      "Epoch [1/2], Step [27200/67476], Loss: 4.6910\n",
      "Epoch [1/2], Step [27210/67476], Loss: 4.3163\n",
      "Epoch [1/2], Step [27220/67476], Loss: 4.6225\n",
      "Epoch [1/2], Step [27230/67476], Loss: 4.5338\n",
      "Epoch [1/2], Step [27240/67476], Loss: 4.5297\n",
      "Epoch [1/2], Step [27250/67476], Loss: 4.5820\n",
      "Epoch [1/2], Step [27260/67476], Loss: 4.6102\n",
      "Epoch [1/2], Step [27270/67476], Loss: 4.4688\n",
      "Epoch [1/2], Step [27280/67476], Loss: 4.5914\n",
      "Epoch [1/2], Step [27290/67476], Loss: 4.5382\n",
      "Epoch [1/2], Step [27300/67476], Loss: 4.7325\n",
      "Epoch [1/2], Step [27310/67476], Loss: 4.6549\n",
      "Epoch [1/2], Step [27320/67476], Loss: 4.6176\n",
      "Epoch [1/2], Step [27330/67476], Loss: 4.3870\n",
      "Epoch [1/2], Step [27340/67476], Loss: 4.4938\n",
      "Epoch [1/2], Step [27350/67476], Loss: 4.4704\n",
      "Epoch [1/2], Step [27360/67476], Loss: 4.3893\n",
      "Epoch [1/2], Step [27370/67476], Loss: 4.6399\n",
      "Epoch [1/2], Step [27380/67476], Loss: 4.6380\n",
      "Epoch [1/2], Step [27390/67476], Loss: 4.5693\n",
      "Epoch [1/2], Step [27400/67476], Loss: 4.3553\n",
      "Epoch [1/2], Step [27410/67476], Loss: 4.5866\n",
      "Epoch [1/2], Step [27420/67476], Loss: 4.5939\n",
      "Epoch [1/2], Step [27430/67476], Loss: 4.6023\n",
      "Epoch [1/2], Step [27440/67476], Loss: 4.4464\n",
      "Epoch [1/2], Step [27450/67476], Loss: 4.4662\n",
      "Epoch [1/2], Step [27460/67476], Loss: 4.6535\n",
      "Epoch [1/2], Step [27470/67476], Loss: 4.6617\n",
      "Epoch [1/2], Step [27480/67476], Loss: 4.7758\n",
      "Epoch [1/2], Step [27490/67476], Loss: 4.5266\n",
      "Epoch [1/2], Step [27500/67476], Loss: 4.5969\n",
      "Epoch [1/2], Step [27510/67476], Loss: 4.5893\n",
      "Epoch [1/2], Step [27520/67476], Loss: 4.6091\n",
      "Epoch [1/2], Step [27530/67476], Loss: 4.5271\n",
      "Epoch [1/2], Step [27540/67476], Loss: 4.7603\n",
      "Epoch [1/2], Step [27550/67476], Loss: 4.7882\n",
      "Epoch [1/2], Step [27560/67476], Loss: 4.6357\n",
      "Epoch [1/2], Step [27570/67476], Loss: 4.3560\n",
      "Epoch [1/2], Step [27580/67476], Loss: 4.6065\n",
      "Epoch [1/2], Step [27590/67476], Loss: 4.5374\n",
      "Epoch [1/2], Step [27600/67476], Loss: 4.5312\n",
      "Epoch [1/2], Step [27610/67476], Loss: 4.7068\n",
      "Epoch [1/2], Step [27620/67476], Loss: 4.8016\n",
      "Epoch [1/2], Step [27630/67476], Loss: 4.4741\n",
      "Epoch [1/2], Step [27640/67476], Loss: 4.5037\n",
      "Epoch [1/2], Step [27650/67476], Loss: 4.4093\n",
      "Epoch [1/2], Step [27660/67476], Loss: 4.6945\n",
      "Epoch [1/2], Step [27670/67476], Loss: 4.4458\n",
      "Epoch [1/2], Step [27680/67476], Loss: 4.4519\n",
      "Epoch [1/2], Step [27690/67476], Loss: 4.8653\n",
      "Epoch [1/2], Step [27700/67476], Loss: 4.3922\n",
      "Epoch [1/2], Step [27710/67476], Loss: 4.8380\n",
      "Epoch [1/2], Step [27720/67476], Loss: 4.6494\n",
      "Epoch [1/2], Step [27730/67476], Loss: 4.6246\n",
      "Epoch [1/2], Step [27740/67476], Loss: 4.4778\n",
      "Epoch [1/2], Step [27750/67476], Loss: 4.5967\n",
      "Epoch [1/2], Step [27760/67476], Loss: 4.4477\n",
      "Epoch [1/2], Step [27770/67476], Loss: 4.4809\n",
      "Epoch [1/2], Step [27780/67476], Loss: 4.5045\n",
      "Epoch [1/2], Step [27790/67476], Loss: 4.4451\n",
      "Epoch [1/2], Step [27800/67476], Loss: 4.6559\n",
      "Epoch [1/2], Step [27810/67476], Loss: 4.7534\n",
      "Epoch [1/2], Step [27820/67476], Loss: 4.4581\n",
      "Epoch [1/2], Step [27830/67476], Loss: 4.5375\n",
      "Epoch [1/2], Step [27840/67476], Loss: 4.4526\n",
      "Epoch [1/2], Step [27850/67476], Loss: 4.4059\n",
      "Epoch [1/2], Step [27860/67476], Loss: 4.3077\n",
      "Epoch [1/2], Step [27870/67476], Loss: 4.7418\n",
      "Epoch [1/2], Step [27880/67476], Loss: 4.3029\n",
      "Epoch [1/2], Step [27890/67476], Loss: 4.5980\n",
      "Epoch [1/2], Step [27900/67476], Loss: 4.5480\n",
      "Epoch [1/2], Step [27910/67476], Loss: 4.4451\n",
      "Epoch [1/2], Step [27920/67476], Loss: 4.5113\n",
      "Epoch [1/2], Step [27930/67476], Loss: 4.6119\n",
      "Epoch [1/2], Step [27940/67476], Loss: 4.4807\n",
      "Epoch [1/2], Step [27950/67476], Loss: 4.4531\n",
      "Epoch [1/2], Step [27960/67476], Loss: 4.5239\n",
      "Epoch [1/2], Step [27970/67476], Loss: 4.4629\n",
      "Epoch [1/2], Step [27980/67476], Loss: 4.5499\n",
      "Epoch [1/2], Step [27990/67476], Loss: 4.6003\n",
      "Epoch [1/2], Step [28000/67476], Loss: 4.7166\n",
      "Epoch [1/2], Step [28010/67476], Loss: 4.7242\n",
      "Epoch [1/2], Step [28020/67476], Loss: 4.5021\n",
      "Epoch [1/2], Step [28030/67476], Loss: 4.4015\n",
      "Epoch [1/2], Step [28040/67476], Loss: 4.6510\n",
      "Epoch [1/2], Step [28050/67476], Loss: 4.4554\n",
      "Epoch [1/2], Step [28060/67476], Loss: 4.4757\n",
      "Epoch [1/2], Step [28070/67476], Loss: 4.4281\n",
      "Epoch [1/2], Step [28080/67476], Loss: 4.5306\n",
      "Epoch [1/2], Step [28090/67476], Loss: 4.5707\n",
      "Epoch [1/2], Step [28100/67476], Loss: 4.3964\n",
      "Epoch [1/2], Step [28110/67476], Loss: 4.6095\n",
      "Epoch [1/2], Step [28120/67476], Loss: 4.6123\n",
      "Epoch [1/2], Step [28130/67476], Loss: 4.5986\n",
      "Epoch [1/2], Step [28140/67476], Loss: 4.6253\n",
      "Epoch [1/2], Step [28150/67476], Loss: 4.7829\n",
      "Epoch [1/2], Step [28160/67476], Loss: 4.6677\n",
      "Epoch [1/2], Step [28170/67476], Loss: 4.5430\n",
      "Epoch [1/2], Step [28180/67476], Loss: 4.6138\n",
      "Epoch [1/2], Step [28190/67476], Loss: 4.5545\n",
      "Epoch [1/2], Step [28200/67476], Loss: 4.5154\n",
      "Epoch [1/2], Step [28210/67476], Loss: 4.5786\n",
      "Epoch [1/2], Step [28220/67476], Loss: 4.3410\n",
      "Epoch [1/2], Step [28230/67476], Loss: 4.6093\n",
      "Epoch [1/2], Step [28240/67476], Loss: 4.6880\n",
      "Epoch [1/2], Step [28250/67476], Loss: 4.8226\n",
      "Epoch [1/2], Step [28260/67476], Loss: 4.4545\n",
      "Epoch [1/2], Step [28270/67476], Loss: 4.4596\n",
      "Epoch [1/2], Step [28280/67476], Loss: 4.5442\n",
      "Epoch [1/2], Step [28290/67476], Loss: 4.6884\n",
      "Epoch [1/2], Step [28300/67476], Loss: 4.7683\n",
      "Epoch [1/2], Step [28310/67476], Loss: 4.5232\n",
      "Epoch [1/2], Step [28320/67476], Loss: 4.5165\n",
      "Epoch [1/2], Step [28330/67476], Loss: 4.6392\n",
      "Epoch [1/2], Step [28340/67476], Loss: 4.4931\n",
      "Epoch [1/2], Step [28350/67476], Loss: 4.6592\n",
      "Epoch [1/2], Step [28360/67476], Loss: 4.5332\n",
      "Epoch [1/2], Step [28370/67476], Loss: 4.6654\n",
      "Epoch [1/2], Step [28380/67476], Loss: 4.5270\n",
      "Epoch [1/2], Step [28390/67476], Loss: 4.3528\n",
      "Epoch [1/2], Step [28400/67476], Loss: 4.4490\n",
      "Epoch [1/2], Step [28410/67476], Loss: 4.4301\n",
      "Epoch [1/2], Step [28420/67476], Loss: 4.5184\n",
      "Epoch [1/2], Step [28430/67476], Loss: 4.6458\n",
      "Epoch [1/2], Step [28440/67476], Loss: 4.5985\n",
      "Epoch [1/2], Step [28450/67476], Loss: 4.6372\n",
      "Epoch [1/2], Step [28460/67476], Loss: 4.6444\n",
      "Epoch [1/2], Step [28470/67476], Loss: 4.5465\n",
      "Epoch [1/2], Step [28480/67476], Loss: 4.5599\n",
      "Epoch [1/2], Step [28490/67476], Loss: 4.7738\n",
      "Epoch [1/2], Step [28500/67476], Loss: 4.3100\n",
      "Epoch [1/2], Step [28510/67476], Loss: 4.6198\n",
      "Epoch [1/2], Step [28520/67476], Loss: 4.6442\n",
      "Epoch [1/2], Step [28530/67476], Loss: 4.5519\n",
      "Epoch [1/2], Step [28540/67476], Loss: 4.4493\n",
      "Epoch [1/2], Step [28550/67476], Loss: 4.6655\n",
      "Epoch [1/2], Step [28560/67476], Loss: 4.5679\n",
      "Epoch [1/2], Step [28570/67476], Loss: 4.7060\n",
      "Epoch [1/2], Step [28580/67476], Loss: 4.6165\n",
      "Epoch [1/2], Step [28590/67476], Loss: 4.6377\n",
      "Epoch [1/2], Step [28600/67476], Loss: 4.5608\n",
      "Epoch [1/2], Step [28610/67476], Loss: 4.5701\n",
      "Epoch [1/2], Step [28620/67476], Loss: 4.6235\n",
      "Epoch [1/2], Step [28630/67476], Loss: 4.5031\n",
      "Epoch [1/2], Step [28640/67476], Loss: 4.6186\n",
      "Epoch [1/2], Step [28650/67476], Loss: 4.4375\n",
      "Epoch [1/2], Step [28660/67476], Loss: 4.4915\n",
      "Epoch [1/2], Step [28670/67476], Loss: 4.7027\n",
      "Epoch [1/2], Step [28680/67476], Loss: 4.5999\n",
      "Epoch [1/2], Step [28690/67476], Loss: 4.7250\n",
      "Epoch [1/2], Step [28700/67476], Loss: 4.6982\n",
      "Epoch [1/2], Step [28710/67476], Loss: 4.5007\n",
      "Epoch [1/2], Step [28720/67476], Loss: 4.6075\n",
      "Epoch [1/2], Step [28730/67476], Loss: 4.5082\n",
      "Epoch [1/2], Step [28740/67476], Loss: 4.6990\n",
      "Epoch [1/2], Step [28750/67476], Loss: 4.5709\n",
      "Epoch [1/2], Step [28760/67476], Loss: 4.5875\n",
      "Epoch [1/2], Step [28770/67476], Loss: 4.5920\n",
      "Epoch [1/2], Step [28780/67476], Loss: 4.6122\n",
      "Epoch [1/2], Step [28790/67476], Loss: 4.6291\n",
      "Epoch [1/2], Step [28800/67476], Loss: 4.7403\n",
      "Epoch [1/2], Step [28810/67476], Loss: 4.5171\n",
      "Epoch [1/2], Step [28820/67476], Loss: 4.7264\n",
      "Epoch [1/2], Step [28830/67476], Loss: 4.4469\n",
      "Epoch [1/2], Step [28840/67476], Loss: 4.5280\n",
      "Epoch [1/2], Step [28850/67476], Loss: 4.6988\n",
      "Epoch [1/2], Step [28860/67476], Loss: 4.7313\n",
      "Epoch [1/2], Step [28870/67476], Loss: 4.4986\n",
      "Epoch [1/2], Step [28880/67476], Loss: 4.5951\n",
      "Epoch [1/2], Step [28890/67476], Loss: 4.5561\n",
      "Epoch [1/2], Step [28900/67476], Loss: 4.5882\n",
      "Epoch [1/2], Step [28910/67476], Loss: 4.6567\n",
      "Epoch [1/2], Step [28920/67476], Loss: 4.7213\n",
      "Epoch [1/2], Step [28930/67476], Loss: 4.4509\n",
      "Epoch [1/2], Step [28940/67476], Loss: 4.6255\n",
      "Epoch [1/2], Step [28950/67476], Loss: 4.5810\n",
      "Epoch [1/2], Step [28960/67476], Loss: 4.3570\n",
      "Epoch [1/2], Step [28970/67476], Loss: 4.4332\n",
      "Epoch [1/2], Step [28980/67476], Loss: 4.6914\n",
      "Epoch [1/2], Step [28990/67476], Loss: 4.3195\n",
      "Epoch [1/2], Step [29000/67476], Loss: 4.3582\n",
      "Epoch [1/2], Step [29010/67476], Loss: 4.6249\n",
      "Epoch [1/2], Step [29020/67476], Loss: 4.5121\n",
      "Epoch [1/2], Step [29030/67476], Loss: 4.4842\n",
      "Epoch [1/2], Step [29040/67476], Loss: 4.5558\n",
      "Epoch [1/2], Step [29050/67476], Loss: 4.5290\n",
      "Epoch [1/2], Step [29060/67476], Loss: 4.5240\n",
      "Epoch [1/2], Step [29070/67476], Loss: 4.6008\n",
      "Epoch [1/2], Step [29080/67476], Loss: 4.5817\n",
      "Epoch [1/2], Step [29090/67476], Loss: 4.7557\n",
      "Epoch [1/2], Step [29100/67476], Loss: 4.5696\n",
      "Epoch [1/2], Step [29110/67476], Loss: 4.6996\n",
      "Epoch [1/2], Step [29120/67476], Loss: 4.8075\n",
      "Epoch [1/2], Step [29130/67476], Loss: 4.6390\n",
      "Epoch [1/2], Step [29140/67476], Loss: 4.6775\n",
      "Epoch [1/2], Step [29150/67476], Loss: 4.5149\n",
      "Epoch [1/2], Step [29160/67476], Loss: 4.6244\n",
      "Epoch [1/2], Step [29170/67476], Loss: 4.5010\n",
      "Epoch [1/2], Step [29180/67476], Loss: 4.5723\n",
      "Epoch [1/2], Step [29190/67476], Loss: 4.5891\n",
      "Epoch [1/2], Step [29200/67476], Loss: 4.6310\n",
      "Epoch [1/2], Step [29210/67476], Loss: 4.4953\n",
      "Epoch [1/2], Step [29220/67476], Loss: 4.5910\n",
      "Epoch [1/2], Step [29230/67476], Loss: 4.5988\n",
      "Epoch [1/2], Step [29240/67476], Loss: 4.5785\n",
      "Epoch [1/2], Step [29250/67476], Loss: 4.5151\n",
      "Epoch [1/2], Step [29260/67476], Loss: 4.6106\n",
      "Epoch [1/2], Step [29270/67476], Loss: 4.7419\n",
      "Epoch [1/2], Step [29280/67476], Loss: 4.5651\n",
      "Epoch [1/2], Step [29290/67476], Loss: 4.5579\n",
      "Epoch [1/2], Step [29300/67476], Loss: 4.7236\n",
      "Epoch [1/2], Step [29310/67476], Loss: 4.6793\n",
      "Epoch [1/2], Step [29320/67476], Loss: 4.5126\n",
      "Epoch [1/2], Step [29330/67476], Loss: 4.5119\n",
      "Epoch [1/2], Step [29340/67476], Loss: 4.7906\n",
      "Epoch [1/2], Step [29350/67476], Loss: 4.5935\n",
      "Epoch [1/2], Step [29360/67476], Loss: 4.4644\n",
      "Epoch [1/2], Step [29370/67476], Loss: 4.4607\n",
      "Epoch [1/2], Step [29380/67476], Loss: 4.4962\n",
      "Epoch [1/2], Step [29390/67476], Loss: 4.7110\n",
      "Epoch [1/2], Step [29400/67476], Loss: 4.5401\n",
      "Epoch [1/2], Step [29410/67476], Loss: 4.5906\n",
      "Epoch [1/2], Step [29420/67476], Loss: 4.6115\n",
      "Epoch [1/2], Step [29430/67476], Loss: 4.6228\n",
      "Epoch [1/2], Step [29440/67476], Loss: 4.7244\n",
      "Epoch [1/2], Step [29450/67476], Loss: 4.5704\n",
      "Epoch [1/2], Step [29460/67476], Loss: 4.5123\n",
      "Epoch [1/2], Step [29470/67476], Loss: 4.5826\n",
      "Epoch [1/2], Step [29480/67476], Loss: 4.6118\n",
      "Epoch [1/2], Step [29490/67476], Loss: 4.6081\n",
      "Epoch [1/2], Step [29500/67476], Loss: 4.4052\n",
      "Epoch [1/2], Step [29510/67476], Loss: 4.4128\n",
      "Epoch [1/2], Step [29520/67476], Loss: 4.5150\n",
      "Epoch [1/2], Step [29530/67476], Loss: 4.6633\n",
      "Epoch [1/2], Step [29540/67476], Loss: 4.6627\n",
      "Epoch [1/2], Step [29550/67476], Loss: 4.7337\n",
      "Epoch [1/2], Step [29560/67476], Loss: 4.3788\n",
      "Epoch [1/2], Step [29570/67476], Loss: 4.5117\n",
      "Epoch [1/2], Step [29580/67476], Loss: 4.7768\n",
      "Epoch [1/2], Step [29590/67476], Loss: 4.6716\n",
      "Epoch [1/2], Step [29600/67476], Loss: 4.5739\n",
      "Epoch [1/2], Step [29610/67476], Loss: 4.4698\n",
      "Epoch [1/2], Step [29620/67476], Loss: 4.6452\n",
      "Epoch [1/2], Step [29630/67476], Loss: 4.5235\n",
      "Epoch [1/2], Step [29640/67476], Loss: 4.6323\n",
      "Epoch [1/2], Step [29650/67476], Loss: 4.5610\n",
      "Epoch [1/2], Step [29660/67476], Loss: 4.6024\n",
      "Epoch [1/2], Step [29670/67476], Loss: 4.5957\n",
      "Epoch [1/2], Step [29680/67476], Loss: 4.6663\n",
      "Epoch [1/2], Step [29690/67476], Loss: 4.6567\n",
      "Epoch [1/2], Step [29700/67476], Loss: 4.3873\n",
      "Epoch [1/2], Step [29710/67476], Loss: 4.5651\n",
      "Epoch [1/2], Step [29720/67476], Loss: 4.5666\n",
      "Epoch [1/2], Step [29730/67476], Loss: 4.6385\n",
      "Epoch [1/2], Step [29740/67476], Loss: 4.6659\n",
      "Epoch [1/2], Step [29750/67476], Loss: 4.5602\n",
      "Epoch [1/2], Step [29760/67476], Loss: 4.4861\n",
      "Epoch [1/2], Step [29770/67476], Loss: 4.7195\n",
      "Epoch [1/2], Step [29780/67476], Loss: 4.6487\n",
      "Epoch [1/2], Step [29790/67476], Loss: 4.6216\n",
      "Epoch [1/2], Step [29800/67476], Loss: 4.6167\n",
      "Epoch [1/2], Step [29810/67476], Loss: 4.6926\n",
      "Epoch [1/2], Step [29820/67476], Loss: 4.5941\n",
      "Epoch [1/2], Step [29830/67476], Loss: 4.7740\n",
      "Epoch [1/2], Step [29840/67476], Loss: 4.7076\n",
      "Epoch [1/2], Step [29850/67476], Loss: 4.6862\n",
      "Epoch [1/2], Step [29860/67476], Loss: 4.5644\n",
      "Epoch [1/2], Step [29870/67476], Loss: 4.6304\n",
      "Epoch [1/2], Step [29880/67476], Loss: 4.6729\n",
      "Epoch [1/2], Step [29890/67476], Loss: 4.4011\n",
      "Epoch [1/2], Step [29900/67476], Loss: 4.6315\n",
      "Epoch [1/2], Step [29910/67476], Loss: 4.3638\n",
      "Epoch [1/2], Step [29920/67476], Loss: 4.3846\n",
      "Epoch [1/2], Step [29930/67476], Loss: 4.4821\n",
      "Epoch [1/2], Step [29940/67476], Loss: 4.3495\n",
      "Epoch [1/2], Step [29950/67476], Loss: 4.7791\n",
      "Epoch [1/2], Step [29960/67476], Loss: 4.4070\n",
      "Epoch [1/2], Step [29970/67476], Loss: 4.5522\n",
      "Epoch [1/2], Step [29980/67476], Loss: 4.5845\n",
      "Epoch [1/2], Step [29990/67476], Loss: 4.5419\n",
      "Epoch [1/2], Step [30000/67476], Loss: 4.5302\n",
      "Epoch [1/2], Step [30010/67476], Loss: 4.4120\n",
      "Epoch [1/2], Step [30020/67476], Loss: 4.6541\n",
      "Epoch [1/2], Step [30030/67476], Loss: 4.6089\n",
      "Epoch [1/2], Step [30040/67476], Loss: 4.4978\n",
      "Epoch [1/2], Step [30050/67476], Loss: 4.6578\n",
      "Epoch [1/2], Step [30060/67476], Loss: 4.8004\n",
      "Epoch [1/2], Step [30070/67476], Loss: 4.6381\n",
      "Epoch [1/2], Step [30080/67476], Loss: 4.5653\n",
      "Epoch [1/2], Step [30090/67476], Loss: 4.4684\n",
      "Epoch [1/2], Step [30100/67476], Loss: 4.5794\n",
      "Epoch [1/2], Step [30110/67476], Loss: 4.5177\n",
      "Epoch [1/2], Step [30120/67476], Loss: 4.6435\n",
      "Epoch [1/2], Step [30130/67476], Loss: 4.5324\n",
      "Epoch [1/2], Step [30140/67476], Loss: 4.6592\n",
      "Epoch [1/2], Step [30150/67476], Loss: 4.8700\n",
      "Epoch [1/2], Step [30160/67476], Loss: 4.5551\n",
      "Epoch [1/2], Step [30170/67476], Loss: 4.5675\n",
      "Epoch [1/2], Step [30180/67476], Loss: 4.6996\n",
      "Epoch [1/2], Step [30190/67476], Loss: 4.6879\n",
      "Epoch [1/2], Step [30200/67476], Loss: 4.4921\n",
      "Epoch [1/2], Step [30210/67476], Loss: 4.5975\n",
      "Epoch [1/2], Step [30220/67476], Loss: 4.5408\n",
      "Epoch [1/2], Step [30230/67476], Loss: 4.5497\n",
      "Epoch [1/2], Step [30240/67476], Loss: 4.4765\n",
      "Epoch [1/2], Step [30250/67476], Loss: 4.4704\n",
      "Epoch [1/2], Step [30260/67476], Loss: 4.6253\n",
      "Epoch [1/2], Step [30270/67476], Loss: 4.3544\n",
      "Epoch [1/2], Step [30280/67476], Loss: 4.4616\n",
      "Epoch [1/2], Step [30290/67476], Loss: 4.6029\n",
      "Epoch [1/2], Step [30300/67476], Loss: 4.6010\n",
      "Epoch [1/2], Step [30310/67476], Loss: 4.5409\n",
      "Epoch [1/2], Step [30320/67476], Loss: 4.6081\n",
      "Epoch [1/2], Step [30330/67476], Loss: 4.6646\n",
      "Epoch [1/2], Step [30340/67476], Loss: 4.4823\n",
      "Epoch [1/2], Step [30350/67476], Loss: 4.5305\n",
      "Epoch [1/2], Step [30360/67476], Loss: 4.4855\n",
      "Epoch [1/2], Step [30370/67476], Loss: 4.6087\n",
      "Epoch [1/2], Step [30380/67476], Loss: 4.4510\n",
      "Epoch [1/2], Step [30390/67476], Loss: 4.5462\n",
      "Epoch [1/2], Step [30400/67476], Loss: 4.7408\n",
      "Epoch [1/2], Step [30410/67476], Loss: 4.3921\n",
      "Epoch [1/2], Step [30420/67476], Loss: 4.5853\n",
      "Epoch [1/2], Step [30430/67476], Loss: 4.5412\n",
      "Epoch [1/2], Step [30440/67476], Loss: 4.3797\n",
      "Epoch [1/2], Step [30450/67476], Loss: 4.6368\n",
      "Epoch [1/2], Step [30460/67476], Loss: 4.4283\n",
      "Epoch [1/2], Step [30470/67476], Loss: 4.6445\n",
      "Epoch [1/2], Step [30480/67476], Loss: 4.5716\n",
      "Epoch [1/2], Step [30490/67476], Loss: 4.7712\n",
      "Epoch [1/2], Step [30500/67476], Loss: 4.5020\n",
      "Epoch [1/2], Step [30510/67476], Loss: 4.6545\n",
      "Epoch [1/2], Step [30520/67476], Loss: 4.7283\n",
      "Epoch [1/2], Step [30530/67476], Loss: 4.6865\n",
      "Epoch [1/2], Step [30540/67476], Loss: 4.8643\n",
      "Epoch [1/2], Step [30550/67476], Loss: 4.5892\n",
      "Epoch [1/2], Step [30560/67476], Loss: 4.5112\n",
      "Epoch [1/2], Step [30570/67476], Loss: 4.6007\n",
      "Epoch [1/2], Step [30580/67476], Loss: 4.6095\n",
      "Epoch [1/2], Step [30590/67476], Loss: 4.5481\n",
      "Epoch [1/2], Step [30600/67476], Loss: 4.5902\n",
      "Epoch [1/2], Step [30610/67476], Loss: 4.6271\n",
      "Epoch [1/2], Step [30620/67476], Loss: 4.4647\n",
      "Epoch [1/2], Step [30630/67476], Loss: 4.6918\n",
      "Epoch [1/2], Step [30640/67476], Loss: 4.5843\n",
      "Epoch [1/2], Step [30650/67476], Loss: 4.6397\n",
      "Epoch [1/2], Step [30660/67476], Loss: 4.5937\n",
      "Epoch [1/2], Step [30670/67476], Loss: 4.3448\n",
      "Epoch [1/2], Step [30680/67476], Loss: 4.4886\n",
      "Epoch [1/2], Step [30690/67476], Loss: 4.5033\n",
      "Epoch [1/2], Step [30700/67476], Loss: 4.5307\n",
      "Epoch [1/2], Step [30710/67476], Loss: 4.5775\n",
      "Epoch [1/2], Step [30720/67476], Loss: 4.6661\n",
      "Epoch [1/2], Step [30730/67476], Loss: 4.7051\n",
      "Epoch [1/2], Step [30740/67476], Loss: 4.7347\n",
      "Epoch [1/2], Step [30750/67476], Loss: 4.6068\n",
      "Epoch [1/2], Step [30760/67476], Loss: 4.5358\n",
      "Epoch [1/2], Step [30770/67476], Loss: 4.4214\n",
      "Epoch [1/2], Step [30780/67476], Loss: 4.7157\n",
      "Epoch [1/2], Step [30790/67476], Loss: 4.6728\n",
      "Epoch [1/2], Step [30800/67476], Loss: 4.7758\n",
      "Epoch [1/2], Step [30810/67476], Loss: 4.6335\n",
      "Epoch [1/2], Step [30820/67476], Loss: 4.6657\n",
      "Epoch [1/2], Step [30830/67476], Loss: 4.5104\n",
      "Epoch [1/2], Step [30840/67476], Loss: 4.5746\n",
      "Epoch [1/2], Step [30850/67476], Loss: 4.5032\n",
      "Epoch [1/2], Step [30860/67476], Loss: 4.6952\n",
      "Epoch [1/2], Step [30870/67476], Loss: 4.7493\n",
      "Epoch [1/2], Step [30880/67476], Loss: 4.5178\n",
      "Epoch [1/2], Step [30890/67476], Loss: 4.6737\n",
      "Epoch [1/2], Step [30900/67476], Loss: 4.4996\n",
      "Epoch [1/2], Step [30910/67476], Loss: 4.4265\n",
      "Epoch [1/2], Step [30920/67476], Loss: 4.6287\n",
      "Epoch [1/2], Step [30930/67476], Loss: 4.7384\n",
      "Epoch [1/2], Step [30940/67476], Loss: 4.5031\n",
      "Epoch [1/2], Step [30950/67476], Loss: 4.5189\n",
      "Epoch [1/2], Step [30960/67476], Loss: 4.8156\n",
      "Epoch [1/2], Step [30970/67476], Loss: 4.7670\n",
      "Epoch [1/2], Step [30980/67476], Loss: 4.6946\n",
      "Epoch [1/2], Step [30990/67476], Loss: 4.7803\n",
      "Epoch [1/2], Step [31000/67476], Loss: 4.4845\n",
      "Epoch [1/2], Step [31010/67476], Loss: 4.6212\n",
      "Epoch [1/2], Step [31020/67476], Loss: 4.3021\n",
      "Epoch [1/2], Step [31030/67476], Loss: 4.6040\n",
      "Epoch [1/2], Step [31040/67476], Loss: 4.7247\n",
      "Epoch [1/2], Step [31050/67476], Loss: 4.5619\n",
      "Epoch [1/2], Step [31060/67476], Loss: 4.7571\n",
      "Epoch [1/2], Step [31070/67476], Loss: 4.5856\n",
      "Epoch [1/2], Step [31080/67476], Loss: 4.4831\n",
      "Epoch [1/2], Step [31090/67476], Loss: 4.5998\n",
      "Epoch [1/2], Step [31100/67476], Loss: 4.2786\n",
      "Epoch [1/2], Step [31110/67476], Loss: 4.7674\n",
      "Epoch [1/2], Step [31120/67476], Loss: 4.5959\n",
      "Epoch [1/2], Step [31130/67476], Loss: 4.6818\n",
      "Epoch [1/2], Step [31140/67476], Loss: 4.5271\n",
      "Epoch [1/2], Step [31150/67476], Loss: 4.6731\n",
      "Epoch [1/2], Step [31160/67476], Loss: 4.6374\n",
      "Epoch [1/2], Step [31170/67476], Loss: 4.6230\n",
      "Epoch [1/2], Step [31180/67476], Loss: 4.4605\n",
      "Epoch [1/2], Step [31190/67476], Loss: 4.6493\n",
      "Epoch [1/2], Step [31200/67476], Loss: 4.7058\n",
      "Epoch [1/2], Step [31210/67476], Loss: 4.7241\n",
      "Epoch [1/2], Step [31220/67476], Loss: 4.5674\n",
      "Epoch [1/2], Step [31230/67476], Loss: 4.4408\n",
      "Epoch [1/2], Step [31240/67476], Loss: 4.7865\n",
      "Epoch [1/2], Step [31250/67476], Loss: 4.5185\n",
      "Epoch [1/2], Step [31260/67476], Loss: 4.6824\n",
      "Epoch [1/2], Step [31270/67476], Loss: 4.7401\n",
      "Epoch [1/2], Step [31280/67476], Loss: 4.4824\n",
      "Epoch [1/2], Step [31290/67476], Loss: 4.5799\n",
      "Epoch [1/2], Step [31300/67476], Loss: 4.4786\n",
      "Epoch [1/2], Step [31310/67476], Loss: 4.4688\n",
      "Epoch [1/2], Step [31320/67476], Loss: 4.6021\n",
      "Epoch [1/2], Step [31330/67476], Loss: 4.3526\n",
      "Epoch [1/2], Step [31340/67476], Loss: 4.6646\n",
      "Epoch [1/2], Step [31350/67476], Loss: 4.5312\n",
      "Epoch [1/2], Step [31360/67476], Loss: 4.4819\n",
      "Epoch [1/2], Step [31370/67476], Loss: 4.5974\n",
      "Epoch [1/2], Step [31380/67476], Loss: 4.4805\n",
      "Epoch [1/2], Step [31390/67476], Loss: 4.5689\n",
      "Epoch [1/2], Step [31400/67476], Loss: 4.7765\n",
      "Epoch [1/2], Step [31410/67476], Loss: 4.4274\n",
      "Epoch [1/2], Step [31420/67476], Loss: 4.4703\n",
      "Epoch [1/2], Step [31430/67476], Loss: 4.6848\n",
      "Epoch [1/2], Step [31440/67476], Loss: 4.5705\n",
      "Epoch [1/2], Step [31450/67476], Loss: 4.4294\n",
      "Epoch [1/2], Step [31460/67476], Loss: 4.8565\n",
      "Epoch [1/2], Step [31470/67476], Loss: 4.5859\n",
      "Epoch [1/2], Step [31480/67476], Loss: 4.5928\n",
      "Epoch [1/2], Step [31490/67476], Loss: 4.7505\n",
      "Epoch [1/2], Step [31500/67476], Loss: 4.4668\n",
      "Epoch [1/2], Step [31510/67476], Loss: 4.5357\n",
      "Epoch [1/2], Step [31520/67476], Loss: 4.4513\n",
      "Epoch [1/2], Step [31530/67476], Loss: 4.5312\n",
      "Epoch [1/2], Step [31540/67476], Loss: 4.8790\n",
      "Epoch [1/2], Step [31550/67476], Loss: 4.7909\n",
      "Epoch [1/2], Step [31560/67476], Loss: 4.6956\n",
      "Epoch [1/2], Step [31570/67476], Loss: 4.5044\n",
      "Epoch [1/2], Step [31580/67476], Loss: 4.4733\n",
      "Epoch [1/2], Step [31590/67476], Loss: 4.6148\n",
      "Epoch [1/2], Step [31600/67476], Loss: 4.8077\n",
      "Epoch [1/2], Step [31610/67476], Loss: 4.5206\n",
      "Epoch [1/2], Step [31620/67476], Loss: 4.5774\n",
      "Epoch [1/2], Step [31630/67476], Loss: 4.6671\n",
      "Epoch [1/2], Step [31640/67476], Loss: 4.5507\n",
      "Epoch [1/2], Step [31650/67476], Loss: 4.6744\n",
      "Epoch [1/2], Step [31660/67476], Loss: 4.3719\n",
      "Epoch [1/2], Step [31670/67476], Loss: 4.4780\n",
      "Epoch [1/2], Step [31680/67476], Loss: 4.5869\n",
      "Epoch [1/2], Step [31690/67476], Loss: 4.5053\n",
      "Epoch [1/2], Step [31700/67476], Loss: 4.6143\n",
      "Epoch [1/2], Step [31710/67476], Loss: 4.3918\n",
      "Epoch [1/2], Step [31720/67476], Loss: 4.5039\n",
      "Epoch [1/2], Step [31730/67476], Loss: 4.6326\n",
      "Epoch [1/2], Step [31740/67476], Loss: 4.6715\n",
      "Epoch [1/2], Step [31750/67476], Loss: 4.4763\n",
      "Epoch [1/2], Step [31760/67476], Loss: 4.6729\n",
      "Epoch [1/2], Step [31770/67476], Loss: 4.6785\n",
      "Epoch [1/2], Step [31780/67476], Loss: 4.6304\n",
      "Epoch [1/2], Step [31790/67476], Loss: 4.6718\n",
      "Epoch [1/2], Step [31800/67476], Loss: 4.6389\n",
      "Epoch [1/2], Step [31810/67476], Loss: 4.5794\n",
      "Epoch [1/2], Step [31820/67476], Loss: 4.6438\n",
      "Epoch [1/2], Step [31830/67476], Loss: 4.6321\n",
      "Epoch [1/2], Step [31840/67476], Loss: 4.4689\n",
      "Epoch [1/2], Step [31850/67476], Loss: 4.5490\n",
      "Epoch [1/2], Step [31860/67476], Loss: 4.6037\n",
      "Epoch [1/2], Step [31870/67476], Loss: 4.4092\n",
      "Epoch [1/2], Step [31880/67476], Loss: 4.6902\n",
      "Epoch [1/2], Step [31890/67476], Loss: 4.4324\n",
      "Epoch [1/2], Step [31900/67476], Loss: 4.6519\n",
      "Epoch [1/2], Step [31910/67476], Loss: 4.6071\n",
      "Epoch [1/2], Step [31920/67476], Loss: 4.5193\n",
      "Epoch [1/2], Step [31930/67476], Loss: 4.4637\n",
      "Epoch [1/2], Step [31940/67476], Loss: 4.7352\n",
      "Epoch [1/2], Step [31950/67476], Loss: 4.5433\n",
      "Epoch [1/2], Step [31960/67476], Loss: 4.6598\n",
      "Epoch [1/2], Step [31970/67476], Loss: 4.4909\n",
      "Epoch [1/2], Step [31980/67476], Loss: 4.5281\n",
      "Epoch [1/2], Step [31990/67476], Loss: 4.4297\n",
      "Epoch [1/2], Step [32000/67476], Loss: 4.6248\n",
      "Epoch [1/2], Step [32010/67476], Loss: 4.3018\n",
      "Epoch [1/2], Step [32020/67476], Loss: 4.4274\n",
      "Epoch [1/2], Step [32030/67476], Loss: 4.6208\n",
      "Epoch [1/2], Step [32040/67476], Loss: 4.7362\n",
      "Epoch [1/2], Step [32050/67476], Loss: 4.6669\n",
      "Epoch [1/2], Step [32060/67476], Loss: 4.6236\n",
      "Epoch [1/2], Step [32070/67476], Loss: 4.7192\n",
      "Epoch [1/2], Step [32080/67476], Loss: 4.3916\n",
      "Epoch [1/2], Step [32090/67476], Loss: 4.7496\n",
      "Epoch [1/2], Step [32100/67476], Loss: 4.5307\n",
      "Epoch [1/2], Step [32110/67476], Loss: 4.6174\n",
      "Epoch [1/2], Step [32120/67476], Loss: 4.6098\n",
      "Epoch [1/2], Step [32130/67476], Loss: 4.4850\n",
      "Epoch [1/2], Step [32140/67476], Loss: 4.5395\n",
      "Epoch [1/2], Step [32150/67476], Loss: 4.7481\n",
      "Epoch [1/2], Step [32160/67476], Loss: 4.5154\n",
      "Epoch [1/2], Step [32170/67476], Loss: 4.6183\n",
      "Epoch [1/2], Step [32180/67476], Loss: 4.3271\n",
      "Epoch [1/2], Step [32190/67476], Loss: 4.8098\n",
      "Epoch [1/2], Step [32200/67476], Loss: 4.3469\n",
      "Epoch [1/2], Step [32210/67476], Loss: 4.4666\n",
      "Epoch [1/2], Step [32220/67476], Loss: 4.6713\n",
      "Epoch [1/2], Step [32230/67476], Loss: 4.5440\n",
      "Epoch [1/2], Step [32240/67476], Loss: 4.6824\n",
      "Epoch [1/2], Step [32250/67476], Loss: 4.6584\n",
      "Epoch [1/2], Step [32260/67476], Loss: 4.4832\n",
      "Epoch [1/2], Step [32270/67476], Loss: 4.3898\n",
      "Epoch [1/2], Step [32280/67476], Loss: 4.5308\n",
      "Epoch [1/2], Step [32290/67476], Loss: 4.6758\n",
      "Epoch [1/2], Step [32300/67476], Loss: 4.5636\n",
      "Epoch [1/2], Step [32310/67476], Loss: 4.6258\n",
      "Epoch [1/2], Step [32320/67476], Loss: 4.3903\n",
      "Epoch [1/2], Step [32330/67476], Loss: 4.3937\n",
      "Epoch [1/2], Step [32340/67476], Loss: 4.7466\n",
      "Epoch [1/2], Step [32350/67476], Loss: 4.7883\n",
      "Epoch [1/2], Step [32360/67476], Loss: 4.6282\n",
      "Epoch [1/2], Step [32370/67476], Loss: 4.3987\n",
      "Epoch [1/2], Step [32380/67476], Loss: 4.5889\n",
      "Epoch [1/2], Step [32390/67476], Loss: 4.7657\n",
      "Epoch [1/2], Step [32400/67476], Loss: 4.6544\n",
      "Epoch [1/2], Step [32410/67476], Loss: 4.5184\n",
      "Epoch [1/2], Step [32420/67476], Loss: 4.5679\n",
      "Epoch [1/2], Step [32430/67476], Loss: 4.6836\n",
      "Epoch [1/2], Step [32440/67476], Loss: 4.7485\n",
      "Epoch [1/2], Step [32450/67476], Loss: 4.6635\n",
      "Epoch [1/2], Step [32460/67476], Loss: 4.4293\n",
      "Epoch [1/2], Step [32470/67476], Loss: 4.5141\n",
      "Epoch [1/2], Step [32480/67476], Loss: 4.7279\n",
      "Epoch [1/2], Step [32490/67476], Loss: 4.5010\n",
      "Epoch [1/2], Step [32500/67476], Loss: 4.4065\n",
      "Epoch [1/2], Step [32510/67476], Loss: 4.3415\n",
      "Epoch [1/2], Step [32520/67476], Loss: 4.7055\n",
      "Epoch [1/2], Step [32530/67476], Loss: 4.7623\n",
      "Epoch [1/2], Step [32540/67476], Loss: 4.5723\n",
      "Epoch [1/2], Step [32550/67476], Loss: 4.5541\n",
      "Epoch [1/2], Step [32560/67476], Loss: 4.7479\n",
      "Epoch [1/2], Step [32570/67476], Loss: 4.4956\n",
      "Epoch [1/2], Step [32580/67476], Loss: 4.4925\n",
      "Epoch [1/2], Step [32590/67476], Loss: 4.3984\n",
      "Epoch [1/2], Step [32600/67476], Loss: 4.5223\n",
      "Epoch [1/2], Step [32610/67476], Loss: 4.6589\n",
      "Epoch [1/2], Step [32620/67476], Loss: 4.6454\n",
      "Epoch [1/2], Step [32630/67476], Loss: 4.6067\n",
      "Epoch [1/2], Step [32640/67476], Loss: 4.4998\n",
      "Epoch [1/2], Step [32650/67476], Loss: 4.6347\n",
      "Epoch [1/2], Step [32660/67476], Loss: 4.7440\n",
      "Epoch [1/2], Step [32670/67476], Loss: 4.7046\n",
      "Epoch [1/2], Step [32680/67476], Loss: 4.6043\n",
      "Epoch [1/2], Step [32690/67476], Loss: 4.6639\n",
      "Epoch [1/2], Step [32700/67476], Loss: 4.5332\n",
      "Epoch [1/2], Step [32710/67476], Loss: 4.5508\n",
      "Epoch [1/2], Step [32720/67476], Loss: 4.5590\n",
      "Epoch [1/2], Step [32730/67476], Loss: 4.6413\n",
      "Epoch [1/2], Step [32740/67476], Loss: 4.4737\n",
      "Epoch [1/2], Step [32750/67476], Loss: 4.5431\n",
      "Epoch [1/2], Step [32760/67476], Loss: 4.5262\n",
      "Epoch [1/2], Step [32770/67476], Loss: 4.6621\n",
      "Epoch [1/2], Step [32780/67476], Loss: 4.5082\n",
      "Epoch [1/2], Step [32790/67476], Loss: 4.5000\n",
      "Epoch [1/2], Step [32800/67476], Loss: 4.6348\n",
      "Epoch [1/2], Step [32810/67476], Loss: 4.4461\n",
      "Epoch [1/2], Step [32820/67476], Loss: 4.4765\n",
      "Epoch [1/2], Step [32830/67476], Loss: 4.5422\n",
      "Epoch [1/2], Step [32840/67476], Loss: 4.4372\n",
      "Epoch [1/2], Step [32850/67476], Loss: 4.6945\n",
      "Epoch [1/2], Step [32860/67476], Loss: 4.3992\n",
      "Epoch [1/2], Step [32870/67476], Loss: 4.5659\n",
      "Epoch [1/2], Step [32880/67476], Loss: 4.6473\n",
      "Epoch [1/2], Step [32890/67476], Loss: 4.5817\n",
      "Epoch [1/2], Step [32900/67476], Loss: 4.8413\n",
      "Epoch [1/2], Step [32910/67476], Loss: 4.2185\n",
      "Epoch [1/2], Step [32920/67476], Loss: 4.4989\n",
      "Epoch [1/2], Step [32930/67476], Loss: 4.6009\n",
      "Epoch [1/2], Step [32940/67476], Loss: 4.4418\n",
      "Epoch [1/2], Step [32950/67476], Loss: 4.6364\n",
      "Epoch [1/2], Step [32960/67476], Loss: 4.4472\n",
      "Epoch [1/2], Step [32970/67476], Loss: 4.5972\n",
      "Epoch [1/2], Step [32980/67476], Loss: 4.6710\n",
      "Epoch [1/2], Step [32990/67476], Loss: 4.7497\n",
      "Epoch [1/2], Step [33000/67476], Loss: 4.4770\n",
      "Epoch [1/2], Step [33010/67476], Loss: 4.6945\n",
      "Epoch [1/2], Step [33020/67476], Loss: 4.4065\n",
      "Epoch [1/2], Step [33030/67476], Loss: 4.4544\n",
      "Epoch [1/2], Step [33040/67476], Loss: 4.5164\n",
      "Epoch [1/2], Step [33050/67476], Loss: 4.4921\n",
      "Epoch [1/2], Step [33060/67476], Loss: 4.6655\n",
      "Epoch [1/2], Step [33070/67476], Loss: 4.5114\n",
      "Epoch [1/2], Step [33080/67476], Loss: 4.5664\n",
      "Epoch [1/2], Step [33090/67476], Loss: 4.6878\n",
      "Epoch [1/2], Step [33100/67476], Loss: 4.4657\n",
      "Epoch [1/2], Step [33110/67476], Loss: 4.5224\n",
      "Epoch [1/2], Step [33120/67476], Loss: 4.6292\n",
      "Epoch [1/2], Step [33130/67476], Loss: 4.6444\n",
      "Epoch [1/2], Step [33140/67476], Loss: 4.5891\n",
      "Epoch [1/2], Step [33150/67476], Loss: 4.5539\n",
      "Epoch [1/2], Step [33160/67476], Loss: 4.4093\n",
      "Epoch [1/2], Step [33170/67476], Loss: 4.4989\n",
      "Epoch [1/2], Step [33180/67476], Loss: 4.6176\n",
      "Epoch [1/2], Step [33190/67476], Loss: 4.6608\n",
      "Epoch [1/2], Step [33200/67476], Loss: 4.4547\n",
      "Epoch [1/2], Step [33210/67476], Loss: 4.6714\n",
      "Epoch [1/2], Step [33220/67476], Loss: 4.6346\n",
      "Epoch [1/2], Step [33230/67476], Loss: 4.5024\n",
      "Epoch [1/2], Step [33240/67476], Loss: 4.7782\n",
      "Epoch [1/2], Step [33250/67476], Loss: 4.6058\n",
      "Epoch [1/2], Step [33260/67476], Loss: 4.4796\n",
      "Epoch [1/2], Step [33270/67476], Loss: 4.6251\n",
      "Epoch [1/2], Step [33280/67476], Loss: 4.6283\n",
      "Epoch [1/2], Step [33290/67476], Loss: 4.6539\n",
      "Epoch [1/2], Step [33300/67476], Loss: 4.3841\n",
      "Epoch [1/2], Step [33310/67476], Loss: 4.6498\n",
      "Epoch [1/2], Step [33320/67476], Loss: 4.6009\n",
      "Epoch [1/2], Step [33330/67476], Loss: 4.3850\n",
      "Epoch [1/2], Step [33340/67476], Loss: 4.4625\n",
      "Epoch [1/2], Step [33350/67476], Loss: 4.4458\n",
      "Epoch [1/2], Step [33360/67476], Loss: 4.4091\n",
      "Epoch [1/2], Step [33370/67476], Loss: 4.4919\n",
      "Epoch [1/2], Step [33380/67476], Loss: 4.3957\n",
      "Epoch [1/2], Step [33390/67476], Loss: 4.5224\n",
      "Epoch [1/2], Step [33400/67476], Loss: 4.5729\n",
      "Epoch [1/2], Step [33410/67476], Loss: 4.5652\n",
      "Epoch [1/2], Step [33420/67476], Loss: 4.6055\n",
      "Epoch [1/2], Step [33430/67476], Loss: 4.4279\n",
      "Epoch [1/2], Step [33440/67476], Loss: 4.6238\n",
      "Epoch [1/2], Step [33450/67476], Loss: 4.5712\n",
      "Epoch [1/2], Step [33460/67476], Loss: 4.4017\n",
      "Epoch [1/2], Step [33470/67476], Loss: 4.7243\n",
      "Epoch [1/2], Step [33480/67476], Loss: 4.6460\n",
      "Epoch [1/2], Step [33490/67476], Loss: 4.3995\n",
      "Epoch [1/2], Step [33500/67476], Loss: 4.4501\n",
      "Epoch [1/2], Step [33510/67476], Loss: 4.5378\n",
      "Epoch [1/2], Step [33520/67476], Loss: 4.6392\n",
      "Epoch [1/2], Step [33530/67476], Loss: 4.7788\n",
      "Epoch [1/2], Step [33540/67476], Loss: 4.9149\n",
      "Epoch [1/2], Step [33550/67476], Loss: 4.6760\n",
      "Epoch [1/2], Step [33560/67476], Loss: 4.7402\n",
      "Epoch [1/2], Step [33570/67476], Loss: 4.4437\n",
      "Epoch [1/2], Step [33580/67476], Loss: 4.5190\n",
      "Epoch [1/2], Step [33590/67476], Loss: 4.4776\n",
      "Epoch [1/2], Step [33600/67476], Loss: 4.4368\n",
      "Epoch [1/2], Step [33610/67476], Loss: 4.7069\n",
      "Epoch [1/2], Step [33620/67476], Loss: 4.5911\n",
      "Epoch [1/2], Step [33630/67476], Loss: 4.5649\n",
      "Epoch [1/2], Step [33640/67476], Loss: 4.6208\n",
      "Epoch [1/2], Step [33650/67476], Loss: 4.4993\n",
      "Epoch [1/2], Step [33660/67476], Loss: 4.6997\n",
      "Epoch [1/2], Step [33670/67476], Loss: 4.4613\n",
      "Epoch [1/2], Step [33680/67476], Loss: 4.7341\n",
      "Epoch [1/2], Step [33690/67476], Loss: 4.5193\n",
      "Epoch [1/2], Step [33700/67476], Loss: 4.5648\n",
      "Epoch [1/2], Step [33710/67476], Loss: 4.4304\n",
      "Epoch [1/2], Step [33720/67476], Loss: 4.5257\n",
      "Epoch [1/2], Step [33730/67476], Loss: 4.6898\n",
      "Epoch [1/2], Step [33740/67476], Loss: 4.6510\n",
      "Epoch [1/2], Step [33750/67476], Loss: 4.4989\n",
      "Epoch [1/2], Step [33760/67476], Loss: 4.5863\n",
      "Epoch [1/2], Step [33770/67476], Loss: 4.4878\n",
      "Epoch [1/2], Step [33780/67476], Loss: 4.7338\n",
      "Epoch [1/2], Step [33790/67476], Loss: 4.5768\n",
      "Epoch [1/2], Step [33800/67476], Loss: 4.5551\n",
      "Epoch [1/2], Step [33810/67476], Loss: 4.7383\n",
      "Epoch [1/2], Step [33820/67476], Loss: 4.5616\n",
      "Epoch [1/2], Step [33830/67476], Loss: 4.6743\n",
      "Epoch [1/2], Step [33840/67476], Loss: 4.5733\n",
      "Epoch [1/2], Step [33850/67476], Loss: 4.6216\n",
      "Epoch [1/2], Step [33860/67476], Loss: 4.4533\n",
      "Epoch [1/2], Step [33870/67476], Loss: 4.6362\n",
      "Epoch [1/2], Step [33880/67476], Loss: 4.5861\n",
      "Epoch [1/2], Step [33890/67476], Loss: 4.2454\n",
      "Epoch [1/2], Step [33900/67476], Loss: 4.6495\n",
      "Epoch [1/2], Step [33910/67476], Loss: 4.7735\n",
      "Epoch [1/2], Step [33920/67476], Loss: 4.6456\n",
      "Epoch [1/2], Step [33930/67476], Loss: 4.5622\n",
      "Epoch [1/2], Step [33940/67476], Loss: 4.4734\n",
      "Epoch [1/2], Step [33950/67476], Loss: 4.4152\n",
      "Epoch [1/2], Step [33960/67476], Loss: 4.6059\n",
      "Epoch [1/2], Step [33970/67476], Loss: 4.5067\n",
      "Epoch [1/2], Step [33980/67476], Loss: 4.6080\n",
      "Epoch [1/2], Step [33990/67476], Loss: 4.5066\n",
      "Epoch [1/2], Step [34000/67476], Loss: 4.5693\n",
      "Epoch [1/2], Step [34010/67476], Loss: 4.3480\n",
      "Epoch [1/2], Step [34020/67476], Loss: 4.4579\n",
      "Epoch [1/2], Step [34030/67476], Loss: 4.7608\n",
      "Epoch [1/2], Step [34040/67476], Loss: 4.3917\n",
      "Epoch [1/2], Step [34050/67476], Loss: 4.5678\n",
      "Epoch [1/2], Step [34060/67476], Loss: 4.3296\n",
      "Epoch [1/2], Step [34070/67476], Loss: 4.4118\n",
      "Epoch [1/2], Step [34080/67476], Loss: 4.5067\n",
      "Epoch [1/2], Step [34090/67476], Loss: 4.4903\n",
      "Epoch [1/2], Step [34100/67476], Loss: 4.6197\n",
      "Epoch [1/2], Step [34110/67476], Loss: 4.6071\n",
      "Epoch [1/2], Step [34120/67476], Loss: 4.4819\n",
      "Epoch [1/2], Step [34130/67476], Loss: 4.4603\n",
      "Epoch [1/2], Step [34140/67476], Loss: 4.4806\n",
      "Epoch [1/2], Step [34150/67476], Loss: 4.6205\n",
      "Epoch [1/2], Step [34160/67476], Loss: 4.6877\n",
      "Epoch [1/2], Step [34170/67476], Loss: 4.4805\n",
      "Epoch [1/2], Step [34180/67476], Loss: 4.6758\n",
      "Epoch [1/2], Step [34190/67476], Loss: 4.6251\n",
      "Epoch [1/2], Step [34200/67476], Loss: 4.4073\n",
      "Epoch [1/2], Step [34210/67476], Loss: 4.6780\n",
      "Epoch [1/2], Step [34220/67476], Loss: 4.5403\n",
      "Epoch [1/2], Step [34230/67476], Loss: 4.4097\n",
      "Epoch [1/2], Step [34240/67476], Loss: 4.6670\n",
      "Epoch [1/2], Step [34250/67476], Loss: 4.6778\n",
      "Epoch [1/2], Step [34260/67476], Loss: 4.5672\n",
      "Epoch [1/2], Step [34270/67476], Loss: 4.2961\n",
      "Epoch [1/2], Step [34280/67476], Loss: 4.6158\n",
      "Epoch [1/2], Step [34290/67476], Loss: 4.8915\n",
      "Epoch [1/2], Step [34300/67476], Loss: 4.5100\n",
      "Epoch [1/2], Step [34310/67476], Loss: 4.5777\n",
      "Epoch [1/2], Step [34320/67476], Loss: 4.3284\n",
      "Epoch [1/2], Step [34330/67476], Loss: 4.6360\n",
      "Epoch [1/2], Step [34340/67476], Loss: 4.5426\n",
      "Epoch [1/2], Step [34350/67476], Loss: 4.5436\n",
      "Epoch [1/2], Step [34360/67476], Loss: 4.5390\n",
      "Epoch [1/2], Step [34370/67476], Loss: 4.7728\n",
      "Epoch [1/2], Step [34380/67476], Loss: 4.4328\n",
      "Epoch [1/2], Step [34390/67476], Loss: 4.4831\n",
      "Epoch [1/2], Step [34400/67476], Loss: 4.6202\n",
      "Epoch [1/2], Step [34410/67476], Loss: 4.8537\n",
      "Epoch [1/2], Step [34420/67476], Loss: 4.6111\n",
      "Epoch [1/2], Step [34430/67476], Loss: 4.5020\n",
      "Epoch [1/2], Step [34440/67476], Loss: 4.4100\n",
      "Epoch [1/2], Step [34450/67476], Loss: 4.6591\n",
      "Epoch [1/2], Step [34460/67476], Loss: 4.5714\n",
      "Epoch [1/2], Step [34470/67476], Loss: 4.6307\n",
      "Epoch [1/2], Step [34480/67476], Loss: 4.6732\n",
      "Epoch [1/2], Step [34490/67476], Loss: 4.5138\n",
      "Epoch [1/2], Step [34500/67476], Loss: 4.8480\n",
      "Epoch [1/2], Step [34510/67476], Loss: 4.6084\n",
      "Epoch [1/2], Step [34520/67476], Loss: 4.6776\n",
      "Epoch [1/2], Step [34530/67476], Loss: 4.5074\n",
      "Epoch [1/2], Step [34540/67476], Loss: 4.4692\n",
      "Epoch [1/2], Step [34550/67476], Loss: 4.5865\n",
      "Epoch [1/2], Step [34560/67476], Loss: 4.4907\n",
      "Epoch [1/2], Step [34570/67476], Loss: 4.5648\n",
      "Epoch [1/2], Step [34580/67476], Loss: 4.5070\n",
      "Epoch [1/2], Step [34590/67476], Loss: 4.4606\n",
      "Epoch [1/2], Step [34600/67476], Loss: 4.5442\n",
      "Epoch [1/2], Step [34610/67476], Loss: 4.5092\n",
      "Epoch [1/2], Step [34620/67476], Loss: 4.6230\n",
      "Epoch [1/2], Step [34630/67476], Loss: 4.6055\n",
      "Epoch [1/2], Step [34640/67476], Loss: 4.6596\n",
      "Epoch [1/2], Step [34650/67476], Loss: 4.6527\n",
      "Epoch [1/2], Step [34660/67476], Loss: 4.4220\n",
      "Epoch [1/2], Step [34670/67476], Loss: 4.5948\n",
      "Epoch [1/2], Step [34680/67476], Loss: 4.6697\n",
      "Epoch [1/2], Step [34690/67476], Loss: 4.4498\n",
      "Epoch [1/2], Step [34700/67476], Loss: 4.6094\n",
      "Epoch [1/2], Step [34710/67476], Loss: 4.6264\n",
      "Epoch [1/2], Step [34720/67476], Loss: 4.6942\n",
      "Epoch [1/2], Step [34730/67476], Loss: 4.5310\n",
      "Epoch [1/2], Step [34740/67476], Loss: 4.5484\n",
      "Epoch [1/2], Step [34750/67476], Loss: 4.4367\n",
      "Epoch [1/2], Step [34760/67476], Loss: 4.6663\n",
      "Epoch [1/2], Step [34770/67476], Loss: 4.5128\n",
      "Epoch [1/2], Step [34780/67476], Loss: 4.4186\n",
      "Epoch [1/2], Step [34790/67476], Loss: 4.6494\n",
      "Epoch [1/2], Step [34800/67476], Loss: 4.2868\n",
      "Epoch [1/2], Step [34810/67476], Loss: 4.2922\n",
      "Epoch [1/2], Step [34820/67476], Loss: 4.7017\n",
      "Epoch [1/2], Step [34830/67476], Loss: 4.5011\n",
      "Epoch [1/2], Step [34840/67476], Loss: 4.8431\n",
      "Epoch [1/2], Step [34850/67476], Loss: 4.6390\n",
      "Epoch [1/2], Step [34860/67476], Loss: 4.6077\n",
      "Epoch [1/2], Step [34870/67476], Loss: 4.4825\n",
      "Epoch [1/2], Step [34880/67476], Loss: 4.4853\n",
      "Epoch [1/2], Step [34890/67476], Loss: 4.5798\n",
      "Epoch [1/2], Step [34900/67476], Loss: 4.6761\n",
      "Epoch [1/2], Step [34910/67476], Loss: 4.4948\n",
      "Epoch [1/2], Step [34920/67476], Loss: 4.5735\n",
      "Epoch [1/2], Step [34930/67476], Loss: 4.5546\n",
      "Epoch [1/2], Step [34940/67476], Loss: 4.6077\n",
      "Epoch [1/2], Step [34950/67476], Loss: 4.4953\n",
      "Epoch [1/2], Step [34960/67476], Loss: 4.6102\n",
      "Epoch [1/2], Step [34970/67476], Loss: 4.4667\n",
      "Epoch [1/2], Step [34980/67476], Loss: 4.5227\n",
      "Epoch [1/2], Step [34990/67476], Loss: 4.7008\n",
      "Epoch [1/2], Step [35000/67476], Loss: 4.6104\n",
      "Epoch [1/2], Step [35010/67476], Loss: 4.5839\n",
      "Epoch [1/2], Step [35020/67476], Loss: 4.6443\n",
      "Epoch [1/2], Step [35030/67476], Loss: 4.4414\n",
      "Epoch [1/2], Step [35040/67476], Loss: 4.5669\n",
      "Epoch [1/2], Step [35050/67476], Loss: 4.4250\n",
      "Epoch [1/2], Step [35060/67476], Loss: 4.4807\n",
      "Epoch [1/2], Step [35070/67476], Loss: 4.6776\n",
      "Epoch [1/2], Step [35080/67476], Loss: 4.8062\n",
      "Epoch [1/2], Step [35090/67476], Loss: 4.5629\n",
      "Epoch [1/2], Step [35100/67476], Loss: 4.5618\n",
      "Epoch [1/2], Step [35110/67476], Loss: 4.5821\n",
      "Epoch [1/2], Step [35120/67476], Loss: 4.6966\n",
      "Epoch [1/2], Step [35130/67476], Loss: 4.3653\n",
      "Epoch [1/2], Step [35140/67476], Loss: 4.6751\n",
      "Epoch [1/2], Step [35150/67476], Loss: 4.8800\n",
      "Epoch [1/2], Step [35160/67476], Loss: 4.6996\n",
      "Epoch [1/2], Step [35170/67476], Loss: 4.5285\n",
      "Epoch [1/2], Step [35180/67476], Loss: 4.5445\n",
      "Epoch [1/2], Step [35190/67476], Loss: 4.5497\n",
      "Epoch [1/2], Step [35200/67476], Loss: 4.4372\n",
      "Epoch [1/2], Step [35210/67476], Loss: 4.5792\n",
      "Epoch [1/2], Step [35220/67476], Loss: 4.4334\n",
      "Epoch [1/2], Step [35230/67476], Loss: 4.7335\n",
      "Epoch [1/2], Step [35240/67476], Loss: 4.6383\n",
      "Epoch [1/2], Step [35250/67476], Loss: 4.6055\n",
      "Epoch [1/2], Step [35260/67476], Loss: 4.7086\n",
      "Epoch [1/2], Step [35270/67476], Loss: 4.6391\n",
      "Epoch [1/2], Step [35280/67476], Loss: 4.4198\n",
      "Epoch [1/2], Step [35290/67476], Loss: 4.4777\n",
      "Epoch [1/2], Step [35300/67476], Loss: 4.5452\n",
      "Epoch [1/2], Step [35310/67476], Loss: 4.2875\n",
      "Epoch [1/2], Step [35320/67476], Loss: 4.6786\n",
      "Epoch [1/2], Step [35330/67476], Loss: 4.5960\n",
      "Epoch [1/2], Step [35340/67476], Loss: 4.7080\n",
      "Epoch [1/2], Step [35350/67476], Loss: 4.6914\n",
      "Epoch [1/2], Step [35360/67476], Loss: 4.5493\n",
      "Epoch [1/2], Step [35370/67476], Loss: 4.6078\n",
      "Epoch [1/2], Step [35380/67476], Loss: 4.6158\n",
      "Epoch [1/2], Step [35390/67476], Loss: 4.7182\n",
      "Epoch [1/2], Step [35400/67476], Loss: 4.5078\n",
      "Epoch [1/2], Step [35410/67476], Loss: 4.6614\n",
      "Epoch [1/2], Step [35420/67476], Loss: 4.4850\n",
      "Epoch [1/2], Step [35430/67476], Loss: 4.4856\n",
      "Epoch [1/2], Step [35440/67476], Loss: 4.5606\n",
      "Epoch [1/2], Step [35450/67476], Loss: 4.5453\n",
      "Epoch [1/2], Step [35460/67476], Loss: 4.5448\n",
      "Epoch [1/2], Step [35470/67476], Loss: 4.5711\n",
      "Epoch [1/2], Step [35480/67476], Loss: 4.6748\n",
      "Epoch [1/2], Step [35490/67476], Loss: 4.5908\n",
      "Epoch [1/2], Step [35500/67476], Loss: 4.7050\n",
      "Epoch [1/2], Step [35510/67476], Loss: 4.5211\n",
      "Epoch [1/2], Step [35520/67476], Loss: 4.5724\n",
      "Epoch [1/2], Step [35530/67476], Loss: 4.4912\n",
      "Epoch [1/2], Step [35540/67476], Loss: 4.7771\n",
      "Epoch [1/2], Step [35550/67476], Loss: 4.6530\n",
      "Epoch [1/2], Step [35560/67476], Loss: 4.6811\n",
      "Epoch [1/2], Step [35570/67476], Loss: 4.5987\n",
      "Epoch [1/2], Step [35580/67476], Loss: 4.5593\n",
      "Epoch [1/2], Step [35590/67476], Loss: 4.4872\n",
      "Epoch [1/2], Step [35600/67476], Loss: 4.7218\n",
      "Epoch [1/2], Step [35610/67476], Loss: 4.6293\n",
      "Epoch [1/2], Step [35620/67476], Loss: 4.6771\n",
      "Epoch [1/2], Step [35630/67476], Loss: 4.5093\n",
      "Epoch [1/2], Step [35640/67476], Loss: 4.6341\n",
      "Epoch [1/2], Step [35650/67476], Loss: 4.7293\n",
      "Epoch [1/2], Step [35660/67476], Loss: 4.4695\n",
      "Epoch [1/2], Step [35670/67476], Loss: 4.6663\n",
      "Epoch [1/2], Step [35680/67476], Loss: 4.6573\n",
      "Epoch [1/2], Step [35690/67476], Loss: 4.4030\n",
      "Epoch [1/2], Step [35700/67476], Loss: 4.7505\n",
      "Epoch [1/2], Step [35710/67476], Loss: 4.5832\n",
      "Epoch [1/2], Step [35720/67476], Loss: 4.4326\n",
      "Epoch [1/2], Step [35730/67476], Loss: 4.6999\n",
      "Epoch [1/2], Step [35740/67476], Loss: 4.4806\n",
      "Epoch [1/2], Step [35750/67476], Loss: 4.7574\n",
      "Epoch [1/2], Step [35760/67476], Loss: 4.7021\n",
      "Epoch [1/2], Step [35770/67476], Loss: 4.5057\n",
      "Epoch [1/2], Step [35780/67476], Loss: 4.4836\n",
      "Epoch [1/2], Step [35790/67476], Loss: 4.6777\n",
      "Epoch [1/2], Step [35800/67476], Loss: 4.5843\n",
      "Epoch [1/2], Step [35810/67476], Loss: 4.6085\n",
      "Epoch [1/2], Step [35820/67476], Loss: 4.6197\n",
      "Epoch [1/2], Step [35830/67476], Loss: 4.5806\n",
      "Epoch [1/2], Step [35840/67476], Loss: 4.7280\n",
      "Epoch [1/2], Step [35850/67476], Loss: 4.6919\n",
      "Epoch [1/2], Step [35860/67476], Loss: 4.7037\n",
      "Epoch [1/2], Step [35870/67476], Loss: 4.5844\n",
      "Epoch [1/2], Step [35880/67476], Loss: 4.5779\n",
      "Epoch [1/2], Step [35890/67476], Loss: 4.5565\n",
      "Epoch [1/2], Step [35900/67476], Loss: 4.6332\n",
      "Epoch [1/2], Step [35910/67476], Loss: 4.5882\n",
      "Epoch [1/2], Step [35920/67476], Loss: 4.5367\n",
      "Epoch [1/2], Step [35930/67476], Loss: 4.5925\n",
      "Epoch [1/2], Step [35940/67476], Loss: 4.6410\n",
      "Epoch [1/2], Step [35950/67476], Loss: 4.5243\n",
      "Epoch [1/2], Step [35960/67476], Loss: 4.5236\n",
      "Epoch [1/2], Step [35970/67476], Loss: 4.6071\n",
      "Epoch [1/2], Step [35980/67476], Loss: 4.6478\n",
      "Epoch [1/2], Step [35990/67476], Loss: 4.4808\n",
      "Epoch [1/2], Step [36000/67476], Loss: 4.7031\n",
      "Epoch [1/2], Step [36010/67476], Loss: 4.6891\n",
      "Epoch [1/2], Step [36020/67476], Loss: 4.6419\n",
      "Epoch [1/2], Step [36030/67476], Loss: 4.5830\n",
      "Epoch [1/2], Step [36040/67476], Loss: 4.5882\n",
      "Epoch [1/2], Step [36050/67476], Loss: 4.5578\n",
      "Epoch [1/2], Step [36060/67476], Loss: 4.7749\n",
      "Epoch [1/2], Step [36070/67476], Loss: 4.4976\n",
      "Epoch [1/2], Step [36080/67476], Loss: 4.6015\n",
      "Epoch [1/2], Step [36090/67476], Loss: 4.5869\n",
      "Epoch [1/2], Step [36100/67476], Loss: 4.5904\n",
      "Epoch [1/2], Step [36110/67476], Loss: 4.7682\n",
      "Epoch [1/2], Step [36120/67476], Loss: 4.9143\n",
      "Epoch [1/2], Step [36130/67476], Loss: 4.4545\n",
      "Epoch [1/2], Step [36140/67476], Loss: 4.6588\n",
      "Epoch [1/2], Step [36150/67476], Loss: 4.6892\n",
      "Epoch [1/2], Step [36160/67476], Loss: 4.5777\n",
      "Epoch [1/2], Step [36170/67476], Loss: 4.5366\n",
      "Epoch [1/2], Step [36180/67476], Loss: 4.3806\n",
      "Epoch [1/2], Step [36190/67476], Loss: 4.6209\n",
      "Epoch [1/2], Step [36200/67476], Loss: 4.6764\n",
      "Epoch [1/2], Step [36210/67476], Loss: 4.5412\n",
      "Epoch [1/2], Step [36220/67476], Loss: 4.4980\n",
      "Epoch [1/2], Step [36230/67476], Loss: 4.4759\n",
      "Epoch [1/2], Step [36240/67476], Loss: 4.6512\n",
      "Epoch [1/2], Step [36250/67476], Loss: 4.7312\n",
      "Epoch [1/2], Step [36260/67476], Loss: 4.4393\n",
      "Epoch [1/2], Step [36270/67476], Loss: 4.5908\n",
      "Epoch [1/2], Step [36280/67476], Loss: 4.4807\n",
      "Epoch [1/2], Step [36290/67476], Loss: 4.3993\n",
      "Epoch [1/2], Step [36300/67476], Loss: 4.6077\n",
      "Epoch [1/2], Step [36310/67476], Loss: 4.6016\n",
      "Epoch [1/2], Step [36320/67476], Loss: 4.4598\n",
      "Epoch [1/2], Step [36330/67476], Loss: 4.3878\n",
      "Epoch [1/2], Step [36340/67476], Loss: 4.6959\n",
      "Epoch [1/2], Step [36350/67476], Loss: 4.6978\n",
      "Epoch [1/2], Step [36360/67476], Loss: 4.4753\n",
      "Epoch [1/2], Step [36370/67476], Loss: 4.6302\n",
      "Epoch [1/2], Step [36380/67476], Loss: 4.4665\n",
      "Epoch [1/2], Step [36390/67476], Loss: 4.6560\n",
      "Epoch [1/2], Step [36400/67476], Loss: 4.5658\n",
      "Epoch [1/2], Step [36410/67476], Loss: 4.6986\n",
      "Epoch [1/2], Step [36420/67476], Loss: 4.6162\n",
      "Epoch [1/2], Step [36430/67476], Loss: 4.8291\n",
      "Epoch [1/2], Step [36440/67476], Loss: 4.7189\n",
      "Epoch [1/2], Step [36450/67476], Loss: 4.6036\n",
      "Epoch [1/2], Step [36460/67476], Loss: 4.4959\n",
      "Epoch [1/2], Step [36470/67476], Loss: 4.5893\n",
      "Epoch [1/2], Step [36480/67476], Loss: 4.5529\n",
      "Epoch [1/2], Step [36490/67476], Loss: 4.6338\n",
      "Epoch [1/2], Step [36500/67476], Loss: 4.6012\n",
      "Epoch [1/2], Step [36510/67476], Loss: 4.5073\n",
      "Epoch [1/2], Step [36520/67476], Loss: 4.5028\n",
      "Epoch [1/2], Step [36530/67476], Loss: 4.4052\n",
      "Epoch [1/2], Step [36540/67476], Loss: 4.6607\n",
      "Epoch [1/2], Step [36550/67476], Loss: 4.5498\n",
      "Epoch [1/2], Step [36560/67476], Loss: 4.6479\n",
      "Epoch [1/2], Step [36570/67476], Loss: 4.6687\n",
      "Epoch [1/2], Step [36580/67476], Loss: 4.6064\n",
      "Epoch [1/2], Step [36590/67476], Loss: 4.4368\n",
      "Epoch [1/2], Step [36600/67476], Loss: 4.8180\n",
      "Epoch [1/2], Step [36610/67476], Loss: 4.6699\n",
      "Epoch [1/2], Step [36620/67476], Loss: 4.5906\n",
      "Epoch [1/2], Step [36630/67476], Loss: 4.7148\n",
      "Epoch [1/2], Step [36640/67476], Loss: 4.5399\n",
      "Epoch [1/2], Step [36650/67476], Loss: 4.7575\n",
      "Epoch [1/2], Step [36660/67476], Loss: 4.5279\n",
      "Epoch [1/2], Step [36670/67476], Loss: 4.5703\n",
      "Epoch [1/2], Step [36680/67476], Loss: 4.7491\n",
      "Epoch [1/2], Step [36690/67476], Loss: 4.7916\n",
      "Epoch [1/2], Step [36700/67476], Loss: 4.5553\n",
      "Epoch [1/2], Step [36710/67476], Loss: 4.4793\n",
      "Epoch [1/2], Step [36720/67476], Loss: 4.4082\n",
      "Epoch [1/2], Step [36730/67476], Loss: 4.5741\n",
      "Epoch [1/2], Step [36740/67476], Loss: 4.7012\n",
      "Epoch [1/2], Step [36750/67476], Loss: 4.5175\n",
      "Epoch [1/2], Step [36760/67476], Loss: 4.5466\n",
      "Epoch [1/2], Step [36770/67476], Loss: 4.5590\n",
      "Epoch [1/2], Step [36780/67476], Loss: 4.5741\n",
      "Epoch [1/2], Step [36790/67476], Loss: 4.7276\n",
      "Epoch [1/2], Step [36800/67476], Loss: 4.3330\n",
      "Epoch [1/2], Step [36810/67476], Loss: 4.8060\n",
      "Epoch [1/2], Step [36820/67476], Loss: 4.5453\n",
      "Epoch [1/2], Step [36830/67476], Loss: 4.4956\n",
      "Epoch [1/2], Step [36840/67476], Loss: 4.6101\n",
      "Epoch [1/2], Step [36850/67476], Loss: 4.5212\n",
      "Epoch [1/2], Step [36860/67476], Loss: 4.4722\n",
      "Epoch [1/2], Step [36870/67476], Loss: 4.6494\n",
      "Epoch [1/2], Step [36880/67476], Loss: 4.7425\n",
      "Epoch [1/2], Step [36890/67476], Loss: 4.5683\n",
      "Epoch [1/2], Step [36900/67476], Loss: 4.6129\n",
      "Epoch [1/2], Step [36910/67476], Loss: 4.6295\n",
      "Epoch [1/2], Step [36920/67476], Loss: 4.5040\n",
      "Epoch [1/2], Step [36930/67476], Loss: 4.7178\n",
      "Epoch [1/2], Step [36940/67476], Loss: 4.5891\n",
      "Epoch [1/2], Step [36950/67476], Loss: 4.4572\n",
      "Epoch [1/2], Step [36960/67476], Loss: 4.4873\n",
      "Epoch [1/2], Step [36970/67476], Loss: 4.4575\n",
      "Epoch [1/2], Step [36980/67476], Loss: 4.6650\n",
      "Epoch [1/2], Step [36990/67476], Loss: 4.5443\n",
      "Epoch [1/2], Step [37000/67476], Loss: 4.3786\n",
      "Epoch [1/2], Step [37010/67476], Loss: 4.5961\n",
      "Epoch [1/2], Step [37020/67476], Loss: 4.6206\n",
      "Epoch [1/2], Step [37030/67476], Loss: 4.4920\n",
      "Epoch [1/2], Step [37040/67476], Loss: 4.5039\n",
      "Epoch [1/2], Step [37050/67476], Loss: 4.4876\n",
      "Epoch [1/2], Step [37060/67476], Loss: 4.6503\n",
      "Epoch [1/2], Step [37070/67476], Loss: 4.6484\n",
      "Epoch [1/2], Step [37080/67476], Loss: 4.5028\n",
      "Epoch [1/2], Step [37090/67476], Loss: 4.6188\n",
      "Epoch [1/2], Step [37100/67476], Loss: 4.4402\n",
      "Epoch [1/2], Step [37110/67476], Loss: 4.3397\n",
      "Epoch [1/2], Step [37120/67476], Loss: 4.5829\n",
      "Epoch [1/2], Step [37130/67476], Loss: 4.4744\n",
      "Epoch [1/2], Step [37140/67476], Loss: 4.6812\n",
      "Epoch [1/2], Step [37150/67476], Loss: 4.6762\n",
      "Epoch [1/2], Step [37160/67476], Loss: 4.7323\n",
      "Epoch [1/2], Step [37170/67476], Loss: 4.4877\n",
      "Epoch [1/2], Step [37180/67476], Loss: 4.5899\n",
      "Epoch [1/2], Step [37190/67476], Loss: 4.4410\n",
      "Epoch [1/2], Step [37200/67476], Loss: 4.5305\n",
      "Epoch [1/2], Step [37210/67476], Loss: 4.5161\n",
      "Epoch [1/2], Step [37220/67476], Loss: 4.4887\n",
      "Epoch [1/2], Step [37230/67476], Loss: 4.5465\n",
      "Epoch [1/2], Step [37240/67476], Loss: 4.5816\n",
      "Epoch [1/2], Step [37250/67476], Loss: 4.7823\n",
      "Epoch [1/2], Step [37260/67476], Loss: 4.6527\n",
      "Epoch [1/2], Step [37270/67476], Loss: 4.6728\n",
      "Epoch [1/2], Step [37280/67476], Loss: 4.3665\n",
      "Epoch [1/2], Step [37290/67476], Loss: 4.6220\n",
      "Epoch [1/2], Step [37300/67476], Loss: 4.4183\n",
      "Epoch [1/2], Step [37310/67476], Loss: 4.6703\n",
      "Epoch [1/2], Step [37320/67476], Loss: 4.5041\n",
      "Epoch [1/2], Step [37330/67476], Loss: 4.6692\n",
      "Epoch [1/2], Step [37340/67476], Loss: 4.5490\n",
      "Epoch [1/2], Step [37350/67476], Loss: 4.3802\n",
      "Epoch [1/2], Step [37360/67476], Loss: 4.5636\n",
      "Epoch [1/2], Step [37370/67476], Loss: 4.5904\n",
      "Epoch [1/2], Step [37380/67476], Loss: 4.7903\n",
      "Epoch [1/2], Step [37390/67476], Loss: 4.7141\n",
      "Epoch [1/2], Step [37400/67476], Loss: 4.5337\n",
      "Epoch [1/2], Step [37410/67476], Loss: 4.5353\n",
      "Epoch [1/2], Step [37420/67476], Loss: 4.5341\n",
      "Epoch [1/2], Step [37430/67476], Loss: 4.5987\n",
      "Epoch [1/2], Step [37440/67476], Loss: 4.6759\n",
      "Epoch [1/2], Step [37450/67476], Loss: 4.5563\n",
      "Epoch [1/2], Step [37460/67476], Loss: 4.6432\n",
      "Epoch [1/2], Step [37470/67476], Loss: 4.5763\n",
      "Epoch [1/2], Step [37480/67476], Loss: 4.5063\n",
      "Epoch [1/2], Step [37490/67476], Loss: 4.5087\n",
      "Epoch [1/2], Step [37500/67476], Loss: 4.6984\n",
      "Epoch [1/2], Step [37510/67476], Loss: 4.7044\n",
      "Epoch [1/2], Step [37520/67476], Loss: 4.6479\n",
      "Epoch [1/2], Step [37530/67476], Loss: 4.6233\n",
      "Epoch [1/2], Step [37540/67476], Loss: 4.6929\n",
      "Epoch [1/2], Step [37550/67476], Loss: 4.6682\n",
      "Epoch [1/2], Step [37560/67476], Loss: 4.6067\n",
      "Epoch [1/2], Step [37570/67476], Loss: 4.4893\n",
      "Epoch [1/2], Step [37580/67476], Loss: 4.5311\n",
      "Epoch [1/2], Step [37590/67476], Loss: 4.5725\n",
      "Epoch [1/2], Step [37600/67476], Loss: 4.4660\n",
      "Epoch [1/2], Step [37610/67476], Loss: 4.5918\n",
      "Epoch [1/2], Step [37620/67476], Loss: 4.9653\n",
      "Epoch [1/2], Step [37630/67476], Loss: 4.7610\n",
      "Epoch [1/2], Step [37640/67476], Loss: 4.5236\n",
      "Epoch [1/2], Step [37650/67476], Loss: 4.7341\n",
      "Epoch [1/2], Step [37660/67476], Loss: 4.3300\n",
      "Epoch [1/2], Step [37670/67476], Loss: 4.5470\n",
      "Epoch [1/2], Step [37680/67476], Loss: 4.4772\n",
      "Epoch [1/2], Step [37690/67476], Loss: 4.6778\n",
      "Epoch [1/2], Step [37700/67476], Loss: 4.4325\n",
      "Epoch [1/2], Step [37710/67476], Loss: 4.6580\n",
      "Epoch [1/2], Step [37720/67476], Loss: 4.6942\n",
      "Epoch [1/2], Step [37730/67476], Loss: 4.6408\n",
      "Epoch [1/2], Step [37740/67476], Loss: 4.4341\n",
      "Epoch [1/2], Step [37750/67476], Loss: 4.7742\n",
      "Epoch [1/2], Step [37760/67476], Loss: 4.6888\n",
      "Epoch [1/2], Step [37770/67476], Loss: 4.7327\n",
      "Epoch [1/2], Step [37780/67476], Loss: 4.6000\n",
      "Epoch [1/2], Step [37790/67476], Loss: 4.5389\n",
      "Epoch [1/2], Step [37800/67476], Loss: 4.6649\n",
      "Epoch [1/2], Step [37810/67476], Loss: 4.4781\n",
      "Epoch [1/2], Step [37820/67476], Loss: 4.5727\n",
      "Epoch [1/2], Step [37830/67476], Loss: 4.6169\n",
      "Epoch [1/2], Step [37840/67476], Loss: 4.3481\n",
      "Epoch [1/2], Step [37850/67476], Loss: 4.5770\n",
      "Epoch [1/2], Step [37860/67476], Loss: 4.4399\n",
      "Epoch [1/2], Step [37870/67476], Loss: 4.5975\n",
      "Epoch [1/2], Step [37880/67476], Loss: 4.7744\n",
      "Epoch [1/2], Step [37890/67476], Loss: 4.4717\n",
      "Epoch [1/2], Step [37900/67476], Loss: 4.6837\n",
      "Epoch [1/2], Step [37910/67476], Loss: 4.3269\n",
      "Epoch [1/2], Step [37920/67476], Loss: 4.6313\n",
      "Epoch [1/2], Step [37930/67476], Loss: 4.4655\n",
      "Epoch [1/2], Step [37940/67476], Loss: 4.5712\n",
      "Epoch [1/2], Step [37950/67476], Loss: 4.6058\n",
      "Epoch [1/2], Step [37960/67476], Loss: 4.6018\n",
      "Epoch [1/2], Step [37970/67476], Loss: 4.4878\n",
      "Epoch [1/2], Step [37980/67476], Loss: 4.5723\n",
      "Epoch [1/2], Step [37990/67476], Loss: 4.3069\n",
      "Epoch [1/2], Step [38000/67476], Loss: 4.8203\n",
      "Epoch [1/2], Step [38010/67476], Loss: 4.7047\n",
      "Epoch [1/2], Step [38020/67476], Loss: 4.5678\n",
      "Epoch [1/2], Step [38030/67476], Loss: 4.3569\n",
      "Epoch [1/2], Step [38040/67476], Loss: 4.5935\n",
      "Epoch [1/2], Step [38050/67476], Loss: 4.4483\n",
      "Epoch [1/2], Step [38060/67476], Loss: 4.5424\n",
      "Epoch [1/2], Step [38070/67476], Loss: 4.5159\n",
      "Epoch [1/2], Step [38080/67476], Loss: 4.5789\n",
      "Epoch [1/2], Step [38090/67476], Loss: 4.6809\n",
      "Epoch [1/2], Step [38100/67476], Loss: 4.5778\n",
      "Epoch [1/2], Step [38110/67476], Loss: 4.6899\n",
      "Epoch [1/2], Step [38120/67476], Loss: 4.5999\n",
      "Epoch [1/2], Step [38130/67476], Loss: 4.6494\n",
      "Epoch [1/2], Step [38140/67476], Loss: 4.4827\n",
      "Epoch [1/2], Step [38150/67476], Loss: 4.7289\n",
      "Epoch [1/2], Step [38160/67476], Loss: 4.5176\n",
      "Epoch [1/2], Step [38170/67476], Loss: 4.4819\n",
      "Epoch [1/2], Step [38180/67476], Loss: 4.5282\n",
      "Epoch [1/2], Step [38190/67476], Loss: 4.5526\n",
      "Epoch [1/2], Step [38200/67476], Loss: 4.6357\n",
      "Epoch [1/2], Step [38210/67476], Loss: 4.4972\n",
      "Epoch [1/2], Step [38220/67476], Loss: 4.4650\n",
      "Epoch [1/2], Step [38230/67476], Loss: 4.4055\n",
      "Epoch [1/2], Step [38240/67476], Loss: 4.5239\n",
      "Epoch [1/2], Step [38250/67476], Loss: 4.5252\n",
      "Epoch [1/2], Step [38260/67476], Loss: 4.5156\n",
      "Epoch [1/2], Step [38270/67476], Loss: 4.6208\n",
      "Epoch [1/2], Step [38280/67476], Loss: 4.2944\n",
      "Epoch [1/2], Step [38290/67476], Loss: 4.5556\n",
      "Epoch [1/2], Step [38300/67476], Loss: 4.5480\n",
      "Epoch [1/2], Step [38310/67476], Loss: 4.6586\n",
      "Epoch [1/2], Step [38320/67476], Loss: 4.8311\n",
      "Epoch [1/2], Step [38330/67476], Loss: 4.5635\n",
      "Epoch [1/2], Step [38340/67476], Loss: 4.5307\n",
      "Epoch [1/2], Step [38350/67476], Loss: 4.4533\n",
      "Epoch [1/2], Step [38360/67476], Loss: 4.5305\n",
      "Epoch [1/2], Step [38370/67476], Loss: 4.6008\n",
      "Epoch [1/2], Step [38380/67476], Loss: 4.5392\n",
      "Epoch [1/2], Step [38390/67476], Loss: 4.7480\n",
      "Epoch [1/2], Step [38400/67476], Loss: 4.6440\n",
      "Epoch [1/2], Step [38410/67476], Loss: 4.7268\n",
      "Epoch [1/2], Step [38420/67476], Loss: 4.5827\n",
      "Epoch [1/2], Step [38430/67476], Loss: 4.7777\n",
      "Epoch [1/2], Step [38440/67476], Loss: 4.7452\n",
      "Epoch [1/2], Step [38450/67476], Loss: 4.4584\n",
      "Epoch [1/2], Step [38460/67476], Loss: 4.9104\n",
      "Epoch [1/2], Step [38470/67476], Loss: 4.6447\n",
      "Epoch [1/2], Step [38480/67476], Loss: 4.4692\n",
      "Epoch [1/2], Step [38490/67476], Loss: 4.5336\n",
      "Epoch [1/2], Step [38500/67476], Loss: 4.6507\n",
      "Epoch [1/2], Step [38510/67476], Loss: 4.6284\n",
      "Epoch [1/2], Step [38520/67476], Loss: 4.7794\n",
      "Epoch [1/2], Step [38530/67476], Loss: 4.5467\n",
      "Epoch [1/2], Step [38540/67476], Loss: 4.5040\n",
      "Epoch [1/2], Step [38550/67476], Loss: 4.5220\n",
      "Epoch [1/2], Step [38560/67476], Loss: 4.6710\n",
      "Epoch [1/2], Step [38570/67476], Loss: 4.7213\n",
      "Epoch [1/2], Step [38580/67476], Loss: 4.4878\n",
      "Epoch [1/2], Step [38590/67476], Loss: 4.5732\n",
      "Epoch [1/2], Step [38600/67476], Loss: 4.8557\n",
      "Epoch [1/2], Step [38610/67476], Loss: 4.4844\n",
      "Epoch [1/2], Step [38620/67476], Loss: 4.2983\n",
      "Epoch [1/2], Step [38630/67476], Loss: 4.5428\n",
      "Epoch [1/2], Step [38640/67476], Loss: 4.5265\n",
      "Epoch [1/2], Step [38650/67476], Loss: 4.6894\n",
      "Epoch [1/2], Step [38660/67476], Loss: 4.5968\n",
      "Epoch [1/2], Step [38670/67476], Loss: 4.5964\n",
      "Epoch [1/2], Step [38680/67476], Loss: 4.7231\n",
      "Epoch [1/2], Step [38690/67476], Loss: 4.8462\n",
      "Epoch [1/2], Step [38700/67476], Loss: 4.6702\n",
      "Epoch [1/2], Step [38710/67476], Loss: 4.5878\n",
      "Epoch [1/2], Step [38720/67476], Loss: 4.6076\n",
      "Epoch [1/2], Step [38730/67476], Loss: 4.5549\n",
      "Epoch [1/2], Step [38740/67476], Loss: 4.4454\n",
      "Epoch [1/2], Step [38750/67476], Loss: 4.5216\n",
      "Epoch [1/2], Step [38760/67476], Loss: 4.6118\n",
      "Epoch [1/2], Step [38770/67476], Loss: 4.4538\n",
      "Epoch [1/2], Step [38780/67476], Loss: 4.8616\n",
      "Epoch [1/2], Step [38790/67476], Loss: 4.4895\n",
      "Epoch [1/2], Step [38800/67476], Loss: 4.5358\n",
      "Epoch [1/2], Step [38810/67476], Loss: 4.5246\n",
      "Epoch [1/2], Step [38820/67476], Loss: 4.4901\n",
      "Epoch [1/2], Step [38830/67476], Loss: 4.4721\n",
      "Epoch [1/2], Step [38840/67476], Loss: 4.4922\n",
      "Epoch [1/2], Step [38850/67476], Loss: 4.4685\n",
      "Epoch [1/2], Step [38860/67476], Loss: 4.5554\n",
      "Epoch [1/2], Step [38870/67476], Loss: 4.7191\n",
      "Epoch [1/2], Step [38880/67476], Loss: 4.6740\n",
      "Epoch [1/2], Step [38890/67476], Loss: 4.4316\n",
      "Epoch [1/2], Step [38900/67476], Loss: 4.5363\n",
      "Epoch [1/2], Step [38910/67476], Loss: 4.5964\n",
      "Epoch [1/2], Step [38920/67476], Loss: 4.4267\n",
      "Epoch [1/2], Step [38930/67476], Loss: 4.5072\n",
      "Epoch [1/2], Step [38940/67476], Loss: 4.5235\n",
      "Epoch [1/2], Step [38950/67476], Loss: 4.6946\n",
      "Epoch [1/2], Step [38960/67476], Loss: 4.5968\n",
      "Epoch [1/2], Step [38970/67476], Loss: 4.6413\n",
      "Epoch [1/2], Step [38980/67476], Loss: 4.5373\n",
      "Epoch [1/2], Step [38990/67476], Loss: 4.3409\n",
      "Epoch [1/2], Step [39000/67476], Loss: 4.4373\n",
      "Epoch [1/2], Step [39010/67476], Loss: 4.7723\n",
      "Epoch [1/2], Step [39020/67476], Loss: 4.6408\n",
      "Epoch [1/2], Step [39030/67476], Loss: 4.6469\n",
      "Epoch [1/2], Step [39040/67476], Loss: 4.6521\n",
      "Epoch [1/2], Step [39050/67476], Loss: 4.7373\n",
      "Epoch [1/2], Step [39060/67476], Loss: 4.5913\n",
      "Epoch [1/2], Step [39070/67476], Loss: 4.6998\n",
      "Epoch [1/2], Step [39080/67476], Loss: 4.6409\n",
      "Epoch [1/2], Step [39090/67476], Loss: 4.3626\n",
      "Epoch [1/2], Step [39100/67476], Loss: 4.8850\n",
      "Epoch [1/2], Step [39110/67476], Loss: 4.6446\n",
      "Epoch [1/2], Step [39120/67476], Loss: 4.5633\n",
      "Epoch [1/2], Step [39130/67476], Loss: 4.3724\n",
      "Epoch [1/2], Step [39140/67476], Loss: 4.6340\n",
      "Epoch [1/2], Step [39150/67476], Loss: 4.5988\n",
      "Epoch [1/2], Step [39160/67476], Loss: 4.4660\n",
      "Epoch [1/2], Step [39170/67476], Loss: 4.4906\n",
      "Epoch [1/2], Step [39180/67476], Loss: 4.4453\n",
      "Epoch [1/2], Step [39190/67476], Loss: 4.5662\n",
      "Epoch [1/2], Step [39200/67476], Loss: 4.4888\n",
      "Epoch [1/2], Step [39210/67476], Loss: 4.5424\n",
      "Epoch [1/2], Step [39220/67476], Loss: 4.4614\n",
      "Epoch [1/2], Step [39230/67476], Loss: 4.5582\n",
      "Epoch [1/2], Step [39240/67476], Loss: 4.8013\n",
      "Epoch [1/2], Step [39250/67476], Loss: 4.4020\n",
      "Epoch [1/2], Step [39260/67476], Loss: 4.3896\n",
      "Epoch [1/2], Step [39270/67476], Loss: 4.7799\n",
      "Epoch [1/2], Step [39280/67476], Loss: 4.5830\n",
      "Epoch [1/2], Step [39290/67476], Loss: 4.5333\n",
      "Epoch [1/2], Step [39300/67476], Loss: 4.5065\n",
      "Epoch [1/2], Step [39310/67476], Loss: 4.5811\n",
      "Epoch [1/2], Step [39320/67476], Loss: 4.4428\n",
      "Epoch [1/2], Step [39330/67476], Loss: 4.6242\n",
      "Epoch [1/2], Step [39340/67476], Loss: 4.6752\n",
      "Epoch [1/2], Step [39350/67476], Loss: 4.5547\n",
      "Epoch [1/2], Step [39360/67476], Loss: 4.5024\n",
      "Epoch [1/2], Step [39370/67476], Loss: 4.6373\n",
      "Epoch [1/2], Step [39380/67476], Loss: 4.6429\n",
      "Epoch [1/2], Step [39390/67476], Loss: 4.5851\n",
      "Epoch [1/2], Step [39400/67476], Loss: 4.6186\n",
      "Epoch [1/2], Step [39410/67476], Loss: 4.5983\n",
      "Epoch [1/2], Step [39420/67476], Loss: 4.3612\n",
      "Epoch [1/2], Step [39430/67476], Loss: 4.6823\n",
      "Epoch [1/2], Step [39440/67476], Loss: 4.4829\n",
      "Epoch [1/2], Step [39450/67476], Loss: 4.8066\n",
      "Epoch [1/2], Step [39460/67476], Loss: 4.8006\n",
      "Epoch [1/2], Step [39470/67476], Loss: 4.5765\n",
      "Epoch [1/2], Step [39480/67476], Loss: 4.6004\n",
      "Epoch [1/2], Step [39490/67476], Loss: 4.5969\n",
      "Epoch [1/2], Step [39500/67476], Loss: 4.6867\n",
      "Epoch [1/2], Step [39510/67476], Loss: 4.7620\n",
      "Epoch [1/2], Step [39520/67476], Loss: 4.5580\n",
      "Epoch [1/2], Step [39530/67476], Loss: 4.5349\n",
      "Epoch [1/2], Step [39540/67476], Loss: 4.3441\n",
      "Epoch [1/2], Step [39550/67476], Loss: 4.3992\n",
      "Epoch [1/2], Step [39560/67476], Loss: 4.5162\n",
      "Epoch [1/2], Step [39570/67476], Loss: 4.5130\n",
      "Epoch [1/2], Step [39580/67476], Loss: 4.8118\n",
      "Epoch [1/2], Step [39590/67476], Loss: 4.5398\n",
      "Epoch [1/2], Step [39600/67476], Loss: 4.6416\n",
      "Epoch [1/2], Step [39610/67476], Loss: 4.5377\n",
      "Epoch [1/2], Step [39620/67476], Loss: 4.4802\n",
      "Epoch [1/2], Step [39630/67476], Loss: 4.5724\n",
      "Epoch [1/2], Step [39640/67476], Loss: 4.7543\n",
      "Epoch [1/2], Step [39650/67476], Loss: 4.4508\n",
      "Epoch [1/2], Step [39660/67476], Loss: 4.5844\n",
      "Epoch [1/2], Step [39670/67476], Loss: 4.5958\n",
      "Epoch [1/2], Step [39680/67476], Loss: 4.7499\n",
      "Epoch [1/2], Step [39690/67476], Loss: 4.8300\n",
      "Epoch [1/2], Step [39700/67476], Loss: 4.3960\n",
      "Epoch [1/2], Step [39710/67476], Loss: 4.6378\n",
      "Epoch [1/2], Step [39720/67476], Loss: 4.4583\n",
      "Epoch [1/2], Step [39730/67476], Loss: 4.4315\n",
      "Epoch [1/2], Step [39740/67476], Loss: 4.4891\n",
      "Epoch [1/2], Step [39750/67476], Loss: 4.7039\n",
      "Epoch [1/2], Step [39760/67476], Loss: 4.5379\n",
      "Epoch [1/2], Step [39770/67476], Loss: 4.6139\n",
      "Epoch [1/2], Step [39780/67476], Loss: 4.6979\n",
      "Epoch [1/2], Step [39790/67476], Loss: 4.8042\n",
      "Epoch [1/2], Step [39800/67476], Loss: 4.5919\n",
      "Epoch [1/2], Step [39810/67476], Loss: 4.7843\n",
      "Epoch [1/2], Step [39820/67476], Loss: 4.6206\n",
      "Epoch [1/2], Step [39830/67476], Loss: 4.4439\n",
      "Epoch [1/2], Step [39840/67476], Loss: 4.7250\n",
      "Epoch [1/2], Step [39850/67476], Loss: 4.5283\n",
      "Epoch [1/2], Step [39860/67476], Loss: 4.6138\n",
      "Epoch [1/2], Step [39870/67476], Loss: 4.6506\n",
      "Epoch [1/2], Step [39880/67476], Loss: 4.5471\n",
      "Epoch [1/2], Step [39890/67476], Loss: 4.4860\n",
      "Epoch [1/2], Step [39900/67476], Loss: 4.6619\n",
      "Epoch [1/2], Step [39910/67476], Loss: 4.5272\n",
      "Epoch [1/2], Step [39920/67476], Loss: 4.6461\n",
      "Epoch [1/2], Step [39930/67476], Loss: 4.6887\n",
      "Epoch [1/2], Step [39940/67476], Loss: 4.5485\n",
      "Epoch [1/2], Step [39950/67476], Loss: 4.4762\n",
      "Epoch [1/2], Step [39960/67476], Loss: 4.6482\n",
      "Epoch [1/2], Step [39970/67476], Loss: 4.6687\n",
      "Epoch [1/2], Step [39980/67476], Loss: 4.5480\n",
      "Epoch [1/2], Step [39990/67476], Loss: 4.7349\n",
      "Epoch [1/2], Step [40000/67476], Loss: 4.6365\n",
      "Epoch [1/2], Step [40010/67476], Loss: 4.3648\n",
      "Epoch [1/2], Step [40020/67476], Loss: 4.6051\n",
      "Epoch [1/2], Step [40030/67476], Loss: 4.5147\n",
      "Epoch [1/2], Step [40040/67476], Loss: 4.3276\n",
      "Epoch [1/2], Step [40050/67476], Loss: 4.5764\n",
      "Epoch [1/2], Step [40060/67476], Loss: 4.6872\n",
      "Epoch [1/2], Step [40070/67476], Loss: 4.5227\n",
      "Epoch [1/2], Step [40080/67476], Loss: 4.4433\n",
      "Epoch [1/2], Step [40090/67476], Loss: 4.6231\n",
      "Epoch [1/2], Step [40100/67476], Loss: 4.5852\n",
      "Epoch [1/2], Step [40110/67476], Loss: 4.5023\n",
      "Epoch [1/2], Step [40120/67476], Loss: 4.4545\n",
      "Epoch [1/2], Step [40130/67476], Loss: 4.6101\n",
      "Epoch [1/2], Step [40140/67476], Loss: 4.4445\n",
      "Epoch [1/2], Step [40150/67476], Loss: 4.5109\n",
      "Epoch [1/2], Step [40160/67476], Loss: 4.5518\n",
      "Epoch [1/2], Step [40170/67476], Loss: 4.5682\n",
      "Epoch [1/2], Step [40180/67476], Loss: 4.4994\n",
      "Epoch [1/2], Step [40190/67476], Loss: 4.3705\n",
      "Epoch [1/2], Step [40200/67476], Loss: 4.6033\n",
      "Epoch [1/2], Step [40210/67476], Loss: 4.7093\n",
      "Epoch [1/2], Step [40220/67476], Loss: 4.8980\n",
      "Epoch [1/2], Step [40230/67476], Loss: 4.5069\n",
      "Epoch [1/2], Step [40240/67476], Loss: 4.5333\n",
      "Epoch [1/2], Step [40250/67476], Loss: 4.4904\n",
      "Epoch [1/2], Step [40260/67476], Loss: 4.4609\n",
      "Epoch [1/2], Step [40270/67476], Loss: 4.7319\n",
      "Epoch [1/2], Step [40280/67476], Loss: 4.5802\n",
      "Epoch [1/2], Step [40290/67476], Loss: 4.5901\n",
      "Epoch [1/2], Step [40300/67476], Loss: 4.1515\n",
      "Epoch [1/2], Step [40310/67476], Loss: 4.5623\n",
      "Epoch [1/2], Step [40320/67476], Loss: 4.7436\n",
      "Epoch [1/2], Step [40330/67476], Loss: 4.7303\n",
      "Epoch [1/2], Step [40340/67476], Loss: 4.6745\n",
      "Epoch [1/2], Step [40350/67476], Loss: 4.3386\n",
      "Epoch [1/2], Step [40360/67476], Loss: 4.6578\n",
      "Epoch [1/2], Step [40370/67476], Loss: 4.5976\n",
      "Epoch [1/2], Step [40380/67476], Loss: 4.5802\n",
      "Epoch [1/2], Step [40390/67476], Loss: 4.6956\n",
      "Epoch [1/2], Step [40400/67476], Loss: 4.6135\n",
      "Epoch [1/2], Step [40410/67476], Loss: 4.5694\n",
      "Epoch [1/2], Step [40420/67476], Loss: 4.5309\n",
      "Epoch [1/2], Step [40430/67476], Loss: 4.7626\n",
      "Epoch [1/2], Step [40440/67476], Loss: 4.4670\n",
      "Epoch [1/2], Step [40450/67476], Loss: 4.6049\n",
      "Epoch [1/2], Step [40460/67476], Loss: 4.6614\n",
      "Epoch [1/2], Step [40470/67476], Loss: 4.6887\n",
      "Epoch [1/2], Step [40480/67476], Loss: 4.5824\n",
      "Epoch [1/2], Step [40490/67476], Loss: 4.7754\n",
      "Epoch [1/2], Step [40500/67476], Loss: 4.5263\n",
      "Epoch [1/2], Step [40510/67476], Loss: 4.6145\n",
      "Epoch [1/2], Step [40520/67476], Loss: 4.4846\n",
      "Epoch [1/2], Step [40530/67476], Loss: 4.6777\n",
      "Epoch [1/2], Step [40540/67476], Loss: 4.6635\n",
      "Epoch [1/2], Step [40550/67476], Loss: 4.6446\n",
      "Epoch [1/2], Step [40560/67476], Loss: 4.7347\n",
      "Epoch [1/2], Step [40570/67476], Loss: 4.3939\n",
      "Epoch [1/2], Step [40580/67476], Loss: 4.5286\n",
      "Epoch [1/2], Step [40590/67476], Loss: 4.3936\n",
      "Epoch [1/2], Step [40600/67476], Loss: 4.7356\n",
      "Epoch [1/2], Step [40610/67476], Loss: 4.6592\n",
      "Epoch [1/2], Step [40620/67476], Loss: 4.5763\n",
      "Epoch [1/2], Step [40630/67476], Loss: 4.5971\n",
      "Epoch [1/2], Step [40640/67476], Loss: 4.6890\n",
      "Epoch [1/2], Step [40650/67476], Loss: 4.7744\n",
      "Epoch [1/2], Step [40660/67476], Loss: 4.4618\n",
      "Epoch [1/2], Step [40670/67476], Loss: 4.5618\n",
      "Epoch [1/2], Step [40680/67476], Loss: 4.7772\n",
      "Epoch [1/2], Step [40690/67476], Loss: 4.5615\n",
      "Epoch [1/2], Step [40700/67476], Loss: 4.4956\n",
      "Epoch [1/2], Step [40710/67476], Loss: 4.6109\n",
      "Epoch [1/2], Step [40720/67476], Loss: 4.5971\n",
      "Epoch [1/2], Step [40730/67476], Loss: 4.6332\n",
      "Epoch [1/2], Step [40740/67476], Loss: 4.5523\n",
      "Epoch [1/2], Step [40750/67476], Loss: 4.6026\n",
      "Epoch [1/2], Step [40760/67476], Loss: 4.6721\n",
      "Epoch [1/2], Step [40770/67476], Loss: 4.4816\n",
      "Epoch [1/2], Step [40780/67476], Loss: 4.6768\n",
      "Epoch [1/2], Step [40790/67476], Loss: 4.4213\n",
      "Epoch [1/2], Step [40800/67476], Loss: 4.4739\n",
      "Epoch [1/2], Step [40810/67476], Loss: 4.5684\n",
      "Epoch [1/2], Step [40820/67476], Loss: 4.5145\n",
      "Epoch [1/2], Step [40830/67476], Loss: 4.3601\n",
      "Epoch [1/2], Step [40840/67476], Loss: 4.5398\n",
      "Epoch [1/2], Step [40850/67476], Loss: 4.4585\n",
      "Epoch [1/2], Step [40860/67476], Loss: 4.6539\n",
      "Epoch [1/2], Step [40870/67476], Loss: 4.5339\n",
      "Epoch [1/2], Step [40880/67476], Loss: 4.3967\n",
      "Epoch [1/2], Step [40890/67476], Loss: 4.5743\n",
      "Epoch [1/2], Step [40900/67476], Loss: 4.7209\n",
      "Epoch [1/2], Step [40910/67476], Loss: 4.6320\n",
      "Epoch [1/2], Step [40920/67476], Loss: 4.6366\n",
      "Epoch [1/2], Step [40930/67476], Loss: 4.5616\n",
      "Epoch [1/2], Step [40940/67476], Loss: 4.6215\n",
      "Epoch [1/2], Step [40950/67476], Loss: 4.7445\n",
      "Epoch [1/2], Step [40960/67476], Loss: 4.8136\n",
      "Epoch [1/2], Step [40970/67476], Loss: 4.4998\n",
      "Epoch [1/2], Step [40980/67476], Loss: 4.5974\n",
      "Epoch [1/2], Step [40990/67476], Loss: 4.5924\n",
      "Epoch [1/2], Step [41000/67476], Loss: 4.6297\n",
      "Epoch [1/2], Step [41010/67476], Loss: 4.7856\n",
      "Epoch [1/2], Step [41020/67476], Loss: 4.5103\n",
      "Epoch [1/2], Step [41030/67476], Loss: 4.3535\n",
      "Epoch [1/2], Step [41040/67476], Loss: 4.4848\n",
      "Epoch [1/2], Step [41050/67476], Loss: 4.6610\n",
      "Epoch [1/2], Step [41060/67476], Loss: 4.5201\n",
      "Epoch [1/2], Step [41070/67476], Loss: 4.6550\n",
      "Epoch [1/2], Step [41080/67476], Loss: 4.5587\n",
      "Epoch [1/2], Step [41090/67476], Loss: 4.8016\n",
      "Epoch [1/2], Step [41100/67476], Loss: 4.5874\n",
      "Epoch [1/2], Step [41110/67476], Loss: 4.6424\n",
      "Epoch [1/2], Step [41120/67476], Loss: 4.5547\n",
      "Epoch [1/2], Step [41130/67476], Loss: 4.3850\n",
      "Epoch [1/2], Step [41140/67476], Loss: 4.4626\n",
      "Epoch [1/2], Step [41150/67476], Loss: 4.5667\n",
      "Epoch [1/2], Step [41160/67476], Loss: 4.6493\n",
      "Epoch [1/2], Step [41170/67476], Loss: 4.2882\n",
      "Epoch [1/2], Step [41180/67476], Loss: 4.4626\n",
      "Epoch [1/2], Step [41190/67476], Loss: 4.4982\n",
      "Epoch [1/2], Step [41200/67476], Loss: 4.5568\n",
      "Epoch [1/2], Step [41210/67476], Loss: 4.4997\n",
      "Epoch [1/2], Step [41220/67476], Loss: 4.4139\n",
      "Epoch [1/2], Step [41230/67476], Loss: 4.4250\n",
      "Epoch [1/2], Step [41240/67476], Loss: 4.5725\n",
      "Epoch [1/2], Step [41250/67476], Loss: 4.5423\n",
      "Epoch [1/2], Step [41260/67476], Loss: 4.6520\n",
      "Epoch [1/2], Step [41270/67476], Loss: 4.4642\n",
      "Epoch [1/2], Step [41280/67476], Loss: 4.5350\n",
      "Epoch [1/2], Step [41290/67476], Loss: 4.4543\n",
      "Epoch [1/2], Step [41300/67476], Loss: 4.5336\n",
      "Epoch [1/2], Step [41310/67476], Loss: 5.0226\n",
      "Epoch [1/2], Step [41320/67476], Loss: 4.2325\n",
      "Epoch [1/2], Step [41330/67476], Loss: 4.6986\n",
      "Epoch [1/2], Step [41340/67476], Loss: 4.7076\n",
      "Epoch [1/2], Step [41350/67476], Loss: 4.6926\n",
      "Epoch [1/2], Step [41360/67476], Loss: 4.5331\n",
      "Epoch [1/2], Step [41370/67476], Loss: 4.4719\n",
      "Epoch [1/2], Step [41380/67476], Loss: 4.6919\n",
      "Epoch [1/2], Step [41390/67476], Loss: 4.6002\n",
      "Epoch [1/2], Step [41400/67476], Loss: 4.5122\n",
      "Epoch [1/2], Step [41410/67476], Loss: 4.6157\n",
      "Epoch [1/2], Step [41420/67476], Loss: 4.4678\n",
      "Epoch [1/2], Step [41430/67476], Loss: 4.4796\n",
      "Epoch [1/2], Step [41440/67476], Loss: 4.4542\n",
      "Epoch [1/2], Step [41450/67476], Loss: 4.6425\n",
      "Epoch [1/2], Step [41460/67476], Loss: 4.5662\n",
      "Epoch [1/2], Step [41470/67476], Loss: 4.6417\n",
      "Epoch [1/2], Step [41480/67476], Loss: 4.6444\n",
      "Epoch [1/2], Step [41490/67476], Loss: 4.5350\n",
      "Epoch [1/2], Step [41500/67476], Loss: 4.6703\n",
      "Epoch [1/2], Step [41510/67476], Loss: 4.7012\n",
      "Epoch [1/2], Step [41520/67476], Loss: 4.5826\n",
      "Epoch [1/2], Step [41530/67476], Loss: 4.4873\n",
      "Epoch [1/2], Step [41540/67476], Loss: 4.3532\n",
      "Epoch [1/2], Step [41550/67476], Loss: 4.3690\n",
      "Epoch [1/2], Step [41560/67476], Loss: 4.5329\n",
      "Epoch [1/2], Step [41570/67476], Loss: 4.7562\n",
      "Epoch [1/2], Step [41580/67476], Loss: 4.6919\n",
      "Epoch [1/2], Step [41590/67476], Loss: 4.5483\n",
      "Epoch [1/2], Step [41600/67476], Loss: 4.8755\n",
      "Epoch [1/2], Step [41610/67476], Loss: 4.8274\n",
      "Epoch [1/2], Step [41620/67476], Loss: 4.5964\n",
      "Epoch [1/2], Step [41630/67476], Loss: 4.4329\n",
      "Epoch [1/2], Step [41640/67476], Loss: 4.5583\n",
      "Epoch [1/2], Step [41650/67476], Loss: 4.6480\n",
      "Epoch [1/2], Step [41660/67476], Loss: 4.7404\n",
      "Epoch [1/2], Step [41670/67476], Loss: 4.4929\n",
      "Epoch [1/2], Step [41680/67476], Loss: 4.5455\n",
      "Epoch [1/2], Step [41690/67476], Loss: 4.5549\n",
      "Epoch [1/2], Step [41700/67476], Loss: 4.3880\n",
      "Epoch [1/2], Step [41710/67476], Loss: 4.6934\n",
      "Epoch [1/2], Step [41720/67476], Loss: 4.4481\n",
      "Epoch [1/2], Step [41730/67476], Loss: 4.5021\n",
      "Epoch [1/2], Step [41740/67476], Loss: 4.5772\n",
      "Epoch [1/2], Step [41750/67476], Loss: 4.5946\n",
      "Epoch [1/2], Step [41760/67476], Loss: 4.6256\n",
      "Epoch [1/2], Step [41770/67476], Loss: 4.5782\n",
      "Epoch [1/2], Step [41780/67476], Loss: 4.5741\n",
      "Epoch [1/2], Step [41790/67476], Loss: 4.5308\n",
      "Epoch [1/2], Step [41800/67476], Loss: 4.6211\n",
      "Epoch [1/2], Step [41810/67476], Loss: 4.6776\n",
      "Epoch [1/2], Step [41820/67476], Loss: 4.6754\n",
      "Epoch [1/2], Step [41830/67476], Loss: 4.4752\n",
      "Epoch [1/2], Step [41840/67476], Loss: 4.7080\n",
      "Epoch [1/2], Step [41850/67476], Loss: 4.6228\n",
      "Epoch [1/2], Step [41860/67476], Loss: 4.6081\n",
      "Epoch [1/2], Step [41870/67476], Loss: 4.6988\n",
      "Epoch [1/2], Step [41880/67476], Loss: 4.3866\n",
      "Epoch [1/2], Step [41890/67476], Loss: 4.5339\n",
      "Epoch [1/2], Step [41900/67476], Loss: 4.7639\n",
      "Epoch [1/2], Step [41910/67476], Loss: 4.6099\n",
      "Epoch [1/2], Step [41920/67476], Loss: 4.4694\n",
      "Epoch [1/2], Step [41930/67476], Loss: 4.5815\n",
      "Epoch [1/2], Step [41940/67476], Loss: 4.6745\n",
      "Epoch [1/2], Step [41950/67476], Loss: 4.5905\n",
      "Epoch [1/2], Step [41960/67476], Loss: 4.5852\n",
      "Epoch [1/2], Step [41970/67476], Loss: 4.4712\n",
      "Epoch [1/2], Step [41980/67476], Loss: 4.5282\n",
      "Epoch [1/2], Step [41990/67476], Loss: 4.7088\n",
      "Epoch [1/2], Step [42000/67476], Loss: 4.6707\n",
      "Epoch [1/2], Step [42010/67476], Loss: 4.5048\n",
      "Epoch [1/2], Step [42020/67476], Loss: 4.4858\n",
      "Epoch [1/2], Step [42030/67476], Loss: 4.7140\n",
      "Epoch [1/2], Step [42040/67476], Loss: 4.5692\n",
      "Epoch [1/2], Step [42050/67476], Loss: 4.7214\n",
      "Epoch [1/2], Step [42060/67476], Loss: 4.4821\n",
      "Epoch [1/2], Step [42070/67476], Loss: 4.6193\n",
      "Epoch [1/2], Step [42080/67476], Loss: 4.6414\n",
      "Epoch [1/2], Step [42090/67476], Loss: 4.3834\n",
      "Epoch [1/2], Step [42100/67476], Loss: 4.5658\n",
      "Epoch [1/2], Step [42110/67476], Loss: 4.3772\n",
      "Epoch [1/2], Step [42120/67476], Loss: 4.5971\n",
      "Epoch [1/2], Step [42130/67476], Loss: 4.6285\n",
      "Epoch [1/2], Step [42140/67476], Loss: 4.3908\n",
      "Epoch [1/2], Step [42150/67476], Loss: 4.6004\n",
      "Epoch [1/2], Step [42160/67476], Loss: 4.6474\n",
      "Epoch [1/2], Step [42170/67476], Loss: 4.3815\n",
      "Epoch [1/2], Step [42180/67476], Loss: 4.8015\n",
      "Epoch [1/2], Step [42190/67476], Loss: 4.7568\n",
      "Epoch [1/2], Step [42200/67476], Loss: 4.6552\n",
      "Epoch [1/2], Step [42210/67476], Loss: 4.5354\n",
      "Epoch [1/2], Step [42220/67476], Loss: 4.5669\n",
      "Epoch [1/2], Step [42230/67476], Loss: 4.5567\n",
      "Epoch [1/2], Step [42240/67476], Loss: 4.7776\n",
      "Epoch [1/2], Step [42250/67476], Loss: 4.6322\n",
      "Epoch [1/2], Step [42260/67476], Loss: 4.5330\n",
      "Epoch [1/2], Step [42270/67476], Loss: 4.7048\n",
      "Epoch [1/2], Step [42280/67476], Loss: 4.6868\n",
      "Epoch [1/2], Step [42290/67476], Loss: 4.6723\n",
      "Epoch [1/2], Step [42300/67476], Loss: 4.6560\n",
      "Epoch [1/2], Step [42310/67476], Loss: 4.5589\n",
      "Epoch [1/2], Step [42320/67476], Loss: 4.6019\n",
      "Epoch [1/2], Step [42330/67476], Loss: 4.5188\n",
      "Epoch [1/2], Step [42340/67476], Loss: 4.6468\n",
      "Epoch [1/2], Step [42350/67476], Loss: 4.3087\n",
      "Epoch [1/2], Step [42360/67476], Loss: 4.5043\n",
      "Epoch [1/2], Step [42370/67476], Loss: 4.6444\n",
      "Epoch [1/2], Step [42380/67476], Loss: 4.6657\n",
      "Epoch [1/2], Step [42390/67476], Loss: 4.6228\n",
      "Epoch [1/2], Step [42400/67476], Loss: 4.6798\n",
      "Epoch [1/2], Step [42410/67476], Loss: 4.6591\n",
      "Epoch [1/2], Step [42420/67476], Loss: 4.5986\n",
      "Epoch [1/2], Step [42430/67476], Loss: 4.5488\n",
      "Epoch [1/2], Step [42440/67476], Loss: 4.4014\n",
      "Epoch [1/2], Step [42450/67476], Loss: 4.5902\n",
      "Epoch [1/2], Step [42460/67476], Loss: 4.4792\n",
      "Epoch [1/2], Step [42470/67476], Loss: 4.8735\n",
      "Epoch [1/2], Step [42480/67476], Loss: 4.5269\n",
      "Epoch [1/2], Step [42490/67476], Loss: 4.5887\n",
      "Epoch [1/2], Step [42500/67476], Loss: 4.6751\n",
      "Epoch [1/2], Step [42510/67476], Loss: 4.5555\n",
      "Epoch [1/2], Step [42520/67476], Loss: 4.6055\n",
      "Epoch [1/2], Step [42530/67476], Loss: 4.4452\n",
      "Epoch [1/2], Step [42540/67476], Loss: 4.5860\n",
      "Epoch [1/2], Step [42550/67476], Loss: 4.6607\n",
      "Epoch [1/2], Step [42560/67476], Loss: 4.8780\n",
      "Epoch [1/2], Step [42570/67476], Loss: 4.6150\n",
      "Epoch [1/2], Step [42580/67476], Loss: 4.5658\n",
      "Epoch [1/2], Step [42590/67476], Loss: 4.5290\n",
      "Epoch [1/2], Step [42600/67476], Loss: 4.6611\n",
      "Epoch [1/2], Step [42610/67476], Loss: 4.5772\n",
      "Epoch [1/2], Step [42620/67476], Loss: 4.5433\n",
      "Epoch [1/2], Step [42630/67476], Loss: 4.5663\n",
      "Epoch [1/2], Step [42640/67476], Loss: 4.7279\n",
      "Epoch [1/2], Step [42650/67476], Loss: 4.6503\n",
      "Epoch [1/2], Step [42660/67476], Loss: 4.6036\n",
      "Epoch [1/2], Step [42670/67476], Loss: 4.3339\n",
      "Epoch [1/2], Step [42680/67476], Loss: 4.5965\n",
      "Epoch [1/2], Step [42690/67476], Loss: 4.4599\n",
      "Epoch [1/2], Step [42700/67476], Loss: 4.5745\n",
      "Epoch [1/2], Step [42710/67476], Loss: 4.5998\n",
      "Epoch [1/2], Step [42720/67476], Loss: 4.4314\n",
      "Epoch [1/2], Step [42730/67476], Loss: 4.4829\n",
      "Epoch [1/2], Step [42740/67476], Loss: 4.5566\n",
      "Epoch [1/2], Step [42750/67476], Loss: 4.7415\n",
      "Epoch [1/2], Step [42760/67476], Loss: 4.5313\n",
      "Epoch [1/2], Step [42770/67476], Loss: 4.6787\n",
      "Epoch [1/2], Step [42780/67476], Loss: 4.4361\n",
      "Epoch [1/2], Step [42790/67476], Loss: 4.6286\n",
      "Epoch [1/2], Step [42800/67476], Loss: 4.4825\n",
      "Epoch [1/2], Step [42810/67476], Loss: 4.6004\n",
      "Epoch [1/2], Step [42820/67476], Loss: 4.4935\n",
      "Epoch [1/2], Step [42830/67476], Loss: 4.6084\n",
      "Epoch [1/2], Step [42840/67476], Loss: 4.5701\n",
      "Epoch [1/2], Step [42850/67476], Loss: 4.4899\n",
      "Epoch [1/2], Step [42860/67476], Loss: 4.6940\n",
      "Epoch [1/2], Step [42870/67476], Loss: 4.5884\n",
      "Epoch [1/2], Step [42880/67476], Loss: 4.7145\n",
      "Epoch [1/2], Step [42890/67476], Loss: 4.6265\n",
      "Epoch [1/2], Step [42900/67476], Loss: 4.6225\n",
      "Epoch [1/2], Step [42910/67476], Loss: 4.7098\n",
      "Epoch [1/2], Step [42920/67476], Loss: 4.6186\n",
      "Epoch [1/2], Step [42930/67476], Loss: 4.7310\n",
      "Epoch [1/2], Step [42940/67476], Loss: 4.6587\n",
      "Epoch [1/2], Step [42950/67476], Loss: 4.5749\n",
      "Epoch [1/2], Step [42960/67476], Loss: 4.4355\n",
      "Epoch [1/2], Step [42970/67476], Loss: 4.6744\n",
      "Epoch [1/2], Step [42980/67476], Loss: 4.5858\n",
      "Epoch [1/2], Step [42990/67476], Loss: 4.7330\n",
      "Epoch [1/2], Step [43000/67476], Loss: 4.7212\n",
      "Epoch [1/2], Step [43010/67476], Loss: 4.5290\n",
      "Epoch [1/2], Step [43020/67476], Loss: 4.4105\n",
      "Epoch [1/2], Step [43030/67476], Loss: 4.6822\n",
      "Epoch [1/2], Step [43040/67476], Loss: 4.4128\n",
      "Epoch [1/2], Step [43050/67476], Loss: 4.6945\n",
      "Epoch [1/2], Step [43060/67476], Loss: 4.5875\n",
      "Epoch [1/2], Step [43070/67476], Loss: 4.7122\n",
      "Epoch [1/2], Step [43080/67476], Loss: 4.6136\n",
      "Epoch [1/2], Step [43090/67476], Loss: 4.6434\n",
      "Epoch [1/2], Step [43100/67476], Loss: 4.3387\n",
      "Epoch [1/2], Step [43110/67476], Loss: 4.6971\n",
      "Epoch [1/2], Step [43120/67476], Loss: 4.5003\n",
      "Epoch [1/2], Step [43130/67476], Loss: 4.6173\n",
      "Epoch [1/2], Step [43140/67476], Loss: 4.7005\n",
      "Epoch [1/2], Step [43150/67476], Loss: 4.7556\n",
      "Epoch [1/2], Step [43160/67476], Loss: 4.4571\n",
      "Epoch [1/2], Step [43170/67476], Loss: 4.4003\n",
      "Epoch [1/2], Step [43180/67476], Loss: 4.6211\n",
      "Epoch [1/2], Step [43190/67476], Loss: 4.6149\n",
      "Epoch [1/2], Step [43200/67476], Loss: 4.6620\n",
      "Epoch [1/2], Step [43210/67476], Loss: 4.6830\n",
      "Epoch [1/2], Step [43220/67476], Loss: 4.5593\n",
      "Epoch [1/2], Step [43230/67476], Loss: 4.4781\n",
      "Epoch [1/2], Step [43240/67476], Loss: 4.4252\n",
      "Epoch [1/2], Step [43250/67476], Loss: 4.6121\n",
      "Epoch [1/2], Step [43260/67476], Loss: 4.5956\n",
      "Epoch [1/2], Step [43270/67476], Loss: 4.5185\n",
      "Epoch [1/2], Step [43280/67476], Loss: 4.4389\n",
      "Epoch [1/2], Step [43290/67476], Loss: 4.5075\n",
      "Epoch [1/2], Step [43300/67476], Loss: 4.6894\n",
      "Epoch [1/2], Step [43310/67476], Loss: 4.4123\n",
      "Epoch [1/2], Step [43320/67476], Loss: 4.4013\n",
      "Epoch [1/2], Step [43330/67476], Loss: 4.4627\n",
      "Epoch [1/2], Step [43340/67476], Loss: 4.4877\n",
      "Epoch [1/2], Step [43350/67476], Loss: 4.6320\n",
      "Epoch [1/2], Step [43360/67476], Loss: 4.4507\n",
      "Epoch [1/2], Step [43370/67476], Loss: 4.4942\n",
      "Epoch [1/2], Step [43380/67476], Loss: 4.6983\n",
      "Epoch [1/2], Step [43390/67476], Loss: 4.3801\n",
      "Epoch [1/2], Step [43400/67476], Loss: 4.6038\n",
      "Epoch [1/2], Step [43410/67476], Loss: 4.7329\n",
      "Epoch [1/2], Step [43420/67476], Loss: 4.5112\n",
      "Epoch [1/2], Step [43430/67476], Loss: 4.6611\n",
      "Epoch [1/2], Step [43440/67476], Loss: 4.5605\n",
      "Epoch [1/2], Step [43450/67476], Loss: 4.5336\n",
      "Epoch [1/2], Step [43460/67476], Loss: 4.6571\n",
      "Epoch [1/2], Step [43470/67476], Loss: 4.3261\n",
      "Epoch [1/2], Step [43480/67476], Loss: 4.6541\n",
      "Epoch [1/2], Step [43490/67476], Loss: 4.5336\n",
      "Epoch [1/2], Step [43500/67476], Loss: 4.5764\n",
      "Epoch [1/2], Step [43510/67476], Loss: 4.4991\n",
      "Epoch [1/2], Step [43520/67476], Loss: 4.3464\n",
      "Epoch [1/2], Step [43530/67476], Loss: 4.4381\n",
      "Epoch [1/2], Step [43540/67476], Loss: 4.3232\n",
      "Epoch [1/2], Step [43550/67476], Loss: 4.6396\n",
      "Epoch [1/2], Step [43560/67476], Loss: 4.7120\n",
      "Epoch [1/2], Step [43570/67476], Loss: 4.7236\n",
      "Epoch [1/2], Step [43580/67476], Loss: 4.4869\n",
      "Epoch [1/2], Step [43590/67476], Loss: 4.6421\n",
      "Epoch [1/2], Step [43600/67476], Loss: 4.4623\n",
      "Epoch [1/2], Step [43610/67476], Loss: 4.6815\n",
      "Epoch [1/2], Step [43620/67476], Loss: 4.5380\n",
      "Epoch [1/2], Step [43630/67476], Loss: 4.5480\n",
      "Epoch [1/2], Step [43640/67476], Loss: 4.5515\n",
      "Epoch [1/2], Step [43650/67476], Loss: 4.7423\n",
      "Epoch [1/2], Step [43660/67476], Loss: 4.5135\n",
      "Epoch [1/2], Step [43670/67476], Loss: 4.7657\n",
      "Epoch [1/2], Step [43680/67476], Loss: 4.4352\n",
      "Epoch [1/2], Step [43690/67476], Loss: 4.5616\n",
      "Epoch [1/2], Step [43700/67476], Loss: 4.4765\n",
      "Epoch [1/2], Step [43710/67476], Loss: 4.6155\n",
      "Epoch [1/2], Step [43720/67476], Loss: 4.4546\n",
      "Epoch [1/2], Step [43730/67476], Loss: 4.5057\n",
      "Epoch [1/2], Step [43740/67476], Loss: 4.6667\n",
      "Epoch [1/2], Step [43750/67476], Loss: 4.6879\n",
      "Epoch [1/2], Step [43760/67476], Loss: 4.6248\n",
      "Epoch [1/2], Step [43770/67476], Loss: 4.7382\n",
      "Epoch [1/2], Step [43780/67476], Loss: 4.5651\n",
      "Epoch [1/2], Step [43790/67476], Loss: 4.4437\n",
      "Epoch [1/2], Step [43800/67476], Loss: 4.4686\n",
      "Epoch [1/2], Step [43810/67476], Loss: 4.3737\n",
      "Epoch [1/2], Step [43820/67476], Loss: 4.8032\n",
      "Epoch [1/2], Step [43830/67476], Loss: 4.5342\n",
      "Epoch [1/2], Step [43840/67476], Loss: 4.5879\n",
      "Epoch [1/2], Step [43850/67476], Loss: 4.5263\n",
      "Epoch [1/2], Step [43860/67476], Loss: 4.4125\n",
      "Epoch [1/2], Step [43870/67476], Loss: 4.4108\n",
      "Epoch [1/2], Step [43880/67476], Loss: 4.8066\n",
      "Epoch [1/2], Step [43890/67476], Loss: 4.5659\n",
      "Epoch [1/2], Step [43900/67476], Loss: 4.6324\n",
      "Epoch [1/2], Step [43910/67476], Loss: 4.8872\n",
      "Epoch [1/2], Step [43920/67476], Loss: 4.6506\n",
      "Epoch [1/2], Step [43930/67476], Loss: 4.3640\n",
      "Epoch [1/2], Step [43940/67476], Loss: 4.8838\n",
      "Epoch [1/2], Step [43950/67476], Loss: 4.6624\n",
      "Epoch [1/2], Step [43960/67476], Loss: 4.4812\n",
      "Epoch [1/2], Step [43970/67476], Loss: 4.3263\n",
      "Epoch [1/2], Step [43980/67476], Loss: 4.5339\n",
      "Epoch [1/2], Step [43990/67476], Loss: 4.7285\n",
      "Epoch [1/2], Step [44000/67476], Loss: 4.6135\n",
      "Epoch [1/2], Step [44010/67476], Loss: 4.5134\n",
      "Epoch [1/2], Step [44020/67476], Loss: 4.5293\n",
      "Epoch [1/2], Step [44030/67476], Loss: 4.6226\n",
      "Epoch [1/2], Step [44040/67476], Loss: 4.5532\n",
      "Epoch [1/2], Step [44050/67476], Loss: 4.6299\n",
      "Epoch [1/2], Step [44060/67476], Loss: 4.5813\n",
      "Epoch [1/2], Step [44070/67476], Loss: 4.6282\n",
      "Epoch [1/2], Step [44080/67476], Loss: 4.5208\n",
      "Epoch [1/2], Step [44090/67476], Loss: 4.6270\n",
      "Epoch [1/2], Step [44100/67476], Loss: 4.7479\n",
      "Epoch [1/2], Step [44110/67476], Loss: 4.4814\n",
      "Epoch [1/2], Step [44120/67476], Loss: 4.5927\n",
      "Epoch [1/2], Step [44130/67476], Loss: 4.5059\n",
      "Epoch [1/2], Step [44140/67476], Loss: 4.6321\n",
      "Epoch [1/2], Step [44150/67476], Loss: 4.6555\n",
      "Epoch [1/2], Step [44160/67476], Loss: 4.5418\n",
      "Epoch [1/2], Step [44170/67476], Loss: 4.4607\n",
      "Epoch [1/2], Step [44180/67476], Loss: 4.4744\n",
      "Epoch [1/2], Step [44190/67476], Loss: 4.5516\n",
      "Epoch [1/2], Step [44200/67476], Loss: 4.4910\n",
      "Epoch [1/2], Step [44210/67476], Loss: 4.5750\n",
      "Epoch [1/2], Step [44220/67476], Loss: 4.5400\n",
      "Epoch [1/2], Step [44230/67476], Loss: 4.4463\n",
      "Epoch [1/2], Step [44240/67476], Loss: 4.3711\n",
      "Epoch [1/2], Step [44250/67476], Loss: 4.4723\n",
      "Epoch [1/2], Step [44260/67476], Loss: 4.7347\n",
      "Epoch [1/2], Step [44270/67476], Loss: 4.5401\n",
      "Epoch [1/2], Step [44280/67476], Loss: 4.6128\n",
      "Epoch [1/2], Step [44290/67476], Loss: 4.6068\n",
      "Epoch [1/2], Step [44300/67476], Loss: 4.4088\n",
      "Epoch [1/2], Step [44310/67476], Loss: 4.4711\n",
      "Epoch [1/2], Step [44320/67476], Loss: 4.5560\n",
      "Epoch [1/2], Step [44330/67476], Loss: 4.6769\n",
      "Epoch [1/2], Step [44340/67476], Loss: 4.5468\n",
      "Epoch [1/2], Step [44350/67476], Loss: 4.6676\n",
      "Epoch [1/2], Step [44360/67476], Loss: 4.6386\n",
      "Epoch [1/2], Step [44370/67476], Loss: 4.5748\n",
      "Epoch [1/2], Step [44380/67476], Loss: 4.3108\n",
      "Epoch [1/2], Step [44390/67476], Loss: 4.4872\n",
      "Epoch [1/2], Step [44400/67476], Loss: 4.4465\n",
      "Epoch [1/2], Step [44410/67476], Loss: 4.6129\n",
      "Epoch [1/2], Step [44420/67476], Loss: 4.4416\n",
      "Epoch [1/2], Step [44430/67476], Loss: 4.6111\n",
      "Epoch [1/2], Step [44440/67476], Loss: 4.6821\n",
      "Epoch [1/2], Step [44450/67476], Loss: 4.5214\n",
      "Epoch [1/2], Step [44460/67476], Loss: 4.5740\n",
      "Epoch [1/2], Step [44470/67476], Loss: 4.5027\n",
      "Epoch [1/2], Step [44480/67476], Loss: 4.7063\n",
      "Epoch [1/2], Step [44490/67476], Loss: 4.8000\n",
      "Epoch [1/2], Step [44500/67476], Loss: 4.6003\n",
      "Epoch [1/2], Step [44510/67476], Loss: 4.6920\n",
      "Epoch [1/2], Step [44520/67476], Loss: 4.6147\n",
      "Epoch [1/2], Step [44530/67476], Loss: 4.5283\n",
      "Epoch [1/2], Step [44540/67476], Loss: 4.5103\n",
      "Epoch [1/2], Step [44550/67476], Loss: 4.6105\n",
      "Epoch [1/2], Step [44560/67476], Loss: 4.6924\n",
      "Epoch [1/2], Step [44570/67476], Loss: 4.5600\n",
      "Epoch [1/2], Step [44580/67476], Loss: 4.5180\n",
      "Epoch [1/2], Step [44590/67476], Loss: 4.4591\n",
      "Epoch [1/2], Step [44600/67476], Loss: 4.5771\n",
      "Epoch [1/2], Step [44610/67476], Loss: 4.4883\n",
      "Epoch [1/2], Step [44620/67476], Loss: 4.5487\n",
      "Epoch [1/2], Step [44630/67476], Loss: 4.7850\n",
      "Epoch [1/2], Step [44640/67476], Loss: 4.5884\n",
      "Epoch [1/2], Step [44650/67476], Loss: 4.4327\n",
      "Epoch [1/2], Step [44660/67476], Loss: 4.5146\n",
      "Epoch [1/2], Step [44670/67476], Loss: 4.6879\n",
      "Epoch [1/2], Step [44680/67476], Loss: 4.6802\n",
      "Epoch [1/2], Step [44690/67476], Loss: 4.4374\n",
      "Epoch [1/2], Step [44700/67476], Loss: 4.6847\n",
      "Epoch [1/2], Step [44710/67476], Loss: 4.6430\n",
      "Epoch [1/2], Step [44720/67476], Loss: 4.5612\n",
      "Epoch [1/2], Step [44730/67476], Loss: 4.4441\n",
      "Epoch [1/2], Step [44740/67476], Loss: 4.6411\n",
      "Epoch [1/2], Step [44750/67476], Loss: 4.6521\n",
      "Epoch [1/2], Step [44760/67476], Loss: 4.6960\n",
      "Epoch [1/2], Step [44770/67476], Loss: 4.5214\n",
      "Epoch [1/2], Step [44780/67476], Loss: 4.4452\n",
      "Epoch [1/2], Step [44790/67476], Loss: 4.4344\n",
      "Epoch [1/2], Step [44800/67476], Loss: 4.6178\n",
      "Epoch [1/2], Step [44810/67476], Loss: 4.5250\n",
      "Epoch [1/2], Step [44820/67476], Loss: 4.5633\n",
      "Epoch [1/2], Step [44830/67476], Loss: 4.6102\n",
      "Epoch [1/2], Step [44840/67476], Loss: 4.5383\n",
      "Epoch [1/2], Step [44850/67476], Loss: 4.6386\n",
      "Epoch [1/2], Step [44860/67476], Loss: 4.7053\n",
      "Epoch [1/2], Step [44870/67476], Loss: 4.5742\n",
      "Epoch [1/2], Step [44880/67476], Loss: 4.6194\n",
      "Epoch [1/2], Step [44890/67476], Loss: 4.5745\n",
      "Epoch [1/2], Step [44900/67476], Loss: 4.4182\n",
      "Epoch [1/2], Step [44910/67476], Loss: 4.6657\n",
      "Epoch [1/2], Step [44920/67476], Loss: 4.7073\n",
      "Epoch [1/2], Step [44930/67476], Loss: 4.5807\n",
      "Epoch [1/2], Step [44940/67476], Loss: 4.6525\n",
      "Epoch [1/2], Step [44950/67476], Loss: 4.5906\n",
      "Epoch [1/2], Step [44960/67476], Loss: 4.6917\n",
      "Epoch [1/2], Step [44970/67476], Loss: 4.5240\n",
      "Epoch [1/2], Step [44980/67476], Loss: 4.7383\n",
      "Epoch [1/2], Step [44990/67476], Loss: 4.6115\n",
      "Epoch [1/2], Step [45000/67476], Loss: 4.7023\n",
      "Epoch [1/2], Step [45010/67476], Loss: 4.5338\n",
      "Epoch [1/2], Step [45020/67476], Loss: 4.5603\n",
      "Epoch [1/2], Step [45030/67476], Loss: 4.5716\n",
      "Epoch [1/2], Step [45040/67476], Loss: 4.6038\n",
      "Epoch [1/2], Step [45050/67476], Loss: 4.3515\n",
      "Epoch [1/2], Step [45060/67476], Loss: 4.5451\n",
      "Epoch [1/2], Step [45070/67476], Loss: 4.5694\n",
      "Epoch [1/2], Step [45080/67476], Loss: 4.5992\n",
      "Epoch [1/2], Step [45090/67476], Loss: 4.4786\n",
      "Epoch [1/2], Step [45100/67476], Loss: 4.6562\n",
      "Epoch [1/2], Step [45110/67476], Loss: 4.4649\n",
      "Epoch [1/2], Step [45120/67476], Loss: 4.4898\n",
      "Epoch [1/2], Step [45130/67476], Loss: 4.6076\n",
      "Epoch [1/2], Step [45140/67476], Loss: 4.6085\n",
      "Epoch [1/2], Step [45150/67476], Loss: 4.7079\n",
      "Epoch [1/2], Step [45160/67476], Loss: 4.5636\n",
      "Epoch [1/2], Step [45170/67476], Loss: 4.6290\n",
      "Epoch [1/2], Step [45180/67476], Loss: 4.4484\n",
      "Epoch [1/2], Step [45190/67476], Loss: 4.5769\n",
      "Epoch [1/2], Step [45200/67476], Loss: 4.4867\n",
      "Epoch [1/2], Step [45210/67476], Loss: 4.5247\n",
      "Epoch [1/2], Step [45220/67476], Loss: 4.6221\n",
      "Epoch [1/2], Step [45230/67476], Loss: 4.5252\n",
      "Epoch [1/2], Step [45240/67476], Loss: 4.7840\n",
      "Epoch [1/2], Step [45250/67476], Loss: 4.5008\n",
      "Epoch [1/2], Step [45260/67476], Loss: 4.5224\n",
      "Epoch [1/2], Step [45270/67476], Loss: 4.5964\n",
      "Epoch [1/2], Step [45280/67476], Loss: 4.6180\n",
      "Epoch [1/2], Step [45290/67476], Loss: 4.5384\n",
      "Epoch [1/2], Step [45300/67476], Loss: 4.5149\n",
      "Epoch [1/2], Step [45310/67476], Loss: 4.6930\n",
      "Epoch [1/2], Step [45320/67476], Loss: 4.5132\n",
      "Epoch [1/2], Step [45330/67476], Loss: 4.5541\n",
      "Epoch [1/2], Step [45340/67476], Loss: 4.5725\n",
      "Epoch [1/2], Step [45350/67476], Loss: 4.5311\n",
      "Epoch [1/2], Step [45360/67476], Loss: 4.6410\n",
      "Epoch [1/2], Step [45370/67476], Loss: 4.6178\n",
      "Epoch [1/2], Step [45380/67476], Loss: 4.5914\n",
      "Epoch [1/2], Step [45390/67476], Loss: 4.6282\n",
      "Epoch [1/2], Step [45400/67476], Loss: 4.7744\n",
      "Epoch [1/2], Step [45410/67476], Loss: 4.7289\n",
      "Epoch [1/2], Step [45420/67476], Loss: 4.5387\n",
      "Epoch [1/2], Step [45430/67476], Loss: 4.4497\n",
      "Epoch [1/2], Step [45440/67476], Loss: 4.4196\n",
      "Epoch [1/2], Step [45450/67476], Loss: 4.4721\n",
      "Epoch [1/2], Step [45460/67476], Loss: 4.3798\n",
      "Epoch [1/2], Step [45470/67476], Loss: 4.6042\n",
      "Epoch [1/2], Step [45480/67476], Loss: 4.5677\n",
      "Epoch [1/2], Step [45490/67476], Loss: 4.6576\n",
      "Epoch [1/2], Step [45500/67476], Loss: 4.5798\n",
      "Epoch [1/2], Step [45510/67476], Loss: 4.6843\n",
      "Epoch [1/2], Step [45520/67476], Loss: 4.3724\n",
      "Epoch [1/2], Step [45530/67476], Loss: 4.6864\n",
      "Epoch [1/2], Step [45540/67476], Loss: 4.6645\n",
      "Epoch [1/2], Step [45550/67476], Loss: 4.5270\n",
      "Epoch [1/2], Step [45560/67476], Loss: 4.7095\n",
      "Epoch [1/2], Step [45570/67476], Loss: 4.7208\n",
      "Epoch [1/2], Step [45580/67476], Loss: 4.6046\n",
      "Epoch [1/2], Step [45590/67476], Loss: 4.6783\n",
      "Epoch [1/2], Step [45600/67476], Loss: 4.6124\n",
      "Epoch [1/2], Step [45610/67476], Loss: 4.5655\n",
      "Epoch [1/2], Step [45620/67476], Loss: 4.5131\n",
      "Epoch [1/2], Step [45630/67476], Loss: 4.3938\n",
      "Epoch [1/2], Step [45640/67476], Loss: 4.5362\n",
      "Epoch [1/2], Step [45650/67476], Loss: 4.6934\n",
      "Epoch [1/2], Step [45660/67476], Loss: 4.6287\n",
      "Epoch [1/2], Step [45670/67476], Loss: 4.5892\n",
      "Epoch [1/2], Step [45680/67476], Loss: 4.5184\n",
      "Epoch [1/2], Step [45690/67476], Loss: 4.6121\n",
      "Epoch [1/2], Step [45700/67476], Loss: 4.6503\n",
      "Epoch [1/2], Step [45710/67476], Loss: 4.5196\n",
      "Epoch [1/2], Step [45720/67476], Loss: 4.5773\n",
      "Epoch [1/2], Step [45730/67476], Loss: 4.4148\n",
      "Epoch [1/2], Step [45740/67476], Loss: 4.5897\n",
      "Epoch [1/2], Step [45750/67476], Loss: 4.5268\n",
      "Epoch [1/2], Step [45760/67476], Loss: 4.6375\n",
      "Epoch [1/2], Step [45770/67476], Loss: 4.4619\n",
      "Epoch [1/2], Step [45780/67476], Loss: 4.6233\n",
      "Epoch [1/2], Step [45790/67476], Loss: 4.5052\n",
      "Epoch [1/2], Step [45800/67476], Loss: 4.6863\n",
      "Epoch [1/2], Step [45810/67476], Loss: 4.6080\n",
      "Epoch [1/2], Step [45820/67476], Loss: 4.8039\n",
      "Epoch [1/2], Step [45830/67476], Loss: 4.4793\n",
      "Epoch [1/2], Step [45840/67476], Loss: 4.6452\n",
      "Epoch [1/2], Step [45850/67476], Loss: 4.7176\n",
      "Epoch [1/2], Step [45860/67476], Loss: 4.6162\n",
      "Epoch [1/2], Step [45870/67476], Loss: 4.5128\n",
      "Epoch [1/2], Step [45880/67476], Loss: 4.5759\n",
      "Epoch [1/2], Step [45890/67476], Loss: 4.6349\n",
      "Epoch [1/2], Step [45900/67476], Loss: 4.7118\n",
      "Epoch [1/2], Step [45910/67476], Loss: 4.5346\n",
      "Epoch [1/2], Step [45920/67476], Loss: 4.6792\n",
      "Epoch [1/2], Step [45930/67476], Loss: 4.7219\n",
      "Epoch [1/2], Step [45940/67476], Loss: 4.5096\n",
      "Epoch [1/2], Step [45950/67476], Loss: 4.5750\n",
      "Epoch [1/2], Step [45960/67476], Loss: 4.4591\n",
      "Epoch [1/2], Step [45970/67476], Loss: 4.6643\n",
      "Epoch [1/2], Step [45980/67476], Loss: 4.5220\n",
      "Epoch [1/2], Step [45990/67476], Loss: 4.5812\n",
      "Epoch [1/2], Step [46000/67476], Loss: 4.6164\n",
      "Epoch [1/2], Step [46010/67476], Loss: 4.7054\n",
      "Epoch [1/2], Step [46020/67476], Loss: 4.6425\n",
      "Epoch [1/2], Step [46030/67476], Loss: 4.6978\n",
      "Epoch [1/2], Step [46040/67476], Loss: 4.5794\n",
      "Epoch [1/2], Step [46050/67476], Loss: 4.5443\n",
      "Epoch [1/2], Step [46060/67476], Loss: 4.5677\n",
      "Epoch [1/2], Step [46070/67476], Loss: 4.4964\n",
      "Epoch [1/2], Step [46080/67476], Loss: 4.5780\n",
      "Epoch [1/2], Step [46090/67476], Loss: 4.8299\n",
      "Epoch [1/2], Step [46100/67476], Loss: 4.8986\n",
      "Epoch [1/2], Step [46110/67476], Loss: 4.7201\n",
      "Epoch [1/2], Step [46120/67476], Loss: 4.4141\n",
      "Epoch [1/2], Step [46130/67476], Loss: 4.6831\n",
      "Epoch [1/2], Step [46140/67476], Loss: 4.3451\n",
      "Epoch [1/2], Step [46150/67476], Loss: 4.5363\n",
      "Epoch [1/2], Step [46160/67476], Loss: 4.6454\n",
      "Epoch [1/2], Step [46170/67476], Loss: 4.6642\n",
      "Epoch [1/2], Step [46180/67476], Loss: 4.4605\n",
      "Epoch [1/2], Step [46190/67476], Loss: 4.4552\n",
      "Epoch [1/2], Step [46200/67476], Loss: 4.6105\n",
      "Epoch [1/2], Step [46210/67476], Loss: 4.6722\n",
      "Epoch [1/2], Step [46220/67476], Loss: 4.7687\n",
      "Epoch [1/2], Step [46230/67476], Loss: 4.7773\n",
      "Epoch [1/2], Step [46240/67476], Loss: 4.5878\n",
      "Epoch [1/2], Step [46250/67476], Loss: 4.4534\n",
      "Epoch [1/2], Step [46260/67476], Loss: 4.5177\n",
      "Epoch [1/2], Step [46270/67476], Loss: 4.4352\n",
      "Epoch [1/2], Step [46280/67476], Loss: 4.5845\n",
      "Epoch [1/2], Step [46290/67476], Loss: 4.4672\n",
      "Epoch [1/2], Step [46300/67476], Loss: 4.6739\n",
      "Epoch [1/2], Step [46310/67476], Loss: 4.7371\n",
      "Epoch [1/2], Step [46320/67476], Loss: 4.6966\n",
      "Epoch [1/2], Step [46330/67476], Loss: 4.6606\n",
      "Epoch [1/2], Step [46340/67476], Loss: 4.7069\n",
      "Epoch [1/2], Step [46350/67476], Loss: 4.7820\n",
      "Epoch [1/2], Step [46360/67476], Loss: 4.4551\n",
      "Epoch [1/2], Step [46370/67476], Loss: 4.4791\n",
      "Epoch [1/2], Step [46380/67476], Loss: 4.5494\n",
      "Epoch [1/2], Step [46390/67476], Loss: 4.7168\n",
      "Epoch [1/2], Step [46400/67476], Loss: 4.6457\n",
      "Epoch [1/2], Step [46410/67476], Loss: 4.7368\n",
      "Epoch [1/2], Step [46420/67476], Loss: 4.4765\n",
      "Epoch [1/2], Step [46430/67476], Loss: 4.6301\n",
      "Epoch [1/2], Step [46440/67476], Loss: 4.5199\n",
      "Epoch [1/2], Step [46450/67476], Loss: 4.6516\n",
      "Epoch [1/2], Step [46460/67476], Loss: 4.5777\n",
      "Epoch [1/2], Step [46470/67476], Loss: 4.6440\n",
      "Epoch [1/2], Step [46480/67476], Loss: 4.5707\n",
      "Epoch [1/2], Step [46490/67476], Loss: 4.5215\n",
      "Epoch [1/2], Step [46500/67476], Loss: 4.4373\n",
      "Epoch [1/2], Step [46510/67476], Loss: 4.7100\n",
      "Epoch [1/2], Step [46520/67476], Loss: 4.2919\n",
      "Epoch [1/2], Step [46530/67476], Loss: 4.5334\n",
      "Epoch [1/2], Step [46540/67476], Loss: 4.6111\n",
      "Epoch [1/2], Step [46550/67476], Loss: 4.4816\n",
      "Epoch [1/2], Step [46560/67476], Loss: 4.6282\n",
      "Epoch [1/2], Step [46570/67476], Loss: 4.4456\n",
      "Epoch [1/2], Step [46580/67476], Loss: 4.7966\n",
      "Epoch [1/2], Step [46590/67476], Loss: 4.6070\n",
      "Epoch [1/2], Step [46600/67476], Loss: 4.5357\n",
      "Epoch [1/2], Step [46610/67476], Loss: 4.6014\n",
      "Epoch [1/2], Step [46620/67476], Loss: 4.5095\n",
      "Epoch [1/2], Step [46630/67476], Loss: 4.5509\n",
      "Epoch [1/2], Step [46640/67476], Loss: 4.6638\n",
      "Epoch [1/2], Step [46650/67476], Loss: 4.6255\n",
      "Epoch [1/2], Step [46660/67476], Loss: 4.5995\n",
      "Epoch [1/2], Step [46670/67476], Loss: 4.6251\n",
      "Epoch [1/2], Step [46680/67476], Loss: 4.7168\n",
      "Epoch [1/2], Step [46690/67476], Loss: 4.5311\n",
      "Epoch [1/2], Step [46700/67476], Loss: 4.5703\n",
      "Epoch [1/2], Step [46710/67476], Loss: 4.6483\n",
      "Epoch [1/2], Step [46720/67476], Loss: 4.5917\n",
      "Epoch [1/2], Step [46730/67476], Loss: 4.6278\n",
      "Epoch [1/2], Step [46740/67476], Loss: 4.6427\n",
      "Epoch [1/2], Step [46750/67476], Loss: 4.7015\n",
      "Epoch [1/2], Step [46760/67476], Loss: 4.7249\n",
      "Epoch [1/2], Step [46770/67476], Loss: 4.4443\n",
      "Epoch [1/2], Step [46780/67476], Loss: 4.5209\n",
      "Epoch [1/2], Step [46790/67476], Loss: 4.7710\n",
      "Epoch [1/2], Step [46800/67476], Loss: 4.6472\n",
      "Epoch [1/2], Step [46810/67476], Loss: 4.5315\n",
      "Epoch [1/2], Step [46820/67476], Loss: 4.7762\n",
      "Epoch [1/2], Step [46830/67476], Loss: 4.5242\n",
      "Epoch [1/2], Step [46840/67476], Loss: 4.6951\n",
      "Epoch [1/2], Step [46850/67476], Loss: 4.5942\n",
      "Epoch [1/2], Step [46860/67476], Loss: 4.4916\n",
      "Epoch [1/2], Step [46870/67476], Loss: 4.3199\n",
      "Epoch [1/2], Step [46880/67476], Loss: 4.7313\n",
      "Epoch [1/2], Step [46890/67476], Loss: 4.6748\n",
      "Epoch [1/2], Step [46900/67476], Loss: 4.6319\n",
      "Epoch [1/2], Step [46910/67476], Loss: 4.6026\n",
      "Epoch [1/2], Step [46920/67476], Loss: 4.5701\n",
      "Epoch [1/2], Step [46930/67476], Loss: 4.6655\n",
      "Epoch [1/2], Step [46940/67476], Loss: 4.4901\n",
      "Epoch [1/2], Step [46950/67476], Loss: 4.7275\n",
      "Epoch [1/2], Step [46960/67476], Loss: 4.5069\n",
      "Epoch [1/2], Step [46970/67476], Loss: 4.4608\n",
      "Epoch [1/2], Step [46980/67476], Loss: 4.5550\n",
      "Epoch [1/2], Step [46990/67476], Loss: 4.6416\n",
      "Epoch [1/2], Step [47000/67476], Loss: 4.7236\n",
      "Epoch [1/2], Step [47010/67476], Loss: 4.4834\n",
      "Epoch [1/2], Step [47020/67476], Loss: 4.7235\n",
      "Epoch [1/2], Step [47030/67476], Loss: 4.7553\n",
      "Epoch [1/2], Step [47040/67476], Loss: 4.4882\n",
      "Epoch [1/2], Step [47050/67476], Loss: 4.3872\n",
      "Epoch [1/2], Step [47060/67476], Loss: 4.7937\n",
      "Epoch [1/2], Step [47070/67476], Loss: 4.4824\n",
      "Epoch [1/2], Step [47080/67476], Loss: 4.4284\n",
      "Epoch [1/2], Step [47090/67476], Loss: 4.5857\n",
      "Epoch [1/2], Step [47100/67476], Loss: 4.7766\n",
      "Epoch [1/2], Step [47110/67476], Loss: 4.4591\n",
      "Epoch [1/2], Step [47120/67476], Loss: 4.5008\n",
      "Epoch [1/2], Step [47130/67476], Loss: 4.7040\n",
      "Epoch [1/2], Step [47140/67476], Loss: 4.5402\n",
      "Epoch [1/2], Step [47150/67476], Loss: 4.6598\n",
      "Epoch [1/2], Step [47160/67476], Loss: 4.7624\n",
      "Epoch [1/2], Step [47170/67476], Loss: 4.5896\n",
      "Epoch [1/2], Step [47180/67476], Loss: 4.6278\n",
      "Epoch [1/2], Step [47190/67476], Loss: 4.6119\n",
      "Epoch [1/2], Step [47200/67476], Loss: 4.5915\n",
      "Epoch [1/2], Step [47210/67476], Loss: 4.4414\n",
      "Epoch [1/2], Step [47220/67476], Loss: 4.6714\n",
      "Epoch [1/2], Step [47230/67476], Loss: 4.4428\n",
      "Epoch [1/2], Step [47240/67476], Loss: 4.5432\n",
      "Epoch [1/2], Step [47250/67476], Loss: 4.3372\n",
      "Epoch [1/2], Step [47260/67476], Loss: 4.5129\n",
      "Epoch [1/2], Step [47270/67476], Loss: 4.6602\n",
      "Epoch [1/2], Step [47280/67476], Loss: 4.6498\n",
      "Epoch [1/2], Step [47290/67476], Loss: 4.6703\n",
      "Epoch [1/2], Step [47300/67476], Loss: 4.4369\n",
      "Epoch [1/2], Step [47310/67476], Loss: 4.4415\n",
      "Epoch [1/2], Step [47320/67476], Loss: 4.6542\n",
      "Epoch [1/2], Step [47330/67476], Loss: 4.5130\n",
      "Epoch [1/2], Step [47340/67476], Loss: 4.7751\n",
      "Epoch [1/2], Step [47350/67476], Loss: 4.7881\n",
      "Epoch [1/2], Step [47360/67476], Loss: 4.4965\n",
      "Epoch [1/2], Step [47370/67476], Loss: 4.4658\n",
      "Epoch [1/2], Step [47380/67476], Loss: 4.6632\n",
      "Epoch [1/2], Step [47390/67476], Loss: 4.5867\n",
      "Epoch [1/2], Step [47400/67476], Loss: 4.6538\n",
      "Epoch [1/2], Step [47410/67476], Loss: 4.5741\n",
      "Epoch [1/2], Step [47420/67476], Loss: 4.6772\n",
      "Epoch [1/2], Step [47430/67476], Loss: 4.5125\n",
      "Epoch [1/2], Step [47440/67476], Loss: 4.4468\n",
      "Epoch [1/2], Step [47450/67476], Loss: 4.4677\n",
      "Epoch [1/2], Step [47460/67476], Loss: 4.5112\n",
      "Epoch [1/2], Step [47470/67476], Loss: 4.6017\n",
      "Epoch [1/2], Step [47480/67476], Loss: 4.7216\n",
      "Epoch [1/2], Step [47490/67476], Loss: 4.3938\n",
      "Epoch [1/2], Step [47500/67476], Loss: 4.5517\n",
      "Epoch [1/2], Step [47510/67476], Loss: 4.4638\n",
      "Epoch [1/2], Step [47520/67476], Loss: 4.5994\n",
      "Epoch [1/2], Step [47530/67476], Loss: 4.5796\n",
      "Epoch [1/2], Step [47540/67476], Loss: 4.7180\n",
      "Epoch [1/2], Step [47550/67476], Loss: 4.5504\n",
      "Epoch [1/2], Step [47560/67476], Loss: 4.7414\n",
      "Epoch [1/2], Step [47570/67476], Loss: 4.5630\n",
      "Epoch [1/2], Step [47580/67476], Loss: 4.7019\n",
      "Epoch [1/2], Step [47590/67476], Loss: 4.7087\n",
      "Epoch [1/2], Step [47600/67476], Loss: 4.8302\n",
      "Epoch [1/2], Step [47610/67476], Loss: 4.5448\n",
      "Epoch [1/2], Step [47620/67476], Loss: 4.5311\n",
      "Epoch [1/2], Step [47630/67476], Loss: 4.7370\n",
      "Epoch [1/2], Step [47640/67476], Loss: 4.5007\n",
      "Epoch [1/2], Step [47650/67476], Loss: 4.6647\n",
      "Epoch [1/2], Step [47660/67476], Loss: 4.6942\n",
      "Epoch [1/2], Step [47670/67476], Loss: 4.5082\n",
      "Epoch [1/2], Step [47680/67476], Loss: 4.6064\n",
      "Epoch [1/2], Step [47690/67476], Loss: 4.4638\n",
      "Epoch [1/2], Step [47700/67476], Loss: 4.5655\n",
      "Epoch [1/2], Step [47710/67476], Loss: 4.5920\n",
      "Epoch [1/2], Step [47720/67476], Loss: 4.6332\n",
      "Epoch [1/2], Step [47730/67476], Loss: 4.6013\n",
      "Epoch [1/2], Step [47740/67476], Loss: 4.5548\n",
      "Epoch [1/2], Step [47750/67476], Loss: 4.4861\n",
      "Epoch [1/2], Step [47760/67476], Loss: 4.5831\n",
      "Epoch [1/2], Step [47770/67476], Loss: 4.4109\n",
      "Epoch [1/2], Step [47780/67476], Loss: 4.5877\n",
      "Epoch [1/2], Step [47790/67476], Loss: 4.6990\n",
      "Epoch [1/2], Step [47800/67476], Loss: 4.6292\n",
      "Epoch [1/2], Step [47810/67476], Loss: 4.6468\n",
      "Epoch [1/2], Step [47820/67476], Loss: 4.4648\n",
      "Epoch [1/2], Step [47830/67476], Loss: 4.6917\n",
      "Epoch [1/2], Step [47840/67476], Loss: 4.4062\n",
      "Epoch [1/2], Step [47850/67476], Loss: 4.5811\n",
      "Epoch [1/2], Step [47860/67476], Loss: 4.5372\n",
      "Epoch [1/2], Step [47870/67476], Loss: 4.7235\n",
      "Epoch [1/2], Step [47880/67476], Loss: 4.8370\n",
      "Epoch [1/2], Step [47890/67476], Loss: 4.5047\n",
      "Epoch [1/2], Step [47900/67476], Loss: 4.6020\n",
      "Epoch [1/2], Step [47910/67476], Loss: 4.3948\n",
      "Epoch [1/2], Step [47920/67476], Loss: 4.3514\n",
      "Epoch [1/2], Step [47930/67476], Loss: 4.5600\n",
      "Epoch [1/2], Step [47940/67476], Loss: 4.5418\n",
      "Epoch [1/2], Step [47950/67476], Loss: 4.4088\n",
      "Epoch [1/2], Step [47960/67476], Loss: 4.6809\n",
      "Epoch [1/2], Step [47970/67476], Loss: 4.5331\n",
      "Epoch [1/2], Step [47980/67476], Loss: 4.4750\n",
      "Epoch [1/2], Step [47990/67476], Loss: 4.6557\n",
      "Epoch [1/2], Step [48000/67476], Loss: 4.3066\n",
      "Epoch [1/2], Step [48010/67476], Loss: 4.3859\n",
      "Epoch [1/2], Step [48020/67476], Loss: 4.4427\n",
      "Epoch [1/2], Step [48030/67476], Loss: 4.4397\n",
      "Epoch [1/2], Step [48040/67476], Loss: 4.5880\n",
      "Epoch [1/2], Step [48050/67476], Loss: 4.4148\n",
      "Epoch [1/2], Step [48060/67476], Loss: 4.7041\n",
      "Epoch [1/2], Step [48070/67476], Loss: 4.3710\n",
      "Epoch [1/2], Step [48080/67476], Loss: 4.3495\n",
      "Epoch [1/2], Step [48090/67476], Loss: 4.7057\n",
      "Epoch [1/2], Step [48100/67476], Loss: 4.9557\n",
      "Epoch [1/2], Step [48110/67476], Loss: 4.6798\n",
      "Epoch [1/2], Step [48120/67476], Loss: 4.4147\n",
      "Epoch [1/2], Step [48130/67476], Loss: 4.6323\n",
      "Epoch [1/2], Step [48140/67476], Loss: 4.6963\n",
      "Epoch [1/2], Step [48150/67476], Loss: 4.3131\n",
      "Epoch [1/2], Step [48160/67476], Loss: 4.5348\n",
      "Epoch [1/2], Step [48170/67476], Loss: 4.6510\n",
      "Epoch [1/2], Step [48180/67476], Loss: 4.6871\n",
      "Epoch [1/2], Step [48190/67476], Loss: 4.5403\n",
      "Epoch [1/2], Step [48200/67476], Loss: 4.6385\n",
      "Epoch [1/2], Step [48210/67476], Loss: 4.6040\n",
      "Epoch [1/2], Step [48220/67476], Loss: 4.4368\n",
      "Epoch [1/2], Step [48230/67476], Loss: 4.4962\n",
      "Epoch [1/2], Step [48240/67476], Loss: 4.8600\n",
      "Epoch [1/2], Step [48250/67476], Loss: 4.6185\n",
      "Epoch [1/2], Step [48260/67476], Loss: 4.4101\n",
      "Epoch [1/2], Step [48270/67476], Loss: 4.6416\n",
      "Epoch [1/2], Step [48280/67476], Loss: 4.7760\n",
      "Epoch [1/2], Step [48290/67476], Loss: 4.4014\n",
      "Epoch [1/2], Step [48300/67476], Loss: 4.5278\n",
      "Epoch [1/2], Step [48310/67476], Loss: 4.5361\n",
      "Epoch [1/2], Step [48320/67476], Loss: 4.4820\n",
      "Epoch [1/2], Step [48330/67476], Loss: 4.6062\n",
      "Epoch [1/2], Step [48340/67476], Loss: 4.5609\n",
      "Epoch [1/2], Step [48350/67476], Loss: 4.6256\n",
      "Epoch [1/2], Step [48360/67476], Loss: 4.5545\n",
      "Epoch [1/2], Step [48370/67476], Loss: 4.6769\n",
      "Epoch [1/2], Step [48380/67476], Loss: 4.4399\n",
      "Epoch [1/2], Step [48390/67476], Loss: 4.3511\n",
      "Epoch [1/2], Step [48400/67476], Loss: 4.6268\n",
      "Epoch [1/2], Step [48410/67476], Loss: 4.3913\n",
      "Epoch [1/2], Step [48420/67476], Loss: 4.4968\n",
      "Epoch [1/2], Step [48430/67476], Loss: 4.6339\n",
      "Epoch [1/2], Step [48440/67476], Loss: 4.6028\n",
      "Epoch [1/2], Step [48450/67476], Loss: 4.4999\n",
      "Epoch [1/2], Step [48460/67476], Loss: 4.7126\n",
      "Epoch [1/2], Step [48470/67476], Loss: 4.7298\n",
      "Epoch [1/2], Step [48480/67476], Loss: 4.6172\n",
      "Epoch [1/2], Step [48490/67476], Loss: 4.3156\n",
      "Epoch [1/2], Step [48500/67476], Loss: 4.6287\n",
      "Epoch [1/2], Step [48510/67476], Loss: 4.6032\n",
      "Epoch [1/2], Step [48520/67476], Loss: 4.5272\n",
      "Epoch [1/2], Step [48530/67476], Loss: 4.5987\n",
      "Epoch [1/2], Step [48540/67476], Loss: 4.6394\n",
      "Epoch [1/2], Step [48550/67476], Loss: 4.6089\n",
      "Epoch [1/2], Step [48560/67476], Loss: 4.3446\n",
      "Epoch [1/2], Step [48570/67476], Loss: 4.5794\n",
      "Epoch [1/2], Step [48580/67476], Loss: 4.3021\n",
      "Epoch [1/2], Step [48590/67476], Loss: 4.5969\n",
      "Epoch [1/2], Step [48600/67476], Loss: 4.6705\n",
      "Epoch [1/2], Step [48610/67476], Loss: 4.6108\n",
      "Epoch [1/2], Step [48620/67476], Loss: 4.6874\n",
      "Epoch [1/2], Step [48630/67476], Loss: 4.5846\n",
      "Epoch [1/2], Step [48640/67476], Loss: 4.4503\n",
      "Epoch [1/2], Step [48650/67476], Loss: 4.5108\n",
      "Epoch [1/2], Step [48660/67476], Loss: 4.5028\n",
      "Epoch [1/2], Step [48670/67476], Loss: 4.4600\n",
      "Epoch [1/2], Step [48680/67476], Loss: 4.6316\n",
      "Epoch [1/2], Step [48690/67476], Loss: 4.5422\n",
      "Epoch [1/2], Step [48700/67476], Loss: 4.4228\n",
      "Epoch [1/2], Step [48710/67476], Loss: 4.7699\n",
      "Epoch [1/2], Step [48720/67476], Loss: 4.5971\n",
      "Epoch [1/2], Step [48730/67476], Loss: 4.5927\n",
      "Epoch [1/2], Step [48740/67476], Loss: 4.5139\n",
      "Epoch [1/2], Step [48750/67476], Loss: 4.5909\n",
      "Epoch [1/2], Step [48760/67476], Loss: 4.6959\n",
      "Epoch [1/2], Step [48770/67476], Loss: 4.9108\n",
      "Epoch [1/2], Step [48780/67476], Loss: 4.5724\n",
      "Epoch [1/2], Step [48790/67476], Loss: 4.7488\n",
      "Epoch [1/2], Step [48800/67476], Loss: 4.5817\n",
      "Epoch [1/2], Step [48810/67476], Loss: 4.5419\n",
      "Epoch [1/2], Step [48820/67476], Loss: 4.4320\n",
      "Epoch [1/2], Step [48830/67476], Loss: 4.4726\n",
      "Epoch [1/2], Step [48840/67476], Loss: 4.6063\n",
      "Epoch [1/2], Step [48850/67476], Loss: 4.4467\n",
      "Epoch [1/2], Step [48860/67476], Loss: 4.7252\n",
      "Epoch [1/2], Step [48870/67476], Loss: 4.6029\n",
      "Epoch [1/2], Step [48880/67476], Loss: 4.4492\n",
      "Epoch [1/2], Step [48890/67476], Loss: 4.6684\n",
      "Epoch [1/2], Step [48900/67476], Loss: 4.5918\n",
      "Epoch [1/2], Step [48910/67476], Loss: 4.8729\n",
      "Epoch [1/2], Step [48920/67476], Loss: 4.7394\n",
      "Epoch [1/2], Step [48930/67476], Loss: 4.6554\n",
      "Epoch [1/2], Step [48940/67476], Loss: 4.7450\n",
      "Epoch [1/2], Step [48950/67476], Loss: 4.6121\n",
      "Epoch [1/2], Step [48960/67476], Loss: 4.6594\n",
      "Epoch [1/2], Step [48970/67476], Loss: 4.5792\n",
      "Epoch [1/2], Step [48980/67476], Loss: 4.7092\n",
      "Epoch [1/2], Step [48990/67476], Loss: 4.6136\n",
      "Epoch [1/2], Step [49000/67476], Loss: 4.6372\n",
      "Epoch [1/2], Step [49010/67476], Loss: 4.3458\n",
      "Epoch [1/2], Step [49020/67476], Loss: 4.4427\n",
      "Epoch [1/2], Step [49030/67476], Loss: 4.7766\n",
      "Epoch [1/2], Step [49040/67476], Loss: 4.6922\n",
      "Epoch [1/2], Step [49050/67476], Loss: 4.6978\n",
      "Epoch [1/2], Step [49060/67476], Loss: 4.2998\n",
      "Epoch [1/2], Step [49070/67476], Loss: 4.4375\n",
      "Epoch [1/2], Step [49080/67476], Loss: 4.6302\n",
      "Epoch [1/2], Step [49090/67476], Loss: 4.4543\n",
      "Epoch [1/2], Step [49100/67476], Loss: 4.7442\n",
      "Epoch [1/2], Step [49110/67476], Loss: 4.7502\n",
      "Epoch [1/2], Step [49120/67476], Loss: 4.7042\n",
      "Epoch [1/2], Step [49130/67476], Loss: 4.5890\n",
      "Epoch [1/2], Step [49140/67476], Loss: 4.5878\n",
      "Epoch [1/2], Step [49150/67476], Loss: 4.3799\n",
      "Epoch [1/2], Step [49160/67476], Loss: 4.4495\n",
      "Epoch [1/2], Step [49170/67476], Loss: 4.5288\n",
      "Epoch [1/2], Step [49180/67476], Loss: 4.6164\n",
      "Epoch [1/2], Step [49190/67476], Loss: 4.5410\n",
      "Epoch [1/2], Step [49200/67476], Loss: 4.5457\n",
      "Epoch [1/2], Step [49210/67476], Loss: 4.8749\n",
      "Epoch [1/2], Step [49220/67476], Loss: 4.5644\n",
      "Epoch [1/2], Step [49230/67476], Loss: 4.5792\n",
      "Epoch [1/2], Step [49240/67476], Loss: 4.3029\n",
      "Epoch [1/2], Step [49250/67476], Loss: 4.5747\n",
      "Epoch [1/2], Step [49260/67476], Loss: 4.4610\n",
      "Epoch [1/2], Step [49270/67476], Loss: 4.5480\n",
      "Epoch [1/2], Step [49280/67476], Loss: 4.6080\n",
      "Epoch [1/2], Step [49290/67476], Loss: 4.5992\n",
      "Epoch [1/2], Step [49300/67476], Loss: 4.5732\n",
      "Epoch [1/2], Step [49310/67476], Loss: 4.5618\n",
      "Epoch [1/2], Step [49320/67476], Loss: 4.5063\n",
      "Epoch [1/2], Step [49330/67476], Loss: 4.3201\n",
      "Epoch [1/2], Step [49340/67476], Loss: 4.4307\n",
      "Epoch [1/2], Step [49350/67476], Loss: 4.5283\n",
      "Epoch [1/2], Step [49360/67476], Loss: 4.5956\n",
      "Epoch [1/2], Step [49370/67476], Loss: 4.4175\n",
      "Epoch [1/2], Step [49380/67476], Loss: 4.6985\n",
      "Epoch [1/2], Step [49390/67476], Loss: 4.6439\n",
      "Epoch [1/2], Step [49400/67476], Loss: 4.4724\n",
      "Epoch [1/2], Step [49410/67476], Loss: 4.4524\n",
      "Epoch [1/2], Step [49420/67476], Loss: 4.8149\n",
      "Epoch [1/2], Step [49430/67476], Loss: 4.5684\n",
      "Epoch [1/2], Step [49440/67476], Loss: 4.6075\n",
      "Epoch [1/2], Step [49450/67476], Loss: 4.6385\n",
      "Epoch [1/2], Step [49460/67476], Loss: 4.4348\n",
      "Epoch [1/2], Step [49470/67476], Loss: 4.4426\n",
      "Epoch [1/2], Step [49480/67476], Loss: 4.6382\n",
      "Epoch [1/2], Step [49490/67476], Loss: 4.5751\n",
      "Epoch [1/2], Step [49500/67476], Loss: 4.4961\n",
      "Epoch [1/2], Step [49510/67476], Loss: 4.6390\n",
      "Epoch [1/2], Step [49520/67476], Loss: 4.5743\n",
      "Epoch [1/2], Step [49530/67476], Loss: 4.4574\n",
      "Epoch [1/2], Step [49540/67476], Loss: 4.7212\n",
      "Epoch [1/2], Step [49550/67476], Loss: 4.6293\n",
      "Epoch [1/2], Step [49560/67476], Loss: 4.5655\n",
      "Epoch [1/2], Step [49570/67476], Loss: 4.4997\n",
      "Epoch [1/2], Step [49580/67476], Loss: 4.4946\n",
      "Epoch [1/2], Step [49590/67476], Loss: 4.4931\n",
      "Epoch [1/2], Step [49600/67476], Loss: 4.4842\n",
      "Epoch [1/2], Step [49610/67476], Loss: 4.6914\n",
      "Epoch [1/2], Step [49620/67476], Loss: 4.7400\n",
      "Epoch [1/2], Step [49630/67476], Loss: 4.4822\n",
      "Epoch [1/2], Step [49640/67476], Loss: 4.7086\n",
      "Epoch [1/2], Step [49650/67476], Loss: 4.5892\n",
      "Epoch [1/2], Step [49660/67476], Loss: 4.5920\n",
      "Epoch [1/2], Step [49670/67476], Loss: 4.4446\n",
      "Epoch [1/2], Step [49680/67476], Loss: 4.3561\n",
      "Epoch [1/2], Step [49690/67476], Loss: 4.5090\n",
      "Epoch [1/2], Step [49700/67476], Loss: 4.5825\n",
      "Epoch [1/2], Step [49710/67476], Loss: 4.6231\n",
      "Epoch [1/2], Step [49720/67476], Loss: 4.6042\n",
      "Epoch [1/2], Step [49730/67476], Loss: 4.5739\n",
      "Epoch [1/2], Step [49740/67476], Loss: 4.5948\n",
      "Epoch [1/2], Step [49750/67476], Loss: 4.6769\n",
      "Epoch [1/2], Step [49760/67476], Loss: 4.6395\n",
      "Epoch [1/2], Step [49770/67476], Loss: 4.4563\n",
      "Epoch [1/2], Step [49780/67476], Loss: 4.7731\n",
      "Epoch [1/2], Step [49790/67476], Loss: 4.5426\n",
      "Epoch [1/2], Step [49800/67476], Loss: 4.5398\n",
      "Epoch [1/2], Step [49810/67476], Loss: 4.5938\n",
      "Epoch [1/2], Step [49820/67476], Loss: 4.7409\n",
      "Epoch [1/2], Step [49830/67476], Loss: 4.6867\n",
      "Epoch [1/2], Step [49840/67476], Loss: 4.4692\n",
      "Epoch [1/2], Step [49850/67476], Loss: 4.6238\n",
      "Epoch [1/2], Step [49860/67476], Loss: 4.4670\n",
      "Epoch [1/2], Step [49870/67476], Loss: 4.4049\n",
      "Epoch [1/2], Step [49880/67476], Loss: 4.6654\n",
      "Epoch [1/2], Step [49890/67476], Loss: 4.4543\n",
      "Epoch [1/2], Step [49900/67476], Loss: 4.7088\n",
      "Epoch [1/2], Step [49910/67476], Loss: 4.6417\n",
      "Epoch [1/2], Step [49920/67476], Loss: 4.4209\n",
      "Epoch [1/2], Step [49930/67476], Loss: 4.5931\n",
      "Epoch [1/2], Step [49940/67476], Loss: 4.6707\n",
      "Epoch [1/2], Step [49950/67476], Loss: 4.4966\n",
      "Epoch [1/2], Step [49960/67476], Loss: 4.5263\n",
      "Epoch [1/2], Step [49970/67476], Loss: 4.5402\n",
      "Epoch [1/2], Step [49980/67476], Loss: 4.3525\n",
      "Epoch [1/2], Step [49990/67476], Loss: 4.4977\n",
      "Epoch [1/2], Step [50000/67476], Loss: 4.3743\n",
      "Epoch [1/2], Step [50010/67476], Loss: 4.3984\n",
      "Epoch [1/2], Step [50020/67476], Loss: 4.3204\n",
      "Epoch [1/2], Step [50030/67476], Loss: 4.7562\n",
      "Epoch [1/2], Step [50040/67476], Loss: 4.4156\n",
      "Epoch [1/2], Step [50050/67476], Loss: 4.3942\n",
      "Epoch [1/2], Step [50060/67476], Loss: 4.4538\n",
      "Epoch [1/2], Step [50070/67476], Loss: 4.5489\n",
      "Epoch [1/2], Step [50080/67476], Loss: 4.5608\n",
      "Epoch [1/2], Step [50090/67476], Loss: 4.5423\n",
      "Epoch [1/2], Step [50100/67476], Loss: 4.4377\n",
      "Epoch [1/2], Step [50110/67476], Loss: 4.6757\n",
      "Epoch [1/2], Step [50120/67476], Loss: 4.4982\n",
      "Epoch [1/2], Step [50130/67476], Loss: 4.4980\n",
      "Epoch [1/2], Step [50140/67476], Loss: 4.7532\n",
      "Epoch [1/2], Step [50150/67476], Loss: 4.4568\n",
      "Epoch [1/2], Step [50160/67476], Loss: 4.6257\n",
      "Epoch [1/2], Step [50170/67476], Loss: 4.4654\n",
      "Epoch [1/2], Step [50180/67476], Loss: 4.5974\n",
      "Epoch [1/2], Step [50190/67476], Loss: 4.4317\n",
      "Epoch [1/2], Step [50200/67476], Loss: 4.5945\n",
      "Epoch [1/2], Step [50210/67476], Loss: 4.4835\n",
      "Epoch [1/2], Step [50220/67476], Loss: 4.5741\n",
      "Epoch [1/2], Step [50230/67476], Loss: 4.6836\n",
      "Epoch [1/2], Step [50240/67476], Loss: 4.6644\n",
      "Epoch [1/2], Step [50250/67476], Loss: 4.7086\n",
      "Epoch [1/2], Step [50260/67476], Loss: 4.6636\n",
      "Epoch [1/2], Step [50270/67476], Loss: 4.5486\n",
      "Epoch [1/2], Step [50280/67476], Loss: 4.3554\n",
      "Epoch [1/2], Step [50290/67476], Loss: 4.6928\n",
      "Epoch [1/2], Step [50300/67476], Loss: 4.4556\n",
      "Epoch [1/2], Step [50310/67476], Loss: 4.3580\n",
      "Epoch [1/2], Step [50320/67476], Loss: 4.7075\n",
      "Epoch [1/2], Step [50330/67476], Loss: 4.5711\n",
      "Epoch [1/2], Step [50340/67476], Loss: 4.5478\n",
      "Epoch [1/2], Step [50350/67476], Loss: 4.7059\n",
      "Epoch [1/2], Step [50360/67476], Loss: 4.6017\n",
      "Epoch [1/2], Step [50370/67476], Loss: 4.7696\n",
      "Epoch [1/2], Step [50380/67476], Loss: 4.4277\n",
      "Epoch [1/2], Step [50390/67476], Loss: 4.6424\n",
      "Epoch [1/2], Step [50400/67476], Loss: 4.5915\n",
      "Epoch [1/2], Step [50410/67476], Loss: 4.5725\n",
      "Epoch [1/2], Step [50420/67476], Loss: 4.5899\n",
      "Epoch [1/2], Step [50430/67476], Loss: 4.7178\n",
      "Epoch [1/2], Step [50440/67476], Loss: 4.6133\n",
      "Epoch [1/2], Step [50450/67476], Loss: 4.3013\n",
      "Epoch [1/2], Step [50460/67476], Loss: 4.5881\n",
      "Epoch [1/2], Step [50470/67476], Loss: 4.6605\n",
      "Epoch [1/2], Step [50480/67476], Loss: 4.5239\n",
      "Epoch [1/2], Step [50490/67476], Loss: 4.5816\n",
      "Epoch [1/2], Step [50500/67476], Loss: 4.7156\n",
      "Epoch [1/2], Step [50510/67476], Loss: 4.4215\n",
      "Epoch [1/2], Step [50520/67476], Loss: 4.4453\n",
      "Epoch [1/2], Step [50530/67476], Loss: 4.4490\n",
      "Epoch [1/2], Step [50540/67476], Loss: 4.4439\n",
      "Epoch [1/2], Step [50550/67476], Loss: 4.5804\n",
      "Epoch [1/2], Step [50560/67476], Loss: 4.5037\n",
      "Epoch [1/2], Step [50570/67476], Loss: 4.4521\n",
      "Epoch [1/2], Step [50580/67476], Loss: 4.5853\n",
      "Epoch [1/2], Step [50590/67476], Loss: 4.6601\n",
      "Epoch [1/2], Step [50600/67476], Loss: 4.4969\n",
      "Epoch [1/2], Step [50610/67476], Loss: 4.4276\n",
      "Epoch [1/2], Step [50620/67476], Loss: 4.6367\n",
      "Epoch [1/2], Step [50630/67476], Loss: 4.3491\n",
      "Epoch [1/2], Step [50640/67476], Loss: 4.4214\n",
      "Epoch [1/2], Step [50650/67476], Loss: 4.5010\n",
      "Epoch [1/2], Step [50660/67476], Loss: 4.6538\n",
      "Epoch [1/2], Step [50670/67476], Loss: 4.6183\n",
      "Epoch [1/2], Step [50680/67476], Loss: 4.6162\n",
      "Epoch [1/2], Step [50690/67476], Loss: 4.6764\n",
      "Epoch [1/2], Step [50700/67476], Loss: 4.6218\n",
      "Epoch [1/2], Step [50710/67476], Loss: 4.8089\n",
      "Epoch [1/2], Step [50720/67476], Loss: 4.6120\n",
      "Epoch [1/2], Step [50730/67476], Loss: 4.5535\n",
      "Epoch [1/2], Step [50740/67476], Loss: 4.3556\n",
      "Epoch [1/2], Step [50750/67476], Loss: 4.5133\n",
      "Epoch [1/2], Step [50760/67476], Loss: 4.3974\n",
      "Epoch [1/2], Step [50770/67476], Loss: 4.6000\n",
      "Epoch [1/2], Step [50780/67476], Loss: 4.5162\n",
      "Epoch [1/2], Step [50790/67476], Loss: 4.5399\n",
      "Epoch [1/2], Step [50800/67476], Loss: 4.6719\n",
      "Epoch [1/2], Step [50810/67476], Loss: 4.6205\n",
      "Epoch [1/2], Step [50820/67476], Loss: 4.4280\n",
      "Epoch [1/2], Step [50830/67476], Loss: 4.5307\n",
      "Epoch [1/2], Step [50840/67476], Loss: 4.4520\n",
      "Epoch [1/2], Step [50850/67476], Loss: 4.5082\n",
      "Epoch [1/2], Step [50860/67476], Loss: 4.8658\n",
      "Epoch [1/2], Step [50870/67476], Loss: 4.4486\n",
      "Epoch [1/2], Step [50880/67476], Loss: 4.5162\n",
      "Epoch [1/2], Step [50890/67476], Loss: 4.6662\n",
      "Epoch [1/2], Step [50900/67476], Loss: 4.4025\n",
      "Epoch [1/2], Step [50910/67476], Loss: 4.5637\n",
      "Epoch [1/2], Step [50920/67476], Loss: 4.6280\n",
      "Epoch [1/2], Step [50930/67476], Loss: 4.6033\n",
      "Epoch [1/2], Step [50940/67476], Loss: 4.7410\n",
      "Epoch [1/2], Step [50950/67476], Loss: 4.7737\n",
      "Epoch [1/2], Step [50960/67476], Loss: 4.5207\n",
      "Epoch [1/2], Step [50970/67476], Loss: 4.5472\n",
      "Epoch [1/2], Step [50980/67476], Loss: 4.6447\n",
      "Epoch [1/2], Step [50990/67476], Loss: 4.5328\n",
      "Epoch [1/2], Step [51000/67476], Loss: 4.7115\n",
      "Epoch [1/2], Step [51010/67476], Loss: 4.4950\n",
      "Epoch [1/2], Step [51020/67476], Loss: 4.5827\n",
      "Epoch [1/2], Step [51030/67476], Loss: 4.4686\n",
      "Epoch [1/2], Step [51040/67476], Loss: 4.4771\n",
      "Epoch [1/2], Step [51050/67476], Loss: 4.4329\n",
      "Epoch [1/2], Step [51060/67476], Loss: 4.5016\n",
      "Epoch [1/2], Step [51070/67476], Loss: 4.7074\n",
      "Epoch [1/2], Step [51080/67476], Loss: 4.6516\n",
      "Epoch [1/2], Step [51090/67476], Loss: 4.6235\n",
      "Epoch [1/2], Step [51100/67476], Loss: 4.6082\n",
      "Epoch [1/2], Step [51110/67476], Loss: 4.3728\n",
      "Epoch [1/2], Step [51120/67476], Loss: 4.5733\n",
      "Epoch [1/2], Step [51130/67476], Loss: 4.3240\n",
      "Epoch [1/2], Step [51140/67476], Loss: 4.4746\n",
      "Epoch [1/2], Step [51150/67476], Loss: 4.5731\n",
      "Epoch [1/2], Step [51160/67476], Loss: 4.3221\n",
      "Epoch [1/2], Step [51170/67476], Loss: 4.5631\n",
      "Epoch [1/2], Step [51180/67476], Loss: 4.5280\n",
      "Epoch [1/2], Step [51190/67476], Loss: 4.5471\n",
      "Epoch [1/2], Step [51200/67476], Loss: 4.6612\n",
      "Epoch [1/2], Step [51210/67476], Loss: 4.4831\n",
      "Epoch [1/2], Step [51220/67476], Loss: 4.5481\n",
      "Epoch [1/2], Step [51230/67476], Loss: 4.5874\n",
      "Epoch [1/2], Step [51240/67476], Loss: 4.6420\n",
      "Epoch [1/2], Step [51250/67476], Loss: 4.4192\n",
      "Epoch [1/2], Step [51260/67476], Loss: 4.5373\n",
      "Epoch [1/2], Step [51270/67476], Loss: 4.7275\n",
      "Epoch [1/2], Step [51280/67476], Loss: 4.5416\n",
      "Epoch [1/2], Step [51290/67476], Loss: 4.6221\n",
      "Epoch [1/2], Step [51300/67476], Loss: 4.8720\n",
      "Epoch [1/2], Step [51310/67476], Loss: 4.4726\n",
      "Epoch [1/2], Step [51320/67476], Loss: 4.6278\n",
      "Epoch [1/2], Step [51330/67476], Loss: 4.7108\n",
      "Epoch [1/2], Step [51340/67476], Loss: 4.6283\n",
      "Epoch [1/2], Step [51350/67476], Loss: 4.6211\n",
      "Epoch [1/2], Step [51360/67476], Loss: 4.7441\n",
      "Epoch [1/2], Step [51370/67476], Loss: 4.6985\n",
      "Epoch [1/2], Step [51380/67476], Loss: 4.5756\n",
      "Epoch [1/2], Step [51390/67476], Loss: 4.5047\n",
      "Epoch [1/2], Step [51400/67476], Loss: 4.6706\n",
      "Epoch [1/2], Step [51410/67476], Loss: 4.5648\n",
      "Epoch [1/2], Step [51420/67476], Loss: 4.6768\n",
      "Epoch [1/2], Step [51430/67476], Loss: 4.7408\n",
      "Epoch [1/2], Step [51440/67476], Loss: 4.5325\n",
      "Epoch [1/2], Step [51450/67476], Loss: 4.7252\n",
      "Epoch [1/2], Step [51460/67476], Loss: 4.5892\n",
      "Epoch [1/2], Step [51470/67476], Loss: 4.5228\n",
      "Epoch [1/2], Step [51480/67476], Loss: 4.8181\n",
      "Epoch [1/2], Step [51490/67476], Loss: 4.7100\n",
      "Epoch [1/2], Step [51500/67476], Loss: 4.4525\n",
      "Epoch [1/2], Step [51510/67476], Loss: 4.4152\n",
      "Epoch [1/2], Step [51520/67476], Loss: 4.6492\n",
      "Epoch [1/2], Step [51530/67476], Loss: 4.8060\n",
      "Epoch [1/2], Step [51540/67476], Loss: 4.5083\n",
      "Epoch [1/2], Step [51550/67476], Loss: 4.5798\n",
      "Epoch [1/2], Step [51560/67476], Loss: 4.9064\n",
      "Epoch [1/2], Step [51570/67476], Loss: 4.5878\n",
      "Epoch [1/2], Step [51580/67476], Loss: 4.3869\n",
      "Epoch [1/2], Step [51590/67476], Loss: 4.6592\n",
      "Epoch [1/2], Step [51600/67476], Loss: 4.6151\n",
      "Epoch [1/2], Step [51610/67476], Loss: 4.5473\n",
      "Epoch [1/2], Step [51620/67476], Loss: 4.5411\n",
      "Epoch [1/2], Step [51630/67476], Loss: 4.5336\n",
      "Epoch [1/2], Step [51640/67476], Loss: 4.7778\n",
      "Epoch [1/2], Step [51650/67476], Loss: 4.6508\n",
      "Epoch [1/2], Step [51660/67476], Loss: 4.5567\n",
      "Epoch [1/2], Step [51670/67476], Loss: 4.5657\n",
      "Epoch [1/2], Step [51680/67476], Loss: 4.6384\n",
      "Epoch [1/2], Step [51690/67476], Loss: 4.7015\n",
      "Epoch [1/2], Step [51700/67476], Loss: 4.6285\n",
      "Epoch [1/2], Step [51710/67476], Loss: 4.7572\n",
      "Epoch [1/2], Step [51720/67476], Loss: 4.4471\n",
      "Epoch [1/2], Step [51730/67476], Loss: 4.5715\n",
      "Epoch [1/2], Step [51740/67476], Loss: 4.5960\n",
      "Epoch [1/2], Step [51750/67476], Loss: 4.4820\n",
      "Epoch [1/2], Step [51760/67476], Loss: 4.4841\n",
      "Epoch [1/2], Step [51770/67476], Loss: 4.5331\n",
      "Epoch [1/2], Step [51780/67476], Loss: 4.7440\n",
      "Epoch [1/2], Step [51790/67476], Loss: 4.5754\n",
      "Epoch [1/2], Step [51800/67476], Loss: 4.4717\n",
      "Epoch [1/2], Step [51810/67476], Loss: 4.8202\n",
      "Epoch [1/2], Step [51820/67476], Loss: 4.6773\n",
      "Epoch [1/2], Step [51830/67476], Loss: 4.4684\n",
      "Epoch [1/2], Step [51840/67476], Loss: 4.5064\n",
      "Epoch [1/2], Step [51850/67476], Loss: 4.6296\n",
      "Epoch [1/2], Step [51860/67476], Loss: 4.3244\n",
      "Epoch [1/2], Step [51870/67476], Loss: 4.6785\n",
      "Epoch [1/2], Step [51880/67476], Loss: 4.5533\n",
      "Epoch [1/2], Step [51890/67476], Loss: 4.7351\n",
      "Epoch [1/2], Step [51900/67476], Loss: 4.6176\n",
      "Epoch [1/2], Step [51910/67476], Loss: 4.4428\n",
      "Epoch [1/2], Step [51920/67476], Loss: 4.5862\n",
      "Epoch [1/2], Step [51930/67476], Loss: 4.4256\n",
      "Epoch [1/2], Step [51940/67476], Loss: 4.5744\n",
      "Epoch [1/2], Step [51950/67476], Loss: 4.6468\n",
      "Epoch [1/2], Step [51960/67476], Loss: 4.5315\n",
      "Epoch [1/2], Step [51970/67476], Loss: 4.5548\n",
      "Epoch [1/2], Step [51980/67476], Loss: 4.7337\n",
      "Epoch [1/2], Step [51990/67476], Loss: 4.4441\n",
      "Epoch [1/2], Step [52000/67476], Loss: 4.2507\n",
      "Epoch [1/2], Step [52010/67476], Loss: 4.5001\n",
      "Epoch [1/2], Step [52020/67476], Loss: 4.5659\n",
      "Epoch [1/2], Step [52030/67476], Loss: 4.4519\n",
      "Epoch [1/2], Step [52040/67476], Loss: 4.5490\n",
      "Epoch [1/2], Step [52050/67476], Loss: 4.5158\n",
      "Epoch [1/2], Step [52060/67476], Loss: 4.7275\n",
      "Epoch [1/2], Step [52070/67476], Loss: 4.6631\n",
      "Epoch [1/2], Step [52080/67476], Loss: 4.7394\n",
      "Epoch [1/2], Step [52090/67476], Loss: 4.4948\n",
      "Epoch [1/2], Step [52100/67476], Loss: 4.6214\n",
      "Epoch [1/2], Step [52110/67476], Loss: 4.6015\n",
      "Epoch [1/2], Step [52120/67476], Loss: 4.5484\n",
      "Epoch [1/2], Step [52130/67476], Loss: 4.4536\n",
      "Epoch [1/2], Step [52140/67476], Loss: 4.5878\n",
      "Epoch [1/2], Step [52150/67476], Loss: 4.4643\n",
      "Epoch [1/2], Step [52160/67476], Loss: 4.5197\n",
      "Epoch [1/2], Step [52170/67476], Loss: 4.5532\n",
      "Epoch [1/2], Step [52180/67476], Loss: 4.5657\n",
      "Epoch [1/2], Step [52190/67476], Loss: 4.7046\n",
      "Epoch [1/2], Step [52200/67476], Loss: 4.5965\n",
      "Epoch [1/2], Step [52210/67476], Loss: 4.6256\n",
      "Epoch [1/2], Step [52220/67476], Loss: 4.3716\n",
      "Epoch [1/2], Step [52230/67476], Loss: 4.5995\n",
      "Epoch [1/2], Step [52240/67476], Loss: 4.6293\n",
      "Epoch [1/2], Step [52250/67476], Loss: 4.3687\n",
      "Epoch [1/2], Step [52260/67476], Loss: 4.4386\n",
      "Epoch [1/2], Step [52270/67476], Loss: 4.5515\n",
      "Epoch [1/2], Step [52280/67476], Loss: 4.6114\n",
      "Epoch [1/2], Step [52290/67476], Loss: 4.5813\n",
      "Epoch [1/2], Step [52300/67476], Loss: 4.5502\n",
      "Epoch [1/2], Step [52310/67476], Loss: 4.3901\n",
      "Epoch [1/2], Step [52320/67476], Loss: 4.3991\n",
      "Epoch [1/2], Step [52330/67476], Loss: 4.8691\n",
      "Epoch [1/2], Step [52340/67476], Loss: 4.5502\n",
      "Epoch [1/2], Step [52350/67476], Loss: 4.5277\n",
      "Epoch [1/2], Step [52360/67476], Loss: 4.5504\n",
      "Epoch [1/2], Step [52370/67476], Loss: 4.6327\n",
      "Epoch [1/2], Step [52380/67476], Loss: 4.7158\n",
      "Epoch [1/2], Step [52390/67476], Loss: 4.4883\n",
      "Epoch [1/2], Step [52400/67476], Loss: 4.5829\n",
      "Epoch [1/2], Step [52410/67476], Loss: 4.7665\n",
      "Epoch [1/2], Step [52420/67476], Loss: 4.6076\n",
      "Epoch [1/2], Step [52430/67476], Loss: 4.4373\n",
      "Epoch [1/2], Step [52440/67476], Loss: 4.3664\n",
      "Epoch [1/2], Step [52450/67476], Loss: 4.8093\n",
      "Epoch [1/2], Step [52460/67476], Loss: 4.6572\n",
      "Epoch [1/2], Step [52470/67476], Loss: 4.7109\n",
      "Epoch [1/2], Step [52480/67476], Loss: 4.5154\n",
      "Epoch [1/2], Step [52490/67476], Loss: 4.7256\n",
      "Epoch [1/2], Step [52500/67476], Loss: 4.6568\n",
      "Epoch [1/2], Step [52510/67476], Loss: 4.5154\n",
      "Epoch [1/2], Step [52520/67476], Loss: 4.7667\n",
      "Epoch [1/2], Step [52530/67476], Loss: 4.5655\n",
      "Epoch [1/2], Step [52540/67476], Loss: 4.5016\n",
      "Epoch [1/2], Step [52550/67476], Loss: 4.4538\n",
      "Epoch [1/2], Step [52560/67476], Loss: 4.6380\n",
      "Epoch [1/2], Step [52570/67476], Loss: 4.3198\n",
      "Epoch [1/2], Step [52580/67476], Loss: 4.3911\n",
      "Epoch [1/2], Step [52590/67476], Loss: 4.7156\n",
      "Epoch [1/2], Step [52600/67476], Loss: 4.6406\n",
      "Epoch [1/2], Step [52610/67476], Loss: 4.3987\n",
      "Epoch [1/2], Step [52620/67476], Loss: 4.5721\n",
      "Epoch [1/2], Step [52630/67476], Loss: 4.6723\n",
      "Epoch [1/2], Step [52640/67476], Loss: 4.6293\n",
      "Epoch [1/2], Step [52650/67476], Loss: 4.7726\n",
      "Epoch [1/2], Step [52660/67476], Loss: 4.5463\n",
      "Epoch [1/2], Step [52670/67476], Loss: 4.6536\n",
      "Epoch [1/2], Step [52680/67476], Loss: 4.7171\n",
      "Epoch [1/2], Step [52690/67476], Loss: 4.5156\n",
      "Epoch [1/2], Step [52700/67476], Loss: 4.6168\n",
      "Epoch [1/2], Step [52710/67476], Loss: 4.3445\n",
      "Epoch [1/2], Step [52720/67476], Loss: 4.8004\n",
      "Epoch [1/2], Step [52730/67476], Loss: 4.5974\n",
      "Epoch [1/2], Step [52740/67476], Loss: 4.5559\n",
      "Epoch [1/2], Step [52750/67476], Loss: 4.6091\n",
      "Epoch [1/2], Step [52760/67476], Loss: 4.5085\n",
      "Epoch [1/2], Step [52770/67476], Loss: 4.7139\n",
      "Epoch [1/2], Step [52780/67476], Loss: 4.4365\n",
      "Epoch [1/2], Step [52790/67476], Loss: 4.6655\n",
      "Epoch [1/2], Step [52800/67476], Loss: 4.5339\n",
      "Epoch [1/2], Step [52810/67476], Loss: 4.7573\n",
      "Epoch [1/2], Step [52820/67476], Loss: 4.7276\n",
      "Epoch [1/2], Step [52830/67476], Loss: 4.4692\n",
      "Epoch [1/2], Step [52840/67476], Loss: 4.6799\n",
      "Epoch [1/2], Step [52850/67476], Loss: 4.5738\n",
      "Epoch [1/2], Step [52860/67476], Loss: 4.5352\n",
      "Epoch [1/2], Step [52870/67476], Loss: 4.4866\n",
      "Epoch [1/2], Step [52880/67476], Loss: 4.6052\n",
      "Epoch [1/2], Step [52890/67476], Loss: 4.5706\n",
      "Epoch [1/2], Step [52900/67476], Loss: 4.6973\n",
      "Epoch [1/2], Step [52910/67476], Loss: 4.2032\n",
      "Epoch [1/2], Step [52920/67476], Loss: 4.5509\n",
      "Epoch [1/2], Step [52930/67476], Loss: 4.6842\n",
      "Epoch [1/2], Step [52940/67476], Loss: 4.5057\n",
      "Epoch [1/2], Step [52950/67476], Loss: 4.4882\n",
      "Epoch [1/2], Step [52960/67476], Loss: 4.5168\n",
      "Epoch [1/2], Step [52970/67476], Loss: 4.4677\n",
      "Epoch [1/2], Step [52980/67476], Loss: 4.5430\n",
      "Epoch [1/2], Step [52990/67476], Loss: 4.4151\n",
      "Epoch [1/2], Step [53000/67476], Loss: 4.4659\n",
      "Epoch [1/2], Step [53010/67476], Loss: 4.7344\n",
      "Epoch [1/2], Step [53020/67476], Loss: 4.5327\n",
      "Epoch [1/2], Step [53030/67476], Loss: 4.6057\n",
      "Epoch [1/2], Step [53040/67476], Loss: 4.5381\n",
      "Epoch [1/2], Step [53050/67476], Loss: 4.5260\n",
      "Epoch [1/2], Step [53060/67476], Loss: 4.4835\n",
      "Epoch [1/2], Step [53070/67476], Loss: 4.4594\n",
      "Epoch [1/2], Step [53080/67476], Loss: 4.6064\n",
      "Epoch [1/2], Step [53090/67476], Loss: 4.5103\n",
      "Epoch [1/2], Step [53100/67476], Loss: 4.3499\n",
      "Epoch [1/2], Step [53110/67476], Loss: 4.6146\n",
      "Epoch [1/2], Step [53120/67476], Loss: 4.7476\n",
      "Epoch [1/2], Step [53130/67476], Loss: 4.4286\n",
      "Epoch [1/2], Step [53140/67476], Loss: 4.7290\n",
      "Epoch [1/2], Step [53150/67476], Loss: 4.5385\n",
      "Epoch [1/2], Step [53160/67476], Loss: 4.6098\n",
      "Epoch [1/2], Step [53170/67476], Loss: 4.6248\n",
      "Epoch [1/2], Step [53180/67476], Loss: 4.4779\n",
      "Epoch [1/2], Step [53190/67476], Loss: 4.6351\n",
      "Epoch [1/2], Step [53200/67476], Loss: 4.5785\n",
      "Epoch [1/2], Step [53210/67476], Loss: 4.7138\n",
      "Epoch [1/2], Step [53220/67476], Loss: 4.6776\n",
      "Epoch [1/2], Step [53230/67476], Loss: 4.3926\n",
      "Epoch [1/2], Step [53240/67476], Loss: 4.6721\n",
      "Epoch [1/2], Step [53250/67476], Loss: 4.4727\n",
      "Epoch [1/2], Step [53260/67476], Loss: 4.3436\n",
      "Epoch [1/2], Step [53270/67476], Loss: 4.5969\n",
      "Epoch [1/2], Step [53280/67476], Loss: 4.6415\n",
      "Epoch [1/2], Step [53290/67476], Loss: 4.6619\n",
      "Epoch [1/2], Step [53300/67476], Loss: 4.4100\n",
      "Epoch [1/2], Step [53310/67476], Loss: 4.5536\n",
      "Epoch [1/2], Step [53320/67476], Loss: 4.4893\n",
      "Epoch [1/2], Step [53330/67476], Loss: 4.3593\n",
      "Epoch [1/2], Step [53340/67476], Loss: 4.5686\n",
      "Epoch [1/2], Step [53350/67476], Loss: 4.6131\n",
      "Epoch [1/2], Step [53360/67476], Loss: 4.4760\n",
      "Epoch [1/2], Step [53370/67476], Loss: 4.5232\n",
      "Epoch [1/2], Step [53380/67476], Loss: 4.6461\n",
      "Epoch [1/2], Step [53390/67476], Loss: 4.3993\n",
      "Epoch [1/2], Step [53400/67476], Loss: 4.7125\n",
      "Epoch [1/2], Step [53410/67476], Loss: 4.6767\n",
      "Epoch [1/2], Step [53420/67476], Loss: 4.5978\n",
      "Epoch [1/2], Step [53430/67476], Loss: 4.5132\n",
      "Epoch [1/2], Step [53440/67476], Loss: 4.4107\n",
      "Epoch [1/2], Step [53450/67476], Loss: 4.7617\n",
      "Epoch [1/2], Step [53460/67476], Loss: 4.4987\n",
      "Epoch [1/2], Step [53470/67476], Loss: 4.5528\n",
      "Epoch [1/2], Step [53480/67476], Loss: 4.3005\n",
      "Epoch [1/2], Step [53490/67476], Loss: 4.7663\n",
      "Epoch [1/2], Step [53500/67476], Loss: 4.4807\n",
      "Epoch [1/2], Step [53510/67476], Loss: 4.4651\n",
      "Epoch [1/2], Step [53520/67476], Loss: 4.4083\n",
      "Epoch [1/2], Step [53530/67476], Loss: 4.5692\n",
      "Epoch [1/2], Step [53540/67476], Loss: 4.5742\n",
      "Epoch [1/2], Step [53550/67476], Loss: 4.5059\n",
      "Epoch [1/2], Step [53560/67476], Loss: 4.6301\n",
      "Epoch [1/2], Step [53570/67476], Loss: 4.5221\n",
      "Epoch [1/2], Step [53580/67476], Loss: 4.4054\n",
      "Epoch [1/2], Step [53590/67476], Loss: 4.6242\n",
      "Epoch [1/2], Step [53600/67476], Loss: 4.5172\n",
      "Epoch [1/2], Step [53610/67476], Loss: 4.4057\n",
      "Epoch [1/2], Step [53620/67476], Loss: 4.6595\n",
      "Epoch [1/2], Step [53630/67476], Loss: 4.7026\n",
      "Epoch [1/2], Step [53640/67476], Loss: 4.8114\n",
      "Epoch [1/2], Step [53650/67476], Loss: 4.7232\n",
      "Epoch [1/2], Step [53660/67476], Loss: 4.5001\n",
      "Epoch [1/2], Step [53670/67476], Loss: 4.6933\n",
      "Epoch [1/2], Step [53680/67476], Loss: 4.5118\n",
      "Epoch [1/2], Step [53690/67476], Loss: 4.4491\n",
      "Epoch [1/2], Step [53700/67476], Loss: 4.6480\n",
      "Epoch [1/2], Step [53710/67476], Loss: 4.3619\n",
      "Epoch [1/2], Step [53720/67476], Loss: 4.4619\n",
      "Epoch [1/2], Step [53730/67476], Loss: 4.6113\n",
      "Epoch [1/2], Step [53740/67476], Loss: 4.6137\n",
      "Epoch [1/2], Step [53750/67476], Loss: 4.5103\n",
      "Epoch [1/2], Step [53760/67476], Loss: 4.6665\n",
      "Epoch [1/2], Step [53770/67476], Loss: 4.3465\n",
      "Epoch [1/2], Step [53780/67476], Loss: 4.6674\n",
      "Epoch [1/2], Step [53790/67476], Loss: 4.4141\n",
      "Epoch [1/2], Step [53800/67476], Loss: 4.6207\n",
      "Epoch [1/2], Step [53810/67476], Loss: 4.6585\n",
      "Epoch [1/2], Step [53820/67476], Loss: 4.6883\n",
      "Epoch [1/2], Step [53830/67476], Loss: 4.5940\n",
      "Epoch [1/2], Step [53840/67476], Loss: 4.5677\n",
      "Epoch [1/2], Step [53850/67476], Loss: 4.4515\n",
      "Epoch [1/2], Step [53860/67476], Loss: 4.6980\n",
      "Epoch [1/2], Step [53870/67476], Loss: 4.8205\n",
      "Epoch [1/2], Step [53880/67476], Loss: 4.6963\n",
      "Epoch [1/2], Step [53890/67476], Loss: 4.4310\n",
      "Epoch [1/2], Step [53900/67476], Loss: 4.6773\n",
      "Epoch [1/2], Step [53910/67476], Loss: 4.4936\n",
      "Epoch [1/2], Step [53920/67476], Loss: 4.4217\n",
      "Epoch [1/2], Step [53930/67476], Loss: 4.5349\n",
      "Epoch [1/2], Step [53940/67476], Loss: 4.6030\n",
      "Epoch [1/2], Step [53950/67476], Loss: 4.5156\n",
      "Epoch [1/2], Step [53960/67476], Loss: 4.5244\n",
      "Epoch [1/2], Step [53970/67476], Loss: 4.6609\n",
      "Epoch [1/2], Step [53980/67476], Loss: 4.4870\n",
      "Epoch [1/2], Step [53990/67476], Loss: 4.6337\n",
      "Epoch [1/2], Step [54000/67476], Loss: 4.5139\n",
      "Epoch [1/2], Step [54010/67476], Loss: 4.4460\n",
      "Epoch [1/2], Step [54020/67476], Loss: 4.8162\n",
      "Epoch [1/2], Step [54030/67476], Loss: 4.5288\n",
      "Epoch [1/2], Step [54040/67476], Loss: 4.5141\n",
      "Epoch [1/2], Step [54050/67476], Loss: 4.6296\n",
      "Epoch [1/2], Step [54060/67476], Loss: 4.4408\n",
      "Epoch [1/2], Step [54070/67476], Loss: 4.5849\n",
      "Epoch [1/2], Step [54080/67476], Loss: 4.4124\n",
      "Epoch [1/2], Step [54090/67476], Loss: 4.3750\n",
      "Epoch [1/2], Step [54100/67476], Loss: 4.4388\n",
      "Epoch [1/2], Step [54110/67476], Loss: 4.6797\n",
      "Epoch [1/2], Step [54120/67476], Loss: 4.3712\n",
      "Epoch [1/2], Step [54130/67476], Loss: 4.5496\n",
      "Epoch [1/2], Step [54140/67476], Loss: 4.5960\n",
      "Epoch [1/2], Step [54150/67476], Loss: 4.4444\n",
      "Epoch [1/2], Step [54160/67476], Loss: 4.5690\n",
      "Epoch [1/2], Step [54170/67476], Loss: 4.4332\n",
      "Epoch [1/2], Step [54180/67476], Loss: 4.5793\n",
      "Epoch [1/2], Step [54190/67476], Loss: 4.4023\n",
      "Epoch [1/2], Step [54200/67476], Loss: 4.6508\n",
      "Epoch [1/2], Step [54210/67476], Loss: 4.6531\n",
      "Epoch [1/2], Step [54220/67476], Loss: 4.6126\n",
      "Epoch [1/2], Step [54230/67476], Loss: 4.5729\n",
      "Epoch [1/2], Step [54240/67476], Loss: 4.3608\n",
      "Epoch [1/2], Step [54250/67476], Loss: 4.6686\n",
      "Epoch [1/2], Step [54260/67476], Loss: 4.4960\n",
      "Epoch [1/2], Step [54270/67476], Loss: 4.4483\n",
      "Epoch [1/2], Step [54280/67476], Loss: 4.4305\n",
      "Epoch [1/2], Step [54290/67476], Loss: 4.5376\n",
      "Epoch [1/2], Step [54300/67476], Loss: 4.6574\n",
      "Epoch [1/2], Step [54310/67476], Loss: 4.5750\n",
      "Epoch [1/2], Step [54320/67476], Loss: 4.7259\n",
      "Epoch [1/2], Step [54330/67476], Loss: 4.7110\n",
      "Epoch [1/2], Step [54340/67476], Loss: 4.4854\n",
      "Epoch [1/2], Step [54350/67476], Loss: 4.5732\n",
      "Epoch [1/2], Step [54360/67476], Loss: 4.6574\n",
      "Epoch [1/2], Step [54370/67476], Loss: 4.8398\n",
      "Epoch [1/2], Step [54380/67476], Loss: 4.5909\n",
      "Epoch [1/2], Step [54390/67476], Loss: 4.5082\n",
      "Epoch [1/2], Step [54400/67476], Loss: 4.7540\n",
      "Epoch [1/2], Step [54410/67476], Loss: 4.8070\n",
      "Epoch [1/2], Step [54420/67476], Loss: 4.5477\n",
      "Epoch [1/2], Step [54430/67476], Loss: 4.5840\n",
      "Epoch [1/2], Step [54440/67476], Loss: 4.5540\n",
      "Epoch [1/2], Step [54450/67476], Loss: 4.5454\n",
      "Epoch [1/2], Step [54460/67476], Loss: 4.5804\n",
      "Epoch [1/2], Step [54470/67476], Loss: 4.6057\n",
      "Epoch [1/2], Step [54480/67476], Loss: 4.3606\n",
      "Epoch [1/2], Step [54490/67476], Loss: 4.7067\n",
      "Epoch [1/2], Step [54500/67476], Loss: 4.5502\n",
      "Epoch [1/2], Step [54510/67476], Loss: 4.5817\n",
      "Epoch [1/2], Step [54520/67476], Loss: 4.6686\n",
      "Epoch [1/2], Step [54530/67476], Loss: 4.6505\n",
      "Epoch [1/2], Step [54540/67476], Loss: 4.5147\n",
      "Epoch [1/2], Step [54550/67476], Loss: 4.4074\n",
      "Epoch [1/2], Step [54560/67476], Loss: 4.6381\n",
      "Epoch [1/2], Step [54570/67476], Loss: 4.4585\n",
      "Epoch [1/2], Step [54580/67476], Loss: 4.6161\n",
      "Epoch [1/2], Step [54590/67476], Loss: 4.5365\n",
      "Epoch [1/2], Step [54600/67476], Loss: 4.5720\n",
      "Epoch [1/2], Step [54610/67476], Loss: 4.5607\n",
      "Epoch [1/2], Step [54620/67476], Loss: 4.7475\n",
      "Epoch [1/2], Step [54630/67476], Loss: 4.4098\n",
      "Epoch [1/2], Step [54640/67476], Loss: 4.4937\n",
      "Epoch [1/2], Step [54650/67476], Loss: 4.4323\n",
      "Epoch [1/2], Step [54660/67476], Loss: 4.5794\n",
      "Epoch [1/2], Step [54670/67476], Loss: 4.5529\n",
      "Epoch [1/2], Step [54680/67476], Loss: 4.6539\n",
      "Epoch [1/2], Step [54690/67476], Loss: 4.4367\n",
      "Epoch [1/2], Step [54700/67476], Loss: 4.5531\n",
      "Epoch [1/2], Step [54710/67476], Loss: 4.6175\n",
      "Epoch [1/2], Step [54720/67476], Loss: 4.6703\n",
      "Epoch [1/2], Step [54730/67476], Loss: 4.4711\n",
      "Epoch [1/2], Step [54740/67476], Loss: 4.5682\n",
      "Epoch [1/2], Step [54750/67476], Loss: 4.5529\n",
      "Epoch [1/2], Step [54760/67476], Loss: 4.3955\n",
      "Epoch [1/2], Step [54770/67476], Loss: 4.5317\n",
      "Epoch [1/2], Step [54780/67476], Loss: 4.5224\n",
      "Epoch [1/2], Step [54790/67476], Loss: 4.4883\n",
      "Epoch [1/2], Step [54800/67476], Loss: 4.7761\n",
      "Epoch [1/2], Step [54810/67476], Loss: 4.7466\n",
      "Epoch [1/2], Step [54820/67476], Loss: 4.3261\n",
      "Epoch [1/2], Step [54830/67476], Loss: 4.5632\n",
      "Epoch [1/2], Step [54840/67476], Loss: 4.6339\n",
      "Epoch [1/2], Step [54850/67476], Loss: 4.8536\n",
      "Epoch [1/2], Step [54860/67476], Loss: 4.8190\n",
      "Epoch [1/2], Step [54870/67476], Loss: 4.5144\n",
      "Epoch [1/2], Step [54880/67476], Loss: 4.7008\n",
      "Epoch [1/2], Step [54890/67476], Loss: 4.5152\n",
      "Epoch [1/2], Step [54900/67476], Loss: 4.8248\n",
      "Epoch [1/2], Step [54910/67476], Loss: 4.3966\n",
      "Epoch [1/2], Step [54920/67476], Loss: 4.5901\n",
      "Epoch [1/2], Step [54930/67476], Loss: 4.6342\n",
      "Epoch [1/2], Step [54940/67476], Loss: 4.4827\n",
      "Epoch [1/2], Step [54950/67476], Loss: 4.5927\n",
      "Epoch [1/2], Step [54960/67476], Loss: 4.4710\n",
      "Epoch [1/2], Step [54970/67476], Loss: 4.6972\n",
      "Epoch [1/2], Step [54980/67476], Loss: 4.7496\n",
      "Epoch [1/2], Step [54990/67476], Loss: 4.7595\n",
      "Epoch [1/2], Step [55000/67476], Loss: 4.4752\n",
      "Epoch [1/2], Step [55010/67476], Loss: 4.6014\n",
      "Epoch [1/2], Step [55020/67476], Loss: 4.7216\n",
      "Epoch [1/2], Step [55030/67476], Loss: 4.6294\n",
      "Epoch [1/2], Step [55040/67476], Loss: 4.5726\n",
      "Epoch [1/2], Step [55050/67476], Loss: 4.4996\n",
      "Epoch [1/2], Step [55060/67476], Loss: 4.4365\n",
      "Epoch [1/2], Step [55070/67476], Loss: 4.5174\n",
      "Epoch [1/2], Step [55080/67476], Loss: 4.5395\n",
      "Epoch [1/2], Step [55090/67476], Loss: 4.7289\n",
      "Epoch [1/2], Step [55100/67476], Loss: 4.5677\n",
      "Epoch [1/2], Step [55110/67476], Loss: 4.6231\n",
      "Epoch [1/2], Step [55120/67476], Loss: 4.2288\n",
      "Epoch [1/2], Step [55130/67476], Loss: 4.5182\n",
      "Epoch [1/2], Step [55140/67476], Loss: 4.7423\n",
      "Epoch [1/2], Step [55150/67476], Loss: 4.6930\n",
      "Epoch [1/2], Step [55160/67476], Loss: 4.6888\n",
      "Epoch [1/2], Step [55170/67476], Loss: 4.6768\n",
      "Epoch [1/2], Step [55180/67476], Loss: 4.5867\n",
      "Epoch [1/2], Step [55190/67476], Loss: 4.5214\n",
      "Epoch [1/2], Step [55200/67476], Loss: 4.5336\n",
      "Epoch [1/2], Step [55210/67476], Loss: 4.5168\n",
      "Epoch [1/2], Step [55220/67476], Loss: 4.7318\n",
      "Epoch [1/2], Step [55230/67476], Loss: 4.5886\n",
      "Epoch [1/2], Step [55240/67476], Loss: 4.8530\n",
      "Epoch [1/2], Step [55250/67476], Loss: 4.5291\n",
      "Epoch [1/2], Step [55260/67476], Loss: 4.4547\n",
      "Epoch [1/2], Step [55270/67476], Loss: 4.7758\n",
      "Epoch [1/2], Step [55280/67476], Loss: 4.5810\n",
      "Epoch [1/2], Step [55290/67476], Loss: 4.5064\n",
      "Epoch [1/2], Step [55300/67476], Loss: 4.3990\n",
      "Epoch [1/2], Step [55310/67476], Loss: 4.5286\n",
      "Epoch [1/2], Step [55320/67476], Loss: 4.5742\n",
      "Epoch [1/2], Step [55330/67476], Loss: 4.5658\n",
      "Epoch [1/2], Step [55340/67476], Loss: 4.8056\n",
      "Epoch [1/2], Step [55350/67476], Loss: 4.5513\n",
      "Epoch [1/2], Step [55360/67476], Loss: 4.5773\n",
      "Epoch [1/2], Step [55370/67476], Loss: 4.5068\n",
      "Epoch [1/2], Step [55380/67476], Loss: 4.7191\n",
      "Epoch [1/2], Step [55390/67476], Loss: 4.6159\n",
      "Epoch [1/2], Step [55400/67476], Loss: 4.5704\n",
      "Epoch [1/2], Step [55410/67476], Loss: 4.4146\n",
      "Epoch [1/2], Step [55420/67476], Loss: 4.6389\n",
      "Epoch [1/2], Step [55430/67476], Loss: 4.6990\n",
      "Epoch [1/2], Step [55440/67476], Loss: 4.6623\n",
      "Epoch [1/2], Step [55450/67476], Loss: 4.5092\n",
      "Epoch [1/2], Step [55460/67476], Loss: 4.7551\n",
      "Epoch [1/2], Step [55470/67476], Loss: 4.5108\n",
      "Epoch [1/2], Step [55480/67476], Loss: 4.6485\n",
      "Epoch [1/2], Step [55490/67476], Loss: 4.4087\n",
      "Epoch [1/2], Step [55500/67476], Loss: 4.3318\n",
      "Epoch [1/2], Step [55510/67476], Loss: 4.6827\n",
      "Epoch [1/2], Step [55520/67476], Loss: 4.5780\n",
      "Epoch [1/2], Step [55530/67476], Loss: 4.6990\n",
      "Epoch [1/2], Step [55540/67476], Loss: 4.6567\n",
      "Epoch [1/2], Step [55550/67476], Loss: 4.4405\n",
      "Epoch [1/2], Step [55560/67476], Loss: 4.5641\n",
      "Epoch [1/2], Step [55570/67476], Loss: 4.7829\n",
      "Epoch [1/2], Step [55580/67476], Loss: 4.4812\n",
      "Epoch [1/2], Step [55590/67476], Loss: 4.6165\n",
      "Epoch [1/2], Step [55600/67476], Loss: 4.6604\n",
      "Epoch [1/2], Step [55610/67476], Loss: 4.5734\n",
      "Epoch [1/2], Step [55620/67476], Loss: 4.5585\n",
      "Epoch [1/2], Step [55630/67476], Loss: 4.5834\n",
      "Epoch [1/2], Step [55640/67476], Loss: 4.6415\n",
      "Epoch [1/2], Step [55650/67476], Loss: 4.7639\n",
      "Epoch [1/2], Step [55660/67476], Loss: 4.7501\n",
      "Epoch [1/2], Step [55670/67476], Loss: 4.6863\n",
      "Epoch [1/2], Step [55680/67476], Loss: 4.4034\n",
      "Epoch [1/2], Step [55690/67476], Loss: 4.4333\n",
      "Epoch [1/2], Step [55700/67476], Loss: 4.5666\n",
      "Epoch [1/2], Step [55710/67476], Loss: 4.3591\n",
      "Epoch [1/2], Step [55720/67476], Loss: 4.5952\n",
      "Epoch [1/2], Step [55730/67476], Loss: 4.5928\n",
      "Epoch [1/2], Step [55740/67476], Loss: 4.7319\n",
      "Epoch [1/2], Step [55750/67476], Loss: 4.6943\n",
      "Epoch [1/2], Step [55760/67476], Loss: 4.6195\n",
      "Epoch [1/2], Step [55770/67476], Loss: 4.5909\n",
      "Epoch [1/2], Step [55780/67476], Loss: 4.5912\n",
      "Epoch [1/2], Step [55790/67476], Loss: 4.5888\n",
      "Epoch [1/2], Step [55800/67476], Loss: 4.5368\n",
      "Epoch [1/2], Step [55810/67476], Loss: 4.4671\n",
      "Epoch [1/2], Step [55820/67476], Loss: 4.5666\n",
      "Epoch [1/2], Step [55830/67476], Loss: 4.5127\n",
      "Epoch [1/2], Step [55840/67476], Loss: 4.5799\n",
      "Epoch [1/2], Step [55850/67476], Loss: 4.5090\n",
      "Epoch [1/2], Step [55860/67476], Loss: 4.5515\n",
      "Epoch [1/2], Step [55870/67476], Loss: 4.5123\n",
      "Epoch [1/2], Step [55880/67476], Loss: 4.6132\n",
      "Epoch [1/2], Step [55890/67476], Loss: 4.6130\n",
      "Epoch [1/2], Step [55900/67476], Loss: 4.3506\n",
      "Epoch [1/2], Step [55910/67476], Loss: 4.5186\n",
      "Epoch [1/2], Step [55920/67476], Loss: 4.5818\n",
      "Epoch [1/2], Step [55930/67476], Loss: 4.5337\n",
      "Epoch [1/2], Step [55940/67476], Loss: 4.3135\n",
      "Epoch [1/2], Step [55950/67476], Loss: 4.5943\n",
      "Epoch [1/2], Step [55960/67476], Loss: 4.4880\n",
      "Epoch [1/2], Step [55970/67476], Loss: 4.6997\n",
      "Epoch [1/2], Step [55980/67476], Loss: 4.5746\n",
      "Epoch [1/2], Step [55990/67476], Loss: 4.4858\n",
      "Epoch [1/2], Step [56000/67476], Loss: 4.4360\n",
      "Epoch [1/2], Step [56010/67476], Loss: 4.6725\n",
      "Epoch [1/2], Step [56020/67476], Loss: 4.4740\n",
      "Epoch [1/2], Step [56030/67476], Loss: 4.4794\n",
      "Epoch [1/2], Step [56040/67476], Loss: 4.4482\n",
      "Epoch [1/2], Step [56050/67476], Loss: 4.6567\n",
      "Epoch [1/2], Step [56060/67476], Loss: 4.6014\n",
      "Epoch [1/2], Step [56070/67476], Loss: 4.5180\n",
      "Epoch [1/2], Step [56080/67476], Loss: 4.5631\n",
      "Epoch [1/2], Step [56090/67476], Loss: 4.6231\n",
      "Epoch [1/2], Step [56100/67476], Loss: 4.6056\n",
      "Epoch [1/2], Step [56110/67476], Loss: 4.8207\n",
      "Epoch [1/2], Step [56120/67476], Loss: 4.5967\n",
      "Epoch [1/2], Step [56130/67476], Loss: 4.4035\n",
      "Epoch [1/2], Step [56140/67476], Loss: 4.6575\n",
      "Epoch [1/2], Step [56150/67476], Loss: 4.6242\n",
      "Epoch [1/2], Step [56160/67476], Loss: 4.6649\n",
      "Epoch [1/2], Step [56170/67476], Loss: 4.5988\n",
      "Epoch [1/2], Step [56180/67476], Loss: 4.6308\n",
      "Epoch [1/2], Step [56190/67476], Loss: 4.6723\n",
      "Epoch [1/2], Step [56200/67476], Loss: 4.6184\n",
      "Epoch [1/2], Step [56210/67476], Loss: 4.6335\n",
      "Epoch [1/2], Step [56220/67476], Loss: 4.5334\n",
      "Epoch [1/2], Step [56230/67476], Loss: 4.6059\n",
      "Epoch [1/2], Step [56240/67476], Loss: 4.5959\n",
      "Epoch [1/2], Step [56250/67476], Loss: 4.5673\n",
      "Epoch [1/2], Step [56260/67476], Loss: 4.5039\n",
      "Epoch [1/2], Step [56270/67476], Loss: 4.6113\n",
      "Epoch [1/2], Step [56280/67476], Loss: 4.4013\n",
      "Epoch [1/2], Step [56290/67476], Loss: 4.3739\n",
      "Epoch [1/2], Step [56300/67476], Loss: 4.6527\n",
      "Epoch [1/2], Step [56310/67476], Loss: 4.6260\n",
      "Epoch [1/2], Step [56320/67476], Loss: 4.4859\n",
      "Epoch [1/2], Step [56330/67476], Loss: 4.4957\n",
      "Epoch [1/2], Step [56340/67476], Loss: 4.5138\n",
      "Epoch [1/2], Step [56350/67476], Loss: 4.6791\n",
      "Epoch [1/2], Step [56360/67476], Loss: 4.5417\n",
      "Epoch [1/2], Step [56370/67476], Loss: 4.6180\n",
      "Epoch [1/2], Step [56380/67476], Loss: 4.7512\n",
      "Epoch [1/2], Step [56390/67476], Loss: 4.7732\n",
      "Epoch [1/2], Step [56400/67476], Loss: 4.4155\n",
      "Epoch [1/2], Step [56410/67476], Loss: 4.5126\n",
      "Epoch [1/2], Step [56420/67476], Loss: 4.6399\n",
      "Epoch [1/2], Step [56430/67476], Loss: 4.6935\n",
      "Epoch [1/2], Step [56440/67476], Loss: 4.6019\n",
      "Epoch [1/2], Step [56450/67476], Loss: 4.6132\n",
      "Epoch [1/2], Step [56460/67476], Loss: 4.4750\n",
      "Epoch [1/2], Step [56470/67476], Loss: 4.7376\n",
      "Epoch [1/2], Step [56480/67476], Loss: 4.5765\n",
      "Epoch [1/2], Step [56490/67476], Loss: 4.6777\n",
      "Epoch [1/2], Step [56500/67476], Loss: 4.4795\n",
      "Epoch [1/2], Step [56510/67476], Loss: 4.5042\n",
      "Epoch [1/2], Step [56520/67476], Loss: 4.5754\n",
      "Epoch [1/2], Step [56530/67476], Loss: 4.4672\n",
      "Epoch [1/2], Step [56540/67476], Loss: 4.5264\n",
      "Epoch [1/2], Step [56550/67476], Loss: 4.5702\n",
      "Epoch [1/2], Step [56560/67476], Loss: 4.4409\n",
      "Epoch [1/2], Step [56570/67476], Loss: 4.7730\n",
      "Epoch [1/2], Step [56580/67476], Loss: 4.4915\n",
      "Epoch [1/2], Step [56590/67476], Loss: 4.3119\n",
      "Epoch [1/2], Step [56600/67476], Loss: 4.4381\n",
      "Epoch [1/2], Step [56610/67476], Loss: 4.4423\n",
      "Epoch [1/2], Step [56620/67476], Loss: 4.4729\n",
      "Epoch [1/2], Step [56630/67476], Loss: 4.5711\n",
      "Epoch [1/2], Step [56640/67476], Loss: 4.5906\n",
      "Epoch [1/2], Step [56650/67476], Loss: 4.6644\n",
      "Epoch [1/2], Step [56660/67476], Loss: 4.4606\n",
      "Epoch [1/2], Step [56670/67476], Loss: 4.6090\n",
      "Epoch [1/2], Step [56680/67476], Loss: 4.7435\n",
      "Epoch [1/2], Step [56690/67476], Loss: 4.7580\n",
      "Epoch [1/2], Step [56700/67476], Loss: 4.4897\n",
      "Epoch [1/2], Step [56710/67476], Loss: 4.4114\n",
      "Epoch [1/2], Step [56720/67476], Loss: 4.6579\n",
      "Epoch [1/2], Step [56730/67476], Loss: 4.6630\n",
      "Epoch [1/2], Step [56740/67476], Loss: 4.5957\n",
      "Epoch [1/2], Step [56750/67476], Loss: 4.5719\n",
      "Epoch [1/2], Step [56760/67476], Loss: 4.5945\n",
      "Epoch [1/2], Step [56770/67476], Loss: 4.7601\n",
      "Epoch [1/2], Step [56780/67476], Loss: 4.6258\n",
      "Epoch [1/2], Step [56790/67476], Loss: 4.6630\n",
      "Epoch [1/2], Step [56800/67476], Loss: 4.4715\n",
      "Epoch [1/2], Step [56810/67476], Loss: 4.4314\n",
      "Epoch [1/2], Step [56820/67476], Loss: 4.5401\n",
      "Epoch [1/2], Step [56830/67476], Loss: 4.6639\n",
      "Epoch [1/2], Step [56840/67476], Loss: 4.6130\n",
      "Epoch [1/2], Step [56850/67476], Loss: 4.4918\n",
      "Epoch [1/2], Step [56860/67476], Loss: 4.5737\n",
      "Epoch [1/2], Step [56870/67476], Loss: 4.5471\n",
      "Epoch [1/2], Step [56880/67476], Loss: 4.5092\n",
      "Epoch [1/2], Step [56890/67476], Loss: 4.7666\n",
      "Epoch [1/2], Step [56900/67476], Loss: 4.3447\n",
      "Epoch [1/2], Step [56910/67476], Loss: 4.6535\n",
      "Epoch [1/2], Step [56920/67476], Loss: 4.7129\n",
      "Epoch [1/2], Step [56930/67476], Loss: 4.4871\n",
      "Epoch [1/2], Step [56940/67476], Loss: 4.3042\n",
      "Epoch [1/2], Step [56950/67476], Loss: 4.5141\n",
      "Epoch [1/2], Step [56960/67476], Loss: 4.6640\n",
      "Epoch [1/2], Step [56970/67476], Loss: 4.5860\n",
      "Epoch [1/2], Step [56980/67476], Loss: 4.5509\n",
      "Epoch [1/2], Step [56990/67476], Loss: 4.3661\n",
      "Epoch [1/2], Step [57000/67476], Loss: 4.5793\n",
      "Epoch [1/2], Step [57010/67476], Loss: 4.7687\n",
      "Epoch [1/2], Step [57020/67476], Loss: 4.5462\n",
      "Epoch [1/2], Step [57030/67476], Loss: 4.5011\n",
      "Epoch [1/2], Step [57040/67476], Loss: 4.5532\n",
      "Epoch [1/2], Step [57050/67476], Loss: 4.6673\n",
      "Epoch [1/2], Step [57060/67476], Loss: 4.4774\n",
      "Epoch [1/2], Step [57070/67476], Loss: 4.7194\n",
      "Epoch [1/2], Step [57080/67476], Loss: 4.6305\n",
      "Epoch [1/2], Step [57090/67476], Loss: 4.6083\n",
      "Epoch [1/2], Step [57100/67476], Loss: 4.4839\n",
      "Epoch [1/2], Step [57110/67476], Loss: 4.7854\n",
      "Epoch [1/2], Step [57120/67476], Loss: 4.6494\n",
      "Epoch [1/2], Step [57130/67476], Loss: 4.5393\n",
      "Epoch [1/2], Step [57140/67476], Loss: 4.6889\n",
      "Epoch [1/2], Step [57150/67476], Loss: 4.7313\n",
      "Epoch [1/2], Step [57160/67476], Loss: 4.5106\n",
      "Epoch [1/2], Step [57170/67476], Loss: 4.4683\n",
      "Epoch [1/2], Step [57180/67476], Loss: 4.7001\n",
      "Epoch [1/2], Step [57190/67476], Loss: 4.6263\n",
      "Epoch [1/2], Step [57200/67476], Loss: 4.7832\n",
      "Epoch [1/2], Step [57210/67476], Loss: 4.5904\n",
      "Epoch [1/2], Step [57220/67476], Loss: 4.5455\n",
      "Epoch [1/2], Step [57230/67476], Loss: 4.5291\n",
      "Epoch [1/2], Step [57240/67476], Loss: 4.5534\n",
      "Epoch [1/2], Step [57250/67476], Loss: 4.6422\n",
      "Epoch [1/2], Step [57260/67476], Loss: 4.7528\n",
      "Epoch [1/2], Step [57270/67476], Loss: 4.5563\n",
      "Epoch [1/2], Step [57280/67476], Loss: 4.5162\n",
      "Epoch [1/2], Step [57290/67476], Loss: 4.5348\n",
      "Epoch [1/2], Step [57300/67476], Loss: 4.6396\n",
      "Epoch [1/2], Step [57310/67476], Loss: 4.6794\n",
      "Epoch [1/2], Step [57320/67476], Loss: 4.7271\n",
      "Epoch [1/2], Step [57330/67476], Loss: 4.5433\n",
      "Epoch [1/2], Step [57340/67476], Loss: 4.5090\n",
      "Epoch [1/2], Step [57350/67476], Loss: 4.5980\n",
      "Epoch [1/2], Step [57360/67476], Loss: 4.5171\n",
      "Epoch [1/2], Step [57370/67476], Loss: 4.5035\n",
      "Epoch [1/2], Step [57380/67476], Loss: 4.6739\n",
      "Epoch [1/2], Step [57390/67476], Loss: 4.4491\n",
      "Epoch [1/2], Step [57400/67476], Loss: 4.6043\n",
      "Epoch [1/2], Step [57410/67476], Loss: 4.7182\n",
      "Epoch [1/2], Step [57420/67476], Loss: 4.5951\n",
      "Epoch [1/2], Step [57430/67476], Loss: 4.8531\n",
      "Epoch [1/2], Step [57440/67476], Loss: 4.6235\n",
      "Epoch [1/2], Step [57450/67476], Loss: 4.4920\n",
      "Epoch [1/2], Step [57460/67476], Loss: 4.7168\n",
      "Epoch [1/2], Step [57470/67476], Loss: 4.4715\n",
      "Epoch [1/2], Step [57480/67476], Loss: 4.3998\n",
      "Epoch [1/2], Step [57490/67476], Loss: 4.6925\n",
      "Epoch [1/2], Step [57500/67476], Loss: 4.3987\n",
      "Epoch [1/2], Step [57510/67476], Loss: 4.6656\n",
      "Epoch [1/2], Step [57520/67476], Loss: 4.5427\n",
      "Epoch [1/2], Step [57530/67476], Loss: 4.7693\n",
      "Epoch [1/2], Step [57540/67476], Loss: 4.7256\n",
      "Epoch [1/2], Step [57550/67476], Loss: 4.5778\n",
      "Epoch [1/2], Step [57560/67476], Loss: 4.4447\n",
      "Epoch [1/2], Step [57570/67476], Loss: 4.5818\n",
      "Epoch [1/2], Step [57580/67476], Loss: 4.5516\n",
      "Epoch [1/2], Step [57590/67476], Loss: 4.6677\n",
      "Epoch [1/2], Step [57600/67476], Loss: 4.6436\n",
      "Epoch [1/2], Step [57610/67476], Loss: 4.7108\n",
      "Epoch [1/2], Step [57620/67476], Loss: 4.6597\n",
      "Epoch [1/2], Step [57630/67476], Loss: 4.4405\n",
      "Epoch [1/2], Step [57640/67476], Loss: 4.5343\n",
      "Epoch [1/2], Step [57650/67476], Loss: 4.5083\n",
      "Epoch [1/2], Step [57660/67476], Loss: 4.5342\n",
      "Epoch [1/2], Step [57670/67476], Loss: 4.4394\n",
      "Epoch [1/2], Step [57680/67476], Loss: 4.4997\n",
      "Epoch [1/2], Step [57690/67476], Loss: 4.5150\n",
      "Epoch [1/2], Step [57700/67476], Loss: 4.3167\n",
      "Epoch [1/2], Step [57710/67476], Loss: 4.6494\n",
      "Epoch [1/2], Step [57720/67476], Loss: 4.6056\n",
      "Epoch [1/2], Step [57730/67476], Loss: 4.5402\n",
      "Epoch [1/2], Step [57740/67476], Loss: 4.5715\n",
      "Epoch [1/2], Step [57750/67476], Loss: 4.7465\n",
      "Epoch [1/2], Step [57760/67476], Loss: 4.4070\n",
      "Epoch [1/2], Step [57770/67476], Loss: 4.4490\n",
      "Epoch [1/2], Step [57780/67476], Loss: 4.4843\n",
      "Epoch [1/2], Step [57790/67476], Loss: 4.6298\n",
      "Epoch [1/2], Step [57800/67476], Loss: 4.4212\n",
      "Epoch [1/2], Step [57810/67476], Loss: 4.7172\n",
      "Epoch [1/2], Step [57820/67476], Loss: 4.6548\n",
      "Epoch [1/2], Step [57830/67476], Loss: 4.6637\n",
      "Epoch [1/2], Step [57840/67476], Loss: 4.7356\n",
      "Epoch [1/2], Step [57850/67476], Loss: 4.5766\n",
      "Epoch [1/2], Step [57860/67476], Loss: 4.5433\n",
      "Epoch [1/2], Step [57870/67476], Loss: 4.5228\n",
      "Epoch [1/2], Step [57880/67476], Loss: 4.6664\n",
      "Epoch [1/2], Step [57890/67476], Loss: 4.5839\n",
      "Epoch [1/2], Step [57900/67476], Loss: 4.7803\n",
      "Epoch [1/2], Step [57910/67476], Loss: 4.5180\n",
      "Epoch [1/2], Step [57920/67476], Loss: 4.4705\n",
      "Epoch [1/2], Step [57930/67476], Loss: 4.4698\n",
      "Epoch [1/2], Step [57940/67476], Loss: 4.3952\n",
      "Epoch [1/2], Step [57950/67476], Loss: 4.5247\n",
      "Epoch [1/2], Step [57960/67476], Loss: 4.4571\n",
      "Epoch [1/2], Step [57970/67476], Loss: 4.3511\n",
      "Epoch [1/2], Step [57980/67476], Loss: 4.6271\n",
      "Epoch [1/2], Step [57990/67476], Loss: 4.6202\n",
      "Epoch [1/2], Step [58000/67476], Loss: 4.4726\n",
      "Epoch [1/2], Step [58010/67476], Loss: 4.5936\n",
      "Epoch [1/2], Step [58020/67476], Loss: 4.5500\n",
      "Epoch [1/2], Step [58030/67476], Loss: 4.7251\n",
      "Epoch [1/2], Step [58040/67476], Loss: 4.5577\n",
      "Epoch [1/2], Step [58050/67476], Loss: 4.6354\n",
      "Epoch [1/2], Step [58060/67476], Loss: 4.6532\n",
      "Epoch [1/2], Step [58070/67476], Loss: 4.4865\n",
      "Epoch [1/2], Step [58080/67476], Loss: 4.5552\n",
      "Epoch [1/2], Step [58090/67476], Loss: 4.6984\n",
      "Epoch [1/2], Step [58100/67476], Loss: 4.4377\n",
      "Epoch [1/2], Step [58110/67476], Loss: 4.5933\n",
      "Epoch [1/2], Step [58120/67476], Loss: 4.3272\n",
      "Epoch [1/2], Step [58130/67476], Loss: 4.6254\n",
      "Epoch [1/2], Step [58140/67476], Loss: 4.6502\n",
      "Epoch [1/2], Step [58150/67476], Loss: 4.4275\n",
      "Epoch [1/2], Step [58160/67476], Loss: 4.5311\n",
      "Epoch [1/2], Step [58170/67476], Loss: 4.6632\n",
      "Epoch [1/2], Step [58180/67476], Loss: 4.4966\n",
      "Epoch [1/2], Step [58190/67476], Loss: 4.6685\n",
      "Epoch [1/2], Step [58200/67476], Loss: 4.6234\n",
      "Epoch [1/2], Step [58210/67476], Loss: 4.6122\n",
      "Epoch [1/2], Step [58220/67476], Loss: 4.3724\n",
      "Epoch [1/2], Step [58230/67476], Loss: 4.2557\n",
      "Epoch [1/2], Step [58240/67476], Loss: 4.6516\n",
      "Epoch [1/2], Step [58250/67476], Loss: 4.5950\n",
      "Epoch [1/2], Step [58260/67476], Loss: 4.6190\n",
      "Epoch [1/2], Step [58270/67476], Loss: 4.6732\n",
      "Epoch [1/2], Step [58280/67476], Loss: 4.6866\n",
      "Epoch [1/2], Step [58290/67476], Loss: 4.8084\n",
      "Epoch [1/2], Step [58300/67476], Loss: 4.6094\n",
      "Epoch [1/2], Step [58310/67476], Loss: 4.5909\n",
      "Epoch [1/2], Step [58320/67476], Loss: 4.3539\n",
      "Epoch [1/2], Step [58330/67476], Loss: 4.5735\n",
      "Epoch [1/2], Step [58340/67476], Loss: 4.4788\n",
      "Epoch [1/2], Step [58350/67476], Loss: 4.7286\n",
      "Epoch [1/2], Step [58360/67476], Loss: 4.4386\n",
      "Epoch [1/2], Step [58370/67476], Loss: 4.4284\n",
      "Epoch [1/2], Step [58380/67476], Loss: 4.5920\n",
      "Epoch [1/2], Step [58390/67476], Loss: 4.4461\n",
      "Epoch [1/2], Step [58400/67476], Loss: 4.6877\n",
      "Epoch [1/2], Step [58410/67476], Loss: 4.4585\n",
      "Epoch [1/2], Step [58420/67476], Loss: 4.5794\n",
      "Epoch [1/2], Step [58430/67476], Loss: 4.6424\n",
      "Epoch [1/2], Step [58440/67476], Loss: 4.4670\n",
      "Epoch [1/2], Step [58450/67476], Loss: 4.3189\n",
      "Epoch [1/2], Step [58460/67476], Loss: 4.7403\n",
      "Epoch [1/2], Step [58470/67476], Loss: 4.5228\n",
      "Epoch [1/2], Step [58480/67476], Loss: 4.5966\n",
      "Epoch [1/2], Step [58490/67476], Loss: 4.8132\n",
      "Epoch [1/2], Step [58500/67476], Loss: 4.4037\n",
      "Epoch [1/2], Step [58510/67476], Loss: 4.5232\n",
      "Epoch [1/2], Step [58520/67476], Loss: 4.6319\n",
      "Epoch [1/2], Step [58530/67476], Loss: 4.7436\n",
      "Epoch [1/2], Step [58540/67476], Loss: 4.6487\n",
      "Epoch [1/2], Step [58550/67476], Loss: 4.5045\n",
      "Epoch [1/2], Step [58560/67476], Loss: 4.5792\n",
      "Epoch [1/2], Step [58570/67476], Loss: 4.6265\n",
      "Epoch [1/2], Step [58580/67476], Loss: 4.7519\n",
      "Epoch [1/2], Step [58590/67476], Loss: 4.7475\n",
      "Epoch [1/2], Step [58600/67476], Loss: 4.5515\n",
      "Epoch [1/2], Step [58610/67476], Loss: 4.5590\n",
      "Epoch [1/2], Step [58620/67476], Loss: 4.4588\n",
      "Epoch [1/2], Step [58630/67476], Loss: 4.5770\n",
      "Epoch [1/2], Step [58640/67476], Loss: 4.5858\n",
      "Epoch [1/2], Step [58650/67476], Loss: 4.5362\n",
      "Epoch [1/2], Step [58660/67476], Loss: 4.5078\n",
      "Epoch [1/2], Step [58670/67476], Loss: 4.4570\n",
      "Epoch [1/2], Step [58680/67476], Loss: 4.5151\n",
      "Epoch [1/2], Step [58690/67476], Loss: 4.7596\n",
      "Epoch [1/2], Step [58700/67476], Loss: 4.4952\n",
      "Epoch [1/2], Step [58710/67476], Loss: 4.4395\n",
      "Epoch [1/2], Step [58720/67476], Loss: 4.5769\n",
      "Epoch [1/2], Step [58730/67476], Loss: 4.4493\n",
      "Epoch [1/2], Step [58740/67476], Loss: 4.4752\n",
      "Epoch [1/2], Step [58750/67476], Loss: 4.4936\n",
      "Epoch [1/2], Step [58760/67476], Loss: 4.4070\n",
      "Epoch [1/2], Step [58770/67476], Loss: 4.3672\n",
      "Epoch [1/2], Step [58780/67476], Loss: 4.6178\n",
      "Epoch [1/2], Step [58790/67476], Loss: 4.4317\n",
      "Epoch [1/2], Step [58800/67476], Loss: 4.5195\n",
      "Epoch [1/2], Step [58810/67476], Loss: 4.4334\n",
      "Epoch [1/2], Step [58820/67476], Loss: 4.5557\n",
      "Epoch [1/2], Step [58830/67476], Loss: 4.5094\n",
      "Epoch [1/2], Step [58840/67476], Loss: 4.4527\n",
      "Epoch [1/2], Step [58850/67476], Loss: 4.5831\n",
      "Epoch [1/2], Step [58860/67476], Loss: 4.6667\n",
      "Epoch [1/2], Step [58870/67476], Loss: 4.4460\n",
      "Epoch [1/2], Step [58880/67476], Loss: 4.7321\n",
      "Epoch [1/2], Step [58890/67476], Loss: 4.5619\n",
      "Epoch [1/2], Step [58900/67476], Loss: 4.5096\n",
      "Epoch [1/2], Step [58910/67476], Loss: 4.5789\n",
      "Epoch [1/2], Step [58920/67476], Loss: 4.2095\n",
      "Epoch [1/2], Step [58930/67476], Loss: 4.4877\n",
      "Epoch [1/2], Step [58940/67476], Loss: 4.5947\n",
      "Epoch [1/2], Step [58950/67476], Loss: 4.6766\n",
      "Epoch [1/2], Step [58960/67476], Loss: 4.7032\n",
      "Epoch [1/2], Step [58970/67476], Loss: 4.4136\n",
      "Epoch [1/2], Step [58980/67476], Loss: 4.7350\n",
      "Epoch [1/2], Step [58990/67476], Loss: 4.6590\n",
      "Epoch [1/2], Step [59000/67476], Loss: 4.7612\n",
      "Epoch [1/2], Step [59010/67476], Loss: 4.4428\n",
      "Epoch [1/2], Step [59020/67476], Loss: 4.6757\n",
      "Epoch [1/2], Step [59030/67476], Loss: 4.3555\n",
      "Epoch [1/2], Step [59040/67476], Loss: 4.5231\n",
      "Epoch [1/2], Step [59050/67476], Loss: 4.5813\n",
      "Epoch [1/2], Step [59060/67476], Loss: 4.5452\n",
      "Epoch [1/2], Step [59070/67476], Loss: 4.4382\n",
      "Epoch [1/2], Step [59080/67476], Loss: 4.6827\n",
      "Epoch [1/2], Step [59090/67476], Loss: 4.6561\n",
      "Epoch [1/2], Step [59100/67476], Loss: 4.3711\n",
      "Epoch [1/2], Step [59110/67476], Loss: 4.4367\n",
      "Epoch [1/2], Step [59120/67476], Loss: 4.4896\n",
      "Epoch [1/2], Step [59130/67476], Loss: 4.5095\n",
      "Epoch [1/2], Step [59140/67476], Loss: 4.5937\n",
      "Epoch [1/2], Step [59150/67476], Loss: 4.3767\n",
      "Epoch [1/2], Step [59160/67476], Loss: 4.6859\n",
      "Epoch [1/2], Step [59170/67476], Loss: 4.6810\n",
      "Epoch [1/2], Step [59180/67476], Loss: 4.7829\n",
      "Epoch [1/2], Step [59190/67476], Loss: 4.4959\n",
      "Epoch [1/2], Step [59200/67476], Loss: 4.3453\n",
      "Epoch [1/2], Step [59210/67476], Loss: 4.5235\n",
      "Epoch [1/2], Step [59220/67476], Loss: 4.4640\n",
      "Epoch [1/2], Step [59230/67476], Loss: 4.6908\n",
      "Epoch [1/2], Step [59240/67476], Loss: 4.5514\n",
      "Epoch [1/2], Step [59250/67476], Loss: 4.6102\n",
      "Epoch [1/2], Step [59260/67476], Loss: 4.6728\n",
      "Epoch [1/2], Step [59270/67476], Loss: 4.6210\n",
      "Epoch [1/2], Step [59280/67476], Loss: 4.5318\n",
      "Epoch [1/2], Step [59290/67476], Loss: 4.3588\n",
      "Epoch [1/2], Step [59300/67476], Loss: 4.6345\n",
      "Epoch [1/2], Step [59310/67476], Loss: 4.8261\n",
      "Epoch [1/2], Step [59320/67476], Loss: 4.5703\n",
      "Epoch [1/2], Step [59330/67476], Loss: 4.5545\n",
      "Epoch [1/2], Step [59340/67476], Loss: 4.6294\n",
      "Epoch [1/2], Step [59350/67476], Loss: 4.7758\n",
      "Epoch [1/2], Step [59360/67476], Loss: 4.4582\n",
      "Epoch [1/2], Step [59370/67476], Loss: 4.7220\n",
      "Epoch [1/2], Step [59380/67476], Loss: 4.4242\n",
      "Epoch [1/2], Step [59390/67476], Loss: 4.3211\n",
      "Epoch [1/2], Step [59400/67476], Loss: 4.7178\n",
      "Epoch [1/2], Step [59410/67476], Loss: 4.7325\n",
      "Epoch [1/2], Step [59420/67476], Loss: 4.5522\n",
      "Epoch [1/2], Step [59430/67476], Loss: 4.5670\n",
      "Epoch [1/2], Step [59440/67476], Loss: 4.5918\n",
      "Epoch [1/2], Step [59450/67476], Loss: 4.6298\n",
      "Epoch [1/2], Step [59460/67476], Loss: 4.4538\n",
      "Epoch [1/2], Step [59470/67476], Loss: 4.7483\n",
      "Epoch [1/2], Step [59480/67476], Loss: 4.4978\n",
      "Epoch [1/2], Step [59490/67476], Loss: 4.5335\n",
      "Epoch [1/2], Step [59500/67476], Loss: 4.5333\n",
      "Epoch [1/2], Step [59510/67476], Loss: 4.3405\n",
      "Epoch [1/2], Step [59520/67476], Loss: 4.6540\n",
      "Epoch [1/2], Step [59530/67476], Loss: 4.7301\n",
      "Epoch [1/2], Step [59540/67476], Loss: 4.5074\n",
      "Epoch [1/2], Step [59550/67476], Loss: 4.7283\n",
      "Epoch [1/2], Step [59560/67476], Loss: 4.6154\n",
      "Epoch [1/2], Step [59570/67476], Loss: 4.5267\n",
      "Epoch [1/2], Step [59580/67476], Loss: 4.3575\n",
      "Epoch [1/2], Step [59590/67476], Loss: 4.6371\n",
      "Epoch [1/2], Step [59600/67476], Loss: 4.6840\n",
      "Epoch [1/2], Step [59610/67476], Loss: 4.5871\n",
      "Epoch [1/2], Step [59620/67476], Loss: 4.6713\n",
      "Epoch [1/2], Step [59630/67476], Loss: 4.4381\n",
      "Epoch [1/2], Step [59640/67476], Loss: 4.6000\n",
      "Epoch [1/2], Step [59650/67476], Loss: 4.4455\n",
      "Epoch [1/2], Step [59660/67476], Loss: 4.4213\n",
      "Epoch [1/2], Step [59670/67476], Loss: 4.5320\n",
      "Epoch [1/2], Step [59680/67476], Loss: 4.5119\n",
      "Epoch [1/2], Step [59690/67476], Loss: 4.5583\n",
      "Epoch [1/2], Step [59700/67476], Loss: 4.5906\n",
      "Epoch [1/2], Step [59710/67476], Loss: 4.4742\n",
      "Epoch [1/2], Step [59720/67476], Loss: 4.6735\n",
      "Epoch [1/2], Step [59730/67476], Loss: 4.4090\n",
      "Epoch [1/2], Step [59740/67476], Loss: 4.4746\n",
      "Epoch [1/2], Step [59750/67476], Loss: 4.8158\n",
      "Epoch [1/2], Step [59760/67476], Loss: 4.6186\n",
      "Epoch [1/2], Step [59770/67476], Loss: 4.9182\n",
      "Epoch [1/2], Step [59780/67476], Loss: 4.7180\n",
      "Epoch [1/2], Step [59790/67476], Loss: 4.6650\n",
      "Epoch [1/2], Step [59800/67476], Loss: 4.4939\n",
      "Epoch [1/2], Step [59810/67476], Loss: 4.5781\n",
      "Epoch [1/2], Step [59820/67476], Loss: 4.6133\n",
      "Epoch [1/2], Step [59830/67476], Loss: 4.5327\n",
      "Epoch [1/2], Step [59840/67476], Loss: 4.6305\n",
      "Epoch [1/2], Step [59850/67476], Loss: 4.4058\n",
      "Epoch [1/2], Step [59860/67476], Loss: 4.5958\n",
      "Epoch [1/2], Step [59870/67476], Loss: 4.4299\n",
      "Epoch [1/2], Step [59880/67476], Loss: 4.7012\n",
      "Epoch [1/2], Step [59890/67476], Loss: 4.4270\n",
      "Epoch [1/2], Step [59900/67476], Loss: 4.6081\n",
      "Epoch [1/2], Step [59910/67476], Loss: 4.6955\n",
      "Epoch [1/2], Step [59920/67476], Loss: 4.6169\n",
      "Epoch [1/2], Step [59930/67476], Loss: 4.3742\n",
      "Epoch [1/2], Step [59940/67476], Loss: 4.6151\n",
      "Epoch [1/2], Step [59950/67476], Loss: 4.4084\n",
      "Epoch [1/2], Step [59960/67476], Loss: 4.7037\n",
      "Epoch [1/2], Step [59970/67476], Loss: 4.4176\n",
      "Epoch [1/2], Step [59980/67476], Loss: 4.4383\n",
      "Epoch [1/2], Step [59990/67476], Loss: 4.4841\n",
      "Epoch [1/2], Step [60000/67476], Loss: 4.3396\n",
      "Epoch [1/2], Step [60010/67476], Loss: 4.6163\n",
      "Epoch [1/2], Step [60020/67476], Loss: 4.7909\n",
      "Epoch [1/2], Step [60030/67476], Loss: 4.6182\n",
      "Epoch [1/2], Step [60040/67476], Loss: 4.4449\n",
      "Epoch [1/2], Step [60050/67476], Loss: 4.4488\n",
      "Epoch [1/2], Step [60060/67476], Loss: 4.3501\n",
      "Epoch [1/2], Step [60070/67476], Loss: 4.7413\n",
      "Epoch [1/2], Step [60080/67476], Loss: 4.6778\n",
      "Epoch [1/2], Step [60090/67476], Loss: 4.6307\n",
      "Epoch [1/2], Step [60100/67476], Loss: 4.5453\n",
      "Epoch [1/2], Step [60110/67476], Loss: 4.5426\n",
      "Epoch [1/2], Step [60120/67476], Loss: 4.4598\n",
      "Epoch [1/2], Step [60130/67476], Loss: 4.4043\n",
      "Epoch [1/2], Step [60140/67476], Loss: 4.4567\n",
      "Epoch [1/2], Step [60150/67476], Loss: 4.4781\n",
      "Epoch [1/2], Step [60160/67476], Loss: 4.4366\n",
      "Epoch [1/2], Step [60170/67476], Loss: 4.3711\n",
      "Epoch [1/2], Step [60180/67476], Loss: 4.5131\n",
      "Epoch [1/2], Step [60190/67476], Loss: 4.6727\n",
      "Epoch [1/2], Step [60200/67476], Loss: 4.3458\n",
      "Epoch [1/2], Step [60210/67476], Loss: 4.4753\n",
      "Epoch [1/2], Step [60220/67476], Loss: 4.5936\n",
      "Epoch [1/2], Step [60230/67476], Loss: 4.5339\n",
      "Epoch [1/2], Step [60240/67476], Loss: 4.6707\n",
      "Epoch [1/2], Step [60250/67476], Loss: 4.6160\n",
      "Epoch [1/2], Step [60260/67476], Loss: 4.6384\n",
      "Epoch [1/2], Step [60270/67476], Loss: 4.4755\n",
      "Epoch [1/2], Step [60280/67476], Loss: 4.4275\n",
      "Epoch [1/2], Step [60290/67476], Loss: 4.8030\n",
      "Epoch [1/2], Step [60300/67476], Loss: 4.5426\n",
      "Epoch [1/2], Step [60310/67476], Loss: 4.5736\n",
      "Epoch [1/2], Step [60320/67476], Loss: 4.7867\n",
      "Epoch [1/2], Step [60330/67476], Loss: 4.5339\n",
      "Epoch [1/2], Step [60340/67476], Loss: 4.5297\n",
      "Epoch [1/2], Step [60350/67476], Loss: 4.6612\n",
      "Epoch [1/2], Step [60360/67476], Loss: 4.5679\n",
      "Epoch [1/2], Step [60370/67476], Loss: 4.3885\n",
      "Epoch [1/2], Step [60380/67476], Loss: 4.6718\n",
      "Epoch [1/2], Step [60390/67476], Loss: 4.5847\n",
      "Epoch [1/2], Step [60400/67476], Loss: 4.5941\n",
      "Epoch [1/2], Step [60410/67476], Loss: 4.5785\n",
      "Epoch [1/2], Step [60420/67476], Loss: 4.5603\n",
      "Epoch [1/2], Step [60430/67476], Loss: 4.6550\n",
      "Epoch [1/2], Step [60440/67476], Loss: 4.5902\n",
      "Epoch [1/2], Step [60450/67476], Loss: 4.4717\n",
      "Epoch [1/2], Step [60460/67476], Loss: 4.6381\n",
      "Epoch [1/2], Step [60470/67476], Loss: 4.4076\n",
      "Epoch [1/2], Step [60480/67476], Loss: 4.3471\n",
      "Epoch [1/2], Step [60490/67476], Loss: 4.5621\n",
      "Epoch [1/2], Step [60500/67476], Loss: 4.6520\n",
      "Epoch [1/2], Step [60510/67476], Loss: 4.8283\n",
      "Epoch [1/2], Step [60520/67476], Loss: 4.5040\n",
      "Epoch [1/2], Step [60530/67476], Loss: 4.4814\n",
      "Epoch [1/2], Step [60540/67476], Loss: 4.4527\n",
      "Epoch [1/2], Step [60550/67476], Loss: 4.5495\n",
      "Epoch [1/2], Step [60560/67476], Loss: 4.6345\n",
      "Epoch [1/2], Step [60570/67476], Loss: 4.6912\n",
      "Epoch [1/2], Step [60580/67476], Loss: 4.6623\n",
      "Epoch [1/2], Step [60590/67476], Loss: 4.5102\n",
      "Epoch [1/2], Step [60600/67476], Loss: 4.5493\n",
      "Epoch [1/2], Step [60610/67476], Loss: 4.6898\n",
      "Epoch [1/2], Step [60620/67476], Loss: 4.6531\n",
      "Epoch [1/2], Step [60630/67476], Loss: 4.5205\n",
      "Epoch [1/2], Step [60640/67476], Loss: 4.6306\n",
      "Epoch [1/2], Step [60650/67476], Loss: 4.6117\n",
      "Epoch [1/2], Step [60660/67476], Loss: 4.3883\n",
      "Epoch [1/2], Step [60670/67476], Loss: 4.5876\n",
      "Epoch [1/2], Step [60680/67476], Loss: 4.4987\n",
      "Epoch [1/2], Step [60690/67476], Loss: 4.5248\n",
      "Epoch [1/2], Step [60700/67476], Loss: 4.6194\n",
      "Epoch [1/2], Step [60710/67476], Loss: 4.5512\n",
      "Epoch [1/2], Step [60720/67476], Loss: 4.5687\n",
      "Epoch [1/2], Step [60730/67476], Loss: 4.5911\n",
      "Epoch [1/2], Step [60740/67476], Loss: 4.4977\n",
      "Epoch [1/2], Step [60750/67476], Loss: 4.4429\n",
      "Epoch [1/2], Step [60760/67476], Loss: 4.4129\n",
      "Epoch [1/2], Step [60770/67476], Loss: 4.8001\n",
      "Epoch [1/2], Step [60780/67476], Loss: 4.8201\n",
      "Epoch [1/2], Step [60790/67476], Loss: 4.5851\n",
      "Epoch [1/2], Step [60800/67476], Loss: 4.4389\n",
      "Epoch [1/2], Step [60810/67476], Loss: 4.4517\n",
      "Epoch [1/2], Step [60820/67476], Loss: 4.5298\n",
      "Epoch [1/2], Step [60830/67476], Loss: 4.5379\n",
      "Epoch [1/2], Step [60840/67476], Loss: 4.6772\n",
      "Epoch [1/2], Step [60850/67476], Loss: 4.7691\n",
      "Epoch [1/2], Step [60860/67476], Loss: 4.3288\n",
      "Epoch [1/2], Step [60870/67476], Loss: 4.5893\n",
      "Epoch [1/2], Step [60880/67476], Loss: 4.5275\n",
      "Epoch [1/2], Step [60890/67476], Loss: 4.5242\n",
      "Epoch [1/2], Step [60900/67476], Loss: 4.5403\n",
      "Epoch [1/2], Step [60910/67476], Loss: 4.5375\n",
      "Epoch [1/2], Step [60920/67476], Loss: 4.4681\n",
      "Epoch [1/2], Step [60930/67476], Loss: 4.5560\n",
      "Epoch [1/2], Step [60940/67476], Loss: 4.4857\n",
      "Epoch [1/2], Step [60950/67476], Loss: 4.3231\n",
      "Epoch [1/2], Step [60960/67476], Loss: 4.6766\n",
      "Epoch [1/2], Step [60970/67476], Loss: 4.4638\n",
      "Epoch [1/2], Step [60980/67476], Loss: 4.6480\n",
      "Epoch [1/2], Step [60990/67476], Loss: 4.6740\n",
      "Epoch [1/2], Step [61000/67476], Loss: 4.5235\n",
      "Epoch [1/2], Step [61010/67476], Loss: 4.8118\n",
      "Epoch [1/2], Step [61020/67476], Loss: 4.6056\n",
      "Epoch [1/2], Step [61030/67476], Loss: 4.4159\n",
      "Epoch [1/2], Step [61040/67476], Loss: 4.7326\n",
      "Epoch [1/2], Step [61050/67476], Loss: 4.6964\n",
      "Epoch [1/2], Step [61060/67476], Loss: 4.5706\n",
      "Epoch [1/2], Step [61070/67476], Loss: 4.4826\n",
      "Epoch [1/2], Step [61080/67476], Loss: 4.4301\n",
      "Epoch [1/2], Step [61090/67476], Loss: 4.6080\n",
      "Epoch [1/2], Step [61100/67476], Loss: 4.5429\n",
      "Epoch [1/2], Step [61110/67476], Loss: 4.7557\n",
      "Epoch [1/2], Step [61120/67476], Loss: 4.6678\n",
      "Epoch [1/2], Step [61130/67476], Loss: 4.5911\n",
      "Epoch [1/2], Step [61140/67476], Loss: 4.6901\n",
      "Epoch [1/2], Step [61150/67476], Loss: 4.5380\n",
      "Epoch [1/2], Step [61160/67476], Loss: 4.5342\n",
      "Epoch [1/2], Step [61170/67476], Loss: 4.4571\n",
      "Epoch [1/2], Step [61180/67476], Loss: 4.3571\n",
      "Epoch [1/2], Step [61190/67476], Loss: 4.5559\n",
      "Epoch [1/2], Step [61200/67476], Loss: 4.6131\n",
      "Epoch [1/2], Step [61210/67476], Loss: 4.5628\n",
      "Epoch [1/2], Step [61220/67476], Loss: 4.4730\n",
      "Epoch [1/2], Step [61230/67476], Loss: 4.3904\n",
      "Epoch [1/2], Step [61240/67476], Loss: 4.6864\n",
      "Epoch [1/2], Step [61250/67476], Loss: 4.5038\n",
      "Epoch [1/2], Step [61260/67476], Loss: 4.3893\n",
      "Epoch [1/2], Step [61270/67476], Loss: 4.5181\n",
      "Epoch [1/2], Step [61280/67476], Loss: 4.5623\n",
      "Epoch [1/2], Step [61290/67476], Loss: 4.4561\n",
      "Epoch [1/2], Step [61300/67476], Loss: 4.6982\n",
      "Epoch [1/2], Step [61310/67476], Loss: 4.6755\n",
      "Epoch [1/2], Step [61320/67476], Loss: 4.5062\n",
      "Epoch [1/2], Step [61330/67476], Loss: 4.6691\n",
      "Epoch [1/2], Step [61340/67476], Loss: 4.6224\n",
      "Epoch [1/2], Step [61350/67476], Loss: 4.5068\n",
      "Epoch [1/2], Step [61360/67476], Loss: 4.5642\n",
      "Epoch [1/2], Step [61370/67476], Loss: 4.5579\n",
      "Epoch [1/2], Step [61380/67476], Loss: 4.4942\n",
      "Epoch [1/2], Step [61390/67476], Loss: 4.5908\n",
      "Epoch [1/2], Step [61400/67476], Loss: 4.3619\n",
      "Epoch [1/2], Step [61410/67476], Loss: 4.4919\n",
      "Epoch [1/2], Step [61420/67476], Loss: 4.6323\n",
      "Epoch [1/2], Step [61430/67476], Loss: 4.6517\n",
      "Epoch [1/2], Step [61440/67476], Loss: 4.4860\n",
      "Epoch [1/2], Step [61450/67476], Loss: 4.4848\n",
      "Epoch [1/2], Step [61460/67476], Loss: 4.7163\n",
      "Epoch [1/2], Step [61470/67476], Loss: 4.6271\n",
      "Epoch [1/2], Step [61480/67476], Loss: 4.6006\n",
      "Epoch [1/2], Step [61490/67476], Loss: 4.5393\n",
      "Epoch [1/2], Step [61500/67476], Loss: 4.5685\n",
      "Epoch [1/2], Step [61510/67476], Loss: 4.4343\n",
      "Epoch [1/2], Step [61520/67476], Loss: 4.4295\n",
      "Epoch [1/2], Step [61530/67476], Loss: 4.5932\n",
      "Epoch [1/2], Step [61540/67476], Loss: 4.6784\n",
      "Epoch [1/2], Step [61550/67476], Loss: 4.5209\n",
      "Epoch [1/2], Step [61560/67476], Loss: 4.6472\n",
      "Epoch [1/2], Step [61570/67476], Loss: 4.5228\n",
      "Epoch [1/2], Step [61580/67476], Loss: 4.6599\n",
      "Epoch [1/2], Step [61590/67476], Loss: 4.6220\n",
      "Epoch [1/2], Step [61600/67476], Loss: 4.4867\n",
      "Epoch [1/2], Step [61610/67476], Loss: 4.5650\n",
      "Epoch [1/2], Step [61620/67476], Loss: 4.4417\n",
      "Epoch [1/2], Step [61630/67476], Loss: 4.6225\n",
      "Epoch [1/2], Step [61640/67476], Loss: 4.8374\n",
      "Epoch [1/2], Step [61650/67476], Loss: 4.4923\n",
      "Epoch [1/2], Step [61660/67476], Loss: 4.5874\n",
      "Epoch [1/2], Step [61670/67476], Loss: 4.6497\n",
      "Epoch [1/2], Step [61680/67476], Loss: 4.6543\n",
      "Epoch [1/2], Step [61690/67476], Loss: 4.5234\n",
      "Epoch [1/2], Step [61700/67476], Loss: 4.5319\n",
      "Epoch [1/2], Step [61710/67476], Loss: 4.4798\n",
      "Epoch [1/2], Step [61720/67476], Loss: 4.7550\n",
      "Epoch [1/2], Step [61730/67476], Loss: 4.7165\n",
      "Epoch [1/2], Step [61740/67476], Loss: 4.6056\n",
      "Epoch [1/2], Step [61750/67476], Loss: 4.7893\n",
      "Epoch [1/2], Step [61760/67476], Loss: 4.3431\n",
      "Epoch [1/2], Step [61770/67476], Loss: 4.4187\n",
      "Epoch [1/2], Step [61780/67476], Loss: 4.4578\n",
      "Epoch [1/2], Step [61790/67476], Loss: 4.5698\n",
      "Epoch [1/2], Step [61800/67476], Loss: 4.6258\n",
      "Epoch [1/2], Step [61810/67476], Loss: 4.5763\n",
      "Epoch [1/2], Step [61820/67476], Loss: 4.6722\n",
      "Epoch [1/2], Step [61830/67476], Loss: 4.5099\n",
      "Epoch [1/2], Step [61840/67476], Loss: 4.4334\n",
      "Epoch [1/2], Step [61850/67476], Loss: 4.6380\n",
      "Epoch [1/2], Step [61860/67476], Loss: 4.5707\n",
      "Epoch [1/2], Step [61870/67476], Loss: 4.5913\n",
      "Epoch [1/2], Step [61880/67476], Loss: 4.7816\n",
      "Epoch [1/2], Step [61890/67476], Loss: 4.7828\n",
      "Epoch [1/2], Step [61900/67476], Loss: 4.4573\n",
      "Epoch [1/2], Step [61910/67476], Loss: 4.6407\n",
      "Epoch [1/2], Step [61920/67476], Loss: 4.6179\n",
      "Epoch [1/2], Step [61930/67476], Loss: 4.3978\n",
      "Epoch [1/2], Step [61940/67476], Loss: 4.5554\n",
      "Epoch [1/2], Step [61950/67476], Loss: 4.4864\n",
      "Epoch [1/2], Step [61960/67476], Loss: 4.6859\n",
      "Epoch [1/2], Step [61970/67476], Loss: 4.5075\n",
      "Epoch [1/2], Step [61980/67476], Loss: 4.6151\n",
      "Epoch [1/2], Step [61990/67476], Loss: 4.5347\n",
      "Epoch [1/2], Step [62000/67476], Loss: 4.2892\n",
      "Epoch [1/2], Step [62010/67476], Loss: 4.4648\n",
      "Epoch [1/2], Step [62020/67476], Loss: 4.6360\n",
      "Epoch [1/2], Step [62030/67476], Loss: 4.5662\n",
      "Epoch [1/2], Step [62040/67476], Loss: 4.5205\n",
      "Epoch [1/2], Step [62050/67476], Loss: 4.5824\n",
      "Epoch [1/2], Step [62060/67476], Loss: 4.6146\n",
      "Epoch [1/2], Step [62070/67476], Loss: 4.6252\n",
      "Epoch [1/2], Step [62080/67476], Loss: 4.3962\n",
      "Epoch [1/2], Step [62090/67476], Loss: 4.5701\n",
      "Epoch [1/2], Step [62100/67476], Loss: 4.7032\n",
      "Epoch [1/2], Step [62110/67476], Loss: 4.5638\n",
      "Epoch [1/2], Step [62120/67476], Loss: 4.5136\n",
      "Epoch [1/2], Step [62130/67476], Loss: 4.7295\n",
      "Epoch [1/2], Step [62140/67476], Loss: 4.5030\n",
      "Epoch [1/2], Step [62150/67476], Loss: 4.4161\n",
      "Epoch [1/2], Step [62160/67476], Loss: 4.3616\n",
      "Epoch [1/2], Step [62170/67476], Loss: 4.6802\n",
      "Epoch [1/2], Step [62180/67476], Loss: 4.5564\n",
      "Epoch [1/2], Step [62190/67476], Loss: 4.5327\n",
      "Epoch [1/2], Step [62200/67476], Loss: 4.5452\n",
      "Epoch [1/2], Step [62210/67476], Loss: 4.4971\n",
      "Epoch [1/2], Step [62220/67476], Loss: 4.6137\n",
      "Epoch [1/2], Step [62230/67476], Loss: 4.5309\n",
      "Epoch [1/2], Step [62240/67476], Loss: 4.3701\n",
      "Epoch [1/2], Step [62250/67476], Loss: 4.4561\n",
      "Epoch [1/2], Step [62260/67476], Loss: 4.6049\n",
      "Epoch [1/2], Step [62270/67476], Loss: 4.4536\n",
      "Epoch [1/2], Step [62280/67476], Loss: 4.3938\n",
      "Epoch [1/2], Step [62290/67476], Loss: 4.5393\n",
      "Epoch [1/2], Step [62300/67476], Loss: 4.5526\n",
      "Epoch [1/2], Step [62310/67476], Loss: 4.5157\n",
      "Epoch [1/2], Step [62320/67476], Loss: 4.5465\n",
      "Epoch [1/2], Step [62330/67476], Loss: 4.5975\n",
      "Epoch [1/2], Step [62340/67476], Loss: 4.5241\n",
      "Epoch [1/2], Step [62350/67476], Loss: 4.4979\n",
      "Epoch [1/2], Step [62360/67476], Loss: 4.5692\n",
      "Epoch [1/2], Step [62370/67476], Loss: 4.5241\n",
      "Epoch [1/2], Step [62380/67476], Loss: 4.5305\n",
      "Epoch [1/2], Step [62390/67476], Loss: 4.3883\n",
      "Epoch [1/2], Step [62400/67476], Loss: 4.6534\n",
      "Epoch [1/2], Step [62410/67476], Loss: 4.5522\n",
      "Epoch [1/2], Step [62420/67476], Loss: 4.3416\n",
      "Epoch [1/2], Step [62430/67476], Loss: 4.5036\n",
      "Epoch [1/2], Step [62440/67476], Loss: 4.5270\n",
      "Epoch [1/2], Step [62450/67476], Loss: 4.6119\n",
      "Epoch [1/2], Step [62460/67476], Loss: 4.7629\n",
      "Epoch [1/2], Step [62470/67476], Loss: 4.6487\n",
      "Epoch [1/2], Step [62480/67476], Loss: 4.5671\n",
      "Epoch [1/2], Step [62490/67476], Loss: 4.6359\n",
      "Epoch [1/2], Step [62500/67476], Loss: 4.6237\n",
      "Epoch [1/2], Step [62510/67476], Loss: 4.6358\n",
      "Epoch [1/2], Step [62520/67476], Loss: 4.6249\n",
      "Epoch [1/2], Step [62530/67476], Loss: 4.4885\n",
      "Epoch [1/2], Step [62540/67476], Loss: 4.7588\n",
      "Epoch [1/2], Step [62550/67476], Loss: 4.6286\n",
      "Epoch [1/2], Step [62560/67476], Loss: 4.5878\n",
      "Epoch [1/2], Step [62570/67476], Loss: 4.6163\n",
      "Epoch [1/2], Step [62580/67476], Loss: 4.5540\n",
      "Epoch [1/2], Step [62590/67476], Loss: 4.5925\n",
      "Epoch [1/2], Step [62600/67476], Loss: 4.7657\n",
      "Epoch [1/2], Step [62610/67476], Loss: 4.6108\n",
      "Epoch [1/2], Step [62620/67476], Loss: 4.6215\n",
      "Epoch [1/2], Step [62630/67476], Loss: 4.5719\n",
      "Epoch [1/2], Step [62640/67476], Loss: 4.5151\n",
      "Epoch [1/2], Step [62650/67476], Loss: 4.6316\n",
      "Epoch [1/2], Step [62660/67476], Loss: 4.6742\n",
      "Epoch [1/2], Step [62670/67476], Loss: 4.5422\n",
      "Epoch [1/2], Step [62680/67476], Loss: 4.6329\n",
      "Epoch [1/2], Step [62690/67476], Loss: 4.3514\n",
      "Epoch [1/2], Step [62700/67476], Loss: 4.5013\n",
      "Epoch [1/2], Step [62710/67476], Loss: 4.5554\n",
      "Epoch [1/2], Step [62720/67476], Loss: 4.3887\n",
      "Epoch [1/2], Step [62730/67476], Loss: 4.5002\n",
      "Epoch [1/2], Step [62740/67476], Loss: 4.5547\n",
      "Epoch [1/2], Step [62750/67476], Loss: 4.7641\n",
      "Epoch [1/2], Step [62760/67476], Loss: 4.4295\n",
      "Epoch [1/2], Step [62770/67476], Loss: 4.6640\n",
      "Epoch [1/2], Step [62780/67476], Loss: 4.6443\n",
      "Epoch [1/2], Step [62790/67476], Loss: 4.5722\n",
      "Epoch [1/2], Step [62800/67476], Loss: 4.6101\n",
      "Epoch [1/2], Step [62810/67476], Loss: 4.5497\n",
      "Epoch [1/2], Step [62820/67476], Loss: 4.7531\n",
      "Epoch [1/2], Step [62830/67476], Loss: 4.6291\n",
      "Epoch [1/2], Step [62840/67476], Loss: 4.6326\n",
      "Epoch [1/2], Step [62850/67476], Loss: 4.5451\n",
      "Epoch [1/2], Step [62860/67476], Loss: 4.5927\n",
      "Epoch [1/2], Step [62870/67476], Loss: 4.4837\n",
      "Epoch [1/2], Step [62880/67476], Loss: 4.4655\n",
      "Epoch [1/2], Step [62890/67476], Loss: 4.6099\n",
      "Epoch [1/2], Step [62900/67476], Loss: 4.6642\n",
      "Epoch [1/2], Step [62910/67476], Loss: 4.7757\n",
      "Epoch [1/2], Step [62920/67476], Loss: 4.4233\n",
      "Epoch [1/2], Step [62930/67476], Loss: 4.6301\n",
      "Epoch [1/2], Step [62940/67476], Loss: 4.6271\n",
      "Epoch [1/2], Step [62950/67476], Loss: 4.6550\n",
      "Epoch [1/2], Step [62960/67476], Loss: 4.5500\n",
      "Epoch [1/2], Step [62970/67476], Loss: 4.7636\n",
      "Epoch [1/2], Step [62980/67476], Loss: 4.8154\n",
      "Epoch [1/2], Step [62990/67476], Loss: 4.6946\n",
      "Epoch [1/2], Step [63000/67476], Loss: 4.5096\n",
      "Epoch [1/2], Step [63010/67476], Loss: 4.6095\n",
      "Epoch [1/2], Step [63020/67476], Loss: 4.5963\n",
      "Epoch [1/2], Step [63030/67476], Loss: 4.6814\n",
      "Epoch [1/2], Step [63040/67476], Loss: 4.7279\n",
      "Epoch [1/2], Step [63050/67476], Loss: 4.5095\n",
      "Epoch [1/2], Step [63060/67476], Loss: 4.5890\n",
      "Epoch [1/2], Step [63070/67476], Loss: 4.7900\n",
      "Epoch [1/2], Step [63080/67476], Loss: 4.5779\n",
      "Epoch [1/2], Step [63090/67476], Loss: 4.6459\n",
      "Epoch [1/2], Step [63100/67476], Loss: 4.7477\n",
      "Epoch [1/2], Step [63110/67476], Loss: 4.5043\n",
      "Epoch [1/2], Step [63120/67476], Loss: 4.7134\n",
      "Epoch [1/2], Step [63130/67476], Loss: 4.7095\n",
      "Epoch [1/2], Step [63140/67476], Loss: 4.6364\n",
      "Epoch [1/2], Step [63150/67476], Loss: 4.4755\n",
      "Epoch [1/2], Step [63160/67476], Loss: 4.7211\n",
      "Epoch [1/2], Step [63170/67476], Loss: 4.5455\n",
      "Epoch [1/2], Step [63180/67476], Loss: 4.4325\n",
      "Epoch [1/2], Step [63190/67476], Loss: 4.5973\n",
      "Epoch [1/2], Step [63200/67476], Loss: 4.5254\n",
      "Epoch [1/2], Step [63210/67476], Loss: 4.7048\n",
      "Epoch [1/2], Step [63220/67476], Loss: 4.6616\n",
      "Epoch [1/2], Step [63230/67476], Loss: 4.8824\n",
      "Epoch [1/2], Step [63240/67476], Loss: 4.4342\n",
      "Epoch [1/2], Step [63250/67476], Loss: 4.3354\n",
      "Epoch [1/2], Step [63260/67476], Loss: 4.6482\n",
      "Epoch [1/2], Step [63270/67476], Loss: 4.5190\n",
      "Epoch [1/2], Step [63280/67476], Loss: 4.5330\n",
      "Epoch [1/2], Step [63290/67476], Loss: 4.6555\n",
      "Epoch [1/2], Step [63300/67476], Loss: 4.6107\n",
      "Epoch [1/2], Step [63310/67476], Loss: 4.5987\n",
      "Epoch [1/2], Step [63320/67476], Loss: 4.7750\n",
      "Epoch [1/2], Step [63330/67476], Loss: 4.4433\n",
      "Epoch [1/2], Step [63340/67476], Loss: 4.3742\n",
      "Epoch [1/2], Step [63350/67476], Loss: 4.2902\n",
      "Epoch [1/2], Step [63360/67476], Loss: 4.5843\n",
      "Epoch [1/2], Step [63370/67476], Loss: 4.6908\n",
      "Epoch [1/2], Step [63380/67476], Loss: 4.4885\n",
      "Epoch [1/2], Step [63390/67476], Loss: 4.4300\n",
      "Epoch [1/2], Step [63400/67476], Loss: 4.6975\n",
      "Epoch [1/2], Step [63410/67476], Loss: 4.5167\n",
      "Epoch [1/2], Step [63420/67476], Loss: 4.4789\n",
      "Epoch [1/2], Step [63430/67476], Loss: 4.4918\n",
      "Epoch [1/2], Step [63440/67476], Loss: 4.6341\n",
      "Epoch [1/2], Step [63450/67476], Loss: 4.5960\n",
      "Epoch [1/2], Step [63460/67476], Loss: 4.5967\n",
      "Epoch [1/2], Step [63470/67476], Loss: 4.6167\n",
      "Epoch [1/2], Step [63480/67476], Loss: 4.3805\n",
      "Epoch [1/2], Step [63490/67476], Loss: 4.6809\n",
      "Epoch [1/2], Step [63500/67476], Loss: 4.5882\n",
      "Epoch [1/2], Step [63510/67476], Loss: 4.4899\n",
      "Epoch [1/2], Step [63520/67476], Loss: 4.7109\n",
      "Epoch [1/2], Step [63530/67476], Loss: 4.6366\n",
      "Epoch [1/2], Step [63540/67476], Loss: 4.5307\n",
      "Epoch [1/2], Step [63550/67476], Loss: 4.6348\n",
      "Epoch [1/2], Step [63560/67476], Loss: 4.6486\n",
      "Epoch [1/2], Step [63570/67476], Loss: 4.6156\n",
      "Epoch [1/2], Step [63580/67476], Loss: 4.5771\n",
      "Epoch [1/2], Step [63590/67476], Loss: 4.6227\n",
      "Epoch [1/2], Step [63600/67476], Loss: 4.4696\n",
      "Epoch [1/2], Step [63610/67476], Loss: 4.6073\n",
      "Epoch [1/2], Step [63620/67476], Loss: 4.6938\n",
      "Epoch [1/2], Step [63630/67476], Loss: 4.5935\n",
      "Epoch [1/2], Step [63640/67476], Loss: 4.6058\n",
      "Epoch [1/2], Step [63650/67476], Loss: 4.7542\n",
      "Epoch [1/2], Step [63660/67476], Loss: 4.7494\n",
      "Epoch [1/2], Step [63670/67476], Loss: 4.5060\n",
      "Epoch [1/2], Step [63680/67476], Loss: 4.5781\n",
      "Epoch [1/2], Step [63690/67476], Loss: 4.5131\n",
      "Epoch [1/2], Step [63700/67476], Loss: 4.4615\n",
      "Epoch [1/2], Step [63710/67476], Loss: 4.8861\n",
      "Epoch [1/2], Step [63720/67476], Loss: 4.5667\n",
      "Epoch [1/2], Step [63730/67476], Loss: 4.6885\n",
      "Epoch [1/2], Step [63740/67476], Loss: 4.8547\n",
      "Epoch [1/2], Step [63750/67476], Loss: 4.5271\n",
      "Epoch [1/2], Step [63760/67476], Loss: 4.3529\n",
      "Epoch [1/2], Step [63770/67476], Loss: 4.6556\n",
      "Epoch [1/2], Step [63780/67476], Loss: 4.6392\n",
      "Epoch [1/2], Step [63790/67476], Loss: 4.5011\n",
      "Epoch [1/2], Step [63800/67476], Loss: 4.5218\n",
      "Epoch [1/2], Step [63810/67476], Loss: 4.4804\n",
      "Epoch [1/2], Step [63820/67476], Loss: 4.6121\n",
      "Epoch [1/2], Step [63830/67476], Loss: 4.1795\n",
      "Epoch [1/2], Step [63840/67476], Loss: 4.5725\n",
      "Epoch [1/2], Step [63850/67476], Loss: 4.7241\n",
      "Epoch [1/2], Step [63860/67476], Loss: 4.5466\n",
      "Epoch [1/2], Step [63870/67476], Loss: 4.5026\n",
      "Epoch [1/2], Step [63880/67476], Loss: 4.7483\n",
      "Epoch [1/2], Step [63890/67476], Loss: 4.4871\n",
      "Epoch [1/2], Step [63900/67476], Loss: 4.3859\n",
      "Epoch [1/2], Step [63910/67476], Loss: 4.5729\n",
      "Epoch [1/2], Step [63920/67476], Loss: 4.4787\n",
      "Epoch [1/2], Step [63930/67476], Loss: 4.5820\n",
      "Epoch [1/2], Step [63940/67476], Loss: 4.5688\n",
      "Epoch [1/2], Step [63950/67476], Loss: 4.7180\n",
      "Epoch [1/2], Step [63960/67476], Loss: 4.4127\n",
      "Epoch [1/2], Step [63970/67476], Loss: 4.7354\n",
      "Epoch [1/2], Step [63980/67476], Loss: 4.5822\n",
      "Epoch [1/2], Step [63990/67476], Loss: 4.4994\n",
      "Epoch [1/2], Step [64000/67476], Loss: 4.5182\n",
      "Epoch [1/2], Step [64010/67476], Loss: 4.3956\n",
      "Epoch [1/2], Step [64020/67476], Loss: 4.6669\n",
      "Epoch [1/2], Step [64030/67476], Loss: 4.5201\n",
      "Epoch [1/2], Step [64040/67476], Loss: 4.5861\n",
      "Epoch [1/2], Step [64050/67476], Loss: 4.4335\n",
      "Epoch [1/2], Step [64060/67476], Loss: 4.6479\n",
      "Epoch [1/2], Step [64070/67476], Loss: 4.4122\n",
      "Epoch [1/2], Step [64080/67476], Loss: 4.3218\n",
      "Epoch [1/2], Step [64090/67476], Loss: 4.7259\n",
      "Epoch [1/2], Step [64100/67476], Loss: 4.8264\n",
      "Epoch [1/2], Step [64110/67476], Loss: 4.4945\n",
      "Epoch [1/2], Step [64120/67476], Loss: 4.7407\n",
      "Epoch [1/2], Step [64130/67476], Loss: 4.5277\n",
      "Epoch [1/2], Step [64140/67476], Loss: 4.6879\n",
      "Epoch [1/2], Step [64150/67476], Loss: 4.4680\n",
      "Epoch [1/2], Step [64160/67476], Loss: 4.7610\n",
      "Epoch [1/2], Step [64170/67476], Loss: 4.4906\n",
      "Epoch [1/2], Step [64180/67476], Loss: 4.4919\n",
      "Epoch [1/2], Step [64190/67476], Loss: 4.4908\n",
      "Epoch [1/2], Step [64200/67476], Loss: 4.5599\n",
      "Epoch [1/2], Step [64210/67476], Loss: 4.6191\n",
      "Epoch [1/2], Step [64220/67476], Loss: 4.5853\n",
      "Epoch [1/2], Step [64230/67476], Loss: 4.3726\n",
      "Epoch [1/2], Step [64240/67476], Loss: 4.6484\n",
      "Epoch [1/2], Step [64250/67476], Loss: 4.5355\n",
      "Epoch [1/2], Step [64260/67476], Loss: 4.4145\n",
      "Epoch [1/2], Step [64270/67476], Loss: 4.5203\n",
      "Epoch [1/2], Step [64280/67476], Loss: 4.5617\n",
      "Epoch [1/2], Step [64290/67476], Loss: 4.6892\n",
      "Epoch [1/2], Step [64300/67476], Loss: 4.4453\n",
      "Epoch [1/2], Step [64310/67476], Loss: 4.6429\n",
      "Epoch [1/2], Step [64320/67476], Loss: 4.5682\n",
      "Epoch [1/2], Step [64330/67476], Loss: 4.6098\n",
      "Epoch [1/2], Step [64340/67476], Loss: 4.5656\n",
      "Epoch [1/2], Step [64350/67476], Loss: 4.7119\n",
      "Epoch [1/2], Step [64360/67476], Loss: 4.6420\n",
      "Epoch [1/2], Step [64370/67476], Loss: 4.6467\n",
      "Epoch [1/2], Step [64380/67476], Loss: 4.3328\n",
      "Epoch [1/2], Step [64390/67476], Loss: 4.6355\n",
      "Epoch [1/2], Step [64400/67476], Loss: 4.7084\n",
      "Epoch [1/2], Step [64410/67476], Loss: 4.4848\n",
      "Epoch [1/2], Step [64420/67476], Loss: 4.2571\n",
      "Epoch [1/2], Step [64430/67476], Loss: 4.5195\n",
      "Epoch [1/2], Step [64440/67476], Loss: 4.4951\n",
      "Epoch [1/2], Step [64450/67476], Loss: 4.5938\n",
      "Epoch [1/2], Step [64460/67476], Loss: 4.6160\n",
      "Epoch [1/2], Step [64470/67476], Loss: 4.4161\n",
      "Epoch [1/2], Step [64480/67476], Loss: 4.7005\n",
      "Epoch [1/2], Step [64490/67476], Loss: 4.6052\n",
      "Epoch [1/2], Step [64500/67476], Loss: 4.7725\n",
      "Epoch [1/2], Step [64510/67476], Loss: 4.6588\n",
      "Epoch [1/2], Step [64520/67476], Loss: 4.6808\n",
      "Epoch [1/2], Step [64530/67476], Loss: 4.5974\n",
      "Epoch [1/2], Step [64540/67476], Loss: 4.6342\n",
      "Epoch [1/2], Step [64550/67476], Loss: 4.6009\n",
      "Epoch [1/2], Step [64560/67476], Loss: 4.5132\n",
      "Epoch [1/2], Step [64570/67476], Loss: 4.5356\n",
      "Epoch [1/2], Step [64580/67476], Loss: 4.5830\n",
      "Epoch [1/2], Step [64590/67476], Loss: 4.3667\n",
      "Epoch [1/2], Step [64600/67476], Loss: 4.5719\n",
      "Epoch [1/2], Step [64610/67476], Loss: 4.4682\n",
      "Epoch [1/2], Step [64620/67476], Loss: 4.4790\n",
      "Epoch [1/2], Step [64630/67476], Loss: 4.5892\n",
      "Epoch [1/2], Step [64640/67476], Loss: 4.6727\n",
      "Epoch [1/2], Step [64650/67476], Loss: 4.5331\n",
      "Epoch [1/2], Step [64660/67476], Loss: 4.5869\n",
      "Epoch [1/2], Step [64670/67476], Loss: 4.6190\n",
      "Epoch [1/2], Step [64680/67476], Loss: 4.6180\n",
      "Epoch [1/2], Step [64690/67476], Loss: 4.7147\n",
      "Epoch [1/2], Step [64700/67476], Loss: 4.4548\n",
      "Epoch [1/2], Step [64710/67476], Loss: 4.4975\n",
      "Epoch [1/2], Step [64720/67476], Loss: 4.6786\n",
      "Epoch [1/2], Step [64730/67476], Loss: 4.3911\n",
      "Epoch [1/2], Step [64740/67476], Loss: 4.6619\n",
      "Epoch [1/2], Step [64750/67476], Loss: 4.6684\n",
      "Epoch [1/2], Step [64760/67476], Loss: 4.6408\n",
      "Epoch [1/2], Step [64770/67476], Loss: 4.6392\n",
      "Epoch [1/2], Step [64780/67476], Loss: 4.6313\n",
      "Epoch [1/2], Step [64790/67476], Loss: 4.4181\n",
      "Epoch [1/2], Step [64800/67476], Loss: 4.5159\n",
      "Epoch [1/2], Step [64810/67476], Loss: 4.5177\n",
      "Epoch [1/2], Step [64820/67476], Loss: 4.3843\n",
      "Epoch [1/2], Step [64830/67476], Loss: 4.6162\n",
      "Epoch [1/2], Step [64840/67476], Loss: 4.5299\n",
      "Epoch [1/2], Step [64850/67476], Loss: 4.3772\n",
      "Epoch [1/2], Step [64860/67476], Loss: 4.5489\n",
      "Epoch [1/2], Step [64870/67476], Loss: 4.5239\n",
      "Epoch [1/2], Step [64880/67476], Loss: 4.5701\n",
      "Epoch [1/2], Step [64890/67476], Loss: 4.5340\n",
      "Epoch [1/2], Step [64900/67476], Loss: 4.4558\n",
      "Epoch [1/2], Step [64910/67476], Loss: 4.6989\n",
      "Epoch [1/2], Step [64920/67476], Loss: 4.5548\n",
      "Epoch [1/2], Step [64930/67476], Loss: 4.6472\n",
      "Epoch [1/2], Step [64940/67476], Loss: 4.5489\n",
      "Epoch [1/2], Step [64950/67476], Loss: 4.7958\n",
      "Epoch [1/2], Step [64960/67476], Loss: 4.4851\n",
      "Epoch [1/2], Step [64970/67476], Loss: 4.5918\n",
      "Epoch [1/2], Step [64980/67476], Loss: 4.6501\n",
      "Epoch [1/2], Step [64990/67476], Loss: 4.5430\n",
      "Epoch [1/2], Step [65000/67476], Loss: 4.4324\n",
      "Epoch [1/2], Step [65010/67476], Loss: 4.4293\n",
      "Epoch [1/2], Step [65020/67476], Loss: 4.5518\n",
      "Epoch [1/2], Step [65030/67476], Loss: 4.6210\n",
      "Epoch [1/2], Step [65040/67476], Loss: 4.5527\n",
      "Epoch [1/2], Step [65050/67476], Loss: 4.4934\n",
      "Epoch [1/2], Step [65060/67476], Loss: 4.6201\n",
      "Epoch [1/2], Step [65070/67476], Loss: 4.3536\n",
      "Epoch [1/2], Step [65080/67476], Loss: 4.5819\n",
      "Epoch [1/2], Step [65090/67476], Loss: 4.5955\n",
      "Epoch [1/2], Step [65100/67476], Loss: 4.5142\n",
      "Epoch [1/2], Step [65110/67476], Loss: 4.3827\n",
      "Epoch [1/2], Step [65120/67476], Loss: 4.5216\n",
      "Epoch [1/2], Step [65130/67476], Loss: 4.4653\n",
      "Epoch [1/2], Step [65140/67476], Loss: 4.4157\n",
      "Epoch [1/2], Step [65150/67476], Loss: 4.6186\n",
      "Epoch [1/2], Step [65160/67476], Loss: 4.5633\n",
      "Epoch [1/2], Step [65170/67476], Loss: 4.4413\n",
      "Epoch [1/2], Step [65180/67476], Loss: 4.4807\n",
      "Epoch [1/2], Step [65190/67476], Loss: 4.3706\n",
      "Epoch [1/2], Step [65200/67476], Loss: 4.4875\n",
      "Epoch [1/2], Step [65210/67476], Loss: 4.6461\n",
      "Epoch [1/2], Step [65220/67476], Loss: 4.4423\n",
      "Epoch [1/2], Step [65230/67476], Loss: 4.4625\n",
      "Epoch [1/2], Step [65240/67476], Loss: 4.4763\n",
      "Epoch [1/2], Step [65250/67476], Loss: 4.4529\n",
      "Epoch [1/2], Step [65260/67476], Loss: 4.5794\n",
      "Epoch [1/2], Step [65270/67476], Loss: 4.5485\n",
      "Epoch [1/2], Step [65280/67476], Loss: 4.5950\n",
      "Epoch [1/2], Step [65290/67476], Loss: 4.6136\n",
      "Epoch [1/2], Step [65300/67476], Loss: 4.5203\n",
      "Epoch [1/2], Step [65310/67476], Loss: 4.7054\n",
      "Epoch [1/2], Step [65320/67476], Loss: 4.5663\n",
      "Epoch [1/2], Step [65330/67476], Loss: 4.6083\n",
      "Epoch [1/2], Step [65340/67476], Loss: 4.5653\n",
      "Epoch [1/2], Step [65350/67476], Loss: 4.5261\n",
      "Epoch [1/2], Step [65360/67476], Loss: 4.5551\n",
      "Epoch [1/2], Step [65370/67476], Loss: 4.6814\n",
      "Epoch [1/2], Step [65380/67476], Loss: 4.5347\n",
      "Epoch [1/2], Step [65390/67476], Loss: 4.6479\n",
      "Epoch [1/2], Step [65400/67476], Loss: 4.5102\n",
      "Epoch [1/2], Step [65410/67476], Loss: 4.4871\n",
      "Epoch [1/2], Step [65420/67476], Loss: 4.4227\n",
      "Epoch [1/2], Step [65430/67476], Loss: 4.4147\n",
      "Epoch [1/2], Step [65440/67476], Loss: 4.5262\n",
      "Epoch [1/2], Step [65450/67476], Loss: 4.7753\n",
      "Epoch [1/2], Step [65460/67476], Loss: 4.7746\n",
      "Epoch [1/2], Step [65470/67476], Loss: 4.6632\n",
      "Epoch [1/2], Step [65480/67476], Loss: 4.6479\n",
      "Epoch [1/2], Step [65490/67476], Loss: 4.5545\n",
      "Epoch [1/2], Step [65500/67476], Loss: 4.6244\n",
      "Epoch [1/2], Step [65510/67476], Loss: 4.4334\n",
      "Epoch [1/2], Step [65520/67476], Loss: 4.6602\n",
      "Epoch [1/2], Step [65530/67476], Loss: 4.5108\n",
      "Epoch [1/2], Step [65540/67476], Loss: 4.6364\n",
      "Epoch [1/2], Step [65550/67476], Loss: 4.6348\n",
      "Epoch [1/2], Step [65560/67476], Loss: 4.4550\n",
      "Epoch [1/2], Step [65570/67476], Loss: 4.5812\n",
      "Epoch [1/2], Step [65580/67476], Loss: 4.5956\n",
      "Epoch [1/2], Step [65590/67476], Loss: 4.2638\n",
      "Epoch [1/2], Step [65600/67476], Loss: 4.6587\n",
      "Epoch [1/2], Step [65610/67476], Loss: 4.4429\n",
      "Epoch [1/2], Step [65620/67476], Loss: 4.4244\n",
      "Epoch [1/2], Step [65630/67476], Loss: 4.4431\n",
      "Epoch [1/2], Step [65640/67476], Loss: 4.7789\n",
      "Epoch [1/2], Step [65650/67476], Loss: 4.7326\n",
      "Epoch [1/2], Step [65660/67476], Loss: 4.5580\n",
      "Epoch [1/2], Step [65670/67476], Loss: 4.5252\n",
      "Epoch [1/2], Step [65680/67476], Loss: 4.4954\n",
      "Epoch [1/2], Step [65690/67476], Loss: 4.6163\n",
      "Epoch [1/2], Step [65700/67476], Loss: 4.5532\n",
      "Epoch [1/2], Step [65710/67476], Loss: 4.5977\n",
      "Epoch [1/2], Step [65720/67476], Loss: 4.3987\n",
      "Epoch [1/2], Step [65730/67476], Loss: 4.6076\n",
      "Epoch [1/2], Step [65740/67476], Loss: 4.4810\n",
      "Epoch [1/2], Step [65750/67476], Loss: 4.4555\n",
      "Epoch [1/2], Step [65760/67476], Loss: 4.4760\n",
      "Epoch [1/2], Step [65770/67476], Loss: 4.5173\n",
      "Epoch [1/2], Step [65780/67476], Loss: 4.6484\n",
      "Epoch [1/2], Step [65790/67476], Loss: 4.6023\n",
      "Epoch [1/2], Step [65800/67476], Loss: 4.4512\n",
      "Epoch [1/2], Step [65810/67476], Loss: 4.4300\n",
      "Epoch [1/2], Step [65820/67476], Loss: 4.5446\n",
      "Epoch [1/2], Step [65830/67476], Loss: 4.4322\n",
      "Epoch [1/2], Step [65840/67476], Loss: 4.4833\n",
      "Epoch [1/2], Step [65850/67476], Loss: 4.5260\n",
      "Epoch [1/2], Step [65860/67476], Loss: 4.5472\n",
      "Epoch [1/2], Step [65870/67476], Loss: 4.5330\n",
      "Epoch [1/2], Step [65880/67476], Loss: 4.6261\n",
      "Epoch [1/2], Step [65890/67476], Loss: 4.7765\n",
      "Epoch [1/2], Step [65900/67476], Loss: 4.5911\n",
      "Epoch [1/2], Step [65910/67476], Loss: 4.4400\n",
      "Epoch [1/2], Step [65920/67476], Loss: 4.6297\n",
      "Epoch [1/2], Step [65930/67476], Loss: 4.6321\n",
      "Epoch [1/2], Step [65940/67476], Loss: 4.6393\n",
      "Epoch [1/2], Step [65950/67476], Loss: 4.5404\n",
      "Epoch [1/2], Step [65960/67476], Loss: 4.8400\n",
      "Epoch [1/2], Step [65970/67476], Loss: 4.8176\n",
      "Epoch [1/2], Step [65980/67476], Loss: 4.6715\n",
      "Epoch [1/2], Step [65990/67476], Loss: 4.7166\n",
      "Epoch [1/2], Step [66000/67476], Loss: 4.3748\n",
      "Epoch [1/2], Step [66010/67476], Loss: 4.5241\n",
      "Epoch [1/2], Step [66020/67476], Loss: 4.6981\n",
      "Epoch [1/2], Step [66030/67476], Loss: 4.5925\n",
      "Epoch [1/2], Step [66040/67476], Loss: 4.5517\n",
      "Epoch [1/2], Step [66050/67476], Loss: 4.4915\n",
      "Epoch [1/2], Step [66060/67476], Loss: 4.6589\n",
      "Epoch [1/2], Step [66070/67476], Loss: 4.6007\n",
      "Epoch [1/2], Step [66080/67476], Loss: 4.5332\n",
      "Epoch [1/2], Step [66090/67476], Loss: 4.5407\n",
      "Epoch [1/2], Step [66100/67476], Loss: 4.6398\n",
      "Epoch [1/2], Step [66110/67476], Loss: 4.7746\n",
      "Epoch [1/2], Step [66120/67476], Loss: 4.6614\n",
      "Epoch [1/2], Step [66130/67476], Loss: 4.7651\n",
      "Epoch [1/2], Step [66140/67476], Loss: 4.4176\n",
      "Epoch [1/2], Step [66150/67476], Loss: 4.5561\n",
      "Epoch [1/2], Step [66160/67476], Loss: 4.6460\n",
      "Epoch [1/2], Step [66170/67476], Loss: 4.5105\n",
      "Epoch [1/2], Step [66180/67476], Loss: 4.8233\n",
      "Epoch [1/2], Step [66190/67476], Loss: 4.4592\n",
      "Epoch [1/2], Step [66200/67476], Loss: 4.6766\n",
      "Epoch [1/2], Step [66210/67476], Loss: 4.5512\n",
      "Epoch [1/2], Step [66220/67476], Loss: 4.5651\n",
      "Epoch [1/2], Step [66230/67476], Loss: 4.6885\n",
      "Epoch [1/2], Step [66240/67476], Loss: 4.3762\n",
      "Epoch [1/2], Step [66250/67476], Loss: 4.6284\n",
      "Epoch [1/2], Step [66260/67476], Loss: 4.6263\n",
      "Epoch [1/2], Step [66270/67476], Loss: 4.4706\n",
      "Epoch [1/2], Step [66280/67476], Loss: 4.5573\n",
      "Epoch [1/2], Step [66290/67476], Loss: 4.4425\n",
      "Epoch [1/2], Step [66300/67476], Loss: 4.5557\n",
      "Epoch [1/2], Step [66310/67476], Loss: 4.6950\n",
      "Epoch [1/2], Step [66320/67476], Loss: 4.5113\n",
      "Epoch [1/2], Step [66330/67476], Loss: 4.6229\n",
      "Epoch [1/2], Step [66340/67476], Loss: 4.3952\n",
      "Epoch [1/2], Step [66350/67476], Loss: 4.7851\n",
      "Epoch [1/2], Step [66360/67476], Loss: 4.3895\n",
      "Epoch [1/2], Step [66370/67476], Loss: 4.5064\n",
      "Epoch [1/2], Step [66380/67476], Loss: 4.7182\n",
      "Epoch [1/2], Step [66390/67476], Loss: 4.2911\n",
      "Epoch [1/2], Step [66400/67476], Loss: 4.4800\n",
      "Epoch [1/2], Step [66410/67476], Loss: 4.7747\n",
      "Epoch [1/2], Step [66420/67476], Loss: 4.5302\n",
      "Epoch [1/2], Step [66430/67476], Loss: 4.7111\n",
      "Epoch [1/2], Step [66440/67476], Loss: 4.5317\n",
      "Epoch [1/2], Step [66450/67476], Loss: 4.5012\n",
      "Epoch [1/2], Step [66460/67476], Loss: 4.6209\n",
      "Epoch [1/2], Step [66470/67476], Loss: 4.6244\n",
      "Epoch [1/2], Step [66480/67476], Loss: 4.5152\n",
      "Epoch [1/2], Step [66490/67476], Loss: 4.4310\n",
      "Epoch [1/2], Step [66500/67476], Loss: 4.6001\n",
      "Epoch [1/2], Step [66510/67476], Loss: 4.5773\n",
      "Epoch [1/2], Step [66520/67476], Loss: 4.4360\n",
      "Epoch [1/2], Step [66530/67476], Loss: 4.4763\n",
      "Epoch [1/2], Step [66540/67476], Loss: 4.5541\n",
      "Epoch [1/2], Step [66550/67476], Loss: 4.4069\n",
      "Epoch [1/2], Step [66560/67476], Loss: 4.6560\n",
      "Epoch [1/2], Step [66570/67476], Loss: 4.7335\n",
      "Epoch [1/2], Step [66580/67476], Loss: 4.3538\n",
      "Epoch [1/2], Step [66590/67476], Loss: 4.5089\n",
      "Epoch [1/2], Step [66600/67476], Loss: 4.6044\n",
      "Epoch [1/2], Step [66610/67476], Loss: 4.6525\n",
      "Epoch [1/2], Step [66620/67476], Loss: 4.4504\n",
      "Epoch [1/2], Step [66630/67476], Loss: 4.3821\n",
      "Epoch [1/2], Step [66640/67476], Loss: 4.5810\n",
      "Epoch [1/2], Step [66650/67476], Loss: 4.6374\n",
      "Epoch [1/2], Step [66660/67476], Loss: 4.3494\n",
      "Epoch [1/2], Step [66670/67476], Loss: 4.7073\n",
      "Epoch [1/2], Step [66680/67476], Loss: 4.7555\n",
      "Epoch [1/2], Step [66690/67476], Loss: 4.8007\n",
      "Epoch [1/2], Step [66700/67476], Loss: 4.6213\n",
      "Epoch [1/2], Step [66710/67476], Loss: 4.3901\n",
      "Epoch [1/2], Step [66720/67476], Loss: 4.6613\n",
      "Epoch [1/2], Step [66730/67476], Loss: 4.6022\n",
      "Epoch [1/2], Step [66740/67476], Loss: 4.4520\n",
      "Epoch [1/2], Step [66750/67476], Loss: 4.5894\n",
      "Epoch [1/2], Step [66760/67476], Loss: 4.3678\n",
      "Epoch [1/2], Step [66770/67476], Loss: 4.6175\n",
      "Epoch [1/2], Step [66780/67476], Loss: 4.6802\n",
      "Epoch [1/2], Step [66790/67476], Loss: 4.7585\n",
      "Epoch [1/2], Step [66800/67476], Loss: 4.6328\n",
      "Epoch [1/2], Step [66810/67476], Loss: 4.4478\n",
      "Epoch [1/2], Step [66820/67476], Loss: 4.6129\n",
      "Epoch [1/2], Step [66830/67476], Loss: 4.6400\n",
      "Epoch [1/2], Step [66840/67476], Loss: 4.5727\n",
      "Epoch [1/2], Step [66850/67476], Loss: 4.5451\n",
      "Epoch [1/2], Step [66860/67476], Loss: 4.4174\n",
      "Epoch [1/2], Step [66870/67476], Loss: 4.3799\n",
      "Epoch [1/2], Step [66880/67476], Loss: 4.4337\n",
      "Epoch [1/2], Step [66890/67476], Loss: 4.4614\n",
      "Epoch [1/2], Step [66900/67476], Loss: 4.5674\n",
      "Epoch [1/2], Step [66910/67476], Loss: 4.5911\n",
      "Epoch [1/2], Step [66920/67476], Loss: 4.3161\n",
      "Epoch [1/2], Step [66930/67476], Loss: 4.5162\n",
      "Epoch [1/2], Step [66940/67476], Loss: 4.6719\n",
      "Epoch [1/2], Step [66950/67476], Loss: 4.4068\n",
      "Epoch [1/2], Step [66960/67476], Loss: 4.5232\n",
      "Epoch [1/2], Step [66970/67476], Loss: 4.5692\n",
      "Epoch [1/2], Step [66980/67476], Loss: 4.6421\n",
      "Epoch [1/2], Step [66990/67476], Loss: 4.4516\n",
      "Epoch [1/2], Step [67000/67476], Loss: 4.6208\n",
      "Epoch [1/2], Step [67010/67476], Loss: 4.5951\n",
      "Epoch [1/2], Step [67020/67476], Loss: 4.6938\n",
      "Epoch [1/2], Step [67030/67476], Loss: 4.4950\n",
      "Epoch [1/2], Step [67040/67476], Loss: 4.4419\n",
      "Epoch [1/2], Step [67050/67476], Loss: 4.5246\n",
      "Epoch [1/2], Step [67060/67476], Loss: 4.3143\n",
      "Epoch [1/2], Step [67070/67476], Loss: 4.3819\n",
      "Epoch [1/2], Step [67080/67476], Loss: 4.3682\n",
      "Epoch [1/2], Step [67090/67476], Loss: 4.7611\n",
      "Epoch [1/2], Step [67100/67476], Loss: 4.6029\n",
      "Epoch [1/2], Step [67110/67476], Loss: 4.4582\n",
      "Epoch [1/2], Step [67120/67476], Loss: 4.4960\n",
      "Epoch [1/2], Step [67130/67476], Loss: 4.6553\n",
      "Epoch [1/2], Step [67140/67476], Loss: 4.6479\n",
      "Epoch [1/2], Step [67150/67476], Loss: 4.4498\n",
      "Epoch [1/2], Step [67160/67476], Loss: 4.5666\n",
      "Epoch [1/2], Step [67170/67476], Loss: 4.6672\n",
      "Epoch [1/2], Step [67180/67476], Loss: 4.5159\n",
      "Epoch [1/2], Step [67190/67476], Loss: 4.5608\n",
      "Epoch [1/2], Step [67200/67476], Loss: 4.7647\n",
      "Epoch [1/2], Step [67210/67476], Loss: 4.4799\n",
      "Epoch [1/2], Step [67220/67476], Loss: 4.4098\n",
      "Epoch [1/2], Step [67230/67476], Loss: 4.5584\n",
      "Epoch [1/2], Step [67240/67476], Loss: 4.5710\n",
      "Epoch [1/2], Step [67250/67476], Loss: 4.6779\n",
      "Epoch [1/2], Step [67260/67476], Loss: 4.4235\n",
      "Epoch [1/2], Step [67270/67476], Loss: 4.5058\n",
      "Epoch [1/2], Step [67280/67476], Loss: 4.5857\n",
      "Epoch [1/2], Step [67290/67476], Loss: 4.6125\n",
      "Epoch [1/2], Step [67300/67476], Loss: 4.7424\n",
      "Epoch [1/2], Step [67310/67476], Loss: 4.5980\n",
      "Epoch [1/2], Step [67320/67476], Loss: 4.5309\n",
      "Epoch [1/2], Step [67330/67476], Loss: 4.5396\n",
      "Epoch [1/2], Step [67340/67476], Loss: 4.6991\n",
      "Epoch [1/2], Step [67350/67476], Loss: 4.6930\n",
      "Epoch [1/2], Step [67360/67476], Loss: 4.4908\n",
      "Epoch [1/2], Step [67370/67476], Loss: 4.5176\n",
      "Epoch [1/2], Step [67380/67476], Loss: 4.6678\n",
      "Epoch [1/2], Step [67390/67476], Loss: 4.5096\n",
      "Epoch [1/2], Step [67400/67476], Loss: 4.6874\n",
      "Epoch [1/2], Step [67410/67476], Loss: 4.4984\n",
      "Epoch [1/2], Step [67420/67476], Loss: 4.6186\n",
      "Epoch [1/2], Step [67430/67476], Loss: 4.9358\n",
      "Epoch [1/2], Step [67440/67476], Loss: 4.5261\n",
      "Epoch [1/2], Step [67450/67476], Loss: 4.4081\n",
      "Epoch [1/2], Step [67460/67476], Loss: 4.7355\n",
      "Epoch [1/2], Step [67470/67476], Loss: 4.3281\n",
      "Epoch [1/2] Average Loss: 4.5828, Perplexity: 97.79\n",
      "Epoch [2/2], Step [0/67476], Loss: 4.5352\n",
      "Epoch [2/2], Step [10/67476], Loss: 4.5197\n",
      "Epoch [2/2], Step [20/67476], Loss: 4.4851\n",
      "Epoch [2/2], Step [30/67476], Loss: 4.6395\n",
      "Epoch [2/2], Step [40/67476], Loss: 4.6870\n",
      "Epoch [2/2], Step [50/67476], Loss: 4.5380\n",
      "Epoch [2/2], Step [60/67476], Loss: 4.4627\n",
      "Epoch [2/2], Step [70/67476], Loss: 4.6185\n",
      "Epoch [2/2], Step [80/67476], Loss: 4.3380\n",
      "Epoch [2/2], Step [90/67476], Loss: 4.6096\n",
      "Epoch [2/2], Step [100/67476], Loss: 4.6310\n",
      "Epoch [2/2], Step [110/67476], Loss: 4.6770\n",
      "Epoch [2/2], Step [120/67476], Loss: 4.6974\n",
      "Epoch [2/2], Step [130/67476], Loss: 4.6845\n",
      "Epoch [2/2], Step [140/67476], Loss: 4.4474\n",
      "Epoch [2/2], Step [150/67476], Loss: 4.6794\n",
      "Epoch [2/2], Step [160/67476], Loss: 4.6248\n",
      "Epoch [2/2], Step [170/67476], Loss: 4.4373\n",
      "Epoch [2/2], Step [180/67476], Loss: 4.6346\n",
      "Epoch [2/2], Step [190/67476], Loss: 4.5991\n",
      "Epoch [2/2], Step [200/67476], Loss: 4.6764\n",
      "Epoch [2/2], Step [210/67476], Loss: 4.4190\n",
      "Epoch [2/2], Step [220/67476], Loss: 4.5392\n",
      "Epoch [2/2], Step [230/67476], Loss: 4.6029\n",
      "Epoch [2/2], Step [240/67476], Loss: 4.4111\n",
      "Epoch [2/2], Step [250/67476], Loss: 4.5858\n",
      "Epoch [2/2], Step [260/67476], Loss: 4.7117\n",
      "Epoch [2/2], Step [270/67476], Loss: 4.4244\n",
      "Epoch [2/2], Step [280/67476], Loss: 4.4418\n",
      "Epoch [2/2], Step [290/67476], Loss: 4.6567\n",
      "Epoch [2/2], Step [300/67476], Loss: 4.6189\n",
      "Epoch [2/2], Step [310/67476], Loss: 4.5929\n",
      "Epoch [2/2], Step [320/67476], Loss: 4.5887\n",
      "Epoch [2/2], Step [330/67476], Loss: 4.5176\n",
      "Epoch [2/2], Step [340/67476], Loss: 4.6175\n",
      "Epoch [2/2], Step [350/67476], Loss: 4.5897\n",
      "Epoch [2/2], Step [360/67476], Loss: 4.4297\n",
      "Epoch [2/2], Step [370/67476], Loss: 4.4410\n",
      "Epoch [2/2], Step [380/67476], Loss: 4.6278\n",
      "Epoch [2/2], Step [390/67476], Loss: 4.4011\n",
      "Epoch [2/2], Step [400/67476], Loss: 4.7063\n",
      "Epoch [2/2], Step [410/67476], Loss: 4.5335\n",
      "Epoch [2/2], Step [420/67476], Loss: 4.4656\n",
      "Epoch [2/2], Step [430/67476], Loss: 4.4636\n",
      "Epoch [2/2], Step [440/67476], Loss: 4.5581\n",
      "Epoch [2/2], Step [450/67476], Loss: 4.5553\n",
      "Epoch [2/2], Step [460/67476], Loss: 4.4560\n",
      "Epoch [2/2], Step [470/67476], Loss: 4.3801\n",
      "Epoch [2/2], Step [480/67476], Loss: 4.4493\n",
      "Epoch [2/2], Step [490/67476], Loss: 4.5256\n",
      "Epoch [2/2], Step [500/67476], Loss: 4.7434\n",
      "Epoch [2/2], Step [510/67476], Loss: 4.6438\n",
      "Epoch [2/2], Step [520/67476], Loss: 4.5015\n",
      "Epoch [2/2], Step [530/67476], Loss: 4.5701\n",
      "Epoch [2/2], Step [540/67476], Loss: 4.4595\n",
      "Epoch [2/2], Step [550/67476], Loss: 4.5970\n",
      "Epoch [2/2], Step [560/67476], Loss: 4.6135\n",
      "Epoch [2/2], Step [570/67476], Loss: 4.6353\n",
      "Epoch [2/2], Step [580/67476], Loss: 4.5524\n",
      "Epoch [2/2], Step [590/67476], Loss: 4.5665\n",
      "Epoch [2/2], Step [600/67476], Loss: 4.4976\n",
      "Epoch [2/2], Step [610/67476], Loss: 4.5164\n",
      "Epoch [2/2], Step [620/67476], Loss: 4.3662\n",
      "Epoch [2/2], Step [630/67476], Loss: 4.3545\n",
      "Epoch [2/2], Step [640/67476], Loss: 4.6794\n",
      "Epoch [2/2], Step [650/67476], Loss: 4.6110\n",
      "Epoch [2/2], Step [660/67476], Loss: 4.4777\n",
      "Epoch [2/2], Step [670/67476], Loss: 4.4494\n",
      "Epoch [2/2], Step [680/67476], Loss: 4.5574\n",
      "Epoch [2/2], Step [690/67476], Loss: 4.4707\n",
      "Epoch [2/2], Step [700/67476], Loss: 4.6488\n",
      "Epoch [2/2], Step [710/67476], Loss: 4.6025\n",
      "Epoch [2/2], Step [720/67476], Loss: 4.6266\n",
      "Epoch [2/2], Step [730/67476], Loss: 4.7136\n",
      "Epoch [2/2], Step [740/67476], Loss: 4.4813\n",
      "Epoch [2/2], Step [750/67476], Loss: 4.6157\n",
      "Epoch [2/2], Step [760/67476], Loss: 4.2165\n",
      "Epoch [2/2], Step [770/67476], Loss: 4.4342\n",
      "Epoch [2/2], Step [780/67476], Loss: 4.7153\n",
      "Epoch [2/2], Step [790/67476], Loss: 4.6119\n",
      "Epoch [2/2], Step [800/67476], Loss: 4.6058\n",
      "Epoch [2/2], Step [810/67476], Loss: 4.9347\n",
      "Epoch [2/2], Step [820/67476], Loss: 4.8225\n",
      "Epoch [2/2], Step [830/67476], Loss: 4.8327\n",
      "Epoch [2/2], Step [840/67476], Loss: 4.5534\n",
      "Epoch [2/2], Step [850/67476], Loss: 4.5304\n",
      "Epoch [2/2], Step [860/67476], Loss: 4.4915\n",
      "Epoch [2/2], Step [870/67476], Loss: 4.4761\n",
      "Epoch [2/2], Step [880/67476], Loss: 4.3729\n",
      "Epoch [2/2], Step [890/67476], Loss: 4.6315\n",
      "Epoch [2/2], Step [900/67476], Loss: 4.6385\n",
      "Epoch [2/2], Step [910/67476], Loss: 4.3835\n",
      "Epoch [2/2], Step [920/67476], Loss: 4.5675\n",
      "Epoch [2/2], Step [930/67476], Loss: 4.5577\n",
      "Epoch [2/2], Step [940/67476], Loss: 4.5329\n",
      "Epoch [2/2], Step [950/67476], Loss: 4.5369\n",
      "Epoch [2/2], Step [960/67476], Loss: 4.6785\n",
      "Epoch [2/2], Step [970/67476], Loss: 4.2457\n",
      "Epoch [2/2], Step [980/67476], Loss: 4.6137\n",
      "Epoch [2/2], Step [990/67476], Loss: 4.4359\n",
      "Epoch [2/2], Step [1000/67476], Loss: 4.3677\n",
      "Epoch [2/2], Step [1010/67476], Loss: 4.5944\n",
      "Epoch [2/2], Step [1020/67476], Loss: 4.5574\n",
      "Epoch [2/2], Step [1030/67476], Loss: 4.5445\n",
      "Epoch [2/2], Step [1040/67476], Loss: 4.7641\n",
      "Epoch [2/2], Step [1050/67476], Loss: 4.5284\n",
      "Epoch [2/2], Step [1060/67476], Loss: 4.5894\n",
      "Epoch [2/2], Step [1070/67476], Loss: 4.5850\n",
      "Epoch [2/2], Step [1080/67476], Loss: 4.5741\n",
      "Epoch [2/2], Step [1090/67476], Loss: 4.4404\n",
      "Epoch [2/2], Step [1100/67476], Loss: 4.4884\n",
      "Epoch [2/2], Step [1110/67476], Loss: 4.4416\n",
      "Epoch [2/2], Step [1120/67476], Loss: 4.5139\n",
      "Epoch [2/2], Step [1130/67476], Loss: 4.5040\n",
      "Epoch [2/2], Step [1140/67476], Loss: 4.7036\n",
      "Epoch [2/2], Step [1150/67476], Loss: 4.6171\n",
      "Epoch [2/2], Step [1160/67476], Loss: 4.7177\n",
      "Epoch [2/2], Step [1170/67476], Loss: 4.5445\n",
      "Epoch [2/2], Step [1180/67476], Loss: 4.5931\n",
      "Epoch [2/2], Step [1190/67476], Loss: 4.5703\n",
      "Epoch [2/2], Step [1200/67476], Loss: 4.8261\n",
      "Epoch [2/2], Step [1210/67476], Loss: 4.5238\n",
      "Epoch [2/2], Step [1220/67476], Loss: 4.5130\n",
      "Epoch [2/2], Step [1230/67476], Loss: 4.7305\n",
      "Epoch [2/2], Step [1240/67476], Loss: 4.6427\n",
      "Epoch [2/2], Step [1250/67476], Loss: 4.5468\n",
      "Epoch [2/2], Step [1260/67476], Loss: 4.5280\n",
      "Epoch [2/2], Step [1270/67476], Loss: 4.6524\n",
      "Epoch [2/2], Step [1280/67476], Loss: 4.6538\n",
      "Epoch [2/2], Step [1290/67476], Loss: 4.3789\n",
      "Epoch [2/2], Step [1300/67476], Loss: 4.5616\n",
      "Epoch [2/2], Step [1310/67476], Loss: 4.4782\n",
      "Epoch [2/2], Step [1320/67476], Loss: 4.7223\n",
      "Epoch [2/2], Step [1330/67476], Loss: 4.7511\n",
      "Epoch [2/2], Step [1340/67476], Loss: 4.5670\n",
      "Epoch [2/2], Step [1350/67476], Loss: 4.6296\n",
      "Epoch [2/2], Step [1360/67476], Loss: 4.4883\n",
      "Epoch [2/2], Step [1370/67476], Loss: 4.3236\n",
      "Epoch [2/2], Step [1380/67476], Loss: 4.6352\n",
      "Epoch [2/2], Step [1390/67476], Loss: 4.4330\n",
      "Epoch [2/2], Step [1400/67476], Loss: 4.6070\n",
      "Epoch [2/2], Step [1410/67476], Loss: 4.5176\n",
      "Epoch [2/2], Step [1420/67476], Loss: 4.5291\n",
      "Epoch [2/2], Step [1430/67476], Loss: 4.4214\n",
      "Epoch [2/2], Step [1440/67476], Loss: 4.4258\n",
      "Epoch [2/2], Step [1450/67476], Loss: 4.5953\n",
      "Epoch [2/2], Step [1460/67476], Loss: 4.5037\n",
      "Epoch [2/2], Step [1470/67476], Loss: 4.5884\n",
      "Epoch [2/2], Step [1480/67476], Loss: 4.4501\n",
      "Epoch [2/2], Step [1490/67476], Loss: 4.7641\n",
      "Epoch [2/2], Step [1500/67476], Loss: 4.5013\n",
      "Epoch [2/2], Step [1510/67476], Loss: 4.6139\n",
      "Epoch [2/2], Step [1520/67476], Loss: 4.5806\n",
      "Epoch [2/2], Step [1530/67476], Loss: 4.4850\n",
      "Epoch [2/2], Step [1540/67476], Loss: 4.7024\n",
      "Epoch [2/2], Step [1550/67476], Loss: 4.4063\n",
      "Epoch [2/2], Step [1560/67476], Loss: 4.4527\n",
      "Epoch [2/2], Step [1570/67476], Loss: 4.5864\n",
      "Epoch [2/2], Step [1580/67476], Loss: 4.8122\n",
      "Epoch [2/2], Step [1590/67476], Loss: 4.5484\n",
      "Epoch [2/2], Step [1600/67476], Loss: 4.5192\n",
      "Epoch [2/2], Step [1610/67476], Loss: 4.4678\n",
      "Epoch [2/2], Step [1620/67476], Loss: 4.5302\n",
      "Epoch [2/2], Step [1630/67476], Loss: 4.7417\n",
      "Epoch [2/2], Step [1640/67476], Loss: 4.5872\n",
      "Epoch [2/2], Step [1650/67476], Loss: 4.6569\n",
      "Epoch [2/2], Step [1660/67476], Loss: 4.7088\n",
      "Epoch [2/2], Step [1670/67476], Loss: 4.6727\n",
      "Epoch [2/2], Step [1680/67476], Loss: 4.5276\n",
      "Epoch [2/2], Step [1690/67476], Loss: 4.5042\n",
      "Epoch [2/2], Step [1700/67476], Loss: 4.4828\n",
      "Epoch [2/2], Step [1710/67476], Loss: 4.3494\n",
      "Epoch [2/2], Step [1720/67476], Loss: 4.6828\n",
      "Epoch [2/2], Step [1730/67476], Loss: 4.7878\n",
      "Epoch [2/2], Step [1740/67476], Loss: 4.5629\n",
      "Epoch [2/2], Step [1750/67476], Loss: 4.3920\n",
      "Epoch [2/2], Step [1760/67476], Loss: 4.4252\n",
      "Epoch [2/2], Step [1770/67476], Loss: 4.7111\n",
      "Epoch [2/2], Step [1780/67476], Loss: 4.5422\n",
      "Epoch [2/2], Step [1790/67476], Loss: 4.6988\n",
      "Epoch [2/2], Step [1800/67476], Loss: 4.6177\n",
      "Epoch [2/2], Step [1810/67476], Loss: 4.5155\n",
      "Epoch [2/2], Step [1820/67476], Loss: 4.5288\n",
      "Epoch [2/2], Step [1830/67476], Loss: 4.4324\n",
      "Epoch [2/2], Step [1840/67476], Loss: 4.8291\n",
      "Epoch [2/2], Step [1850/67476], Loss: 4.3468\n",
      "Epoch [2/2], Step [1860/67476], Loss: 4.5539\n",
      "Epoch [2/2], Step [1870/67476], Loss: 4.4554\n",
      "Epoch [2/2], Step [1880/67476], Loss: 4.6246\n",
      "Epoch [2/2], Step [1890/67476], Loss: 4.5988\n",
      "Epoch [2/2], Step [1900/67476], Loss: 4.3948\n",
      "Epoch [2/2], Step [1910/67476], Loss: 4.6546\n",
      "Epoch [2/2], Step [1920/67476], Loss: 4.7092\n",
      "Epoch [2/2], Step [1930/67476], Loss: 4.5754\n",
      "Epoch [2/2], Step [1940/67476], Loss: 4.4619\n",
      "Epoch [2/2], Step [1950/67476], Loss: 4.4838\n",
      "Epoch [2/2], Step [1960/67476], Loss: 4.4937\n",
      "Epoch [2/2], Step [1970/67476], Loss: 4.6174\n",
      "Epoch [2/2], Step [1980/67476], Loss: 4.8169\n",
      "Epoch [2/2], Step [1990/67476], Loss: 4.5373\n",
      "Epoch [2/2], Step [2000/67476], Loss: 4.4357\n",
      "Epoch [2/2], Step [2010/67476], Loss: 4.5776\n",
      "Epoch [2/2], Step [2020/67476], Loss: 4.4615\n",
      "Epoch [2/2], Step [2030/67476], Loss: 4.6616\n",
      "Epoch [2/2], Step [2040/67476], Loss: 4.3512\n",
      "Epoch [2/2], Step [2050/67476], Loss: 4.3723\n",
      "Epoch [2/2], Step [2060/67476], Loss: 4.7129\n",
      "Epoch [2/2], Step [2070/67476], Loss: 4.6180\n",
      "Epoch [2/2], Step [2080/67476], Loss: 4.5637\n",
      "Epoch [2/2], Step [2090/67476], Loss: 4.6405\n",
      "Epoch [2/2], Step [2100/67476], Loss: 4.5609\n",
      "Epoch [2/2], Step [2110/67476], Loss: 4.5302\n",
      "Epoch [2/2], Step [2120/67476], Loss: 4.4830\n",
      "Epoch [2/2], Step [2130/67476], Loss: 4.4058\n",
      "Epoch [2/2], Step [2140/67476], Loss: 4.6329\n",
      "Epoch [2/2], Step [2150/67476], Loss: 4.5532\n",
      "Epoch [2/2], Step [2160/67476], Loss: 4.4958\n",
      "Epoch [2/2], Step [2170/67476], Loss: 4.5312\n",
      "Epoch [2/2], Step [2180/67476], Loss: 4.3603\n",
      "Epoch [2/2], Step [2190/67476], Loss: 4.5163\n",
      "Epoch [2/2], Step [2200/67476], Loss: 4.4987\n",
      "Epoch [2/2], Step [2210/67476], Loss: 4.5237\n",
      "Epoch [2/2], Step [2220/67476], Loss: 4.7303\n",
      "Epoch [2/2], Step [2230/67476], Loss: 4.5329\n",
      "Epoch [2/2], Step [2240/67476], Loss: 4.5503\n",
      "Epoch [2/2], Step [2250/67476], Loss: 4.6274\n",
      "Epoch [2/2], Step [2260/67476], Loss: 4.6526\n",
      "Epoch [2/2], Step [2270/67476], Loss: 4.6142\n",
      "Epoch [2/2], Step [2280/67476], Loss: 4.6009\n",
      "Epoch [2/2], Step [2290/67476], Loss: 4.6509\n",
      "Epoch [2/2], Step [2300/67476], Loss: 4.5534\n",
      "Epoch [2/2], Step [2310/67476], Loss: 4.5505\n",
      "Epoch [2/2], Step [2320/67476], Loss: 4.6264\n",
      "Epoch [2/2], Step [2330/67476], Loss: 4.4854\n",
      "Epoch [2/2], Step [2340/67476], Loss: 4.5918\n",
      "Epoch [2/2], Step [2350/67476], Loss: 4.5849\n",
      "Epoch [2/2], Step [2360/67476], Loss: 4.6690\n",
      "Epoch [2/2], Step [2370/67476], Loss: 4.5230\n",
      "Epoch [2/2], Step [2380/67476], Loss: 4.5057\n",
      "Epoch [2/2], Step [2390/67476], Loss: 4.5339\n",
      "Epoch [2/2], Step [2400/67476], Loss: 4.5985\n",
      "Epoch [2/2], Step [2410/67476], Loss: 4.4528\n",
      "Epoch [2/2], Step [2420/67476], Loss: 4.6747\n",
      "Epoch [2/2], Step [2430/67476], Loss: 4.4556\n",
      "Epoch [2/2], Step [2440/67476], Loss: 4.5430\n",
      "Epoch [2/2], Step [2450/67476], Loss: 4.7282\n",
      "Epoch [2/2], Step [2460/67476], Loss: 4.5321\n",
      "Epoch [2/2], Step [2470/67476], Loss: 4.3982\n",
      "Epoch [2/2], Step [2480/67476], Loss: 4.5459\n",
      "Epoch [2/2], Step [2490/67476], Loss: 4.5028\n",
      "Epoch [2/2], Step [2500/67476], Loss: 4.6299\n",
      "Epoch [2/2], Step [2510/67476], Loss: 4.5511\n",
      "Epoch [2/2], Step [2520/67476], Loss: 4.4647\n",
      "Epoch [2/2], Step [2530/67476], Loss: 4.3287\n",
      "Epoch [2/2], Step [2540/67476], Loss: 4.5415\n",
      "Epoch [2/2], Step [2550/67476], Loss: 4.7167\n",
      "Epoch [2/2], Step [2560/67476], Loss: 4.5558\n",
      "Epoch [2/2], Step [2570/67476], Loss: 4.6421\n",
      "Epoch [2/2], Step [2580/67476], Loss: 4.6127\n",
      "Epoch [2/2], Step [2590/67476], Loss: 4.3897\n",
      "Epoch [2/2], Step [2600/67476], Loss: 4.5368\n",
      "Epoch [2/2], Step [2610/67476], Loss: 4.5906\n",
      "Epoch [2/2], Step [2620/67476], Loss: 4.3274\n",
      "Epoch [2/2], Step [2630/67476], Loss: 4.7116\n",
      "Epoch [2/2], Step [2640/67476], Loss: 4.7210\n",
      "Epoch [2/2], Step [2650/67476], Loss: 4.5660\n",
      "Epoch [2/2], Step [2660/67476], Loss: 4.7506\n",
      "Epoch [2/2], Step [2670/67476], Loss: 4.4130\n",
      "Epoch [2/2], Step [2680/67476], Loss: 4.6309\n",
      "Epoch [2/2], Step [2690/67476], Loss: 4.8629\n",
      "Epoch [2/2], Step [2700/67476], Loss: 4.5817\n",
      "Epoch [2/2], Step [2710/67476], Loss: 4.6558\n",
      "Epoch [2/2], Step [2720/67476], Loss: 4.3703\n",
      "Epoch [2/2], Step [2730/67476], Loss: 4.5999\n",
      "Epoch [2/2], Step [2740/67476], Loss: 4.7218\n",
      "Epoch [2/2], Step [2750/67476], Loss: 4.4172\n",
      "Epoch [2/2], Step [2760/67476], Loss: 4.5097\n",
      "Epoch [2/2], Step [2770/67476], Loss: 4.4226\n",
      "Epoch [2/2], Step [2780/67476], Loss: 4.5572\n",
      "Epoch [2/2], Step [2790/67476], Loss: 4.6700\n",
      "Epoch [2/2], Step [2800/67476], Loss: 4.4305\n",
      "Epoch [2/2], Step [2810/67476], Loss: 4.6312\n",
      "Epoch [2/2], Step [2820/67476], Loss: 4.5270\n",
      "Epoch [2/2], Step [2830/67476], Loss: 4.6405\n",
      "Epoch [2/2], Step [2840/67476], Loss: 4.4110\n",
      "Epoch [2/2], Step [2850/67476], Loss: 4.4234\n",
      "Epoch [2/2], Step [2860/67476], Loss: 4.6829\n",
      "Epoch [2/2], Step [2870/67476], Loss: 4.5327\n",
      "Epoch [2/2], Step [2880/67476], Loss: 4.2943\n",
      "Epoch [2/2], Step [2890/67476], Loss: 4.5037\n",
      "Epoch [2/2], Step [2900/67476], Loss: 4.3891\n",
      "Epoch [2/2], Step [2910/67476], Loss: 4.5917\n",
      "Epoch [2/2], Step [2920/67476], Loss: 4.7405\n",
      "Epoch [2/2], Step [2930/67476], Loss: 4.3270\n",
      "Epoch [2/2], Step [2940/67476], Loss: 4.5914\n",
      "Epoch [2/2], Step [2950/67476], Loss: 4.5015\n",
      "Epoch [2/2], Step [2960/67476], Loss: 4.5699\n",
      "Epoch [2/2], Step [2970/67476], Loss: 4.7122\n",
      "Epoch [2/2], Step [2980/67476], Loss: 4.5746\n",
      "Epoch [2/2], Step [2990/67476], Loss: 4.4448\n",
      "Epoch [2/2], Step [3000/67476], Loss: 4.5428\n",
      "Epoch [2/2], Step [3010/67476], Loss: 4.2784\n",
      "Epoch [2/2], Step [3020/67476], Loss: 4.5612\n",
      "Epoch [2/2], Step [3030/67476], Loss: 4.7106\n",
      "Epoch [2/2], Step [3040/67476], Loss: 4.8120\n",
      "Epoch [2/2], Step [3050/67476], Loss: 4.5800\n",
      "Epoch [2/2], Step [3060/67476], Loss: 4.4864\n",
      "Epoch [2/2], Step [3070/67476], Loss: 4.5048\n",
      "Epoch [2/2], Step [3080/67476], Loss: 4.4465\n",
      "Epoch [2/2], Step [3090/67476], Loss: 4.4825\n",
      "Epoch [2/2], Step [3100/67476], Loss: 4.5149\n",
      "Epoch [2/2], Step [3110/67476], Loss: 4.5674\n",
      "Epoch [2/2], Step [3120/67476], Loss: 4.4386\n",
      "Epoch [2/2], Step [3130/67476], Loss: 4.4014\n",
      "Epoch [2/2], Step [3140/67476], Loss: 4.4158\n",
      "Epoch [2/2], Step [3150/67476], Loss: 4.2956\n",
      "Epoch [2/2], Step [3160/67476], Loss: 4.6333\n",
      "Epoch [2/2], Step [3170/67476], Loss: 4.5848\n",
      "Epoch [2/2], Step [3180/67476], Loss: 4.7416\n",
      "Epoch [2/2], Step [3190/67476], Loss: 4.8039\n",
      "Epoch [2/2], Step [3200/67476], Loss: 4.8330\n",
      "Epoch [2/2], Step [3210/67476], Loss: 4.4024\n",
      "Epoch [2/2], Step [3220/67476], Loss: 4.6631\n",
      "Epoch [2/2], Step [3230/67476], Loss: 4.3259\n",
      "Epoch [2/2], Step [3240/67476], Loss: 4.6244\n",
      "Epoch [2/2], Step [3250/67476], Loss: 4.5672\n",
      "Epoch [2/2], Step [3260/67476], Loss: 4.5208\n",
      "Epoch [2/2], Step [3270/67476], Loss: 4.7054\n",
      "Epoch [2/2], Step [3280/67476], Loss: 4.6142\n",
      "Epoch [2/2], Step [3290/67476], Loss: 4.7026\n",
      "Epoch [2/2], Step [3300/67476], Loss: 4.4898\n",
      "Epoch [2/2], Step [3310/67476], Loss: 4.5820\n",
      "Epoch [2/2], Step [3320/67476], Loss: 4.6619\n",
      "Epoch [2/2], Step [3330/67476], Loss: 4.6632\n",
      "Epoch [2/2], Step [3340/67476], Loss: 4.6524\n",
      "Epoch [2/2], Step [3350/67476], Loss: 4.5484\n",
      "Epoch [2/2], Step [3360/67476], Loss: 4.6700\n",
      "Epoch [2/2], Step [3370/67476], Loss: 4.4642\n",
      "Epoch [2/2], Step [3380/67476], Loss: 4.5943\n",
      "Epoch [2/2], Step [3390/67476], Loss: 4.6811\n",
      "Epoch [2/2], Step [3400/67476], Loss: 4.3388\n",
      "Epoch [2/2], Step [3410/67476], Loss: 4.4174\n",
      "Epoch [2/2], Step [3420/67476], Loss: 4.5932\n",
      "Epoch [2/2], Step [3430/67476], Loss: 4.6350\n",
      "Epoch [2/2], Step [3440/67476], Loss: 4.7208\n",
      "Epoch [2/2], Step [3450/67476], Loss: 4.5807\n",
      "Epoch [2/2], Step [3460/67476], Loss: 4.3112\n",
      "Epoch [2/2], Step [3470/67476], Loss: 4.6588\n",
      "Epoch [2/2], Step [3480/67476], Loss: 4.5227\n",
      "Epoch [2/2], Step [3490/67476], Loss: 4.6318\n",
      "Epoch [2/2], Step [3500/67476], Loss: 4.5673\n",
      "Epoch [2/2], Step [3510/67476], Loss: 4.6595\n",
      "Epoch [2/2], Step [3520/67476], Loss: 4.7182\n",
      "Epoch [2/2], Step [3530/67476], Loss: 4.5865\n",
      "Epoch [2/2], Step [3540/67476], Loss: 5.0228\n",
      "Epoch [2/2], Step [3550/67476], Loss: 4.3084\n",
      "Epoch [2/2], Step [3560/67476], Loss: 4.4831\n",
      "Epoch [2/2], Step [3570/67476], Loss: 4.4066\n",
      "Epoch [2/2], Step [3580/67476], Loss: 4.7571\n",
      "Epoch [2/2], Step [3590/67476], Loss: 4.5454\n",
      "Epoch [2/2], Step [3600/67476], Loss: 4.6487\n",
      "Epoch [2/2], Step [3610/67476], Loss: 4.7260\n",
      "Epoch [2/2], Step [3620/67476], Loss: 4.5819\n",
      "Epoch [2/2], Step [3630/67476], Loss: 4.4592\n",
      "Epoch [2/2], Step [3640/67476], Loss: 4.7443\n",
      "Epoch [2/2], Step [3650/67476], Loss: 4.6021\n",
      "Epoch [2/2], Step [3660/67476], Loss: 4.6753\n",
      "Epoch [2/2], Step [3670/67476], Loss: 4.3456\n",
      "Epoch [2/2], Step [3680/67476], Loss: 4.5198\n",
      "Epoch [2/2], Step [3690/67476], Loss: 4.5074\n",
      "Epoch [2/2], Step [3700/67476], Loss: 4.4925\n",
      "Epoch [2/2], Step [3710/67476], Loss: 4.4136\n",
      "Epoch [2/2], Step [3720/67476], Loss: 4.7236\n",
      "Epoch [2/2], Step [3730/67476], Loss: 4.6225\n",
      "Epoch [2/2], Step [3740/67476], Loss: 4.6414\n",
      "Epoch [2/2], Step [3750/67476], Loss: 4.6103\n",
      "Epoch [2/2], Step [3760/67476], Loss: 4.6618\n",
      "Epoch [2/2], Step [3770/67476], Loss: 4.5109\n",
      "Epoch [2/2], Step [3780/67476], Loss: 4.4697\n",
      "Epoch [2/2], Step [3790/67476], Loss: 4.5324\n",
      "Epoch [2/2], Step [3800/67476], Loss: 4.3624\n",
      "Epoch [2/2], Step [3810/67476], Loss: 4.5127\n",
      "Epoch [2/2], Step [3820/67476], Loss: 4.5566\n",
      "Epoch [2/2], Step [3830/67476], Loss: 4.4356\n",
      "Epoch [2/2], Step [3840/67476], Loss: 4.7807\n",
      "Epoch [2/2], Step [3850/67476], Loss: 4.3777\n",
      "Epoch [2/2], Step [3860/67476], Loss: 4.6736\n",
      "Epoch [2/2], Step [3870/67476], Loss: 4.5511\n",
      "Epoch [2/2], Step [3880/67476], Loss: 4.6325\n",
      "Epoch [2/2], Step [3890/67476], Loss: 4.4446\n",
      "Epoch [2/2], Step [3900/67476], Loss: 4.5837\n",
      "Epoch [2/2], Step [3910/67476], Loss: 4.5375\n",
      "Epoch [2/2], Step [3920/67476], Loss: 4.4211\n",
      "Epoch [2/2], Step [3930/67476], Loss: 4.6421\n",
      "Epoch [2/2], Step [3940/67476], Loss: 4.6168\n",
      "Epoch [2/2], Step [3950/67476], Loss: 4.6426\n",
      "Epoch [2/2], Step [3960/67476], Loss: 4.4839\n",
      "Epoch [2/2], Step [3970/67476], Loss: 4.6801\n",
      "Epoch [2/2], Step [3980/67476], Loss: 4.5571\n",
      "Epoch [2/2], Step [3990/67476], Loss: 4.5392\n",
      "Epoch [2/2], Step [4000/67476], Loss: 4.5725\n",
      "Epoch [2/2], Step [4010/67476], Loss: 4.4816\n",
      "Epoch [2/2], Step [4020/67476], Loss: 4.7180\n",
      "Epoch [2/2], Step [4030/67476], Loss: 4.5052\n",
      "Epoch [2/2], Step [4040/67476], Loss: 4.3985\n",
      "Epoch [2/2], Step [4050/67476], Loss: 4.8975\n",
      "Epoch [2/2], Step [4060/67476], Loss: 4.5024\n",
      "Epoch [2/2], Step [4070/67476], Loss: 4.8295\n",
      "Epoch [2/2], Step [4080/67476], Loss: 4.5158\n",
      "Epoch [2/2], Step [4090/67476], Loss: 4.5591\n",
      "Epoch [2/2], Step [4100/67476], Loss: 4.5665\n",
      "Epoch [2/2], Step [4110/67476], Loss: 4.4423\n",
      "Epoch [2/2], Step [4120/67476], Loss: 4.5335\n",
      "Epoch [2/2], Step [4130/67476], Loss: 4.4727\n",
      "Epoch [2/2], Step [4140/67476], Loss: 4.7150\n",
      "Epoch [2/2], Step [4150/67476], Loss: 4.4451\n",
      "Epoch [2/2], Step [4160/67476], Loss: 4.4366\n",
      "Epoch [2/2], Step [4170/67476], Loss: 4.6874\n",
      "Epoch [2/2], Step [4180/67476], Loss: 4.6685\n",
      "Epoch [2/2], Step [4190/67476], Loss: 4.5893\n",
      "Epoch [2/2], Step [4200/67476], Loss: 4.4871\n",
      "Epoch [2/2], Step [4210/67476], Loss: 4.5004\n",
      "Epoch [2/2], Step [4220/67476], Loss: 4.4248\n",
      "Epoch [2/2], Step [4230/67476], Loss: 4.5357\n",
      "Epoch [2/2], Step [4240/67476], Loss: 4.5666\n",
      "Epoch [2/2], Step [4250/67476], Loss: 4.3299\n",
      "Epoch [2/2], Step [4260/67476], Loss: 4.4608\n",
      "Epoch [2/2], Step [4270/67476], Loss: 4.6127\n",
      "Epoch [2/2], Step [4280/67476], Loss: 4.5648\n",
      "Epoch [2/2], Step [4290/67476], Loss: 4.6277\n",
      "Epoch [2/2], Step [4300/67476], Loss: 4.6117\n",
      "Epoch [2/2], Step [4310/67476], Loss: 4.6006\n",
      "Epoch [2/2], Step [4320/67476], Loss: 4.5002\n",
      "Epoch [2/2], Step [4330/67476], Loss: 4.4945\n",
      "Epoch [2/2], Step [4340/67476], Loss: 4.5325\n",
      "Epoch [2/2], Step [4350/67476], Loss: 4.6924\n",
      "Epoch [2/2], Step [4360/67476], Loss: 4.5556\n",
      "Epoch [2/2], Step [4370/67476], Loss: 4.6281\n",
      "Epoch [2/2], Step [4380/67476], Loss: 4.8126\n",
      "Epoch [2/2], Step [4390/67476], Loss: 4.4908\n",
      "Epoch [2/2], Step [4400/67476], Loss: 4.4393\n",
      "Epoch [2/2], Step [4410/67476], Loss: 4.3137\n",
      "Epoch [2/2], Step [4420/67476], Loss: 4.4961\n",
      "Epoch [2/2], Step [4430/67476], Loss: 4.4330\n",
      "Epoch [2/2], Step [4440/67476], Loss: 4.4942\n",
      "Epoch [2/2], Step [4450/67476], Loss: 4.6729\n",
      "Epoch [2/2], Step [4460/67476], Loss: 4.6911\n",
      "Epoch [2/2], Step [4470/67476], Loss: 4.2926\n",
      "Epoch [2/2], Step [4480/67476], Loss: 4.8185\n",
      "Epoch [2/2], Step [4490/67476], Loss: 4.4424\n",
      "Epoch [2/2], Step [4500/67476], Loss: 4.5418\n",
      "Epoch [2/2], Step [4510/67476], Loss: 4.5089\n",
      "Epoch [2/2], Step [4520/67476], Loss: 4.6261\n",
      "Epoch [2/2], Step [4530/67476], Loss: 4.5886\n",
      "Epoch [2/2], Step [4540/67476], Loss: 4.6010\n",
      "Epoch [2/2], Step [4550/67476], Loss: 4.4287\n",
      "Epoch [2/2], Step [4560/67476], Loss: 4.5272\n",
      "Epoch [2/2], Step [4570/67476], Loss: 4.4949\n",
      "Epoch [2/2], Step [4580/67476], Loss: 4.5300\n",
      "Epoch [2/2], Step [4590/67476], Loss: 4.6915\n",
      "Epoch [2/2], Step [4600/67476], Loss: 4.7367\n",
      "Epoch [2/2], Step [4610/67476], Loss: 4.4798\n",
      "Epoch [2/2], Step [4620/67476], Loss: 4.3296\n",
      "Epoch [2/2], Step [4630/67476], Loss: 4.6499\n",
      "Epoch [2/2], Step [4640/67476], Loss: 4.4748\n",
      "Epoch [2/2], Step [4650/67476], Loss: 4.6358\n",
      "Epoch [2/2], Step [4660/67476], Loss: 4.5158\n",
      "Epoch [2/2], Step [4670/67476], Loss: 4.5136\n",
      "Epoch [2/2], Step [4680/67476], Loss: 4.5832\n",
      "Epoch [2/2], Step [4690/67476], Loss: 4.5430\n",
      "Epoch [2/2], Step [4700/67476], Loss: 4.6884\n",
      "Epoch [2/2], Step [4710/67476], Loss: 4.5785\n",
      "Epoch [2/2], Step [4720/67476], Loss: 4.5915\n",
      "Epoch [2/2], Step [4730/67476], Loss: 4.5657\n",
      "Epoch [2/2], Step [4740/67476], Loss: 4.6063\n",
      "Epoch [2/2], Step [4750/67476], Loss: 4.6231\n",
      "Epoch [2/2], Step [4760/67476], Loss: 4.5960\n",
      "Epoch [2/2], Step [4770/67476], Loss: 4.8505\n",
      "Epoch [2/2], Step [4780/67476], Loss: 4.5908\n",
      "Epoch [2/2], Step [4790/67476], Loss: 4.4742\n",
      "Epoch [2/2], Step [4800/67476], Loss: 4.6858\n",
      "Epoch [2/2], Step [4810/67476], Loss: 4.7072\n",
      "Epoch [2/2], Step [4820/67476], Loss: 4.5823\n",
      "Epoch [2/2], Step [4830/67476], Loss: 4.3971\n",
      "Epoch [2/2], Step [4840/67476], Loss: 4.4021\n",
      "Epoch [2/2], Step [4850/67476], Loss: 4.7643\n",
      "Epoch [2/2], Step [4860/67476], Loss: 4.4879\n",
      "Epoch [2/2], Step [4870/67476], Loss: 4.7320\n",
      "Epoch [2/2], Step [4880/67476], Loss: 4.6288\n",
      "Epoch [2/2], Step [4890/67476], Loss: 4.3113\n",
      "Epoch [2/2], Step [4900/67476], Loss: 4.3992\n",
      "Epoch [2/2], Step [4910/67476], Loss: 4.5951\n",
      "Epoch [2/2], Step [4920/67476], Loss: 4.7083\n",
      "Epoch [2/2], Step [4930/67476], Loss: 4.5340\n",
      "Epoch [2/2], Step [4940/67476], Loss: 4.5815\n",
      "Epoch [2/2], Step [4950/67476], Loss: 4.5254\n",
      "Epoch [2/2], Step [4960/67476], Loss: 4.7343\n",
      "Epoch [2/2], Step [4970/67476], Loss: 4.7219\n",
      "Epoch [2/2], Step [4980/67476], Loss: 4.5968\n",
      "Epoch [2/2], Step [4990/67476], Loss: 4.7783\n",
      "Epoch [2/2], Step [5000/67476], Loss: 4.6935\n",
      "Epoch [2/2], Step [5010/67476], Loss: 4.5928\n",
      "Epoch [2/2], Step [5020/67476], Loss: 4.4302\n",
      "Epoch [2/2], Step [5030/67476], Loss: 4.7463\n",
      "Epoch [2/2], Step [5040/67476], Loss: 4.5662\n",
      "Epoch [2/2], Step [5050/67476], Loss: 4.5146\n",
      "Epoch [2/2], Step [5060/67476], Loss: 4.4367\n",
      "Epoch [2/2], Step [5070/67476], Loss: 4.6078\n",
      "Epoch [2/2], Step [5080/67476], Loss: 4.5440\n",
      "Epoch [2/2], Step [5090/67476], Loss: 4.5836\n",
      "Epoch [2/2], Step [5100/67476], Loss: 4.4477\n",
      "Epoch [2/2], Step [5110/67476], Loss: 4.5495\n",
      "Epoch [2/2], Step [5120/67476], Loss: 4.5465\n",
      "Epoch [2/2], Step [5130/67476], Loss: 4.6300\n",
      "Epoch [2/2], Step [5140/67476], Loss: 4.5438\n",
      "Epoch [2/2], Step [5150/67476], Loss: 4.5315\n",
      "Epoch [2/2], Step [5160/67476], Loss: 4.4524\n",
      "Epoch [2/2], Step [5170/67476], Loss: 4.5096\n",
      "Epoch [2/2], Step [5180/67476], Loss: 4.4951\n",
      "Epoch [2/2], Step [5190/67476], Loss: 4.6634\n",
      "Epoch [2/2], Step [5200/67476], Loss: 4.5110\n",
      "Epoch [2/2], Step [5210/67476], Loss: 4.5203\n",
      "Epoch [2/2], Step [5220/67476], Loss: 4.5879\n",
      "Epoch [2/2], Step [5230/67476], Loss: 4.5154\n",
      "Epoch [2/2], Step [5240/67476], Loss: 4.5060\n",
      "Epoch [2/2], Step [5250/67476], Loss: 4.6190\n",
      "Epoch [2/2], Step [5260/67476], Loss: 4.6355\n",
      "Epoch [2/2], Step [5270/67476], Loss: 4.4431\n",
      "Epoch [2/2], Step [5280/67476], Loss: 4.4638\n",
      "Epoch [2/2], Step [5290/67476], Loss: 4.6678\n",
      "Epoch [2/2], Step [5300/67476], Loss: 4.4826\n",
      "Epoch [2/2], Step [5310/67476], Loss: 4.3956\n",
      "Epoch [2/2], Step [5320/67476], Loss: 4.5418\n",
      "Epoch [2/2], Step [5330/67476], Loss: 4.6139\n",
      "Epoch [2/2], Step [5340/67476], Loss: 4.6466\n",
      "Epoch [2/2], Step [5350/67476], Loss: 4.3210\n",
      "Epoch [2/2], Step [5360/67476], Loss: 4.6006\n",
      "Epoch [2/2], Step [5370/67476], Loss: 4.5253\n",
      "Epoch [2/2], Step [5380/67476], Loss: 4.6068\n",
      "Epoch [2/2], Step [5390/67476], Loss: 4.6691\n",
      "Epoch [2/2], Step [5400/67476], Loss: 4.5689\n",
      "Epoch [2/2], Step [5410/67476], Loss: 4.7560\n",
      "Epoch [2/2], Step [5420/67476], Loss: 4.5207\n",
      "Epoch [2/2], Step [5430/67476], Loss: 4.3646\n",
      "Epoch [2/2], Step [5440/67476], Loss: 4.5559\n",
      "Epoch [2/2], Step [5450/67476], Loss: 4.6159\n",
      "Epoch [2/2], Step [5460/67476], Loss: 4.4305\n",
      "Epoch [2/2], Step [5470/67476], Loss: 4.4403\n",
      "Epoch [2/2], Step [5480/67476], Loss: 4.5558\n",
      "Epoch [2/2], Step [5490/67476], Loss: 4.6213\n",
      "Epoch [2/2], Step [5500/67476], Loss: 4.5237\n",
      "Epoch [2/2], Step [5510/67476], Loss: 4.5343\n",
      "Epoch [2/2], Step [5520/67476], Loss: 4.6514\n",
      "Epoch [2/2], Step [5530/67476], Loss: 4.5188\n",
      "Epoch [2/2], Step [5540/67476], Loss: 4.4828\n",
      "Epoch [2/2], Step [5550/67476], Loss: 4.6522\n",
      "Epoch [2/2], Step [5560/67476], Loss: 4.6279\n",
      "Epoch [2/2], Step [5570/67476], Loss: 4.5079\n",
      "Epoch [2/2], Step [5580/67476], Loss: 4.4130\n",
      "Epoch [2/2], Step [5590/67476], Loss: 4.5785\n",
      "Epoch [2/2], Step [5600/67476], Loss: 4.5763\n",
      "Epoch [2/2], Step [5610/67476], Loss: 4.4901\n",
      "Epoch [2/2], Step [5620/67476], Loss: 4.4856\n",
      "Epoch [2/2], Step [5630/67476], Loss: 4.6032\n",
      "Epoch [2/2], Step [5640/67476], Loss: 4.5067\n",
      "Epoch [2/2], Step [5650/67476], Loss: 4.4776\n",
      "Epoch [2/2], Step [5660/67476], Loss: 4.4115\n",
      "Epoch [2/2], Step [5670/67476], Loss: 4.6255\n",
      "Epoch [2/2], Step [5680/67476], Loss: 4.3850\n",
      "Epoch [2/2], Step [5690/67476], Loss: 4.6096\n",
      "Epoch [2/2], Step [5700/67476], Loss: 4.5631\n",
      "Epoch [2/2], Step [5710/67476], Loss: 4.6229\n",
      "Epoch [2/2], Step [5720/67476], Loss: 4.5261\n",
      "Epoch [2/2], Step [5730/67476], Loss: 4.4933\n",
      "Epoch [2/2], Step [5740/67476], Loss: 4.7161\n",
      "Epoch [2/2], Step [5750/67476], Loss: 4.4551\n",
      "Epoch [2/2], Step [5760/67476], Loss: 4.6398\n",
      "Epoch [2/2], Step [5770/67476], Loss: 4.5140\n",
      "Epoch [2/2], Step [5780/67476], Loss: 4.5264\n",
      "Epoch [2/2], Step [5790/67476], Loss: 4.5325\n",
      "Epoch [2/2], Step [5800/67476], Loss: 4.3706\n",
      "Epoch [2/2], Step [5810/67476], Loss: 4.2298\n",
      "Epoch [2/2], Step [5820/67476], Loss: 4.4461\n",
      "Epoch [2/2], Step [5830/67476], Loss: 4.5586\n",
      "Epoch [2/2], Step [5840/67476], Loss: 4.7487\n",
      "Epoch [2/2], Step [5850/67476], Loss: 4.4140\n",
      "Epoch [2/2], Step [5860/67476], Loss: 4.4961\n",
      "Epoch [2/2], Step [5870/67476], Loss: 4.4804\n",
      "Epoch [2/2], Step [5880/67476], Loss: 4.5526\n",
      "Epoch [2/2], Step [5890/67476], Loss: 4.4815\n",
      "Epoch [2/2], Step [5900/67476], Loss: 4.7078\n",
      "Epoch [2/2], Step [5910/67476], Loss: 4.4100\n",
      "Epoch [2/2], Step [5920/67476], Loss: 4.5315\n",
      "Epoch [2/2], Step [5930/67476], Loss: 4.5914\n",
      "Epoch [2/2], Step [5940/67476], Loss: 4.4972\n",
      "Epoch [2/2], Step [5950/67476], Loss: 4.5319\n",
      "Epoch [2/2], Step [5960/67476], Loss: 4.6048\n",
      "Epoch [2/2], Step [5970/67476], Loss: 4.5273\n",
      "Epoch [2/2], Step [5980/67476], Loss: 4.7067\n",
      "Epoch [2/2], Step [5990/67476], Loss: 4.4987\n",
      "Epoch [2/2], Step [6000/67476], Loss: 4.7064\n",
      "Epoch [2/2], Step [6010/67476], Loss: 4.5390\n",
      "Epoch [2/2], Step [6020/67476], Loss: 4.7092\n",
      "Epoch [2/2], Step [6030/67476], Loss: 4.5957\n",
      "Epoch [2/2], Step [6040/67476], Loss: 4.4948\n",
      "Epoch [2/2], Step [6050/67476], Loss: 4.6728\n",
      "Epoch [2/2], Step [6060/67476], Loss: 4.5220\n",
      "Epoch [2/2], Step [6070/67476], Loss: 4.5773\n",
      "Epoch [2/2], Step [6080/67476], Loss: 4.5116\n",
      "Epoch [2/2], Step [6090/67476], Loss: 4.4412\n",
      "Epoch [2/2], Step [6100/67476], Loss: 4.7365\n",
      "Epoch [2/2], Step [6110/67476], Loss: 4.4374\n",
      "Epoch [2/2], Step [6120/67476], Loss: 4.3576\n",
      "Epoch [2/2], Step [6130/67476], Loss: 4.5582\n",
      "Epoch [2/2], Step [6140/67476], Loss: 4.4577\n",
      "Epoch [2/2], Step [6150/67476], Loss: 4.4610\n",
      "Epoch [2/2], Step [6160/67476], Loss: 4.4859\n",
      "Epoch [2/2], Step [6170/67476], Loss: 4.5684\n",
      "Epoch [2/2], Step [6180/67476], Loss: 4.8715\n",
      "Epoch [2/2], Step [6190/67476], Loss: 4.6827\n",
      "Epoch [2/2], Step [6200/67476], Loss: 4.6059\n",
      "Epoch [2/2], Step [6210/67476], Loss: 4.2780\n",
      "Epoch [2/2], Step [6220/67476], Loss: 4.4197\n",
      "Epoch [2/2], Step [6230/67476], Loss: 4.5695\n",
      "Epoch [2/2], Step [6240/67476], Loss: 4.6998\n",
      "Epoch [2/2], Step [6250/67476], Loss: 4.5467\n",
      "Epoch [2/2], Step [6260/67476], Loss: 4.4162\n",
      "Epoch [2/2], Step [6270/67476], Loss: 4.6525\n",
      "Epoch [2/2], Step [6280/67476], Loss: 4.4078\n",
      "Epoch [2/2], Step [6290/67476], Loss: 4.4608\n",
      "Epoch [2/2], Step [6300/67476], Loss: 4.6772\n",
      "Epoch [2/2], Step [6310/67476], Loss: 4.5021\n",
      "Epoch [2/2], Step [6320/67476], Loss: 4.5547\n",
      "Epoch [2/2], Step [6330/67476], Loss: 4.4992\n",
      "Epoch [2/2], Step [6340/67476], Loss: 4.5549\n",
      "Epoch [2/2], Step [6350/67476], Loss: 4.7387\n",
      "Epoch [2/2], Step [6360/67476], Loss: 4.5850\n",
      "Epoch [2/2], Step [6370/67476], Loss: 4.7918\n",
      "Epoch [2/2], Step [6380/67476], Loss: 4.5957\n",
      "Epoch [2/2], Step [6390/67476], Loss: 4.4439\n",
      "Epoch [2/2], Step [6400/67476], Loss: 4.4406\n",
      "Epoch [2/2], Step [6410/67476], Loss: 4.5668\n",
      "Epoch [2/2], Step [6420/67476], Loss: 4.4225\n",
      "Epoch [2/2], Step [6430/67476], Loss: 4.7600\n",
      "Epoch [2/2], Step [6440/67476], Loss: 4.6045\n",
      "Epoch [2/2], Step [6450/67476], Loss: 4.4488\n",
      "Epoch [2/2], Step [6460/67476], Loss: 4.5606\n",
      "Epoch [2/2], Step [6470/67476], Loss: 4.5099\n",
      "Epoch [2/2], Step [6480/67476], Loss: 4.6127\n",
      "Epoch [2/2], Step [6490/67476], Loss: 4.5110\n",
      "Epoch [2/2], Step [6500/67476], Loss: 4.5200\n",
      "Epoch [2/2], Step [6510/67476], Loss: 4.4484\n",
      "Epoch [2/2], Step [6520/67476], Loss: 4.6493\n",
      "Epoch [2/2], Step [6530/67476], Loss: 4.3338\n",
      "Epoch [2/2], Step [6540/67476], Loss: 4.7251\n",
      "Epoch [2/2], Step [6550/67476], Loss: 4.4001\n",
      "Epoch [2/2], Step [6560/67476], Loss: 4.6502\n",
      "Epoch [2/2], Step [6570/67476], Loss: 4.7549\n",
      "Epoch [2/2], Step [6580/67476], Loss: 4.5012\n",
      "Epoch [2/2], Step [6590/67476], Loss: 4.5126\n",
      "Epoch [2/2], Step [6600/67476], Loss: 4.5616\n",
      "Epoch [2/2], Step [6610/67476], Loss: 4.4317\n",
      "Epoch [2/2], Step [6620/67476], Loss: 4.7293\n",
      "Epoch [2/2], Step [6630/67476], Loss: 4.5977\n",
      "Epoch [2/2], Step [6640/67476], Loss: 4.6431\n",
      "Epoch [2/2], Step [6650/67476], Loss: 4.8535\n",
      "Epoch [2/2], Step [6660/67476], Loss: 4.4879\n",
      "Epoch [2/2], Step [6670/67476], Loss: 4.5745\n",
      "Epoch [2/2], Step [6680/67476], Loss: 4.5670\n",
      "Epoch [2/2], Step [6690/67476], Loss: 4.5117\n",
      "Epoch [2/2], Step [6700/67476], Loss: 4.6272\n",
      "Epoch [2/2], Step [6710/67476], Loss: 4.6526\n",
      "Epoch [2/2], Step [6720/67476], Loss: 4.5353\n",
      "Epoch [2/2], Step [6730/67476], Loss: 4.6786\n",
      "Epoch [2/2], Step [6740/67476], Loss: 4.5007\n",
      "Epoch [2/2], Step [6750/67476], Loss: 4.4030\n",
      "Epoch [2/2], Step [6760/67476], Loss: 4.5140\n",
      "Epoch [2/2], Step [6770/67476], Loss: 4.5321\n",
      "Epoch [2/2], Step [6780/67476], Loss: 4.6232\n",
      "Epoch [2/2], Step [6790/67476], Loss: 4.3480\n",
      "Epoch [2/2], Step [6800/67476], Loss: 4.6866\n",
      "Epoch [2/2], Step [6810/67476], Loss: 4.8104\n",
      "Epoch [2/2], Step [6820/67476], Loss: 4.3254\n",
      "Epoch [2/2], Step [6830/67476], Loss: 4.6840\n",
      "Epoch [2/2], Step [6840/67476], Loss: 4.6382\n",
      "Epoch [2/2], Step [6850/67476], Loss: 4.5720\n",
      "Epoch [2/2], Step [6860/67476], Loss: 4.5632\n",
      "Epoch [2/2], Step [6870/67476], Loss: 4.6415\n",
      "Epoch [2/2], Step [6880/67476], Loss: 4.6203\n",
      "Epoch [2/2], Step [6890/67476], Loss: 4.2961\n",
      "Epoch [2/2], Step [6900/67476], Loss: 4.5137\n",
      "Epoch [2/2], Step [6910/67476], Loss: 4.4328\n",
      "Epoch [2/2], Step [6920/67476], Loss: 4.5879\n",
      "Epoch [2/2], Step [6930/67476], Loss: 4.4870\n",
      "Epoch [2/2], Step [6940/67476], Loss: 4.5485\n",
      "Epoch [2/2], Step [6950/67476], Loss: 4.6564\n",
      "Epoch [2/2], Step [6960/67476], Loss: 4.5673\n",
      "Epoch [2/2], Step [6970/67476], Loss: 4.6819\n",
      "Epoch [2/2], Step [6980/67476], Loss: 4.4249\n",
      "Epoch [2/2], Step [6990/67476], Loss: 4.4377\n",
      "Epoch [2/2], Step [7000/67476], Loss: 4.6054\n",
      "Epoch [2/2], Step [7010/67476], Loss: 4.5926\n",
      "Epoch [2/2], Step [7020/67476], Loss: 4.4484\n",
      "Epoch [2/2], Step [7030/67476], Loss: 4.6150\n",
      "Epoch [2/2], Step [7040/67476], Loss: 4.4120\n",
      "Epoch [2/2], Step [7050/67476], Loss: 4.7706\n",
      "Epoch [2/2], Step [7060/67476], Loss: 4.5641\n",
      "Epoch [2/2], Step [7070/67476], Loss: 4.6438\n",
      "Epoch [2/2], Step [7080/67476], Loss: 4.6951\n",
      "Epoch [2/2], Step [7090/67476], Loss: 4.6188\n",
      "Epoch [2/2], Step [7100/67476], Loss: 4.6032\n",
      "Epoch [2/2], Step [7110/67476], Loss: 4.7720\n",
      "Epoch [2/2], Step [7120/67476], Loss: 4.5873\n",
      "Epoch [2/2], Step [7130/67476], Loss: 4.4585\n",
      "Epoch [2/2], Step [7140/67476], Loss: 4.5990\n",
      "Epoch [2/2], Step [7150/67476], Loss: 4.6190\n",
      "Epoch [2/2], Step [7160/67476], Loss: 4.3821\n",
      "Epoch [2/2], Step [7170/67476], Loss: 4.3882\n",
      "Epoch [2/2], Step [7180/67476], Loss: 4.8622\n",
      "Epoch [2/2], Step [7190/67476], Loss: 4.5822\n",
      "Epoch [2/2], Step [7200/67476], Loss: 4.4594\n",
      "Epoch [2/2], Step [7210/67476], Loss: 4.7339\n",
      "Epoch [2/2], Step [7220/67476], Loss: 4.4796\n",
      "Epoch [2/2], Step [7230/67476], Loss: 4.5068\n",
      "Epoch [2/2], Step [7240/67476], Loss: 4.6236\n",
      "Epoch [2/2], Step [7250/67476], Loss: 4.6521\n",
      "Epoch [2/2], Step [7260/67476], Loss: 4.7104\n",
      "Epoch [2/2], Step [7270/67476], Loss: 4.4752\n",
      "Epoch [2/2], Step [7280/67476], Loss: 4.2866\n",
      "Epoch [2/2], Step [7290/67476], Loss: 4.4934\n",
      "Epoch [2/2], Step [7300/67476], Loss: 4.6196\n",
      "Epoch [2/2], Step [7310/67476], Loss: 4.5433\n",
      "Epoch [2/2], Step [7320/67476], Loss: 4.6072\n",
      "Epoch [2/2], Step [7330/67476], Loss: 4.5905\n",
      "Epoch [2/2], Step [7340/67476], Loss: 4.6214\n",
      "Epoch [2/2], Step [7350/67476], Loss: 4.6824\n",
      "Epoch [2/2], Step [7360/67476], Loss: 4.3925\n",
      "Epoch [2/2], Step [7370/67476], Loss: 4.7360\n",
      "Epoch [2/2], Step [7380/67476], Loss: 4.5602\n",
      "Epoch [2/2], Step [7390/67476], Loss: 4.5860\n",
      "Epoch [2/2], Step [7400/67476], Loss: 4.5474\n",
      "Epoch [2/2], Step [7410/67476], Loss: 4.4563\n",
      "Epoch [2/2], Step [7420/67476], Loss: 4.5014\n",
      "Epoch [2/2], Step [7430/67476], Loss: 4.5410\n",
      "Epoch [2/2], Step [7440/67476], Loss: 4.6954\n",
      "Epoch [2/2], Step [7450/67476], Loss: 4.5474\n",
      "Epoch [2/2], Step [7460/67476], Loss: 4.4353\n",
      "Epoch [2/2], Step [7470/67476], Loss: 4.7188\n",
      "Epoch [2/2], Step [7480/67476], Loss: 4.3377\n",
      "Epoch [2/2], Step [7490/67476], Loss: 4.4405\n",
      "Epoch [2/2], Step [7500/67476], Loss: 4.3820\n",
      "Epoch [2/2], Step [7510/67476], Loss: 4.4312\n",
      "Epoch [2/2], Step [7520/67476], Loss: 4.6645\n",
      "Epoch [2/2], Step [7530/67476], Loss: 4.7255\n",
      "Epoch [2/2], Step [7540/67476], Loss: 4.7326\n",
      "Epoch [2/2], Step [7550/67476], Loss: 4.5757\n",
      "Epoch [2/2], Step [7560/67476], Loss: 4.6579\n",
      "Epoch [2/2], Step [7570/67476], Loss: 4.5740\n",
      "Epoch [2/2], Step [7580/67476], Loss: 4.5483\n",
      "Epoch [2/2], Step [7590/67476], Loss: 4.4424\n",
      "Epoch [2/2], Step [7600/67476], Loss: 4.6577\n",
      "Epoch [2/2], Step [7610/67476], Loss: 4.5174\n",
      "Epoch [2/2], Step [7620/67476], Loss: 4.6289\n",
      "Epoch [2/2], Step [7630/67476], Loss: 4.6544\n",
      "Epoch [2/2], Step [7640/67476], Loss: 4.6498\n",
      "Epoch [2/2], Step [7650/67476], Loss: 4.5269\n",
      "Epoch [2/2], Step [7660/67476], Loss: 4.5280\n",
      "Epoch [2/2], Step [7670/67476], Loss: 4.6689\n",
      "Epoch [2/2], Step [7680/67476], Loss: 4.5314\n",
      "Epoch [2/2], Step [7690/67476], Loss: 4.4861\n",
      "Epoch [2/2], Step [7700/67476], Loss: 4.6229\n",
      "Epoch [2/2], Step [7710/67476], Loss: 4.6647\n",
      "Epoch [2/2], Step [7720/67476], Loss: 4.5635\n",
      "Epoch [2/2], Step [7730/67476], Loss: 4.5367\n",
      "Epoch [2/2], Step [7740/67476], Loss: 4.4693\n",
      "Epoch [2/2], Step [7750/67476], Loss: 4.5472\n",
      "Epoch [2/2], Step [7760/67476], Loss: 4.5595\n",
      "Epoch [2/2], Step [7770/67476], Loss: 4.5239\n",
      "Epoch [2/2], Step [7780/67476], Loss: 4.7448\n",
      "Epoch [2/2], Step [7790/67476], Loss: 4.5350\n",
      "Epoch [2/2], Step [7800/67476], Loss: 4.5578\n",
      "Epoch [2/2], Step [7810/67476], Loss: 4.5670\n",
      "Epoch [2/2], Step [7820/67476], Loss: 4.4511\n",
      "Epoch [2/2], Step [7830/67476], Loss: 4.6754\n",
      "Epoch [2/2], Step [7840/67476], Loss: 4.6822\n",
      "Epoch [2/2], Step [7850/67476], Loss: 4.5686\n",
      "Epoch [2/2], Step [7860/67476], Loss: 4.5232\n",
      "Epoch [2/2], Step [7870/67476], Loss: 4.4050\n",
      "Epoch [2/2], Step [7880/67476], Loss: 4.6472\n",
      "Epoch [2/2], Step [7890/67476], Loss: 4.5704\n",
      "Epoch [2/2], Step [7900/67476], Loss: 4.5176\n",
      "Epoch [2/2], Step [7910/67476], Loss: 4.7412\n",
      "Epoch [2/2], Step [7920/67476], Loss: 4.6585\n",
      "Epoch [2/2], Step [7930/67476], Loss: 4.5906\n",
      "Epoch [2/2], Step [7940/67476], Loss: 4.5569\n",
      "Epoch [2/2], Step [7950/67476], Loss: 4.6411\n",
      "Epoch [2/2], Step [7960/67476], Loss: 4.7053\n",
      "Epoch [2/2], Step [7970/67476], Loss: 4.5721\n",
      "Epoch [2/2], Step [7980/67476], Loss: 4.6519\n",
      "Epoch [2/2], Step [7990/67476], Loss: 4.6406\n",
      "Epoch [2/2], Step [8000/67476], Loss: 4.6127\n",
      "Epoch [2/2], Step [8010/67476], Loss: 4.5964\n",
      "Epoch [2/2], Step [8020/67476], Loss: 4.6140\n",
      "Epoch [2/2], Step [8030/67476], Loss: 4.6986\n",
      "Epoch [2/2], Step [8040/67476], Loss: 4.5661\n",
      "Epoch [2/2], Step [8050/67476], Loss: 4.2509\n",
      "Epoch [2/2], Step [8060/67476], Loss: 4.8098\n",
      "Epoch [2/2], Step [8070/67476], Loss: 4.4511\n",
      "Epoch [2/2], Step [8080/67476], Loss: 4.6078\n",
      "Epoch [2/2], Step [8090/67476], Loss: 4.3399\n",
      "Epoch [2/2], Step [8100/67476], Loss: 4.6479\n",
      "Epoch [2/2], Step [8110/67476], Loss: 4.7201\n",
      "Epoch [2/2], Step [8120/67476], Loss: 4.4868\n",
      "Epoch [2/2], Step [8130/67476], Loss: 4.6902\n",
      "Epoch [2/2], Step [8140/67476], Loss: 4.4335\n",
      "Epoch [2/2], Step [8150/67476], Loss: 4.4445\n",
      "Epoch [2/2], Step [8160/67476], Loss: 4.4482\n",
      "Epoch [2/2], Step [8170/67476], Loss: 4.7828\n",
      "Epoch [2/2], Step [8180/67476], Loss: 4.5985\n",
      "Epoch [2/2], Step [8190/67476], Loss: 4.5326\n",
      "Epoch [2/2], Step [8200/67476], Loss: 4.6743\n",
      "Epoch [2/2], Step [8210/67476], Loss: 4.6060\n",
      "Epoch [2/2], Step [8220/67476], Loss: 4.5313\n",
      "Epoch [2/2], Step [8230/67476], Loss: 4.7071\n",
      "Epoch [2/2], Step [8240/67476], Loss: 4.7164\n",
      "Epoch [2/2], Step [8250/67476], Loss: 4.7171\n",
      "Epoch [2/2], Step [8260/67476], Loss: 4.4506\n",
      "Epoch [2/2], Step [8270/67476], Loss: 4.6998\n",
      "Epoch [2/2], Step [8280/67476], Loss: 4.5667\n",
      "Epoch [2/2], Step [8290/67476], Loss: 4.6134\n",
      "Epoch [2/2], Step [8300/67476], Loss: 4.5645\n",
      "Epoch [2/2], Step [8310/67476], Loss: 4.5493\n",
      "Epoch [2/2], Step [8320/67476], Loss: 4.5009\n",
      "Epoch [2/2], Step [8330/67476], Loss: 4.5609\n",
      "Epoch [2/2], Step [8340/67476], Loss: 4.5593\n",
      "Epoch [2/2], Step [8350/67476], Loss: 4.5223\n",
      "Epoch [2/2], Step [8360/67476], Loss: 4.5764\n",
      "Epoch [2/2], Step [8370/67476], Loss: 4.4057\n",
      "Epoch [2/2], Step [8380/67476], Loss: 4.5048\n",
      "Epoch [2/2], Step [8390/67476], Loss: 4.6536\n",
      "Epoch [2/2], Step [8400/67476], Loss: 4.4666\n",
      "Epoch [2/2], Step [8410/67476], Loss: 4.4665\n",
      "Epoch [2/2], Step [8420/67476], Loss: 4.7292\n",
      "Epoch [2/2], Step [8430/67476], Loss: 4.4860\n",
      "Epoch [2/2], Step [8440/67476], Loss: 4.4864\n",
      "Epoch [2/2], Step [8450/67476], Loss: 4.6615\n",
      "Epoch [2/2], Step [8460/67476], Loss: 4.5065\n",
      "Epoch [2/2], Step [8470/67476], Loss: 4.6031\n",
      "Epoch [2/2], Step [8480/67476], Loss: 4.4801\n",
      "Epoch [2/2], Step [8490/67476], Loss: 4.6485\n",
      "Epoch [2/2], Step [8500/67476], Loss: 4.7691\n",
      "Epoch [2/2], Step [8510/67476], Loss: 4.6570\n",
      "Epoch [2/2], Step [8520/67476], Loss: 4.6491\n",
      "Epoch [2/2], Step [8530/67476], Loss: 4.3447\n",
      "Epoch [2/2], Step [8540/67476], Loss: 4.5369\n",
      "Epoch [2/2], Step [8550/67476], Loss: 4.6122\n",
      "Epoch [2/2], Step [8560/67476], Loss: 4.3007\n",
      "Epoch [2/2], Step [8570/67476], Loss: 4.5603\n",
      "Epoch [2/2], Step [8580/67476], Loss: 4.4779\n",
      "Epoch [2/2], Step [8590/67476], Loss: 4.4683\n",
      "Epoch [2/2], Step [8600/67476], Loss: 4.4681\n",
      "Epoch [2/2], Step [8610/67476], Loss: 4.3671\n",
      "Epoch [2/2], Step [8620/67476], Loss: 4.5801\n",
      "Epoch [2/2], Step [8630/67476], Loss: 4.5643\n",
      "Epoch [2/2], Step [8640/67476], Loss: 4.8826\n",
      "Epoch [2/2], Step [8650/67476], Loss: 4.4746\n",
      "Epoch [2/2], Step [8660/67476], Loss: 4.6049\n",
      "Epoch [2/2], Step [8670/67476], Loss: 4.4896\n",
      "Epoch [2/2], Step [8680/67476], Loss: 4.5251\n",
      "Epoch [2/2], Step [8690/67476], Loss: 4.7082\n",
      "Epoch [2/2], Step [8700/67476], Loss: 4.3544\n",
      "Epoch [2/2], Step [8710/67476], Loss: 4.5329\n",
      "Epoch [2/2], Step [8720/67476], Loss: 4.6829\n",
      "Epoch [2/2], Step [8730/67476], Loss: 4.5832\n",
      "Epoch [2/2], Step [8740/67476], Loss: 4.3982\n",
      "Epoch [2/2], Step [8750/67476], Loss: 4.5951\n",
      "Epoch [2/2], Step [8760/67476], Loss: 4.3483\n",
      "Epoch [2/2], Step [8770/67476], Loss: 4.5159\n",
      "Epoch [2/2], Step [8780/67476], Loss: 4.6940\n",
      "Epoch [2/2], Step [8790/67476], Loss: 4.6523\n",
      "Epoch [2/2], Step [8800/67476], Loss: 4.6691\n",
      "Epoch [2/2], Step [8810/67476], Loss: 4.3654\n",
      "Epoch [2/2], Step [8820/67476], Loss: 4.4680\n",
      "Epoch [2/2], Step [8830/67476], Loss: 4.6086\n",
      "Epoch [2/2], Step [8840/67476], Loss: 4.4640\n",
      "Epoch [2/2], Step [8850/67476], Loss: 4.4668\n",
      "Epoch [2/2], Step [8860/67476], Loss: 4.5553\n",
      "Epoch [2/2], Step [8870/67476], Loss: 4.3960\n",
      "Epoch [2/2], Step [8880/67476], Loss: 4.4197\n",
      "Epoch [2/2], Step [8890/67476], Loss: 4.5390\n",
      "Epoch [2/2], Step [8900/67476], Loss: 4.5966\n",
      "Epoch [2/2], Step [8910/67476], Loss: 4.5823\n",
      "Epoch [2/2], Step [8920/67476], Loss: 4.3922\n",
      "Epoch [2/2], Step [8930/67476], Loss: 4.5638\n",
      "Epoch [2/2], Step [8940/67476], Loss: 4.3475\n",
      "Epoch [2/2], Step [8950/67476], Loss: 4.5475\n",
      "Epoch [2/2], Step [8960/67476], Loss: 4.5322\n",
      "Epoch [2/2], Step [8970/67476], Loss: 4.9209\n",
      "Epoch [2/2], Step [8980/67476], Loss: 4.6348\n",
      "Epoch [2/2], Step [8990/67476], Loss: 4.6503\n",
      "Epoch [2/2], Step [9000/67476], Loss: 4.4368\n",
      "Epoch [2/2], Step [9010/67476], Loss: 4.5576\n",
      "Epoch [2/2], Step [9020/67476], Loss: 4.5318\n",
      "Epoch [2/2], Step [9030/67476], Loss: 4.5583\n",
      "Epoch [2/2], Step [9040/67476], Loss: 4.5893\n",
      "Epoch [2/2], Step [9050/67476], Loss: 4.5365\n",
      "Epoch [2/2], Step [9060/67476], Loss: 4.6840\n",
      "Epoch [2/2], Step [9070/67476], Loss: 4.3599\n",
      "Epoch [2/2], Step [9080/67476], Loss: 4.6397\n",
      "Epoch [2/2], Step [9090/67476], Loss: 4.5932\n",
      "Epoch [2/2], Step [9100/67476], Loss: 4.7170\n",
      "Epoch [2/2], Step [9110/67476], Loss: 4.6788\n",
      "Epoch [2/2], Step [9120/67476], Loss: 4.5516\n",
      "Epoch [2/2], Step [9130/67476], Loss: 4.5276\n",
      "Epoch [2/2], Step [9140/67476], Loss: 4.6760\n",
      "Epoch [2/2], Step [9150/67476], Loss: 4.7778\n",
      "Epoch [2/2], Step [9160/67476], Loss: 4.3371\n",
      "Epoch [2/2], Step [9170/67476], Loss: 4.5472\n",
      "Epoch [2/2], Step [9180/67476], Loss: 4.7334\n",
      "Epoch [2/2], Step [9190/67476], Loss: 4.5767\n",
      "Epoch [2/2], Step [9200/67476], Loss: 4.4851\n",
      "Epoch [2/2], Step [9210/67476], Loss: 4.5839\n",
      "Epoch [2/2], Step [9220/67476], Loss: 4.7099\n",
      "Epoch [2/2], Step [9230/67476], Loss: 4.4322\n",
      "Epoch [2/2], Step [9240/67476], Loss: 4.5201\n",
      "Epoch [2/2], Step [9250/67476], Loss: 4.5013\n",
      "Epoch [2/2], Step [9260/67476], Loss: 4.5726\n",
      "Epoch [2/2], Step [9270/67476], Loss: 4.6497\n",
      "Epoch [2/2], Step [9280/67476], Loss: 4.5219\n",
      "Epoch [2/2], Step [9290/67476], Loss: 4.6476\n",
      "Epoch [2/2], Step [9300/67476], Loss: 4.5916\n",
      "Epoch [2/2], Step [9310/67476], Loss: 4.5900\n",
      "Epoch [2/2], Step [9320/67476], Loss: 4.5082\n",
      "Epoch [2/2], Step [9330/67476], Loss: 4.4357\n",
      "Epoch [2/2], Step [9340/67476], Loss: 4.5146\n",
      "Epoch [2/2], Step [9350/67476], Loss: 4.5990\n",
      "Epoch [2/2], Step [9360/67476], Loss: 4.6126\n",
      "Epoch [2/2], Step [9370/67476], Loss: 4.5938\n",
      "Epoch [2/2], Step [9380/67476], Loss: 4.4860\n",
      "Epoch [2/2], Step [9390/67476], Loss: 4.5720\n",
      "Epoch [2/2], Step [9400/67476], Loss: 4.7298\n",
      "Epoch [2/2], Step [9410/67476], Loss: 4.6380\n",
      "Epoch [2/2], Step [9420/67476], Loss: 4.5867\n",
      "Epoch [2/2], Step [9430/67476], Loss: 4.5273\n",
      "Epoch [2/2], Step [9440/67476], Loss: 4.5115\n",
      "Epoch [2/2], Step [9450/67476], Loss: 4.6124\n",
      "Epoch [2/2], Step [9460/67476], Loss: 4.5352\n",
      "Epoch [2/2], Step [9470/67476], Loss: 4.5722\n",
      "Epoch [2/2], Step [9480/67476], Loss: 4.5377\n",
      "Epoch [2/2], Step [9490/67476], Loss: 4.5549\n",
      "Epoch [2/2], Step [9500/67476], Loss: 4.4775\n",
      "Epoch [2/2], Step [9510/67476], Loss: 4.5239\n",
      "Epoch [2/2], Step [9520/67476], Loss: 4.3387\n",
      "Epoch [2/2], Step [9530/67476], Loss: 4.4322\n",
      "Epoch [2/2], Step [9540/67476], Loss: 4.4260\n",
      "Epoch [2/2], Step [9550/67476], Loss: 4.7759\n",
      "Epoch [2/2], Step [9560/67476], Loss: 4.4781\n",
      "Epoch [2/2], Step [9570/67476], Loss: 4.5099\n",
      "Epoch [2/2], Step [9580/67476], Loss: 4.6525\n",
      "Epoch [2/2], Step [9590/67476], Loss: 4.5486\n",
      "Epoch [2/2], Step [9600/67476], Loss: 4.4418\n",
      "Epoch [2/2], Step [9610/67476], Loss: 4.4927\n",
      "Epoch [2/2], Step [9620/67476], Loss: 4.7254\n",
      "Epoch [2/2], Step [9630/67476], Loss: 4.6045\n",
      "Epoch [2/2], Step [9640/67476], Loss: 4.5927\n",
      "Epoch [2/2], Step [9650/67476], Loss: 4.6283\n",
      "Epoch [2/2], Step [9660/67476], Loss: 4.3370\n",
      "Epoch [2/2], Step [9670/67476], Loss: 4.5581\n",
      "Epoch [2/2], Step [9680/67476], Loss: 4.5916\n",
      "Epoch [2/2], Step [9690/67476], Loss: 4.5407\n",
      "Epoch [2/2], Step [9700/67476], Loss: 4.3844\n",
      "Epoch [2/2], Step [9710/67476], Loss: 4.5422\n",
      "Epoch [2/2], Step [9720/67476], Loss: 4.5812\n",
      "Epoch [2/2], Step [9730/67476], Loss: 4.4725\n",
      "Epoch [2/2], Step [9740/67476], Loss: 4.6577\n",
      "Epoch [2/2], Step [9750/67476], Loss: 4.4535\n",
      "Epoch [2/2], Step [9760/67476], Loss: 4.8952\n",
      "Epoch [2/2], Step [9770/67476], Loss: 4.4789\n",
      "Epoch [2/2], Step [9780/67476], Loss: 4.6350\n",
      "Epoch [2/2], Step [9790/67476], Loss: 4.5772\n",
      "Epoch [2/2], Step [9800/67476], Loss: 4.5834\n",
      "Epoch [2/2], Step [9810/67476], Loss: 4.3776\n",
      "Epoch [2/2], Step [9820/67476], Loss: 4.5546\n",
      "Epoch [2/2], Step [9830/67476], Loss: 4.4988\n",
      "Epoch [2/2], Step [9840/67476], Loss: 4.4679\n",
      "Epoch [2/2], Step [9850/67476], Loss: 4.6993\n",
      "Epoch [2/2], Step [9860/67476], Loss: 4.6464\n",
      "Epoch [2/2], Step [9870/67476], Loss: 4.7280\n",
      "Epoch [2/2], Step [9880/67476], Loss: 4.6358\n",
      "Epoch [2/2], Step [9890/67476], Loss: 4.5304\n",
      "Epoch [2/2], Step [9900/67476], Loss: 4.4684\n",
      "Epoch [2/2], Step [9910/67476], Loss: 4.5344\n",
      "Epoch [2/2], Step [9920/67476], Loss: 4.5196\n",
      "Epoch [2/2], Step [9930/67476], Loss: 4.6086\n",
      "Epoch [2/2], Step [9940/67476], Loss: 4.7172\n",
      "Epoch [2/2], Step [9950/67476], Loss: 4.3342\n",
      "Epoch [2/2], Step [9960/67476], Loss: 4.5334\n",
      "Epoch [2/2], Step [9970/67476], Loss: 4.5710\n",
      "Epoch [2/2], Step [9980/67476], Loss: 4.5030\n",
      "Epoch [2/2], Step [9990/67476], Loss: 4.6363\n",
      "Epoch [2/2], Step [10000/67476], Loss: 4.6909\n",
      "Epoch [2/2], Step [10010/67476], Loss: 4.4937\n",
      "Epoch [2/2], Step [10020/67476], Loss: 4.3087\n",
      "Epoch [2/2], Step [10030/67476], Loss: 4.6585\n",
      "Epoch [2/2], Step [10040/67476], Loss: 4.5960\n",
      "Epoch [2/2], Step [10050/67476], Loss: 4.5769\n",
      "Epoch [2/2], Step [10060/67476], Loss: 4.5444\n",
      "Epoch [2/2], Step [10070/67476], Loss: 4.5523\n",
      "Epoch [2/2], Step [10080/67476], Loss: 4.6152\n",
      "Epoch [2/2], Step [10090/67476], Loss: 4.4983\n",
      "Epoch [2/2], Step [10100/67476], Loss: 4.4264\n",
      "Epoch [2/2], Step [10110/67476], Loss: 4.4038\n",
      "Epoch [2/2], Step [10120/67476], Loss: 4.3203\n",
      "Epoch [2/2], Step [10130/67476], Loss: 4.5875\n",
      "Epoch [2/2], Step [10140/67476], Loss: 4.6403\n",
      "Epoch [2/2], Step [10150/67476], Loss: 4.5753\n",
      "Epoch [2/2], Step [10160/67476], Loss: 4.3522\n",
      "Epoch [2/2], Step [10170/67476], Loss: 4.5850\n",
      "Epoch [2/2], Step [10180/67476], Loss: 4.8026\n",
      "Epoch [2/2], Step [10190/67476], Loss: 4.5322\n",
      "Epoch [2/2], Step [10200/67476], Loss: 4.5069\n",
      "Epoch [2/2], Step [10210/67476], Loss: 4.3784\n",
      "Epoch [2/2], Step [10220/67476], Loss: 4.6306\n",
      "Epoch [2/2], Step [10230/67476], Loss: 4.6807\n",
      "Epoch [2/2], Step [10240/67476], Loss: 4.3773\n",
      "Epoch [2/2], Step [10250/67476], Loss: 4.7427\n",
      "Epoch [2/2], Step [10260/67476], Loss: 4.7181\n",
      "Epoch [2/2], Step [10270/67476], Loss: 4.6058\n",
      "Epoch [2/2], Step [10280/67476], Loss: 4.4924\n",
      "Epoch [2/2], Step [10290/67476], Loss: 4.6370\n",
      "Epoch [2/2], Step [10300/67476], Loss: 4.4951\n",
      "Epoch [2/2], Step [10310/67476], Loss: 4.7646\n",
      "Epoch [2/2], Step [10320/67476], Loss: 4.6391\n",
      "Epoch [2/2], Step [10330/67476], Loss: 4.5941\n",
      "Epoch [2/2], Step [10340/67476], Loss: 4.6339\n",
      "Epoch [2/2], Step [10350/67476], Loss: 4.6348\n",
      "Epoch [2/2], Step [10360/67476], Loss: 4.4854\n",
      "Epoch [2/2], Step [10370/67476], Loss: 4.4921\n",
      "Epoch [2/2], Step [10380/67476], Loss: 4.2993\n",
      "Epoch [2/2], Step [10390/67476], Loss: 4.5916\n",
      "Epoch [2/2], Step [10400/67476], Loss: 4.4946\n",
      "Epoch [2/2], Step [10410/67476], Loss: 4.5989\n",
      "Epoch [2/2], Step [10420/67476], Loss: 4.5199\n",
      "Epoch [2/2], Step [10430/67476], Loss: 4.2812\n",
      "Epoch [2/2], Step [10440/67476], Loss: 4.5075\n",
      "Epoch [2/2], Step [10450/67476], Loss: 4.8131\n",
      "Epoch [2/2], Step [10460/67476], Loss: 4.3275\n",
      "Epoch [2/2], Step [10470/67476], Loss: 4.6236\n",
      "Epoch [2/2], Step [10480/67476], Loss: 4.7567\n",
      "Epoch [2/2], Step [10490/67476], Loss: 4.4129\n",
      "Epoch [2/2], Step [10500/67476], Loss: 4.6427\n",
      "Epoch [2/2], Step [10510/67476], Loss: 4.4613\n",
      "Epoch [2/2], Step [10520/67476], Loss: 4.5296\n",
      "Epoch [2/2], Step [10530/67476], Loss: 4.4691\n",
      "Epoch [2/2], Step [10540/67476], Loss: 4.6450\n",
      "Epoch [2/2], Step [10550/67476], Loss: 4.6068\n",
      "Epoch [2/2], Step [10560/67476], Loss: 4.5924\n",
      "Epoch [2/2], Step [10570/67476], Loss: 4.5470\n",
      "Epoch [2/2], Step [10580/67476], Loss: 4.4825\n",
      "Epoch [2/2], Step [10590/67476], Loss: 4.7755\n",
      "Epoch [2/2], Step [10600/67476], Loss: 4.3513\n",
      "Epoch [2/2], Step [10610/67476], Loss: 4.5275\n",
      "Epoch [2/2], Step [10620/67476], Loss: 4.5807\n",
      "Epoch [2/2], Step [10630/67476], Loss: 4.6002\n",
      "Epoch [2/2], Step [10640/67476], Loss: 4.5426\n",
      "Epoch [2/2], Step [10650/67476], Loss: 4.5645\n",
      "Epoch [2/2], Step [10660/67476], Loss: 4.5487\n",
      "Epoch [2/2], Step [10670/67476], Loss: 4.3952\n",
      "Epoch [2/2], Step [10680/67476], Loss: 4.5188\n",
      "Epoch [2/2], Step [10690/67476], Loss: 4.5288\n",
      "Epoch [2/2], Step [10700/67476], Loss: 4.5604\n",
      "Epoch [2/2], Step [10710/67476], Loss: 4.3544\n",
      "Epoch [2/2], Step [10720/67476], Loss: 4.6197\n",
      "Epoch [2/2], Step [10730/67476], Loss: 4.6074\n",
      "Epoch [2/2], Step [10740/67476], Loss: 4.6810\n",
      "Epoch [2/2], Step [10750/67476], Loss: 4.6978\n",
      "Epoch [2/2], Step [10760/67476], Loss: 4.5096\n",
      "Epoch [2/2], Step [10770/67476], Loss: 4.5442\n",
      "Epoch [2/2], Step [10780/67476], Loss: 4.5595\n",
      "Epoch [2/2], Step [10790/67476], Loss: 4.4682\n",
      "Epoch [2/2], Step [10800/67476], Loss: 4.4797\n",
      "Epoch [2/2], Step [10810/67476], Loss: 4.6874\n",
      "Epoch [2/2], Step [10820/67476], Loss: 4.6681\n",
      "Epoch [2/2], Step [10830/67476], Loss: 4.6016\n",
      "Epoch [2/2], Step [10840/67476], Loss: 4.5340\n",
      "Epoch [2/2], Step [10850/67476], Loss: 4.4701\n",
      "Epoch [2/2], Step [10860/67476], Loss: 4.6388\n",
      "Epoch [2/2], Step [10870/67476], Loss: 4.3992\n",
      "Epoch [2/2], Step [10880/67476], Loss: 4.5381\n",
      "Epoch [2/2], Step [10890/67476], Loss: 4.5767\n",
      "Epoch [2/2], Step [10900/67476], Loss: 4.3646\n",
      "Epoch [2/2], Step [10910/67476], Loss: 4.7182\n",
      "Epoch [2/2], Step [10920/67476], Loss: 4.5067\n",
      "Epoch [2/2], Step [10930/67476], Loss: 4.5853\n",
      "Epoch [2/2], Step [10940/67476], Loss: 4.5058\n",
      "Epoch [2/2], Step [10950/67476], Loss: 4.5536\n",
      "Epoch [2/2], Step [10960/67476], Loss: 4.3552\n",
      "Epoch [2/2], Step [10970/67476], Loss: 4.6809\n",
      "Epoch [2/2], Step [10980/67476], Loss: 4.5028\n",
      "Epoch [2/2], Step [10990/67476], Loss: 4.5994\n",
      "Epoch [2/2], Step [11000/67476], Loss: 4.7647\n",
      "Epoch [2/2], Step [11010/67476], Loss: 4.5000\n",
      "Epoch [2/2], Step [11020/67476], Loss: 4.7794\n",
      "Epoch [2/2], Step [11030/67476], Loss: 4.4944\n",
      "Epoch [2/2], Step [11040/67476], Loss: 4.6676\n",
      "Epoch [2/2], Step [11050/67476], Loss: 4.5825\n",
      "Epoch [2/2], Step [11060/67476], Loss: 4.7075\n",
      "Epoch [2/2], Step [11070/67476], Loss: 4.5307\n",
      "Epoch [2/2], Step [11080/67476], Loss: 4.4978\n",
      "Epoch [2/2], Step [11090/67476], Loss: 4.4172\n",
      "Epoch [2/2], Step [11100/67476], Loss: 4.5796\n",
      "Epoch [2/2], Step [11110/67476], Loss: 4.6198\n",
      "Epoch [2/2], Step [11120/67476], Loss: 4.6526\n",
      "Epoch [2/2], Step [11130/67476], Loss: 4.5182\n",
      "Epoch [2/2], Step [11140/67476], Loss: 4.5968\n",
      "Epoch [2/2], Step [11150/67476], Loss: 4.2944\n",
      "Epoch [2/2], Step [11160/67476], Loss: 4.4436\n",
      "Epoch [2/2], Step [11170/67476], Loss: 4.6261\n",
      "Epoch [2/2], Step [11180/67476], Loss: 4.6491\n",
      "Epoch [2/2], Step [11190/67476], Loss: 4.6419\n",
      "Epoch [2/2], Step [11200/67476], Loss: 4.6113\n",
      "Epoch [2/2], Step [11210/67476], Loss: 4.6422\n",
      "Epoch [2/2], Step [11220/67476], Loss: 4.4232\n",
      "Epoch [2/2], Step [11230/67476], Loss: 4.6350\n",
      "Epoch [2/2], Step [11240/67476], Loss: 4.7481\n",
      "Epoch [2/2], Step [11250/67476], Loss: 4.5049\n",
      "Epoch [2/2], Step [11260/67476], Loss: 4.4406\n",
      "Epoch [2/2], Step [11270/67476], Loss: 4.5397\n",
      "Epoch [2/2], Step [11280/67476], Loss: 4.4450\n",
      "Epoch [2/2], Step [11290/67476], Loss: 4.6292\n",
      "Epoch [2/2], Step [11300/67476], Loss: 4.6717\n",
      "Epoch [2/2], Step [11310/67476], Loss: 4.3637\n",
      "Epoch [2/2], Step [11320/67476], Loss: 4.4623\n",
      "Epoch [2/2], Step [11330/67476], Loss: 4.6509\n",
      "Epoch [2/2], Step [11340/67476], Loss: 4.5614\n",
      "Epoch [2/2], Step [11350/67476], Loss: 4.4660\n",
      "Epoch [2/2], Step [11360/67476], Loss: 4.5276\n",
      "Epoch [2/2], Step [11370/67476], Loss: 4.5784\n",
      "Epoch [2/2], Step [11380/67476], Loss: 4.5111\n",
      "Epoch [2/2], Step [11390/67476], Loss: 4.5450\n",
      "Epoch [2/2], Step [11400/67476], Loss: 4.6441\n",
      "Epoch [2/2], Step [11410/67476], Loss: 4.6867\n",
      "Epoch [2/2], Step [11420/67476], Loss: 4.7462\n",
      "Epoch [2/2], Step [11430/67476], Loss: 4.5352\n",
      "Epoch [2/2], Step [11440/67476], Loss: 4.5526\n",
      "Epoch [2/2], Step [11450/67476], Loss: 4.6742\n",
      "Epoch [2/2], Step [11460/67476], Loss: 4.4355\n",
      "Epoch [2/2], Step [11470/67476], Loss: 4.4513\n",
      "Epoch [2/2], Step [11480/67476], Loss: 4.5039\n",
      "Epoch [2/2], Step [11490/67476], Loss: 4.4643\n",
      "Epoch [2/2], Step [11500/67476], Loss: 4.5796\n",
      "Epoch [2/2], Step [11510/67476], Loss: 4.5357\n",
      "Epoch [2/2], Step [11520/67476], Loss: 4.3046\n",
      "Epoch [2/2], Step [11530/67476], Loss: 4.5979\n",
      "Epoch [2/2], Step [11540/67476], Loss: 4.5285\n",
      "Epoch [2/2], Step [11550/67476], Loss: 4.5958\n",
      "Epoch [2/2], Step [11560/67476], Loss: 4.4960\n",
      "Epoch [2/2], Step [11570/67476], Loss: 4.6281\n",
      "Epoch [2/2], Step [11580/67476], Loss: 4.5534\n",
      "Epoch [2/2], Step [11590/67476], Loss: 4.3382\n",
      "Epoch [2/2], Step [11600/67476], Loss: 4.3450\n",
      "Epoch [2/2], Step [11610/67476], Loss: 4.2947\n",
      "Epoch [2/2], Step [11620/67476], Loss: 4.2945\n",
      "Epoch [2/2], Step [11630/67476], Loss: 4.2830\n",
      "Epoch [2/2], Step [11640/67476], Loss: 4.6092\n",
      "Epoch [2/2], Step [11650/67476], Loss: 4.4753\n",
      "Epoch [2/2], Step [11660/67476], Loss: 4.4923\n",
      "Epoch [2/2], Step [11670/67476], Loss: 4.5690\n",
      "Epoch [2/2], Step [11680/67476], Loss: 4.6276\n",
      "Epoch [2/2], Step [11690/67476], Loss: 4.7193\n",
      "Epoch [2/2], Step [11700/67476], Loss: 4.4659\n",
      "Epoch [2/2], Step [11710/67476], Loss: 4.5519\n",
      "Epoch [2/2], Step [11720/67476], Loss: 4.3321\n",
      "Epoch [2/2], Step [11730/67476], Loss: 4.5199\n",
      "Epoch [2/2], Step [11740/67476], Loss: 4.6718\n",
      "Epoch [2/2], Step [11750/67476], Loss: 4.7276\n",
      "Epoch [2/2], Step [11760/67476], Loss: 4.5385\n",
      "Epoch [2/2], Step [11770/67476], Loss: 4.6378\n",
      "Epoch [2/2], Step [11780/67476], Loss: 4.4927\n",
      "Epoch [2/2], Step [11790/67476], Loss: 4.5399\n",
      "Epoch [2/2], Step [11800/67476], Loss: 4.4553\n",
      "Epoch [2/2], Step [11810/67476], Loss: 4.5127\n",
      "Epoch [2/2], Step [11820/67476], Loss: 4.5984\n",
      "Epoch [2/2], Step [11830/67476], Loss: 4.4682\n",
      "Epoch [2/2], Step [11840/67476], Loss: 4.5209\n",
      "Epoch [2/2], Step [11850/67476], Loss: 4.5436\n",
      "Epoch [2/2], Step [11860/67476], Loss: 4.6552\n",
      "Epoch [2/2], Step [11870/67476], Loss: 4.6385\n",
      "Epoch [2/2], Step [11880/67476], Loss: 4.5475\n",
      "Epoch [2/2], Step [11890/67476], Loss: 4.3941\n",
      "Epoch [2/2], Step [11900/67476], Loss: 4.4917\n",
      "Epoch [2/2], Step [11910/67476], Loss: 4.5855\n",
      "Epoch [2/2], Step [11920/67476], Loss: 4.4393\n",
      "Epoch [2/2], Step [11930/67476], Loss: 4.3854\n",
      "Epoch [2/2], Step [11940/67476], Loss: 4.6898\n",
      "Epoch [2/2], Step [11950/67476], Loss: 4.6330\n",
      "Epoch [2/2], Step [11960/67476], Loss: 4.4720\n",
      "Epoch [2/2], Step [11970/67476], Loss: 4.6641\n",
      "Epoch [2/2], Step [11980/67476], Loss: 4.5539\n",
      "Epoch [2/2], Step [11990/67476], Loss: 4.7165\n",
      "Epoch [2/2], Step [12000/67476], Loss: 4.6651\n",
      "Epoch [2/2], Step [12010/67476], Loss: 4.6089\n",
      "Epoch [2/2], Step [12020/67476], Loss: 4.5742\n",
      "Epoch [2/2], Step [12030/67476], Loss: 4.5615\n",
      "Epoch [2/2], Step [12040/67476], Loss: 4.5982\n",
      "Epoch [2/2], Step [12050/67476], Loss: 4.5253\n",
      "Epoch [2/2], Step [12060/67476], Loss: 4.7453\n",
      "Epoch [2/2], Step [12070/67476], Loss: 4.5308\n",
      "Epoch [2/2], Step [12080/67476], Loss: 4.6259\n",
      "Epoch [2/2], Step [12090/67476], Loss: 4.5330\n",
      "Epoch [2/2], Step [12100/67476], Loss: 4.5671\n",
      "Epoch [2/2], Step [12110/67476], Loss: 4.4232\n",
      "Epoch [2/2], Step [12120/67476], Loss: 4.6495\n",
      "Epoch [2/2], Step [12130/67476], Loss: 4.7166\n",
      "Epoch [2/2], Step [12140/67476], Loss: 4.7989\n",
      "Epoch [2/2], Step [12150/67476], Loss: 4.6303\n",
      "Epoch [2/2], Step [12160/67476], Loss: 4.5006\n",
      "Epoch [2/2], Step [12170/67476], Loss: 4.5011\n",
      "Epoch [2/2], Step [12180/67476], Loss: 4.4960\n",
      "Epoch [2/2], Step [12190/67476], Loss: 4.4188\n",
      "Epoch [2/2], Step [12200/67476], Loss: 4.5758\n",
      "Epoch [2/2], Step [12210/67476], Loss: 4.6882\n",
      "Epoch [2/2], Step [12220/67476], Loss: 4.6836\n",
      "Epoch [2/2], Step [12230/67476], Loss: 4.5984\n",
      "Epoch [2/2], Step [12240/67476], Loss: 4.4674\n",
      "Epoch [2/2], Step [12250/67476], Loss: 4.4178\n",
      "Epoch [2/2], Step [12260/67476], Loss: 4.5152\n",
      "Epoch [2/2], Step [12270/67476], Loss: 4.6630\n",
      "Epoch [2/2], Step [12280/67476], Loss: 4.4702\n",
      "Epoch [2/2], Step [12290/67476], Loss: 4.5222\n",
      "Epoch [2/2], Step [12300/67476], Loss: 4.5569\n",
      "Epoch [2/2], Step [12310/67476], Loss: 4.5124\n",
      "Epoch [2/2], Step [12320/67476], Loss: 4.5152\n",
      "Epoch [2/2], Step [12330/67476], Loss: 4.4958\n",
      "Epoch [2/2], Step [12340/67476], Loss: 4.4524\n",
      "Epoch [2/2], Step [12350/67476], Loss: 4.5047\n",
      "Epoch [2/2], Step [12360/67476], Loss: 4.6866\n",
      "Epoch [2/2], Step [12370/67476], Loss: 4.5472\n",
      "Epoch [2/2], Step [12380/67476], Loss: 4.5862\n",
      "Epoch [2/2], Step [12390/67476], Loss: 4.6201\n",
      "Epoch [2/2], Step [12400/67476], Loss: 4.3678\n",
      "Epoch [2/2], Step [12410/67476], Loss: 4.5641\n",
      "Epoch [2/2], Step [12420/67476], Loss: 4.5059\n",
      "Epoch [2/2], Step [12430/67476], Loss: 4.5453\n",
      "Epoch [2/2], Step [12440/67476], Loss: 4.5473\n",
      "Epoch [2/2], Step [12450/67476], Loss: 4.4790\n",
      "Epoch [2/2], Step [12460/67476], Loss: 4.5491\n",
      "Epoch [2/2], Step [12470/67476], Loss: 4.4512\n",
      "Epoch [2/2], Step [12480/67476], Loss: 4.7089\n",
      "Epoch [2/2], Step [12490/67476], Loss: 4.5213\n",
      "Epoch [2/2], Step [12500/67476], Loss: 4.5220\n",
      "Epoch [2/2], Step [12510/67476], Loss: 4.6162\n",
      "Epoch [2/2], Step [12520/67476], Loss: 4.5309\n",
      "Epoch [2/2], Step [12530/67476], Loss: 4.7014\n",
      "Epoch [2/2], Step [12540/67476], Loss: 4.8436\n",
      "Epoch [2/2], Step [12550/67476], Loss: 4.5350\n",
      "Epoch [2/2], Step [12560/67476], Loss: 4.6534\n",
      "Epoch [2/2], Step [12570/67476], Loss: 4.7648\n",
      "Epoch [2/2], Step [12580/67476], Loss: 4.5913\n",
      "Epoch [2/2], Step [12590/67476], Loss: 4.4517\n",
      "Epoch [2/2], Step [12600/67476], Loss: 4.6134\n",
      "Epoch [2/2], Step [12610/67476], Loss: 4.8072\n",
      "Epoch [2/2], Step [12620/67476], Loss: 4.6985\n",
      "Epoch [2/2], Step [12630/67476], Loss: 4.4373\n",
      "Epoch [2/2], Step [12640/67476], Loss: 4.5363\n",
      "Epoch [2/2], Step [12650/67476], Loss: 4.3837\n",
      "Epoch [2/2], Step [12660/67476], Loss: 4.5351\n",
      "Epoch [2/2], Step [12670/67476], Loss: 4.5183\n",
      "Epoch [2/2], Step [12680/67476], Loss: 4.6245\n",
      "Epoch [2/2], Step [12690/67476], Loss: 4.3901\n",
      "Epoch [2/2], Step [12700/67476], Loss: 4.5866\n",
      "Epoch [2/2], Step [12710/67476], Loss: 4.3904\n",
      "Epoch [2/2], Step [12720/67476], Loss: 4.6470\n",
      "Epoch [2/2], Step [12730/67476], Loss: 4.6422\n",
      "Epoch [2/2], Step [12740/67476], Loss: 4.6097\n",
      "Epoch [2/2], Step [12750/67476], Loss: 4.7189\n",
      "Epoch [2/2], Step [12760/67476], Loss: 4.7035\n",
      "Epoch [2/2], Step [12770/67476], Loss: 4.6827\n",
      "Epoch [2/2], Step [12780/67476], Loss: 4.5411\n",
      "Epoch [2/2], Step [12790/67476], Loss: 4.6978\n",
      "Epoch [2/2], Step [12800/67476], Loss: 4.3966\n",
      "Epoch [2/2], Step [12810/67476], Loss: 4.5819\n",
      "Epoch [2/2], Step [12820/67476], Loss: 4.6067\n",
      "Epoch [2/2], Step [12830/67476], Loss: 4.6586\n",
      "Epoch [2/2], Step [12840/67476], Loss: 4.5102\n",
      "Epoch [2/2], Step [12850/67476], Loss: 4.3925\n",
      "Epoch [2/2], Step [12860/67476], Loss: 4.6605\n",
      "Epoch [2/2], Step [12870/67476], Loss: 4.3767\n",
      "Epoch [2/2], Step [12880/67476], Loss: 4.5047\n",
      "Epoch [2/2], Step [12890/67476], Loss: 4.5408\n",
      "Epoch [2/2], Step [12900/67476], Loss: 4.7372\n",
      "Epoch [2/2], Step [12910/67476], Loss: 4.4886\n",
      "Epoch [2/2], Step [12920/67476], Loss: 4.3547\n",
      "Epoch [2/2], Step [12930/67476], Loss: 4.2488\n",
      "Epoch [2/2], Step [12940/67476], Loss: 4.7116\n",
      "Epoch [2/2], Step [12950/67476], Loss: 4.6976\n",
      "Epoch [2/2], Step [12960/67476], Loss: 4.8505\n",
      "Epoch [2/2], Step [12970/67476], Loss: 4.6204\n",
      "Epoch [2/2], Step [12980/67476], Loss: 4.4972\n",
      "Epoch [2/2], Step [12990/67476], Loss: 4.3420\n",
      "Epoch [2/2], Step [13000/67476], Loss: 4.5285\n",
      "Epoch [2/2], Step [13010/67476], Loss: 4.5425\n",
      "Epoch [2/2], Step [13020/67476], Loss: 4.6006\n",
      "Epoch [2/2], Step [13030/67476], Loss: 4.4744\n",
      "Epoch [2/2], Step [13040/67476], Loss: 4.4367\n",
      "Epoch [2/2], Step [13050/67476], Loss: 4.6481\n",
      "Epoch [2/2], Step [13060/67476], Loss: 4.4620\n",
      "Epoch [2/2], Step [13070/67476], Loss: 4.4952\n",
      "Epoch [2/2], Step [13080/67476], Loss: 4.4684\n",
      "Epoch [2/2], Step [13090/67476], Loss: 4.4301\n",
      "Epoch [2/2], Step [13100/67476], Loss: 4.4931\n",
      "Epoch [2/2], Step [13110/67476], Loss: 4.4263\n",
      "Epoch [2/2], Step [13120/67476], Loss: 4.5976\n",
      "Epoch [2/2], Step [13130/67476], Loss: 4.6572\n",
      "Epoch [2/2], Step [13140/67476], Loss: 4.5162\n",
      "Epoch [2/2], Step [13150/67476], Loss: 4.6505\n",
      "Epoch [2/2], Step [13160/67476], Loss: 4.3891\n",
      "Epoch [2/2], Step [13170/67476], Loss: 4.4413\n",
      "Epoch [2/2], Step [13180/67476], Loss: 4.5336\n",
      "Epoch [2/2], Step [13190/67476], Loss: 4.7140\n",
      "Epoch [2/2], Step [13200/67476], Loss: 4.7167\n",
      "Epoch [2/2], Step [13210/67476], Loss: 4.4179\n",
      "Epoch [2/2], Step [13220/67476], Loss: 4.5833\n",
      "Epoch [2/2], Step [13230/67476], Loss: 4.4120\n",
      "Epoch [2/2], Step [13240/67476], Loss: 4.5111\n",
      "Epoch [2/2], Step [13250/67476], Loss: 4.5562\n",
      "Epoch [2/2], Step [13260/67476], Loss: 4.5489\n",
      "Epoch [2/2], Step [13270/67476], Loss: 4.3143\n",
      "Epoch [2/2], Step [13280/67476], Loss: 4.6911\n",
      "Epoch [2/2], Step [13290/67476], Loss: 4.5951\n",
      "Epoch [2/2], Step [13300/67476], Loss: 4.7733\n",
      "Epoch [2/2], Step [13310/67476], Loss: 4.5980\n",
      "Epoch [2/2], Step [13320/67476], Loss: 4.7768\n",
      "Epoch [2/2], Step [13330/67476], Loss: 4.3136\n",
      "Epoch [2/2], Step [13340/67476], Loss: 4.5823\n",
      "Epoch [2/2], Step [13350/67476], Loss: 4.4924\n",
      "Epoch [2/2], Step [13360/67476], Loss: 4.5237\n",
      "Epoch [2/2], Step [13370/67476], Loss: 4.4987\n",
      "Epoch [2/2], Step [13380/67476], Loss: 4.6676\n",
      "Epoch [2/2], Step [13390/67476], Loss: 4.5290\n",
      "Epoch [2/2], Step [13400/67476], Loss: 4.3986\n",
      "Epoch [2/2], Step [13410/67476], Loss: 4.3871\n",
      "Epoch [2/2], Step [13420/67476], Loss: 4.6850\n",
      "Epoch [2/2], Step [13430/67476], Loss: 4.4800\n",
      "Epoch [2/2], Step [13440/67476], Loss: 4.4748\n",
      "Epoch [2/2], Step [13450/67476], Loss: 4.5766\n",
      "Epoch [2/2], Step [13460/67476], Loss: 4.5630\n",
      "Epoch [2/2], Step [13470/67476], Loss: 4.4220\n",
      "Epoch [2/2], Step [13480/67476], Loss: 4.6158\n",
      "Epoch [2/2], Step [13490/67476], Loss: 4.5593\n",
      "Epoch [2/2], Step [13500/67476], Loss: 4.5501\n",
      "Epoch [2/2], Step [13510/67476], Loss: 4.3810\n",
      "Epoch [2/2], Step [13520/67476], Loss: 4.5911\n",
      "Epoch [2/2], Step [13530/67476], Loss: 4.4242\n",
      "Epoch [2/2], Step [13540/67476], Loss: 4.4375\n",
      "Epoch [2/2], Step [13550/67476], Loss: 4.6130\n",
      "Epoch [2/2], Step [13560/67476], Loss: 4.6162\n",
      "Epoch [2/2], Step [13570/67476], Loss: 4.7031\n",
      "Epoch [2/2], Step [13580/67476], Loss: 4.4394\n",
      "Epoch [2/2], Step [13590/67476], Loss: 4.4817\n",
      "Epoch [2/2], Step [13600/67476], Loss: 4.4220\n",
      "Epoch [2/2], Step [13610/67476], Loss: 4.7924\n",
      "Epoch [2/2], Step [13620/67476], Loss: 4.4587\n",
      "Epoch [2/2], Step [13630/67476], Loss: 4.4640\n",
      "Epoch [2/2], Step [13640/67476], Loss: 4.3592\n",
      "Epoch [2/2], Step [13650/67476], Loss: 4.7891\n",
      "Epoch [2/2], Step [13660/67476], Loss: 4.7206\n",
      "Epoch [2/2], Step [13670/67476], Loss: 4.6387\n",
      "Epoch [2/2], Step [13680/67476], Loss: 4.5208\n",
      "Epoch [2/2], Step [13690/67476], Loss: 4.4784\n",
      "Epoch [2/2], Step [13700/67476], Loss: 4.6290\n",
      "Epoch [2/2], Step [13710/67476], Loss: 4.7872\n",
      "Epoch [2/2], Step [13720/67476], Loss: 4.5616\n",
      "Epoch [2/2], Step [13730/67476], Loss: 4.4897\n",
      "Epoch [2/2], Step [13740/67476], Loss: 4.4822\n",
      "Epoch [2/2], Step [13750/67476], Loss: 4.7274\n",
      "Epoch [2/2], Step [13760/67476], Loss: 4.5576\n",
      "Epoch [2/2], Step [13770/67476], Loss: 4.4792\n",
      "Epoch [2/2], Step [13780/67476], Loss: 4.5303\n",
      "Epoch [2/2], Step [13790/67476], Loss: 4.5477\n",
      "Epoch [2/2], Step [13800/67476], Loss: 4.3993\n",
      "Epoch [2/2], Step [13810/67476], Loss: 4.6076\n",
      "Epoch [2/2], Step [13820/67476], Loss: 4.3777\n",
      "Epoch [2/2], Step [13830/67476], Loss: 4.5750\n",
      "Epoch [2/2], Step [13840/67476], Loss: 4.6121\n",
      "Epoch [2/2], Step [13850/67476], Loss: 4.3116\n",
      "Epoch [2/2], Step [13860/67476], Loss: 4.3816\n",
      "Epoch [2/2], Step [13870/67476], Loss: 4.5533\n",
      "Epoch [2/2], Step [13880/67476], Loss: 4.6850\n",
      "Epoch [2/2], Step [13890/67476], Loss: 4.5928\n",
      "Epoch [2/2], Step [13900/67476], Loss: 4.6101\n",
      "Epoch [2/2], Step [13910/67476], Loss: 4.4358\n",
      "Epoch [2/2], Step [13920/67476], Loss: 4.3705\n",
      "Epoch [2/2], Step [13930/67476], Loss: 4.6408\n",
      "Epoch [2/2], Step [13940/67476], Loss: 4.5583\n",
      "Epoch [2/2], Step [13950/67476], Loss: 4.6110\n",
      "Epoch [2/2], Step [13960/67476], Loss: 4.5024\n",
      "Epoch [2/2], Step [13970/67476], Loss: 4.3315\n",
      "Epoch [2/2], Step [13980/67476], Loss: 4.4202\n",
      "Epoch [2/2], Step [13990/67476], Loss: 4.8134\n",
      "Epoch [2/2], Step [14000/67476], Loss: 4.4732\n",
      "Epoch [2/2], Step [14010/67476], Loss: 4.4952\n",
      "Epoch [2/2], Step [14020/67476], Loss: 4.5838\n",
      "Epoch [2/2], Step [14030/67476], Loss: 4.4528\n",
      "Epoch [2/2], Step [14040/67476], Loss: 4.6377\n",
      "Epoch [2/2], Step [14050/67476], Loss: 4.5828\n",
      "Epoch [2/2], Step [14060/67476], Loss: 4.4690\n",
      "Epoch [2/2], Step [14070/67476], Loss: 4.5567\n",
      "Epoch [2/2], Step [14080/67476], Loss: 4.5103\n",
      "Epoch [2/2], Step [14090/67476], Loss: 4.5831\n",
      "Epoch [2/2], Step [14100/67476], Loss: 4.4838\n",
      "Epoch [2/2], Step [14110/67476], Loss: 4.4071\n",
      "Epoch [2/2], Step [14120/67476], Loss: 4.5139\n",
      "Epoch [2/2], Step [14130/67476], Loss: 4.3966\n",
      "Epoch [2/2], Step [14140/67476], Loss: 4.6981\n",
      "Epoch [2/2], Step [14150/67476], Loss: 4.5807\n",
      "Epoch [2/2], Step [14160/67476], Loss: 4.5624\n",
      "Epoch [2/2], Step [14170/67476], Loss: 4.5231\n",
      "Epoch [2/2], Step [14180/67476], Loss: 4.3757\n",
      "Epoch [2/2], Step [14190/67476], Loss: 4.5998\n",
      "Epoch [2/2], Step [14200/67476], Loss: 4.6141\n",
      "Epoch [2/2], Step [14210/67476], Loss: 4.4690\n",
      "Epoch [2/2], Step [14220/67476], Loss: 4.6014\n",
      "Epoch [2/2], Step [14230/67476], Loss: 4.6729\n",
      "Epoch [2/2], Step [14240/67476], Loss: 4.6206\n",
      "Epoch [2/2], Step [14250/67476], Loss: 4.5749\n",
      "Epoch [2/2], Step [14260/67476], Loss: 4.2935\n",
      "Epoch [2/2], Step [14270/67476], Loss: 4.5960\n",
      "Epoch [2/2], Step [14280/67476], Loss: 4.6048\n",
      "Epoch [2/2], Step [14290/67476], Loss: 4.5089\n",
      "Epoch [2/2], Step [14300/67476], Loss: 4.5664\n",
      "Epoch [2/2], Step [14310/67476], Loss: 4.2522\n",
      "Epoch [2/2], Step [14320/67476], Loss: 4.6824\n",
      "Epoch [2/2], Step [14330/67476], Loss: 4.3444\n",
      "Epoch [2/2], Step [14340/67476], Loss: 4.6733\n",
      "Epoch [2/2], Step [14350/67476], Loss: 4.5510\n",
      "Epoch [2/2], Step [14360/67476], Loss: 4.4321\n",
      "Epoch [2/2], Step [14370/67476], Loss: 4.5270\n",
      "Epoch [2/2], Step [14380/67476], Loss: 4.4229\n",
      "Epoch [2/2], Step [14390/67476], Loss: 4.4667\n",
      "Epoch [2/2], Step [14400/67476], Loss: 4.5985\n",
      "Epoch [2/2], Step [14410/67476], Loss: 4.7186\n",
      "Epoch [2/2], Step [14420/67476], Loss: 4.6518\n",
      "Epoch [2/2], Step [14430/67476], Loss: 4.5607\n",
      "Epoch [2/2], Step [14440/67476], Loss: 4.5492\n",
      "Epoch [2/2], Step [14450/67476], Loss: 4.7578\n",
      "Epoch [2/2], Step [14460/67476], Loss: 4.6229\n",
      "Epoch [2/2], Step [14470/67476], Loss: 4.5620\n",
      "Epoch [2/2], Step [14480/67476], Loss: 4.6774\n",
      "Epoch [2/2], Step [14490/67476], Loss: 4.7706\n",
      "Epoch [2/2], Step [14500/67476], Loss: 4.3045\n",
      "Epoch [2/2], Step [14510/67476], Loss: 4.9821\n",
      "Epoch [2/2], Step [14520/67476], Loss: 4.7022\n",
      "Epoch [2/2], Step [14530/67476], Loss: 4.4741\n",
      "Epoch [2/2], Step [14540/67476], Loss: 4.5687\n",
      "Epoch [2/2], Step [14550/67476], Loss: 4.7104\n",
      "Epoch [2/2], Step [14560/67476], Loss: 4.7445\n",
      "Epoch [2/2], Step [14570/67476], Loss: 4.4883\n",
      "Epoch [2/2], Step [14580/67476], Loss: 4.7591\n",
      "Epoch [2/2], Step [14590/67476], Loss: 4.5765\n",
      "Epoch [2/2], Step [14600/67476], Loss: 4.4315\n",
      "Epoch [2/2], Step [14610/67476], Loss: 4.5455\n",
      "Epoch [2/2], Step [14620/67476], Loss: 4.3887\n",
      "Epoch [2/2], Step [14630/67476], Loss: 4.4934\n",
      "Epoch [2/2], Step [14640/67476], Loss: 4.4339\n",
      "Epoch [2/2], Step [14650/67476], Loss: 4.5018\n",
      "Epoch [2/2], Step [14660/67476], Loss: 4.6041\n",
      "Epoch [2/2], Step [14670/67476], Loss: 4.5755\n",
      "Epoch [2/2], Step [14680/67476], Loss: 4.4985\n",
      "Epoch [2/2], Step [14690/67476], Loss: 4.7009\n",
      "Epoch [2/2], Step [14700/67476], Loss: 4.5275\n",
      "Epoch [2/2], Step [14710/67476], Loss: 4.5389\n",
      "Epoch [2/2], Step [14720/67476], Loss: 4.6257\n",
      "Epoch [2/2], Step [14730/67476], Loss: 4.6282\n",
      "Epoch [2/2], Step [14740/67476], Loss: 4.3882\n",
      "Epoch [2/2], Step [14750/67476], Loss: 4.5425\n",
      "Epoch [2/2], Step [14760/67476], Loss: 4.4041\n",
      "Epoch [2/2], Step [14770/67476], Loss: 4.6484\n",
      "Epoch [2/2], Step [14780/67476], Loss: 4.4420\n",
      "Epoch [2/2], Step [14790/67476], Loss: 4.7514\n",
      "Epoch [2/2], Step [14800/67476], Loss: 4.6483\n",
      "Epoch [2/2], Step [14810/67476], Loss: 4.5924\n",
      "Epoch [2/2], Step [14820/67476], Loss: 4.5128\n",
      "Epoch [2/2], Step [14830/67476], Loss: 4.6034\n",
      "Epoch [2/2], Step [14840/67476], Loss: 4.5107\n",
      "Epoch [2/2], Step [14850/67476], Loss: 4.7624\n",
      "Epoch [2/2], Step [14860/67476], Loss: 4.5880\n",
      "Epoch [2/2], Step [14870/67476], Loss: 4.6448\n",
      "Epoch [2/2], Step [14880/67476], Loss: 4.5496\n",
      "Epoch [2/2], Step [14890/67476], Loss: 4.5063\n",
      "Epoch [2/2], Step [14900/67476], Loss: 4.4307\n",
      "Epoch [2/2], Step [14910/67476], Loss: 4.6440\n",
      "Epoch [2/2], Step [14920/67476], Loss: 4.6281\n",
      "Epoch [2/2], Step [14930/67476], Loss: 4.6612\n",
      "Epoch [2/2], Step [14940/67476], Loss: 4.5741\n",
      "Epoch [2/2], Step [14950/67476], Loss: 4.4818\n",
      "Epoch [2/2], Step [14960/67476], Loss: 4.5873\n",
      "Epoch [2/2], Step [14970/67476], Loss: 4.5244\n",
      "Epoch [2/2], Step [14980/67476], Loss: 4.6114\n",
      "Epoch [2/2], Step [14990/67476], Loss: 4.3902\n",
      "Epoch [2/2], Step [15000/67476], Loss: 4.5043\n",
      "Epoch [2/2], Step [15010/67476], Loss: 4.6478\n",
      "Epoch [2/2], Step [15020/67476], Loss: 4.7025\n",
      "Epoch [2/2], Step [15030/67476], Loss: 4.5533\n",
      "Epoch [2/2], Step [15040/67476], Loss: 4.4828\n",
      "Epoch [2/2], Step [15050/67476], Loss: 4.4613\n",
      "Epoch [2/2], Step [15060/67476], Loss: 4.6199\n",
      "Epoch [2/2], Step [15070/67476], Loss: 4.4624\n",
      "Epoch [2/2], Step [15080/67476], Loss: 4.5991\n",
      "Epoch [2/2], Step [15090/67476], Loss: 4.4586\n",
      "Epoch [2/2], Step [15100/67476], Loss: 4.6488\n",
      "Epoch [2/2], Step [15110/67476], Loss: 4.7158\n",
      "Epoch [2/2], Step [15120/67476], Loss: 4.4792\n",
      "Epoch [2/2], Step [15130/67476], Loss: 4.4394\n",
      "Epoch [2/2], Step [15140/67476], Loss: 4.5562\n",
      "Epoch [2/2], Step [15150/67476], Loss: 4.7438\n",
      "Epoch [2/2], Step [15160/67476], Loss: 4.6297\n",
      "Epoch [2/2], Step [15170/67476], Loss: 4.6582\n",
      "Epoch [2/2], Step [15180/67476], Loss: 4.6428\n",
      "Epoch [2/2], Step [15190/67476], Loss: 4.5920\n",
      "Epoch [2/2], Step [15200/67476], Loss: 4.5090\n",
      "Epoch [2/2], Step [15210/67476], Loss: 4.5625\n",
      "Epoch [2/2], Step [15220/67476], Loss: 4.5592\n",
      "Epoch [2/2], Step [15230/67476], Loss: 4.6168\n",
      "Epoch [2/2], Step [15240/67476], Loss: 4.5706\n",
      "Epoch [2/2], Step [15250/67476], Loss: 4.6612\n",
      "Epoch [2/2], Step [15260/67476], Loss: 4.7191\n",
      "Epoch [2/2], Step [15270/67476], Loss: 4.5780\n",
      "Epoch [2/2], Step [15280/67476], Loss: 4.4227\n",
      "Epoch [2/2], Step [15290/67476], Loss: 4.4137\n",
      "Epoch [2/2], Step [15300/67476], Loss: 4.4994\n",
      "Epoch [2/2], Step [15310/67476], Loss: 4.4184\n",
      "Epoch [2/2], Step [15320/67476], Loss: 4.4545\n",
      "Epoch [2/2], Step [15330/67476], Loss: 4.4630\n",
      "Epoch [2/2], Step [15340/67476], Loss: 4.6443\n",
      "Epoch [2/2], Step [15350/67476], Loss: 4.6264\n",
      "Epoch [2/2], Step [15360/67476], Loss: 4.5431\n",
      "Epoch [2/2], Step [15370/67476], Loss: 4.4089\n",
      "Epoch [2/2], Step [15380/67476], Loss: 4.5975\n",
      "Epoch [2/2], Step [15390/67476], Loss: 4.5358\n",
      "Epoch [2/2], Step [15400/67476], Loss: 4.6685\n",
      "Epoch [2/2], Step [15410/67476], Loss: 4.5453\n",
      "Epoch [2/2], Step [15420/67476], Loss: 4.5069\n",
      "Epoch [2/2], Step [15430/67476], Loss: 4.5702\n",
      "Epoch [2/2], Step [15440/67476], Loss: 4.3111\n",
      "Epoch [2/2], Step [15450/67476], Loss: 4.5719\n",
      "Epoch [2/2], Step [15460/67476], Loss: 4.4941\n",
      "Epoch [2/2], Step [15470/67476], Loss: 4.4861\n",
      "Epoch [2/2], Step [15480/67476], Loss: 4.5352\n",
      "Epoch [2/2], Step [15490/67476], Loss: 4.5029\n",
      "Epoch [2/2], Step [15500/67476], Loss: 4.5205\n",
      "Epoch [2/2], Step [15510/67476], Loss: 4.5765\n",
      "Epoch [2/2], Step [15520/67476], Loss: 4.5512\n",
      "Epoch [2/2], Step [15530/67476], Loss: 4.3835\n",
      "Epoch [2/2], Step [15540/67476], Loss: 4.6831\n",
      "Epoch [2/2], Step [15550/67476], Loss: 4.6077\n",
      "Epoch [2/2], Step [15560/67476], Loss: 4.6601\n",
      "Epoch [2/2], Step [15570/67476], Loss: 4.4687\n",
      "Epoch [2/2], Step [15580/67476], Loss: 4.1555\n",
      "Epoch [2/2], Step [15590/67476], Loss: 4.5186\n",
      "Epoch [2/2], Step [15600/67476], Loss: 4.4272\n",
      "Epoch [2/2], Step [15610/67476], Loss: 4.6376\n",
      "Epoch [2/2], Step [15620/67476], Loss: 4.4495\n",
      "Epoch [2/2], Step [15630/67476], Loss: 4.6875\n",
      "Epoch [2/2], Step [15640/67476], Loss: 4.4959\n",
      "Epoch [2/2], Step [15650/67476], Loss: 4.3955\n",
      "Epoch [2/2], Step [15660/67476], Loss: 4.3384\n",
      "Epoch [2/2], Step [15670/67476], Loss: 4.5870\n",
      "Epoch [2/2], Step [15680/67476], Loss: 4.5420\n",
      "Epoch [2/2], Step [15690/67476], Loss: 4.5887\n",
      "Epoch [2/2], Step [15700/67476], Loss: 4.5883\n",
      "Epoch [2/2], Step [15710/67476], Loss: 4.6143\n",
      "Epoch [2/2], Step [15720/67476], Loss: 4.2743\n",
      "Epoch [2/2], Step [15730/67476], Loss: 4.6850\n",
      "Epoch [2/2], Step [15740/67476], Loss: 4.5151\n",
      "Epoch [2/2], Step [15750/67476], Loss: 4.5524\n",
      "Epoch [2/2], Step [15760/67476], Loss: 4.5278\n",
      "Epoch [2/2], Step [15770/67476], Loss: 4.4238\n",
      "Epoch [2/2], Step [15780/67476], Loss: 4.4780\n",
      "Epoch [2/2], Step [15790/67476], Loss: 4.5408\n",
      "Epoch [2/2], Step [15800/67476], Loss: 4.5078\n",
      "Epoch [2/2], Step [15810/67476], Loss: 4.5322\n",
      "Epoch [2/2], Step [15820/67476], Loss: 4.5859\n",
      "Epoch [2/2], Step [15830/67476], Loss: 4.6935\n",
      "Epoch [2/2], Step [15840/67476], Loss: 4.5299\n",
      "Epoch [2/2], Step [15850/67476], Loss: 4.7922\n",
      "Epoch [2/2], Step [15860/67476], Loss: 4.6093\n",
      "Epoch [2/2], Step [15870/67476], Loss: 4.6034\n",
      "Epoch [2/2], Step [15880/67476], Loss: 4.5782\n",
      "Epoch [2/2], Step [15890/67476], Loss: 4.5487\n",
      "Epoch [2/2], Step [15900/67476], Loss: 4.6186\n",
      "Epoch [2/2], Step [15910/67476], Loss: 4.6281\n",
      "Epoch [2/2], Step [15920/67476], Loss: 4.5474\n",
      "Epoch [2/2], Step [15930/67476], Loss: 4.4346\n",
      "Epoch [2/2], Step [15940/67476], Loss: 4.4815\n",
      "Epoch [2/2], Step [15950/67476], Loss: 4.5033\n",
      "Epoch [2/2], Step [15960/67476], Loss: 4.2940\n",
      "Epoch [2/2], Step [15970/67476], Loss: 4.6343\n",
      "Epoch [2/2], Step [15980/67476], Loss: 4.6185\n",
      "Epoch [2/2], Step [15990/67476], Loss: 4.2608\n",
      "Epoch [2/2], Step [16000/67476], Loss: 4.4454\n",
      "Epoch [2/2], Step [16010/67476], Loss: 4.3960\n",
      "Epoch [2/2], Step [16020/67476], Loss: 4.6398\n",
      "Epoch [2/2], Step [16030/67476], Loss: 4.3286\n",
      "Epoch [2/2], Step [16040/67476], Loss: 4.6513\n",
      "Epoch [2/2], Step [16050/67476], Loss: 4.5095\n",
      "Epoch [2/2], Step [16060/67476], Loss: 4.3888\n",
      "Epoch [2/2], Step [16070/67476], Loss: 4.5138\n",
      "Epoch [2/2], Step [16080/67476], Loss: 4.5173\n",
      "Epoch [2/2], Step [16090/67476], Loss: 4.5651\n",
      "Epoch [2/2], Step [16100/67476], Loss: 4.5267\n",
      "Epoch [2/2], Step [16110/67476], Loss: 4.4868\n",
      "Epoch [2/2], Step [16120/67476], Loss: 4.4620\n",
      "Epoch [2/2], Step [16130/67476], Loss: 4.5217\n",
      "Epoch [2/2], Step [16140/67476], Loss: 4.7592\n",
      "Epoch [2/2], Step [16150/67476], Loss: 4.6176\n",
      "Epoch [2/2], Step [16160/67476], Loss: 4.4317\n",
      "Epoch [2/2], Step [16170/67476], Loss: 4.7299\n",
      "Epoch [2/2], Step [16180/67476], Loss: 4.3584\n",
      "Epoch [2/2], Step [16190/67476], Loss: 4.4905\n",
      "Epoch [2/2], Step [16200/67476], Loss: 4.6174\n",
      "Epoch [2/2], Step [16210/67476], Loss: 4.5935\n",
      "Epoch [2/2], Step [16220/67476], Loss: 4.6001\n",
      "Epoch [2/2], Step [16230/67476], Loss: 4.4914\n",
      "Epoch [2/2], Step [16240/67476], Loss: 4.5273\n",
      "Epoch [2/2], Step [16250/67476], Loss: 4.5325\n",
      "Epoch [2/2], Step [16260/67476], Loss: 4.7333\n",
      "Epoch [2/2], Step [16270/67476], Loss: 4.5184\n",
      "Epoch [2/2], Step [16280/67476], Loss: 4.4923\n",
      "Epoch [2/2], Step [16290/67476], Loss: 4.5740\n",
      "Epoch [2/2], Step [16300/67476], Loss: 4.4802\n",
      "Epoch [2/2], Step [16310/67476], Loss: 4.5591\n",
      "Epoch [2/2], Step [16320/67476], Loss: 4.5981\n",
      "Epoch [2/2], Step [16330/67476], Loss: 4.7664\n",
      "Epoch [2/2], Step [16340/67476], Loss: 4.4715\n",
      "Epoch [2/2], Step [16350/67476], Loss: 4.5664\n",
      "Epoch [2/2], Step [16360/67476], Loss: 4.5553\n",
      "Epoch [2/2], Step [16370/67476], Loss: 4.6150\n",
      "Epoch [2/2], Step [16380/67476], Loss: 4.5078\n",
      "Epoch [2/2], Step [16390/67476], Loss: 4.6731\n",
      "Epoch [2/2], Step [16400/67476], Loss: 4.6695\n",
      "Epoch [2/2], Step [16410/67476], Loss: 4.7673\n",
      "Epoch [2/2], Step [16420/67476], Loss: 4.7392\n",
      "Epoch [2/2], Step [16430/67476], Loss: 4.4530\n",
      "Epoch [2/2], Step [16440/67476], Loss: 4.2745\n",
      "Epoch [2/2], Step [16450/67476], Loss: 4.4611\n",
      "Epoch [2/2], Step [16460/67476], Loss: 4.6304\n",
      "Epoch [2/2], Step [16470/67476], Loss: 4.6258\n",
      "Epoch [2/2], Step [16480/67476], Loss: 4.6923\n",
      "Epoch [2/2], Step [16490/67476], Loss: 4.8049\n",
      "Epoch [2/2], Step [16500/67476], Loss: 4.6437\n",
      "Epoch [2/2], Step [16510/67476], Loss: 4.5533\n",
      "Epoch [2/2], Step [16520/67476], Loss: 4.6554\n",
      "Epoch [2/2], Step [16530/67476], Loss: 4.7659\n",
      "Epoch [2/2], Step [16540/67476], Loss: 4.6714\n",
      "Epoch [2/2], Step [16550/67476], Loss: 4.5967\n",
      "Epoch [2/2], Step [16560/67476], Loss: 4.5925\n",
      "Epoch [2/2], Step [16570/67476], Loss: 4.4611\n",
      "Epoch [2/2], Step [16580/67476], Loss: 4.5530\n",
      "Epoch [2/2], Step [16590/67476], Loss: 4.5815\n",
      "Epoch [2/2], Step [16600/67476], Loss: 4.4582\n",
      "Epoch [2/2], Step [16610/67476], Loss: 4.7330\n",
      "Epoch [2/2], Step [16620/67476], Loss: 4.6143\n",
      "Epoch [2/2], Step [16630/67476], Loss: 4.7348\n",
      "Epoch [2/2], Step [16640/67476], Loss: 4.6918\n",
      "Epoch [2/2], Step [16650/67476], Loss: 4.4880\n",
      "Epoch [2/2], Step [16660/67476], Loss: 4.5933\n",
      "Epoch [2/2], Step [16670/67476], Loss: 4.4721\n",
      "Epoch [2/2], Step [16680/67476], Loss: 4.4593\n",
      "Epoch [2/2], Step [16690/67476], Loss: 4.6514\n",
      "Epoch [2/2], Step [16700/67476], Loss: 4.6127\n",
      "Epoch [2/2], Step [16710/67476], Loss: 4.6288\n",
      "Epoch [2/2], Step [16720/67476], Loss: 4.7150\n",
      "Epoch [2/2], Step [16730/67476], Loss: 4.4956\n",
      "Epoch [2/2], Step [16740/67476], Loss: 4.5228\n",
      "Epoch [2/2], Step [16750/67476], Loss: 4.5526\n",
      "Epoch [2/2], Step [16760/67476], Loss: 4.5383\n",
      "Epoch [2/2], Step [16770/67476], Loss: 4.5446\n",
      "Epoch [2/2], Step [16780/67476], Loss: 4.4683\n",
      "Epoch [2/2], Step [16790/67476], Loss: 4.3344\n",
      "Epoch [2/2], Step [16800/67476], Loss: 4.6706\n",
      "Epoch [2/2], Step [16810/67476], Loss: 4.4557\n",
      "Epoch [2/2], Step [16820/67476], Loss: 4.6233\n",
      "Epoch [2/2], Step [16830/67476], Loss: 4.5018\n",
      "Epoch [2/2], Step [16840/67476], Loss: 4.6222\n",
      "Epoch [2/2], Step [16850/67476], Loss: 4.6084\n",
      "Epoch [2/2], Step [16860/67476], Loss: 4.5113\n",
      "Epoch [2/2], Step [16870/67476], Loss: 4.6126\n",
      "Epoch [2/2], Step [16880/67476], Loss: 4.5571\n",
      "Epoch [2/2], Step [16890/67476], Loss: 4.7473\n",
      "Epoch [2/2], Step [16900/67476], Loss: 4.6942\n",
      "Epoch [2/2], Step [16910/67476], Loss: 4.5084\n",
      "Epoch [2/2], Step [16920/67476], Loss: 4.5908\n",
      "Epoch [2/2], Step [16930/67476], Loss: 4.2720\n",
      "Epoch [2/2], Step [16940/67476], Loss: 4.4691\n",
      "Epoch [2/2], Step [16950/67476], Loss: 4.6806\n",
      "Epoch [2/2], Step [16960/67476], Loss: 4.6226\n",
      "Epoch [2/2], Step [16970/67476], Loss: 4.4926\n",
      "Epoch [2/2], Step [16980/67476], Loss: 4.6367\n",
      "Epoch [2/2], Step [16990/67476], Loss: 4.6486\n",
      "Epoch [2/2], Step [17000/67476], Loss: 4.6307\n",
      "Epoch [2/2], Step [17010/67476], Loss: 4.5829\n",
      "Epoch [2/2], Step [17020/67476], Loss: 4.4949\n",
      "Epoch [2/2], Step [17030/67476], Loss: 4.5412\n",
      "Epoch [2/2], Step [17040/67476], Loss: 4.7253\n",
      "Epoch [2/2], Step [17050/67476], Loss: 4.4109\n",
      "Epoch [2/2], Step [17060/67476], Loss: 4.5345\n",
      "Epoch [2/2], Step [17070/67476], Loss: 4.7852\n",
      "Epoch [2/2], Step [17080/67476], Loss: 4.5810\n",
      "Epoch [2/2], Step [17090/67476], Loss: 4.5032\n",
      "Epoch [2/2], Step [17100/67476], Loss: 4.4398\n",
      "Epoch [2/2], Step [17110/67476], Loss: 4.3903\n",
      "Epoch [2/2], Step [17120/67476], Loss: 4.5894\n",
      "Epoch [2/2], Step [17130/67476], Loss: 4.4706\n",
      "Epoch [2/2], Step [17140/67476], Loss: 4.6864\n",
      "Epoch [2/2], Step [17150/67476], Loss: 4.6175\n",
      "Epoch [2/2], Step [17160/67476], Loss: 4.5797\n",
      "Epoch [2/2], Step [17170/67476], Loss: 4.6416\n",
      "Epoch [2/2], Step [17180/67476], Loss: 4.4401\n",
      "Epoch [2/2], Step [17190/67476], Loss: 4.8004\n",
      "Epoch [2/2], Step [17200/67476], Loss: 4.5083\n",
      "Epoch [2/2], Step [17210/67476], Loss: 4.4985\n",
      "Epoch [2/2], Step [17220/67476], Loss: 4.4594\n",
      "Epoch [2/2], Step [17230/67476], Loss: 4.5135\n",
      "Epoch [2/2], Step [17240/67476], Loss: 4.6871\n",
      "Epoch [2/2], Step [17250/67476], Loss: 4.6588\n",
      "Epoch [2/2], Step [17260/67476], Loss: 4.4856\n",
      "Epoch [2/2], Step [17270/67476], Loss: 4.4317\n",
      "Epoch [2/2], Step [17280/67476], Loss: 4.7318\n",
      "Epoch [2/2], Step [17290/67476], Loss: 4.6311\n",
      "Epoch [2/2], Step [17300/67476], Loss: 4.6673\n",
      "Epoch [2/2], Step [17310/67476], Loss: 4.5391\n",
      "Epoch [2/2], Step [17320/67476], Loss: 4.6353\n",
      "Epoch [2/2], Step [17330/67476], Loss: 4.5912\n",
      "Epoch [2/2], Step [17340/67476], Loss: 4.6725\n",
      "Epoch [2/2], Step [17350/67476], Loss: 4.6878\n",
      "Epoch [2/2], Step [17360/67476], Loss: 4.4057\n",
      "Epoch [2/2], Step [17370/67476], Loss: 4.6951\n",
      "Epoch [2/2], Step [17380/67476], Loss: 4.6574\n",
      "Epoch [2/2], Step [17390/67476], Loss: 4.7226\n",
      "Epoch [2/2], Step [17400/67476], Loss: 4.6321\n",
      "Epoch [2/2], Step [17410/67476], Loss: 4.3593\n",
      "Epoch [2/2], Step [17420/67476], Loss: 4.5630\n",
      "Epoch [2/2], Step [17430/67476], Loss: 4.5943\n",
      "Epoch [2/2], Step [17440/67476], Loss: 4.5013\n",
      "Epoch [2/2], Step [17450/67476], Loss: 4.4597\n",
      "Epoch [2/2], Step [17460/67476], Loss: 4.5287\n",
      "Epoch [2/2], Step [17470/67476], Loss: 4.2435\n",
      "Epoch [2/2], Step [17480/67476], Loss: 4.5089\n",
      "Epoch [2/2], Step [17490/67476], Loss: 4.7478\n",
      "Epoch [2/2], Step [17500/67476], Loss: 4.5482\n",
      "Epoch [2/2], Step [17510/67476], Loss: 4.4893\n",
      "Epoch [2/2], Step [17520/67476], Loss: 4.3455\n",
      "Epoch [2/2], Step [17530/67476], Loss: 4.5126\n",
      "Epoch [2/2], Step [17540/67476], Loss: 4.6518\n",
      "Epoch [2/2], Step [17550/67476], Loss: 4.5037\n",
      "Epoch [2/2], Step [17560/67476], Loss: 4.5571\n",
      "Epoch [2/2], Step [17570/67476], Loss: 4.4518\n",
      "Epoch [2/2], Step [17580/67476], Loss: 4.5218\n",
      "Epoch [2/2], Step [17590/67476], Loss: 4.3558\n",
      "Epoch [2/2], Step [17600/67476], Loss: 4.6718\n",
      "Epoch [2/2], Step [17610/67476], Loss: 4.5575\n",
      "Epoch [2/2], Step [17620/67476], Loss: 4.6301\n",
      "Epoch [2/2], Step [17630/67476], Loss: 4.3794\n",
      "Epoch [2/2], Step [17640/67476], Loss: 4.3692\n",
      "Epoch [2/2], Step [17650/67476], Loss: 4.3705\n",
      "Epoch [2/2], Step [17660/67476], Loss: 4.8375\n",
      "Epoch [2/2], Step [17670/67476], Loss: 4.5842\n",
      "Epoch [2/2], Step [17680/67476], Loss: 4.5867\n",
      "Epoch [2/2], Step [17690/67476], Loss: 4.3658\n",
      "Epoch [2/2], Step [17700/67476], Loss: 4.8316\n",
      "Epoch [2/2], Step [17710/67476], Loss: 4.4916\n",
      "Epoch [2/2], Step [17720/67476], Loss: 4.6699\n",
      "Epoch [2/2], Step [17730/67476], Loss: 4.5951\n",
      "Epoch [2/2], Step [17740/67476], Loss: 4.5013\n",
      "Epoch [2/2], Step [17750/67476], Loss: 4.6023\n",
      "Epoch [2/2], Step [17760/67476], Loss: 4.5641\n",
      "Epoch [2/2], Step [17770/67476], Loss: 4.6163\n",
      "Epoch [2/2], Step [17780/67476], Loss: 4.4687\n",
      "Epoch [2/2], Step [17790/67476], Loss: 4.4129\n",
      "Epoch [2/2], Step [17800/67476], Loss: 4.5769\n",
      "Epoch [2/2], Step [17810/67476], Loss: 4.6335\n",
      "Epoch [2/2], Step [17820/67476], Loss: 4.3893\n",
      "Epoch [2/2], Step [17830/67476], Loss: 4.5058\n",
      "Epoch [2/2], Step [17840/67476], Loss: 4.2121\n",
      "Epoch [2/2], Step [17850/67476], Loss: 4.4853\n",
      "Epoch [2/2], Step [17860/67476], Loss: 4.5375\n",
      "Epoch [2/2], Step [17870/67476], Loss: 4.6585\n",
      "Epoch [2/2], Step [17880/67476], Loss: 4.6524\n",
      "Epoch [2/2], Step [17890/67476], Loss: 4.3952\n",
      "Epoch [2/2], Step [17900/67476], Loss: 4.4393\n",
      "Epoch [2/2], Step [17910/67476], Loss: 4.4634\n",
      "Epoch [2/2], Step [17920/67476], Loss: 4.5782\n",
      "Epoch [2/2], Step [17930/67476], Loss: 4.4159\n",
      "Epoch [2/2], Step [17940/67476], Loss: 4.7202\n",
      "Epoch [2/2], Step [17950/67476], Loss: 4.4208\n",
      "Epoch [2/2], Step [17960/67476], Loss: 4.6330\n",
      "Epoch [2/2], Step [17970/67476], Loss: 4.4003\n",
      "Epoch [2/2], Step [17980/67476], Loss: 4.5144\n",
      "Epoch [2/2], Step [17990/67476], Loss: 4.6721\n",
      "Epoch [2/2], Step [18000/67476], Loss: 4.6558\n",
      "Epoch [2/2], Step [18010/67476], Loss: 4.5353\n",
      "Epoch [2/2], Step [18020/67476], Loss: 4.5506\n",
      "Epoch [2/2], Step [18030/67476], Loss: 4.4298\n",
      "Epoch [2/2], Step [18040/67476], Loss: 4.5469\n",
      "Epoch [2/2], Step [18050/67476], Loss: 4.4267\n",
      "Epoch [2/2], Step [18060/67476], Loss: 4.3976\n",
      "Epoch [2/2], Step [18070/67476], Loss: 4.6141\n",
      "Epoch [2/2], Step [18080/67476], Loss: 4.7318\n",
      "Epoch [2/2], Step [18090/67476], Loss: 4.4961\n",
      "Epoch [2/2], Step [18100/67476], Loss: 4.5399\n",
      "Epoch [2/2], Step [18110/67476], Loss: 4.6040\n",
      "Epoch [2/2], Step [18120/67476], Loss: 4.5809\n",
      "Epoch [2/2], Step [18130/67476], Loss: 4.5359\n",
      "Epoch [2/2], Step [18140/67476], Loss: 4.3862\n",
      "Epoch [2/2], Step [18150/67476], Loss: 4.7010\n",
      "Epoch [2/2], Step [18160/67476], Loss: 4.5584\n",
      "Epoch [2/2], Step [18170/67476], Loss: 4.4318\n",
      "Epoch [2/2], Step [18180/67476], Loss: 4.6216\n",
      "Epoch [2/2], Step [18190/67476], Loss: 4.6107\n",
      "Epoch [2/2], Step [18200/67476], Loss: 4.6059\n",
      "Epoch [2/2], Step [18210/67476], Loss: 4.6739\n",
      "Epoch [2/2], Step [18220/67476], Loss: 4.6554\n",
      "Epoch [2/2], Step [18230/67476], Loss: 4.4648\n",
      "Epoch [2/2], Step [18240/67476], Loss: 4.3720\n",
      "Epoch [2/2], Step [18250/67476], Loss: 4.3673\n",
      "Epoch [2/2], Step [18260/67476], Loss: 4.6214\n",
      "Epoch [2/2], Step [18270/67476], Loss: 4.5224\n",
      "Epoch [2/2], Step [18280/67476], Loss: 4.5443\n",
      "Epoch [2/2], Step [18290/67476], Loss: 4.5322\n",
      "Epoch [2/2], Step [18300/67476], Loss: 4.6426\n",
      "Epoch [2/2], Step [18310/67476], Loss: 4.4800\n",
      "Epoch [2/2], Step [18320/67476], Loss: 4.7542\n",
      "Epoch [2/2], Step [18330/67476], Loss: 4.5688\n",
      "Epoch [2/2], Step [18340/67476], Loss: 4.7561\n",
      "Epoch [2/2], Step [18350/67476], Loss: 4.5398\n",
      "Epoch [2/2], Step [18360/67476], Loss: 4.5814\n",
      "Epoch [2/2], Step [18370/67476], Loss: 4.6647\n",
      "Epoch [2/2], Step [18380/67476], Loss: 4.5513\n",
      "Epoch [2/2], Step [18390/67476], Loss: 4.4494\n",
      "Epoch [2/2], Step [18400/67476], Loss: 4.4552\n",
      "Epoch [2/2], Step [18410/67476], Loss: 4.7634\n",
      "Epoch [2/2], Step [18420/67476], Loss: 4.2655\n",
      "Epoch [2/2], Step [18430/67476], Loss: 4.4529\n",
      "Epoch [2/2], Step [18440/67476], Loss: 4.6253\n",
      "Epoch [2/2], Step [18450/67476], Loss: 4.5695\n",
      "Epoch [2/2], Step [18460/67476], Loss: 4.5326\n",
      "Epoch [2/2], Step [18470/67476], Loss: 4.5337\n",
      "Epoch [2/2], Step [18480/67476], Loss: 4.5274\n",
      "Epoch [2/2], Step [18490/67476], Loss: 4.5067\n",
      "Epoch [2/2], Step [18500/67476], Loss: 4.6817\n",
      "Epoch [2/2], Step [18510/67476], Loss: 4.2904\n",
      "Epoch [2/2], Step [18520/67476], Loss: 4.5056\n",
      "Epoch [2/2], Step [18530/67476], Loss: 4.5997\n",
      "Epoch [2/2], Step [18540/67476], Loss: 4.6306\n",
      "Epoch [2/2], Step [18550/67476], Loss: 4.3407\n",
      "Epoch [2/2], Step [18560/67476], Loss: 4.6155\n",
      "Epoch [2/2], Step [18570/67476], Loss: 4.5928\n",
      "Epoch [2/2], Step [18580/67476], Loss: 4.6105\n",
      "Epoch [2/2], Step [18590/67476], Loss: 4.5819\n",
      "Epoch [2/2], Step [18600/67476], Loss: 4.6989\n",
      "Epoch [2/2], Step [18610/67476], Loss: 4.7815\n",
      "Epoch [2/2], Step [18620/67476], Loss: 4.5574\n",
      "Epoch [2/2], Step [18630/67476], Loss: 4.5455\n",
      "Epoch [2/2], Step [18640/67476], Loss: 4.5483\n",
      "Epoch [2/2], Step [18650/67476], Loss: 4.5187\n",
      "Epoch [2/2], Step [18660/67476], Loss: 4.5322\n",
      "Epoch [2/2], Step [18670/67476], Loss: 4.4497\n",
      "Epoch [2/2], Step [18680/67476], Loss: 4.5190\n",
      "Epoch [2/2], Step [18690/67476], Loss: 4.4551\n",
      "Epoch [2/2], Step [18700/67476], Loss: 4.4982\n",
      "Epoch [2/2], Step [18710/67476], Loss: 4.5903\n",
      "Epoch [2/2], Step [18720/67476], Loss: 4.5382\n",
      "Epoch [2/2], Step [18730/67476], Loss: 4.6695\n",
      "Epoch [2/2], Step [18740/67476], Loss: 4.5589\n",
      "Epoch [2/2], Step [18750/67476], Loss: 4.5931\n",
      "Epoch [2/2], Step [18760/67476], Loss: 4.5197\n",
      "Epoch [2/2], Step [18770/67476], Loss: 4.7353\n",
      "Epoch [2/2], Step [18780/67476], Loss: 4.8394\n",
      "Epoch [2/2], Step [18790/67476], Loss: 4.4614\n",
      "Epoch [2/2], Step [18800/67476], Loss: 4.7361\n",
      "Epoch [2/2], Step [18810/67476], Loss: 4.7182\n",
      "Epoch [2/2], Step [18820/67476], Loss: 4.4925\n",
      "Epoch [2/2], Step [18830/67476], Loss: 4.4820\n",
      "Epoch [2/2], Step [18840/67476], Loss: 4.5664\n",
      "Epoch [2/2], Step [18850/67476], Loss: 4.5174\n",
      "Epoch [2/2], Step [18860/67476], Loss: 4.4039\n",
      "Epoch [2/2], Step [18870/67476], Loss: 4.4531\n",
      "Epoch [2/2], Step [18880/67476], Loss: 4.4421\n",
      "Epoch [2/2], Step [18890/67476], Loss: 4.2774\n",
      "Epoch [2/2], Step [18900/67476], Loss: 4.3273\n",
      "Epoch [2/2], Step [18910/67476], Loss: 4.5749\n",
      "Epoch [2/2], Step [18920/67476], Loss: 4.2831\n",
      "Epoch [2/2], Step [18930/67476], Loss: 4.4916\n",
      "Epoch [2/2], Step [18940/67476], Loss: 4.5402\n",
      "Epoch [2/2], Step [18950/67476], Loss: 4.5421\n",
      "Epoch [2/2], Step [18960/67476], Loss: 4.7260\n",
      "Epoch [2/2], Step [18970/67476], Loss: 4.6824\n",
      "Epoch [2/2], Step [18980/67476], Loss: 4.5788\n",
      "Epoch [2/2], Step [18990/67476], Loss: 4.6161\n",
      "Epoch [2/2], Step [19000/67476], Loss: 4.5631\n",
      "Epoch [2/2], Step [19010/67476], Loss: 4.6101\n",
      "Epoch [2/2], Step [19020/67476], Loss: 4.6792\n",
      "Epoch [2/2], Step [19030/67476], Loss: 4.3754\n",
      "Epoch [2/2], Step [19040/67476], Loss: 4.5193\n",
      "Epoch [2/2], Step [19050/67476], Loss: 4.7047\n",
      "Epoch [2/2], Step [19060/67476], Loss: 4.4126\n",
      "Epoch [2/2], Step [19070/67476], Loss: 4.6977\n",
      "Epoch [2/2], Step [19080/67476], Loss: 4.2825\n",
      "Epoch [2/2], Step [19090/67476], Loss: 4.4935\n",
      "Epoch [2/2], Step [19100/67476], Loss: 4.5525\n",
      "Epoch [2/2], Step [19110/67476], Loss: 4.6671\n",
      "Epoch [2/2], Step [19120/67476], Loss: 4.5036\n",
      "Epoch [2/2], Step [19130/67476], Loss: 4.4257\n",
      "Epoch [2/2], Step [19140/67476], Loss: 4.6772\n",
      "Epoch [2/2], Step [19150/67476], Loss: 4.5894\n",
      "Epoch [2/2], Step [19160/67476], Loss: 4.5748\n",
      "Epoch [2/2], Step [19170/67476], Loss: 4.4424\n",
      "Epoch [2/2], Step [19180/67476], Loss: 4.6683\n",
      "Epoch [2/2], Step [19190/67476], Loss: 4.6604\n",
      "Epoch [2/2], Step [19200/67476], Loss: 4.6403\n",
      "Epoch [2/2], Step [19210/67476], Loss: 4.3929\n",
      "Epoch [2/2], Step [19220/67476], Loss: 4.5915\n",
      "Epoch [2/2], Step [19230/67476], Loss: 4.6301\n",
      "Epoch [2/2], Step [19240/67476], Loss: 4.4341\n",
      "Epoch [2/2], Step [19250/67476], Loss: 4.5093\n",
      "Epoch [2/2], Step [19260/67476], Loss: 4.7884\n",
      "Epoch [2/2], Step [19270/67476], Loss: 4.6310\n",
      "Epoch [2/2], Step [19280/67476], Loss: 4.5513\n",
      "Epoch [2/2], Step [19290/67476], Loss: 4.7878\n",
      "Epoch [2/2], Step [19300/67476], Loss: 4.5289\n",
      "Epoch [2/2], Step [19310/67476], Loss: 4.7173\n",
      "Epoch [2/2], Step [19320/67476], Loss: 4.5900\n",
      "Epoch [2/2], Step [19330/67476], Loss: 4.4095\n",
      "Epoch [2/2], Step [19340/67476], Loss: 4.3973\n",
      "Epoch [2/2], Step [19350/67476], Loss: 4.5144\n",
      "Epoch [2/2], Step [19360/67476], Loss: 4.6009\n",
      "Epoch [2/2], Step [19370/67476], Loss: 4.4074\n",
      "Epoch [2/2], Step [19380/67476], Loss: 4.6847\n",
      "Epoch [2/2], Step [19390/67476], Loss: 4.7394\n",
      "Epoch [2/2], Step [19400/67476], Loss: 4.5385\n",
      "Epoch [2/2], Step [19410/67476], Loss: 4.4827\n",
      "Epoch [2/2], Step [19420/67476], Loss: 4.4668\n",
      "Epoch [2/2], Step [19430/67476], Loss: 4.7315\n",
      "Epoch [2/2], Step [19440/67476], Loss: 4.3261\n",
      "Epoch [2/2], Step [19450/67476], Loss: 4.4047\n",
      "Epoch [2/2], Step [19460/67476], Loss: 4.2681\n",
      "Epoch [2/2], Step [19470/67476], Loss: 4.7231\n",
      "Epoch [2/2], Step [19480/67476], Loss: 4.5459\n",
      "Epoch [2/2], Step [19490/67476], Loss: 4.5232\n",
      "Epoch [2/2], Step [19500/67476], Loss: 4.5927\n",
      "Epoch [2/2], Step [19510/67476], Loss: 4.2386\n",
      "Epoch [2/2], Step [19520/67476], Loss: 4.4778\n",
      "Epoch [2/2], Step [19530/67476], Loss: 4.4901\n",
      "Epoch [2/2], Step [19540/67476], Loss: 4.5896\n",
      "Epoch [2/2], Step [19550/67476], Loss: 4.6030\n",
      "Epoch [2/2], Step [19560/67476], Loss: 4.5227\n",
      "Epoch [2/2], Step [19570/67476], Loss: 4.5943\n",
      "Epoch [2/2], Step [19580/67476], Loss: 4.4097\n",
      "Epoch [2/2], Step [19590/67476], Loss: 4.4513\n",
      "Epoch [2/2], Step [19600/67476], Loss: 4.6337\n",
      "Epoch [2/2], Step [19610/67476], Loss: 4.3453\n",
      "Epoch [2/2], Step [19620/67476], Loss: 4.5531\n",
      "Epoch [2/2], Step [19630/67476], Loss: 4.5349\n",
      "Epoch [2/2], Step [19640/67476], Loss: 4.5993\n",
      "Epoch [2/2], Step [19650/67476], Loss: 4.3212\n",
      "Epoch [2/2], Step [19660/67476], Loss: 4.6620\n",
      "Epoch [2/2], Step [19670/67476], Loss: 4.5859\n",
      "Epoch [2/2], Step [19680/67476], Loss: 4.5336\n",
      "Epoch [2/2], Step [19690/67476], Loss: 4.4846\n",
      "Epoch [2/2], Step [19700/67476], Loss: 4.2683\n",
      "Epoch [2/2], Step [19710/67476], Loss: 4.5442\n",
      "Epoch [2/2], Step [19720/67476], Loss: 4.5974\n",
      "Epoch [2/2], Step [19730/67476], Loss: 4.6848\n",
      "Epoch [2/2], Step [19740/67476], Loss: 4.4906\n",
      "Epoch [2/2], Step [19750/67476], Loss: 4.5620\n",
      "Epoch [2/2], Step [19760/67476], Loss: 4.5649\n",
      "Epoch [2/2], Step [19770/67476], Loss: 4.6184\n",
      "Epoch [2/2], Step [19780/67476], Loss: 4.4534\n",
      "Epoch [2/2], Step [19790/67476], Loss: 4.4478\n",
      "Epoch [2/2], Step [19800/67476], Loss: 4.5166\n",
      "Epoch [2/2], Step [19810/67476], Loss: 4.6778\n",
      "Epoch [2/2], Step [19820/67476], Loss: 4.5122\n",
      "Epoch [2/2], Step [19830/67476], Loss: 4.6336\n",
      "Epoch [2/2], Step [19840/67476], Loss: 4.5712\n",
      "Epoch [2/2], Step [19850/67476], Loss: 4.5663\n",
      "Epoch [2/2], Step [19860/67476], Loss: 4.4706\n",
      "Epoch [2/2], Step [19870/67476], Loss: 4.6134\n",
      "Epoch [2/2], Step [19880/67476], Loss: 4.5345\n",
      "Epoch [2/2], Step [19890/67476], Loss: 4.6301\n",
      "Epoch [2/2], Step [19900/67476], Loss: 4.6138\n",
      "Epoch [2/2], Step [19910/67476], Loss: 4.5134\n",
      "Epoch [2/2], Step [19920/67476], Loss: 4.4417\n",
      "Epoch [2/2], Step [19930/67476], Loss: 4.3013\n",
      "Epoch [2/2], Step [19940/67476], Loss: 4.5758\n",
      "Epoch [2/2], Step [19950/67476], Loss: 4.5987\n",
      "Epoch [2/2], Step [19960/67476], Loss: 4.6436\n",
      "Epoch [2/2], Step [19970/67476], Loss: 4.4500\n",
      "Epoch [2/2], Step [19980/67476], Loss: 4.7237\n",
      "Epoch [2/2], Step [19990/67476], Loss: 4.3321\n",
      "Epoch [2/2], Step [20000/67476], Loss: 4.5292\n",
      "Epoch [2/2], Step [20010/67476], Loss: 4.5792\n",
      "Epoch [2/2], Step [20020/67476], Loss: 4.4285\n",
      "Epoch [2/2], Step [20030/67476], Loss: 4.5185\n",
      "Epoch [2/2], Step [20040/67476], Loss: 4.4203\n",
      "Epoch [2/2], Step [20050/67476], Loss: 4.5236\n",
      "Epoch [2/2], Step [20060/67476], Loss: 4.5816\n",
      "Epoch [2/2], Step [20070/67476], Loss: 4.6215\n",
      "Epoch [2/2], Step [20080/67476], Loss: 4.6417\n",
      "Epoch [2/2], Step [20090/67476], Loss: 4.5143\n",
      "Epoch [2/2], Step [20100/67476], Loss: 4.5650\n",
      "Epoch [2/2], Step [20110/67476], Loss: 4.5511\n",
      "Epoch [2/2], Step [20120/67476], Loss: 4.6820\n",
      "Epoch [2/2], Step [20130/67476], Loss: 4.4684\n",
      "Epoch [2/2], Step [20140/67476], Loss: 4.6593\n",
      "Epoch [2/2], Step [20150/67476], Loss: 4.5740\n",
      "Epoch [2/2], Step [20160/67476], Loss: 4.5764\n",
      "Epoch [2/2], Step [20170/67476], Loss: 4.5570\n",
      "Epoch [2/2], Step [20180/67476], Loss: 4.4864\n",
      "Epoch [2/2], Step [20190/67476], Loss: 4.4273\n",
      "Epoch [2/2], Step [20200/67476], Loss: 4.7473\n",
      "Epoch [2/2], Step [20210/67476], Loss: 4.5980\n",
      "Epoch [2/2], Step [20220/67476], Loss: 4.8205\n",
      "Epoch [2/2], Step [20230/67476], Loss: 4.6321\n",
      "Epoch [2/2], Step [20240/67476], Loss: 4.4309\n",
      "Epoch [2/2], Step [20250/67476], Loss: 4.6960\n",
      "Epoch [2/2], Step [20260/67476], Loss: 4.6753\n",
      "Epoch [2/2], Step [20270/67476], Loss: 4.8025\n",
      "Epoch [2/2], Step [20280/67476], Loss: 4.4983\n",
      "Epoch [2/2], Step [20290/67476], Loss: 4.6433\n",
      "Epoch [2/2], Step [20300/67476], Loss: 4.6598\n",
      "Epoch [2/2], Step [20310/67476], Loss: 4.6917\n",
      "Epoch [2/2], Step [20320/67476], Loss: 4.5918\n",
      "Epoch [2/2], Step [20330/67476], Loss: 4.6481\n",
      "Epoch [2/2], Step [20340/67476], Loss: 4.5470\n",
      "Epoch [2/2], Step [20350/67476], Loss: 4.4880\n",
      "Epoch [2/2], Step [20360/67476], Loss: 4.8161\n",
      "Epoch [2/2], Step [20370/67476], Loss: 4.4245\n",
      "Epoch [2/2], Step [20380/67476], Loss: 4.5766\n",
      "Epoch [2/2], Step [20390/67476], Loss: 4.6494\n",
      "Epoch [2/2], Step [20400/67476], Loss: 4.6170\n",
      "Epoch [2/2], Step [20410/67476], Loss: 4.5248\n",
      "Epoch [2/2], Step [20420/67476], Loss: 4.4129\n",
      "Epoch [2/2], Step [20430/67476], Loss: 4.7947\n",
      "Epoch [2/2], Step [20440/67476], Loss: 4.4364\n",
      "Epoch [2/2], Step [20450/67476], Loss: 4.6488\n",
      "Epoch [2/2], Step [20460/67476], Loss: 4.6326\n",
      "Epoch [2/2], Step [20470/67476], Loss: 4.5975\n",
      "Epoch [2/2], Step [20480/67476], Loss: 4.5664\n",
      "Epoch [2/2], Step [20490/67476], Loss: 4.5214\n",
      "Epoch [2/2], Step [20500/67476], Loss: 4.6565\n",
      "Epoch [2/2], Step [20510/67476], Loss: 4.7886\n",
      "Epoch [2/2], Step [20520/67476], Loss: 4.5721\n",
      "Epoch [2/2], Step [20530/67476], Loss: 4.6767\n",
      "Epoch [2/2], Step [20540/67476], Loss: 4.6441\n",
      "Epoch [2/2], Step [20550/67476], Loss: 4.6814\n",
      "Epoch [2/2], Step [20560/67476], Loss: 4.7310\n",
      "Epoch [2/2], Step [20570/67476], Loss: 4.3766\n",
      "Epoch [2/2], Step [20580/67476], Loss: 4.7625\n",
      "Epoch [2/2], Step [20590/67476], Loss: 4.5017\n",
      "Epoch [2/2], Step [20600/67476], Loss: 4.4460\n",
      "Epoch [2/2], Step [20610/67476], Loss: 4.3625\n",
      "Epoch [2/2], Step [20620/67476], Loss: 4.6379\n",
      "Epoch [2/2], Step [20630/67476], Loss: 4.6071\n",
      "Epoch [2/2], Step [20640/67476], Loss: 4.5563\n",
      "Epoch [2/2], Step [20650/67476], Loss: 4.4486\n",
      "Epoch [2/2], Step [20660/67476], Loss: 4.8122\n",
      "Epoch [2/2], Step [20670/67476], Loss: 4.6591\n",
      "Epoch [2/2], Step [20680/67476], Loss: 4.6148\n",
      "Epoch [2/2], Step [20690/67476], Loss: 4.2982\n",
      "Epoch [2/2], Step [20700/67476], Loss: 4.6761\n",
      "Epoch [2/2], Step [20710/67476], Loss: 4.7477\n",
      "Epoch [2/2], Step [20720/67476], Loss: 4.7120\n",
      "Epoch [2/2], Step [20730/67476], Loss: 4.5655\n",
      "Epoch [2/2], Step [20740/67476], Loss: 4.5182\n",
      "Epoch [2/2], Step [20750/67476], Loss: 4.6155\n",
      "Epoch [2/2], Step [20760/67476], Loss: 4.5409\n",
      "Epoch [2/2], Step [20770/67476], Loss: 4.3119\n",
      "Epoch [2/2], Step [20780/67476], Loss: 4.6020\n",
      "Epoch [2/2], Step [20790/67476], Loss: 4.4968\n",
      "Epoch [2/2], Step [20800/67476], Loss: 4.7618\n",
      "Epoch [2/2], Step [20810/67476], Loss: 4.4844\n",
      "Epoch [2/2], Step [20820/67476], Loss: 4.6243\n",
      "Epoch [2/2], Step [20830/67476], Loss: 4.4191\n",
      "Epoch [2/2], Step [20840/67476], Loss: 4.4256\n",
      "Epoch [2/2], Step [20850/67476], Loss: 4.6635\n",
      "Epoch [2/2], Step [20860/67476], Loss: 4.5858\n",
      "Epoch [2/2], Step [20870/67476], Loss: 4.5527\n",
      "Epoch [2/2], Step [20880/67476], Loss: 4.5459\n",
      "Epoch [2/2], Step [20890/67476], Loss: 4.5132\n",
      "Epoch [2/2], Step [20900/67476], Loss: 4.6334\n",
      "Epoch [2/2], Step [20910/67476], Loss: 4.6262\n",
      "Epoch [2/2], Step [20920/67476], Loss: 4.5652\n",
      "Epoch [2/2], Step [20930/67476], Loss: 4.7562\n",
      "Epoch [2/2], Step [20940/67476], Loss: 4.6666\n",
      "Epoch [2/2], Step [20950/67476], Loss: 4.3572\n",
      "Epoch [2/2], Step [20960/67476], Loss: 4.5793\n",
      "Epoch [2/2], Step [20970/67476], Loss: 4.4971\n",
      "Epoch [2/2], Step [20980/67476], Loss: 4.7723\n",
      "Epoch [2/2], Step [20990/67476], Loss: 4.5743\n",
      "Epoch [2/2], Step [21000/67476], Loss: 4.5250\n",
      "Epoch [2/2], Step [21010/67476], Loss: 4.5199\n",
      "Epoch [2/2], Step [21020/67476], Loss: 4.6495\n",
      "Epoch [2/2], Step [21030/67476], Loss: 4.6012\n",
      "Epoch [2/2], Step [21040/67476], Loss: 4.6276\n",
      "Epoch [2/2], Step [21050/67476], Loss: 4.5857\n",
      "Epoch [2/2], Step [21060/67476], Loss: 4.7192\n",
      "Epoch [2/2], Step [21070/67476], Loss: 4.5212\n",
      "Epoch [2/2], Step [21080/67476], Loss: 4.4233\n",
      "Epoch [2/2], Step [21090/67476], Loss: 4.5388\n",
      "Epoch [2/2], Step [21100/67476], Loss: 4.2933\n",
      "Epoch [2/2], Step [21110/67476], Loss: 4.5877\n",
      "Epoch [2/2], Step [21120/67476], Loss: 4.5554\n",
      "Epoch [2/2], Step [21130/67476], Loss: 4.5594\n",
      "Epoch [2/2], Step [21140/67476], Loss: 4.6441\n",
      "Epoch [2/2], Step [21150/67476], Loss: 4.4610\n",
      "Epoch [2/2], Step [21160/67476], Loss: 4.5985\n",
      "Epoch [2/2], Step [21170/67476], Loss: 4.4274\n",
      "Epoch [2/2], Step [21180/67476], Loss: 4.5049\n",
      "Epoch [2/2], Step [21190/67476], Loss: 4.2000\n",
      "Epoch [2/2], Step [21200/67476], Loss: 4.7427\n",
      "Epoch [2/2], Step [21210/67476], Loss: 4.4181\n",
      "Epoch [2/2], Step [21220/67476], Loss: 4.6601\n",
      "Epoch [2/2], Step [21230/67476], Loss: 4.5055\n",
      "Epoch [2/2], Step [21240/67476], Loss: 4.7522\n",
      "Epoch [2/2], Step [21250/67476], Loss: 4.7238\n",
      "Epoch [2/2], Step [21260/67476], Loss: 4.5248\n",
      "Epoch [2/2], Step [21270/67476], Loss: 4.7181\n",
      "Epoch [2/2], Step [21280/67476], Loss: 4.5211\n",
      "Epoch [2/2], Step [21290/67476], Loss: 4.4976\n",
      "Epoch [2/2], Step [21300/67476], Loss: 4.5580\n",
      "Epoch [2/2], Step [21310/67476], Loss: 4.7498\n",
      "Epoch [2/2], Step [21320/67476], Loss: 4.3387\n",
      "Epoch [2/2], Step [21330/67476], Loss: 4.6392\n",
      "Epoch [2/2], Step [21340/67476], Loss: 4.5776\n",
      "Epoch [2/2], Step [21350/67476], Loss: 4.4785\n",
      "Epoch [2/2], Step [21360/67476], Loss: 4.6430\n",
      "Epoch [2/2], Step [21370/67476], Loss: 4.5052\n",
      "Epoch [2/2], Step [21380/67476], Loss: 4.4854\n",
      "Epoch [2/2], Step [21390/67476], Loss: 4.7065\n",
      "Epoch [2/2], Step [21400/67476], Loss: 4.5913\n",
      "Epoch [2/2], Step [21410/67476], Loss: 4.5330\n",
      "Epoch [2/2], Step [21420/67476], Loss: 4.4742\n",
      "Epoch [2/2], Step [21430/67476], Loss: 4.7198\n",
      "Epoch [2/2], Step [21440/67476], Loss: 4.5003\n",
      "Epoch [2/2], Step [21450/67476], Loss: 4.6502\n",
      "Epoch [2/2], Step [21460/67476], Loss: 4.3621\n",
      "Epoch [2/2], Step [21470/67476], Loss: 4.6381\n",
      "Epoch [2/2], Step [21480/67476], Loss: 4.5698\n",
      "Epoch [2/2], Step [21490/67476], Loss: 4.7375\n",
      "Epoch [2/2], Step [21500/67476], Loss: 4.7418\n",
      "Epoch [2/2], Step [21510/67476], Loss: 4.5274\n",
      "Epoch [2/2], Step [21520/67476], Loss: 4.4216\n",
      "Epoch [2/2], Step [21530/67476], Loss: 4.4898\n",
      "Epoch [2/2], Step [21540/67476], Loss: 4.4969\n",
      "Epoch [2/2], Step [21550/67476], Loss: 4.4591\n",
      "Epoch [2/2], Step [21560/67476], Loss: 4.5298\n",
      "Epoch [2/2], Step [21570/67476], Loss: 4.5638\n",
      "Epoch [2/2], Step [21580/67476], Loss: 4.4070\n",
      "Epoch [2/2], Step [21590/67476], Loss: 4.6269\n",
      "Epoch [2/2], Step [21600/67476], Loss: 4.8445\n",
      "Epoch [2/2], Step [21610/67476], Loss: 4.4321\n",
      "Epoch [2/2], Step [21620/67476], Loss: 4.6670\n",
      "Epoch [2/2], Step [21630/67476], Loss: 4.4559\n",
      "Epoch [2/2], Step [21640/67476], Loss: 4.6071\n",
      "Epoch [2/2], Step [21650/67476], Loss: 4.4255\n",
      "Epoch [2/2], Step [21660/67476], Loss: 4.4140\n",
      "Epoch [2/2], Step [21670/67476], Loss: 4.7774\n",
      "Epoch [2/2], Step [21680/67476], Loss: 4.5979\n",
      "Epoch [2/2], Step [21690/67476], Loss: 4.5196\n",
      "Epoch [2/2], Step [21700/67476], Loss: 4.6148\n",
      "Epoch [2/2], Step [21710/67476], Loss: 4.4194\n",
      "Epoch [2/2], Step [21720/67476], Loss: 4.5579\n",
      "Epoch [2/2], Step [21730/67476], Loss: 4.4804\n",
      "Epoch [2/2], Step [21740/67476], Loss: 4.4995\n",
      "Epoch [2/2], Step [21750/67476], Loss: 4.5479\n",
      "Epoch [2/2], Step [21760/67476], Loss: 4.4477\n",
      "Epoch [2/2], Step [21770/67476], Loss: 4.4634\n",
      "Epoch [2/2], Step [21780/67476], Loss: 4.4883\n",
      "Epoch [2/2], Step [21790/67476], Loss: 4.5629\n",
      "Epoch [2/2], Step [21800/67476], Loss: 4.3762\n",
      "Epoch [2/2], Step [21810/67476], Loss: 4.5326\n",
      "Epoch [2/2], Step [21820/67476], Loss: 4.4035\n",
      "Epoch [2/2], Step [21830/67476], Loss: 4.5009\n",
      "Epoch [2/2], Step [21840/67476], Loss: 4.4398\n",
      "Epoch [2/2], Step [21850/67476], Loss: 4.5557\n",
      "Epoch [2/2], Step [21860/67476], Loss: 4.6378\n",
      "Epoch [2/2], Step [21870/67476], Loss: 4.8503\n",
      "Epoch [2/2], Step [21880/67476], Loss: 4.5373\n",
      "Epoch [2/2], Step [21890/67476], Loss: 4.6183\n",
      "Epoch [2/2], Step [21900/67476], Loss: 4.5551\n",
      "Epoch [2/2], Step [21910/67476], Loss: 4.5354\n",
      "Epoch [2/2], Step [21920/67476], Loss: 4.4624\n",
      "Epoch [2/2], Step [21930/67476], Loss: 4.3929\n",
      "Epoch [2/2], Step [21940/67476], Loss: 4.6844\n",
      "Epoch [2/2], Step [21950/67476], Loss: 4.3644\n",
      "Epoch [2/2], Step [21960/67476], Loss: 4.6191\n",
      "Epoch [2/2], Step [21970/67476], Loss: 4.4298\n",
      "Epoch [2/2], Step [21980/67476], Loss: 4.6043\n",
      "Epoch [2/2], Step [21990/67476], Loss: 4.2545\n",
      "Epoch [2/2], Step [22000/67476], Loss: 4.6429\n",
      "Epoch [2/2], Step [22010/67476], Loss: 4.5561\n",
      "Epoch [2/2], Step [22020/67476], Loss: 4.4785\n",
      "Epoch [2/2], Step [22030/67476], Loss: 4.5995\n",
      "Epoch [2/2], Step [22040/67476], Loss: 4.5236\n",
      "Epoch [2/2], Step [22050/67476], Loss: 4.7972\n",
      "Epoch [2/2], Step [22060/67476], Loss: 4.4481\n",
      "Epoch [2/2], Step [22070/67476], Loss: 4.4590\n",
      "Epoch [2/2], Step [22080/67476], Loss: 4.6465\n",
      "Epoch [2/2], Step [22090/67476], Loss: 4.5804\n",
      "Epoch [2/2], Step [22100/67476], Loss: 4.7575\n",
      "Epoch [2/2], Step [22110/67476], Loss: 4.4056\n",
      "Epoch [2/2], Step [22120/67476], Loss: 4.5003\n",
      "Epoch [2/2], Step [22130/67476], Loss: 4.5982\n",
      "Epoch [2/2], Step [22140/67476], Loss: 4.5389\n",
      "Epoch [2/2], Step [22150/67476], Loss: 4.4520\n",
      "Epoch [2/2], Step [22160/67476], Loss: 4.4345\n",
      "Epoch [2/2], Step [22170/67476], Loss: 4.5618\n",
      "Epoch [2/2], Step [22180/67476], Loss: 4.2939\n",
      "Epoch [2/2], Step [22190/67476], Loss: 4.5574\n",
      "Epoch [2/2], Step [22200/67476], Loss: 4.4475\n",
      "Epoch [2/2], Step [22210/67476], Loss: 4.6983\n",
      "Epoch [2/2], Step [22220/67476], Loss: 4.5944\n",
      "Epoch [2/2], Step [22230/67476], Loss: 4.5878\n",
      "Epoch [2/2], Step [22240/67476], Loss: 4.5191\n",
      "Epoch [2/2], Step [22250/67476], Loss: 4.6322\n",
      "Epoch [2/2], Step [22260/67476], Loss: 4.6516\n",
      "Epoch [2/2], Step [22270/67476], Loss: 4.6192\n",
      "Epoch [2/2], Step [22280/67476], Loss: 4.4182\n",
      "Epoch [2/2], Step [22290/67476], Loss: 4.4192\n",
      "Epoch [2/2], Step [22300/67476], Loss: 4.4552\n",
      "Epoch [2/2], Step [22310/67476], Loss: 4.7388\n",
      "Epoch [2/2], Step [22320/67476], Loss: 4.4619\n",
      "Epoch [2/2], Step [22330/67476], Loss: 4.3437\n",
      "Epoch [2/2], Step [22340/67476], Loss: 4.6045\n",
      "Epoch [2/2], Step [22350/67476], Loss: 4.4198\n",
      "Epoch [2/2], Step [22360/67476], Loss: 4.5879\n",
      "Epoch [2/2], Step [22370/67476], Loss: 4.5040\n",
      "Epoch [2/2], Step [22380/67476], Loss: 4.4469\n",
      "Epoch [2/2], Step [22390/67476], Loss: 4.5787\n",
      "Epoch [2/2], Step [22400/67476], Loss: 4.4930\n",
      "Epoch [2/2], Step [22410/67476], Loss: 4.5340\n",
      "Epoch [2/2], Step [22420/67476], Loss: 4.5114\n",
      "Epoch [2/2], Step [22430/67476], Loss: 4.5410\n",
      "Epoch [2/2], Step [22440/67476], Loss: 4.6825\n",
      "Epoch [2/2], Step [22450/67476], Loss: 4.6996\n",
      "Epoch [2/2], Step [22460/67476], Loss: 4.6164\n",
      "Epoch [2/2], Step [22470/67476], Loss: 4.5276\n",
      "Epoch [2/2], Step [22480/67476], Loss: 4.5946\n",
      "Epoch [2/2], Step [22490/67476], Loss: 4.5415\n",
      "Epoch [2/2], Step [22500/67476], Loss: 4.6205\n",
      "Epoch [2/2], Step [22510/67476], Loss: 4.4167\n",
      "Epoch [2/2], Step [22520/67476], Loss: 4.5136\n",
      "Epoch [2/2], Step [22530/67476], Loss: 4.4491\n",
      "Epoch [2/2], Step [22540/67476], Loss: 4.7931\n",
      "Epoch [2/2], Step [22550/67476], Loss: 4.5964\n",
      "Epoch [2/2], Step [22560/67476], Loss: 4.7488\n",
      "Epoch [2/2], Step [22570/67476], Loss: 4.4425\n",
      "Epoch [2/2], Step [22580/67476], Loss: 4.2872\n",
      "Epoch [2/2], Step [22590/67476], Loss: 4.4790\n",
      "Epoch [2/2], Step [22600/67476], Loss: 4.4855\n",
      "Epoch [2/2], Step [22610/67476], Loss: 4.6280\n",
      "Epoch [2/2], Step [22620/67476], Loss: 4.5535\n",
      "Epoch [2/2], Step [22630/67476], Loss: 4.5142\n",
      "Epoch [2/2], Step [22640/67476], Loss: 4.5484\n",
      "Epoch [2/2], Step [22650/67476], Loss: 4.4887\n",
      "Epoch [2/2], Step [22660/67476], Loss: 4.6893\n",
      "Epoch [2/2], Step [22670/67476], Loss: 4.6626\n",
      "Epoch [2/2], Step [22680/67476], Loss: 4.5866\n",
      "Epoch [2/2], Step [22690/67476], Loss: 4.5821\n",
      "Epoch [2/2], Step [22700/67476], Loss: 4.4750\n",
      "Epoch [2/2], Step [22710/67476], Loss: 4.3820\n",
      "Epoch [2/2], Step [22720/67476], Loss: 4.4446\n",
      "Epoch [2/2], Step [22730/67476], Loss: 4.5118\n",
      "Epoch [2/2], Step [22740/67476], Loss: 4.6232\n",
      "Epoch [2/2], Step [22750/67476], Loss: 4.5108\n",
      "Epoch [2/2], Step [22760/67476], Loss: 4.4108\n",
      "Epoch [2/2], Step [22770/67476], Loss: 4.4864\n",
      "Epoch [2/2], Step [22780/67476], Loss: 4.6244\n",
      "Epoch [2/2], Step [22790/67476], Loss: 4.7637\n",
      "Epoch [2/2], Step [22800/67476], Loss: 4.5045\n",
      "Epoch [2/2], Step [22810/67476], Loss: 4.6059\n",
      "Epoch [2/2], Step [22820/67476], Loss: 4.5381\n",
      "Epoch [2/2], Step [22830/67476], Loss: 4.7190\n",
      "Epoch [2/2], Step [22840/67476], Loss: 4.3952\n",
      "Epoch [2/2], Step [22850/67476], Loss: 4.5454\n",
      "Epoch [2/2], Step [22860/67476], Loss: 4.6186\n",
      "Epoch [2/2], Step [22870/67476], Loss: 4.7238\n",
      "Epoch [2/2], Step [22880/67476], Loss: 4.5448\n",
      "Epoch [2/2], Step [22890/67476], Loss: 4.5674\n",
      "Epoch [2/2], Step [22900/67476], Loss: 4.5772\n",
      "Epoch [2/2], Step [22910/67476], Loss: 4.4894\n",
      "Epoch [2/2], Step [22920/67476], Loss: 4.5125\n",
      "Epoch [2/2], Step [22930/67476], Loss: 4.7863\n",
      "Epoch [2/2], Step [22940/67476], Loss: 4.4576\n",
      "Epoch [2/2], Step [22950/67476], Loss: 4.4695\n",
      "Epoch [2/2], Step [22960/67476], Loss: 4.4910\n",
      "Epoch [2/2], Step [22970/67476], Loss: 4.3725\n",
      "Epoch [2/2], Step [22980/67476], Loss: 4.5730\n",
      "Epoch [2/2], Step [22990/67476], Loss: 4.6554\n",
      "Epoch [2/2], Step [23000/67476], Loss: 4.4668\n",
      "Epoch [2/2], Step [23010/67476], Loss: 4.5064\n",
      "Epoch [2/2], Step [23020/67476], Loss: 4.5936\n",
      "Epoch [2/2], Step [23030/67476], Loss: 4.3683\n",
      "Epoch [2/2], Step [23040/67476], Loss: 4.5264\n",
      "Epoch [2/2], Step [23050/67476], Loss: 4.4860\n",
      "Epoch [2/2], Step [23060/67476], Loss: 4.3952\n",
      "Epoch [2/2], Step [23070/67476], Loss: 4.6792\n",
      "Epoch [2/2], Step [23080/67476], Loss: 4.6006\n",
      "Epoch [2/2], Step [23090/67476], Loss: 4.5215\n",
      "Epoch [2/2], Step [23100/67476], Loss: 4.4612\n",
      "Epoch [2/2], Step [23110/67476], Loss: 4.6392\n",
      "Epoch [2/2], Step [23120/67476], Loss: 4.6375\n",
      "Epoch [2/2], Step [23130/67476], Loss: 4.4444\n",
      "Epoch [2/2], Step [23140/67476], Loss: 4.6002\n",
      "Epoch [2/2], Step [23150/67476], Loss: 4.3898\n",
      "Epoch [2/2], Step [23160/67476], Loss: 4.4131\n",
      "Epoch [2/2], Step [23170/67476], Loss: 4.3342\n",
      "Epoch [2/2], Step [23180/67476], Loss: 4.6394\n",
      "Epoch [2/2], Step [23190/67476], Loss: 4.6095\n",
      "Epoch [2/2], Step [23200/67476], Loss: 4.5387\n",
      "Epoch [2/2], Step [23210/67476], Loss: 4.5074\n",
      "Epoch [2/2], Step [23220/67476], Loss: 4.5355\n",
      "Epoch [2/2], Step [23230/67476], Loss: 4.4444\n",
      "Epoch [2/2], Step [23240/67476], Loss: 4.6311\n",
      "Epoch [2/2], Step [23250/67476], Loss: 4.5604\n",
      "Epoch [2/2], Step [23260/67476], Loss: 4.4284\n",
      "Epoch [2/2], Step [23270/67476], Loss: 4.5292\n",
      "Epoch [2/2], Step [23280/67476], Loss: 4.4950\n",
      "Epoch [2/2], Step [23290/67476], Loss: 4.5917\n",
      "Epoch [2/2], Step [23300/67476], Loss: 4.5599\n",
      "Epoch [2/2], Step [23310/67476], Loss: 4.7546\n",
      "Epoch [2/2], Step [23320/67476], Loss: 4.5344\n",
      "Epoch [2/2], Step [23330/67476], Loss: 4.5579\n",
      "Epoch [2/2], Step [23340/67476], Loss: 4.8231\n",
      "Epoch [2/2], Step [23350/67476], Loss: 4.7590\n",
      "Epoch [2/2], Step [23360/67476], Loss: 4.6518\n",
      "Epoch [2/2], Step [23370/67476], Loss: 4.5490\n",
      "Epoch [2/2], Step [23380/67476], Loss: 4.6328\n",
      "Epoch [2/2], Step [23390/67476], Loss: 4.4626\n",
      "Epoch [2/2], Step [23400/67476], Loss: 4.6326\n",
      "Epoch [2/2], Step [23410/67476], Loss: 4.7504\n",
      "Epoch [2/2], Step [23420/67476], Loss: 4.7595\n",
      "Epoch [2/2], Step [23430/67476], Loss: 4.5800\n",
      "Epoch [2/2], Step [23440/67476], Loss: 4.4452\n",
      "Epoch [2/2], Step [23450/67476], Loss: 4.7633\n",
      "Epoch [2/2], Step [23460/67476], Loss: 4.4468\n",
      "Epoch [2/2], Step [23470/67476], Loss: 4.6891\n",
      "Epoch [2/2], Step [23480/67476], Loss: 4.6484\n",
      "Epoch [2/2], Step [23490/67476], Loss: 4.7352\n",
      "Epoch [2/2], Step [23500/67476], Loss: 4.5435\n",
      "Epoch [2/2], Step [23510/67476], Loss: 4.5330\n",
      "Epoch [2/2], Step [23520/67476], Loss: 4.7041\n",
      "Epoch [2/2], Step [23530/67476], Loss: 4.5397\n",
      "Epoch [2/2], Step [23540/67476], Loss: 4.7775\n",
      "Epoch [2/2], Step [23550/67476], Loss: 4.5488\n",
      "Epoch [2/2], Step [23560/67476], Loss: 4.4045\n",
      "Epoch [2/2], Step [23570/67476], Loss: 4.5868\n",
      "Epoch [2/2], Step [23580/67476], Loss: 4.4915\n",
      "Epoch [2/2], Step [23590/67476], Loss: 4.4062\n",
      "Epoch [2/2], Step [23600/67476], Loss: 4.4232\n",
      "Epoch [2/2], Step [23610/67476], Loss: 4.7411\n",
      "Epoch [2/2], Step [23620/67476], Loss: 4.6367\n",
      "Epoch [2/2], Step [23630/67476], Loss: 4.4008\n",
      "Epoch [2/2], Step [23640/67476], Loss: 4.4482\n",
      "Epoch [2/2], Step [23650/67476], Loss: 4.5916\n",
      "Epoch [2/2], Step [23660/67476], Loss: 4.4625\n",
      "Epoch [2/2], Step [23670/67476], Loss: 4.3705\n",
      "Epoch [2/2], Step [23680/67476], Loss: 4.4840\n",
      "Epoch [2/2], Step [23690/67476], Loss: 4.5195\n",
      "Epoch [2/2], Step [23700/67476], Loss: 4.3833\n",
      "Epoch [2/2], Step [23710/67476], Loss: 4.5733\n",
      "Epoch [2/2], Step [23720/67476], Loss: 4.5734\n",
      "Epoch [2/2], Step [23730/67476], Loss: 4.6275\n",
      "Epoch [2/2], Step [23740/67476], Loss: 4.4588\n",
      "Epoch [2/2], Step [23750/67476], Loss: 4.4879\n",
      "Epoch [2/2], Step [23760/67476], Loss: 4.6843\n",
      "Epoch [2/2], Step [23770/67476], Loss: 4.4664\n",
      "Epoch [2/2], Step [23780/67476], Loss: 4.4117\n",
      "Epoch [2/2], Step [23790/67476], Loss: 4.6275\n",
      "Epoch [2/2], Step [23800/67476], Loss: 4.6743\n",
      "Epoch [2/2], Step [23810/67476], Loss: 4.6801\n",
      "Epoch [2/2], Step [23820/67476], Loss: 4.7102\n",
      "Epoch [2/2], Step [23830/67476], Loss: 4.4689\n",
      "Epoch [2/2], Step [23840/67476], Loss: 4.5402\n",
      "Epoch [2/2], Step [23850/67476], Loss: 4.3611\n",
      "Epoch [2/2], Step [23860/67476], Loss: 4.4848\n",
      "Epoch [2/2], Step [23870/67476], Loss: 4.5358\n",
      "Epoch [2/2], Step [23880/67476], Loss: 4.8017\n",
      "Epoch [2/2], Step [23890/67476], Loss: 4.6063\n",
      "Epoch [2/2], Step [23900/67476], Loss: 4.4655\n",
      "Epoch [2/2], Step [23910/67476], Loss: 4.5257\n",
      "Epoch [2/2], Step [23920/67476], Loss: 4.6028\n",
      "Epoch [2/2], Step [23930/67476], Loss: 4.5855\n",
      "Epoch [2/2], Step [23940/67476], Loss: 4.6796\n",
      "Epoch [2/2], Step [23950/67476], Loss: 4.5918\n",
      "Epoch [2/2], Step [23960/67476], Loss: 4.6669\n",
      "Epoch [2/2], Step [23970/67476], Loss: 4.6778\n",
      "Epoch [2/2], Step [23980/67476], Loss: 4.6365\n",
      "Epoch [2/2], Step [23990/67476], Loss: 4.4960\n",
      "Epoch [2/2], Step [24000/67476], Loss: 4.5789\n",
      "Epoch [2/2], Step [24010/67476], Loss: 4.4439\n",
      "Epoch [2/2], Step [24020/67476], Loss: 4.5318\n",
      "Epoch [2/2], Step [24030/67476], Loss: 4.8072\n",
      "Epoch [2/2], Step [24040/67476], Loss: 4.5767\n",
      "Epoch [2/2], Step [24050/67476], Loss: 4.6545\n",
      "Epoch [2/2], Step [24060/67476], Loss: 4.4341\n",
      "Epoch [2/2], Step [24070/67476], Loss: 4.4124\n",
      "Epoch [2/2], Step [24080/67476], Loss: 4.5302\n",
      "Epoch [2/2], Step [24090/67476], Loss: 4.5726\n",
      "Epoch [2/2], Step [24100/67476], Loss: 4.6099\n",
      "Epoch [2/2], Step [24110/67476], Loss: 4.4084\n",
      "Epoch [2/2], Step [24120/67476], Loss: 4.5888\n",
      "Epoch [2/2], Step [24130/67476], Loss: 4.5571\n",
      "Epoch [2/2], Step [24140/67476], Loss: 4.4488\n",
      "Epoch [2/2], Step [24150/67476], Loss: 4.5788\n",
      "Epoch [2/2], Step [24160/67476], Loss: 4.6211\n",
      "Epoch [2/2], Step [24170/67476], Loss: 4.8413\n",
      "Epoch [2/2], Step [24180/67476], Loss: 4.6128\n",
      "Epoch [2/2], Step [24190/67476], Loss: 4.5622\n",
      "Epoch [2/2], Step [24200/67476], Loss: 4.3595\n",
      "Epoch [2/2], Step [24210/67476], Loss: 4.6666\n",
      "Epoch [2/2], Step [24220/67476], Loss: 4.5864\n",
      "Epoch [2/2], Step [24230/67476], Loss: 4.4105\n",
      "Epoch [2/2], Step [24240/67476], Loss: 4.4325\n",
      "Epoch [2/2], Step [24250/67476], Loss: 4.5524\n",
      "Epoch [2/2], Step [24260/67476], Loss: 4.5256\n",
      "Epoch [2/2], Step [24270/67476], Loss: 4.6339\n",
      "Epoch [2/2], Step [24280/67476], Loss: 4.5417\n",
      "Epoch [2/2], Step [24290/67476], Loss: 4.6543\n",
      "Epoch [2/2], Step [24300/67476], Loss: 4.7706\n",
      "Epoch [2/2], Step [24310/67476], Loss: 4.5812\n",
      "Epoch [2/2], Step [24320/67476], Loss: 4.6367\n",
      "Epoch [2/2], Step [24330/67476], Loss: 4.4095\n",
      "Epoch [2/2], Step [24340/67476], Loss: 4.3945\n",
      "Epoch [2/2], Step [24350/67476], Loss: 4.5494\n",
      "Epoch [2/2], Step [24360/67476], Loss: 4.5056\n",
      "Epoch [2/2], Step [24370/67476], Loss: 4.5176\n",
      "Epoch [2/2], Step [24380/67476], Loss: 4.7934\n",
      "Epoch [2/2], Step [24390/67476], Loss: 4.5376\n",
      "Epoch [2/2], Step [24400/67476], Loss: 4.5697\n",
      "Epoch [2/2], Step [24410/67476], Loss: 4.6874\n",
      "Epoch [2/2], Step [24420/67476], Loss: 4.5371\n",
      "Epoch [2/2], Step [24430/67476], Loss: 4.5394\n",
      "Epoch [2/2], Step [24440/67476], Loss: 4.3194\n",
      "Epoch [2/2], Step [24450/67476], Loss: 4.4678\n",
      "Epoch [2/2], Step [24460/67476], Loss: 4.4488\n",
      "Epoch [2/2], Step [24470/67476], Loss: 4.6493\n",
      "Epoch [2/2], Step [24480/67476], Loss: 4.4285\n",
      "Epoch [2/2], Step [24490/67476], Loss: 4.7467\n",
      "Epoch [2/2], Step [24500/67476], Loss: 4.4281\n",
      "Epoch [2/2], Step [24510/67476], Loss: 4.5504\n",
      "Epoch [2/2], Step [24520/67476], Loss: 4.6650\n",
      "Epoch [2/2], Step [24530/67476], Loss: 4.5344\n",
      "Epoch [2/2], Step [24540/67476], Loss: 4.5334\n",
      "Epoch [2/2], Step [24550/67476], Loss: 4.3542\n",
      "Epoch [2/2], Step [24560/67476], Loss: 4.3558\n",
      "Epoch [2/2], Step [24570/67476], Loss: 4.6173\n",
      "Epoch [2/2], Step [24580/67476], Loss: 4.4210\n",
      "Epoch [2/2], Step [24590/67476], Loss: 4.7084\n",
      "Epoch [2/2], Step [24600/67476], Loss: 4.4721\n",
      "Epoch [2/2], Step [24610/67476], Loss: 4.5383\n",
      "Epoch [2/2], Step [24620/67476], Loss: 4.4493\n",
      "Epoch [2/2], Step [24630/67476], Loss: 4.4904\n",
      "Epoch [2/2], Step [24640/67476], Loss: 4.5914\n",
      "Epoch [2/2], Step [24650/67476], Loss: 4.2627\n",
      "Epoch [2/2], Step [24660/67476], Loss: 4.4194\n",
      "Epoch [2/2], Step [24670/67476], Loss: 4.5306\n",
      "Epoch [2/2], Step [24680/67476], Loss: 4.6672\n",
      "Epoch [2/2], Step [24690/67476], Loss: 4.7026\n",
      "Epoch [2/2], Step [24700/67476], Loss: 4.5561\n",
      "Epoch [2/2], Step [24710/67476], Loss: 4.4844\n",
      "Epoch [2/2], Step [24720/67476], Loss: 4.5302\n",
      "Epoch [2/2], Step [24730/67476], Loss: 4.5520\n",
      "Epoch [2/2], Step [24740/67476], Loss: 4.4259\n",
      "Epoch [2/2], Step [24750/67476], Loss: 4.6367\n",
      "Epoch [2/2], Step [24760/67476], Loss: 4.8668\n",
      "Epoch [2/2], Step [24770/67476], Loss: 4.5653\n",
      "Epoch [2/2], Step [24780/67476], Loss: 4.5250\n",
      "Epoch [2/2], Step [24790/67476], Loss: 4.5019\n",
      "Epoch [2/2], Step [24800/67476], Loss: 4.4659\n",
      "Epoch [2/2], Step [24810/67476], Loss: 4.5397\n",
      "Epoch [2/2], Step [24820/67476], Loss: 4.3796\n",
      "Epoch [2/2], Step [24830/67476], Loss: 4.5349\n",
      "Epoch [2/2], Step [24840/67476], Loss: 4.5301\n",
      "Epoch [2/2], Step [24850/67476], Loss: 4.6027\n",
      "Epoch [2/2], Step [24860/67476], Loss: 4.6476\n",
      "Epoch [2/2], Step [24870/67476], Loss: 4.4162\n",
      "Epoch [2/2], Step [24880/67476], Loss: 4.4706\n",
      "Epoch [2/2], Step [24890/67476], Loss: 4.4718\n",
      "Epoch [2/2], Step [24900/67476], Loss: 4.5366\n",
      "Epoch [2/2], Step [24910/67476], Loss: 4.5871\n",
      "Epoch [2/2], Step [24920/67476], Loss: 4.6136\n",
      "Epoch [2/2], Step [24930/67476], Loss: 4.5762\n",
      "Epoch [2/2], Step [24940/67476], Loss: 4.3593\n",
      "Epoch [2/2], Step [24950/67476], Loss: 4.4628\n",
      "Epoch [2/2], Step [24960/67476], Loss: 4.4025\n",
      "Epoch [2/2], Step [24970/67476], Loss: 4.3961\n",
      "Epoch [2/2], Step [24980/67476], Loss: 4.5293\n",
      "Epoch [2/2], Step [24990/67476], Loss: 4.5445\n",
      "Epoch [2/2], Step [25000/67476], Loss: 4.7403\n",
      "Epoch [2/2], Step [25010/67476], Loss: 4.6297\n",
      "Epoch [2/2], Step [25020/67476], Loss: 4.6110\n",
      "Epoch [2/2], Step [25030/67476], Loss: 4.5079\n",
      "Epoch [2/2], Step [25040/67476], Loss: 4.6377\n",
      "Epoch [2/2], Step [25050/67476], Loss: 4.5527\n",
      "Epoch [2/2], Step [25060/67476], Loss: 4.5260\n",
      "Epoch [2/2], Step [25070/67476], Loss: 4.4959\n",
      "Epoch [2/2], Step [25080/67476], Loss: 4.5664\n",
      "Epoch [2/2], Step [25090/67476], Loss: 4.3327\n",
      "Epoch [2/2], Step [25100/67476], Loss: 4.5682\n",
      "Epoch [2/2], Step [25110/67476], Loss: 4.5609\n",
      "Epoch [2/2], Step [25120/67476], Loss: 4.6216\n",
      "Epoch [2/2], Step [25130/67476], Loss: 4.5533\n",
      "Epoch [2/2], Step [25140/67476], Loss: 4.4031\n",
      "Epoch [2/2], Step [25150/67476], Loss: 4.6579\n",
      "Epoch [2/2], Step [25160/67476], Loss: 4.5355\n",
      "Epoch [2/2], Step [25170/67476], Loss: 4.5506\n",
      "Epoch [2/2], Step [25180/67476], Loss: 4.6420\n",
      "Epoch [2/2], Step [25190/67476], Loss: 4.8303\n",
      "Epoch [2/2], Step [25200/67476], Loss: 4.5280\n",
      "Epoch [2/2], Step [25210/67476], Loss: 4.6766\n",
      "Epoch [2/2], Step [25220/67476], Loss: 4.4412\n",
      "Epoch [2/2], Step [25230/67476], Loss: 4.4715\n",
      "Epoch [2/2], Step [25240/67476], Loss: 4.4719\n",
      "Epoch [2/2], Step [25250/67476], Loss: 4.5688\n",
      "Epoch [2/2], Step [25260/67476], Loss: 4.5294\n",
      "Epoch [2/2], Step [25270/67476], Loss: 4.6799\n",
      "Epoch [2/2], Step [25280/67476], Loss: 4.5945\n",
      "Epoch [2/2], Step [25290/67476], Loss: 4.4697\n",
      "Epoch [2/2], Step [25300/67476], Loss: 4.6902\n",
      "Epoch [2/2], Step [25310/67476], Loss: 4.4900\n",
      "Epoch [2/2], Step [25320/67476], Loss: 4.4937\n",
      "Epoch [2/2], Step [25330/67476], Loss: 4.3327\n",
      "Epoch [2/2], Step [25340/67476], Loss: 4.5290\n",
      "Epoch [2/2], Step [25350/67476], Loss: 4.5643\n",
      "Epoch [2/2], Step [25360/67476], Loss: 4.4808\n",
      "Epoch [2/2], Step [25370/67476], Loss: 4.6483\n",
      "Epoch [2/2], Step [25380/67476], Loss: 4.4742\n",
      "Epoch [2/2], Step [25390/67476], Loss: 4.5113\n",
      "Epoch [2/2], Step [25400/67476], Loss: 4.6251\n",
      "Epoch [2/2], Step [25410/67476], Loss: 4.7607\n",
      "Epoch [2/2], Step [25420/67476], Loss: 4.5849\n",
      "Epoch [2/2], Step [25430/67476], Loss: 4.6350\n",
      "Epoch [2/2], Step [25440/67476], Loss: 4.4822\n",
      "Epoch [2/2], Step [25450/67476], Loss: 4.4884\n",
      "Epoch [2/2], Step [25460/67476], Loss: 4.6271\n",
      "Epoch [2/2], Step [25470/67476], Loss: 4.5292\n",
      "Epoch [2/2], Step [25480/67476], Loss: 4.4279\n",
      "Epoch [2/2], Step [25490/67476], Loss: 4.5763\n",
      "Epoch [2/2], Step [25500/67476], Loss: 4.7643\n",
      "Epoch [2/2], Step [25510/67476], Loss: 4.7379\n",
      "Epoch [2/2], Step [25520/67476], Loss: 4.4092\n",
      "Epoch [2/2], Step [25530/67476], Loss: 4.5907\n",
      "Epoch [2/2], Step [25540/67476], Loss: 4.4661\n",
      "Epoch [2/2], Step [25550/67476], Loss: 4.5491\n",
      "Epoch [2/2], Step [25560/67476], Loss: 4.7658\n",
      "Epoch [2/2], Step [25570/67476], Loss: 4.6041\n",
      "Epoch [2/2], Step [25580/67476], Loss: 4.5915\n",
      "Epoch [2/2], Step [25590/67476], Loss: 4.6709\n",
      "Epoch [2/2], Step [25600/67476], Loss: 4.5790\n",
      "Epoch [2/2], Step [25610/67476], Loss: 4.6520\n",
      "Epoch [2/2], Step [25620/67476], Loss: 4.4962\n",
      "Epoch [2/2], Step [25630/67476], Loss: 4.4166\n",
      "Epoch [2/2], Step [25640/67476], Loss: 4.5788\n",
      "Epoch [2/2], Step [25650/67476], Loss: 4.5797\n",
      "Epoch [2/2], Step [25660/67476], Loss: 4.2869\n",
      "Epoch [2/2], Step [25670/67476], Loss: 4.5510\n",
      "Epoch [2/2], Step [25680/67476], Loss: 4.5020\n",
      "Epoch [2/2], Step [25690/67476], Loss: 4.4774\n",
      "Epoch [2/2], Step [25700/67476], Loss: 4.5772\n",
      "Epoch [2/2], Step [25710/67476], Loss: 4.7154\n",
      "Epoch [2/2], Step [25720/67476], Loss: 4.2787\n",
      "Epoch [2/2], Step [25730/67476], Loss: 4.5335\n",
      "Epoch [2/2], Step [25740/67476], Loss: 4.3797\n",
      "Epoch [2/2], Step [25750/67476], Loss: 4.5228\n",
      "Epoch [2/2], Step [25760/67476], Loss: 4.6600\n",
      "Epoch [2/2], Step [25770/67476], Loss: 4.5060\n",
      "Epoch [2/2], Step [25780/67476], Loss: 4.7275\n",
      "Epoch [2/2], Step [25790/67476], Loss: 4.6982\n",
      "Epoch [2/2], Step [25800/67476], Loss: 4.4873\n",
      "Epoch [2/2], Step [25810/67476], Loss: 4.6466\n",
      "Epoch [2/2], Step [25820/67476], Loss: 4.5470\n",
      "Epoch [2/2], Step [25830/67476], Loss: 4.6247\n",
      "Epoch [2/2], Step [25840/67476], Loss: 4.5064\n",
      "Epoch [2/2], Step [25850/67476], Loss: 4.4342\n",
      "Epoch [2/2], Step [25860/67476], Loss: 4.4303\n",
      "Epoch [2/2], Step [25870/67476], Loss: 4.3458\n",
      "Epoch [2/2], Step [25880/67476], Loss: 4.4870\n",
      "Epoch [2/2], Step [25890/67476], Loss: 4.6873\n",
      "Epoch [2/2], Step [25900/67476], Loss: 4.5492\n",
      "Epoch [2/2], Step [25910/67476], Loss: 4.5655\n",
      "Epoch [2/2], Step [25920/67476], Loss: 4.5303\n",
      "Epoch [2/2], Step [25930/67476], Loss: 4.6948\n",
      "Epoch [2/2], Step [25940/67476], Loss: 4.5492\n",
      "Epoch [2/2], Step [25950/67476], Loss: 4.5632\n",
      "Epoch [2/2], Step [25960/67476], Loss: 4.5157\n",
      "Epoch [2/2], Step [25970/67476], Loss: 4.6377\n",
      "Epoch [2/2], Step [25980/67476], Loss: 4.6128\n",
      "Epoch [2/2], Step [25990/67476], Loss: 4.7921\n",
      "Epoch [2/2], Step [26000/67476], Loss: 4.4665\n",
      "Epoch [2/2], Step [26010/67476], Loss: 4.6539\n",
      "Epoch [2/2], Step [26020/67476], Loss: 4.5609\n",
      "Epoch [2/2], Step [26030/67476], Loss: 4.5806\n",
      "Epoch [2/2], Step [26040/67476], Loss: 4.5673\n",
      "Epoch [2/2], Step [26050/67476], Loss: 4.6577\n",
      "Epoch [2/2], Step [26060/67476], Loss: 4.5803\n",
      "Epoch [2/2], Step [26070/67476], Loss: 4.5546\n",
      "Epoch [2/2], Step [26080/67476], Loss: 4.5644\n",
      "Epoch [2/2], Step [26090/67476], Loss: 4.5647\n",
      "Epoch [2/2], Step [26100/67476], Loss: 4.7082\n",
      "Epoch [2/2], Step [26110/67476], Loss: 4.5884\n",
      "Epoch [2/2], Step [26120/67476], Loss: 4.6447\n",
      "Epoch [2/2], Step [26130/67476], Loss: 4.5515\n",
      "Epoch [2/2], Step [26140/67476], Loss: 4.5396\n",
      "Epoch [2/2], Step [26150/67476], Loss: 4.4916\n",
      "Epoch [2/2], Step [26160/67476], Loss: 4.4773\n",
      "Epoch [2/2], Step [26170/67476], Loss: 4.4924\n",
      "Epoch [2/2], Step [26180/67476], Loss: 4.6095\n",
      "Epoch [2/2], Step [26190/67476], Loss: 4.5395\n",
      "Epoch [2/2], Step [26200/67476], Loss: 4.5760\n",
      "Epoch [2/2], Step [26210/67476], Loss: 4.4970\n",
      "Epoch [2/2], Step [26220/67476], Loss: 4.7344\n",
      "Epoch [2/2], Step [26230/67476], Loss: 4.6836\n",
      "Epoch [2/2], Step [26240/67476], Loss: 4.4301\n",
      "Epoch [2/2], Step [26250/67476], Loss: 4.4593\n",
      "Epoch [2/2], Step [26260/67476], Loss: 4.5412\n",
      "Epoch [2/2], Step [26270/67476], Loss: 4.5176\n",
      "Epoch [2/2], Step [26280/67476], Loss: 4.6095\n",
      "Epoch [2/2], Step [26290/67476], Loss: 4.4588\n",
      "Epoch [2/2], Step [26300/67476], Loss: 4.4389\n",
      "Epoch [2/2], Step [26310/67476], Loss: 4.4968\n",
      "Epoch [2/2], Step [26320/67476], Loss: 4.4349\n",
      "Epoch [2/2], Step [26330/67476], Loss: 4.5396\n",
      "Epoch [2/2], Step [26340/67476], Loss: 4.8058\n",
      "Epoch [2/2], Step [26350/67476], Loss: 4.4162\n",
      "Epoch [2/2], Step [26360/67476], Loss: 4.2538\n",
      "Epoch [2/2], Step [26370/67476], Loss: 4.3686\n",
      "Epoch [2/2], Step [26380/67476], Loss: 4.5356\n",
      "Epoch [2/2], Step [26390/67476], Loss: 4.4334\n",
      "Epoch [2/2], Step [26400/67476], Loss: 4.6845\n",
      "Epoch [2/2], Step [26410/67476], Loss: 4.5758\n",
      "Epoch [2/2], Step [26420/67476], Loss: 4.4542\n",
      "Epoch [2/2], Step [26430/67476], Loss: 4.4356\n",
      "Epoch [2/2], Step [26440/67476], Loss: 4.6547\n",
      "Epoch [2/2], Step [26450/67476], Loss: 4.4678\n",
      "Epoch [2/2], Step [26460/67476], Loss: 4.3557\n",
      "Epoch [2/2], Step [26470/67476], Loss: 4.5301\n",
      "Epoch [2/2], Step [26480/67476], Loss: 4.4520\n",
      "Epoch [2/2], Step [26490/67476], Loss: 4.3697\n",
      "Epoch [2/2], Step [26500/67476], Loss: 4.6236\n",
      "Epoch [2/2], Step [26510/67476], Loss: 4.5729\n",
      "Epoch [2/2], Step [26520/67476], Loss: 4.6809\n",
      "Epoch [2/2], Step [26530/67476], Loss: 4.7130\n",
      "Epoch [2/2], Step [26540/67476], Loss: 4.5714\n",
      "Epoch [2/2], Step [26550/67476], Loss: 4.3283\n",
      "Epoch [2/2], Step [26560/67476], Loss: 4.5861\n",
      "Epoch [2/2], Step [26570/67476], Loss: 4.6147\n",
      "Epoch [2/2], Step [26580/67476], Loss: 4.6896\n",
      "Epoch [2/2], Step [26590/67476], Loss: 4.4409\n",
      "Epoch [2/2], Step [26600/67476], Loss: 4.5191\n",
      "Epoch [2/2], Step [26610/67476], Loss: 4.5976\n",
      "Epoch [2/2], Step [26620/67476], Loss: 4.2933\n",
      "Epoch [2/2], Step [26630/67476], Loss: 4.6476\n",
      "Epoch [2/2], Step [26640/67476], Loss: 4.6700\n",
      "Epoch [2/2], Step [26650/67476], Loss: 4.4598\n",
      "Epoch [2/2], Step [26660/67476], Loss: 4.5768\n",
      "Epoch [2/2], Step [26670/67476], Loss: 4.6143\n",
      "Epoch [2/2], Step [26680/67476], Loss: 4.5181\n",
      "Epoch [2/2], Step [26690/67476], Loss: 4.4639\n",
      "Epoch [2/2], Step [26700/67476], Loss: 4.7041\n",
      "Epoch [2/2], Step [26710/67476], Loss: 4.3474\n",
      "Epoch [2/2], Step [26720/67476], Loss: 4.6348\n",
      "Epoch [2/2], Step [26730/67476], Loss: 4.5969\n",
      "Epoch [2/2], Step [26740/67476], Loss: 4.5404\n",
      "Epoch [2/2], Step [26750/67476], Loss: 4.2832\n",
      "Epoch [2/2], Step [26760/67476], Loss: 4.4237\n",
      "Epoch [2/2], Step [26770/67476], Loss: 4.4843\n",
      "Epoch [2/2], Step [26780/67476], Loss: 4.4440\n",
      "Epoch [2/2], Step [26790/67476], Loss: 4.4779\n",
      "Epoch [2/2], Step [26800/67476], Loss: 4.5130\n",
      "Epoch [2/2], Step [26810/67476], Loss: 4.8371\n",
      "Epoch [2/2], Step [26820/67476], Loss: 4.6500\n",
      "Epoch [2/2], Step [26830/67476], Loss: 4.5985\n",
      "Epoch [2/2], Step [26840/67476], Loss: 4.3105\n",
      "Epoch [2/2], Step [26850/67476], Loss: 4.5609\n",
      "Epoch [2/2], Step [26860/67476], Loss: 4.5930\n",
      "Epoch [2/2], Step [26870/67476], Loss: 4.3567\n",
      "Epoch [2/2], Step [26880/67476], Loss: 4.6294\n",
      "Epoch [2/2], Step [26890/67476], Loss: 4.5621\n",
      "Epoch [2/2], Step [26900/67476], Loss: 4.4201\n",
      "Epoch [2/2], Step [26910/67476], Loss: 4.6248\n",
      "Epoch [2/2], Step [26920/67476], Loss: 4.5561\n",
      "Epoch [2/2], Step [26930/67476], Loss: 4.5770\n",
      "Epoch [2/2], Step [26940/67476], Loss: 4.6831\n",
      "Epoch [2/2], Step [26950/67476], Loss: 4.4804\n",
      "Epoch [2/2], Step [26960/67476], Loss: 4.7319\n",
      "Epoch [2/2], Step [26970/67476], Loss: 4.5796\n",
      "Epoch [2/2], Step [26980/67476], Loss: 4.5319\n",
      "Epoch [2/2], Step [26990/67476], Loss: 4.5397\n",
      "Epoch [2/2], Step [27000/67476], Loss: 4.6803\n",
      "Epoch [2/2], Step [27010/67476], Loss: 4.5180\n",
      "Epoch [2/2], Step [27020/67476], Loss: 4.4908\n",
      "Epoch [2/2], Step [27030/67476], Loss: 4.4198\n",
      "Epoch [2/2], Step [27040/67476], Loss: 4.4253\n",
      "Epoch [2/2], Step [27050/67476], Loss: 4.3674\n",
      "Epoch [2/2], Step [27060/67476], Loss: 4.4805\n",
      "Epoch [2/2], Step [27070/67476], Loss: 4.5873\n",
      "Epoch [2/2], Step [27080/67476], Loss: 4.5691\n",
      "Epoch [2/2], Step [27090/67476], Loss: 4.6278\n",
      "Epoch [2/2], Step [27100/67476], Loss: 4.5764\n",
      "Epoch [2/2], Step [27110/67476], Loss: 4.5659\n",
      "Epoch [2/2], Step [27120/67476], Loss: 4.4462\n",
      "Epoch [2/2], Step [27130/67476], Loss: 4.3876\n",
      "Epoch [2/2], Step [27140/67476], Loss: 4.4950\n",
      "Epoch [2/2], Step [27150/67476], Loss: 4.4837\n",
      "Epoch [2/2], Step [27160/67476], Loss: 4.5151\n",
      "Epoch [2/2], Step [27170/67476], Loss: 4.4978\n",
      "Epoch [2/2], Step [27180/67476], Loss: 4.5527\n",
      "Epoch [2/2], Step [27190/67476], Loss: 4.5127\n",
      "Epoch [2/2], Step [27200/67476], Loss: 4.5647\n",
      "Epoch [2/2], Step [27210/67476], Loss: 4.5059\n",
      "Epoch [2/2], Step [27220/67476], Loss: 4.7109\n",
      "Epoch [2/2], Step [27230/67476], Loss: 4.5366\n",
      "Epoch [2/2], Step [27240/67476], Loss: 4.4079\n",
      "Epoch [2/2], Step [27250/67476], Loss: 4.5644\n",
      "Epoch [2/2], Step [27260/67476], Loss: 4.5534\n",
      "Epoch [2/2], Step [27270/67476], Loss: 4.6148\n",
      "Epoch [2/2], Step [27280/67476], Loss: 4.8088\n",
      "Epoch [2/2], Step [27290/67476], Loss: 4.6748\n",
      "Epoch [2/2], Step [27300/67476], Loss: 4.3923\n",
      "Epoch [2/2], Step [27310/67476], Loss: 4.3721\n",
      "Epoch [2/2], Step [27320/67476], Loss: 4.5597\n",
      "Epoch [2/2], Step [27330/67476], Loss: 4.4720\n",
      "Epoch [2/2], Step [27340/67476], Loss: 4.7196\n",
      "Epoch [2/2], Step [27350/67476], Loss: 4.4684\n",
      "Epoch [2/2], Step [27360/67476], Loss: 4.5986\n",
      "Epoch [2/2], Step [27370/67476], Loss: 4.6747\n",
      "Epoch [2/2], Step [27380/67476], Loss: 4.5899\n",
      "Epoch [2/2], Step [27390/67476], Loss: 4.4062\n",
      "Epoch [2/2], Step [27400/67476], Loss: 4.1403\n",
      "Epoch [2/2], Step [27410/67476], Loss: 4.5020\n",
      "Epoch [2/2], Step [27420/67476], Loss: 4.6611\n",
      "Epoch [2/2], Step [27430/67476], Loss: 4.5095\n",
      "Epoch [2/2], Step [27440/67476], Loss: 4.4708\n",
      "Epoch [2/2], Step [27450/67476], Loss: 4.4489\n",
      "Epoch [2/2], Step [27460/67476], Loss: 4.5986\n",
      "Epoch [2/2], Step [27470/67476], Loss: 4.5683\n",
      "Epoch [2/2], Step [27480/67476], Loss: 4.4940\n",
      "Epoch [2/2], Step [27490/67476], Loss: 4.6710\n",
      "Epoch [2/2], Step [27500/67476], Loss: 4.4302\n",
      "Epoch [2/2], Step [27510/67476], Loss: 4.5181\n",
      "Epoch [2/2], Step [27520/67476], Loss: 4.3714\n",
      "Epoch [2/2], Step [27530/67476], Loss: 4.7132\n",
      "Epoch [2/2], Step [27540/67476], Loss: 4.5950\n",
      "Epoch [2/2], Step [27550/67476], Loss: 4.3193\n",
      "Epoch [2/2], Step [27560/67476], Loss: 4.6761\n",
      "Epoch [2/2], Step [27570/67476], Loss: 4.5279\n",
      "Epoch [2/2], Step [27580/67476], Loss: 4.5022\n",
      "Epoch [2/2], Step [27590/67476], Loss: 4.4302\n",
      "Epoch [2/2], Step [27600/67476], Loss: 4.5347\n",
      "Epoch [2/2], Step [27610/67476], Loss: 4.6737\n",
      "Epoch [2/2], Step [27620/67476], Loss: 4.6347\n",
      "Epoch [2/2], Step [27630/67476], Loss: 4.4116\n",
      "Epoch [2/2], Step [27640/67476], Loss: 4.6313\n",
      "Epoch [2/2], Step [27650/67476], Loss: 4.6500\n",
      "Epoch [2/2], Step [27660/67476], Loss: 4.5160\n",
      "Epoch [2/2], Step [27670/67476], Loss: 4.5694\n",
      "Epoch [2/2], Step [27680/67476], Loss: 4.5981\n",
      "Epoch [2/2], Step [27690/67476], Loss: 4.2916\n",
      "Epoch [2/2], Step [27700/67476], Loss: 4.4972\n",
      "Epoch [2/2], Step [27710/67476], Loss: 4.7865\n",
      "Epoch [2/2], Step [27720/67476], Loss: 4.5315\n",
      "Epoch [2/2], Step [27730/67476], Loss: 4.5068\n",
      "Epoch [2/2], Step [27740/67476], Loss: 4.3910\n",
      "Epoch [2/2], Step [27750/67476], Loss: 4.3716\n",
      "Epoch [2/2], Step [27760/67476], Loss: 4.4198\n",
      "Epoch [2/2], Step [27770/67476], Loss: 4.4327\n",
      "Epoch [2/2], Step [27780/67476], Loss: 4.3844\n",
      "Epoch [2/2], Step [27790/67476], Loss: 4.3396\n",
      "Epoch [2/2], Step [27800/67476], Loss: 4.5579\n",
      "Epoch [2/2], Step [27810/67476], Loss: 4.7056\n",
      "Epoch [2/2], Step [27820/67476], Loss: 4.5056\n",
      "Epoch [2/2], Step [27830/67476], Loss: 4.3367\n",
      "Epoch [2/2], Step [27840/67476], Loss: 4.6073\n",
      "Epoch [2/2], Step [27850/67476], Loss: 4.6455\n",
      "Epoch [2/2], Step [27860/67476], Loss: 4.6395\n",
      "Epoch [2/2], Step [27870/67476], Loss: 4.7420\n",
      "Epoch [2/2], Step [27880/67476], Loss: 4.4834\n",
      "Epoch [2/2], Step [27890/67476], Loss: 4.5637\n",
      "Epoch [2/2], Step [27900/67476], Loss: 4.5651\n",
      "Epoch [2/2], Step [27910/67476], Loss: 4.5256\n",
      "Epoch [2/2], Step [27920/67476], Loss: 4.4687\n",
      "Epoch [2/2], Step [27930/67476], Loss: 4.5113\n",
      "Epoch [2/2], Step [27940/67476], Loss: 4.5213\n",
      "Epoch [2/2], Step [27950/67476], Loss: 4.6174\n",
      "Epoch [2/2], Step [27960/67476], Loss: 4.7430\n",
      "Epoch [2/2], Step [27970/67476], Loss: 4.5523\n",
      "Epoch [2/2], Step [27980/67476], Loss: 4.4226\n",
      "Epoch [2/2], Step [27990/67476], Loss: 4.6165\n",
      "Epoch [2/2], Step [28000/67476], Loss: 4.6040\n",
      "Epoch [2/2], Step [28010/67476], Loss: 4.4828\n",
      "Epoch [2/2], Step [28020/67476], Loss: 4.5824\n",
      "Epoch [2/2], Step [28030/67476], Loss: 4.3757\n",
      "Epoch [2/2], Step [28040/67476], Loss: 4.4093\n",
      "Epoch [2/2], Step [28050/67476], Loss: 4.4962\n",
      "Epoch [2/2], Step [28060/67476], Loss: 4.5041\n",
      "Epoch [2/2], Step [28070/67476], Loss: 4.4263\n",
      "Epoch [2/2], Step [28080/67476], Loss: 4.6301\n",
      "Epoch [2/2], Step [28090/67476], Loss: 4.7635\n",
      "Epoch [2/2], Step [28100/67476], Loss: 4.6694\n",
      "Epoch [2/2], Step [28110/67476], Loss: 4.7125\n",
      "Epoch [2/2], Step [28120/67476], Loss: 4.3810\n",
      "Epoch [2/2], Step [28130/67476], Loss: 4.4637\n",
      "Epoch [2/2], Step [28140/67476], Loss: 4.7582\n",
      "Epoch [2/2], Step [28150/67476], Loss: 4.5184\n",
      "Epoch [2/2], Step [28160/67476], Loss: 4.5491\n",
      "Epoch [2/2], Step [28170/67476], Loss: 4.5033\n",
      "Epoch [2/2], Step [28180/67476], Loss: 4.4714\n",
      "Epoch [2/2], Step [28190/67476], Loss: 4.5668\n",
      "Epoch [2/2], Step [28200/67476], Loss: 4.4348\n",
      "Epoch [2/2], Step [28210/67476], Loss: 4.6929\n",
      "Epoch [2/2], Step [28220/67476], Loss: 4.4809\n",
      "Epoch [2/2], Step [28230/67476], Loss: 4.5509\n",
      "Epoch [2/2], Step [28240/67476], Loss: 4.4650\n",
      "Epoch [2/2], Step [28250/67476], Loss: 4.6779\n",
      "Epoch [2/2], Step [28260/67476], Loss: 4.7772\n",
      "Epoch [2/2], Step [28270/67476], Loss: 4.5196\n",
      "Epoch [2/2], Step [28280/67476], Loss: 4.2629\n",
      "Epoch [2/2], Step [28290/67476], Loss: 4.5396\n",
      "Epoch [2/2], Step [28300/67476], Loss: 4.6308\n",
      "Epoch [2/2], Step [28310/67476], Loss: 4.4274\n",
      "Epoch [2/2], Step [28320/67476], Loss: 4.4448\n",
      "Epoch [2/2], Step [28330/67476], Loss: 4.3381\n",
      "Epoch [2/2], Step [28340/67476], Loss: 4.6169\n",
      "Epoch [2/2], Step [28350/67476], Loss: 4.5680\n",
      "Epoch [2/2], Step [28360/67476], Loss: 4.5829\n",
      "Epoch [2/2], Step [28370/67476], Loss: 4.3520\n",
      "Epoch [2/2], Step [28380/67476], Loss: 4.6102\n",
      "Epoch [2/2], Step [28390/67476], Loss: 4.7757\n",
      "Epoch [2/2], Step [28400/67476], Loss: 4.4742\n",
      "Epoch [2/2], Step [28410/67476], Loss: 4.4309\n",
      "Epoch [2/2], Step [28420/67476], Loss: 4.6799\n",
      "Epoch [2/2], Step [28430/67476], Loss: 4.4316\n",
      "Epoch [2/2], Step [28440/67476], Loss: 4.5951\n",
      "Epoch [2/2], Step [28450/67476], Loss: 4.3429\n",
      "Epoch [2/2], Step [28460/67476], Loss: 4.7352\n",
      "Epoch [2/2], Step [28470/67476], Loss: 4.5115\n",
      "Epoch [2/2], Step [28480/67476], Loss: 4.6570\n",
      "Epoch [2/2], Step [28490/67476], Loss: 4.7664\n",
      "Epoch [2/2], Step [28500/67476], Loss: 4.5555\n",
      "Epoch [2/2], Step [28510/67476], Loss: 4.6362\n",
      "Epoch [2/2], Step [28520/67476], Loss: 4.4983\n",
      "Epoch [2/2], Step [28530/67476], Loss: 4.4988\n",
      "Epoch [2/2], Step [28540/67476], Loss: 4.4007\n",
      "Epoch [2/2], Step [28550/67476], Loss: 4.5282\n",
      "Epoch [2/2], Step [28560/67476], Loss: 4.5095\n",
      "Epoch [2/2], Step [28570/67476], Loss: 4.6452\n",
      "Epoch [2/2], Step [28580/67476], Loss: 4.4710\n",
      "Epoch [2/2], Step [28590/67476], Loss: 4.6488\n",
      "Epoch [2/2], Step [28600/67476], Loss: 4.5959\n",
      "Epoch [2/2], Step [28610/67476], Loss: 4.5424\n",
      "Epoch [2/2], Step [28620/67476], Loss: 4.6144\n",
      "Epoch [2/2], Step [28630/67476], Loss: 4.6329\n",
      "Epoch [2/2], Step [28640/67476], Loss: 4.6281\n",
      "Epoch [2/2], Step [28650/67476], Loss: 4.5084\n",
      "Epoch [2/2], Step [28660/67476], Loss: 4.5389\n",
      "Epoch [2/2], Step [28670/67476], Loss: 4.6397\n",
      "Epoch [2/2], Step [28680/67476], Loss: 4.5074\n",
      "Epoch [2/2], Step [28690/67476], Loss: 4.2731\n",
      "Epoch [2/2], Step [28700/67476], Loss: 4.5758\n",
      "Epoch [2/2], Step [28710/67476], Loss: 4.3981\n",
      "Epoch [2/2], Step [28720/67476], Loss: 4.5756\n",
      "Epoch [2/2], Step [28730/67476], Loss: 4.4961\n",
      "Epoch [2/2], Step [28740/67476], Loss: 4.7364\n",
      "Epoch [2/2], Step [28750/67476], Loss: 4.4750\n",
      "Epoch [2/2], Step [28760/67476], Loss: 4.6174\n",
      "Epoch [2/2], Step [28770/67476], Loss: 4.4465\n",
      "Epoch [2/2], Step [28780/67476], Loss: 4.6194\n",
      "Epoch [2/2], Step [28790/67476], Loss: 4.4891\n",
      "Epoch [2/2], Step [28800/67476], Loss: 4.6054\n",
      "Epoch [2/2], Step [28810/67476], Loss: 4.5022\n",
      "Epoch [2/2], Step [28820/67476], Loss: 4.4124\n",
      "Epoch [2/2], Step [28830/67476], Loss: 4.4511\n",
      "Epoch [2/2], Step [28840/67476], Loss: 4.3065\n",
      "Epoch [2/2], Step [28850/67476], Loss: 4.6828\n",
      "Epoch [2/2], Step [28860/67476], Loss: 4.4930\n",
      "Epoch [2/2], Step [28870/67476], Loss: 4.6350\n",
      "Epoch [2/2], Step [28880/67476], Loss: 4.6831\n",
      "Epoch [2/2], Step [28890/67476], Loss: 4.5040\n",
      "Epoch [2/2], Step [28900/67476], Loss: 4.3566\n",
      "Epoch [2/2], Step [28910/67476], Loss: 4.3011\n",
      "Epoch [2/2], Step [28920/67476], Loss: 4.5608\n",
      "Epoch [2/2], Step [28930/67476], Loss: 4.8089\n",
      "Epoch [2/2], Step [28940/67476], Loss: 4.5011\n",
      "Epoch [2/2], Step [28950/67476], Loss: 4.4184\n",
      "Epoch [2/2], Step [28960/67476], Loss: 4.5240\n",
      "Epoch [2/2], Step [28970/67476], Loss: 4.4824\n",
      "Epoch [2/2], Step [28980/67476], Loss: 4.5616\n",
      "Epoch [2/2], Step [28990/67476], Loss: 4.5053\n",
      "Epoch [2/2], Step [29000/67476], Loss: 4.7192\n",
      "Epoch [2/2], Step [29010/67476], Loss: 4.4110\n",
      "Epoch [2/2], Step [29020/67476], Loss: 4.4341\n",
      "Epoch [2/2], Step [29030/67476], Loss: 4.4585\n",
      "Epoch [2/2], Step [29040/67476], Loss: 4.5656\n",
      "Epoch [2/2], Step [29050/67476], Loss: 4.6214\n",
      "Epoch [2/2], Step [29060/67476], Loss: 4.5958\n",
      "Epoch [2/2], Step [29070/67476], Loss: 4.7692\n",
      "Epoch [2/2], Step [29080/67476], Loss: 4.4158\n",
      "Epoch [2/2], Step [29090/67476], Loss: 4.4294\n",
      "Epoch [2/2], Step [29100/67476], Loss: 4.4268\n",
      "Epoch [2/2], Step [29110/67476], Loss: 4.3770\n",
      "Epoch [2/2], Step [29120/67476], Loss: 4.6752\n",
      "Epoch [2/2], Step [29130/67476], Loss: 4.5542\n",
      "Epoch [2/2], Step [29140/67476], Loss: 4.5795\n",
      "Epoch [2/2], Step [29150/67476], Loss: 4.6396\n",
      "Epoch [2/2], Step [29160/67476], Loss: 4.5162\n",
      "Epoch [2/2], Step [29170/67476], Loss: 4.4528\n",
      "Epoch [2/2], Step [29180/67476], Loss: 4.5071\n",
      "Epoch [2/2], Step [29190/67476], Loss: 4.6783\n",
      "Epoch [2/2], Step [29200/67476], Loss: 4.5695\n",
      "Epoch [2/2], Step [29210/67476], Loss: 4.5958\n",
      "Epoch [2/2], Step [29220/67476], Loss: 4.6039\n",
      "Epoch [2/2], Step [29230/67476], Loss: 4.5812\n",
      "Epoch [2/2], Step [29240/67476], Loss: 4.6817\n",
      "Epoch [2/2], Step [29250/67476], Loss: 4.6030\n",
      "Epoch [2/2], Step [29260/67476], Loss: 4.5389\n",
      "Epoch [2/2], Step [29270/67476], Loss: 4.4506\n",
      "Epoch [2/2], Step [29280/67476], Loss: 4.6882\n",
      "Epoch [2/2], Step [29290/67476], Loss: 4.4385\n",
      "Epoch [2/2], Step [29300/67476], Loss: 4.5930\n",
      "Epoch [2/2], Step [29310/67476], Loss: 4.5789\n",
      "Epoch [2/2], Step [29320/67476], Loss: 4.4497\n",
      "Epoch [2/2], Step [29330/67476], Loss: 4.6026\n",
      "Epoch [2/2], Step [29340/67476], Loss: 4.5198\n",
      "Epoch [2/2], Step [29350/67476], Loss: 4.5004\n",
      "Epoch [2/2], Step [29360/67476], Loss: 4.5393\n",
      "Epoch [2/2], Step [29370/67476], Loss: 4.5853\n",
      "Epoch [2/2], Step [29380/67476], Loss: 4.5565\n",
      "Epoch [2/2], Step [29390/67476], Loss: 4.7059\n",
      "Epoch [2/2], Step [29400/67476], Loss: 4.7509\n",
      "Epoch [2/2], Step [29410/67476], Loss: 4.4863\n",
      "Epoch [2/2], Step [29420/67476], Loss: 4.4776\n",
      "Epoch [2/2], Step [29430/67476], Loss: 4.5216\n",
      "Epoch [2/2], Step [29440/67476], Loss: 4.6023\n",
      "Epoch [2/2], Step [29450/67476], Loss: 4.5286\n",
      "Epoch [2/2], Step [29460/67476], Loss: 4.4825\n",
      "Epoch [2/2], Step [29470/67476], Loss: 4.8837\n",
      "Epoch [2/2], Step [29480/67476], Loss: 4.4126\n",
      "Epoch [2/2], Step [29490/67476], Loss: 4.2450\n",
      "Epoch [2/2], Step [29500/67476], Loss: 4.4989\n",
      "Epoch [2/2], Step [29510/67476], Loss: 4.7499\n",
      "Epoch [2/2], Step [29520/67476], Loss: 4.5729\n",
      "Epoch [2/2], Step [29530/67476], Loss: 4.6576\n",
      "Epoch [2/2], Step [29540/67476], Loss: 4.5493\n",
      "Epoch [2/2], Step [29550/67476], Loss: 4.6040\n",
      "Epoch [2/2], Step [29560/67476], Loss: 4.4353\n",
      "Epoch [2/2], Step [29570/67476], Loss: 4.6023\n",
      "Epoch [2/2], Step [29580/67476], Loss: 4.6708\n",
      "Epoch [2/2], Step [29590/67476], Loss: 4.5608\n",
      "Epoch [2/2], Step [29600/67476], Loss: 4.5780\n",
      "Epoch [2/2], Step [29610/67476], Loss: 4.6481\n",
      "Epoch [2/2], Step [29620/67476], Loss: 4.6872\n",
      "Epoch [2/2], Step [29630/67476], Loss: 4.5070\n",
      "Epoch [2/2], Step [29640/67476], Loss: 4.5535\n",
      "Epoch [2/2], Step [29650/67476], Loss: 4.6416\n",
      "Epoch [2/2], Step [29660/67476], Loss: 4.5827\n",
      "Epoch [2/2], Step [29670/67476], Loss: 4.6187\n",
      "Epoch [2/2], Step [29680/67476], Loss: 4.5482\n",
      "Epoch [2/2], Step [29690/67476], Loss: 4.4300\n",
      "Epoch [2/2], Step [29700/67476], Loss: 4.5685\n",
      "Epoch [2/2], Step [29710/67476], Loss: 4.4094\n",
      "Epoch [2/2], Step [29720/67476], Loss: 4.6208\n",
      "Epoch [2/2], Step [29730/67476], Loss: 4.4097\n",
      "Epoch [2/2], Step [29740/67476], Loss: 4.4969\n",
      "Epoch [2/2], Step [29750/67476], Loss: 4.5812\n",
      "Epoch [2/2], Step [29760/67476], Loss: 4.5969\n",
      "Epoch [2/2], Step [29770/67476], Loss: 4.7405\n",
      "Epoch [2/2], Step [29780/67476], Loss: 4.8649\n",
      "Epoch [2/2], Step [29790/67476], Loss: 4.5701\n",
      "Epoch [2/2], Step [29800/67476], Loss: 4.5717\n",
      "Epoch [2/2], Step [29810/67476], Loss: 4.5202\n",
      "Epoch [2/2], Step [29820/67476], Loss: 4.5664\n",
      "Epoch [2/2], Step [29830/67476], Loss: 4.9115\n",
      "Epoch [2/2], Step [29840/67476], Loss: 4.6524\n",
      "Epoch [2/2], Step [29850/67476], Loss: 4.6130\n",
      "Epoch [2/2], Step [29860/67476], Loss: 4.4734\n",
      "Epoch [2/2], Step [29870/67476], Loss: 4.6255\n",
      "Epoch [2/2], Step [29880/67476], Loss: 4.5699\n",
      "Epoch [2/2], Step [29890/67476], Loss: 4.7455\n",
      "Epoch [2/2], Step [29900/67476], Loss: 4.4023\n",
      "Epoch [2/2], Step [29910/67476], Loss: 4.5588\n",
      "Epoch [2/2], Step [29920/67476], Loss: 4.6632\n",
      "Epoch [2/2], Step [29930/67476], Loss: 4.3868\n",
      "Epoch [2/2], Step [29940/67476], Loss: 4.5191\n",
      "Epoch [2/2], Step [29950/67476], Loss: 4.6953\n",
      "Epoch [2/2], Step [29960/67476], Loss: 4.6065\n",
      "Epoch [2/2], Step [29970/67476], Loss: 4.6855\n",
      "Epoch [2/2], Step [29980/67476], Loss: 4.5261\n",
      "Epoch [2/2], Step [29990/67476], Loss: 4.5735\n",
      "Epoch [2/2], Step [30000/67476], Loss: 4.5373\n",
      "Epoch [2/2], Step [30010/67476], Loss: 4.4662\n",
      "Epoch [2/2], Step [30020/67476], Loss: 4.6350\n",
      "Epoch [2/2], Step [30030/67476], Loss: 4.3858\n",
      "Epoch [2/2], Step [30040/67476], Loss: 4.5998\n",
      "Epoch [2/2], Step [30050/67476], Loss: 4.4629\n",
      "Epoch [2/2], Step [30060/67476], Loss: 4.4314\n",
      "Epoch [2/2], Step [30070/67476], Loss: 4.3444\n",
      "Epoch [2/2], Step [30080/67476], Loss: 4.4889\n",
      "Epoch [2/2], Step [30090/67476], Loss: 4.5720\n",
      "Epoch [2/2], Step [30100/67476], Loss: 4.6503\n",
      "Epoch [2/2], Step [30110/67476], Loss: 4.5480\n",
      "Epoch [2/2], Step [30120/67476], Loss: 4.4795\n",
      "Epoch [2/2], Step [30130/67476], Loss: 4.5611\n",
      "Epoch [2/2], Step [30140/67476], Loss: 4.6362\n",
      "Epoch [2/2], Step [30150/67476], Loss: 4.5700\n",
      "Epoch [2/2], Step [30160/67476], Loss: 4.7778\n",
      "Epoch [2/2], Step [30170/67476], Loss: 4.6555\n",
      "Epoch [2/2], Step [30180/67476], Loss: 4.5455\n",
      "Epoch [2/2], Step [30190/67476], Loss: 4.4751\n",
      "Epoch [2/2], Step [30200/67476], Loss: 4.7076\n",
      "Epoch [2/2], Step [30210/67476], Loss: 4.6852\n",
      "Epoch [2/2], Step [30220/67476], Loss: 4.6180\n",
      "Epoch [2/2], Step [30230/67476], Loss: 4.4316\n",
      "Epoch [2/2], Step [30240/67476], Loss: 4.4930\n",
      "Epoch [2/2], Step [30250/67476], Loss: 4.7818\n",
      "Epoch [2/2], Step [30260/67476], Loss: 4.4380\n",
      "Epoch [2/2], Step [30270/67476], Loss: 4.5300\n",
      "Epoch [2/2], Step [30280/67476], Loss: 4.4121\n",
      "Epoch [2/2], Step [30290/67476], Loss: 4.4938\n",
      "Epoch [2/2], Step [30300/67476], Loss: 4.6958\n",
      "Epoch [2/2], Step [30310/67476], Loss: 4.4722\n",
      "Epoch [2/2], Step [30320/67476], Loss: 4.3820\n",
      "Epoch [2/2], Step [30330/67476], Loss: 4.5999\n",
      "Epoch [2/2], Step [30340/67476], Loss: 4.2889\n",
      "Epoch [2/2], Step [30350/67476], Loss: 4.5507\n",
      "Epoch [2/2], Step [30360/67476], Loss: 4.5789\n",
      "Epoch [2/2], Step [30370/67476], Loss: 4.8216\n",
      "Epoch [2/2], Step [30380/67476], Loss: 4.5375\n",
      "Epoch [2/2], Step [30390/67476], Loss: 4.2477\n",
      "Epoch [2/2], Step [30400/67476], Loss: 4.4984\n",
      "Epoch [2/2], Step [30410/67476], Loss: 4.5033\n",
      "Epoch [2/2], Step [30420/67476], Loss: 4.4501\n",
      "Epoch [2/2], Step [30430/67476], Loss: 4.5549\n",
      "Epoch [2/2], Step [30440/67476], Loss: 4.4640\n",
      "Epoch [2/2], Step [30450/67476], Loss: 4.3092\n",
      "Epoch [2/2], Step [30460/67476], Loss: 4.5406\n",
      "Epoch [2/2], Step [30470/67476], Loss: 4.5597\n",
      "Epoch [2/2], Step [30480/67476], Loss: 4.4039\n",
      "Epoch [2/2], Step [30490/67476], Loss: 4.4988\n",
      "Epoch [2/2], Step [30500/67476], Loss: 4.5461\n",
      "Epoch [2/2], Step [30510/67476], Loss: 4.5360\n",
      "Epoch [2/2], Step [30520/67476], Loss: 4.5833\n",
      "Epoch [2/2], Step [30530/67476], Loss: 4.6135\n",
      "Epoch [2/2], Step [30540/67476], Loss: 4.3558\n",
      "Epoch [2/2], Step [30550/67476], Loss: 4.5560\n",
      "Epoch [2/2], Step [30560/67476], Loss: 4.6448\n",
      "Epoch [2/2], Step [30570/67476], Loss: 4.4341\n",
      "Epoch [2/2], Step [30580/67476], Loss: 4.5631\n",
      "Epoch [2/2], Step [30590/67476], Loss: 4.3802\n",
      "Epoch [2/2], Step [30600/67476], Loss: 4.3675\n",
      "Epoch [2/2], Step [30610/67476], Loss: 4.5849\n",
      "Epoch [2/2], Step [30620/67476], Loss: 4.6487\n",
      "Epoch [2/2], Step [30630/67476], Loss: 4.4859\n",
      "Epoch [2/2], Step [30640/67476], Loss: 4.5426\n",
      "Epoch [2/2], Step [30650/67476], Loss: 4.5152\n",
      "Epoch [2/2], Step [30660/67476], Loss: 4.5935\n",
      "Epoch [2/2], Step [30670/67476], Loss: 4.2593\n",
      "Epoch [2/2], Step [30680/67476], Loss: 4.3060\n",
      "Epoch [2/2], Step [30690/67476], Loss: 4.7047\n",
      "Epoch [2/2], Step [30700/67476], Loss: 4.4557\n",
      "Epoch [2/2], Step [30710/67476], Loss: 4.4648\n",
      "Epoch [2/2], Step [30720/67476], Loss: 4.5762\n",
      "Epoch [2/2], Step [30730/67476], Loss: 4.4607\n",
      "Epoch [2/2], Step [30740/67476], Loss: 4.7398\n",
      "Epoch [2/2], Step [30750/67476], Loss: 4.4849\n",
      "Epoch [2/2], Step [30760/67476], Loss: 4.3181\n",
      "Epoch [2/2], Step [30770/67476], Loss: 4.5858\n",
      "Epoch [2/2], Step [30780/67476], Loss: 4.5260\n",
      "Epoch [2/2], Step [30790/67476], Loss: 4.5511\n",
      "Epoch [2/2], Step [30800/67476], Loss: 4.8251\n",
      "Epoch [2/2], Step [30810/67476], Loss: 4.3604\n",
      "Epoch [2/2], Step [30820/67476], Loss: 4.7576\n",
      "Epoch [2/2], Step [30830/67476], Loss: 4.5483\n",
      "Epoch [2/2], Step [30840/67476], Loss: 4.5614\n",
      "Epoch [2/2], Step [30850/67476], Loss: 4.8050\n",
      "Epoch [2/2], Step [30860/67476], Loss: 4.5447\n",
      "Epoch [2/2], Step [30870/67476], Loss: 4.5664\n",
      "Epoch [2/2], Step [30880/67476], Loss: 4.4091\n",
      "Epoch [2/2], Step [30890/67476], Loss: 4.4144\n",
      "Epoch [2/2], Step [30900/67476], Loss: 4.6046\n",
      "Epoch [2/2], Step [30910/67476], Loss: 4.6597\n",
      "Epoch [2/2], Step [30920/67476], Loss: 4.4340\n",
      "Epoch [2/2], Step [30930/67476], Loss: 4.4083\n",
      "Epoch [2/2], Step [30940/67476], Loss: 4.5692\n",
      "Epoch [2/2], Step [30950/67476], Loss: 4.4906\n",
      "Epoch [2/2], Step [30960/67476], Loss: 4.5660\n",
      "Epoch [2/2], Step [30970/67476], Loss: 4.5672\n",
      "Epoch [2/2], Step [30980/67476], Loss: 4.5096\n",
      "Epoch [2/2], Step [30990/67476], Loss: 4.5045\n",
      "Epoch [2/2], Step [31000/67476], Loss: 4.6419\n",
      "Epoch [2/2], Step [31010/67476], Loss: 4.5152\n",
      "Epoch [2/2], Step [31020/67476], Loss: 4.6297\n",
      "Epoch [2/2], Step [31030/67476], Loss: 4.4940\n",
      "Epoch [2/2], Step [31040/67476], Loss: 4.6357\n",
      "Epoch [2/2], Step [31050/67476], Loss: 4.6614\n",
      "Epoch [2/2], Step [31060/67476], Loss: 4.4570\n",
      "Epoch [2/2], Step [31070/67476], Loss: 4.7888\n",
      "Epoch [2/2], Step [31080/67476], Loss: 4.5205\n",
      "Epoch [2/2], Step [31090/67476], Loss: 4.5687\n",
      "Epoch [2/2], Step [31100/67476], Loss: 4.4662\n",
      "Epoch [2/2], Step [31110/67476], Loss: 4.6520\n",
      "Epoch [2/2], Step [31120/67476], Loss: 4.6437\n",
      "Epoch [2/2], Step [31130/67476], Loss: 4.4067\n",
      "Epoch [2/2], Step [31140/67476], Loss: 4.7466\n",
      "Epoch [2/2], Step [31150/67476], Loss: 4.6011\n",
      "Epoch [2/2], Step [31160/67476], Loss: 4.4615\n",
      "Epoch [2/2], Step [31170/67476], Loss: 4.6180\n",
      "Epoch [2/2], Step [31180/67476], Loss: 4.4036\n",
      "Epoch [2/2], Step [31190/67476], Loss: 4.3767\n",
      "Epoch [2/2], Step [31200/67476], Loss: 4.6188\n",
      "Epoch [2/2], Step [31210/67476], Loss: 4.6362\n",
      "Epoch [2/2], Step [31220/67476], Loss: 4.6419\n",
      "Epoch [2/2], Step [31230/67476], Loss: 4.5823\n",
      "Epoch [2/2], Step [31240/67476], Loss: 4.7593\n",
      "Epoch [2/2], Step [31250/67476], Loss: 4.6164\n",
      "Epoch [2/2], Step [31260/67476], Loss: 4.6611\n",
      "Epoch [2/2], Step [31270/67476], Loss: 4.3601\n",
      "Epoch [2/2], Step [31280/67476], Loss: 4.6841\n",
      "Epoch [2/2], Step [31290/67476], Loss: 4.4814\n",
      "Epoch [2/2], Step [31300/67476], Loss: 4.5932\n",
      "Epoch [2/2], Step [31310/67476], Loss: 4.6500\n",
      "Epoch [2/2], Step [31320/67476], Loss: 4.7097\n",
      "Epoch [2/2], Step [31330/67476], Loss: 4.6128\n",
      "Epoch [2/2], Step [31340/67476], Loss: 4.5739\n",
      "Epoch [2/2], Step [31350/67476], Loss: 4.3922\n",
      "Epoch [2/2], Step [31360/67476], Loss: 4.6419\n",
      "Epoch [2/2], Step [31370/67476], Loss: 4.4868\n",
      "Epoch [2/2], Step [31380/67476], Loss: 4.6506\n",
      "Epoch [2/2], Step [31390/67476], Loss: 4.3667\n",
      "Epoch [2/2], Step [31400/67476], Loss: 4.5817\n",
      "Epoch [2/2], Step [31410/67476], Loss: 4.4135\n",
      "Epoch [2/2], Step [31420/67476], Loss: 4.5977\n",
      "Epoch [2/2], Step [31430/67476], Loss: 4.5135\n",
      "Epoch [2/2], Step [31440/67476], Loss: 4.7808\n",
      "Epoch [2/2], Step [31450/67476], Loss: 4.6040\n",
      "Epoch [2/2], Step [31460/67476], Loss: 4.6390\n",
      "Epoch [2/2], Step [31470/67476], Loss: 4.6977\n",
      "Epoch [2/2], Step [31480/67476], Loss: 4.5468\n",
      "Epoch [2/2], Step [31490/67476], Loss: 4.7686\n",
      "Epoch [2/2], Step [31500/67476], Loss: 4.6803\n",
      "Epoch [2/2], Step [31510/67476], Loss: 4.1890\n",
      "Epoch [2/2], Step [31520/67476], Loss: 4.5693\n",
      "Epoch [2/2], Step [31530/67476], Loss: 4.3562\n",
      "Epoch [2/2], Step [31540/67476], Loss: 4.6942\n",
      "Epoch [2/2], Step [31550/67476], Loss: 4.5824\n",
      "Epoch [2/2], Step [31560/67476], Loss: 4.4744\n",
      "Epoch [2/2], Step [31570/67476], Loss: 4.4876\n",
      "Epoch [2/2], Step [31580/67476], Loss: 4.3384\n",
      "Epoch [2/2], Step [31590/67476], Loss: 4.4303\n",
      "Epoch [2/2], Step [31600/67476], Loss: 4.4808\n",
      "Epoch [2/2], Step [31610/67476], Loss: 4.5092\n",
      "Epoch [2/2], Step [31620/67476], Loss: 4.4968\n",
      "Epoch [2/2], Step [31630/67476], Loss: 4.5117\n",
      "Epoch [2/2], Step [31640/67476], Loss: 4.6995\n",
      "Epoch [2/2], Step [31650/67476], Loss: 4.3882\n",
      "Epoch [2/2], Step [31660/67476], Loss: 4.5384\n",
      "Epoch [2/2], Step [31670/67476], Loss: 4.5841\n",
      "Epoch [2/2], Step [31680/67476], Loss: 4.5128\n",
      "Epoch [2/2], Step [31690/67476], Loss: 4.6167\n",
      "Epoch [2/2], Step [31700/67476], Loss: 4.4401\n",
      "Epoch [2/2], Step [31710/67476], Loss: 4.5463\n",
      "Epoch [2/2], Step [31720/67476], Loss: 4.5890\n",
      "Epoch [2/2], Step [31730/67476], Loss: 4.3149\n",
      "Epoch [2/2], Step [31740/67476], Loss: 4.5752\n",
      "Epoch [2/2], Step [31750/67476], Loss: 4.3476\n",
      "Epoch [2/2], Step [31760/67476], Loss: 4.5300\n",
      "Epoch [2/2], Step [31770/67476], Loss: 4.3614\n",
      "Epoch [2/2], Step [31780/67476], Loss: 4.3966\n",
      "Epoch [2/2], Step [31790/67476], Loss: 4.5368\n",
      "Epoch [2/2], Step [31800/67476], Loss: 4.5864\n",
      "Epoch [2/2], Step [31810/67476], Loss: 4.4393\n",
      "Epoch [2/2], Step [31820/67476], Loss: 4.6034\n",
      "Epoch [2/2], Step [31830/67476], Loss: 4.4750\n",
      "Epoch [2/2], Step [31840/67476], Loss: 4.6454\n",
      "Epoch [2/2], Step [31850/67476], Loss: 4.4772\n",
      "Epoch [2/2], Step [31860/67476], Loss: 4.5601\n",
      "Epoch [2/2], Step [31870/67476], Loss: 4.4383\n",
      "Epoch [2/2], Step [31880/67476], Loss: 4.2014\n",
      "Epoch [2/2], Step [31890/67476], Loss: 4.4188\n",
      "Epoch [2/2], Step [31900/67476], Loss: 4.6689\n",
      "Epoch [2/2], Step [31910/67476], Loss: 4.7271\n",
      "Epoch [2/2], Step [31920/67476], Loss: 4.5513\n",
      "Epoch [2/2], Step [31930/67476], Loss: 4.4967\n",
      "Epoch [2/2], Step [31940/67476], Loss: 4.5749\n",
      "Epoch [2/2], Step [31950/67476], Loss: 4.4384\n",
      "Epoch [2/2], Step [31960/67476], Loss: 4.6256\n",
      "Epoch [2/2], Step [31970/67476], Loss: 4.7539\n",
      "Epoch [2/2], Step [31980/67476], Loss: 4.5961\n",
      "Epoch [2/2], Step [31990/67476], Loss: 4.4998\n",
      "Epoch [2/2], Step [32000/67476], Loss: 4.4307\n",
      "Epoch [2/2], Step [32010/67476], Loss: 4.4709\n",
      "Epoch [2/2], Step [32020/67476], Loss: 4.6450\n",
      "Epoch [2/2], Step [32030/67476], Loss: 4.5093\n",
      "Epoch [2/2], Step [32040/67476], Loss: 4.4827\n",
      "Epoch [2/2], Step [32050/67476], Loss: 4.5061\n",
      "Epoch [2/2], Step [32060/67476], Loss: 4.5450\n",
      "Epoch [2/2], Step [32070/67476], Loss: 4.6929\n",
      "Epoch [2/2], Step [32080/67476], Loss: 4.4840\n",
      "Epoch [2/2], Step [32090/67476], Loss: 4.3962\n",
      "Epoch [2/2], Step [32100/67476], Loss: 4.5585\n",
      "Epoch [2/2], Step [32110/67476], Loss: 4.6417\n",
      "Epoch [2/2], Step [32120/67476], Loss: 4.4238\n",
      "Epoch [2/2], Step [32130/67476], Loss: 4.5981\n",
      "Epoch [2/2], Step [32140/67476], Loss: 4.6925\n",
      "Epoch [2/2], Step [32150/67476], Loss: 4.5813\n",
      "Epoch [2/2], Step [32160/67476], Loss: 4.4598\n",
      "Epoch [2/2], Step [32170/67476], Loss: 4.6227\n",
      "Epoch [2/2], Step [32180/67476], Loss: 4.4676\n",
      "Epoch [2/2], Step [32190/67476], Loss: 4.4225\n",
      "Epoch [2/2], Step [32200/67476], Loss: 4.4282\n",
      "Epoch [2/2], Step [32210/67476], Loss: 4.4721\n",
      "Epoch [2/2], Step [32220/67476], Loss: 4.4996\n",
      "Epoch [2/2], Step [32230/67476], Loss: 4.6009\n",
      "Epoch [2/2], Step [32240/67476], Loss: 4.4981\n",
      "Epoch [2/2], Step [32250/67476], Loss: 4.3763\n",
      "Epoch [2/2], Step [32260/67476], Loss: 4.4997\n",
      "Epoch [2/2], Step [32270/67476], Loss: 4.5179\n",
      "Epoch [2/2], Step [32280/67476], Loss: 4.5197\n",
      "Epoch [2/2], Step [32290/67476], Loss: 4.5302\n",
      "Epoch [2/2], Step [32300/67476], Loss: 4.5247\n",
      "Epoch [2/2], Step [32310/67476], Loss: 4.5479\n",
      "Epoch [2/2], Step [32320/67476], Loss: 4.4760\n",
      "Epoch [2/2], Step [32330/67476], Loss: 4.5993\n",
      "Epoch [2/2], Step [32340/67476], Loss: 4.2717\n",
      "Epoch [2/2], Step [32350/67476], Loss: 4.5855\n",
      "Epoch [2/2], Step [32360/67476], Loss: 4.3670\n",
      "Epoch [2/2], Step [32370/67476], Loss: 4.7411\n",
      "Epoch [2/2], Step [32380/67476], Loss: 4.4818\n",
      "Epoch [2/2], Step [32390/67476], Loss: 4.4827\n",
      "Epoch [2/2], Step [32400/67476], Loss: 4.5820\n",
      "Epoch [2/2], Step [32410/67476], Loss: 4.5199\n",
      "Epoch [2/2], Step [32420/67476], Loss: 4.3887\n",
      "Epoch [2/2], Step [32430/67476], Loss: 4.6301\n",
      "Epoch [2/2], Step [32440/67476], Loss: 4.3961\n",
      "Epoch [2/2], Step [32450/67476], Loss: 4.4888\n",
      "Epoch [2/2], Step [32460/67476], Loss: 4.6113\n",
      "Epoch [2/2], Step [32470/67476], Loss: 4.5336\n",
      "Epoch [2/2], Step [32480/67476], Loss: 4.4127\n",
      "Epoch [2/2], Step [32490/67476], Loss: 4.4234\n",
      "Epoch [2/2], Step [32500/67476], Loss: 4.5464\n",
      "Epoch [2/2], Step [32510/67476], Loss: 4.4143\n",
      "Epoch [2/2], Step [32520/67476], Loss: 4.6362\n",
      "Epoch [2/2], Step [32530/67476], Loss: 4.5079\n",
      "Epoch [2/2], Step [32540/67476], Loss: 4.5298\n",
      "Epoch [2/2], Step [32550/67476], Loss: 4.5663\n",
      "Epoch [2/2], Step [32560/67476], Loss: 4.6109\n",
      "Epoch [2/2], Step [32570/67476], Loss: 4.5595\n",
      "Epoch [2/2], Step [32580/67476], Loss: 4.6531\n",
      "Epoch [2/2], Step [32590/67476], Loss: 4.5283\n",
      "Epoch [2/2], Step [32600/67476], Loss: 4.4652\n",
      "Epoch [2/2], Step [32610/67476], Loss: 4.6384\n",
      "Epoch [2/2], Step [32620/67476], Loss: 4.3863\n",
      "Epoch [2/2], Step [32630/67476], Loss: 4.8296\n",
      "Epoch [2/2], Step [32640/67476], Loss: 4.6301\n",
      "Epoch [2/2], Step [32650/67476], Loss: 4.4759\n",
      "Epoch [2/2], Step [32660/67476], Loss: 4.4940\n",
      "Epoch [2/2], Step [32670/67476], Loss: 4.6397\n",
      "Epoch [2/2], Step [32680/67476], Loss: 4.6142\n",
      "Epoch [2/2], Step [32690/67476], Loss: 4.6877\n",
      "Epoch [2/2], Step [32700/67476], Loss: 4.3918\n",
      "Epoch [2/2], Step [32710/67476], Loss: 4.5463\n",
      "Epoch [2/2], Step [32720/67476], Loss: 4.4598\n",
      "Epoch [2/2], Step [32730/67476], Loss: 4.5259\n",
      "Epoch [2/2], Step [32740/67476], Loss: 4.6780\n",
      "Epoch [2/2], Step [32750/67476], Loss: 4.6103\n",
      "Epoch [2/2], Step [32760/67476], Loss: 4.5789\n",
      "Epoch [2/2], Step [32770/67476], Loss: 4.3796\n",
      "Epoch [2/2], Step [32780/67476], Loss: 4.6953\n",
      "Epoch [2/2], Step [32790/67476], Loss: 4.4421\n",
      "Epoch [2/2], Step [32800/67476], Loss: 4.6536\n",
      "Epoch [2/2], Step [32810/67476], Loss: 4.7448\n",
      "Epoch [2/2], Step [32820/67476], Loss: 4.7493\n",
      "Epoch [2/2], Step [32830/67476], Loss: 4.4503\n",
      "Epoch [2/2], Step [32840/67476], Loss: 4.5196\n",
      "Epoch [2/2], Step [32850/67476], Loss: 4.4997\n",
      "Epoch [2/2], Step [32860/67476], Loss: 4.5670\n",
      "Epoch [2/2], Step [32870/67476], Loss: 4.5446\n",
      "Epoch [2/2], Step [32880/67476], Loss: 4.4504\n",
      "Epoch [2/2], Step [32890/67476], Loss: 4.5011\n",
      "Epoch [2/2], Step [32900/67476], Loss: 4.6182\n",
      "Epoch [2/2], Step [32910/67476], Loss: 4.6443\n",
      "Epoch [2/2], Step [32920/67476], Loss: 4.5932\n",
      "Epoch [2/2], Step [32930/67476], Loss: 4.8688\n",
      "Epoch [2/2], Step [32940/67476], Loss: 4.3514\n",
      "Epoch [2/2], Step [32950/67476], Loss: 4.6359\n",
      "Epoch [2/2], Step [32960/67476], Loss: 4.3265\n",
      "Epoch [2/2], Step [32970/67476], Loss: 4.6725\n",
      "Epoch [2/2], Step [32980/67476], Loss: 4.7009\n",
      "Epoch [2/2], Step [32990/67476], Loss: 4.3419\n",
      "Epoch [2/2], Step [33000/67476], Loss: 4.6397\n",
      "Epoch [2/2], Step [33010/67476], Loss: 4.3922\n",
      "Epoch [2/2], Step [33020/67476], Loss: 4.7022\n",
      "Epoch [2/2], Step [33030/67476], Loss: 4.5796\n",
      "Epoch [2/2], Step [33040/67476], Loss: 4.5212\n",
      "Epoch [2/2], Step [33050/67476], Loss: 4.4473\n",
      "Epoch [2/2], Step [33060/67476], Loss: 4.4111\n",
      "Epoch [2/2], Step [33070/67476], Loss: 4.4281\n",
      "Epoch [2/2], Step [33080/67476], Loss: 4.5692\n",
      "Epoch [2/2], Step [33090/67476], Loss: 4.5257\n",
      "Epoch [2/2], Step [33100/67476], Loss: 4.6143\n",
      "Epoch [2/2], Step [33110/67476], Loss: 4.5098\n",
      "Epoch [2/2], Step [33120/67476], Loss: 4.3951\n",
      "Epoch [2/2], Step [33130/67476], Loss: 4.6410\n",
      "Epoch [2/2], Step [33140/67476], Loss: 4.6724\n",
      "Epoch [2/2], Step [33150/67476], Loss: 4.4811\n",
      "Epoch [2/2], Step [33160/67476], Loss: 4.4032\n",
      "Epoch [2/2], Step [33170/67476], Loss: 4.9723\n",
      "Epoch [2/2], Step [33180/67476], Loss: 4.3436\n",
      "Epoch [2/2], Step [33190/67476], Loss: 4.6975\n",
      "Epoch [2/2], Step [33200/67476], Loss: 4.6244\n",
      "Epoch [2/2], Step [33210/67476], Loss: 4.6682\n",
      "Epoch [2/2], Step [33220/67476], Loss: 4.5033\n",
      "Epoch [2/2], Step [33230/67476], Loss: 4.5299\n",
      "Epoch [2/2], Step [33240/67476], Loss: 4.3569\n",
      "Epoch [2/2], Step [33250/67476], Loss: 4.8013\n",
      "Epoch [2/2], Step [33260/67476], Loss: 4.4421\n",
      "Epoch [2/2], Step [33270/67476], Loss: 4.5232\n",
      "Epoch [2/2], Step [33280/67476], Loss: 4.5230\n",
      "Epoch [2/2], Step [33290/67476], Loss: 4.4585\n",
      "Epoch [2/2], Step [33300/67476], Loss: 4.5075\n",
      "Epoch [2/2], Step [33310/67476], Loss: 4.4790\n",
      "Epoch [2/2], Step [33320/67476], Loss: 4.6861\n",
      "Epoch [2/2], Step [33330/67476], Loss: 4.6479\n",
      "Epoch [2/2], Step [33340/67476], Loss: 4.4976\n",
      "Epoch [2/2], Step [33350/67476], Loss: 4.4271\n",
      "Epoch [2/2], Step [33360/67476], Loss: 4.4979\n",
      "Epoch [2/2], Step [33370/67476], Loss: 4.5747\n",
      "Epoch [2/2], Step [33380/67476], Loss: 4.5400\n",
      "Epoch [2/2], Step [33390/67476], Loss: 4.4478\n",
      "Epoch [2/2], Step [33400/67476], Loss: 4.4872\n",
      "Epoch [2/2], Step [33410/67476], Loss: 4.5082\n",
      "Epoch [2/2], Step [33420/67476], Loss: 4.5661\n",
      "Epoch [2/2], Step [33430/67476], Loss: 4.6081\n",
      "Epoch [2/2], Step [33440/67476], Loss: 4.9102\n",
      "Epoch [2/2], Step [33450/67476], Loss: 4.4437\n",
      "Epoch [2/2], Step [33460/67476], Loss: 4.4824\n",
      "Epoch [2/2], Step [33470/67476], Loss: 4.5501\n",
      "Epoch [2/2], Step [33480/67476], Loss: 4.5991\n",
      "Epoch [2/2], Step [33490/67476], Loss: 4.5782\n",
      "Epoch [2/2], Step [33500/67476], Loss: 4.5094\n",
      "Epoch [2/2], Step [33510/67476], Loss: 4.3707\n",
      "Epoch [2/2], Step [33520/67476], Loss: 4.7325\n",
      "Epoch [2/2], Step [33530/67476], Loss: 4.4347\n",
      "Epoch [2/2], Step [33540/67476], Loss: 4.6511\n",
      "Epoch [2/2], Step [33550/67476], Loss: 4.5927\n",
      "Epoch [2/2], Step [33560/67476], Loss: 4.6444\n",
      "Epoch [2/2], Step [33570/67476], Loss: 4.4776\n",
      "Epoch [2/2], Step [33580/67476], Loss: 4.5508\n",
      "Epoch [2/2], Step [33590/67476], Loss: 4.5558\n",
      "Epoch [2/2], Step [33600/67476], Loss: 4.7808\n",
      "Epoch [2/2], Step [33610/67476], Loss: 4.7525\n",
      "Epoch [2/2], Step [33620/67476], Loss: 4.8154\n",
      "Epoch [2/2], Step [33630/67476], Loss: 4.5570\n",
      "Epoch [2/2], Step [33640/67476], Loss: 4.4662\n",
      "Epoch [2/2], Step [33650/67476], Loss: 4.8356\n",
      "Epoch [2/2], Step [33660/67476], Loss: 4.4584\n",
      "Epoch [2/2], Step [33670/67476], Loss: 4.6489\n",
      "Epoch [2/2], Step [33680/67476], Loss: 4.6090\n",
      "Epoch [2/2], Step [33690/67476], Loss: 4.5919\n",
      "Epoch [2/2], Step [33700/67476], Loss: 4.3715\n",
      "Epoch [2/2], Step [33710/67476], Loss: 4.6367\n",
      "Epoch [2/2], Step [33720/67476], Loss: 4.8435\n",
      "Epoch [2/2], Step [33730/67476], Loss: 4.3187\n",
      "Epoch [2/2], Step [33740/67476], Loss: 4.5211\n",
      "Epoch [2/2], Step [33750/67476], Loss: 4.7257\n",
      "Epoch [2/2], Step [33760/67476], Loss: 4.6535\n",
      "Epoch [2/2], Step [33770/67476], Loss: 4.3846\n",
      "Epoch [2/2], Step [33780/67476], Loss: 4.5505\n",
      "Epoch [2/2], Step [33790/67476], Loss: 4.5516\n",
      "Epoch [2/2], Step [33800/67476], Loss: 4.6793\n",
      "Epoch [2/2], Step [33810/67476], Loss: 4.3849\n",
      "Epoch [2/2], Step [33820/67476], Loss: 4.3598\n",
      "Epoch [2/2], Step [33830/67476], Loss: 4.4191\n",
      "Epoch [2/2], Step [33840/67476], Loss: 4.5239\n",
      "Epoch [2/2], Step [33850/67476], Loss: 4.6185\n",
      "Epoch [2/2], Step [33860/67476], Loss: 4.5687\n",
      "Epoch [2/2], Step [33870/67476], Loss: 4.3824\n",
      "Epoch [2/2], Step [33880/67476], Loss: 4.4243\n",
      "Epoch [2/2], Step [33890/67476], Loss: 4.4793\n",
      "Epoch [2/2], Step [33900/67476], Loss: 4.5029\n",
      "Epoch [2/2], Step [33910/67476], Loss: 4.6772\n",
      "Epoch [2/2], Step [33920/67476], Loss: 4.5344\n",
      "Epoch [2/2], Step [33930/67476], Loss: 4.3999\n",
      "Epoch [2/2], Step [33940/67476], Loss: 4.5449\n",
      "Epoch [2/2], Step [33950/67476], Loss: 4.5613\n",
      "Epoch [2/2], Step [33960/67476], Loss: 4.3026\n",
      "Epoch [2/2], Step [33970/67476], Loss: 4.6056\n",
      "Epoch [2/2], Step [33980/67476], Loss: 4.6578\n",
      "Epoch [2/2], Step [33990/67476], Loss: 4.3940\n",
      "Epoch [2/2], Step [34000/67476], Loss: 4.4110\n",
      "Epoch [2/2], Step [34010/67476], Loss: 4.5474\n",
      "Epoch [2/2], Step [34020/67476], Loss: 4.3940\n",
      "Epoch [2/2], Step [34030/67476], Loss: 4.3575\n",
      "Epoch [2/2], Step [34040/67476], Loss: 4.4716\n",
      "Epoch [2/2], Step [34050/67476], Loss: 4.4539\n",
      "Epoch [2/2], Step [34060/67476], Loss: 4.3380\n",
      "Epoch [2/2], Step [34070/67476], Loss: 4.2736\n",
      "Epoch [2/2], Step [34080/67476], Loss: 4.3817\n",
      "Epoch [2/2], Step [34090/67476], Loss: 4.5147\n",
      "Epoch [2/2], Step [34100/67476], Loss: 4.4122\n",
      "Epoch [2/2], Step [34110/67476], Loss: 4.6200\n",
      "Epoch [2/2], Step [34120/67476], Loss: 4.7317\n",
      "Epoch [2/2], Step [34130/67476], Loss: 4.5924\n",
      "Epoch [2/2], Step [34140/67476], Loss: 4.5940\n",
      "Epoch [2/2], Step [34150/67476], Loss: 4.6889\n",
      "Epoch [2/2], Step [34160/67476], Loss: 4.6573\n",
      "Epoch [2/2], Step [34170/67476], Loss: 4.6564\n",
      "Epoch [2/2], Step [34180/67476], Loss: 4.3958\n",
      "Epoch [2/2], Step [34190/67476], Loss: 4.4264\n",
      "Epoch [2/2], Step [34200/67476], Loss: 4.5742\n",
      "Epoch [2/2], Step [34210/67476], Loss: 4.3451\n",
      "Epoch [2/2], Step [34220/67476], Loss: 4.7116\n",
      "Epoch [2/2], Step [34230/67476], Loss: 4.6469\n",
      "Epoch [2/2], Step [34240/67476], Loss: 4.6430\n",
      "Epoch [2/2], Step [34250/67476], Loss: 4.3387\n",
      "Epoch [2/2], Step [34260/67476], Loss: 4.4475\n",
      "Epoch [2/2], Step [34270/67476], Loss: 4.5847\n",
      "Epoch [2/2], Step [34280/67476], Loss: 4.4677\n",
      "Epoch [2/2], Step [34290/67476], Loss: 4.5860\n",
      "Epoch [2/2], Step [34300/67476], Loss: 4.4949\n",
      "Epoch [2/2], Step [34310/67476], Loss: 4.4411\n",
      "Epoch [2/2], Step [34320/67476], Loss: 4.5563\n",
      "Epoch [2/2], Step [34330/67476], Loss: 4.7516\n",
      "Epoch [2/2], Step [34340/67476], Loss: 4.5619\n",
      "Epoch [2/2], Step [34350/67476], Loss: 4.4632\n",
      "Epoch [2/2], Step [34360/67476], Loss: 4.5293\n",
      "Epoch [2/2], Step [34370/67476], Loss: 4.6595\n",
      "Epoch [2/2], Step [34380/67476], Loss: 4.4752\n",
      "Epoch [2/2], Step [34390/67476], Loss: 4.5228\n",
      "Epoch [2/2], Step [34400/67476], Loss: 4.6026\n",
      "Epoch [2/2], Step [34410/67476], Loss: 4.6602\n",
      "Epoch [2/2], Step [34420/67476], Loss: 4.5261\n",
      "Epoch [2/2], Step [34430/67476], Loss: 4.4909\n",
      "Epoch [2/2], Step [34440/67476], Loss: 4.4945\n",
      "Epoch [2/2], Step [34450/67476], Loss: 4.7085\n",
      "Epoch [2/2], Step [34460/67476], Loss: 4.5114\n",
      "Epoch [2/2], Step [34470/67476], Loss: 4.5947\n",
      "Epoch [2/2], Step [34480/67476], Loss: 4.5527\n",
      "Epoch [2/2], Step [34490/67476], Loss: 4.5876\n",
      "Epoch [2/2], Step [34500/67476], Loss: 4.4481\n",
      "Epoch [2/2], Step [34510/67476], Loss: 4.5014\n",
      "Epoch [2/2], Step [34520/67476], Loss: 4.4492\n",
      "Epoch [2/2], Step [34530/67476], Loss: 4.7134\n",
      "Epoch [2/2], Step [34540/67476], Loss: 4.5635\n",
      "Epoch [2/2], Step [34550/67476], Loss: 4.7647\n",
      "Epoch [2/2], Step [34560/67476], Loss: 4.4919\n",
      "Epoch [2/2], Step [34570/67476], Loss: 4.5663\n",
      "Epoch [2/2], Step [34580/67476], Loss: 4.4701\n",
      "Epoch [2/2], Step [34590/67476], Loss: 4.5620\n",
      "Epoch [2/2], Step [34600/67476], Loss: 4.5686\n",
      "Epoch [2/2], Step [34610/67476], Loss: 4.5838\n",
      "Epoch [2/2], Step [34620/67476], Loss: 4.5816\n",
      "Epoch [2/2], Step [34630/67476], Loss: 4.5435\n",
      "Epoch [2/2], Step [34640/67476], Loss: 4.5564\n",
      "Epoch [2/2], Step [34650/67476], Loss: 4.5504\n",
      "Epoch [2/2], Step [34660/67476], Loss: 4.6558\n",
      "Epoch [2/2], Step [34670/67476], Loss: 4.7414\n",
      "Epoch [2/2], Step [34680/67476], Loss: 4.6491\n",
      "Epoch [2/2], Step [34690/67476], Loss: 4.5744\n",
      "Epoch [2/2], Step [34700/67476], Loss: 4.6163\n",
      "Epoch [2/2], Step [34710/67476], Loss: 4.3478\n",
      "Epoch [2/2], Step [34720/67476], Loss: 4.5090\n",
      "Epoch [2/2], Step [34730/67476], Loss: 4.5822\n",
      "Epoch [2/2], Step [34740/67476], Loss: 4.4383\n",
      "Epoch [2/2], Step [34750/67476], Loss: 4.4107\n",
      "Epoch [2/2], Step [34760/67476], Loss: 4.5737\n",
      "Epoch [2/2], Step [34770/67476], Loss: 4.5203\n",
      "Epoch [2/2], Step [34780/67476], Loss: 4.6304\n",
      "Epoch [2/2], Step [34790/67476], Loss: 4.5436\n",
      "Epoch [2/2], Step [34800/67476], Loss: 4.6627\n",
      "Epoch [2/2], Step [34810/67476], Loss: 4.4282\n",
      "Epoch [2/2], Step [34820/67476], Loss: 4.6621\n",
      "Epoch [2/2], Step [34830/67476], Loss: 4.4316\n",
      "Epoch [2/2], Step [34840/67476], Loss: 4.7097\n",
      "Epoch [2/2], Step [34850/67476], Loss: 4.5608\n",
      "Epoch [2/2], Step [34860/67476], Loss: 4.4554\n",
      "Epoch [2/2], Step [34870/67476], Loss: 4.6733\n",
      "Epoch [2/2], Step [34880/67476], Loss: 4.6896\n",
      "Epoch [2/2], Step [34890/67476], Loss: 4.5726\n",
      "Epoch [2/2], Step [34900/67476], Loss: 4.5644\n",
      "Epoch [2/2], Step [34910/67476], Loss: 4.6883\n",
      "Epoch [2/2], Step [34920/67476], Loss: 4.3738\n",
      "Epoch [2/2], Step [34930/67476], Loss: 4.3363\n",
      "Epoch [2/2], Step [34940/67476], Loss: 4.4934\n",
      "Epoch [2/2], Step [34950/67476], Loss: 4.3584\n",
      "Epoch [2/2], Step [34960/67476], Loss: 4.3733\n",
      "Epoch [2/2], Step [34970/67476], Loss: 4.5517\n",
      "Epoch [2/2], Step [34980/67476], Loss: 4.4550\n",
      "Epoch [2/2], Step [34990/67476], Loss: 4.5424\n",
      "Epoch [2/2], Step [35000/67476], Loss: 4.4212\n",
      "Epoch [2/2], Step [35010/67476], Loss: 4.9759\n",
      "Epoch [2/2], Step [35020/67476], Loss: 4.6917\n",
      "Epoch [2/2], Step [35030/67476], Loss: 4.4562\n",
      "Epoch [2/2], Step [35040/67476], Loss: 4.5720\n",
      "Epoch [2/2], Step [35050/67476], Loss: 4.4815\n",
      "Epoch [2/2], Step [35060/67476], Loss: 4.4676\n",
      "Epoch [2/2], Step [35070/67476], Loss: 4.7256\n",
      "Epoch [2/2], Step [35080/67476], Loss: 4.5218\n",
      "Epoch [2/2], Step [35090/67476], Loss: 4.5852\n",
      "Epoch [2/2], Step [35100/67476], Loss: 4.3867\n",
      "Epoch [2/2], Step [35110/67476], Loss: 4.6887\n",
      "Epoch [2/2], Step [35120/67476], Loss: 4.6563\n",
      "Epoch [2/2], Step [35130/67476], Loss: 4.4383\n",
      "Epoch [2/2], Step [35140/67476], Loss: 4.6991\n",
      "Epoch [2/2], Step [35150/67476], Loss: 4.6111\n",
      "Epoch [2/2], Step [35160/67476], Loss: 4.5377\n",
      "Epoch [2/2], Step [35170/67476], Loss: 4.5333\n",
      "Epoch [2/2], Step [35180/67476], Loss: 4.5283\n",
      "Epoch [2/2], Step [35190/67476], Loss: 4.7248\n",
      "Epoch [2/2], Step [35200/67476], Loss: 4.5827\n",
      "Epoch [2/2], Step [35210/67476], Loss: 4.2840\n",
      "Epoch [2/2], Step [35220/67476], Loss: 4.6843\n",
      "Epoch [2/2], Step [35230/67476], Loss: 4.3895\n",
      "Epoch [2/2], Step [35240/67476], Loss: 4.5961\n",
      "Epoch [2/2], Step [35250/67476], Loss: 4.4693\n",
      "Epoch [2/2], Step [35260/67476], Loss: 4.6707\n",
      "Epoch [2/2], Step [35270/67476], Loss: 4.4671\n",
      "Epoch [2/2], Step [35280/67476], Loss: 4.5663\n",
      "Epoch [2/2], Step [35290/67476], Loss: 4.4913\n",
      "Epoch [2/2], Step [35300/67476], Loss: 4.5561\n",
      "Epoch [2/2], Step [35310/67476], Loss: 4.4897\n",
      "Epoch [2/2], Step [35320/67476], Loss: 4.5107\n",
      "Epoch [2/2], Step [35330/67476], Loss: 4.6298\n",
      "Epoch [2/2], Step [35340/67476], Loss: 4.6039\n",
      "Epoch [2/2], Step [35350/67476], Loss: 4.5905\n",
      "Epoch [2/2], Step [35360/67476], Loss: 4.5302\n",
      "Epoch [2/2], Step [35370/67476], Loss: 4.6806\n",
      "Epoch [2/2], Step [35380/67476], Loss: 4.5339\n",
      "Epoch [2/2], Step [35390/67476], Loss: 4.4663\n",
      "Epoch [2/2], Step [35400/67476], Loss: 4.4434\n",
      "Epoch [2/2], Step [35410/67476], Loss: 4.4586\n",
      "Epoch [2/2], Step [35420/67476], Loss: 4.4047\n",
      "Epoch [2/2], Step [35430/67476], Loss: 4.8270\n",
      "Epoch [2/2], Step [35440/67476], Loss: 4.4753\n",
      "Epoch [2/2], Step [35450/67476], Loss: 4.4332\n",
      "Epoch [2/2], Step [35460/67476], Loss: 4.6120\n",
      "Epoch [2/2], Step [35470/67476], Loss: 4.6376\n",
      "Epoch [2/2], Step [35480/67476], Loss: 4.4898\n",
      "Epoch [2/2], Step [35490/67476], Loss: 4.4720\n",
      "Epoch [2/2], Step [35500/67476], Loss: 4.6477\n",
      "Epoch [2/2], Step [35510/67476], Loss: 4.5107\n",
      "Epoch [2/2], Step [35520/67476], Loss: 4.4193\n",
      "Epoch [2/2], Step [35530/67476], Loss: 4.3461\n",
      "Epoch [2/2], Step [35540/67476], Loss: 4.5930\n",
      "Epoch [2/2], Step [35550/67476], Loss: 4.4508\n",
      "Epoch [2/2], Step [35560/67476], Loss: 4.7643\n",
      "Epoch [2/2], Step [35570/67476], Loss: 4.5861\n",
      "Epoch [2/2], Step [35580/67476], Loss: 4.5853\n",
      "Epoch [2/2], Step [35590/67476], Loss: 4.5365\n",
      "Epoch [2/2], Step [35600/67476], Loss: 4.5786\n",
      "Epoch [2/2], Step [35610/67476], Loss: 4.5113\n",
      "Epoch [2/2], Step [35620/67476], Loss: 4.5737\n",
      "Epoch [2/2], Step [35630/67476], Loss: 4.5358\n",
      "Epoch [2/2], Step [35640/67476], Loss: 4.5520\n",
      "Epoch [2/2], Step [35650/67476], Loss: 4.4268\n",
      "Epoch [2/2], Step [35660/67476], Loss: 4.5561\n",
      "Epoch [2/2], Step [35670/67476], Loss: 4.5253\n",
      "Epoch [2/2], Step [35680/67476], Loss: 4.6594\n",
      "Epoch [2/2], Step [35690/67476], Loss: 4.3404\n",
      "Epoch [2/2], Step [35700/67476], Loss: 4.7457\n",
      "Epoch [2/2], Step [35710/67476], Loss: 4.4706\n",
      "Epoch [2/2], Step [35720/67476], Loss: 4.7031\n",
      "Epoch [2/2], Step [35730/67476], Loss: 4.7643\n",
      "Epoch [2/2], Step [35740/67476], Loss: 4.4028\n",
      "Epoch [2/2], Step [35750/67476], Loss: 4.2077\n",
      "Epoch [2/2], Step [35760/67476], Loss: 4.6956\n",
      "Epoch [2/2], Step [35770/67476], Loss: 4.4035\n",
      "Epoch [2/2], Step [35780/67476], Loss: 4.6223\n",
      "Epoch [2/2], Step [35790/67476], Loss: 4.5233\n",
      "Epoch [2/2], Step [35800/67476], Loss: 4.5459\n",
      "Epoch [2/2], Step [35810/67476], Loss: 4.5292\n",
      "Epoch [2/2], Step [35820/67476], Loss: 4.7485\n",
      "Epoch [2/2], Step [35830/67476], Loss: 4.5490\n",
      "Epoch [2/2], Step [35840/67476], Loss: 4.7271\n",
      "Epoch [2/2], Step [35850/67476], Loss: 4.6884\n",
      "Epoch [2/2], Step [35860/67476], Loss: 4.4704\n",
      "Epoch [2/2], Step [35870/67476], Loss: 4.5647\n",
      "Epoch [2/2], Step [35880/67476], Loss: 4.4269\n",
      "Epoch [2/2], Step [35890/67476], Loss: 4.5198\n",
      "Epoch [2/2], Step [35900/67476], Loss: 4.6745\n",
      "Epoch [2/2], Step [35910/67476], Loss: 4.6963\n",
      "Epoch [2/2], Step [35920/67476], Loss: 4.6214\n",
      "Epoch [2/2], Step [35930/67476], Loss: 4.5578\n",
      "Epoch [2/2], Step [35940/67476], Loss: 4.7102\n",
      "Epoch [2/2], Step [35950/67476], Loss: 4.5716\n",
      "Epoch [2/2], Step [35960/67476], Loss: 4.5840\n",
      "Epoch [2/2], Step [35970/67476], Loss: 4.4512\n",
      "Epoch [2/2], Step [35980/67476], Loss: 4.6673\n",
      "Epoch [2/2], Step [35990/67476], Loss: 4.5352\n",
      "Epoch [2/2], Step [36000/67476], Loss: 4.5083\n",
      "Epoch [2/2], Step [36010/67476], Loss: 4.5418\n",
      "Epoch [2/2], Step [36020/67476], Loss: 4.5148\n",
      "Epoch [2/2], Step [36030/67476], Loss: 4.4912\n",
      "Epoch [2/2], Step [36040/67476], Loss: 4.5042\n",
      "Epoch [2/2], Step [36050/67476], Loss: 4.6547\n",
      "Epoch [2/2], Step [36060/67476], Loss: 4.4591\n",
      "Epoch [2/2], Step [36070/67476], Loss: 4.6431\n",
      "Epoch [2/2], Step [36080/67476], Loss: 4.4400\n",
      "Epoch [2/2], Step [36090/67476], Loss: 4.5230\n",
      "Epoch [2/2], Step [36100/67476], Loss: 4.5491\n",
      "Epoch [2/2], Step [36110/67476], Loss: 4.3918\n",
      "Epoch [2/2], Step [36120/67476], Loss: 4.5123\n",
      "Epoch [2/2], Step [36130/67476], Loss: 4.7358\n",
      "Epoch [2/2], Step [36140/67476], Loss: 4.6428\n",
      "Epoch [2/2], Step [36150/67476], Loss: 4.3512\n",
      "Epoch [2/2], Step [36160/67476], Loss: 4.5848\n",
      "Epoch [2/2], Step [36170/67476], Loss: 4.5289\n",
      "Epoch [2/2], Step [36180/67476], Loss: 4.5423\n",
      "Epoch [2/2], Step [36190/67476], Loss: 4.7140\n",
      "Epoch [2/2], Step [36200/67476], Loss: 4.7074\n",
      "Epoch [2/2], Step [36210/67476], Loss: 4.5468\n",
      "Epoch [2/2], Step [36220/67476], Loss: 4.3373\n",
      "Epoch [2/2], Step [36230/67476], Loss: 4.5768\n",
      "Epoch [2/2], Step [36240/67476], Loss: 4.3878\n",
      "Epoch [2/2], Step [36250/67476], Loss: 4.6443\n",
      "Epoch [2/2], Step [36260/67476], Loss: 4.5233\n",
      "Epoch [2/2], Step [36270/67476], Loss: 4.4921\n",
      "Epoch [2/2], Step [36280/67476], Loss: 4.3501\n",
      "Epoch [2/2], Step [36290/67476], Loss: 4.6457\n",
      "Epoch [2/2], Step [36300/67476], Loss: 4.5652\n",
      "Epoch [2/2], Step [36310/67476], Loss: 4.6181\n",
      "Epoch [2/2], Step [36320/67476], Loss: 4.5913\n",
      "Epoch [2/2], Step [36330/67476], Loss: 4.6961\n",
      "Epoch [2/2], Step [36340/67476], Loss: 4.6657\n",
      "Epoch [2/2], Step [36350/67476], Loss: 4.4035\n",
      "Epoch [2/2], Step [36360/67476], Loss: 4.6834\n",
      "Epoch [2/2], Step [36370/67476], Loss: 4.4571\n",
      "Epoch [2/2], Step [36380/67476], Loss: 4.3885\n",
      "Epoch [2/2], Step [36390/67476], Loss: 4.7211\n",
      "Epoch [2/2], Step [36400/67476], Loss: 4.3404\n",
      "Epoch [2/2], Step [36410/67476], Loss: 4.4695\n",
      "Epoch [2/2], Step [36420/67476], Loss: 4.2858\n",
      "Epoch [2/2], Step [36430/67476], Loss: 4.6567\n",
      "Epoch [2/2], Step [36440/67476], Loss: 4.6798\n",
      "Epoch [2/2], Step [36450/67476], Loss: 4.6625\n",
      "Epoch [2/2], Step [36460/67476], Loss: 4.4820\n",
      "Epoch [2/2], Step [36470/67476], Loss: 4.6746\n",
      "Epoch [2/2], Step [36480/67476], Loss: 4.5863\n",
      "Epoch [2/2], Step [36490/67476], Loss: 4.5184\n",
      "Epoch [2/2], Step [36500/67476], Loss: 4.6546\n",
      "Epoch [2/2], Step [36510/67476], Loss: 4.5766\n",
      "Epoch [2/2], Step [36520/67476], Loss: 4.5799\n",
      "Epoch [2/2], Step [36530/67476], Loss: 4.5580\n",
      "Epoch [2/2], Step [36540/67476], Loss: 4.6375\n",
      "Epoch [2/2], Step [36550/67476], Loss: 4.3965\n",
      "Epoch [2/2], Step [36560/67476], Loss: 4.5018\n",
      "Epoch [2/2], Step [36570/67476], Loss: 4.4346\n",
      "Epoch [2/2], Step [36580/67476], Loss: 4.6100\n",
      "Epoch [2/2], Step [36590/67476], Loss: 4.6126\n",
      "Epoch [2/2], Step [36600/67476], Loss: 4.4130\n",
      "Epoch [2/2], Step [36610/67476], Loss: 4.5985\n",
      "Epoch [2/2], Step [36620/67476], Loss: 4.3042\n",
      "Epoch [2/2], Step [36630/67476], Loss: 4.5643\n",
      "Epoch [2/2], Step [36640/67476], Loss: 4.4381\n",
      "Epoch [2/2], Step [36650/67476], Loss: 4.5076\n",
      "Epoch [2/2], Step [36660/67476], Loss: 4.4307\n",
      "Epoch [2/2], Step [36670/67476], Loss: 4.5007\n",
      "Epoch [2/2], Step [36680/67476], Loss: 4.3773\n",
      "Epoch [2/2], Step [36690/67476], Loss: 4.5011\n",
      "Epoch [2/2], Step [36700/67476], Loss: 4.6734\n",
      "Epoch [2/2], Step [36710/67476], Loss: 4.7106\n",
      "Epoch [2/2], Step [36720/67476], Loss: 4.3862\n",
      "Epoch [2/2], Step [36730/67476], Loss: 4.6376\n",
      "Epoch [2/2], Step [36740/67476], Loss: 4.6248\n",
      "Epoch [2/2], Step [36750/67476], Loss: 4.5288\n",
      "Epoch [2/2], Step [36760/67476], Loss: 4.6156\n",
      "Epoch [2/2], Step [36770/67476], Loss: 4.5317\n",
      "Epoch [2/2], Step [36780/67476], Loss: 4.4749\n",
      "Epoch [2/2], Step [36790/67476], Loss: 4.6566\n",
      "Epoch [2/2], Step [36800/67476], Loss: 4.6418\n",
      "Epoch [2/2], Step [36810/67476], Loss: 4.5313\n",
      "Epoch [2/2], Step [36820/67476], Loss: 4.4614\n",
      "Epoch [2/2], Step [36830/67476], Loss: 4.4702\n",
      "Epoch [2/2], Step [36840/67476], Loss: 4.2481\n",
      "Epoch [2/2], Step [36850/67476], Loss: 4.5477\n",
      "Epoch [2/2], Step [36860/67476], Loss: 4.5638\n",
      "Epoch [2/2], Step [36870/67476], Loss: 4.6228\n",
      "Epoch [2/2], Step [36880/67476], Loss: 4.4367\n",
      "Epoch [2/2], Step [36890/67476], Loss: 4.6780\n",
      "Epoch [2/2], Step [36900/67476], Loss: 4.4918\n",
      "Epoch [2/2], Step [36910/67476], Loss: 4.4898\n",
      "Epoch [2/2], Step [36920/67476], Loss: 4.5558\n",
      "Epoch [2/2], Step [36930/67476], Loss: 4.6382\n",
      "Epoch [2/2], Step [36940/67476], Loss: 4.6314\n",
      "Epoch [2/2], Step [36950/67476], Loss: 4.5612\n",
      "Epoch [2/2], Step [36960/67476], Loss: 4.4573\n",
      "Epoch [2/2], Step [36970/67476], Loss: 4.5686\n",
      "Epoch [2/2], Step [36980/67476], Loss: 4.6383\n",
      "Epoch [2/2], Step [36990/67476], Loss: 4.3640\n",
      "Epoch [2/2], Step [37000/67476], Loss: 4.7165\n",
      "Epoch [2/2], Step [37010/67476], Loss: 4.4497\n",
      "Epoch [2/2], Step [37020/67476], Loss: 4.4927\n",
      "Epoch [2/2], Step [37030/67476], Loss: 4.6190\n",
      "Epoch [2/2], Step [37040/67476], Loss: 4.5511\n",
      "Epoch [2/2], Step [37050/67476], Loss: 4.4407\n",
      "Epoch [2/2], Step [37060/67476], Loss: 4.4731\n",
      "Epoch [2/2], Step [37070/67476], Loss: 4.5984\n",
      "Epoch [2/2], Step [37080/67476], Loss: 4.5635\n",
      "Epoch [2/2], Step [37090/67476], Loss: 4.4204\n",
      "Epoch [2/2], Step [37100/67476], Loss: 4.5495\n",
      "Epoch [2/2], Step [37110/67476], Loss: 4.5186\n",
      "Epoch [2/2], Step [37120/67476], Loss: 4.6059\n",
      "Epoch [2/2], Step [37130/67476], Loss: 4.5182\n",
      "Epoch [2/2], Step [37140/67476], Loss: 4.4127\n",
      "Epoch [2/2], Step [37150/67476], Loss: 4.6286\n",
      "Epoch [2/2], Step [37160/67476], Loss: 4.5977\n",
      "Epoch [2/2], Step [37170/67476], Loss: 4.5229\n",
      "Epoch [2/2], Step [37180/67476], Loss: 4.5117\n",
      "Epoch [2/2], Step [37190/67476], Loss: 4.6593\n",
      "Epoch [2/2], Step [37200/67476], Loss: 4.5597\n",
      "Epoch [2/2], Step [37210/67476], Loss: 4.6755\n",
      "Epoch [2/2], Step [37220/67476], Loss: 4.3676\n",
      "Epoch [2/2], Step [37230/67476], Loss: 4.6165\n",
      "Epoch [2/2], Step [37240/67476], Loss: 4.5772\n",
      "Epoch [2/2], Step [37250/67476], Loss: 4.5995\n",
      "Epoch [2/2], Step [37260/67476], Loss: 4.5787\n",
      "Epoch [2/2], Step [37270/67476], Loss: 4.3695\n",
      "Epoch [2/2], Step [37280/67476], Loss: 4.3665\n",
      "Epoch [2/2], Step [37290/67476], Loss: 4.5365\n",
      "Epoch [2/2], Step [37300/67476], Loss: 4.5510\n",
      "Epoch [2/2], Step [37310/67476], Loss: 4.4381\n",
      "Epoch [2/2], Step [37320/67476], Loss: 4.5489\n",
      "Epoch [2/2], Step [37330/67476], Loss: 4.5867\n",
      "Epoch [2/2], Step [37340/67476], Loss: 4.6430\n",
      "Epoch [2/2], Step [37350/67476], Loss: 4.5915\n",
      "Epoch [2/2], Step [37360/67476], Loss: 4.6180\n",
      "Epoch [2/2], Step [37370/67476], Loss: 4.5276\n",
      "Epoch [2/2], Step [37380/67476], Loss: 4.6227\n",
      "Epoch [2/2], Step [37390/67476], Loss: 4.6199\n",
      "Epoch [2/2], Step [37400/67476], Loss: 4.7290\n",
      "Epoch [2/2], Step [37410/67476], Loss: 4.5419\n",
      "Epoch [2/2], Step [37420/67476], Loss: 4.6568\n",
      "Epoch [2/2], Step [37430/67476], Loss: 4.5894\n",
      "Epoch [2/2], Step [37440/67476], Loss: 4.4678\n",
      "Epoch [2/2], Step [37450/67476], Loss: 4.4862\n",
      "Epoch [2/2], Step [37460/67476], Loss: 4.5122\n",
      "Epoch [2/2], Step [37470/67476], Loss: 4.6190\n",
      "Epoch [2/2], Step [37480/67476], Loss: 4.2807\n",
      "Epoch [2/2], Step [37490/67476], Loss: 4.7300\n",
      "Epoch [2/2], Step [37500/67476], Loss: 4.6662\n",
      "Epoch [2/2], Step [37510/67476], Loss: 4.4886\n",
      "Epoch [2/2], Step [37520/67476], Loss: 4.5297\n",
      "Epoch [2/2], Step [37530/67476], Loss: 4.5361\n",
      "Epoch [2/2], Step [37540/67476], Loss: 4.6853\n",
      "Epoch [2/2], Step [37550/67476], Loss: 4.5521\n",
      "Epoch [2/2], Step [37560/67476], Loss: 4.6209\n",
      "Epoch [2/2], Step [37570/67476], Loss: 4.6573\n",
      "Epoch [2/2], Step [37580/67476], Loss: 4.5192\n",
      "Epoch [2/2], Step [37590/67476], Loss: 4.6145\n",
      "Epoch [2/2], Step [37600/67476], Loss: 4.3918\n",
      "Epoch [2/2], Step [37610/67476], Loss: 4.5838\n",
      "Epoch [2/2], Step [37620/67476], Loss: 4.3543\n",
      "Epoch [2/2], Step [37630/67476], Loss: 4.3918\n",
      "Epoch [2/2], Step [37640/67476], Loss: 4.6012\n",
      "Epoch [2/2], Step [37650/67476], Loss: 4.6066\n",
      "Epoch [2/2], Step [37660/67476], Loss: 4.7024\n",
      "Epoch [2/2], Step [37670/67476], Loss: 4.6021\n",
      "Epoch [2/2], Step [37680/67476], Loss: 4.5780\n",
      "Epoch [2/2], Step [37690/67476], Loss: 4.6701\n",
      "Epoch [2/2], Step [37700/67476], Loss: 4.7505\n",
      "Epoch [2/2], Step [37710/67476], Loss: 4.3908\n",
      "Epoch [2/2], Step [37720/67476], Loss: 4.4670\n",
      "Epoch [2/2], Step [37730/67476], Loss: 4.4622\n",
      "Epoch [2/2], Step [37740/67476], Loss: 4.4756\n",
      "Epoch [2/2], Step [37750/67476], Loss: 4.6447\n",
      "Epoch [2/2], Step [37760/67476], Loss: 4.6900\n",
      "Epoch [2/2], Step [37770/67476], Loss: 4.4480\n",
      "Epoch [2/2], Step [37780/67476], Loss: 4.5368\n",
      "Epoch [2/2], Step [37790/67476], Loss: 4.5741\n",
      "Epoch [2/2], Step [37800/67476], Loss: 4.4197\n",
      "Epoch [2/2], Step [37810/67476], Loss: 4.6672\n",
      "Epoch [2/2], Step [37820/67476], Loss: 4.5436\n",
      "Epoch [2/2], Step [37830/67476], Loss: 4.4401\n",
      "Epoch [2/2], Step [37840/67476], Loss: 4.5547\n",
      "Epoch [2/2], Step [37850/67476], Loss: 4.5549\n",
      "Epoch [2/2], Step [37860/67476], Loss: 4.5859\n",
      "Epoch [2/2], Step [37870/67476], Loss: 4.6216\n",
      "Epoch [2/2], Step [37880/67476], Loss: 4.4484\n",
      "Epoch [2/2], Step [37890/67476], Loss: 4.4793\n",
      "Epoch [2/2], Step [37900/67476], Loss: 4.5159\n",
      "Epoch [2/2], Step [37910/67476], Loss: 4.6637\n",
      "Epoch [2/2], Step [37920/67476], Loss: 4.5150\n",
      "Epoch [2/2], Step [37930/67476], Loss: 4.2791\n",
      "Epoch [2/2], Step [37940/67476], Loss: 4.5900\n",
      "Epoch [2/2], Step [37950/67476], Loss: 4.4547\n",
      "Epoch [2/2], Step [37960/67476], Loss: 4.5544\n",
      "Epoch [2/2], Step [37970/67476], Loss: 4.4517\n",
      "Epoch [2/2], Step [37980/67476], Loss: 4.4209\n",
      "Epoch [2/2], Step [37990/67476], Loss: 4.6138\n",
      "Epoch [2/2], Step [38000/67476], Loss: 4.5773\n",
      "Epoch [2/2], Step [38010/67476], Loss: 4.5757\n",
      "Epoch [2/2], Step [38020/67476], Loss: 4.4997\n",
      "Epoch [2/2], Step [38030/67476], Loss: 4.4903\n",
      "Epoch [2/2], Step [38040/67476], Loss: 4.4861\n",
      "Epoch [2/2], Step [38050/67476], Loss: 4.7121\n",
      "Epoch [2/2], Step [38060/67476], Loss: 4.7032\n",
      "Epoch [2/2], Step [38070/67476], Loss: 4.5770\n",
      "Epoch [2/2], Step [38080/67476], Loss: 4.5330\n",
      "Epoch [2/2], Step [38090/67476], Loss: 4.6429\n",
      "Epoch [2/2], Step [38100/67476], Loss: 4.5966\n",
      "Epoch [2/2], Step [38110/67476], Loss: 4.8204\n",
      "Epoch [2/2], Step [38120/67476], Loss: 4.3451\n",
      "Epoch [2/2], Step [38130/67476], Loss: 4.6107\n",
      "Epoch [2/2], Step [38140/67476], Loss: 4.6347\n",
      "Epoch [2/2], Step [38150/67476], Loss: 4.3445\n",
      "Epoch [2/2], Step [38160/67476], Loss: 4.5552\n",
      "Epoch [2/2], Step [38170/67476], Loss: 4.4251\n",
      "Epoch [2/2], Step [38180/67476], Loss: 4.6830\n",
      "Epoch [2/2], Step [38190/67476], Loss: 4.6913\n",
      "Epoch [2/2], Step [38200/67476], Loss: 4.5115\n",
      "Epoch [2/2], Step [38210/67476], Loss: 4.5173\n",
      "Epoch [2/2], Step [38220/67476], Loss: 4.6271\n",
      "Epoch [2/2], Step [38230/67476], Loss: 4.6822\n",
      "Epoch [2/2], Step [38240/67476], Loss: 4.3395\n",
      "Epoch [2/2], Step [38250/67476], Loss: 4.4856\n",
      "Epoch [2/2], Step [38260/67476], Loss: 4.4190\n",
      "Epoch [2/2], Step [38270/67476], Loss: 4.3733\n",
      "Epoch [2/2], Step [38280/67476], Loss: 4.5743\n",
      "Epoch [2/2], Step [38290/67476], Loss: 4.5074\n",
      "Epoch [2/2], Step [38300/67476], Loss: 4.3270\n",
      "Epoch [2/2], Step [38310/67476], Loss: 4.8256\n",
      "Epoch [2/2], Step [38320/67476], Loss: 4.8478\n",
      "Epoch [2/2], Step [38330/67476], Loss: 4.6576\n",
      "Epoch [2/2], Step [38340/67476], Loss: 4.6404\n",
      "Epoch [2/2], Step [38350/67476], Loss: 4.3778\n",
      "Epoch [2/2], Step [38360/67476], Loss: 4.5613\n",
      "Epoch [2/2], Step [38370/67476], Loss: 4.5738\n",
      "Epoch [2/2], Step [38380/67476], Loss: 4.7879\n",
      "Epoch [2/2], Step [38390/67476], Loss: 4.5996\n",
      "Epoch [2/2], Step [38400/67476], Loss: 4.5323\n",
      "Epoch [2/2], Step [38410/67476], Loss: 4.4781\n",
      "Epoch [2/2], Step [38420/67476], Loss: 4.6461\n",
      "Epoch [2/2], Step [38430/67476], Loss: 4.6685\n",
      "Epoch [2/2], Step [38440/67476], Loss: 4.6130\n",
      "Epoch [2/2], Step [38450/67476], Loss: 4.6172\n",
      "Epoch [2/2], Step [38460/67476], Loss: 4.3668\n",
      "Epoch [2/2], Step [38470/67476], Loss: 4.4949\n",
      "Epoch [2/2], Step [38480/67476], Loss: 4.6127\n",
      "Epoch [2/2], Step [38490/67476], Loss: 4.6925\n",
      "Epoch [2/2], Step [38500/67476], Loss: 4.6066\n",
      "Epoch [2/2], Step [38510/67476], Loss: 4.3470\n",
      "Epoch [2/2], Step [38520/67476], Loss: 4.6141\n",
      "Epoch [2/2], Step [38530/67476], Loss: 4.4623\n",
      "Epoch [2/2], Step [38540/67476], Loss: 4.4846\n",
      "Epoch [2/2], Step [38550/67476], Loss: 4.5802\n",
      "Epoch [2/2], Step [38560/67476], Loss: 4.4974\n",
      "Epoch [2/2], Step [38570/67476], Loss: 4.6094\n",
      "Epoch [2/2], Step [38580/67476], Loss: 4.3126\n",
      "Epoch [2/2], Step [38590/67476], Loss: 4.5503\n",
      "Epoch [2/2], Step [38600/67476], Loss: 4.5202\n",
      "Epoch [2/2], Step [38610/67476], Loss: 4.6648\n",
      "Epoch [2/2], Step [38620/67476], Loss: 4.7395\n",
      "Epoch [2/2], Step [38630/67476], Loss: 4.5795\n",
      "Epoch [2/2], Step [38640/67476], Loss: 4.5206\n",
      "Epoch [2/2], Step [38650/67476], Loss: 4.5069\n",
      "Epoch [2/2], Step [38660/67476], Loss: 4.6642\n",
      "Epoch [2/2], Step [38670/67476], Loss: 4.5132\n",
      "Epoch [2/2], Step [38680/67476], Loss: 4.0924\n",
      "Epoch [2/2], Step [38690/67476], Loss: 4.5506\n",
      "Epoch [2/2], Step [38700/67476], Loss: 4.5744\n",
      "Epoch [2/2], Step [38710/67476], Loss: 4.6023\n",
      "Epoch [2/2], Step [38720/67476], Loss: 4.5862\n",
      "Epoch [2/2], Step [38730/67476], Loss: 4.6456\n",
      "Epoch [2/2], Step [38740/67476], Loss: 4.4068\n",
      "Epoch [2/2], Step [38750/67476], Loss: 4.5237\n",
      "Epoch [2/2], Step [38760/67476], Loss: 4.6546\n",
      "Epoch [2/2], Step [38770/67476], Loss: 4.5028\n",
      "Epoch [2/2], Step [38780/67476], Loss: 4.5165\n",
      "Epoch [2/2], Step [38790/67476], Loss: 4.5744\n",
      "Epoch [2/2], Step [38800/67476], Loss: 4.6920\n",
      "Epoch [2/2], Step [38810/67476], Loss: 4.5944\n",
      "Epoch [2/2], Step [38820/67476], Loss: 4.4307\n",
      "Epoch [2/2], Step [38830/67476], Loss: 4.5578\n",
      "Epoch [2/2], Step [38840/67476], Loss: 4.4779\n",
      "Epoch [2/2], Step [38850/67476], Loss: 4.5594\n",
      "Epoch [2/2], Step [38860/67476], Loss: 4.3917\n",
      "Epoch [2/2], Step [38870/67476], Loss: 4.5137\n",
      "Epoch [2/2], Step [38880/67476], Loss: 4.5843\n",
      "Epoch [2/2], Step [38890/67476], Loss: 4.4302\n",
      "Epoch [2/2], Step [38900/67476], Loss: 4.4113\n",
      "Epoch [2/2], Step [38910/67476], Loss: 4.5414\n",
      "Epoch [2/2], Step [38920/67476], Loss: 4.6108\n",
      "Epoch [2/2], Step [38930/67476], Loss: 4.6197\n",
      "Epoch [2/2], Step [38940/67476], Loss: 4.5510\n",
      "Epoch [2/2], Step [38950/67476], Loss: 4.3997\n",
      "Epoch [2/2], Step [38960/67476], Loss: 4.5909\n",
      "Epoch [2/2], Step [38970/67476], Loss: 4.6298\n",
      "Epoch [2/2], Step [38980/67476], Loss: 4.5790\n",
      "Epoch [2/2], Step [38990/67476], Loss: 4.6761\n",
      "Epoch [2/2], Step [39000/67476], Loss: 4.6874\n",
      "Epoch [2/2], Step [39010/67476], Loss: 4.7048\n",
      "Epoch [2/2], Step [39020/67476], Loss: 4.5723\n",
      "Epoch [2/2], Step [39030/67476], Loss: 4.4520\n",
      "Epoch [2/2], Step [39040/67476], Loss: 4.4967\n",
      "Epoch [2/2], Step [39050/67476], Loss: 4.5424\n",
      "Epoch [2/2], Step [39060/67476], Loss: 4.4370\n",
      "Epoch [2/2], Step [39070/67476], Loss: 4.6483\n",
      "Epoch [2/2], Step [39080/67476], Loss: 4.6206\n",
      "Epoch [2/2], Step [39090/67476], Loss: 4.5278\n",
      "Epoch [2/2], Step [39100/67476], Loss: 4.5258\n",
      "Epoch [2/2], Step [39110/67476], Loss: 4.4361\n",
      "Epoch [2/2], Step [39120/67476], Loss: 4.8042\n",
      "Epoch [2/2], Step [39130/67476], Loss: 4.6453\n",
      "Epoch [2/2], Step [39140/67476], Loss: 4.6671\n",
      "Epoch [2/2], Step [39150/67476], Loss: 4.4224\n",
      "Epoch [2/2], Step [39160/67476], Loss: 4.6079\n",
      "Epoch [2/2], Step [39170/67476], Loss: 4.5300\n",
      "Epoch [2/2], Step [39180/67476], Loss: 4.4933\n",
      "Epoch [2/2], Step [39190/67476], Loss: 4.6877\n",
      "Epoch [2/2], Step [39200/67476], Loss: 4.7344\n",
      "Epoch [2/2], Step [39210/67476], Loss: 4.4557\n",
      "Epoch [2/2], Step [39220/67476], Loss: 4.5635\n",
      "Epoch [2/2], Step [39230/67476], Loss: 4.5174\n",
      "Epoch [2/2], Step [39240/67476], Loss: 4.4634\n",
      "Epoch [2/2], Step [39250/67476], Loss: 4.5653\n",
      "Epoch [2/2], Step [39260/67476], Loss: 4.5326\n",
      "Epoch [2/2], Step [39270/67476], Loss: 4.5164\n",
      "Epoch [2/2], Step [39280/67476], Loss: 4.5294\n",
      "Epoch [2/2], Step [39290/67476], Loss: 4.5267\n",
      "Epoch [2/2], Step [39300/67476], Loss: 4.5204\n",
      "Epoch [2/2], Step [39310/67476], Loss: 4.5033\n",
      "Epoch [2/2], Step [39320/67476], Loss: 4.6046\n",
      "Epoch [2/2], Step [39330/67476], Loss: 4.7626\n",
      "Epoch [2/2], Step [39340/67476], Loss: 4.4633\n",
      "Epoch [2/2], Step [39350/67476], Loss: 4.5985\n",
      "Epoch [2/2], Step [39360/67476], Loss: 4.6773\n",
      "Epoch [2/2], Step [39370/67476], Loss: 4.7783\n",
      "Epoch [2/2], Step [39380/67476], Loss: 4.4949\n",
      "Epoch [2/2], Step [39390/67476], Loss: 4.5776\n",
      "Epoch [2/2], Step [39400/67476], Loss: 4.6779\n",
      "Epoch [2/2], Step [39410/67476], Loss: 4.7604\n",
      "Epoch [2/2], Step [39420/67476], Loss: 4.5100\n",
      "Epoch [2/2], Step [39430/67476], Loss: 4.5547\n",
      "Epoch [2/2], Step [39440/67476], Loss: 4.3395\n",
      "Epoch [2/2], Step [39450/67476], Loss: 4.7425\n",
      "Epoch [2/2], Step [39460/67476], Loss: 4.6331\n",
      "Epoch [2/2], Step [39470/67476], Loss: 4.4592\n",
      "Epoch [2/2], Step [39480/67476], Loss: 4.6333\n",
      "Epoch [2/2], Step [39490/67476], Loss: 4.5897\n",
      "Epoch [2/2], Step [39500/67476], Loss: 4.6717\n",
      "Epoch [2/2], Step [39510/67476], Loss: 4.4705\n",
      "Epoch [2/2], Step [39520/67476], Loss: 4.5526\n",
      "Epoch [2/2], Step [39530/67476], Loss: 4.5108\n",
      "Epoch [2/2], Step [39540/67476], Loss: 4.4905\n",
      "Epoch [2/2], Step [39550/67476], Loss: 4.5099\n",
      "Epoch [2/2], Step [39560/67476], Loss: 4.4254\n",
      "Epoch [2/2], Step [39570/67476], Loss: 4.4554\n",
      "Epoch [2/2], Step [39580/67476], Loss: 4.5133\n",
      "Epoch [2/2], Step [39590/67476], Loss: 4.4525\n",
      "Epoch [2/2], Step [39600/67476], Loss: 4.3706\n",
      "Epoch [2/2], Step [39610/67476], Loss: 4.6722\n",
      "Epoch [2/2], Step [39620/67476], Loss: 4.5495\n",
      "Epoch [2/2], Step [39630/67476], Loss: 4.5205\n",
      "Epoch [2/2], Step [39640/67476], Loss: 4.6076\n",
      "Epoch [2/2], Step [39650/67476], Loss: 4.4762\n",
      "Epoch [2/2], Step [39660/67476], Loss: 4.6581\n",
      "Epoch [2/2], Step [39670/67476], Loss: 4.7966\n",
      "Epoch [2/2], Step [39680/67476], Loss: 4.4706\n",
      "Epoch [2/2], Step [39690/67476], Loss: 4.4769\n",
      "Epoch [2/2], Step [39700/67476], Loss: 4.6276\n",
      "Epoch [2/2], Step [39710/67476], Loss: 4.6269\n",
      "Epoch [2/2], Step [39720/67476], Loss: 4.6276\n",
      "Epoch [2/2], Step [39730/67476], Loss: 4.5853\n",
      "Epoch [2/2], Step [39740/67476], Loss: 4.6504\n",
      "Epoch [2/2], Step [39750/67476], Loss: 4.5601\n",
      "Epoch [2/2], Step [39760/67476], Loss: 4.6663\n",
      "Epoch [2/2], Step [39770/67476], Loss: 4.5585\n",
      "Epoch [2/2], Step [39780/67476], Loss: 4.7526\n",
      "Epoch [2/2], Step [39790/67476], Loss: 4.5623\n",
      "Epoch [2/2], Step [39800/67476], Loss: 4.6360\n",
      "Epoch [2/2], Step [39810/67476], Loss: 4.5152\n",
      "Epoch [2/2], Step [39820/67476], Loss: 4.7603\n",
      "Epoch [2/2], Step [39830/67476], Loss: 4.5162\n",
      "Epoch [2/2], Step [39840/67476], Loss: 4.6688\n",
      "Epoch [2/2], Step [39850/67476], Loss: 4.3631\n",
      "Epoch [2/2], Step [39860/67476], Loss: 4.5082\n",
      "Epoch [2/2], Step [39870/67476], Loss: 4.5987\n",
      "Epoch [2/2], Step [39880/67476], Loss: 4.4005\n",
      "Epoch [2/2], Step [39890/67476], Loss: 4.4101\n",
      "Epoch [2/2], Step [39900/67476], Loss: 4.5635\n",
      "Epoch [2/2], Step [39910/67476], Loss: 4.5104\n",
      "Epoch [2/2], Step [39920/67476], Loss: 4.6149\n",
      "Epoch [2/2], Step [39930/67476], Loss: 4.5931\n",
      "Epoch [2/2], Step [39940/67476], Loss: 4.5465\n",
      "Epoch [2/2], Step [39950/67476], Loss: 4.4660\n",
      "Epoch [2/2], Step [39960/67476], Loss: 4.6530\n",
      "Epoch [2/2], Step [39970/67476], Loss: 4.6243\n",
      "Epoch [2/2], Step [39980/67476], Loss: 4.3859\n",
      "Epoch [2/2], Step [39990/67476], Loss: 4.6431\n",
      "Epoch [2/2], Step [40000/67476], Loss: 4.6373\n",
      "Epoch [2/2], Step [40010/67476], Loss: 4.5398\n",
      "Epoch [2/2], Step [40020/67476], Loss: 4.4570\n",
      "Epoch [2/2], Step [40030/67476], Loss: 4.5003\n",
      "Epoch [2/2], Step [40040/67476], Loss: 4.4917\n",
      "Epoch [2/2], Step [40050/67476], Loss: 4.6175\n",
      "Epoch [2/2], Step [40060/67476], Loss: 4.4398\n",
      "Epoch [2/2], Step [40070/67476], Loss: 4.7477\n",
      "Epoch [2/2], Step [40080/67476], Loss: 4.4861\n",
      "Epoch [2/2], Step [40090/67476], Loss: 4.4425\n",
      "Epoch [2/2], Step [40100/67476], Loss: 4.4814\n",
      "Epoch [2/2], Step [40110/67476], Loss: 4.6062\n",
      "Epoch [2/2], Step [40120/67476], Loss: 4.6523\n",
      "Epoch [2/2], Step [40130/67476], Loss: 4.5197\n",
      "Epoch [2/2], Step [40140/67476], Loss: 4.6201\n",
      "Epoch [2/2], Step [40150/67476], Loss: 4.5002\n",
      "Epoch [2/2], Step [40160/67476], Loss: 4.5361\n",
      "Epoch [2/2], Step [40170/67476], Loss: 4.5232\n",
      "Epoch [2/2], Step [40180/67476], Loss: 4.5780\n",
      "Epoch [2/2], Step [40190/67476], Loss: 4.4806\n",
      "Epoch [2/2], Step [40200/67476], Loss: 4.5702\n",
      "Epoch [2/2], Step [40210/67476], Loss: 4.5294\n",
      "Epoch [2/2], Step [40220/67476], Loss: 4.8359\n",
      "Epoch [2/2], Step [40230/67476], Loss: 4.5181\n",
      "Epoch [2/2], Step [40240/67476], Loss: 4.7690\n",
      "Epoch [2/2], Step [40250/67476], Loss: 4.5371\n",
      "Epoch [2/2], Step [40260/67476], Loss: 4.4483\n",
      "Epoch [2/2], Step [40270/67476], Loss: 4.6093\n",
      "Epoch [2/2], Step [40280/67476], Loss: 4.4434\n",
      "Epoch [2/2], Step [40290/67476], Loss: 4.6592\n",
      "Epoch [2/2], Step [40300/67476], Loss: 4.5664\n",
      "Epoch [2/2], Step [40310/67476], Loss: 4.2859\n",
      "Epoch [2/2], Step [40320/67476], Loss: 4.4163\n",
      "Epoch [2/2], Step [40330/67476], Loss: 4.6178\n",
      "Epoch [2/2], Step [40340/67476], Loss: 4.5465\n",
      "Epoch [2/2], Step [40350/67476], Loss: 4.5009\n",
      "Epoch [2/2], Step [40360/67476], Loss: 4.6715\n",
      "Epoch [2/2], Step [40370/67476], Loss: 4.6756\n",
      "Epoch [2/2], Step [40380/67476], Loss: 4.6149\n",
      "Epoch [2/2], Step [40390/67476], Loss: 4.5552\n",
      "Epoch [2/2], Step [40400/67476], Loss: 4.5162\n",
      "Epoch [2/2], Step [40410/67476], Loss: 4.5308\n",
      "Epoch [2/2], Step [40420/67476], Loss: 4.4303\n",
      "Epoch [2/2], Step [40430/67476], Loss: 4.6026\n",
      "Epoch [2/2], Step [40440/67476], Loss: 4.5964\n",
      "Epoch [2/2], Step [40450/67476], Loss: 4.5885\n",
      "Epoch [2/2], Step [40460/67476], Loss: 4.4319\n",
      "Epoch [2/2], Step [40470/67476], Loss: 4.4680\n",
      "Epoch [2/2], Step [40480/67476], Loss: 4.6153\n",
      "Epoch [2/2], Step [40490/67476], Loss: 4.6977\n",
      "Epoch [2/2], Step [40500/67476], Loss: 4.5227\n",
      "Epoch [2/2], Step [40510/67476], Loss: 4.5965\n",
      "Epoch [2/2], Step [40520/67476], Loss: 4.6285\n",
      "Epoch [2/2], Step [40530/67476], Loss: 4.6238\n",
      "Epoch [2/2], Step [40540/67476], Loss: 4.4648\n",
      "Epoch [2/2], Step [40550/67476], Loss: 4.5607\n",
      "Epoch [2/2], Step [40560/67476], Loss: 4.6160\n",
      "Epoch [2/2], Step [40570/67476], Loss: 4.4648\n",
      "Epoch [2/2], Step [40580/67476], Loss: 4.6239\n",
      "Epoch [2/2], Step [40590/67476], Loss: 4.3821\n",
      "Epoch [2/2], Step [40600/67476], Loss: 4.5223\n",
      "Epoch [2/2], Step [40610/67476], Loss: 4.3947\n",
      "Epoch [2/2], Step [40620/67476], Loss: 4.8223\n",
      "Epoch [2/2], Step [40630/67476], Loss: 4.6093\n",
      "Epoch [2/2], Step [40640/67476], Loss: 4.5366\n",
      "Epoch [2/2], Step [40650/67476], Loss: 4.6538\n",
      "Epoch [2/2], Step [40660/67476], Loss: 4.6915\n",
      "Epoch [2/2], Step [40670/67476], Loss: 4.4570\n",
      "Epoch [2/2], Step [40680/67476], Loss: 4.7495\n",
      "Epoch [2/2], Step [40690/67476], Loss: 4.5015\n",
      "Epoch [2/2], Step [40700/67476], Loss: 4.4712\n",
      "Epoch [2/2], Step [40710/67476], Loss: 4.5600\n",
      "Epoch [2/2], Step [40720/67476], Loss: 4.4772\n",
      "Epoch [2/2], Step [40730/67476], Loss: 4.6275\n",
      "Epoch [2/2], Step [40740/67476], Loss: 4.3817\n",
      "Epoch [2/2], Step [40750/67476], Loss: 4.4969\n",
      "Epoch [2/2], Step [40760/67476], Loss: 4.5040\n",
      "Epoch [2/2], Step [40770/67476], Loss: 4.6677\n",
      "Epoch [2/2], Step [40780/67476], Loss: 4.2717\n",
      "Epoch [2/2], Step [40790/67476], Loss: 4.4527\n",
      "Epoch [2/2], Step [40800/67476], Loss: 4.6521\n",
      "Epoch [2/2], Step [40810/67476], Loss: 4.5186\n",
      "Epoch [2/2], Step [40820/67476], Loss: 4.5106\n",
      "Epoch [2/2], Step [40830/67476], Loss: 4.5715\n",
      "Epoch [2/2], Step [40840/67476], Loss: 4.5301\n",
      "Epoch [2/2], Step [40850/67476], Loss: 4.7998\n",
      "Epoch [2/2], Step [40860/67476], Loss: 4.4157\n",
      "Epoch [2/2], Step [40870/67476], Loss: 4.5923\n",
      "Epoch [2/2], Step [40880/67476], Loss: 4.7212\n",
      "Epoch [2/2], Step [40890/67476], Loss: 4.6290\n",
      "Epoch [2/2], Step [40900/67476], Loss: 4.6730\n",
      "Epoch [2/2], Step [40910/67476], Loss: 4.5817\n",
      "Epoch [2/2], Step [40920/67476], Loss: 4.5444\n",
      "Epoch [2/2], Step [40930/67476], Loss: 4.4994\n",
      "Epoch [2/2], Step [40940/67476], Loss: 4.6420\n",
      "Epoch [2/2], Step [40950/67476], Loss: 4.4634\n",
      "Epoch [2/2], Step [40960/67476], Loss: 4.5648\n",
      "Epoch [2/2], Step [40970/67476], Loss: 4.6206\n",
      "Epoch [2/2], Step [40980/67476], Loss: 4.5759\n",
      "Epoch [2/2], Step [40990/67476], Loss: 4.5461\n",
      "Epoch [2/2], Step [41000/67476], Loss: 4.6243\n",
      "Epoch [2/2], Step [41010/67476], Loss: 4.4748\n",
      "Epoch [2/2], Step [41020/67476], Loss: 4.7430\n",
      "Epoch [2/2], Step [41030/67476], Loss: 4.7503\n",
      "Epoch [2/2], Step [41040/67476], Loss: 4.4461\n",
      "Epoch [2/2], Step [41050/67476], Loss: 4.4523\n",
      "Epoch [2/2], Step [41060/67476], Loss: 4.4789\n",
      "Epoch [2/2], Step [41070/67476], Loss: 4.4689\n",
      "Epoch [2/2], Step [41080/67476], Loss: 4.4353\n",
      "Epoch [2/2], Step [41090/67476], Loss: 4.4950\n",
      "Epoch [2/2], Step [41100/67476], Loss: 4.4436\n",
      "Epoch [2/2], Step [41110/67476], Loss: 4.6204\n",
      "Epoch [2/2], Step [41120/67476], Loss: 4.8183\n",
      "Epoch [2/2], Step [41130/67476], Loss: 4.6112\n",
      "Epoch [2/2], Step [41140/67476], Loss: 4.6805\n",
      "Epoch [2/2], Step [41150/67476], Loss: 4.6568\n",
      "Epoch [2/2], Step [41160/67476], Loss: 4.7080\n",
      "Epoch [2/2], Step [41170/67476], Loss: 4.5458\n",
      "Epoch [2/2], Step [41180/67476], Loss: 4.5930\n",
      "Epoch [2/2], Step [41190/67476], Loss: 4.4172\n",
      "Epoch [2/2], Step [41200/67476], Loss: 4.5242\n",
      "Epoch [2/2], Step [41210/67476], Loss: 4.5301\n",
      "Epoch [2/2], Step [41220/67476], Loss: 4.5955\n",
      "Epoch [2/2], Step [41230/67476], Loss: 4.5478\n",
      "Epoch [2/2], Step [41240/67476], Loss: 4.6326\n",
      "Epoch [2/2], Step [41250/67476], Loss: 4.3893\n",
      "Epoch [2/2], Step [41260/67476], Loss: 4.3317\n",
      "Epoch [2/2], Step [41270/67476], Loss: 4.4944\n",
      "Epoch [2/2], Step [41280/67476], Loss: 4.5095\n",
      "Epoch [2/2], Step [41290/67476], Loss: 4.5796\n",
      "Epoch [2/2], Step [41300/67476], Loss: 4.4634\n",
      "Epoch [2/2], Step [41310/67476], Loss: 4.5148\n",
      "Epoch [2/2], Step [41320/67476], Loss: 4.5767\n",
      "Epoch [2/2], Step [41330/67476], Loss: 4.6031\n",
      "Epoch [2/2], Step [41340/67476], Loss: 4.4699\n",
      "Epoch [2/2], Step [41350/67476], Loss: 4.5253\n",
      "Epoch [2/2], Step [41360/67476], Loss: 4.4503\n",
      "Epoch [2/2], Step [41370/67476], Loss: 4.7252\n",
      "Epoch [2/2], Step [41380/67476], Loss: 4.6446\n",
      "Epoch [2/2], Step [41390/67476], Loss: 4.5728\n",
      "Epoch [2/2], Step [41400/67476], Loss: 4.5709\n",
      "Epoch [2/2], Step [41410/67476], Loss: 4.6249\n",
      "Epoch [2/2], Step [41420/67476], Loss: 4.4978\n",
      "Epoch [2/2], Step [41430/67476], Loss: 4.2643\n",
      "Epoch [2/2], Step [41440/67476], Loss: 4.5694\n",
      "Epoch [2/2], Step [41450/67476], Loss: 4.5921\n",
      "Epoch [2/2], Step [41460/67476], Loss: 4.5755\n",
      "Epoch [2/2], Step [41470/67476], Loss: 4.6280\n",
      "Epoch [2/2], Step [41480/67476], Loss: 4.5240\n",
      "Epoch [2/2], Step [41490/67476], Loss: 4.5772\n",
      "Epoch [2/2], Step [41500/67476], Loss: 4.4668\n",
      "Epoch [2/2], Step [41510/67476], Loss: 4.5011\n",
      "Epoch [2/2], Step [41520/67476], Loss: 4.6780\n",
      "Epoch [2/2], Step [41530/67476], Loss: 4.6550\n",
      "Epoch [2/2], Step [41540/67476], Loss: 4.3743\n",
      "Epoch [2/2], Step [41550/67476], Loss: 4.6736\n",
      "Epoch [2/2], Step [41560/67476], Loss: 4.4497\n",
      "Epoch [2/2], Step [41570/67476], Loss: 4.5512\n",
      "Epoch [2/2], Step [41580/67476], Loss: 4.6315\n",
      "Epoch [2/2], Step [41590/67476], Loss: 4.3146\n",
      "Epoch [2/2], Step [41600/67476], Loss: 4.4419\n",
      "Epoch [2/2], Step [41610/67476], Loss: 4.5554\n",
      "Epoch [2/2], Step [41620/67476], Loss: 4.4667\n",
      "Epoch [2/2], Step [41630/67476], Loss: 4.5379\n",
      "Epoch [2/2], Step [41640/67476], Loss: 4.5841\n",
      "Epoch [2/2], Step [41650/67476], Loss: 4.4434\n",
      "Epoch [2/2], Step [41660/67476], Loss: 4.4297\n",
      "Epoch [2/2], Step [41670/67476], Loss: 4.6139\n",
      "Epoch [2/2], Step [41680/67476], Loss: 4.5046\n",
      "Epoch [2/2], Step [41690/67476], Loss: 4.6493\n",
      "Epoch [2/2], Step [41700/67476], Loss: 4.4425\n",
      "Epoch [2/2], Step [41710/67476], Loss: 4.5545\n",
      "Epoch [2/2], Step [41720/67476], Loss: 4.5334\n",
      "Epoch [2/2], Step [41730/67476], Loss: 4.4713\n",
      "Epoch [2/2], Step [41740/67476], Loss: 4.6133\n",
      "Epoch [2/2], Step [41750/67476], Loss: 4.5754\n",
      "Epoch [2/2], Step [41760/67476], Loss: 4.5157\n",
      "Epoch [2/2], Step [41770/67476], Loss: 4.5482\n",
      "Epoch [2/2], Step [41780/67476], Loss: 4.6398\n",
      "Epoch [2/2], Step [41790/67476], Loss: 4.4443\n",
      "Epoch [2/2], Step [41800/67476], Loss: 4.7368\n",
      "Epoch [2/2], Step [41810/67476], Loss: 4.3571\n",
      "Epoch [2/2], Step [41820/67476], Loss: 4.4519\n",
      "Epoch [2/2], Step [41830/67476], Loss: 4.4667\n",
      "Epoch [2/2], Step [41840/67476], Loss: 4.6304\n",
      "Epoch [2/2], Step [41850/67476], Loss: 4.6338\n",
      "Epoch [2/2], Step [41860/67476], Loss: 4.5009\n",
      "Epoch [2/2], Step [41870/67476], Loss: 4.6125\n",
      "Epoch [2/2], Step [41880/67476], Loss: 4.5570\n",
      "Epoch [2/2], Step [41890/67476], Loss: 4.5574\n",
      "Epoch [2/2], Step [41900/67476], Loss: 4.5437\n",
      "Epoch [2/2], Step [41910/67476], Loss: 4.7750\n",
      "Epoch [2/2], Step [41920/67476], Loss: 4.6682\n",
      "Epoch [2/2], Step [41930/67476], Loss: 4.3812\n",
      "Epoch [2/2], Step [41940/67476], Loss: 4.5669\n",
      "Epoch [2/2], Step [41950/67476], Loss: 4.4488\n",
      "Epoch [2/2], Step [41960/67476], Loss: 4.6658\n",
      "Epoch [2/2], Step [41970/67476], Loss: 4.4186\n",
      "Epoch [2/2], Step [41980/67476], Loss: 4.3747\n",
      "Epoch [2/2], Step [41990/67476], Loss: 4.6792\n",
      "Epoch [2/2], Step [42000/67476], Loss: 4.5435\n",
      "Epoch [2/2], Step [42010/67476], Loss: 4.4027\n",
      "Epoch [2/2], Step [42020/67476], Loss: 4.4324\n",
      "Epoch [2/2], Step [42030/67476], Loss: 4.4405\n",
      "Epoch [2/2], Step [42040/67476], Loss: 4.5949\n",
      "Epoch [2/2], Step [42050/67476], Loss: 4.5325\n",
      "Epoch [2/2], Step [42060/67476], Loss: 4.4482\n",
      "Epoch [2/2], Step [42070/67476], Loss: 4.5011\n",
      "Epoch [2/2], Step [42080/67476], Loss: 4.5729\n",
      "Epoch [2/2], Step [42090/67476], Loss: 4.3990\n",
      "Epoch [2/2], Step [42100/67476], Loss: 4.4685\n",
      "Epoch [2/2], Step [42110/67476], Loss: 4.5903\n",
      "Epoch [2/2], Step [42120/67476], Loss: 4.5581\n",
      "Epoch [2/2], Step [42130/67476], Loss: 4.5009\n",
      "Epoch [2/2], Step [42140/67476], Loss: 4.6664\n",
      "Epoch [2/2], Step [42150/67476], Loss: 4.6117\n",
      "Epoch [2/2], Step [42160/67476], Loss: 4.4536\n",
      "Epoch [2/2], Step [42170/67476], Loss: 4.6367\n",
      "Epoch [2/2], Step [42180/67476], Loss: 4.7277\n",
      "Epoch [2/2], Step [42190/67476], Loss: 4.4757\n",
      "Epoch [2/2], Step [42200/67476], Loss: 4.5524\n",
      "Epoch [2/2], Step [42210/67476], Loss: 4.5726\n",
      "Epoch [2/2], Step [42220/67476], Loss: 4.4139\n",
      "Epoch [2/2], Step [42230/67476], Loss: 4.5865\n",
      "Epoch [2/2], Step [42240/67476], Loss: 4.5682\n",
      "Epoch [2/2], Step [42250/67476], Loss: 4.6303\n",
      "Epoch [2/2], Step [42260/67476], Loss: 4.6618\n",
      "Epoch [2/2], Step [42270/67476], Loss: 4.4059\n",
      "Epoch [2/2], Step [42280/67476], Loss: 4.5785\n",
      "Epoch [2/2], Step [42290/67476], Loss: 4.4588\n",
      "Epoch [2/2], Step [42300/67476], Loss: 4.4702\n",
      "Epoch [2/2], Step [42310/67476], Loss: 4.5744\n",
      "Epoch [2/2], Step [42320/67476], Loss: 4.4336\n",
      "Epoch [2/2], Step [42330/67476], Loss: 4.5362\n",
      "Epoch [2/2], Step [42340/67476], Loss: 4.5758\n",
      "Epoch [2/2], Step [42350/67476], Loss: 4.4156\n",
      "Epoch [2/2], Step [42360/67476], Loss: 4.6129\n",
      "Epoch [2/2], Step [42370/67476], Loss: 4.5845\n",
      "Epoch [2/2], Step [42380/67476], Loss: 4.4315\n",
      "Epoch [2/2], Step [42390/67476], Loss: 4.5879\n",
      "Epoch [2/2], Step [42400/67476], Loss: 4.6470\n",
      "Epoch [2/2], Step [42410/67476], Loss: 4.3794\n",
      "Epoch [2/2], Step [42420/67476], Loss: 4.4351\n",
      "Epoch [2/2], Step [42430/67476], Loss: 4.5387\n",
      "Epoch [2/2], Step [42440/67476], Loss: 4.5754\n",
      "Epoch [2/2], Step [42450/67476], Loss: 4.5394\n",
      "Epoch [2/2], Step [42460/67476], Loss: 4.6619\n",
      "Epoch [2/2], Step [42470/67476], Loss: 4.4570\n",
      "Epoch [2/2], Step [42480/67476], Loss: 4.2535\n",
      "Epoch [2/2], Step [42490/67476], Loss: 4.5520\n",
      "Epoch [2/2], Step [42500/67476], Loss: 4.4877\n",
      "Epoch [2/2], Step [42510/67476], Loss: 4.5474\n",
      "Epoch [2/2], Step [42520/67476], Loss: 4.4188\n",
      "Epoch [2/2], Step [42530/67476], Loss: 4.4917\n",
      "Epoch [2/2], Step [42540/67476], Loss: 4.6415\n",
      "Epoch [2/2], Step [42550/67476], Loss: 4.5413\n",
      "Epoch [2/2], Step [42560/67476], Loss: 4.6368\n",
      "Epoch [2/2], Step [42570/67476], Loss: 4.3959\n",
      "Epoch [2/2], Step [42580/67476], Loss: 4.4414\n",
      "Epoch [2/2], Step [42590/67476], Loss: 4.6030\n",
      "Epoch [2/2], Step [42600/67476], Loss: 4.5062\n",
      "Epoch [2/2], Step [42610/67476], Loss: 4.6306\n",
      "Epoch [2/2], Step [42620/67476], Loss: 4.4637\n",
      "Epoch [2/2], Step [42630/67476], Loss: 4.6444\n",
      "Epoch [2/2], Step [42640/67476], Loss: 4.8558\n",
      "Epoch [2/2], Step [42650/67476], Loss: 4.5758\n",
      "Epoch [2/2], Step [42660/67476], Loss: 4.6547\n",
      "Epoch [2/2], Step [42670/67476], Loss: 4.5655\n",
      "Epoch [2/2], Step [42680/67476], Loss: 4.6236\n",
      "Epoch [2/2], Step [42690/67476], Loss: 4.6331\n",
      "Epoch [2/2], Step [42700/67476], Loss: 4.4324\n",
      "Epoch [2/2], Step [42710/67476], Loss: 4.4603\n",
      "Epoch [2/2], Step [42720/67476], Loss: 4.4813\n",
      "Epoch [2/2], Step [42730/67476], Loss: 4.6857\n",
      "Epoch [2/2], Step [42740/67476], Loss: 4.5784\n",
      "Epoch [2/2], Step [42750/67476], Loss: 4.5910\n",
      "Epoch [2/2], Step [42760/67476], Loss: 4.5831\n",
      "Epoch [2/2], Step [42770/67476], Loss: 4.3597\n",
      "Epoch [2/2], Step [42780/67476], Loss: 4.4400\n",
      "Epoch [2/2], Step [42790/67476], Loss: 4.5277\n",
      "Epoch [2/2], Step [42800/67476], Loss: 4.5345\n",
      "Epoch [2/2], Step [42810/67476], Loss: 4.5322\n",
      "Epoch [2/2], Step [42820/67476], Loss: 4.5523\n",
      "Epoch [2/2], Step [42830/67476], Loss: 4.5072\n",
      "Epoch [2/2], Step [42840/67476], Loss: 4.5071\n",
      "Epoch [2/2], Step [42850/67476], Loss: 4.5634\n",
      "Epoch [2/2], Step [42860/67476], Loss: 4.4133\n",
      "Epoch [2/2], Step [42870/67476], Loss: 4.6000\n",
      "Epoch [2/2], Step [42880/67476], Loss: 4.4565\n",
      "Epoch [2/2], Step [42890/67476], Loss: 4.5764\n",
      "Epoch [2/2], Step [42900/67476], Loss: 4.4244\n",
      "Epoch [2/2], Step [42910/67476], Loss: 4.6020\n",
      "Epoch [2/2], Step [42920/67476], Loss: 4.7472\n",
      "Epoch [2/2], Step [42930/67476], Loss: 4.5545\n",
      "Epoch [2/2], Step [42940/67476], Loss: 4.5552\n",
      "Epoch [2/2], Step [42950/67476], Loss: 4.5576\n",
      "Epoch [2/2], Step [42960/67476], Loss: 4.4690\n",
      "Epoch [2/2], Step [42970/67476], Loss: 4.5862\n",
      "Epoch [2/2], Step [42980/67476], Loss: 4.5447\n",
      "Epoch [2/2], Step [42990/67476], Loss: 4.7221\n",
      "Epoch [2/2], Step [43000/67476], Loss: 4.4431\n",
      "Epoch [2/2], Step [43010/67476], Loss: 4.4524\n",
      "Epoch [2/2], Step [43020/67476], Loss: 4.4157\n",
      "Epoch [2/2], Step [43030/67476], Loss: 4.5022\n",
      "Epoch [2/2], Step [43040/67476], Loss: 4.4677\n",
      "Epoch [2/2], Step [43050/67476], Loss: 4.7705\n",
      "Epoch [2/2], Step [43060/67476], Loss: 4.3740\n",
      "Epoch [2/2], Step [43070/67476], Loss: 4.4129\n",
      "Epoch [2/2], Step [43080/67476], Loss: 4.5127\n",
      "Epoch [2/2], Step [43090/67476], Loss: 4.5117\n",
      "Epoch [2/2], Step [43100/67476], Loss: 4.5858\n",
      "Epoch [2/2], Step [43110/67476], Loss: 4.4889\n",
      "Epoch [2/2], Step [43120/67476], Loss: 4.6808\n",
      "Epoch [2/2], Step [43130/67476], Loss: 4.4663\n",
      "Epoch [2/2], Step [43140/67476], Loss: 4.6333\n",
      "Epoch [2/2], Step [43150/67476], Loss: 4.5025\n",
      "Epoch [2/2], Step [43160/67476], Loss: 4.4271\n",
      "Epoch [2/2], Step [43170/67476], Loss: 4.3660\n",
      "Epoch [2/2], Step [43180/67476], Loss: 4.4781\n",
      "Epoch [2/2], Step [43190/67476], Loss: 4.4568\n",
      "Epoch [2/2], Step [43200/67476], Loss: 4.5079\n",
      "Epoch [2/2], Step [43210/67476], Loss: 4.7807\n",
      "Epoch [2/2], Step [43220/67476], Loss: 4.4816\n",
      "Epoch [2/2], Step [43230/67476], Loss: 4.4087\n",
      "Epoch [2/2], Step [43240/67476], Loss: 4.5470\n",
      "Epoch [2/2], Step [43250/67476], Loss: 4.5373\n",
      "Epoch [2/2], Step [43260/67476], Loss: 4.5440\n",
      "Epoch [2/2], Step [43270/67476], Loss: 4.4064\n",
      "Epoch [2/2], Step [43280/67476], Loss: 4.6866\n",
      "Epoch [2/2], Step [43290/67476], Loss: 4.4594\n",
      "Epoch [2/2], Step [43300/67476], Loss: 4.6039\n",
      "Epoch [2/2], Step [43310/67476], Loss: 4.5715\n",
      "Epoch [2/2], Step [43320/67476], Loss: 4.6728\n",
      "Epoch [2/2], Step [43330/67476], Loss: 4.5189\n",
      "Epoch [2/2], Step [43340/67476], Loss: 4.3948\n",
      "Epoch [2/2], Step [43350/67476], Loss: 4.4512\n",
      "Epoch [2/2], Step [43360/67476], Loss: 4.3111\n",
      "Epoch [2/2], Step [43370/67476], Loss: 4.5334\n",
      "Epoch [2/2], Step [43380/67476], Loss: 4.5676\n",
      "Epoch [2/2], Step [43390/67476], Loss: 4.5176\n",
      "Epoch [2/2], Step [43400/67476], Loss: 4.5629\n",
      "Epoch [2/2], Step [43410/67476], Loss: 4.6997\n",
      "Epoch [2/2], Step [43420/67476], Loss: 4.6150\n",
      "Epoch [2/2], Step [43430/67476], Loss: 4.5082\n",
      "Epoch [2/2], Step [43440/67476], Loss: 4.4938\n",
      "Epoch [2/2], Step [43450/67476], Loss: 4.5726\n",
      "Epoch [2/2], Step [43460/67476], Loss: 4.6311\n",
      "Epoch [2/2], Step [43470/67476], Loss: 4.5000\n",
      "Epoch [2/2], Step [43480/67476], Loss: 4.5549\n",
      "Epoch [2/2], Step [43490/67476], Loss: 4.4381\n",
      "Epoch [2/2], Step [43500/67476], Loss: 4.5826\n",
      "Epoch [2/2], Step [43510/67476], Loss: 4.4454\n",
      "Epoch [2/2], Step [43520/67476], Loss: 4.5898\n",
      "Epoch [2/2], Step [43530/67476], Loss: 4.5938\n",
      "Epoch [2/2], Step [43540/67476], Loss: 4.6411\n",
      "Epoch [2/2], Step [43550/67476], Loss: 4.6887\n",
      "Epoch [2/2], Step [43560/67476], Loss: 4.4994\n",
      "Epoch [2/2], Step [43570/67476], Loss: 4.5946\n",
      "Epoch [2/2], Step [43580/67476], Loss: 4.7829\n",
      "Epoch [2/2], Step [43590/67476], Loss: 4.7784\n",
      "Epoch [2/2], Step [43600/67476], Loss: 4.5280\n",
      "Epoch [2/2], Step [43610/67476], Loss: 4.5351\n",
      "Epoch [2/2], Step [43620/67476], Loss: 4.4070\n",
      "Epoch [2/2], Step [43630/67476], Loss: 4.2535\n",
      "Epoch [2/2], Step [43640/67476], Loss: 4.4526\n",
      "Epoch [2/2], Step [43650/67476], Loss: 4.4468\n",
      "Epoch [2/2], Step [43660/67476], Loss: 4.3729\n",
      "Epoch [2/2], Step [43670/67476], Loss: 4.7238\n",
      "Epoch [2/2], Step [43680/67476], Loss: 4.5580\n",
      "Epoch [2/2], Step [43690/67476], Loss: 4.4764\n",
      "Epoch [2/2], Step [43700/67476], Loss: 4.6687\n",
      "Epoch [2/2], Step [43710/67476], Loss: 4.6371\n",
      "Epoch [2/2], Step [43720/67476], Loss: 4.4335\n",
      "Epoch [2/2], Step [43730/67476], Loss: 4.5167\n",
      "Epoch [2/2], Step [43740/67476], Loss: 4.6921\n",
      "Epoch [2/2], Step [43750/67476], Loss: 4.4638\n",
      "Epoch [2/2], Step [43760/67476], Loss: 4.4098\n",
      "Epoch [2/2], Step [43770/67476], Loss: 4.6161\n",
      "Epoch [2/2], Step [43780/67476], Loss: 4.6668\n",
      "Epoch [2/2], Step [43790/67476], Loss: 4.4936\n",
      "Epoch [2/2], Step [43800/67476], Loss: 4.7916\n",
      "Epoch [2/2], Step [43810/67476], Loss: 4.5481\n",
      "Epoch [2/2], Step [43820/67476], Loss: 4.5403\n",
      "Epoch [2/2], Step [43830/67476], Loss: 4.4241\n",
      "Epoch [2/2], Step [43840/67476], Loss: 4.5163\n",
      "Epoch [2/2], Step [43850/67476], Loss: 4.5779\n",
      "Epoch [2/2], Step [43860/67476], Loss: 4.4022\n",
      "Epoch [2/2], Step [43870/67476], Loss: 4.4214\n",
      "Epoch [2/2], Step [43880/67476], Loss: 4.7332\n",
      "Epoch [2/2], Step [43890/67476], Loss: 4.5805\n",
      "Epoch [2/2], Step [43900/67476], Loss: 4.4089\n",
      "Epoch [2/2], Step [43910/67476], Loss: 4.5164\n",
      "Epoch [2/2], Step [43920/67476], Loss: 4.5500\n",
      "Epoch [2/2], Step [43930/67476], Loss: 4.4193\n",
      "Epoch [2/2], Step [43940/67476], Loss: 4.4449\n",
      "Epoch [2/2], Step [43950/67476], Loss: 4.5304\n",
      "Epoch [2/2], Step [43960/67476], Loss: 4.5969\n",
      "Epoch [2/2], Step [43970/67476], Loss: 4.4302\n",
      "Epoch [2/2], Step [43980/67476], Loss: 4.4789\n",
      "Epoch [2/2], Step [43990/67476], Loss: 4.6521\n",
      "Epoch [2/2], Step [44000/67476], Loss: 4.6668\n",
      "Epoch [2/2], Step [44010/67476], Loss: 4.5199\n",
      "Epoch [2/2], Step [44020/67476], Loss: 4.5458\n",
      "Epoch [2/2], Step [44030/67476], Loss: 4.6670\n",
      "Epoch [2/2], Step [44040/67476], Loss: 4.5739\n",
      "Epoch [2/2], Step [44050/67476], Loss: 4.7011\n",
      "Epoch [2/2], Step [44060/67476], Loss: 4.5024\n",
      "Epoch [2/2], Step [44070/67476], Loss: 4.4534\n",
      "Epoch [2/2], Step [44080/67476], Loss: 4.7168\n",
      "Epoch [2/2], Step [44090/67476], Loss: 4.7635\n",
      "Epoch [2/2], Step [44100/67476], Loss: 4.5638\n",
      "Epoch [2/2], Step [44110/67476], Loss: 4.6076\n",
      "Epoch [2/2], Step [44120/67476], Loss: 4.5127\n",
      "Epoch [2/2], Step [44130/67476], Loss: 4.6254\n",
      "Epoch [2/2], Step [44140/67476], Loss: 4.4582\n",
      "Epoch [2/2], Step [44150/67476], Loss: 4.5853\n",
      "Epoch [2/2], Step [44160/67476], Loss: 4.5928\n",
      "Epoch [2/2], Step [44170/67476], Loss: 4.6517\n",
      "Epoch [2/2], Step [44180/67476], Loss: 4.6117\n",
      "Epoch [2/2], Step [44190/67476], Loss: 4.4195\n",
      "Epoch [2/2], Step [44200/67476], Loss: 4.5762\n",
      "Epoch [2/2], Step [44210/67476], Loss: 4.5847\n",
      "Epoch [2/2], Step [44220/67476], Loss: 4.5886\n",
      "Epoch [2/2], Step [44230/67476], Loss: 4.5790\n",
      "Epoch [2/2], Step [44240/67476], Loss: 4.5825\n",
      "Epoch [2/2], Step [44250/67476], Loss: 4.5048\n",
      "Epoch [2/2], Step [44260/67476], Loss: 4.5724\n",
      "Epoch [2/2], Step [44270/67476], Loss: 4.6320\n",
      "Epoch [2/2], Step [44280/67476], Loss: 4.4316\n",
      "Epoch [2/2], Step [44290/67476], Loss: 4.4564\n",
      "Epoch [2/2], Step [44300/67476], Loss: 4.7174\n",
      "Epoch [2/2], Step [44310/67476], Loss: 4.4319\n",
      "Epoch [2/2], Step [44320/67476], Loss: 4.4579\n",
      "Epoch [2/2], Step [44330/67476], Loss: 4.5433\n",
      "Epoch [2/2], Step [44340/67476], Loss: 4.4433\n",
      "Epoch [2/2], Step [44350/67476], Loss: 4.5440\n",
      "Epoch [2/2], Step [44360/67476], Loss: 4.4589\n",
      "Epoch [2/2], Step [44370/67476], Loss: 4.5930\n",
      "Epoch [2/2], Step [44380/67476], Loss: 4.5189\n",
      "Epoch [2/2], Step [44390/67476], Loss: 4.4472\n",
      "Epoch [2/2], Step [44400/67476], Loss: 4.5926\n",
      "Epoch [2/2], Step [44410/67476], Loss: 4.8380\n",
      "Epoch [2/2], Step [44420/67476], Loss: 4.5349\n",
      "Epoch [2/2], Step [44430/67476], Loss: 4.4178\n",
      "Epoch [2/2], Step [44440/67476], Loss: 4.6372\n",
      "Epoch [2/2], Step [44450/67476], Loss: 4.4540\n",
      "Epoch [2/2], Step [44460/67476], Loss: 4.6473\n",
      "Epoch [2/2], Step [44470/67476], Loss: 4.5164\n",
      "Epoch [2/2], Step [44480/67476], Loss: 4.6575\n",
      "Epoch [2/2], Step [44490/67476], Loss: 4.4735\n",
      "Epoch [2/2], Step [44500/67476], Loss: 4.4220\n",
      "Epoch [2/2], Step [44510/67476], Loss: 4.3794\n",
      "Epoch [2/2], Step [44520/67476], Loss: 4.5127\n",
      "Epoch [2/2], Step [44530/67476], Loss: 4.3455\n",
      "Epoch [2/2], Step [44540/67476], Loss: 4.5060\n",
      "Epoch [2/2], Step [44550/67476], Loss: 4.6245\n",
      "Epoch [2/2], Step [44560/67476], Loss: 4.5942\n",
      "Epoch [2/2], Step [44570/67476], Loss: 4.4897\n",
      "Epoch [2/2], Step [44580/67476], Loss: 4.6035\n",
      "Epoch [2/2], Step [44590/67476], Loss: 4.4911\n",
      "Epoch [2/2], Step [44600/67476], Loss: 4.5678\n",
      "Epoch [2/2], Step [44610/67476], Loss: 4.4045\n",
      "Epoch [2/2], Step [44620/67476], Loss: 4.5289\n",
      "Epoch [2/2], Step [44630/67476], Loss: 4.6153\n",
      "Epoch [2/2], Step [44640/67476], Loss: 4.6205\n",
      "Epoch [2/2], Step [44650/67476], Loss: 4.5942\n",
      "Epoch [2/2], Step [44660/67476], Loss: 4.4395\n",
      "Epoch [2/2], Step [44670/67476], Loss: 4.4798\n",
      "Epoch [2/2], Step [44680/67476], Loss: 4.5085\n",
      "Epoch [2/2], Step [44690/67476], Loss: 4.4842\n",
      "Epoch [2/2], Step [44700/67476], Loss: 4.5250\n",
      "Epoch [2/2], Step [44710/67476], Loss: 4.3766\n",
      "Epoch [2/2], Step [44720/67476], Loss: 4.6924\n",
      "Epoch [2/2], Step [44730/67476], Loss: 4.4972\n",
      "Epoch [2/2], Step [44740/67476], Loss: 4.6054\n",
      "Epoch [2/2], Step [44750/67476], Loss: 4.5302\n",
      "Epoch [2/2], Step [44760/67476], Loss: 4.4043\n",
      "Epoch [2/2], Step [44770/67476], Loss: 4.6296\n",
      "Epoch [2/2], Step [44780/67476], Loss: 4.6330\n",
      "Epoch [2/2], Step [44790/67476], Loss: 4.7672\n",
      "Epoch [2/2], Step [44800/67476], Loss: 4.5411\n",
      "Epoch [2/2], Step [44810/67476], Loss: 4.5224\n",
      "Epoch [2/2], Step [44820/67476], Loss: 4.6471\n",
      "Epoch [2/2], Step [44830/67476], Loss: 4.6712\n",
      "Epoch [2/2], Step [44840/67476], Loss: 4.6616\n",
      "Epoch [2/2], Step [44850/67476], Loss: 4.5431\n",
      "Epoch [2/2], Step [44860/67476], Loss: 4.5305\n",
      "Epoch [2/2], Step [44870/67476], Loss: 4.6825\n",
      "Epoch [2/2], Step [44880/67476], Loss: 4.7094\n",
      "Epoch [2/2], Step [44890/67476], Loss: 4.5079\n",
      "Epoch [2/2], Step [44900/67476], Loss: 4.4223\n",
      "Epoch [2/2], Step [44910/67476], Loss: 4.5582\n",
      "Epoch [2/2], Step [44920/67476], Loss: 4.6983\n",
      "Epoch [2/2], Step [44930/67476], Loss: 4.4510\n",
      "Epoch [2/2], Step [44940/67476], Loss: 4.4435\n",
      "Epoch [2/2], Step [44950/67476], Loss: 4.5007\n",
      "Epoch [2/2], Step [44960/67476], Loss: 4.5126\n",
      "Epoch [2/2], Step [44970/67476], Loss: 4.5598\n",
      "Epoch [2/2], Step [44980/67476], Loss: 4.5154\n",
      "Epoch [2/2], Step [44990/67476], Loss: 4.5367\n",
      "Epoch [2/2], Step [45000/67476], Loss: 4.4051\n",
      "Epoch [2/2], Step [45010/67476], Loss: 4.5653\n",
      "Epoch [2/2], Step [45020/67476], Loss: 4.3350\n",
      "Epoch [2/2], Step [45030/67476], Loss: 4.7173\n",
      "Epoch [2/2], Step [45040/67476], Loss: 4.7851\n",
      "Epoch [2/2], Step [45050/67476], Loss: 4.5066\n",
      "Epoch [2/2], Step [45060/67476], Loss: 4.5661\n",
      "Epoch [2/2], Step [45070/67476], Loss: 4.6207\n",
      "Epoch [2/2], Step [45080/67476], Loss: 4.5317\n",
      "Epoch [2/2], Step [45090/67476], Loss: 4.3497\n",
      "Epoch [2/2], Step [45100/67476], Loss: 4.6407\n",
      "Epoch [2/2], Step [45110/67476], Loss: 4.6153\n",
      "Epoch [2/2], Step [45120/67476], Loss: 4.3437\n",
      "Epoch [2/2], Step [45130/67476], Loss: 4.5268\n",
      "Epoch [2/2], Step [45140/67476], Loss: 4.4310\n",
      "Epoch [2/2], Step [45150/67476], Loss: 4.5157\n",
      "Epoch [2/2], Step [45160/67476], Loss: 4.5669\n",
      "Epoch [2/2], Step [45170/67476], Loss: 4.4525\n",
      "Epoch [2/2], Step [45180/67476], Loss: 4.5041\n",
      "Epoch [2/2], Step [45190/67476], Loss: 4.5550\n",
      "Epoch [2/2], Step [45200/67476], Loss: 4.3327\n",
      "Epoch [2/2], Step [45210/67476], Loss: 4.6119\n",
      "Epoch [2/2], Step [45220/67476], Loss: 4.5944\n",
      "Epoch [2/2], Step [45230/67476], Loss: 4.2810\n",
      "Epoch [2/2], Step [45240/67476], Loss: 4.6469\n",
      "Epoch [2/2], Step [45250/67476], Loss: 4.5487\n",
      "Epoch [2/2], Step [45260/67476], Loss: 4.4979\n",
      "Epoch [2/2], Step [45270/67476], Loss: 4.5694\n",
      "Epoch [2/2], Step [45280/67476], Loss: 4.5057\n",
      "Epoch [2/2], Step [45290/67476], Loss: 4.4925\n",
      "Epoch [2/2], Step [45300/67476], Loss: 4.7374\n",
      "Epoch [2/2], Step [45310/67476], Loss: 4.6652\n",
      "Epoch [2/2], Step [45320/67476], Loss: 4.4039\n",
      "Epoch [2/2], Step [45330/67476], Loss: 4.5155\n",
      "Epoch [2/2], Step [45340/67476], Loss: 4.7809\n",
      "Epoch [2/2], Step [45350/67476], Loss: 4.5865\n",
      "Epoch [2/2], Step [45360/67476], Loss: 4.2563\n",
      "Epoch [2/2], Step [45370/67476], Loss: 4.5705\n",
      "Epoch [2/2], Step [45380/67476], Loss: 4.6300\n",
      "Epoch [2/2], Step [45390/67476], Loss: 4.6438\n",
      "Epoch [2/2], Step [45400/67476], Loss: 4.4212\n",
      "Epoch [2/2], Step [45410/67476], Loss: 4.5733\n",
      "Epoch [2/2], Step [45420/67476], Loss: 4.4621\n",
      "Epoch [2/2], Step [45430/67476], Loss: 4.4447\n",
      "Epoch [2/2], Step [45440/67476], Loss: 4.6532\n",
      "Epoch [2/2], Step [45450/67476], Loss: 4.2791\n",
      "Epoch [2/2], Step [45460/67476], Loss: 4.2595\n",
      "Epoch [2/2], Step [45470/67476], Loss: 4.4106\n",
      "Epoch [2/2], Step [45480/67476], Loss: 4.6097\n",
      "Epoch [2/2], Step [45490/67476], Loss: 4.6062\n",
      "Epoch [2/2], Step [45500/67476], Loss: 4.5872\n",
      "Epoch [2/2], Step [45510/67476], Loss: 4.3790\n",
      "Epoch [2/2], Step [45520/67476], Loss: 4.5486\n",
      "Epoch [2/2], Step [45530/67476], Loss: 4.2947\n",
      "Epoch [2/2], Step [45540/67476], Loss: 4.5550\n",
      "Epoch [2/2], Step [45550/67476], Loss: 4.7310\n",
      "Epoch [2/2], Step [45560/67476], Loss: 4.5403\n",
      "Epoch [2/2], Step [45570/67476], Loss: 4.4787\n",
      "Epoch [2/2], Step [45580/67476], Loss: 4.5820\n",
      "Epoch [2/2], Step [45590/67476], Loss: 4.6086\n",
      "Epoch [2/2], Step [45600/67476], Loss: 4.5284\n",
      "Epoch [2/2], Step [45610/67476], Loss: 4.6520\n",
      "Epoch [2/2], Step [45620/67476], Loss: 4.4197\n",
      "Epoch [2/2], Step [45630/67476], Loss: 4.4082\n",
      "Epoch [2/2], Step [45640/67476], Loss: 4.4673\n",
      "Epoch [2/2], Step [45650/67476], Loss: 4.5421\n",
      "Epoch [2/2], Step [45660/67476], Loss: 4.6173\n",
      "Epoch [2/2], Step [45670/67476], Loss: 4.4195\n",
      "Epoch [2/2], Step [45680/67476], Loss: 4.4842\n",
      "Epoch [2/2], Step [45690/67476], Loss: 4.5745\n",
      "Epoch [2/2], Step [45700/67476], Loss: 4.3516\n",
      "Epoch [2/2], Step [45710/67476], Loss: 4.7497\n",
      "Epoch [2/2], Step [45720/67476], Loss: 4.4412\n",
      "Epoch [2/2], Step [45730/67476], Loss: 4.5280\n",
      "Epoch [2/2], Step [45740/67476], Loss: 4.6255\n",
      "Epoch [2/2], Step [45750/67476], Loss: 4.7084\n",
      "Epoch [2/2], Step [45760/67476], Loss: 4.4958\n",
      "Epoch [2/2], Step [45770/67476], Loss: 4.5035\n",
      "Epoch [2/2], Step [45780/67476], Loss: 4.3312\n",
      "Epoch [2/2], Step [45790/67476], Loss: 4.5330\n",
      "Epoch [2/2], Step [45800/67476], Loss: 4.4897\n",
      "Epoch [2/2], Step [45810/67476], Loss: 4.2785\n",
      "Epoch [2/2], Step [45820/67476], Loss: 4.5541\n",
      "Epoch [2/2], Step [45830/67476], Loss: 4.4135\n",
      "Epoch [2/2], Step [45840/67476], Loss: 4.6415\n",
      "Epoch [2/2], Step [45850/67476], Loss: 4.5294\n",
      "Epoch [2/2], Step [45860/67476], Loss: 4.7777\n",
      "Epoch [2/2], Step [45870/67476], Loss: 4.5041\n",
      "Epoch [2/2], Step [45880/67476], Loss: 4.4790\n",
      "Epoch [2/2], Step [45890/67476], Loss: 4.3381\n",
      "Epoch [2/2], Step [45900/67476], Loss: 4.4614\n",
      "Epoch [2/2], Step [45910/67476], Loss: 4.4509\n",
      "Epoch [2/2], Step [45920/67476], Loss: 4.6064\n",
      "Epoch [2/2], Step [45930/67476], Loss: 4.5253\n",
      "Epoch [2/2], Step [45940/67476], Loss: 4.4800\n",
      "Epoch [2/2], Step [45950/67476], Loss: 4.0817\n",
      "Epoch [2/2], Step [45960/67476], Loss: 4.5294\n",
      "Epoch [2/2], Step [45970/67476], Loss: 4.6136\n",
      "Epoch [2/2], Step [45980/67476], Loss: 4.4484\n",
      "Epoch [2/2], Step [45990/67476], Loss: 4.5839\n",
      "Epoch [2/2], Step [46000/67476], Loss: 4.4734\n",
      "Epoch [2/2], Step [46010/67476], Loss: 4.5360\n",
      "Epoch [2/2], Step [46020/67476], Loss: 4.5900\n",
      "Epoch [2/2], Step [46030/67476], Loss: 4.5696\n",
      "Epoch [2/2], Step [46040/67476], Loss: 4.6166\n",
      "Epoch [2/2], Step [46050/67476], Loss: 4.6171\n",
      "Epoch [2/2], Step [46060/67476], Loss: 4.4059\n",
      "Epoch [2/2], Step [46070/67476], Loss: 4.5540\n",
      "Epoch [2/2], Step [46080/67476], Loss: 4.5164\n",
      "Epoch [2/2], Step [46090/67476], Loss: 4.6056\n",
      "Epoch [2/2], Step [46100/67476], Loss: 4.5882\n",
      "Epoch [2/2], Step [46110/67476], Loss: 4.6835\n",
      "Epoch [2/2], Step [46120/67476], Loss: 4.4433\n",
      "Epoch [2/2], Step [46130/67476], Loss: 4.6253\n",
      "Epoch [2/2], Step [46140/67476], Loss: 4.4916\n",
      "Epoch [2/2], Step [46150/67476], Loss: 4.5405\n",
      "Epoch [2/2], Step [46160/67476], Loss: 4.7303\n",
      "Epoch [2/2], Step [46170/67476], Loss: 4.3253\n",
      "Epoch [2/2], Step [46180/67476], Loss: 4.5464\n",
      "Epoch [2/2], Step [46190/67476], Loss: 4.5399\n",
      "Epoch [2/2], Step [46200/67476], Loss: 4.6607\n",
      "Epoch [2/2], Step [46210/67476], Loss: 4.6198\n",
      "Epoch [2/2], Step [46220/67476], Loss: 4.6696\n",
      "Epoch [2/2], Step [46230/67476], Loss: 4.4392\n",
      "Epoch [2/2], Step [46240/67476], Loss: 4.5845\n",
      "Epoch [2/2], Step [46250/67476], Loss: 4.4379\n",
      "Epoch [2/2], Step [46260/67476], Loss: 4.5990\n",
      "Epoch [2/2], Step [46270/67476], Loss: 4.8304\n",
      "Epoch [2/2], Step [46280/67476], Loss: 4.4334\n",
      "Epoch [2/2], Step [46290/67476], Loss: 4.5847\n",
      "Epoch [2/2], Step [46300/67476], Loss: 4.4863\n",
      "Epoch [2/2], Step [46310/67476], Loss: 4.5882\n",
      "Epoch [2/2], Step [46320/67476], Loss: 4.4339\n",
      "Epoch [2/2], Step [46330/67476], Loss: 4.7050\n",
      "Epoch [2/2], Step [46340/67476], Loss: 4.6127\n",
      "Epoch [2/2], Step [46350/67476], Loss: 4.7199\n",
      "Epoch [2/2], Step [46360/67476], Loss: 4.5081\n",
      "Epoch [2/2], Step [46370/67476], Loss: 4.5426\n",
      "Epoch [2/2], Step [46380/67476], Loss: 4.6306\n",
      "Epoch [2/2], Step [46390/67476], Loss: 4.7402\n",
      "Epoch [2/2], Step [46400/67476], Loss: 4.5581\n",
      "Epoch [2/2], Step [46410/67476], Loss: 4.5476\n",
      "Epoch [2/2], Step [46420/67476], Loss: 4.5257\n",
      "Epoch [2/2], Step [46430/67476], Loss: 4.6481\n",
      "Epoch [2/2], Step [46440/67476], Loss: 4.4239\n",
      "Epoch [2/2], Step [46450/67476], Loss: 4.6704\n",
      "Epoch [2/2], Step [46460/67476], Loss: 4.6087\n",
      "Epoch [2/2], Step [46470/67476], Loss: 4.5496\n",
      "Epoch [2/2], Step [46480/67476], Loss: 4.7128\n",
      "Epoch [2/2], Step [46490/67476], Loss: 4.6028\n",
      "Epoch [2/2], Step [46500/67476], Loss: 4.4965\n",
      "Epoch [2/2], Step [46510/67476], Loss: 4.6341\n",
      "Epoch [2/2], Step [46520/67476], Loss: 4.4932\n",
      "Epoch [2/2], Step [46530/67476], Loss: 4.5153\n",
      "Epoch [2/2], Step [46540/67476], Loss: 4.3423\n",
      "Epoch [2/2], Step [46550/67476], Loss: 4.5981\n",
      "Epoch [2/2], Step [46560/67476], Loss: 4.5355\n",
      "Epoch [2/2], Step [46570/67476], Loss: 4.6088\n",
      "Epoch [2/2], Step [46580/67476], Loss: 4.4290\n",
      "Epoch [2/2], Step [46590/67476], Loss: 4.5583\n",
      "Epoch [2/2], Step [46600/67476], Loss: 4.8001\n",
      "Epoch [2/2], Step [46610/67476], Loss: 4.6471\n",
      "Epoch [2/2], Step [46620/67476], Loss: 4.4837\n",
      "Epoch [2/2], Step [46630/67476], Loss: 4.4902\n",
      "Epoch [2/2], Step [46640/67476], Loss: 4.5828\n",
      "Epoch [2/2], Step [46650/67476], Loss: 4.4924\n",
      "Epoch [2/2], Step [46660/67476], Loss: 4.5941\n",
      "Epoch [2/2], Step [46670/67476], Loss: 4.5987\n",
      "Epoch [2/2], Step [46680/67476], Loss: 4.4882\n",
      "Epoch [2/2], Step [46690/67476], Loss: 4.5815\n",
      "Epoch [2/2], Step [46700/67476], Loss: 4.4654\n",
      "Epoch [2/2], Step [46710/67476], Loss: 4.4893\n",
      "Epoch [2/2], Step [46720/67476], Loss: 4.5427\n",
      "Epoch [2/2], Step [46730/67476], Loss: 4.5323\n",
      "Epoch [2/2], Step [46740/67476], Loss: 4.3540\n",
      "Epoch [2/2], Step [46750/67476], Loss: 4.4720\n",
      "Epoch [2/2], Step [46760/67476], Loss: 4.5649\n",
      "Epoch [2/2], Step [46770/67476], Loss: 4.5945\n",
      "Epoch [2/2], Step [46780/67476], Loss: 4.5443\n",
      "Epoch [2/2], Step [46790/67476], Loss: 4.3813\n",
      "Epoch [2/2], Step [46800/67476], Loss: 4.6867\n",
      "Epoch [2/2], Step [46810/67476], Loss: 4.3609\n",
      "Epoch [2/2], Step [46820/67476], Loss: 4.5579\n",
      "Epoch [2/2], Step [46830/67476], Loss: 4.7729\n",
      "Epoch [2/2], Step [46840/67476], Loss: 4.5590\n",
      "Epoch [2/2], Step [46850/67476], Loss: 4.4963\n",
      "Epoch [2/2], Step [46860/67476], Loss: 4.6274\n",
      "Epoch [2/2], Step [46870/67476], Loss: 4.5434\n",
      "Epoch [2/2], Step [46880/67476], Loss: 4.7522\n",
      "Epoch [2/2], Step [46890/67476], Loss: 4.3712\n",
      "Epoch [2/2], Step [46900/67476], Loss: 4.4062\n",
      "Epoch [2/2], Step [46910/67476], Loss: 4.4971\n",
      "Epoch [2/2], Step [46920/67476], Loss: 4.5421\n",
      "Epoch [2/2], Step [46930/67476], Loss: 4.4061\n",
      "Epoch [2/2], Step [46940/67476], Loss: 4.7095\n",
      "Epoch [2/2], Step [46950/67476], Loss: 4.3312\n",
      "Epoch [2/2], Step [46960/67476], Loss: 4.9330\n",
      "Epoch [2/2], Step [46970/67476], Loss: 4.4442\n",
      "Epoch [2/2], Step [46980/67476], Loss: 4.7114\n",
      "Epoch [2/2], Step [46990/67476], Loss: 4.4990\n",
      "Epoch [2/2], Step [47000/67476], Loss: 4.2006\n",
      "Epoch [2/2], Step [47010/67476], Loss: 4.4119\n",
      "Epoch [2/2], Step [47020/67476], Loss: 4.6180\n",
      "Epoch [2/2], Step [47030/67476], Loss: 4.5511\n",
      "Epoch [2/2], Step [47040/67476], Loss: 4.6983\n",
      "Epoch [2/2], Step [47050/67476], Loss: 4.5308\n",
      "Epoch [2/2], Step [47060/67476], Loss: 4.6927\n",
      "Epoch [2/2], Step [47070/67476], Loss: 4.5569\n",
      "Epoch [2/2], Step [47080/67476], Loss: 4.3949\n",
      "Epoch [2/2], Step [47090/67476], Loss: 4.5849\n",
      "Epoch [2/2], Step [47100/67476], Loss: 4.5423\n",
      "Epoch [2/2], Step [47110/67476], Loss: 4.4579\n",
      "Epoch [2/2], Step [47120/67476], Loss: 4.6752\n",
      "Epoch [2/2], Step [47130/67476], Loss: 4.4356\n",
      "Epoch [2/2], Step [47140/67476], Loss: 4.4577\n",
      "Epoch [2/2], Step [47150/67476], Loss: 4.5727\n",
      "Epoch [2/2], Step [47160/67476], Loss: 4.4716\n",
      "Epoch [2/2], Step [47170/67476], Loss: 4.4468\n",
      "Epoch [2/2], Step [47180/67476], Loss: 4.6823\n",
      "Epoch [2/2], Step [47190/67476], Loss: 4.6734\n",
      "Epoch [2/2], Step [47200/67476], Loss: 4.6209\n",
      "Epoch [2/2], Step [47210/67476], Loss: 4.5621\n",
      "Epoch [2/2], Step [47220/67476], Loss: 4.8255\n",
      "Epoch [2/2], Step [47230/67476], Loss: 4.4196\n",
      "Epoch [2/2], Step [47240/67476], Loss: 4.5973\n",
      "Epoch [2/2], Step [47250/67476], Loss: 4.6881\n",
      "Epoch [2/2], Step [47260/67476], Loss: 4.5031\n",
      "Epoch [2/2], Step [47270/67476], Loss: 4.5855\n",
      "Epoch [2/2], Step [47280/67476], Loss: 4.5422\n",
      "Epoch [2/2], Step [47290/67476], Loss: 4.4931\n",
      "Epoch [2/2], Step [47300/67476], Loss: 4.4977\n",
      "Epoch [2/2], Step [47310/67476], Loss: 4.6151\n",
      "Epoch [2/2], Step [47320/67476], Loss: 4.5799\n",
      "Epoch [2/2], Step [47330/67476], Loss: 4.5628\n",
      "Epoch [2/2], Step [47340/67476], Loss: 4.3994\n",
      "Epoch [2/2], Step [47350/67476], Loss: 4.3959\n",
      "Epoch [2/2], Step [47360/67476], Loss: 4.4616\n",
      "Epoch [2/2], Step [47370/67476], Loss: 4.6215\n",
      "Epoch [2/2], Step [47380/67476], Loss: 4.5359\n",
      "Epoch [2/2], Step [47390/67476], Loss: 4.4800\n",
      "Epoch [2/2], Step [47400/67476], Loss: 4.6055\n",
      "Epoch [2/2], Step [47410/67476], Loss: 4.5287\n",
      "Epoch [2/2], Step [47420/67476], Loss: 4.5093\n",
      "Epoch [2/2], Step [47430/67476], Loss: 4.3448\n",
      "Epoch [2/2], Step [47440/67476], Loss: 4.5304\n",
      "Epoch [2/2], Step [47450/67476], Loss: 4.5281\n",
      "Epoch [2/2], Step [47460/67476], Loss: 4.6923\n",
      "Epoch [2/2], Step [47470/67476], Loss: 4.4276\n",
      "Epoch [2/2], Step [47480/67476], Loss: 4.2477\n",
      "Epoch [2/2], Step [47490/67476], Loss: 4.6851\n",
      "Epoch [2/2], Step [47500/67476], Loss: 4.3855\n",
      "Epoch [2/2], Step [47510/67476], Loss: 4.5012\n",
      "Epoch [2/2], Step [47520/67476], Loss: 4.6738\n",
      "Epoch [2/2], Step [47530/67476], Loss: 4.8210\n",
      "Epoch [2/2], Step [47540/67476], Loss: 4.3683\n",
      "Epoch [2/2], Step [47550/67476], Loss: 4.6055\n",
      "Epoch [2/2], Step [47560/67476], Loss: 4.5698\n",
      "Epoch [2/2], Step [47570/67476], Loss: 4.5344\n",
      "Epoch [2/2], Step [47580/67476], Loss: 4.4942\n",
      "Epoch [2/2], Step [47590/67476], Loss: 4.4603\n",
      "Epoch [2/2], Step [47600/67476], Loss: 4.5642\n",
      "Epoch [2/2], Step [47610/67476], Loss: 4.4125\n",
      "Epoch [2/2], Step [47620/67476], Loss: 4.7076\n",
      "Epoch [2/2], Step [47630/67476], Loss: 4.1953\n",
      "Epoch [2/2], Step [47640/67476], Loss: 4.5636\n",
      "Epoch [2/2], Step [47650/67476], Loss: 4.4472\n",
      "Epoch [2/2], Step [47660/67476], Loss: 4.5106\n",
      "Epoch [2/2], Step [47670/67476], Loss: 4.6752\n",
      "Epoch [2/2], Step [47680/67476], Loss: 4.7177\n",
      "Epoch [2/2], Step [47690/67476], Loss: 4.6729\n",
      "Epoch [2/2], Step [47700/67476], Loss: 4.6708\n",
      "Epoch [2/2], Step [47710/67476], Loss: 4.6365\n",
      "Epoch [2/2], Step [47720/67476], Loss: 4.4343\n",
      "Epoch [2/2], Step [47730/67476], Loss: 4.4189\n",
      "Epoch [2/2], Step [47740/67476], Loss: 4.6339\n",
      "Epoch [2/2], Step [47750/67476], Loss: 4.5383\n",
      "Epoch [2/2], Step [47760/67476], Loss: 4.6622\n",
      "Epoch [2/2], Step [47770/67476], Loss: 4.8802\n",
      "Epoch [2/2], Step [47780/67476], Loss: 4.5555\n",
      "Epoch [2/2], Step [47790/67476], Loss: 4.1895\n",
      "Epoch [2/2], Step [47800/67476], Loss: 4.5035\n",
      "Epoch [2/2], Step [47810/67476], Loss: 4.5044\n",
      "Epoch [2/2], Step [47820/67476], Loss: 4.4797\n",
      "Epoch [2/2], Step [47830/67476], Loss: 4.7221\n",
      "Epoch [2/2], Step [47840/67476], Loss: 4.5286\n",
      "Epoch [2/2], Step [47850/67476], Loss: 4.4208\n",
      "Epoch [2/2], Step [47860/67476], Loss: 4.4136\n",
      "Epoch [2/2], Step [47870/67476], Loss: 4.6997\n",
      "Epoch [2/2], Step [47880/67476], Loss: 4.6116\n",
      "Epoch [2/2], Step [47890/67476], Loss: 4.5942\n",
      "Epoch [2/2], Step [47900/67476], Loss: 4.4273\n",
      "Epoch [2/2], Step [47910/67476], Loss: 4.3952\n",
      "Epoch [2/2], Step [47920/67476], Loss: 4.3340\n",
      "Epoch [2/2], Step [47930/67476], Loss: 4.7940\n",
      "Epoch [2/2], Step [47940/67476], Loss: 4.5375\n",
      "Epoch [2/2], Step [47950/67476], Loss: 4.5598\n",
      "Epoch [2/2], Step [47960/67476], Loss: 4.4418\n",
      "Epoch [2/2], Step [47970/67476], Loss: 4.5329\n",
      "Epoch [2/2], Step [47980/67476], Loss: 4.5034\n",
      "Epoch [2/2], Step [47990/67476], Loss: 4.5834\n",
      "Epoch [2/2], Step [48000/67476], Loss: 4.5679\n",
      "Epoch [2/2], Step [48010/67476], Loss: 4.3238\n",
      "Epoch [2/2], Step [48020/67476], Loss: 4.6287\n",
      "Epoch [2/2], Step [48030/67476], Loss: 4.4096\n",
      "Epoch [2/2], Step [48040/67476], Loss: 4.6469\n",
      "Epoch [2/2], Step [48050/67476], Loss: 4.3719\n",
      "Epoch [2/2], Step [48060/67476], Loss: 4.6181\n",
      "Epoch [2/2], Step [48070/67476], Loss: 4.5445\n",
      "Epoch [2/2], Step [48080/67476], Loss: 4.7792\n",
      "Epoch [2/2], Step [48090/67476], Loss: 4.4695\n",
      "Epoch [2/2], Step [48100/67476], Loss: 4.5248\n",
      "Epoch [2/2], Step [48110/67476], Loss: 4.5179\n",
      "Epoch [2/2], Step [48120/67476], Loss: 4.7244\n",
      "Epoch [2/2], Step [48130/67476], Loss: 4.6769\n",
      "Epoch [2/2], Step [48140/67476], Loss: 4.5169\n",
      "Epoch [2/2], Step [48150/67476], Loss: 4.5528\n",
      "Epoch [2/2], Step [48160/67476], Loss: 4.7013\n",
      "Epoch [2/2], Step [48170/67476], Loss: 4.4868\n",
      "Epoch [2/2], Step [48180/67476], Loss: 4.4484\n",
      "Epoch [2/2], Step [48190/67476], Loss: 4.6787\n",
      "Epoch [2/2], Step [48200/67476], Loss: 4.3899\n",
      "Epoch [2/2], Step [48210/67476], Loss: 4.7092\n",
      "Epoch [2/2], Step [48220/67476], Loss: 4.6821\n",
      "Epoch [2/2], Step [48230/67476], Loss: 4.5639\n",
      "Epoch [2/2], Step [48240/67476], Loss: 4.8352\n",
      "Epoch [2/2], Step [48250/67476], Loss: 4.5651\n",
      "Epoch [2/2], Step [48260/67476], Loss: 4.5063\n",
      "Epoch [2/2], Step [48270/67476], Loss: 4.4051\n",
      "Epoch [2/2], Step [48280/67476], Loss: 4.5845\n",
      "Epoch [2/2], Step [48290/67476], Loss: 4.5957\n",
      "Epoch [2/2], Step [48300/67476], Loss: 4.3969\n",
      "Epoch [2/2], Step [48310/67476], Loss: 4.4402\n",
      "Epoch [2/2], Step [48320/67476], Loss: 4.4501\n",
      "Epoch [2/2], Step [48330/67476], Loss: 4.4271\n",
      "Epoch [2/2], Step [48340/67476], Loss: 4.2802\n",
      "Epoch [2/2], Step [48350/67476], Loss: 4.6726\n",
      "Epoch [2/2], Step [48360/67476], Loss: 4.2635\n",
      "Epoch [2/2], Step [48370/67476], Loss: 4.7409\n",
      "Epoch [2/2], Step [48380/67476], Loss: 4.4377\n",
      "Epoch [2/2], Step [48390/67476], Loss: 4.6327\n",
      "Epoch [2/2], Step [48400/67476], Loss: 4.6227\n",
      "Epoch [2/2], Step [48410/67476], Loss: 4.3673\n",
      "Epoch [2/2], Step [48420/67476], Loss: 4.8079\n",
      "Epoch [2/2], Step [48430/67476], Loss: 4.6275\n",
      "Epoch [2/2], Step [48440/67476], Loss: 4.5143\n",
      "Epoch [2/2], Step [48450/67476], Loss: 4.7274\n",
      "Epoch [2/2], Step [48460/67476], Loss: 4.6047\n",
      "Epoch [2/2], Step [48470/67476], Loss: 4.3384\n",
      "Epoch [2/2], Step [48480/67476], Loss: 4.4249\n",
      "Epoch [2/2], Step [48490/67476], Loss: 4.4714\n",
      "Epoch [2/2], Step [48500/67476], Loss: 4.4006\n",
      "Epoch [2/2], Step [48510/67476], Loss: 4.5599\n",
      "Epoch [2/2], Step [48520/67476], Loss: 4.5361\n",
      "Epoch [2/2], Step [48530/67476], Loss: 4.5111\n",
      "Epoch [2/2], Step [48540/67476], Loss: 4.5499\n",
      "Epoch [2/2], Step [48550/67476], Loss: 4.5532\n",
      "Epoch [2/2], Step [48560/67476], Loss: 4.7013\n",
      "Epoch [2/2], Step [48570/67476], Loss: 4.6068\n",
      "Epoch [2/2], Step [48580/67476], Loss: 4.5762\n",
      "Epoch [2/2], Step [48590/67476], Loss: 4.5827\n",
      "Epoch [2/2], Step [48600/67476], Loss: 4.7635\n",
      "Epoch [2/2], Step [48610/67476], Loss: 4.5571\n",
      "Epoch [2/2], Step [48620/67476], Loss: 4.6335\n",
      "Epoch [2/2], Step [48630/67476], Loss: 4.6480\n",
      "Epoch [2/2], Step [48640/67476], Loss: 4.5555\n",
      "Epoch [2/2], Step [48650/67476], Loss: 4.6146\n",
      "Epoch [2/2], Step [48660/67476], Loss: 4.6529\n",
      "Epoch [2/2], Step [48670/67476], Loss: 4.3902\n",
      "Epoch [2/2], Step [48680/67476], Loss: 4.5605\n",
      "Epoch [2/2], Step [48690/67476], Loss: 4.6613\n",
      "Epoch [2/2], Step [48700/67476], Loss: 4.5554\n",
      "Epoch [2/2], Step [48710/67476], Loss: 4.5419\n",
      "Epoch [2/2], Step [48720/67476], Loss: 4.5530\n",
      "Epoch [2/2], Step [48730/67476], Loss: 4.4893\n",
      "Epoch [2/2], Step [48740/67476], Loss: 4.6510\n",
      "Epoch [2/2], Step [48750/67476], Loss: 4.4702\n",
      "Epoch [2/2], Step [48760/67476], Loss: 4.4367\n",
      "Epoch [2/2], Step [48770/67476], Loss: 4.4309\n",
      "Epoch [2/2], Step [48780/67476], Loss: 4.5472\n",
      "Epoch [2/2], Step [48790/67476], Loss: 4.3789\n",
      "Epoch [2/2], Step [48800/67476], Loss: 4.5364\n",
      "Epoch [2/2], Step [48810/67476], Loss: 4.4552\n",
      "Epoch [2/2], Step [48820/67476], Loss: 4.6390\n",
      "Epoch [2/2], Step [48830/67476], Loss: 4.4786\n",
      "Epoch [2/2], Step [48840/67476], Loss: 4.6212\n",
      "Epoch [2/2], Step [48850/67476], Loss: 4.6301\n",
      "Epoch [2/2], Step [48860/67476], Loss: 4.6736\n",
      "Epoch [2/2], Step [48870/67476], Loss: 4.4513\n",
      "Epoch [2/2], Step [48880/67476], Loss: 4.5657\n",
      "Epoch [2/2], Step [48890/67476], Loss: 4.8099\n",
      "Epoch [2/2], Step [48900/67476], Loss: 4.7249\n",
      "Epoch [2/2], Step [48910/67476], Loss: 4.5324\n",
      "Epoch [2/2], Step [48920/67476], Loss: 4.5375\n",
      "Epoch [2/2], Step [48930/67476], Loss: 4.4052\n",
      "Epoch [2/2], Step [48940/67476], Loss: 4.5976\n",
      "Epoch [2/2], Step [48950/67476], Loss: 4.5518\n",
      "Epoch [2/2], Step [48960/67476], Loss: 4.4982\n",
      "Epoch [2/2], Step [48970/67476], Loss: 4.5442\n",
      "Epoch [2/2], Step [48980/67476], Loss: 4.6663\n",
      "Epoch [2/2], Step [48990/67476], Loss: 4.6202\n",
      "Epoch [2/2], Step [49000/67476], Loss: 4.4058\n",
      "Epoch [2/2], Step [49010/67476], Loss: 4.6145\n",
      "Epoch [2/2], Step [49020/67476], Loss: 4.6179\n",
      "Epoch [2/2], Step [49030/67476], Loss: 4.3826\n",
      "Epoch [2/2], Step [49040/67476], Loss: 4.4899\n",
      "Epoch [2/2], Step [49050/67476], Loss: 4.3350\n",
      "Epoch [2/2], Step [49060/67476], Loss: 4.5498\n",
      "Epoch [2/2], Step [49070/67476], Loss: 4.4827\n",
      "Epoch [2/2], Step [49080/67476], Loss: 4.5247\n",
      "Epoch [2/2], Step [49090/67476], Loss: 4.4880\n",
      "Epoch [2/2], Step [49100/67476], Loss: 4.3918\n",
      "Epoch [2/2], Step [49110/67476], Loss: 4.7552\n",
      "Epoch [2/2], Step [49120/67476], Loss: 4.5722\n",
      "Epoch [2/2], Step [49130/67476], Loss: 4.5027\n",
      "Epoch [2/2], Step [49140/67476], Loss: 4.4143\n",
      "Epoch [2/2], Step [49150/67476], Loss: 4.6255\n",
      "Epoch [2/2], Step [49160/67476], Loss: 4.4148\n",
      "Epoch [2/2], Step [49170/67476], Loss: 4.5077\n",
      "Epoch [2/2], Step [49180/67476], Loss: 4.5848\n",
      "Epoch [2/2], Step [49190/67476], Loss: 4.5439\n",
      "Epoch [2/2], Step [49200/67476], Loss: 4.7077\n",
      "Epoch [2/2], Step [49210/67476], Loss: 4.5524\n",
      "Epoch [2/2], Step [49220/67476], Loss: 4.4914\n",
      "Epoch [2/2], Step [49230/67476], Loss: 4.5847\n",
      "Epoch [2/2], Step [49240/67476], Loss: 4.7374\n",
      "Epoch [2/2], Step [49250/67476], Loss: 4.3505\n",
      "Epoch [2/2], Step [49260/67476], Loss: 4.4291\n",
      "Epoch [2/2], Step [49270/67476], Loss: 4.5917\n",
      "Epoch [2/2], Step [49280/67476], Loss: 4.4959\n",
      "Epoch [2/2], Step [49290/67476], Loss: 4.5072\n",
      "Epoch [2/2], Step [49300/67476], Loss: 4.4606\n",
      "Epoch [2/2], Step [49310/67476], Loss: 4.6030\n",
      "Epoch [2/2], Step [49320/67476], Loss: 4.4557\n",
      "Epoch [2/2], Step [49330/67476], Loss: 4.7101\n",
      "Epoch [2/2], Step [49340/67476], Loss: 4.4382\n",
      "Epoch [2/2], Step [49350/67476], Loss: 4.5241\n",
      "Epoch [2/2], Step [49360/67476], Loss: 4.6258\n",
      "Epoch [2/2], Step [49370/67476], Loss: 4.4463\n",
      "Epoch [2/2], Step [49380/67476], Loss: 4.6072\n",
      "Epoch [2/2], Step [49390/67476], Loss: 4.4595\n",
      "Epoch [2/2], Step [49400/67476], Loss: 4.3853\n",
      "Epoch [2/2], Step [49410/67476], Loss: 4.3228\n",
      "Epoch [2/2], Step [49420/67476], Loss: 4.4291\n",
      "Epoch [2/2], Step [49430/67476], Loss: 4.6278\n",
      "Epoch [2/2], Step [49440/67476], Loss: 4.2579\n",
      "Epoch [2/2], Step [49450/67476], Loss: 4.5833\n",
      "Epoch [2/2], Step [49460/67476], Loss: 4.4791\n",
      "Epoch [2/2], Step [49470/67476], Loss: 4.6231\n",
      "Epoch [2/2], Step [49480/67476], Loss: 4.4343\n",
      "Epoch [2/2], Step [49490/67476], Loss: 4.5476\n",
      "Epoch [2/2], Step [49500/67476], Loss: 4.5743\n",
      "Epoch [2/2], Step [49510/67476], Loss: 4.5140\n",
      "Epoch [2/2], Step [49520/67476], Loss: 4.3322\n",
      "Epoch [2/2], Step [49530/67476], Loss: 4.5352\n",
      "Epoch [2/2], Step [49540/67476], Loss: 4.5218\n",
      "Epoch [2/2], Step [49550/67476], Loss: 4.7783\n",
      "Epoch [2/2], Step [49560/67476], Loss: 4.3107\n",
      "Epoch [2/2], Step [49570/67476], Loss: 4.4961\n",
      "Epoch [2/2], Step [49580/67476], Loss: 4.7080\n",
      "Epoch [2/2], Step [49590/67476], Loss: 4.3590\n",
      "Epoch [2/2], Step [49600/67476], Loss: 4.4515\n",
      "Epoch [2/2], Step [49610/67476], Loss: 4.6947\n",
      "Epoch [2/2], Step [49620/67476], Loss: 4.6080\n",
      "Epoch [2/2], Step [49630/67476], Loss: 4.4653\n",
      "Epoch [2/2], Step [49640/67476], Loss: 4.6279\n",
      "Epoch [2/2], Step [49650/67476], Loss: 4.6966\n",
      "Epoch [2/2], Step [49660/67476], Loss: 4.3484\n",
      "Epoch [2/2], Step [49670/67476], Loss: 4.7871\n",
      "Epoch [2/2], Step [49680/67476], Loss: 4.4936\n",
      "Epoch [2/2], Step [49690/67476], Loss: 4.5549\n",
      "Epoch [2/2], Step [49700/67476], Loss: 4.6727\n",
      "Epoch [2/2], Step [49710/67476], Loss: 4.6186\n",
      "Epoch [2/2], Step [49720/67476], Loss: 4.5619\n",
      "Epoch [2/2], Step [49730/67476], Loss: 4.4085\n",
      "Epoch [2/2], Step [49740/67476], Loss: 4.5720\n",
      "Epoch [2/2], Step [49750/67476], Loss: 4.6358\n",
      "Epoch [2/2], Step [49760/67476], Loss: 4.5775\n",
      "Epoch [2/2], Step [49770/67476], Loss: 4.6649\n",
      "Epoch [2/2], Step [49780/67476], Loss: 4.6604\n",
      "Epoch [2/2], Step [49790/67476], Loss: 4.6029\n",
      "Epoch [2/2], Step [49800/67476], Loss: 4.3763\n",
      "Epoch [2/2], Step [49810/67476], Loss: 4.4067\n",
      "Epoch [2/2], Step [49820/67476], Loss: 4.4241\n",
      "Epoch [2/2], Step [49830/67476], Loss: 4.5490\n",
      "Epoch [2/2], Step [49840/67476], Loss: 4.3944\n",
      "Epoch [2/2], Step [49850/67476], Loss: 4.5591\n",
      "Epoch [2/2], Step [49860/67476], Loss: 4.5397\n",
      "Epoch [2/2], Step [49870/67476], Loss: 4.4922\n",
      "Epoch [2/2], Step [49880/67476], Loss: 4.3677\n",
      "Epoch [2/2], Step [49890/67476], Loss: 4.3810\n",
      "Epoch [2/2], Step [49900/67476], Loss: 4.6326\n",
      "Epoch [2/2], Step [49910/67476], Loss: 4.4065\n",
      "Epoch [2/2], Step [49920/67476], Loss: 4.5009\n",
      "Epoch [2/2], Step [49930/67476], Loss: 4.4889\n",
      "Epoch [2/2], Step [49940/67476], Loss: 4.4352\n",
      "Epoch [2/2], Step [49950/67476], Loss: 4.4757\n",
      "Epoch [2/2], Step [49960/67476], Loss: 4.3788\n",
      "Epoch [2/2], Step [49970/67476], Loss: 4.4734\n",
      "Epoch [2/2], Step [49980/67476], Loss: 4.5836\n",
      "Epoch [2/2], Step [49990/67476], Loss: 4.4710\n",
      "Epoch [2/2], Step [50000/67476], Loss: 4.4904\n",
      "Epoch [2/2], Step [50010/67476], Loss: 4.7180\n",
      "Epoch [2/2], Step [50020/67476], Loss: 4.4840\n",
      "Epoch [2/2], Step [50030/67476], Loss: 4.4397\n",
      "Epoch [2/2], Step [50040/67476], Loss: 4.7322\n",
      "Epoch [2/2], Step [50050/67476], Loss: 4.5679\n",
      "Epoch [2/2], Step [50060/67476], Loss: 4.6332\n",
      "Epoch [2/2], Step [50070/67476], Loss: 4.6050\n",
      "Epoch [2/2], Step [50080/67476], Loss: 4.4541\n",
      "Epoch [2/2], Step [50090/67476], Loss: 4.5030\n",
      "Epoch [2/2], Step [50100/67476], Loss: 4.5617\n",
      "Epoch [2/2], Step [50110/67476], Loss: 4.7974\n",
      "Epoch [2/2], Step [50120/67476], Loss: 4.5616\n",
      "Epoch [2/2], Step [50130/67476], Loss: 4.3679\n",
      "Epoch [2/2], Step [50140/67476], Loss: 4.4926\n",
      "Epoch [2/2], Step [50150/67476], Loss: 4.7306\n",
      "Epoch [2/2], Step [50160/67476], Loss: 4.4004\n",
      "Epoch [2/2], Step [50170/67476], Loss: 4.5746\n",
      "Epoch [2/2], Step [50180/67476], Loss: 4.4222\n",
      "Epoch [2/2], Step [50190/67476], Loss: 4.4576\n",
      "Epoch [2/2], Step [50200/67476], Loss: 4.2920\n",
      "Epoch [2/2], Step [50210/67476], Loss: 4.5139\n",
      "Epoch [2/2], Step [50220/67476], Loss: 4.5151\n",
      "Epoch [2/2], Step [50230/67476], Loss: 4.7565\n",
      "Epoch [2/2], Step [50240/67476], Loss: 4.5407\n",
      "Epoch [2/2], Step [50250/67476], Loss: 4.7439\n",
      "Epoch [2/2], Step [50260/67476], Loss: 4.8238\n",
      "Epoch [2/2], Step [50270/67476], Loss: 4.3199\n",
      "Epoch [2/2], Step [50280/67476], Loss: 4.4250\n",
      "Epoch [2/2], Step [50290/67476], Loss: 4.5728\n",
      "Epoch [2/2], Step [50300/67476], Loss: 4.5610\n",
      "Epoch [2/2], Step [50310/67476], Loss: 4.3933\n",
      "Epoch [2/2], Step [50320/67476], Loss: 4.4267\n",
      "Epoch [2/2], Step [50330/67476], Loss: 4.5973\n",
      "Epoch [2/2], Step [50340/67476], Loss: 4.5253\n",
      "Epoch [2/2], Step [50350/67476], Loss: 4.5561\n",
      "Epoch [2/2], Step [50360/67476], Loss: 4.6417\n",
      "Epoch [2/2], Step [50370/67476], Loss: 4.6993\n",
      "Epoch [2/2], Step [50380/67476], Loss: 4.5752\n",
      "Epoch [2/2], Step [50390/67476], Loss: 4.6493\n",
      "Epoch [2/2], Step [50400/67476], Loss: 4.4982\n",
      "Epoch [2/2], Step [50410/67476], Loss: 4.3942\n",
      "Epoch [2/2], Step [50420/67476], Loss: 4.3145\n",
      "Epoch [2/2], Step [50430/67476], Loss: 4.5952\n",
      "Epoch [2/2], Step [50440/67476], Loss: 4.5140\n",
      "Epoch [2/2], Step [50450/67476], Loss: 4.6256\n",
      "Epoch [2/2], Step [50460/67476], Loss: 4.5030\n",
      "Epoch [2/2], Step [50470/67476], Loss: 4.5463\n",
      "Epoch [2/2], Step [50480/67476], Loss: 4.6003\n",
      "Epoch [2/2], Step [50490/67476], Loss: 4.6902\n",
      "Epoch [2/2], Step [50500/67476], Loss: 4.5412\n",
      "Epoch [2/2], Step [50510/67476], Loss: 4.6445\n",
      "Epoch [2/2], Step [50520/67476], Loss: 4.7209\n",
      "Epoch [2/2], Step [50530/67476], Loss: 4.5876\n",
      "Epoch [2/2], Step [50540/67476], Loss: 4.6089\n",
      "Epoch [2/2], Step [50550/67476], Loss: 4.6179\n",
      "Epoch [2/2], Step [50560/67476], Loss: 4.4327\n",
      "Epoch [2/2], Step [50570/67476], Loss: 4.4442\n",
      "Epoch [2/2], Step [50580/67476], Loss: 4.5533\n",
      "Epoch [2/2], Step [50590/67476], Loss: 4.6439\n",
      "Epoch [2/2], Step [50600/67476], Loss: 4.5388\n",
      "Epoch [2/2], Step [50610/67476], Loss: 4.5017\n",
      "Epoch [2/2], Step [50620/67476], Loss: 4.5623\n",
      "Epoch [2/2], Step [50630/67476], Loss: 4.6485\n",
      "Epoch [2/2], Step [50640/67476], Loss: 4.6229\n",
      "Epoch [2/2], Step [50650/67476], Loss: 4.4187\n",
      "Epoch [2/2], Step [50660/67476], Loss: 4.5930\n",
      "Epoch [2/2], Step [50670/67476], Loss: 4.6030\n",
      "Epoch [2/2], Step [50680/67476], Loss: 4.5973\n",
      "Epoch [2/2], Step [50690/67476], Loss: 4.4673\n",
      "Epoch [2/2], Step [50700/67476], Loss: 4.3985\n",
      "Epoch [2/2], Step [50710/67476], Loss: 4.4103\n",
      "Epoch [2/2], Step [50720/67476], Loss: 4.6199\n",
      "Epoch [2/2], Step [50730/67476], Loss: 4.6067\n",
      "Epoch [2/2], Step [50740/67476], Loss: 4.4605\n",
      "Epoch [2/2], Step [50750/67476], Loss: 4.3403\n",
      "Epoch [2/2], Step [50760/67476], Loss: 4.6606\n",
      "Epoch [2/2], Step [50770/67476], Loss: 4.4965\n",
      "Epoch [2/2], Step [50780/67476], Loss: 4.7638\n",
      "Epoch [2/2], Step [50790/67476], Loss: 4.4850\n",
      "Epoch [2/2], Step [50800/67476], Loss: 4.7765\n",
      "Epoch [2/2], Step [50810/67476], Loss: 4.3825\n",
      "Epoch [2/2], Step [50820/67476], Loss: 4.3612\n",
      "Epoch [2/2], Step [50830/67476], Loss: 4.5943\n",
      "Epoch [2/2], Step [50840/67476], Loss: 4.8640\n",
      "Epoch [2/2], Step [50850/67476], Loss: 4.3869\n",
      "Epoch [2/2], Step [50860/67476], Loss: 4.6585\n",
      "Epoch [2/2], Step [50870/67476], Loss: 4.7090\n",
      "Epoch [2/2], Step [50880/67476], Loss: 4.5679\n",
      "Epoch [2/2], Step [50890/67476], Loss: 4.6435\n",
      "Epoch [2/2], Step [50900/67476], Loss: 4.5119\n",
      "Epoch [2/2], Step [50910/67476], Loss: 4.4817\n",
      "Epoch [2/2], Step [50920/67476], Loss: 4.6387\n",
      "Epoch [2/2], Step [50930/67476], Loss: 4.3975\n",
      "Epoch [2/2], Step [50940/67476], Loss: 4.3484\n",
      "Epoch [2/2], Step [50950/67476], Loss: 4.6494\n",
      "Epoch [2/2], Step [50960/67476], Loss: 4.6125\n",
      "Epoch [2/2], Step [50970/67476], Loss: 4.2876\n",
      "Epoch [2/2], Step [50980/67476], Loss: 4.5757\n",
      "Epoch [2/2], Step [50990/67476], Loss: 4.6319\n",
      "Epoch [2/2], Step [51000/67476], Loss: 4.8054\n",
      "Epoch [2/2], Step [51010/67476], Loss: 4.6518\n",
      "Epoch [2/2], Step [51020/67476], Loss: 4.6497\n",
      "Epoch [2/2], Step [51030/67476], Loss: 4.4152\n",
      "Epoch [2/2], Step [51040/67476], Loss: 4.6526\n",
      "Epoch [2/2], Step [51050/67476], Loss: 4.7022\n",
      "Epoch [2/2], Step [51060/67476], Loss: 4.4032\n",
      "Epoch [2/2], Step [51070/67476], Loss: 4.5591\n",
      "Epoch [2/2], Step [51080/67476], Loss: 4.6776\n",
      "Epoch [2/2], Step [51090/67476], Loss: 4.5051\n",
      "Epoch [2/2], Step [51100/67476], Loss: 4.6988\n",
      "Epoch [2/2], Step [51110/67476], Loss: 4.5647\n",
      "Epoch [2/2], Step [51120/67476], Loss: 4.6523\n",
      "Epoch [2/2], Step [51130/67476], Loss: 4.3672\n",
      "Epoch [2/2], Step [51140/67476], Loss: 4.6174\n",
      "Epoch [2/2], Step [51150/67476], Loss: 4.5311\n",
      "Epoch [2/2], Step [51160/67476], Loss: 4.7019\n",
      "Epoch [2/2], Step [51170/67476], Loss: 4.4289\n",
      "Epoch [2/2], Step [51180/67476], Loss: 4.5411\n",
      "Epoch [2/2], Step [51190/67476], Loss: 4.4348\n",
      "Epoch [2/2], Step [51200/67476], Loss: 4.6677\n",
      "Epoch [2/2], Step [51210/67476], Loss: 4.4420\n",
      "Epoch [2/2], Step [51220/67476], Loss: 4.5108\n",
      "Epoch [2/2], Step [51230/67476], Loss: 4.4012\n",
      "Epoch [2/2], Step [51240/67476], Loss: 4.5011\n",
      "Epoch [2/2], Step [51250/67476], Loss: 4.4602\n",
      "Epoch [2/2], Step [51260/67476], Loss: 4.4339\n",
      "Epoch [2/2], Step [51270/67476], Loss: 4.7926\n",
      "Epoch [2/2], Step [51280/67476], Loss: 4.4570\n",
      "Epoch [2/2], Step [51290/67476], Loss: 4.5844\n",
      "Epoch [2/2], Step [51300/67476], Loss: 4.6672\n",
      "Epoch [2/2], Step [51310/67476], Loss: 4.6240\n",
      "Epoch [2/2], Step [51320/67476], Loss: 4.6321\n",
      "Epoch [2/2], Step [51330/67476], Loss: 4.5302\n",
      "Epoch [2/2], Step [51340/67476], Loss: 4.4782\n",
      "Epoch [2/2], Step [51350/67476], Loss: 4.5271\n",
      "Epoch [2/2], Step [51360/67476], Loss: 4.4506\n",
      "Epoch [2/2], Step [51370/67476], Loss: 4.6319\n",
      "Epoch [2/2], Step [51380/67476], Loss: 4.5918\n",
      "Epoch [2/2], Step [51390/67476], Loss: 4.5009\n",
      "Epoch [2/2], Step [51400/67476], Loss: 4.6407\n",
      "Epoch [2/2], Step [51410/67476], Loss: 4.5539\n",
      "Epoch [2/2], Step [51420/67476], Loss: 4.5458\n",
      "Epoch [2/2], Step [51430/67476], Loss: 4.3926\n",
      "Epoch [2/2], Step [51440/67476], Loss: 4.4839\n",
      "Epoch [2/2], Step [51450/67476], Loss: 4.6198\n",
      "Epoch [2/2], Step [51460/67476], Loss: 4.3913\n",
      "Epoch [2/2], Step [51470/67476], Loss: 4.5189\n",
      "Epoch [2/2], Step [51480/67476], Loss: 4.7365\n",
      "Epoch [2/2], Step [51490/67476], Loss: 4.4775\n",
      "Epoch [2/2], Step [51500/67476], Loss: 4.4158\n",
      "Epoch [2/2], Step [51510/67476], Loss: 4.6277\n",
      "Epoch [2/2], Step [51520/67476], Loss: 4.5715\n",
      "Epoch [2/2], Step [51530/67476], Loss: 4.6480\n",
      "Epoch [2/2], Step [51540/67476], Loss: 4.4742\n",
      "Epoch [2/2], Step [51550/67476], Loss: 4.6014\n",
      "Epoch [2/2], Step [51560/67476], Loss: 4.2511\n",
      "Epoch [2/2], Step [51570/67476], Loss: 4.5161\n",
      "Epoch [2/2], Step [51580/67476], Loss: 4.5504\n",
      "Epoch [2/2], Step [51590/67476], Loss: 4.6264\n",
      "Epoch [2/2], Step [51600/67476], Loss: 4.4593\n",
      "Epoch [2/2], Step [51610/67476], Loss: 4.4937\n",
      "Epoch [2/2], Step [51620/67476], Loss: 4.5834\n",
      "Epoch [2/2], Step [51630/67476], Loss: 4.6584\n",
      "Epoch [2/2], Step [51640/67476], Loss: 4.4081\n",
      "Epoch [2/2], Step [51650/67476], Loss: 4.4946\n",
      "Epoch [2/2], Step [51660/67476], Loss: 4.6040\n",
      "Epoch [2/2], Step [51670/67476], Loss: 4.4563\n",
      "Epoch [2/2], Step [51680/67476], Loss: 4.7413\n",
      "Epoch [2/2], Step [51690/67476], Loss: 4.6197\n",
      "Epoch [2/2], Step [51700/67476], Loss: 4.6582\n",
      "Epoch [2/2], Step [51710/67476], Loss: 4.5831\n",
      "Epoch [2/2], Step [51720/67476], Loss: 4.5361\n",
      "Epoch [2/2], Step [51730/67476], Loss: 4.6117\n",
      "Epoch [2/2], Step [51740/67476], Loss: 4.5732\n",
      "Epoch [2/2], Step [51750/67476], Loss: 4.6317\n",
      "Epoch [2/2], Step [51760/67476], Loss: 4.4267\n",
      "Epoch [2/2], Step [51770/67476], Loss: 4.4776\n",
      "Epoch [2/2], Step [51780/67476], Loss: 4.5757\n",
      "Epoch [2/2], Step [51790/67476], Loss: 4.5558\n",
      "Epoch [2/2], Step [51800/67476], Loss: 4.7343\n",
      "Epoch [2/2], Step [51810/67476], Loss: 4.8140\n",
      "Epoch [2/2], Step [51820/67476], Loss: 4.4814\n",
      "Epoch [2/2], Step [51830/67476], Loss: 4.4869\n",
      "Epoch [2/2], Step [51840/67476], Loss: 4.5579\n",
      "Epoch [2/2], Step [51850/67476], Loss: 4.5914\n",
      "Epoch [2/2], Step [51860/67476], Loss: 4.7200\n",
      "Epoch [2/2], Step [51870/67476], Loss: 4.3440\n",
      "Epoch [2/2], Step [51880/67476], Loss: 4.5664\n",
      "Epoch [2/2], Step [51890/67476], Loss: 4.3063\n",
      "Epoch [2/2], Step [51900/67476], Loss: 4.5071\n",
      "Epoch [2/2], Step [51910/67476], Loss: 4.4640\n",
      "Epoch [2/2], Step [51920/67476], Loss: 4.6496\n",
      "Epoch [2/2], Step [51930/67476], Loss: 4.6308\n",
      "Epoch [2/2], Step [51940/67476], Loss: 4.7076\n",
      "Epoch [2/2], Step [51950/67476], Loss: 4.4966\n",
      "Epoch [2/2], Step [51960/67476], Loss: 4.5723\n",
      "Epoch [2/2], Step [51970/67476], Loss: 4.6715\n",
      "Epoch [2/2], Step [51980/67476], Loss: 4.5341\n",
      "Epoch [2/2], Step [51990/67476], Loss: 4.5519\n",
      "Epoch [2/2], Step [52000/67476], Loss: 4.4403\n",
      "Epoch [2/2], Step [52010/67476], Loss: 4.5820\n",
      "Epoch [2/2], Step [52020/67476], Loss: 4.7153\n",
      "Epoch [2/2], Step [52030/67476], Loss: 4.5991\n",
      "Epoch [2/2], Step [52040/67476], Loss: 4.4956\n",
      "Epoch [2/2], Step [52050/67476], Loss: 4.5894\n",
      "Epoch [2/2], Step [52060/67476], Loss: 4.4820\n",
      "Epoch [2/2], Step [52070/67476], Loss: 4.5646\n",
      "Epoch [2/2], Step [52080/67476], Loss: 4.3721\n",
      "Epoch [2/2], Step [52090/67476], Loss: 4.4447\n",
      "Epoch [2/2], Step [52100/67476], Loss: 4.5924\n",
      "Epoch [2/2], Step [52110/67476], Loss: 4.5444\n",
      "Epoch [2/2], Step [52120/67476], Loss: 4.5138\n",
      "Epoch [2/2], Step [52130/67476], Loss: 4.5399\n",
      "Epoch [2/2], Step [52140/67476], Loss: 4.5384\n",
      "Epoch [2/2], Step [52150/67476], Loss: 4.8581\n",
      "Epoch [2/2], Step [52160/67476], Loss: 4.4777\n",
      "Epoch [2/2], Step [52170/67476], Loss: 4.4796\n",
      "Epoch [2/2], Step [52180/67476], Loss: 4.4685\n",
      "Epoch [2/2], Step [52190/67476], Loss: 4.7013\n",
      "Epoch [2/2], Step [52200/67476], Loss: 4.4332\n",
      "Epoch [2/2], Step [52210/67476], Loss: 4.4545\n",
      "Epoch [2/2], Step [52220/67476], Loss: 4.5832\n",
      "Epoch [2/2], Step [52230/67476], Loss: 4.5375\n",
      "Epoch [2/2], Step [52240/67476], Loss: 4.5067\n",
      "Epoch [2/2], Step [52250/67476], Loss: 4.4015\n",
      "Epoch [2/2], Step [52260/67476], Loss: 4.4809\n",
      "Epoch [2/2], Step [52270/67476], Loss: 4.3840\n",
      "Epoch [2/2], Step [52280/67476], Loss: 4.7030\n",
      "Epoch [2/2], Step [52290/67476], Loss: 4.5134\n",
      "Epoch [2/2], Step [52300/67476], Loss: 4.3909\n",
      "Epoch [2/2], Step [52310/67476], Loss: 4.5000\n",
      "Epoch [2/2], Step [52320/67476], Loss: 4.7863\n",
      "Epoch [2/2], Step [52330/67476], Loss: 4.4408\n",
      "Epoch [2/2], Step [52340/67476], Loss: 4.7866\n",
      "Epoch [2/2], Step [52350/67476], Loss: 4.5330\n",
      "Epoch [2/2], Step [52360/67476], Loss: 4.3906\n",
      "Epoch [2/2], Step [52370/67476], Loss: 4.6829\n",
      "Epoch [2/2], Step [52380/67476], Loss: 4.3138\n",
      "Epoch [2/2], Step [52390/67476], Loss: 4.5393\n",
      "Epoch [2/2], Step [52400/67476], Loss: 4.3843\n",
      "Epoch [2/2], Step [52410/67476], Loss: 4.5454\n",
      "Epoch [2/2], Step [52420/67476], Loss: 4.7777\n",
      "Epoch [2/2], Step [52430/67476], Loss: 4.5962\n",
      "Epoch [2/2], Step [52440/67476], Loss: 4.4406\n",
      "Epoch [2/2], Step [52450/67476], Loss: 4.5409\n",
      "Epoch [2/2], Step [52460/67476], Loss: 4.4702\n",
      "Epoch [2/2], Step [52470/67476], Loss: 4.4896\n",
      "Epoch [2/2], Step [52480/67476], Loss: 4.4406\n",
      "Epoch [2/2], Step [52490/67476], Loss: 4.4532\n",
      "Epoch [2/2], Step [52500/67476], Loss: 4.4669\n",
      "Epoch [2/2], Step [52510/67476], Loss: 4.5098\n",
      "Epoch [2/2], Step [52520/67476], Loss: 4.5450\n",
      "Epoch [2/2], Step [52530/67476], Loss: 4.6392\n",
      "Epoch [2/2], Step [52540/67476], Loss: 4.4951\n",
      "Epoch [2/2], Step [52550/67476], Loss: 4.5692\n",
      "Epoch [2/2], Step [52560/67476], Loss: 4.5212\n",
      "Epoch [2/2], Step [52570/67476], Loss: 4.6834\n",
      "Epoch [2/2], Step [52580/67476], Loss: 4.5896\n",
      "Epoch [2/2], Step [52590/67476], Loss: 4.5038\n",
      "Epoch [2/2], Step [52600/67476], Loss: 4.4863\n",
      "Epoch [2/2], Step [52610/67476], Loss: 4.4958\n",
      "Epoch [2/2], Step [52620/67476], Loss: 4.5012\n",
      "Epoch [2/2], Step [52630/67476], Loss: 4.3768\n",
      "Epoch [2/2], Step [52640/67476], Loss: 4.6367\n",
      "Epoch [2/2], Step [52650/67476], Loss: 4.4498\n",
      "Epoch [2/2], Step [52660/67476], Loss: 4.5398\n",
      "Epoch [2/2], Step [52670/67476], Loss: 4.4839\n",
      "Epoch [2/2], Step [52680/67476], Loss: 4.5128\n",
      "Epoch [2/2], Step [52690/67476], Loss: 4.6190\n",
      "Epoch [2/2], Step [52700/67476], Loss: 4.4096\n",
      "Epoch [2/2], Step [52710/67476], Loss: 4.5080\n",
      "Epoch [2/2], Step [52720/67476], Loss: 4.4930\n",
      "Epoch [2/2], Step [52730/67476], Loss: 4.4016\n",
      "Epoch [2/2], Step [52740/67476], Loss: 4.6592\n",
      "Epoch [2/2], Step [52750/67476], Loss: 4.6272\n",
      "Epoch [2/2], Step [52760/67476], Loss: 4.5216\n",
      "Epoch [2/2], Step [52770/67476], Loss: 4.5068\n",
      "Epoch [2/2], Step [52780/67476], Loss: 4.4180\n",
      "Epoch [2/2], Step [52790/67476], Loss: 4.5023\n",
      "Epoch [2/2], Step [52800/67476], Loss: 4.4549\n",
      "Epoch [2/2], Step [52810/67476], Loss: 4.5040\n",
      "Epoch [2/2], Step [52820/67476], Loss: 4.6750\n",
      "Epoch [2/2], Step [52830/67476], Loss: 4.4495\n",
      "Epoch [2/2], Step [52840/67476], Loss: 4.3787\n",
      "Epoch [2/2], Step [52850/67476], Loss: 4.4749\n",
      "Epoch [2/2], Step [52860/67476], Loss: 4.4166\n",
      "Epoch [2/2], Step [52870/67476], Loss: 4.4484\n",
      "Epoch [2/2], Step [52880/67476], Loss: 4.5420\n",
      "Epoch [2/2], Step [52890/67476], Loss: 4.3445\n",
      "Epoch [2/2], Step [52900/67476], Loss: 4.4232\n",
      "Epoch [2/2], Step [52910/67476], Loss: 4.3309\n",
      "Epoch [2/2], Step [52920/67476], Loss: 4.4765\n",
      "Epoch [2/2], Step [52930/67476], Loss: 4.6174\n",
      "Epoch [2/2], Step [52940/67476], Loss: 4.3745\n",
      "Epoch [2/2], Step [52950/67476], Loss: 4.7589\n",
      "Epoch [2/2], Step [52960/67476], Loss: 4.6312\n",
      "Epoch [2/2], Step [52970/67476], Loss: 4.3751\n",
      "Epoch [2/2], Step [52980/67476], Loss: 4.4775\n",
      "Epoch [2/2], Step [52990/67476], Loss: 4.5892\n",
      "Epoch [2/2], Step [53000/67476], Loss: 4.6880\n",
      "Epoch [2/2], Step [53010/67476], Loss: 4.5931\n",
      "Epoch [2/2], Step [53020/67476], Loss: 4.5458\n",
      "Epoch [2/2], Step [53030/67476], Loss: 4.3334\n",
      "Epoch [2/2], Step [53040/67476], Loss: 4.3866\n",
      "Epoch [2/2], Step [53050/67476], Loss: 4.5751\n",
      "Epoch [2/2], Step [53060/67476], Loss: 4.4186\n",
      "Epoch [2/2], Step [53070/67476], Loss: 4.5739\n",
      "Epoch [2/2], Step [53080/67476], Loss: 4.4475\n",
      "Epoch [2/2], Step [53090/67476], Loss: 4.5473\n",
      "Epoch [2/2], Step [53100/67476], Loss: 4.6736\n",
      "Epoch [2/2], Step [53110/67476], Loss: 4.5007\n",
      "Epoch [2/2], Step [53120/67476], Loss: 4.5233\n",
      "Epoch [2/2], Step [53130/67476], Loss: 4.4320\n",
      "Epoch [2/2], Step [53140/67476], Loss: 4.5216\n",
      "Epoch [2/2], Step [53150/67476], Loss: 4.7681\n",
      "Epoch [2/2], Step [53160/67476], Loss: 4.5357\n",
      "Epoch [2/2], Step [53170/67476], Loss: 4.5220\n",
      "Epoch [2/2], Step [53180/67476], Loss: 4.5209\n",
      "Epoch [2/2], Step [53190/67476], Loss: 4.6102\n",
      "Epoch [2/2], Step [53200/67476], Loss: 4.4585\n",
      "Epoch [2/2], Step [53210/67476], Loss: 4.5061\n",
      "Epoch [2/2], Step [53220/67476], Loss: 4.2418\n",
      "Epoch [2/2], Step [53230/67476], Loss: 4.4760\n",
      "Epoch [2/2], Step [53240/67476], Loss: 4.8215\n",
      "Epoch [2/2], Step [53250/67476], Loss: 4.6145\n",
      "Epoch [2/2], Step [53260/67476], Loss: 4.4283\n",
      "Epoch [2/2], Step [53270/67476], Loss: 4.4921\n",
      "Epoch [2/2], Step [53280/67476], Loss: 4.4746\n",
      "Epoch [2/2], Step [53290/67476], Loss: 4.6256\n",
      "Epoch [2/2], Step [53300/67476], Loss: 4.5549\n",
      "Epoch [2/2], Step [53310/67476], Loss: 4.6605\n",
      "Epoch [2/2], Step [53320/67476], Loss: 4.5205\n",
      "Epoch [2/2], Step [53330/67476], Loss: 4.6389\n",
      "Epoch [2/2], Step [53340/67476], Loss: 4.5297\n",
      "Epoch [2/2], Step [53350/67476], Loss: 4.5170\n",
      "Epoch [2/2], Step [53360/67476], Loss: 4.5446\n",
      "Epoch [2/2], Step [53370/67476], Loss: 4.4377\n",
      "Epoch [2/2], Step [53380/67476], Loss: 4.5629\n",
      "Epoch [2/2], Step [53390/67476], Loss: 4.5658\n",
      "Epoch [2/2], Step [53400/67476], Loss: 4.5887\n",
      "Epoch [2/2], Step [53410/67476], Loss: 4.5729\n",
      "Epoch [2/2], Step [53420/67476], Loss: 4.5783\n",
      "Epoch [2/2], Step [53430/67476], Loss: 4.4339\n",
      "Epoch [2/2], Step [53440/67476], Loss: 4.6014\n",
      "Epoch [2/2], Step [53450/67476], Loss: 4.6777\n",
      "Epoch [2/2], Step [53460/67476], Loss: 4.4024\n",
      "Epoch [2/2], Step [53470/67476], Loss: 4.3467\n",
      "Epoch [2/2], Step [53480/67476], Loss: 4.5578\n",
      "Epoch [2/2], Step [53490/67476], Loss: 4.5742\n",
      "Epoch [2/2], Step [53500/67476], Loss: 4.7168\n",
      "Epoch [2/2], Step [53510/67476], Loss: 4.6609\n",
      "Epoch [2/2], Step [53520/67476], Loss: 4.3901\n",
      "Epoch [2/2], Step [53530/67476], Loss: 4.3167\n",
      "Epoch [2/2], Step [53540/67476], Loss: 4.7458\n",
      "Epoch [2/2], Step [53550/67476], Loss: 4.5236\n",
      "Epoch [2/2], Step [53560/67476], Loss: 4.6329\n",
      "Epoch [2/2], Step [53570/67476], Loss: 4.5975\n",
      "Epoch [2/2], Step [53580/67476], Loss: 4.7898\n",
      "Epoch [2/2], Step [53590/67476], Loss: 4.2643\n",
      "Epoch [2/2], Step [53600/67476], Loss: 4.4969\n",
      "Epoch [2/2], Step [53610/67476], Loss: 4.4621\n",
      "Epoch [2/2], Step [53620/67476], Loss: 4.3375\n",
      "Epoch [2/2], Step [53630/67476], Loss: 4.4214\n",
      "Epoch [2/2], Step [53640/67476], Loss: 4.5795\n",
      "Epoch [2/2], Step [53650/67476], Loss: 4.5691\n",
      "Epoch [2/2], Step [53660/67476], Loss: 4.7566\n",
      "Epoch [2/2], Step [53670/67476], Loss: 4.7390\n",
      "Epoch [2/2], Step [53680/67476], Loss: 4.5184\n",
      "Epoch [2/2], Step [53690/67476], Loss: 4.4926\n",
      "Epoch [2/2], Step [53700/67476], Loss: 4.4734\n",
      "Epoch [2/2], Step [53710/67476], Loss: 4.5906\n",
      "Epoch [2/2], Step [53720/67476], Loss: 4.5788\n",
      "Epoch [2/2], Step [53730/67476], Loss: 4.5634\n",
      "Epoch [2/2], Step [53740/67476], Loss: 4.5055\n",
      "Epoch [2/2], Step [53750/67476], Loss: 4.5444\n",
      "Epoch [2/2], Step [53760/67476], Loss: 4.7085\n",
      "Epoch [2/2], Step [53770/67476], Loss: 4.2445\n",
      "Epoch [2/2], Step [53780/67476], Loss: 4.3656\n",
      "Epoch [2/2], Step [53790/67476], Loss: 4.5518\n",
      "Epoch [2/2], Step [53800/67476], Loss: 4.6021\n",
      "Epoch [2/2], Step [53810/67476], Loss: 4.5616\n",
      "Epoch [2/2], Step [53820/67476], Loss: 4.3653\n",
      "Epoch [2/2], Step [53830/67476], Loss: 4.6045\n",
      "Epoch [2/2], Step [53840/67476], Loss: 4.4998\n",
      "Epoch [2/2], Step [53850/67476], Loss: 4.6053\n",
      "Epoch [2/2], Step [53860/67476], Loss: 4.6916\n",
      "Epoch [2/2], Step [53870/67476], Loss: 4.6016\n",
      "Epoch [2/2], Step [53880/67476], Loss: 4.4258\n",
      "Epoch [2/2], Step [53890/67476], Loss: 4.5913\n",
      "Epoch [2/2], Step [53900/67476], Loss: 4.6478\n",
      "Epoch [2/2], Step [53910/67476], Loss: 4.4195\n",
      "Epoch [2/2], Step [53920/67476], Loss: 4.3966\n",
      "Epoch [2/2], Step [53930/67476], Loss: 4.6086\n",
      "Epoch [2/2], Step [53940/67476], Loss: 4.6551\n",
      "Epoch [2/2], Step [53950/67476], Loss: 4.5140\n",
      "Epoch [2/2], Step [53960/67476], Loss: 4.5603\n",
      "Epoch [2/2], Step [53970/67476], Loss: 4.6198\n",
      "Epoch [2/2], Step [53980/67476], Loss: 4.6859\n",
      "Epoch [2/2], Step [53990/67476], Loss: 4.5472\n",
      "Epoch [2/2], Step [54000/67476], Loss: 4.4479\n",
      "Epoch [2/2], Step [54010/67476], Loss: 4.4138\n",
      "Epoch [2/2], Step [54020/67476], Loss: 4.3221\n",
      "Epoch [2/2], Step [54030/67476], Loss: 4.6708\n",
      "Epoch [2/2], Step [54040/67476], Loss: 4.3918\n",
      "Epoch [2/2], Step [54050/67476], Loss: 4.3199\n",
      "Epoch [2/2], Step [54060/67476], Loss: 4.6110\n",
      "Epoch [2/2], Step [54070/67476], Loss: 4.4792\n",
      "Epoch [2/2], Step [54080/67476], Loss: 4.5076\n",
      "Epoch [2/2], Step [54090/67476], Loss: 4.5028\n",
      "Epoch [2/2], Step [54100/67476], Loss: 4.6297\n",
      "Epoch [2/2], Step [54110/67476], Loss: 4.5299\n",
      "Epoch [2/2], Step [54120/67476], Loss: 4.3878\n",
      "Epoch [2/2], Step [54130/67476], Loss: 4.6384\n",
      "Epoch [2/2], Step [54140/67476], Loss: 4.4149\n",
      "Epoch [2/2], Step [54150/67476], Loss: 4.7967\n",
      "Epoch [2/2], Step [54160/67476], Loss: 4.6108\n",
      "Epoch [2/2], Step [54170/67476], Loss: 4.5196\n",
      "Epoch [2/2], Step [54180/67476], Loss: 4.3264\n",
      "Epoch [2/2], Step [54190/67476], Loss: 4.6004\n",
      "Epoch [2/2], Step [54200/67476], Loss: 4.4183\n",
      "Epoch [2/2], Step [54210/67476], Loss: 4.4533\n",
      "Epoch [2/2], Step [54220/67476], Loss: 4.6909\n",
      "Epoch [2/2], Step [54230/67476], Loss: 4.6288\n",
      "Epoch [2/2], Step [54240/67476], Loss: 4.5329\n",
      "Epoch [2/2], Step [54250/67476], Loss: 4.4917\n",
      "Epoch [2/2], Step [54260/67476], Loss: 4.3221\n",
      "Epoch [2/2], Step [54270/67476], Loss: 4.7008\n",
      "Epoch [2/2], Step [54280/67476], Loss: 4.4513\n",
      "Epoch [2/2], Step [54290/67476], Loss: 4.5576\n",
      "Epoch [2/2], Step [54300/67476], Loss: 4.3806\n",
      "Epoch [2/2], Step [54310/67476], Loss: 4.5670\n",
      "Epoch [2/2], Step [54320/67476], Loss: 4.5032\n",
      "Epoch [2/2], Step [54330/67476], Loss: 4.2443\n",
      "Epoch [2/2], Step [54340/67476], Loss: 4.6659\n",
      "Epoch [2/2], Step [54350/67476], Loss: 4.5413\n",
      "Epoch [2/2], Step [54360/67476], Loss: 4.6561\n",
      "Epoch [2/2], Step [54370/67476], Loss: 4.7151\n",
      "Epoch [2/2], Step [54380/67476], Loss: 4.5019\n",
      "Epoch [2/2], Step [54390/67476], Loss: 4.4985\n",
      "Epoch [2/2], Step [54400/67476], Loss: 4.3950\n",
      "Epoch [2/2], Step [54410/67476], Loss: 4.4977\n",
      "Epoch [2/2], Step [54420/67476], Loss: 4.7343\n",
      "Epoch [2/2], Step [54430/67476], Loss: 4.5410\n",
      "Epoch [2/2], Step [54440/67476], Loss: 4.4637\n",
      "Epoch [2/2], Step [54450/67476], Loss: 4.7935\n",
      "Epoch [2/2], Step [54460/67476], Loss: 4.5292\n",
      "Epoch [2/2], Step [54470/67476], Loss: 4.7704\n",
      "Epoch [2/2], Step [54480/67476], Loss: 4.5985\n",
      "Epoch [2/2], Step [54490/67476], Loss: 4.5994\n",
      "Epoch [2/2], Step [54500/67476], Loss: 4.6305\n",
      "Epoch [2/2], Step [54510/67476], Loss: 4.6286\n",
      "Epoch [2/2], Step [54520/67476], Loss: 4.4043\n",
      "Epoch [2/2], Step [54530/67476], Loss: 4.6673\n",
      "Epoch [2/2], Step [54540/67476], Loss: 4.4547\n",
      "Epoch [2/2], Step [54550/67476], Loss: 4.5077\n",
      "Epoch [2/2], Step [54560/67476], Loss: 4.4272\n",
      "Epoch [2/2], Step [54570/67476], Loss: 4.7122\n",
      "Epoch [2/2], Step [54580/67476], Loss: 4.6915\n",
      "Epoch [2/2], Step [54590/67476], Loss: 4.5164\n",
      "Epoch [2/2], Step [54600/67476], Loss: 4.6324\n",
      "Epoch [2/2], Step [54610/67476], Loss: 4.5444\n",
      "Epoch [2/2], Step [54620/67476], Loss: 4.5323\n",
      "Epoch [2/2], Step [54630/67476], Loss: 4.7399\n",
      "Epoch [2/2], Step [54640/67476], Loss: 4.4957\n",
      "Epoch [2/2], Step [54650/67476], Loss: 4.5326\n",
      "Epoch [2/2], Step [54660/67476], Loss: 4.4361\n",
      "Epoch [2/2], Step [54670/67476], Loss: 4.6516\n",
      "Epoch [2/2], Step [54680/67476], Loss: 4.3938\n",
      "Epoch [2/2], Step [54690/67476], Loss: 4.3744\n",
      "Epoch [2/2], Step [54700/67476], Loss: 4.3987\n",
      "Epoch [2/2], Step [54710/67476], Loss: 4.6347\n",
      "Epoch [2/2], Step [54720/67476], Loss: 4.4594\n",
      "Epoch [2/2], Step [54730/67476], Loss: 4.5365\n",
      "Epoch [2/2], Step [54740/67476], Loss: 4.4364\n",
      "Epoch [2/2], Step [54750/67476], Loss: 4.5325\n",
      "Epoch [2/2], Step [54760/67476], Loss: 4.5051\n",
      "Epoch [2/2], Step [54770/67476], Loss: 4.5641\n",
      "Epoch [2/2], Step [54780/67476], Loss: 4.5900\n",
      "Epoch [2/2], Step [54790/67476], Loss: 4.6220\n",
      "Epoch [2/2], Step [54800/67476], Loss: 4.5330\n",
      "Epoch [2/2], Step [54810/67476], Loss: 4.4284\n",
      "Epoch [2/2], Step [54820/67476], Loss: 4.5805\n",
      "Epoch [2/2], Step [54830/67476], Loss: 4.4861\n",
      "Epoch [2/2], Step [54840/67476], Loss: 4.3678\n",
      "Epoch [2/2], Step [54850/67476], Loss: 4.4711\n",
      "Epoch [2/2], Step [54860/67476], Loss: 4.6857\n",
      "Epoch [2/2], Step [54870/67476], Loss: 4.4340\n",
      "Epoch [2/2], Step [54880/67476], Loss: 4.5327\n",
      "Epoch [2/2], Step [54890/67476], Loss: 4.7192\n",
      "Epoch [2/2], Step [54900/67476], Loss: 4.5890\n",
      "Epoch [2/2], Step [54910/67476], Loss: 4.4430\n",
      "Epoch [2/2], Step [54920/67476], Loss: 4.4904\n",
      "Epoch [2/2], Step [54930/67476], Loss: 4.5960\n",
      "Epoch [2/2], Step [54940/67476], Loss: 4.3166\n",
      "Epoch [2/2], Step [54950/67476], Loss: 4.3308\n",
      "Epoch [2/2], Step [54960/67476], Loss: 4.3437\n",
      "Epoch [2/2], Step [54970/67476], Loss: 4.5733\n",
      "Epoch [2/2], Step [54980/67476], Loss: 4.5888\n",
      "Epoch [2/2], Step [54990/67476], Loss: 4.6998\n",
      "Epoch [2/2], Step [55000/67476], Loss: 4.4700\n",
      "Epoch [2/2], Step [55010/67476], Loss: 4.6441\n",
      "Epoch [2/2], Step [55020/67476], Loss: 4.6248\n",
      "Epoch [2/2], Step [55030/67476], Loss: 4.4239\n",
      "Epoch [2/2], Step [55040/67476], Loss: 4.4305\n",
      "Epoch [2/2], Step [55050/67476], Loss: 4.6063\n",
      "Epoch [2/2], Step [55060/67476], Loss: 4.5717\n",
      "Epoch [2/2], Step [55070/67476], Loss: 4.4863\n",
      "Epoch [2/2], Step [55080/67476], Loss: 4.3493\n",
      "Epoch [2/2], Step [55090/67476], Loss: 4.5169\n",
      "Epoch [2/2], Step [55100/67476], Loss: 4.4560\n",
      "Epoch [2/2], Step [55110/67476], Loss: 4.6659\n",
      "Epoch [2/2], Step [55120/67476], Loss: 4.6596\n",
      "Epoch [2/2], Step [55130/67476], Loss: 4.6277\n",
      "Epoch [2/2], Step [55140/67476], Loss: 4.5501\n",
      "Epoch [2/2], Step [55150/67476], Loss: 4.6553\n",
      "Epoch [2/2], Step [55160/67476], Loss: 4.3963\n",
      "Epoch [2/2], Step [55170/67476], Loss: 4.6212\n",
      "Epoch [2/2], Step [55180/67476], Loss: 4.5076\n",
      "Epoch [2/2], Step [55190/67476], Loss: 4.4304\n",
      "Epoch [2/2], Step [55200/67476], Loss: 4.7429\n",
      "Epoch [2/2], Step [55210/67476], Loss: 4.4696\n",
      "Epoch [2/2], Step [55220/67476], Loss: 4.4881\n",
      "Epoch [2/2], Step [55230/67476], Loss: 4.5260\n",
      "Epoch [2/2], Step [55240/67476], Loss: 4.4005\n",
      "Epoch [2/2], Step [55250/67476], Loss: 4.6747\n",
      "Epoch [2/2], Step [55260/67476], Loss: 4.7108\n",
      "Epoch [2/2], Step [55270/67476], Loss: 4.4941\n",
      "Epoch [2/2], Step [55280/67476], Loss: 4.5418\n",
      "Epoch [2/2], Step [55290/67476], Loss: 4.4872\n",
      "Epoch [2/2], Step [55300/67476], Loss: 4.4046\n",
      "Epoch [2/2], Step [55310/67476], Loss: 4.5391\n",
      "Epoch [2/2], Step [55320/67476], Loss: 4.5462\n",
      "Epoch [2/2], Step [55330/67476], Loss: 4.5145\n",
      "Epoch [2/2], Step [55340/67476], Loss: 4.5682\n",
      "Epoch [2/2], Step [55350/67476], Loss: 4.5177\n",
      "Epoch [2/2], Step [55360/67476], Loss: 4.4985\n",
      "Epoch [2/2], Step [55370/67476], Loss: 4.3046\n",
      "Epoch [2/2], Step [55380/67476], Loss: 4.7134\n",
      "Epoch [2/2], Step [55390/67476], Loss: 4.5195\n",
      "Epoch [2/2], Step [55400/67476], Loss: 4.4773\n",
      "Epoch [2/2], Step [55410/67476], Loss: 4.8114\n",
      "Epoch [2/2], Step [55420/67476], Loss: 4.3038\n",
      "Epoch [2/2], Step [55430/67476], Loss: 4.5934\n",
      "Epoch [2/2], Step [55440/67476], Loss: 4.4546\n",
      "Epoch [2/2], Step [55450/67476], Loss: 4.3060\n",
      "Epoch [2/2], Step [55460/67476], Loss: 4.6891\n",
      "Epoch [2/2], Step [55470/67476], Loss: 4.7310\n",
      "Epoch [2/2], Step [55480/67476], Loss: 4.8193\n",
      "Epoch [2/2], Step [55490/67476], Loss: 4.6114\n",
      "Epoch [2/2], Step [55500/67476], Loss: 4.6013\n",
      "Epoch [2/2], Step [55510/67476], Loss: 4.5499\n",
      "Epoch [2/2], Step [55520/67476], Loss: 4.5352\n",
      "Epoch [2/2], Step [55530/67476], Loss: 4.6206\n",
      "Epoch [2/2], Step [55540/67476], Loss: 4.5346\n",
      "Epoch [2/2], Step [55550/67476], Loss: 4.5120\n",
      "Epoch [2/2], Step [55560/67476], Loss: 4.4409\n",
      "Epoch [2/2], Step [55570/67476], Loss: 4.5452\n",
      "Epoch [2/2], Step [55580/67476], Loss: 4.5570\n",
      "Epoch [2/2], Step [55590/67476], Loss: 4.6339\n",
      "Epoch [2/2], Step [55600/67476], Loss: 4.6399\n",
      "Epoch [2/2], Step [55610/67476], Loss: 4.6349\n",
      "Epoch [2/2], Step [55620/67476], Loss: 4.5301\n",
      "Epoch [2/2], Step [55630/67476], Loss: 4.3382\n",
      "Epoch [2/2], Step [55640/67476], Loss: 4.6277\n",
      "Epoch [2/2], Step [55650/67476], Loss: 4.4854\n",
      "Epoch [2/2], Step [55660/67476], Loss: 4.4136\n",
      "Epoch [2/2], Step [55670/67476], Loss: 4.6906\n",
      "Epoch [2/2], Step [55680/67476], Loss: 4.6309\n",
      "Epoch [2/2], Step [55690/67476], Loss: 4.7799\n",
      "Epoch [2/2], Step [55700/67476], Loss: 4.4712\n",
      "Epoch [2/2], Step [55710/67476], Loss: 4.4139\n",
      "Epoch [2/2], Step [55720/67476], Loss: 4.4145\n",
      "Epoch [2/2], Step [55730/67476], Loss: 4.4230\n",
      "Epoch [2/2], Step [55740/67476], Loss: 4.4310\n",
      "Epoch [2/2], Step [55750/67476], Loss: 4.6645\n",
      "Epoch [2/2], Step [55760/67476], Loss: 4.5367\n",
      "Epoch [2/2], Step [55770/67476], Loss: 4.6740\n",
      "Epoch [2/2], Step [55780/67476], Loss: 4.4160\n",
      "Epoch [2/2], Step [55790/67476], Loss: 4.4888\n",
      "Epoch [2/2], Step [55800/67476], Loss: 4.5082\n",
      "Epoch [2/2], Step [55810/67476], Loss: 4.7450\n",
      "Epoch [2/2], Step [55820/67476], Loss: 4.3832\n",
      "Epoch [2/2], Step [55830/67476], Loss: 4.7696\n",
      "Epoch [2/2], Step [55840/67476], Loss: 4.5728\n",
      "Epoch [2/2], Step [55850/67476], Loss: 4.4188\n",
      "Epoch [2/2], Step [55860/67476], Loss: 4.5864\n",
      "Epoch [2/2], Step [55870/67476], Loss: 4.5414\n",
      "Epoch [2/2], Step [55880/67476], Loss: 4.3853\n",
      "Epoch [2/2], Step [55890/67476], Loss: 4.5631\n",
      "Epoch [2/2], Step [55900/67476], Loss: 4.6006\n",
      "Epoch [2/2], Step [55910/67476], Loss: 4.7108\n",
      "Epoch [2/2], Step [55920/67476], Loss: 4.7134\n",
      "Epoch [2/2], Step [55930/67476], Loss: 4.6150\n",
      "Epoch [2/2], Step [55940/67476], Loss: 4.5216\n",
      "Epoch [2/2], Step [55950/67476], Loss: 4.5293\n",
      "Epoch [2/2], Step [55960/67476], Loss: 4.5348\n",
      "Epoch [2/2], Step [55970/67476], Loss: 4.5493\n",
      "Epoch [2/2], Step [55980/67476], Loss: 4.5899\n",
      "Epoch [2/2], Step [55990/67476], Loss: 4.5936\n",
      "Epoch [2/2], Step [56000/67476], Loss: 4.3096\n",
      "Epoch [2/2], Step [56010/67476], Loss: 4.4029\n",
      "Epoch [2/2], Step [56020/67476], Loss: 4.6668\n",
      "Epoch [2/2], Step [56030/67476], Loss: 4.4667\n",
      "Epoch [2/2], Step [56040/67476], Loss: 4.4984\n",
      "Epoch [2/2], Step [56050/67476], Loss: 4.4261\n",
      "Epoch [2/2], Step [56060/67476], Loss: 4.5683\n",
      "Epoch [2/2], Step [56070/67476], Loss: 4.5123\n",
      "Epoch [2/2], Step [56080/67476], Loss: 4.5146\n",
      "Epoch [2/2], Step [56090/67476], Loss: 4.5202\n",
      "Epoch [2/2], Step [56100/67476], Loss: 4.4096\n",
      "Epoch [2/2], Step [56110/67476], Loss: 4.4026\n",
      "Epoch [2/2], Step [56120/67476], Loss: 4.6976\n",
      "Epoch [2/2], Step [56130/67476], Loss: 4.5688\n",
      "Epoch [2/2], Step [56140/67476], Loss: 4.5008\n",
      "Epoch [2/2], Step [56150/67476], Loss: 4.5454\n",
      "Epoch [2/2], Step [56160/67476], Loss: 4.5316\n",
      "Epoch [2/2], Step [56170/67476], Loss: 4.6332\n",
      "Epoch [2/2], Step [56180/67476], Loss: 4.4334\n",
      "Epoch [2/2], Step [56190/67476], Loss: 4.5037\n",
      "Epoch [2/2], Step [56200/67476], Loss: 4.4301\n",
      "Epoch [2/2], Step [56210/67476], Loss: 4.6657\n",
      "Epoch [2/2], Step [56220/67476], Loss: 4.4914\n",
      "Epoch [2/2], Step [56230/67476], Loss: 4.5401\n",
      "Epoch [2/2], Step [56240/67476], Loss: 4.6236\n",
      "Epoch [2/2], Step [56250/67476], Loss: 4.6799\n",
      "Epoch [2/2], Step [56260/67476], Loss: 4.5160\n",
      "Epoch [2/2], Step [56270/67476], Loss: 4.4775\n",
      "Epoch [2/2], Step [56280/67476], Loss: 4.5426\n",
      "Epoch [2/2], Step [56290/67476], Loss: 4.8065\n",
      "Epoch [2/2], Step [56300/67476], Loss: 4.7107\n",
      "Epoch [2/2], Step [56310/67476], Loss: 4.5390\n",
      "Epoch [2/2], Step [56320/67476], Loss: 4.5028\n",
      "Epoch [2/2], Step [56330/67476], Loss: 4.5652\n",
      "Epoch [2/2], Step [56340/67476], Loss: 4.5881\n",
      "Epoch [2/2], Step [56350/67476], Loss: 4.4069\n",
      "Epoch [2/2], Step [56360/67476], Loss: 4.3028\n",
      "Epoch [2/2], Step [56370/67476], Loss: 4.4835\n",
      "Epoch [2/2], Step [56380/67476], Loss: 4.6006\n",
      "Epoch [2/2], Step [56390/67476], Loss: 4.5114\n",
      "Epoch [2/2], Step [56400/67476], Loss: 4.5671\n",
      "Epoch [2/2], Step [56410/67476], Loss: 4.5364\n",
      "Epoch [2/2], Step [56420/67476], Loss: 4.4495\n",
      "Epoch [2/2], Step [56430/67476], Loss: 4.4717\n",
      "Epoch [2/2], Step [56440/67476], Loss: 4.4951\n",
      "Epoch [2/2], Step [56450/67476], Loss: 4.5432\n",
      "Epoch [2/2], Step [56460/67476], Loss: 4.6643\n",
      "Epoch [2/2], Step [56470/67476], Loss: 4.5306\n",
      "Epoch [2/2], Step [56480/67476], Loss: 4.4694\n",
      "Epoch [2/2], Step [56490/67476], Loss: 4.4732\n",
      "Epoch [2/2], Step [56500/67476], Loss: 4.3979\n",
      "Epoch [2/2], Step [56510/67476], Loss: 4.8190\n",
      "Epoch [2/2], Step [56520/67476], Loss: 4.3609\n",
      "Epoch [2/2], Step [56530/67476], Loss: 4.7197\n",
      "Epoch [2/2], Step [56540/67476], Loss: 4.6233\n",
      "Epoch [2/2], Step [56550/67476], Loss: 4.3561\n",
      "Epoch [2/2], Step [56560/67476], Loss: 4.4222\n",
      "Epoch [2/2], Step [56570/67476], Loss: 4.4690\n",
      "Epoch [2/2], Step [56580/67476], Loss: 4.4136\n",
      "Epoch [2/2], Step [56590/67476], Loss: 4.4405\n",
      "Epoch [2/2], Step [56600/67476], Loss: 4.5585\n",
      "Epoch [2/2], Step [56610/67476], Loss: 4.5082\n",
      "Epoch [2/2], Step [56620/67476], Loss: 4.7031\n",
      "Epoch [2/2], Step [56630/67476], Loss: 4.3792\n",
      "Epoch [2/2], Step [56640/67476], Loss: 4.4854\n",
      "Epoch [2/2], Step [56650/67476], Loss: 4.3662\n",
      "Epoch [2/2], Step [56660/67476], Loss: 4.7210\n",
      "Epoch [2/2], Step [56670/67476], Loss: 4.5767\n",
      "Epoch [2/2], Step [56680/67476], Loss: 4.6052\n",
      "Epoch [2/2], Step [56690/67476], Loss: 4.5120\n",
      "Epoch [2/2], Step [56700/67476], Loss: 4.5426\n",
      "Epoch [2/2], Step [56710/67476], Loss: 4.6071\n",
      "Epoch [2/2], Step [56720/67476], Loss: 4.6223\n",
      "Epoch [2/2], Step [56730/67476], Loss: 4.3767\n",
      "Epoch [2/2], Step [56740/67476], Loss: 4.4187\n",
      "Epoch [2/2], Step [56750/67476], Loss: 4.2877\n",
      "Epoch [2/2], Step [56760/67476], Loss: 4.6890\n",
      "Epoch [2/2], Step [56770/67476], Loss: 4.6428\n",
      "Epoch [2/2], Step [56780/67476], Loss: 4.5534\n",
      "Epoch [2/2], Step [56790/67476], Loss: 4.6577\n",
      "Epoch [2/2], Step [56800/67476], Loss: 4.5543\n",
      "Epoch [2/2], Step [56810/67476], Loss: 4.3199\n",
      "Epoch [2/2], Step [56820/67476], Loss: 4.4313\n",
      "Epoch [2/2], Step [56830/67476], Loss: 4.4152\n",
      "Epoch [2/2], Step [56840/67476], Loss: 4.6097\n",
      "Epoch [2/2], Step [56850/67476], Loss: 4.6063\n",
      "Epoch [2/2], Step [56860/67476], Loss: 4.6717\n",
      "Epoch [2/2], Step [56870/67476], Loss: 4.7957\n",
      "Epoch [2/2], Step [56880/67476], Loss: 4.6059\n",
      "Epoch [2/2], Step [56890/67476], Loss: 4.4546\n",
      "Epoch [2/2], Step [56900/67476], Loss: 4.5552\n",
      "Epoch [2/2], Step [56910/67476], Loss: 4.6015\n",
      "Epoch [2/2], Step [56920/67476], Loss: 4.6126\n",
      "Epoch [2/2], Step [56930/67476], Loss: 4.5337\n",
      "Epoch [2/2], Step [56940/67476], Loss: 4.5345\n",
      "Epoch [2/2], Step [56950/67476], Loss: 4.5503\n",
      "Epoch [2/2], Step [56960/67476], Loss: 4.5235\n",
      "Epoch [2/2], Step [56970/67476], Loss: 4.5129\n",
      "Epoch [2/2], Step [56980/67476], Loss: 4.4905\n",
      "Epoch [2/2], Step [56990/67476], Loss: 4.5821\n",
      "Epoch [2/2], Step [57000/67476], Loss: 4.6129\n",
      "Epoch [2/2], Step [57010/67476], Loss: 4.4755\n",
      "Epoch [2/2], Step [57020/67476], Loss: 4.7633\n",
      "Epoch [2/2], Step [57030/67476], Loss: 4.3415\n",
      "Epoch [2/2], Step [57040/67476], Loss: 4.4597\n",
      "Epoch [2/2], Step [57050/67476], Loss: 4.4768\n",
      "Epoch [2/2], Step [57060/67476], Loss: 4.6682\n",
      "Epoch [2/2], Step [57070/67476], Loss: 4.5294\n",
      "Epoch [2/2], Step [57080/67476], Loss: 4.4962\n",
      "Epoch [2/2], Step [57090/67476], Loss: 4.5874\n",
      "Epoch [2/2], Step [57100/67476], Loss: 4.4294\n",
      "Epoch [2/2], Step [57110/67476], Loss: 4.5704\n",
      "Epoch [2/2], Step [57120/67476], Loss: 4.2168\n",
      "Epoch [2/2], Step [57130/67476], Loss: 4.5939\n",
      "Epoch [2/2], Step [57140/67476], Loss: 4.4412\n",
      "Epoch [2/2], Step [57150/67476], Loss: 4.4342\n",
      "Epoch [2/2], Step [57160/67476], Loss: 4.4905\n",
      "Epoch [2/2], Step [57170/67476], Loss: 4.7524\n",
      "Epoch [2/2], Step [57180/67476], Loss: 4.4573\n",
      "Epoch [2/2], Step [57190/67476], Loss: 4.6211\n",
      "Epoch [2/2], Step [57200/67476], Loss: 4.6940\n",
      "Epoch [2/2], Step [57210/67476], Loss: 4.4660\n",
      "Epoch [2/2], Step [57220/67476], Loss: 4.6875\n",
      "Epoch [2/2], Step [57230/67476], Loss: 4.7824\n",
      "Epoch [2/2], Step [57240/67476], Loss: 4.5215\n",
      "Epoch [2/2], Step [57250/67476], Loss: 4.4138\n",
      "Epoch [2/2], Step [57260/67476], Loss: 4.6135\n",
      "Epoch [2/2], Step [57270/67476], Loss: 4.5843\n",
      "Epoch [2/2], Step [57280/67476], Loss: 4.3262\n",
      "Epoch [2/2], Step [57290/67476], Loss: 4.5115\n",
      "Epoch [2/2], Step [57300/67476], Loss: 4.7838\n",
      "Epoch [2/2], Step [57310/67476], Loss: 4.6574\n",
      "Epoch [2/2], Step [57320/67476], Loss: 4.4946\n",
      "Epoch [2/2], Step [57330/67476], Loss: 4.5774\n",
      "Epoch [2/2], Step [57340/67476], Loss: 4.4766\n",
      "Epoch [2/2], Step [57350/67476], Loss: 4.3621\n",
      "Epoch [2/2], Step [57360/67476], Loss: 4.5466\n",
      "Epoch [2/2], Step [57370/67476], Loss: 4.4299\n",
      "Epoch [2/2], Step [57380/67476], Loss: 4.4839\n",
      "Epoch [2/2], Step [57390/67476], Loss: 4.3241\n",
      "Epoch [2/2], Step [57400/67476], Loss: 4.5896\n",
      "Epoch [2/2], Step [57410/67476], Loss: 4.4755\n",
      "Epoch [2/2], Step [57420/67476], Loss: 4.5972\n",
      "Epoch [2/2], Step [57430/67476], Loss: 4.5190\n",
      "Epoch [2/2], Step [57440/67476], Loss: 4.5740\n",
      "Epoch [2/2], Step [57450/67476], Loss: 4.4570\n",
      "Epoch [2/2], Step [57460/67476], Loss: 4.7216\n",
      "Epoch [2/2], Step [57470/67476], Loss: 4.5704\n",
      "Epoch [2/2], Step [57480/67476], Loss: 4.3536\n",
      "Epoch [2/2], Step [57490/67476], Loss: 4.3603\n",
      "Epoch [2/2], Step [57500/67476], Loss: 4.4505\n",
      "Epoch [2/2], Step [57510/67476], Loss: 4.5969\n",
      "Epoch [2/2], Step [57520/67476], Loss: 4.5845\n",
      "Epoch [2/2], Step [57530/67476], Loss: 4.4720\n",
      "Epoch [2/2], Step [57540/67476], Loss: 4.6623\n",
      "Epoch [2/2], Step [57550/67476], Loss: 4.5115\n",
      "Epoch [2/2], Step [57560/67476], Loss: 4.6791\n",
      "Epoch [2/2], Step [57570/67476], Loss: 4.4682\n",
      "Epoch [2/2], Step [57580/67476], Loss: 4.4982\n",
      "Epoch [2/2], Step [57590/67476], Loss: 4.6566\n",
      "Epoch [2/2], Step [57600/67476], Loss: 4.5592\n",
      "Epoch [2/2], Step [57610/67476], Loss: 4.4349\n",
      "Epoch [2/2], Step [57620/67476], Loss: 4.5768\n",
      "Epoch [2/2], Step [57630/67476], Loss: 4.4517\n",
      "Epoch [2/2], Step [57640/67476], Loss: 4.4337\n",
      "Epoch [2/2], Step [57650/67476], Loss: 4.3691\n",
      "Epoch [2/2], Step [57660/67476], Loss: 4.5681\n",
      "Epoch [2/2], Step [57670/67476], Loss: 4.3868\n",
      "Epoch [2/2], Step [57680/67476], Loss: 4.3650\n",
      "Epoch [2/2], Step [57690/67476], Loss: 4.8413\n",
      "Epoch [2/2], Step [57700/67476], Loss: 4.3874\n",
      "Epoch [2/2], Step [57710/67476], Loss: 4.7212\n",
      "Epoch [2/2], Step [57720/67476], Loss: 4.5670\n",
      "Epoch [2/2], Step [57730/67476], Loss: 4.5938\n",
      "Epoch [2/2], Step [57740/67476], Loss: 4.6580\n",
      "Epoch [2/2], Step [57750/67476], Loss: 4.5721\n",
      "Epoch [2/2], Step [57760/67476], Loss: 4.4895\n",
      "Epoch [2/2], Step [57770/67476], Loss: 4.5051\n",
      "Epoch [2/2], Step [57780/67476], Loss: 4.5604\n",
      "Epoch [2/2], Step [57790/67476], Loss: 4.4926\n",
      "Epoch [2/2], Step [57800/67476], Loss: 4.3969\n",
      "Epoch [2/2], Step [57810/67476], Loss: 4.5241\n",
      "Epoch [2/2], Step [57820/67476], Loss: 4.4433\n",
      "Epoch [2/2], Step [57830/67476], Loss: 4.7252\n",
      "Epoch [2/2], Step [57840/67476], Loss: 4.6215\n",
      "Epoch [2/2], Step [57850/67476], Loss: 4.5980\n",
      "Epoch [2/2], Step [57860/67476], Loss: 4.5525\n",
      "Epoch [2/2], Step [57870/67476], Loss: 4.5790\n",
      "Epoch [2/2], Step [57880/67476], Loss: 4.1372\n",
      "Epoch [2/2], Step [57890/67476], Loss: 4.6976\n",
      "Epoch [2/2], Step [57900/67476], Loss: 4.4712\n",
      "Epoch [2/2], Step [57910/67476], Loss: 4.6009\n",
      "Epoch [2/2], Step [57920/67476], Loss: 4.6246\n",
      "Epoch [2/2], Step [57930/67476], Loss: 4.4889\n",
      "Epoch [2/2], Step [57940/67476], Loss: 4.6709\n",
      "Epoch [2/2], Step [57950/67476], Loss: 4.6639\n",
      "Epoch [2/2], Step [57960/67476], Loss: 4.7629\n",
      "Epoch [2/2], Step [57970/67476], Loss: 4.5681\n",
      "Epoch [2/2], Step [57980/67476], Loss: 4.5842\n",
      "Epoch [2/2], Step [57990/67476], Loss: 4.4989\n",
      "Epoch [2/2], Step [58000/67476], Loss: 4.5844\n",
      "Epoch [2/2], Step [58010/67476], Loss: 4.3493\n",
      "Epoch [2/2], Step [58020/67476], Loss: 4.4157\n",
      "Epoch [2/2], Step [58030/67476], Loss: 4.6181\n",
      "Epoch [2/2], Step [58040/67476], Loss: 4.6003\n",
      "Epoch [2/2], Step [58050/67476], Loss: 4.4963\n",
      "Epoch [2/2], Step [58060/67476], Loss: 4.1814\n",
      "Epoch [2/2], Step [58070/67476], Loss: 4.5877\n",
      "Epoch [2/2], Step [58080/67476], Loss: 4.6556\n",
      "Epoch [2/2], Step [58090/67476], Loss: 4.6515\n",
      "Epoch [2/2], Step [58100/67476], Loss: 4.6632\n",
      "Epoch [2/2], Step [58110/67476], Loss: 4.6348\n",
      "Epoch [2/2], Step [58120/67476], Loss: 4.3607\n",
      "Epoch [2/2], Step [58130/67476], Loss: 4.5633\n",
      "Epoch [2/2], Step [58140/67476], Loss: 4.6605\n",
      "Epoch [2/2], Step [58150/67476], Loss: 4.6394\n",
      "Epoch [2/2], Step [58160/67476], Loss: 4.4076\n",
      "Epoch [2/2], Step [58170/67476], Loss: 4.3349\n",
      "Epoch [2/2], Step [58180/67476], Loss: 4.5968\n",
      "Epoch [2/2], Step [58190/67476], Loss: 4.6569\n",
      "Epoch [2/2], Step [58200/67476], Loss: 4.6024\n",
      "Epoch [2/2], Step [58210/67476], Loss: 4.5515\n",
      "Epoch [2/2], Step [58220/67476], Loss: 4.6278\n",
      "Epoch [2/2], Step [58230/67476], Loss: 4.7011\n",
      "Epoch [2/2], Step [58240/67476], Loss: 4.3629\n",
      "Epoch [2/2], Step [58250/67476], Loss: 4.5270\n",
      "Epoch [2/2], Step [58260/67476], Loss: 4.5722\n",
      "Epoch [2/2], Step [58270/67476], Loss: 4.2696\n",
      "Epoch [2/2], Step [58280/67476], Loss: 4.6268\n",
      "Epoch [2/2], Step [58290/67476], Loss: 4.4330\n",
      "Epoch [2/2], Step [58300/67476], Loss: 4.6862\n",
      "Epoch [2/2], Step [58310/67476], Loss: 4.5113\n",
      "Epoch [2/2], Step [58320/67476], Loss: 4.6054\n",
      "Epoch [2/2], Step [58330/67476], Loss: 4.5581\n",
      "Epoch [2/2], Step [58340/67476], Loss: 4.2858\n",
      "Epoch [2/2], Step [58350/67476], Loss: 4.5378\n",
      "Epoch [2/2], Step [58360/67476], Loss: 4.5102\n",
      "Epoch [2/2], Step [58370/67476], Loss: 4.5869\n",
      "Epoch [2/2], Step [58380/67476], Loss: 4.6906\n",
      "Epoch [2/2], Step [58390/67476], Loss: 4.6239\n",
      "Epoch [2/2], Step [58400/67476], Loss: 4.5960\n",
      "Epoch [2/2], Step [58410/67476], Loss: 4.7174\n",
      "Epoch [2/2], Step [58420/67476], Loss: 4.5893\n",
      "Epoch [2/2], Step [58430/67476], Loss: 4.7307\n",
      "Epoch [2/2], Step [58440/67476], Loss: 4.5922\n",
      "Epoch [2/2], Step [58450/67476], Loss: 4.6910\n",
      "Epoch [2/2], Step [58460/67476], Loss: 4.5096\n",
      "Epoch [2/2], Step [58470/67476], Loss: 4.4230\n",
      "Epoch [2/2], Step [58480/67476], Loss: 4.5470\n",
      "Epoch [2/2], Step [58490/67476], Loss: 4.5421\n",
      "Epoch [2/2], Step [58500/67476], Loss: 4.3512\n",
      "Epoch [2/2], Step [58510/67476], Loss: 4.6841\n",
      "Epoch [2/2], Step [58520/67476], Loss: 4.4187\n",
      "Epoch [2/2], Step [58530/67476], Loss: 4.3694\n",
      "Epoch [2/2], Step [58540/67476], Loss: 4.6018\n",
      "Epoch [2/2], Step [58550/67476], Loss: 4.6483\n",
      "Epoch [2/2], Step [58560/67476], Loss: 4.4477\n",
      "Epoch [2/2], Step [58570/67476], Loss: 4.3424\n",
      "Epoch [2/2], Step [58580/67476], Loss: 4.4527\n",
      "Epoch [2/2], Step [58590/67476], Loss: 4.4282\n",
      "Epoch [2/2], Step [58600/67476], Loss: 4.3630\n",
      "Epoch [2/2], Step [58610/67476], Loss: 4.6259\n",
      "Epoch [2/2], Step [58620/67476], Loss: 4.2788\n",
      "Epoch [2/2], Step [58630/67476], Loss: 4.4844\n",
      "Epoch [2/2], Step [58640/67476], Loss: 4.6014\n",
      "Epoch [2/2], Step [58650/67476], Loss: 4.3138\n",
      "Epoch [2/2], Step [58660/67476], Loss: 4.5051\n",
      "Epoch [2/2], Step [58670/67476], Loss: 4.7049\n",
      "Epoch [2/2], Step [58680/67476], Loss: 4.3539\n",
      "Epoch [2/2], Step [58690/67476], Loss: 4.4815\n",
      "Epoch [2/2], Step [58700/67476], Loss: 4.4959\n",
      "Epoch [2/2], Step [58710/67476], Loss: 4.4177\n",
      "Epoch [2/2], Step [58720/67476], Loss: 4.6058\n",
      "Epoch [2/2], Step [58730/67476], Loss: 4.5310\n",
      "Epoch [2/2], Step [58740/67476], Loss: 4.4862\n",
      "Epoch [2/2], Step [58750/67476], Loss: 4.5221\n",
      "Epoch [2/2], Step [58760/67476], Loss: 4.6783\n",
      "Epoch [2/2], Step [58770/67476], Loss: 4.4591\n",
      "Epoch [2/2], Step [58780/67476], Loss: 4.4709\n",
      "Epoch [2/2], Step [58790/67476], Loss: 4.4642\n",
      "Epoch [2/2], Step [58800/67476], Loss: 4.5646\n",
      "Epoch [2/2], Step [58810/67476], Loss: 4.5365\n",
      "Epoch [2/2], Step [58820/67476], Loss: 4.5230\n",
      "Epoch [2/2], Step [58830/67476], Loss: 4.6809\n",
      "Epoch [2/2], Step [58840/67476], Loss: 4.3769\n",
      "Epoch [2/2], Step [58850/67476], Loss: 4.7538\n",
      "Epoch [2/2], Step [58860/67476], Loss: 4.3394\n",
      "Epoch [2/2], Step [58870/67476], Loss: 4.4800\n",
      "Epoch [2/2], Step [58880/67476], Loss: 4.6652\n",
      "Epoch [2/2], Step [58890/67476], Loss: 4.3278\n",
      "Epoch [2/2], Step [58900/67476], Loss: 4.6965\n",
      "Epoch [2/2], Step [58910/67476], Loss: 4.3965\n",
      "Epoch [2/2], Step [58920/67476], Loss: 4.4602\n",
      "Epoch [2/2], Step [58930/67476], Loss: 4.2873\n",
      "Epoch [2/2], Step [58940/67476], Loss: 4.4985\n",
      "Epoch [2/2], Step [58950/67476], Loss: 4.5926\n",
      "Epoch [2/2], Step [58960/67476], Loss: 4.3862\n",
      "Epoch [2/2], Step [58970/67476], Loss: 4.5555\n",
      "Epoch [2/2], Step [58980/67476], Loss: 4.5518\n",
      "Epoch [2/2], Step [58990/67476], Loss: 4.7269\n",
      "Epoch [2/2], Step [59000/67476], Loss: 4.5806\n",
      "Epoch [2/2], Step [59010/67476], Loss: 4.4624\n",
      "Epoch [2/2], Step [59020/67476], Loss: 4.5595\n",
      "Epoch [2/2], Step [59030/67476], Loss: 4.5029\n",
      "Epoch [2/2], Step [59040/67476], Loss: 4.6921\n",
      "Epoch [2/2], Step [59050/67476], Loss: 4.3313\n",
      "Epoch [2/2], Step [59060/67476], Loss: 4.6457\n",
      "Epoch [2/2], Step [59070/67476], Loss: 4.6318\n",
      "Epoch [2/2], Step [59080/67476], Loss: 4.4774\n",
      "Epoch [2/2], Step [59090/67476], Loss: 4.5610\n",
      "Epoch [2/2], Step [59100/67476], Loss: 4.5594\n",
      "Epoch [2/2], Step [59110/67476], Loss: 4.4374\n",
      "Epoch [2/2], Step [59120/67476], Loss: 4.5436\n",
      "Epoch [2/2], Step [59130/67476], Loss: 4.6938\n",
      "Epoch [2/2], Step [59140/67476], Loss: 4.4363\n",
      "Epoch [2/2], Step [59150/67476], Loss: 4.3367\n",
      "Epoch [2/2], Step [59160/67476], Loss: 4.6386\n",
      "Epoch [2/2], Step [59170/67476], Loss: 4.6223\n",
      "Epoch [2/2], Step [59180/67476], Loss: 4.4948\n",
      "Epoch [2/2], Step [59190/67476], Loss: 4.5382\n",
      "Epoch [2/2], Step [59200/67476], Loss: 4.4243\n",
      "Epoch [2/2], Step [59210/67476], Loss: 4.6910\n",
      "Epoch [2/2], Step [59220/67476], Loss: 4.8905\n",
      "Epoch [2/2], Step [59230/67476], Loss: 4.3749\n",
      "Epoch [2/2], Step [59240/67476], Loss: 4.4495\n",
      "Epoch [2/2], Step [59250/67476], Loss: 4.5278\n",
      "Epoch [2/2], Step [59260/67476], Loss: 4.7623\n",
      "Epoch [2/2], Step [59270/67476], Loss: 4.2531\n",
      "Epoch [2/2], Step [59280/67476], Loss: 4.6236\n",
      "Epoch [2/2], Step [59290/67476], Loss: 4.4144\n",
      "Epoch [2/2], Step [59300/67476], Loss: 4.6828\n",
      "Epoch [2/2], Step [59310/67476], Loss: 4.4998\n",
      "Epoch [2/2], Step [59320/67476], Loss: 4.6033\n",
      "Epoch [2/2], Step [59330/67476], Loss: 4.6928\n",
      "Epoch [2/2], Step [59340/67476], Loss: 4.3507\n",
      "Epoch [2/2], Step [59350/67476], Loss: 4.4025\n",
      "Epoch [2/2], Step [59360/67476], Loss: 4.5038\n",
      "Epoch [2/2], Step [59370/67476], Loss: 4.3577\n",
      "Epoch [2/2], Step [59380/67476], Loss: 4.4677\n",
      "Epoch [2/2], Step [59390/67476], Loss: 4.2746\n",
      "Epoch [2/2], Step [59400/67476], Loss: 4.5746\n",
      "Epoch [2/2], Step [59410/67476], Loss: 4.6263\n",
      "Epoch [2/2], Step [59420/67476], Loss: 4.4809\n",
      "Epoch [2/2], Step [59430/67476], Loss: 4.4824\n",
      "Epoch [2/2], Step [59440/67476], Loss: 4.6011\n",
      "Epoch [2/2], Step [59450/67476], Loss: 4.5308\n",
      "Epoch [2/2], Step [59460/67476], Loss: 4.6103\n",
      "Epoch [2/2], Step [59470/67476], Loss: 4.4750\n",
      "Epoch [2/2], Step [59480/67476], Loss: 4.6064\n",
      "Epoch [2/2], Step [59490/67476], Loss: 4.4098\n",
      "Epoch [2/2], Step [59500/67476], Loss: 4.6724\n",
      "Epoch [2/2], Step [59510/67476], Loss: 4.5526\n",
      "Epoch [2/2], Step [59520/67476], Loss: 4.6210\n",
      "Epoch [2/2], Step [59530/67476], Loss: 4.3584\n",
      "Epoch [2/2], Step [59540/67476], Loss: 4.6002\n",
      "Epoch [2/2], Step [59550/67476], Loss: 4.3994\n",
      "Epoch [2/2], Step [59560/67476], Loss: 4.4968\n",
      "Epoch [2/2], Step [59570/67476], Loss: 4.3482\n",
      "Epoch [2/2], Step [59580/67476], Loss: 4.4984\n",
      "Epoch [2/2], Step [59590/67476], Loss: 4.4935\n",
      "Epoch [2/2], Step [59600/67476], Loss: 4.5457\n",
      "Epoch [2/2], Step [59610/67476], Loss: 4.3821\n",
      "Epoch [2/2], Step [59620/67476], Loss: 4.6000\n",
      "Epoch [2/2], Step [59630/67476], Loss: 4.5002\n",
      "Epoch [2/2], Step [59640/67476], Loss: 4.4758\n",
      "Epoch [2/2], Step [59650/67476], Loss: 4.3996\n",
      "Epoch [2/2], Step [59660/67476], Loss: 4.6595\n",
      "Epoch [2/2], Step [59670/67476], Loss: 4.5067\n",
      "Epoch [2/2], Step [59680/67476], Loss: 4.5750\n",
      "Epoch [2/2], Step [59690/67476], Loss: 4.7215\n",
      "Epoch [2/2], Step [59700/67476], Loss: 4.4749\n",
      "Epoch [2/2], Step [59710/67476], Loss: 4.3085\n",
      "Epoch [2/2], Step [59720/67476], Loss: 4.5273\n",
      "Epoch [2/2], Step [59730/67476], Loss: 4.6147\n",
      "Epoch [2/2], Step [59740/67476], Loss: 4.4264\n",
      "Epoch [2/2], Step [59750/67476], Loss: 4.5392\n",
      "Epoch [2/2], Step [59760/67476], Loss: 4.4096\n",
      "Epoch [2/2], Step [59770/67476], Loss: 4.5063\n",
      "Epoch [2/2], Step [59780/67476], Loss: 4.4987\n",
      "Epoch [2/2], Step [59790/67476], Loss: 4.8184\n",
      "Epoch [2/2], Step [59800/67476], Loss: 4.7372\n",
      "Epoch [2/2], Step [59810/67476], Loss: 4.5267\n",
      "Epoch [2/2], Step [59820/67476], Loss: 4.6923\n",
      "Epoch [2/2], Step [59830/67476], Loss: 4.6422\n",
      "Epoch [2/2], Step [59840/67476], Loss: 4.7371\n",
      "Epoch [2/2], Step [59850/67476], Loss: 4.4302\n",
      "Epoch [2/2], Step [59860/67476], Loss: 4.6368\n",
      "Epoch [2/2], Step [59870/67476], Loss: 4.5174\n",
      "Epoch [2/2], Step [59880/67476], Loss: 4.6436\n",
      "Epoch [2/2], Step [59890/67476], Loss: 4.4034\n",
      "Epoch [2/2], Step [59900/67476], Loss: 4.4216\n",
      "Epoch [2/2], Step [59910/67476], Loss: 4.4472\n",
      "Epoch [2/2], Step [59920/67476], Loss: 4.5167\n",
      "Epoch [2/2], Step [59930/67476], Loss: 4.5137\n",
      "Epoch [2/2], Step [59940/67476], Loss: 4.4822\n",
      "Epoch [2/2], Step [59950/67476], Loss: 4.5851\n",
      "Epoch [2/2], Step [59960/67476], Loss: 4.3573\n",
      "Epoch [2/2], Step [59970/67476], Loss: 4.4751\n",
      "Epoch [2/2], Step [59980/67476], Loss: 4.4042\n",
      "Epoch [2/2], Step [59990/67476], Loss: 4.3681\n",
      "Epoch [2/2], Step [60000/67476], Loss: 4.5410\n",
      "Epoch [2/2], Step [60010/67476], Loss: 4.6786\n",
      "Epoch [2/2], Step [60020/67476], Loss: 4.4012\n",
      "Epoch [2/2], Step [60030/67476], Loss: 4.6340\n",
      "Epoch [2/2], Step [60040/67476], Loss: 4.5321\n",
      "Epoch [2/2], Step [60050/67476], Loss: 4.7615\n",
      "Epoch [2/2], Step [60060/67476], Loss: 4.7328\n",
      "Epoch [2/2], Step [60070/67476], Loss: 4.3597\n",
      "Epoch [2/2], Step [60080/67476], Loss: 4.7231\n",
      "Epoch [2/2], Step [60090/67476], Loss: 4.5158\n",
      "Epoch [2/2], Step [60100/67476], Loss: 4.6694\n",
      "Epoch [2/2], Step [60110/67476], Loss: 4.3697\n",
      "Epoch [2/2], Step [60120/67476], Loss: 4.5859\n",
      "Epoch [2/2], Step [60130/67476], Loss: 4.5269\n",
      "Epoch [2/2], Step [60140/67476], Loss: 4.6586\n",
      "Epoch [2/2], Step [60150/67476], Loss: 4.5266\n",
      "Epoch [2/2], Step [60160/67476], Loss: 4.3992\n",
      "Epoch [2/2], Step [60170/67476], Loss: 4.5620\n",
      "Epoch [2/2], Step [60180/67476], Loss: 4.5361\n",
      "Epoch [2/2], Step [60190/67476], Loss: 4.6800\n",
      "Epoch [2/2], Step [60200/67476], Loss: 4.6055\n",
      "Epoch [2/2], Step [60210/67476], Loss: 4.6207\n",
      "Epoch [2/2], Step [60220/67476], Loss: 4.6111\n",
      "Epoch [2/2], Step [60230/67476], Loss: 4.7336\n",
      "Epoch [2/2], Step [60240/67476], Loss: 4.4338\n",
      "Epoch [2/2], Step [60250/67476], Loss: 4.5710\n",
      "Epoch [2/2], Step [60260/67476], Loss: 4.5681\n",
      "Epoch [2/2], Step [60270/67476], Loss: 4.5931\n",
      "Epoch [2/2], Step [60280/67476], Loss: 4.6374\n",
      "Epoch [2/2], Step [60290/67476], Loss: 4.6774\n",
      "Epoch [2/2], Step [60300/67476], Loss: 4.6159\n",
      "Epoch [2/2], Step [60310/67476], Loss: 4.7129\n",
      "Epoch [2/2], Step [60320/67476], Loss: 4.6399\n",
      "Epoch [2/2], Step [60330/67476], Loss: 4.6001\n",
      "Epoch [2/2], Step [60340/67476], Loss: 4.6749\n",
      "Epoch [2/2], Step [60350/67476], Loss: 4.6202\n",
      "Epoch [2/2], Step [60360/67476], Loss: 4.4800\n",
      "Epoch [2/2], Step [60370/67476], Loss: 4.5728\n",
      "Epoch [2/2], Step [60380/67476], Loss: 4.5951\n",
      "Epoch [2/2], Step [60390/67476], Loss: 4.4634\n",
      "Epoch [2/2], Step [60400/67476], Loss: 4.3860\n",
      "Epoch [2/2], Step [60410/67476], Loss: 4.6109\n",
      "Epoch [2/2], Step [60420/67476], Loss: 4.3903\n",
      "Epoch [2/2], Step [60430/67476], Loss: 4.4903\n",
      "Epoch [2/2], Step [60440/67476], Loss: 4.4946\n",
      "Epoch [2/2], Step [60450/67476], Loss: 4.6374\n",
      "Epoch [2/2], Step [60460/67476], Loss: 4.6405\n",
      "Epoch [2/2], Step [60470/67476], Loss: 4.4813\n",
      "Epoch [2/2], Step [60480/67476], Loss: 4.5600\n",
      "Epoch [2/2], Step [60490/67476], Loss: 4.4467\n",
      "Epoch [2/2], Step [60500/67476], Loss: 4.4170\n",
      "Epoch [2/2], Step [60510/67476], Loss: 4.5673\n",
      "Epoch [2/2], Step [60520/67476], Loss: 4.5240\n",
      "Epoch [2/2], Step [60530/67476], Loss: 4.4085\n",
      "Epoch [2/2], Step [60540/67476], Loss: 4.5856\n",
      "Epoch [2/2], Step [60550/67476], Loss: 4.6331\n",
      "Epoch [2/2], Step [60560/67476], Loss: 4.6668\n",
      "Epoch [2/2], Step [60570/67476], Loss: 4.4558\n",
      "Epoch [2/2], Step [60580/67476], Loss: 4.6163\n",
      "Epoch [2/2], Step [60590/67476], Loss: 4.6161\n",
      "Epoch [2/2], Step [60600/67476], Loss: 4.4947\n",
      "Epoch [2/2], Step [60610/67476], Loss: 4.5255\n",
      "Epoch [2/2], Step [60620/67476], Loss: 4.5458\n",
      "Epoch [2/2], Step [60630/67476], Loss: 4.6006\n",
      "Epoch [2/2], Step [60640/67476], Loss: 4.6207\n",
      "Epoch [2/2], Step [60650/67476], Loss: 4.3552\n",
      "Epoch [2/2], Step [60660/67476], Loss: 4.3402\n",
      "Epoch [2/2], Step [60670/67476], Loss: 4.4332\n",
      "Epoch [2/2], Step [60680/67476], Loss: 4.5969\n",
      "Epoch [2/2], Step [60690/67476], Loss: 4.4955\n",
      "Epoch [2/2], Step [60700/67476], Loss: 4.6373\n",
      "Epoch [2/2], Step [60710/67476], Loss: 4.7468\n",
      "Epoch [2/2], Step [60720/67476], Loss: 4.5122\n",
      "Epoch [2/2], Step [60730/67476], Loss: 4.5831\n",
      "Epoch [2/2], Step [60740/67476], Loss: 4.5348\n",
      "Epoch [2/2], Step [60750/67476], Loss: 4.6533\n",
      "Epoch [2/2], Step [60760/67476], Loss: 4.6010\n",
      "Epoch [2/2], Step [60770/67476], Loss: 4.7967\n",
      "Epoch [2/2], Step [60780/67476], Loss: 4.7314\n",
      "Epoch [2/2], Step [60790/67476], Loss: 4.5400\n",
      "Epoch [2/2], Step [60800/67476], Loss: 4.8281\n",
      "Epoch [2/2], Step [60810/67476], Loss: 4.6088\n",
      "Epoch [2/2], Step [60820/67476], Loss: 4.6179\n",
      "Epoch [2/2], Step [60830/67476], Loss: 4.1806\n",
      "Epoch [2/2], Step [60840/67476], Loss: 4.6412\n",
      "Epoch [2/2], Step [60850/67476], Loss: 4.5257\n",
      "Epoch [2/2], Step [60860/67476], Loss: 4.3984\n",
      "Epoch [2/2], Step [60870/67476], Loss: 4.4399\n",
      "Epoch [2/2], Step [60880/67476], Loss: 4.5297\n",
      "Epoch [2/2], Step [60890/67476], Loss: 4.6305\n",
      "Epoch [2/2], Step [60900/67476], Loss: 4.7511\n",
      "Epoch [2/2], Step [60910/67476], Loss: 4.5539\n",
      "Epoch [2/2], Step [60920/67476], Loss: 4.5066\n",
      "Epoch [2/2], Step [60930/67476], Loss: 4.4535\n",
      "Epoch [2/2], Step [60940/67476], Loss: 4.4539\n",
      "Epoch [2/2], Step [60950/67476], Loss: 4.6744\n",
      "Epoch [2/2], Step [60960/67476], Loss: 4.4085\n",
      "Epoch [2/2], Step [60970/67476], Loss: 4.5217\n",
      "Epoch [2/2], Step [60980/67476], Loss: 4.4514\n",
      "Epoch [2/2], Step [60990/67476], Loss: 4.5532\n",
      "Epoch [2/2], Step [61000/67476], Loss: 4.4089\n",
      "Epoch [2/2], Step [61010/67476], Loss: 4.5443\n",
      "Epoch [2/2], Step [61020/67476], Loss: 4.5836\n",
      "Epoch [2/2], Step [61030/67476], Loss: 4.7236\n",
      "Epoch [2/2], Step [61040/67476], Loss: 4.3309\n",
      "Epoch [2/2], Step [61050/67476], Loss: 4.6466\n",
      "Epoch [2/2], Step [61060/67476], Loss: 4.4803\n",
      "Epoch [2/2], Step [61070/67476], Loss: 4.5590\n",
      "Epoch [2/2], Step [61080/67476], Loss: 4.3833\n",
      "Epoch [2/2], Step [61090/67476], Loss: 4.5400\n",
      "Epoch [2/2], Step [61100/67476], Loss: 4.4269\n",
      "Epoch [2/2], Step [61110/67476], Loss: 4.6112\n",
      "Epoch [2/2], Step [61120/67476], Loss: 4.5649\n",
      "Epoch [2/2], Step [61130/67476], Loss: 4.5058\n",
      "Epoch [2/2], Step [61140/67476], Loss: 4.5962\n",
      "Epoch [2/2], Step [61150/67476], Loss: 4.5391\n",
      "Epoch [2/2], Step [61160/67476], Loss: 4.7347\n",
      "Epoch [2/2], Step [61170/67476], Loss: 4.4358\n",
      "Epoch [2/2], Step [61180/67476], Loss: 4.4607\n",
      "Epoch [2/2], Step [61190/67476], Loss: 4.4713\n",
      "Epoch [2/2], Step [61200/67476], Loss: 4.5401\n",
      "Epoch [2/2], Step [61210/67476], Loss: 4.4399\n",
      "Epoch [2/2], Step [61220/67476], Loss: 4.6283\n",
      "Epoch [2/2], Step [61230/67476], Loss: 4.6163\n",
      "Epoch [2/2], Step [61240/67476], Loss: 4.3469\n",
      "Epoch [2/2], Step [61250/67476], Loss: 4.4262\n",
      "Epoch [2/2], Step [61260/67476], Loss: 4.3900\n",
      "Epoch [2/2], Step [61270/67476], Loss: 4.3970\n",
      "Epoch [2/2], Step [61280/67476], Loss: 4.3122\n",
      "Epoch [2/2], Step [61290/67476], Loss: 4.4934\n",
      "Epoch [2/2], Step [61300/67476], Loss: 4.4499\n",
      "Epoch [2/2], Step [61310/67476], Loss: 4.5934\n",
      "Epoch [2/2], Step [61320/67476], Loss: 4.4782\n",
      "Epoch [2/2], Step [61330/67476], Loss: 4.4664\n",
      "Epoch [2/2], Step [61340/67476], Loss: 4.5865\n",
      "Epoch [2/2], Step [61350/67476], Loss: 4.6322\n",
      "Epoch [2/2], Step [61360/67476], Loss: 4.4626\n",
      "Epoch [2/2], Step [61370/67476], Loss: 4.4783\n",
      "Epoch [2/2], Step [61380/67476], Loss: 4.4715\n",
      "Epoch [2/2], Step [61390/67476], Loss: 4.6823\n",
      "Epoch [2/2], Step [61400/67476], Loss: 4.6060\n",
      "Epoch [2/2], Step [61410/67476], Loss: 4.5158\n",
      "Epoch [2/2], Step [61420/67476], Loss: 4.6630\n",
      "Epoch [2/2], Step [61430/67476], Loss: 4.6926\n",
      "Epoch [2/2], Step [61440/67476], Loss: 4.5837\n",
      "Epoch [2/2], Step [61450/67476], Loss: 4.4131\n",
      "Epoch [2/2], Step [61460/67476], Loss: 4.5373\n",
      "Epoch [2/2], Step [61470/67476], Loss: 4.6132\n",
      "Epoch [2/2], Step [61480/67476], Loss: 4.4136\n",
      "Epoch [2/2], Step [61490/67476], Loss: 4.3660\n",
      "Epoch [2/2], Step [61500/67476], Loss: 4.7202\n",
      "Epoch [2/2], Step [61510/67476], Loss: 4.5088\n",
      "Epoch [2/2], Step [61520/67476], Loss: 4.4251\n",
      "Epoch [2/2], Step [61530/67476], Loss: 4.4977\n",
      "Epoch [2/2], Step [61540/67476], Loss: 4.5749\n",
      "Epoch [2/2], Step [61550/67476], Loss: 4.5717\n",
      "Epoch [2/2], Step [61560/67476], Loss: 4.6240\n",
      "Epoch [2/2], Step [61570/67476], Loss: 4.5590\n",
      "Epoch [2/2], Step [61580/67476], Loss: 4.7279\n",
      "Epoch [2/2], Step [61590/67476], Loss: 4.5710\n",
      "Epoch [2/2], Step [61600/67476], Loss: 4.5895\n",
      "Epoch [2/2], Step [61610/67476], Loss: 4.4604\n",
      "Epoch [2/2], Step [61620/67476], Loss: 4.7122\n",
      "Epoch [2/2], Step [61630/67476], Loss: 4.6110\n",
      "Epoch [2/2], Step [61640/67476], Loss: 4.4139\n",
      "Epoch [2/2], Step [61650/67476], Loss: 4.4127\n",
      "Epoch [2/2], Step [61660/67476], Loss: 4.5882\n",
      "Epoch [2/2], Step [61670/67476], Loss: 4.7055\n",
      "Epoch [2/2], Step [61680/67476], Loss: 4.7229\n",
      "Epoch [2/2], Step [61690/67476], Loss: 4.4534\n",
      "Epoch [2/2], Step [61700/67476], Loss: 4.7561\n",
      "Epoch [2/2], Step [61710/67476], Loss: 4.7070\n",
      "Epoch [2/2], Step [61720/67476], Loss: 4.6120\n",
      "Epoch [2/2], Step [61730/67476], Loss: 4.7220\n",
      "Epoch [2/2], Step [61740/67476], Loss: 4.8044\n",
      "Epoch [2/2], Step [61750/67476], Loss: 4.5242\n",
      "Epoch [2/2], Step [61760/67476], Loss: 4.3697\n",
      "Epoch [2/2], Step [61770/67476], Loss: 4.5698\n",
      "Epoch [2/2], Step [61780/67476], Loss: 4.3685\n",
      "Epoch [2/2], Step [61790/67476], Loss: 4.4789\n",
      "Epoch [2/2], Step [61800/67476], Loss: 4.5419\n",
      "Epoch [2/2], Step [61810/67476], Loss: 4.5738\n",
      "Epoch [2/2], Step [61820/67476], Loss: 4.4696\n",
      "Epoch [2/2], Step [61830/67476], Loss: 4.6524\n",
      "Epoch [2/2], Step [61840/67476], Loss: 4.7237\n",
      "Epoch [2/2], Step [61850/67476], Loss: 4.5696\n",
      "Epoch [2/2], Step [61860/67476], Loss: 4.4776\n",
      "Epoch [2/2], Step [61870/67476], Loss: 4.7037\n",
      "Epoch [2/2], Step [61880/67476], Loss: 4.5080\n",
      "Epoch [2/2], Step [61890/67476], Loss: 4.2663\n",
      "Epoch [2/2], Step [61900/67476], Loss: 4.5068\n",
      "Epoch [2/2], Step [61910/67476], Loss: 4.7529\n",
      "Epoch [2/2], Step [61920/67476], Loss: 4.5356\n",
      "Epoch [2/2], Step [61930/67476], Loss: 4.4424\n",
      "Epoch [2/2], Step [61940/67476], Loss: 4.4545\n",
      "Epoch [2/2], Step [61950/67476], Loss: 4.5268\n",
      "Epoch [2/2], Step [61960/67476], Loss: 4.4060\n",
      "Epoch [2/2], Step [61970/67476], Loss: 4.7452\n",
      "Epoch [2/2], Step [61980/67476], Loss: 4.5665\n",
      "Epoch [2/2], Step [61990/67476], Loss: 4.7258\n",
      "Epoch [2/2], Step [62000/67476], Loss: 4.4780\n",
      "Epoch [2/2], Step [62010/67476], Loss: 4.6263\n",
      "Epoch [2/2], Step [62020/67476], Loss: 4.7107\n",
      "Epoch [2/2], Step [62030/67476], Loss: 4.5316\n",
      "Epoch [2/2], Step [62040/67476], Loss: 4.5263\n",
      "Epoch [2/2], Step [62050/67476], Loss: 4.5872\n",
      "Epoch [2/2], Step [62060/67476], Loss: 4.4216\n",
      "Epoch [2/2], Step [62070/67476], Loss: 4.7369\n",
      "Epoch [2/2], Step [62080/67476], Loss: 4.6056\n",
      "Epoch [2/2], Step [62090/67476], Loss: 4.6686\n",
      "Epoch [2/2], Step [62100/67476], Loss: 4.5484\n",
      "Epoch [2/2], Step [62110/67476], Loss: 4.6801\n",
      "Epoch [2/2], Step [62120/67476], Loss: 4.3253\n",
      "Epoch [2/2], Step [62130/67476], Loss: 4.5204\n",
      "Epoch [2/2], Step [62140/67476], Loss: 4.7903\n",
      "Epoch [2/2], Step [62150/67476], Loss: 4.5613\n",
      "Epoch [2/2], Step [62160/67476], Loss: 4.6850\n",
      "Epoch [2/2], Step [62170/67476], Loss: 4.4290\n",
      "Epoch [2/2], Step [62180/67476], Loss: 4.5409\n",
      "Epoch [2/2], Step [62190/67476], Loss: 4.4681\n",
      "Epoch [2/2], Step [62200/67476], Loss: 4.6568\n",
      "Epoch [2/2], Step [62210/67476], Loss: 4.5719\n",
      "Epoch [2/2], Step [62220/67476], Loss: 4.4602\n",
      "Epoch [2/2], Step [62230/67476], Loss: 4.4997\n",
      "Epoch [2/2], Step [62240/67476], Loss: 4.5291\n",
      "Epoch [2/2], Step [62250/67476], Loss: 4.4968\n",
      "Epoch [2/2], Step [62260/67476], Loss: 4.5062\n",
      "Epoch [2/2], Step [62270/67476], Loss: 4.6113\n",
      "Epoch [2/2], Step [62280/67476], Loss: 4.5615\n",
      "Epoch [2/2], Step [62290/67476], Loss: 4.4121\n",
      "Epoch [2/2], Step [62300/67476], Loss: 4.4760\n",
      "Epoch [2/2], Step [62310/67476], Loss: 4.5227\n",
      "Epoch [2/2], Step [62320/67476], Loss: 4.5217\n",
      "Epoch [2/2], Step [62330/67476], Loss: 4.5857\n",
      "Epoch [2/2], Step [62340/67476], Loss: 4.6479\n",
      "Epoch [2/2], Step [62350/67476], Loss: 4.5255\n",
      "Epoch [2/2], Step [62360/67476], Loss: 4.5102\n",
      "Epoch [2/2], Step [62370/67476], Loss: 4.6410\n",
      "Epoch [2/2], Step [62380/67476], Loss: 4.5700\n",
      "Epoch [2/2], Step [62390/67476], Loss: 4.6596\n",
      "Epoch [2/2], Step [62400/67476], Loss: 4.5063\n",
      "Epoch [2/2], Step [62410/67476], Loss: 4.5345\n",
      "Epoch [2/2], Step [62420/67476], Loss: 4.6582\n",
      "Epoch [2/2], Step [62430/67476], Loss: 4.4841\n",
      "Epoch [2/2], Step [62440/67476], Loss: 4.6146\n",
      "Epoch [2/2], Step [62450/67476], Loss: 4.6399\n",
      "Epoch [2/2], Step [62460/67476], Loss: 4.6116\n",
      "Epoch [2/2], Step [62470/67476], Loss: 4.7090\n",
      "Epoch [2/2], Step [62480/67476], Loss: 4.5282\n",
      "Epoch [2/2], Step [62490/67476], Loss: 4.6464\n",
      "Epoch [2/2], Step [62500/67476], Loss: 4.5746\n",
      "Epoch [2/2], Step [62510/67476], Loss: 4.5515\n",
      "Epoch [2/2], Step [62520/67476], Loss: 4.5316\n",
      "Epoch [2/2], Step [62530/67476], Loss: 4.5425\n",
      "Epoch [2/2], Step [62540/67476], Loss: 4.5685\n",
      "Epoch [2/2], Step [62550/67476], Loss: 4.4482\n",
      "Epoch [2/2], Step [62560/67476], Loss: 4.6306\n",
      "Epoch [2/2], Step [62570/67476], Loss: 4.6358\n",
      "Epoch [2/2], Step [62580/67476], Loss: 4.4580\n",
      "Epoch [2/2], Step [62590/67476], Loss: 4.5481\n",
      "Epoch [2/2], Step [62600/67476], Loss: 4.6043\n",
      "Epoch [2/2], Step [62610/67476], Loss: 4.6002\n",
      "Epoch [2/2], Step [62620/67476], Loss: 4.4369\n",
      "Epoch [2/2], Step [62630/67476], Loss: 4.5626\n",
      "Epoch [2/2], Step [62640/67476], Loss: 4.5125\n",
      "Epoch [2/2], Step [62650/67476], Loss: 4.6477\n",
      "Epoch [2/2], Step [62660/67476], Loss: 4.6268\n",
      "Epoch [2/2], Step [62670/67476], Loss: 4.4693\n",
      "Epoch [2/2], Step [62680/67476], Loss: 4.5437\n",
      "Epoch [2/2], Step [62690/67476], Loss: 4.6655\n",
      "Epoch [2/2], Step [62700/67476], Loss: 4.7034\n",
      "Epoch [2/2], Step [62710/67476], Loss: 4.9028\n",
      "Epoch [2/2], Step [62720/67476], Loss: 4.4786\n",
      "Epoch [2/2], Step [62730/67476], Loss: 4.5750\n",
      "Epoch [2/2], Step [62740/67476], Loss: 4.3926\n",
      "Epoch [2/2], Step [62750/67476], Loss: 4.6518\n",
      "Epoch [2/2], Step [62760/67476], Loss: 4.4664\n",
      "Epoch [2/2], Step [62770/67476], Loss: 4.4386\n",
      "Epoch [2/2], Step [62780/67476], Loss: 4.6915\n",
      "Epoch [2/2], Step [62790/67476], Loss: 4.5358\n",
      "Epoch [2/2], Step [62800/67476], Loss: 4.5210\n",
      "Epoch [2/2], Step [62810/67476], Loss: 4.5617\n",
      "Epoch [2/2], Step [62820/67476], Loss: 4.5787\n",
      "Epoch [2/2], Step [62830/67476], Loss: 4.5855\n",
      "Epoch [2/2], Step [62840/67476], Loss: 4.5257\n",
      "Epoch [2/2], Step [62850/67476], Loss: 4.7320\n",
      "Epoch [2/2], Step [62860/67476], Loss: 4.4157\n",
      "Epoch [2/2], Step [62870/67476], Loss: 4.5253\n",
      "Epoch [2/2], Step [62880/67476], Loss: 4.5669\n",
      "Epoch [2/2], Step [62890/67476], Loss: 4.6380\n",
      "Epoch [2/2], Step [62900/67476], Loss: 4.6815\n",
      "Epoch [2/2], Step [62910/67476], Loss: 4.6616\n",
      "Epoch [2/2], Step [62920/67476], Loss: 4.5757\n",
      "Epoch [2/2], Step [62930/67476], Loss: 4.5901\n",
      "Epoch [2/2], Step [62940/67476], Loss: 4.5643\n",
      "Epoch [2/2], Step [62950/67476], Loss: 4.4924\n",
      "Epoch [2/2], Step [62960/67476], Loss: 4.4573\n",
      "Epoch [2/2], Step [62970/67476], Loss: 4.4705\n",
      "Epoch [2/2], Step [62980/67476], Loss: 4.5106\n",
      "Epoch [2/2], Step [62990/67476], Loss: 4.4996\n",
      "Epoch [2/2], Step [63000/67476], Loss: 4.3937\n",
      "Epoch [2/2], Step [63010/67476], Loss: 4.6228\n",
      "Epoch [2/2], Step [63020/67476], Loss: 4.4100\n",
      "Epoch [2/2], Step [63030/67476], Loss: 4.5302\n",
      "Epoch [2/2], Step [63040/67476], Loss: 4.6991\n",
      "Epoch [2/2], Step [63050/67476], Loss: 4.5948\n",
      "Epoch [2/2], Step [63060/67476], Loss: 4.5964\n",
      "Epoch [2/2], Step [63070/67476], Loss: 4.5445\n",
      "Epoch [2/2], Step [63080/67476], Loss: 4.4020\n",
      "Epoch [2/2], Step [63090/67476], Loss: 4.6261\n",
      "Epoch [2/2], Step [63100/67476], Loss: 4.4891\n",
      "Epoch [2/2], Step [63110/67476], Loss: 4.5677\n",
      "Epoch [2/2], Step [63120/67476], Loss: 4.3842\n",
      "Epoch [2/2], Step [63130/67476], Loss: 4.6082\n",
      "Epoch [2/2], Step [63140/67476], Loss: 4.7206\n",
      "Epoch [2/2], Step [63150/67476], Loss: 4.4604\n",
      "Epoch [2/2], Step [63160/67476], Loss: 4.6853\n",
      "Epoch [2/2], Step [63170/67476], Loss: 4.5897\n",
      "Epoch [2/2], Step [63180/67476], Loss: 4.4905\n",
      "Epoch [2/2], Step [63190/67476], Loss: 4.2781\n",
      "Epoch [2/2], Step [63200/67476], Loss: 4.4913\n",
      "Epoch [2/2], Step [63210/67476], Loss: 4.2284\n",
      "Epoch [2/2], Step [63220/67476], Loss: 4.5867\n",
      "Epoch [2/2], Step [63230/67476], Loss: 4.6276\n",
      "Epoch [2/2], Step [63240/67476], Loss: 4.5212\n",
      "Epoch [2/2], Step [63250/67476], Loss: 4.4743\n",
      "Epoch [2/2], Step [63260/67476], Loss: 4.5979\n",
      "Epoch [2/2], Step [63270/67476], Loss: 4.4461\n",
      "Epoch [2/2], Step [63280/67476], Loss: 4.5177\n",
      "Epoch [2/2], Step [63290/67476], Loss: 4.5792\n",
      "Epoch [2/2], Step [63300/67476], Loss: 4.4825\n",
      "Epoch [2/2], Step [63310/67476], Loss: 4.5550\n",
      "Epoch [2/2], Step [63320/67476], Loss: 4.6317\n",
      "Epoch [2/2], Step [63330/67476], Loss: 4.5202\n",
      "Epoch [2/2], Step [63340/67476], Loss: 4.6553\n",
      "Epoch [2/2], Step [63350/67476], Loss: 4.4618\n",
      "Epoch [2/2], Step [63360/67476], Loss: 4.6757\n",
      "Epoch [2/2], Step [63370/67476], Loss: 4.5426\n",
      "Epoch [2/2], Step [63380/67476], Loss: 4.6710\n",
      "Epoch [2/2], Step [63390/67476], Loss: 4.6984\n",
      "Epoch [2/2], Step [63400/67476], Loss: 4.5712\n",
      "Epoch [2/2], Step [63410/67476], Loss: 4.5729\n",
      "Epoch [2/2], Step [63420/67476], Loss: 4.6416\n",
      "Epoch [2/2], Step [63430/67476], Loss: 4.6257\n",
      "Epoch [2/2], Step [63440/67476], Loss: 4.7590\n",
      "Epoch [2/2], Step [63450/67476], Loss: 4.2920\n",
      "Epoch [2/2], Step [63460/67476], Loss: 4.7088\n",
      "Epoch [2/2], Step [63470/67476], Loss: 4.4441\n",
      "Epoch [2/2], Step [63480/67476], Loss: 4.4891\n",
      "Epoch [2/2], Step [63490/67476], Loss: 4.3855\n",
      "Epoch [2/2], Step [63500/67476], Loss: 4.3705\n",
      "Epoch [2/2], Step [63510/67476], Loss: 4.4438\n",
      "Epoch [2/2], Step [63520/67476], Loss: 4.5092\n",
      "Epoch [2/2], Step [63530/67476], Loss: 4.5787\n",
      "Epoch [2/2], Step [63540/67476], Loss: 4.5320\n",
      "Epoch [2/2], Step [63550/67476], Loss: 4.6168\n",
      "Epoch [2/2], Step [63560/67476], Loss: 4.6381\n",
      "Epoch [2/2], Step [63570/67476], Loss: 4.4185\n",
      "Epoch [2/2], Step [63580/67476], Loss: 4.5399\n",
      "Epoch [2/2], Step [63590/67476], Loss: 4.6128\n",
      "Epoch [2/2], Step [63600/67476], Loss: 4.6998\n",
      "Epoch [2/2], Step [63610/67476], Loss: 4.4532\n",
      "Epoch [2/2], Step [63620/67476], Loss: 4.5825\n",
      "Epoch [2/2], Step [63630/67476], Loss: 4.5299\n",
      "Epoch [2/2], Step [63640/67476], Loss: 4.5223\n",
      "Epoch [2/2], Step [63650/67476], Loss: 4.4674\n",
      "Epoch [2/2], Step [63660/67476], Loss: 4.5711\n",
      "Epoch [2/2], Step [63670/67476], Loss: 4.5993\n",
      "Epoch [2/2], Step [63680/67476], Loss: 4.5391\n",
      "Epoch [2/2], Step [63690/67476], Loss: 4.3847\n",
      "Epoch [2/2], Step [63700/67476], Loss: 4.5742\n",
      "Epoch [2/2], Step [63710/67476], Loss: 4.5018\n",
      "Epoch [2/2], Step [63720/67476], Loss: 4.7688\n",
      "Epoch [2/2], Step [63730/67476], Loss: 4.5451\n",
      "Epoch [2/2], Step [63740/67476], Loss: 4.3963\n",
      "Epoch [2/2], Step [63750/67476], Loss: 4.3235\n",
      "Epoch [2/2], Step [63760/67476], Loss: 4.6146\n",
      "Epoch [2/2], Step [63770/67476], Loss: 4.4869\n",
      "Epoch [2/2], Step [63780/67476], Loss: 4.5328\n",
      "Epoch [2/2], Step [63790/67476], Loss: 4.7176\n",
      "Epoch [2/2], Step [63800/67476], Loss: 4.6709\n",
      "Epoch [2/2], Step [63810/67476], Loss: 4.5488\n",
      "Epoch [2/2], Step [63820/67476], Loss: 4.5786\n",
      "Epoch [2/2], Step [63830/67476], Loss: 4.5296\n",
      "Epoch [2/2], Step [63840/67476], Loss: 4.6016\n",
      "Epoch [2/2], Step [63850/67476], Loss: 4.3536\n",
      "Epoch [2/2], Step [63860/67476], Loss: 4.5011\n",
      "Epoch [2/2], Step [63870/67476], Loss: 4.6296\n",
      "Epoch [2/2], Step [63880/67476], Loss: 4.5489\n",
      "Epoch [2/2], Step [63890/67476], Loss: 4.4975\n",
      "Epoch [2/2], Step [63900/67476], Loss: 4.5561\n",
      "Epoch [2/2], Step [63910/67476], Loss: 4.4130\n",
      "Epoch [2/2], Step [63920/67476], Loss: 4.4652\n",
      "Epoch [2/2], Step [63930/67476], Loss: 4.6612\n",
      "Epoch [2/2], Step [63940/67476], Loss: 4.5223\n",
      "Epoch [2/2], Step [63950/67476], Loss: 4.3842\n",
      "Epoch [2/2], Step [63960/67476], Loss: 4.4540\n",
      "Epoch [2/2], Step [63970/67476], Loss: 4.6050\n",
      "Epoch [2/2], Step [63980/67476], Loss: 4.6315\n",
      "Epoch [2/2], Step [63990/67476], Loss: 4.4563\n",
      "Epoch [2/2], Step [64000/67476], Loss: 4.3150\n",
      "Epoch [2/2], Step [64010/67476], Loss: 4.4257\n",
      "Epoch [2/2], Step [64020/67476], Loss: 4.5234\n",
      "Epoch [2/2], Step [64030/67476], Loss: 4.5661\n",
      "Epoch [2/2], Step [64040/67476], Loss: 4.5779\n",
      "Epoch [2/2], Step [64050/67476], Loss: 4.6995\n",
      "Epoch [2/2], Step [64060/67476], Loss: 4.6563\n",
      "Epoch [2/2], Step [64070/67476], Loss: 4.5952\n",
      "Epoch [2/2], Step [64080/67476], Loss: 4.4733\n",
      "Epoch [2/2], Step [64090/67476], Loss: 4.7800\n",
      "Epoch [2/2], Step [64100/67476], Loss: 4.4463\n",
      "Epoch [2/2], Step [64110/67476], Loss: 4.3479\n",
      "Epoch [2/2], Step [64120/67476], Loss: 4.4505\n",
      "Epoch [2/2], Step [64130/67476], Loss: 4.6289\n",
      "Epoch [2/2], Step [64140/67476], Loss: 4.4456\n",
      "Epoch [2/2], Step [64150/67476], Loss: 4.6782\n",
      "Epoch [2/2], Step [64160/67476], Loss: 4.6483\n",
      "Epoch [2/2], Step [64170/67476], Loss: 4.4322\n",
      "Epoch [2/2], Step [64180/67476], Loss: 4.5326\n",
      "Epoch [2/2], Step [64190/67476], Loss: 4.5104\n",
      "Epoch [2/2], Step [64200/67476], Loss: 4.5880\n",
      "Epoch [2/2], Step [64210/67476], Loss: 4.4511\n",
      "Epoch [2/2], Step [64220/67476], Loss: 4.5800\n",
      "Epoch [2/2], Step [64230/67476], Loss: 4.7695\n",
      "Epoch [2/2], Step [64240/67476], Loss: 4.3963\n",
      "Epoch [2/2], Step [64250/67476], Loss: 4.4084\n",
      "Epoch [2/2], Step [64260/67476], Loss: 4.4936\n",
      "Epoch [2/2], Step [64270/67476], Loss: 4.5949\n",
      "Epoch [2/2], Step [64280/67476], Loss: 4.5043\n",
      "Epoch [2/2], Step [64290/67476], Loss: 4.7417\n",
      "Epoch [2/2], Step [64300/67476], Loss: 4.6696\n",
      "Epoch [2/2], Step [64310/67476], Loss: 4.7528\n",
      "Epoch [2/2], Step [64320/67476], Loss: 4.6204\n",
      "Epoch [2/2], Step [64330/67476], Loss: 4.4682\n",
      "Epoch [2/2], Step [64340/67476], Loss: 4.4331\n",
      "Epoch [2/2], Step [64350/67476], Loss: 4.4661\n",
      "Epoch [2/2], Step [64360/67476], Loss: 4.4168\n",
      "Epoch [2/2], Step [64370/67476], Loss: 4.6904\n",
      "Epoch [2/2], Step [64380/67476], Loss: 4.6713\n",
      "Epoch [2/2], Step [64390/67476], Loss: 4.4406\n",
      "Epoch [2/2], Step [64400/67476], Loss: 4.6307\n",
      "Epoch [2/2], Step [64410/67476], Loss: 4.3355\n",
      "Epoch [2/2], Step [64420/67476], Loss: 4.3981\n",
      "Epoch [2/2], Step [64430/67476], Loss: 4.5569\n",
      "Epoch [2/2], Step [64440/67476], Loss: 4.5028\n",
      "Epoch [2/2], Step [64450/67476], Loss: 4.5673\n",
      "Epoch [2/2], Step [64460/67476], Loss: 4.5926\n",
      "Epoch [2/2], Step [64470/67476], Loss: 4.3686\n",
      "Epoch [2/2], Step [64480/67476], Loss: 4.6328\n",
      "Epoch [2/2], Step [64490/67476], Loss: 4.5237\n",
      "Epoch [2/2], Step [64500/67476], Loss: 4.7121\n",
      "Epoch [2/2], Step [64510/67476], Loss: 4.6544\n",
      "Epoch [2/2], Step [64520/67476], Loss: 4.4540\n",
      "Epoch [2/2], Step [64530/67476], Loss: 4.4458\n",
      "Epoch [2/2], Step [64540/67476], Loss: 4.5527\n",
      "Epoch [2/2], Step [64550/67476], Loss: 4.6054\n",
      "Epoch [2/2], Step [64560/67476], Loss: 4.3422\n",
      "Epoch [2/2], Step [64570/67476], Loss: 4.4505\n",
      "Epoch [2/2], Step [64580/67476], Loss: 4.3642\n",
      "Epoch [2/2], Step [64590/67476], Loss: 4.5417\n",
      "Epoch [2/2], Step [64600/67476], Loss: 4.4938\n",
      "Epoch [2/2], Step [64610/67476], Loss: 4.4935\n",
      "Epoch [2/2], Step [64620/67476], Loss: 4.5263\n",
      "Epoch [2/2], Step [64630/67476], Loss: 4.5912\n",
      "Epoch [2/2], Step [64640/67476], Loss: 4.6739\n",
      "Epoch [2/2], Step [64650/67476], Loss: 4.5874\n",
      "Epoch [2/2], Step [64660/67476], Loss: 4.5529\n",
      "Epoch [2/2], Step [64670/67476], Loss: 4.3350\n",
      "Epoch [2/2], Step [64680/67476], Loss: 4.5034\n",
      "Epoch [2/2], Step [64690/67476], Loss: 4.5985\n",
      "Epoch [2/2], Step [64700/67476], Loss: 4.3865\n",
      "Epoch [2/2], Step [64710/67476], Loss: 4.5170\n",
      "Epoch [2/2], Step [64720/67476], Loss: 4.5154\n",
      "Epoch [2/2], Step [64730/67476], Loss: 4.6691\n",
      "Epoch [2/2], Step [64740/67476], Loss: 4.5043\n",
      "Epoch [2/2], Step [64750/67476], Loss: 4.4689\n",
      "Epoch [2/2], Step [64760/67476], Loss: 4.6770\n",
      "Epoch [2/2], Step [64770/67476], Loss: 4.4893\n",
      "Epoch [2/2], Step [64780/67476], Loss: 4.6651\n",
      "Epoch [2/2], Step [64790/67476], Loss: 4.5482\n",
      "Epoch [2/2], Step [64800/67476], Loss: 4.4758\n",
      "Epoch [2/2], Step [64810/67476], Loss: 4.3231\n",
      "Epoch [2/2], Step [64820/67476], Loss: 4.5315\n",
      "Epoch [2/2], Step [64830/67476], Loss: 4.5095\n",
      "Epoch [2/2], Step [64840/67476], Loss: 4.5163\n",
      "Epoch [2/2], Step [64850/67476], Loss: 4.5155\n",
      "Epoch [2/2], Step [64860/67476], Loss: 4.4067\n",
      "Epoch [2/2], Step [64870/67476], Loss: 4.6938\n",
      "Epoch [2/2], Step [64880/67476], Loss: 4.4735\n",
      "Epoch [2/2], Step [64890/67476], Loss: 4.5803\n",
      "Epoch [2/2], Step [64900/67476], Loss: 4.4690\n",
      "Epoch [2/2], Step [64910/67476], Loss: 4.3413\n",
      "Epoch [2/2], Step [64920/67476], Loss: 4.6251\n",
      "Epoch [2/2], Step [64930/67476], Loss: 4.6491\n",
      "Epoch [2/2], Step [64940/67476], Loss: 4.4345\n",
      "Epoch [2/2], Step [64950/67476], Loss: 4.4528\n",
      "Epoch [2/2], Step [64960/67476], Loss: 4.3881\n",
      "Epoch [2/2], Step [64970/67476], Loss: 4.5430\n",
      "Epoch [2/2], Step [64980/67476], Loss: 4.3319\n",
      "Epoch [2/2], Step [64990/67476], Loss: 4.6499\n",
      "Epoch [2/2], Step [65000/67476], Loss: 4.5579\n",
      "Epoch [2/2], Step [65010/67476], Loss: 4.5950\n",
      "Epoch [2/2], Step [65020/67476], Loss: 4.3296\n",
      "Epoch [2/2], Step [65030/67476], Loss: 4.5884\n",
      "Epoch [2/2], Step [65040/67476], Loss: 4.7335\n",
      "Epoch [2/2], Step [65050/67476], Loss: 4.4427\n",
      "Epoch [2/2], Step [65060/67476], Loss: 4.6056\n",
      "Epoch [2/2], Step [65070/67476], Loss: 4.4843\n",
      "Epoch [2/2], Step [65080/67476], Loss: 4.6800\n",
      "Epoch [2/2], Step [65090/67476], Loss: 4.3689\n",
      "Epoch [2/2], Step [65100/67476], Loss: 4.6122\n",
      "Epoch [2/2], Step [65110/67476], Loss: 4.4234\n",
      "Epoch [2/2], Step [65120/67476], Loss: 4.6207\n",
      "Epoch [2/2], Step [65130/67476], Loss: 4.4684\n",
      "Epoch [2/2], Step [65140/67476], Loss: 4.3822\n",
      "Epoch [2/2], Step [65150/67476], Loss: 4.4093\n",
      "Epoch [2/2], Step [65160/67476], Loss: 4.4739\n",
      "Epoch [2/2], Step [65170/67476], Loss: 4.6147\n",
      "Epoch [2/2], Step [65180/67476], Loss: 4.5847\n",
      "Epoch [2/2], Step [65190/67476], Loss: 4.5963\n",
      "Epoch [2/2], Step [65200/67476], Loss: 4.5687\n",
      "Epoch [2/2], Step [65210/67476], Loss: 4.4646\n",
      "Epoch [2/2], Step [65220/67476], Loss: 4.5434\n",
      "Epoch [2/2], Step [65230/67476], Loss: 4.5459\n",
      "Epoch [2/2], Step [65240/67476], Loss: 4.4499\n",
      "Epoch [2/2], Step [65250/67476], Loss: 4.5717\n",
      "Epoch [2/2], Step [65260/67476], Loss: 4.5558\n",
      "Epoch [2/2], Step [65270/67476], Loss: 4.4995\n",
      "Epoch [2/2], Step [65280/67476], Loss: 4.5509\n",
      "Epoch [2/2], Step [65290/67476], Loss: 4.6579\n",
      "Epoch [2/2], Step [65300/67476], Loss: 4.3374\n",
      "Epoch [2/2], Step [65310/67476], Loss: 4.5743\n",
      "Epoch [2/2], Step [65320/67476], Loss: 4.5540\n",
      "Epoch [2/2], Step [65330/67476], Loss: 4.4137\n",
      "Epoch [2/2], Step [65340/67476], Loss: 4.4570\n",
      "Epoch [2/2], Step [65350/67476], Loss: 4.6134\n",
      "Epoch [2/2], Step [65360/67476], Loss: 4.3815\n",
      "Epoch [2/2], Step [65370/67476], Loss: 4.6870\n",
      "Epoch [2/2], Step [65380/67476], Loss: 4.6625\n",
      "Epoch [2/2], Step [65390/67476], Loss: 4.4752\n",
      "Epoch [2/2], Step [65400/67476], Loss: 4.4895\n",
      "Epoch [2/2], Step [65410/67476], Loss: 4.4866\n",
      "Epoch [2/2], Step [65420/67476], Loss: 4.5062\n",
      "Epoch [2/2], Step [65430/67476], Loss: 4.2841\n",
      "Epoch [2/2], Step [65440/67476], Loss: 4.7031\n",
      "Epoch [2/2], Step [65450/67476], Loss: 4.5613\n",
      "Epoch [2/2], Step [65460/67476], Loss: 4.6408\n",
      "Epoch [2/2], Step [65470/67476], Loss: 4.6823\n",
      "Epoch [2/2], Step [65480/67476], Loss: 4.6442\n",
      "Epoch [2/2], Step [65490/67476], Loss: 4.4819\n",
      "Epoch [2/2], Step [65500/67476], Loss: 4.5921\n",
      "Epoch [2/2], Step [65510/67476], Loss: 4.6102\n",
      "Epoch [2/2], Step [65520/67476], Loss: 4.4865\n",
      "Epoch [2/2], Step [65530/67476], Loss: 4.4155\n",
      "Epoch [2/2], Step [65540/67476], Loss: 4.5471\n",
      "Epoch [2/2], Step [65550/67476], Loss: 4.7430\n",
      "Epoch [2/2], Step [65560/67476], Loss: 4.5427\n",
      "Epoch [2/2], Step [65570/67476], Loss: 4.8386\n",
      "Epoch [2/2], Step [65580/67476], Loss: 4.4833\n",
      "Epoch [2/2], Step [65590/67476], Loss: 4.6882\n",
      "Epoch [2/2], Step [65600/67476], Loss: 4.6780\n",
      "Epoch [2/2], Step [65610/67476], Loss: 4.6634\n",
      "Epoch [2/2], Step [65620/67476], Loss: 4.6672\n",
      "Epoch [2/2], Step [65630/67476], Loss: 4.7034\n",
      "Epoch [2/2], Step [65640/67476], Loss: 4.5947\n",
      "Epoch [2/2], Step [65650/67476], Loss: 4.5703\n",
      "Epoch [2/2], Step [65660/67476], Loss: 4.4376\n",
      "Epoch [2/2], Step [65670/67476], Loss: 4.7313\n",
      "Epoch [2/2], Step [65680/67476], Loss: 4.4703\n",
      "Epoch [2/2], Step [65690/67476], Loss: 4.5048\n",
      "Epoch [2/2], Step [65700/67476], Loss: 4.5270\n",
      "Epoch [2/2], Step [65710/67476], Loss: 4.5662\n",
      "Epoch [2/2], Step [65720/67476], Loss: 4.4248\n",
      "Epoch [2/2], Step [65730/67476], Loss: 4.4224\n",
      "Epoch [2/2], Step [65740/67476], Loss: 4.5183\n",
      "Epoch [2/2], Step [65750/67476], Loss: 4.5287\n",
      "Epoch [2/2], Step [65760/67476], Loss: 4.5249\n",
      "Epoch [2/2], Step [65770/67476], Loss: 4.6666\n",
      "Epoch [2/2], Step [65780/67476], Loss: 4.5848\n",
      "Epoch [2/2], Step [65790/67476], Loss: 4.4683\n",
      "Epoch [2/2], Step [65800/67476], Loss: 4.6434\n",
      "Epoch [2/2], Step [65810/67476], Loss: 4.5118\n",
      "Epoch [2/2], Step [65820/67476], Loss: 4.7176\n",
      "Epoch [2/2], Step [65830/67476], Loss: 4.5739\n",
      "Epoch [2/2], Step [65840/67476], Loss: 4.4936\n",
      "Epoch [2/2], Step [65850/67476], Loss: 4.4575\n",
      "Epoch [2/2], Step [65860/67476], Loss: 4.4550\n",
      "Epoch [2/2], Step [65870/67476], Loss: 4.6098\n",
      "Epoch [2/2], Step [65880/67476], Loss: 4.5959\n",
      "Epoch [2/2], Step [65890/67476], Loss: 4.4117\n",
      "Epoch [2/2], Step [65900/67476], Loss: 4.4352\n",
      "Epoch [2/2], Step [65910/67476], Loss: 4.7509\n",
      "Epoch [2/2], Step [65920/67476], Loss: 4.4702\n",
      "Epoch [2/2], Step [65930/67476], Loss: 4.5541\n",
      "Epoch [2/2], Step [65940/67476], Loss: 4.6098\n",
      "Epoch [2/2], Step [65950/67476], Loss: 4.4196\n",
      "Epoch [2/2], Step [65960/67476], Loss: 4.5414\n",
      "Epoch [2/2], Step [65970/67476], Loss: 4.3794\n",
      "Epoch [2/2], Step [65980/67476], Loss: 4.4729\n",
      "Epoch [2/2], Step [65990/67476], Loss: 4.5401\n",
      "Epoch [2/2], Step [66000/67476], Loss: 4.4204\n",
      "Epoch [2/2], Step [66010/67476], Loss: 4.4310\n",
      "Epoch [2/2], Step [66020/67476], Loss: 4.5039\n",
      "Epoch [2/2], Step [66030/67476], Loss: 4.5772\n",
      "Epoch [2/2], Step [66040/67476], Loss: 4.5939\n",
      "Epoch [2/2], Step [66050/67476], Loss: 4.7438\n",
      "Epoch [2/2], Step [66060/67476], Loss: 4.5938\n",
      "Epoch [2/2], Step [66070/67476], Loss: 4.5426\n",
      "Epoch [2/2], Step [66080/67476], Loss: 4.5505\n",
      "Epoch [2/2], Step [66090/67476], Loss: 4.4942\n",
      "Epoch [2/2], Step [66100/67476], Loss: 4.5327\n",
      "Epoch [2/2], Step [66110/67476], Loss: 4.6698\n",
      "Epoch [2/2], Step [66120/67476], Loss: 4.4312\n",
      "Epoch [2/2], Step [66130/67476], Loss: 4.5065\n",
      "Epoch [2/2], Step [66140/67476], Loss: 4.4856\n",
      "Epoch [2/2], Step [66150/67476], Loss: 4.5242\n",
      "Epoch [2/2], Step [66160/67476], Loss: 4.3284\n",
      "Epoch [2/2], Step [66170/67476], Loss: 4.4174\n",
      "Epoch [2/2], Step [66180/67476], Loss: 4.6019\n",
      "Epoch [2/2], Step [66190/67476], Loss: 4.4690\n",
      "Epoch [2/2], Step [66200/67476], Loss: 4.6035\n",
      "Epoch [2/2], Step [66210/67476], Loss: 4.7837\n",
      "Epoch [2/2], Step [66220/67476], Loss: 4.6212\n",
      "Epoch [2/2], Step [66230/67476], Loss: 4.5532\n",
      "Epoch [2/2], Step [66240/67476], Loss: 4.5374\n",
      "Epoch [2/2], Step [66250/67476], Loss: 4.3226\n",
      "Epoch [2/2], Step [66260/67476], Loss: 4.6937\n",
      "Epoch [2/2], Step [66270/67476], Loss: 4.8453\n",
      "Epoch [2/2], Step [66280/67476], Loss: 4.6316\n",
      "Epoch [2/2], Step [66290/67476], Loss: 4.7537\n",
      "Epoch [2/2], Step [66300/67476], Loss: 4.3760\n",
      "Epoch [2/2], Step [66310/67476], Loss: 4.5173\n",
      "Epoch [2/2], Step [66320/67476], Loss: 4.6120\n",
      "Epoch [2/2], Step [66330/67476], Loss: 4.4982\n",
      "Epoch [2/2], Step [66340/67476], Loss: 4.2685\n",
      "Epoch [2/2], Step [66350/67476], Loss: 4.7002\n",
      "Epoch [2/2], Step [66360/67476], Loss: 4.4718\n",
      "Epoch [2/2], Step [66370/67476], Loss: 4.5585\n",
      "Epoch [2/2], Step [66380/67476], Loss: 4.5324\n",
      "Epoch [2/2], Step [66390/67476], Loss: 4.4728\n",
      "Epoch [2/2], Step [66400/67476], Loss: 4.7318\n",
      "Epoch [2/2], Step [66410/67476], Loss: 4.6220\n",
      "Epoch [2/2], Step [66420/67476], Loss: 4.5148\n",
      "Epoch [2/2], Step [66430/67476], Loss: 4.4321\n",
      "Epoch [2/2], Step [66440/67476], Loss: 4.4803\n",
      "Epoch [2/2], Step [66450/67476], Loss: 4.4271\n",
      "Epoch [2/2], Step [66460/67476], Loss: 4.6469\n",
      "Epoch [2/2], Step [66470/67476], Loss: 4.6018\n",
      "Epoch [2/2], Step [66480/67476], Loss: 4.5106\n",
      "Epoch [2/2], Step [66490/67476], Loss: 4.6966\n",
      "Epoch [2/2], Step [66500/67476], Loss: 4.6036\n",
      "Epoch [2/2], Step [66510/67476], Loss: 4.2663\n",
      "Epoch [2/2], Step [66520/67476], Loss: 4.4647\n",
      "Epoch [2/2], Step [66530/67476], Loss: 4.5498\n",
      "Epoch [2/2], Step [66540/67476], Loss: 4.6742\n",
      "Epoch [2/2], Step [66550/67476], Loss: 4.6343\n",
      "Epoch [2/2], Step [66560/67476], Loss: 4.5811\n",
      "Epoch [2/2], Step [66570/67476], Loss: 4.4979\n",
      "Epoch [2/2], Step [66580/67476], Loss: 4.5573\n",
      "Epoch [2/2], Step [66590/67476], Loss: 4.4440\n",
      "Epoch [2/2], Step [66600/67476], Loss: 4.5247\n",
      "Epoch [2/2], Step [66610/67476], Loss: 4.5624\n",
      "Epoch [2/2], Step [66620/67476], Loss: 4.6527\n",
      "Epoch [2/2], Step [66630/67476], Loss: 4.5782\n",
      "Epoch [2/2], Step [66640/67476], Loss: 4.4636\n",
      "Epoch [2/2], Step [66650/67476], Loss: 4.6462\n",
      "Epoch [2/2], Step [66660/67476], Loss: 4.5346\n",
      "Epoch [2/2], Step [66670/67476], Loss: 4.6763\n",
      "Epoch [2/2], Step [66680/67476], Loss: 4.4954\n",
      "Epoch [2/2], Step [66690/67476], Loss: 4.5759\n",
      "Epoch [2/2], Step [66700/67476], Loss: 4.5425\n",
      "Epoch [2/2], Step [66710/67476], Loss: 4.5219\n",
      "Epoch [2/2], Step [66720/67476], Loss: 4.3404\n",
      "Epoch [2/2], Step [66730/67476], Loss: 4.3732\n",
      "Epoch [2/2], Step [66740/67476], Loss: 4.6422\n",
      "Epoch [2/2], Step [66750/67476], Loss: 4.5403\n",
      "Epoch [2/2], Step [66760/67476], Loss: 4.4640\n",
      "Epoch [2/2], Step [66770/67476], Loss: 4.3967\n",
      "Epoch [2/2], Step [66780/67476], Loss: 4.3415\n",
      "Epoch [2/2], Step [66790/67476], Loss: 4.6804\n",
      "Epoch [2/2], Step [66800/67476], Loss: 4.6167\n",
      "Epoch [2/2], Step [66810/67476], Loss: 4.4830\n",
      "Epoch [2/2], Step [66820/67476], Loss: 4.5344\n",
      "Epoch [2/2], Step [66830/67476], Loss: 4.5644\n",
      "Epoch [2/2], Step [66840/67476], Loss: 4.4300\n",
      "Epoch [2/2], Step [66850/67476], Loss: 4.3666\n",
      "Epoch [2/2], Step [66860/67476], Loss: 4.4277\n",
      "Epoch [2/2], Step [66870/67476], Loss: 4.7641\n",
      "Epoch [2/2], Step [66880/67476], Loss: 4.6860\n",
      "Epoch [2/2], Step [66890/67476], Loss: 4.6823\n",
      "Epoch [2/2], Step [66900/67476], Loss: 4.4251\n",
      "Epoch [2/2], Step [66910/67476], Loss: 4.5493\n",
      "Epoch [2/2], Step [66920/67476], Loss: 4.5060\n",
      "Epoch [2/2], Step [66930/67476], Loss: 4.6180\n",
      "Epoch [2/2], Step [66940/67476], Loss: 4.5253\n",
      "Epoch [2/2], Step [66950/67476], Loss: 4.5200\n",
      "Epoch [2/2], Step [66960/67476], Loss: 4.4913\n",
      "Epoch [2/2], Step [66970/67476], Loss: 4.3260\n",
      "Epoch [2/2], Step [66980/67476], Loss: 4.4212\n",
      "Epoch [2/2], Step [66990/67476], Loss: 4.5309\n",
      "Epoch [2/2], Step [67000/67476], Loss: 4.4946\n",
      "Epoch [2/2], Step [67010/67476], Loss: 4.5239\n",
      "Epoch [2/2], Step [67020/67476], Loss: 4.4920\n",
      "Epoch [2/2], Step [67030/67476], Loss: 4.5064\n",
      "Epoch [2/2], Step [67040/67476], Loss: 4.6296\n",
      "Epoch [2/2], Step [67050/67476], Loss: 4.7047\n",
      "Epoch [2/2], Step [67060/67476], Loss: 4.4193\n",
      "Epoch [2/2], Step [67070/67476], Loss: 4.5457\n",
      "Epoch [2/2], Step [67080/67476], Loss: 4.6248\n",
      "Epoch [2/2], Step [67090/67476], Loss: 4.6181\n",
      "Epoch [2/2], Step [67100/67476], Loss: 4.6042\n",
      "Epoch [2/2], Step [67110/67476], Loss: 4.7022\n",
      "Epoch [2/2], Step [67120/67476], Loss: 4.4986\n",
      "Epoch [2/2], Step [67130/67476], Loss: 4.5031\n",
      "Epoch [2/2], Step [67140/67476], Loss: 4.8142\n",
      "Epoch [2/2], Step [67150/67476], Loss: 4.2150\n",
      "Epoch [2/2], Step [67160/67476], Loss: 4.5848\n",
      "Epoch [2/2], Step [67170/67476], Loss: 4.3787\n",
      "Epoch [2/2], Step [67180/67476], Loss: 4.6756\n",
      "Epoch [2/2], Step [67190/67476], Loss: 4.6212\n",
      "Epoch [2/2], Step [67200/67476], Loss: 4.4593\n",
      "Epoch [2/2], Step [67210/67476], Loss: 4.5291\n",
      "Epoch [2/2], Step [67220/67476], Loss: 4.6879\n",
      "Epoch [2/2], Step [67230/67476], Loss: 4.3343\n",
      "Epoch [2/2], Step [67240/67476], Loss: 4.6086\n",
      "Epoch [2/2], Step [67250/67476], Loss: 4.2840\n",
      "Epoch [2/2], Step [67260/67476], Loss: 4.4236\n",
      "Epoch [2/2], Step [67270/67476], Loss: 4.6685\n",
      "Epoch [2/2], Step [67280/67476], Loss: 4.5710\n",
      "Epoch [2/2], Step [67290/67476], Loss: 4.5069\n",
      "Epoch [2/2], Step [67300/67476], Loss: 4.4109\n",
      "Epoch [2/2], Step [67310/67476], Loss: 4.7626\n",
      "Epoch [2/2], Step [67320/67476], Loss: 4.5062\n",
      "Epoch [2/2], Step [67330/67476], Loss: 4.4140\n",
      "Epoch [2/2], Step [67340/67476], Loss: 4.7826\n",
      "Epoch [2/2], Step [67350/67476], Loss: 4.5007\n",
      "Epoch [2/2], Step [67360/67476], Loss: 4.5811\n",
      "Epoch [2/2], Step [67370/67476], Loss: 4.5219\n",
      "Epoch [2/2], Step [67380/67476], Loss: 4.5264\n",
      "Epoch [2/2], Step [67390/67476], Loss: 4.4501\n",
      "Epoch [2/2], Step [67400/67476], Loss: 4.6439\n",
      "Epoch [2/2], Step [67410/67476], Loss: 4.7625\n",
      "Epoch [2/2], Step [67420/67476], Loss: 4.5663\n",
      "Epoch [2/2], Step [67430/67476], Loss: 4.5162\n",
      "Epoch [2/2], Step [67440/67476], Loss: 4.4198\n",
      "Epoch [2/2], Step [67450/67476], Loss: 4.6003\n",
      "Epoch [2/2], Step [67460/67476], Loss: 4.4769\n",
      "Epoch [2/2], Step [67470/67476], Loss: 4.5079\n",
      "Epoch [2/2] Average Loss: 4.5485, Perplexity: 94.49\n"
     ]
    }
   ],
   "source": [
    "train_losses_attention = []\n",
    "perplexities_attention = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(x)\n",
    "        \n",
    "        # Reshape logits and targets for loss computation\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step \"\n",
    "                  f\"[{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    train_losses_attention.append(avg_loss)\n",
    "    perplexities_attention.append(perplexity)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T15:41:28.735931Z",
     "iopub.status.busy": "2024-12-14T15:41:28.735556Z",
     "iopub.status.idle": "2024-12-14T15:41:50.112359Z",
     "shell.execute_reply": "2024-12-14T15:41:50.111370Z",
     "shell.execute_reply.started": "2024-12-14T15:41:28.735888Z"
    },
    "id": "9J5knB1po-q4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for the model with attention: 74.47\n"
     ]
    }
   ],
   "source": [
    "steps = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(steps, train_losses_attention, label='Training Loss')\n",
    "plt.title('Model with attention: Training loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps, perplexities_attention, label='Perplexity')\n",
    "plt.title('Model with attention: Perplexity over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in dev_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits, _ = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(dev_dataloader)\n",
    "perplexity_attention = math.exp(avg_loss)\n",
    "print(f\"Perplexity for the model with attention: {perplexity_attention:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T15:41:59.240381Z",
     "iopub.status.busy": "2024-12-14T15:41:59.239715Z",
     "iopub.status.idle": "2024-12-14T15:41:59.274840Z",
     "shell.execute_reply": "2024-12-14T15:41:59.273981Z",
     "shell.execute_reply.started": "2024-12-14T15:41:59.240350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "\n",
      "[CLS] من در راه[SEP] هوهمی نوشتوگویی پیشگاه ، الحشد و سپس با کار کارگردان «سیدجواد یعقوبی با اعلام شد و به اودرآورند\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, start_text, context_length=32, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = tokenizer.encode(start_text)\n",
    "    context = torch.tensor(generated, dtype=torch.long,\n",
    "                          device=device).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(context_length):\n",
    "            if context.size(1) >= context_length:\n",
    "                break\n",
    "            logits, _ = model(context)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "            context = torch.cat(\n",
    "                [context, next_token_id.unsqueeze(0)], dim=1\n",
    "            )\n",
    "    \n",
    "    generated_text = tokenizer.decode(context[0].tolist())\n",
    "    return generated_text\n",
    "\n",
    "start_text = \" من در راه\"\n",
    "generated_text = generate_text(model, tokenizer, start_text, context_length=32)\n",
    "print(\"Generated Text:\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "؟؟؟  من در راه ؟؟؟ هوهمی نوشتوگویی پیشگاه ، الحشد و سپس با کار کارگردان «سیدجواد یعقوبی با اعلام شد و به اودرآورند"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer with pre-processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-14T15:43:06.118981Z",
     "iopub.status.busy": "2024-12-14T15:43:06.118598Z",
     "iopub.status.idle": "2024-12-14T16:00:04.022676Z",
     "shell.execute_reply": "2024-12-14T16:00:04.021720Z",
     "shell.execute_reply.started": "2024-12-14T15:43:06.118950Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/2], Step [0/64305], Loss: 10.1387\n",
      "Epoch [1/2], Step [10/64305], Loss: 9.9929\n",
      "Epoch [1/2], Step [20/64305], Loss: 8.8446\n",
      "Epoch [1/2], Step [30/64305], Loss: 7.5650\n",
      "Epoch [1/2], Step [40/64305], Loss: 7.1424\n",
      "Epoch [1/2], Step [50/64305], Loss: 7.2266\n",
      "Epoch [1/2], Step [60/64305], Loss: 7.0328\n",
      "Epoch [1/2], Step [70/64305], Loss: 6.7566\n",
      "Epoch [1/2], Step [80/64305], Loss: 6.7321\n",
      "Epoch [1/2], Step [90/64305], Loss: 6.7959\n",
      "Epoch [1/2], Step [100/64305], Loss: 6.9179\n",
      "Epoch [1/2], Step [110/64305], Loss: 6.8096\n",
      "Epoch [1/2], Step [120/64305], Loss: 6.6753\n",
      "Epoch [1/2], Step [130/64305], Loss: 6.7246\n",
      "Epoch [1/2], Step [140/64305], Loss: 6.6534\n",
      "Epoch [1/2], Step [150/64305], Loss: 6.6062\n",
      "Epoch [1/2], Step [160/64305], Loss: 6.6395\n",
      "Epoch [1/2], Step [170/64305], Loss: 6.7370\n",
      "Epoch [1/2], Step [180/64305], Loss: 6.5364\n",
      "Epoch [1/2], Step [190/64305], Loss: 6.6499\n",
      "Epoch [1/2], Step [200/64305], Loss: 6.4982\n",
      "Epoch [1/2], Step [210/64305], Loss: 6.5006\n",
      "Epoch [1/2], Step [220/64305], Loss: 6.5280\n",
      "Epoch [1/2], Step [230/64305], Loss: 6.5760\n",
      "Epoch [1/2], Step [240/64305], Loss: 6.6974\n",
      "Epoch [1/2], Step [250/64305], Loss: 6.5763\n",
      "Epoch [1/2], Step [260/64305], Loss: 6.5439\n",
      "Epoch [1/2], Step [270/64305], Loss: 6.4615\n",
      "Epoch [1/2], Step [280/64305], Loss: 6.5927\n",
      "Epoch [1/2], Step [290/64305], Loss: 6.4283\n",
      "Epoch [1/2], Step [300/64305], Loss: 6.3088\n",
      "Epoch [1/2], Step [310/64305], Loss: 6.3587\n",
      "Epoch [1/2], Step [320/64305], Loss: 6.4308\n",
      "Epoch [1/2], Step [330/64305], Loss: 6.5604\n",
      "Epoch [1/2], Step [340/64305], Loss: 6.4345\n",
      "Epoch [1/2], Step [350/64305], Loss: 6.3535\n",
      "Epoch [1/2], Step [360/64305], Loss: 6.3453\n",
      "Epoch [1/2], Step [370/64305], Loss: 6.4529\n",
      "Epoch [1/2], Step [380/64305], Loss: 6.4351\n",
      "Epoch [1/2], Step [390/64305], Loss: 6.4706\n",
      "Epoch [1/2], Step [400/64305], Loss: 6.5611\n",
      "Epoch [1/2], Step [410/64305], Loss: 6.2165\n",
      "Epoch [1/2], Step [420/64305], Loss: 6.5066\n",
      "Epoch [1/2], Step [430/64305], Loss: 6.3743\n",
      "Epoch [1/2], Step [440/64305], Loss: 6.3742\n",
      "Epoch [1/2], Step [450/64305], Loss: 6.6651\n",
      "Epoch [1/2], Step [460/64305], Loss: 6.1340\n",
      "Epoch [1/2], Step [470/64305], Loss: 6.3026\n",
      "Epoch [1/2], Step [480/64305], Loss: 6.3997\n",
      "Epoch [1/2], Step [490/64305], Loss: 6.3197\n",
      "Epoch [1/2], Step [500/64305], Loss: 6.5274\n",
      "Epoch [1/2], Step [510/64305], Loss: 6.2679\n",
      "Epoch [1/2], Step [520/64305], Loss: 6.3994\n",
      "Epoch [1/2], Step [530/64305], Loss: 6.1499\n",
      "Epoch [1/2], Step [540/64305], Loss: 6.3386\n",
      "Epoch [1/2], Step [550/64305], Loss: 6.0141\n",
      "Epoch [1/2], Step [560/64305], Loss: 6.1094\n",
      "Epoch [1/2], Step [570/64305], Loss: 6.2874\n",
      "Epoch [1/2], Step [580/64305], Loss: 6.3807\n",
      "Epoch [1/2], Step [590/64305], Loss: 6.2903\n",
      "Epoch [1/2], Step [600/64305], Loss: 6.3161\n",
      "Epoch [1/2], Step [610/64305], Loss: 6.1815\n",
      "Epoch [1/2], Step [620/64305], Loss: 6.3134\n",
      "Epoch [1/2], Step [630/64305], Loss: 6.1937\n",
      "Epoch [1/2], Step [640/64305], Loss: 6.1811\n",
      "Epoch [1/2], Step [650/64305], Loss: 6.3331\n",
      "Epoch [1/2], Step [660/64305], Loss: 6.2454\n",
      "Epoch [1/2], Step [670/64305], Loss: 6.3616\n",
      "Epoch [1/2], Step [680/64305], Loss: 6.4965\n",
      "Epoch [1/2], Step [690/64305], Loss: 6.1053\n",
      "Epoch [1/2], Step [700/64305], Loss: 6.2582\n",
      "Epoch [1/2], Step [710/64305], Loss: 6.1031\n",
      "Epoch [1/2], Step [720/64305], Loss: 6.2461\n",
      "Epoch [1/2], Step [730/64305], Loss: 6.0561\n",
      "Epoch [1/2], Step [740/64305], Loss: 6.1241\n",
      "Epoch [1/2], Step [750/64305], Loss: 6.1182\n",
      "Epoch [1/2], Step [760/64305], Loss: 6.2043\n",
      "Epoch [1/2], Step [770/64305], Loss: 6.0687\n",
      "Epoch [1/2], Step [780/64305], Loss: 6.1017\n",
      "Epoch [1/2], Step [790/64305], Loss: 6.1048\n",
      "Epoch [1/2], Step [800/64305], Loss: 6.1674\n",
      "Epoch [1/2], Step [810/64305], Loss: 6.2760\n",
      "Epoch [1/2], Step [820/64305], Loss: 5.9746\n",
      "Epoch [1/2], Step [830/64305], Loss: 6.2027\n",
      "Epoch [1/2], Step [840/64305], Loss: 6.1840\n",
      "Epoch [1/2], Step [850/64305], Loss: 6.0064\n",
      "Epoch [1/2], Step [860/64305], Loss: 6.0343\n",
      "Epoch [1/2], Step [870/64305], Loss: 6.0395\n",
      "Epoch [1/2], Step [880/64305], Loss: 6.1144\n",
      "Epoch [1/2], Step [890/64305], Loss: 6.0024\n",
      "Epoch [1/2], Step [900/64305], Loss: 6.2667\n",
      "Epoch [1/2], Step [910/64305], Loss: 6.0852\n",
      "Epoch [1/2], Step [920/64305], Loss: 6.0237\n",
      "Epoch [1/2], Step [930/64305], Loss: 6.1093\n",
      "Epoch [1/2], Step [940/64305], Loss: 6.1319\n",
      "Epoch [1/2], Step [950/64305], Loss: 6.0291\n",
      "Epoch [1/2], Step [960/64305], Loss: 6.4276\n",
      "Epoch [1/2], Step [970/64305], Loss: 6.0557\n",
      "Epoch [1/2], Step [980/64305], Loss: 5.9528\n",
      "Epoch [1/2], Step [990/64305], Loss: 6.3252\n",
      "Epoch [1/2], Step [1000/64305], Loss: 6.1668\n",
      "Epoch [1/2], Step [1010/64305], Loss: 6.0854\n",
      "Epoch [1/2], Step [1020/64305], Loss: 6.2215\n",
      "Epoch [1/2], Step [1030/64305], Loss: 6.0121\n",
      "Epoch [1/2], Step [1040/64305], Loss: 6.1772\n",
      "Epoch [1/2], Step [1050/64305], Loss: 6.3374\n",
      "Epoch [1/2], Step [1060/64305], Loss: 6.1537\n",
      "Epoch [1/2], Step [1070/64305], Loss: 6.1488\n",
      "Epoch [1/2], Step [1080/64305], Loss: 6.2438\n",
      "Epoch [1/2], Step [1090/64305], Loss: 6.1059\n",
      "Epoch [1/2], Step [1100/64305], Loss: 6.0626\n",
      "Epoch [1/2], Step [1110/64305], Loss: 5.9007\n",
      "Epoch [1/2], Step [1120/64305], Loss: 6.2296\n",
      "Epoch [1/2], Step [1130/64305], Loss: 6.0816\n",
      "Epoch [1/2], Step [1140/64305], Loss: 6.2502\n",
      "Epoch [1/2], Step [1150/64305], Loss: 6.2068\n",
      "Epoch [1/2], Step [1160/64305], Loss: 6.0118\n",
      "Epoch [1/2], Step [1170/64305], Loss: 6.2502\n",
      "Epoch [1/2], Step [1180/64305], Loss: 6.0686\n",
      "Epoch [1/2], Step [1190/64305], Loss: 6.0252\n",
      "Epoch [1/2], Step [1200/64305], Loss: 6.0124\n",
      "Epoch [1/2], Step [1210/64305], Loss: 6.0978\n",
      "Epoch [1/2], Step [1220/64305], Loss: 5.9881\n",
      "Epoch [1/2], Step [1230/64305], Loss: 6.2336\n",
      "Epoch [1/2], Step [1240/64305], Loss: 5.8372\n",
      "Epoch [1/2], Step [1250/64305], Loss: 5.8899\n",
      "Epoch [1/2], Step [1260/64305], Loss: 6.0461\n",
      "Epoch [1/2], Step [1270/64305], Loss: 6.0073\n",
      "Epoch [1/2], Step [1280/64305], Loss: 5.9122\n",
      "Epoch [1/2], Step [1290/64305], Loss: 6.0940\n",
      "Epoch [1/2], Step [1300/64305], Loss: 6.2103\n",
      "Epoch [1/2], Step [1310/64305], Loss: 5.8964\n",
      "Epoch [1/2], Step [1320/64305], Loss: 6.0803\n",
      "Epoch [1/2], Step [1330/64305], Loss: 6.3258\n",
      "Epoch [1/2], Step [1340/64305], Loss: 5.8375\n",
      "Epoch [1/2], Step [1350/64305], Loss: 5.9944\n",
      "Epoch [1/2], Step [1360/64305], Loss: 6.3340\n",
      "Epoch [1/2], Step [1370/64305], Loss: 6.0351\n",
      "Epoch [1/2], Step [1380/64305], Loss: 6.1453\n",
      "Epoch [1/2], Step [1390/64305], Loss: 5.8685\n",
      "Epoch [1/2], Step [1400/64305], Loss: 5.9354\n",
      "Epoch [1/2], Step [1410/64305], Loss: 6.2259\n",
      "Epoch [1/2], Step [1420/64305], Loss: 5.6544\n",
      "Epoch [1/2], Step [1430/64305], Loss: 5.9348\n",
      "Epoch [1/2], Step [1440/64305], Loss: 6.0351\n",
      "Epoch [1/2], Step [1450/64305], Loss: 5.9211\n",
      "Epoch [1/2], Step [1460/64305], Loss: 6.0356\n",
      "Epoch [1/2], Step [1470/64305], Loss: 5.9148\n",
      "Epoch [1/2], Step [1480/64305], Loss: 5.9166\n",
      "Epoch [1/2], Step [1490/64305], Loss: 5.7519\n",
      "Epoch [1/2], Step [1500/64305], Loss: 6.0005\n",
      "Epoch [1/2], Step [1510/64305], Loss: 6.1934\n",
      "Epoch [1/2], Step [1520/64305], Loss: 6.0378\n",
      "Epoch [1/2], Step [1530/64305], Loss: 5.9700\n",
      "Epoch [1/2], Step [1540/64305], Loss: 5.8730\n",
      "Epoch [1/2], Step [1550/64305], Loss: 6.0069\n",
      "Epoch [1/2], Step [1560/64305], Loss: 5.9931\n",
      "Epoch [1/2], Step [1570/64305], Loss: 5.8811\n",
      "Epoch [1/2], Step [1580/64305], Loss: 6.0835\n",
      "Epoch [1/2], Step [1590/64305], Loss: 5.8691\n",
      "Epoch [1/2], Step [1600/64305], Loss: 6.0579\n",
      "Epoch [1/2], Step [1610/64305], Loss: 5.9336\n",
      "Epoch [1/2], Step [1620/64305], Loss: 5.9059\n",
      "Epoch [1/2], Step [1630/64305], Loss: 6.3858\n",
      "Epoch [1/2], Step [1640/64305], Loss: 5.8583\n",
      "Epoch [1/2], Step [1650/64305], Loss: 5.8515\n",
      "Epoch [1/2], Step [1660/64305], Loss: 5.8037\n",
      "Epoch [1/2], Step [1670/64305], Loss: 5.8877\n",
      "Epoch [1/2], Step [1680/64305], Loss: 5.9997\n",
      "Epoch [1/2], Step [1690/64305], Loss: 6.0125\n",
      "Epoch [1/2], Step [1700/64305], Loss: 5.8881\n",
      "Epoch [1/2], Step [1710/64305], Loss: 5.9071\n",
      "Epoch [1/2], Step [1720/64305], Loss: 5.9357\n",
      "Epoch [1/2], Step [1730/64305], Loss: 6.0687\n",
      "Epoch [1/2], Step [1740/64305], Loss: 6.1358\n",
      "Epoch [1/2], Step [1750/64305], Loss: 5.9438\n",
      "Epoch [1/2], Step [1760/64305], Loss: 5.8554\n",
      "Epoch [1/2], Step [1770/64305], Loss: 5.9215\n",
      "Epoch [1/2], Step [1780/64305], Loss: 6.0364\n",
      "Epoch [1/2], Step [1790/64305], Loss: 5.9878\n",
      "Epoch [1/2], Step [1800/64305], Loss: 5.7935\n",
      "Epoch [1/2], Step [1810/64305], Loss: 5.7825\n",
      "Epoch [1/2], Step [1820/64305], Loss: 6.0479\n",
      "Epoch [1/2], Step [1830/64305], Loss: 6.0171\n",
      "Epoch [1/2], Step [1840/64305], Loss: 5.9486\n",
      "Epoch [1/2], Step [1850/64305], Loss: 5.9556\n",
      "Epoch [1/2], Step [1860/64305], Loss: 6.0009\n",
      "Epoch [1/2], Step [1870/64305], Loss: 6.0817\n",
      "Epoch [1/2], Step [1880/64305], Loss: 6.0171\n",
      "Epoch [1/2], Step [1890/64305], Loss: 5.8827\n",
      "Epoch [1/2], Step [1900/64305], Loss: 5.8282\n",
      "Epoch [1/2], Step [1910/64305], Loss: 5.9548\n",
      "Epoch [1/2], Step [1920/64305], Loss: 5.9453\n",
      "Epoch [1/2], Step [1930/64305], Loss: 6.1917\n",
      "Epoch [1/2], Step [1940/64305], Loss: 5.9582\n",
      "Epoch [1/2], Step [1950/64305], Loss: 5.8788\n",
      "Epoch [1/2], Step [1960/64305], Loss: 5.8495\n",
      "Epoch [1/2], Step [1970/64305], Loss: 5.8444\n",
      "Epoch [1/2], Step [1980/64305], Loss: 5.7704\n",
      "Epoch [1/2], Step [1990/64305], Loss: 5.9047\n",
      "Epoch [1/2], Step [2000/64305], Loss: 6.0175\n",
      "Epoch [1/2], Step [2010/64305], Loss: 6.0465\n",
      "Epoch [1/2], Step [2020/64305], Loss: 5.9961\n",
      "Epoch [1/2], Step [2030/64305], Loss: 5.8591\n",
      "Epoch [1/2], Step [2040/64305], Loss: 5.8515\n",
      "Epoch [1/2], Step [2050/64305], Loss: 5.9331\n",
      "Epoch [1/2], Step [2060/64305], Loss: 5.9186\n",
      "Epoch [1/2], Step [2070/64305], Loss: 6.0020\n",
      "Epoch [1/2], Step [2080/64305], Loss: 6.0088\n",
      "Epoch [1/2], Step [2090/64305], Loss: 6.1378\n",
      "Epoch [1/2], Step [2100/64305], Loss: 5.7536\n",
      "Epoch [1/2], Step [2110/64305], Loss: 5.8111\n",
      "Epoch [1/2], Step [2120/64305], Loss: 6.0272\n",
      "Epoch [1/2], Step [2130/64305], Loss: 5.8062\n",
      "Epoch [1/2], Step [2140/64305], Loss: 5.8110\n",
      "Epoch [1/2], Step [2150/64305], Loss: 5.8036\n",
      "Epoch [1/2], Step [2160/64305], Loss: 5.8239\n",
      "Epoch [1/2], Step [2170/64305], Loss: 6.1633\n",
      "Epoch [1/2], Step [2180/64305], Loss: 5.7141\n",
      "Epoch [1/2], Step [2190/64305], Loss: 5.9948\n",
      "Epoch [1/2], Step [2200/64305], Loss: 5.7613\n",
      "Epoch [1/2], Step [2210/64305], Loss: 5.9004\n",
      "Epoch [1/2], Step [2220/64305], Loss: 5.8864\n",
      "Epoch [1/2], Step [2230/64305], Loss: 5.8227\n",
      "Epoch [1/2], Step [2240/64305], Loss: 5.8677\n",
      "Epoch [1/2], Step [2250/64305], Loss: 5.9645\n",
      "Epoch [1/2], Step [2260/64305], Loss: 5.9409\n",
      "Epoch [1/2], Step [2270/64305], Loss: 5.9010\n",
      "Epoch [1/2], Step [2280/64305], Loss: 5.9260\n",
      "Epoch [1/2], Step [2290/64305], Loss: 5.6609\n",
      "Epoch [1/2], Step [2300/64305], Loss: 6.0346\n",
      "Epoch [1/2], Step [2310/64305], Loss: 5.6957\n",
      "Epoch [1/2], Step [2320/64305], Loss: 6.1281\n",
      "Epoch [1/2], Step [2330/64305], Loss: 5.8034\n",
      "Epoch [1/2], Step [2340/64305], Loss: 5.8248\n",
      "Epoch [1/2], Step [2350/64305], Loss: 5.6750\n",
      "Epoch [1/2], Step [2360/64305], Loss: 5.9617\n",
      "Epoch [1/2], Step [2370/64305], Loss: 5.9295\n",
      "Epoch [1/2], Step [2380/64305], Loss: 6.0667\n",
      "Epoch [1/2], Step [2390/64305], Loss: 5.9071\n",
      "Epoch [1/2], Step [2400/64305], Loss: 5.9719\n",
      "Epoch [1/2], Step [2410/64305], Loss: 6.0452\n",
      "Epoch [1/2], Step [2420/64305], Loss: 5.7028\n",
      "Epoch [1/2], Step [2430/64305], Loss: 6.1056\n",
      "Epoch [1/2], Step [2440/64305], Loss: 5.9878\n",
      "Epoch [1/2], Step [2450/64305], Loss: 5.9947\n",
      "Epoch [1/2], Step [2460/64305], Loss: 5.7458\n",
      "Epoch [1/2], Step [2470/64305], Loss: 5.7657\n",
      "Epoch [1/2], Step [2480/64305], Loss: 5.6660\n",
      "Epoch [1/2], Step [2490/64305], Loss: 5.6927\n",
      "Epoch [1/2], Step [2500/64305], Loss: 5.8724\n",
      "Epoch [1/2], Step [2510/64305], Loss: 5.7561\n",
      "Epoch [1/2], Step [2520/64305], Loss: 5.7906\n",
      "Epoch [1/2], Step [2530/64305], Loss: 5.9262\n",
      "Epoch [1/2], Step [2540/64305], Loss: 5.8744\n",
      "Epoch [1/2], Step [2550/64305], Loss: 5.8055\n",
      "Epoch [1/2], Step [2560/64305], Loss: 5.6730\n",
      "Epoch [1/2], Step [2570/64305], Loss: 5.8673\n",
      "Epoch [1/2], Step [2580/64305], Loss: 5.7788\n",
      "Epoch [1/2], Step [2590/64305], Loss: 5.6858\n",
      "Epoch [1/2], Step [2600/64305], Loss: 5.7883\n",
      "Epoch [1/2], Step [2610/64305], Loss: 5.6253\n",
      "Epoch [1/2], Step [2620/64305], Loss: 5.9214\n",
      "Epoch [1/2], Step [2630/64305], Loss: 5.7879\n",
      "Epoch [1/2], Step [2640/64305], Loss: 5.9370\n",
      "Epoch [1/2], Step [2650/64305], Loss: 6.0000\n",
      "Epoch [1/2], Step [2660/64305], Loss: 6.0028\n",
      "Epoch [1/2], Step [2670/64305], Loss: 5.9992\n",
      "Epoch [1/2], Step [2680/64305], Loss: 5.7381\n",
      "Epoch [1/2], Step [2690/64305], Loss: 5.7552\n",
      "Epoch [1/2], Step [2700/64305], Loss: 5.6287\n",
      "Epoch [1/2], Step [2710/64305], Loss: 5.8643\n",
      "Epoch [1/2], Step [2720/64305], Loss: 5.7797\n",
      "Epoch [1/2], Step [2730/64305], Loss: 6.0613\n",
      "Epoch [1/2], Step [2740/64305], Loss: 5.8531\n",
      "Epoch [1/2], Step [2750/64305], Loss: 5.7476\n",
      "Epoch [1/2], Step [2760/64305], Loss: 5.8602\n",
      "Epoch [1/2], Step [2770/64305], Loss: 5.8402\n",
      "Epoch [1/2], Step [2780/64305], Loss: 5.8433\n",
      "Epoch [1/2], Step [2790/64305], Loss: 5.5912\n",
      "Epoch [1/2], Step [2800/64305], Loss: 5.8487\n",
      "Epoch [1/2], Step [2810/64305], Loss: 5.8111\n",
      "Epoch [1/2], Step [2820/64305], Loss: 5.7569\n",
      "Epoch [1/2], Step [2830/64305], Loss: 5.6686\n",
      "Epoch [1/2], Step [2840/64305], Loss: 5.7223\n",
      "Epoch [1/2], Step [2850/64305], Loss: 5.7017\n",
      "Epoch [1/2], Step [2860/64305], Loss: 5.7434\n",
      "Epoch [1/2], Step [2870/64305], Loss: 5.8128\n",
      "Epoch [1/2], Step [2880/64305], Loss: 5.8064\n",
      "Epoch [1/2], Step [2890/64305], Loss: 5.7074\n",
      "Epoch [1/2], Step [2900/64305], Loss: 5.8623\n",
      "Epoch [1/2], Step [2910/64305], Loss: 5.7273\n",
      "Epoch [1/2], Step [2920/64305], Loss: 5.8438\n",
      "Epoch [1/2], Step [2930/64305], Loss: 5.7395\n",
      "Epoch [1/2], Step [2940/64305], Loss: 5.8332\n",
      "Epoch [1/2], Step [2950/64305], Loss: 5.6060\n",
      "Epoch [1/2], Step [2960/64305], Loss: 5.6265\n",
      "Epoch [1/2], Step [2970/64305], Loss: 5.7570\n",
      "Epoch [1/2], Step [2980/64305], Loss: 5.7310\n",
      "Epoch [1/2], Step [2990/64305], Loss: 5.6601\n",
      "Epoch [1/2], Step [3000/64305], Loss: 5.7765\n",
      "Epoch [1/2], Step [3010/64305], Loss: 5.6426\n",
      "Epoch [1/2], Step [3020/64305], Loss: 5.8953\n",
      "Epoch [1/2], Step [3030/64305], Loss: 5.9505\n",
      "Epoch [1/2], Step [3040/64305], Loss: 5.6778\n",
      "Epoch [1/2], Step [3050/64305], Loss: 5.8284\n",
      "Epoch [1/2], Step [3060/64305], Loss: 5.7430\n",
      "Epoch [1/2], Step [3070/64305], Loss: 5.8265\n",
      "Epoch [1/2], Step [3080/64305], Loss: 5.8251\n",
      "Epoch [1/2], Step [3090/64305], Loss: 5.8368\n",
      "Epoch [1/2], Step [3100/64305], Loss: 5.9063\n",
      "Epoch [1/2], Step [3110/64305], Loss: 5.6906\n",
      "Epoch [1/2], Step [3120/64305], Loss: 5.8246\n",
      "Epoch [1/2], Step [3130/64305], Loss: 5.3487\n",
      "Epoch [1/2], Step [3140/64305], Loss: 5.8708\n",
      "Epoch [1/2], Step [3150/64305], Loss: 5.6853\n",
      "Epoch [1/2], Step [3160/64305], Loss: 5.8375\n",
      "Epoch [1/2], Step [3170/64305], Loss: 5.7267\n",
      "Epoch [1/2], Step [3180/64305], Loss: 5.7454\n",
      "Epoch [1/2], Step [3190/64305], Loss: 5.4501\n",
      "Epoch [1/2], Step [3200/64305], Loss: 5.8065\n",
      "Epoch [1/2], Step [3210/64305], Loss: 5.8971\n",
      "Epoch [1/2], Step [3220/64305], Loss: 5.7589\n",
      "Epoch [1/2], Step [3230/64305], Loss: 5.8431\n",
      "Epoch [1/2], Step [3240/64305], Loss: 5.4381\n",
      "Epoch [1/2], Step [3250/64305], Loss: 5.7574\n",
      "Epoch [1/2], Step [3260/64305], Loss: 5.6962\n",
      "Epoch [1/2], Step [3270/64305], Loss: 5.7211\n",
      "Epoch [1/2], Step [3280/64305], Loss: 5.7661\n",
      "Epoch [1/2], Step [3290/64305], Loss: 5.5729\n",
      "Epoch [1/2], Step [3300/64305], Loss: 5.7263\n",
      "Epoch [1/2], Step [3310/64305], Loss: 5.8090\n",
      "Epoch [1/2], Step [3320/64305], Loss: 5.7950\n",
      "Epoch [1/2], Step [3330/64305], Loss: 5.9060\n",
      "Epoch [1/2], Step [3340/64305], Loss: 5.6973\n",
      "Epoch [1/2], Step [3350/64305], Loss: 5.7287\n",
      "Epoch [1/2], Step [3360/64305], Loss: 5.5869\n",
      "Epoch [1/2], Step [3370/64305], Loss: 5.8662\n",
      "Epoch [1/2], Step [3380/64305], Loss: 5.5845\n",
      "Epoch [1/2], Step [3390/64305], Loss: 5.6836\n",
      "Epoch [1/2], Step [3400/64305], Loss: 5.6078\n",
      "Epoch [1/2], Step [3410/64305], Loss: 5.6162\n",
      "Epoch [1/2], Step [3420/64305], Loss: 5.8060\n",
      "Epoch [1/2], Step [3430/64305], Loss: 5.8704\n",
      "Epoch [1/2], Step [3440/64305], Loss: 5.8324\n",
      "Epoch [1/2], Step [3450/64305], Loss: 5.9689\n",
      "Epoch [1/2], Step [3460/64305], Loss: 5.6721\n",
      "Epoch [1/2], Step [3470/64305], Loss: 5.8085\n",
      "Epoch [1/2], Step [3480/64305], Loss: 5.6396\n",
      "Epoch [1/2], Step [3490/64305], Loss: 5.6848\n",
      "Epoch [1/2], Step [3500/64305], Loss: 5.6678\n",
      "Epoch [1/2], Step [3510/64305], Loss: 5.7519\n",
      "Epoch [1/2], Step [3520/64305], Loss: 5.7497\n",
      "Epoch [1/2], Step [3530/64305], Loss: 5.9391\n",
      "Epoch [1/2], Step [3540/64305], Loss: 5.8786\n",
      "Epoch [1/2], Step [3550/64305], Loss: 5.5044\n",
      "Epoch [1/2], Step [3560/64305], Loss: 5.8715\n",
      "Epoch [1/2], Step [3570/64305], Loss: 5.7873\n",
      "Epoch [1/2], Step [3580/64305], Loss: 5.5317\n",
      "Epoch [1/2], Step [3590/64305], Loss: 5.7705\n",
      "Epoch [1/2], Step [3600/64305], Loss: 6.0146\n",
      "Epoch [1/2], Step [3610/64305], Loss: 5.6676\n",
      "Epoch [1/2], Step [3620/64305], Loss: 5.7866\n",
      "Epoch [1/2], Step [3630/64305], Loss: 5.8938\n",
      "Epoch [1/2], Step [3640/64305], Loss: 5.6669\n",
      "Epoch [1/2], Step [3650/64305], Loss: 5.8557\n",
      "Epoch [1/2], Step [3660/64305], Loss: 5.8540\n",
      "Epoch [1/2], Step [3670/64305], Loss: 5.6734\n",
      "Epoch [1/2], Step [3680/64305], Loss: 5.6123\n",
      "Epoch [1/2], Step [3690/64305], Loss: 5.7889\n",
      "Epoch [1/2], Step [3700/64305], Loss: 5.5421\n",
      "Epoch [1/2], Step [3710/64305], Loss: 5.7614\n",
      "Epoch [1/2], Step [3720/64305], Loss: 5.5141\n",
      "Epoch [1/2], Step [3730/64305], Loss: 5.7560\n",
      "Epoch [1/2], Step [3740/64305], Loss: 5.7525\n",
      "Epoch [1/2], Step [3750/64305], Loss: 5.9692\n",
      "Epoch [1/2], Step [3760/64305], Loss: 5.6800\n",
      "Epoch [1/2], Step [3770/64305], Loss: 5.4669\n",
      "Epoch [1/2], Step [3780/64305], Loss: 5.8035\n",
      "Epoch [1/2], Step [3790/64305], Loss: 5.7910\n",
      "Epoch [1/2], Step [3800/64305], Loss: 5.8436\n",
      "Epoch [1/2], Step [3810/64305], Loss: 5.5885\n",
      "Epoch [1/2], Step [3820/64305], Loss: 5.6846\n",
      "Epoch [1/2], Step [3830/64305], Loss: 5.7445\n",
      "Epoch [1/2], Step [3840/64305], Loss: 5.5507\n",
      "Epoch [1/2], Step [3850/64305], Loss: 5.6871\n",
      "Epoch [1/2], Step [3860/64305], Loss: 5.6707\n",
      "Epoch [1/2], Step [3870/64305], Loss: 5.6516\n",
      "Epoch [1/2], Step [3880/64305], Loss: 5.6290\n",
      "Epoch [1/2], Step [3890/64305], Loss: 5.7218\n",
      "Epoch [1/2], Step [3900/64305], Loss: 5.7805\n",
      "Epoch [1/2], Step [3910/64305], Loss: 5.5579\n",
      "Epoch [1/2], Step [3920/64305], Loss: 5.8092\n",
      "Epoch [1/2], Step [3930/64305], Loss: 5.7390\n",
      "Epoch [1/2], Step [3940/64305], Loss: 5.5759\n",
      "Epoch [1/2], Step [3950/64305], Loss: 5.6095\n",
      "Epoch [1/2], Step [3960/64305], Loss: 5.6156\n",
      "Epoch [1/2], Step [3970/64305], Loss: 5.7892\n",
      "Epoch [1/2], Step [3980/64305], Loss: 5.6318\n",
      "Epoch [1/2], Step [3990/64305], Loss: 5.4773\n",
      "Epoch [1/2], Step [4000/64305], Loss: 5.5860\n",
      "Epoch [1/2], Step [4010/64305], Loss: 5.5801\n",
      "Epoch [1/2], Step [4020/64305], Loss: 5.6122\n",
      "Epoch [1/2], Step [4030/64305], Loss: 5.5926\n",
      "Epoch [1/2], Step [4040/64305], Loss: 5.6999\n",
      "Epoch [1/2], Step [4050/64305], Loss: 5.6982\n",
      "Epoch [1/2], Step [4060/64305], Loss: 5.4753\n",
      "Epoch [1/2], Step [4070/64305], Loss: 5.9405\n",
      "Epoch [1/2], Step [4080/64305], Loss: 5.8091\n",
      "Epoch [1/2], Step [4090/64305], Loss: 5.8121\n",
      "Epoch [1/2], Step [4100/64305], Loss: 5.5625\n",
      "Epoch [1/2], Step [4110/64305], Loss: 5.7782\n",
      "Epoch [1/2], Step [4120/64305], Loss: 5.5560\n",
      "Epoch [1/2], Step [4130/64305], Loss: 5.8576\n",
      "Epoch [1/2], Step [4140/64305], Loss: 5.8431\n",
      "Epoch [1/2], Step [4150/64305], Loss: 5.5592\n",
      "Epoch [1/2], Step [4160/64305], Loss: 5.6127\n",
      "Epoch [1/2], Step [4170/64305], Loss: 5.6716\n",
      "Epoch [1/2], Step [4180/64305], Loss: 5.6109\n",
      "Epoch [1/2], Step [4190/64305], Loss: 5.7782\n",
      "Epoch [1/2], Step [4200/64305], Loss: 5.6036\n",
      "Epoch [1/2], Step [4210/64305], Loss: 5.7306\n",
      "Epoch [1/2], Step [4220/64305], Loss: 5.3766\n",
      "Epoch [1/2], Step [4230/64305], Loss: 5.6268\n",
      "Epoch [1/2], Step [4240/64305], Loss: 5.3919\n",
      "Epoch [1/2], Step [4250/64305], Loss: 5.5532\n",
      "Epoch [1/2], Step [4260/64305], Loss: 5.5833\n",
      "Epoch [1/2], Step [4270/64305], Loss: 5.5373\n",
      "Epoch [1/2], Step [4280/64305], Loss: 5.6639\n",
      "Epoch [1/2], Step [4290/64305], Loss: 5.6888\n",
      "Epoch [1/2], Step [4300/64305], Loss: 5.5105\n",
      "Epoch [1/2], Step [4310/64305], Loss: 5.6066\n",
      "Epoch [1/2], Step [4320/64305], Loss: 5.5349\n",
      "Epoch [1/2], Step [4330/64305], Loss: 5.8028\n",
      "Epoch [1/2], Step [4340/64305], Loss: 5.4571\n",
      "Epoch [1/2], Step [4350/64305], Loss: 5.6066\n",
      "Epoch [1/2], Step [4360/64305], Loss: 5.7354\n",
      "Epoch [1/2], Step [4370/64305], Loss: 5.4884\n",
      "Epoch [1/2], Step [4380/64305], Loss: 5.6627\n",
      "Epoch [1/2], Step [4390/64305], Loss: 5.7100\n",
      "Epoch [1/2], Step [4400/64305], Loss: 5.6377\n",
      "Epoch [1/2], Step [4410/64305], Loss: 5.5779\n",
      "Epoch [1/2], Step [4420/64305], Loss: 5.4536\n",
      "Epoch [1/2], Step [4430/64305], Loss: 5.5922\n",
      "Epoch [1/2], Step [4440/64305], Loss: 5.6552\n",
      "Epoch [1/2], Step [4450/64305], Loss: 5.5401\n",
      "Epoch [1/2], Step [4460/64305], Loss: 5.6915\n",
      "Epoch [1/2], Step [4470/64305], Loss: 5.4942\n",
      "Epoch [1/2], Step [4480/64305], Loss: 5.4608\n",
      "Epoch [1/2], Step [4490/64305], Loss: 5.5325\n",
      "Epoch [1/2], Step [4500/64305], Loss: 5.7751\n",
      "Epoch [1/2], Step [4510/64305], Loss: 5.5464\n",
      "Epoch [1/2], Step [4520/64305], Loss: 5.6474\n",
      "Epoch [1/2], Step [4530/64305], Loss: 5.6683\n",
      "Epoch [1/2], Step [4540/64305], Loss: 5.5911\n",
      "Epoch [1/2], Step [4550/64305], Loss: 5.7377\n",
      "Epoch [1/2], Step [4560/64305], Loss: 5.5699\n",
      "Epoch [1/2], Step [4570/64305], Loss: 5.5098\n",
      "Epoch [1/2], Step [4580/64305], Loss: 5.6612\n",
      "Epoch [1/2], Step [4590/64305], Loss: 5.5829\n",
      "Epoch [1/2], Step [4600/64305], Loss: 5.4358\n",
      "Epoch [1/2], Step [4610/64305], Loss: 5.5825\n",
      "Epoch [1/2], Step [4620/64305], Loss: 5.8519\n",
      "Epoch [1/2], Step [4630/64305], Loss: 5.5728\n",
      "Epoch [1/2], Step [4640/64305], Loss: 5.5736\n",
      "Epoch [1/2], Step [4650/64305], Loss: 5.5191\n",
      "Epoch [1/2], Step [4660/64305], Loss: 5.6352\n",
      "Epoch [1/2], Step [4670/64305], Loss: 5.5913\n",
      "Epoch [1/2], Step [4680/64305], Loss: 5.8787\n",
      "Epoch [1/2], Step [4690/64305], Loss: 5.5109\n",
      "Epoch [1/2], Step [4700/64305], Loss: 5.7149\n",
      "Epoch [1/2], Step [4710/64305], Loss: 5.6171\n",
      "Epoch [1/2], Step [4720/64305], Loss: 5.6169\n",
      "Epoch [1/2], Step [4730/64305], Loss: 5.5989\n",
      "Epoch [1/2], Step [4740/64305], Loss: 5.5876\n",
      "Epoch [1/2], Step [4750/64305], Loss: 5.7222\n",
      "Epoch [1/2], Step [4760/64305], Loss: 5.5136\n",
      "Epoch [1/2], Step [4770/64305], Loss: 5.7893\n",
      "Epoch [1/2], Step [4780/64305], Loss: 5.6823\n",
      "Epoch [1/2], Step [4790/64305], Loss: 5.5392\n",
      "Epoch [1/2], Step [4800/64305], Loss: 5.5194\n",
      "Epoch [1/2], Step [4810/64305], Loss: 5.5222\n",
      "Epoch [1/2], Step [4820/64305], Loss: 5.8283\n",
      "Epoch [1/2], Step [4830/64305], Loss: 5.3738\n",
      "Epoch [1/2], Step [4840/64305], Loss: 5.4860\n",
      "Epoch [1/2], Step [4850/64305], Loss: 5.5402\n",
      "Epoch [1/2], Step [4860/64305], Loss: 5.7062\n",
      "Epoch [1/2], Step [4870/64305], Loss: 5.7502\n",
      "Epoch [1/2], Step [4880/64305], Loss: 5.7912\n",
      "Epoch [1/2], Step [4890/64305], Loss: 5.4676\n",
      "Epoch [1/2], Step [4900/64305], Loss: 5.4425\n",
      "Epoch [1/2], Step [4910/64305], Loss: 5.6720\n",
      "Epoch [1/2], Step [4920/64305], Loss: 5.6134\n",
      "Epoch [1/2], Step [4930/64305], Loss: 5.7253\n",
      "Epoch [1/2], Step [4940/64305], Loss: 5.5572\n",
      "Epoch [1/2], Step [4950/64305], Loss: 5.4450\n",
      "Epoch [1/2], Step [4960/64305], Loss: 5.5337\n",
      "Epoch [1/2], Step [4970/64305], Loss: 5.4372\n",
      "Epoch [1/2], Step [4980/64305], Loss: 5.5261\n",
      "Epoch [1/2], Step [4990/64305], Loss: 5.6961\n",
      "Epoch [1/2], Step [5000/64305], Loss: 5.7158\n",
      "Epoch [1/2], Step [5010/64305], Loss: 5.8130\n",
      "Epoch [1/2], Step [5020/64305], Loss: 5.5894\n",
      "Epoch [1/2], Step [5030/64305], Loss: 5.6536\n",
      "Epoch [1/2], Step [5040/64305], Loss: 5.5301\n",
      "Epoch [1/2], Step [5050/64305], Loss: 5.5240\n",
      "Epoch [1/2], Step [5060/64305], Loss: 5.8675\n",
      "Epoch [1/2], Step [5070/64305], Loss: 5.5493\n",
      "Epoch [1/2], Step [5080/64305], Loss: 5.4992\n",
      "Epoch [1/2], Step [5090/64305], Loss: 5.3299\n",
      "Epoch [1/2], Step [5100/64305], Loss: 5.6794\n",
      "Epoch [1/2], Step [5110/64305], Loss: 5.7318\n",
      "Epoch [1/2], Step [5120/64305], Loss: 5.7689\n",
      "Epoch [1/2], Step [5130/64305], Loss: 5.6229\n",
      "Epoch [1/2], Step [5140/64305], Loss: 5.4423\n",
      "Epoch [1/2], Step [5150/64305], Loss: 5.5642\n",
      "Epoch [1/2], Step [5160/64305], Loss: 5.4741\n",
      "Epoch [1/2], Step [5170/64305], Loss: 5.5825\n",
      "Epoch [1/2], Step [5180/64305], Loss: 5.7258\n",
      "Epoch [1/2], Step [5190/64305], Loss: 5.7174\n",
      "Epoch [1/2], Step [5200/64305], Loss: 5.5533\n",
      "Epoch [1/2], Step [5210/64305], Loss: 5.5887\n",
      "Epoch [1/2], Step [5220/64305], Loss: 5.4586\n",
      "Epoch [1/2], Step [5230/64305], Loss: 5.6693\n",
      "Epoch [1/2], Step [5240/64305], Loss: 5.6575\n",
      "Epoch [1/2], Step [5250/64305], Loss: 5.5264\n",
      "Epoch [1/2], Step [5260/64305], Loss: 5.5974\n",
      "Epoch [1/2], Step [5270/64305], Loss: 5.5010\n",
      "Epoch [1/2], Step [5280/64305], Loss: 5.5299\n",
      "Epoch [1/2], Step [5290/64305], Loss: 5.7925\n",
      "Epoch [1/2], Step [5300/64305], Loss: 5.4369\n",
      "Epoch [1/2], Step [5310/64305], Loss: 5.7176\n",
      "Epoch [1/2], Step [5320/64305], Loss: 5.2979\n",
      "Epoch [1/2], Step [5330/64305], Loss: 5.7640\n",
      "Epoch [1/2], Step [5340/64305], Loss: 5.5767\n",
      "Epoch [1/2], Step [5350/64305], Loss: 5.6275\n",
      "Epoch [1/2], Step [5360/64305], Loss: 5.6986\n",
      "Epoch [1/2], Step [5370/64305], Loss: 5.6341\n",
      "Epoch [1/2], Step [5380/64305], Loss: 5.4954\n",
      "Epoch [1/2], Step [5390/64305], Loss: 5.4244\n",
      "Epoch [1/2], Step [5400/64305], Loss: 5.6025\n",
      "Epoch [1/2], Step [5410/64305], Loss: 5.4938\n",
      "Epoch [1/2], Step [5420/64305], Loss: 5.6927\n",
      "Epoch [1/2], Step [5430/64305], Loss: 5.7460\n",
      "Epoch [1/2], Step [5440/64305], Loss: 5.5420\n",
      "Epoch [1/2], Step [5450/64305], Loss: 5.4784\n",
      "Epoch [1/2], Step [5460/64305], Loss: 5.6239\n",
      "Epoch [1/2], Step [5470/64305], Loss: 5.5620\n",
      "Epoch [1/2], Step [5480/64305], Loss: 5.4181\n",
      "Epoch [1/2], Step [5490/64305], Loss: 5.4972\n",
      "Epoch [1/2], Step [5500/64305], Loss: 5.7457\n",
      "Epoch [1/2], Step [5510/64305], Loss: 5.4853\n",
      "Epoch [1/2], Step [5520/64305], Loss: 5.4332\n",
      "Epoch [1/2], Step [5530/64305], Loss: 5.6079\n",
      "Epoch [1/2], Step [5540/64305], Loss: 5.4637\n",
      "Epoch [1/2], Step [5550/64305], Loss: 5.3775\n",
      "Epoch [1/2], Step [5560/64305], Loss: 5.4278\n",
      "Epoch [1/2], Step [5570/64305], Loss: 5.7417\n",
      "Epoch [1/2], Step [5580/64305], Loss: 5.4621\n",
      "Epoch [1/2], Step [5590/64305], Loss: 5.6604\n",
      "Epoch [1/2], Step [5600/64305], Loss: 5.3296\n",
      "Epoch [1/2], Step [5610/64305], Loss: 5.4949\n",
      "Epoch [1/2], Step [5620/64305], Loss: 5.5833\n",
      "Epoch [1/2], Step [5630/64305], Loss: 5.6454\n",
      "Epoch [1/2], Step [5640/64305], Loss: 5.5601\n",
      "Epoch [1/2], Step [5650/64305], Loss: 5.5538\n",
      "Epoch [1/2], Step [5660/64305], Loss: 5.5220\n",
      "Epoch [1/2], Step [5670/64305], Loss: 5.6054\n",
      "Epoch [1/2], Step [5680/64305], Loss: 5.5919\n",
      "Epoch [1/2], Step [5690/64305], Loss: 5.4394\n",
      "Epoch [1/2], Step [5700/64305], Loss: 5.5068\n",
      "Epoch [1/2], Step [5710/64305], Loss: 5.5225\n",
      "Epoch [1/2], Step [5720/64305], Loss: 5.5394\n",
      "Epoch [1/2], Step [5730/64305], Loss: 5.4576\n",
      "Epoch [1/2], Step [5740/64305], Loss: 5.4598\n",
      "Epoch [1/2], Step [5750/64305], Loss: 5.3705\n",
      "Epoch [1/2], Step [5760/64305], Loss: 5.5945\n",
      "Epoch [1/2], Step [5770/64305], Loss: 5.5516\n",
      "Epoch [1/2], Step [5780/64305], Loss: 5.5836\n",
      "Epoch [1/2], Step [5790/64305], Loss: 5.5255\n",
      "Epoch [1/2], Step [5800/64305], Loss: 5.6233\n",
      "Epoch [1/2], Step [5810/64305], Loss: 5.4170\n",
      "Epoch [1/2], Step [5820/64305], Loss: 5.6170\n",
      "Epoch [1/2], Step [5830/64305], Loss: 5.4490\n",
      "Epoch [1/2], Step [5840/64305], Loss: 5.6803\n",
      "Epoch [1/2], Step [5850/64305], Loss: 5.5360\n",
      "Epoch [1/2], Step [5860/64305], Loss: 5.6390\n",
      "Epoch [1/2], Step [5870/64305], Loss: 5.3976\n",
      "Epoch [1/2], Step [5880/64305], Loss: 5.5764\n",
      "Epoch [1/2], Step [5890/64305], Loss: 5.5064\n",
      "Epoch [1/2], Step [5900/64305], Loss: 5.5583\n",
      "Epoch [1/2], Step [5910/64305], Loss: 5.5717\n",
      "Epoch [1/2], Step [5920/64305], Loss: 5.3582\n",
      "Epoch [1/2], Step [5930/64305], Loss: 5.5889\n",
      "Epoch [1/2], Step [5940/64305], Loss: 5.4957\n",
      "Epoch [1/2], Step [5950/64305], Loss: 5.5743\n",
      "Epoch [1/2], Step [5960/64305], Loss: 5.5961\n",
      "Epoch [1/2], Step [5970/64305], Loss: 5.4628\n",
      "Epoch [1/2], Step [5980/64305], Loss: 5.3628\n",
      "Epoch [1/2], Step [5990/64305], Loss: 5.6487\n",
      "Epoch [1/2], Step [6000/64305], Loss: 5.6819\n",
      "Epoch [1/2], Step [6010/64305], Loss: 5.4613\n",
      "Epoch [1/2], Step [6020/64305], Loss: 5.4652\n",
      "Epoch [1/2], Step [6030/64305], Loss: 5.3835\n",
      "Epoch [1/2], Step [6040/64305], Loss: 5.5991\n",
      "Epoch [1/2], Step [6050/64305], Loss: 5.4667\n",
      "Epoch [1/2], Step [6060/64305], Loss: 5.5452\n",
      "Epoch [1/2], Step [6070/64305], Loss: 5.4717\n",
      "Epoch [1/2], Step [6080/64305], Loss: 5.3053\n",
      "Epoch [1/2], Step [6090/64305], Loss: 5.3852\n",
      "Epoch [1/2], Step [6100/64305], Loss: 5.6320\n",
      "Epoch [1/2], Step [6110/64305], Loss: 5.5625\n",
      "Epoch [1/2], Step [6120/64305], Loss: 5.5548\n",
      "Epoch [1/2], Step [6130/64305], Loss: 5.5026\n",
      "Epoch [1/2], Step [6140/64305], Loss: 5.4011\n",
      "Epoch [1/2], Step [6150/64305], Loss: 5.5511\n",
      "Epoch [1/2], Step [6160/64305], Loss: 5.5660\n",
      "Epoch [1/2], Step [6170/64305], Loss: 5.5874\n",
      "Epoch [1/2], Step [6180/64305], Loss: 5.5334\n",
      "Epoch [1/2], Step [6190/64305], Loss: 5.8070\n",
      "Epoch [1/2], Step [6200/64305], Loss: 5.4967\n",
      "Epoch [1/2], Step [6210/64305], Loss: 5.5584\n",
      "Epoch [1/2], Step [6220/64305], Loss: 5.4326\n",
      "Epoch [1/2], Step [6230/64305], Loss: 5.4670\n",
      "Epoch [1/2], Step [6240/64305], Loss: 5.4574\n",
      "Epoch [1/2], Step [6250/64305], Loss: 5.8028\n",
      "Epoch [1/2], Step [6260/64305], Loss: 5.5058\n",
      "Epoch [1/2], Step [6270/64305], Loss: 5.3089\n",
      "Epoch [1/2], Step [6280/64305], Loss: 5.5721\n",
      "Epoch [1/2], Step [6290/64305], Loss: 5.6702\n",
      "Epoch [1/2], Step [6300/64305], Loss: 5.3889\n",
      "Epoch [1/2], Step [6310/64305], Loss: 5.3984\n",
      "Epoch [1/2], Step [6320/64305], Loss: 5.4432\n",
      "Epoch [1/2], Step [6330/64305], Loss: 5.3034\n",
      "Epoch [1/2], Step [6340/64305], Loss: 5.4560\n",
      "Epoch [1/2], Step [6350/64305], Loss: 5.2663\n",
      "Epoch [1/2], Step [6360/64305], Loss: 5.4791\n",
      "Epoch [1/2], Step [6370/64305], Loss: 5.5806\n",
      "Epoch [1/2], Step [6380/64305], Loss: 5.5811\n",
      "Epoch [1/2], Step [6390/64305], Loss: 5.5113\n",
      "Epoch [1/2], Step [6400/64305], Loss: 5.5319\n",
      "Epoch [1/2], Step [6410/64305], Loss: 5.3211\n",
      "Epoch [1/2], Step [6420/64305], Loss: 5.4060\n",
      "Epoch [1/2], Step [6430/64305], Loss: 5.3511\n",
      "Epoch [1/2], Step [6440/64305], Loss: 5.4823\n",
      "Epoch [1/2], Step [6450/64305], Loss: 5.5518\n",
      "Epoch [1/2], Step [6460/64305], Loss: 5.4444\n",
      "Epoch [1/2], Step [6470/64305], Loss: 5.4999\n",
      "Epoch [1/2], Step [6480/64305], Loss: 5.2971\n",
      "Epoch [1/2], Step [6490/64305], Loss: 5.5301\n",
      "Epoch [1/2], Step [6500/64305], Loss: 5.4675\n",
      "Epoch [1/2], Step [6510/64305], Loss: 5.4929\n",
      "Epoch [1/2], Step [6520/64305], Loss: 5.5076\n",
      "Epoch [1/2], Step [6530/64305], Loss: 5.5092\n",
      "Epoch [1/2], Step [6540/64305], Loss: 5.4285\n",
      "Epoch [1/2], Step [6550/64305], Loss: 5.5232\n",
      "Epoch [1/2], Step [6560/64305], Loss: 5.5216\n",
      "Epoch [1/2], Step [6570/64305], Loss: 5.3716\n",
      "Epoch [1/2], Step [6580/64305], Loss: 5.4685\n",
      "Epoch [1/2], Step [6590/64305], Loss: 5.5702\n",
      "Epoch [1/2], Step [6600/64305], Loss: 5.7023\n",
      "Epoch [1/2], Step [6610/64305], Loss: 5.0912\n",
      "Epoch [1/2], Step [6620/64305], Loss: 5.4174\n",
      "Epoch [1/2], Step [6630/64305], Loss: 5.5713\n",
      "Epoch [1/2], Step [6640/64305], Loss: 5.4710\n",
      "Epoch [1/2], Step [6650/64305], Loss: 5.4068\n",
      "Epoch [1/2], Step [6660/64305], Loss: 5.5103\n",
      "Epoch [1/2], Step [6670/64305], Loss: 5.5960\n",
      "Epoch [1/2], Step [6680/64305], Loss: 5.2160\n",
      "Epoch [1/2], Step [6690/64305], Loss: 5.7808\n",
      "Epoch [1/2], Step [6700/64305], Loss: 5.3848\n",
      "Epoch [1/2], Step [6710/64305], Loss: 5.4600\n",
      "Epoch [1/2], Step [6720/64305], Loss: 5.4143\n",
      "Epoch [1/2], Step [6730/64305], Loss: 5.4895\n",
      "Epoch [1/2], Step [6740/64305], Loss: 5.3544\n",
      "Epoch [1/2], Step [6750/64305], Loss: 5.7395\n",
      "Epoch [1/2], Step [6760/64305], Loss: 5.2097\n",
      "Epoch [1/2], Step [6770/64305], Loss: 5.3205\n",
      "Epoch [1/2], Step [6780/64305], Loss: 5.4001\n",
      "Epoch [1/2], Step [6790/64305], Loss: 5.5001\n",
      "Epoch [1/2], Step [6800/64305], Loss: 5.4606\n",
      "Epoch [1/2], Step [6810/64305], Loss: 5.5014\n",
      "Epoch [1/2], Step [6820/64305], Loss: 5.4844\n",
      "Epoch [1/2], Step [6830/64305], Loss: 5.4862\n",
      "Epoch [1/2], Step [6840/64305], Loss: 5.3467\n",
      "Epoch [1/2], Step [6850/64305], Loss: 5.5069\n",
      "Epoch [1/2], Step [6860/64305], Loss: 5.2841\n",
      "Epoch [1/2], Step [6870/64305], Loss: 5.2551\n",
      "Epoch [1/2], Step [6880/64305], Loss: 5.2657\n",
      "Epoch [1/2], Step [6890/64305], Loss: 5.4758\n",
      "Epoch [1/2], Step [6900/64305], Loss: 5.4020\n",
      "Epoch [1/2], Step [6910/64305], Loss: 5.2977\n",
      "Epoch [1/2], Step [6920/64305], Loss: 5.4468\n",
      "Epoch [1/2], Step [6930/64305], Loss: 5.5793\n",
      "Epoch [1/2], Step [6940/64305], Loss: 5.6422\n",
      "Epoch [1/2], Step [6950/64305], Loss: 5.2712\n",
      "Epoch [1/2], Step [6960/64305], Loss: 5.4745\n",
      "Epoch [1/2], Step [6970/64305], Loss: 5.6061\n",
      "Epoch [1/2], Step [6980/64305], Loss: 5.5580\n",
      "Epoch [1/2], Step [6990/64305], Loss: 5.2431\n",
      "Epoch [1/2], Step [7000/64305], Loss: 5.5791\n",
      "Epoch [1/2], Step [7010/64305], Loss: 5.4487\n",
      "Epoch [1/2], Step [7020/64305], Loss: 5.2749\n",
      "Epoch [1/2], Step [7030/64305], Loss: 5.3943\n",
      "Epoch [1/2], Step [7040/64305], Loss: 5.4823\n",
      "Epoch [1/2], Step [7050/64305], Loss: 5.3126\n",
      "Epoch [1/2], Step [7060/64305], Loss: 5.3608\n",
      "Epoch [1/2], Step [7070/64305], Loss: 5.5184\n",
      "Epoch [1/2], Step [7080/64305], Loss: 5.4377\n",
      "Epoch [1/2], Step [7090/64305], Loss: 5.3926\n",
      "Epoch [1/2], Step [7100/64305], Loss: 5.4264\n",
      "Epoch [1/2], Step [7110/64305], Loss: 5.4167\n",
      "Epoch [1/2], Step [7120/64305], Loss: 5.3681\n",
      "Epoch [1/2], Step [7130/64305], Loss: 5.3552\n",
      "Epoch [1/2], Step [7140/64305], Loss: 5.4384\n",
      "Epoch [1/2], Step [7150/64305], Loss: 5.3909\n",
      "Epoch [1/2], Step [7160/64305], Loss: 5.5450\n",
      "Epoch [1/2], Step [7170/64305], Loss: 5.5976\n",
      "Epoch [1/2], Step [7180/64305], Loss: 5.4814\n",
      "Epoch [1/2], Step [7190/64305], Loss: 5.4011\n",
      "Epoch [1/2], Step [7200/64305], Loss: 5.4252\n",
      "Epoch [1/2], Step [7210/64305], Loss: 5.4932\n",
      "Epoch [1/2], Step [7220/64305], Loss: 5.2844\n",
      "Epoch [1/2], Step [7230/64305], Loss: 5.4521\n",
      "Epoch [1/2], Step [7240/64305], Loss: 5.6196\n",
      "Epoch [1/2], Step [7250/64305], Loss: 5.4242\n",
      "Epoch [1/2], Step [7260/64305], Loss: 5.5144\n",
      "Epoch [1/2], Step [7270/64305], Loss: 5.3749\n",
      "Epoch [1/2], Step [7280/64305], Loss: 5.4414\n",
      "Epoch [1/2], Step [7290/64305], Loss: 5.4517\n",
      "Epoch [1/2], Step [7300/64305], Loss: 5.6172\n",
      "Epoch [1/2], Step [7310/64305], Loss: 5.5189\n",
      "Epoch [1/2], Step [7320/64305], Loss: 5.6632\n",
      "Epoch [1/2], Step [7330/64305], Loss: 5.3823\n",
      "Epoch [1/2], Step [7340/64305], Loss: 5.3548\n",
      "Epoch [1/2], Step [7350/64305], Loss: 5.5166\n",
      "Epoch [1/2], Step [7360/64305], Loss: 5.7595\n",
      "Epoch [1/2], Step [7370/64305], Loss: 5.5049\n",
      "Epoch [1/2], Step [7380/64305], Loss: 5.4129\n",
      "Epoch [1/2], Step [7390/64305], Loss: 5.1669\n",
      "Epoch [1/2], Step [7400/64305], Loss: 5.2081\n",
      "Epoch [1/2], Step [7410/64305], Loss: 5.6285\n",
      "Epoch [1/2], Step [7420/64305], Loss: 5.6088\n",
      "Epoch [1/2], Step [7430/64305], Loss: 5.2066\n",
      "Epoch [1/2], Step [7440/64305], Loss: 5.3402\n",
      "Epoch [1/2], Step [7450/64305], Loss: 5.3196\n",
      "Epoch [1/2], Step [7460/64305], Loss: 5.4793\n",
      "Epoch [1/2], Step [7470/64305], Loss: 5.3307\n",
      "Epoch [1/2], Step [7480/64305], Loss: 5.1929\n",
      "Epoch [1/2], Step [7490/64305], Loss: 5.3195\n",
      "Epoch [1/2], Step [7500/64305], Loss: 5.4707\n",
      "Epoch [1/2], Step [7510/64305], Loss: 5.5706\n",
      "Epoch [1/2], Step [7520/64305], Loss: 5.2782\n",
      "Epoch [1/2], Step [7530/64305], Loss: 5.3370\n",
      "Epoch [1/2], Step [7540/64305], Loss: 5.3045\n",
      "Epoch [1/2], Step [7550/64305], Loss: 5.3760\n",
      "Epoch [1/2], Step [7560/64305], Loss: 5.1682\n",
      "Epoch [1/2], Step [7570/64305], Loss: 5.5002\n",
      "Epoch [1/2], Step [7580/64305], Loss: 5.3622\n",
      "Epoch [1/2], Step [7590/64305], Loss: 5.3668\n",
      "Epoch [1/2], Step [7600/64305], Loss: 5.5774\n",
      "Epoch [1/2], Step [7610/64305], Loss: 5.3927\n",
      "Epoch [1/2], Step [7620/64305], Loss: 5.2887\n",
      "Epoch [1/2], Step [7630/64305], Loss: 5.3936\n",
      "Epoch [1/2], Step [7640/64305], Loss: 5.4740\n",
      "Epoch [1/2], Step [7650/64305], Loss: 5.5750\n",
      "Epoch [1/2], Step [7660/64305], Loss: 5.5008\n",
      "Epoch [1/2], Step [7670/64305], Loss: 5.4361\n",
      "Epoch [1/2], Step [7680/64305], Loss: 5.3433\n",
      "Epoch [1/2], Step [7690/64305], Loss: 5.6811\n",
      "Epoch [1/2], Step [7700/64305], Loss: 5.4082\n",
      "Epoch [1/2], Step [7710/64305], Loss: 5.5741\n",
      "Epoch [1/2], Step [7720/64305], Loss: 5.5325\n",
      "Epoch [1/2], Step [7730/64305], Loss: 5.4524\n",
      "Epoch [1/2], Step [7740/64305], Loss: 5.4440\n",
      "Epoch [1/2], Step [7750/64305], Loss: 5.2176\n",
      "Epoch [1/2], Step [7760/64305], Loss: 5.4219\n",
      "Epoch [1/2], Step [7770/64305], Loss: 5.5324\n",
      "Epoch [1/2], Step [7780/64305], Loss: 5.3181\n",
      "Epoch [1/2], Step [7790/64305], Loss: 5.2659\n",
      "Epoch [1/2], Step [7800/64305], Loss: 5.3731\n",
      "Epoch [1/2], Step [7810/64305], Loss: 5.4233\n",
      "Epoch [1/2], Step [7820/64305], Loss: 5.3511\n",
      "Epoch [1/2], Step [7830/64305], Loss: 5.4570\n",
      "Epoch [1/2], Step [7840/64305], Loss: 5.4866\n",
      "Epoch [1/2], Step [7850/64305], Loss: 5.4423\n",
      "Epoch [1/2], Step [7860/64305], Loss: 5.3605\n",
      "Epoch [1/2], Step [7870/64305], Loss: 5.4802\n",
      "Epoch [1/2], Step [7880/64305], Loss: 5.5381\n",
      "Epoch [1/2], Step [7890/64305], Loss: 5.4794\n",
      "Epoch [1/2], Step [7900/64305], Loss: 5.6193\n",
      "Epoch [1/2], Step [7910/64305], Loss: 5.3250\n",
      "Epoch [1/2], Step [7920/64305], Loss: 5.5185\n",
      "Epoch [1/2], Step [7930/64305], Loss: 5.2129\n",
      "Epoch [1/2], Step [7940/64305], Loss: 5.3826\n",
      "Epoch [1/2], Step [7950/64305], Loss: 5.4134\n",
      "Epoch [1/2], Step [7960/64305], Loss: 5.6304\n",
      "Epoch [1/2], Step [7970/64305], Loss: 5.2125\n",
      "Epoch [1/2], Step [7980/64305], Loss: 5.4516\n",
      "Epoch [1/2], Step [7990/64305], Loss: 5.3984\n",
      "Epoch [1/2], Step [8000/64305], Loss: 5.4368\n",
      "Epoch [1/2], Step [8010/64305], Loss: 5.4499\n",
      "Epoch [1/2], Step [8020/64305], Loss: 5.5089\n",
      "Epoch [1/2], Step [8030/64305], Loss: 5.4415\n",
      "Epoch [1/2], Step [8040/64305], Loss: 5.3685\n",
      "Epoch [1/2], Step [8050/64305], Loss: 5.4943\n",
      "Epoch [1/2], Step [8060/64305], Loss: 5.5757\n",
      "Epoch [1/2], Step [8070/64305], Loss: 5.3192\n",
      "Epoch [1/2], Step [8080/64305], Loss: 5.2812\n",
      "Epoch [1/2], Step [8090/64305], Loss: 5.3517\n",
      "Epoch [1/2], Step [8100/64305], Loss: 5.2887\n",
      "Epoch [1/2], Step [8110/64305], Loss: 5.4205\n",
      "Epoch [1/2], Step [8120/64305], Loss: 5.4163\n",
      "Epoch [1/2], Step [8130/64305], Loss: 5.4194\n",
      "Epoch [1/2], Step [8140/64305], Loss: 5.3398\n",
      "Epoch [1/2], Step [8150/64305], Loss: 5.2481\n",
      "Epoch [1/2], Step [8160/64305], Loss: 5.3550\n",
      "Epoch [1/2], Step [8170/64305], Loss: 5.3408\n",
      "Epoch [1/2], Step [8180/64305], Loss: 5.4773\n",
      "Epoch [1/2], Step [8190/64305], Loss: 5.3814\n",
      "Epoch [1/2], Step [8200/64305], Loss: 5.4878\n",
      "Epoch [1/2], Step [8210/64305], Loss: 5.4536\n",
      "Epoch [1/2], Step [8220/64305], Loss: 5.2206\n",
      "Epoch [1/2], Step [8230/64305], Loss: 5.4479\n",
      "Epoch [1/2], Step [8240/64305], Loss: 5.2083\n",
      "Epoch [1/2], Step [8250/64305], Loss: 5.5235\n",
      "Epoch [1/2], Step [8260/64305], Loss: 5.4125\n",
      "Epoch [1/2], Step [8270/64305], Loss: 5.4396\n",
      "Epoch [1/2], Step [8280/64305], Loss: 5.3416\n",
      "Epoch [1/2], Step [8290/64305], Loss: 5.4082\n",
      "Epoch [1/2], Step [8300/64305], Loss: 5.3892\n",
      "Epoch [1/2], Step [8310/64305], Loss: 5.0987\n",
      "Epoch [1/2], Step [8320/64305], Loss: 5.2535\n",
      "Epoch [1/2], Step [8330/64305], Loss: 5.3269\n",
      "Epoch [1/2], Step [8340/64305], Loss: 5.3465\n",
      "Epoch [1/2], Step [8350/64305], Loss: 5.6687\n",
      "Epoch [1/2], Step [8360/64305], Loss: 5.4213\n",
      "Epoch [1/2], Step [8370/64305], Loss: 5.5131\n",
      "Epoch [1/2], Step [8380/64305], Loss: 5.4985\n",
      "Epoch [1/2], Step [8390/64305], Loss: 5.4977\n",
      "Epoch [1/2], Step [8400/64305], Loss: 5.4799\n",
      "Epoch [1/2], Step [8410/64305], Loss: 5.3001\n",
      "Epoch [1/2], Step [8420/64305], Loss: 5.5070\n",
      "Epoch [1/2], Step [8430/64305], Loss: 5.4155\n",
      "Epoch [1/2], Step [8440/64305], Loss: 5.3833\n",
      "Epoch [1/2], Step [8450/64305], Loss: 5.4902\n",
      "Epoch [1/2], Step [8460/64305], Loss: 5.5114\n",
      "Epoch [1/2], Step [8470/64305], Loss: 5.3253\n",
      "Epoch [1/2], Step [8480/64305], Loss: 5.3234\n",
      "Epoch [1/2], Step [8490/64305], Loss: 5.2243\n",
      "Epoch [1/2], Step [8500/64305], Loss: 5.2871\n",
      "Epoch [1/2], Step [8510/64305], Loss: 5.4811\n",
      "Epoch [1/2], Step [8520/64305], Loss: 5.1595\n",
      "Epoch [1/2], Step [8530/64305], Loss: 5.2762\n",
      "Epoch [1/2], Step [8540/64305], Loss: 5.3527\n",
      "Epoch [1/2], Step [8550/64305], Loss: 5.4413\n",
      "Epoch [1/2], Step [8560/64305], Loss: 5.5736\n",
      "Epoch [1/2], Step [8570/64305], Loss: 5.2171\n",
      "Epoch [1/2], Step [8580/64305], Loss: 5.5636\n",
      "Epoch [1/2], Step [8590/64305], Loss: 5.6300\n",
      "Epoch [1/2], Step [8600/64305], Loss: 5.5322\n",
      "Epoch [1/2], Step [8610/64305], Loss: 5.3559\n",
      "Epoch [1/2], Step [8620/64305], Loss: 5.3766\n",
      "Epoch [1/2], Step [8630/64305], Loss: 5.3494\n",
      "Epoch [1/2], Step [8640/64305], Loss: 5.4225\n",
      "Epoch [1/2], Step [8650/64305], Loss: 5.3650\n",
      "Epoch [1/2], Step [8660/64305], Loss: 5.3099\n",
      "Epoch [1/2], Step [8670/64305], Loss: 5.6334\n",
      "Epoch [1/2], Step [8680/64305], Loss: 5.5698\n",
      "Epoch [1/2], Step [8690/64305], Loss: 5.2198\n",
      "Epoch [1/2], Step [8700/64305], Loss: 5.3804\n",
      "Epoch [1/2], Step [8710/64305], Loss: 5.2623\n",
      "Epoch [1/2], Step [8720/64305], Loss: 5.3741\n",
      "Epoch [1/2], Step [8730/64305], Loss: 5.6066\n",
      "Epoch [1/2], Step [8740/64305], Loss: 5.3977\n",
      "Epoch [1/2], Step [8750/64305], Loss: 5.3057\n",
      "Epoch [1/2], Step [8760/64305], Loss: 5.3673\n",
      "Epoch [1/2], Step [8770/64305], Loss: 5.2782\n",
      "Epoch [1/2], Step [8780/64305], Loss: 5.3762\n",
      "Epoch [1/2], Step [8790/64305], Loss: 5.0632\n",
      "Epoch [1/2], Step [8800/64305], Loss: 5.3974\n",
      "Epoch [1/2], Step [8810/64305], Loss: 5.2665\n",
      "Epoch [1/2], Step [8820/64305], Loss: 5.3807\n",
      "Epoch [1/2], Step [8830/64305], Loss: 5.3035\n",
      "Epoch [1/2], Step [8840/64305], Loss: 5.5663\n",
      "Epoch [1/2], Step [8850/64305], Loss: 5.4334\n",
      "Epoch [1/2], Step [8860/64305], Loss: 5.1273\n",
      "Epoch [1/2], Step [8870/64305], Loss: 5.4957\n",
      "Epoch [1/2], Step [8880/64305], Loss: 5.3954\n",
      "Epoch [1/2], Step [8890/64305], Loss: 5.4694\n",
      "Epoch [1/2], Step [8900/64305], Loss: 5.4440\n",
      "Epoch [1/2], Step [8910/64305], Loss: 5.3320\n",
      "Epoch [1/2], Step [8920/64305], Loss: 5.4669\n",
      "Epoch [1/2], Step [8930/64305], Loss: 5.5134\n",
      "Epoch [1/2], Step [8940/64305], Loss: 5.3225\n",
      "Epoch [1/2], Step [8950/64305], Loss: 5.4099\n",
      "Epoch [1/2], Step [8960/64305], Loss: 5.4359\n",
      "Epoch [1/2], Step [8970/64305], Loss: 5.4260\n",
      "Epoch [1/2], Step [8980/64305], Loss: 5.4085\n",
      "Epoch [1/2], Step [8990/64305], Loss: 5.3617\n",
      "Epoch [1/2], Step [9000/64305], Loss: 5.2097\n",
      "Epoch [1/2], Step [9010/64305], Loss: 5.3449\n",
      "Epoch [1/2], Step [9020/64305], Loss: 5.5542\n",
      "Epoch [1/2], Step [9030/64305], Loss: 5.4927\n",
      "Epoch [1/2], Step [9040/64305], Loss: 5.4956\n",
      "Epoch [1/2], Step [9050/64305], Loss: 5.3169\n",
      "Epoch [1/2], Step [9060/64305], Loss: 5.2579\n",
      "Epoch [1/2], Step [9070/64305], Loss: 5.4349\n",
      "Epoch [1/2], Step [9080/64305], Loss: 5.5476\n",
      "Epoch [1/2], Step [9090/64305], Loss: 5.3997\n",
      "Epoch [1/2], Step [9100/64305], Loss: 5.2930\n",
      "Epoch [1/2], Step [9110/64305], Loss: 5.1570\n",
      "Epoch [1/2], Step [9120/64305], Loss: 5.3023\n",
      "Epoch [1/2], Step [9130/64305], Loss: 5.4743\n",
      "Epoch [1/2], Step [9140/64305], Loss: 5.4675\n",
      "Epoch [1/2], Step [9150/64305], Loss: 5.2471\n",
      "Epoch [1/2], Step [9160/64305], Loss: 5.3944\n",
      "Epoch [1/2], Step [9170/64305], Loss: 5.1586\n",
      "Epoch [1/2], Step [9180/64305], Loss: 5.1762\n",
      "Epoch [1/2], Step [9190/64305], Loss: 5.5858\n",
      "Epoch [1/2], Step [9200/64305], Loss: 5.4863\n",
      "Epoch [1/2], Step [9210/64305], Loss: 5.3498\n",
      "Epoch [1/2], Step [9220/64305], Loss: 5.4614\n",
      "Epoch [1/2], Step [9230/64305], Loss: 5.4882\n",
      "Epoch [1/2], Step [9240/64305], Loss: 5.1167\n",
      "Epoch [1/2], Step [9250/64305], Loss: 5.3587\n",
      "Epoch [1/2], Step [9260/64305], Loss: 5.3877\n",
      "Epoch [1/2], Step [9270/64305], Loss: 5.4874\n",
      "Epoch [1/2], Step [9280/64305], Loss: 5.3508\n",
      "Epoch [1/2], Step [9290/64305], Loss: 5.3875\n",
      "Epoch [1/2], Step [9300/64305], Loss: 5.3313\n",
      "Epoch [1/2], Step [9310/64305], Loss: 5.4115\n",
      "Epoch [1/2], Step [9320/64305], Loss: 5.1049\n",
      "Epoch [1/2], Step [9330/64305], Loss: 5.4034\n",
      "Epoch [1/2], Step [9340/64305], Loss: 5.3492\n",
      "Epoch [1/2], Step [9350/64305], Loss: 5.2813\n",
      "Epoch [1/2], Step [9360/64305], Loss: 5.2747\n",
      "Epoch [1/2], Step [9370/64305], Loss: 5.2678\n",
      "Epoch [1/2], Step [9380/64305], Loss: 5.3183\n",
      "Epoch [1/2], Step [9390/64305], Loss: 5.3845\n",
      "Epoch [1/2], Step [9400/64305], Loss: 5.3191\n",
      "Epoch [1/2], Step [9410/64305], Loss: 5.4029\n",
      "Epoch [1/2], Step [9420/64305], Loss: 5.3501\n",
      "Epoch [1/2], Step [9430/64305], Loss: 5.1045\n",
      "Epoch [1/2], Step [9440/64305], Loss: 5.4411\n",
      "Epoch [1/2], Step [9450/64305], Loss: 5.4122\n",
      "Epoch [1/2], Step [9460/64305], Loss: 5.2530\n",
      "Epoch [1/2], Step [9470/64305], Loss: 5.2150\n",
      "Epoch [1/2], Step [9480/64305], Loss: 5.3614\n",
      "Epoch [1/2], Step [9490/64305], Loss: 5.3269\n",
      "Epoch [1/2], Step [9500/64305], Loss: 5.4055\n",
      "Epoch [1/2], Step [9510/64305], Loss: 5.4244\n",
      "Epoch [1/2], Step [9520/64305], Loss: 5.2944\n",
      "Epoch [1/2], Step [9530/64305], Loss: 5.2030\n",
      "Epoch [1/2], Step [9540/64305], Loss: 5.3753\n",
      "Epoch [1/2], Step [9550/64305], Loss: 5.1031\n",
      "Epoch [1/2], Step [9560/64305], Loss: 5.4728\n",
      "Epoch [1/2], Step [9570/64305], Loss: 5.4041\n",
      "Epoch [1/2], Step [9580/64305], Loss: 5.4216\n",
      "Epoch [1/2], Step [9590/64305], Loss: 5.3901\n",
      "Epoch [1/2], Step [9600/64305], Loss: 5.5216\n",
      "Epoch [1/2], Step [9610/64305], Loss: 5.3902\n",
      "Epoch [1/2], Step [9620/64305], Loss: 5.1593\n",
      "Epoch [1/2], Step [9630/64305], Loss: 5.2893\n",
      "Epoch [1/2], Step [9640/64305], Loss: 5.1517\n",
      "Epoch [1/2], Step [9650/64305], Loss: 5.4371\n",
      "Epoch [1/2], Step [9660/64305], Loss: 5.2994\n",
      "Epoch [1/2], Step [9670/64305], Loss: 5.4885\n",
      "Epoch [1/2], Step [9680/64305], Loss: 5.3008\n",
      "Epoch [1/2], Step [9690/64305], Loss: 5.2387\n",
      "Epoch [1/2], Step [9700/64305], Loss: 5.2912\n",
      "Epoch [1/2], Step [9710/64305], Loss: 5.3519\n",
      "Epoch [1/2], Step [9720/64305], Loss: 5.3432\n",
      "Epoch [1/2], Step [9730/64305], Loss: 5.2110\n",
      "Epoch [1/2], Step [9740/64305], Loss: 5.3197\n",
      "Epoch [1/2], Step [9750/64305], Loss: 5.4238\n",
      "Epoch [1/2], Step [9760/64305], Loss: 5.4181\n",
      "Epoch [1/2], Step [9770/64305], Loss: 5.5647\n",
      "Epoch [1/2], Step [9780/64305], Loss: 5.3090\n",
      "Epoch [1/2], Step [9790/64305], Loss: 5.2691\n",
      "Epoch [1/2], Step [9800/64305], Loss: 5.4794\n",
      "Epoch [1/2], Step [9810/64305], Loss: 5.4703\n",
      "Epoch [1/2], Step [9820/64305], Loss: 5.4266\n",
      "Epoch [1/2], Step [9830/64305], Loss: 5.4178\n",
      "Epoch [1/2], Step [9840/64305], Loss: 5.2691\n",
      "Epoch [1/2], Step [9850/64305], Loss: 5.6899\n",
      "Epoch [1/2], Step [9860/64305], Loss: 5.3651\n",
      "Epoch [1/2], Step [9870/64305], Loss: 5.4044\n",
      "Epoch [1/2], Step [9880/64305], Loss: 5.2401\n",
      "Epoch [1/2], Step [9890/64305], Loss: 5.1884\n",
      "Epoch [1/2], Step [9900/64305], Loss: 5.5631\n",
      "Epoch [1/2], Step [9910/64305], Loss: 5.1344\n",
      "Epoch [1/2], Step [9920/64305], Loss: 5.4914\n",
      "Epoch [1/2], Step [9930/64305], Loss: 5.6021\n",
      "Epoch [1/2], Step [9940/64305], Loss: 5.2584\n",
      "Epoch [1/2], Step [9950/64305], Loss: 5.2957\n",
      "Epoch [1/2], Step [9960/64305], Loss: 5.3319\n",
      "Epoch [1/2], Step [9970/64305], Loss: 5.3553\n",
      "Epoch [1/2], Step [9980/64305], Loss: 5.3115\n",
      "Epoch [1/2], Step [9990/64305], Loss: 5.4656\n",
      "Epoch [1/2], Step [10000/64305], Loss: 5.2904\n",
      "Epoch [1/2], Step [10010/64305], Loss: 5.1500\n",
      "Epoch [1/2], Step [10020/64305], Loss: 5.2487\n",
      "Epoch [1/2], Step [10030/64305], Loss: 5.0764\n",
      "Epoch [1/2], Step [10040/64305], Loss: 5.2747\n",
      "Epoch [1/2], Step [10050/64305], Loss: 5.3829\n",
      "Epoch [1/2], Step [10060/64305], Loss: 5.4207\n",
      "Epoch [1/2], Step [10070/64305], Loss: 5.3083\n",
      "Epoch [1/2], Step [10080/64305], Loss: 5.2871\n",
      "Epoch [1/2], Step [10090/64305], Loss: 5.4540\n",
      "Epoch [1/2], Step [10100/64305], Loss: 5.3449\n",
      "Epoch [1/2], Step [10110/64305], Loss: 5.1454\n",
      "Epoch [1/2], Step [10120/64305], Loss: 5.3345\n",
      "Epoch [1/2], Step [10130/64305], Loss: 5.2341\n",
      "Epoch [1/2], Step [10140/64305], Loss: 5.4663\n",
      "Epoch [1/2], Step [10150/64305], Loss: 5.4073\n",
      "Epoch [1/2], Step [10160/64305], Loss: 5.1786\n",
      "Epoch [1/2], Step [10170/64305], Loss: 5.2741\n",
      "Epoch [1/2], Step [10180/64305], Loss: 5.2862\n",
      "Epoch [1/2], Step [10190/64305], Loss: 5.3083\n",
      "Epoch [1/2], Step [10200/64305], Loss: 5.3564\n",
      "Epoch [1/2], Step [10210/64305], Loss: 5.2301\n",
      "Epoch [1/2], Step [10220/64305], Loss: 5.2952\n",
      "Epoch [1/2], Step [10230/64305], Loss: 5.2252\n",
      "Epoch [1/2], Step [10240/64305], Loss: 5.2189\n",
      "Epoch [1/2], Step [10250/64305], Loss: 5.4450\n",
      "Epoch [1/2], Step [10260/64305], Loss: 5.4016\n",
      "Epoch [1/2], Step [10270/64305], Loss: 4.9771\n",
      "Epoch [1/2], Step [10280/64305], Loss: 5.1547\n",
      "Epoch [1/2], Step [10290/64305], Loss: 5.1419\n",
      "Epoch [1/2], Step [10300/64305], Loss: 5.2414\n",
      "Epoch [1/2], Step [10310/64305], Loss: 5.3485\n",
      "Epoch [1/2], Step [10320/64305], Loss: 5.3820\n",
      "Epoch [1/2], Step [10330/64305], Loss: 5.2681\n",
      "Epoch [1/2], Step [10340/64305], Loss: 5.6779\n",
      "Epoch [1/2], Step [10350/64305], Loss: 5.1654\n",
      "Epoch [1/2], Step [10360/64305], Loss: 5.3181\n",
      "Epoch [1/2], Step [10370/64305], Loss: 5.4719\n",
      "Epoch [1/2], Step [10380/64305], Loss: 5.2672\n",
      "Epoch [1/2], Step [10390/64305], Loss: 5.1619\n",
      "Epoch [1/2], Step [10400/64305], Loss: 5.3939\n",
      "Epoch [1/2], Step [10410/64305], Loss: 5.4084\n",
      "Epoch [1/2], Step [10420/64305], Loss: 5.3219\n",
      "Epoch [1/2], Step [10430/64305], Loss: 5.4860\n",
      "Epoch [1/2], Step [10440/64305], Loss: 5.4021\n",
      "Epoch [1/2], Step [10450/64305], Loss: 5.2663\n",
      "Epoch [1/2], Step [10460/64305], Loss: 5.4006\n",
      "Epoch [1/2], Step [10470/64305], Loss: 5.4675\n",
      "Epoch [1/2], Step [10480/64305], Loss: 5.5215\n",
      "Epoch [1/2], Step [10490/64305], Loss: 5.1444\n",
      "Epoch [1/2], Step [10500/64305], Loss: 5.3768\n",
      "Epoch [1/2], Step [10510/64305], Loss: 5.3168\n",
      "Epoch [1/2], Step [10520/64305], Loss: 5.0456\n",
      "Epoch [1/2], Step [10530/64305], Loss: 5.2428\n",
      "Epoch [1/2], Step [10540/64305], Loss: 5.4076\n",
      "Epoch [1/2], Step [10550/64305], Loss: 5.2715\n",
      "Epoch [1/2], Step [10560/64305], Loss: 5.2397\n",
      "Epoch [1/2], Step [10570/64305], Loss: 5.4675\n",
      "Epoch [1/2], Step [10580/64305], Loss: 5.2626\n",
      "Epoch [1/2], Step [10590/64305], Loss: 5.3532\n",
      "Epoch [1/2], Step [10600/64305], Loss: 5.1997\n",
      "Epoch [1/2], Step [10610/64305], Loss: 5.2904\n",
      "Epoch [1/2], Step [10620/64305], Loss: 5.3480\n",
      "Epoch [1/2], Step [10630/64305], Loss: 5.1849\n",
      "Epoch [1/2], Step [10640/64305], Loss: 5.3466\n",
      "Epoch [1/2], Step [10650/64305], Loss: 5.3306\n",
      "Epoch [1/2], Step [10660/64305], Loss: 5.1379\n",
      "Epoch [1/2], Step [10670/64305], Loss: 5.1267\n",
      "Epoch [1/2], Step [10680/64305], Loss: 5.3783\n",
      "Epoch [1/2], Step [10690/64305], Loss: 5.4294\n",
      "Epoch [1/2], Step [10700/64305], Loss: 5.2696\n",
      "Epoch [1/2], Step [10710/64305], Loss: 5.2811\n",
      "Epoch [1/2], Step [10720/64305], Loss: 5.1911\n",
      "Epoch [1/2], Step [10730/64305], Loss: 5.2963\n",
      "Epoch [1/2], Step [10740/64305], Loss: 5.3318\n",
      "Epoch [1/2], Step [10750/64305], Loss: 5.1342\n",
      "Epoch [1/2], Step [10760/64305], Loss: 5.3129\n",
      "Epoch [1/2], Step [10770/64305], Loss: 5.2371\n",
      "Epoch [1/2], Step [10780/64305], Loss: 5.0890\n",
      "Epoch [1/2], Step [10790/64305], Loss: 5.3458\n",
      "Epoch [1/2], Step [10800/64305], Loss: 5.2150\n",
      "Epoch [1/2], Step [10810/64305], Loss: 5.2931\n",
      "Epoch [1/2], Step [10820/64305], Loss: 5.6233\n",
      "Epoch [1/2], Step [10830/64305], Loss: 5.5034\n",
      "Epoch [1/2], Step [10840/64305], Loss: 5.5032\n",
      "Epoch [1/2], Step [10850/64305], Loss: 5.2409\n",
      "Epoch [1/2], Step [10860/64305], Loss: 5.3340\n",
      "Epoch [1/2], Step [10870/64305], Loss: 5.2613\n",
      "Epoch [1/2], Step [10880/64305], Loss: 5.3830\n",
      "Epoch [1/2], Step [10890/64305], Loss: 5.4293\n",
      "Epoch [1/2], Step [10900/64305], Loss: 5.4244\n",
      "Epoch [1/2], Step [10910/64305], Loss: 5.4142\n",
      "Epoch [1/2], Step [10920/64305], Loss: 5.2270\n",
      "Epoch [1/2], Step [10930/64305], Loss: 5.1718\n",
      "Epoch [1/2], Step [10940/64305], Loss: 5.3468\n",
      "Epoch [1/2], Step [10950/64305], Loss: 5.2145\n",
      "Epoch [1/2], Step [10960/64305], Loss: 5.3195\n",
      "Epoch [1/2], Step [10970/64305], Loss: 5.2228\n",
      "Epoch [1/2], Step [10980/64305], Loss: 5.2251\n",
      "Epoch [1/2], Step [10990/64305], Loss: 5.3627\n",
      "Epoch [1/2], Step [11000/64305], Loss: 5.2928\n",
      "Epoch [1/2], Step [11010/64305], Loss: 5.3891\n",
      "Epoch [1/2], Step [11020/64305], Loss: 5.0583\n",
      "Epoch [1/2], Step [11030/64305], Loss: 5.4354\n",
      "Epoch [1/2], Step [11040/64305], Loss: 5.1589\n",
      "Epoch [1/2], Step [11050/64305], Loss: 5.3665\n",
      "Epoch [1/2], Step [11060/64305], Loss: 5.3592\n",
      "Epoch [1/2], Step [11070/64305], Loss: 5.4070\n",
      "Epoch [1/2], Step [11080/64305], Loss: 5.2068\n",
      "Epoch [1/2], Step [11090/64305], Loss: 5.3488\n",
      "Epoch [1/2], Step [11100/64305], Loss: 5.2651\n",
      "Epoch [1/2], Step [11110/64305], Loss: 5.2486\n",
      "Epoch [1/2], Step [11120/64305], Loss: 5.3738\n",
      "Epoch [1/2], Step [11130/64305], Loss: 5.3236\n",
      "Epoch [1/2], Step [11140/64305], Loss: 5.1187\n",
      "Epoch [1/2], Step [11150/64305], Loss: 5.3703\n",
      "Epoch [1/2], Step [11160/64305], Loss: 5.3856\n",
      "Epoch [1/2], Step [11170/64305], Loss: 5.2817\n",
      "Epoch [1/2], Step [11180/64305], Loss: 5.3078\n",
      "Epoch [1/2], Step [11190/64305], Loss: 5.2385\n",
      "Epoch [1/2], Step [11200/64305], Loss: 5.2248\n",
      "Epoch [1/2], Step [11210/64305], Loss: 4.9622\n",
      "Epoch [1/2], Step [11220/64305], Loss: 5.3013\n",
      "Epoch [1/2], Step [11230/64305], Loss: 5.3015\n",
      "Epoch [1/2], Step [11240/64305], Loss: 5.4643\n",
      "Epoch [1/2], Step [11250/64305], Loss: 5.2055\n",
      "Epoch [1/2], Step [11260/64305], Loss: 5.3496\n",
      "Epoch [1/2], Step [11270/64305], Loss: 5.2292\n",
      "Epoch [1/2], Step [11280/64305], Loss: 4.9659\n",
      "Epoch [1/2], Step [11290/64305], Loss: 5.1895\n",
      "Epoch [1/2], Step [11300/64305], Loss: 5.4048\n",
      "Epoch [1/2], Step [11310/64305], Loss: 5.4227\n",
      "Epoch [1/2], Step [11320/64305], Loss: 5.4242\n",
      "Epoch [1/2], Step [11330/64305], Loss: 5.3298\n",
      "Epoch [1/2], Step [11340/64305], Loss: 5.2801\n",
      "Epoch [1/2], Step [11350/64305], Loss: 5.4782\n",
      "Epoch [1/2], Step [11360/64305], Loss: 5.1974\n",
      "Epoch [1/2], Step [11370/64305], Loss: 5.0879\n",
      "Epoch [1/2], Step [11380/64305], Loss: 5.3366\n",
      "Epoch [1/2], Step [11390/64305], Loss: 5.2459\n",
      "Epoch [1/2], Step [11400/64305], Loss: 5.1079\n",
      "Epoch [1/2], Step [11410/64305], Loss: 5.2295\n",
      "Epoch [1/2], Step [11420/64305], Loss: 5.3902\n",
      "Epoch [1/2], Step [11430/64305], Loss: 5.2384\n",
      "Epoch [1/2], Step [11440/64305], Loss: 5.1133\n",
      "Epoch [1/2], Step [11450/64305], Loss: 5.1389\n",
      "Epoch [1/2], Step [11460/64305], Loss: 5.3969\n",
      "Epoch [1/2], Step [11470/64305], Loss: 5.1755\n",
      "Epoch [1/2], Step [11480/64305], Loss: 5.0668\n",
      "Epoch [1/2], Step [11490/64305], Loss: 5.1609\n",
      "Epoch [1/2], Step [11500/64305], Loss: 5.3562\n",
      "Epoch [1/2], Step [11510/64305], Loss: 5.0965\n",
      "Epoch [1/2], Step [11520/64305], Loss: 5.3033\n",
      "Epoch [1/2], Step [11530/64305], Loss: 5.5530\n",
      "Epoch [1/2], Step [11540/64305], Loss: 5.2445\n",
      "Epoch [1/2], Step [11550/64305], Loss: 5.1360\n",
      "Epoch [1/2], Step [11560/64305], Loss: 5.2456\n",
      "Epoch [1/2], Step [11570/64305], Loss: 5.1902\n",
      "Epoch [1/2], Step [11580/64305], Loss: 5.2968\n",
      "Epoch [1/2], Step [11590/64305], Loss: 5.0966\n",
      "Epoch [1/2], Step [11600/64305], Loss: 5.1421\n",
      "Epoch [1/2], Step [11610/64305], Loss: 5.2247\n",
      "Epoch [1/2], Step [11620/64305], Loss: 5.1937\n",
      "Epoch [1/2], Step [11630/64305], Loss: 5.4264\n",
      "Epoch [1/2], Step [11640/64305], Loss: 5.5328\n",
      "Epoch [1/2], Step [11650/64305], Loss: 5.1934\n",
      "Epoch [1/2], Step [11660/64305], Loss: 5.3142\n",
      "Epoch [1/2], Step [11670/64305], Loss: 5.1874\n",
      "Epoch [1/2], Step [11680/64305], Loss: 5.1195\n",
      "Epoch [1/2], Step [11690/64305], Loss: 5.4540\n",
      "Epoch [1/2], Step [11700/64305], Loss: 5.5176\n",
      "Epoch [1/2], Step [11710/64305], Loss: 5.2557\n",
      "Epoch [1/2], Step [11720/64305], Loss: 5.4761\n",
      "Epoch [1/2], Step [11730/64305], Loss: 5.3101\n",
      "Epoch [1/2], Step [11740/64305], Loss: 5.0889\n",
      "Epoch [1/2], Step [11750/64305], Loss: 5.2647\n",
      "Epoch [1/2], Step [11760/64305], Loss: 5.0935\n",
      "Epoch [1/2], Step [11770/64305], Loss: 5.4188\n",
      "Epoch [1/2], Step [11780/64305], Loss: 5.2993\n",
      "Epoch [1/2], Step [11790/64305], Loss: 5.1238\n",
      "Epoch [1/2], Step [11800/64305], Loss: 5.4363\n",
      "Epoch [1/2], Step [11810/64305], Loss: 5.2050\n",
      "Epoch [1/2], Step [11820/64305], Loss: 5.3789\n",
      "Epoch [1/2], Step [11830/64305], Loss: 5.3868\n",
      "Epoch [1/2], Step [11840/64305], Loss: 5.4343\n",
      "Epoch [1/2], Step [11850/64305], Loss: 5.3742\n",
      "Epoch [1/2], Step [11860/64305], Loss: 5.0743\n",
      "Epoch [1/2], Step [11870/64305], Loss: 5.5553\n",
      "Epoch [1/2], Step [11880/64305], Loss: 5.1096\n",
      "Epoch [1/2], Step [11890/64305], Loss: 5.2504\n",
      "Epoch [1/2], Step [11900/64305], Loss: 5.1363\n",
      "Epoch [1/2], Step [11910/64305], Loss: 5.1518\n",
      "Epoch [1/2], Step [11920/64305], Loss: 5.0146\n",
      "Epoch [1/2], Step [11930/64305], Loss: 5.4105\n",
      "Epoch [1/2], Step [11940/64305], Loss: 5.2918\n",
      "Epoch [1/2], Step [11950/64305], Loss: 5.3718\n",
      "Epoch [1/2], Step [11960/64305], Loss: 5.2748\n",
      "Epoch [1/2], Step [11970/64305], Loss: 5.0822\n",
      "Epoch [1/2], Step [11980/64305], Loss: 5.4353\n",
      "Epoch [1/2], Step [11990/64305], Loss: 5.1845\n",
      "Epoch [1/2], Step [12000/64305], Loss: 5.1582\n",
      "Epoch [1/2], Step [12010/64305], Loss: 5.3525\n",
      "Epoch [1/2], Step [12020/64305], Loss: 5.4073\n",
      "Epoch [1/2], Step [12030/64305], Loss: 5.2827\n",
      "Epoch [1/2], Step [12040/64305], Loss: 5.2333\n",
      "Epoch [1/2], Step [12050/64305], Loss: 5.2502\n",
      "Epoch [1/2], Step [12060/64305], Loss: 5.1738\n",
      "Epoch [1/2], Step [12070/64305], Loss: 5.2076\n",
      "Epoch [1/2], Step [12080/64305], Loss: 5.2029\n",
      "Epoch [1/2], Step [12090/64305], Loss: 5.2290\n",
      "Epoch [1/2], Step [12100/64305], Loss: 5.1916\n",
      "Epoch [1/2], Step [12110/64305], Loss: 5.5562\n",
      "Epoch [1/2], Step [12120/64305], Loss: 5.2195\n",
      "Epoch [1/2], Step [12130/64305], Loss: 5.3771\n",
      "Epoch [1/2], Step [12140/64305], Loss: 5.2471\n",
      "Epoch [1/2], Step [12150/64305], Loss: 5.3885\n",
      "Epoch [1/2], Step [12160/64305], Loss: 5.1719\n",
      "Epoch [1/2], Step [12170/64305], Loss: 5.1415\n",
      "Epoch [1/2], Step [12180/64305], Loss: 5.1117\n",
      "Epoch [1/2], Step [12190/64305], Loss: 5.2374\n",
      "Epoch [1/2], Step [12200/64305], Loss: 5.3820\n",
      "Epoch [1/2], Step [12210/64305], Loss: 5.2328\n",
      "Epoch [1/2], Step [12220/64305], Loss: 5.3860\n",
      "Epoch [1/2], Step [12230/64305], Loss: 5.1384\n",
      "Epoch [1/2], Step [12240/64305], Loss: 5.3620\n",
      "Epoch [1/2], Step [12250/64305], Loss: 5.0984\n",
      "Epoch [1/2], Step [12260/64305], Loss: 4.9273\n",
      "Epoch [1/2], Step [12270/64305], Loss: 5.3344\n",
      "Epoch [1/2], Step [12280/64305], Loss: 5.3675\n",
      "Epoch [1/2], Step [12290/64305], Loss: 5.2625\n",
      "Epoch [1/2], Step [12300/64305], Loss: 5.4747\n",
      "Epoch [1/2], Step [12310/64305], Loss: 5.2642\n",
      "Epoch [1/2], Step [12320/64305], Loss: 5.3180\n",
      "Epoch [1/2], Step [12330/64305], Loss: 5.4037\n",
      "Epoch [1/2], Step [12340/64305], Loss: 5.0166\n",
      "Epoch [1/2], Step [12350/64305], Loss: 5.0590\n",
      "Epoch [1/2], Step [12360/64305], Loss: 5.0347\n",
      "Epoch [1/2], Step [12370/64305], Loss: 5.3338\n",
      "Epoch [1/2], Step [12380/64305], Loss: 5.0910\n",
      "Epoch [1/2], Step [12390/64305], Loss: 5.1480\n",
      "Epoch [1/2], Step [12400/64305], Loss: 5.3356\n",
      "Epoch [1/2], Step [12410/64305], Loss: 5.2143\n",
      "Epoch [1/2], Step [12420/64305], Loss: 5.2826\n",
      "Epoch [1/2], Step [12430/64305], Loss: 5.1821\n",
      "Epoch [1/2], Step [12440/64305], Loss: 5.0369\n",
      "Epoch [1/2], Step [12450/64305], Loss: 4.9740\n",
      "Epoch [1/2], Step [12460/64305], Loss: 5.4272\n",
      "Epoch [1/2], Step [12470/64305], Loss: 5.0738\n",
      "Epoch [1/2], Step [12480/64305], Loss: 5.4712\n",
      "Epoch [1/2], Step [12490/64305], Loss: 5.3881\n",
      "Epoch [1/2], Step [12500/64305], Loss: 5.3057\n",
      "Epoch [1/2], Step [12510/64305], Loss: 5.1437\n",
      "Epoch [1/2], Step [12520/64305], Loss: 5.2526\n",
      "Epoch [1/2], Step [12530/64305], Loss: 5.5127\n",
      "Epoch [1/2], Step [12540/64305], Loss: 5.3869\n",
      "Epoch [1/2], Step [12550/64305], Loss: 5.2284\n",
      "Epoch [1/2], Step [12560/64305], Loss: 5.2198\n",
      "Epoch [1/2], Step [12570/64305], Loss: 5.2283\n",
      "Epoch [1/2], Step [12580/64305], Loss: 5.2107\n",
      "Epoch [1/2], Step [12590/64305], Loss: 5.4323\n",
      "Epoch [1/2], Step [12600/64305], Loss: 5.4425\n",
      "Epoch [1/2], Step [12610/64305], Loss: 5.1117\n",
      "Epoch [1/2], Step [12620/64305], Loss: 5.1643\n",
      "Epoch [1/2], Step [12630/64305], Loss: 5.1901\n",
      "Epoch [1/2], Step [12640/64305], Loss: 5.0455\n",
      "Epoch [1/2], Step [12650/64305], Loss: 5.2193\n",
      "Epoch [1/2], Step [12660/64305], Loss: 5.2752\n",
      "Epoch [1/2], Step [12670/64305], Loss: 5.3735\n",
      "Epoch [1/2], Step [12680/64305], Loss: 5.2548\n",
      "Epoch [1/2], Step [12690/64305], Loss: 5.2970\n",
      "Epoch [1/2], Step [12700/64305], Loss: 5.2593\n",
      "Epoch [1/2], Step [12710/64305], Loss: 5.2987\n",
      "Epoch [1/2], Step [12720/64305], Loss: 5.0580\n",
      "Epoch [1/2], Step [12730/64305], Loss: 5.1569\n",
      "Epoch [1/2], Step [12740/64305], Loss: 5.0645\n",
      "Epoch [1/2], Step [12750/64305], Loss: 5.0753\n",
      "Epoch [1/2], Step [12760/64305], Loss: 4.9974\n",
      "Epoch [1/2], Step [12770/64305], Loss: 5.0551\n",
      "Epoch [1/2], Step [12780/64305], Loss: 5.2140\n",
      "Epoch [1/2], Step [12790/64305], Loss: 5.2395\n",
      "Epoch [1/2], Step [12800/64305], Loss: 5.2876\n",
      "Epoch [1/2], Step [12810/64305], Loss: 5.2926\n",
      "Epoch [1/2], Step [12820/64305], Loss: 4.9247\n",
      "Epoch [1/2], Step [12830/64305], Loss: 5.4701\n",
      "Epoch [1/2], Step [12840/64305], Loss: 5.2622\n",
      "Epoch [1/2], Step [12850/64305], Loss: 5.1706\n",
      "Epoch [1/2], Step [12860/64305], Loss: 5.2030\n",
      "Epoch [1/2], Step [12870/64305], Loss: 5.3197\n",
      "Epoch [1/2], Step [12880/64305], Loss: 4.9949\n",
      "Epoch [1/2], Step [12890/64305], Loss: 5.2328\n",
      "Epoch [1/2], Step [12900/64305], Loss: 5.0902\n",
      "Epoch [1/2], Step [12910/64305], Loss: 5.1776\n",
      "Epoch [1/2], Step [12920/64305], Loss: 5.1170\n",
      "Epoch [1/2], Step [12930/64305], Loss: 5.2060\n",
      "Epoch [1/2], Step [12940/64305], Loss: 5.2456\n",
      "Epoch [1/2], Step [12950/64305], Loss: 5.3876\n",
      "Epoch [1/2], Step [12960/64305], Loss: 5.1316\n",
      "Epoch [1/2], Step [12970/64305], Loss: 5.3261\n",
      "Epoch [1/2], Step [12980/64305], Loss: 5.1413\n",
      "Epoch [1/2], Step [12990/64305], Loss: 5.2440\n",
      "Epoch [1/2], Step [13000/64305], Loss: 5.2927\n",
      "Epoch [1/2], Step [13010/64305], Loss: 5.2256\n",
      "Epoch [1/2], Step [13020/64305], Loss: 5.2843\n",
      "Epoch [1/2], Step [13030/64305], Loss: 5.1662\n",
      "Epoch [1/2], Step [13040/64305], Loss: 4.9161\n",
      "Epoch [1/2], Step [13050/64305], Loss: 5.3277\n",
      "Epoch [1/2], Step [13060/64305], Loss: 5.1655\n",
      "Epoch [1/2], Step [13070/64305], Loss: 4.9784\n",
      "Epoch [1/2], Step [13080/64305], Loss: 5.4508\n",
      "Epoch [1/2], Step [13090/64305], Loss: 5.3400\n",
      "Epoch [1/2], Step [13100/64305], Loss: 5.1365\n",
      "Epoch [1/2], Step [13110/64305], Loss: 5.3268\n",
      "Epoch [1/2], Step [13120/64305], Loss: 5.2320\n",
      "Epoch [1/2], Step [13130/64305], Loss: 5.1573\n",
      "Epoch [1/2], Step [13140/64305], Loss: 5.0370\n",
      "Epoch [1/2], Step [13150/64305], Loss: 5.2405\n",
      "Epoch [1/2], Step [13160/64305], Loss: 5.2300\n",
      "Epoch [1/2], Step [13170/64305], Loss: 5.1845\n",
      "Epoch [1/2], Step [13180/64305], Loss: 5.2180\n",
      "Epoch [1/2], Step [13190/64305], Loss: 5.1698\n",
      "Epoch [1/2], Step [13200/64305], Loss: 5.3392\n",
      "Epoch [1/2], Step [13210/64305], Loss: 5.5832\n",
      "Epoch [1/2], Step [13220/64305], Loss: 5.2427\n",
      "Epoch [1/2], Step [13230/64305], Loss: 5.2860\n",
      "Epoch [1/2], Step [13240/64305], Loss: 5.2625\n",
      "Epoch [1/2], Step [13250/64305], Loss: 5.0536\n",
      "Epoch [1/2], Step [13260/64305], Loss: 4.9354\n",
      "Epoch [1/2], Step [13270/64305], Loss: 5.2297\n",
      "Epoch [1/2], Step [13280/64305], Loss: 5.1859\n",
      "Epoch [1/2], Step [13290/64305], Loss: 5.3467\n",
      "Epoch [1/2], Step [13300/64305], Loss: 5.1208\n",
      "Epoch [1/2], Step [13310/64305], Loss: 5.3056\n",
      "Epoch [1/2], Step [13320/64305], Loss: 5.0959\n",
      "Epoch [1/2], Step [13330/64305], Loss: 5.2195\n",
      "Epoch [1/2], Step [13340/64305], Loss: 5.4331\n",
      "Epoch [1/2], Step [13350/64305], Loss: 5.2527\n",
      "Epoch [1/2], Step [13360/64305], Loss: 5.2181\n",
      "Epoch [1/2], Step [13370/64305], Loss: 5.3230\n",
      "Epoch [1/2], Step [13380/64305], Loss: 5.2467\n",
      "Epoch [1/2], Step [13390/64305], Loss: 5.0903\n",
      "Epoch [1/2], Step [13400/64305], Loss: 5.1641\n",
      "Epoch [1/2], Step [13410/64305], Loss: 4.9342\n",
      "Epoch [1/2], Step [13420/64305], Loss: 5.0262\n",
      "Epoch [1/2], Step [13430/64305], Loss: 5.2825\n",
      "Epoch [1/2], Step [13440/64305], Loss: 5.1766\n",
      "Epoch [1/2], Step [13450/64305], Loss: 5.3206\n",
      "Epoch [1/2], Step [13460/64305], Loss: 5.0739\n",
      "Epoch [1/2], Step [13470/64305], Loss: 5.2179\n",
      "Epoch [1/2], Step [13480/64305], Loss: 4.9510\n",
      "Epoch [1/2], Step [13490/64305], Loss: 5.2653\n",
      "Epoch [1/2], Step [13500/64305], Loss: 5.5130\n",
      "Epoch [1/2], Step [13510/64305], Loss: 5.2218\n",
      "Epoch [1/2], Step [13520/64305], Loss: 5.2097\n",
      "Epoch [1/2], Step [13530/64305], Loss: 5.1253\n",
      "Epoch [1/2], Step [13540/64305], Loss: 5.1190\n",
      "Epoch [1/2], Step [13550/64305], Loss: 5.2693\n",
      "Epoch [1/2], Step [13560/64305], Loss: 5.3648\n",
      "Epoch [1/2], Step [13570/64305], Loss: 5.1134\n",
      "Epoch [1/2], Step [13580/64305], Loss: 5.0135\n",
      "Epoch [1/2], Step [13590/64305], Loss: 5.1934\n",
      "Epoch [1/2], Step [13600/64305], Loss: 5.1158\n",
      "Epoch [1/2], Step [13610/64305], Loss: 5.1974\n",
      "Epoch [1/2], Step [13620/64305], Loss: 5.2973\n",
      "Epoch [1/2], Step [13630/64305], Loss: 5.3202\n",
      "Epoch [1/2], Step [13640/64305], Loss: 5.1116\n",
      "Epoch [1/2], Step [13650/64305], Loss: 5.1734\n",
      "Epoch [1/2], Step [13660/64305], Loss: 5.4780\n",
      "Epoch [1/2], Step [13670/64305], Loss: 5.4492\n",
      "Epoch [1/2], Step [13680/64305], Loss: 5.1565\n",
      "Epoch [1/2], Step [13690/64305], Loss: 5.2717\n",
      "Epoch [1/2], Step [13700/64305], Loss: 5.0728\n",
      "Epoch [1/2], Step [13710/64305], Loss: 5.4687\n",
      "Epoch [1/2], Step [13720/64305], Loss: 5.3668\n",
      "Epoch [1/2], Step [13730/64305], Loss: 5.1777\n",
      "Epoch [1/2], Step [13740/64305], Loss: 5.2925\n",
      "Epoch [1/2], Step [13750/64305], Loss: 5.2839\n",
      "Epoch [1/2], Step [13760/64305], Loss: 5.1040\n",
      "Epoch [1/2], Step [13770/64305], Loss: 5.2440\n",
      "Epoch [1/2], Step [13780/64305], Loss: 5.1953\n",
      "Epoch [1/2], Step [13790/64305], Loss: 5.1757\n",
      "Epoch [1/2], Step [13800/64305], Loss: 5.2177\n",
      "Epoch [1/2], Step [13810/64305], Loss: 5.1750\n",
      "Epoch [1/2], Step [13820/64305], Loss: 5.2912\n",
      "Epoch [1/2], Step [13830/64305], Loss: 5.4164\n",
      "Epoch [1/2], Step [13840/64305], Loss: 5.4620\n",
      "Epoch [1/2], Step [13850/64305], Loss: 5.1570\n",
      "Epoch [1/2], Step [13860/64305], Loss: 5.2418\n",
      "Epoch [1/2], Step [13870/64305], Loss: 5.3597\n",
      "Epoch [1/2], Step [13880/64305], Loss: 5.0139\n",
      "Epoch [1/2], Step [13890/64305], Loss: 5.3478\n",
      "Epoch [1/2], Step [13900/64305], Loss: 5.2725\n",
      "Epoch [1/2], Step [13910/64305], Loss: 5.1427\n",
      "Epoch [1/2], Step [13920/64305], Loss: 5.2523\n",
      "Epoch [1/2], Step [13930/64305], Loss: 4.9667\n",
      "Epoch [1/2], Step [13940/64305], Loss: 5.3319\n",
      "Epoch [1/2], Step [13950/64305], Loss: 5.2567\n",
      "Epoch [1/2], Step [13960/64305], Loss: 5.2188\n",
      "Epoch [1/2], Step [13970/64305], Loss: 5.1929\n",
      "Epoch [1/2], Step [13980/64305], Loss: 5.2773\n",
      "Epoch [1/2], Step [13990/64305], Loss: 5.1344\n",
      "Epoch [1/2], Step [14000/64305], Loss: 5.2889\n",
      "Epoch [1/2], Step [14010/64305], Loss: 4.9392\n",
      "Epoch [1/2], Step [14020/64305], Loss: 5.3632\n",
      "Epoch [1/2], Step [14030/64305], Loss: 5.1240\n",
      "Epoch [1/2], Step [14040/64305], Loss: 4.9409\n",
      "Epoch [1/2], Step [14050/64305], Loss: 5.1833\n",
      "Epoch [1/2], Step [14060/64305], Loss: 5.3461\n",
      "Epoch [1/2], Step [14070/64305], Loss: 5.3857\n",
      "Epoch [1/2], Step [14080/64305], Loss: 5.2219\n",
      "Epoch [1/2], Step [14090/64305], Loss: 5.0566\n",
      "Epoch [1/2], Step [14100/64305], Loss: 5.1980\n",
      "Epoch [1/2], Step [14110/64305], Loss: 5.1301\n",
      "Epoch [1/2], Step [14120/64305], Loss: 5.1057\n",
      "Epoch [1/2], Step [14130/64305], Loss: 5.1612\n",
      "Epoch [1/2], Step [14140/64305], Loss: 5.3090\n",
      "Epoch [1/2], Step [14150/64305], Loss: 5.2560\n",
      "Epoch [1/2], Step [14160/64305], Loss: 5.3186\n",
      "Epoch [1/2], Step [14170/64305], Loss: 5.1775\n",
      "Epoch [1/2], Step [14180/64305], Loss: 5.3078\n",
      "Epoch [1/2], Step [14190/64305], Loss: 5.2566\n",
      "Epoch [1/2], Step [14200/64305], Loss: 5.2549\n",
      "Epoch [1/2], Step [14210/64305], Loss: 5.0535\n",
      "Epoch [1/2], Step [14220/64305], Loss: 5.2881\n",
      "Epoch [1/2], Step [14230/64305], Loss: 5.1747\n",
      "Epoch [1/2], Step [14240/64305], Loss: 5.1848\n",
      "Epoch [1/2], Step [14250/64305], Loss: 5.3576\n",
      "Epoch [1/2], Step [14260/64305], Loss: 5.2113\n",
      "Epoch [1/2], Step [14270/64305], Loss: 5.1972\n",
      "Epoch [1/2], Step [14280/64305], Loss: 5.2294\n",
      "Epoch [1/2], Step [14290/64305], Loss: 5.1798\n",
      "Epoch [1/2], Step [14300/64305], Loss: 5.4448\n",
      "Epoch [1/2], Step [14310/64305], Loss: 5.1500\n",
      "Epoch [1/2], Step [14320/64305], Loss: 5.4856\n",
      "Epoch [1/2], Step [14330/64305], Loss: 5.4770\n",
      "Epoch [1/2], Step [14340/64305], Loss: 5.1130\n",
      "Epoch [1/2], Step [14350/64305], Loss: 4.9038\n",
      "Epoch [1/2], Step [14360/64305], Loss: 5.1251\n",
      "Epoch [1/2], Step [14370/64305], Loss: 5.0428\n",
      "Epoch [1/2], Step [14380/64305], Loss: 5.3109\n",
      "Epoch [1/2], Step [14390/64305], Loss: 5.0312\n",
      "Epoch [1/2], Step [14400/64305], Loss: 5.1401\n",
      "Epoch [1/2], Step [14410/64305], Loss: 5.3197\n",
      "Epoch [1/2], Step [14420/64305], Loss: 5.2691\n",
      "Epoch [1/2], Step [14430/64305], Loss: 5.1319\n",
      "Epoch [1/2], Step [14440/64305], Loss: 5.1857\n",
      "Epoch [1/2], Step [14450/64305], Loss: 5.1449\n",
      "Epoch [1/2], Step [14460/64305], Loss: 5.1020\n",
      "Epoch [1/2], Step [14470/64305], Loss: 5.3264\n",
      "Epoch [1/2], Step [14480/64305], Loss: 5.2512\n",
      "Epoch [1/2], Step [14490/64305], Loss: 5.1875\n",
      "Epoch [1/2], Step [14500/64305], Loss: 5.1250\n",
      "Epoch [1/2], Step [14510/64305], Loss: 5.3815\n",
      "Epoch [1/2], Step [14520/64305], Loss: 5.2504\n",
      "Epoch [1/2], Step [14530/64305], Loss: 5.1441\n",
      "Epoch [1/2], Step [14540/64305], Loss: 5.0703\n",
      "Epoch [1/2], Step [14550/64305], Loss: 5.2365\n",
      "Epoch [1/2], Step [14560/64305], Loss: 5.3410\n",
      "Epoch [1/2], Step [14570/64305], Loss: 4.9414\n",
      "Epoch [1/2], Step [14580/64305], Loss: 5.2796\n",
      "Epoch [1/2], Step [14590/64305], Loss: 5.2646\n",
      "Epoch [1/2], Step [14600/64305], Loss: 5.2708\n",
      "Epoch [1/2], Step [14610/64305], Loss: 5.1545\n",
      "Epoch [1/2], Step [14620/64305], Loss: 5.2418\n",
      "Epoch [1/2], Step [14630/64305], Loss: 5.2156\n",
      "Epoch [1/2], Step [14640/64305], Loss: 5.2151\n",
      "Epoch [1/2], Step [14650/64305], Loss: 5.1932\n",
      "Epoch [1/2], Step [14660/64305], Loss: 5.1119\n",
      "Epoch [1/2], Step [14670/64305], Loss: 5.1661\n",
      "Epoch [1/2], Step [14680/64305], Loss: 5.1728\n",
      "Epoch [1/2], Step [14690/64305], Loss: 5.1532\n",
      "Epoch [1/2], Step [14700/64305], Loss: 5.2082\n",
      "Epoch [1/2], Step [14710/64305], Loss: 5.1136\n",
      "Epoch [1/2], Step [14720/64305], Loss: 5.3288\n",
      "Epoch [1/2], Step [14730/64305], Loss: 5.1326\n",
      "Epoch [1/2], Step [14740/64305], Loss: 5.6094\n",
      "Epoch [1/2], Step [14750/64305], Loss: 5.1808\n",
      "Epoch [1/2], Step [14760/64305], Loss: 5.1995\n",
      "Epoch [1/2], Step [14770/64305], Loss: 5.3425\n",
      "Epoch [1/2], Step [14780/64305], Loss: 5.2767\n",
      "Epoch [1/2], Step [14790/64305], Loss: 5.0892\n",
      "Epoch [1/2], Step [14800/64305], Loss: 5.0643\n",
      "Epoch [1/2], Step [14810/64305], Loss: 5.2158\n",
      "Epoch [1/2], Step [14820/64305], Loss: 5.2027\n",
      "Epoch [1/2], Step [14830/64305], Loss: 5.0702\n",
      "Epoch [1/2], Step [14840/64305], Loss: 5.2637\n",
      "Epoch [1/2], Step [14850/64305], Loss: 5.1566\n",
      "Epoch [1/2], Step [14860/64305], Loss: 5.2010\n",
      "Epoch [1/2], Step [14870/64305], Loss: 5.2813\n",
      "Epoch [1/2], Step [14880/64305], Loss: 5.0901\n",
      "Epoch [1/2], Step [14890/64305], Loss: 5.2811\n",
      "Epoch [1/2], Step [14900/64305], Loss: 5.2074\n",
      "Epoch [1/2], Step [14910/64305], Loss: 5.2147\n",
      "Epoch [1/2], Step [14920/64305], Loss: 5.0842\n",
      "Epoch [1/2], Step [14930/64305], Loss: 5.2124\n",
      "Epoch [1/2], Step [14940/64305], Loss: 5.1443\n",
      "Epoch [1/2], Step [14950/64305], Loss: 5.2515\n",
      "Epoch [1/2], Step [14960/64305], Loss: 5.2030\n",
      "Epoch [1/2], Step [14970/64305], Loss: 5.2245\n",
      "Epoch [1/2], Step [14980/64305], Loss: 5.3431\n",
      "Epoch [1/2], Step [14990/64305], Loss: 5.2241\n",
      "Epoch [1/2], Step [15000/64305], Loss: 5.1954\n",
      "Epoch [1/2], Step [15010/64305], Loss: 5.1359\n",
      "Epoch [1/2], Step [15020/64305], Loss: 5.3278\n",
      "Epoch [1/2], Step [15030/64305], Loss: 5.1195\n",
      "Epoch [1/2], Step [15040/64305], Loss: 5.1386\n",
      "Epoch [1/2], Step [15050/64305], Loss: 5.2160\n",
      "Epoch [1/2], Step [15060/64305], Loss: 5.0729\n",
      "Epoch [1/2], Step [15070/64305], Loss: 5.2861\n",
      "Epoch [1/2], Step [15080/64305], Loss: 5.0480\n",
      "Epoch [1/2], Step [15090/64305], Loss: 5.3041\n",
      "Epoch [1/2], Step [15100/64305], Loss: 5.3143\n",
      "Epoch [1/2], Step [15110/64305], Loss: 5.1581\n",
      "Epoch [1/2], Step [15120/64305], Loss: 5.3396\n",
      "Epoch [1/2], Step [15130/64305], Loss: 5.1999\n",
      "Epoch [1/2], Step [15140/64305], Loss: 5.2353\n",
      "Epoch [1/2], Step [15150/64305], Loss: 5.1312\n",
      "Epoch [1/2], Step [15160/64305], Loss: 5.2909\n",
      "Epoch [1/2], Step [15170/64305], Loss: 5.0369\n",
      "Epoch [1/2], Step [15180/64305], Loss: 5.1869\n",
      "Epoch [1/2], Step [15190/64305], Loss: 5.1283\n",
      "Epoch [1/2], Step [15200/64305], Loss: 5.3166\n",
      "Epoch [1/2], Step [15210/64305], Loss: 5.1164\n",
      "Epoch [1/2], Step [15220/64305], Loss: 5.1499\n",
      "Epoch [1/2], Step [15230/64305], Loss: 5.3288\n",
      "Epoch [1/2], Step [15240/64305], Loss: 5.1637\n",
      "Epoch [1/2], Step [15250/64305], Loss: 5.0919\n",
      "Epoch [1/2], Step [15260/64305], Loss: 5.3045\n",
      "Epoch [1/2], Step [15270/64305], Loss: 5.3435\n",
      "Epoch [1/2], Step [15280/64305], Loss: 5.1014\n",
      "Epoch [1/2], Step [15290/64305], Loss: 5.1298\n",
      "Epoch [1/2], Step [15300/64305], Loss: 5.0176\n",
      "Epoch [1/2], Step [15310/64305], Loss: 5.2673\n",
      "Epoch [1/2], Step [15320/64305], Loss: 5.2482\n",
      "Epoch [1/2], Step [15330/64305], Loss: 5.2104\n",
      "Epoch [1/2], Step [15340/64305], Loss: 5.0501\n",
      "Epoch [1/2], Step [15350/64305], Loss: 5.0977\n",
      "Epoch [1/2], Step [15360/64305], Loss: 5.1556\n",
      "Epoch [1/2], Step [15370/64305], Loss: 5.3621\n",
      "Epoch [1/2], Step [15380/64305], Loss: 4.9655\n",
      "Epoch [1/2], Step [15390/64305], Loss: 5.2935\n",
      "Epoch [1/2], Step [15400/64305], Loss: 5.4194\n",
      "Epoch [1/2], Step [15410/64305], Loss: 5.2779\n",
      "Epoch [1/2], Step [15420/64305], Loss: 5.3552\n",
      "Epoch [1/2], Step [15430/64305], Loss: 5.0886\n",
      "Epoch [1/2], Step [15440/64305], Loss: 5.2491\n",
      "Epoch [1/2], Step [15450/64305], Loss: 5.3040\n",
      "Epoch [1/2], Step [15460/64305], Loss: 5.0471\n",
      "Epoch [1/2], Step [15470/64305], Loss: 5.0838\n",
      "Epoch [1/2], Step [15480/64305], Loss: 5.3134\n",
      "Epoch [1/2], Step [15490/64305], Loss: 5.1195\n",
      "Epoch [1/2], Step [15500/64305], Loss: 5.3678\n",
      "Epoch [1/2], Step [15510/64305], Loss: 5.1538\n",
      "Epoch [1/2], Step [15520/64305], Loss: 5.1903\n",
      "Epoch [1/2], Step [15530/64305], Loss: 5.2000\n",
      "Epoch [1/2], Step [15540/64305], Loss: 5.2481\n",
      "Epoch [1/2], Step [15550/64305], Loss: 5.0985\n",
      "Epoch [1/2], Step [15560/64305], Loss: 5.3201\n",
      "Epoch [1/2], Step [15570/64305], Loss: 5.0947\n",
      "Epoch [1/2], Step [15580/64305], Loss: 5.1965\n",
      "Epoch [1/2], Step [15590/64305], Loss: 5.3682\n",
      "Epoch [1/2], Step [15600/64305], Loss: 5.0325\n",
      "Epoch [1/2], Step [15610/64305], Loss: 5.3636\n",
      "Epoch [1/2], Step [15620/64305], Loss: 5.2207\n",
      "Epoch [1/2], Step [15630/64305], Loss: 5.1096\n",
      "Epoch [1/2], Step [15640/64305], Loss: 5.4719\n",
      "Epoch [1/2], Step [15650/64305], Loss: 5.0320\n",
      "Epoch [1/2], Step [15660/64305], Loss: 5.0653\n",
      "Epoch [1/2], Step [15670/64305], Loss: 5.3043\n",
      "Epoch [1/2], Step [15680/64305], Loss: 5.0959\n",
      "Epoch [1/2], Step [15690/64305], Loss: 5.0963\n",
      "Epoch [1/2], Step [15700/64305], Loss: 5.0906\n",
      "Epoch [1/2], Step [15710/64305], Loss: 5.1168\n",
      "Epoch [1/2], Step [15720/64305], Loss: 5.0632\n",
      "Epoch [1/2], Step [15730/64305], Loss: 5.0180\n",
      "Epoch [1/2], Step [15740/64305], Loss: 5.3060\n",
      "Epoch [1/2], Step [15750/64305], Loss: 5.5721\n",
      "Epoch [1/2], Step [15760/64305], Loss: 5.2033\n",
      "Epoch [1/2], Step [15770/64305], Loss: 5.3064\n",
      "Epoch [1/2], Step [15780/64305], Loss: 5.0520\n",
      "Epoch [1/2], Step [15790/64305], Loss: 5.3059\n",
      "Epoch [1/2], Step [15800/64305], Loss: 5.2533\n",
      "Epoch [1/2], Step [15810/64305], Loss: 5.1182\n",
      "Epoch [1/2], Step [15820/64305], Loss: 5.2230\n",
      "Epoch [1/2], Step [15830/64305], Loss: 5.3089\n",
      "Epoch [1/2], Step [15840/64305], Loss: 5.2116\n",
      "Epoch [1/2], Step [15850/64305], Loss: 5.5178\n",
      "Epoch [1/2], Step [15860/64305], Loss: 5.1797\n",
      "Epoch [1/2], Step [15870/64305], Loss: 4.8964\n",
      "Epoch [1/2], Step [15880/64305], Loss: 5.1391\n",
      "Epoch [1/2], Step [15890/64305], Loss: 4.9652\n",
      "Epoch [1/2], Step [15900/64305], Loss: 5.2138\n",
      "Epoch [1/2], Step [15910/64305], Loss: 5.2454\n",
      "Epoch [1/2], Step [15920/64305], Loss: 5.1476\n",
      "Epoch [1/2], Step [15930/64305], Loss: 4.9939\n",
      "Epoch [1/2], Step [15940/64305], Loss: 5.2094\n",
      "Epoch [1/2], Step [15950/64305], Loss: 5.1911\n",
      "Epoch [1/2], Step [15960/64305], Loss: 5.2220\n",
      "Epoch [1/2], Step [15970/64305], Loss: 5.1727\n",
      "Epoch [1/2], Step [15980/64305], Loss: 5.1474\n",
      "Epoch [1/2], Step [15990/64305], Loss: 5.0519\n",
      "Epoch [1/2], Step [16000/64305], Loss: 5.1725\n",
      "Epoch [1/2], Step [16010/64305], Loss: 4.9739\n",
      "Epoch [1/2], Step [16020/64305], Loss: 5.2313\n",
      "Epoch [1/2], Step [16030/64305], Loss: 5.0906\n",
      "Epoch [1/2], Step [16040/64305], Loss: 5.1850\n",
      "Epoch [1/2], Step [16050/64305], Loss: 5.1605\n",
      "Epoch [1/2], Step [16060/64305], Loss: 4.9016\n",
      "Epoch [1/2], Step [16070/64305], Loss: 5.1735\n",
      "Epoch [1/2], Step [16080/64305], Loss: 5.1416\n",
      "Epoch [1/2], Step [16090/64305], Loss: 5.0820\n",
      "Epoch [1/2], Step [16100/64305], Loss: 5.1028\n",
      "Epoch [1/2], Step [16110/64305], Loss: 5.0475\n",
      "Epoch [1/2], Step [16120/64305], Loss: 5.1817\n",
      "Epoch [1/2], Step [16130/64305], Loss: 5.0398\n",
      "Epoch [1/2], Step [16140/64305], Loss: 5.0862\n",
      "Epoch [1/2], Step [16150/64305], Loss: 4.9933\n",
      "Epoch [1/2], Step [16160/64305], Loss: 5.1327\n",
      "Epoch [1/2], Step [16170/64305], Loss: 5.2405\n",
      "Epoch [1/2], Step [16180/64305], Loss: 5.1445\n",
      "Epoch [1/2], Step [16190/64305], Loss: 5.1461\n",
      "Epoch [1/2], Step [16200/64305], Loss: 5.1516\n",
      "Epoch [1/2], Step [16210/64305], Loss: 4.9993\n",
      "Epoch [1/2], Step [16220/64305], Loss: 5.0935\n",
      "Epoch [1/2], Step [16230/64305], Loss: 5.0415\n",
      "Epoch [1/2], Step [16240/64305], Loss: 5.1544\n",
      "Epoch [1/2], Step [16250/64305], Loss: 5.2129\n",
      "Epoch [1/2], Step [16260/64305], Loss: 5.2490\n",
      "Epoch [1/2], Step [16270/64305], Loss: 5.0912\n",
      "Epoch [1/2], Step [16280/64305], Loss: 5.2773\n",
      "Epoch [1/2], Step [16290/64305], Loss: 5.4438\n",
      "Epoch [1/2], Step [16300/64305], Loss: 5.4665\n",
      "Epoch [1/2], Step [16310/64305], Loss: 5.1155\n",
      "Epoch [1/2], Step [16320/64305], Loss: 5.1786\n",
      "Epoch [1/2], Step [16330/64305], Loss: 5.0900\n",
      "Epoch [1/2], Step [16340/64305], Loss: 5.0125\n",
      "Epoch [1/2], Step [16350/64305], Loss: 5.2244\n",
      "Epoch [1/2], Step [16360/64305], Loss: 5.2665\n",
      "Epoch [1/2], Step [16370/64305], Loss: 5.1563\n",
      "Epoch [1/2], Step [16380/64305], Loss: 5.1146\n",
      "Epoch [1/2], Step [16390/64305], Loss: 5.0548\n",
      "Epoch [1/2], Step [16400/64305], Loss: 5.0569\n",
      "Epoch [1/2], Step [16410/64305], Loss: 5.1465\n",
      "Epoch [1/2], Step [16420/64305], Loss: 5.2081\n",
      "Epoch [1/2], Step [16430/64305], Loss: 5.1253\n",
      "Epoch [1/2], Step [16440/64305], Loss: 5.1152\n",
      "Epoch [1/2], Step [16450/64305], Loss: 5.2056\n",
      "Epoch [1/2], Step [16460/64305], Loss: 5.1797\n",
      "Epoch [1/2], Step [16470/64305], Loss: 5.1322\n",
      "Epoch [1/2], Step [16480/64305], Loss: 5.0089\n",
      "Epoch [1/2], Step [16490/64305], Loss: 4.9496\n",
      "Epoch [1/2], Step [16500/64305], Loss: 5.4024\n",
      "Epoch [1/2], Step [16510/64305], Loss: 5.1415\n",
      "Epoch [1/2], Step [16520/64305], Loss: 5.2101\n",
      "Epoch [1/2], Step [16530/64305], Loss: 5.3120\n",
      "Epoch [1/2], Step [16540/64305], Loss: 5.1701\n",
      "Epoch [1/2], Step [16550/64305], Loss: 5.2773\n",
      "Epoch [1/2], Step [16560/64305], Loss: 5.1180\n",
      "Epoch [1/2], Step [16570/64305], Loss: 5.4513\n",
      "Epoch [1/2], Step [16580/64305], Loss: 5.2468\n",
      "Epoch [1/2], Step [16590/64305], Loss: 5.2950\n",
      "Epoch [1/2], Step [16600/64305], Loss: 5.1306\n",
      "Epoch [1/2], Step [16610/64305], Loss: 4.9691\n",
      "Epoch [1/2], Step [16620/64305], Loss: 5.1239\n",
      "Epoch [1/2], Step [16630/64305], Loss: 5.1311\n",
      "Epoch [1/2], Step [16640/64305], Loss: 5.2150\n",
      "Epoch [1/2], Step [16650/64305], Loss: 5.1940\n",
      "Epoch [1/2], Step [16660/64305], Loss: 5.2270\n",
      "Epoch [1/2], Step [16670/64305], Loss: 4.9491\n",
      "Epoch [1/2], Step [16680/64305], Loss: 5.1020\n",
      "Epoch [1/2], Step [16690/64305], Loss: 5.1081\n",
      "Epoch [1/2], Step [16700/64305], Loss: 5.2653\n",
      "Epoch [1/2], Step [16710/64305], Loss: 5.1666\n",
      "Epoch [1/2], Step [16720/64305], Loss: 5.0639\n",
      "Epoch [1/2], Step [16730/64305], Loss: 5.1921\n",
      "Epoch [1/2], Step [16740/64305], Loss: 5.1786\n",
      "Epoch [1/2], Step [16750/64305], Loss: 5.1780\n",
      "Epoch [1/2], Step [16760/64305], Loss: 4.9088\n",
      "Epoch [1/2], Step [16770/64305], Loss: 5.1238\n",
      "Epoch [1/2], Step [16780/64305], Loss: 5.1476\n",
      "Epoch [1/2], Step [16790/64305], Loss: 5.0252\n",
      "Epoch [1/2], Step [16800/64305], Loss: 5.3211\n",
      "Epoch [1/2], Step [16810/64305], Loss: 5.2233\n",
      "Epoch [1/2], Step [16820/64305], Loss: 4.9808\n",
      "Epoch [1/2], Step [16830/64305], Loss: 5.2545\n",
      "Epoch [1/2], Step [16840/64305], Loss: 4.9020\n",
      "Epoch [1/2], Step [16850/64305], Loss: 5.1405\n",
      "Epoch [1/2], Step [16860/64305], Loss: 5.0797\n",
      "Epoch [1/2], Step [16870/64305], Loss: 5.0849\n",
      "Epoch [1/2], Step [16880/64305], Loss: 5.2773\n",
      "Epoch [1/2], Step [16890/64305], Loss: 5.3040\n",
      "Epoch [1/2], Step [16900/64305], Loss: 5.0661\n",
      "Epoch [1/2], Step [16910/64305], Loss: 5.0256\n",
      "Epoch [1/2], Step [16920/64305], Loss: 5.1768\n",
      "Epoch [1/2], Step [16930/64305], Loss: 5.4623\n",
      "Epoch [1/2], Step [16940/64305], Loss: 5.0828\n",
      "Epoch [1/2], Step [16950/64305], Loss: 5.0174\n",
      "Epoch [1/2], Step [16960/64305], Loss: 4.9846\n",
      "Epoch [1/2], Step [16970/64305], Loss: 5.1302\n",
      "Epoch [1/2], Step [16980/64305], Loss: 5.1707\n",
      "Epoch [1/2], Step [16990/64305], Loss: 5.0515\n",
      "Epoch [1/2], Step [17000/64305], Loss: 5.2748\n",
      "Epoch [1/2], Step [17010/64305], Loss: 5.1795\n",
      "Epoch [1/2], Step [17020/64305], Loss: 5.0979\n",
      "Epoch [1/2], Step [17030/64305], Loss: 5.4281\n",
      "Epoch [1/2], Step [17040/64305], Loss: 5.0335\n",
      "Epoch [1/2], Step [17050/64305], Loss: 5.1921\n",
      "Epoch [1/2], Step [17060/64305], Loss: 5.1418\n",
      "Epoch [1/2], Step [17070/64305], Loss: 5.0383\n",
      "Epoch [1/2], Step [17080/64305], Loss: 5.1377\n",
      "Epoch [1/2], Step [17090/64305], Loss: 4.9707\n",
      "Epoch [1/2], Step [17100/64305], Loss: 5.2081\n",
      "Epoch [1/2], Step [17110/64305], Loss: 5.3136\n",
      "Epoch [1/2], Step [17120/64305], Loss: 5.2862\n",
      "Epoch [1/2], Step [17130/64305], Loss: 4.9457\n",
      "Epoch [1/2], Step [17140/64305], Loss: 5.0201\n",
      "Epoch [1/2], Step [17150/64305], Loss: 5.2660\n",
      "Epoch [1/2], Step [17160/64305], Loss: 4.7710\n",
      "Epoch [1/2], Step [17170/64305], Loss: 5.1499\n",
      "Epoch [1/2], Step [17180/64305], Loss: 5.2361\n",
      "Epoch [1/2], Step [17190/64305], Loss: 5.1757\n",
      "Epoch [1/2], Step [17200/64305], Loss: 5.1805\n",
      "Epoch [1/2], Step [17210/64305], Loss: 5.1737\n",
      "Epoch [1/2], Step [17220/64305], Loss: 5.4307\n",
      "Epoch [1/2], Step [17230/64305], Loss: 5.0330\n",
      "Epoch [1/2], Step [17240/64305], Loss: 5.3277\n",
      "Epoch [1/2], Step [17250/64305], Loss: 5.3108\n",
      "Epoch [1/2], Step [17260/64305], Loss: 5.1555\n",
      "Epoch [1/2], Step [17270/64305], Loss: 5.2233\n",
      "Epoch [1/2], Step [17280/64305], Loss: 5.2507\n",
      "Epoch [1/2], Step [17290/64305], Loss: 5.0086\n",
      "Epoch [1/2], Step [17300/64305], Loss: 5.2419\n",
      "Epoch [1/2], Step [17310/64305], Loss: 5.0391\n",
      "Epoch [1/2], Step [17320/64305], Loss: 5.1884\n",
      "Epoch [1/2], Step [17330/64305], Loss: 5.2353\n",
      "Epoch [1/2], Step [17340/64305], Loss: 5.2333\n",
      "Epoch [1/2], Step [17350/64305], Loss: 5.0721\n",
      "Epoch [1/2], Step [17360/64305], Loss: 5.2091\n",
      "Epoch [1/2], Step [17370/64305], Loss: 5.0049\n",
      "Epoch [1/2], Step [17380/64305], Loss: 5.0403\n",
      "Epoch [1/2], Step [17390/64305], Loss: 5.1961\n",
      "Epoch [1/2], Step [17400/64305], Loss: 5.1626\n",
      "Epoch [1/2], Step [17410/64305], Loss: 5.1933\n",
      "Epoch [1/2], Step [17420/64305], Loss: 5.0364\n",
      "Epoch [1/2], Step [17430/64305], Loss: 5.2614\n",
      "Epoch [1/2], Step [17440/64305], Loss: 5.1071\n",
      "Epoch [1/2], Step [17450/64305], Loss: 5.0681\n",
      "Epoch [1/2], Step [17460/64305], Loss: 5.2516\n",
      "Epoch [1/2], Step [17470/64305], Loss: 5.2504\n",
      "Epoch [1/2], Step [17480/64305], Loss: 4.9186\n",
      "Epoch [1/2], Step [17490/64305], Loss: 5.3759\n",
      "Epoch [1/2], Step [17500/64305], Loss: 5.2742\n",
      "Epoch [1/2], Step [17510/64305], Loss: 5.1255\n",
      "Epoch [1/2], Step [17520/64305], Loss: 5.0888\n",
      "Epoch [1/2], Step [17530/64305], Loss: 5.2381\n",
      "Epoch [1/2], Step [17540/64305], Loss: 5.1686\n",
      "Epoch [1/2], Step [17550/64305], Loss: 4.9779\n",
      "Epoch [1/2], Step [17560/64305], Loss: 5.1071\n",
      "Epoch [1/2], Step [17570/64305], Loss: 5.0567\n",
      "Epoch [1/2], Step [17580/64305], Loss: 5.1120\n",
      "Epoch [1/2], Step [17590/64305], Loss: 5.1156\n",
      "Epoch [1/2], Step [17600/64305], Loss: 4.9897\n",
      "Epoch [1/2], Step [17610/64305], Loss: 5.1134\n",
      "Epoch [1/2], Step [17620/64305], Loss: 5.0912\n",
      "Epoch [1/2], Step [17630/64305], Loss: 5.0368\n",
      "Epoch [1/2], Step [17640/64305], Loss: 5.1705\n",
      "Epoch [1/2], Step [17650/64305], Loss: 5.0547\n",
      "Epoch [1/2], Step [17660/64305], Loss: 5.3848\n",
      "Epoch [1/2], Step [17670/64305], Loss: 5.3431\n",
      "Epoch [1/2], Step [17680/64305], Loss: 5.3107\n",
      "Epoch [1/2], Step [17690/64305], Loss: 5.2223\n",
      "Epoch [1/2], Step [17700/64305], Loss: 5.1419\n",
      "Epoch [1/2], Step [17710/64305], Loss: 5.1709\n",
      "Epoch [1/2], Step [17720/64305], Loss: 4.8763\n",
      "Epoch [1/2], Step [17730/64305], Loss: 4.9519\n",
      "Epoch [1/2], Step [17740/64305], Loss: 5.1582\n",
      "Epoch [1/2], Step [17750/64305], Loss: 5.0160\n",
      "Epoch [1/2], Step [17760/64305], Loss: 5.1762\n",
      "Epoch [1/2], Step [17770/64305], Loss: 5.0172\n",
      "Epoch [1/2], Step [17780/64305], Loss: 5.0797\n",
      "Epoch [1/2], Step [17790/64305], Loss: 5.1498\n",
      "Epoch [1/2], Step [17800/64305], Loss: 5.0807\n",
      "Epoch [1/2], Step [17810/64305], Loss: 5.0042\n",
      "Epoch [1/2], Step [17820/64305], Loss: 5.1682\n",
      "Epoch [1/2], Step [17830/64305], Loss: 5.2619\n",
      "Epoch [1/2], Step [17840/64305], Loss: 5.0052\n",
      "Epoch [1/2], Step [17850/64305], Loss: 5.2019\n",
      "Epoch [1/2], Step [17860/64305], Loss: 5.2148\n",
      "Epoch [1/2], Step [17870/64305], Loss: 5.2198\n",
      "Epoch [1/2], Step [17880/64305], Loss: 5.0108\n",
      "Epoch [1/2], Step [17890/64305], Loss: 4.9929\n",
      "Epoch [1/2], Step [17900/64305], Loss: 4.9029\n",
      "Epoch [1/2], Step [17910/64305], Loss: 5.1317\n",
      "Epoch [1/2], Step [17920/64305], Loss: 5.1648\n",
      "Epoch [1/2], Step [17930/64305], Loss: 4.9644\n",
      "Epoch [1/2], Step [17940/64305], Loss: 4.7707\n",
      "Epoch [1/2], Step [17950/64305], Loss: 5.1906\n",
      "Epoch [1/2], Step [17960/64305], Loss: 5.2468\n",
      "Epoch [1/2], Step [17970/64305], Loss: 5.0714\n",
      "Epoch [1/2], Step [17980/64305], Loss: 5.0230\n",
      "Epoch [1/2], Step [17990/64305], Loss: 5.2007\n",
      "Epoch [1/2], Step [18000/64305], Loss: 5.0183\n",
      "Epoch [1/2], Step [18010/64305], Loss: 5.0633\n",
      "Epoch [1/2], Step [18020/64305], Loss: 5.1329\n",
      "Epoch [1/2], Step [18030/64305], Loss: 4.9599\n",
      "Epoch [1/2], Step [18040/64305], Loss: 5.0358\n",
      "Epoch [1/2], Step [18050/64305], Loss: 5.0010\n",
      "Epoch [1/2], Step [18060/64305], Loss: 5.3929\n",
      "Epoch [1/2], Step [18070/64305], Loss: 5.0194\n",
      "Epoch [1/2], Step [18080/64305], Loss: 5.2659\n",
      "Epoch [1/2], Step [18090/64305], Loss: 5.1358\n",
      "Epoch [1/2], Step [18100/64305], Loss: 5.0600\n",
      "Epoch [1/2], Step [18110/64305], Loss: 4.9966\n",
      "Epoch [1/2], Step [18120/64305], Loss: 4.7963\n",
      "Epoch [1/2], Step [18130/64305], Loss: 4.9669\n",
      "Epoch [1/2], Step [18140/64305], Loss: 5.1221\n",
      "Epoch [1/2], Step [18150/64305], Loss: 5.0781\n",
      "Epoch [1/2], Step [18160/64305], Loss: 4.8500\n",
      "Epoch [1/2], Step [18170/64305], Loss: 4.9084\n",
      "Epoch [1/2], Step [18180/64305], Loss: 5.0261\n",
      "Epoch [1/2], Step [18190/64305], Loss: 5.1979\n",
      "Epoch [1/2], Step [18200/64305], Loss: 5.1423\n",
      "Epoch [1/2], Step [18210/64305], Loss: 5.1262\n",
      "Epoch [1/2], Step [18220/64305], Loss: 5.1925\n",
      "Epoch [1/2], Step [18230/64305], Loss: 5.3041\n",
      "Epoch [1/2], Step [18240/64305], Loss: 5.1137\n",
      "Epoch [1/2], Step [18250/64305], Loss: 5.2665\n",
      "Epoch [1/2], Step [18260/64305], Loss: 5.0905\n",
      "Epoch [1/2], Step [18270/64305], Loss: 5.5434\n",
      "Epoch [1/2], Step [18280/64305], Loss: 5.1710\n",
      "Epoch [1/2], Step [18290/64305], Loss: 4.7890\n",
      "Epoch [1/2], Step [18300/64305], Loss: 5.1365\n",
      "Epoch [1/2], Step [18310/64305], Loss: 5.1107\n",
      "Epoch [1/2], Step [18320/64305], Loss: 5.0999\n",
      "Epoch [1/2], Step [18330/64305], Loss: 5.0890\n",
      "Epoch [1/2], Step [18340/64305], Loss: 5.1377\n",
      "Epoch [1/2], Step [18350/64305], Loss: 5.1964\n",
      "Epoch [1/2], Step [18360/64305], Loss: 4.9716\n",
      "Epoch [1/2], Step [18370/64305], Loss: 5.1931\n",
      "Epoch [1/2], Step [18380/64305], Loss: 5.1338\n",
      "Epoch [1/2], Step [18390/64305], Loss: 5.0807\n",
      "Epoch [1/2], Step [18400/64305], Loss: 5.0517\n",
      "Epoch [1/2], Step [18410/64305], Loss: 5.0176\n",
      "Epoch [1/2], Step [18420/64305], Loss: 4.9522\n",
      "Epoch [1/2], Step [18430/64305], Loss: 5.1534\n",
      "Epoch [1/2], Step [18440/64305], Loss: 5.2933\n",
      "Epoch [1/2], Step [18450/64305], Loss: 5.0825\n",
      "Epoch [1/2], Step [18460/64305], Loss: 4.9259\n",
      "Epoch [1/2], Step [18470/64305], Loss: 5.2131\n",
      "Epoch [1/2], Step [18480/64305], Loss: 5.3174\n",
      "Epoch [1/2], Step [18490/64305], Loss: 5.0287\n",
      "Epoch [1/2], Step [18500/64305], Loss: 5.1017\n",
      "Epoch [1/2], Step [18510/64305], Loss: 5.3973\n",
      "Epoch [1/2], Step [18520/64305], Loss: 5.4197\n",
      "Epoch [1/2], Step [18530/64305], Loss: 5.1128\n",
      "Epoch [1/2], Step [18540/64305], Loss: 5.1333\n",
      "Epoch [1/2], Step [18550/64305], Loss: 5.0263\n",
      "Epoch [1/2], Step [18560/64305], Loss: 5.2521\n",
      "Epoch [1/2], Step [18570/64305], Loss: 5.0793\n",
      "Epoch [1/2], Step [18580/64305], Loss: 5.1147\n",
      "Epoch [1/2], Step [18590/64305], Loss: 5.2258\n",
      "Epoch [1/2], Step [18600/64305], Loss: 5.3514\n",
      "Epoch [1/2], Step [18610/64305], Loss: 5.0945\n",
      "Epoch [1/2], Step [18620/64305], Loss: 5.0914\n",
      "Epoch [1/2], Step [18630/64305], Loss: 5.3258\n",
      "Epoch [1/2], Step [18640/64305], Loss: 5.1265\n",
      "Epoch [1/2], Step [18650/64305], Loss: 4.9793\n",
      "Epoch [1/2], Step [18660/64305], Loss: 5.1803\n",
      "Epoch [1/2], Step [18670/64305], Loss: 5.0526\n",
      "Epoch [1/2], Step [18680/64305], Loss: 5.0537\n",
      "Epoch [1/2], Step [18690/64305], Loss: 5.2047\n",
      "Epoch [1/2], Step [18700/64305], Loss: 5.1529\n",
      "Epoch [1/2], Step [18710/64305], Loss: 5.2700\n",
      "Epoch [1/2], Step [18720/64305], Loss: 5.1321\n",
      "Epoch [1/2], Step [18730/64305], Loss: 5.2162\n",
      "Epoch [1/2], Step [18740/64305], Loss: 5.1919\n",
      "Epoch [1/2], Step [18750/64305], Loss: 4.9452\n",
      "Epoch [1/2], Step [18760/64305], Loss: 5.2039\n",
      "Epoch [1/2], Step [18770/64305], Loss: 5.2771\n",
      "Epoch [1/2], Step [18780/64305], Loss: 5.3517\n",
      "Epoch [1/2], Step [18790/64305], Loss: 5.2953\n",
      "Epoch [1/2], Step [18800/64305], Loss: 5.2023\n",
      "Epoch [1/2], Step [18810/64305], Loss: 5.1415\n",
      "Epoch [1/2], Step [18820/64305], Loss: 5.1764\n",
      "Epoch [1/2], Step [18830/64305], Loss: 5.2994\n",
      "Epoch [1/2], Step [18840/64305], Loss: 4.9144\n",
      "Epoch [1/2], Step [18850/64305], Loss: 5.2400\n",
      "Epoch [1/2], Step [18860/64305], Loss: 5.1938\n",
      "Epoch [1/2], Step [18870/64305], Loss: 5.1247\n",
      "Epoch [1/2], Step [18880/64305], Loss: 4.9417\n",
      "Epoch [1/2], Step [18890/64305], Loss: 4.9290\n",
      "Epoch [1/2], Step [18900/64305], Loss: 5.0430\n",
      "Epoch [1/2], Step [18910/64305], Loss: 5.2295\n",
      "Epoch [1/2], Step [18920/64305], Loss: 5.1263\n",
      "Epoch [1/2], Step [18930/64305], Loss: 5.3141\n",
      "Epoch [1/2], Step [18940/64305], Loss: 5.0630\n",
      "Epoch [1/2], Step [18950/64305], Loss: 5.0097\n",
      "Epoch [1/2], Step [18960/64305], Loss: 5.2082\n",
      "Epoch [1/2], Step [18970/64305], Loss: 5.2955\n",
      "Epoch [1/2], Step [18980/64305], Loss: 5.1023\n",
      "Epoch [1/2], Step [18990/64305], Loss: 5.1560\n",
      "Epoch [1/2], Step [19000/64305], Loss: 5.1112\n",
      "Epoch [1/2], Step [19010/64305], Loss: 5.0900\n",
      "Epoch [1/2], Step [19020/64305], Loss: 4.8991\n",
      "Epoch [1/2], Step [19030/64305], Loss: 5.1081\n",
      "Epoch [1/2], Step [19040/64305], Loss: 5.1151\n",
      "Epoch [1/2], Step [19050/64305], Loss: 5.1638\n",
      "Epoch [1/2], Step [19060/64305], Loss: 5.1987\n",
      "Epoch [1/2], Step [19070/64305], Loss: 5.1119\n",
      "Epoch [1/2], Step [19080/64305], Loss: 5.1179\n",
      "Epoch [1/2], Step [19090/64305], Loss: 5.0642\n",
      "Epoch [1/2], Step [19100/64305], Loss: 5.1703\n",
      "Epoch [1/2], Step [19110/64305], Loss: 4.9387\n",
      "Epoch [1/2], Step [19120/64305], Loss: 4.9400\n",
      "Epoch [1/2], Step [19130/64305], Loss: 5.1063\n",
      "Epoch [1/2], Step [19140/64305], Loss: 5.1567\n",
      "Epoch [1/2], Step [19150/64305], Loss: 5.0956\n",
      "Epoch [1/2], Step [19160/64305], Loss: 5.1017\n",
      "Epoch [1/2], Step [19170/64305], Loss: 4.7794\n",
      "Epoch [1/2], Step [19180/64305], Loss: 5.1479\n",
      "Epoch [1/2], Step [19190/64305], Loss: 4.9091\n",
      "Epoch [1/2], Step [19200/64305], Loss: 5.0968\n",
      "Epoch [1/2], Step [19210/64305], Loss: 4.8542\n",
      "Epoch [1/2], Step [19220/64305], Loss: 5.2299\n",
      "Epoch [1/2], Step [19230/64305], Loss: 5.1436\n",
      "Epoch [1/2], Step [19240/64305], Loss: 5.1027\n",
      "Epoch [1/2], Step [19250/64305], Loss: 5.1700\n",
      "Epoch [1/2], Step [19260/64305], Loss: 4.9576\n",
      "Epoch [1/2], Step [19270/64305], Loss: 4.8958\n",
      "Epoch [1/2], Step [19280/64305], Loss: 5.1606\n",
      "Epoch [1/2], Step [19290/64305], Loss: 5.1838\n",
      "Epoch [1/2], Step [19300/64305], Loss: 5.1051\n",
      "Epoch [1/2], Step [19310/64305], Loss: 4.9775\n",
      "Epoch [1/2], Step [19320/64305], Loss: 5.0511\n",
      "Epoch [1/2], Step [19330/64305], Loss: 5.2380\n",
      "Epoch [1/2], Step [19340/64305], Loss: 5.0446\n",
      "Epoch [1/2], Step [19350/64305], Loss: 5.0383\n",
      "Epoch [1/2], Step [19360/64305], Loss: 5.2172\n",
      "Epoch [1/2], Step [19370/64305], Loss: 5.0776\n",
      "Epoch [1/2], Step [19380/64305], Loss: 5.2647\n",
      "Epoch [1/2], Step [19390/64305], Loss: 5.2309\n",
      "Epoch [1/2], Step [19400/64305], Loss: 4.9865\n",
      "Epoch [1/2], Step [19410/64305], Loss: 5.0958\n",
      "Epoch [1/2], Step [19420/64305], Loss: 5.0996\n",
      "Epoch [1/2], Step [19430/64305], Loss: 5.1604\n",
      "Epoch [1/2], Step [19440/64305], Loss: 5.1196\n",
      "Epoch [1/2], Step [19450/64305], Loss: 5.3157\n",
      "Epoch [1/2], Step [19460/64305], Loss: 4.8472\n",
      "Epoch [1/2], Step [19470/64305], Loss: 5.0057\n",
      "Epoch [1/2], Step [19480/64305], Loss: 5.0477\n",
      "Epoch [1/2], Step [19490/64305], Loss: 5.0657\n",
      "Epoch [1/2], Step [19500/64305], Loss: 5.3450\n",
      "Epoch [1/2], Step [19510/64305], Loss: 5.0240\n",
      "Epoch [1/2], Step [19520/64305], Loss: 4.8416\n",
      "Epoch [1/2], Step [19530/64305], Loss: 5.1015\n",
      "Epoch [1/2], Step [19540/64305], Loss: 5.0074\n",
      "Epoch [1/2], Step [19550/64305], Loss: 5.3757\n",
      "Epoch [1/2], Step [19560/64305], Loss: 5.1709\n",
      "Epoch [1/2], Step [19570/64305], Loss: 5.1249\n",
      "Epoch [1/2], Step [19580/64305], Loss: 4.9245\n",
      "Epoch [1/2], Step [19590/64305], Loss: 5.2268\n",
      "Epoch [1/2], Step [19600/64305], Loss: 5.1391\n",
      "Epoch [1/2], Step [19610/64305], Loss: 4.9961\n",
      "Epoch [1/2], Step [19620/64305], Loss: 5.0162\n",
      "Epoch [1/2], Step [19630/64305], Loss: 5.1758\n",
      "Epoch [1/2], Step [19640/64305], Loss: 5.1052\n",
      "Epoch [1/2], Step [19650/64305], Loss: 5.0938\n",
      "Epoch [1/2], Step [19660/64305], Loss: 4.8112\n",
      "Epoch [1/2], Step [19670/64305], Loss: 5.0072\n",
      "Epoch [1/2], Step [19680/64305], Loss: 5.1375\n",
      "Epoch [1/2], Step [19690/64305], Loss: 5.2380\n",
      "Epoch [1/2], Step [19700/64305], Loss: 5.1922\n",
      "Epoch [1/2], Step [19710/64305], Loss: 5.0577\n",
      "Epoch [1/2], Step [19720/64305], Loss: 5.0228\n",
      "Epoch [1/2], Step [19730/64305], Loss: 4.9689\n",
      "Epoch [1/2], Step [19740/64305], Loss: 5.2047\n",
      "Epoch [1/2], Step [19750/64305], Loss: 5.2625\n",
      "Epoch [1/2], Step [19760/64305], Loss: 4.8152\n",
      "Epoch [1/2], Step [19770/64305], Loss: 5.0068\n",
      "Epoch [1/2], Step [19780/64305], Loss: 5.3498\n",
      "Epoch [1/2], Step [19790/64305], Loss: 5.2839\n",
      "Epoch [1/2], Step [19800/64305], Loss: 5.0851\n",
      "Epoch [1/2], Step [19810/64305], Loss: 5.2736\n",
      "Epoch [1/2], Step [19820/64305], Loss: 4.9572\n",
      "Epoch [1/2], Step [19830/64305], Loss: 5.0307\n",
      "Epoch [1/2], Step [19840/64305], Loss: 4.9757\n",
      "Epoch [1/2], Step [19850/64305], Loss: 5.3443\n",
      "Epoch [1/2], Step [19860/64305], Loss: 5.2617\n",
      "Epoch [1/2], Step [19870/64305], Loss: 4.9483\n",
      "Epoch [1/2], Step [19880/64305], Loss: 5.3596\n",
      "Epoch [1/2], Step [19890/64305], Loss: 5.1967\n",
      "Epoch [1/2], Step [19900/64305], Loss: 5.1167\n",
      "Epoch [1/2], Step [19910/64305], Loss: 5.2641\n",
      "Epoch [1/2], Step [19920/64305], Loss: 5.2282\n",
      "Epoch [1/2], Step [19930/64305], Loss: 5.2746\n",
      "Epoch [1/2], Step [19940/64305], Loss: 5.2295\n",
      "Epoch [1/2], Step [19950/64305], Loss: 5.0908\n",
      "Epoch [1/2], Step [19960/64305], Loss: 5.2720\n",
      "Epoch [1/2], Step [19970/64305], Loss: 5.2495\n",
      "Epoch [1/2], Step [19980/64305], Loss: 5.0681\n",
      "Epoch [1/2], Step [19990/64305], Loss: 5.1000\n",
      "Epoch [1/2], Step [20000/64305], Loss: 5.2111\n",
      "Epoch [1/2], Step [20010/64305], Loss: 5.1712\n",
      "Epoch [1/2], Step [20020/64305], Loss: 5.1294\n",
      "Epoch [1/2], Step [20030/64305], Loss: 5.3076\n",
      "Epoch [1/2], Step [20040/64305], Loss: 5.1929\n",
      "Epoch [1/2], Step [20050/64305], Loss: 5.1541\n",
      "Epoch [1/2], Step [20060/64305], Loss: 5.1768\n",
      "Epoch [1/2], Step [20070/64305], Loss: 5.0532\n",
      "Epoch [1/2], Step [20080/64305], Loss: 5.2186\n",
      "Epoch [1/2], Step [20090/64305], Loss: 4.8832\n",
      "Epoch [1/2], Step [20100/64305], Loss: 5.1002\n",
      "Epoch [1/2], Step [20110/64305], Loss: 4.9850\n",
      "Epoch [1/2], Step [20120/64305], Loss: 5.4191\n",
      "Epoch [1/2], Step [20130/64305], Loss: 5.1155\n",
      "Epoch [1/2], Step [20140/64305], Loss: 5.0248\n",
      "Epoch [1/2], Step [20150/64305], Loss: 5.2531\n",
      "Epoch [1/2], Step [20160/64305], Loss: 5.1961\n",
      "Epoch [1/2], Step [20170/64305], Loss: 5.1805\n",
      "Epoch [1/2], Step [20180/64305], Loss: 4.9691\n",
      "Epoch [1/2], Step [20190/64305], Loss: 5.0061\n",
      "Epoch [1/2], Step [20200/64305], Loss: 5.0979\n",
      "Epoch [1/2], Step [20210/64305], Loss: 5.2901\n",
      "Epoch [1/2], Step [20220/64305], Loss: 5.0408\n",
      "Epoch [1/2], Step [20230/64305], Loss: 5.1538\n",
      "Epoch [1/2], Step [20240/64305], Loss: 4.9405\n",
      "Epoch [1/2], Step [20250/64305], Loss: 5.1292\n",
      "Epoch [1/2], Step [20260/64305], Loss: 5.1163\n",
      "Epoch [1/2], Step [20270/64305], Loss: 5.0728\n",
      "Epoch [1/2], Step [20280/64305], Loss: 5.0621\n",
      "Epoch [1/2], Step [20290/64305], Loss: 5.0918\n",
      "Epoch [1/2], Step [20300/64305], Loss: 5.1232\n",
      "Epoch [1/2], Step [20310/64305], Loss: 5.0752\n",
      "Epoch [1/2], Step [20320/64305], Loss: 4.9630\n",
      "Epoch [1/2], Step [20330/64305], Loss: 5.0562\n",
      "Epoch [1/2], Step [20340/64305], Loss: 5.1455\n",
      "Epoch [1/2], Step [20350/64305], Loss: 5.1488\n",
      "Epoch [1/2], Step [20360/64305], Loss: 5.1322\n",
      "Epoch [1/2], Step [20370/64305], Loss: 5.0739\n",
      "Epoch [1/2], Step [20380/64305], Loss: 5.1056\n",
      "Epoch [1/2], Step [20390/64305], Loss: 5.2073\n",
      "Epoch [1/2], Step [20400/64305], Loss: 5.1272\n",
      "Epoch [1/2], Step [20410/64305], Loss: 5.2814\n",
      "Epoch [1/2], Step [20420/64305], Loss: 4.9110\n",
      "Epoch [1/2], Step [20430/64305], Loss: 5.2442\n",
      "Epoch [1/2], Step [20440/64305], Loss: 5.5604\n",
      "Epoch [1/2], Step [20450/64305], Loss: 5.3134\n",
      "Epoch [1/2], Step [20460/64305], Loss: 4.8031\n",
      "Epoch [1/2], Step [20470/64305], Loss: 4.9912\n",
      "Epoch [1/2], Step [20480/64305], Loss: 4.9876\n",
      "Epoch [1/2], Step [20490/64305], Loss: 4.9832\n",
      "Epoch [1/2], Step [20500/64305], Loss: 5.0534\n",
      "Epoch [1/2], Step [20510/64305], Loss: 5.2697\n",
      "Epoch [1/2], Step [20520/64305], Loss: 5.0060\n",
      "Epoch [1/2], Step [20530/64305], Loss: 4.9418\n",
      "Epoch [1/2], Step [20540/64305], Loss: 5.0264\n",
      "Epoch [1/2], Step [20550/64305], Loss: 5.0621\n",
      "Epoch [1/2], Step [20560/64305], Loss: 5.0152\n",
      "Epoch [1/2], Step [20570/64305], Loss: 5.0683\n",
      "Epoch [1/2], Step [20580/64305], Loss: 4.9960\n",
      "Epoch [1/2], Step [20590/64305], Loss: 4.8301\n",
      "Epoch [1/2], Step [20600/64305], Loss: 5.2383\n",
      "Epoch [1/2], Step [20610/64305], Loss: 5.0559\n",
      "Epoch [1/2], Step [20620/64305], Loss: 4.9598\n",
      "Epoch [1/2], Step [20630/64305], Loss: 5.1350\n",
      "Epoch [1/2], Step [20640/64305], Loss: 4.9393\n",
      "Epoch [1/2], Step [20650/64305], Loss: 5.0067\n",
      "Epoch [1/2], Step [20660/64305], Loss: 5.0865\n",
      "Epoch [1/2], Step [20670/64305], Loss: 5.1580\n",
      "Epoch [1/2], Step [20680/64305], Loss: 4.9234\n",
      "Epoch [1/2], Step [20690/64305], Loss: 5.0952\n",
      "Epoch [1/2], Step [20700/64305], Loss: 5.1803\n",
      "Epoch [1/2], Step [20710/64305], Loss: 5.0545\n",
      "Epoch [1/2], Step [20720/64305], Loss: 4.8645\n",
      "Epoch [1/2], Step [20730/64305], Loss: 5.1161\n",
      "Epoch [1/2], Step [20740/64305], Loss: 5.1797\n",
      "Epoch [1/2], Step [20750/64305], Loss: 4.9551\n",
      "Epoch [1/2], Step [20760/64305], Loss: 5.0403\n",
      "Epoch [1/2], Step [20770/64305], Loss: 5.1174\n",
      "Epoch [1/2], Step [20780/64305], Loss: 4.8997\n",
      "Epoch [1/2], Step [20790/64305], Loss: 5.0177\n",
      "Epoch [1/2], Step [20800/64305], Loss: 5.0869\n",
      "Epoch [1/2], Step [20810/64305], Loss: 4.9610\n",
      "Epoch [1/2], Step [20820/64305], Loss: 5.0445\n",
      "Epoch [1/2], Step [20830/64305], Loss: 5.2662\n",
      "Epoch [1/2], Step [20840/64305], Loss: 5.0171\n",
      "Epoch [1/2], Step [20850/64305], Loss: 4.9852\n",
      "Epoch [1/2], Step [20860/64305], Loss: 4.9886\n",
      "Epoch [1/2], Step [20870/64305], Loss: 5.0308\n",
      "Epoch [1/2], Step [20880/64305], Loss: 5.0963\n",
      "Epoch [1/2], Step [20890/64305], Loss: 5.0379\n",
      "Epoch [1/2], Step [20900/64305], Loss: 5.1993\n",
      "Epoch [1/2], Step [20910/64305], Loss: 5.0085\n",
      "Epoch [1/2], Step [20920/64305], Loss: 5.2100\n",
      "Epoch [1/2], Step [20930/64305], Loss: 4.8435\n",
      "Epoch [1/2], Step [20940/64305], Loss: 5.0288\n",
      "Epoch [1/2], Step [20950/64305], Loss: 4.8483\n",
      "Epoch [1/2], Step [20960/64305], Loss: 5.2412\n",
      "Epoch [1/2], Step [20970/64305], Loss: 5.0206\n",
      "Epoch [1/2], Step [20980/64305], Loss: 4.9947\n",
      "Epoch [1/2], Step [20990/64305], Loss: 5.3204\n",
      "Epoch [1/2], Step [21000/64305], Loss: 5.2289\n",
      "Epoch [1/2], Step [21010/64305], Loss: 5.0320\n",
      "Epoch [1/2], Step [21020/64305], Loss: 5.3318\n",
      "Epoch [1/2], Step [21030/64305], Loss: 5.1545\n",
      "Epoch [1/2], Step [21040/64305], Loss: 5.1093\n",
      "Epoch [1/2], Step [21050/64305], Loss: 4.9222\n",
      "Epoch [1/2], Step [21060/64305], Loss: 5.1578\n",
      "Epoch [1/2], Step [21070/64305], Loss: 5.0950\n",
      "Epoch [1/2], Step [21080/64305], Loss: 4.8568\n",
      "Epoch [1/2], Step [21090/64305], Loss: 4.9485\n",
      "Epoch [1/2], Step [21100/64305], Loss: 5.3047\n",
      "Epoch [1/2], Step [21110/64305], Loss: 5.0402\n",
      "Epoch [1/2], Step [21120/64305], Loss: 5.1805\n",
      "Epoch [1/2], Step [21130/64305], Loss: 5.0455\n",
      "Epoch [1/2], Step [21140/64305], Loss: 5.0415\n",
      "Epoch [1/2], Step [21150/64305], Loss: 5.2180\n",
      "Epoch [1/2], Step [21160/64305], Loss: 5.0901\n",
      "Epoch [1/2], Step [21170/64305], Loss: 5.0331\n",
      "Epoch [1/2], Step [21180/64305], Loss: 5.2191\n",
      "Epoch [1/2], Step [21190/64305], Loss: 5.0372\n",
      "Epoch [1/2], Step [21200/64305], Loss: 5.0532\n",
      "Epoch [1/2], Step [21210/64305], Loss: 5.0229\n",
      "Epoch [1/2], Step [21220/64305], Loss: 5.1275\n",
      "Epoch [1/2], Step [21230/64305], Loss: 5.0455\n",
      "Epoch [1/2], Step [21240/64305], Loss: 4.8301\n",
      "Epoch [1/2], Step [21250/64305], Loss: 5.1901\n",
      "Epoch [1/2], Step [21260/64305], Loss: 5.5926\n",
      "Epoch [1/2], Step [21270/64305], Loss: 4.9952\n",
      "Epoch [1/2], Step [21280/64305], Loss: 5.1482\n",
      "Epoch [1/2], Step [21290/64305], Loss: 5.0870\n",
      "Epoch [1/2], Step [21300/64305], Loss: 4.9376\n",
      "Epoch [1/2], Step [21310/64305], Loss: 4.9692\n",
      "Epoch [1/2], Step [21320/64305], Loss: 4.9287\n",
      "Epoch [1/2], Step [21330/64305], Loss: 5.0500\n",
      "Epoch [1/2], Step [21340/64305], Loss: 5.0264\n",
      "Epoch [1/2], Step [21350/64305], Loss: 5.0340\n",
      "Epoch [1/2], Step [21360/64305], Loss: 4.8189\n",
      "Epoch [1/2], Step [21370/64305], Loss: 5.2607\n",
      "Epoch [1/2], Step [21380/64305], Loss: 4.9351\n",
      "Epoch [1/2], Step [21390/64305], Loss: 5.2121\n",
      "Epoch [1/2], Step [21400/64305], Loss: 5.2306\n",
      "Epoch [1/2], Step [21410/64305], Loss: 4.9263\n",
      "Epoch [1/2], Step [21420/64305], Loss: 5.1466\n",
      "Epoch [1/2], Step [21430/64305], Loss: 5.1943\n",
      "Epoch [1/2], Step [21440/64305], Loss: 5.1091\n",
      "Epoch [1/2], Step [21450/64305], Loss: 5.0443\n",
      "Epoch [1/2], Step [21460/64305], Loss: 5.0858\n",
      "Epoch [1/2], Step [21470/64305], Loss: 5.1577\n",
      "Epoch [1/2], Step [21480/64305], Loss: 5.1294\n",
      "Epoch [1/2], Step [21490/64305], Loss: 5.0615\n",
      "Epoch [1/2], Step [21500/64305], Loss: 5.1923\n",
      "Epoch [1/2], Step [21510/64305], Loss: 5.0119\n",
      "Epoch [1/2], Step [21520/64305], Loss: 5.0869\n",
      "Epoch [1/2], Step [21530/64305], Loss: 5.1721\n",
      "Epoch [1/2], Step [21540/64305], Loss: 5.2390\n",
      "Epoch [1/2], Step [21550/64305], Loss: 4.8917\n",
      "Epoch [1/2], Step [21560/64305], Loss: 5.1544\n",
      "Epoch [1/2], Step [21570/64305], Loss: 4.9871\n",
      "Epoch [1/2], Step [21580/64305], Loss: 4.9698\n",
      "Epoch [1/2], Step [21590/64305], Loss: 4.8005\n",
      "Epoch [1/2], Step [21600/64305], Loss: 5.0259\n",
      "Epoch [1/2], Step [21610/64305], Loss: 4.9879\n",
      "Epoch [1/2], Step [21620/64305], Loss: 5.1385\n",
      "Epoch [1/2], Step [21630/64305], Loss: 5.1421\n",
      "Epoch [1/2], Step [21640/64305], Loss: 5.2838\n",
      "Epoch [1/2], Step [21650/64305], Loss: 5.1443\n",
      "Epoch [1/2], Step [21660/64305], Loss: 5.0975\n",
      "Epoch [1/2], Step [21670/64305], Loss: 5.1056\n",
      "Epoch [1/2], Step [21680/64305], Loss: 5.0340\n",
      "Epoch [1/2], Step [21690/64305], Loss: 5.0141\n",
      "Epoch [1/2], Step [21700/64305], Loss: 5.1182\n",
      "Epoch [1/2], Step [21710/64305], Loss: 5.1012\n",
      "Epoch [1/2], Step [21720/64305], Loss: 5.2415\n",
      "Epoch [1/2], Step [21730/64305], Loss: 4.8934\n",
      "Epoch [1/2], Step [21740/64305], Loss: 5.1514\n",
      "Epoch [1/2], Step [21750/64305], Loss: 4.9202\n",
      "Epoch [1/2], Step [21760/64305], Loss: 4.8887\n",
      "Epoch [1/2], Step [21770/64305], Loss: 4.7855\n",
      "Epoch [1/2], Step [21780/64305], Loss: 4.9588\n",
      "Epoch [1/2], Step [21790/64305], Loss: 4.8907\n",
      "Epoch [1/2], Step [21800/64305], Loss: 4.8804\n",
      "Epoch [1/2], Step [21810/64305], Loss: 5.2157\n",
      "Epoch [1/2], Step [21820/64305], Loss: 4.9736\n",
      "Epoch [1/2], Step [21830/64305], Loss: 4.9761\n",
      "Epoch [1/2], Step [21840/64305], Loss: 5.1472\n",
      "Epoch [1/2], Step [21850/64305], Loss: 5.1963\n",
      "Epoch [1/2], Step [21860/64305], Loss: 5.1311\n",
      "Epoch [1/2], Step [21870/64305], Loss: 4.9335\n",
      "Epoch [1/2], Step [21880/64305], Loss: 5.1458\n",
      "Epoch [1/2], Step [21890/64305], Loss: 5.2221\n",
      "Epoch [1/2], Step [21900/64305], Loss: 5.1311\n",
      "Epoch [1/2], Step [21910/64305], Loss: 5.0483\n",
      "Epoch [1/2], Step [21920/64305], Loss: 4.9716\n",
      "Epoch [1/2], Step [21930/64305], Loss: 5.1767\n",
      "Epoch [1/2], Step [21940/64305], Loss: 5.1834\n",
      "Epoch [1/2], Step [21950/64305], Loss: 5.1020\n",
      "Epoch [1/2], Step [21960/64305], Loss: 5.2284\n",
      "Epoch [1/2], Step [21970/64305], Loss: 5.0146\n",
      "Epoch [1/2], Step [21980/64305], Loss: 4.9279\n",
      "Epoch [1/2], Step [21990/64305], Loss: 5.2067\n",
      "Epoch [1/2], Step [22000/64305], Loss: 4.9230\n",
      "Epoch [1/2], Step [22010/64305], Loss: 5.2377\n",
      "Epoch [1/2], Step [22020/64305], Loss: 4.9651\n",
      "Epoch [1/2], Step [22030/64305], Loss: 5.1145\n",
      "Epoch [1/2], Step [22040/64305], Loss: 5.2662\n",
      "Epoch [1/2], Step [22050/64305], Loss: 4.9620\n",
      "Epoch [1/2], Step [22060/64305], Loss: 5.0600\n",
      "Epoch [1/2], Step [22070/64305], Loss: 5.0602\n",
      "Epoch [1/2], Step [22080/64305], Loss: 5.1474\n",
      "Epoch [1/2], Step [22090/64305], Loss: 5.1194\n",
      "Epoch [1/2], Step [22100/64305], Loss: 4.9516\n",
      "Epoch [1/2], Step [22110/64305], Loss: 4.9127\n",
      "Epoch [1/2], Step [22120/64305], Loss: 4.9407\n",
      "Epoch [1/2], Step [22130/64305], Loss: 5.0724\n",
      "Epoch [1/2], Step [22140/64305], Loss: 5.0738\n",
      "Epoch [1/2], Step [22150/64305], Loss: 5.1402\n",
      "Epoch [1/2], Step [22160/64305], Loss: 4.8023\n",
      "Epoch [1/2], Step [22170/64305], Loss: 5.1190\n",
      "Epoch [1/2], Step [22180/64305], Loss: 5.0777\n",
      "Epoch [1/2], Step [22190/64305], Loss: 5.0222\n",
      "Epoch [1/2], Step [22200/64305], Loss: 4.9819\n",
      "Epoch [1/2], Step [22210/64305], Loss: 5.0645\n",
      "Epoch [1/2], Step [22220/64305], Loss: 5.1698\n",
      "Epoch [1/2], Step [22230/64305], Loss: 5.1691\n",
      "Epoch [1/2], Step [22240/64305], Loss: 5.2504\n",
      "Epoch [1/2], Step [22250/64305], Loss: 4.9592\n",
      "Epoch [1/2], Step [22260/64305], Loss: 5.2489\n",
      "Epoch [1/2], Step [22270/64305], Loss: 5.1109\n",
      "Epoch [1/2], Step [22280/64305], Loss: 5.2557\n",
      "Epoch [1/2], Step [22290/64305], Loss: 5.0572\n",
      "Epoch [1/2], Step [22300/64305], Loss: 4.9665\n",
      "Epoch [1/2], Step [22310/64305], Loss: 5.1446\n",
      "Epoch [1/2], Step [22320/64305], Loss: 5.2959\n",
      "Epoch [1/2], Step [22330/64305], Loss: 5.1717\n",
      "Epoch [1/2], Step [22340/64305], Loss: 5.0278\n",
      "Epoch [1/2], Step [22350/64305], Loss: 5.0630\n",
      "Epoch [1/2], Step [22360/64305], Loss: 5.1127\n",
      "Epoch [1/2], Step [22370/64305], Loss: 5.0868\n",
      "Epoch [1/2], Step [22380/64305], Loss: 5.1743\n",
      "Epoch [1/2], Step [22390/64305], Loss: 4.9659\n",
      "Epoch [1/2], Step [22400/64305], Loss: 5.0966\n",
      "Epoch [1/2], Step [22410/64305], Loss: 5.1386\n",
      "Epoch [1/2], Step [22420/64305], Loss: 5.2237\n",
      "Epoch [1/2], Step [22430/64305], Loss: 5.3103\n",
      "Epoch [1/2], Step [22440/64305], Loss: 5.2449\n",
      "Epoch [1/2], Step [22450/64305], Loss: 5.2364\n",
      "Epoch [1/2], Step [22460/64305], Loss: 4.9721\n",
      "Epoch [1/2], Step [22470/64305], Loss: 5.1711\n",
      "Epoch [1/2], Step [22480/64305], Loss: 5.2316\n",
      "Epoch [1/2], Step [22490/64305], Loss: 5.0091\n",
      "Epoch [1/2], Step [22500/64305], Loss: 5.3902\n",
      "Epoch [1/2], Step [22510/64305], Loss: 5.2127\n",
      "Epoch [1/2], Step [22520/64305], Loss: 4.8968\n",
      "Epoch [1/2], Step [22530/64305], Loss: 5.2144\n",
      "Epoch [1/2], Step [22540/64305], Loss: 5.0165\n",
      "Epoch [1/2], Step [22550/64305], Loss: 5.1157\n",
      "Epoch [1/2], Step [22560/64305], Loss: 4.8551\n",
      "Epoch [1/2], Step [22570/64305], Loss: 5.0048\n",
      "Epoch [1/2], Step [22580/64305], Loss: 5.1331\n",
      "Epoch [1/2], Step [22590/64305], Loss: 4.9669\n",
      "Epoch [1/2], Step [22600/64305], Loss: 4.9730\n",
      "Epoch [1/2], Step [22610/64305], Loss: 4.9585\n",
      "Epoch [1/2], Step [22620/64305], Loss: 5.0888\n",
      "Epoch [1/2], Step [22630/64305], Loss: 5.1202\n",
      "Epoch [1/2], Step [22640/64305], Loss: 4.8845\n",
      "Epoch [1/2], Step [22650/64305], Loss: 5.2273\n",
      "Epoch [1/2], Step [22660/64305], Loss: 5.1058\n",
      "Epoch [1/2], Step [22670/64305], Loss: 5.1210\n",
      "Epoch [1/2], Step [22680/64305], Loss: 4.9913\n",
      "Epoch [1/2], Step [22690/64305], Loss: 5.0283\n",
      "Epoch [1/2], Step [22700/64305], Loss: 5.1093\n",
      "Epoch [1/2], Step [22710/64305], Loss: 5.2687\n",
      "Epoch [1/2], Step [22720/64305], Loss: 5.3420\n",
      "Epoch [1/2], Step [22730/64305], Loss: 5.1452\n",
      "Epoch [1/2], Step [22740/64305], Loss: 5.1055\n",
      "Epoch [1/2], Step [22750/64305], Loss: 5.0000\n",
      "Epoch [1/2], Step [22760/64305], Loss: 5.0416\n",
      "Epoch [1/2], Step [22770/64305], Loss: 5.0810\n",
      "Epoch [1/2], Step [22780/64305], Loss: 5.2236\n",
      "Epoch [1/2], Step [22790/64305], Loss: 5.0873\n",
      "Epoch [1/2], Step [22800/64305], Loss: 4.9466\n",
      "Epoch [1/2], Step [22810/64305], Loss: 5.0439\n",
      "Epoch [1/2], Step [22820/64305], Loss: 5.0542\n",
      "Epoch [1/2], Step [22830/64305], Loss: 5.0437\n",
      "Epoch [1/2], Step [22840/64305], Loss: 5.0229\n",
      "Epoch [1/2], Step [22850/64305], Loss: 4.9485\n",
      "Epoch [1/2], Step [22860/64305], Loss: 5.1465\n",
      "Epoch [1/2], Step [22870/64305], Loss: 5.0306\n",
      "Epoch [1/2], Step [22880/64305], Loss: 5.0571\n",
      "Epoch [1/2], Step [22890/64305], Loss: 4.8774\n",
      "Epoch [1/2], Step [22900/64305], Loss: 4.9997\n",
      "Epoch [1/2], Step [22910/64305], Loss: 4.9860\n",
      "Epoch [1/2], Step [22920/64305], Loss: 5.3005\n",
      "Epoch [1/2], Step [22930/64305], Loss: 5.0190\n",
      "Epoch [1/2], Step [22940/64305], Loss: 5.3184\n",
      "Epoch [1/2], Step [22950/64305], Loss: 5.0001\n",
      "Epoch [1/2], Step [22960/64305], Loss: 5.0836\n",
      "Epoch [1/2], Step [22970/64305], Loss: 4.9559\n",
      "Epoch [1/2], Step [22980/64305], Loss: 5.1976\n",
      "Epoch [1/2], Step [22990/64305], Loss: 5.1121\n",
      "Epoch [1/2], Step [23000/64305], Loss: 4.9596\n",
      "Epoch [1/2], Step [23010/64305], Loss: 5.1165\n",
      "Epoch [1/2], Step [23020/64305], Loss: 5.0190\n",
      "Epoch [1/2], Step [23030/64305], Loss: 5.0216\n",
      "Epoch [1/2], Step [23040/64305], Loss: 4.8918\n",
      "Epoch [1/2], Step [23050/64305], Loss: 4.9817\n",
      "Epoch [1/2], Step [23060/64305], Loss: 5.1079\n",
      "Epoch [1/2], Step [23070/64305], Loss: 4.9374\n",
      "Epoch [1/2], Step [23080/64305], Loss: 5.1283\n",
      "Epoch [1/2], Step [23090/64305], Loss: 5.1819\n",
      "Epoch [1/2], Step [23100/64305], Loss: 5.2209\n",
      "Epoch [1/2], Step [23110/64305], Loss: 5.0241\n",
      "Epoch [1/2], Step [23120/64305], Loss: 5.1998\n",
      "Epoch [1/2], Step [23130/64305], Loss: 4.9856\n",
      "Epoch [1/2], Step [23140/64305], Loss: 5.0897\n",
      "Epoch [1/2], Step [23150/64305], Loss: 5.1362\n",
      "Epoch [1/2], Step [23160/64305], Loss: 5.1302\n",
      "Epoch [1/2], Step [23170/64305], Loss: 5.0344\n",
      "Epoch [1/2], Step [23180/64305], Loss: 5.0661\n",
      "Epoch [1/2], Step [23190/64305], Loss: 5.1975\n",
      "Epoch [1/2], Step [23200/64305], Loss: 5.0954\n",
      "Epoch [1/2], Step [23210/64305], Loss: 5.0071\n",
      "Epoch [1/2], Step [23220/64305], Loss: 4.9273\n",
      "Epoch [1/2], Step [23230/64305], Loss: 5.1091\n",
      "Epoch [1/2], Step [23240/64305], Loss: 5.1146\n",
      "Epoch [1/2], Step [23250/64305], Loss: 5.1944\n",
      "Epoch [1/2], Step [23260/64305], Loss: 4.9856\n",
      "Epoch [1/2], Step [23270/64305], Loss: 4.9891\n",
      "Epoch [1/2], Step [23280/64305], Loss: 5.0259\n",
      "Epoch [1/2], Step [23290/64305], Loss: 5.0261\n",
      "Epoch [1/2], Step [23300/64305], Loss: 5.1432\n",
      "Epoch [1/2], Step [23310/64305], Loss: 5.0693\n",
      "Epoch [1/2], Step [23320/64305], Loss: 4.9483\n",
      "Epoch [1/2], Step [23330/64305], Loss: 4.8640\n",
      "Epoch [1/2], Step [23340/64305], Loss: 5.2111\n",
      "Epoch [1/2], Step [23350/64305], Loss: 5.1434\n",
      "Epoch [1/2], Step [23360/64305], Loss: 5.0402\n",
      "Epoch [1/2], Step [23370/64305], Loss: 4.8829\n",
      "Epoch [1/2], Step [23380/64305], Loss: 5.0876\n",
      "Epoch [1/2], Step [23390/64305], Loss: 5.1343\n",
      "Epoch [1/2], Step [23400/64305], Loss: 5.0548\n",
      "Epoch [1/2], Step [23410/64305], Loss: 5.0009\n",
      "Epoch [1/2], Step [23420/64305], Loss: 4.8911\n",
      "Epoch [1/2], Step [23430/64305], Loss: 5.1806\n",
      "Epoch [1/2], Step [23440/64305], Loss: 5.1954\n",
      "Epoch [1/2], Step [23450/64305], Loss: 5.2178\n",
      "Epoch [1/2], Step [23460/64305], Loss: 5.0631\n",
      "Epoch [1/2], Step [23470/64305], Loss: 4.9138\n",
      "Epoch [1/2], Step [23480/64305], Loss: 5.0727\n",
      "Epoch [1/2], Step [23490/64305], Loss: 4.9556\n",
      "Epoch [1/2], Step [23500/64305], Loss: 5.0902\n",
      "Epoch [1/2], Step [23510/64305], Loss: 4.9523\n",
      "Epoch [1/2], Step [23520/64305], Loss: 5.0473\n",
      "Epoch [1/2], Step [23530/64305], Loss: 5.0232\n",
      "Epoch [1/2], Step [23540/64305], Loss: 5.0496\n",
      "Epoch [1/2], Step [23550/64305], Loss: 5.1195\n",
      "Epoch [1/2], Step [23560/64305], Loss: 5.0982\n",
      "Epoch [1/2], Step [23570/64305], Loss: 4.9356\n",
      "Epoch [1/2], Step [23580/64305], Loss: 5.1083\n",
      "Epoch [1/2], Step [23590/64305], Loss: 5.1740\n",
      "Epoch [1/2], Step [23600/64305], Loss: 4.9990\n",
      "Epoch [1/2], Step [23610/64305], Loss: 4.9683\n",
      "Epoch [1/2], Step [23620/64305], Loss: 5.2366\n",
      "Epoch [1/2], Step [23630/64305], Loss: 4.9985\n",
      "Epoch [1/2], Step [23640/64305], Loss: 5.1218\n",
      "Epoch [1/2], Step [23650/64305], Loss: 5.2111\n",
      "Epoch [1/2], Step [23660/64305], Loss: 5.0920\n",
      "Epoch [1/2], Step [23670/64305], Loss: 5.0891\n",
      "Epoch [1/2], Step [23680/64305], Loss: 5.0031\n",
      "Epoch [1/2], Step [23690/64305], Loss: 4.9865\n",
      "Epoch [1/2], Step [23700/64305], Loss: 4.9736\n",
      "Epoch [1/2], Step [23710/64305], Loss: 4.7775\n",
      "Epoch [1/2], Step [23720/64305], Loss: 5.0432\n",
      "Epoch [1/2], Step [23730/64305], Loss: 4.8012\n",
      "Epoch [1/2], Step [23740/64305], Loss: 4.9488\n",
      "Epoch [1/2], Step [23750/64305], Loss: 5.1080\n",
      "Epoch [1/2], Step [23760/64305], Loss: 5.2462\n",
      "Epoch [1/2], Step [23770/64305], Loss: 5.0675\n",
      "Epoch [1/2], Step [23780/64305], Loss: 5.2016\n",
      "Epoch [1/2], Step [23790/64305], Loss: 5.1003\n",
      "Epoch [1/2], Step [23800/64305], Loss: 5.0123\n",
      "Epoch [1/2], Step [23810/64305], Loss: 4.9682\n",
      "Epoch [1/2], Step [23820/64305], Loss: 5.1679\n",
      "Epoch [1/2], Step [23830/64305], Loss: 4.9527\n",
      "Epoch [1/2], Step [23840/64305], Loss: 4.9865\n",
      "Epoch [1/2], Step [23850/64305], Loss: 4.9731\n",
      "Epoch [1/2], Step [23860/64305], Loss: 4.9796\n",
      "Epoch [1/2], Step [23870/64305], Loss: 4.8499\n",
      "Epoch [1/2], Step [23880/64305], Loss: 5.1634\n",
      "Epoch [1/2], Step [23890/64305], Loss: 4.9012\n",
      "Epoch [1/2], Step [23900/64305], Loss: 5.0968\n",
      "Epoch [1/2], Step [23910/64305], Loss: 5.0034\n",
      "Epoch [1/2], Step [23920/64305], Loss: 5.1199\n",
      "Epoch [1/2], Step [23930/64305], Loss: 5.2651\n",
      "Epoch [1/2], Step [23940/64305], Loss: 5.1936\n",
      "Epoch [1/2], Step [23950/64305], Loss: 5.1636\n",
      "Epoch [1/2], Step [23960/64305], Loss: 4.9951\n",
      "Epoch [1/2], Step [23970/64305], Loss: 5.0550\n",
      "Epoch [1/2], Step [23980/64305], Loss: 4.7948\n",
      "Epoch [1/2], Step [23990/64305], Loss: 4.9802\n",
      "Epoch [1/2], Step [24000/64305], Loss: 4.9020\n",
      "Epoch [1/2], Step [24010/64305], Loss: 5.1899\n",
      "Epoch [1/2], Step [24020/64305], Loss: 5.0490\n",
      "Epoch [1/2], Step [24030/64305], Loss: 4.9737\n",
      "Epoch [1/2], Step [24040/64305], Loss: 5.1767\n",
      "Epoch [1/2], Step [24050/64305], Loss: 5.0364\n",
      "Epoch [1/2], Step [24060/64305], Loss: 4.9700\n",
      "Epoch [1/2], Step [24070/64305], Loss: 5.1395\n",
      "Epoch [1/2], Step [24080/64305], Loss: 4.9150\n",
      "Epoch [1/2], Step [24090/64305], Loss: 5.1033\n",
      "Epoch [1/2], Step [24100/64305], Loss: 4.9473\n",
      "Epoch [1/2], Step [24110/64305], Loss: 5.0133\n",
      "Epoch [1/2], Step [24120/64305], Loss: 5.0514\n",
      "Epoch [1/2], Step [24130/64305], Loss: 5.1072\n",
      "Epoch [1/2], Step [24140/64305], Loss: 5.0374\n",
      "Epoch [1/2], Step [24150/64305], Loss: 4.9902\n",
      "Epoch [1/2], Step [24160/64305], Loss: 5.0062\n",
      "Epoch [1/2], Step [24170/64305], Loss: 4.9034\n",
      "Epoch [1/2], Step [24180/64305], Loss: 5.0403\n",
      "Epoch [1/2], Step [24190/64305], Loss: 5.0933\n",
      "Epoch [1/2], Step [24200/64305], Loss: 5.0922\n",
      "Epoch [1/2], Step [24210/64305], Loss: 4.8558\n",
      "Epoch [1/2], Step [24220/64305], Loss: 5.0183\n",
      "Epoch [1/2], Step [24230/64305], Loss: 5.0354\n",
      "Epoch [1/2], Step [24240/64305], Loss: 5.1176\n",
      "Epoch [1/2], Step [24250/64305], Loss: 4.9067\n",
      "Epoch [1/2], Step [24260/64305], Loss: 5.1309\n",
      "Epoch [1/2], Step [24270/64305], Loss: 4.9731\n",
      "Epoch [1/2], Step [24280/64305], Loss: 5.2526\n",
      "Epoch [1/2], Step [24290/64305], Loss: 5.1733\n",
      "Epoch [1/2], Step [24300/64305], Loss: 4.8297\n",
      "Epoch [1/2], Step [24310/64305], Loss: 4.9255\n",
      "Epoch [1/2], Step [24320/64305], Loss: 5.1852\n",
      "Epoch [1/2], Step [24330/64305], Loss: 5.1111\n",
      "Epoch [1/2], Step [24340/64305], Loss: 4.8828\n",
      "Epoch [1/2], Step [24350/64305], Loss: 5.2705\n",
      "Epoch [1/2], Step [24360/64305], Loss: 5.3108\n",
      "Epoch [1/2], Step [24370/64305], Loss: 4.9918\n",
      "Epoch [1/2], Step [24380/64305], Loss: 5.0420\n",
      "Epoch [1/2], Step [24390/64305], Loss: 5.0538\n",
      "Epoch [1/2], Step [24400/64305], Loss: 4.8889\n",
      "Epoch [1/2], Step [24410/64305], Loss: 5.1987\n",
      "Epoch [1/2], Step [24420/64305], Loss: 5.0135\n",
      "Epoch [1/2], Step [24430/64305], Loss: 5.0741\n",
      "Epoch [1/2], Step [24440/64305], Loss: 5.1216\n",
      "Epoch [1/2], Step [24450/64305], Loss: 5.0025\n",
      "Epoch [1/2], Step [24460/64305], Loss: 5.0531\n",
      "Epoch [1/2], Step [24470/64305], Loss: 5.2981\n",
      "Epoch [1/2], Step [24480/64305], Loss: 5.1265\n",
      "Epoch [1/2], Step [24490/64305], Loss: 5.2365\n",
      "Epoch [1/2], Step [24500/64305], Loss: 5.0693\n",
      "Epoch [1/2], Step [24510/64305], Loss: 4.9837\n",
      "Epoch [1/2], Step [24520/64305], Loss: 5.2514\n",
      "Epoch [1/2], Step [24530/64305], Loss: 4.9599\n",
      "Epoch [1/2], Step [24540/64305], Loss: 5.0371\n",
      "Epoch [1/2], Step [24550/64305], Loss: 5.1408\n",
      "Epoch [1/2], Step [24560/64305], Loss: 5.1134\n",
      "Epoch [1/2], Step [24570/64305], Loss: 5.0199\n",
      "Epoch [1/2], Step [24580/64305], Loss: 4.9067\n",
      "Epoch [1/2], Step [24590/64305], Loss: 5.3736\n",
      "Epoch [1/2], Step [24600/64305], Loss: 5.0237\n",
      "Epoch [1/2], Step [24610/64305], Loss: 4.9407\n",
      "Epoch [1/2], Step [24620/64305], Loss: 5.1223\n",
      "Epoch [1/2], Step [24630/64305], Loss: 5.0552\n",
      "Epoch [1/2], Step [24640/64305], Loss: 5.1272\n",
      "Epoch [1/2], Step [24650/64305], Loss: 5.0260\n",
      "Epoch [1/2], Step [24660/64305], Loss: 4.9830\n",
      "Epoch [1/2], Step [24670/64305], Loss: 5.2368\n",
      "Epoch [1/2], Step [24680/64305], Loss: 4.8885\n",
      "Epoch [1/2], Step [24690/64305], Loss: 5.1619\n",
      "Epoch [1/2], Step [24700/64305], Loss: 5.2210\n",
      "Epoch [1/2], Step [24710/64305], Loss: 5.0927\n",
      "Epoch [1/2], Step [24720/64305], Loss: 4.8373\n",
      "Epoch [1/2], Step [24730/64305], Loss: 4.9500\n",
      "Epoch [1/2], Step [24740/64305], Loss: 4.9065\n",
      "Epoch [1/2], Step [24750/64305], Loss: 4.9815\n",
      "Epoch [1/2], Step [24760/64305], Loss: 5.1557\n",
      "Epoch [1/2], Step [24770/64305], Loss: 5.0448\n",
      "Epoch [1/2], Step [24780/64305], Loss: 4.9457\n",
      "Epoch [1/2], Step [24790/64305], Loss: 5.1020\n",
      "Epoch [1/2], Step [24800/64305], Loss: 5.1570\n",
      "Epoch [1/2], Step [24810/64305], Loss: 5.2257\n",
      "Epoch [1/2], Step [24820/64305], Loss: 5.1173\n",
      "Epoch [1/2], Step [24830/64305], Loss: 5.0420\n",
      "Epoch [1/2], Step [24840/64305], Loss: 4.8135\n",
      "Epoch [1/2], Step [24850/64305], Loss: 5.1024\n",
      "Epoch [1/2], Step [24860/64305], Loss: 5.0331\n",
      "Epoch [1/2], Step [24870/64305], Loss: 5.2418\n",
      "Epoch [1/2], Step [24880/64305], Loss: 5.0762\n",
      "Epoch [1/2], Step [24890/64305], Loss: 5.0121\n",
      "Epoch [1/2], Step [24900/64305], Loss: 4.9145\n",
      "Epoch [1/2], Step [24910/64305], Loss: 5.2845\n",
      "Epoch [1/2], Step [24920/64305], Loss: 4.8484\n",
      "Epoch [1/2], Step [24930/64305], Loss: 5.0320\n",
      "Epoch [1/2], Step [24940/64305], Loss: 5.2340\n",
      "Epoch [1/2], Step [24950/64305], Loss: 4.9353\n",
      "Epoch [1/2], Step [24960/64305], Loss: 4.9384\n",
      "Epoch [1/2], Step [24970/64305], Loss: 5.0282\n",
      "Epoch [1/2], Step [24980/64305], Loss: 5.2111\n",
      "Epoch [1/2], Step [24990/64305], Loss: 4.9597\n",
      "Epoch [1/2], Step [25000/64305], Loss: 5.0378\n",
      "Epoch [1/2], Step [25010/64305], Loss: 5.1696\n",
      "Epoch [1/2], Step [25020/64305], Loss: 5.0769\n",
      "Epoch [1/2], Step [25030/64305], Loss: 4.9602\n",
      "Epoch [1/2], Step [25040/64305], Loss: 5.1109\n",
      "Epoch [1/2], Step [25050/64305], Loss: 4.9165\n",
      "Epoch [1/2], Step [25060/64305], Loss: 5.0749\n",
      "Epoch [1/2], Step [25070/64305], Loss: 4.8565\n",
      "Epoch [1/2], Step [25080/64305], Loss: 5.0076\n",
      "Epoch [1/2], Step [25090/64305], Loss: 5.1567\n",
      "Epoch [1/2], Step [25100/64305], Loss: 5.0917\n",
      "Epoch [1/2], Step [25110/64305], Loss: 5.2333\n",
      "Epoch [1/2], Step [25120/64305], Loss: 4.9153\n",
      "Epoch [1/2], Step [25130/64305], Loss: 4.9499\n",
      "Epoch [1/2], Step [25140/64305], Loss: 4.9141\n",
      "Epoch [1/2], Step [25150/64305], Loss: 5.0574\n",
      "Epoch [1/2], Step [25160/64305], Loss: 5.1882\n",
      "Epoch [1/2], Step [25170/64305], Loss: 4.9891\n",
      "Epoch [1/2], Step [25180/64305], Loss: 5.1079\n",
      "Epoch [1/2], Step [25190/64305], Loss: 4.7099\n",
      "Epoch [1/2], Step [25200/64305], Loss: 4.8577\n",
      "Epoch [1/2], Step [25210/64305], Loss: 5.1658\n",
      "Epoch [1/2], Step [25220/64305], Loss: 5.0481\n",
      "Epoch [1/2], Step [25230/64305], Loss: 5.0638\n",
      "Epoch [1/2], Step [25240/64305], Loss: 5.0757\n",
      "Epoch [1/2], Step [25250/64305], Loss: 5.1572\n",
      "Epoch [1/2], Step [25260/64305], Loss: 5.1748\n",
      "Epoch [1/2], Step [25270/64305], Loss: 4.9517\n",
      "Epoch [1/2], Step [25280/64305], Loss: 5.0830\n",
      "Epoch [1/2], Step [25290/64305], Loss: 5.1260\n",
      "Epoch [1/2], Step [25300/64305], Loss: 5.1067\n",
      "Epoch [1/2], Step [25310/64305], Loss: 5.1389\n",
      "Epoch [1/2], Step [25320/64305], Loss: 5.2036\n",
      "Epoch [1/2], Step [25330/64305], Loss: 5.2642\n",
      "Epoch [1/2], Step [25340/64305], Loss: 5.1111\n",
      "Epoch [1/2], Step [25350/64305], Loss: 5.0763\n",
      "Epoch [1/2], Step [25360/64305], Loss: 4.9681\n",
      "Epoch [1/2], Step [25370/64305], Loss: 4.9374\n",
      "Epoch [1/2], Step [25380/64305], Loss: 5.1152\n",
      "Epoch [1/2], Step [25390/64305], Loss: 4.9608\n",
      "Epoch [1/2], Step [25400/64305], Loss: 5.0679\n",
      "Epoch [1/2], Step [25410/64305], Loss: 4.9586\n",
      "Epoch [1/2], Step [25420/64305], Loss: 4.8446\n",
      "Epoch [1/2], Step [25430/64305], Loss: 4.9227\n",
      "Epoch [1/2], Step [25440/64305], Loss: 5.1422\n",
      "Epoch [1/2], Step [25450/64305], Loss: 4.9813\n",
      "Epoch [1/2], Step [25460/64305], Loss: 5.0788\n",
      "Epoch [1/2], Step [25470/64305], Loss: 5.0895\n",
      "Epoch [1/2], Step [25480/64305], Loss: 4.9967\n",
      "Epoch [1/2], Step [25490/64305], Loss: 5.0318\n",
      "Epoch [1/2], Step [25500/64305], Loss: 5.0578\n",
      "Epoch [1/2], Step [25510/64305], Loss: 4.9848\n",
      "Epoch [1/2], Step [25520/64305], Loss: 4.9898\n",
      "Epoch [1/2], Step [25530/64305], Loss: 5.1523\n",
      "Epoch [1/2], Step [25540/64305], Loss: 5.1388\n",
      "Epoch [1/2], Step [25550/64305], Loss: 4.9909\n",
      "Epoch [1/2], Step [25560/64305], Loss: 5.1842\n",
      "Epoch [1/2], Step [25570/64305], Loss: 4.9896\n",
      "Epoch [1/2], Step [25580/64305], Loss: 5.2319\n",
      "Epoch [1/2], Step [25590/64305], Loss: 5.0038\n",
      "Epoch [1/2], Step [25600/64305], Loss: 4.7785\n",
      "Epoch [1/2], Step [25610/64305], Loss: 5.2807\n",
      "Epoch [1/2], Step [25620/64305], Loss: 4.9288\n",
      "Epoch [1/2], Step [25630/64305], Loss: 4.8690\n",
      "Epoch [1/2], Step [25640/64305], Loss: 4.8218\n",
      "Epoch [1/2], Step [25650/64305], Loss: 4.8278\n",
      "Epoch [1/2], Step [25660/64305], Loss: 4.9207\n",
      "Epoch [1/2], Step [25670/64305], Loss: 5.0818\n",
      "Epoch [1/2], Step [25680/64305], Loss: 5.2219\n",
      "Epoch [1/2], Step [25690/64305], Loss: 4.9928\n",
      "Epoch [1/2], Step [25700/64305], Loss: 5.2065\n",
      "Epoch [1/2], Step [25710/64305], Loss: 4.8284\n",
      "Epoch [1/2], Step [25720/64305], Loss: 4.9696\n",
      "Epoch [1/2], Step [25730/64305], Loss: 5.1637\n",
      "Epoch [1/2], Step [25740/64305], Loss: 4.9186\n",
      "Epoch [1/2], Step [25750/64305], Loss: 4.9476\n",
      "Epoch [1/2], Step [25760/64305], Loss: 4.8127\n",
      "Epoch [1/2], Step [25770/64305], Loss: 5.0371\n",
      "Epoch [1/2], Step [25780/64305], Loss: 5.0991\n",
      "Epoch [1/2], Step [25790/64305], Loss: 5.1583\n",
      "Epoch [1/2], Step [25800/64305], Loss: 5.1408\n",
      "Epoch [1/2], Step [25810/64305], Loss: 5.0831\n",
      "Epoch [1/2], Step [25820/64305], Loss: 4.9638\n",
      "Epoch [1/2], Step [25830/64305], Loss: 5.0425\n",
      "Epoch [1/2], Step [25840/64305], Loss: 4.8530\n",
      "Epoch [1/2], Step [25850/64305], Loss: 4.9550\n",
      "Epoch [1/2], Step [25860/64305], Loss: 4.9725\n",
      "Epoch [1/2], Step [25870/64305], Loss: 5.0413\n",
      "Epoch [1/2], Step [25880/64305], Loss: 4.8692\n",
      "Epoch [1/2], Step [25890/64305], Loss: 5.1984\n",
      "Epoch [1/2], Step [25900/64305], Loss: 4.9174\n",
      "Epoch [1/2], Step [25910/64305], Loss: 4.8377\n",
      "Epoch [1/2], Step [25920/64305], Loss: 4.8522\n",
      "Epoch [1/2], Step [25930/64305], Loss: 4.9779\n",
      "Epoch [1/2], Step [25940/64305], Loss: 5.1727\n",
      "Epoch [1/2], Step [25950/64305], Loss: 5.0400\n",
      "Epoch [1/2], Step [25960/64305], Loss: 5.2446\n",
      "Epoch [1/2], Step [25970/64305], Loss: 5.0723\n",
      "Epoch [1/2], Step [25980/64305], Loss: 5.2153\n",
      "Epoch [1/2], Step [25990/64305], Loss: 5.2034\n",
      "Epoch [1/2], Step [26000/64305], Loss: 5.0802\n",
      "Epoch [1/2], Step [26010/64305], Loss: 4.9188\n",
      "Epoch [1/2], Step [26020/64305], Loss: 4.9770\n",
      "Epoch [1/2], Step [26030/64305], Loss: 5.1433\n",
      "Epoch [1/2], Step [26040/64305], Loss: 5.1258\n",
      "Epoch [1/2], Step [26050/64305], Loss: 5.0811\n",
      "Epoch [1/2], Step [26060/64305], Loss: 5.0190\n",
      "Epoch [1/2], Step [26070/64305], Loss: 4.8065\n",
      "Epoch [1/2], Step [26080/64305], Loss: 5.2569\n",
      "Epoch [1/2], Step [26090/64305], Loss: 4.8465\n",
      "Epoch [1/2], Step [26100/64305], Loss: 5.0741\n",
      "Epoch [1/2], Step [26110/64305], Loss: 5.0594\n",
      "Epoch [1/2], Step [26120/64305], Loss: 4.8013\n",
      "Epoch [1/2], Step [26130/64305], Loss: 5.0339\n",
      "Epoch [1/2], Step [26140/64305], Loss: 4.9922\n",
      "Epoch [1/2], Step [26150/64305], Loss: 5.1265\n",
      "Epoch [1/2], Step [26160/64305], Loss: 5.0238\n",
      "Epoch [1/2], Step [26170/64305], Loss: 4.9377\n",
      "Epoch [1/2], Step [26180/64305], Loss: 5.0342\n",
      "Epoch [1/2], Step [26190/64305], Loss: 4.8995\n",
      "Epoch [1/2], Step [26200/64305], Loss: 5.2034\n",
      "Epoch [1/2], Step [26210/64305], Loss: 5.0614\n",
      "Epoch [1/2], Step [26220/64305], Loss: 5.0012\n",
      "Epoch [1/2], Step [26230/64305], Loss: 4.8982\n",
      "Epoch [1/2], Step [26240/64305], Loss: 5.0767\n",
      "Epoch [1/2], Step [26250/64305], Loss: 4.8910\n",
      "Epoch [1/2], Step [26260/64305], Loss: 5.0282\n",
      "Epoch [1/2], Step [26270/64305], Loss: 5.0955\n",
      "Epoch [1/2], Step [26280/64305], Loss: 5.1494\n",
      "Epoch [1/2], Step [26290/64305], Loss: 5.1246\n",
      "Epoch [1/2], Step [26300/64305], Loss: 4.9456\n",
      "Epoch [1/2], Step [26310/64305], Loss: 5.0661\n",
      "Epoch [1/2], Step [26320/64305], Loss: 4.9434\n",
      "Epoch [1/2], Step [26330/64305], Loss: 5.1757\n",
      "Epoch [1/2], Step [26340/64305], Loss: 5.1493\n",
      "Epoch [1/2], Step [26350/64305], Loss: 5.0373\n",
      "Epoch [1/2], Step [26360/64305], Loss: 5.0696\n",
      "Epoch [1/2], Step [26370/64305], Loss: 4.9989\n",
      "Epoch [1/2], Step [26380/64305], Loss: 5.0399\n",
      "Epoch [1/2], Step [26390/64305], Loss: 5.0189\n",
      "Epoch [1/2], Step [26400/64305], Loss: 4.9430\n",
      "Epoch [1/2], Step [26410/64305], Loss: 5.0714\n",
      "Epoch [1/2], Step [26420/64305], Loss: 5.2492\n",
      "Epoch [1/2], Step [26430/64305], Loss: 5.1063\n",
      "Epoch [1/2], Step [26440/64305], Loss: 5.1756\n",
      "Epoch [1/2], Step [26450/64305], Loss: 4.9973\n",
      "Epoch [1/2], Step [26460/64305], Loss: 5.0945\n",
      "Epoch [1/2], Step [26470/64305], Loss: 4.8895\n",
      "Epoch [1/2], Step [26480/64305], Loss: 4.8724\n",
      "Epoch [1/2], Step [26490/64305], Loss: 5.1925\n",
      "Epoch [1/2], Step [26500/64305], Loss: 5.1004\n",
      "Epoch [1/2], Step [26510/64305], Loss: 5.1677\n",
      "Epoch [1/2], Step [26520/64305], Loss: 5.1472\n",
      "Epoch [1/2], Step [26530/64305], Loss: 5.1063\n",
      "Epoch [1/2], Step [26540/64305], Loss: 4.9315\n",
      "Epoch [1/2], Step [26550/64305], Loss: 5.1745\n",
      "Epoch [1/2], Step [26560/64305], Loss: 4.9795\n",
      "Epoch [1/2], Step [26570/64305], Loss: 4.9280\n",
      "Epoch [1/2], Step [26580/64305], Loss: 5.1360\n",
      "Epoch [1/2], Step [26590/64305], Loss: 4.9068\n",
      "Epoch [1/2], Step [26600/64305], Loss: 5.0353\n",
      "Epoch [1/2], Step [26610/64305], Loss: 4.8428\n",
      "Epoch [1/2], Step [26620/64305], Loss: 5.1335\n",
      "Epoch [1/2], Step [26630/64305], Loss: 4.9907\n",
      "Epoch [1/2], Step [26640/64305], Loss: 4.9443\n",
      "Epoch [1/2], Step [26650/64305], Loss: 5.0106\n",
      "Epoch [1/2], Step [26660/64305], Loss: 5.1222\n",
      "Epoch [1/2], Step [26670/64305], Loss: 5.1922\n",
      "Epoch [1/2], Step [26680/64305], Loss: 4.9531\n",
      "Epoch [1/2], Step [26690/64305], Loss: 5.1227\n",
      "Epoch [1/2], Step [26700/64305], Loss: 5.2210\n",
      "Epoch [1/2], Step [26710/64305], Loss: 4.8580\n",
      "Epoch [1/2], Step [26720/64305], Loss: 5.1139\n",
      "Epoch [1/2], Step [26730/64305], Loss: 5.0158\n",
      "Epoch [1/2], Step [26740/64305], Loss: 4.9648\n",
      "Epoch [1/2], Step [26750/64305], Loss: 4.9483\n",
      "Epoch [1/2], Step [26760/64305], Loss: 5.0345\n",
      "Epoch [1/2], Step [26770/64305], Loss: 5.0169\n",
      "Epoch [1/2], Step [26780/64305], Loss: 5.1901\n",
      "Epoch [1/2], Step [26790/64305], Loss: 4.9944\n",
      "Epoch [1/2], Step [26800/64305], Loss: 5.0924\n",
      "Epoch [1/2], Step [26810/64305], Loss: 5.1364\n",
      "Epoch [1/2], Step [26820/64305], Loss: 4.9080\n",
      "Epoch [1/2], Step [26830/64305], Loss: 4.7803\n",
      "Epoch [1/2], Step [26840/64305], Loss: 5.0664\n",
      "Epoch [1/2], Step [26850/64305], Loss: 5.0138\n",
      "Epoch [1/2], Step [26860/64305], Loss: 5.0050\n",
      "Epoch [1/2], Step [26870/64305], Loss: 5.2116\n",
      "Epoch [1/2], Step [26880/64305], Loss: 4.7347\n",
      "Epoch [1/2], Step [26890/64305], Loss: 4.9139\n",
      "Epoch [1/2], Step [26900/64305], Loss: 4.9139\n",
      "Epoch [1/2], Step [26910/64305], Loss: 4.9188\n",
      "Epoch [1/2], Step [26920/64305], Loss: 4.8329\n",
      "Epoch [1/2], Step [26930/64305], Loss: 4.8725\n",
      "Epoch [1/2], Step [26940/64305], Loss: 5.0590\n",
      "Epoch [1/2], Step [26950/64305], Loss: 5.2174\n",
      "Epoch [1/2], Step [26960/64305], Loss: 5.0715\n",
      "Epoch [1/2], Step [26970/64305], Loss: 5.3869\n",
      "Epoch [1/2], Step [26980/64305], Loss: 5.1775\n",
      "Epoch [1/2], Step [26990/64305], Loss: 5.0192\n",
      "Epoch [1/2], Step [27000/64305], Loss: 4.9918\n",
      "Epoch [1/2], Step [27010/64305], Loss: 5.1903\n",
      "Epoch [1/2], Step [27020/64305], Loss: 5.0680\n",
      "Epoch [1/2], Step [27030/64305], Loss: 5.0362\n",
      "Epoch [1/2], Step [27040/64305], Loss: 5.2468\n",
      "Epoch [1/2], Step [27050/64305], Loss: 5.0785\n",
      "Epoch [1/2], Step [27060/64305], Loss: 5.0472\n",
      "Epoch [1/2], Step [27070/64305], Loss: 5.2307\n",
      "Epoch [1/2], Step [27080/64305], Loss: 5.2983\n",
      "Epoch [1/2], Step [27090/64305], Loss: 5.0499\n",
      "Epoch [1/2], Step [27100/64305], Loss: 5.0275\n",
      "Epoch [1/2], Step [27110/64305], Loss: 5.1820\n",
      "Epoch [1/2], Step [27120/64305], Loss: 4.9375\n",
      "Epoch [1/2], Step [27130/64305], Loss: 4.8990\n",
      "Epoch [1/2], Step [27140/64305], Loss: 4.9179\n",
      "Epoch [1/2], Step [27150/64305], Loss: 5.1380\n",
      "Epoch [1/2], Step [27160/64305], Loss: 5.1259\n",
      "Epoch [1/2], Step [27170/64305], Loss: 4.9203\n",
      "Epoch [1/2], Step [27180/64305], Loss: 4.9403\n",
      "Epoch [1/2], Step [27190/64305], Loss: 5.0660\n",
      "Epoch [1/2], Step [27200/64305], Loss: 5.1369\n",
      "Epoch [1/2], Step [27210/64305], Loss: 4.9453\n",
      "Epoch [1/2], Step [27220/64305], Loss: 4.9615\n",
      "Epoch [1/2], Step [27230/64305], Loss: 4.9351\n",
      "Epoch [1/2], Step [27240/64305], Loss: 5.1722\n",
      "Epoch [1/2], Step [27250/64305], Loss: 4.8806\n",
      "Epoch [1/2], Step [27260/64305], Loss: 5.0539\n",
      "Epoch [1/2], Step [27270/64305], Loss: 4.9534\n",
      "Epoch [1/2], Step [27280/64305], Loss: 4.9971\n",
      "Epoch [1/2], Step [27290/64305], Loss: 4.9609\n",
      "Epoch [1/2], Step [27300/64305], Loss: 5.0757\n",
      "Epoch [1/2], Step [27310/64305], Loss: 4.8015\n",
      "Epoch [1/2], Step [27320/64305], Loss: 4.8659\n",
      "Epoch [1/2], Step [27330/64305], Loss: 4.8763\n",
      "Epoch [1/2], Step [27340/64305], Loss: 5.0747\n",
      "Epoch [1/2], Step [27350/64305], Loss: 5.0058\n",
      "Epoch [1/2], Step [27360/64305], Loss: 4.7035\n",
      "Epoch [1/2], Step [27370/64305], Loss: 4.9208\n",
      "Epoch [1/2], Step [27380/64305], Loss: 4.9589\n",
      "Epoch [1/2], Step [27390/64305], Loss: 5.1409\n",
      "Epoch [1/2], Step [27400/64305], Loss: 5.0612\n",
      "Epoch [1/2], Step [27410/64305], Loss: 5.1751\n",
      "Epoch [1/2], Step [27420/64305], Loss: 4.9454\n",
      "Epoch [1/2], Step [27430/64305], Loss: 4.9077\n",
      "Epoch [1/2], Step [27440/64305], Loss: 5.2412\n",
      "Epoch [1/2], Step [27450/64305], Loss: 4.9960\n",
      "Epoch [1/2], Step [27460/64305], Loss: 4.8885\n",
      "Epoch [1/2], Step [27470/64305], Loss: 4.9439\n",
      "Epoch [1/2], Step [27480/64305], Loss: 5.0058\n",
      "Epoch [1/2], Step [27490/64305], Loss: 5.1121\n",
      "Epoch [1/2], Step [27500/64305], Loss: 4.8479\n",
      "Epoch [1/2], Step [27510/64305], Loss: 4.9888\n",
      "Epoch [1/2], Step [27520/64305], Loss: 4.7489\n",
      "Epoch [1/2], Step [27530/64305], Loss: 5.1784\n",
      "Epoch [1/2], Step [27540/64305], Loss: 5.1525\n",
      "Epoch [1/2], Step [27550/64305], Loss: 5.0226\n",
      "Epoch [1/2], Step [27560/64305], Loss: 4.9402\n",
      "Epoch [1/2], Step [27570/64305], Loss: 5.0089\n",
      "Epoch [1/2], Step [27580/64305], Loss: 4.9876\n",
      "Epoch [1/2], Step [27590/64305], Loss: 4.9190\n",
      "Epoch [1/2], Step [27600/64305], Loss: 4.9887\n",
      "Epoch [1/2], Step [27610/64305], Loss: 5.0695\n",
      "Epoch [1/2], Step [27620/64305], Loss: 4.9506\n",
      "Epoch [1/2], Step [27630/64305], Loss: 4.8817\n",
      "Epoch [1/2], Step [27640/64305], Loss: 5.0542\n",
      "Epoch [1/2], Step [27650/64305], Loss: 4.9788\n",
      "Epoch [1/2], Step [27660/64305], Loss: 4.9187\n",
      "Epoch [1/2], Step [27670/64305], Loss: 5.1836\n",
      "Epoch [1/2], Step [27680/64305], Loss: 4.9826\n",
      "Epoch [1/2], Step [27690/64305], Loss: 5.0376\n",
      "Epoch [1/2], Step [27700/64305], Loss: 4.8584\n",
      "Epoch [1/2], Step [27710/64305], Loss: 5.1489\n",
      "Epoch [1/2], Step [27720/64305], Loss: 4.9394\n",
      "Epoch [1/2], Step [27730/64305], Loss: 5.1542\n",
      "Epoch [1/2], Step [27740/64305], Loss: 4.9986\n",
      "Epoch [1/2], Step [27750/64305], Loss: 5.1052\n",
      "Epoch [1/2], Step [27760/64305], Loss: 5.2651\n",
      "Epoch [1/2], Step [27770/64305], Loss: 5.1135\n",
      "Epoch [1/2], Step [27780/64305], Loss: 4.9247\n",
      "Epoch [1/2], Step [27790/64305], Loss: 5.2185\n",
      "Epoch [1/2], Step [27800/64305], Loss: 5.1487\n",
      "Epoch [1/2], Step [27810/64305], Loss: 5.0059\n",
      "Epoch [1/2], Step [27820/64305], Loss: 4.9535\n",
      "Epoch [1/2], Step [27830/64305], Loss: 4.8351\n",
      "Epoch [1/2], Step [27840/64305], Loss: 4.9519\n",
      "Epoch [1/2], Step [27850/64305], Loss: 4.8924\n",
      "Epoch [1/2], Step [27860/64305], Loss: 5.1681\n",
      "Epoch [1/2], Step [27870/64305], Loss: 4.9726\n",
      "Epoch [1/2], Step [27880/64305], Loss: 4.9179\n",
      "Epoch [1/2], Step [27890/64305], Loss: 5.0470\n",
      "Epoch [1/2], Step [27900/64305], Loss: 5.1707\n",
      "Epoch [1/2], Step [27910/64305], Loss: 5.0218\n",
      "Epoch [1/2], Step [27920/64305], Loss: 5.0436\n",
      "Epoch [1/2], Step [27930/64305], Loss: 5.0688\n",
      "Epoch [1/2], Step [27940/64305], Loss: 4.9535\n",
      "Epoch [1/2], Step [27950/64305], Loss: 5.1526\n",
      "Epoch [1/2], Step [27960/64305], Loss: 5.0434\n",
      "Epoch [1/2], Step [27970/64305], Loss: 5.3009\n",
      "Epoch [1/2], Step [27980/64305], Loss: 4.8537\n",
      "Epoch [1/2], Step [27990/64305], Loss: 4.9933\n",
      "Epoch [1/2], Step [28000/64305], Loss: 5.2015\n",
      "Epoch [1/2], Step [28010/64305], Loss: 5.1066\n",
      "Epoch [1/2], Step [28020/64305], Loss: 4.9219\n",
      "Epoch [1/2], Step [28030/64305], Loss: 4.9691\n",
      "Epoch [1/2], Step [28040/64305], Loss: 5.0813\n",
      "Epoch [1/2], Step [28050/64305], Loss: 4.8531\n",
      "Epoch [1/2], Step [28060/64305], Loss: 5.1191\n",
      "Epoch [1/2], Step [28070/64305], Loss: 4.8617\n",
      "Epoch [1/2], Step [28080/64305], Loss: 5.0619\n",
      "Epoch [1/2], Step [28090/64305], Loss: 5.1698\n",
      "Epoch [1/2], Step [28100/64305], Loss: 4.9259\n",
      "Epoch [1/2], Step [28110/64305], Loss: 5.0894\n",
      "Epoch [1/2], Step [28120/64305], Loss: 5.0157\n",
      "Epoch [1/2], Step [28130/64305], Loss: 5.1361\n",
      "Epoch [1/2], Step [28140/64305], Loss: 4.9587\n",
      "Epoch [1/2], Step [28150/64305], Loss: 4.8786\n",
      "Epoch [1/2], Step [28160/64305], Loss: 5.0760\n",
      "Epoch [1/2], Step [28170/64305], Loss: 5.0917\n",
      "Epoch [1/2], Step [28180/64305], Loss: 5.1621\n",
      "Epoch [1/2], Step [28190/64305], Loss: 5.0305\n",
      "Epoch [1/2], Step [28200/64305], Loss: 5.0105\n",
      "Epoch [1/2], Step [28210/64305], Loss: 5.0247\n",
      "Epoch [1/2], Step [28220/64305], Loss: 4.8490\n",
      "Epoch [1/2], Step [28230/64305], Loss: 4.8193\n",
      "Epoch [1/2], Step [28240/64305], Loss: 4.9915\n",
      "Epoch [1/2], Step [28250/64305], Loss: 5.0103\n",
      "Epoch [1/2], Step [28260/64305], Loss: 5.0468\n",
      "Epoch [1/2], Step [28270/64305], Loss: 5.1293\n",
      "Epoch [1/2], Step [28280/64305], Loss: 5.1263\n",
      "Epoch [1/2], Step [28290/64305], Loss: 4.9037\n",
      "Epoch [1/2], Step [28300/64305], Loss: 5.3061\n",
      "Epoch [1/2], Step [28310/64305], Loss: 5.0786\n",
      "Epoch [1/2], Step [28320/64305], Loss: 5.2725\n",
      "Epoch [1/2], Step [28330/64305], Loss: 4.7979\n",
      "Epoch [1/2], Step [28340/64305], Loss: 4.9940\n",
      "Epoch [1/2], Step [28350/64305], Loss: 5.0100\n",
      "Epoch [1/2], Step [28360/64305], Loss: 4.9933\n",
      "Epoch [1/2], Step [28370/64305], Loss: 4.9986\n",
      "Epoch [1/2], Step [28380/64305], Loss: 4.8796\n",
      "Epoch [1/2], Step [28390/64305], Loss: 5.1164\n",
      "Epoch [1/2], Step [28400/64305], Loss: 4.7764\n",
      "Epoch [1/2], Step [28410/64305], Loss: 4.9880\n",
      "Epoch [1/2], Step [28420/64305], Loss: 5.1998\n",
      "Epoch [1/2], Step [28430/64305], Loss: 5.1053\n",
      "Epoch [1/2], Step [28440/64305], Loss: 5.0996\n",
      "Epoch [1/2], Step [28450/64305], Loss: 5.0672\n",
      "Epoch [1/2], Step [28460/64305], Loss: 4.9701\n",
      "Epoch [1/2], Step [28470/64305], Loss: 5.1501\n",
      "Epoch [1/2], Step [28480/64305], Loss: 5.0345\n",
      "Epoch [1/2], Step [28490/64305], Loss: 5.1967\n",
      "Epoch [1/2], Step [28500/64305], Loss: 5.1571\n",
      "Epoch [1/2], Step [28510/64305], Loss: 4.9151\n",
      "Epoch [1/2], Step [28520/64305], Loss: 4.9441\n",
      "Epoch [1/2], Step [28530/64305], Loss: 5.0069\n",
      "Epoch [1/2], Step [28540/64305], Loss: 4.9051\n",
      "Epoch [1/2], Step [28550/64305], Loss: 5.2518\n",
      "Epoch [1/2], Step [28560/64305], Loss: 5.1473\n",
      "Epoch [1/2], Step [28570/64305], Loss: 5.0153\n",
      "Epoch [1/2], Step [28580/64305], Loss: 4.9819\n",
      "Epoch [1/2], Step [28590/64305], Loss: 4.7459\n",
      "Epoch [1/2], Step [28600/64305], Loss: 5.2684\n",
      "Epoch [1/2], Step [28610/64305], Loss: 4.9862\n",
      "Epoch [1/2], Step [28620/64305], Loss: 4.9864\n",
      "Epoch [1/2], Step [28630/64305], Loss: 4.9133\n",
      "Epoch [1/2], Step [28640/64305], Loss: 5.1375\n",
      "Epoch [1/2], Step [28650/64305], Loss: 5.0292\n",
      "Epoch [1/2], Step [28660/64305], Loss: 4.8295\n",
      "Epoch [1/2], Step [28670/64305], Loss: 4.9836\n",
      "Epoch [1/2], Step [28680/64305], Loss: 4.8969\n",
      "Epoch [1/2], Step [28690/64305], Loss: 5.0222\n",
      "Epoch [1/2], Step [28700/64305], Loss: 4.9459\n",
      "Epoch [1/2], Step [28710/64305], Loss: 5.1379\n",
      "Epoch [1/2], Step [28720/64305], Loss: 4.9504\n",
      "Epoch [1/2], Step [28730/64305], Loss: 5.2037\n",
      "Epoch [1/2], Step [28740/64305], Loss: 4.7823\n",
      "Epoch [1/2], Step [28750/64305], Loss: 5.0709\n",
      "Epoch [1/2], Step [28760/64305], Loss: 4.7199\n",
      "Epoch [1/2], Step [28770/64305], Loss: 5.0042\n",
      "Epoch [1/2], Step [28780/64305], Loss: 5.0662\n",
      "Epoch [1/2], Step [28790/64305], Loss: 5.1660\n",
      "Epoch [1/2], Step [28800/64305], Loss: 5.0910\n",
      "Epoch [1/2], Step [28810/64305], Loss: 4.9485\n",
      "Epoch [1/2], Step [28820/64305], Loss: 5.1145\n",
      "Epoch [1/2], Step [28830/64305], Loss: 4.8570\n",
      "Epoch [1/2], Step [28840/64305], Loss: 5.3867\n",
      "Epoch [1/2], Step [28850/64305], Loss: 5.1510\n",
      "Epoch [1/2], Step [28860/64305], Loss: 5.0623\n",
      "Epoch [1/2], Step [28870/64305], Loss: 5.0196\n",
      "Epoch [1/2], Step [28880/64305], Loss: 5.1681\n",
      "Epoch [1/2], Step [28890/64305], Loss: 5.1675\n",
      "Epoch [1/2], Step [28900/64305], Loss: 5.0232\n",
      "Epoch [1/2], Step [28910/64305], Loss: 5.0474\n",
      "Epoch [1/2], Step [28920/64305], Loss: 5.0280\n",
      "Epoch [1/2], Step [28930/64305], Loss: 5.0255\n",
      "Epoch [1/2], Step [28940/64305], Loss: 5.0756\n",
      "Epoch [1/2], Step [28950/64305], Loss: 4.9510\n",
      "Epoch [1/2], Step [28960/64305], Loss: 4.9353\n",
      "Epoch [1/2], Step [28970/64305], Loss: 4.9900\n",
      "Epoch [1/2], Step [28980/64305], Loss: 5.1289\n",
      "Epoch [1/2], Step [28990/64305], Loss: 4.8900\n",
      "Epoch [1/2], Step [29000/64305], Loss: 5.0524\n",
      "Epoch [1/2], Step [29010/64305], Loss: 5.0567\n",
      "Epoch [1/2], Step [29020/64305], Loss: 4.9194\n",
      "Epoch [1/2], Step [29030/64305], Loss: 4.8482\n",
      "Epoch [1/2], Step [29040/64305], Loss: 5.3403\n",
      "Epoch [1/2], Step [29050/64305], Loss: 5.1224\n",
      "Epoch [1/2], Step [29060/64305], Loss: 4.9525\n",
      "Epoch [1/2], Step [29070/64305], Loss: 4.9992\n",
      "Epoch [1/2], Step [29080/64305], Loss: 4.8276\n",
      "Epoch [1/2], Step [29090/64305], Loss: 4.7956\n",
      "Epoch [1/2], Step [29100/64305], Loss: 5.1519\n",
      "Epoch [1/2], Step [29110/64305], Loss: 5.1755\n",
      "Epoch [1/2], Step [29120/64305], Loss: 5.0147\n",
      "Epoch [1/2], Step [29130/64305], Loss: 4.9046\n",
      "Epoch [1/2], Step [29140/64305], Loss: 5.1421\n",
      "Epoch [1/2], Step [29150/64305], Loss: 5.0074\n",
      "Epoch [1/2], Step [29160/64305], Loss: 4.8832\n",
      "Epoch [1/2], Step [29170/64305], Loss: 5.0447\n",
      "Epoch [1/2], Step [29180/64305], Loss: 5.1136\n",
      "Epoch [1/2], Step [29190/64305], Loss: 5.0528\n",
      "Epoch [1/2], Step [29200/64305], Loss: 5.1314\n",
      "Epoch [1/2], Step [29210/64305], Loss: 4.9642\n",
      "Epoch [1/2], Step [29220/64305], Loss: 4.7121\n",
      "Epoch [1/2], Step [29230/64305], Loss: 4.7847\n",
      "Epoch [1/2], Step [29240/64305], Loss: 5.0489\n",
      "Epoch [1/2], Step [29250/64305], Loss: 5.1628\n",
      "Epoch [1/2], Step [29260/64305], Loss: 4.8018\n",
      "Epoch [1/2], Step [29270/64305], Loss: 5.1129\n",
      "Epoch [1/2], Step [29280/64305], Loss: 5.0413\n",
      "Epoch [1/2], Step [29290/64305], Loss: 5.0920\n",
      "Epoch [1/2], Step [29300/64305], Loss: 5.1861\n",
      "Epoch [1/2], Step [29310/64305], Loss: 4.9163\n",
      "Epoch [1/2], Step [29320/64305], Loss: 4.9305\n",
      "Epoch [1/2], Step [29330/64305], Loss: 5.0654\n",
      "Epoch [1/2], Step [29340/64305], Loss: 4.9676\n",
      "Epoch [1/2], Step [29350/64305], Loss: 5.1090\n",
      "Epoch [1/2], Step [29360/64305], Loss: 5.0887\n",
      "Epoch [1/2], Step [29370/64305], Loss: 5.0085\n",
      "Epoch [1/2], Step [29380/64305], Loss: 4.7682\n",
      "Epoch [1/2], Step [29390/64305], Loss: 5.1369\n",
      "Epoch [1/2], Step [29400/64305], Loss: 5.1159\n",
      "Epoch [1/2], Step [29410/64305], Loss: 4.8505\n",
      "Epoch [1/2], Step [29420/64305], Loss: 4.9972\n",
      "Epoch [1/2], Step [29430/64305], Loss: 4.9946\n",
      "Epoch [1/2], Step [29440/64305], Loss: 4.8224\n",
      "Epoch [1/2], Step [29450/64305], Loss: 5.0428\n",
      "Epoch [1/2], Step [29460/64305], Loss: 5.1660\n",
      "Epoch [1/2], Step [29470/64305], Loss: 4.8686\n",
      "Epoch [1/2], Step [29480/64305], Loss: 5.0900\n",
      "Epoch [1/2], Step [29490/64305], Loss: 5.0867\n",
      "Epoch [1/2], Step [29500/64305], Loss: 5.0356\n",
      "Epoch [1/2], Step [29510/64305], Loss: 5.0716\n",
      "Epoch [1/2], Step [29520/64305], Loss: 5.1352\n",
      "Epoch [1/2], Step [29530/64305], Loss: 5.0442\n",
      "Epoch [1/2], Step [29540/64305], Loss: 5.0152\n",
      "Epoch [1/2], Step [29550/64305], Loss: 4.9983\n",
      "Epoch [1/2], Step [29560/64305], Loss: 4.9639\n",
      "Epoch [1/2], Step [29570/64305], Loss: 5.0081\n",
      "Epoch [1/2], Step [29580/64305], Loss: 4.9063\n",
      "Epoch [1/2], Step [29590/64305], Loss: 4.9102\n",
      "Epoch [1/2], Step [29600/64305], Loss: 4.9157\n",
      "Epoch [1/2], Step [29610/64305], Loss: 4.9055\n",
      "Epoch [1/2], Step [29620/64305], Loss: 5.0474\n",
      "Epoch [1/2], Step [29630/64305], Loss: 4.9628\n",
      "Epoch [1/2], Step [29640/64305], Loss: 4.9731\n",
      "Epoch [1/2], Step [29650/64305], Loss: 4.9691\n",
      "Epoch [1/2], Step [29660/64305], Loss: 4.9225\n",
      "Epoch [1/2], Step [29670/64305], Loss: 4.8537\n",
      "Epoch [1/2], Step [29680/64305], Loss: 5.0125\n",
      "Epoch [1/2], Step [29690/64305], Loss: 5.0223\n",
      "Epoch [1/2], Step [29700/64305], Loss: 5.2005\n",
      "Epoch [1/2], Step [29710/64305], Loss: 4.9635\n",
      "Epoch [1/2], Step [29720/64305], Loss: 4.8401\n",
      "Epoch [1/2], Step [29730/64305], Loss: 5.0001\n",
      "Epoch [1/2], Step [29740/64305], Loss: 4.9083\n",
      "Epoch [1/2], Step [29750/64305], Loss: 4.9757\n",
      "Epoch [1/2], Step [29760/64305], Loss: 5.1218\n",
      "Epoch [1/2], Step [29770/64305], Loss: 5.0197\n",
      "Epoch [1/2], Step [29780/64305], Loss: 5.0244\n",
      "Epoch [1/2], Step [29790/64305], Loss: 4.6637\n",
      "Epoch [1/2], Step [29800/64305], Loss: 4.8301\n",
      "Epoch [1/2], Step [29810/64305], Loss: 5.1396\n",
      "Epoch [1/2], Step [29820/64305], Loss: 5.0952\n",
      "Epoch [1/2], Step [29830/64305], Loss: 4.9895\n",
      "Epoch [1/2], Step [29840/64305], Loss: 5.1336\n",
      "Epoch [1/2], Step [29850/64305], Loss: 5.0985\n",
      "Epoch [1/2], Step [29860/64305], Loss: 4.8955\n",
      "Epoch [1/2], Step [29870/64305], Loss: 5.0577\n",
      "Epoch [1/2], Step [29880/64305], Loss: 4.7602\n",
      "Epoch [1/2], Step [29890/64305], Loss: 4.9800\n",
      "Epoch [1/2], Step [29900/64305], Loss: 5.1157\n",
      "Epoch [1/2], Step [29910/64305], Loss: 4.8614\n",
      "Epoch [1/2], Step [29920/64305], Loss: 4.7709\n",
      "Epoch [1/2], Step [29930/64305], Loss: 4.9295\n",
      "Epoch [1/2], Step [29940/64305], Loss: 5.1722\n",
      "Epoch [1/2], Step [29950/64305], Loss: 5.0304\n",
      "Epoch [1/2], Step [29960/64305], Loss: 4.8837\n",
      "Epoch [1/2], Step [29970/64305], Loss: 5.1031\n",
      "Epoch [1/2], Step [29980/64305], Loss: 4.9871\n",
      "Epoch [1/2], Step [29990/64305], Loss: 4.9894\n",
      "Epoch [1/2], Step [30000/64305], Loss: 5.0182\n",
      "Epoch [1/2], Step [30010/64305], Loss: 5.0525\n",
      "Epoch [1/2], Step [30020/64305], Loss: 4.9556\n",
      "Epoch [1/2], Step [30030/64305], Loss: 5.0989\n",
      "Epoch [1/2], Step [30040/64305], Loss: 5.0602\n",
      "Epoch [1/2], Step [30050/64305], Loss: 4.9556\n",
      "Epoch [1/2], Step [30060/64305], Loss: 5.0325\n",
      "Epoch [1/2], Step [30070/64305], Loss: 4.8131\n",
      "Epoch [1/2], Step [30080/64305], Loss: 5.1611\n",
      "Epoch [1/2], Step [30090/64305], Loss: 5.1897\n",
      "Epoch [1/2], Step [30100/64305], Loss: 4.9764\n",
      "Epoch [1/2], Step [30110/64305], Loss: 5.0941\n",
      "Epoch [1/2], Step [30120/64305], Loss: 5.0060\n",
      "Epoch [1/2], Step [30130/64305], Loss: 4.8429\n",
      "Epoch [1/2], Step [30140/64305], Loss: 4.8984\n",
      "Epoch [1/2], Step [30150/64305], Loss: 4.9148\n",
      "Epoch [1/2], Step [30160/64305], Loss: 4.8787\n",
      "Epoch [1/2], Step [30170/64305], Loss: 4.9968\n",
      "Epoch [1/2], Step [30180/64305], Loss: 5.1547\n",
      "Epoch [1/2], Step [30190/64305], Loss: 5.1372\n",
      "Epoch [1/2], Step [30200/64305], Loss: 5.2268\n",
      "Epoch [1/2], Step [30210/64305], Loss: 5.0566\n",
      "Epoch [1/2], Step [30220/64305], Loss: 5.0106\n",
      "Epoch [1/2], Step [30230/64305], Loss: 4.9905\n",
      "Epoch [1/2], Step [30240/64305], Loss: 5.0135\n",
      "Epoch [1/2], Step [30250/64305], Loss: 4.8000\n",
      "Epoch [1/2], Step [30260/64305], Loss: 5.0571\n",
      "Epoch [1/2], Step [30270/64305], Loss: 4.9602\n",
      "Epoch [1/2], Step [30280/64305], Loss: 5.0823\n",
      "Epoch [1/2], Step [30290/64305], Loss: 4.8201\n",
      "Epoch [1/2], Step [30300/64305], Loss: 4.8250\n",
      "Epoch [1/2], Step [30310/64305], Loss: 4.8209\n",
      "Epoch [1/2], Step [30320/64305], Loss: 4.9053\n",
      "Epoch [1/2], Step [30330/64305], Loss: 5.0679\n",
      "Epoch [1/2], Step [30340/64305], Loss: 4.9252\n",
      "Epoch [1/2], Step [30350/64305], Loss: 5.2554\n",
      "Epoch [1/2], Step [30360/64305], Loss: 5.3345\n",
      "Epoch [1/2], Step [30370/64305], Loss: 4.9674\n",
      "Epoch [1/2], Step [30380/64305], Loss: 4.9848\n",
      "Epoch [1/2], Step [30390/64305], Loss: 5.0116\n",
      "Epoch [1/2], Step [30400/64305], Loss: 5.1062\n",
      "Epoch [1/2], Step [30410/64305], Loss: 4.6755\n",
      "Epoch [1/2], Step [30420/64305], Loss: 5.1414\n",
      "Epoch [1/2], Step [30430/64305], Loss: 5.1005\n",
      "Epoch [1/2], Step [30440/64305], Loss: 4.9179\n",
      "Epoch [1/2], Step [30450/64305], Loss: 5.1443\n",
      "Epoch [1/2], Step [30460/64305], Loss: 5.2136\n",
      "Epoch [1/2], Step [30470/64305], Loss: 4.8349\n",
      "Epoch [1/2], Step [30480/64305], Loss: 4.9349\n",
      "Epoch [1/2], Step [30490/64305], Loss: 5.0444\n",
      "Epoch [1/2], Step [30500/64305], Loss: 5.0833\n",
      "Epoch [1/2], Step [30510/64305], Loss: 5.0876\n",
      "Epoch [1/2], Step [30520/64305], Loss: 4.7987\n",
      "Epoch [1/2], Step [30530/64305], Loss: 4.9926\n",
      "Epoch [1/2], Step [30540/64305], Loss: 5.0247\n",
      "Epoch [1/2], Step [30550/64305], Loss: 4.9876\n",
      "Epoch [1/2], Step [30560/64305], Loss: 4.9965\n",
      "Epoch [1/2], Step [30570/64305], Loss: 5.0202\n",
      "Epoch [1/2], Step [30580/64305], Loss: 4.9652\n",
      "Epoch [1/2], Step [30590/64305], Loss: 4.9427\n",
      "Epoch [1/2], Step [30600/64305], Loss: 4.7694\n",
      "Epoch [1/2], Step [30610/64305], Loss: 5.1315\n",
      "Epoch [1/2], Step [30620/64305], Loss: 4.9759\n",
      "Epoch [1/2], Step [30630/64305], Loss: 5.0753\n",
      "Epoch [1/2], Step [30640/64305], Loss: 5.0470\n",
      "Epoch [1/2], Step [30650/64305], Loss: 5.1706\n",
      "Epoch [1/2], Step [30660/64305], Loss: 5.0071\n",
      "Epoch [1/2], Step [30670/64305], Loss: 5.1423\n",
      "Epoch [1/2], Step [30680/64305], Loss: 4.8890\n",
      "Epoch [1/2], Step [30690/64305], Loss: 4.9962\n",
      "Epoch [1/2], Step [30700/64305], Loss: 5.0768\n",
      "Epoch [1/2], Step [30710/64305], Loss: 5.0734\n",
      "Epoch [1/2], Step [30720/64305], Loss: 5.0237\n",
      "Epoch [1/2], Step [30730/64305], Loss: 4.7869\n",
      "Epoch [1/2], Step [30740/64305], Loss: 5.0147\n",
      "Epoch [1/2], Step [30750/64305], Loss: 4.7263\n",
      "Epoch [1/2], Step [30760/64305], Loss: 5.0518\n",
      "Epoch [1/2], Step [30770/64305], Loss: 5.0776\n",
      "Epoch [1/2], Step [30780/64305], Loss: 5.0172\n",
      "Epoch [1/2], Step [30790/64305], Loss: 5.1406\n",
      "Epoch [1/2], Step [30800/64305], Loss: 5.0473\n",
      "Epoch [1/2], Step [30810/64305], Loss: 4.7182\n",
      "Epoch [1/2], Step [30820/64305], Loss: 4.6913\n",
      "Epoch [1/2], Step [30830/64305], Loss: 5.0465\n",
      "Epoch [1/2], Step [30840/64305], Loss: 4.8605\n",
      "Epoch [1/2], Step [30850/64305], Loss: 4.9235\n",
      "Epoch [1/2], Step [30860/64305], Loss: 5.0823\n",
      "Epoch [1/2], Step [30870/64305], Loss: 5.0030\n",
      "Epoch [1/2], Step [30880/64305], Loss: 5.0959\n",
      "Epoch [1/2], Step [30890/64305], Loss: 4.9813\n",
      "Epoch [1/2], Step [30900/64305], Loss: 5.0577\n",
      "Epoch [1/2], Step [30910/64305], Loss: 5.0372\n",
      "Epoch [1/2], Step [30920/64305], Loss: 4.8946\n",
      "Epoch [1/2], Step [30930/64305], Loss: 5.0473\n",
      "Epoch [1/2], Step [30940/64305], Loss: 4.9484\n",
      "Epoch [1/2], Step [30950/64305], Loss: 5.1393\n",
      "Epoch [1/2], Step [30960/64305], Loss: 4.7799\n",
      "Epoch [1/2], Step [30970/64305], Loss: 5.0830\n",
      "Epoch [1/2], Step [30980/64305], Loss: 4.9951\n",
      "Epoch [1/2], Step [30990/64305], Loss: 4.8512\n",
      "Epoch [1/2], Step [31000/64305], Loss: 4.9755\n",
      "Epoch [1/2], Step [31010/64305], Loss: 4.7806\n",
      "Epoch [1/2], Step [31020/64305], Loss: 4.9157\n",
      "Epoch [1/2], Step [31030/64305], Loss: 5.1403\n",
      "Epoch [1/2], Step [31040/64305], Loss: 4.9141\n",
      "Epoch [1/2], Step [31050/64305], Loss: 5.2275\n",
      "Epoch [1/2], Step [31060/64305], Loss: 5.0633\n",
      "Epoch [1/2], Step [31070/64305], Loss: 5.2752\n",
      "Epoch [1/2], Step [31080/64305], Loss: 5.0663\n",
      "Epoch [1/2], Step [31090/64305], Loss: 4.8111\n",
      "Epoch [1/2], Step [31100/64305], Loss: 4.9087\n",
      "Epoch [1/2], Step [31110/64305], Loss: 5.2437\n",
      "Epoch [1/2], Step [31120/64305], Loss: 4.6857\n",
      "Epoch [1/2], Step [31130/64305], Loss: 5.1321\n",
      "Epoch [1/2], Step [31140/64305], Loss: 4.8997\n",
      "Epoch [1/2], Step [31150/64305], Loss: 4.8824\n",
      "Epoch [1/2], Step [31160/64305], Loss: 4.9465\n",
      "Epoch [1/2], Step [31170/64305], Loss: 4.9266\n",
      "Epoch [1/2], Step [31180/64305], Loss: 4.9250\n",
      "Epoch [1/2], Step [31190/64305], Loss: 5.0401\n",
      "Epoch [1/2], Step [31200/64305], Loss: 4.9282\n",
      "Epoch [1/2], Step [31210/64305], Loss: 5.1732\n",
      "Epoch [1/2], Step [31220/64305], Loss: 5.0136\n",
      "Epoch [1/2], Step [31230/64305], Loss: 4.9879\n",
      "Epoch [1/2], Step [31240/64305], Loss: 5.1220\n",
      "Epoch [1/2], Step [31250/64305], Loss: 4.8382\n",
      "Epoch [1/2], Step [31260/64305], Loss: 5.0246\n",
      "Epoch [1/2], Step [31270/64305], Loss: 4.9742\n",
      "Epoch [1/2], Step [31280/64305], Loss: 4.9993\n",
      "Epoch [1/2], Step [31290/64305], Loss: 5.0675\n",
      "Epoch [1/2], Step [31300/64305], Loss: 4.8592\n",
      "Epoch [1/2], Step [31310/64305], Loss: 5.2375\n",
      "Epoch [1/2], Step [31320/64305], Loss: 4.9577\n",
      "Epoch [1/2], Step [31330/64305], Loss: 4.9197\n",
      "Epoch [1/2], Step [31340/64305], Loss: 5.1580\n",
      "Epoch [1/2], Step [31350/64305], Loss: 5.0532\n",
      "Epoch [1/2], Step [31360/64305], Loss: 4.7741\n",
      "Epoch [1/2], Step [31370/64305], Loss: 4.8688\n",
      "Epoch [1/2], Step [31380/64305], Loss: 4.9206\n",
      "Epoch [1/2], Step [31390/64305], Loss: 5.0615\n",
      "Epoch [1/2], Step [31400/64305], Loss: 5.0063\n",
      "Epoch [1/2], Step [31410/64305], Loss: 4.8700\n",
      "Epoch [1/2], Step [31420/64305], Loss: 4.6972\n",
      "Epoch [1/2], Step [31430/64305], Loss: 4.9054\n",
      "Epoch [1/2], Step [31440/64305], Loss: 5.1767\n",
      "Epoch [1/2], Step [31450/64305], Loss: 4.9101\n",
      "Epoch [1/2], Step [31460/64305], Loss: 4.9509\n",
      "Epoch [1/2], Step [31470/64305], Loss: 4.8293\n",
      "Epoch [1/2], Step [31480/64305], Loss: 4.6761\n",
      "Epoch [1/2], Step [31490/64305], Loss: 4.7838\n",
      "Epoch [1/2], Step [31500/64305], Loss: 5.1246\n",
      "Epoch [1/2], Step [31510/64305], Loss: 4.9572\n",
      "Epoch [1/2], Step [31520/64305], Loss: 4.9734\n",
      "Epoch [1/2], Step [31530/64305], Loss: 4.8954\n",
      "Epoch [1/2], Step [31540/64305], Loss: 5.0220\n",
      "Epoch [1/2], Step [31550/64305], Loss: 5.1832\n",
      "Epoch [1/2], Step [31560/64305], Loss: 4.9976\n",
      "Epoch [1/2], Step [31570/64305], Loss: 4.9300\n",
      "Epoch [1/2], Step [31580/64305], Loss: 4.9570\n",
      "Epoch [1/2], Step [31590/64305], Loss: 4.9616\n",
      "Epoch [1/2], Step [31600/64305], Loss: 4.9713\n",
      "Epoch [1/2], Step [31610/64305], Loss: 4.7702\n",
      "Epoch [1/2], Step [31620/64305], Loss: 5.0083\n",
      "Epoch [1/2], Step [31630/64305], Loss: 5.0463\n",
      "Epoch [1/2], Step [31640/64305], Loss: 5.1172\n",
      "Epoch [1/2], Step [31650/64305], Loss: 5.0580\n",
      "Epoch [1/2], Step [31660/64305], Loss: 4.9726\n",
      "Epoch [1/2], Step [31670/64305], Loss: 5.0239\n",
      "Epoch [1/2], Step [31680/64305], Loss: 5.0647\n",
      "Epoch [1/2], Step [31690/64305], Loss: 4.8301\n",
      "Epoch [1/2], Step [31700/64305], Loss: 5.1577\n",
      "Epoch [1/2], Step [31710/64305], Loss: 4.6531\n",
      "Epoch [1/2], Step [31720/64305], Loss: 5.1023\n",
      "Epoch [1/2], Step [31730/64305], Loss: 4.9201\n",
      "Epoch [1/2], Step [31740/64305], Loss: 5.1048\n",
      "Epoch [1/2], Step [31750/64305], Loss: 5.0506\n",
      "Epoch [1/2], Step [31760/64305], Loss: 5.0565\n",
      "Epoch [1/2], Step [31770/64305], Loss: 4.9316\n",
      "Epoch [1/2], Step [31780/64305], Loss: 4.9190\n",
      "Epoch [1/2], Step [31790/64305], Loss: 4.8095\n",
      "Epoch [1/2], Step [31800/64305], Loss: 4.9984\n",
      "Epoch [1/2], Step [31810/64305], Loss: 5.0277\n",
      "Epoch [1/2], Step [31820/64305], Loss: 4.8369\n",
      "Epoch [1/2], Step [31830/64305], Loss: 5.0291\n",
      "Epoch [1/2], Step [31840/64305], Loss: 4.9740\n",
      "Epoch [1/2], Step [31850/64305], Loss: 5.1644\n",
      "Epoch [1/2], Step [31860/64305], Loss: 5.0820\n",
      "Epoch [1/2], Step [31870/64305], Loss: 5.2879\n",
      "Epoch [1/2], Step [31880/64305], Loss: 4.8163\n",
      "Epoch [1/2], Step [31890/64305], Loss: 5.0634\n",
      "Epoch [1/2], Step [31900/64305], Loss: 4.9407\n",
      "Epoch [1/2], Step [31910/64305], Loss: 5.0558\n",
      "Epoch [1/2], Step [31920/64305], Loss: 5.0423\n",
      "Epoch [1/2], Step [31930/64305], Loss: 5.0822\n",
      "Epoch [1/2], Step [31940/64305], Loss: 5.0068\n",
      "Epoch [1/2], Step [31950/64305], Loss: 5.1570\n",
      "Epoch [1/2], Step [31960/64305], Loss: 4.9488\n",
      "Epoch [1/2], Step [31970/64305], Loss: 5.2127\n",
      "Epoch [1/2], Step [31980/64305], Loss: 5.0148\n",
      "Epoch [1/2], Step [31990/64305], Loss: 5.0015\n",
      "Epoch [1/2], Step [32000/64305], Loss: 5.0011\n",
      "Epoch [1/2], Step [32010/64305], Loss: 4.8868\n",
      "Epoch [1/2], Step [32020/64305], Loss: 5.0777\n",
      "Epoch [1/2], Step [32030/64305], Loss: 5.0888\n",
      "Epoch [1/2], Step [32040/64305], Loss: 5.0117\n",
      "Epoch [1/2], Step [32050/64305], Loss: 4.8424\n",
      "Epoch [1/2], Step [32060/64305], Loss: 5.1962\n",
      "Epoch [1/2], Step [32070/64305], Loss: 5.1905\n",
      "Epoch [1/2], Step [32080/64305], Loss: 4.9834\n",
      "Epoch [1/2], Step [32090/64305], Loss: 4.9260\n",
      "Epoch [1/2], Step [32100/64305], Loss: 4.8624\n",
      "Epoch [1/2], Step [32110/64305], Loss: 5.3288\n",
      "Epoch [1/2], Step [32120/64305], Loss: 4.9458\n",
      "Epoch [1/2], Step [32130/64305], Loss: 4.9773\n",
      "Epoch [1/2], Step [32140/64305], Loss: 4.9791\n",
      "Epoch [1/2], Step [32150/64305], Loss: 4.8223\n",
      "Epoch [1/2], Step [32160/64305], Loss: 4.8853\n",
      "Epoch [1/2], Step [32170/64305], Loss: 5.0028\n",
      "Epoch [1/2], Step [32180/64305], Loss: 4.9192\n",
      "Epoch [1/2], Step [32190/64305], Loss: 4.9940\n",
      "Epoch [1/2], Step [32200/64305], Loss: 5.0021\n",
      "Epoch [1/2], Step [32210/64305], Loss: 5.0623\n",
      "Epoch [1/2], Step [32220/64305], Loss: 4.9065\n",
      "Epoch [1/2], Step [32230/64305], Loss: 5.0290\n",
      "Epoch [1/2], Step [32240/64305], Loss: 5.0995\n",
      "Epoch [1/2], Step [32250/64305], Loss: 4.8965\n",
      "Epoch [1/2], Step [32260/64305], Loss: 4.6468\n",
      "Epoch [1/2], Step [32270/64305], Loss: 4.8323\n",
      "Epoch [1/2], Step [32280/64305], Loss: 4.9186\n",
      "Epoch [1/2], Step [32290/64305], Loss: 4.9592\n",
      "Epoch [1/2], Step [32300/64305], Loss: 4.9101\n",
      "Epoch [1/2], Step [32310/64305], Loss: 5.0676\n",
      "Epoch [1/2], Step [32320/64305], Loss: 4.8631\n",
      "Epoch [1/2], Step [32330/64305], Loss: 4.9055\n",
      "Epoch [1/2], Step [32340/64305], Loss: 5.0247\n",
      "Epoch [1/2], Step [32350/64305], Loss: 5.0636\n",
      "Epoch [1/2], Step [32360/64305], Loss: 4.9868\n",
      "Epoch [1/2], Step [32370/64305], Loss: 5.1303\n",
      "Epoch [1/2], Step [32380/64305], Loss: 5.1088\n",
      "Epoch [1/2], Step [32390/64305], Loss: 5.0664\n",
      "Epoch [1/2], Step [32400/64305], Loss: 4.9882\n",
      "Epoch [1/2], Step [32410/64305], Loss: 4.8106\n",
      "Epoch [1/2], Step [32420/64305], Loss: 4.9584\n",
      "Epoch [1/2], Step [32430/64305], Loss: 4.8943\n",
      "Epoch [1/2], Step [32440/64305], Loss: 4.9182\n",
      "Epoch [1/2], Step [32450/64305], Loss: 4.9680\n",
      "Epoch [1/2], Step [32460/64305], Loss: 4.9975\n",
      "Epoch [1/2], Step [32470/64305], Loss: 4.8269\n",
      "Epoch [1/2], Step [32480/64305], Loss: 5.1100\n",
      "Epoch [1/2], Step [32490/64305], Loss: 5.0273\n",
      "Epoch [1/2], Step [32500/64305], Loss: 5.0155\n",
      "Epoch [1/2], Step [32510/64305], Loss: 4.8917\n",
      "Epoch [1/2], Step [32520/64305], Loss: 4.9074\n",
      "Epoch [1/2], Step [32530/64305], Loss: 4.9682\n",
      "Epoch [1/2], Step [32540/64305], Loss: 5.0314\n",
      "Epoch [1/2], Step [32550/64305], Loss: 4.9035\n",
      "Epoch [1/2], Step [32560/64305], Loss: 4.9439\n",
      "Epoch [1/2], Step [32570/64305], Loss: 4.9987\n",
      "Epoch [1/2], Step [32580/64305], Loss: 4.8800\n",
      "Epoch [1/2], Step [32590/64305], Loss: 4.7938\n",
      "Epoch [1/2], Step [32600/64305], Loss: 4.8786\n",
      "Epoch [1/2], Step [32610/64305], Loss: 5.0405\n",
      "Epoch [1/2], Step [32620/64305], Loss: 4.8842\n",
      "Epoch [1/2], Step [32630/64305], Loss: 5.0701\n",
      "Epoch [1/2], Step [32640/64305], Loss: 4.8852\n",
      "Epoch [1/2], Step [32650/64305], Loss: 4.9175\n",
      "Epoch [1/2], Step [32660/64305], Loss: 4.8868\n",
      "Epoch [1/2], Step [32670/64305], Loss: 5.1450\n",
      "Epoch [1/2], Step [32680/64305], Loss: 4.9335\n",
      "Epoch [1/2], Step [32690/64305], Loss: 5.1986\n",
      "Epoch [1/2], Step [32700/64305], Loss: 4.8189\n",
      "Epoch [1/2], Step [32710/64305], Loss: 5.0544\n",
      "Epoch [1/2], Step [32720/64305], Loss: 4.8229\n",
      "Epoch [1/2], Step [32730/64305], Loss: 4.9066\n",
      "Epoch [1/2], Step [32740/64305], Loss: 4.9628\n",
      "Epoch [1/2], Step [32750/64305], Loss: 4.9543\n",
      "Epoch [1/2], Step [32760/64305], Loss: 4.7917\n",
      "Epoch [1/2], Step [32770/64305], Loss: 5.0762\n",
      "Epoch [1/2], Step [32780/64305], Loss: 5.0180\n",
      "Epoch [1/2], Step [32790/64305], Loss: 5.1436\n",
      "Epoch [1/2], Step [32800/64305], Loss: 4.8633\n",
      "Epoch [1/2], Step [32810/64305], Loss: 4.8433\n",
      "Epoch [1/2], Step [32820/64305], Loss: 4.7712\n",
      "Epoch [1/2], Step [32830/64305], Loss: 5.1887\n",
      "Epoch [1/2], Step [32840/64305], Loss: 4.8339\n",
      "Epoch [1/2], Step [32850/64305], Loss: 4.7542\n",
      "Epoch [1/2], Step [32860/64305], Loss: 4.9236\n",
      "Epoch [1/2], Step [32870/64305], Loss: 5.0478\n",
      "Epoch [1/2], Step [32880/64305], Loss: 5.0071\n",
      "Epoch [1/2], Step [32890/64305], Loss: 4.9858\n",
      "Epoch [1/2], Step [32900/64305], Loss: 5.1751\n",
      "Epoch [1/2], Step [32910/64305], Loss: 5.0875\n",
      "Epoch [1/2], Step [32920/64305], Loss: 4.7765\n",
      "Epoch [1/2], Step [32930/64305], Loss: 4.8917\n",
      "Epoch [1/2], Step [32940/64305], Loss: 4.8173\n",
      "Epoch [1/2], Step [32950/64305], Loss: 4.8916\n",
      "Epoch [1/2], Step [32960/64305], Loss: 5.0333\n",
      "Epoch [1/2], Step [32970/64305], Loss: 4.9094\n",
      "Epoch [1/2], Step [32980/64305], Loss: 4.6907\n",
      "Epoch [1/2], Step [32990/64305], Loss: 5.1259\n",
      "Epoch [1/2], Step [33000/64305], Loss: 4.9733\n",
      "Epoch [1/2], Step [33010/64305], Loss: 4.9503\n",
      "Epoch [1/2], Step [33020/64305], Loss: 5.0871\n",
      "Epoch [1/2], Step [33030/64305], Loss: 5.0687\n",
      "Epoch [1/2], Step [33040/64305], Loss: 5.1216\n",
      "Epoch [1/2], Step [33050/64305], Loss: 4.9994\n",
      "Epoch [1/2], Step [33060/64305], Loss: 4.8865\n",
      "Epoch [1/2], Step [33070/64305], Loss: 5.0657\n",
      "Epoch [1/2], Step [33080/64305], Loss: 5.1165\n",
      "Epoch [1/2], Step [33090/64305], Loss: 4.8503\n",
      "Epoch [1/2], Step [33100/64305], Loss: 5.0023\n",
      "Epoch [1/2], Step [33110/64305], Loss: 4.8635\n",
      "Epoch [1/2], Step [33120/64305], Loss: 5.1195\n",
      "Epoch [1/2], Step [33130/64305], Loss: 5.0239\n",
      "Epoch [1/2], Step [33140/64305], Loss: 5.0272\n",
      "Epoch [1/2], Step [33150/64305], Loss: 5.1052\n",
      "Epoch [1/2], Step [33160/64305], Loss: 5.0843\n",
      "Epoch [1/2], Step [33170/64305], Loss: 5.0484\n",
      "Epoch [1/2], Step [33180/64305], Loss: 5.1491\n",
      "Epoch [1/2], Step [33190/64305], Loss: 5.0542\n",
      "Epoch [1/2], Step [33200/64305], Loss: 5.0139\n",
      "Epoch [1/2], Step [33210/64305], Loss: 4.9290\n",
      "Epoch [1/2], Step [33220/64305], Loss: 4.7266\n",
      "Epoch [1/2], Step [33230/64305], Loss: 4.9696\n",
      "Epoch [1/2], Step [33240/64305], Loss: 4.9976\n",
      "Epoch [1/2], Step [33250/64305], Loss: 4.9365\n",
      "Epoch [1/2], Step [33260/64305], Loss: 4.9227\n",
      "Epoch [1/2], Step [33270/64305], Loss: 4.9946\n",
      "Epoch [1/2], Step [33280/64305], Loss: 5.0466\n",
      "Epoch [1/2], Step [33290/64305], Loss: 5.2212\n",
      "Epoch [1/2], Step [33300/64305], Loss: 4.9807\n",
      "Epoch [1/2], Step [33310/64305], Loss: 4.9967\n",
      "Epoch [1/2], Step [33320/64305], Loss: 4.4597\n",
      "Epoch [1/2], Step [33330/64305], Loss: 4.9987\n",
      "Epoch [1/2], Step [33340/64305], Loss: 4.9860\n",
      "Epoch [1/2], Step [33350/64305], Loss: 4.9871\n",
      "Epoch [1/2], Step [33360/64305], Loss: 4.8736\n",
      "Epoch [1/2], Step [33370/64305], Loss: 4.6697\n",
      "Epoch [1/2], Step [33380/64305], Loss: 5.1615\n",
      "Epoch [1/2], Step [33390/64305], Loss: 5.0817\n",
      "Epoch [1/2], Step [33400/64305], Loss: 5.0968\n",
      "Epoch [1/2], Step [33410/64305], Loss: 4.8933\n",
      "Epoch [1/2], Step [33420/64305], Loss: 4.9970\n",
      "Epoch [1/2], Step [33430/64305], Loss: 4.9267\n",
      "Epoch [1/2], Step [33440/64305], Loss: 5.2119\n",
      "Epoch [1/2], Step [33450/64305], Loss: 4.9397\n",
      "Epoch [1/2], Step [33460/64305], Loss: 4.8953\n",
      "Epoch [1/2], Step [33470/64305], Loss: 4.9741\n",
      "Epoch [1/2], Step [33480/64305], Loss: 5.0231\n",
      "Epoch [1/2], Step [33490/64305], Loss: 4.8091\n",
      "Epoch [1/2], Step [33500/64305], Loss: 5.0651\n",
      "Epoch [1/2], Step [33510/64305], Loss: 5.0133\n",
      "Epoch [1/2], Step [33520/64305], Loss: 4.9998\n",
      "Epoch [1/2], Step [33530/64305], Loss: 5.0411\n",
      "Epoch [1/2], Step [33540/64305], Loss: 4.9646\n",
      "Epoch [1/2], Step [33550/64305], Loss: 4.9599\n",
      "Epoch [1/2], Step [33560/64305], Loss: 4.7883\n",
      "Epoch [1/2], Step [33570/64305], Loss: 5.0363\n",
      "Epoch [1/2], Step [33580/64305], Loss: 4.9538\n",
      "Epoch [1/2], Step [33590/64305], Loss: 4.9572\n",
      "Epoch [1/2], Step [33600/64305], Loss: 4.8954\n",
      "Epoch [1/2], Step [33610/64305], Loss: 4.7339\n",
      "Epoch [1/2], Step [33620/64305], Loss: 5.2425\n",
      "Epoch [1/2], Step [33630/64305], Loss: 4.7582\n",
      "Epoch [1/2], Step [33640/64305], Loss: 5.0898\n",
      "Epoch [1/2], Step [33650/64305], Loss: 4.8239\n",
      "Epoch [1/2], Step [33660/64305], Loss: 4.9357\n",
      "Epoch [1/2], Step [33670/64305], Loss: 4.9368\n",
      "Epoch [1/2], Step [33680/64305], Loss: 4.9833\n",
      "Epoch [1/2], Step [33690/64305], Loss: 4.8731\n",
      "Epoch [1/2], Step [33700/64305], Loss: 5.0810\n",
      "Epoch [1/2], Step [33710/64305], Loss: 4.9526\n",
      "Epoch [1/2], Step [33720/64305], Loss: 4.9615\n",
      "Epoch [1/2], Step [33730/64305], Loss: 4.9916\n",
      "Epoch [1/2], Step [33740/64305], Loss: 4.9470\n",
      "Epoch [1/2], Step [33750/64305], Loss: 4.8125\n",
      "Epoch [1/2], Step [33760/64305], Loss: 4.9333\n",
      "Epoch [1/2], Step [33770/64305], Loss: 4.8681\n",
      "Epoch [1/2], Step [33780/64305], Loss: 5.0313\n",
      "Epoch [1/2], Step [33790/64305], Loss: 4.8753\n",
      "Epoch [1/2], Step [33800/64305], Loss: 4.8230\n",
      "Epoch [1/2], Step [33810/64305], Loss: 4.8861\n",
      "Epoch [1/2], Step [33820/64305], Loss: 5.0990\n",
      "Epoch [1/2], Step [33830/64305], Loss: 4.9057\n",
      "Epoch [1/2], Step [33840/64305], Loss: 4.8820\n",
      "Epoch [1/2], Step [33850/64305], Loss: 5.0420\n",
      "Epoch [1/2], Step [33860/64305], Loss: 4.9590\n",
      "Epoch [1/2], Step [33870/64305], Loss: 5.0220\n",
      "Epoch [1/2], Step [33880/64305], Loss: 4.9776\n",
      "Epoch [1/2], Step [33890/64305], Loss: 4.8883\n",
      "Epoch [1/2], Step [33900/64305], Loss: 4.8719\n",
      "Epoch [1/2], Step [33910/64305], Loss: 4.9013\n",
      "Epoch [1/2], Step [33920/64305], Loss: 4.8533\n",
      "Epoch [1/2], Step [33930/64305], Loss: 5.0593\n",
      "Epoch [1/2], Step [33940/64305], Loss: 4.9396\n",
      "Epoch [1/2], Step [33950/64305], Loss: 4.7318\n",
      "Epoch [1/2], Step [33960/64305], Loss: 4.9522\n",
      "Epoch [1/2], Step [33970/64305], Loss: 4.9080\n",
      "Epoch [1/2], Step [33980/64305], Loss: 4.9246\n",
      "Epoch [1/2], Step [33990/64305], Loss: 4.9629\n",
      "Epoch [1/2], Step [34000/64305], Loss: 4.9124\n",
      "Epoch [1/2], Step [34010/64305], Loss: 5.1359\n",
      "Epoch [1/2], Step [34020/64305], Loss: 5.0636\n",
      "Epoch [1/2], Step [34030/64305], Loss: 5.2937\n",
      "Epoch [1/2], Step [34040/64305], Loss: 4.8681\n",
      "Epoch [1/2], Step [34050/64305], Loss: 5.1215\n",
      "Epoch [1/2], Step [34060/64305], Loss: 4.8797\n",
      "Epoch [1/2], Step [34070/64305], Loss: 4.7380\n",
      "Epoch [1/2], Step [34080/64305], Loss: 5.0268\n",
      "Epoch [1/2], Step [34090/64305], Loss: 4.8050\n",
      "Epoch [1/2], Step [34100/64305], Loss: 4.9080\n",
      "Epoch [1/2], Step [34110/64305], Loss: 5.0714\n",
      "Epoch [1/2], Step [34120/64305], Loss: 4.8463\n",
      "Epoch [1/2], Step [34130/64305], Loss: 5.1073\n",
      "Epoch [1/2], Step [34140/64305], Loss: 5.0625\n",
      "Epoch [1/2], Step [34150/64305], Loss: 4.8565\n",
      "Epoch [1/2], Step [34160/64305], Loss: 4.9503\n",
      "Epoch [1/2], Step [34170/64305], Loss: 4.9164\n",
      "Epoch [1/2], Step [34180/64305], Loss: 4.8765\n",
      "Epoch [1/2], Step [34190/64305], Loss: 5.1479\n",
      "Epoch [1/2], Step [34200/64305], Loss: 5.0070\n",
      "Epoch [1/2], Step [34210/64305], Loss: 4.9496\n",
      "Epoch [1/2], Step [34220/64305], Loss: 5.0393\n",
      "Epoch [1/2], Step [34230/64305], Loss: 4.9993\n",
      "Epoch [1/2], Step [34240/64305], Loss: 4.8762\n",
      "Epoch [1/2], Step [34250/64305], Loss: 5.0280\n",
      "Epoch [1/2], Step [34260/64305], Loss: 4.8838\n",
      "Epoch [1/2], Step [34270/64305], Loss: 5.1009\n",
      "Epoch [1/2], Step [34280/64305], Loss: 5.1845\n",
      "Epoch [1/2], Step [34290/64305], Loss: 4.9222\n",
      "Epoch [1/2], Step [34300/64305], Loss: 4.8097\n",
      "Epoch [1/2], Step [34310/64305], Loss: 5.1640\n",
      "Epoch [1/2], Step [34320/64305], Loss: 4.9661\n",
      "Epoch [1/2], Step [34330/64305], Loss: 5.0865\n",
      "Epoch [1/2], Step [34340/64305], Loss: 4.8622\n",
      "Epoch [1/2], Step [34350/64305], Loss: 4.8062\n",
      "Epoch [1/2], Step [34360/64305], Loss: 5.1294\n",
      "Epoch [1/2], Step [34370/64305], Loss: 4.9476\n",
      "Epoch [1/2], Step [34380/64305], Loss: 5.0436\n",
      "Epoch [1/2], Step [34390/64305], Loss: 4.9465\n",
      "Epoch [1/2], Step [34400/64305], Loss: 5.0495\n",
      "Epoch [1/2], Step [34410/64305], Loss: 5.1190\n",
      "Epoch [1/2], Step [34420/64305], Loss: 4.9918\n",
      "Epoch [1/2], Step [34430/64305], Loss: 4.8742\n",
      "Epoch [1/2], Step [34440/64305], Loss: 5.0900\n",
      "Epoch [1/2], Step [34450/64305], Loss: 4.8308\n",
      "Epoch [1/2], Step [34460/64305], Loss: 4.8838\n",
      "Epoch [1/2], Step [34470/64305], Loss: 4.9637\n",
      "Epoch [1/2], Step [34480/64305], Loss: 5.0608\n",
      "Epoch [1/2], Step [34490/64305], Loss: 4.9138\n",
      "Epoch [1/2], Step [34500/64305], Loss: 5.1304\n",
      "Epoch [1/2], Step [34510/64305], Loss: 5.0257\n",
      "Epoch [1/2], Step [34520/64305], Loss: 4.9097\n",
      "Epoch [1/2], Step [34530/64305], Loss: 4.7001\n",
      "Epoch [1/2], Step [34540/64305], Loss: 4.8987\n",
      "Epoch [1/2], Step [34550/64305], Loss: 5.0006\n",
      "Epoch [1/2], Step [34560/64305], Loss: 4.9776\n",
      "Epoch [1/2], Step [34570/64305], Loss: 5.0232\n",
      "Epoch [1/2], Step [34580/64305], Loss: 4.8807\n",
      "Epoch [1/2], Step [34590/64305], Loss: 4.9617\n",
      "Epoch [1/2], Step [34600/64305], Loss: 4.8698\n",
      "Epoch [1/2], Step [34610/64305], Loss: 4.9732\n",
      "Epoch [1/2], Step [34620/64305], Loss: 5.0333\n",
      "Epoch [1/2], Step [34630/64305], Loss: 4.8460\n",
      "Epoch [1/2], Step [34640/64305], Loss: 5.0535\n",
      "Epoch [1/2], Step [34650/64305], Loss: 5.1328\n",
      "Epoch [1/2], Step [34660/64305], Loss: 5.0921\n",
      "Epoch [1/2], Step [34670/64305], Loss: 4.8871\n",
      "Epoch [1/2], Step [34680/64305], Loss: 4.9338\n",
      "Epoch [1/2], Step [34690/64305], Loss: 4.8015\n",
      "Epoch [1/2], Step [34700/64305], Loss: 5.0469\n",
      "Epoch [1/2], Step [34710/64305], Loss: 5.0572\n",
      "Epoch [1/2], Step [34720/64305], Loss: 5.1388\n",
      "Epoch [1/2], Step [34730/64305], Loss: 4.8454\n",
      "Epoch [1/2], Step [34740/64305], Loss: 5.1004\n",
      "Epoch [1/2], Step [34750/64305], Loss: 5.0227\n",
      "Epoch [1/2], Step [34760/64305], Loss: 4.9621\n",
      "Epoch [1/2], Step [34770/64305], Loss: 4.8040\n",
      "Epoch [1/2], Step [34780/64305], Loss: 4.8550\n",
      "Epoch [1/2], Step [34790/64305], Loss: 5.0896\n",
      "Epoch [1/2], Step [34800/64305], Loss: 4.9189\n",
      "Epoch [1/2], Step [34810/64305], Loss: 4.7779\n",
      "Epoch [1/2], Step [34820/64305], Loss: 5.0466\n",
      "Epoch [1/2], Step [34830/64305], Loss: 4.9055\n",
      "Epoch [1/2], Step [34840/64305], Loss: 5.0434\n",
      "Epoch [1/2], Step [34850/64305], Loss: 4.8949\n",
      "Epoch [1/2], Step [34860/64305], Loss: 4.9882\n",
      "Epoch [1/2], Step [34870/64305], Loss: 4.8479\n",
      "Epoch [1/2], Step [34880/64305], Loss: 4.9319\n",
      "Epoch [1/2], Step [34890/64305], Loss: 4.7498\n",
      "Epoch [1/2], Step [34900/64305], Loss: 5.1126\n",
      "Epoch [1/2], Step [34910/64305], Loss: 5.1298\n",
      "Epoch [1/2], Step [34920/64305], Loss: 5.2597\n",
      "Epoch [1/2], Step [34930/64305], Loss: 5.1145\n",
      "Epoch [1/2], Step [34940/64305], Loss: 4.8885\n",
      "Epoch [1/2], Step [34950/64305], Loss: 5.0297\n",
      "Epoch [1/2], Step [34960/64305], Loss: 5.0673\n",
      "Epoch [1/2], Step [34970/64305], Loss: 5.0928\n",
      "Epoch [1/2], Step [34980/64305], Loss: 4.9512\n",
      "Epoch [1/2], Step [34990/64305], Loss: 4.8423\n",
      "Epoch [1/2], Step [35000/64305], Loss: 4.9908\n",
      "Epoch [1/2], Step [35010/64305], Loss: 4.9235\n",
      "Epoch [1/2], Step [35020/64305], Loss: 5.1241\n",
      "Epoch [1/2], Step [35030/64305], Loss: 4.8095\n",
      "Epoch [1/2], Step [35040/64305], Loss: 4.9340\n",
      "Epoch [1/2], Step [35050/64305], Loss: 5.0807\n",
      "Epoch [1/2], Step [35060/64305], Loss: 5.0345\n",
      "Epoch [1/2], Step [35070/64305], Loss: 5.1389\n",
      "Epoch [1/2], Step [35080/64305], Loss: 4.7963\n",
      "Epoch [1/2], Step [35090/64305], Loss: 5.0441\n",
      "Epoch [1/2], Step [35100/64305], Loss: 5.0482\n",
      "Epoch [1/2], Step [35110/64305], Loss: 4.8335\n",
      "Epoch [1/2], Step [35120/64305], Loss: 5.1078\n",
      "Epoch [1/2], Step [35130/64305], Loss: 4.9615\n",
      "Epoch [1/2], Step [35140/64305], Loss: 5.0859\n",
      "Epoch [1/2], Step [35150/64305], Loss: 4.8928\n",
      "Epoch [1/2], Step [35160/64305], Loss: 4.9695\n",
      "Epoch [1/2], Step [35170/64305], Loss: 4.8034\n",
      "Epoch [1/2], Step [35180/64305], Loss: 4.8968\n",
      "Epoch [1/2], Step [35190/64305], Loss: 5.0626\n",
      "Epoch [1/2], Step [35200/64305], Loss: 4.8540\n",
      "Epoch [1/2], Step [35210/64305], Loss: 5.1337\n",
      "Epoch [1/2], Step [35220/64305], Loss: 4.8975\n",
      "Epoch [1/2], Step [35230/64305], Loss: 5.0754\n",
      "Epoch [1/2], Step [35240/64305], Loss: 4.7869\n",
      "Epoch [1/2], Step [35250/64305], Loss: 5.3290\n",
      "Epoch [1/2], Step [35260/64305], Loss: 4.9188\n",
      "Epoch [1/2], Step [35270/64305], Loss: 4.7513\n",
      "Epoch [1/2], Step [35280/64305], Loss: 4.9210\n",
      "Epoch [1/2], Step [35290/64305], Loss: 4.7550\n",
      "Epoch [1/2], Step [35300/64305], Loss: 5.0688\n",
      "Epoch [1/2], Step [35310/64305], Loss: 5.0111\n",
      "Epoch [1/2], Step [35320/64305], Loss: 4.9177\n",
      "Epoch [1/2], Step [35330/64305], Loss: 5.0294\n",
      "Epoch [1/2], Step [35340/64305], Loss: 4.8008\n",
      "Epoch [1/2], Step [35350/64305], Loss: 4.7681\n",
      "Epoch [1/2], Step [35360/64305], Loss: 4.9616\n",
      "Epoch [1/2], Step [35370/64305], Loss: 4.9962\n",
      "Epoch [1/2], Step [35380/64305], Loss: 4.8443\n",
      "Epoch [1/2], Step [35390/64305], Loss: 4.8019\n",
      "Epoch [1/2], Step [35400/64305], Loss: 4.7974\n",
      "Epoch [1/2], Step [35410/64305], Loss: 4.9598\n",
      "Epoch [1/2], Step [35420/64305], Loss: 5.0159\n",
      "Epoch [1/2], Step [35430/64305], Loss: 4.9533\n",
      "Epoch [1/2], Step [35440/64305], Loss: 4.8134\n",
      "Epoch [1/2], Step [35450/64305], Loss: 4.6757\n",
      "Epoch [1/2], Step [35460/64305], Loss: 4.9064\n",
      "Epoch [1/2], Step [35470/64305], Loss: 4.9300\n",
      "Epoch [1/2], Step [35480/64305], Loss: 4.8893\n",
      "Epoch [1/2], Step [35490/64305], Loss: 4.9593\n",
      "Epoch [1/2], Step [35500/64305], Loss: 5.0221\n",
      "Epoch [1/2], Step [35510/64305], Loss: 4.9890\n",
      "Epoch [1/2], Step [35520/64305], Loss: 5.0507\n",
      "Epoch [1/2], Step [35530/64305], Loss: 4.9666\n",
      "Epoch [1/2], Step [35540/64305], Loss: 5.0840\n",
      "Epoch [1/2], Step [35550/64305], Loss: 5.0449\n",
      "Epoch [1/2], Step [35560/64305], Loss: 5.0582\n",
      "Epoch [1/2], Step [35570/64305], Loss: 4.9390\n",
      "Epoch [1/2], Step [35580/64305], Loss: 4.9192\n",
      "Epoch [1/2], Step [35590/64305], Loss: 4.8256\n",
      "Epoch [1/2], Step [35600/64305], Loss: 5.0057\n",
      "Epoch [1/2], Step [35610/64305], Loss: 5.1012\n",
      "Epoch [1/2], Step [35620/64305], Loss: 5.1253\n",
      "Epoch [1/2], Step [35630/64305], Loss: 4.9299\n",
      "Epoch [1/2], Step [35640/64305], Loss: 4.7237\n",
      "Epoch [1/2], Step [35650/64305], Loss: 4.9841\n",
      "Epoch [1/2], Step [35660/64305], Loss: 4.7829\n",
      "Epoch [1/2], Step [35670/64305], Loss: 5.2228\n",
      "Epoch [1/2], Step [35680/64305], Loss: 4.8855\n",
      "Epoch [1/2], Step [35690/64305], Loss: 5.0383\n",
      "Epoch [1/2], Step [35700/64305], Loss: 5.1241\n",
      "Epoch [1/2], Step [35710/64305], Loss: 4.9447\n",
      "Epoch [1/2], Step [35720/64305], Loss: 4.8676\n",
      "Epoch [1/2], Step [35730/64305], Loss: 4.9623\n",
      "Epoch [1/2], Step [35740/64305], Loss: 5.0101\n",
      "Epoch [1/2], Step [35750/64305], Loss: 4.9581\n",
      "Epoch [1/2], Step [35760/64305], Loss: 5.1279\n",
      "Epoch [1/2], Step [35770/64305], Loss: 4.9280\n",
      "Epoch [1/2], Step [35780/64305], Loss: 5.2509\n",
      "Epoch [1/2], Step [35790/64305], Loss: 4.8776\n",
      "Epoch [1/2], Step [35800/64305], Loss: 4.8805\n",
      "Epoch [1/2], Step [35810/64305], Loss: 4.9002\n",
      "Epoch [1/2], Step [35820/64305], Loss: 4.8191\n",
      "Epoch [1/2], Step [35830/64305], Loss: 4.9643\n",
      "Epoch [1/2], Step [35840/64305], Loss: 4.9169\n",
      "Epoch [1/2], Step [35850/64305], Loss: 4.7308\n",
      "Epoch [1/2], Step [35860/64305], Loss: 4.8657\n",
      "Epoch [1/2], Step [35870/64305], Loss: 5.1645\n",
      "Epoch [1/2], Step [35880/64305], Loss: 5.1066\n",
      "Epoch [1/2], Step [35890/64305], Loss: 4.9982\n",
      "Epoch [1/2], Step [35900/64305], Loss: 4.9426\n",
      "Epoch [1/2], Step [35910/64305], Loss: 5.0025\n",
      "Epoch [1/2], Step [35920/64305], Loss: 4.9212\n",
      "Epoch [1/2], Step [35930/64305], Loss: 4.9791\n",
      "Epoch [1/2], Step [35940/64305], Loss: 4.7544\n",
      "Epoch [1/2], Step [35950/64305], Loss: 4.8932\n",
      "Epoch [1/2], Step [35960/64305], Loss: 5.0356\n",
      "Epoch [1/2], Step [35970/64305], Loss: 5.1078\n",
      "Epoch [1/2], Step [35980/64305], Loss: 5.0157\n",
      "Epoch [1/2], Step [35990/64305], Loss: 4.9336\n",
      "Epoch [1/2], Step [36000/64305], Loss: 4.8364\n",
      "Epoch [1/2], Step [36010/64305], Loss: 4.8359\n",
      "Epoch [1/2], Step [36020/64305], Loss: 4.9098\n",
      "Epoch [1/2], Step [36030/64305], Loss: 4.9795\n",
      "Epoch [1/2], Step [36040/64305], Loss: 5.0040\n",
      "Epoch [1/2], Step [36050/64305], Loss: 4.8999\n",
      "Epoch [1/2], Step [36060/64305], Loss: 5.0540\n",
      "Epoch [1/2], Step [36070/64305], Loss: 5.0513\n",
      "Epoch [1/2], Step [36080/64305], Loss: 5.0355\n",
      "Epoch [1/2], Step [36090/64305], Loss: 5.0720\n",
      "Epoch [1/2], Step [36100/64305], Loss: 5.0620\n",
      "Epoch [1/2], Step [36110/64305], Loss: 4.9191\n",
      "Epoch [1/2], Step [36120/64305], Loss: 4.8661\n",
      "Epoch [1/2], Step [36130/64305], Loss: 5.0420\n",
      "Epoch [1/2], Step [36140/64305], Loss: 5.0839\n",
      "Epoch [1/2], Step [36150/64305], Loss: 4.9261\n",
      "Epoch [1/2], Step [36160/64305], Loss: 4.9037\n",
      "Epoch [1/2], Step [36170/64305], Loss: 5.0163\n",
      "Epoch [1/2], Step [36180/64305], Loss: 5.0813\n",
      "Epoch [1/2], Step [36190/64305], Loss: 5.0598\n",
      "Epoch [1/2], Step [36200/64305], Loss: 4.9091\n",
      "Epoch [1/2], Step [36210/64305], Loss: 4.8780\n",
      "Epoch [1/2], Step [36220/64305], Loss: 4.9032\n",
      "Epoch [1/2], Step [36230/64305], Loss: 4.8333\n",
      "Epoch [1/2], Step [36240/64305], Loss: 5.0215\n",
      "Epoch [1/2], Step [36250/64305], Loss: 4.9113\n",
      "Epoch [1/2], Step [36260/64305], Loss: 5.0020\n",
      "Epoch [1/2], Step [36270/64305], Loss: 4.9619\n",
      "Epoch [1/2], Step [36280/64305], Loss: 5.1851\n",
      "Epoch [1/2], Step [36290/64305], Loss: 4.8001\n",
      "Epoch [1/2], Step [36300/64305], Loss: 4.8517\n",
      "Epoch [1/2], Step [36310/64305], Loss: 4.9750\n",
      "Epoch [1/2], Step [36320/64305], Loss: 5.0745\n",
      "Epoch [1/2], Step [36330/64305], Loss: 5.1250\n",
      "Epoch [1/2], Step [36340/64305], Loss: 4.9226\n",
      "Epoch [1/2], Step [36350/64305], Loss: 4.9919\n",
      "Epoch [1/2], Step [36360/64305], Loss: 5.0149\n",
      "Epoch [1/2], Step [36370/64305], Loss: 5.0401\n",
      "Epoch [1/2], Step [36380/64305], Loss: 4.9092\n",
      "Epoch [1/2], Step [36390/64305], Loss: 5.0330\n",
      "Epoch [1/2], Step [36400/64305], Loss: 4.8760\n",
      "Epoch [1/2], Step [36410/64305], Loss: 4.9583\n",
      "Epoch [1/2], Step [36420/64305], Loss: 4.8772\n",
      "Epoch [1/2], Step [36430/64305], Loss: 5.0087\n",
      "Epoch [1/2], Step [36440/64305], Loss: 4.8667\n",
      "Epoch [1/2], Step [36450/64305], Loss: 5.0471\n",
      "Epoch [1/2], Step [36460/64305], Loss: 5.0262\n",
      "Epoch [1/2], Step [36470/64305], Loss: 4.9275\n",
      "Epoch [1/2], Step [36480/64305], Loss: 5.0106\n",
      "Epoch [1/2], Step [36490/64305], Loss: 4.9894\n",
      "Epoch [1/2], Step [36500/64305], Loss: 4.9764\n",
      "Epoch [1/2], Step [36510/64305], Loss: 5.1845\n",
      "Epoch [1/2], Step [36520/64305], Loss: 5.0946\n",
      "Epoch [1/2], Step [36530/64305], Loss: 5.0830\n",
      "Epoch [1/2], Step [36540/64305], Loss: 5.0836\n",
      "Epoch [1/2], Step [36550/64305], Loss: 5.0515\n",
      "Epoch [1/2], Step [36560/64305], Loss: 5.0365\n",
      "Epoch [1/2], Step [36570/64305], Loss: 4.9565\n",
      "Epoch [1/2], Step [36580/64305], Loss: 4.9290\n",
      "Epoch [1/2], Step [36590/64305], Loss: 5.0650\n",
      "Epoch [1/2], Step [36600/64305], Loss: 4.7570\n",
      "Epoch [1/2], Step [36610/64305], Loss: 4.9203\n",
      "Epoch [1/2], Step [36620/64305], Loss: 4.8607\n",
      "Epoch [1/2], Step [36630/64305], Loss: 4.8804\n",
      "Epoch [1/2], Step [36640/64305], Loss: 4.7922\n",
      "Epoch [1/2], Step [36650/64305], Loss: 4.9616\n",
      "Epoch [1/2], Step [36660/64305], Loss: 5.0152\n",
      "Epoch [1/2], Step [36670/64305], Loss: 5.1457\n",
      "Epoch [1/2], Step [36680/64305], Loss: 4.6744\n",
      "Epoch [1/2], Step [36690/64305], Loss: 4.9412\n",
      "Epoch [1/2], Step [36700/64305], Loss: 4.8662\n",
      "Epoch [1/2], Step [36710/64305], Loss: 4.9713\n",
      "Epoch [1/2], Step [36720/64305], Loss: 5.0842\n",
      "Epoch [1/2], Step [36730/64305], Loss: 4.9555\n",
      "Epoch [1/2], Step [36740/64305], Loss: 4.9268\n",
      "Epoch [1/2], Step [36750/64305], Loss: 4.8130\n",
      "Epoch [1/2], Step [36760/64305], Loss: 5.0257\n",
      "Epoch [1/2], Step [36770/64305], Loss: 5.1181\n",
      "Epoch [1/2], Step [36780/64305], Loss: 5.0464\n",
      "Epoch [1/2], Step [36790/64305], Loss: 4.9965\n",
      "Epoch [1/2], Step [36800/64305], Loss: 4.8301\n",
      "Epoch [1/2], Step [36810/64305], Loss: 5.0997\n",
      "Epoch [1/2], Step [36820/64305], Loss: 4.9085\n",
      "Epoch [1/2], Step [36830/64305], Loss: 4.9853\n",
      "Epoch [1/2], Step [36840/64305], Loss: 5.0220\n",
      "Epoch [1/2], Step [36850/64305], Loss: 5.1016\n",
      "Epoch [1/2], Step [36860/64305], Loss: 4.9206\n",
      "Epoch [1/2], Step [36870/64305], Loss: 4.7955\n",
      "Epoch [1/2], Step [36880/64305], Loss: 4.9504\n",
      "Epoch [1/2], Step [36890/64305], Loss: 4.6811\n",
      "Epoch [1/2], Step [36900/64305], Loss: 5.2214\n",
      "Epoch [1/2], Step [36910/64305], Loss: 4.8206\n",
      "Epoch [1/2], Step [36920/64305], Loss: 4.8248\n",
      "Epoch [1/2], Step [36930/64305], Loss: 4.8209\n",
      "Epoch [1/2], Step [36940/64305], Loss: 4.9566\n",
      "Epoch [1/2], Step [36950/64305], Loss: 4.6665\n",
      "Epoch [1/2], Step [36960/64305], Loss: 5.1255\n",
      "Epoch [1/2], Step [36970/64305], Loss: 4.9586\n",
      "Epoch [1/2], Step [36980/64305], Loss: 5.0533\n",
      "Epoch [1/2], Step [36990/64305], Loss: 4.6851\n",
      "Epoch [1/2], Step [37000/64305], Loss: 5.0223\n",
      "Epoch [1/2], Step [37010/64305], Loss: 4.7546\n",
      "Epoch [1/2], Step [37020/64305], Loss: 4.8333\n",
      "Epoch [1/2], Step [37030/64305], Loss: 4.9338\n",
      "Epoch [1/2], Step [37040/64305], Loss: 4.8608\n",
      "Epoch [1/2], Step [37050/64305], Loss: 4.8128\n",
      "Epoch [1/2], Step [37060/64305], Loss: 4.9771\n",
      "Epoch [1/2], Step [37070/64305], Loss: 4.9477\n",
      "Epoch [1/2], Step [37080/64305], Loss: 5.0075\n",
      "Epoch [1/2], Step [37090/64305], Loss: 5.0993\n",
      "Epoch [1/2], Step [37100/64305], Loss: 5.0326\n",
      "Epoch [1/2], Step [37110/64305], Loss: 5.0310\n",
      "Epoch [1/2], Step [37120/64305], Loss: 5.2162\n",
      "Epoch [1/2], Step [37130/64305], Loss: 5.0397\n",
      "Epoch [1/2], Step [37140/64305], Loss: 5.1176\n",
      "Epoch [1/2], Step [37150/64305], Loss: 4.9886\n",
      "Epoch [1/2], Step [37160/64305], Loss: 4.8932\n",
      "Epoch [1/2], Step [37170/64305], Loss: 4.8441\n",
      "Epoch [1/2], Step [37180/64305], Loss: 4.8832\n",
      "Epoch [1/2], Step [37190/64305], Loss: 4.8558\n",
      "Epoch [1/2], Step [37200/64305], Loss: 4.8154\n",
      "Epoch [1/2], Step [37210/64305], Loss: 4.8339\n",
      "Epoch [1/2], Step [37220/64305], Loss: 4.9185\n",
      "Epoch [1/2], Step [37230/64305], Loss: 4.9604\n",
      "Epoch [1/2], Step [37240/64305], Loss: 4.8464\n",
      "Epoch [1/2], Step [37250/64305], Loss: 4.7765\n",
      "Epoch [1/2], Step [37260/64305], Loss: 4.7176\n",
      "Epoch [1/2], Step [37270/64305], Loss: 4.8271\n",
      "Epoch [1/2], Step [37280/64305], Loss: 5.1665\n",
      "Epoch [1/2], Step [37290/64305], Loss: 4.8892\n",
      "Epoch [1/2], Step [37300/64305], Loss: 4.9306\n",
      "Epoch [1/2], Step [37310/64305], Loss: 4.9385\n",
      "Epoch [1/2], Step [37320/64305], Loss: 4.9091\n",
      "Epoch [1/2], Step [37330/64305], Loss: 4.9771\n",
      "Epoch [1/2], Step [37340/64305], Loss: 4.8092\n",
      "Epoch [1/2], Step [37350/64305], Loss: 5.1079\n",
      "Epoch [1/2], Step [37360/64305], Loss: 5.0531\n",
      "Epoch [1/2], Step [37370/64305], Loss: 5.0855\n",
      "Epoch [1/2], Step [37380/64305], Loss: 4.7093\n",
      "Epoch [1/2], Step [37390/64305], Loss: 5.0159\n",
      "Epoch [1/2], Step [37400/64305], Loss: 5.0548\n",
      "Epoch [1/2], Step [37410/64305], Loss: 4.8870\n",
      "Epoch [1/2], Step [37420/64305], Loss: 4.8620\n",
      "Epoch [1/2], Step [37430/64305], Loss: 4.8243\n",
      "Epoch [1/2], Step [37440/64305], Loss: 4.7295\n",
      "Epoch [1/2], Step [37450/64305], Loss: 4.8802\n",
      "Epoch [1/2], Step [37460/64305], Loss: 4.9647\n",
      "Epoch [1/2], Step [37470/64305], Loss: 4.8388\n",
      "Epoch [1/2], Step [37480/64305], Loss: 5.0727\n",
      "Epoch [1/2], Step [37490/64305], Loss: 4.9135\n",
      "Epoch [1/2], Step [37500/64305], Loss: 4.8908\n",
      "Epoch [1/2], Step [37510/64305], Loss: 4.9844\n",
      "Epoch [1/2], Step [37520/64305], Loss: 4.9583\n",
      "Epoch [1/2], Step [37530/64305], Loss: 5.0296\n",
      "Epoch [1/2], Step [37540/64305], Loss: 4.8401\n",
      "Epoch [1/2], Step [37550/64305], Loss: 4.8933\n",
      "Epoch [1/2], Step [37560/64305], Loss: 4.9596\n",
      "Epoch [1/2], Step [37570/64305], Loss: 4.8213\n",
      "Epoch [1/2], Step [37580/64305], Loss: 4.8626\n",
      "Epoch [1/2], Step [37590/64305], Loss: 5.0556\n",
      "Epoch [1/2], Step [37600/64305], Loss: 4.8835\n",
      "Epoch [1/2], Step [37610/64305], Loss: 4.8446\n",
      "Epoch [1/2], Step [37620/64305], Loss: 4.9545\n",
      "Epoch [1/2], Step [37630/64305], Loss: 5.1579\n",
      "Epoch [1/2], Step [37640/64305], Loss: 4.8137\n",
      "Epoch [1/2], Step [37650/64305], Loss: 4.9844\n",
      "Epoch [1/2], Step [37660/64305], Loss: 4.7797\n",
      "Epoch [1/2], Step [37670/64305], Loss: 5.1318\n",
      "Epoch [1/2], Step [37680/64305], Loss: 5.0939\n",
      "Epoch [1/2], Step [37690/64305], Loss: 5.0661\n",
      "Epoch [1/2], Step [37700/64305], Loss: 5.1044\n",
      "Epoch [1/2], Step [37710/64305], Loss: 4.8749\n",
      "Epoch [1/2], Step [37720/64305], Loss: 4.7824\n",
      "Epoch [1/2], Step [37730/64305], Loss: 4.9467\n",
      "Epoch [1/2], Step [37740/64305], Loss: 4.7977\n",
      "Epoch [1/2], Step [37750/64305], Loss: 4.8743\n",
      "Epoch [1/2], Step [37760/64305], Loss: 4.8938\n",
      "Epoch [1/2], Step [37770/64305], Loss: 5.0044\n",
      "Epoch [1/2], Step [37780/64305], Loss: 5.1028\n",
      "Epoch [1/2], Step [37790/64305], Loss: 4.9797\n",
      "Epoch [1/2], Step [37800/64305], Loss: 4.9949\n",
      "Epoch [1/2], Step [37810/64305], Loss: 5.0172\n",
      "Epoch [1/2], Step [37820/64305], Loss: 5.0814\n",
      "Epoch [1/2], Step [37830/64305], Loss: 5.0195\n",
      "Epoch [1/2], Step [37840/64305], Loss: 5.1134\n",
      "Epoch [1/2], Step [37850/64305], Loss: 5.1381\n",
      "Epoch [1/2], Step [37860/64305], Loss: 4.8407\n",
      "Epoch [1/2], Step [37870/64305], Loss: 4.8196\n",
      "Epoch [1/2], Step [37880/64305], Loss: 4.9271\n",
      "Epoch [1/2], Step [37890/64305], Loss: 4.8233\n",
      "Epoch [1/2], Step [37900/64305], Loss: 4.9086\n",
      "Epoch [1/2], Step [37910/64305], Loss: 4.7810\n",
      "Epoch [1/2], Step [37920/64305], Loss: 4.9776\n",
      "Epoch [1/2], Step [37930/64305], Loss: 4.9338\n",
      "Epoch [1/2], Step [37940/64305], Loss: 4.9852\n",
      "Epoch [1/2], Step [37950/64305], Loss: 5.0813\n",
      "Epoch [1/2], Step [37960/64305], Loss: 4.7113\n",
      "Epoch [1/2], Step [37970/64305], Loss: 5.0240\n",
      "Epoch [1/2], Step [37980/64305], Loss: 5.0856\n",
      "Epoch [1/2], Step [37990/64305], Loss: 4.8095\n",
      "Epoch [1/2], Step [38000/64305], Loss: 4.9515\n",
      "Epoch [1/2], Step [38010/64305], Loss: 5.0240\n",
      "Epoch [1/2], Step [38020/64305], Loss: 4.9465\n",
      "Epoch [1/2], Step [38030/64305], Loss: 5.0893\n",
      "Epoch [1/2], Step [38040/64305], Loss: 4.8873\n",
      "Epoch [1/2], Step [38050/64305], Loss: 5.0702\n",
      "Epoch [1/2], Step [38060/64305], Loss: 4.9766\n",
      "Epoch [1/2], Step [38070/64305], Loss: 4.8470\n",
      "Epoch [1/2], Step [38080/64305], Loss: 4.8294\n",
      "Epoch [1/2], Step [38090/64305], Loss: 5.0108\n",
      "Epoch [1/2], Step [38100/64305], Loss: 4.8621\n",
      "Epoch [1/2], Step [38110/64305], Loss: 5.2075\n",
      "Epoch [1/2], Step [38120/64305], Loss: 4.8113\n",
      "Epoch [1/2], Step [38130/64305], Loss: 5.1441\n",
      "Epoch [1/2], Step [38140/64305], Loss: 4.8859\n",
      "Epoch [1/2], Step [38150/64305], Loss: 4.6936\n",
      "Epoch [1/2], Step [38160/64305], Loss: 4.9274\n",
      "Epoch [1/2], Step [38170/64305], Loss: 4.9031\n",
      "Epoch [1/2], Step [38180/64305], Loss: 5.1420\n",
      "Epoch [1/2], Step [38190/64305], Loss: 4.9519\n",
      "Epoch [1/2], Step [38200/64305], Loss: 4.8115\n",
      "Epoch [1/2], Step [38210/64305], Loss: 5.0442\n",
      "Epoch [1/2], Step [38220/64305], Loss: 4.9434\n",
      "Epoch [1/2], Step [38230/64305], Loss: 5.1916\n",
      "Epoch [1/2], Step [38240/64305], Loss: 4.7981\n",
      "Epoch [1/2], Step [38250/64305], Loss: 4.7971\n",
      "Epoch [1/2], Step [38260/64305], Loss: 4.9700\n",
      "Epoch [1/2], Step [38270/64305], Loss: 4.7410\n",
      "Epoch [1/2], Step [38280/64305], Loss: 4.9689\n",
      "Epoch [1/2], Step [38290/64305], Loss: 5.0513\n",
      "Epoch [1/2], Step [38300/64305], Loss: 4.9548\n",
      "Epoch [1/2], Step [38310/64305], Loss: 5.1465\n",
      "Epoch [1/2], Step [38320/64305], Loss: 4.9685\n",
      "Epoch [1/2], Step [38330/64305], Loss: 5.0248\n",
      "Epoch [1/2], Step [38340/64305], Loss: 4.8771\n",
      "Epoch [1/2], Step [38350/64305], Loss: 4.7997\n",
      "Epoch [1/2], Step [38360/64305], Loss: 4.8297\n",
      "Epoch [1/2], Step [38370/64305], Loss: 5.0175\n",
      "Epoch [1/2], Step [38380/64305], Loss: 5.0669\n",
      "Epoch [1/2], Step [38390/64305], Loss: 4.9462\n",
      "Epoch [1/2], Step [38400/64305], Loss: 5.0807\n",
      "Epoch [1/2], Step [38410/64305], Loss: 4.8678\n",
      "Epoch [1/2], Step [38420/64305], Loss: 4.9514\n",
      "Epoch [1/2], Step [38430/64305], Loss: 4.9758\n",
      "Epoch [1/2], Step [38440/64305], Loss: 4.8143\n",
      "Epoch [1/2], Step [38450/64305], Loss: 4.8866\n",
      "Epoch [1/2], Step [38460/64305], Loss: 4.7821\n",
      "Epoch [1/2], Step [38470/64305], Loss: 4.9465\n",
      "Epoch [1/2], Step [38480/64305], Loss: 5.0336\n",
      "Epoch [1/2], Step [38490/64305], Loss: 4.8330\n",
      "Epoch [1/2], Step [38500/64305], Loss: 4.9761\n",
      "Epoch [1/2], Step [38510/64305], Loss: 4.8058\n",
      "Epoch [1/2], Step [38520/64305], Loss: 4.6784\n",
      "Epoch [1/2], Step [38530/64305], Loss: 4.9005\n",
      "Epoch [1/2], Step [38540/64305], Loss: 4.8275\n",
      "Epoch [1/2], Step [38550/64305], Loss: 5.1023\n",
      "Epoch [1/2], Step [38560/64305], Loss: 5.0012\n",
      "Epoch [1/2], Step [38570/64305], Loss: 4.9296\n",
      "Epoch [1/2], Step [38580/64305], Loss: 4.9126\n",
      "Epoch [1/2], Step [38590/64305], Loss: 4.9853\n",
      "Epoch [1/2], Step [38600/64305], Loss: 4.7656\n",
      "Epoch [1/2], Step [38610/64305], Loss: 4.7643\n",
      "Epoch [1/2], Step [38620/64305], Loss: 4.9007\n",
      "Epoch [1/2], Step [38630/64305], Loss: 4.7968\n",
      "Epoch [1/2], Step [38640/64305], Loss: 4.9919\n",
      "Epoch [1/2], Step [38650/64305], Loss: 5.1400\n",
      "Epoch [1/2], Step [38660/64305], Loss: 4.9704\n",
      "Epoch [1/2], Step [38670/64305], Loss: 4.8517\n",
      "Epoch [1/2], Step [38680/64305], Loss: 5.0645\n",
      "Epoch [1/2], Step [38690/64305], Loss: 4.8697\n",
      "Epoch [1/2], Step [38700/64305], Loss: 5.0381\n",
      "Epoch [1/2], Step [38710/64305], Loss: 5.0591\n",
      "Epoch [1/2], Step [38720/64305], Loss: 4.8403\n",
      "Epoch [1/2], Step [38730/64305], Loss: 4.9983\n",
      "Epoch [1/2], Step [38740/64305], Loss: 4.8787\n",
      "Epoch [1/2], Step [38750/64305], Loss: 4.9950\n",
      "Epoch [1/2], Step [38760/64305], Loss: 4.8319\n",
      "Epoch [1/2], Step [38770/64305], Loss: 5.0227\n",
      "Epoch [1/2], Step [38780/64305], Loss: 5.1566\n",
      "Epoch [1/2], Step [38790/64305], Loss: 4.8072\n",
      "Epoch [1/2], Step [38800/64305], Loss: 4.9932\n",
      "Epoch [1/2], Step [38810/64305], Loss: 4.8297\n",
      "Epoch [1/2], Step [38820/64305], Loss: 4.8871\n",
      "Epoch [1/2], Step [38830/64305], Loss: 5.1288\n",
      "Epoch [1/2], Step [38840/64305], Loss: 5.0599\n",
      "Epoch [1/2], Step [38850/64305], Loss: 4.9627\n",
      "Epoch [1/2], Step [38860/64305], Loss: 4.8151\n",
      "Epoch [1/2], Step [38870/64305], Loss: 4.8228\n",
      "Epoch [1/2], Step [38880/64305], Loss: 4.8721\n",
      "Epoch [1/2], Step [38890/64305], Loss: 5.0452\n",
      "Epoch [1/2], Step [38900/64305], Loss: 4.8536\n",
      "Epoch [1/2], Step [38910/64305], Loss: 4.9137\n",
      "Epoch [1/2], Step [38920/64305], Loss: 5.1652\n",
      "Epoch [1/2], Step [38930/64305], Loss: 4.9400\n",
      "Epoch [1/2], Step [38940/64305], Loss: 4.9148\n",
      "Epoch [1/2], Step [38950/64305], Loss: 4.7959\n",
      "Epoch [1/2], Step [38960/64305], Loss: 4.7635\n",
      "Epoch [1/2], Step [38970/64305], Loss: 5.0760\n",
      "Epoch [1/2], Step [38980/64305], Loss: 4.8834\n",
      "Epoch [1/2], Step [38990/64305], Loss: 4.8939\n",
      "Epoch [1/2], Step [39000/64305], Loss: 4.7982\n",
      "Epoch [1/2], Step [39010/64305], Loss: 4.8878\n",
      "Epoch [1/2], Step [39020/64305], Loss: 4.9307\n",
      "Epoch [1/2], Step [39030/64305], Loss: 5.0280\n",
      "Epoch [1/2], Step [39040/64305], Loss: 4.8853\n",
      "Epoch [1/2], Step [39050/64305], Loss: 5.1828\n",
      "Epoch [1/2], Step [39060/64305], Loss: 4.9867\n",
      "Epoch [1/2], Step [39070/64305], Loss: 4.9290\n",
      "Epoch [1/2], Step [39080/64305], Loss: 4.7981\n",
      "Epoch [1/2], Step [39090/64305], Loss: 4.6683\n",
      "Epoch [1/2], Step [39100/64305], Loss: 4.9158\n",
      "Epoch [1/2], Step [39110/64305], Loss: 4.9251\n",
      "Epoch [1/2], Step [39120/64305], Loss: 5.1461\n",
      "Epoch [1/2], Step [39130/64305], Loss: 4.7656\n",
      "Epoch [1/2], Step [39140/64305], Loss: 5.0099\n",
      "Epoch [1/2], Step [39150/64305], Loss: 4.9724\n",
      "Epoch [1/2], Step [39160/64305], Loss: 5.0697\n",
      "Epoch [1/2], Step [39170/64305], Loss: 5.1371\n",
      "Epoch [1/2], Step [39180/64305], Loss: 4.9205\n",
      "Epoch [1/2], Step [39190/64305], Loss: 4.8537\n",
      "Epoch [1/2], Step [39200/64305], Loss: 4.8273\n",
      "Epoch [1/2], Step [39210/64305], Loss: 4.7415\n",
      "Epoch [1/2], Step [39220/64305], Loss: 4.9337\n",
      "Epoch [1/2], Step [39230/64305], Loss: 4.8733\n",
      "Epoch [1/2], Step [39240/64305], Loss: 4.9008\n",
      "Epoch [1/2], Step [39250/64305], Loss: 4.9573\n",
      "Epoch [1/2], Step [39260/64305], Loss: 4.8093\n",
      "Epoch [1/2], Step [39270/64305], Loss: 4.9699\n",
      "Epoch [1/2], Step [39280/64305], Loss: 4.9516\n",
      "Epoch [1/2], Step [39290/64305], Loss: 4.8062\n",
      "Epoch [1/2], Step [39300/64305], Loss: 5.0715\n",
      "Epoch [1/2], Step [39310/64305], Loss: 4.9604\n",
      "Epoch [1/2], Step [39320/64305], Loss: 4.8148\n",
      "Epoch [1/2], Step [39330/64305], Loss: 5.0278\n",
      "Epoch [1/2], Step [39340/64305], Loss: 5.0895\n",
      "Epoch [1/2], Step [39350/64305], Loss: 4.8594\n",
      "Epoch [1/2], Step [39360/64305], Loss: 4.7765\n",
      "Epoch [1/2], Step [39370/64305], Loss: 5.0115\n",
      "Epoch [1/2], Step [39380/64305], Loss: 4.9291\n",
      "Epoch [1/2], Step [39390/64305], Loss: 4.9524\n",
      "Epoch [1/2], Step [39400/64305], Loss: 4.7810\n",
      "Epoch [1/2], Step [39410/64305], Loss: 5.0559\n",
      "Epoch [1/2], Step [39420/64305], Loss: 4.8593\n",
      "Epoch [1/2], Step [39430/64305], Loss: 4.7097\n",
      "Epoch [1/2], Step [39440/64305], Loss: 4.7943\n",
      "Epoch [1/2], Step [39450/64305], Loss: 4.7887\n",
      "Epoch [1/2], Step [39460/64305], Loss: 4.8701\n",
      "Epoch [1/2], Step [39470/64305], Loss: 4.8646\n",
      "Epoch [1/2], Step [39480/64305], Loss: 4.9295\n",
      "Epoch [1/2], Step [39490/64305], Loss: 4.8648\n",
      "Epoch [1/2], Step [39500/64305], Loss: 5.0651\n",
      "Epoch [1/2], Step [39510/64305], Loss: 4.9545\n",
      "Epoch [1/2], Step [39520/64305], Loss: 4.9756\n",
      "Epoch [1/2], Step [39530/64305], Loss: 5.0332\n",
      "Epoch [1/2], Step [39540/64305], Loss: 5.1507\n",
      "Epoch [1/2], Step [39550/64305], Loss: 5.1145\n",
      "Epoch [1/2], Step [39560/64305], Loss: 4.9020\n",
      "Epoch [1/2], Step [39570/64305], Loss: 4.9395\n",
      "Epoch [1/2], Step [39580/64305], Loss: 5.0843\n",
      "Epoch [1/2], Step [39590/64305], Loss: 4.8349\n",
      "Epoch [1/2], Step [39600/64305], Loss: 5.0882\n",
      "Epoch [1/2], Step [39610/64305], Loss: 4.8339\n",
      "Epoch [1/2], Step [39620/64305], Loss: 4.9069\n",
      "Epoch [1/2], Step [39630/64305], Loss: 4.8794\n",
      "Epoch [1/2], Step [39640/64305], Loss: 5.0833\n",
      "Epoch [1/2], Step [39650/64305], Loss: 4.6768\n",
      "Epoch [1/2], Step [39660/64305], Loss: 5.0712\n",
      "Epoch [1/2], Step [39670/64305], Loss: 4.9958\n",
      "Epoch [1/2], Step [39680/64305], Loss: 4.9398\n",
      "Epoch [1/2], Step [39690/64305], Loss: 4.9645\n",
      "Epoch [1/2], Step [39700/64305], Loss: 4.9667\n",
      "Epoch [1/2], Step [39710/64305], Loss: 5.1426\n",
      "Epoch [1/2], Step [39720/64305], Loss: 5.0723\n",
      "Epoch [1/2], Step [39730/64305], Loss: 5.0710\n",
      "Epoch [1/2], Step [39740/64305], Loss: 4.9374\n",
      "Epoch [1/2], Step [39750/64305], Loss: 5.0241\n",
      "Epoch [1/2], Step [39760/64305], Loss: 4.9096\n",
      "Epoch [1/2], Step [39770/64305], Loss: 4.7907\n",
      "Epoch [1/2], Step [39780/64305], Loss: 4.8708\n",
      "Epoch [1/2], Step [39790/64305], Loss: 5.0276\n",
      "Epoch [1/2], Step [39800/64305], Loss: 4.9179\n",
      "Epoch [1/2], Step [39810/64305], Loss: 4.9096\n",
      "Epoch [1/2], Step [39820/64305], Loss: 4.7848\n",
      "Epoch [1/2], Step [39830/64305], Loss: 4.9751\n",
      "Epoch [1/2], Step [39840/64305], Loss: 4.8635\n",
      "Epoch [1/2], Step [39850/64305], Loss: 4.9537\n",
      "Epoch [1/2], Step [39860/64305], Loss: 4.9124\n",
      "Epoch [1/2], Step [39870/64305], Loss: 4.8246\n",
      "Epoch [1/2], Step [39880/64305], Loss: 5.1037\n",
      "Epoch [1/2], Step [39890/64305], Loss: 4.9593\n",
      "Epoch [1/2], Step [39900/64305], Loss: 4.9466\n",
      "Epoch [1/2], Step [39910/64305], Loss: 4.8086\n",
      "Epoch [1/2], Step [39920/64305], Loss: 4.9564\n",
      "Epoch [1/2], Step [39930/64305], Loss: 4.7420\n",
      "Epoch [1/2], Step [39940/64305], Loss: 5.0269\n",
      "Epoch [1/2], Step [39950/64305], Loss: 4.8973\n",
      "Epoch [1/2], Step [39960/64305], Loss: 4.8927\n",
      "Epoch [1/2], Step [39970/64305], Loss: 5.0746\n",
      "Epoch [1/2], Step [39980/64305], Loss: 5.0748\n",
      "Epoch [1/2], Step [39990/64305], Loss: 5.0450\n",
      "Epoch [1/2], Step [40000/64305], Loss: 4.8040\n",
      "Epoch [1/2], Step [40010/64305], Loss: 5.0625\n",
      "Epoch [1/2], Step [40020/64305], Loss: 5.0436\n",
      "Epoch [1/2], Step [40030/64305], Loss: 4.8974\n",
      "Epoch [1/2], Step [40040/64305], Loss: 4.8958\n",
      "Epoch [1/2], Step [40050/64305], Loss: 4.8296\n",
      "Epoch [1/2], Step [40060/64305], Loss: 4.9442\n",
      "Epoch [1/2], Step [40070/64305], Loss: 4.7867\n",
      "Epoch [1/2], Step [40080/64305], Loss: 4.8314\n",
      "Epoch [1/2], Step [40090/64305], Loss: 4.6642\n",
      "Epoch [1/2], Step [40100/64305], Loss: 4.8582\n",
      "Epoch [1/2], Step [40110/64305], Loss: 5.0893\n",
      "Epoch [1/2], Step [40120/64305], Loss: 4.7412\n",
      "Epoch [1/2], Step [40130/64305], Loss: 4.9259\n",
      "Epoch [1/2], Step [40140/64305], Loss: 4.8438\n",
      "Epoch [1/2], Step [40150/64305], Loss: 4.5047\n",
      "Epoch [1/2], Step [40160/64305], Loss: 5.0125\n",
      "Epoch [1/2], Step [40170/64305], Loss: 4.8479\n",
      "Epoch [1/2], Step [40180/64305], Loss: 4.9472\n",
      "Epoch [1/2], Step [40190/64305], Loss: 4.6879\n",
      "Epoch [1/2], Step [40200/64305], Loss: 4.9778\n",
      "Epoch [1/2], Step [40210/64305], Loss: 5.0542\n",
      "Epoch [1/2], Step [40220/64305], Loss: 4.8138\n",
      "Epoch [1/2], Step [40230/64305], Loss: 4.9048\n",
      "Epoch [1/2], Step [40240/64305], Loss: 4.6596\n",
      "Epoch [1/2], Step [40250/64305], Loss: 4.9266\n",
      "Epoch [1/2], Step [40260/64305], Loss: 4.6994\n",
      "Epoch [1/2], Step [40270/64305], Loss: 4.7460\n",
      "Epoch [1/2], Step [40280/64305], Loss: 4.9624\n",
      "Epoch [1/2], Step [40290/64305], Loss: 4.9169\n",
      "Epoch [1/2], Step [40300/64305], Loss: 4.9360\n",
      "Epoch [1/2], Step [40310/64305], Loss: 4.9907\n",
      "Epoch [1/2], Step [40320/64305], Loss: 5.0669\n",
      "Epoch [1/2], Step [40330/64305], Loss: 4.9746\n",
      "Epoch [1/2], Step [40340/64305], Loss: 4.8032\n",
      "Epoch [1/2], Step [40350/64305], Loss: 5.0236\n",
      "Epoch [1/2], Step [40360/64305], Loss: 4.7501\n",
      "Epoch [1/2], Step [40370/64305], Loss: 4.9986\n",
      "Epoch [1/2], Step [40380/64305], Loss: 5.1839\n",
      "Epoch [1/2], Step [40390/64305], Loss: 4.9760\n",
      "Epoch [1/2], Step [40400/64305], Loss: 5.0316\n",
      "Epoch [1/2], Step [40410/64305], Loss: 5.0021\n",
      "Epoch [1/2], Step [40420/64305], Loss: 4.8724\n",
      "Epoch [1/2], Step [40430/64305], Loss: 4.8454\n",
      "Epoch [1/2], Step [40440/64305], Loss: 4.8039\n",
      "Epoch [1/2], Step [40450/64305], Loss: 4.9373\n",
      "Epoch [1/2], Step [40460/64305], Loss: 5.1419\n",
      "Epoch [1/2], Step [40470/64305], Loss: 5.0010\n",
      "Epoch [1/2], Step [40480/64305], Loss: 5.0144\n",
      "Epoch [1/2], Step [40490/64305], Loss: 4.7264\n",
      "Epoch [1/2], Step [40500/64305], Loss: 4.8386\n",
      "Epoch [1/2], Step [40510/64305], Loss: 4.8039\n",
      "Epoch [1/2], Step [40520/64305], Loss: 4.7790\n",
      "Epoch [1/2], Step [40530/64305], Loss: 4.7298\n",
      "Epoch [1/2], Step [40540/64305], Loss: 5.0113\n",
      "Epoch [1/2], Step [40550/64305], Loss: 4.7763\n",
      "Epoch [1/2], Step [40560/64305], Loss: 4.9945\n",
      "Epoch [1/2], Step [40570/64305], Loss: 4.9949\n",
      "Epoch [1/2], Step [40580/64305], Loss: 4.8079\n",
      "Epoch [1/2], Step [40590/64305], Loss: 4.8866\n",
      "Epoch [1/2], Step [40600/64305], Loss: 4.7435\n",
      "Epoch [1/2], Step [40610/64305], Loss: 4.9307\n",
      "Epoch [1/2], Step [40620/64305], Loss: 5.0592\n",
      "Epoch [1/2], Step [40630/64305], Loss: 5.1393\n",
      "Epoch [1/2], Step [40640/64305], Loss: 4.9462\n",
      "Epoch [1/2], Step [40650/64305], Loss: 4.7574\n",
      "Epoch [1/2], Step [40660/64305], Loss: 4.8819\n",
      "Epoch [1/2], Step [40670/64305], Loss: 4.9421\n",
      "Epoch [1/2], Step [40680/64305], Loss: 4.7884\n",
      "Epoch [1/2], Step [40690/64305], Loss: 4.8790\n",
      "Epoch [1/2], Step [40700/64305], Loss: 4.8259\n",
      "Epoch [1/2], Step [40710/64305], Loss: 5.0573\n",
      "Epoch [1/2], Step [40720/64305], Loss: 5.0090\n",
      "Epoch [1/2], Step [40730/64305], Loss: 4.8198\n",
      "Epoch [1/2], Step [40740/64305], Loss: 4.9676\n",
      "Epoch [1/2], Step [40750/64305], Loss: 4.9625\n",
      "Epoch [1/2], Step [40760/64305], Loss: 4.8010\n",
      "Epoch [1/2], Step [40770/64305], Loss: 4.8675\n",
      "Epoch [1/2], Step [40780/64305], Loss: 4.7965\n",
      "Epoch [1/2], Step [40790/64305], Loss: 4.8885\n",
      "Epoch [1/2], Step [40800/64305], Loss: 4.9502\n",
      "Epoch [1/2], Step [40810/64305], Loss: 4.7884\n",
      "Epoch [1/2], Step [40820/64305], Loss: 4.8934\n",
      "Epoch [1/2], Step [40830/64305], Loss: 4.9608\n",
      "Epoch [1/2], Step [40840/64305], Loss: 4.7693\n",
      "Epoch [1/2], Step [40850/64305], Loss: 4.7179\n",
      "Epoch [1/2], Step [40860/64305], Loss: 5.2017\n",
      "Epoch [1/2], Step [40870/64305], Loss: 4.8948\n",
      "Epoch [1/2], Step [40880/64305], Loss: 4.9665\n",
      "Epoch [1/2], Step [40890/64305], Loss: 4.7921\n",
      "Epoch [1/2], Step [40900/64305], Loss: 4.9920\n",
      "Epoch [1/2], Step [40910/64305], Loss: 5.0000\n",
      "Epoch [1/2], Step [40920/64305], Loss: 4.8886\n",
      "Epoch [1/2], Step [40930/64305], Loss: 5.0166\n",
      "Epoch [1/2], Step [40940/64305], Loss: 4.9253\n",
      "Epoch [1/2], Step [40950/64305], Loss: 4.9918\n",
      "Epoch [1/2], Step [40960/64305], Loss: 4.7203\n",
      "Epoch [1/2], Step [40970/64305], Loss: 4.8381\n",
      "Epoch [1/2], Step [40980/64305], Loss: 4.7140\n",
      "Epoch [1/2], Step [40990/64305], Loss: 4.9161\n",
      "Epoch [1/2], Step [41000/64305], Loss: 4.8689\n",
      "Epoch [1/2], Step [41010/64305], Loss: 4.9387\n",
      "Epoch [1/2], Step [41020/64305], Loss: 5.0892\n",
      "Epoch [1/2], Step [41030/64305], Loss: 4.8289\n",
      "Epoch [1/2], Step [41040/64305], Loss: 5.0626\n",
      "Epoch [1/2], Step [41050/64305], Loss: 5.0270\n",
      "Epoch [1/2], Step [41060/64305], Loss: 4.9834\n",
      "Epoch [1/2], Step [41070/64305], Loss: 4.9645\n",
      "Epoch [1/2], Step [41080/64305], Loss: 4.8395\n",
      "Epoch [1/2], Step [41090/64305], Loss: 5.1275\n",
      "Epoch [1/2], Step [41100/64305], Loss: 4.8603\n",
      "Epoch [1/2], Step [41110/64305], Loss: 4.9245\n",
      "Epoch [1/2], Step [41120/64305], Loss: 4.9945\n",
      "Epoch [1/2], Step [41130/64305], Loss: 4.9968\n",
      "Epoch [1/2], Step [41140/64305], Loss: 5.2002\n",
      "Epoch [1/2], Step [41150/64305], Loss: 4.6145\n",
      "Epoch [1/2], Step [41160/64305], Loss: 5.0332\n",
      "Epoch [1/2], Step [41170/64305], Loss: 4.8996\n",
      "Epoch [1/2], Step [41180/64305], Loss: 4.7807\n",
      "Epoch [1/2], Step [41190/64305], Loss: 4.8736\n",
      "Epoch [1/2], Step [41200/64305], Loss: 4.9075\n",
      "Epoch [1/2], Step [41210/64305], Loss: 5.0901\n",
      "Epoch [1/2], Step [41220/64305], Loss: 4.8682\n",
      "Epoch [1/2], Step [41230/64305], Loss: 4.6572\n",
      "Epoch [1/2], Step [41240/64305], Loss: 4.9552\n",
      "Epoch [1/2], Step [41250/64305], Loss: 5.0334\n",
      "Epoch [1/2], Step [41260/64305], Loss: 4.8686\n",
      "Epoch [1/2], Step [41270/64305], Loss: 4.9123\n",
      "Epoch [1/2], Step [41280/64305], Loss: 5.0760\n",
      "Epoch [1/2], Step [41290/64305], Loss: 4.7706\n",
      "Epoch [1/2], Step [41300/64305], Loss: 4.8474\n",
      "Epoch [1/2], Step [41310/64305], Loss: 5.0959\n",
      "Epoch [1/2], Step [41320/64305], Loss: 5.0135\n",
      "Epoch [1/2], Step [41330/64305], Loss: 4.9261\n",
      "Epoch [1/2], Step [41340/64305], Loss: 4.9222\n",
      "Epoch [1/2], Step [41350/64305], Loss: 4.7635\n",
      "Epoch [1/2], Step [41360/64305], Loss: 4.7897\n",
      "Epoch [1/2], Step [41370/64305], Loss: 4.7848\n",
      "Epoch [1/2], Step [41380/64305], Loss: 4.8923\n",
      "Epoch [1/2], Step [41390/64305], Loss: 4.9031\n",
      "Epoch [1/2], Step [41400/64305], Loss: 4.8928\n",
      "Epoch [1/2], Step [41410/64305], Loss: 4.9403\n",
      "Epoch [1/2], Step [41420/64305], Loss: 4.9334\n",
      "Epoch [1/2], Step [41430/64305], Loss: 4.8442\n",
      "Epoch [1/2], Step [41440/64305], Loss: 4.7097\n",
      "Epoch [1/2], Step [41450/64305], Loss: 5.0111\n",
      "Epoch [1/2], Step [41460/64305], Loss: 5.0373\n",
      "Epoch [1/2], Step [41470/64305], Loss: 4.8273\n",
      "Epoch [1/2], Step [41480/64305], Loss: 5.0037\n",
      "Epoch [1/2], Step [41490/64305], Loss: 4.9337\n",
      "Epoch [1/2], Step [41500/64305], Loss: 4.8926\n",
      "Epoch [1/2], Step [41510/64305], Loss: 4.8427\n",
      "Epoch [1/2], Step [41520/64305], Loss: 4.9712\n",
      "Epoch [1/2], Step [41530/64305], Loss: 4.7746\n",
      "Epoch [1/2], Step [41540/64305], Loss: 4.7076\n",
      "Epoch [1/2], Step [41550/64305], Loss: 4.8897\n",
      "Epoch [1/2], Step [41560/64305], Loss: 5.0309\n",
      "Epoch [1/2], Step [41570/64305], Loss: 4.8552\n",
      "Epoch [1/2], Step [41580/64305], Loss: 4.8792\n",
      "Epoch [1/2], Step [41590/64305], Loss: 4.8812\n",
      "Epoch [1/2], Step [41600/64305], Loss: 4.8188\n",
      "Epoch [1/2], Step [41610/64305], Loss: 4.7983\n",
      "Epoch [1/2], Step [41620/64305], Loss: 4.9296\n",
      "Epoch [1/2], Step [41630/64305], Loss: 4.8476\n",
      "Epoch [1/2], Step [41640/64305], Loss: 4.8352\n",
      "Epoch [1/2], Step [41650/64305], Loss: 4.7988\n",
      "Epoch [1/2], Step [41660/64305], Loss: 4.8197\n",
      "Epoch [1/2], Step [41670/64305], Loss: 4.9760\n",
      "Epoch [1/2], Step [41680/64305], Loss: 4.8130\n",
      "Epoch [1/2], Step [41690/64305], Loss: 4.8528\n",
      "Epoch [1/2], Step [41700/64305], Loss: 4.9939\n",
      "Epoch [1/2], Step [41710/64305], Loss: 4.9595\n",
      "Epoch [1/2], Step [41720/64305], Loss: 4.7582\n",
      "Epoch [1/2], Step [41730/64305], Loss: 5.0280\n",
      "Epoch [1/2], Step [41740/64305], Loss: 4.9797\n",
      "Epoch [1/2], Step [41750/64305], Loss: 4.9242\n",
      "Epoch [1/2], Step [41760/64305], Loss: 4.8972\n",
      "Epoch [1/2], Step [41770/64305], Loss: 4.6680\n",
      "Epoch [1/2], Step [41780/64305], Loss: 4.9793\n",
      "Epoch [1/2], Step [41790/64305], Loss: 4.9557\n",
      "Epoch [1/2], Step [41800/64305], Loss: 4.8182\n",
      "Epoch [1/2], Step [41810/64305], Loss: 5.0183\n",
      "Epoch [1/2], Step [41820/64305], Loss: 4.7725\n",
      "Epoch [1/2], Step [41830/64305], Loss: 5.0244\n",
      "Epoch [1/2], Step [41840/64305], Loss: 4.9568\n",
      "Epoch [1/2], Step [41850/64305], Loss: 4.8045\n",
      "Epoch [1/2], Step [41860/64305], Loss: 5.0298\n",
      "Epoch [1/2], Step [41870/64305], Loss: 4.8355\n",
      "Epoch [1/2], Step [41880/64305], Loss: 4.9506\n",
      "Epoch [1/2], Step [41890/64305], Loss: 4.8457\n",
      "Epoch [1/2], Step [41900/64305], Loss: 5.0510\n",
      "Epoch [1/2], Step [41910/64305], Loss: 4.8626\n",
      "Epoch [1/2], Step [41920/64305], Loss: 5.1081\n",
      "Epoch [1/2], Step [41930/64305], Loss: 4.9016\n",
      "Epoch [1/2], Step [41940/64305], Loss: 4.9219\n",
      "Epoch [1/2], Step [41950/64305], Loss: 4.8754\n",
      "Epoch [1/2], Step [41960/64305], Loss: 4.8667\n",
      "Epoch [1/2], Step [41970/64305], Loss: 4.9430\n",
      "Epoch [1/2], Step [41980/64305], Loss: 5.0388\n",
      "Epoch [1/2], Step [41990/64305], Loss: 4.7871\n",
      "Epoch [1/2], Step [42000/64305], Loss: 5.0047\n",
      "Epoch [1/2], Step [42010/64305], Loss: 4.5785\n",
      "Epoch [1/2], Step [42020/64305], Loss: 4.8927\n",
      "Epoch [1/2], Step [42030/64305], Loss: 4.8163\n",
      "Epoch [1/2], Step [42040/64305], Loss: 5.1049\n",
      "Epoch [1/2], Step [42050/64305], Loss: 4.7984\n",
      "Epoch [1/2], Step [42060/64305], Loss: 5.0824\n",
      "Epoch [1/2], Step [42070/64305], Loss: 4.7451\n",
      "Epoch [1/2], Step [42080/64305], Loss: 4.9340\n",
      "Epoch [1/2], Step [42090/64305], Loss: 5.1335\n",
      "Epoch [1/2], Step [42100/64305], Loss: 4.7682\n",
      "Epoch [1/2], Step [42110/64305], Loss: 4.6434\n",
      "Epoch [1/2], Step [42120/64305], Loss: 4.7842\n",
      "Epoch [1/2], Step [42130/64305], Loss: 4.8672\n",
      "Epoch [1/2], Step [42140/64305], Loss: 5.0091\n",
      "Epoch [1/2], Step [42150/64305], Loss: 5.0445\n",
      "Epoch [1/2], Step [42160/64305], Loss: 5.0029\n",
      "Epoch [1/2], Step [42170/64305], Loss: 4.9918\n",
      "Epoch [1/2], Step [42180/64305], Loss: 4.9135\n",
      "Epoch [1/2], Step [42190/64305], Loss: 4.9012\n",
      "Epoch [1/2], Step [42200/64305], Loss: 4.8889\n",
      "Epoch [1/2], Step [42210/64305], Loss: 4.8545\n",
      "Epoch [1/2], Step [42220/64305], Loss: 4.9637\n",
      "Epoch [1/2], Step [42230/64305], Loss: 4.9938\n",
      "Epoch [1/2], Step [42240/64305], Loss: 4.9794\n",
      "Epoch [1/2], Step [42250/64305], Loss: 4.9120\n",
      "Epoch [1/2], Step [42260/64305], Loss: 4.8846\n",
      "Epoch [1/2], Step [42270/64305], Loss: 4.9371\n",
      "Epoch [1/2], Step [42280/64305], Loss: 4.8825\n",
      "Epoch [1/2], Step [42290/64305], Loss: 5.0147\n",
      "Epoch [1/2], Step [42300/64305], Loss: 4.8197\n",
      "Epoch [1/2], Step [42310/64305], Loss: 5.0332\n",
      "Epoch [1/2], Step [42320/64305], Loss: 4.9155\n",
      "Epoch [1/2], Step [42330/64305], Loss: 5.0531\n",
      "Epoch [1/2], Step [42340/64305], Loss: 5.0431\n",
      "Epoch [1/2], Step [42350/64305], Loss: 4.9003\n",
      "Epoch [1/2], Step [42360/64305], Loss: 4.9078\n",
      "Epoch [1/2], Step [42370/64305], Loss: 5.1326\n",
      "Epoch [1/2], Step [42380/64305], Loss: 5.0404\n",
      "Epoch [1/2], Step [42390/64305], Loss: 4.8742\n",
      "Epoch [1/2], Step [42400/64305], Loss: 4.8551\n",
      "Epoch [1/2], Step [42410/64305], Loss: 4.8897\n",
      "Epoch [1/2], Step [42420/64305], Loss: 4.9539\n",
      "Epoch [1/2], Step [42430/64305], Loss: 4.9668\n",
      "Epoch [1/2], Step [42440/64305], Loss: 4.9256\n",
      "Epoch [1/2], Step [42450/64305], Loss: 4.8973\n",
      "Epoch [1/2], Step [42460/64305], Loss: 4.8688\n",
      "Epoch [1/2], Step [42470/64305], Loss: 5.0805\n",
      "Epoch [1/2], Step [42480/64305], Loss: 5.0778\n",
      "Epoch [1/2], Step [42490/64305], Loss: 4.9759\n",
      "Epoch [1/2], Step [42500/64305], Loss: 4.7070\n",
      "Epoch [1/2], Step [42510/64305], Loss: 4.9490\n",
      "Epoch [1/2], Step [42520/64305], Loss: 4.9609\n",
      "Epoch [1/2], Step [42530/64305], Loss: 4.9681\n",
      "Epoch [1/2], Step [42540/64305], Loss: 5.0107\n",
      "Epoch [1/2], Step [42550/64305], Loss: 5.0712\n",
      "Epoch [1/2], Step [42560/64305], Loss: 5.1468\n",
      "Epoch [1/2], Step [42570/64305], Loss: 4.6983\n",
      "Epoch [1/2], Step [42580/64305], Loss: 4.7227\n",
      "Epoch [1/2], Step [42590/64305], Loss: 4.9865\n",
      "Epoch [1/2], Step [42600/64305], Loss: 5.0682\n",
      "Epoch [1/2], Step [42610/64305], Loss: 4.8972\n",
      "Epoch [1/2], Step [42620/64305], Loss: 4.8633\n",
      "Epoch [1/2], Step [42630/64305], Loss: 4.9513\n",
      "Epoch [1/2], Step [42640/64305], Loss: 4.9791\n",
      "Epoch [1/2], Step [42650/64305], Loss: 4.9388\n",
      "Epoch [1/2], Step [42660/64305], Loss: 4.9902\n",
      "Epoch [1/2], Step [42670/64305], Loss: 5.1034\n",
      "Epoch [1/2], Step [42680/64305], Loss: 4.9390\n",
      "Epoch [1/2], Step [42690/64305], Loss: 5.0925\n",
      "Epoch [1/2], Step [42700/64305], Loss: 4.7586\n",
      "Epoch [1/2], Step [42710/64305], Loss: 4.8684\n",
      "Epoch [1/2], Step [42720/64305], Loss: 4.8086\n",
      "Epoch [1/2], Step [42730/64305], Loss: 4.6101\n",
      "Epoch [1/2], Step [42740/64305], Loss: 4.6117\n",
      "Epoch [1/2], Step [42750/64305], Loss: 4.8264\n",
      "Epoch [1/2], Step [42760/64305], Loss: 5.0406\n",
      "Epoch [1/2], Step [42770/64305], Loss: 5.0431\n",
      "Epoch [1/2], Step [42780/64305], Loss: 4.9063\n",
      "Epoch [1/2], Step [42790/64305], Loss: 5.2043\n",
      "Epoch [1/2], Step [42800/64305], Loss: 5.0065\n",
      "Epoch [1/2], Step [42810/64305], Loss: 4.9058\n",
      "Epoch [1/2], Step [42820/64305], Loss: 4.8740\n",
      "Epoch [1/2], Step [42830/64305], Loss: 4.7707\n",
      "Epoch [1/2], Step [42840/64305], Loss: 4.9646\n",
      "Epoch [1/2], Step [42850/64305], Loss: 5.0310\n",
      "Epoch [1/2], Step [42860/64305], Loss: 4.9168\n",
      "Epoch [1/2], Step [42870/64305], Loss: 5.0373\n",
      "Epoch [1/2], Step [42880/64305], Loss: 5.1278\n",
      "Epoch [1/2], Step [42890/64305], Loss: 5.0166\n",
      "Epoch [1/2], Step [42900/64305], Loss: 4.9387\n",
      "Epoch [1/2], Step [42910/64305], Loss: 4.8598\n",
      "Epoch [1/2], Step [42920/64305], Loss: 5.2260\n",
      "Epoch [1/2], Step [42930/64305], Loss: 4.7697\n",
      "Epoch [1/2], Step [42940/64305], Loss: 4.8433\n",
      "Epoch [1/2], Step [42950/64305], Loss: 4.9212\n",
      "Epoch [1/2], Step [42960/64305], Loss: 4.9878\n",
      "Epoch [1/2], Step [42970/64305], Loss: 4.7693\n",
      "Epoch [1/2], Step [42980/64305], Loss: 4.8784\n",
      "Epoch [1/2], Step [42990/64305], Loss: 5.1370\n",
      "Epoch [1/2], Step [43000/64305], Loss: 5.0716\n",
      "Epoch [1/2], Step [43010/64305], Loss: 4.9125\n",
      "Epoch [1/2], Step [43020/64305], Loss: 4.8118\n",
      "Epoch [1/2], Step [43030/64305], Loss: 4.9936\n",
      "Epoch [1/2], Step [43040/64305], Loss: 5.0170\n",
      "Epoch [1/2], Step [43050/64305], Loss: 4.8807\n",
      "Epoch [1/2], Step [43060/64305], Loss: 4.9064\n",
      "Epoch [1/2], Step [43070/64305], Loss: 4.9951\n",
      "Epoch [1/2], Step [43080/64305], Loss: 4.9498\n",
      "Epoch [1/2], Step [43090/64305], Loss: 5.0096\n",
      "Epoch [1/2], Step [43100/64305], Loss: 4.9293\n",
      "Epoch [1/2], Step [43110/64305], Loss: 5.0115\n",
      "Epoch [1/2], Step [43120/64305], Loss: 4.9169\n",
      "Epoch [1/2], Step [43130/64305], Loss: 4.7878\n",
      "Epoch [1/2], Step [43140/64305], Loss: 5.0663\n",
      "Epoch [1/2], Step [43150/64305], Loss: 5.0078\n",
      "Epoch [1/2], Step [43160/64305], Loss: 4.8615\n",
      "Epoch [1/2], Step [43170/64305], Loss: 4.9315\n",
      "Epoch [1/2], Step [43180/64305], Loss: 4.9305\n",
      "Epoch [1/2], Step [43190/64305], Loss: 4.9484\n",
      "Epoch [1/2], Step [43200/64305], Loss: 4.7278\n",
      "Epoch [1/2], Step [43210/64305], Loss: 4.6454\n",
      "Epoch [1/2], Step [43220/64305], Loss: 4.9363\n",
      "Epoch [1/2], Step [43230/64305], Loss: 5.0291\n",
      "Epoch [1/2], Step [43240/64305], Loss: 4.8776\n",
      "Epoch [1/2], Step [43250/64305], Loss: 5.0388\n",
      "Epoch [1/2], Step [43260/64305], Loss: 4.6542\n",
      "Epoch [1/2], Step [43270/64305], Loss: 5.1521\n",
      "Epoch [1/2], Step [43280/64305], Loss: 4.8655\n",
      "Epoch [1/2], Step [43290/64305], Loss: 4.9456\n",
      "Epoch [1/2], Step [43300/64305], Loss: 4.9464\n",
      "Epoch [1/2], Step [43310/64305], Loss: 5.1685\n",
      "Epoch [1/2], Step [43320/64305], Loss: 4.8283\n",
      "Epoch [1/2], Step [43330/64305], Loss: 4.9353\n",
      "Epoch [1/2], Step [43340/64305], Loss: 4.9118\n",
      "Epoch [1/2], Step [43350/64305], Loss: 5.0435\n",
      "Epoch [1/2], Step [43360/64305], Loss: 4.8917\n",
      "Epoch [1/2], Step [43370/64305], Loss: 4.9914\n",
      "Epoch [1/2], Step [43380/64305], Loss: 4.9770\n",
      "Epoch [1/2], Step [43390/64305], Loss: 4.9897\n",
      "Epoch [1/2], Step [43400/64305], Loss: 5.0055\n",
      "Epoch [1/2], Step [43410/64305], Loss: 4.9632\n",
      "Epoch [1/2], Step [43420/64305], Loss: 4.8927\n",
      "Epoch [1/2], Step [43430/64305], Loss: 4.9514\n",
      "Epoch [1/2], Step [43440/64305], Loss: 4.9541\n",
      "Epoch [1/2], Step [43450/64305], Loss: 4.9716\n",
      "Epoch [1/2], Step [43460/64305], Loss: 4.9835\n",
      "Epoch [1/2], Step [43470/64305], Loss: 4.8249\n",
      "Epoch [1/2], Step [43480/64305], Loss: 4.8545\n",
      "Epoch [1/2], Step [43490/64305], Loss: 4.8410\n",
      "Epoch [1/2], Step [43500/64305], Loss: 4.7631\n",
      "Epoch [1/2], Step [43510/64305], Loss: 4.9327\n",
      "Epoch [1/2], Step [43520/64305], Loss: 4.9763\n",
      "Epoch [1/2], Step [43530/64305], Loss: 4.8314\n",
      "Epoch [1/2], Step [43540/64305], Loss: 4.9079\n",
      "Epoch [1/2], Step [43550/64305], Loss: 4.8925\n",
      "Epoch [1/2], Step [43560/64305], Loss: 4.8754\n",
      "Epoch [1/2], Step [43570/64305], Loss: 4.8179\n",
      "Epoch [1/2], Step [43580/64305], Loss: 5.0104\n",
      "Epoch [1/2], Step [43590/64305], Loss: 4.8450\n",
      "Epoch [1/2], Step [43600/64305], Loss: 4.8487\n",
      "Epoch [1/2], Step [43610/64305], Loss: 4.8860\n",
      "Epoch [1/2], Step [43620/64305], Loss: 4.8685\n",
      "Epoch [1/2], Step [43630/64305], Loss: 4.9462\n",
      "Epoch [1/2], Step [43640/64305], Loss: 4.8379\n",
      "Epoch [1/2], Step [43650/64305], Loss: 4.7338\n",
      "Epoch [1/2], Step [43660/64305], Loss: 4.7864\n",
      "Epoch [1/2], Step [43670/64305], Loss: 4.9970\n",
      "Epoch [1/2], Step [43680/64305], Loss: 5.2232\n",
      "Epoch [1/2], Step [43690/64305], Loss: 4.9898\n",
      "Epoch [1/2], Step [43700/64305], Loss: 4.8054\n",
      "Epoch [1/2], Step [43710/64305], Loss: 4.8957\n",
      "Epoch [1/2], Step [43720/64305], Loss: 5.0419\n",
      "Epoch [1/2], Step [43730/64305], Loss: 4.6249\n",
      "Epoch [1/2], Step [43740/64305], Loss: 4.8356\n",
      "Epoch [1/2], Step [43750/64305], Loss: 4.7256\n",
      "Epoch [1/2], Step [43760/64305], Loss: 4.9544\n",
      "Epoch [1/2], Step [43770/64305], Loss: 5.1244\n",
      "Epoch [1/2], Step [43780/64305], Loss: 4.7881\n",
      "Epoch [1/2], Step [43790/64305], Loss: 4.9508\n",
      "Epoch [1/2], Step [43800/64305], Loss: 4.8013\n",
      "Epoch [1/2], Step [43810/64305], Loss: 4.7940\n",
      "Epoch [1/2], Step [43820/64305], Loss: 5.1257\n",
      "Epoch [1/2], Step [43830/64305], Loss: 4.9003\n",
      "Epoch [1/2], Step [43840/64305], Loss: 4.7933\n",
      "Epoch [1/2], Step [43850/64305], Loss: 4.7877\n",
      "Epoch [1/2], Step [43860/64305], Loss: 4.7929\n",
      "Epoch [1/2], Step [43870/64305], Loss: 5.0837\n",
      "Epoch [1/2], Step [43880/64305], Loss: 4.9217\n",
      "Epoch [1/2], Step [43890/64305], Loss: 4.8956\n",
      "Epoch [1/2], Step [43900/64305], Loss: 5.1322\n",
      "Epoch [1/2], Step [43910/64305], Loss: 4.9662\n",
      "Epoch [1/2], Step [43920/64305], Loss: 4.9473\n",
      "Epoch [1/2], Step [43930/64305], Loss: 4.9342\n",
      "Epoch [1/2], Step [43940/64305], Loss: 4.8789\n",
      "Epoch [1/2], Step [43950/64305], Loss: 4.7077\n",
      "Epoch [1/2], Step [43960/64305], Loss: 4.8697\n",
      "Epoch [1/2], Step [43970/64305], Loss: 4.8616\n",
      "Epoch [1/2], Step [43980/64305], Loss: 4.9311\n",
      "Epoch [1/2], Step [43990/64305], Loss: 5.0195\n",
      "Epoch [1/2], Step [44000/64305], Loss: 4.8505\n",
      "Epoch [1/2], Step [44010/64305], Loss: 4.9096\n",
      "Epoch [1/2], Step [44020/64305], Loss: 4.9444\n",
      "Epoch [1/2], Step [44030/64305], Loss: 4.8165\n",
      "Epoch [1/2], Step [44040/64305], Loss: 4.7667\n",
      "Epoch [1/2], Step [44050/64305], Loss: 4.9545\n",
      "Epoch [1/2], Step [44060/64305], Loss: 4.8854\n",
      "Epoch [1/2], Step [44070/64305], Loss: 4.7987\n",
      "Epoch [1/2], Step [44080/64305], Loss: 4.7573\n",
      "Epoch [1/2], Step [44090/64305], Loss: 4.9865\n",
      "Epoch [1/2], Step [44100/64305], Loss: 4.8882\n",
      "Epoch [1/2], Step [44110/64305], Loss: 5.0323\n",
      "Epoch [1/2], Step [44120/64305], Loss: 4.8779\n",
      "Epoch [1/2], Step [44130/64305], Loss: 4.8079\n",
      "Epoch [1/2], Step [44140/64305], Loss: 5.1417\n",
      "Epoch [1/2], Step [44150/64305], Loss: 4.8613\n",
      "Epoch [1/2], Step [44160/64305], Loss: 5.0281\n",
      "Epoch [1/2], Step [44170/64305], Loss: 4.9628\n",
      "Epoch [1/2], Step [44180/64305], Loss: 5.0041\n",
      "Epoch [1/2], Step [44190/64305], Loss: 5.0259\n",
      "Epoch [1/2], Step [44200/64305], Loss: 4.9721\n",
      "Epoch [1/2], Step [44210/64305], Loss: 5.0537\n",
      "Epoch [1/2], Step [44220/64305], Loss: 4.8765\n",
      "Epoch [1/2], Step [44230/64305], Loss: 5.0245\n",
      "Epoch [1/2], Step [44240/64305], Loss: 4.8872\n",
      "Epoch [1/2], Step [44250/64305], Loss: 5.1019\n",
      "Epoch [1/2], Step [44260/64305], Loss: 4.8813\n",
      "Epoch [1/2], Step [44270/64305], Loss: 4.7789\n",
      "Epoch [1/2], Step [44280/64305], Loss: 4.8453\n",
      "Epoch [1/2], Step [44290/64305], Loss: 4.9233\n",
      "Epoch [1/2], Step [44300/64305], Loss: 4.8004\n",
      "Epoch [1/2], Step [44310/64305], Loss: 4.7983\n",
      "Epoch [1/2], Step [44320/64305], Loss: 4.7477\n",
      "Epoch [1/2], Step [44330/64305], Loss: 4.9214\n",
      "Epoch [1/2], Step [44340/64305], Loss: 4.7806\n",
      "Epoch [1/2], Step [44350/64305], Loss: 4.9148\n",
      "Epoch [1/2], Step [44360/64305], Loss: 4.9373\n",
      "Epoch [1/2], Step [44370/64305], Loss: 5.0167\n",
      "Epoch [1/2], Step [44380/64305], Loss: 4.8001\n",
      "Epoch [1/2], Step [44390/64305], Loss: 4.9427\n",
      "Epoch [1/2], Step [44400/64305], Loss: 4.9136\n",
      "Epoch [1/2], Step [44410/64305], Loss: 4.9443\n",
      "Epoch [1/2], Step [44420/64305], Loss: 4.9843\n",
      "Epoch [1/2], Step [44430/64305], Loss: 4.9708\n",
      "Epoch [1/2], Step [44440/64305], Loss: 5.0027\n",
      "Epoch [1/2], Step [44450/64305], Loss: 5.0134\n",
      "Epoch [1/2], Step [44460/64305], Loss: 4.8867\n",
      "Epoch [1/2], Step [44470/64305], Loss: 5.1678\n",
      "Epoch [1/2], Step [44480/64305], Loss: 4.8998\n",
      "Epoch [1/2], Step [44490/64305], Loss: 4.9138\n",
      "Epoch [1/2], Step [44500/64305], Loss: 4.8368\n",
      "Epoch [1/2], Step [44510/64305], Loss: 5.1433\n",
      "Epoch [1/2], Step [44520/64305], Loss: 4.8359\n",
      "Epoch [1/2], Step [44530/64305], Loss: 5.0106\n",
      "Epoch [1/2], Step [44540/64305], Loss: 4.8324\n",
      "Epoch [1/2], Step [44550/64305], Loss: 4.8756\n",
      "Epoch [1/2], Step [44560/64305], Loss: 4.7662\n",
      "Epoch [1/2], Step [44570/64305], Loss: 4.8823\n",
      "Epoch [1/2], Step [44580/64305], Loss: 4.7779\n",
      "Epoch [1/2], Step [44590/64305], Loss: 4.8853\n",
      "Epoch [1/2], Step [44600/64305], Loss: 4.9226\n",
      "Epoch [1/2], Step [44610/64305], Loss: 4.9119\n",
      "Epoch [1/2], Step [44620/64305], Loss: 4.9516\n",
      "Epoch [1/2], Step [44630/64305], Loss: 4.7145\n",
      "Epoch [1/2], Step [44640/64305], Loss: 4.9121\n",
      "Epoch [1/2], Step [44650/64305], Loss: 4.8790\n",
      "Epoch [1/2], Step [44660/64305], Loss: 4.9108\n",
      "Epoch [1/2], Step [44670/64305], Loss: 5.0026\n",
      "Epoch [1/2], Step [44680/64305], Loss: 4.8349\n",
      "Epoch [1/2], Step [44690/64305], Loss: 5.0544\n",
      "Epoch [1/2], Step [44700/64305], Loss: 4.8724\n",
      "Epoch [1/2], Step [44710/64305], Loss: 5.0407\n",
      "Epoch [1/2], Step [44720/64305], Loss: 4.9970\n",
      "Epoch [1/2], Step [44730/64305], Loss: 4.8826\n",
      "Epoch [1/2], Step [44740/64305], Loss: 4.6424\n",
      "Epoch [1/2], Step [44750/64305], Loss: 4.9384\n",
      "Epoch [1/2], Step [44760/64305], Loss: 5.0119\n",
      "Epoch [1/2], Step [44770/64305], Loss: 4.6970\n",
      "Epoch [1/2], Step [44780/64305], Loss: 4.8645\n",
      "Epoch [1/2], Step [44790/64305], Loss: 4.8851\n",
      "Epoch [1/2], Step [44800/64305], Loss: 4.8288\n",
      "Epoch [1/2], Step [44810/64305], Loss: 4.8823\n",
      "Epoch [1/2], Step [44820/64305], Loss: 4.8531\n",
      "Epoch [1/2], Step [44830/64305], Loss: 4.9845\n",
      "Epoch [1/2], Step [44840/64305], Loss: 4.8152\n",
      "Epoch [1/2], Step [44850/64305], Loss: 4.9534\n",
      "Epoch [1/2], Step [44860/64305], Loss: 5.0089\n",
      "Epoch [1/2], Step [44870/64305], Loss: 4.8733\n",
      "Epoch [1/2], Step [44880/64305], Loss: 4.8023\n",
      "Epoch [1/2], Step [44890/64305], Loss: 5.0210\n",
      "Epoch [1/2], Step [44900/64305], Loss: 5.0663\n",
      "Epoch [1/2], Step [44910/64305], Loss: 4.9105\n",
      "Epoch [1/2], Step [44920/64305], Loss: 4.9224\n",
      "Epoch [1/2], Step [44930/64305], Loss: 4.8390\n",
      "Epoch [1/2], Step [44940/64305], Loss: 5.0077\n",
      "Epoch [1/2], Step [44950/64305], Loss: 4.7528\n",
      "Epoch [1/2], Step [44960/64305], Loss: 5.0023\n",
      "Epoch [1/2], Step [44970/64305], Loss: 4.9163\n",
      "Epoch [1/2], Step [44980/64305], Loss: 4.7864\n",
      "Epoch [1/2], Step [44990/64305], Loss: 4.8790\n",
      "Epoch [1/2], Step [45000/64305], Loss: 4.9542\n",
      "Epoch [1/2], Step [45010/64305], Loss: 4.9667\n",
      "Epoch [1/2], Step [45020/64305], Loss: 5.1415\n",
      "Epoch [1/2], Step [45030/64305], Loss: 5.1399\n",
      "Epoch [1/2], Step [45040/64305], Loss: 5.0316\n",
      "Epoch [1/2], Step [45050/64305], Loss: 4.9834\n",
      "Epoch [1/2], Step [45060/64305], Loss: 5.1020\n",
      "Epoch [1/2], Step [45070/64305], Loss: 4.8075\n",
      "Epoch [1/2], Step [45080/64305], Loss: 4.9738\n",
      "Epoch [1/2], Step [45090/64305], Loss: 4.6180\n",
      "Epoch [1/2], Step [45100/64305], Loss: 4.9117\n",
      "Epoch [1/2], Step [45110/64305], Loss: 4.8426\n",
      "Epoch [1/2], Step [45120/64305], Loss: 4.8453\n",
      "Epoch [1/2], Step [45130/64305], Loss: 4.9126\n",
      "Epoch [1/2], Step [45140/64305], Loss: 4.8521\n",
      "Epoch [1/2], Step [45150/64305], Loss: 4.7370\n",
      "Epoch [1/2], Step [45160/64305], Loss: 4.9849\n",
      "Epoch [1/2], Step [45170/64305], Loss: 5.0063\n",
      "Epoch [1/2], Step [45180/64305], Loss: 4.9364\n",
      "Epoch [1/2], Step [45190/64305], Loss: 4.8926\n",
      "Epoch [1/2], Step [45200/64305], Loss: 4.9681\n",
      "Epoch [1/2], Step [45210/64305], Loss: 4.6538\n",
      "Epoch [1/2], Step [45220/64305], Loss: 4.8530\n",
      "Epoch [1/2], Step [45230/64305], Loss: 4.6674\n",
      "Epoch [1/2], Step [45240/64305], Loss: 5.0334\n",
      "Epoch [1/2], Step [45250/64305], Loss: 5.1099\n",
      "Epoch [1/2], Step [45260/64305], Loss: 4.9172\n",
      "Epoch [1/2], Step [45270/64305], Loss: 4.9560\n",
      "Epoch [1/2], Step [45280/64305], Loss: 4.9992\n",
      "Epoch [1/2], Step [45290/64305], Loss: 4.7995\n",
      "Epoch [1/2], Step [45300/64305], Loss: 4.9323\n",
      "Epoch [1/2], Step [45310/64305], Loss: 4.8087\n",
      "Epoch [1/2], Step [45320/64305], Loss: 4.9215\n",
      "Epoch [1/2], Step [45330/64305], Loss: 4.9537\n",
      "Epoch [1/2], Step [45340/64305], Loss: 4.9538\n",
      "Epoch [1/2], Step [45350/64305], Loss: 4.8042\n",
      "Epoch [1/2], Step [45360/64305], Loss: 4.8160\n",
      "Epoch [1/2], Step [45370/64305], Loss: 4.7747\n",
      "Epoch [1/2], Step [45380/64305], Loss: 5.0888\n",
      "Epoch [1/2], Step [45390/64305], Loss: 4.7587\n",
      "Epoch [1/2], Step [45400/64305], Loss: 4.8476\n",
      "Epoch [1/2], Step [45410/64305], Loss: 4.9439\n",
      "Epoch [1/2], Step [45420/64305], Loss: 4.8403\n",
      "Epoch [1/2], Step [45430/64305], Loss: 4.7556\n",
      "Epoch [1/2], Step [45440/64305], Loss: 4.9862\n",
      "Epoch [1/2], Step [45450/64305], Loss: 4.9344\n",
      "Epoch [1/2], Step [45460/64305], Loss: 4.9088\n",
      "Epoch [1/2], Step [45470/64305], Loss: 4.8099\n",
      "Epoch [1/2], Step [45480/64305], Loss: 5.0411\n",
      "Epoch [1/2], Step [45490/64305], Loss: 4.8673\n",
      "Epoch [1/2], Step [45500/64305], Loss: 5.1215\n",
      "Epoch [1/2], Step [45510/64305], Loss: 4.8004\n",
      "Epoch [1/2], Step [45520/64305], Loss: 4.8678\n",
      "Epoch [1/2], Step [45530/64305], Loss: 4.9051\n",
      "Epoch [1/2], Step [45540/64305], Loss: 4.9769\n",
      "Epoch [1/2], Step [45550/64305], Loss: 4.8408\n",
      "Epoch [1/2], Step [45560/64305], Loss: 4.8090\n",
      "Epoch [1/2], Step [45570/64305], Loss: 4.8060\n",
      "Epoch [1/2], Step [45580/64305], Loss: 5.1045\n",
      "Epoch [1/2], Step [45590/64305], Loss: 4.7846\n",
      "Epoch [1/2], Step [45600/64305], Loss: 4.9964\n",
      "Epoch [1/2], Step [45610/64305], Loss: 4.9792\n",
      "Epoch [1/2], Step [45620/64305], Loss: 5.0886\n",
      "Epoch [1/2], Step [45630/64305], Loss: 4.7483\n",
      "Epoch [1/2], Step [45640/64305], Loss: 4.8078\n",
      "Epoch [1/2], Step [45650/64305], Loss: 4.7709\n",
      "Epoch [1/2], Step [45660/64305], Loss: 4.8927\n",
      "Epoch [1/2], Step [45670/64305], Loss: 5.1368\n",
      "Epoch [1/2], Step [45680/64305], Loss: 4.8914\n",
      "Epoch [1/2], Step [45690/64305], Loss: 4.9175\n",
      "Epoch [1/2], Step [45700/64305], Loss: 4.9353\n",
      "Epoch [1/2], Step [45710/64305], Loss: 4.6880\n",
      "Epoch [1/2], Step [45720/64305], Loss: 4.7202\n",
      "Epoch [1/2], Step [45730/64305], Loss: 4.9548\n",
      "Epoch [1/2], Step [45740/64305], Loss: 4.8037\n",
      "Epoch [1/2], Step [45750/64305], Loss: 4.8304\n",
      "Epoch [1/2], Step [45760/64305], Loss: 4.7188\n",
      "Epoch [1/2], Step [45770/64305], Loss: 4.9279\n",
      "Epoch [1/2], Step [45780/64305], Loss: 5.0290\n",
      "Epoch [1/2], Step [45790/64305], Loss: 4.7731\n",
      "Epoch [1/2], Step [45800/64305], Loss: 4.8303\n",
      "Epoch [1/2], Step [45810/64305], Loss: 4.8926\n",
      "Epoch [1/2], Step [45820/64305], Loss: 4.8340\n",
      "Epoch [1/2], Step [45830/64305], Loss: 4.8817\n",
      "Epoch [1/2], Step [45840/64305], Loss: 5.0036\n",
      "Epoch [1/2], Step [45850/64305], Loss: 4.9637\n",
      "Epoch [1/2], Step [45860/64305], Loss: 4.9008\n",
      "Epoch [1/2], Step [45870/64305], Loss: 4.8340\n",
      "Epoch [1/2], Step [45880/64305], Loss: 4.7949\n",
      "Epoch [1/2], Step [45890/64305], Loss: 4.7664\n",
      "Epoch [1/2], Step [45900/64305], Loss: 4.6898\n",
      "Epoch [1/2], Step [45910/64305], Loss: 4.8187\n",
      "Epoch [1/2], Step [45920/64305], Loss: 4.8465\n",
      "Epoch [1/2], Step [45930/64305], Loss: 4.7495\n",
      "Epoch [1/2], Step [45940/64305], Loss: 4.9848\n",
      "Epoch [1/2], Step [45950/64305], Loss: 5.0705\n",
      "Epoch [1/2], Step [45960/64305], Loss: 5.0804\n",
      "Epoch [1/2], Step [45970/64305], Loss: 4.7915\n",
      "Epoch [1/2], Step [45980/64305], Loss: 4.9601\n",
      "Epoch [1/2], Step [45990/64305], Loss: 5.0058\n",
      "Epoch [1/2], Step [46000/64305], Loss: 4.7602\n",
      "Epoch [1/2], Step [46010/64305], Loss: 4.9210\n",
      "Epoch [1/2], Step [46020/64305], Loss: 4.9744\n",
      "Epoch [1/2], Step [46030/64305], Loss: 4.7664\n",
      "Epoch [1/2], Step [46040/64305], Loss: 4.8390\n",
      "Epoch [1/2], Step [46050/64305], Loss: 4.8167\n",
      "Epoch [1/2], Step [46060/64305], Loss: 5.0703\n",
      "Epoch [1/2], Step [46070/64305], Loss: 4.9268\n",
      "Epoch [1/2], Step [46080/64305], Loss: 4.9152\n",
      "Epoch [1/2], Step [46090/64305], Loss: 4.8937\n",
      "Epoch [1/2], Step [46100/64305], Loss: 4.6633\n",
      "Epoch [1/2], Step [46110/64305], Loss: 4.9194\n",
      "Epoch [1/2], Step [46120/64305], Loss: 4.6345\n",
      "Epoch [1/2], Step [46130/64305], Loss: 4.8219\n",
      "Epoch [1/2], Step [46140/64305], Loss: 4.8832\n",
      "Epoch [1/2], Step [46150/64305], Loss: 4.9488\n",
      "Epoch [1/2], Step [46160/64305], Loss: 4.9061\n",
      "Epoch [1/2], Step [46170/64305], Loss: 4.7553\n",
      "Epoch [1/2], Step [46180/64305], Loss: 4.8340\n",
      "Epoch [1/2], Step [46190/64305], Loss: 4.5714\n",
      "Epoch [1/2], Step [46200/64305], Loss: 4.8415\n",
      "Epoch [1/2], Step [46210/64305], Loss: 4.8476\n",
      "Epoch [1/2], Step [46220/64305], Loss: 4.7781\n",
      "Epoch [1/2], Step [46230/64305], Loss: 4.8768\n",
      "Epoch [1/2], Step [46240/64305], Loss: 5.1173\n",
      "Epoch [1/2], Step [46250/64305], Loss: 4.9233\n",
      "Epoch [1/2], Step [46260/64305], Loss: 4.8326\n",
      "Epoch [1/2], Step [46270/64305], Loss: 4.7663\n",
      "Epoch [1/2], Step [46280/64305], Loss: 4.8825\n",
      "Epoch [1/2], Step [46290/64305], Loss: 4.8730\n",
      "Epoch [1/2], Step [46300/64305], Loss: 4.9521\n",
      "Epoch [1/2], Step [46310/64305], Loss: 4.9331\n",
      "Epoch [1/2], Step [46320/64305], Loss: 5.0247\n",
      "Epoch [1/2], Step [46330/64305], Loss: 5.0254\n",
      "Epoch [1/2], Step [46340/64305], Loss: 4.9873\n",
      "Epoch [1/2], Step [46350/64305], Loss: 4.9850\n",
      "Epoch [1/2], Step [46360/64305], Loss: 5.1928\n",
      "Epoch [1/2], Step [46370/64305], Loss: 4.9584\n",
      "Epoch [1/2], Step [46380/64305], Loss: 4.8783\n",
      "Epoch [1/2], Step [46390/64305], Loss: 4.9427\n",
      "Epoch [1/2], Step [46400/64305], Loss: 5.0385\n",
      "Epoch [1/2], Step [46410/64305], Loss: 4.8561\n",
      "Epoch [1/2], Step [46420/64305], Loss: 4.6684\n",
      "Epoch [1/2], Step [46430/64305], Loss: 5.0943\n",
      "Epoch [1/2], Step [46440/64305], Loss: 4.8047\n",
      "Epoch [1/2], Step [46450/64305], Loss: 5.0763\n",
      "Epoch [1/2], Step [46460/64305], Loss: 4.8966\n",
      "Epoch [1/2], Step [46470/64305], Loss: 4.8628\n",
      "Epoch [1/2], Step [46480/64305], Loss: 4.9027\n",
      "Epoch [1/2], Step [46490/64305], Loss: 4.9186\n",
      "Epoch [1/2], Step [46500/64305], Loss: 4.9061\n",
      "Epoch [1/2], Step [46510/64305], Loss: 4.8236\n",
      "Epoch [1/2], Step [46520/64305], Loss: 4.6899\n",
      "Epoch [1/2], Step [46530/64305], Loss: 5.0366\n",
      "Epoch [1/2], Step [46540/64305], Loss: 4.8644\n",
      "Epoch [1/2], Step [46550/64305], Loss: 4.9070\n",
      "Epoch [1/2], Step [46560/64305], Loss: 4.7644\n",
      "Epoch [1/2], Step [46570/64305], Loss: 4.8600\n",
      "Epoch [1/2], Step [46580/64305], Loss: 4.7408\n",
      "Epoch [1/2], Step [46590/64305], Loss: 5.2164\n",
      "Epoch [1/2], Step [46600/64305], Loss: 4.8061\n",
      "Epoch [1/2], Step [46610/64305], Loss: 4.9801\n",
      "Epoch [1/2], Step [46620/64305], Loss: 5.0938\n",
      "Epoch [1/2], Step [46630/64305], Loss: 4.8746\n",
      "Epoch [1/2], Step [46640/64305], Loss: 5.1634\n",
      "Epoch [1/2], Step [46650/64305], Loss: 4.8205\n",
      "Epoch [1/2], Step [46660/64305], Loss: 4.8325\n",
      "Epoch [1/2], Step [46670/64305], Loss: 4.8290\n",
      "Epoch [1/2], Step [46680/64305], Loss: 4.8778\n",
      "Epoch [1/2], Step [46690/64305], Loss: 4.8170\n",
      "Epoch [1/2], Step [46700/64305], Loss: 4.9097\n",
      "Epoch [1/2], Step [46710/64305], Loss: 4.8543\n",
      "Epoch [1/2], Step [46720/64305], Loss: 4.8823\n",
      "Epoch [1/2], Step [46730/64305], Loss: 5.0448\n",
      "Epoch [1/2], Step [46740/64305], Loss: 5.0995\n",
      "Epoch [1/2], Step [46750/64305], Loss: 4.9111\n",
      "Epoch [1/2], Step [46760/64305], Loss: 4.8423\n",
      "Epoch [1/2], Step [46770/64305], Loss: 4.8724\n",
      "Epoch [1/2], Step [46780/64305], Loss: 4.8449\n",
      "Epoch [1/2], Step [46790/64305], Loss: 4.8848\n",
      "Epoch [1/2], Step [46800/64305], Loss: 4.9190\n",
      "Epoch [1/2], Step [46810/64305], Loss: 4.7794\n",
      "Epoch [1/2], Step [46820/64305], Loss: 4.7380\n",
      "Epoch [1/2], Step [46830/64305], Loss: 4.9425\n",
      "Epoch [1/2], Step [46840/64305], Loss: 4.9191\n",
      "Epoch [1/2], Step [46850/64305], Loss: 4.8989\n",
      "Epoch [1/2], Step [46860/64305], Loss: 4.9236\n",
      "Epoch [1/2], Step [46870/64305], Loss: 4.9071\n",
      "Epoch [1/2], Step [46880/64305], Loss: 5.0176\n",
      "Epoch [1/2], Step [46890/64305], Loss: 4.7700\n",
      "Epoch [1/2], Step [46900/64305], Loss: 4.7235\n",
      "Epoch [1/2], Step [46910/64305], Loss: 4.8002\n",
      "Epoch [1/2], Step [46920/64305], Loss: 4.9686\n",
      "Epoch [1/2], Step [46930/64305], Loss: 4.8389\n",
      "Epoch [1/2], Step [46940/64305], Loss: 4.9378\n",
      "Epoch [1/2], Step [46950/64305], Loss: 4.8174\n",
      "Epoch [1/2], Step [46960/64305], Loss: 4.8852\n",
      "Epoch [1/2], Step [46970/64305], Loss: 4.9805\n",
      "Epoch [1/2], Step [46980/64305], Loss: 4.9729\n",
      "Epoch [1/2], Step [46990/64305], Loss: 4.9213\n",
      "Epoch [1/2], Step [47000/64305], Loss: 4.8408\n",
      "Epoch [1/2], Step [47010/64305], Loss: 4.7886\n",
      "Epoch [1/2], Step [47020/64305], Loss: 4.8810\n",
      "Epoch [1/2], Step [47030/64305], Loss: 4.9209\n",
      "Epoch [1/2], Step [47040/64305], Loss: 4.7576\n",
      "Epoch [1/2], Step [47050/64305], Loss: 4.9804\n",
      "Epoch [1/2], Step [47060/64305], Loss: 4.9455\n",
      "Epoch [1/2], Step [47070/64305], Loss: 4.9209\n",
      "Epoch [1/2], Step [47080/64305], Loss: 4.7981\n",
      "Epoch [1/2], Step [47090/64305], Loss: 5.0267\n",
      "Epoch [1/2], Step [47100/64305], Loss: 4.8732\n",
      "Epoch [1/2], Step [47110/64305], Loss: 4.8310\n",
      "Epoch [1/2], Step [47120/64305], Loss: 4.9074\n",
      "Epoch [1/2], Step [47130/64305], Loss: 4.8609\n",
      "Epoch [1/2], Step [47140/64305], Loss: 4.8764\n",
      "Epoch [1/2], Step [47150/64305], Loss: 4.8236\n",
      "Epoch [1/2], Step [47160/64305], Loss: 4.9259\n",
      "Epoch [1/2], Step [47170/64305], Loss: 4.9182\n",
      "Epoch [1/2], Step [47180/64305], Loss: 4.9097\n",
      "Epoch [1/2], Step [47190/64305], Loss: 4.7300\n",
      "Epoch [1/2], Step [47200/64305], Loss: 4.8735\n",
      "Epoch [1/2], Step [47210/64305], Loss: 4.9829\n",
      "Epoch [1/2], Step [47220/64305], Loss: 5.0543\n",
      "Epoch [1/2], Step [47230/64305], Loss: 4.8790\n",
      "Epoch [1/2], Step [47240/64305], Loss: 4.8743\n",
      "Epoch [1/2], Step [47250/64305], Loss: 4.8447\n",
      "Epoch [1/2], Step [47260/64305], Loss: 4.8996\n",
      "Epoch [1/2], Step [47270/64305], Loss: 4.9030\n",
      "Epoch [1/2], Step [47280/64305], Loss: 4.7496\n",
      "Epoch [1/2], Step [47290/64305], Loss: 4.7978\n",
      "Epoch [1/2], Step [47300/64305], Loss: 4.9423\n",
      "Epoch [1/2], Step [47310/64305], Loss: 5.2522\n",
      "Epoch [1/2], Step [47320/64305], Loss: 4.7940\n",
      "Epoch [1/2], Step [47330/64305], Loss: 4.9065\n",
      "Epoch [1/2], Step [47340/64305], Loss: 4.9101\n",
      "Epoch [1/2], Step [47350/64305], Loss: 4.7975\n",
      "Epoch [1/2], Step [47360/64305], Loss: 4.7663\n",
      "Epoch [1/2], Step [47370/64305], Loss: 4.9286\n",
      "Epoch [1/2], Step [47380/64305], Loss: 4.8885\n",
      "Epoch [1/2], Step [47390/64305], Loss: 5.0211\n",
      "Epoch [1/2], Step [47400/64305], Loss: 5.0724\n",
      "Epoch [1/2], Step [47410/64305], Loss: 4.8895\n",
      "Epoch [1/2], Step [47420/64305], Loss: 4.9036\n",
      "Epoch [1/2], Step [47430/64305], Loss: 4.9763\n",
      "Epoch [1/2], Step [47440/64305], Loss: 4.8427\n",
      "Epoch [1/2], Step [47450/64305], Loss: 4.9019\n",
      "Epoch [1/2], Step [47460/64305], Loss: 4.8793\n",
      "Epoch [1/2], Step [47470/64305], Loss: 4.8955\n",
      "Epoch [1/2], Step [47480/64305], Loss: 5.0959\n",
      "Epoch [1/2], Step [47490/64305], Loss: 4.8935\n",
      "Epoch [1/2], Step [47500/64305], Loss: 4.9432\n",
      "Epoch [1/2], Step [47510/64305], Loss: 4.8820\n",
      "Epoch [1/2], Step [47520/64305], Loss: 4.8995\n",
      "Epoch [1/2], Step [47530/64305], Loss: 5.0746\n",
      "Epoch [1/2], Step [47540/64305], Loss: 4.9570\n",
      "Epoch [1/2], Step [47550/64305], Loss: 5.0009\n",
      "Epoch [1/2], Step [47560/64305], Loss: 5.0031\n",
      "Epoch [1/2], Step [47570/64305], Loss: 4.9727\n",
      "Epoch [1/2], Step [47580/64305], Loss: 5.0245\n",
      "Epoch [1/2], Step [47590/64305], Loss: 4.7349\n",
      "Epoch [1/2], Step [47600/64305], Loss: 4.8190\n",
      "Epoch [1/2], Step [47610/64305], Loss: 4.8777\n",
      "Epoch [1/2], Step [47620/64305], Loss: 4.9293\n",
      "Epoch [1/2], Step [47630/64305], Loss: 4.9310\n",
      "Epoch [1/2], Step [47640/64305], Loss: 4.7275\n",
      "Epoch [1/2], Step [47650/64305], Loss: 5.0100\n",
      "Epoch [1/2], Step [47660/64305], Loss: 5.2285\n",
      "Epoch [1/2], Step [47670/64305], Loss: 5.0004\n",
      "Epoch [1/2], Step [47680/64305], Loss: 4.9172\n",
      "Epoch [1/2], Step [47690/64305], Loss: 4.8615\n",
      "Epoch [1/2], Step [47700/64305], Loss: 4.9571\n",
      "Epoch [1/2], Step [47710/64305], Loss: 4.7135\n",
      "Epoch [1/2], Step [47720/64305], Loss: 5.0797\n",
      "Epoch [1/2], Step [47730/64305], Loss: 4.9454\n",
      "Epoch [1/2], Step [47740/64305], Loss: 4.8386\n",
      "Epoch [1/2], Step [47750/64305], Loss: 5.0694\n",
      "Epoch [1/2], Step [47760/64305], Loss: 4.8684\n",
      "Epoch [1/2], Step [47770/64305], Loss: 4.8072\n",
      "Epoch [1/2], Step [47780/64305], Loss: 4.5666\n",
      "Epoch [1/2], Step [47790/64305], Loss: 4.9633\n",
      "Epoch [1/2], Step [47800/64305], Loss: 4.9258\n",
      "Epoch [1/2], Step [47810/64305], Loss: 4.8564\n",
      "Epoch [1/2], Step [47820/64305], Loss: 4.7442\n",
      "Epoch [1/2], Step [47830/64305], Loss: 4.8249\n",
      "Epoch [1/2], Step [47840/64305], Loss: 5.1405\n",
      "Epoch [1/2], Step [47850/64305], Loss: 4.9611\n",
      "Epoch [1/2], Step [47860/64305], Loss: 4.9043\n",
      "Epoch [1/2], Step [47870/64305], Loss: 4.8989\n",
      "Epoch [1/2], Step [47880/64305], Loss: 5.0009\n",
      "Epoch [1/2], Step [47890/64305], Loss: 4.8375\n",
      "Epoch [1/2], Step [47900/64305], Loss: 4.9949\n",
      "Epoch [1/2], Step [47910/64305], Loss: 4.7533\n",
      "Epoch [1/2], Step [47920/64305], Loss: 4.9486\n",
      "Epoch [1/2], Step [47930/64305], Loss: 5.0103\n",
      "Epoch [1/2], Step [47940/64305], Loss: 4.8061\n",
      "Epoch [1/2], Step [47950/64305], Loss: 4.8667\n",
      "Epoch [1/2], Step [47960/64305], Loss: 4.8657\n",
      "Epoch [1/2], Step [47970/64305], Loss: 4.8876\n",
      "Epoch [1/2], Step [47980/64305], Loss: 5.0879\n",
      "Epoch [1/2], Step [47990/64305], Loss: 4.9490\n",
      "Epoch [1/2], Step [48000/64305], Loss: 5.0215\n",
      "Epoch [1/2], Step [48010/64305], Loss: 4.8337\n",
      "Epoch [1/2], Step [48020/64305], Loss: 4.6916\n",
      "Epoch [1/2], Step [48030/64305], Loss: 4.7021\n",
      "Epoch [1/2], Step [48040/64305], Loss: 4.7606\n",
      "Epoch [1/2], Step [48050/64305], Loss: 4.5481\n",
      "Epoch [1/2], Step [48060/64305], Loss: 4.9115\n",
      "Epoch [1/2], Step [48070/64305], Loss: 4.8686\n",
      "Epoch [1/2], Step [48080/64305], Loss: 4.8383\n",
      "Epoch [1/2], Step [48090/64305], Loss: 5.0339\n",
      "Epoch [1/2], Step [48100/64305], Loss: 4.7398\n",
      "Epoch [1/2], Step [48110/64305], Loss: 4.7930\n",
      "Epoch [1/2], Step [48120/64305], Loss: 4.9151\n",
      "Epoch [1/2], Step [48130/64305], Loss: 5.1102\n",
      "Epoch [1/2], Step [48140/64305], Loss: 4.9031\n",
      "Epoch [1/2], Step [48150/64305], Loss: 4.6403\n",
      "Epoch [1/2], Step [48160/64305], Loss: 4.7777\n",
      "Epoch [1/2], Step [48170/64305], Loss: 4.6854\n",
      "Epoch [1/2], Step [48180/64305], Loss: 5.0244\n",
      "Epoch [1/2], Step [48190/64305], Loss: 4.7664\n",
      "Epoch [1/2], Step [48200/64305], Loss: 5.0715\n",
      "Epoch [1/2], Step [48210/64305], Loss: 4.9162\n",
      "Epoch [1/2], Step [48220/64305], Loss: 4.7882\n",
      "Epoch [1/2], Step [48230/64305], Loss: 4.9144\n",
      "Epoch [1/2], Step [48240/64305], Loss: 4.9386\n",
      "Epoch [1/2], Step [48250/64305], Loss: 4.8142\n",
      "Epoch [1/2], Step [48260/64305], Loss: 4.8784\n",
      "Epoch [1/2], Step [48270/64305], Loss: 4.9130\n",
      "Epoch [1/2], Step [48280/64305], Loss: 4.7428\n",
      "Epoch [1/2], Step [48290/64305], Loss: 4.9281\n",
      "Epoch [1/2], Step [48300/64305], Loss: 4.8599\n",
      "Epoch [1/2], Step [48310/64305], Loss: 4.9335\n",
      "Epoch [1/2], Step [48320/64305], Loss: 4.8912\n",
      "Epoch [1/2], Step [48330/64305], Loss: 4.8580\n",
      "Epoch [1/2], Step [48340/64305], Loss: 4.8680\n",
      "Epoch [1/2], Step [48350/64305], Loss: 4.9522\n",
      "Epoch [1/2], Step [48360/64305], Loss: 4.7317\n",
      "Epoch [1/2], Step [48370/64305], Loss: 4.9240\n",
      "Epoch [1/2], Step [48380/64305], Loss: 4.7159\n",
      "Epoch [1/2], Step [48390/64305], Loss: 4.8082\n",
      "Epoch [1/2], Step [48400/64305], Loss: 4.8921\n",
      "Epoch [1/2], Step [48410/64305], Loss: 4.7644\n",
      "Epoch [1/2], Step [48420/64305], Loss: 4.8505\n",
      "Epoch [1/2], Step [48430/64305], Loss: 4.9633\n",
      "Epoch [1/2], Step [48440/64305], Loss: 4.8542\n",
      "Epoch [1/2], Step [48450/64305], Loss: 4.7284\n",
      "Epoch [1/2], Step [48460/64305], Loss: 4.9742\n",
      "Epoch [1/2], Step [48470/64305], Loss: 4.8375\n",
      "Epoch [1/2], Step [48480/64305], Loss: 4.8456\n",
      "Epoch [1/2], Step [48490/64305], Loss: 4.8774\n",
      "Epoch [1/2], Step [48500/64305], Loss: 5.0155\n",
      "Epoch [1/2], Step [48510/64305], Loss: 4.8925\n",
      "Epoch [1/2], Step [48520/64305], Loss: 4.9388\n",
      "Epoch [1/2], Step [48530/64305], Loss: 4.8995\n",
      "Epoch [1/2], Step [48540/64305], Loss: 4.7886\n",
      "Epoch [1/2], Step [48550/64305], Loss: 5.2367\n",
      "Epoch [1/2], Step [48560/64305], Loss: 4.8667\n",
      "Epoch [1/2], Step [48570/64305], Loss: 4.7851\n",
      "Epoch [1/2], Step [48580/64305], Loss: 4.9563\n",
      "Epoch [1/2], Step [48590/64305], Loss: 4.8516\n",
      "Epoch [1/2], Step [48600/64305], Loss: 4.8041\n",
      "Epoch [1/2], Step [48610/64305], Loss: 4.7710\n",
      "Epoch [1/2], Step [48620/64305], Loss: 4.8135\n",
      "Epoch [1/2], Step [48630/64305], Loss: 4.8541\n",
      "Epoch [1/2], Step [48640/64305], Loss: 5.1135\n",
      "Epoch [1/2], Step [48650/64305], Loss: 4.8010\n",
      "Epoch [1/2], Step [48660/64305], Loss: 5.0112\n",
      "Epoch [1/2], Step [48670/64305], Loss: 4.7665\n",
      "Epoch [1/2], Step [48680/64305], Loss: 5.0528\n",
      "Epoch [1/2], Step [48690/64305], Loss: 5.0011\n",
      "Epoch [1/2], Step [48700/64305], Loss: 4.7289\n",
      "Epoch [1/2], Step [48710/64305], Loss: 4.9329\n",
      "Epoch [1/2], Step [48720/64305], Loss: 4.6902\n",
      "Epoch [1/2], Step [48730/64305], Loss: 4.8537\n",
      "Epoch [1/2], Step [48740/64305], Loss: 4.6947\n",
      "Epoch [1/2], Step [48750/64305], Loss: 4.5862\n",
      "Epoch [1/2], Step [48760/64305], Loss: 4.9513\n",
      "Epoch [1/2], Step [48770/64305], Loss: 4.8082\n",
      "Epoch [1/2], Step [48780/64305], Loss: 5.1581\n",
      "Epoch [1/2], Step [48790/64305], Loss: 4.7563\n",
      "Epoch [1/2], Step [48800/64305], Loss: 4.8691\n",
      "Epoch [1/2], Step [48810/64305], Loss: 4.8258\n",
      "Epoch [1/2], Step [48820/64305], Loss: 5.0505\n",
      "Epoch [1/2], Step [48830/64305], Loss: 4.8069\n",
      "Epoch [1/2], Step [48840/64305], Loss: 5.1372\n",
      "Epoch [1/2], Step [48850/64305], Loss: 4.6728\n",
      "Epoch [1/2], Step [48860/64305], Loss: 4.9156\n",
      "Epoch [1/2], Step [48870/64305], Loss: 4.7977\n",
      "Epoch [1/2], Step [48880/64305], Loss: 4.9500\n",
      "Epoch [1/2], Step [48890/64305], Loss: 4.8254\n",
      "Epoch [1/2], Step [48900/64305], Loss: 5.1153\n",
      "Epoch [1/2], Step [48910/64305], Loss: 4.7671\n",
      "Epoch [1/2], Step [48920/64305], Loss: 4.9844\n",
      "Epoch [1/2], Step [48930/64305], Loss: 4.7098\n",
      "Epoch [1/2], Step [48940/64305], Loss: 4.8905\n",
      "Epoch [1/2], Step [48950/64305], Loss: 4.8603\n",
      "Epoch [1/2], Step [48960/64305], Loss: 4.7864\n",
      "Epoch [1/2], Step [48970/64305], Loss: 4.8620\n",
      "Epoch [1/2], Step [48980/64305], Loss: 4.8829\n",
      "Epoch [1/2], Step [48990/64305], Loss: 4.7063\n",
      "Epoch [1/2], Step [49000/64305], Loss: 4.9403\n",
      "Epoch [1/2], Step [49010/64305], Loss: 4.7798\n",
      "Epoch [1/2], Step [49020/64305], Loss: 4.9531\n",
      "Epoch [1/2], Step [49030/64305], Loss: 4.8570\n",
      "Epoch [1/2], Step [49040/64305], Loss: 4.8941\n",
      "Epoch [1/2], Step [49050/64305], Loss: 5.0660\n",
      "Epoch [1/2], Step [49060/64305], Loss: 4.8907\n",
      "Epoch [1/2], Step [49070/64305], Loss: 4.6637\n",
      "Epoch [1/2], Step [49080/64305], Loss: 5.0404\n",
      "Epoch [1/2], Step [49090/64305], Loss: 4.8075\n",
      "Epoch [1/2], Step [49100/64305], Loss: 4.7972\n",
      "Epoch [1/2], Step [49110/64305], Loss: 4.7787\n",
      "Epoch [1/2], Step [49120/64305], Loss: 4.8401\n",
      "Epoch [1/2], Step [49130/64305], Loss: 4.8555\n",
      "Epoch [1/2], Step [49140/64305], Loss: 4.8820\n",
      "Epoch [1/2], Step [49150/64305], Loss: 4.9354\n",
      "Epoch [1/2], Step [49160/64305], Loss: 4.7994\n",
      "Epoch [1/2], Step [49170/64305], Loss: 4.8031\n",
      "Epoch [1/2], Step [49180/64305], Loss: 4.6849\n",
      "Epoch [1/2], Step [49190/64305], Loss: 4.8861\n",
      "Epoch [1/2], Step [49200/64305], Loss: 4.6134\n",
      "Epoch [1/2], Step [49210/64305], Loss: 4.9078\n",
      "Epoch [1/2], Step [49220/64305], Loss: 5.0024\n",
      "Epoch [1/2], Step [49230/64305], Loss: 4.8096\n",
      "Epoch [1/2], Step [49240/64305], Loss: 4.8844\n",
      "Epoch [1/2], Step [49250/64305], Loss: 5.0017\n",
      "Epoch [1/2], Step [49260/64305], Loss: 4.9175\n",
      "Epoch [1/2], Step [49270/64305], Loss: 4.9452\n",
      "Epoch [1/2], Step [49280/64305], Loss: 4.9219\n",
      "Epoch [1/2], Step [49290/64305], Loss: 4.7612\n",
      "Epoch [1/2], Step [49300/64305], Loss: 4.8908\n",
      "Epoch [1/2], Step [49310/64305], Loss: 4.8171\n",
      "Epoch [1/2], Step [49320/64305], Loss: 5.0463\n",
      "Epoch [1/2], Step [49330/64305], Loss: 4.9036\n",
      "Epoch [1/2], Step [49340/64305], Loss: 4.6284\n",
      "Epoch [1/2], Step [49350/64305], Loss: 4.9791\n",
      "Epoch [1/2], Step [49360/64305], Loss: 4.8528\n",
      "Epoch [1/2], Step [49370/64305], Loss: 4.7759\n",
      "Epoch [1/2], Step [49380/64305], Loss: 5.0779\n",
      "Epoch [1/2], Step [49390/64305], Loss: 4.7530\n",
      "Epoch [1/2], Step [49400/64305], Loss: 4.7366\n",
      "Epoch [1/2], Step [49410/64305], Loss: 4.7969\n",
      "Epoch [1/2], Step [49420/64305], Loss: 4.7932\n",
      "Epoch [1/2], Step [49430/64305], Loss: 4.8582\n",
      "Epoch [1/2], Step [49440/64305], Loss: 4.8846\n",
      "Epoch [1/2], Step [49450/64305], Loss: 4.8845\n",
      "Epoch [1/2], Step [49460/64305], Loss: 4.7948\n",
      "Epoch [1/2], Step [49470/64305], Loss: 4.8091\n",
      "Epoch [1/2], Step [49480/64305], Loss: 4.7536\n",
      "Epoch [1/2], Step [49490/64305], Loss: 4.5060\n",
      "Epoch [1/2], Step [49500/64305], Loss: 4.9778\n",
      "Epoch [1/2], Step [49510/64305], Loss: 4.8194\n",
      "Epoch [1/2], Step [49520/64305], Loss: 4.9931\n",
      "Epoch [1/2], Step [49530/64305], Loss: 4.8464\n",
      "Epoch [1/2], Step [49540/64305], Loss: 4.7040\n",
      "Epoch [1/2], Step [49550/64305], Loss: 4.7251\n",
      "Epoch [1/2], Step [49560/64305], Loss: 4.9980\n",
      "Epoch [1/2], Step [49570/64305], Loss: 4.8035\n",
      "Epoch [1/2], Step [49580/64305], Loss: 4.8943\n",
      "Epoch [1/2], Step [49590/64305], Loss: 4.9336\n",
      "Epoch [1/2], Step [49600/64305], Loss: 5.1672\n",
      "Epoch [1/2], Step [49610/64305], Loss: 4.9508\n",
      "Epoch [1/2], Step [49620/64305], Loss: 4.9733\n",
      "Epoch [1/2], Step [49630/64305], Loss: 4.8959\n",
      "Epoch [1/2], Step [49640/64305], Loss: 4.7730\n",
      "Epoch [1/2], Step [49650/64305], Loss: 5.0252\n",
      "Epoch [1/2], Step [49660/64305], Loss: 4.9388\n",
      "Epoch [1/2], Step [49670/64305], Loss: 4.8522\n",
      "Epoch [1/2], Step [49680/64305], Loss: 4.8100\n",
      "Epoch [1/2], Step [49690/64305], Loss: 5.0434\n",
      "Epoch [1/2], Step [49700/64305], Loss: 4.8222\n",
      "Epoch [1/2], Step [49710/64305], Loss: 4.8744\n",
      "Epoch [1/2], Step [49720/64305], Loss: 4.8088\n",
      "Epoch [1/2], Step [49730/64305], Loss: 4.8253\n",
      "Epoch [1/2], Step [49740/64305], Loss: 4.8024\n",
      "Epoch [1/2], Step [49750/64305], Loss: 4.8986\n",
      "Epoch [1/2], Step [49760/64305], Loss: 4.9544\n",
      "Epoch [1/2], Step [49770/64305], Loss: 4.7940\n",
      "Epoch [1/2], Step [49780/64305], Loss: 4.8663\n",
      "Epoch [1/2], Step [49790/64305], Loss: 4.9298\n",
      "Epoch [1/2], Step [49800/64305], Loss: 4.9032\n",
      "Epoch [1/2], Step [49810/64305], Loss: 5.0723\n",
      "Epoch [1/2], Step [49820/64305], Loss: 4.7559\n",
      "Epoch [1/2], Step [49830/64305], Loss: 4.9624\n",
      "Epoch [1/2], Step [49840/64305], Loss: 4.8564\n",
      "Epoch [1/2], Step [49850/64305], Loss: 4.7906\n",
      "Epoch [1/2], Step [49860/64305], Loss: 5.0253\n",
      "Epoch [1/2], Step [49870/64305], Loss: 4.7971\n",
      "Epoch [1/2], Step [49880/64305], Loss: 4.8514\n",
      "Epoch [1/2], Step [49890/64305], Loss: 5.0111\n",
      "Epoch [1/2], Step [49900/64305], Loss: 4.8416\n",
      "Epoch [1/2], Step [49910/64305], Loss: 4.8428\n",
      "Epoch [1/2], Step [49920/64305], Loss: 4.7949\n",
      "Epoch [1/2], Step [49930/64305], Loss: 4.9623\n",
      "Epoch [1/2], Step [49940/64305], Loss: 4.6510\n",
      "Epoch [1/2], Step [49950/64305], Loss: 4.9467\n",
      "Epoch [1/2], Step [49960/64305], Loss: 5.0471\n",
      "Epoch [1/2], Step [49970/64305], Loss: 4.8916\n",
      "Epoch [1/2], Step [49980/64305], Loss: 4.9790\n",
      "Epoch [1/2], Step [49990/64305], Loss: 4.8729\n",
      "Epoch [1/2], Step [50000/64305], Loss: 4.8717\n",
      "Epoch [1/2], Step [50010/64305], Loss: 4.7770\n",
      "Epoch [1/2], Step [50020/64305], Loss: 4.9381\n",
      "Epoch [1/2], Step [50030/64305], Loss: 4.9208\n",
      "Epoch [1/2], Step [50040/64305], Loss: 4.7276\n",
      "Epoch [1/2], Step [50050/64305], Loss: 4.9604\n",
      "Epoch [1/2], Step [50060/64305], Loss: 4.8206\n",
      "Epoch [1/2], Step [50070/64305], Loss: 4.7502\n",
      "Epoch [1/2], Step [50080/64305], Loss: 5.0393\n",
      "Epoch [1/2], Step [50090/64305], Loss: 5.0148\n",
      "Epoch [1/2], Step [50100/64305], Loss: 4.9224\n",
      "Epoch [1/2], Step [50110/64305], Loss: 4.9970\n",
      "Epoch [1/2], Step [50120/64305], Loss: 5.0084\n",
      "Epoch [1/2], Step [50130/64305], Loss: 4.8072\n",
      "Epoch [1/2], Step [50140/64305], Loss: 4.7495\n",
      "Epoch [1/2], Step [50150/64305], Loss: 4.9513\n",
      "Epoch [1/2], Step [50160/64305], Loss: 4.9091\n",
      "Epoch [1/2], Step [50170/64305], Loss: 4.6707\n",
      "Epoch [1/2], Step [50180/64305], Loss: 4.8507\n",
      "Epoch [1/2], Step [50190/64305], Loss: 5.0132\n",
      "Epoch [1/2], Step [50200/64305], Loss: 4.8003\n",
      "Epoch [1/2], Step [50210/64305], Loss: 4.9169\n",
      "Epoch [1/2], Step [50220/64305], Loss: 4.7468\n",
      "Epoch [1/2], Step [50230/64305], Loss: 5.2109\n",
      "Epoch [1/2], Step [50240/64305], Loss: 4.8201\n",
      "Epoch [1/2], Step [50250/64305], Loss: 4.7927\n",
      "Epoch [1/2], Step [50260/64305], Loss: 4.8157\n",
      "Epoch [1/2], Step [50270/64305], Loss: 4.9520\n",
      "Epoch [1/2], Step [50280/64305], Loss: 5.0254\n",
      "Epoch [1/2], Step [50290/64305], Loss: 4.9904\n",
      "Epoch [1/2], Step [50300/64305], Loss: 4.9889\n",
      "Epoch [1/2], Step [50310/64305], Loss: 4.6671\n",
      "Epoch [1/2], Step [50320/64305], Loss: 4.6990\n",
      "Epoch [1/2], Step [50330/64305], Loss: 5.1427\n",
      "Epoch [1/2], Step [50340/64305], Loss: 4.9137\n",
      "Epoch [1/2], Step [50350/64305], Loss: 4.6922\n",
      "Epoch [1/2], Step [50360/64305], Loss: 4.8744\n",
      "Epoch [1/2], Step [50370/64305], Loss: 4.8561\n",
      "Epoch [1/2], Step [50380/64305], Loss: 4.6571\n",
      "Epoch [1/2], Step [50390/64305], Loss: 4.9966\n",
      "Epoch [1/2], Step [50400/64305], Loss: 4.9597\n",
      "Epoch [1/2], Step [50410/64305], Loss: 4.5904\n",
      "Epoch [1/2], Step [50420/64305], Loss: 4.7692\n",
      "Epoch [1/2], Step [50430/64305], Loss: 4.9828\n",
      "Epoch [1/2], Step [50440/64305], Loss: 4.7320\n",
      "Epoch [1/2], Step [50450/64305], Loss: 4.7458\n",
      "Epoch [1/2], Step [50460/64305], Loss: 4.7889\n",
      "Epoch [1/2], Step [50470/64305], Loss: 4.8794\n",
      "Epoch [1/2], Step [50480/64305], Loss: 5.0061\n",
      "Epoch [1/2], Step [50490/64305], Loss: 4.8500\n",
      "Epoch [1/2], Step [50500/64305], Loss: 4.8596\n",
      "Epoch [1/2], Step [50510/64305], Loss: 4.8707\n",
      "Epoch [1/2], Step [50520/64305], Loss: 4.8335\n",
      "Epoch [1/2], Step [50530/64305], Loss: 4.9617\n",
      "Epoch [1/2], Step [50540/64305], Loss: 4.9061\n",
      "Epoch [1/2], Step [50550/64305], Loss: 4.8351\n",
      "Epoch [1/2], Step [50560/64305], Loss: 5.0044\n",
      "Epoch [1/2], Step [50570/64305], Loss: 4.8453\n",
      "Epoch [1/2], Step [50580/64305], Loss: 4.6448\n",
      "Epoch [1/2], Step [50590/64305], Loss: 4.8067\n",
      "Epoch [1/2], Step [50600/64305], Loss: 4.7365\n",
      "Epoch [1/2], Step [50610/64305], Loss: 5.0281\n",
      "Epoch [1/2], Step [50620/64305], Loss: 4.8570\n",
      "Epoch [1/2], Step [50630/64305], Loss: 4.8847\n",
      "Epoch [1/2], Step [50640/64305], Loss: 4.9577\n",
      "Epoch [1/2], Step [50650/64305], Loss: 4.9308\n",
      "Epoch [1/2], Step [50660/64305], Loss: 4.9263\n",
      "Epoch [1/2], Step [50670/64305], Loss: 4.9347\n",
      "Epoch [1/2], Step [50680/64305], Loss: 4.9419\n",
      "Epoch [1/2], Step [50690/64305], Loss: 4.9319\n",
      "Epoch [1/2], Step [50700/64305], Loss: 4.8347\n",
      "Epoch [1/2], Step [50710/64305], Loss: 4.8826\n",
      "Epoch [1/2], Step [50720/64305], Loss: 4.8711\n",
      "Epoch [1/2], Step [50730/64305], Loss: 4.8366\n",
      "Epoch [1/2], Step [50740/64305], Loss: 4.7104\n",
      "Epoch [1/2], Step [50750/64305], Loss: 4.8299\n",
      "Epoch [1/2], Step [50760/64305], Loss: 5.0486\n",
      "Epoch [1/2], Step [50770/64305], Loss: 4.9561\n",
      "Epoch [1/2], Step [50780/64305], Loss: 4.9052\n",
      "Epoch [1/2], Step [50790/64305], Loss: 4.7712\n",
      "Epoch [1/2], Step [50800/64305], Loss: 4.8929\n",
      "Epoch [1/2], Step [50810/64305], Loss: 4.8824\n",
      "Epoch [1/2], Step [50820/64305], Loss: 4.7032\n",
      "Epoch [1/2], Step [50830/64305], Loss: 4.8351\n",
      "Epoch [1/2], Step [50840/64305], Loss: 4.9560\n",
      "Epoch [1/2], Step [50850/64305], Loss: 4.9147\n",
      "Epoch [1/2], Step [50860/64305], Loss: 5.0248\n",
      "Epoch [1/2], Step [50870/64305], Loss: 4.8969\n",
      "Epoch [1/2], Step [50880/64305], Loss: 4.7968\n",
      "Epoch [1/2], Step [50890/64305], Loss: 4.7308\n",
      "Epoch [1/2], Step [50900/64305], Loss: 4.7547\n",
      "Epoch [1/2], Step [50910/64305], Loss: 4.8445\n",
      "Epoch [1/2], Step [50920/64305], Loss: 4.9159\n",
      "Epoch [1/2], Step [50930/64305], Loss: 4.9570\n",
      "Epoch [1/2], Step [50940/64305], Loss: 4.7465\n",
      "Epoch [1/2], Step [50950/64305], Loss: 4.9339\n",
      "Epoch [1/2], Step [50960/64305], Loss: 4.9267\n",
      "Epoch [1/2], Step [50970/64305], Loss: 5.0348\n",
      "Epoch [1/2], Step [50980/64305], Loss: 4.8244\n",
      "Epoch [1/2], Step [50990/64305], Loss: 4.5604\n",
      "Epoch [1/2], Step [51000/64305], Loss: 4.9366\n",
      "Epoch [1/2], Step [51010/64305], Loss: 4.9547\n",
      "Epoch [1/2], Step [51020/64305], Loss: 4.7741\n",
      "Epoch [1/2], Step [51030/64305], Loss: 4.8286\n",
      "Epoch [1/2], Step [51040/64305], Loss: 4.9355\n",
      "Epoch [1/2], Step [51050/64305], Loss: 5.0545\n",
      "Epoch [1/2], Step [51060/64305], Loss: 4.9498\n",
      "Epoch [1/2], Step [51070/64305], Loss: 4.7934\n",
      "Epoch [1/2], Step [51080/64305], Loss: 4.8377\n",
      "Epoch [1/2], Step [51090/64305], Loss: 4.9834\n",
      "Epoch [1/2], Step [51100/64305], Loss: 4.8568\n",
      "Epoch [1/2], Step [51110/64305], Loss: 4.7072\n",
      "Epoch [1/2], Step [51120/64305], Loss: 4.9201\n",
      "Epoch [1/2], Step [51130/64305], Loss: 5.0826\n",
      "Epoch [1/2], Step [51140/64305], Loss: 5.0052\n",
      "Epoch [1/2], Step [51150/64305], Loss: 4.8027\n",
      "Epoch [1/2], Step [51160/64305], Loss: 4.7111\n",
      "Epoch [1/2], Step [51170/64305], Loss: 4.8866\n",
      "Epoch [1/2], Step [51180/64305], Loss: 4.7508\n",
      "Epoch [1/2], Step [51190/64305], Loss: 4.9859\n",
      "Epoch [1/2], Step [51200/64305], Loss: 4.7287\n",
      "Epoch [1/2], Step [51210/64305], Loss: 5.0574\n",
      "Epoch [1/2], Step [51220/64305], Loss: 4.8346\n",
      "Epoch [1/2], Step [51230/64305], Loss: 4.8651\n",
      "Epoch [1/2], Step [51240/64305], Loss: 4.7720\n",
      "Epoch [1/2], Step [51250/64305], Loss: 4.8556\n",
      "Epoch [1/2], Step [51260/64305], Loss: 4.7611\n",
      "Epoch [1/2], Step [51270/64305], Loss: 4.9942\n",
      "Epoch [1/2], Step [51280/64305], Loss: 4.8950\n",
      "Epoch [1/2], Step [51290/64305], Loss: 4.8283\n",
      "Epoch [1/2], Step [51300/64305], Loss: 4.8821\n",
      "Epoch [1/2], Step [51310/64305], Loss: 4.7088\n",
      "Epoch [1/2], Step [51320/64305], Loss: 4.8170\n",
      "Epoch [1/2], Step [51330/64305], Loss: 4.8386\n",
      "Epoch [1/2], Step [51340/64305], Loss: 4.9419\n",
      "Epoch [1/2], Step [51350/64305], Loss: 4.9250\n",
      "Epoch [1/2], Step [51360/64305], Loss: 4.9161\n",
      "Epoch [1/2], Step [51370/64305], Loss: 4.7742\n",
      "Epoch [1/2], Step [51380/64305], Loss: 4.7624\n",
      "Epoch [1/2], Step [51390/64305], Loss: 4.7792\n",
      "Epoch [1/2], Step [51400/64305], Loss: 4.8305\n",
      "Epoch [1/2], Step [51410/64305], Loss: 4.8735\n",
      "Epoch [1/2], Step [51420/64305], Loss: 4.9924\n",
      "Epoch [1/2], Step [51430/64305], Loss: 4.8106\n",
      "Epoch [1/2], Step [51440/64305], Loss: 4.9937\n",
      "Epoch [1/2], Step [51450/64305], Loss: 4.6854\n",
      "Epoch [1/2], Step [51460/64305], Loss: 4.9644\n",
      "Epoch [1/2], Step [51470/64305], Loss: 5.0195\n",
      "Epoch [1/2], Step [51480/64305], Loss: 4.8794\n",
      "Epoch [1/2], Step [51490/64305], Loss: 4.7442\n",
      "Epoch [1/2], Step [51500/64305], Loss: 4.8313\n",
      "Epoch [1/2], Step [51510/64305], Loss: 4.9631\n",
      "Epoch [1/2], Step [51520/64305], Loss: 4.9985\n",
      "Epoch [1/2], Step [51530/64305], Loss: 4.7721\n",
      "Epoch [1/2], Step [51540/64305], Loss: 4.9177\n",
      "Epoch [1/2], Step [51550/64305], Loss: 4.9271\n",
      "Epoch [1/2], Step [51560/64305], Loss: 4.8151\n",
      "Epoch [1/2], Step [51570/64305], Loss: 4.9566\n",
      "Epoch [1/2], Step [51580/64305], Loss: 4.8410\n",
      "Epoch [1/2], Step [51590/64305], Loss: 4.9799\n",
      "Epoch [1/2], Step [51600/64305], Loss: 4.8584\n",
      "Epoch [1/2], Step [51610/64305], Loss: 4.9530\n",
      "Epoch [1/2], Step [51620/64305], Loss: 4.8997\n",
      "Epoch [1/2], Step [51630/64305], Loss: 4.8971\n",
      "Epoch [1/2], Step [51640/64305], Loss: 4.8863\n",
      "Epoch [1/2], Step [51650/64305], Loss: 4.8759\n",
      "Epoch [1/2], Step [51660/64305], Loss: 4.8897\n",
      "Epoch [1/2], Step [51670/64305], Loss: 4.9721\n",
      "Epoch [1/2], Step [51680/64305], Loss: 4.9094\n",
      "Epoch [1/2], Step [51690/64305], Loss: 4.9267\n",
      "Epoch [1/2], Step [51700/64305], Loss: 4.8495\n",
      "Epoch [1/2], Step [51710/64305], Loss: 5.0145\n",
      "Epoch [1/2], Step [51720/64305], Loss: 4.9512\n",
      "Epoch [1/2], Step [51730/64305], Loss: 4.9739\n",
      "Epoch [1/2], Step [51740/64305], Loss: 4.8842\n",
      "Epoch [1/2], Step [51750/64305], Loss: 4.8257\n",
      "Epoch [1/2], Step [51760/64305], Loss: 4.8853\n",
      "Epoch [1/2], Step [51770/64305], Loss: 4.7454\n",
      "Epoch [1/2], Step [51780/64305], Loss: 4.9963\n",
      "Epoch [1/2], Step [51790/64305], Loss: 4.9296\n",
      "Epoch [1/2], Step [51800/64305], Loss: 4.9205\n",
      "Epoch [1/2], Step [51810/64305], Loss: 4.8902\n",
      "Epoch [1/2], Step [51820/64305], Loss: 4.8204\n",
      "Epoch [1/2], Step [51830/64305], Loss: 4.7897\n",
      "Epoch [1/2], Step [51840/64305], Loss: 4.7790\n",
      "Epoch [1/2], Step [51850/64305], Loss: 4.6821\n",
      "Epoch [1/2], Step [51860/64305], Loss: 4.6778\n",
      "Epoch [1/2], Step [51870/64305], Loss: 4.8727\n",
      "Epoch [1/2], Step [51880/64305], Loss: 4.5774\n",
      "Epoch [1/2], Step [51890/64305], Loss: 4.8069\n",
      "Epoch [1/2], Step [51900/64305], Loss: 4.6551\n",
      "Epoch [1/2], Step [51910/64305], Loss: 4.9448\n",
      "Epoch [1/2], Step [51920/64305], Loss: 4.9217\n",
      "Epoch [1/2], Step [51930/64305], Loss: 5.0212\n",
      "Epoch [1/2], Step [51940/64305], Loss: 4.7242\n",
      "Epoch [1/2], Step [51950/64305], Loss: 4.7146\n",
      "Epoch [1/2], Step [51960/64305], Loss: 4.8904\n",
      "Epoch [1/2], Step [51970/64305], Loss: 4.8969\n",
      "Epoch [1/2], Step [51980/64305], Loss: 5.0243\n",
      "Epoch [1/2], Step [51990/64305], Loss: 4.8865\n",
      "Epoch [1/2], Step [52000/64305], Loss: 5.0243\n",
      "Epoch [1/2], Step [52010/64305], Loss: 4.7907\n",
      "Epoch [1/2], Step [52020/64305], Loss: 4.8655\n",
      "Epoch [1/2], Step [52030/64305], Loss: 4.8584\n",
      "Epoch [1/2], Step [52040/64305], Loss: 4.7404\n",
      "Epoch [1/2], Step [52050/64305], Loss: 5.0344\n",
      "Epoch [1/2], Step [52060/64305], Loss: 4.5298\n",
      "Epoch [1/2], Step [52070/64305], Loss: 4.7569\n",
      "Epoch [1/2], Step [52080/64305], Loss: 5.1028\n",
      "Epoch [1/2], Step [52090/64305], Loss: 4.9257\n",
      "Epoch [1/2], Step [52100/64305], Loss: 4.9095\n",
      "Epoch [1/2], Step [52110/64305], Loss: 4.9045\n",
      "Epoch [1/2], Step [52120/64305], Loss: 4.9141\n",
      "Epoch [1/2], Step [52130/64305], Loss: 4.9563\n",
      "Epoch [1/2], Step [52140/64305], Loss: 4.9106\n",
      "Epoch [1/2], Step [52150/64305], Loss: 5.0491\n",
      "Epoch [1/2], Step [52160/64305], Loss: 4.7065\n",
      "Epoch [1/2], Step [52170/64305], Loss: 5.0405\n",
      "Epoch [1/2], Step [52180/64305], Loss: 4.7345\n",
      "Epoch [1/2], Step [52190/64305], Loss: 5.0143\n",
      "Epoch [1/2], Step [52200/64305], Loss: 4.9913\n",
      "Epoch [1/2], Step [52210/64305], Loss: 4.9319\n",
      "Epoch [1/2], Step [52220/64305], Loss: 4.8650\n",
      "Epoch [1/2], Step [52230/64305], Loss: 4.7432\n",
      "Epoch [1/2], Step [52240/64305], Loss: 4.9067\n",
      "Epoch [1/2], Step [52250/64305], Loss: 4.8386\n",
      "Epoch [1/2], Step [52260/64305], Loss: 4.7792\n",
      "Epoch [1/2], Step [52270/64305], Loss: 4.9952\n",
      "Epoch [1/2], Step [52280/64305], Loss: 4.9626\n",
      "Epoch [1/2], Step [52290/64305], Loss: 4.7149\n",
      "Epoch [1/2], Step [52300/64305], Loss: 4.8277\n",
      "Epoch [1/2], Step [52310/64305], Loss: 4.8292\n",
      "Epoch [1/2], Step [52320/64305], Loss: 4.9797\n",
      "Epoch [1/2], Step [52330/64305], Loss: 5.0737\n",
      "Epoch [1/2], Step [52340/64305], Loss: 4.6280\n",
      "Epoch [1/2], Step [52350/64305], Loss: 4.7001\n",
      "Epoch [1/2], Step [52360/64305], Loss: 4.6280\n",
      "Epoch [1/2], Step [52370/64305], Loss: 4.9774\n",
      "Epoch [1/2], Step [52380/64305], Loss: 4.9174\n",
      "Epoch [1/2], Step [52390/64305], Loss: 4.9241\n",
      "Epoch [1/2], Step [52400/64305], Loss: 5.0499\n",
      "Epoch [1/2], Step [52410/64305], Loss: 4.8439\n",
      "Epoch [1/2], Step [52420/64305], Loss: 4.9588\n",
      "Epoch [1/2], Step [52430/64305], Loss: 5.0322\n",
      "Epoch [1/2], Step [52440/64305], Loss: 4.9394\n",
      "Epoch [1/2], Step [52450/64305], Loss: 5.0177\n",
      "Epoch [1/2], Step [52460/64305], Loss: 4.7827\n",
      "Epoch [1/2], Step [52470/64305], Loss: 4.9011\n",
      "Epoch [1/2], Step [52480/64305], Loss: 4.8832\n",
      "Epoch [1/2], Step [52490/64305], Loss: 4.8724\n",
      "Epoch [1/2], Step [52500/64305], Loss: 4.7363\n",
      "Epoch [1/2], Step [52510/64305], Loss: 4.8968\n",
      "Epoch [1/2], Step [52520/64305], Loss: 4.8062\n",
      "Epoch [1/2], Step [52530/64305], Loss: 5.0699\n",
      "Epoch [1/2], Step [52540/64305], Loss: 4.7781\n",
      "Epoch [1/2], Step [52550/64305], Loss: 4.8768\n",
      "Epoch [1/2], Step [52560/64305], Loss: 4.9940\n",
      "Epoch [1/2], Step [52570/64305], Loss: 4.9493\n",
      "Epoch [1/2], Step [52580/64305], Loss: 4.8466\n",
      "Epoch [1/2], Step [52590/64305], Loss: 4.8694\n",
      "Epoch [1/2], Step [52600/64305], Loss: 4.7347\n",
      "Epoch [1/2], Step [52610/64305], Loss: 4.8497\n",
      "Epoch [1/2], Step [52620/64305], Loss: 5.0331\n",
      "Epoch [1/2], Step [52630/64305], Loss: 4.8218\n",
      "Epoch [1/2], Step [52640/64305], Loss: 4.9940\n",
      "Epoch [1/2], Step [52650/64305], Loss: 5.1127\n",
      "Epoch [1/2], Step [52660/64305], Loss: 4.7921\n",
      "Epoch [1/2], Step [52670/64305], Loss: 4.7037\n",
      "Epoch [1/2], Step [52680/64305], Loss: 5.0724\n",
      "Epoch [1/2], Step [52690/64305], Loss: 4.8530\n",
      "Epoch [1/2], Step [52700/64305], Loss: 4.9554\n",
      "Epoch [1/2], Step [52710/64305], Loss: 4.8481\n",
      "Epoch [1/2], Step [52720/64305], Loss: 4.9701\n",
      "Epoch [1/2], Step [52730/64305], Loss: 4.8386\n",
      "Epoch [1/2], Step [52740/64305], Loss: 5.0151\n",
      "Epoch [1/2], Step [52750/64305], Loss: 4.9345\n",
      "Epoch [1/2], Step [52760/64305], Loss: 4.7535\n",
      "Epoch [1/2], Step [52770/64305], Loss: 4.8141\n",
      "Epoch [1/2], Step [52780/64305], Loss: 4.7691\n",
      "Epoch [1/2], Step [52790/64305], Loss: 4.7484\n",
      "Epoch [1/2], Step [52800/64305], Loss: 4.8239\n",
      "Epoch [1/2], Step [52810/64305], Loss: 4.7349\n",
      "Epoch [1/2], Step [52820/64305], Loss: 4.7675\n",
      "Epoch [1/2], Step [52830/64305], Loss: 4.7419\n",
      "Epoch [1/2], Step [52840/64305], Loss: 4.9703\n",
      "Epoch [1/2], Step [52850/64305], Loss: 4.6158\n",
      "Epoch [1/2], Step [52860/64305], Loss: 4.9099\n",
      "Epoch [1/2], Step [52870/64305], Loss: 4.6585\n",
      "Epoch [1/2], Step [52880/64305], Loss: 4.9134\n",
      "Epoch [1/2], Step [52890/64305], Loss: 4.7065\n",
      "Epoch [1/2], Step [52900/64305], Loss: 4.9674\n",
      "Epoch [1/2], Step [52910/64305], Loss: 4.7629\n",
      "Epoch [1/2], Step [52920/64305], Loss: 5.0646\n",
      "Epoch [1/2], Step [52930/64305], Loss: 4.8723\n",
      "Epoch [1/2], Step [52940/64305], Loss: 4.8635\n",
      "Epoch [1/2], Step [52950/64305], Loss: 4.9460\n",
      "Epoch [1/2], Step [52960/64305], Loss: 4.7602\n",
      "Epoch [1/2], Step [52970/64305], Loss: 4.7972\n",
      "Epoch [1/2], Step [52980/64305], Loss: 4.9579\n",
      "Epoch [1/2], Step [52990/64305], Loss: 5.0246\n",
      "Epoch [1/2], Step [53000/64305], Loss: 4.8292\n",
      "Epoch [1/2], Step [53010/64305], Loss: 5.0661\n",
      "Epoch [1/2], Step [53020/64305], Loss: 4.8597\n",
      "Epoch [1/2], Step [53030/64305], Loss: 4.9924\n",
      "Epoch [1/2], Step [53040/64305], Loss: 5.0685\n",
      "Epoch [1/2], Step [53050/64305], Loss: 4.7816\n",
      "Epoch [1/2], Step [53060/64305], Loss: 4.8313\n",
      "Epoch [1/2], Step [53070/64305], Loss: 4.5853\n",
      "Epoch [1/2], Step [53080/64305], Loss: 4.8923\n",
      "Epoch [1/2], Step [53090/64305], Loss: 4.7946\n",
      "Epoch [1/2], Step [53100/64305], Loss: 4.9090\n",
      "Epoch [1/2], Step [53110/64305], Loss: 4.8982\n",
      "Epoch [1/2], Step [53120/64305], Loss: 4.9089\n",
      "Epoch [1/2], Step [53130/64305], Loss: 4.7564\n",
      "Epoch [1/2], Step [53140/64305], Loss: 4.8148\n",
      "Epoch [1/2], Step [53150/64305], Loss: 4.9299\n",
      "Epoch [1/2], Step [53160/64305], Loss: 4.8027\n",
      "Epoch [1/2], Step [53170/64305], Loss: 4.7748\n",
      "Epoch [1/2], Step [53180/64305], Loss: 4.8243\n",
      "Epoch [1/2], Step [53190/64305], Loss: 4.8070\n",
      "Epoch [1/2], Step [53200/64305], Loss: 4.9688\n",
      "Epoch [1/2], Step [53210/64305], Loss: 4.9750\n",
      "Epoch [1/2], Step [53220/64305], Loss: 4.8220\n",
      "Epoch [1/2], Step [53230/64305], Loss: 4.8846\n",
      "Epoch [1/2], Step [53240/64305], Loss: 5.0157\n",
      "Epoch [1/2], Step [53250/64305], Loss: 4.7475\n",
      "Epoch [1/2], Step [53260/64305], Loss: 4.7096\n",
      "Epoch [1/2], Step [53270/64305], Loss: 4.7229\n",
      "Epoch [1/2], Step [53280/64305], Loss: 4.9301\n",
      "Epoch [1/2], Step [53290/64305], Loss: 5.0234\n",
      "Epoch [1/2], Step [53300/64305], Loss: 4.9348\n",
      "Epoch [1/2], Step [53310/64305], Loss: 4.8259\n",
      "Epoch [1/2], Step [53320/64305], Loss: 4.8199\n",
      "Epoch [1/2], Step [53330/64305], Loss: 4.9329\n",
      "Epoch [1/2], Step [53340/64305], Loss: 4.7048\n",
      "Epoch [1/2], Step [53350/64305], Loss: 4.7493\n",
      "Epoch [1/2], Step [53360/64305], Loss: 4.8533\n",
      "Epoch [1/2], Step [53370/64305], Loss: 4.8179\n",
      "Epoch [1/2], Step [53380/64305], Loss: 4.8022\n",
      "Epoch [1/2], Step [53390/64305], Loss: 4.9286\n",
      "Epoch [1/2], Step [53400/64305], Loss: 4.6889\n",
      "Epoch [1/2], Step [53410/64305], Loss: 4.8590\n",
      "Epoch [1/2], Step [53420/64305], Loss: 4.8270\n",
      "Epoch [1/2], Step [53430/64305], Loss: 4.8182\n",
      "Epoch [1/2], Step [53440/64305], Loss: 4.9384\n",
      "Epoch [1/2], Step [53450/64305], Loss: 4.8675\n",
      "Epoch [1/2], Step [53460/64305], Loss: 4.8662\n",
      "Epoch [1/2], Step [53470/64305], Loss: 4.8412\n",
      "Epoch [1/2], Step [53480/64305], Loss: 4.8001\n",
      "Epoch [1/2], Step [53490/64305], Loss: 5.2045\n",
      "Epoch [1/2], Step [53500/64305], Loss: 4.9189\n",
      "Epoch [1/2], Step [53510/64305], Loss: 4.6683\n",
      "Epoch [1/2], Step [53520/64305], Loss: 4.9289\n",
      "Epoch [1/2], Step [53530/64305], Loss: 4.9261\n",
      "Epoch [1/2], Step [53540/64305], Loss: 5.0722\n",
      "Epoch [1/2], Step [53550/64305], Loss: 4.8528\n",
      "Epoch [1/2], Step [53560/64305], Loss: 4.9962\n",
      "Epoch [1/2], Step [53570/64305], Loss: 4.7278\n",
      "Epoch [1/2], Step [53580/64305], Loss: 4.8847\n",
      "Epoch [1/2], Step [53590/64305], Loss: 4.7787\n",
      "Epoch [1/2], Step [53600/64305], Loss: 4.6413\n",
      "Epoch [1/2], Step [53610/64305], Loss: 4.8635\n",
      "Epoch [1/2], Step [53620/64305], Loss: 5.0127\n",
      "Epoch [1/2], Step [53630/64305], Loss: 5.1052\n",
      "Epoch [1/2], Step [53640/64305], Loss: 4.7889\n",
      "Epoch [1/2], Step [53650/64305], Loss: 4.7427\n",
      "Epoch [1/2], Step [53660/64305], Loss: 4.8404\n",
      "Epoch [1/2], Step [53670/64305], Loss: 5.0211\n",
      "Epoch [1/2], Step [53680/64305], Loss: 4.9148\n",
      "Epoch [1/2], Step [53690/64305], Loss: 4.8085\n",
      "Epoch [1/2], Step [53700/64305], Loss: 4.7812\n",
      "Epoch [1/2], Step [53710/64305], Loss: 4.8686\n",
      "Epoch [1/2], Step [53720/64305], Loss: 4.8563\n",
      "Epoch [1/2], Step [53730/64305], Loss: 4.7861\n",
      "Epoch [1/2], Step [53740/64305], Loss: 4.7945\n",
      "Epoch [1/2], Step [53750/64305], Loss: 4.8467\n",
      "Epoch [1/2], Step [53760/64305], Loss: 4.7521\n",
      "Epoch [1/2], Step [53770/64305], Loss: 4.7679\n",
      "Epoch [1/2], Step [53780/64305], Loss: 4.7668\n",
      "Epoch [1/2], Step [53790/64305], Loss: 4.8768\n",
      "Epoch [1/2], Step [53800/64305], Loss: 4.7658\n",
      "Epoch [1/2], Step [53810/64305], Loss: 5.0095\n",
      "Epoch [1/2], Step [53820/64305], Loss: 4.6672\n",
      "Epoch [1/2], Step [53830/64305], Loss: 4.9218\n",
      "Epoch [1/2], Step [53840/64305], Loss: 4.8218\n",
      "Epoch [1/2], Step [53850/64305], Loss: 4.7971\n",
      "Epoch [1/2], Step [53860/64305], Loss: 4.7352\n",
      "Epoch [1/2], Step [53870/64305], Loss: 4.9326\n",
      "Epoch [1/2], Step [53880/64305], Loss: 4.9829\n",
      "Epoch [1/2], Step [53890/64305], Loss: 4.6832\n",
      "Epoch [1/2], Step [53900/64305], Loss: 4.7206\n",
      "Epoch [1/2], Step [53910/64305], Loss: 4.8265\n",
      "Epoch [1/2], Step [53920/64305], Loss: 5.0838\n",
      "Epoch [1/2], Step [53930/64305], Loss: 5.0251\n",
      "Epoch [1/2], Step [53940/64305], Loss: 4.7376\n",
      "Epoch [1/2], Step [53950/64305], Loss: 4.8485\n",
      "Epoch [1/2], Step [53960/64305], Loss: 4.8870\n",
      "Epoch [1/2], Step [53970/64305], Loss: 4.7388\n",
      "Epoch [1/2], Step [53980/64305], Loss: 5.0534\n",
      "Epoch [1/2], Step [53990/64305], Loss: 4.7483\n",
      "Epoch [1/2], Step [54000/64305], Loss: 4.8663\n",
      "Epoch [1/2], Step [54010/64305], Loss: 5.0564\n",
      "Epoch [1/2], Step [54020/64305], Loss: 4.7791\n",
      "Epoch [1/2], Step [54030/64305], Loss: 4.9377\n",
      "Epoch [1/2], Step [54040/64305], Loss: 4.9515\n",
      "Epoch [1/2], Step [54050/64305], Loss: 4.9320\n",
      "Epoch [1/2], Step [54060/64305], Loss: 4.9229\n",
      "Epoch [1/2], Step [54070/64305], Loss: 4.9750\n",
      "Epoch [1/2], Step [54080/64305], Loss: 4.7638\n",
      "Epoch [1/2], Step [54090/64305], Loss: 4.7652\n",
      "Epoch [1/2], Step [54100/64305], Loss: 4.8495\n",
      "Epoch [1/2], Step [54110/64305], Loss: 4.6947\n",
      "Epoch [1/2], Step [54120/64305], Loss: 4.8335\n",
      "Epoch [1/2], Step [54130/64305], Loss: 4.9590\n",
      "Epoch [1/2], Step [54140/64305], Loss: 4.8915\n",
      "Epoch [1/2], Step [54150/64305], Loss: 4.9348\n",
      "Epoch [1/2], Step [54160/64305], Loss: 4.8273\n",
      "Epoch [1/2], Step [54170/64305], Loss: 5.1053\n",
      "Epoch [1/2], Step [54180/64305], Loss: 5.0294\n",
      "Epoch [1/2], Step [54190/64305], Loss: 4.9777\n",
      "Epoch [1/2], Step [54200/64305], Loss: 4.6928\n",
      "Epoch [1/2], Step [54210/64305], Loss: 4.9725\n",
      "Epoch [1/2], Step [54220/64305], Loss: 4.9407\n",
      "Epoch [1/2], Step [54230/64305], Loss: 4.6701\n",
      "Epoch [1/2], Step [54240/64305], Loss: 4.8441\n",
      "Epoch [1/2], Step [54250/64305], Loss: 4.9161\n",
      "Epoch [1/2], Step [54260/64305], Loss: 4.8097\n",
      "Epoch [1/2], Step [54270/64305], Loss: 4.7421\n",
      "Epoch [1/2], Step [54280/64305], Loss: 4.9147\n",
      "Epoch [1/2], Step [54290/64305], Loss: 4.6877\n",
      "Epoch [1/2], Step [54300/64305], Loss: 4.8823\n",
      "Epoch [1/2], Step [54310/64305], Loss: 4.9897\n",
      "Epoch [1/2], Step [54320/64305], Loss: 4.9627\n",
      "Epoch [1/2], Step [54330/64305], Loss: 4.6852\n",
      "Epoch [1/2], Step [54340/64305], Loss: 4.9023\n",
      "Epoch [1/2], Step [54350/64305], Loss: 4.9046\n",
      "Epoch [1/2], Step [54360/64305], Loss: 4.7315\n",
      "Epoch [1/2], Step [54370/64305], Loss: 4.9398\n",
      "Epoch [1/2], Step [54380/64305], Loss: 4.9495\n",
      "Epoch [1/2], Step [54390/64305], Loss: 4.9035\n",
      "Epoch [1/2], Step [54400/64305], Loss: 4.8752\n",
      "Epoch [1/2], Step [54410/64305], Loss: 4.8966\n",
      "Epoch [1/2], Step [54420/64305], Loss: 4.9915\n",
      "Epoch [1/2], Step [54430/64305], Loss: 4.9383\n",
      "Epoch [1/2], Step [54440/64305], Loss: 4.6093\n",
      "Epoch [1/2], Step [54450/64305], Loss: 4.9555\n",
      "Epoch [1/2], Step [54460/64305], Loss: 4.9261\n",
      "Epoch [1/2], Step [54470/64305], Loss: 4.7755\n",
      "Epoch [1/2], Step [54480/64305], Loss: 4.8559\n",
      "Epoch [1/2], Step [54490/64305], Loss: 4.8383\n",
      "Epoch [1/2], Step [54500/64305], Loss: 5.0118\n",
      "Epoch [1/2], Step [54510/64305], Loss: 4.7820\n",
      "Epoch [1/2], Step [54520/64305], Loss: 4.7810\n",
      "Epoch [1/2], Step [54530/64305], Loss: 4.5509\n",
      "Epoch [1/2], Step [54540/64305], Loss: 4.9846\n",
      "Epoch [1/2], Step [54550/64305], Loss: 4.8097\n",
      "Epoch [1/2], Step [54560/64305], Loss: 4.8507\n",
      "Epoch [1/2], Step [54570/64305], Loss: 5.1254\n",
      "Epoch [1/2], Step [54580/64305], Loss: 4.8748\n",
      "Epoch [1/2], Step [54590/64305], Loss: 4.8679\n",
      "Epoch [1/2], Step [54600/64305], Loss: 4.7361\n",
      "Epoch [1/2], Step [54610/64305], Loss: 4.6596\n",
      "Epoch [1/2], Step [54620/64305], Loss: 4.7412\n",
      "Epoch [1/2], Step [54630/64305], Loss: 4.8760\n",
      "Epoch [1/2], Step [54640/64305], Loss: 4.8693\n",
      "Epoch [1/2], Step [54650/64305], Loss: 4.7066\n",
      "Epoch [1/2], Step [54660/64305], Loss: 4.7571\n",
      "Epoch [1/2], Step [54670/64305], Loss: 5.0715\n",
      "Epoch [1/2], Step [54680/64305], Loss: 5.0886\n",
      "Epoch [1/2], Step [54690/64305], Loss: 4.6759\n",
      "Epoch [1/2], Step [54700/64305], Loss: 4.8790\n",
      "Epoch [1/2], Step [54710/64305], Loss: 5.0090\n",
      "Epoch [1/2], Step [54720/64305], Loss: 4.9201\n",
      "Epoch [1/2], Step [54730/64305], Loss: 4.8197\n",
      "Epoch [1/2], Step [54740/64305], Loss: 4.7826\n",
      "Epoch [1/2], Step [54750/64305], Loss: 5.0973\n",
      "Epoch [1/2], Step [54760/64305], Loss: 4.8766\n",
      "Epoch [1/2], Step [54770/64305], Loss: 4.9106\n",
      "Epoch [1/2], Step [54780/64305], Loss: 4.7276\n",
      "Epoch [1/2], Step [54790/64305], Loss: 4.9394\n",
      "Epoch [1/2], Step [54800/64305], Loss: 4.8181\n",
      "Epoch [1/2], Step [54810/64305], Loss: 4.8539\n",
      "Epoch [1/2], Step [54820/64305], Loss: 4.8213\n",
      "Epoch [1/2], Step [54830/64305], Loss: 4.7233\n",
      "Epoch [1/2], Step [54840/64305], Loss: 4.9958\n",
      "Epoch [1/2], Step [54850/64305], Loss: 4.9407\n",
      "Epoch [1/2], Step [54860/64305], Loss: 4.8513\n",
      "Epoch [1/2], Step [54870/64305], Loss: 4.8145\n",
      "Epoch [1/2], Step [54880/64305], Loss: 4.8996\n",
      "Epoch [1/2], Step [54890/64305], Loss: 4.8856\n",
      "Epoch [1/2], Step [54900/64305], Loss: 5.1042\n",
      "Epoch [1/2], Step [54910/64305], Loss: 4.7416\n",
      "Epoch [1/2], Step [54920/64305], Loss: 4.9038\n",
      "Epoch [1/2], Step [54930/64305], Loss: 4.8056\n",
      "Epoch [1/2], Step [54940/64305], Loss: 4.7796\n",
      "Epoch [1/2], Step [54950/64305], Loss: 4.8677\n",
      "Epoch [1/2], Step [54960/64305], Loss: 4.9499\n",
      "Epoch [1/2], Step [54970/64305], Loss: 5.0757\n",
      "Epoch [1/2], Step [54980/64305], Loss: 4.8567\n",
      "Epoch [1/2], Step [54990/64305], Loss: 4.8951\n",
      "Epoch [1/2], Step [55000/64305], Loss: 4.7160\n",
      "Epoch [1/2], Step [55010/64305], Loss: 4.7396\n",
      "Epoch [1/2], Step [55020/64305], Loss: 4.8438\n",
      "Epoch [1/2], Step [55030/64305], Loss: 4.8257\n",
      "Epoch [1/2], Step [55040/64305], Loss: 4.8963\n",
      "Epoch [1/2], Step [55050/64305], Loss: 4.8440\n",
      "Epoch [1/2], Step [55060/64305], Loss: 4.7366\n",
      "Epoch [1/2], Step [55070/64305], Loss: 4.9276\n",
      "Epoch [1/2], Step [55080/64305], Loss: 4.8308\n",
      "Epoch [1/2], Step [55090/64305], Loss: 4.7535\n",
      "Epoch [1/2], Step [55100/64305], Loss: 4.7033\n",
      "Epoch [1/2], Step [55110/64305], Loss: 4.8289\n",
      "Epoch [1/2], Step [55120/64305], Loss: 4.8319\n",
      "Epoch [1/2], Step [55130/64305], Loss: 4.8612\n",
      "Epoch [1/2], Step [55140/64305], Loss: 4.7405\n",
      "Epoch [1/2], Step [55150/64305], Loss: 4.9221\n",
      "Epoch [1/2], Step [55160/64305], Loss: 4.9782\n",
      "Epoch [1/2], Step [55170/64305], Loss: 4.9060\n",
      "Epoch [1/2], Step [55180/64305], Loss: 4.7977\n",
      "Epoch [1/2], Step [55190/64305], Loss: 4.9333\n",
      "Epoch [1/2], Step [55200/64305], Loss: 4.7408\n",
      "Epoch [1/2], Step [55210/64305], Loss: 4.8138\n",
      "Epoch [1/2], Step [55220/64305], Loss: 4.8539\n",
      "Epoch [1/2], Step [55230/64305], Loss: 4.8245\n",
      "Epoch [1/2], Step [55240/64305], Loss: 4.7682\n",
      "Epoch [1/2], Step [55250/64305], Loss: 4.7108\n",
      "Epoch [1/2], Step [55260/64305], Loss: 4.9319\n",
      "Epoch [1/2], Step [55270/64305], Loss: 5.0487\n",
      "Epoch [1/2], Step [55280/64305], Loss: 4.9751\n",
      "Epoch [1/2], Step [55290/64305], Loss: 4.9051\n",
      "Epoch [1/2], Step [55300/64305], Loss: 4.7364\n",
      "Epoch [1/2], Step [55310/64305], Loss: 4.9023\n",
      "Epoch [1/2], Step [55320/64305], Loss: 5.0177\n",
      "Epoch [1/2], Step [55330/64305], Loss: 4.8913\n",
      "Epoch [1/2], Step [55340/64305], Loss: 4.7905\n",
      "Epoch [1/2], Step [55350/64305], Loss: 4.9703\n",
      "Epoch [1/2], Step [55360/64305], Loss: 4.7292\n",
      "Epoch [1/2], Step [55370/64305], Loss: 4.9662\n",
      "Epoch [1/2], Step [55380/64305], Loss: 4.8959\n",
      "Epoch [1/2], Step [55390/64305], Loss: 4.7785\n",
      "Epoch [1/2], Step [55400/64305], Loss: 4.7572\n",
      "Epoch [1/2], Step [55410/64305], Loss: 4.8704\n",
      "Epoch [1/2], Step [55420/64305], Loss: 4.8070\n",
      "Epoch [1/2], Step [55430/64305], Loss: 4.9179\n",
      "Epoch [1/2], Step [55440/64305], Loss: 5.0344\n",
      "Epoch [1/2], Step [55450/64305], Loss: 4.7698\n",
      "Epoch [1/2], Step [55460/64305], Loss: 4.8181\n",
      "Epoch [1/2], Step [55470/64305], Loss: 4.9022\n",
      "Epoch [1/2], Step [55480/64305], Loss: 4.9748\n",
      "Epoch [1/2], Step [55490/64305], Loss: 4.9209\n",
      "Epoch [1/2], Step [55500/64305], Loss: 4.7984\n",
      "Epoch [1/2], Step [55510/64305], Loss: 4.8180\n",
      "Epoch [1/2], Step [55520/64305], Loss: 5.0189\n",
      "Epoch [1/2], Step [55530/64305], Loss: 4.8843\n",
      "Epoch [1/2], Step [55540/64305], Loss: 4.8862\n",
      "Epoch [1/2], Step [55550/64305], Loss: 4.6472\n",
      "Epoch [1/2], Step [55560/64305], Loss: 4.8307\n",
      "Epoch [1/2], Step [55570/64305], Loss: 4.7756\n",
      "Epoch [1/2], Step [55580/64305], Loss: 5.0676\n",
      "Epoch [1/2], Step [55590/64305], Loss: 4.8647\n",
      "Epoch [1/2], Step [55600/64305], Loss: 4.7913\n",
      "Epoch [1/2], Step [55610/64305], Loss: 4.9166\n",
      "Epoch [1/2], Step [55620/64305], Loss: 4.6794\n",
      "Epoch [1/2], Step [55630/64305], Loss: 4.8832\n",
      "Epoch [1/2], Step [55640/64305], Loss: 4.7701\n",
      "Epoch [1/2], Step [55650/64305], Loss: 4.8606\n",
      "Epoch [1/2], Step [55660/64305], Loss: 4.5892\n",
      "Epoch [1/2], Step [55670/64305], Loss: 4.8419\n",
      "Epoch [1/2], Step [55680/64305], Loss: 4.8738\n",
      "Epoch [1/2], Step [55690/64305], Loss: 5.0363\n",
      "Epoch [1/2], Step [55700/64305], Loss: 4.6750\n",
      "Epoch [1/2], Step [55710/64305], Loss: 4.8083\n",
      "Epoch [1/2], Step [55720/64305], Loss: 4.8741\n",
      "Epoch [1/2], Step [55730/64305], Loss: 4.8672\n",
      "Epoch [1/2], Step [55740/64305], Loss: 4.8402\n",
      "Epoch [1/2], Step [55750/64305], Loss: 4.8737\n",
      "Epoch [1/2], Step [55760/64305], Loss: 4.9709\n",
      "Epoch [1/2], Step [55770/64305], Loss: 4.8904\n",
      "Epoch [1/2], Step [55780/64305], Loss: 4.6426\n",
      "Epoch [1/2], Step [55790/64305], Loss: 4.9454\n",
      "Epoch [1/2], Step [55800/64305], Loss: 4.7236\n",
      "Epoch [1/2], Step [55810/64305], Loss: 4.7427\n",
      "Epoch [1/2], Step [55820/64305], Loss: 4.6886\n",
      "Epoch [1/2], Step [55830/64305], Loss: 4.7853\n",
      "Epoch [1/2], Step [55840/64305], Loss: 4.7537\n",
      "Epoch [1/2], Step [55850/64305], Loss: 4.9675\n",
      "Epoch [1/2], Step [55860/64305], Loss: 4.8613\n",
      "Epoch [1/2], Step [55870/64305], Loss: 4.9147\n",
      "Epoch [1/2], Step [55880/64305], Loss: 4.6962\n",
      "Epoch [1/2], Step [55890/64305], Loss: 4.6391\n",
      "Epoch [1/2], Step [55900/64305], Loss: 4.9772\n",
      "Epoch [1/2], Step [55910/64305], Loss: 4.9303\n",
      "Epoch [1/2], Step [55920/64305], Loss: 4.8312\n",
      "Epoch [1/2], Step [55930/64305], Loss: 4.8439\n",
      "Epoch [1/2], Step [55940/64305], Loss: 4.8010\n",
      "Epoch [1/2], Step [55950/64305], Loss: 4.9857\n",
      "Epoch [1/2], Step [55960/64305], Loss: 4.9266\n",
      "Epoch [1/2], Step [55970/64305], Loss: 4.8240\n",
      "Epoch [1/2], Step [55980/64305], Loss: 4.8162\n",
      "Epoch [1/2], Step [55990/64305], Loss: 5.0544\n",
      "Epoch [1/2], Step [56000/64305], Loss: 4.6908\n",
      "Epoch [1/2], Step [56010/64305], Loss: 4.8329\n",
      "Epoch [1/2], Step [56020/64305], Loss: 4.8171\n",
      "Epoch [1/2], Step [56030/64305], Loss: 4.9824\n",
      "Epoch [1/2], Step [56040/64305], Loss: 4.8396\n",
      "Epoch [1/2], Step [56050/64305], Loss: 4.8402\n",
      "Epoch [1/2], Step [56060/64305], Loss: 4.9791\n",
      "Epoch [1/2], Step [56070/64305], Loss: 4.6179\n",
      "Epoch [1/2], Step [56080/64305], Loss: 4.7589\n",
      "Epoch [1/2], Step [56090/64305], Loss: 4.9153\n",
      "Epoch [1/2], Step [56100/64305], Loss: 5.0919\n",
      "Epoch [1/2], Step [56110/64305], Loss: 4.8739\n",
      "Epoch [1/2], Step [56120/64305], Loss: 4.8803\n",
      "Epoch [1/2], Step [56130/64305], Loss: 4.9800\n",
      "Epoch [1/2], Step [56140/64305], Loss: 4.7292\n",
      "Epoch [1/2], Step [56150/64305], Loss: 4.9857\n",
      "Epoch [1/2], Step [56160/64305], Loss: 4.7242\n",
      "Epoch [1/2], Step [56170/64305], Loss: 4.6861\n",
      "Epoch [1/2], Step [56180/64305], Loss: 4.7741\n",
      "Epoch [1/2], Step [56190/64305], Loss: 4.8540\n",
      "Epoch [1/2], Step [56200/64305], Loss: 4.8056\n",
      "Epoch [1/2], Step [56210/64305], Loss: 4.9394\n",
      "Epoch [1/2], Step [56220/64305], Loss: 4.7902\n",
      "Epoch [1/2], Step [56230/64305], Loss: 4.8250\n",
      "Epoch [1/2], Step [56240/64305], Loss: 4.8039\n",
      "Epoch [1/2], Step [56250/64305], Loss: 5.0560\n",
      "Epoch [1/2], Step [56260/64305], Loss: 4.5993\n",
      "Epoch [1/2], Step [56270/64305], Loss: 4.7773\n",
      "Epoch [1/2], Step [56280/64305], Loss: 4.8809\n",
      "Epoch [1/2], Step [56290/64305], Loss: 4.8532\n",
      "Epoch [1/2], Step [56300/64305], Loss: 5.0435\n",
      "Epoch [1/2], Step [56310/64305], Loss: 4.7535\n",
      "Epoch [1/2], Step [56320/64305], Loss: 4.8119\n",
      "Epoch [1/2], Step [56330/64305], Loss: 4.7368\n",
      "Epoch [1/2], Step [56340/64305], Loss: 4.9508\n",
      "Epoch [1/2], Step [56350/64305], Loss: 4.9705\n",
      "Epoch [1/2], Step [56360/64305], Loss: 4.8365\n",
      "Epoch [1/2], Step [56370/64305], Loss: 4.8776\n",
      "Epoch [1/2], Step [56380/64305], Loss: 4.8639\n",
      "Epoch [1/2], Step [56390/64305], Loss: 4.6547\n",
      "Epoch [1/2], Step [56400/64305], Loss: 4.8339\n",
      "Epoch [1/2], Step [56410/64305], Loss: 4.8324\n",
      "Epoch [1/2], Step [56420/64305], Loss: 4.7689\n",
      "Epoch [1/2], Step [56430/64305], Loss: 4.9162\n",
      "Epoch [1/2], Step [56440/64305], Loss: 4.9284\n",
      "Epoch [1/2], Step [56450/64305], Loss: 4.8553\n",
      "Epoch [1/2], Step [56460/64305], Loss: 4.9599\n",
      "Epoch [1/2], Step [56470/64305], Loss: 4.9187\n",
      "Epoch [1/2], Step [56480/64305], Loss: 4.8867\n",
      "Epoch [1/2], Step [56490/64305], Loss: 4.7518\n",
      "Epoch [1/2], Step [56500/64305], Loss: 4.7910\n",
      "Epoch [1/2], Step [56510/64305], Loss: 4.9889\n",
      "Epoch [1/2], Step [56520/64305], Loss: 4.9045\n",
      "Epoch [1/2], Step [56530/64305], Loss: 4.7562\n",
      "Epoch [1/2], Step [56540/64305], Loss: 4.8496\n",
      "Epoch [1/2], Step [56550/64305], Loss: 4.7697\n",
      "Epoch [1/2], Step [56560/64305], Loss: 4.8742\n",
      "Epoch [1/2], Step [56570/64305], Loss: 4.8934\n",
      "Epoch [1/2], Step [56580/64305], Loss: 4.7540\n",
      "Epoch [1/2], Step [56590/64305], Loss: 4.8461\n",
      "Epoch [1/2], Step [56600/64305], Loss: 4.8236\n",
      "Epoch [1/2], Step [56610/64305], Loss: 5.0416\n",
      "Epoch [1/2], Step [56620/64305], Loss: 4.9704\n",
      "Epoch [1/2], Step [56630/64305], Loss: 4.8087\n",
      "Epoch [1/2], Step [56640/64305], Loss: 4.8749\n",
      "Epoch [1/2], Step [56650/64305], Loss: 4.8562\n",
      "Epoch [1/2], Step [56660/64305], Loss: 4.8654\n",
      "Epoch [1/2], Step [56670/64305], Loss: 4.8013\n",
      "Epoch [1/2], Step [56680/64305], Loss: 4.9947\n",
      "Epoch [1/2], Step [56690/64305], Loss: 4.9254\n",
      "Epoch [1/2], Step [56700/64305], Loss: 4.9438\n",
      "Epoch [1/2], Step [56710/64305], Loss: 4.7032\n",
      "Epoch [1/2], Step [56720/64305], Loss: 5.0627\n",
      "Epoch [1/2], Step [56730/64305], Loss: 4.8212\n",
      "Epoch [1/2], Step [56740/64305], Loss: 4.7603\n",
      "Epoch [1/2], Step [56750/64305], Loss: 5.0424\n",
      "Epoch [1/2], Step [56760/64305], Loss: 4.7817\n",
      "Epoch [1/2], Step [56770/64305], Loss: 4.9908\n",
      "Epoch [1/2], Step [56780/64305], Loss: 4.8779\n",
      "Epoch [1/2], Step [56790/64305], Loss: 4.9134\n",
      "Epoch [1/2], Step [56800/64305], Loss: 4.9497\n",
      "Epoch [1/2], Step [56810/64305], Loss: 4.7988\n",
      "Epoch [1/2], Step [56820/64305], Loss: 4.7771\n",
      "Epoch [1/2], Step [56830/64305], Loss: 4.6645\n",
      "Epoch [1/2], Step [56840/64305], Loss: 4.6081\n",
      "Epoch [1/2], Step [56850/64305], Loss: 4.8377\n",
      "Epoch [1/2], Step [56860/64305], Loss: 5.1412\n",
      "Epoch [1/2], Step [56870/64305], Loss: 5.0501\n",
      "Epoch [1/2], Step [56880/64305], Loss: 4.8820\n",
      "Epoch [1/2], Step [56890/64305], Loss: 4.9865\n",
      "Epoch [1/2], Step [56900/64305], Loss: 4.8635\n",
      "Epoch [1/2], Step [56910/64305], Loss: 4.7873\n",
      "Epoch [1/2], Step [56920/64305], Loss: 4.6621\n",
      "Epoch [1/2], Step [56930/64305], Loss: 4.8625\n",
      "Epoch [1/2], Step [56940/64305], Loss: 4.8707\n",
      "Epoch [1/2], Step [56950/64305], Loss: 4.9012\n",
      "Epoch [1/2], Step [56960/64305], Loss: 4.5263\n",
      "Epoch [1/2], Step [56970/64305], Loss: 4.7616\n",
      "Epoch [1/2], Step [56980/64305], Loss: 4.8047\n",
      "Epoch [1/2], Step [56990/64305], Loss: 4.9275\n",
      "Epoch [1/2], Step [57000/64305], Loss: 4.8538\n",
      "Epoch [1/2], Step [57010/64305], Loss: 4.8141\n",
      "Epoch [1/2], Step [57020/64305], Loss: 4.8828\n",
      "Epoch [1/2], Step [57030/64305], Loss: 4.8899\n",
      "Epoch [1/2], Step [57040/64305], Loss: 4.8099\n",
      "Epoch [1/2], Step [57050/64305], Loss: 5.0115\n",
      "Epoch [1/2], Step [57060/64305], Loss: 4.7411\n",
      "Epoch [1/2], Step [57070/64305], Loss: 4.9212\n",
      "Epoch [1/2], Step [57080/64305], Loss: 4.9089\n",
      "Epoch [1/2], Step [57090/64305], Loss: 4.7203\n",
      "Epoch [1/2], Step [57100/64305], Loss: 4.8020\n",
      "Epoch [1/2], Step [57110/64305], Loss: 4.8003\n",
      "Epoch [1/2], Step [57120/64305], Loss: 5.1292\n",
      "Epoch [1/2], Step [57130/64305], Loss: 4.7473\n",
      "Epoch [1/2], Step [57140/64305], Loss: 4.9929\n",
      "Epoch [1/2], Step [57150/64305], Loss: 4.7518\n",
      "Epoch [1/2], Step [57160/64305], Loss: 4.7246\n",
      "Epoch [1/2], Step [57170/64305], Loss: 4.9657\n",
      "Epoch [1/2], Step [57180/64305], Loss: 5.0818\n",
      "Epoch [1/2], Step [57190/64305], Loss: 4.8253\n",
      "Epoch [1/2], Step [57200/64305], Loss: 4.8097\n",
      "Epoch [1/2], Step [57210/64305], Loss: 4.8958\n",
      "Epoch [1/2], Step [57220/64305], Loss: 4.8157\n",
      "Epoch [1/2], Step [57230/64305], Loss: 4.7705\n",
      "Epoch [1/2], Step [57240/64305], Loss: 4.8652\n",
      "Epoch [1/2], Step [57250/64305], Loss: 4.8902\n",
      "Epoch [1/2], Step [57260/64305], Loss: 4.8525\n",
      "Epoch [1/2], Step [57270/64305], Loss: 4.8625\n",
      "Epoch [1/2], Step [57280/64305], Loss: 4.7749\n",
      "Epoch [1/2], Step [57290/64305], Loss: 4.7700\n",
      "Epoch [1/2], Step [57300/64305], Loss: 4.7630\n",
      "Epoch [1/2], Step [57310/64305], Loss: 4.7865\n",
      "Epoch [1/2], Step [57320/64305], Loss: 4.8860\n",
      "Epoch [1/2], Step [57330/64305], Loss: 4.8264\n",
      "Epoch [1/2], Step [57340/64305], Loss: 4.8322\n",
      "Epoch [1/2], Step [57350/64305], Loss: 4.7876\n",
      "Epoch [1/2], Step [57360/64305], Loss: 4.8339\n",
      "Epoch [1/2], Step [57370/64305], Loss: 4.9059\n",
      "Epoch [1/2], Step [57380/64305], Loss: 4.8240\n",
      "Epoch [1/2], Step [57390/64305], Loss: 4.6862\n",
      "Epoch [1/2], Step [57400/64305], Loss: 4.7990\n",
      "Epoch [1/2], Step [57410/64305], Loss: 4.7630\n",
      "Epoch [1/2], Step [57420/64305], Loss: 4.8156\n",
      "Epoch [1/2], Step [57430/64305], Loss: 4.8614\n",
      "Epoch [1/2], Step [57440/64305], Loss: 5.1706\n",
      "Epoch [1/2], Step [57450/64305], Loss: 4.9432\n",
      "Epoch [1/2], Step [57460/64305], Loss: 4.8479\n",
      "Epoch [1/2], Step [57470/64305], Loss: 4.7694\n",
      "Epoch [1/2], Step [57480/64305], Loss: 4.8524\n",
      "Epoch [1/2], Step [57490/64305], Loss: 4.8067\n",
      "Epoch [1/2], Step [57500/64305], Loss: 4.8467\n",
      "Epoch [1/2], Step [57510/64305], Loss: 4.7950\n",
      "Epoch [1/2], Step [57520/64305], Loss: 4.7400\n",
      "Epoch [1/2], Step [57530/64305], Loss: 4.7695\n",
      "Epoch [1/2], Step [57540/64305], Loss: 4.8821\n",
      "Epoch [1/2], Step [57550/64305], Loss: 4.7385\n",
      "Epoch [1/2], Step [57560/64305], Loss: 4.7813\n",
      "Epoch [1/2], Step [57570/64305], Loss: 4.9095\n",
      "Epoch [1/2], Step [57580/64305], Loss: 4.8325\n",
      "Epoch [1/2], Step [57590/64305], Loss: 4.8280\n",
      "Epoch [1/2], Step [57600/64305], Loss: 5.0584\n",
      "Epoch [1/2], Step [57610/64305], Loss: 4.8089\n",
      "Epoch [1/2], Step [57620/64305], Loss: 4.7526\n",
      "Epoch [1/2], Step [57630/64305], Loss: 4.9223\n",
      "Epoch [1/2], Step [57640/64305], Loss: 4.8852\n",
      "Epoch [1/2], Step [57650/64305], Loss: 4.8056\n",
      "Epoch [1/2], Step [57660/64305], Loss: 4.9235\n",
      "Epoch [1/2], Step [57670/64305], Loss: 4.8384\n",
      "Epoch [1/2], Step [57680/64305], Loss: 4.8866\n",
      "Epoch [1/2], Step [57690/64305], Loss: 4.9322\n",
      "Epoch [1/2], Step [57700/64305], Loss: 4.8036\n",
      "Epoch [1/2], Step [57710/64305], Loss: 4.7601\n",
      "Epoch [1/2], Step [57720/64305], Loss: 4.8925\n",
      "Epoch [1/2], Step [57730/64305], Loss: 4.8258\n",
      "Epoch [1/2], Step [57740/64305], Loss: 4.8202\n",
      "Epoch [1/2], Step [57750/64305], Loss: 4.8790\n",
      "Epoch [1/2], Step [57760/64305], Loss: 4.7683\n",
      "Epoch [1/2], Step [57770/64305], Loss: 4.6837\n",
      "Epoch [1/2], Step [57780/64305], Loss: 4.7647\n",
      "Epoch [1/2], Step [57790/64305], Loss: 4.7014\n",
      "Epoch [1/2], Step [57800/64305], Loss: 4.7824\n",
      "Epoch [1/2], Step [57810/64305], Loss: 4.9313\n",
      "Epoch [1/2], Step [57820/64305], Loss: 4.7937\n",
      "Epoch [1/2], Step [57830/64305], Loss: 4.7912\n",
      "Epoch [1/2], Step [57840/64305], Loss: 4.8025\n",
      "Epoch [1/2], Step [57850/64305], Loss: 4.9208\n",
      "Epoch [1/2], Step [57860/64305], Loss: 4.8268\n",
      "Epoch [1/2], Step [57870/64305], Loss: 4.8701\n",
      "Epoch [1/2], Step [57880/64305], Loss: 4.8943\n",
      "Epoch [1/2], Step [57890/64305], Loss: 4.8136\n",
      "Epoch [1/2], Step [57900/64305], Loss: 4.9690\n",
      "Epoch [1/2], Step [57910/64305], Loss: 4.7819\n",
      "Epoch [1/2], Step [57920/64305], Loss: 5.1663\n",
      "Epoch [1/2], Step [57930/64305], Loss: 4.7614\n",
      "Epoch [1/2], Step [57940/64305], Loss: 4.8315\n",
      "Epoch [1/2], Step [57950/64305], Loss: 4.9021\n",
      "Epoch [1/2], Step [57960/64305], Loss: 4.8264\n",
      "Epoch [1/2], Step [57970/64305], Loss: 4.7917\n",
      "Epoch [1/2], Step [57980/64305], Loss: 4.7843\n",
      "Epoch [1/2], Step [57990/64305], Loss: 4.9982\n",
      "Epoch [1/2], Step [58000/64305], Loss: 4.7550\n",
      "Epoch [1/2], Step [58010/64305], Loss: 5.0391\n",
      "Epoch [1/2], Step [58020/64305], Loss: 4.8941\n",
      "Epoch [1/2], Step [58030/64305], Loss: 4.7209\n",
      "Epoch [1/2], Step [58040/64305], Loss: 4.9556\n",
      "Epoch [1/2], Step [58050/64305], Loss: 4.7526\n",
      "Epoch [1/2], Step [58060/64305], Loss: 4.9386\n",
      "Epoch [1/2], Step [58070/64305], Loss: 4.9255\n",
      "Epoch [1/2], Step [58080/64305], Loss: 4.9050\n",
      "Epoch [1/2], Step [58090/64305], Loss: 4.7781\n",
      "Epoch [1/2], Step [58100/64305], Loss: 4.8783\n",
      "Epoch [1/2], Step [58110/64305], Loss: 4.9510\n",
      "Epoch [1/2], Step [58120/64305], Loss: 4.9690\n",
      "Epoch [1/2], Step [58130/64305], Loss: 4.9000\n",
      "Epoch [1/2], Step [58140/64305], Loss: 4.7054\n",
      "Epoch [1/2], Step [58150/64305], Loss: 4.8077\n",
      "Epoch [1/2], Step [58160/64305], Loss: 4.6700\n",
      "Epoch [1/2], Step [58170/64305], Loss: 4.8239\n",
      "Epoch [1/2], Step [58180/64305], Loss: 4.8358\n",
      "Epoch [1/2], Step [58190/64305], Loss: 4.8776\n",
      "Epoch [1/2], Step [58200/64305], Loss: 4.7230\n",
      "Epoch [1/2], Step [58210/64305], Loss: 4.6900\n",
      "Epoch [1/2], Step [58220/64305], Loss: 4.8431\n",
      "Epoch [1/2], Step [58230/64305], Loss: 4.7881\n",
      "Epoch [1/2], Step [58240/64305], Loss: 4.8190\n",
      "Epoch [1/2], Step [58250/64305], Loss: 4.7121\n",
      "Epoch [1/2], Step [58260/64305], Loss: 5.0248\n",
      "Epoch [1/2], Step [58270/64305], Loss: 4.7371\n",
      "Epoch [1/2], Step [58280/64305], Loss: 4.7488\n",
      "Epoch [1/2], Step [58290/64305], Loss: 5.0576\n",
      "Epoch [1/2], Step [58300/64305], Loss: 4.8438\n",
      "Epoch [1/2], Step [58310/64305], Loss: 4.9127\n",
      "Epoch [1/2], Step [58320/64305], Loss: 4.7427\n",
      "Epoch [1/2], Step [58330/64305], Loss: 5.0364\n",
      "Epoch [1/2], Step [58340/64305], Loss: 4.7057\n",
      "Epoch [1/2], Step [58350/64305], Loss: 4.9973\n",
      "Epoch [1/2], Step [58360/64305], Loss: 4.8702\n",
      "Epoch [1/2], Step [58370/64305], Loss: 4.8295\n",
      "Epoch [1/2], Step [58380/64305], Loss: 4.8203\n",
      "Epoch [1/2], Step [58390/64305], Loss: 4.9600\n",
      "Epoch [1/2], Step [58400/64305], Loss: 4.7478\n",
      "Epoch [1/2], Step [58410/64305], Loss: 4.7164\n",
      "Epoch [1/2], Step [58420/64305], Loss: 4.8093\n",
      "Epoch [1/2], Step [58430/64305], Loss: 4.9097\n",
      "Epoch [1/2], Step [58440/64305], Loss: 4.6443\n",
      "Epoch [1/2], Step [58450/64305], Loss: 4.8450\n",
      "Epoch [1/2], Step [58460/64305], Loss: 4.9431\n",
      "Epoch [1/2], Step [58470/64305], Loss: 4.8777\n",
      "Epoch [1/2], Step [58480/64305], Loss: 4.6569\n",
      "Epoch [1/2], Step [58490/64305], Loss: 4.8414\n",
      "Epoch [1/2], Step [58500/64305], Loss: 4.8244\n",
      "Epoch [1/2], Step [58510/64305], Loss: 4.9251\n",
      "Epoch [1/2], Step [58520/64305], Loss: 4.9908\n",
      "Epoch [1/2], Step [58530/64305], Loss: 4.8316\n",
      "Epoch [1/2], Step [58540/64305], Loss: 4.9106\n",
      "Epoch [1/2], Step [58550/64305], Loss: 4.7390\n",
      "Epoch [1/2], Step [58560/64305], Loss: 5.0620\n",
      "Epoch [1/2], Step [58570/64305], Loss: 4.8366\n",
      "Epoch [1/2], Step [58580/64305], Loss: 4.9687\n",
      "Epoch [1/2], Step [58590/64305], Loss: 4.8560\n",
      "Epoch [1/2], Step [58600/64305], Loss: 4.9255\n",
      "Epoch [1/2], Step [58610/64305], Loss: 4.7580\n",
      "Epoch [1/2], Step [58620/64305], Loss: 4.7024\n",
      "Epoch [1/2], Step [58630/64305], Loss: 4.8276\n",
      "Epoch [1/2], Step [58640/64305], Loss: 4.8618\n",
      "Epoch [1/2], Step [58650/64305], Loss: 4.8226\n",
      "Epoch [1/2], Step [58660/64305], Loss: 4.7409\n",
      "Epoch [1/2], Step [58670/64305], Loss: 4.8267\n",
      "Epoch [1/2], Step [58680/64305], Loss: 4.7586\n",
      "Epoch [1/2], Step [58690/64305], Loss: 4.7319\n",
      "Epoch [1/2], Step [58700/64305], Loss: 4.8387\n",
      "Epoch [1/2], Step [58710/64305], Loss: 4.8525\n",
      "Epoch [1/2], Step [58720/64305], Loss: 4.6297\n",
      "Epoch [1/2], Step [58730/64305], Loss: 4.8520\n",
      "Epoch [1/2], Step [58740/64305], Loss: 4.8599\n",
      "Epoch [1/2], Step [58750/64305], Loss: 4.8470\n",
      "Epoch [1/2], Step [58760/64305], Loss: 4.7738\n",
      "Epoch [1/2], Step [58770/64305], Loss: 4.7849\n",
      "Epoch [1/2], Step [58780/64305], Loss: 4.7491\n",
      "Epoch [1/2], Step [58790/64305], Loss: 4.8811\n",
      "Epoch [1/2], Step [58800/64305], Loss: 4.7092\n",
      "Epoch [1/2], Step [58810/64305], Loss: 4.8231\n",
      "Epoch [1/2], Step [58820/64305], Loss: 4.9153\n",
      "Epoch [1/2], Step [58830/64305], Loss: 4.7046\n",
      "Epoch [1/2], Step [58840/64305], Loss: 4.6915\n",
      "Epoch [1/2], Step [58850/64305], Loss: 4.9561\n",
      "Epoch [1/2], Step [58860/64305], Loss: 4.8352\n",
      "Epoch [1/2], Step [58870/64305], Loss: 4.8721\n",
      "Epoch [1/2], Step [58880/64305], Loss: 4.9296\n",
      "Epoch [1/2], Step [58890/64305], Loss: 4.8762\n",
      "Epoch [1/2], Step [58900/64305], Loss: 4.8973\n",
      "Epoch [1/2], Step [58910/64305], Loss: 4.7494\n",
      "Epoch [1/2], Step [58920/64305], Loss: 5.0277\n",
      "Epoch [1/2], Step [58930/64305], Loss: 5.0269\n",
      "Epoch [1/2], Step [58940/64305], Loss: 4.9576\n",
      "Epoch [1/2], Step [58950/64305], Loss: 4.8499\n",
      "Epoch [1/2], Step [58960/64305], Loss: 5.0003\n",
      "Epoch [1/2], Step [58970/64305], Loss: 4.7395\n",
      "Epoch [1/2], Step [58980/64305], Loss: 4.8841\n",
      "Epoch [1/2], Step [58990/64305], Loss: 4.8805\n",
      "Epoch [1/2], Step [59000/64305], Loss: 4.7403\n",
      "Epoch [1/2], Step [59010/64305], Loss: 4.9193\n",
      "Epoch [1/2], Step [59020/64305], Loss: 4.9010\n",
      "Epoch [1/2], Step [59030/64305], Loss: 4.9834\n",
      "Epoch [1/2], Step [59040/64305], Loss: 4.5422\n",
      "Epoch [1/2], Step [59050/64305], Loss: 4.6873\n",
      "Epoch [1/2], Step [59060/64305], Loss: 4.7338\n",
      "Epoch [1/2], Step [59070/64305], Loss: 4.6944\n",
      "Epoch [1/2], Step [59080/64305], Loss: 4.8223\n",
      "Epoch [1/2], Step [59090/64305], Loss: 4.9852\n",
      "Epoch [1/2], Step [59100/64305], Loss: 4.8044\n",
      "Epoch [1/2], Step [59110/64305], Loss: 4.6203\n",
      "Epoch [1/2], Step [59120/64305], Loss: 5.0378\n",
      "Epoch [1/2], Step [59130/64305], Loss: 4.9946\n",
      "Epoch [1/2], Step [59140/64305], Loss: 4.8285\n",
      "Epoch [1/2], Step [59150/64305], Loss: 4.9453\n",
      "Epoch [1/2], Step [59160/64305], Loss: 4.8940\n",
      "Epoch [1/2], Step [59170/64305], Loss: 4.7321\n",
      "Epoch [1/2], Step [59180/64305], Loss: 4.8756\n",
      "Epoch [1/2], Step [59190/64305], Loss: 4.9095\n",
      "Epoch [1/2], Step [59200/64305], Loss: 4.9154\n",
      "Epoch [1/2], Step [59210/64305], Loss: 5.0400\n",
      "Epoch [1/2], Step [59220/64305], Loss: 4.7405\n",
      "Epoch [1/2], Step [59230/64305], Loss: 4.7845\n",
      "Epoch [1/2], Step [59240/64305], Loss: 4.8805\n",
      "Epoch [1/2], Step [59250/64305], Loss: 4.9138\n",
      "Epoch [1/2], Step [59260/64305], Loss: 4.8873\n",
      "Epoch [1/2], Step [59270/64305], Loss: 4.7180\n",
      "Epoch [1/2], Step [59280/64305], Loss: 4.8070\n",
      "Epoch [1/2], Step [59290/64305], Loss: 4.8140\n",
      "Epoch [1/2], Step [59300/64305], Loss: 4.8600\n",
      "Epoch [1/2], Step [59310/64305], Loss: 4.7931\n",
      "Epoch [1/2], Step [59320/64305], Loss: 4.7977\n",
      "Epoch [1/2], Step [59330/64305], Loss: 4.9628\n",
      "Epoch [1/2], Step [59340/64305], Loss: 4.6572\n",
      "Epoch [1/2], Step [59350/64305], Loss: 4.8491\n",
      "Epoch [1/2], Step [59360/64305], Loss: 4.7874\n",
      "Epoch [1/2], Step [59370/64305], Loss: 4.8626\n",
      "Epoch [1/2], Step [59380/64305], Loss: 4.8346\n",
      "Epoch [1/2], Step [59390/64305], Loss: 4.7962\n",
      "Epoch [1/2], Step [59400/64305], Loss: 4.9329\n",
      "Epoch [1/2], Step [59410/64305], Loss: 4.7842\n",
      "Epoch [1/2], Step [59420/64305], Loss: 4.7609\n",
      "Epoch [1/2], Step [59430/64305], Loss: 4.8552\n",
      "Epoch [1/2], Step [59440/64305], Loss: 4.9383\n",
      "Epoch [1/2], Step [59450/64305], Loss: 4.8509\n",
      "Epoch [1/2], Step [59460/64305], Loss: 4.8219\n",
      "Epoch [1/2], Step [59470/64305], Loss: 4.6779\n",
      "Epoch [1/2], Step [59480/64305], Loss: 4.8627\n",
      "Epoch [1/2], Step [59490/64305], Loss: 4.7202\n",
      "Epoch [1/2], Step [59500/64305], Loss: 4.7196\n",
      "Epoch [1/2], Step [59510/64305], Loss: 4.9686\n",
      "Epoch [1/2], Step [59520/64305], Loss: 5.0514\n",
      "Epoch [1/2], Step [59530/64305], Loss: 4.9323\n",
      "Epoch [1/2], Step [59540/64305], Loss: 5.1107\n",
      "Epoch [1/2], Step [59550/64305], Loss: 4.9143\n",
      "Epoch [1/2], Step [59560/64305], Loss: 4.7006\n",
      "Epoch [1/2], Step [59570/64305], Loss: 4.7873\n",
      "Epoch [1/2], Step [59580/64305], Loss: 4.8107\n",
      "Epoch [1/2], Step [59590/64305], Loss: 4.6809\n",
      "Epoch [1/2], Step [59600/64305], Loss: 5.1125\n",
      "Epoch [1/2], Step [59610/64305], Loss: 4.7653\n",
      "Epoch [1/2], Step [59620/64305], Loss: 4.5976\n",
      "Epoch [1/2], Step [59630/64305], Loss: 5.1326\n",
      "Epoch [1/2], Step [59640/64305], Loss: 4.6409\n",
      "Epoch [1/2], Step [59650/64305], Loss: 4.8656\n",
      "Epoch [1/2], Step [59660/64305], Loss: 4.8617\n",
      "Epoch [1/2], Step [59670/64305], Loss: 4.8896\n",
      "Epoch [1/2], Step [59680/64305], Loss: 4.8462\n",
      "Epoch [1/2], Step [59690/64305], Loss: 4.9750\n",
      "Epoch [1/2], Step [59700/64305], Loss: 4.8971\n",
      "Epoch [1/2], Step [59710/64305], Loss: 5.0029\n",
      "Epoch [1/2], Step [59720/64305], Loss: 4.8821\n",
      "Epoch [1/2], Step [59730/64305], Loss: 4.7324\n",
      "Epoch [1/2], Step [59740/64305], Loss: 4.9678\n",
      "Epoch [1/2], Step [59750/64305], Loss: 4.8245\n",
      "Epoch [1/2], Step [59760/64305], Loss: 4.7182\n",
      "Epoch [1/2], Step [59770/64305], Loss: 4.6360\n",
      "Epoch [1/2], Step [59780/64305], Loss: 4.8190\n",
      "Epoch [1/2], Step [59790/64305], Loss: 4.9182\n",
      "Epoch [1/2], Step [59800/64305], Loss: 4.9787\n",
      "Epoch [1/2], Step [59810/64305], Loss: 4.6720\n",
      "Epoch [1/2], Step [59820/64305], Loss: 4.8507\n",
      "Epoch [1/2], Step [59830/64305], Loss: 4.9999\n",
      "Epoch [1/2], Step [59840/64305], Loss: 4.8249\n",
      "Epoch [1/2], Step [59850/64305], Loss: 4.7178\n",
      "Epoch [1/2], Step [59860/64305], Loss: 4.8375\n",
      "Epoch [1/2], Step [59870/64305], Loss: 4.8521\n",
      "Epoch [1/2], Step [59880/64305], Loss: 4.7903\n",
      "Epoch [1/2], Step [59890/64305], Loss: 4.9310\n",
      "Epoch [1/2], Step [59900/64305], Loss: 4.9833\n",
      "Epoch [1/2], Step [59910/64305], Loss: 4.6929\n",
      "Epoch [1/2], Step [59920/64305], Loss: 4.8715\n",
      "Epoch [1/2], Step [59930/64305], Loss: 5.0613\n",
      "Epoch [1/2], Step [59940/64305], Loss: 4.9088\n",
      "Epoch [1/2], Step [59950/64305], Loss: 4.5945\n",
      "Epoch [1/2], Step [59960/64305], Loss: 4.8647\n",
      "Epoch [1/2], Step [59970/64305], Loss: 4.9092\n",
      "Epoch [1/2], Step [59980/64305], Loss: 4.7443\n",
      "Epoch [1/2], Step [59990/64305], Loss: 4.7928\n",
      "Epoch [1/2], Step [60000/64305], Loss: 4.9463\n",
      "Epoch [1/2], Step [60010/64305], Loss: 4.9057\n",
      "Epoch [1/2], Step [60020/64305], Loss: 4.6669\n",
      "Epoch [1/2], Step [60030/64305], Loss: 4.6784\n",
      "Epoch [1/2], Step [60040/64305], Loss: 4.6239\n",
      "Epoch [1/2], Step [60050/64305], Loss: 5.0291\n",
      "Epoch [1/2], Step [60060/64305], Loss: 4.8799\n",
      "Epoch [1/2], Step [60070/64305], Loss: 4.5440\n",
      "Epoch [1/2], Step [60080/64305], Loss: 4.7235\n",
      "Epoch [1/2], Step [60090/64305], Loss: 4.8130\n",
      "Epoch [1/2], Step [60100/64305], Loss: 4.6272\n",
      "Epoch [1/2], Step [60110/64305], Loss: 4.9595\n",
      "Epoch [1/2], Step [60120/64305], Loss: 4.8556\n",
      "Epoch [1/2], Step [60130/64305], Loss: 4.7704\n",
      "Epoch [1/2], Step [60140/64305], Loss: 5.0747\n",
      "Epoch [1/2], Step [60150/64305], Loss: 4.7726\n",
      "Epoch [1/2], Step [60160/64305], Loss: 4.8626\n",
      "Epoch [1/2], Step [60170/64305], Loss: 4.6045\n",
      "Epoch [1/2], Step [60180/64305], Loss: 4.7597\n",
      "Epoch [1/2], Step [60190/64305], Loss: 5.0661\n",
      "Epoch [1/2], Step [60200/64305], Loss: 4.8388\n",
      "Epoch [1/2], Step [60210/64305], Loss: 4.8364\n",
      "Epoch [1/2], Step [60220/64305], Loss: 4.7888\n",
      "Epoch [1/2], Step [60230/64305], Loss: 4.8040\n",
      "Epoch [1/2], Step [60240/64305], Loss: 4.9317\n",
      "Epoch [1/2], Step [60250/64305], Loss: 4.8444\n",
      "Epoch [1/2], Step [60260/64305], Loss: 4.8515\n",
      "Epoch [1/2], Step [60270/64305], Loss: 4.8537\n",
      "Epoch [1/2], Step [60280/64305], Loss: 4.8576\n",
      "Epoch [1/2], Step [60290/64305], Loss: 4.6288\n",
      "Epoch [1/2], Step [60300/64305], Loss: 4.7886\n",
      "Epoch [1/2], Step [60310/64305], Loss: 4.6821\n",
      "Epoch [1/2], Step [60320/64305], Loss: 4.7061\n",
      "Epoch [1/2], Step [60330/64305], Loss: 4.7594\n",
      "Epoch [1/2], Step [60340/64305], Loss: 4.8716\n",
      "Epoch [1/2], Step [60350/64305], Loss: 4.6906\n",
      "Epoch [1/2], Step [60360/64305], Loss: 4.9478\n",
      "Epoch [1/2], Step [60370/64305], Loss: 4.8073\n",
      "Epoch [1/2], Step [60380/64305], Loss: 4.9215\n",
      "Epoch [1/2], Step [60390/64305], Loss: 4.7717\n",
      "Epoch [1/2], Step [60400/64305], Loss: 4.6887\n",
      "Epoch [1/2], Step [60410/64305], Loss: 4.7639\n",
      "Epoch [1/2], Step [60420/64305], Loss: 4.8168\n",
      "Epoch [1/2], Step [60430/64305], Loss: 4.8037\n",
      "Epoch [1/2], Step [60440/64305], Loss: 4.8386\n",
      "Epoch [1/2], Step [60450/64305], Loss: 4.9742\n",
      "Epoch [1/2], Step [60460/64305], Loss: 4.9401\n",
      "Epoch [1/2], Step [60470/64305], Loss: 4.7050\n",
      "Epoch [1/2], Step [60480/64305], Loss: 4.9574\n",
      "Epoch [1/2], Step [60490/64305], Loss: 5.0351\n",
      "Epoch [1/2], Step [60500/64305], Loss: 4.7274\n",
      "Epoch [1/2], Step [60510/64305], Loss: 4.8144\n",
      "Epoch [1/2], Step [60520/64305], Loss: 4.8401\n",
      "Epoch [1/2], Step [60530/64305], Loss: 4.8936\n",
      "Epoch [1/2], Step [60540/64305], Loss: 4.7173\n",
      "Epoch [1/2], Step [60550/64305], Loss: 4.8426\n",
      "Epoch [1/2], Step [60560/64305], Loss: 4.8928\n",
      "Epoch [1/2], Step [60570/64305], Loss: 4.7674\n",
      "Epoch [1/2], Step [60580/64305], Loss: 4.7925\n",
      "Epoch [1/2], Step [60590/64305], Loss: 4.8723\n",
      "Epoch [1/2], Step [60600/64305], Loss: 4.9424\n",
      "Epoch [1/2], Step [60610/64305], Loss: 4.9456\n",
      "Epoch [1/2], Step [60620/64305], Loss: 4.8133\n",
      "Epoch [1/2], Step [60630/64305], Loss: 4.9604\n",
      "Epoch [1/2], Step [60640/64305], Loss: 4.9135\n",
      "Epoch [1/2], Step [60650/64305], Loss: 4.8097\n",
      "Epoch [1/2], Step [60660/64305], Loss: 4.8252\n",
      "Epoch [1/2], Step [60670/64305], Loss: 4.8094\n",
      "Epoch [1/2], Step [60680/64305], Loss: 4.8443\n",
      "Epoch [1/2], Step [60690/64305], Loss: 5.0550\n",
      "Epoch [1/2], Step [60700/64305], Loss: 4.8261\n",
      "Epoch [1/2], Step [60710/64305], Loss: 4.8546\n",
      "Epoch [1/2], Step [60720/64305], Loss: 5.0123\n",
      "Epoch [1/2], Step [60730/64305], Loss: 4.7969\n",
      "Epoch [1/2], Step [60740/64305], Loss: 4.9456\n",
      "Epoch [1/2], Step [60750/64305], Loss: 5.0565\n",
      "Epoch [1/2], Step [60760/64305], Loss: 4.6539\n",
      "Epoch [1/2], Step [60770/64305], Loss: 4.8035\n",
      "Epoch [1/2], Step [60780/64305], Loss: 5.0221\n",
      "Epoch [1/2], Step [60790/64305], Loss: 4.8806\n",
      "Epoch [1/2], Step [60800/64305], Loss: 4.7230\n",
      "Epoch [1/2], Step [60810/64305], Loss: 4.6988\n",
      "Epoch [1/2], Step [60820/64305], Loss: 5.0149\n",
      "Epoch [1/2], Step [60830/64305], Loss: 4.7930\n",
      "Epoch [1/2], Step [60840/64305], Loss: 4.8103\n",
      "Epoch [1/2], Step [60850/64305], Loss: 5.0699\n",
      "Epoch [1/2], Step [60860/64305], Loss: 4.6743\n",
      "Epoch [1/2], Step [60870/64305], Loss: 4.7367\n",
      "Epoch [1/2], Step [60880/64305], Loss: 4.6448\n",
      "Epoch [1/2], Step [60890/64305], Loss: 5.0019\n",
      "Epoch [1/2], Step [60900/64305], Loss: 4.7293\n",
      "Epoch [1/2], Step [60910/64305], Loss: 4.7138\n",
      "Epoch [1/2], Step [60920/64305], Loss: 4.7518\n",
      "Epoch [1/2], Step [60930/64305], Loss: 4.7828\n",
      "Epoch [1/2], Step [60940/64305], Loss: 4.7508\n",
      "Epoch [1/2], Step [60950/64305], Loss: 4.8845\n",
      "Epoch [1/2], Step [60960/64305], Loss: 4.8443\n",
      "Epoch [1/2], Step [60970/64305], Loss: 4.6437\n",
      "Epoch [1/2], Step [60980/64305], Loss: 4.8573\n",
      "Epoch [1/2], Step [60990/64305], Loss: 4.7228\n",
      "Epoch [1/2], Step [61000/64305], Loss: 4.7514\n",
      "Epoch [1/2], Step [61010/64305], Loss: 4.8041\n",
      "Epoch [1/2], Step [61020/64305], Loss: 4.8799\n",
      "Epoch [1/2], Step [61030/64305], Loss: 4.7845\n",
      "Epoch [1/2], Step [61040/64305], Loss: 4.9263\n",
      "Epoch [1/2], Step [61050/64305], Loss: 4.9545\n",
      "Epoch [1/2], Step [61060/64305], Loss: 4.9012\n",
      "Epoch [1/2], Step [61070/64305], Loss: 4.5726\n",
      "Epoch [1/2], Step [61080/64305], Loss: 4.7222\n",
      "Epoch [1/2], Step [61090/64305], Loss: 4.7184\n",
      "Epoch [1/2], Step [61100/64305], Loss: 4.8251\n",
      "Epoch [1/2], Step [61110/64305], Loss: 4.9297\n",
      "Epoch [1/2], Step [61120/64305], Loss: 4.6043\n",
      "Epoch [1/2], Step [61130/64305], Loss: 4.8416\n",
      "Epoch [1/2], Step [61140/64305], Loss: 4.9046\n",
      "Epoch [1/2], Step [61150/64305], Loss: 4.6538\n",
      "Epoch [1/2], Step [61160/64305], Loss: 4.9536\n",
      "Epoch [1/2], Step [61170/64305], Loss: 4.9117\n",
      "Epoch [1/2], Step [61180/64305], Loss: 4.9362\n",
      "Epoch [1/2], Step [61190/64305], Loss: 4.7662\n",
      "Epoch [1/2], Step [61200/64305], Loss: 4.7547\n",
      "Epoch [1/2], Step [61210/64305], Loss: 4.9304\n",
      "Epoch [1/2], Step [61220/64305], Loss: 4.9310\n",
      "Epoch [1/2], Step [61230/64305], Loss: 4.7860\n",
      "Epoch [1/2], Step [61240/64305], Loss: 4.8751\n",
      "Epoch [1/2], Step [61250/64305], Loss: 4.5850\n",
      "Epoch [1/2], Step [61260/64305], Loss: 4.9716\n",
      "Epoch [1/2], Step [61270/64305], Loss: 4.8944\n",
      "Epoch [1/2], Step [61280/64305], Loss: 4.7663\n",
      "Epoch [1/2], Step [61290/64305], Loss: 4.8172\n",
      "Epoch [1/2], Step [61300/64305], Loss: 4.6847\n",
      "Epoch [1/2], Step [61310/64305], Loss: 4.9514\n",
      "Epoch [1/2], Step [61320/64305], Loss: 4.9062\n",
      "Epoch [1/2], Step [61330/64305], Loss: 4.9177\n",
      "Epoch [1/2], Step [61340/64305], Loss: 4.7434\n",
      "Epoch [1/2], Step [61350/64305], Loss: 5.0972\n",
      "Epoch [1/2], Step [61360/64305], Loss: 4.8208\n",
      "Epoch [1/2], Step [61370/64305], Loss: 5.0076\n",
      "Epoch [1/2], Step [61380/64305], Loss: 5.0826\n",
      "Epoch [1/2], Step [61390/64305], Loss: 4.8631\n",
      "Epoch [1/2], Step [61400/64305], Loss: 4.7555\n",
      "Epoch [1/2], Step [61410/64305], Loss: 4.6918\n",
      "Epoch [1/2], Step [61420/64305], Loss: 4.5221\n",
      "Epoch [1/2], Step [61430/64305], Loss: 4.7022\n",
      "Epoch [1/2], Step [61440/64305], Loss: 4.7921\n",
      "Epoch [1/2], Step [61450/64305], Loss: 4.9853\n",
      "Epoch [1/2], Step [61460/64305], Loss: 4.9386\n",
      "Epoch [1/2], Step [61470/64305], Loss: 4.9208\n",
      "Epoch [1/2], Step [61480/64305], Loss: 4.8330\n",
      "Epoch [1/2], Step [61490/64305], Loss: 4.8451\n",
      "Epoch [1/2], Step [61500/64305], Loss: 4.9290\n",
      "Epoch [1/2], Step [61510/64305], Loss: 4.8316\n",
      "Epoch [1/2], Step [61520/64305], Loss: 4.8361\n",
      "Epoch [1/2], Step [61530/64305], Loss: 5.0484\n",
      "Epoch [1/2], Step [61540/64305], Loss: 4.7992\n",
      "Epoch [1/2], Step [61550/64305], Loss: 4.9380\n",
      "Epoch [1/2], Step [61560/64305], Loss: 4.8254\n",
      "Epoch [1/2], Step [61570/64305], Loss: 4.8676\n",
      "Epoch [1/2], Step [61580/64305], Loss: 4.7214\n",
      "Epoch [1/2], Step [61590/64305], Loss: 4.9114\n",
      "Epoch [1/2], Step [61600/64305], Loss: 4.8268\n",
      "Epoch [1/2], Step [61610/64305], Loss: 4.8966\n",
      "Epoch [1/2], Step [61620/64305], Loss: 5.0468\n",
      "Epoch [1/2], Step [61630/64305], Loss: 4.9420\n",
      "Epoch [1/2], Step [61640/64305], Loss: 4.9739\n",
      "Epoch [1/2], Step [61650/64305], Loss: 4.8060\n",
      "Epoch [1/2], Step [61660/64305], Loss: 4.9316\n",
      "Epoch [1/2], Step [61670/64305], Loss: 4.7244\n",
      "Epoch [1/2], Step [61680/64305], Loss: 4.6717\n",
      "Epoch [1/2], Step [61690/64305], Loss: 4.7534\n",
      "Epoch [1/2], Step [61700/64305], Loss: 4.7017\n",
      "Epoch [1/2], Step [61710/64305], Loss: 4.7980\n",
      "Epoch [1/2], Step [61720/64305], Loss: 4.6608\n",
      "Epoch [1/2], Step [61730/64305], Loss: 4.7366\n",
      "Epoch [1/2], Step [61740/64305], Loss: 4.7532\n",
      "Epoch [1/2], Step [61750/64305], Loss: 4.6519\n",
      "Epoch [1/2], Step [61760/64305], Loss: 5.0102\n",
      "Epoch [1/2], Step [61770/64305], Loss: 4.9682\n",
      "Epoch [1/2], Step [61780/64305], Loss: 4.8148\n",
      "Epoch [1/2], Step [61790/64305], Loss: 4.8773\n",
      "Epoch [1/2], Step [61800/64305], Loss: 4.9788\n",
      "Epoch [1/2], Step [61810/64305], Loss: 4.9154\n",
      "Epoch [1/2], Step [61820/64305], Loss: 4.9019\n",
      "Epoch [1/2], Step [61830/64305], Loss: 4.9159\n",
      "Epoch [1/2], Step [61840/64305], Loss: 4.7463\n",
      "Epoch [1/2], Step [61850/64305], Loss: 4.8963\n",
      "Epoch [1/2], Step [61860/64305], Loss: 5.1076\n",
      "Epoch [1/2], Step [61870/64305], Loss: 4.7644\n",
      "Epoch [1/2], Step [61880/64305], Loss: 4.8449\n",
      "Epoch [1/2], Step [61890/64305], Loss: 4.6501\n",
      "Epoch [1/2], Step [61900/64305], Loss: 4.7775\n",
      "Epoch [1/2], Step [61910/64305], Loss: 5.0031\n",
      "Epoch [1/2], Step [61920/64305], Loss: 4.7370\n",
      "Epoch [1/2], Step [61930/64305], Loss: 4.8564\n",
      "Epoch [1/2], Step [61940/64305], Loss: 4.8838\n",
      "Epoch [1/2], Step [61950/64305], Loss: 4.9337\n",
      "Epoch [1/2], Step [61960/64305], Loss: 4.8169\n",
      "Epoch [1/2], Step [61970/64305], Loss: 4.7005\n",
      "Epoch [1/2], Step [61980/64305], Loss: 4.9263\n",
      "Epoch [1/2], Step [61990/64305], Loss: 4.8494\n",
      "Epoch [1/2], Step [62000/64305], Loss: 4.9332\n",
      "Epoch [1/2], Step [62010/64305], Loss: 4.9025\n",
      "Epoch [1/2], Step [62020/64305], Loss: 4.7928\n",
      "Epoch [1/2], Step [62030/64305], Loss: 4.9514\n",
      "Epoch [1/2], Step [62040/64305], Loss: 4.9639\n",
      "Epoch [1/2], Step [62050/64305], Loss: 4.8792\n",
      "Epoch [1/2], Step [62060/64305], Loss: 4.6429\n",
      "Epoch [1/2], Step [62070/64305], Loss: 4.6931\n",
      "Epoch [1/2], Step [62080/64305], Loss: 5.0193\n",
      "Epoch [1/2], Step [62090/64305], Loss: 4.6320\n",
      "Epoch [1/2], Step [62100/64305], Loss: 4.8301\n",
      "Epoch [1/2], Step [62110/64305], Loss: 4.6888\n",
      "Epoch [1/2], Step [62120/64305], Loss: 4.6768\n",
      "Epoch [1/2], Step [62130/64305], Loss: 4.6717\n",
      "Epoch [1/2], Step [62140/64305], Loss: 4.9218\n",
      "Epoch [1/2], Step [62150/64305], Loss: 4.7895\n",
      "Epoch [1/2], Step [62160/64305], Loss: 4.8691\n",
      "Epoch [1/2], Step [62170/64305], Loss: 4.8291\n",
      "Epoch [1/2], Step [62180/64305], Loss: 4.8601\n",
      "Epoch [1/2], Step [62190/64305], Loss: 4.8642\n",
      "Epoch [1/2], Step [62200/64305], Loss: 4.8757\n",
      "Epoch [1/2], Step [62210/64305], Loss: 4.8876\n",
      "Epoch [1/2], Step [62220/64305], Loss: 5.0536\n",
      "Epoch [1/2], Step [62230/64305], Loss: 4.8760\n",
      "Epoch [1/2], Step [62240/64305], Loss: 4.6824\n",
      "Epoch [1/2], Step [62250/64305], Loss: 4.6607\n",
      "Epoch [1/2], Step [62260/64305], Loss: 4.8429\n",
      "Epoch [1/2], Step [62270/64305], Loss: 4.8463\n",
      "Epoch [1/2], Step [62280/64305], Loss: 4.8431\n",
      "Epoch [1/2], Step [62290/64305], Loss: 4.9059\n",
      "Epoch [1/2], Step [62300/64305], Loss: 4.8288\n",
      "Epoch [1/2], Step [62310/64305], Loss: 4.8396\n",
      "Epoch [1/2], Step [62320/64305], Loss: 4.8402\n",
      "Epoch [1/2], Step [62330/64305], Loss: 5.0527\n",
      "Epoch [1/2], Step [62340/64305], Loss: 4.7463\n",
      "Epoch [1/2], Step [62350/64305], Loss: 4.9674\n",
      "Epoch [1/2], Step [62360/64305], Loss: 4.7019\n",
      "Epoch [1/2], Step [62370/64305], Loss: 5.0800\n",
      "Epoch [1/2], Step [62380/64305], Loss: 5.0525\n",
      "Epoch [1/2], Step [62390/64305], Loss: 4.9545\n",
      "Epoch [1/2], Step [62400/64305], Loss: 4.8315\n",
      "Epoch [1/2], Step [62410/64305], Loss: 4.6172\n",
      "Epoch [1/2], Step [62420/64305], Loss: 4.8450\n",
      "Epoch [1/2], Step [62430/64305], Loss: 4.7935\n",
      "Epoch [1/2], Step [62440/64305], Loss: 5.0749\n",
      "Epoch [1/2], Step [62450/64305], Loss: 4.8836\n",
      "Epoch [1/2], Step [62460/64305], Loss: 4.7294\n",
      "Epoch [1/2], Step [62470/64305], Loss: 4.8476\n",
      "Epoch [1/2], Step [62480/64305], Loss: 5.0508\n",
      "Epoch [1/2], Step [62490/64305], Loss: 4.7053\n",
      "Epoch [1/2], Step [62500/64305], Loss: 4.7049\n",
      "Epoch [1/2], Step [62510/64305], Loss: 4.8461\n",
      "Epoch [1/2], Step [62520/64305], Loss: 4.5926\n",
      "Epoch [1/2], Step [62530/64305], Loss: 4.7050\n",
      "Epoch [1/2], Step [62540/64305], Loss: 4.9055\n",
      "Epoch [1/2], Step [62550/64305], Loss: 4.8282\n",
      "Epoch [1/2], Step [62560/64305], Loss: 4.8409\n",
      "Epoch [1/2], Step [62570/64305], Loss: 4.7235\n",
      "Epoch [1/2], Step [62580/64305], Loss: 4.8245\n",
      "Epoch [1/2], Step [62590/64305], Loss: 4.7778\n",
      "Epoch [1/2], Step [62600/64305], Loss: 4.7099\n",
      "Epoch [1/2], Step [62610/64305], Loss: 4.9262\n",
      "Epoch [1/2], Step [62620/64305], Loss: 4.9458\n",
      "Epoch [1/2], Step [62630/64305], Loss: 4.8275\n",
      "Epoch [1/2], Step [62640/64305], Loss: 4.8340\n",
      "Epoch [1/2], Step [62650/64305], Loss: 4.6338\n",
      "Epoch [1/2], Step [62660/64305], Loss: 4.7695\n",
      "Epoch [1/2], Step [62670/64305], Loss: 4.9478\n",
      "Epoch [1/2], Step [62680/64305], Loss: 4.6848\n",
      "Epoch [1/2], Step [62690/64305], Loss: 4.6300\n",
      "Epoch [1/2], Step [62700/64305], Loss: 4.7524\n",
      "Epoch [1/2], Step [62710/64305], Loss: 4.8285\n",
      "Epoch [1/2], Step [62720/64305], Loss: 4.9090\n",
      "Epoch [1/2], Step [62730/64305], Loss: 4.7225\n",
      "Epoch [1/2], Step [62740/64305], Loss: 4.8790\n",
      "Epoch [1/2], Step [62750/64305], Loss: 4.7775\n",
      "Epoch [1/2], Step [62760/64305], Loss: 4.6897\n",
      "Epoch [1/2], Step [62770/64305], Loss: 4.8948\n",
      "Epoch [1/2], Step [62780/64305], Loss: 4.9229\n",
      "Epoch [1/2], Step [62790/64305], Loss: 4.7619\n",
      "Epoch [1/2], Step [62800/64305], Loss: 4.7648\n",
      "Epoch [1/2], Step [62810/64305], Loss: 4.9487\n",
      "Epoch [1/2], Step [62820/64305], Loss: 4.9796\n",
      "Epoch [1/2], Step [62830/64305], Loss: 4.7634\n",
      "Epoch [1/2], Step [62840/64305], Loss: 4.7804\n",
      "Epoch [1/2], Step [62850/64305], Loss: 4.6140\n",
      "Epoch [1/2], Step [62860/64305], Loss: 4.8296\n",
      "Epoch [1/2], Step [62870/64305], Loss: 4.9272\n",
      "Epoch [1/2], Step [62880/64305], Loss: 4.8484\n",
      "Epoch [1/2], Step [62890/64305], Loss: 5.0563\n",
      "Epoch [1/2], Step [62900/64305], Loss: 4.8441\n",
      "Epoch [1/2], Step [62910/64305], Loss: 4.8963\n",
      "Epoch [1/2], Step [62920/64305], Loss: 4.9257\n",
      "Epoch [1/2], Step [62930/64305], Loss: 4.8761\n",
      "Epoch [1/2], Step [62940/64305], Loss: 4.7802\n",
      "Epoch [1/2], Step [62950/64305], Loss: 5.0330\n",
      "Epoch [1/2], Step [62960/64305], Loss: 4.8098\n",
      "Epoch [1/2], Step [62970/64305], Loss: 4.9243\n",
      "Epoch [1/2], Step [62980/64305], Loss: 4.7866\n",
      "Epoch [1/2], Step [62990/64305], Loss: 4.7290\n",
      "Epoch [1/2], Step [63000/64305], Loss: 4.8301\n",
      "Epoch [1/2], Step [63010/64305], Loss: 4.8712\n",
      "Epoch [1/2], Step [63020/64305], Loss: 4.7211\n",
      "Epoch [1/2], Step [63030/64305], Loss: 4.7446\n",
      "Epoch [1/2], Step [63040/64305], Loss: 4.6750\n",
      "Epoch [1/2], Step [63050/64305], Loss: 4.8981\n",
      "Epoch [1/2], Step [63060/64305], Loss: 4.6668\n",
      "Epoch [1/2], Step [63070/64305], Loss: 4.8376\n",
      "Epoch [1/2], Step [63080/64305], Loss: 4.8281\n",
      "Epoch [1/2], Step [63090/64305], Loss: 4.7061\n",
      "Epoch [1/2], Step [63100/64305], Loss: 4.8824\n",
      "Epoch [1/2], Step [63110/64305], Loss: 4.7413\n",
      "Epoch [1/2], Step [63120/64305], Loss: 4.7262\n",
      "Epoch [1/2], Step [63130/64305], Loss: 4.8558\n",
      "Epoch [1/2], Step [63140/64305], Loss: 4.8105\n",
      "Epoch [1/2], Step [63150/64305], Loss: 4.8211\n",
      "Epoch [1/2], Step [63160/64305], Loss: 4.7494\n",
      "Epoch [1/2], Step [63170/64305], Loss: 4.8668\n",
      "Epoch [1/2], Step [63180/64305], Loss: 4.7888\n",
      "Epoch [1/2], Step [63190/64305], Loss: 4.9673\n",
      "Epoch [1/2], Step [63200/64305], Loss: 4.9100\n",
      "Epoch [1/2], Step [63210/64305], Loss: 4.8417\n",
      "Epoch [1/2], Step [63220/64305], Loss: 4.8820\n",
      "Epoch [1/2], Step [63230/64305], Loss: 4.9253\n",
      "Epoch [1/2], Step [63240/64305], Loss: 4.7471\n",
      "Epoch [1/2], Step [63250/64305], Loss: 4.8356\n",
      "Epoch [1/2], Step [63260/64305], Loss: 4.5986\n",
      "Epoch [1/2], Step [63270/64305], Loss: 4.8331\n",
      "Epoch [1/2], Step [63280/64305], Loss: 4.7400\n",
      "Epoch [1/2], Step [63290/64305], Loss: 4.6902\n",
      "Epoch [1/2], Step [63300/64305], Loss: 4.8587\n",
      "Epoch [1/2], Step [63310/64305], Loss: 4.8388\n",
      "Epoch [1/2], Step [63320/64305], Loss: 4.7946\n",
      "Epoch [1/2], Step [63330/64305], Loss: 4.8968\n",
      "Epoch [1/2], Step [63340/64305], Loss: 4.8623\n",
      "Epoch [1/2], Step [63350/64305], Loss: 4.9556\n",
      "Epoch [1/2], Step [63360/64305], Loss: 4.6445\n",
      "Epoch [1/2], Step [63370/64305], Loss: 4.8257\n",
      "Epoch [1/2], Step [63380/64305], Loss: 4.7376\n",
      "Epoch [1/2], Step [63390/64305], Loss: 4.9481\n",
      "Epoch [1/2], Step [63400/64305], Loss: 4.9344\n",
      "Epoch [1/2], Step [63410/64305], Loss: 5.0162\n",
      "Epoch [1/2], Step [63420/64305], Loss: 4.5254\n",
      "Epoch [1/2], Step [63430/64305], Loss: 4.6494\n",
      "Epoch [1/2], Step [63440/64305], Loss: 4.6662\n",
      "Epoch [1/2], Step [63450/64305], Loss: 4.8622\n",
      "Epoch [1/2], Step [63460/64305], Loss: 4.8250\n",
      "Epoch [1/2], Step [63470/64305], Loss: 4.7643\n",
      "Epoch [1/2], Step [63480/64305], Loss: 4.9608\n",
      "Epoch [1/2], Step [63490/64305], Loss: 4.9343\n",
      "Epoch [1/2], Step [63500/64305], Loss: 4.9439\n",
      "Epoch [1/2], Step [63510/64305], Loss: 4.9694\n",
      "Epoch [1/2], Step [63520/64305], Loss: 4.7027\n",
      "Epoch [1/2], Step [63530/64305], Loss: 4.6877\n",
      "Epoch [1/2], Step [63540/64305], Loss: 4.7916\n",
      "Epoch [1/2], Step [63550/64305], Loss: 4.6915\n",
      "Epoch [1/2], Step [63560/64305], Loss: 4.8444\n",
      "Epoch [1/2], Step [63570/64305], Loss: 4.7632\n",
      "Epoch [1/2], Step [63580/64305], Loss: 5.0291\n",
      "Epoch [1/2], Step [63590/64305], Loss: 4.8446\n",
      "Epoch [1/2], Step [63600/64305], Loss: 4.7358\n",
      "Epoch [1/2], Step [63610/64305], Loss: 4.7431\n",
      "Epoch [1/2], Step [63620/64305], Loss: 4.9334\n",
      "Epoch [1/2], Step [63630/64305], Loss: 4.9780\n",
      "Epoch [1/2], Step [63640/64305], Loss: 4.6831\n",
      "Epoch [1/2], Step [63650/64305], Loss: 4.9080\n",
      "Epoch [1/2], Step [63660/64305], Loss: 4.8668\n",
      "Epoch [1/2], Step [63670/64305], Loss: 4.4876\n",
      "Epoch [1/2], Step [63680/64305], Loss: 4.9459\n",
      "Epoch [1/2], Step [63690/64305], Loss: 4.7674\n",
      "Epoch [1/2], Step [63700/64305], Loss: 4.7622\n",
      "Epoch [1/2], Step [63710/64305], Loss: 4.7928\n",
      "Epoch [1/2], Step [63720/64305], Loss: 4.8620\n",
      "Epoch [1/2], Step [63730/64305], Loss: 4.8054\n",
      "Epoch [1/2], Step [63740/64305], Loss: 5.0111\n",
      "Epoch [1/2], Step [63750/64305], Loss: 4.9691\n",
      "Epoch [1/2], Step [63760/64305], Loss: 4.8385\n",
      "Epoch [1/2], Step [63770/64305], Loss: 4.7781\n",
      "Epoch [1/2], Step [63780/64305], Loss: 4.7985\n",
      "Epoch [1/2], Step [63790/64305], Loss: 4.8163\n",
      "Epoch [1/2], Step [63800/64305], Loss: 4.8018\n",
      "Epoch [1/2], Step [63810/64305], Loss: 4.9415\n",
      "Epoch [1/2], Step [63820/64305], Loss: 4.8348\n",
      "Epoch [1/2], Step [63830/64305], Loss: 4.8888\n",
      "Epoch [1/2], Step [63840/64305], Loss: 5.0010\n",
      "Epoch [1/2], Step [63850/64305], Loss: 4.9932\n",
      "Epoch [1/2], Step [63860/64305], Loss: 4.8788\n",
      "Epoch [1/2], Step [63870/64305], Loss: 4.8963\n",
      "Epoch [1/2], Step [63880/64305], Loss: 4.6977\n",
      "Epoch [1/2], Step [63890/64305], Loss: 4.7499\n",
      "Epoch [1/2], Step [63900/64305], Loss: 4.8704\n",
      "Epoch [1/2], Step [63910/64305], Loss: 4.9275\n",
      "Epoch [1/2], Step [63920/64305], Loss: 4.6787\n",
      "Epoch [1/2], Step [63930/64305], Loss: 4.7484\n",
      "Epoch [1/2], Step [63940/64305], Loss: 4.8082\n",
      "Epoch [1/2], Step [63950/64305], Loss: 5.0530\n",
      "Epoch [1/2], Step [63960/64305], Loss: 4.7981\n",
      "Epoch [1/2], Step [63970/64305], Loss: 4.7462\n",
      "Epoch [1/2], Step [63980/64305], Loss: 4.8672\n",
      "Epoch [1/2], Step [63990/64305], Loss: 4.7652\n",
      "Epoch [1/2], Step [64000/64305], Loss: 4.8013\n",
      "Epoch [1/2], Step [64010/64305], Loss: 4.8941\n",
      "Epoch [1/2], Step [64020/64305], Loss: 4.7034\n",
      "Epoch [1/2], Step [64030/64305], Loss: 4.9576\n",
      "Epoch [1/2], Step [64040/64305], Loss: 4.8803\n",
      "Epoch [1/2], Step [64050/64305], Loss: 4.9335\n",
      "Epoch [1/2], Step [64060/64305], Loss: 4.9185\n",
      "Epoch [1/2], Step [64070/64305], Loss: 4.7424\n",
      "Epoch [1/2], Step [64080/64305], Loss: 4.8802\n",
      "Epoch [1/2], Step [64090/64305], Loss: 4.9124\n",
      "Epoch [1/2], Step [64100/64305], Loss: 4.6871\n",
      "Epoch [1/2], Step [64110/64305], Loss: 4.8857\n",
      "Epoch [1/2], Step [64120/64305], Loss: 4.9753\n",
      "Epoch [1/2], Step [64130/64305], Loss: 4.9834\n",
      "Epoch [1/2], Step [64140/64305], Loss: 4.8743\n",
      "Epoch [1/2], Step [64150/64305], Loss: 4.6350\n",
      "Epoch [1/2], Step [64160/64305], Loss: 4.6728\n",
      "Epoch [1/2], Step [64170/64305], Loss: 4.8566\n",
      "Epoch [1/2], Step [64180/64305], Loss: 4.7645\n",
      "Epoch [1/2], Step [64190/64305], Loss: 4.8100\n",
      "Epoch [1/2], Step [64200/64305], Loss: 4.8804\n",
      "Epoch [1/2], Step [64210/64305], Loss: 4.6877\n",
      "Epoch [1/2], Step [64220/64305], Loss: 4.6865\n",
      "Epoch [1/2], Step [64230/64305], Loss: 4.8479\n",
      "Epoch [1/2], Step [64240/64305], Loss: 4.8773\n",
      "Epoch [1/2], Step [64250/64305], Loss: 4.8087\n",
      "Epoch [1/2], Step [64260/64305], Loss: 4.9066\n",
      "Epoch [1/2], Step [64270/64305], Loss: 4.9477\n",
      "Epoch [1/2], Step [64280/64305], Loss: 4.9113\n",
      "Epoch [1/2], Step [64290/64305], Loss: 4.8887\n",
      "Epoch [1/2], Step [64300/64305], Loss: 4.8336\n",
      "Epoch [1/2] Average Loss: 5.0900, Perplexity: 162.39\n",
      "Epoch [2/2], Step [0/64305], Loss: 4.8659\n",
      "Epoch [2/2], Step [10/64305], Loss: 4.9728\n",
      "Epoch [2/2], Step [20/64305], Loss: 4.9174\n",
      "Epoch [2/2], Step [30/64305], Loss: 4.8889\n",
      "Epoch [2/2], Step [40/64305], Loss: 4.8278\n",
      "Epoch [2/2], Step [50/64305], Loss: 4.7927\n",
      "Epoch [2/2], Step [60/64305], Loss: 4.8537\n",
      "Epoch [2/2], Step [70/64305], Loss: 4.7996\n",
      "Epoch [2/2], Step [80/64305], Loss: 4.7117\n",
      "Epoch [2/2], Step [90/64305], Loss: 4.9222\n",
      "Epoch [2/2], Step [100/64305], Loss: 4.8761\n",
      "Epoch [2/2], Step [110/64305], Loss: 4.8215\n",
      "Epoch [2/2], Step [120/64305], Loss: 4.7872\n",
      "Epoch [2/2], Step [130/64305], Loss: 4.9296\n",
      "Epoch [2/2], Step [140/64305], Loss: 4.8081\n",
      "Epoch [2/2], Step [150/64305], Loss: 4.7617\n",
      "Epoch [2/2], Step [160/64305], Loss: 4.6906\n",
      "Epoch [2/2], Step [170/64305], Loss: 4.7986\n",
      "Epoch [2/2], Step [180/64305], Loss: 4.7468\n",
      "Epoch [2/2], Step [190/64305], Loss: 4.8601\n",
      "Epoch [2/2], Step [200/64305], Loss: 4.6527\n",
      "Epoch [2/2], Step [210/64305], Loss: 4.6994\n",
      "Epoch [2/2], Step [220/64305], Loss: 4.8808\n",
      "Epoch [2/2], Step [230/64305], Loss: 4.8895\n",
      "Epoch [2/2], Step [240/64305], Loss: 4.8370\n",
      "Epoch [2/2], Step [250/64305], Loss: 4.8632\n",
      "Epoch [2/2], Step [260/64305], Loss: 4.7247\n",
      "Epoch [2/2], Step [270/64305], Loss: 4.8358\n",
      "Epoch [2/2], Step [280/64305], Loss: 4.9384\n",
      "Epoch [2/2], Step [290/64305], Loss: 4.7056\n",
      "Epoch [2/2], Step [300/64305], Loss: 4.8345\n",
      "Epoch [2/2], Step [310/64305], Loss: 4.9279\n",
      "Epoch [2/2], Step [320/64305], Loss: 4.6762\n",
      "Epoch [2/2], Step [330/64305], Loss: 5.0187\n",
      "Epoch [2/2], Step [340/64305], Loss: 4.7239\n",
      "Epoch [2/2], Step [350/64305], Loss: 4.9044\n",
      "Epoch [2/2], Step [360/64305], Loss: 4.6638\n",
      "Epoch [2/2], Step [370/64305], Loss: 4.9147\n",
      "Epoch [2/2], Step [380/64305], Loss: 4.5697\n",
      "Epoch [2/2], Step [390/64305], Loss: 4.9224\n",
      "Epoch [2/2], Step [400/64305], Loss: 4.9502\n",
      "Epoch [2/2], Step [410/64305], Loss: 4.7214\n",
      "Epoch [2/2], Step [420/64305], Loss: 4.8493\n",
      "Epoch [2/2], Step [430/64305], Loss: 4.9922\n",
      "Epoch [2/2], Step [440/64305], Loss: 4.8351\n",
      "Epoch [2/2], Step [450/64305], Loss: 4.6627\n",
      "Epoch [2/2], Step [460/64305], Loss: 5.0638\n",
      "Epoch [2/2], Step [470/64305], Loss: 4.7600\n",
      "Epoch [2/2], Step [480/64305], Loss: 4.7698\n",
      "Epoch [2/2], Step [490/64305], Loss: 4.7909\n",
      "Epoch [2/2], Step [500/64305], Loss: 4.9848\n",
      "Epoch [2/2], Step [510/64305], Loss: 4.6852\n",
      "Epoch [2/2], Step [520/64305], Loss: 4.9016\n",
      "Epoch [2/2], Step [530/64305], Loss: 4.8720\n",
      "Epoch [2/2], Step [540/64305], Loss: 4.7965\n",
      "Epoch [2/2], Step [550/64305], Loss: 4.9058\n",
      "Epoch [2/2], Step [560/64305], Loss: 4.9818\n",
      "Epoch [2/2], Step [570/64305], Loss: 4.8168\n",
      "Epoch [2/2], Step [580/64305], Loss: 4.8248\n",
      "Epoch [2/2], Step [590/64305], Loss: 4.8877\n",
      "Epoch [2/2], Step [600/64305], Loss: 4.8649\n",
      "Epoch [2/2], Step [610/64305], Loss: 4.9504\n",
      "Epoch [2/2], Step [620/64305], Loss: 4.9208\n",
      "Epoch [2/2], Step [630/64305], Loss: 5.0739\n",
      "Epoch [2/2], Step [640/64305], Loss: 4.9837\n",
      "Epoch [2/2], Step [650/64305], Loss: 4.6863\n",
      "Epoch [2/2], Step [660/64305], Loss: 4.9276\n",
      "Epoch [2/2], Step [670/64305], Loss: 4.8348\n",
      "Epoch [2/2], Step [680/64305], Loss: 4.8196\n",
      "Epoch [2/2], Step [690/64305], Loss: 4.7888\n",
      "Epoch [2/2], Step [700/64305], Loss: 4.6946\n",
      "Epoch [2/2], Step [710/64305], Loss: 4.6895\n",
      "Epoch [2/2], Step [720/64305], Loss: 4.9634\n",
      "Epoch [2/2], Step [730/64305], Loss: 4.7324\n",
      "Epoch [2/2], Step [740/64305], Loss: 4.7122\n",
      "Epoch [2/2], Step [750/64305], Loss: 4.6868\n",
      "Epoch [2/2], Step [760/64305], Loss: 4.8424\n",
      "Epoch [2/2], Step [770/64305], Loss: 4.7864\n",
      "Epoch [2/2], Step [780/64305], Loss: 4.7871\n",
      "Epoch [2/2], Step [790/64305], Loss: 4.7553\n",
      "Epoch [2/2], Step [800/64305], Loss: 4.8613\n",
      "Epoch [2/2], Step [810/64305], Loss: 4.6142\n",
      "Epoch [2/2], Step [820/64305], Loss: 4.7686\n",
      "Epoch [2/2], Step [830/64305], Loss: 4.8697\n",
      "Epoch [2/2], Step [840/64305], Loss: 4.6874\n",
      "Epoch [2/2], Step [850/64305], Loss: 4.8970\n",
      "Epoch [2/2], Step [860/64305], Loss: 4.7323\n",
      "Epoch [2/2], Step [870/64305], Loss: 4.5455\n",
      "Epoch [2/2], Step [880/64305], Loss: 4.9483\n",
      "Epoch [2/2], Step [890/64305], Loss: 4.8486\n",
      "Epoch [2/2], Step [900/64305], Loss: 5.0744\n",
      "Epoch [2/2], Step [910/64305], Loss: 4.8409\n",
      "Epoch [2/2], Step [920/64305], Loss: 4.7999\n",
      "Epoch [2/2], Step [930/64305], Loss: 5.0415\n",
      "Epoch [2/2], Step [940/64305], Loss: 4.7578\n",
      "Epoch [2/2], Step [950/64305], Loss: 4.8320\n",
      "Epoch [2/2], Step [960/64305], Loss: 4.7975\n",
      "Epoch [2/2], Step [970/64305], Loss: 4.9548\n",
      "Epoch [2/2], Step [980/64305], Loss: 4.9985\n",
      "Epoch [2/2], Step [990/64305], Loss: 4.6573\n",
      "Epoch [2/2], Step [1000/64305], Loss: 4.9204\n",
      "Epoch [2/2], Step [1010/64305], Loss: 4.9585\n",
      "Epoch [2/2], Step [1020/64305], Loss: 5.0624\n",
      "Epoch [2/2], Step [1030/64305], Loss: 4.7986\n",
      "Epoch [2/2], Step [1040/64305], Loss: 4.8600\n",
      "Epoch [2/2], Step [1050/64305], Loss: 4.6517\n",
      "Epoch [2/2], Step [1060/64305], Loss: 4.9195\n",
      "Epoch [2/2], Step [1070/64305], Loss: 4.6749\n",
      "Epoch [2/2], Step [1080/64305], Loss: 4.8682\n",
      "Epoch [2/2], Step [1090/64305], Loss: 4.6008\n",
      "Epoch [2/2], Step [1100/64305], Loss: 4.8775\n",
      "Epoch [2/2], Step [1110/64305], Loss: 4.8758\n",
      "Epoch [2/2], Step [1120/64305], Loss: 4.8641\n",
      "Epoch [2/2], Step [1130/64305], Loss: 4.8175\n",
      "Epoch [2/2], Step [1140/64305], Loss: 4.8327\n",
      "Epoch [2/2], Step [1150/64305], Loss: 4.8395\n",
      "Epoch [2/2], Step [1160/64305], Loss: 4.8079\n",
      "Epoch [2/2], Step [1170/64305], Loss: 4.8520\n",
      "Epoch [2/2], Step [1180/64305], Loss: 4.9717\n",
      "Epoch [2/2], Step [1190/64305], Loss: 5.0594\n",
      "Epoch [2/2], Step [1200/64305], Loss: 5.0675\n",
      "Epoch [2/2], Step [1210/64305], Loss: 4.9548\n",
      "Epoch [2/2], Step [1220/64305], Loss: 4.7925\n",
      "Epoch [2/2], Step [1230/64305], Loss: 4.6833\n",
      "Epoch [2/2], Step [1240/64305], Loss: 4.8324\n",
      "Epoch [2/2], Step [1250/64305], Loss: 4.9985\n",
      "Epoch [2/2], Step [1260/64305], Loss: 4.9295\n",
      "Epoch [2/2], Step [1270/64305], Loss: 4.8234\n",
      "Epoch [2/2], Step [1280/64305], Loss: 4.7857\n",
      "Epoch [2/2], Step [1290/64305], Loss: 4.6495\n",
      "Epoch [2/2], Step [1300/64305], Loss: 5.0407\n",
      "Epoch [2/2], Step [1310/64305], Loss: 4.6581\n",
      "Epoch [2/2], Step [1320/64305], Loss: 5.0109\n",
      "Epoch [2/2], Step [1330/64305], Loss: 4.9374\n",
      "Epoch [2/2], Step [1340/64305], Loss: 4.9650\n",
      "Epoch [2/2], Step [1350/64305], Loss: 4.8852\n",
      "Epoch [2/2], Step [1360/64305], Loss: 5.0303\n",
      "Epoch [2/2], Step [1370/64305], Loss: 5.0032\n",
      "Epoch [2/2], Step [1380/64305], Loss: 4.7053\n",
      "Epoch [2/2], Step [1390/64305], Loss: 4.8442\n",
      "Epoch [2/2], Step [1400/64305], Loss: 4.7603\n",
      "Epoch [2/2], Step [1410/64305], Loss: 4.8428\n",
      "Epoch [2/2], Step [1420/64305], Loss: 4.9548\n",
      "Epoch [2/2], Step [1430/64305], Loss: 5.1748\n",
      "Epoch [2/2], Step [1440/64305], Loss: 4.5964\n",
      "Epoch [2/2], Step [1450/64305], Loss: 4.8430\n",
      "Epoch [2/2], Step [1460/64305], Loss: 4.7406\n",
      "Epoch [2/2], Step [1470/64305], Loss: 4.9146\n",
      "Epoch [2/2], Step [1480/64305], Loss: 4.8932\n",
      "Epoch [2/2], Step [1490/64305], Loss: 4.7251\n",
      "Epoch [2/2], Step [1500/64305], Loss: 4.7265\n",
      "Epoch [2/2], Step [1510/64305], Loss: 4.9830\n",
      "Epoch [2/2], Step [1520/64305], Loss: 4.8268\n",
      "Epoch [2/2], Step [1530/64305], Loss: 4.9790\n",
      "Epoch [2/2], Step [1540/64305], Loss: 4.6796\n",
      "Epoch [2/2], Step [1550/64305], Loss: 4.8833\n",
      "Epoch [2/2], Step [1560/64305], Loss: 4.7274\n",
      "Epoch [2/2], Step [1570/64305], Loss: 4.7276\n",
      "Epoch [2/2], Step [1580/64305], Loss: 4.7409\n",
      "Epoch [2/2], Step [1590/64305], Loss: 4.9579\n",
      "Epoch [2/2], Step [1600/64305], Loss: 4.7190\n",
      "Epoch [2/2], Step [1610/64305], Loss: 4.9479\n",
      "Epoch [2/2], Step [1620/64305], Loss: 4.7099\n",
      "Epoch [2/2], Step [1630/64305], Loss: 4.7922\n",
      "Epoch [2/2], Step [1640/64305], Loss: 4.9149\n",
      "Epoch [2/2], Step [1650/64305], Loss: 4.8979\n",
      "Epoch [2/2], Step [1660/64305], Loss: 4.7301\n",
      "Epoch [2/2], Step [1670/64305], Loss: 4.8803\n",
      "Epoch [2/2], Step [1680/64305], Loss: 4.6917\n",
      "Epoch [2/2], Step [1690/64305], Loss: 4.7200\n",
      "Epoch [2/2], Step [1700/64305], Loss: 4.9640\n",
      "Epoch [2/2], Step [1710/64305], Loss: 5.0353\n",
      "Epoch [2/2], Step [1720/64305], Loss: 4.5703\n",
      "Epoch [2/2], Step [1730/64305], Loss: 4.7231\n",
      "Epoch [2/2], Step [1740/64305], Loss: 4.7779\n",
      "Epoch [2/2], Step [1750/64305], Loss: 5.1455\n",
      "Epoch [2/2], Step [1760/64305], Loss: 4.9410\n",
      "Epoch [2/2], Step [1770/64305], Loss: 4.7674\n",
      "Epoch [2/2], Step [1780/64305], Loss: 4.8428\n",
      "Epoch [2/2], Step [1790/64305], Loss: 4.9454\n",
      "Epoch [2/2], Step [1800/64305], Loss: 4.8103\n",
      "Epoch [2/2], Step [1810/64305], Loss: 4.8817\n",
      "Epoch [2/2], Step [1820/64305], Loss: 4.9747\n",
      "Epoch [2/2], Step [1830/64305], Loss: 4.8906\n",
      "Epoch [2/2], Step [1840/64305], Loss: 4.8017\n",
      "Epoch [2/2], Step [1850/64305], Loss: 4.8627\n",
      "Epoch [2/2], Step [1860/64305], Loss: 5.0093\n",
      "Epoch [2/2], Step [1870/64305], Loss: 4.6680\n",
      "Epoch [2/2], Step [1880/64305], Loss: 4.7004\n",
      "Epoch [2/2], Step [1890/64305], Loss: 4.6684\n",
      "Epoch [2/2], Step [1900/64305], Loss: 5.1120\n",
      "Epoch [2/2], Step [1910/64305], Loss: 4.7188\n",
      "Epoch [2/2], Step [1920/64305], Loss: 5.0184\n",
      "Epoch [2/2], Step [1930/64305], Loss: 4.8038\n",
      "Epoch [2/2], Step [1940/64305], Loss: 4.8787\n",
      "Epoch [2/2], Step [1950/64305], Loss: 5.0107\n",
      "Epoch [2/2], Step [1960/64305], Loss: 4.9207\n",
      "Epoch [2/2], Step [1970/64305], Loss: 4.7185\n",
      "Epoch [2/2], Step [1980/64305], Loss: 4.6933\n",
      "Epoch [2/2], Step [1990/64305], Loss: 4.9513\n",
      "Epoch [2/2], Step [2000/64305], Loss: 4.6805\n",
      "Epoch [2/2], Step [2010/64305], Loss: 4.9148\n",
      "Epoch [2/2], Step [2020/64305], Loss: 4.7174\n",
      "Epoch [2/2], Step [2030/64305], Loss: 4.9053\n",
      "Epoch [2/2], Step [2040/64305], Loss: 4.8465\n",
      "Epoch [2/2], Step [2050/64305], Loss: 4.8344\n",
      "Epoch [2/2], Step [2060/64305], Loss: 5.0058\n",
      "Epoch [2/2], Step [2070/64305], Loss: 4.7269\n",
      "Epoch [2/2], Step [2080/64305], Loss: 4.9976\n",
      "Epoch [2/2], Step [2090/64305], Loss: 4.8000\n",
      "Epoch [2/2], Step [2100/64305], Loss: 4.9249\n",
      "Epoch [2/2], Step [2110/64305], Loss: 4.8259\n",
      "Epoch [2/2], Step [2120/64305], Loss: 4.7133\n",
      "Epoch [2/2], Step [2130/64305], Loss: 4.8651\n",
      "Epoch [2/2], Step [2140/64305], Loss: 4.6003\n",
      "Epoch [2/2], Step [2150/64305], Loss: 4.9045\n",
      "Epoch [2/2], Step [2160/64305], Loss: 4.7687\n",
      "Epoch [2/2], Step [2170/64305], Loss: 4.8837\n",
      "Epoch [2/2], Step [2180/64305], Loss: 4.8948\n",
      "Epoch [2/2], Step [2190/64305], Loss: 4.9568\n",
      "Epoch [2/2], Step [2200/64305], Loss: 4.8114\n",
      "Epoch [2/2], Step [2210/64305], Loss: 4.6292\n",
      "Epoch [2/2], Step [2220/64305], Loss: 4.8092\n",
      "Epoch [2/2], Step [2230/64305], Loss: 4.9356\n",
      "Epoch [2/2], Step [2240/64305], Loss: 4.9596\n",
      "Epoch [2/2], Step [2250/64305], Loss: 4.8461\n",
      "Epoch [2/2], Step [2260/64305], Loss: 4.7822\n",
      "Epoch [2/2], Step [2270/64305], Loss: 4.6614\n",
      "Epoch [2/2], Step [2280/64305], Loss: 4.7391\n",
      "Epoch [2/2], Step [2290/64305], Loss: 4.8631\n",
      "Epoch [2/2], Step [2300/64305], Loss: 4.9982\n",
      "Epoch [2/2], Step [2310/64305], Loss: 4.8159\n",
      "Epoch [2/2], Step [2320/64305], Loss: 4.8263\n",
      "Epoch [2/2], Step [2330/64305], Loss: 4.8682\n",
      "Epoch [2/2], Step [2340/64305], Loss: 4.7322\n",
      "Epoch [2/2], Step [2350/64305], Loss: 4.8323\n",
      "Epoch [2/2], Step [2360/64305], Loss: 4.6999\n",
      "Epoch [2/2], Step [2370/64305], Loss: 4.6837\n",
      "Epoch [2/2], Step [2380/64305], Loss: 4.5989\n",
      "Epoch [2/2], Step [2390/64305], Loss: 4.8625\n",
      "Epoch [2/2], Step [2400/64305], Loss: 4.7292\n",
      "Epoch [2/2], Step [2410/64305], Loss: 4.8115\n",
      "Epoch [2/2], Step [2420/64305], Loss: 4.8690\n",
      "Epoch [2/2], Step [2430/64305], Loss: 4.7345\n",
      "Epoch [2/2], Step [2440/64305], Loss: 4.7827\n",
      "Epoch [2/2], Step [2450/64305], Loss: 4.9997\n",
      "Epoch [2/2], Step [2460/64305], Loss: 4.7364\n",
      "Epoch [2/2], Step [2470/64305], Loss: 4.8920\n",
      "Epoch [2/2], Step [2480/64305], Loss: 5.0066\n",
      "Epoch [2/2], Step [2490/64305], Loss: 4.9751\n",
      "Epoch [2/2], Step [2500/64305], Loss: 4.8844\n",
      "Epoch [2/2], Step [2510/64305], Loss: 4.9962\n",
      "Epoch [2/2], Step [2520/64305], Loss: 4.7634\n",
      "Epoch [2/2], Step [2530/64305], Loss: 4.9518\n",
      "Epoch [2/2], Step [2540/64305], Loss: 4.8702\n",
      "Epoch [2/2], Step [2550/64305], Loss: 4.7183\n",
      "Epoch [2/2], Step [2560/64305], Loss: 4.6915\n",
      "Epoch [2/2], Step [2570/64305], Loss: 4.9865\n",
      "Epoch [2/2], Step [2580/64305], Loss: 4.8826\n",
      "Epoch [2/2], Step [2590/64305], Loss: 4.8252\n",
      "Epoch [2/2], Step [2600/64305], Loss: 4.9850\n",
      "Epoch [2/2], Step [2610/64305], Loss: 4.7855\n",
      "Epoch [2/2], Step [2620/64305], Loss: 5.0455\n",
      "Epoch [2/2], Step [2630/64305], Loss: 4.8878\n",
      "Epoch [2/2], Step [2640/64305], Loss: 4.9580\n",
      "Epoch [2/2], Step [2650/64305], Loss: 4.8744\n",
      "Epoch [2/2], Step [2660/64305], Loss: 4.9551\n",
      "Epoch [2/2], Step [2670/64305], Loss: 4.7205\n",
      "Epoch [2/2], Step [2680/64305], Loss: 4.8497\n",
      "Epoch [2/2], Step [2690/64305], Loss: 4.7671\n",
      "Epoch [2/2], Step [2700/64305], Loss: 5.1465\n",
      "Epoch [2/2], Step [2710/64305], Loss: 4.6887\n",
      "Epoch [2/2], Step [2720/64305], Loss: 4.8717\n",
      "Epoch [2/2], Step [2730/64305], Loss: 4.6864\n",
      "Epoch [2/2], Step [2740/64305], Loss: 4.9767\n",
      "Epoch [2/2], Step [2750/64305], Loss: 4.6607\n",
      "Epoch [2/2], Step [2760/64305], Loss: 4.6014\n",
      "Epoch [2/2], Step [2770/64305], Loss: 4.7619\n",
      "Epoch [2/2], Step [2780/64305], Loss: 4.7155\n",
      "Epoch [2/2], Step [2790/64305], Loss: 4.8600\n",
      "Epoch [2/2], Step [2800/64305], Loss: 4.8031\n",
      "Epoch [2/2], Step [2810/64305], Loss: 4.7802\n",
      "Epoch [2/2], Step [2820/64305], Loss: 4.7923\n",
      "Epoch [2/2], Step [2830/64305], Loss: 4.6055\n",
      "Epoch [2/2], Step [2840/64305], Loss: 4.8669\n",
      "Epoch [2/2], Step [2850/64305], Loss: 5.0594\n",
      "Epoch [2/2], Step [2860/64305], Loss: 4.7297\n",
      "Epoch [2/2], Step [2870/64305], Loss: 4.9245\n",
      "Epoch [2/2], Step [2880/64305], Loss: 4.6734\n",
      "Epoch [2/2], Step [2890/64305], Loss: 4.9814\n",
      "Epoch [2/2], Step [2900/64305], Loss: 4.7410\n",
      "Epoch [2/2], Step [2910/64305], Loss: 4.8986\n",
      "Epoch [2/2], Step [2920/64305], Loss: 4.8367\n",
      "Epoch [2/2], Step [2930/64305], Loss: 4.8633\n",
      "Epoch [2/2], Step [2940/64305], Loss: 4.8540\n",
      "Epoch [2/2], Step [2950/64305], Loss: 4.7589\n",
      "Epoch [2/2], Step [2960/64305], Loss: 4.9333\n",
      "Epoch [2/2], Step [2970/64305], Loss: 4.7386\n",
      "Epoch [2/2], Step [2980/64305], Loss: 4.8285\n",
      "Epoch [2/2], Step [2990/64305], Loss: 4.9339\n",
      "Epoch [2/2], Step [3000/64305], Loss: 4.6369\n",
      "Epoch [2/2], Step [3010/64305], Loss: 4.6469\n",
      "Epoch [2/2], Step [3020/64305], Loss: 4.7971\n",
      "Epoch [2/2], Step [3030/64305], Loss: 4.8628\n",
      "Epoch [2/2], Step [3040/64305], Loss: 4.9029\n",
      "Epoch [2/2], Step [3050/64305], Loss: 4.9319\n",
      "Epoch [2/2], Step [3060/64305], Loss: 4.8603\n",
      "Epoch [2/2], Step [3070/64305], Loss: 4.9083\n",
      "Epoch [2/2], Step [3080/64305], Loss: 4.5204\n",
      "Epoch [2/2], Step [3090/64305], Loss: 4.9678\n",
      "Epoch [2/2], Step [3100/64305], Loss: 4.7794\n",
      "Epoch [2/2], Step [3110/64305], Loss: 4.7795\n",
      "Epoch [2/2], Step [3120/64305], Loss: 4.8754\n",
      "Epoch [2/2], Step [3130/64305], Loss: 4.9123\n",
      "Epoch [2/2], Step [3140/64305], Loss: 4.9068\n",
      "Epoch [2/2], Step [3150/64305], Loss: 4.8632\n",
      "Epoch [2/2], Step [3160/64305], Loss: 4.7223\n",
      "Epoch [2/2], Step [3170/64305], Loss: 4.7137\n",
      "Epoch [2/2], Step [3180/64305], Loss: 5.0333\n",
      "Epoch [2/2], Step [3190/64305], Loss: 4.8725\n",
      "Epoch [2/2], Step [3200/64305], Loss: 4.9500\n",
      "Epoch [2/2], Step [3210/64305], Loss: 4.8590\n",
      "Epoch [2/2], Step [3220/64305], Loss: 4.8256\n",
      "Epoch [2/2], Step [3230/64305], Loss: 5.0504\n",
      "Epoch [2/2], Step [3240/64305], Loss: 4.9590\n",
      "Epoch [2/2], Step [3250/64305], Loss: 4.8882\n",
      "Epoch [2/2], Step [3260/64305], Loss: 4.6946\n",
      "Epoch [2/2], Step [3270/64305], Loss: 4.7629\n",
      "Epoch [2/2], Step [3280/64305], Loss: 4.8041\n",
      "Epoch [2/2], Step [3290/64305], Loss: 4.8060\n",
      "Epoch [2/2], Step [3300/64305], Loss: 5.0187\n",
      "Epoch [2/2], Step [3310/64305], Loss: 4.8329\n",
      "Epoch [2/2], Step [3320/64305], Loss: 4.6849\n",
      "Epoch [2/2], Step [3330/64305], Loss: 5.0147\n",
      "Epoch [2/2], Step [3340/64305], Loss: 4.8938\n",
      "Epoch [2/2], Step [3350/64305], Loss: 4.5483\n",
      "Epoch [2/2], Step [3360/64305], Loss: 4.9515\n",
      "Epoch [2/2], Step [3370/64305], Loss: 4.6760\n",
      "Epoch [2/2], Step [3380/64305], Loss: 4.5778\n",
      "Epoch [2/2], Step [3390/64305], Loss: 4.9098\n",
      "Epoch [2/2], Step [3400/64305], Loss: 4.8414\n",
      "Epoch [2/2], Step [3410/64305], Loss: 4.8333\n",
      "Epoch [2/2], Step [3420/64305], Loss: 4.8783\n",
      "Epoch [2/2], Step [3430/64305], Loss: 4.8349\n",
      "Epoch [2/2], Step [3440/64305], Loss: 4.8345\n",
      "Epoch [2/2], Step [3450/64305], Loss: 5.1078\n",
      "Epoch [2/2], Step [3460/64305], Loss: 4.8529\n",
      "Epoch [2/2], Step [3470/64305], Loss: 5.0057\n",
      "Epoch [2/2], Step [3480/64305], Loss: 4.6957\n",
      "Epoch [2/2], Step [3490/64305], Loss: 4.5733\n",
      "Epoch [2/2], Step [3500/64305], Loss: 4.8069\n",
      "Epoch [2/2], Step [3510/64305], Loss: 4.9716\n",
      "Epoch [2/2], Step [3520/64305], Loss: 4.9934\n",
      "Epoch [2/2], Step [3530/64305], Loss: 4.6773\n",
      "Epoch [2/2], Step [3540/64305], Loss: 4.8570\n",
      "Epoch [2/2], Step [3550/64305], Loss: 4.7873\n",
      "Epoch [2/2], Step [3560/64305], Loss: 4.7593\n",
      "Epoch [2/2], Step [3570/64305], Loss: 4.8934\n",
      "Epoch [2/2], Step [3580/64305], Loss: 4.9160\n",
      "Epoch [2/2], Step [3590/64305], Loss: 4.9048\n",
      "Epoch [2/2], Step [3600/64305], Loss: 4.7480\n",
      "Epoch [2/2], Step [3610/64305], Loss: 4.8620\n",
      "Epoch [2/2], Step [3620/64305], Loss: 4.8504\n",
      "Epoch [2/2], Step [3630/64305], Loss: 4.7775\n",
      "Epoch [2/2], Step [3640/64305], Loss: 4.7961\n",
      "Epoch [2/2], Step [3650/64305], Loss: 4.9249\n",
      "Epoch [2/2], Step [3660/64305], Loss: 4.8568\n",
      "Epoch [2/2], Step [3670/64305], Loss: 4.7039\n",
      "Epoch [2/2], Step [3680/64305], Loss: 4.7882\n",
      "Epoch [2/2], Step [3690/64305], Loss: 4.9000\n",
      "Epoch [2/2], Step [3700/64305], Loss: 4.8064\n",
      "Epoch [2/2], Step [3710/64305], Loss: 4.7962\n",
      "Epoch [2/2], Step [3720/64305], Loss: 4.9225\n",
      "Epoch [2/2], Step [3730/64305], Loss: 4.6788\n",
      "Epoch [2/2], Step [3740/64305], Loss: 4.6881\n",
      "Epoch [2/2], Step [3750/64305], Loss: 4.9035\n",
      "Epoch [2/2], Step [3760/64305], Loss: 5.0452\n",
      "Epoch [2/2], Step [3770/64305], Loss: 4.7256\n",
      "Epoch [2/2], Step [3780/64305], Loss: 4.7050\n",
      "Epoch [2/2], Step [3790/64305], Loss: 5.0738\n",
      "Epoch [2/2], Step [3800/64305], Loss: 4.8384\n",
      "Epoch [2/2], Step [3810/64305], Loss: 4.8399\n",
      "Epoch [2/2], Step [3820/64305], Loss: 4.8910\n",
      "Epoch [2/2], Step [3830/64305], Loss: 4.7543\n",
      "Epoch [2/2], Step [3840/64305], Loss: 5.1862\n",
      "Epoch [2/2], Step [3850/64305], Loss: 4.7161\n",
      "Epoch [2/2], Step [3860/64305], Loss: 4.8637\n",
      "Epoch [2/2], Step [3870/64305], Loss: 4.8587\n",
      "Epoch [2/2], Step [3880/64305], Loss: 4.7010\n",
      "Epoch [2/2], Step [3890/64305], Loss: 4.7170\n",
      "Epoch [2/2], Step [3900/64305], Loss: 4.8716\n",
      "Epoch [2/2], Step [3910/64305], Loss: 4.8105\n",
      "Epoch [2/2], Step [3920/64305], Loss: 4.8505\n",
      "Epoch [2/2], Step [3930/64305], Loss: 4.9043\n",
      "Epoch [2/2], Step [3940/64305], Loss: 4.8294\n",
      "Epoch [2/2], Step [3950/64305], Loss: 4.5995\n",
      "Epoch [2/2], Step [3960/64305], Loss: 4.9810\n",
      "Epoch [2/2], Step [3970/64305], Loss: 4.7906\n",
      "Epoch [2/2], Step [3980/64305], Loss: 4.8867\n",
      "Epoch [2/2], Step [3990/64305], Loss: 4.7354\n",
      "Epoch [2/2], Step [4000/64305], Loss: 4.6491\n",
      "Epoch [2/2], Step [4010/64305], Loss: 4.9385\n",
      "Epoch [2/2], Step [4020/64305], Loss: 4.7507\n",
      "Epoch [2/2], Step [4030/64305], Loss: 4.9863\n",
      "Epoch [2/2], Step [4040/64305], Loss: 5.0247\n",
      "Epoch [2/2], Step [4050/64305], Loss: 4.7347\n",
      "Epoch [2/2], Step [4060/64305], Loss: 4.8127\n",
      "Epoch [2/2], Step [4070/64305], Loss: 4.7017\n",
      "Epoch [2/2], Step [4080/64305], Loss: 4.7208\n",
      "Epoch [2/2], Step [4090/64305], Loss: 4.8269\n",
      "Epoch [2/2], Step [4100/64305], Loss: 4.8203\n",
      "Epoch [2/2], Step [4110/64305], Loss: 4.7896\n",
      "Epoch [2/2], Step [4120/64305], Loss: 4.9106\n",
      "Epoch [2/2], Step [4130/64305], Loss: 4.9084\n",
      "Epoch [2/2], Step [4140/64305], Loss: 5.2780\n",
      "Epoch [2/2], Step [4150/64305], Loss: 4.7764\n",
      "Epoch [2/2], Step [4160/64305], Loss: 4.7803\n",
      "Epoch [2/2], Step [4170/64305], Loss: 5.0634\n",
      "Epoch [2/2], Step [4180/64305], Loss: 4.6110\n",
      "Epoch [2/2], Step [4190/64305], Loss: 4.7778\n",
      "Epoch [2/2], Step [4200/64305], Loss: 4.8317\n",
      "Epoch [2/2], Step [4210/64305], Loss: 4.7761\n",
      "Epoch [2/2], Step [4220/64305], Loss: 4.7583\n",
      "Epoch [2/2], Step [4230/64305], Loss: 4.6724\n",
      "Epoch [2/2], Step [4240/64305], Loss: 4.7885\n",
      "Epoch [2/2], Step [4250/64305], Loss: 4.8661\n",
      "Epoch [2/2], Step [4260/64305], Loss: 4.6602\n",
      "Epoch [2/2], Step [4270/64305], Loss: 4.8814\n",
      "Epoch [2/2], Step [4280/64305], Loss: 4.7793\n",
      "Epoch [2/2], Step [4290/64305], Loss: 4.9750\n",
      "Epoch [2/2], Step [4300/64305], Loss: 4.8297\n",
      "Epoch [2/2], Step [4310/64305], Loss: 4.8198\n",
      "Epoch [2/2], Step [4320/64305], Loss: 4.9784\n",
      "Epoch [2/2], Step [4330/64305], Loss: 4.8946\n",
      "Epoch [2/2], Step [4340/64305], Loss: 4.7520\n",
      "Epoch [2/2], Step [4350/64305], Loss: 4.7564\n",
      "Epoch [2/2], Step [4360/64305], Loss: 4.6539\n",
      "Epoch [2/2], Step [4370/64305], Loss: 4.9232\n",
      "Epoch [2/2], Step [4380/64305], Loss: 5.0319\n",
      "Epoch [2/2], Step [4390/64305], Loss: 4.7806\n",
      "Epoch [2/2], Step [4400/64305], Loss: 4.9230\n",
      "Epoch [2/2], Step [4410/64305], Loss: 4.7070\n",
      "Epoch [2/2], Step [4420/64305], Loss: 4.9250\n",
      "Epoch [2/2], Step [4430/64305], Loss: 4.9248\n",
      "Epoch [2/2], Step [4440/64305], Loss: 4.8792\n",
      "Epoch [2/2], Step [4450/64305], Loss: 4.9171\n",
      "Epoch [2/2], Step [4460/64305], Loss: 4.9179\n",
      "Epoch [2/2], Step [4470/64305], Loss: 4.6893\n",
      "Epoch [2/2], Step [4480/64305], Loss: 4.7428\n",
      "Epoch [2/2], Step [4490/64305], Loss: 4.9251\n",
      "Epoch [2/2], Step [4500/64305], Loss: 4.9416\n",
      "Epoch [2/2], Step [4510/64305], Loss: 4.6986\n",
      "Epoch [2/2], Step [4520/64305], Loss: 4.8913\n",
      "Epoch [2/2], Step [4530/64305], Loss: 4.7729\n",
      "Epoch [2/2], Step [4540/64305], Loss: 4.7291\n",
      "Epoch [2/2], Step [4550/64305], Loss: 4.9580\n",
      "Epoch [2/2], Step [4560/64305], Loss: 4.8927\n",
      "Epoch [2/2], Step [4570/64305], Loss: 4.8775\n",
      "Epoch [2/2], Step [4580/64305], Loss: 4.8548\n",
      "Epoch [2/2], Step [4590/64305], Loss: 5.1659\n",
      "Epoch [2/2], Step [4600/64305], Loss: 4.8067\n",
      "Epoch [2/2], Step [4610/64305], Loss: 4.8485\n",
      "Epoch [2/2], Step [4620/64305], Loss: 4.6787\n",
      "Epoch [2/2], Step [4630/64305], Loss: 4.6988\n",
      "Epoch [2/2], Step [4640/64305], Loss: 4.8396\n",
      "Epoch [2/2], Step [4650/64305], Loss: 4.7875\n",
      "Epoch [2/2], Step [4660/64305], Loss: 4.8306\n",
      "Epoch [2/2], Step [4670/64305], Loss: 4.8416\n",
      "Epoch [2/2], Step [4680/64305], Loss: 4.6446\n",
      "Epoch [2/2], Step [4690/64305], Loss: 5.0580\n",
      "Epoch [2/2], Step [4700/64305], Loss: 4.9735\n",
      "Epoch [2/2], Step [4710/64305], Loss: 4.7918\n",
      "Epoch [2/2], Step [4720/64305], Loss: 4.9430\n",
      "Epoch [2/2], Step [4730/64305], Loss: 4.8937\n",
      "Epoch [2/2], Step [4740/64305], Loss: 4.8719\n",
      "Epoch [2/2], Step [4750/64305], Loss: 4.8969\n",
      "Epoch [2/2], Step [4760/64305], Loss: 4.7657\n",
      "Epoch [2/2], Step [4770/64305], Loss: 4.6163\n",
      "Epoch [2/2], Step [4780/64305], Loss: 4.7948\n",
      "Epoch [2/2], Step [4790/64305], Loss: 4.7276\n",
      "Epoch [2/2], Step [4800/64305], Loss: 4.8003\n",
      "Epoch [2/2], Step [4810/64305], Loss: 4.8836\n",
      "Epoch [2/2], Step [4820/64305], Loss: 4.8981\n",
      "Epoch [2/2], Step [4830/64305], Loss: 4.9468\n",
      "Epoch [2/2], Step [4840/64305], Loss: 4.8545\n",
      "Epoch [2/2], Step [4850/64305], Loss: 4.9052\n",
      "Epoch [2/2], Step [4860/64305], Loss: 4.9691\n",
      "Epoch [2/2], Step [4870/64305], Loss: 4.8154\n",
      "Epoch [2/2], Step [4880/64305], Loss: 4.9377\n",
      "Epoch [2/2], Step [4890/64305], Loss: 4.7317\n",
      "Epoch [2/2], Step [4900/64305], Loss: 5.0281\n",
      "Epoch [2/2], Step [4910/64305], Loss: 4.7798\n",
      "Epoch [2/2], Step [4920/64305], Loss: 4.6772\n",
      "Epoch [2/2], Step [4930/64305], Loss: 4.8284\n",
      "Epoch [2/2], Step [4940/64305], Loss: 4.7393\n",
      "Epoch [2/2], Step [4950/64305], Loss: 4.9076\n",
      "Epoch [2/2], Step [4960/64305], Loss: 4.8094\n",
      "Epoch [2/2], Step [4970/64305], Loss: 4.7314\n",
      "Epoch [2/2], Step [4980/64305], Loss: 4.7842\n",
      "Epoch [2/2], Step [4990/64305], Loss: 4.7795\n",
      "Epoch [2/2], Step [5000/64305], Loss: 4.8944\n",
      "Epoch [2/2], Step [5010/64305], Loss: 5.1303\n",
      "Epoch [2/2], Step [5020/64305], Loss: 4.6790\n",
      "Epoch [2/2], Step [5030/64305], Loss: 4.7025\n",
      "Epoch [2/2], Step [5040/64305], Loss: 4.6618\n",
      "Epoch [2/2], Step [5050/64305], Loss: 4.9523\n",
      "Epoch [2/2], Step [5060/64305], Loss: 4.8416\n",
      "Epoch [2/2], Step [5070/64305], Loss: 4.8228\n",
      "Epoch [2/2], Step [5080/64305], Loss: 4.8930\n",
      "Epoch [2/2], Step [5090/64305], Loss: 4.9383\n",
      "Epoch [2/2], Step [5100/64305], Loss: 4.5939\n",
      "Epoch [2/2], Step [5110/64305], Loss: 4.8095\n",
      "Epoch [2/2], Step [5120/64305], Loss: 4.7943\n",
      "Epoch [2/2], Step [5130/64305], Loss: 4.6817\n",
      "Epoch [2/2], Step [5140/64305], Loss: 4.7444\n",
      "Epoch [2/2], Step [5150/64305], Loss: 4.6437\n",
      "Epoch [2/2], Step [5160/64305], Loss: 4.8140\n",
      "Epoch [2/2], Step [5170/64305], Loss: 4.5761\n",
      "Epoch [2/2], Step [5180/64305], Loss: 4.8306\n",
      "Epoch [2/2], Step [5190/64305], Loss: 4.6610\n",
      "Epoch [2/2], Step [5200/64305], Loss: 4.9697\n",
      "Epoch [2/2], Step [5210/64305], Loss: 4.6021\n",
      "Epoch [2/2], Step [5220/64305], Loss: 4.7593\n",
      "Epoch [2/2], Step [5230/64305], Loss: 4.9228\n",
      "Epoch [2/2], Step [5240/64305], Loss: 4.7929\n",
      "Epoch [2/2], Step [5250/64305], Loss: 4.7790\n",
      "Epoch [2/2], Step [5260/64305], Loss: 4.7804\n",
      "Epoch [2/2], Step [5270/64305], Loss: 4.8099\n",
      "Epoch [2/2], Step [5280/64305], Loss: 4.6918\n",
      "Epoch [2/2], Step [5290/64305], Loss: 4.6576\n",
      "Epoch [2/2], Step [5300/64305], Loss: 4.7165\n",
      "Epoch [2/2], Step [5310/64305], Loss: 4.9843\n",
      "Epoch [2/2], Step [5320/64305], Loss: 4.9379\n",
      "Epoch [2/2], Step [5330/64305], Loss: 4.6861\n",
      "Epoch [2/2], Step [5340/64305], Loss: 4.8624\n",
      "Epoch [2/2], Step [5350/64305], Loss: 4.8737\n",
      "Epoch [2/2], Step [5360/64305], Loss: 4.7159\n",
      "Epoch [2/2], Step [5370/64305], Loss: 4.8231\n",
      "Epoch [2/2], Step [5380/64305], Loss: 4.6726\n",
      "Epoch [2/2], Step [5390/64305], Loss: 4.7773\n",
      "Epoch [2/2], Step [5400/64305], Loss: 4.7635\n",
      "Epoch [2/2], Step [5410/64305], Loss: 4.9451\n",
      "Epoch [2/2], Step [5420/64305], Loss: 4.9476\n",
      "Epoch [2/2], Step [5430/64305], Loss: 4.8871\n",
      "Epoch [2/2], Step [5440/64305], Loss: 4.9988\n",
      "Epoch [2/2], Step [5450/64305], Loss: 4.8818\n",
      "Epoch [2/2], Step [5460/64305], Loss: 4.9146\n",
      "Epoch [2/2], Step [5470/64305], Loss: 4.7179\n",
      "Epoch [2/2], Step [5480/64305], Loss: 4.9099\n",
      "Epoch [2/2], Step [5490/64305], Loss: 4.6333\n",
      "Epoch [2/2], Step [5500/64305], Loss: 4.7690\n",
      "Epoch [2/2], Step [5510/64305], Loss: 4.9055\n",
      "Epoch [2/2], Step [5520/64305], Loss: 4.8076\n",
      "Epoch [2/2], Step [5530/64305], Loss: 4.8921\n",
      "Epoch [2/2], Step [5540/64305], Loss: 4.9418\n",
      "Epoch [2/2], Step [5550/64305], Loss: 4.7453\n",
      "Epoch [2/2], Step [5560/64305], Loss: 4.6115\n",
      "Epoch [2/2], Step [5570/64305], Loss: 4.6128\n",
      "Epoch [2/2], Step [5580/64305], Loss: 4.8743\n",
      "Epoch [2/2], Step [5590/64305], Loss: 4.7127\n",
      "Epoch [2/2], Step [5600/64305], Loss: 4.7050\n",
      "Epoch [2/2], Step [5610/64305], Loss: 4.9054\n",
      "Epoch [2/2], Step [5620/64305], Loss: 4.9561\n",
      "Epoch [2/2], Step [5630/64305], Loss: 4.6829\n",
      "Epoch [2/2], Step [5640/64305], Loss: 4.6641\n",
      "Epoch [2/2], Step [5650/64305], Loss: 4.8008\n",
      "Epoch [2/2], Step [5660/64305], Loss: 4.6293\n",
      "Epoch [2/2], Step [5670/64305], Loss: 4.7998\n",
      "Epoch [2/2], Step [5680/64305], Loss: 4.8500\n",
      "Epoch [2/2], Step [5690/64305], Loss: 5.0557\n",
      "Epoch [2/2], Step [5700/64305], Loss: 4.6007\n",
      "Epoch [2/2], Step [5710/64305], Loss: 4.6797\n",
      "Epoch [2/2], Step [5720/64305], Loss: 4.6625\n",
      "Epoch [2/2], Step [5730/64305], Loss: 4.9333\n",
      "Epoch [2/2], Step [5740/64305], Loss: 4.7203\n",
      "Epoch [2/2], Step [5750/64305], Loss: 4.9928\n",
      "Epoch [2/2], Step [5760/64305], Loss: 4.8416\n",
      "Epoch [2/2], Step [5770/64305], Loss: 4.8176\n",
      "Epoch [2/2], Step [5780/64305], Loss: 4.8290\n",
      "Epoch [2/2], Step [5790/64305], Loss: 4.8950\n",
      "Epoch [2/2], Step [5800/64305], Loss: 4.9591\n",
      "Epoch [2/2], Step [5810/64305], Loss: 4.7893\n",
      "Epoch [2/2], Step [5820/64305], Loss: 4.8714\n",
      "Epoch [2/2], Step [5830/64305], Loss: 4.9363\n",
      "Epoch [2/2], Step [5840/64305], Loss: 4.7839\n",
      "Epoch [2/2], Step [5850/64305], Loss: 4.7668\n",
      "Epoch [2/2], Step [5860/64305], Loss: 4.7222\n",
      "Epoch [2/2], Step [5870/64305], Loss: 4.8178\n",
      "Epoch [2/2], Step [5880/64305], Loss: 4.7096\n",
      "Epoch [2/2], Step [5890/64305], Loss: 4.8452\n",
      "Epoch [2/2], Step [5900/64305], Loss: 4.7775\n",
      "Epoch [2/2], Step [5910/64305], Loss: 4.7771\n",
      "Epoch [2/2], Step [5920/64305], Loss: 4.8752\n",
      "Epoch [2/2], Step [5930/64305], Loss: 4.7054\n",
      "Epoch [2/2], Step [5940/64305], Loss: 4.7851\n",
      "Epoch [2/2], Step [5950/64305], Loss: 4.8444\n",
      "Epoch [2/2], Step [5960/64305], Loss: 4.8240\n",
      "Epoch [2/2], Step [5970/64305], Loss: 4.6659\n",
      "Epoch [2/2], Step [5980/64305], Loss: 4.8007\n",
      "Epoch [2/2], Step [5990/64305], Loss: 4.9072\n",
      "Epoch [2/2], Step [6000/64305], Loss: 4.8976\n",
      "Epoch [2/2], Step [6010/64305], Loss: 4.7596\n",
      "Epoch [2/2], Step [6020/64305], Loss: 4.8356\n",
      "Epoch [2/2], Step [6030/64305], Loss: 4.6760\n",
      "Epoch [2/2], Step [6040/64305], Loss: 5.0447\n",
      "Epoch [2/2], Step [6050/64305], Loss: 4.9488\n",
      "Epoch [2/2], Step [6060/64305], Loss: 4.8221\n",
      "Epoch [2/2], Step [6070/64305], Loss: 4.8364\n",
      "Epoch [2/2], Step [6080/64305], Loss: 5.1228\n",
      "Epoch [2/2], Step [6090/64305], Loss: 4.5146\n",
      "Epoch [2/2], Step [6100/64305], Loss: 4.7729\n",
      "Epoch [2/2], Step [6110/64305], Loss: 5.0494\n",
      "Epoch [2/2], Step [6120/64305], Loss: 4.7888\n",
      "Epoch [2/2], Step [6130/64305], Loss: 5.0134\n",
      "Epoch [2/2], Step [6140/64305], Loss: 4.7413\n",
      "Epoch [2/2], Step [6150/64305], Loss: 4.7356\n",
      "Epoch [2/2], Step [6160/64305], Loss: 4.6860\n",
      "Epoch [2/2], Step [6170/64305], Loss: 4.6428\n",
      "Epoch [2/2], Step [6180/64305], Loss: 4.6881\n",
      "Epoch [2/2], Step [6190/64305], Loss: 4.6774\n",
      "Epoch [2/2], Step [6200/64305], Loss: 4.8561\n",
      "Epoch [2/2], Step [6210/64305], Loss: 4.9382\n",
      "Epoch [2/2], Step [6220/64305], Loss: 4.6389\n",
      "Epoch [2/2], Step [6230/64305], Loss: 4.7584\n",
      "Epoch [2/2], Step [6240/64305], Loss: 4.6496\n",
      "Epoch [2/2], Step [6250/64305], Loss: 4.7884\n",
      "Epoch [2/2], Step [6260/64305], Loss: 4.7057\n",
      "Epoch [2/2], Step [6270/64305], Loss: 4.7946\n",
      "Epoch [2/2], Step [6280/64305], Loss: 4.6351\n",
      "Epoch [2/2], Step [6290/64305], Loss: 4.8975\n",
      "Epoch [2/2], Step [6300/64305], Loss: 4.9951\n",
      "Epoch [2/2], Step [6310/64305], Loss: 4.6874\n",
      "Epoch [2/2], Step [6320/64305], Loss: 4.8250\n",
      "Epoch [2/2], Step [6330/64305], Loss: 4.7873\n",
      "Epoch [2/2], Step [6340/64305], Loss: 4.7987\n",
      "Epoch [2/2], Step [6350/64305], Loss: 4.9659\n",
      "Epoch [2/2], Step [6360/64305], Loss: 4.9038\n",
      "Epoch [2/2], Step [6370/64305], Loss: 4.6948\n",
      "Epoch [2/2], Step [6380/64305], Loss: 4.8114\n",
      "Epoch [2/2], Step [6390/64305], Loss: 4.9350\n",
      "Epoch [2/2], Step [6400/64305], Loss: 4.7407\n",
      "Epoch [2/2], Step [6410/64305], Loss: 4.9253\n",
      "Epoch [2/2], Step [6420/64305], Loss: 4.7957\n",
      "Epoch [2/2], Step [6430/64305], Loss: 5.0219\n",
      "Epoch [2/2], Step [6440/64305], Loss: 4.8814\n",
      "Epoch [2/2], Step [6450/64305], Loss: 5.0256\n",
      "Epoch [2/2], Step [6460/64305], Loss: 4.8374\n",
      "Epoch [2/2], Step [6470/64305], Loss: 4.8075\n",
      "Epoch [2/2], Step [6480/64305], Loss: 4.8186\n",
      "Epoch [2/2], Step [6490/64305], Loss: 4.8217\n",
      "Epoch [2/2], Step [6500/64305], Loss: 4.8397\n",
      "Epoch [2/2], Step [6510/64305], Loss: 4.7839\n",
      "Epoch [2/2], Step [6520/64305], Loss: 4.8234\n",
      "Epoch [2/2], Step [6530/64305], Loss: 4.6405\n",
      "Epoch [2/2], Step [6540/64305], Loss: 4.8021\n",
      "Epoch [2/2], Step [6550/64305], Loss: 4.8548\n",
      "Epoch [2/2], Step [6560/64305], Loss: 4.7448\n",
      "Epoch [2/2], Step [6570/64305], Loss: 4.9073\n",
      "Epoch [2/2], Step [6580/64305], Loss: 4.7141\n",
      "Epoch [2/2], Step [6590/64305], Loss: 4.7873\n",
      "Epoch [2/2], Step [6600/64305], Loss: 4.8050\n",
      "Epoch [2/2], Step [6610/64305], Loss: 4.7545\n",
      "Epoch [2/2], Step [6620/64305], Loss: 4.7102\n",
      "Epoch [2/2], Step [6630/64305], Loss: 4.9836\n",
      "Epoch [2/2], Step [6640/64305], Loss: 4.9107\n",
      "Epoch [2/2], Step [6650/64305], Loss: 4.9161\n",
      "Epoch [2/2], Step [6660/64305], Loss: 4.6838\n",
      "Epoch [2/2], Step [6670/64305], Loss: 4.7529\n",
      "Epoch [2/2], Step [6680/64305], Loss: 4.7067\n",
      "Epoch [2/2], Step [6690/64305], Loss: 4.7133\n",
      "Epoch [2/2], Step [6700/64305], Loss: 4.7022\n",
      "Epoch [2/2], Step [6710/64305], Loss: 4.7769\n",
      "Epoch [2/2], Step [6720/64305], Loss: 4.7338\n",
      "Epoch [2/2], Step [6730/64305], Loss: 4.9955\n",
      "Epoch [2/2], Step [6740/64305], Loss: 4.8339\n",
      "Epoch [2/2], Step [6750/64305], Loss: 4.8417\n",
      "Epoch [2/2], Step [6760/64305], Loss: 4.8124\n",
      "Epoch [2/2], Step [6770/64305], Loss: 4.7099\n",
      "Epoch [2/2], Step [6780/64305], Loss: 4.8519\n",
      "Epoch [2/2], Step [6790/64305], Loss: 4.6932\n",
      "Epoch [2/2], Step [6800/64305], Loss: 4.7818\n",
      "Epoch [2/2], Step [6810/64305], Loss: 4.7649\n",
      "Epoch [2/2], Step [6820/64305], Loss: 4.9421\n",
      "Epoch [2/2], Step [6830/64305], Loss: 4.6079\n",
      "Epoch [2/2], Step [6840/64305], Loss: 4.7761\n",
      "Epoch [2/2], Step [6850/64305], Loss: 5.0380\n",
      "Epoch [2/2], Step [6860/64305], Loss: 4.7391\n",
      "Epoch [2/2], Step [6870/64305], Loss: 4.8956\n",
      "Epoch [2/2], Step [6880/64305], Loss: 4.6961\n",
      "Epoch [2/2], Step [6890/64305], Loss: 4.9194\n",
      "Epoch [2/2], Step [6900/64305], Loss: 4.7192\n",
      "Epoch [2/2], Step [6910/64305], Loss: 4.9872\n",
      "Epoch [2/2], Step [6920/64305], Loss: 4.7397\n",
      "Epoch [2/2], Step [6930/64305], Loss: 4.5708\n",
      "Epoch [2/2], Step [6940/64305], Loss: 4.6869\n",
      "Epoch [2/2], Step [6950/64305], Loss: 4.5979\n",
      "Epoch [2/2], Step [6960/64305], Loss: 4.7604\n",
      "Epoch [2/2], Step [6970/64305], Loss: 4.9557\n",
      "Epoch [2/2], Step [6980/64305], Loss: 4.8034\n",
      "Epoch [2/2], Step [6990/64305], Loss: 4.8968\n",
      "Epoch [2/2], Step [7000/64305], Loss: 4.9929\n",
      "Epoch [2/2], Step [7010/64305], Loss: 4.7794\n",
      "Epoch [2/2], Step [7020/64305], Loss: 4.9404\n",
      "Epoch [2/2], Step [7030/64305], Loss: 4.5172\n",
      "Epoch [2/2], Step [7040/64305], Loss: 4.7379\n",
      "Epoch [2/2], Step [7050/64305], Loss: 4.8390\n",
      "Epoch [2/2], Step [7060/64305], Loss: 4.9243\n",
      "Epoch [2/2], Step [7070/64305], Loss: 4.8350\n",
      "Epoch [2/2], Step [7080/64305], Loss: 4.7094\n",
      "Epoch [2/2], Step [7090/64305], Loss: 4.8647\n",
      "Epoch [2/2], Step [7100/64305], Loss: 4.9043\n",
      "Epoch [2/2], Step [7110/64305], Loss: 4.8227\n",
      "Epoch [2/2], Step [7120/64305], Loss: 5.0201\n",
      "Epoch [2/2], Step [7130/64305], Loss: 4.7482\n",
      "Epoch [2/2], Step [7140/64305], Loss: 4.8042\n",
      "Epoch [2/2], Step [7150/64305], Loss: 4.8627\n",
      "Epoch [2/2], Step [7160/64305], Loss: 4.7314\n",
      "Epoch [2/2], Step [7170/64305], Loss: 4.7787\n",
      "Epoch [2/2], Step [7180/64305], Loss: 4.7155\n",
      "Epoch [2/2], Step [7190/64305], Loss: 4.8921\n",
      "Epoch [2/2], Step [7200/64305], Loss: 4.8999\n",
      "Epoch [2/2], Step [7210/64305], Loss: 4.8750\n",
      "Epoch [2/2], Step [7220/64305], Loss: 5.0685\n",
      "Epoch [2/2], Step [7230/64305], Loss: 4.7551\n",
      "Epoch [2/2], Step [7240/64305], Loss: 4.8801\n",
      "Epoch [2/2], Step [7250/64305], Loss: 4.7186\n",
      "Epoch [2/2], Step [7260/64305], Loss: 4.8240\n",
      "Epoch [2/2], Step [7270/64305], Loss: 4.9412\n",
      "Epoch [2/2], Step [7280/64305], Loss: 4.8283\n",
      "Epoch [2/2], Step [7290/64305], Loss: 4.8740\n",
      "Epoch [2/2], Step [7300/64305], Loss: 4.8329\n",
      "Epoch [2/2], Step [7310/64305], Loss: 4.8023\n",
      "Epoch [2/2], Step [7320/64305], Loss: 4.9226\n",
      "Epoch [2/2], Step [7330/64305], Loss: 4.9101\n",
      "Epoch [2/2], Step [7340/64305], Loss: 4.6462\n",
      "Epoch [2/2], Step [7350/64305], Loss: 4.8122\n",
      "Epoch [2/2], Step [7360/64305], Loss: 4.8877\n",
      "Epoch [2/2], Step [7370/64305], Loss: 4.7638\n",
      "Epoch [2/2], Step [7380/64305], Loss: 4.8603\n",
      "Epoch [2/2], Step [7390/64305], Loss: 4.8045\n",
      "Epoch [2/2], Step [7400/64305], Loss: 4.6567\n",
      "Epoch [2/2], Step [7410/64305], Loss: 4.8940\n",
      "Epoch [2/2], Step [7420/64305], Loss: 4.6722\n",
      "Epoch [2/2], Step [7430/64305], Loss: 4.9655\n",
      "Epoch [2/2], Step [7440/64305], Loss: 4.6652\n",
      "Epoch [2/2], Step [7450/64305], Loss: 4.7953\n",
      "Epoch [2/2], Step [7460/64305], Loss: 4.8145\n",
      "Epoch [2/2], Step [7470/64305], Loss: 4.8968\n",
      "Epoch [2/2], Step [7480/64305], Loss: 4.6018\n",
      "Epoch [2/2], Step [7490/64305], Loss: 4.7504\n",
      "Epoch [2/2], Step [7500/64305], Loss: 4.8712\n",
      "Epoch [2/2], Step [7510/64305], Loss: 4.7208\n",
      "Epoch [2/2], Step [7520/64305], Loss: 4.8704\n",
      "Epoch [2/2], Step [7530/64305], Loss: 4.8048\n",
      "Epoch [2/2], Step [7540/64305], Loss: 4.7816\n",
      "Epoch [2/2], Step [7550/64305], Loss: 4.6628\n",
      "Epoch [2/2], Step [7560/64305], Loss: 4.8043\n",
      "Epoch [2/2], Step [7570/64305], Loss: 4.8625\n",
      "Epoch [2/2], Step [7580/64305], Loss: 4.7630\n",
      "Epoch [2/2], Step [7590/64305], Loss: 4.5884\n",
      "Epoch [2/2], Step [7600/64305], Loss: 4.7590\n",
      "Epoch [2/2], Step [7610/64305], Loss: 4.7382\n",
      "Epoch [2/2], Step [7620/64305], Loss: 4.7738\n",
      "Epoch [2/2], Step [7630/64305], Loss: 4.7706\n",
      "Epoch [2/2], Step [7640/64305], Loss: 4.9193\n",
      "Epoch [2/2], Step [7650/64305], Loss: 4.8526\n",
      "Epoch [2/2], Step [7660/64305], Loss: 4.7863\n",
      "Epoch [2/2], Step [7670/64305], Loss: 4.7268\n",
      "Epoch [2/2], Step [7680/64305], Loss: 4.6890\n",
      "Epoch [2/2], Step [7690/64305], Loss: 4.7206\n",
      "Epoch [2/2], Step [7700/64305], Loss: 4.9081\n",
      "Epoch [2/2], Step [7710/64305], Loss: 4.8303\n",
      "Epoch [2/2], Step [7720/64305], Loss: 4.5942\n",
      "Epoch [2/2], Step [7730/64305], Loss: 4.5278\n",
      "Epoch [2/2], Step [7740/64305], Loss: 4.8443\n",
      "Epoch [2/2], Step [7750/64305], Loss: 4.8714\n",
      "Epoch [2/2], Step [7760/64305], Loss: 4.8625\n",
      "Epoch [2/2], Step [7770/64305], Loss: 4.7913\n",
      "Epoch [2/2], Step [7780/64305], Loss: 5.0294\n",
      "Epoch [2/2], Step [7790/64305], Loss: 4.7603\n",
      "Epoch [2/2], Step [7800/64305], Loss: 4.6824\n",
      "Epoch [2/2], Step [7810/64305], Loss: 4.8685\n",
      "Epoch [2/2], Step [7820/64305], Loss: 4.9819\n",
      "Epoch [2/2], Step [7830/64305], Loss: 4.7588\n",
      "Epoch [2/2], Step [7840/64305], Loss: 4.8139\n",
      "Epoch [2/2], Step [7850/64305], Loss: 4.7407\n",
      "Epoch [2/2], Step [7860/64305], Loss: 4.7900\n",
      "Epoch [2/2], Step [7870/64305], Loss: 4.6817\n",
      "Epoch [2/2], Step [7880/64305], Loss: 4.7665\n",
      "Epoch [2/2], Step [7890/64305], Loss: 4.7416\n",
      "Epoch [2/2], Step [7900/64305], Loss: 5.0143\n",
      "Epoch [2/2], Step [7910/64305], Loss: 5.0309\n",
      "Epoch [2/2], Step [7920/64305], Loss: 4.7384\n",
      "Epoch [2/2], Step [7930/64305], Loss: 4.9249\n",
      "Epoch [2/2], Step [7940/64305], Loss: 4.7855\n",
      "Epoch [2/2], Step [7950/64305], Loss: 4.7760\n",
      "Epoch [2/2], Step [7960/64305], Loss: 4.7548\n",
      "Epoch [2/2], Step [7970/64305], Loss: 5.1284\n",
      "Epoch [2/2], Step [7980/64305], Loss: 4.8327\n",
      "Epoch [2/2], Step [7990/64305], Loss: 4.8388\n",
      "Epoch [2/2], Step [8000/64305], Loss: 4.8434\n",
      "Epoch [2/2], Step [8010/64305], Loss: 4.8951\n",
      "Epoch [2/2], Step [8020/64305], Loss: 4.8232\n",
      "Epoch [2/2], Step [8030/64305], Loss: 4.6277\n",
      "Epoch [2/2], Step [8040/64305], Loss: 4.7998\n",
      "Epoch [2/2], Step [8050/64305], Loss: 4.7321\n",
      "Epoch [2/2], Step [8060/64305], Loss: 4.8070\n",
      "Epoch [2/2], Step [8070/64305], Loss: 4.8120\n",
      "Epoch [2/2], Step [8080/64305], Loss: 4.8144\n",
      "Epoch [2/2], Step [8090/64305], Loss: 4.9175\n",
      "Epoch [2/2], Step [8100/64305], Loss: 4.9340\n",
      "Epoch [2/2], Step [8110/64305], Loss: 4.8027\n",
      "Epoch [2/2], Step [8120/64305], Loss: 5.1286\n",
      "Epoch [2/2], Step [8130/64305], Loss: 4.8012\n",
      "Epoch [2/2], Step [8140/64305], Loss: 4.7816\n",
      "Epoch [2/2], Step [8150/64305], Loss: 4.7505\n",
      "Epoch [2/2], Step [8160/64305], Loss: 4.7283\n",
      "Epoch [2/2], Step [8170/64305], Loss: 4.8561\n",
      "Epoch [2/2], Step [8180/64305], Loss: 4.9410\n",
      "Epoch [2/2], Step [8190/64305], Loss: 4.8647\n",
      "Epoch [2/2], Step [8200/64305], Loss: 4.5853\n",
      "Epoch [2/2], Step [8210/64305], Loss: 4.7489\n",
      "Epoch [2/2], Step [8220/64305], Loss: 4.7121\n",
      "Epoch [2/2], Step [8230/64305], Loss: 4.8672\n",
      "Epoch [2/2], Step [8240/64305], Loss: 4.6221\n",
      "Epoch [2/2], Step [8250/64305], Loss: 4.7344\n",
      "Epoch [2/2], Step [8260/64305], Loss: 4.6416\n",
      "Epoch [2/2], Step [8270/64305], Loss: 4.8676\n",
      "Epoch [2/2], Step [8280/64305], Loss: 4.9179\n",
      "Epoch [2/2], Step [8290/64305], Loss: 4.9335\n",
      "Epoch [2/2], Step [8300/64305], Loss: 4.9147\n",
      "Epoch [2/2], Step [8310/64305], Loss: 4.7852\n",
      "Epoch [2/2], Step [8320/64305], Loss: 4.7954\n",
      "Epoch [2/2], Step [8330/64305], Loss: 4.8029\n",
      "Epoch [2/2], Step [8340/64305], Loss: 4.7296\n",
      "Epoch [2/2], Step [8350/64305], Loss: 5.0264\n",
      "Epoch [2/2], Step [8360/64305], Loss: 4.7384\n",
      "Epoch [2/2], Step [8370/64305], Loss: 4.8932\n",
      "Epoch [2/2], Step [8380/64305], Loss: 4.7805\n",
      "Epoch [2/2], Step [8390/64305], Loss: 4.9021\n",
      "Epoch [2/2], Step [8400/64305], Loss: 4.6354\n",
      "Epoch [2/2], Step [8410/64305], Loss: 5.0139\n",
      "Epoch [2/2], Step [8420/64305], Loss: 4.6383\n",
      "Epoch [2/2], Step [8430/64305], Loss: 4.8844\n",
      "Epoch [2/2], Step [8440/64305], Loss: 5.1106\n",
      "Epoch [2/2], Step [8450/64305], Loss: 4.8665\n",
      "Epoch [2/2], Step [8460/64305], Loss: 5.0088\n",
      "Epoch [2/2], Step [8470/64305], Loss: 4.9055\n",
      "Epoch [2/2], Step [8480/64305], Loss: 4.4940\n",
      "Epoch [2/2], Step [8490/64305], Loss: 4.9411\n",
      "Epoch [2/2], Step [8500/64305], Loss: 4.7021\n",
      "Epoch [2/2], Step [8510/64305], Loss: 4.7652\n",
      "Epoch [2/2], Step [8520/64305], Loss: 4.7036\n",
      "Epoch [2/2], Step [8530/64305], Loss: 4.7237\n",
      "Epoch [2/2], Step [8540/64305], Loss: 4.9158\n",
      "Epoch [2/2], Step [8550/64305], Loss: 5.0436\n",
      "Epoch [2/2], Step [8560/64305], Loss: 4.9182\n",
      "Epoch [2/2], Step [8570/64305], Loss: 4.9274\n",
      "Epoch [2/2], Step [8580/64305], Loss: 5.0183\n",
      "Epoch [2/2], Step [8590/64305], Loss: 4.9391\n",
      "Epoch [2/2], Step [8600/64305], Loss: 4.8643\n",
      "Epoch [2/2], Step [8610/64305], Loss: 4.7103\n",
      "Epoch [2/2], Step [8620/64305], Loss: 4.8674\n",
      "Epoch [2/2], Step [8630/64305], Loss: 4.6420\n",
      "Epoch [2/2], Step [8640/64305], Loss: 4.9140\n",
      "Epoch [2/2], Step [8650/64305], Loss: 4.7335\n",
      "Epoch [2/2], Step [8660/64305], Loss: 4.6230\n",
      "Epoch [2/2], Step [8670/64305], Loss: 4.8761\n",
      "Epoch [2/2], Step [8680/64305], Loss: 4.9853\n",
      "Epoch [2/2], Step [8690/64305], Loss: 4.5978\n",
      "Epoch [2/2], Step [8700/64305], Loss: 4.7776\n",
      "Epoch [2/2], Step [8710/64305], Loss: 4.8105\n",
      "Epoch [2/2], Step [8720/64305], Loss: 4.9126\n",
      "Epoch [2/2], Step [8730/64305], Loss: 4.8986\n",
      "Epoch [2/2], Step [8740/64305], Loss: 4.8096\n",
      "Epoch [2/2], Step [8750/64305], Loss: 4.8856\n",
      "Epoch [2/2], Step [8760/64305], Loss: 4.8820\n",
      "Epoch [2/2], Step [8770/64305], Loss: 4.8945\n",
      "Epoch [2/2], Step [8780/64305], Loss: 4.5949\n",
      "Epoch [2/2], Step [8790/64305], Loss: 4.8689\n",
      "Epoch [2/2], Step [8800/64305], Loss: 4.8172\n",
      "Epoch [2/2], Step [8810/64305], Loss: 4.7445\n",
      "Epoch [2/2], Step [8820/64305], Loss: 4.8441\n",
      "Epoch [2/2], Step [8830/64305], Loss: 4.8781\n",
      "Epoch [2/2], Step [8840/64305], Loss: 4.8369\n",
      "Epoch [2/2], Step [8850/64305], Loss: 4.5475\n",
      "Epoch [2/2], Step [8860/64305], Loss: 4.9771\n",
      "Epoch [2/2], Step [8870/64305], Loss: 4.8807\n",
      "Epoch [2/2], Step [8880/64305], Loss: 4.7815\n",
      "Epoch [2/2], Step [8890/64305], Loss: 4.9133\n",
      "Epoch [2/2], Step [8900/64305], Loss: 4.7442\n",
      "Epoch [2/2], Step [8910/64305], Loss: 4.7443\n",
      "Epoch [2/2], Step [8920/64305], Loss: 4.7913\n",
      "Epoch [2/2], Step [8930/64305], Loss: 4.6913\n",
      "Epoch [2/2], Step [8940/64305], Loss: 4.8359\n",
      "Epoch [2/2], Step [8950/64305], Loss: 4.7935\n",
      "Epoch [2/2], Step [8960/64305], Loss: 4.9271\n",
      "Epoch [2/2], Step [8970/64305], Loss: 4.6618\n",
      "Epoch [2/2], Step [8980/64305], Loss: 5.0117\n",
      "Epoch [2/2], Step [8990/64305], Loss: 4.6927\n",
      "Epoch [2/2], Step [9000/64305], Loss: 4.7251\n",
      "Epoch [2/2], Step [9010/64305], Loss: 4.6385\n",
      "Epoch [2/2], Step [9020/64305], Loss: 4.7599\n",
      "Epoch [2/2], Step [9030/64305], Loss: 4.7308\n",
      "Epoch [2/2], Step [9040/64305], Loss: 4.9438\n",
      "Epoch [2/2], Step [9050/64305], Loss: 4.9816\n",
      "Epoch [2/2], Step [9060/64305], Loss: 4.6467\n",
      "Epoch [2/2], Step [9070/64305], Loss: 4.6943\n",
      "Epoch [2/2], Step [9080/64305], Loss: 4.7388\n",
      "Epoch [2/2], Step [9090/64305], Loss: 4.8419\n",
      "Epoch [2/2], Step [9100/64305], Loss: 4.8351\n",
      "Epoch [2/2], Step [9110/64305], Loss: 4.8853\n",
      "Epoch [2/2], Step [9120/64305], Loss: 4.9130\n",
      "Epoch [2/2], Step [9130/64305], Loss: 4.8150\n",
      "Epoch [2/2], Step [9140/64305], Loss: 4.9210\n",
      "Epoch [2/2], Step [9150/64305], Loss: 4.5431\n",
      "Epoch [2/2], Step [9160/64305], Loss: 4.7583\n",
      "Epoch [2/2], Step [9170/64305], Loss: 4.7211\n",
      "Epoch [2/2], Step [9180/64305], Loss: 5.0790\n",
      "Epoch [2/2], Step [9190/64305], Loss: 5.0397\n",
      "Epoch [2/2], Step [9200/64305], Loss: 4.8044\n",
      "Epoch [2/2], Step [9210/64305], Loss: 4.7905\n",
      "Epoch [2/2], Step [9220/64305], Loss: 4.6849\n",
      "Epoch [2/2], Step [9230/64305], Loss: 4.7085\n",
      "Epoch [2/2], Step [9240/64305], Loss: 5.0407\n",
      "Epoch [2/2], Step [9250/64305], Loss: 4.7306\n",
      "Epoch [2/2], Step [9260/64305], Loss: 5.0283\n",
      "Epoch [2/2], Step [9270/64305], Loss: 4.8064\n",
      "Epoch [2/2], Step [9280/64305], Loss: 4.6582\n",
      "Epoch [2/2], Step [9290/64305], Loss: 4.7597\n",
      "Epoch [2/2], Step [9300/64305], Loss: 4.7359\n",
      "Epoch [2/2], Step [9310/64305], Loss: 4.8712\n",
      "Epoch [2/2], Step [9320/64305], Loss: 4.9469\n",
      "Epoch [2/2], Step [9330/64305], Loss: 4.7978\n",
      "Epoch [2/2], Step [9340/64305], Loss: 4.6821\n",
      "Epoch [2/2], Step [9350/64305], Loss: 5.0063\n",
      "Epoch [2/2], Step [9360/64305], Loss: 4.7731\n",
      "Epoch [2/2], Step [9370/64305], Loss: 4.7770\n",
      "Epoch [2/2], Step [9380/64305], Loss: 4.6905\n",
      "Epoch [2/2], Step [9390/64305], Loss: 4.8517\n",
      "Epoch [2/2], Step [9400/64305], Loss: 4.8656\n",
      "Epoch [2/2], Step [9410/64305], Loss: 4.7530\n",
      "Epoch [2/2], Step [9420/64305], Loss: 4.7646\n",
      "Epoch [2/2], Step [9430/64305], Loss: 5.1343\n",
      "Epoch [2/2], Step [9440/64305], Loss: 4.7486\n",
      "Epoch [2/2], Step [9450/64305], Loss: 4.6016\n",
      "Epoch [2/2], Step [9460/64305], Loss: 4.7983\n",
      "Epoch [2/2], Step [9470/64305], Loss: 4.9343\n",
      "Epoch [2/2], Step [9480/64305], Loss: 4.8386\n",
      "Epoch [2/2], Step [9490/64305], Loss: 4.6115\n",
      "Epoch [2/2], Step [9500/64305], Loss: 4.7890\n",
      "Epoch [2/2], Step [9510/64305], Loss: 4.7201\n",
      "Epoch [2/2], Step [9520/64305], Loss: 4.9987\n",
      "Epoch [2/2], Step [9530/64305], Loss: 4.7349\n",
      "Epoch [2/2], Step [9540/64305], Loss: 4.9685\n",
      "Epoch [2/2], Step [9550/64305], Loss: 4.6746\n",
      "Epoch [2/2], Step [9560/64305], Loss: 4.8802\n",
      "Epoch [2/2], Step [9570/64305], Loss: 4.9277\n",
      "Epoch [2/2], Step [9580/64305], Loss: 4.8284\n",
      "Epoch [2/2], Step [9590/64305], Loss: 4.7164\n",
      "Epoch [2/2], Step [9600/64305], Loss: 4.8449\n",
      "Epoch [2/2], Step [9610/64305], Loss: 4.7727\n",
      "Epoch [2/2], Step [9620/64305], Loss: 5.0892\n",
      "Epoch [2/2], Step [9630/64305], Loss: 4.9847\n",
      "Epoch [2/2], Step [9640/64305], Loss: 4.7845\n",
      "Epoch [2/2], Step [9650/64305], Loss: 4.8895\n",
      "Epoch [2/2], Step [9660/64305], Loss: 4.6208\n",
      "Epoch [2/2], Step [9670/64305], Loss: 4.7892\n",
      "Epoch [2/2], Step [9680/64305], Loss: 4.7750\n",
      "Epoch [2/2], Step [9690/64305], Loss: 4.7532\n",
      "Epoch [2/2], Step [9700/64305], Loss: 4.8956\n",
      "Epoch [2/2], Step [9710/64305], Loss: 4.6913\n",
      "Epoch [2/2], Step [9720/64305], Loss: 4.6883\n",
      "Epoch [2/2], Step [9730/64305], Loss: 4.9111\n",
      "Epoch [2/2], Step [9740/64305], Loss: 4.9247\n",
      "Epoch [2/2], Step [9750/64305], Loss: 4.7607\n",
      "Epoch [2/2], Step [9760/64305], Loss: 4.8696\n",
      "Epoch [2/2], Step [9770/64305], Loss: 4.8517\n",
      "Epoch [2/2], Step [9780/64305], Loss: 4.7808\n",
      "Epoch [2/2], Step [9790/64305], Loss: 4.7087\n",
      "Epoch [2/2], Step [9800/64305], Loss: 4.9090\n",
      "Epoch [2/2], Step [9810/64305], Loss: 4.9530\n",
      "Epoch [2/2], Step [9820/64305], Loss: 4.7225\n",
      "Epoch [2/2], Step [9830/64305], Loss: 4.8601\n",
      "Epoch [2/2], Step [9840/64305], Loss: 4.9088\n",
      "Epoch [2/2], Step [9850/64305], Loss: 4.6330\n",
      "Epoch [2/2], Step [9860/64305], Loss: 4.8546\n",
      "Epoch [2/2], Step [9870/64305], Loss: 4.7901\n",
      "Epoch [2/2], Step [9880/64305], Loss: 4.7634\n",
      "Epoch [2/2], Step [9890/64305], Loss: 4.7309\n",
      "Epoch [2/2], Step [9900/64305], Loss: 4.7015\n",
      "Epoch [2/2], Step [9910/64305], Loss: 4.9322\n",
      "Epoch [2/2], Step [9920/64305], Loss: 4.7073\n",
      "Epoch [2/2], Step [9930/64305], Loss: 4.7582\n",
      "Epoch [2/2], Step [9940/64305], Loss: 4.7784\n",
      "Epoch [2/2], Step [9950/64305], Loss: 4.8047\n",
      "Epoch [2/2], Step [9960/64305], Loss: 5.0332\n",
      "Epoch [2/2], Step [9970/64305], Loss: 4.6854\n",
      "Epoch [2/2], Step [9980/64305], Loss: 4.9747\n",
      "Epoch [2/2], Step [9990/64305], Loss: 4.7463\n",
      "Epoch [2/2], Step [10000/64305], Loss: 4.8733\n",
      "Epoch [2/2], Step [10010/64305], Loss: 4.6492\n",
      "Epoch [2/2], Step [10020/64305], Loss: 4.8805\n",
      "Epoch [2/2], Step [10030/64305], Loss: 4.9566\n",
      "Epoch [2/2], Step [10040/64305], Loss: 4.6607\n",
      "Epoch [2/2], Step [10050/64305], Loss: 4.7397\n",
      "Epoch [2/2], Step [10060/64305], Loss: 4.7067\n",
      "Epoch [2/2], Step [10070/64305], Loss: 4.8050\n",
      "Epoch [2/2], Step [10080/64305], Loss: 4.8126\n",
      "Epoch [2/2], Step [10090/64305], Loss: 4.7630\n",
      "Epoch [2/2], Step [10100/64305], Loss: 4.8485\n",
      "Epoch [2/2], Step [10110/64305], Loss: 4.6801\n",
      "Epoch [2/2], Step [10120/64305], Loss: 4.7131\n",
      "Epoch [2/2], Step [10130/64305], Loss: 4.9016\n",
      "Epoch [2/2], Step [10140/64305], Loss: 4.7729\n",
      "Epoch [2/2], Step [10150/64305], Loss: 4.7440\n",
      "Epoch [2/2], Step [10160/64305], Loss: 4.7410\n",
      "Epoch [2/2], Step [10170/64305], Loss: 4.6821\n",
      "Epoch [2/2], Step [10180/64305], Loss: 4.7525\n",
      "Epoch [2/2], Step [10190/64305], Loss: 4.6713\n",
      "Epoch [2/2], Step [10200/64305], Loss: 4.5591\n",
      "Epoch [2/2], Step [10210/64305], Loss: 4.7156\n",
      "Epoch [2/2], Step [10220/64305], Loss: 4.7463\n",
      "Epoch [2/2], Step [10230/64305], Loss: 4.7487\n",
      "Epoch [2/2], Step [10240/64305], Loss: 4.6943\n",
      "Epoch [2/2], Step [10250/64305], Loss: 5.0095\n",
      "Epoch [2/2], Step [10260/64305], Loss: 5.0284\n",
      "Epoch [2/2], Step [10270/64305], Loss: 4.7224\n",
      "Epoch [2/2], Step [10280/64305], Loss: 4.6189\n",
      "Epoch [2/2], Step [10290/64305], Loss: 4.8557\n",
      "Epoch [2/2], Step [10300/64305], Loss: 4.8626\n",
      "Epoch [2/2], Step [10310/64305], Loss: 4.9401\n",
      "Epoch [2/2], Step [10320/64305], Loss: 4.7015\n",
      "Epoch [2/2], Step [10330/64305], Loss: 4.8311\n",
      "Epoch [2/2], Step [10340/64305], Loss: 4.9646\n",
      "Epoch [2/2], Step [10350/64305], Loss: 4.6551\n",
      "Epoch [2/2], Step [10360/64305], Loss: 4.7927\n",
      "Epoch [2/2], Step [10370/64305], Loss: 4.8935\n",
      "Epoch [2/2], Step [10380/64305], Loss: 4.5585\n",
      "Epoch [2/2], Step [10390/64305], Loss: 4.7328\n",
      "Epoch [2/2], Step [10400/64305], Loss: 4.7270\n",
      "Epoch [2/2], Step [10410/64305], Loss: 4.7440\n",
      "Epoch [2/2], Step [10420/64305], Loss: 4.7341\n",
      "Epoch [2/2], Step [10430/64305], Loss: 4.8626\n",
      "Epoch [2/2], Step [10440/64305], Loss: 4.6452\n",
      "Epoch [2/2], Step [10450/64305], Loss: 4.7152\n",
      "Epoch [2/2], Step [10460/64305], Loss: 4.5358\n",
      "Epoch [2/2], Step [10470/64305], Loss: 4.8378\n",
      "Epoch [2/2], Step [10480/64305], Loss: 4.8900\n",
      "Epoch [2/2], Step [10490/64305], Loss: 4.7529\n",
      "Epoch [2/2], Step [10500/64305], Loss: 4.7925\n",
      "Epoch [2/2], Step [10510/64305], Loss: 4.9551\n",
      "Epoch [2/2], Step [10520/64305], Loss: 5.0761\n",
      "Epoch [2/2], Step [10530/64305], Loss: 4.6505\n",
      "Epoch [2/2], Step [10540/64305], Loss: 4.7325\n",
      "Epoch [2/2], Step [10550/64305], Loss: 4.7119\n",
      "Epoch [2/2], Step [10560/64305], Loss: 4.8605\n",
      "Epoch [2/2], Step [10570/64305], Loss: 5.0656\n",
      "Epoch [2/2], Step [10580/64305], Loss: 4.9758\n",
      "Epoch [2/2], Step [10590/64305], Loss: 4.8559\n",
      "Epoch [2/2], Step [10600/64305], Loss: 4.7845\n",
      "Epoch [2/2], Step [10610/64305], Loss: 4.8217\n",
      "Epoch [2/2], Step [10620/64305], Loss: 4.9175\n",
      "Epoch [2/2], Step [10630/64305], Loss: 4.7779\n",
      "Epoch [2/2], Step [10640/64305], Loss: 4.9095\n",
      "Epoch [2/2], Step [10650/64305], Loss: 4.7363\n",
      "Epoch [2/2], Step [10660/64305], Loss: 4.6375\n",
      "Epoch [2/2], Step [10670/64305], Loss: 4.7216\n",
      "Epoch [2/2], Step [10680/64305], Loss: 4.9898\n",
      "Epoch [2/2], Step [10690/64305], Loss: 4.6389\n",
      "Epoch [2/2], Step [10700/64305], Loss: 4.6486\n",
      "Epoch [2/2], Step [10710/64305], Loss: 4.8623\n",
      "Epoch [2/2], Step [10720/64305], Loss: 4.8147\n",
      "Epoch [2/2], Step [10730/64305], Loss: 4.7682\n",
      "Epoch [2/2], Step [10740/64305], Loss: 4.7413\n",
      "Epoch [2/2], Step [10750/64305], Loss: 4.9220\n",
      "Epoch [2/2], Step [10760/64305], Loss: 4.7291\n",
      "Epoch [2/2], Step [10770/64305], Loss: 4.9067\n",
      "Epoch [2/2], Step [10780/64305], Loss: 4.8785\n",
      "Epoch [2/2], Step [10790/64305], Loss: 4.8680\n",
      "Epoch [2/2], Step [10800/64305], Loss: 4.9675\n",
      "Epoch [2/2], Step [10810/64305], Loss: 4.9169\n",
      "Epoch [2/2], Step [10820/64305], Loss: 4.8932\n",
      "Epoch [2/2], Step [10830/64305], Loss: 4.9830\n",
      "Epoch [2/2], Step [10840/64305], Loss: 4.9548\n",
      "Epoch [2/2], Step [10850/64305], Loss: 4.7766\n",
      "Epoch [2/2], Step [10860/64305], Loss: 4.7578\n",
      "Epoch [2/2], Step [10870/64305], Loss: 4.6089\n",
      "Epoch [2/2], Step [10880/64305], Loss: 4.9617\n",
      "Epoch [2/2], Step [10890/64305], Loss: 4.6893\n",
      "Epoch [2/2], Step [10900/64305], Loss: 4.8787\n",
      "Epoch [2/2], Step [10910/64305], Loss: 4.7945\n",
      "Epoch [2/2], Step [10920/64305], Loss: 4.8499\n",
      "Epoch [2/2], Step [10930/64305], Loss: 4.8867\n",
      "Epoch [2/2], Step [10940/64305], Loss: 4.6675\n",
      "Epoch [2/2], Step [10950/64305], Loss: 4.9055\n",
      "Epoch [2/2], Step [10960/64305], Loss: 4.6775\n",
      "Epoch [2/2], Step [10970/64305], Loss: 4.7891\n",
      "Epoch [2/2], Step [10980/64305], Loss: 4.9004\n",
      "Epoch [2/2], Step [10990/64305], Loss: 4.8092\n",
      "Epoch [2/2], Step [11000/64305], Loss: 4.6061\n",
      "Epoch [2/2], Step [11010/64305], Loss: 4.8507\n",
      "Epoch [2/2], Step [11020/64305], Loss: 4.6617\n",
      "Epoch [2/2], Step [11030/64305], Loss: 4.8095\n",
      "Epoch [2/2], Step [11040/64305], Loss: 4.7435\n",
      "Epoch [2/2], Step [11050/64305], Loss: 4.8483\n",
      "Epoch [2/2], Step [11060/64305], Loss: 4.7519\n",
      "Epoch [2/2], Step [11070/64305], Loss: 4.9350\n",
      "Epoch [2/2], Step [11080/64305], Loss: 4.9076\n",
      "Epoch [2/2], Step [11090/64305], Loss: 4.6335\n",
      "Epoch [2/2], Step [11100/64305], Loss: 4.7637\n",
      "Epoch [2/2], Step [11110/64305], Loss: 4.6276\n",
      "Epoch [2/2], Step [11120/64305], Loss: 4.7580\n",
      "Epoch [2/2], Step [11130/64305], Loss: 4.9368\n",
      "Epoch [2/2], Step [11140/64305], Loss: 4.8841\n",
      "Epoch [2/2], Step [11150/64305], Loss: 4.7515\n",
      "Epoch [2/2], Step [11160/64305], Loss: 4.7529\n",
      "Epoch [2/2], Step [11170/64305], Loss: 4.9488\n",
      "Epoch [2/2], Step [11180/64305], Loss: 4.8502\n",
      "Epoch [2/2], Step [11190/64305], Loss: 4.8887\n",
      "Epoch [2/2], Step [11200/64305], Loss: 4.7077\n",
      "Epoch [2/2], Step [11210/64305], Loss: 4.7667\n",
      "Epoch [2/2], Step [11220/64305], Loss: 4.9736\n",
      "Epoch [2/2], Step [11230/64305], Loss: 4.7144\n",
      "Epoch [2/2], Step [11240/64305], Loss: 4.7022\n",
      "Epoch [2/2], Step [11250/64305], Loss: 4.9840\n",
      "Epoch [2/2], Step [11260/64305], Loss: 4.8049\n",
      "Epoch [2/2], Step [11270/64305], Loss: 4.5319\n",
      "Epoch [2/2], Step [11280/64305], Loss: 4.8148\n",
      "Epoch [2/2], Step [11290/64305], Loss: 4.7122\n",
      "Epoch [2/2], Step [11300/64305], Loss: 4.5854\n",
      "Epoch [2/2], Step [11310/64305], Loss: 4.7566\n",
      "Epoch [2/2], Step [11320/64305], Loss: 4.8693\n",
      "Epoch [2/2], Step [11330/64305], Loss: 4.7654\n",
      "Epoch [2/2], Step [11340/64305], Loss: 4.7807\n",
      "Epoch [2/2], Step [11350/64305], Loss: 4.8385\n",
      "Epoch [2/2], Step [11360/64305], Loss: 4.8757\n",
      "Epoch [2/2], Step [11370/64305], Loss: 4.7923\n",
      "Epoch [2/2], Step [11380/64305], Loss: 4.7391\n",
      "Epoch [2/2], Step [11390/64305], Loss: 4.7488\n",
      "Epoch [2/2], Step [11400/64305], Loss: 4.9140\n",
      "Epoch [2/2], Step [11410/64305], Loss: 4.8949\n",
      "Epoch [2/2], Step [11420/64305], Loss: 4.7573\n",
      "Epoch [2/2], Step [11430/64305], Loss: 4.8518\n",
      "Epoch [2/2], Step [11440/64305], Loss: 4.8720\n",
      "Epoch [2/2], Step [11450/64305], Loss: 4.9435\n",
      "Epoch [2/2], Step [11460/64305], Loss: 4.5599\n",
      "Epoch [2/2], Step [11470/64305], Loss: 4.7202\n",
      "Epoch [2/2], Step [11480/64305], Loss: 4.7507\n",
      "Epoch [2/2], Step [11490/64305], Loss: 4.8996\n",
      "Epoch [2/2], Step [11500/64305], Loss: 4.9075\n",
      "Epoch [2/2], Step [11510/64305], Loss: 5.0216\n",
      "Epoch [2/2], Step [11520/64305], Loss: 4.6785\n",
      "Epoch [2/2], Step [11530/64305], Loss: 4.9006\n",
      "Epoch [2/2], Step [11540/64305], Loss: 4.6166\n",
      "Epoch [2/2], Step [11550/64305], Loss: 4.8333\n",
      "Epoch [2/2], Step [11560/64305], Loss: 4.8261\n",
      "Epoch [2/2], Step [11570/64305], Loss: 4.7641\n",
      "Epoch [2/2], Step [11580/64305], Loss: 4.7667\n",
      "Epoch [2/2], Step [11590/64305], Loss: 4.9627\n",
      "Epoch [2/2], Step [11600/64305], Loss: 4.6369\n",
      "Epoch [2/2], Step [11610/64305], Loss: 4.8332\n",
      "Epoch [2/2], Step [11620/64305], Loss: 4.8245\n",
      "Epoch [2/2], Step [11630/64305], Loss: 4.8245\n",
      "Epoch [2/2], Step [11640/64305], Loss: 4.8087\n",
      "Epoch [2/2], Step [11650/64305], Loss: 4.7044\n",
      "Epoch [2/2], Step [11660/64305], Loss: 4.7647\n",
      "Epoch [2/2], Step [11670/64305], Loss: 4.8569\n",
      "Epoch [2/2], Step [11680/64305], Loss: 4.6801\n",
      "Epoch [2/2], Step [11690/64305], Loss: 4.8946\n",
      "Epoch [2/2], Step [11700/64305], Loss: 4.9294\n",
      "Epoch [2/2], Step [11710/64305], Loss: 4.7490\n",
      "Epoch [2/2], Step [11720/64305], Loss: 4.6644\n",
      "Epoch [2/2], Step [11730/64305], Loss: 4.8082\n",
      "Epoch [2/2], Step [11740/64305], Loss: 5.0290\n",
      "Epoch [2/2], Step [11750/64305], Loss: 4.8998\n",
      "Epoch [2/2], Step [11760/64305], Loss: 4.7309\n",
      "Epoch [2/2], Step [11770/64305], Loss: 4.8189\n",
      "Epoch [2/2], Step [11780/64305], Loss: 4.7952\n",
      "Epoch [2/2], Step [11790/64305], Loss: 4.7679\n",
      "Epoch [2/2], Step [11800/64305], Loss: 4.7105\n",
      "Epoch [2/2], Step [11810/64305], Loss: 4.9136\n",
      "Epoch [2/2], Step [11820/64305], Loss: 4.6972\n",
      "Epoch [2/2], Step [11830/64305], Loss: 4.4390\n",
      "Epoch [2/2], Step [11840/64305], Loss: 4.8198\n",
      "Epoch [2/2], Step [11850/64305], Loss: 4.9009\n",
      "Epoch [2/2], Step [11860/64305], Loss: 4.9535\n",
      "Epoch [2/2], Step [11870/64305], Loss: 4.7301\n",
      "Epoch [2/2], Step [11880/64305], Loss: 4.5573\n",
      "Epoch [2/2], Step [11890/64305], Loss: 4.7578\n",
      "Epoch [2/2], Step [11900/64305], Loss: 4.8946\n",
      "Epoch [2/2], Step [11910/64305], Loss: 4.8580\n",
      "Epoch [2/2], Step [11920/64305], Loss: 4.8774\n",
      "Epoch [2/2], Step [11930/64305], Loss: 4.8551\n",
      "Epoch [2/2], Step [11940/64305], Loss: 5.0889\n",
      "Epoch [2/2], Step [11950/64305], Loss: 4.7413\n",
      "Epoch [2/2], Step [11960/64305], Loss: 4.7276\n",
      "Epoch [2/2], Step [11970/64305], Loss: 4.7902\n",
      "Epoch [2/2], Step [11980/64305], Loss: 4.9415\n",
      "Epoch [2/2], Step [11990/64305], Loss: 4.7330\n",
      "Epoch [2/2], Step [12000/64305], Loss: 4.7262\n",
      "Epoch [2/2], Step [12010/64305], Loss: 4.9426\n",
      "Epoch [2/2], Step [12020/64305], Loss: 4.6017\n",
      "Epoch [2/2], Step [12030/64305], Loss: 4.6086\n",
      "Epoch [2/2], Step [12040/64305], Loss: 4.9646\n",
      "Epoch [2/2], Step [12050/64305], Loss: 4.5233\n",
      "Epoch [2/2], Step [12060/64305], Loss: 4.9167\n",
      "Epoch [2/2], Step [12070/64305], Loss: 4.9210\n",
      "Epoch [2/2], Step [12080/64305], Loss: 4.8370\n",
      "Epoch [2/2], Step [12090/64305], Loss: 4.9242\n",
      "Epoch [2/2], Step [12100/64305], Loss: 4.7105\n",
      "Epoch [2/2], Step [12110/64305], Loss: 4.6687\n",
      "Epoch [2/2], Step [12120/64305], Loss: 5.0262\n",
      "Epoch [2/2], Step [12130/64305], Loss: 4.8246\n",
      "Epoch [2/2], Step [12140/64305], Loss: 4.8599\n",
      "Epoch [2/2], Step [12150/64305], Loss: 4.8694\n",
      "Epoch [2/2], Step [12160/64305], Loss: 4.8469\n",
      "Epoch [2/2], Step [12170/64305], Loss: 4.7819\n",
      "Epoch [2/2], Step [12180/64305], Loss: 4.6450\n",
      "Epoch [2/2], Step [12190/64305], Loss: 4.9506\n",
      "Epoch [2/2], Step [12200/64305], Loss: 4.7309\n",
      "Epoch [2/2], Step [12210/64305], Loss: 4.9197\n",
      "Epoch [2/2], Step [12220/64305], Loss: 4.9867\n",
      "Epoch [2/2], Step [12230/64305], Loss: 4.8235\n",
      "Epoch [2/2], Step [12240/64305], Loss: 4.7312\n",
      "Epoch [2/2], Step [12250/64305], Loss: 4.9692\n",
      "Epoch [2/2], Step [12260/64305], Loss: 4.6913\n",
      "Epoch [2/2], Step [12270/64305], Loss: 4.6620\n",
      "Epoch [2/2], Step [12280/64305], Loss: 4.7586\n",
      "Epoch [2/2], Step [12290/64305], Loss: 4.8332\n",
      "Epoch [2/2], Step [12300/64305], Loss: 4.6688\n",
      "Epoch [2/2], Step [12310/64305], Loss: 4.8210\n",
      "Epoch [2/2], Step [12320/64305], Loss: 4.8605\n",
      "Epoch [2/2], Step [12330/64305], Loss: 4.8593\n",
      "Epoch [2/2], Step [12340/64305], Loss: 4.8034\n",
      "Epoch [2/2], Step [12350/64305], Loss: 4.7901\n",
      "Epoch [2/2], Step [12360/64305], Loss: 4.7520\n",
      "Epoch [2/2], Step [12370/64305], Loss: 4.7771\n",
      "Epoch [2/2], Step [12380/64305], Loss: 4.9726\n",
      "Epoch [2/2], Step [12390/64305], Loss: 4.6644\n",
      "Epoch [2/2], Step [12400/64305], Loss: 4.9643\n",
      "Epoch [2/2], Step [12410/64305], Loss: 4.9932\n",
      "Epoch [2/2], Step [12420/64305], Loss: 4.8864\n",
      "Epoch [2/2], Step [12430/64305], Loss: 4.8397\n",
      "Epoch [2/2], Step [12440/64305], Loss: 4.7973\n",
      "Epoch [2/2], Step [12450/64305], Loss: 4.9533\n",
      "Epoch [2/2], Step [12460/64305], Loss: 4.7801\n",
      "Epoch [2/2], Step [12470/64305], Loss: 4.8174\n",
      "Epoch [2/2], Step [12480/64305], Loss: 4.6766\n",
      "Epoch [2/2], Step [12490/64305], Loss: 4.9895\n",
      "Epoch [2/2], Step [12500/64305], Loss: 4.7997\n",
      "Epoch [2/2], Step [12510/64305], Loss: 4.7502\n",
      "Epoch [2/2], Step [12520/64305], Loss: 4.9387\n",
      "Epoch [2/2], Step [12530/64305], Loss: 4.7722\n",
      "Epoch [2/2], Step [12540/64305], Loss: 4.7714\n",
      "Epoch [2/2], Step [12550/64305], Loss: 4.6784\n",
      "Epoch [2/2], Step [12560/64305], Loss: 4.7714\n",
      "Epoch [2/2], Step [12570/64305], Loss: 4.6949\n",
      "Epoch [2/2], Step [12580/64305], Loss: 4.9433\n",
      "Epoch [2/2], Step [12590/64305], Loss: 4.6313\n",
      "Epoch [2/2], Step [12600/64305], Loss: 4.8101\n",
      "Epoch [2/2], Step [12610/64305], Loss: 4.7673\n",
      "Epoch [2/2], Step [12620/64305], Loss: 4.7130\n",
      "Epoch [2/2], Step [12630/64305], Loss: 4.7759\n",
      "Epoch [2/2], Step [12640/64305], Loss: 4.6931\n",
      "Epoch [2/2], Step [12650/64305], Loss: 4.8453\n",
      "Epoch [2/2], Step [12660/64305], Loss: 4.6229\n",
      "Epoch [2/2], Step [12670/64305], Loss: 4.5455\n",
      "Epoch [2/2], Step [12680/64305], Loss: 4.6246\n",
      "Epoch [2/2], Step [12690/64305], Loss: 4.8535\n",
      "Epoch [2/2], Step [12700/64305], Loss: 4.9720\n",
      "Epoch [2/2], Step [12710/64305], Loss: 4.7139\n",
      "Epoch [2/2], Step [12720/64305], Loss: 4.8376\n",
      "Epoch [2/2], Step [12730/64305], Loss: 4.6289\n",
      "Epoch [2/2], Step [12740/64305], Loss: 4.8623\n",
      "Epoch [2/2], Step [12750/64305], Loss: 4.9755\n",
      "Epoch [2/2], Step [12760/64305], Loss: 4.5735\n",
      "Epoch [2/2], Step [12770/64305], Loss: 4.8242\n",
      "Epoch [2/2], Step [12780/64305], Loss: 4.8286\n",
      "Epoch [2/2], Step [12790/64305], Loss: 4.6440\n",
      "Epoch [2/2], Step [12800/64305], Loss: 4.8584\n",
      "Epoch [2/2], Step [12810/64305], Loss: 4.8272\n",
      "Epoch [2/2], Step [12820/64305], Loss: 4.9191\n",
      "Epoch [2/2], Step [12830/64305], Loss: 4.6198\n",
      "Epoch [2/2], Step [12840/64305], Loss: 4.8895\n",
      "Epoch [2/2], Step [12850/64305], Loss: 4.6457\n",
      "Epoch [2/2], Step [12860/64305], Loss: 4.7162\n",
      "Epoch [2/2], Step [12870/64305], Loss: 4.8259\n",
      "Epoch [2/2], Step [12880/64305], Loss: 4.7979\n",
      "Epoch [2/2], Step [12890/64305], Loss: 4.5979\n",
      "Epoch [2/2], Step [12900/64305], Loss: 4.9437\n",
      "Epoch [2/2], Step [12910/64305], Loss: 5.0341\n",
      "Epoch [2/2], Step [12920/64305], Loss: 4.7147\n",
      "Epoch [2/2], Step [12930/64305], Loss: 4.9882\n",
      "Epoch [2/2], Step [12940/64305], Loss: 4.7157\n",
      "Epoch [2/2], Step [12950/64305], Loss: 4.7870\n",
      "Epoch [2/2], Step [12960/64305], Loss: 4.7162\n",
      "Epoch [2/2], Step [12970/64305], Loss: 4.9797\n",
      "Epoch [2/2], Step [12980/64305], Loss: 4.7066\n",
      "Epoch [2/2], Step [12990/64305], Loss: 4.7175\n",
      "Epoch [2/2], Step [13000/64305], Loss: 4.8034\n",
      "Epoch [2/2], Step [13010/64305], Loss: 4.8415\n",
      "Epoch [2/2], Step [13020/64305], Loss: 4.8309\n",
      "Epoch [2/2], Step [13030/64305], Loss: 4.8913\n",
      "Epoch [2/2], Step [13040/64305], Loss: 4.8891\n",
      "Epoch [2/2], Step [13050/64305], Loss: 4.7321\n",
      "Epoch [2/2], Step [13060/64305], Loss: 4.5874\n",
      "Epoch [2/2], Step [13070/64305], Loss: 4.7379\n",
      "Epoch [2/2], Step [13080/64305], Loss: 4.6713\n",
      "Epoch [2/2], Step [13090/64305], Loss: 4.7129\n",
      "Epoch [2/2], Step [13100/64305], Loss: 4.8417\n",
      "Epoch [2/2], Step [13110/64305], Loss: 4.8233\n",
      "Epoch [2/2], Step [13120/64305], Loss: 4.6647\n",
      "Epoch [2/2], Step [13130/64305], Loss: 4.9593\n",
      "Epoch [2/2], Step [13140/64305], Loss: 4.8866\n",
      "Epoch [2/2], Step [13150/64305], Loss: 5.0250\n",
      "Epoch [2/2], Step [13160/64305], Loss: 4.8559\n",
      "Epoch [2/2], Step [13170/64305], Loss: 5.0747\n",
      "Epoch [2/2], Step [13180/64305], Loss: 4.5611\n",
      "Epoch [2/2], Step [13190/64305], Loss: 4.8434\n",
      "Epoch [2/2], Step [13200/64305], Loss: 4.7167\n",
      "Epoch [2/2], Step [13210/64305], Loss: 4.8603\n",
      "Epoch [2/2], Step [13220/64305], Loss: 4.9057\n",
      "Epoch [2/2], Step [13230/64305], Loss: 4.7190\n",
      "Epoch [2/2], Step [13240/64305], Loss: 4.9640\n",
      "Epoch [2/2], Step [13250/64305], Loss: 4.8049\n",
      "Epoch [2/2], Step [13260/64305], Loss: 4.7663\n",
      "Epoch [2/2], Step [13270/64305], Loss: 4.9369\n",
      "Epoch [2/2], Step [13280/64305], Loss: 4.7655\n",
      "Epoch [2/2], Step [13290/64305], Loss: 4.8154\n",
      "Epoch [2/2], Step [13300/64305], Loss: 4.8546\n",
      "Epoch [2/2], Step [13310/64305], Loss: 4.7381\n",
      "Epoch [2/2], Step [13320/64305], Loss: 4.8992\n",
      "Epoch [2/2], Step [13330/64305], Loss: 5.0004\n",
      "Epoch [2/2], Step [13340/64305], Loss: 4.7237\n",
      "Epoch [2/2], Step [13350/64305], Loss: 4.7984\n",
      "Epoch [2/2], Step [13360/64305], Loss: 4.8097\n",
      "Epoch [2/2], Step [13370/64305], Loss: 4.7943\n",
      "Epoch [2/2], Step [13380/64305], Loss: 4.5077\n",
      "Epoch [2/2], Step [13390/64305], Loss: 4.7767\n",
      "Epoch [2/2], Step [13400/64305], Loss: 4.4402\n",
      "Epoch [2/2], Step [13410/64305], Loss: 4.9653\n",
      "Epoch [2/2], Step [13420/64305], Loss: 4.7659\n",
      "Epoch [2/2], Step [13430/64305], Loss: 4.8067\n",
      "Epoch [2/2], Step [13440/64305], Loss: 4.7610\n",
      "Epoch [2/2], Step [13450/64305], Loss: 4.5949\n",
      "Epoch [2/2], Step [13460/64305], Loss: 4.7741\n",
      "Epoch [2/2], Step [13470/64305], Loss: 4.8903\n",
      "Epoch [2/2], Step [13480/64305], Loss: 4.6331\n",
      "Epoch [2/2], Step [13490/64305], Loss: 4.5738\n",
      "Epoch [2/2], Step [13500/64305], Loss: 4.6986\n",
      "Epoch [2/2], Step [13510/64305], Loss: 4.8281\n",
      "Epoch [2/2], Step [13520/64305], Loss: 4.8974\n",
      "Epoch [2/2], Step [13530/64305], Loss: 4.8222\n",
      "Epoch [2/2], Step [13540/64305], Loss: 4.8714\n",
      "Epoch [2/2], Step [13550/64305], Loss: 4.6545\n",
      "Epoch [2/2], Step [13560/64305], Loss: 4.7734\n",
      "Epoch [2/2], Step [13570/64305], Loss: 4.9486\n",
      "Epoch [2/2], Step [13580/64305], Loss: 4.7675\n",
      "Epoch [2/2], Step [13590/64305], Loss: 4.9068\n",
      "Epoch [2/2], Step [13600/64305], Loss: 4.6871\n",
      "Epoch [2/2], Step [13610/64305], Loss: 4.6148\n",
      "Epoch [2/2], Step [13620/64305], Loss: 4.8435\n",
      "Epoch [2/2], Step [13630/64305], Loss: 4.7663\n",
      "Epoch [2/2], Step [13640/64305], Loss: 4.5405\n",
      "Epoch [2/2], Step [13650/64305], Loss: 4.7163\n",
      "Epoch [2/2], Step [13660/64305], Loss: 4.7352\n",
      "Epoch [2/2], Step [13670/64305], Loss: 4.8661\n",
      "Epoch [2/2], Step [13680/64305], Loss: 4.9226\n",
      "Epoch [2/2], Step [13690/64305], Loss: 4.6554\n",
      "Epoch [2/2], Step [13700/64305], Loss: 4.8784\n",
      "Epoch [2/2], Step [13710/64305], Loss: 4.7179\n",
      "Epoch [2/2], Step [13720/64305], Loss: 4.9228\n",
      "Epoch [2/2], Step [13730/64305], Loss: 4.7508\n",
      "Epoch [2/2], Step [13740/64305], Loss: 4.8891\n",
      "Epoch [2/2], Step [13750/64305], Loss: 4.8263\n",
      "Epoch [2/2], Step [13760/64305], Loss: 4.8996\n",
      "Epoch [2/2], Step [13770/64305], Loss: 4.7402\n",
      "Epoch [2/2], Step [13780/64305], Loss: 4.8237\n",
      "Epoch [2/2], Step [13790/64305], Loss: 4.9217\n",
      "Epoch [2/2], Step [13800/64305], Loss: 4.9386\n",
      "Epoch [2/2], Step [13810/64305], Loss: 4.7750\n",
      "Epoch [2/2], Step [13820/64305], Loss: 4.7194\n",
      "Epoch [2/2], Step [13830/64305], Loss: 4.7886\n",
      "Epoch [2/2], Step [13840/64305], Loss: 4.7742\n",
      "Epoch [2/2], Step [13850/64305], Loss: 4.7109\n",
      "Epoch [2/2], Step [13860/64305], Loss: 4.6613\n",
      "Epoch [2/2], Step [13870/64305], Loss: 4.4932\n",
      "Epoch [2/2], Step [13880/64305], Loss: 4.9817\n",
      "Epoch [2/2], Step [13890/64305], Loss: 4.8520\n",
      "Epoch [2/2], Step [13900/64305], Loss: 4.8288\n",
      "Epoch [2/2], Step [13910/64305], Loss: 4.8314\n",
      "Epoch [2/2], Step [13920/64305], Loss: 4.8276\n",
      "Epoch [2/2], Step [13930/64305], Loss: 4.6486\n",
      "Epoch [2/2], Step [13940/64305], Loss: 4.8030\n",
      "Epoch [2/2], Step [13950/64305], Loss: 4.8497\n",
      "Epoch [2/2], Step [13960/64305], Loss: 4.7687\n",
      "Epoch [2/2], Step [13970/64305], Loss: 5.0134\n",
      "Epoch [2/2], Step [13980/64305], Loss: 4.7708\n",
      "Epoch [2/2], Step [13990/64305], Loss: 4.5692\n",
      "Epoch [2/2], Step [14000/64305], Loss: 5.0662\n",
      "Epoch [2/2], Step [14010/64305], Loss: 4.9741\n",
      "Epoch [2/2], Step [14020/64305], Loss: 4.8966\n",
      "Epoch [2/2], Step [14030/64305], Loss: 4.8663\n",
      "Epoch [2/2], Step [14040/64305], Loss: 4.7998\n",
      "Epoch [2/2], Step [14050/64305], Loss: 4.7036\n",
      "Epoch [2/2], Step [14060/64305], Loss: 4.9001\n",
      "Epoch [2/2], Step [14070/64305], Loss: 4.5822\n",
      "Epoch [2/2], Step [14080/64305], Loss: 4.8794\n",
      "Epoch [2/2], Step [14090/64305], Loss: 4.8623\n",
      "Epoch [2/2], Step [14100/64305], Loss: 4.9243\n",
      "Epoch [2/2], Step [14110/64305], Loss: 4.9152\n",
      "Epoch [2/2], Step [14120/64305], Loss: 4.9706\n",
      "Epoch [2/2], Step [14130/64305], Loss: 4.9625\n",
      "Epoch [2/2], Step [14140/64305], Loss: 4.9728\n",
      "Epoch [2/2], Step [14150/64305], Loss: 4.7772\n",
      "Epoch [2/2], Step [14160/64305], Loss: 4.8620\n",
      "Epoch [2/2], Step [14170/64305], Loss: 4.7219\n",
      "Epoch [2/2], Step [14180/64305], Loss: 4.7935\n",
      "Epoch [2/2], Step [14190/64305], Loss: 4.8845\n",
      "Epoch [2/2], Step [14200/64305], Loss: 4.7659\n",
      "Epoch [2/2], Step [14210/64305], Loss: 4.8964\n",
      "Epoch [2/2], Step [14220/64305], Loss: 4.8762\n",
      "Epoch [2/2], Step [14230/64305], Loss: 4.7944\n",
      "Epoch [2/2], Step [14240/64305], Loss: 4.6935\n",
      "Epoch [2/2], Step [14250/64305], Loss: 4.7649\n",
      "Epoch [2/2], Step [14260/64305], Loss: 4.6393\n",
      "Epoch [2/2], Step [14270/64305], Loss: 4.7558\n",
      "Epoch [2/2], Step [14280/64305], Loss: 4.7450\n",
      "Epoch [2/2], Step [14290/64305], Loss: 4.8847\n",
      "Epoch [2/2], Step [14300/64305], Loss: 4.7725\n",
      "Epoch [2/2], Step [14310/64305], Loss: 4.7639\n",
      "Epoch [2/2], Step [14320/64305], Loss: 4.9537\n",
      "Epoch [2/2], Step [14330/64305], Loss: 4.6193\n",
      "Epoch [2/2], Step [14340/64305], Loss: 4.6143\n",
      "Epoch [2/2], Step [14350/64305], Loss: 4.6853\n",
      "Epoch [2/2], Step [14360/64305], Loss: 4.8380\n",
      "Epoch [2/2], Step [14370/64305], Loss: 4.7923\n",
      "Epoch [2/2], Step [14380/64305], Loss: 4.8838\n",
      "Epoch [2/2], Step [14390/64305], Loss: 4.9172\n",
      "Epoch [2/2], Step [14400/64305], Loss: 4.5275\n",
      "Epoch [2/2], Step [14410/64305], Loss: 4.7453\n",
      "Epoch [2/2], Step [14420/64305], Loss: 4.8054\n",
      "Epoch [2/2], Step [14430/64305], Loss: 4.7577\n",
      "Epoch [2/2], Step [14440/64305], Loss: 4.8221\n",
      "Epoch [2/2], Step [14450/64305], Loss: 4.5970\n",
      "Epoch [2/2], Step [14460/64305], Loss: 4.6795\n",
      "Epoch [2/2], Step [14470/64305], Loss: 4.8074\n",
      "Epoch [2/2], Step [14480/64305], Loss: 4.6355\n",
      "Epoch [2/2], Step [14490/64305], Loss: 5.0135\n",
      "Epoch [2/2], Step [14500/64305], Loss: 4.6378\n",
      "Epoch [2/2], Step [14510/64305], Loss: 4.8040\n",
      "Epoch [2/2], Step [14520/64305], Loss: 4.6284\n",
      "Epoch [2/2], Step [14530/64305], Loss: 4.8082\n",
      "Epoch [2/2], Step [14540/64305], Loss: 4.7079\n",
      "Epoch [2/2], Step [14550/64305], Loss: 4.7370\n",
      "Epoch [2/2], Step [14560/64305], Loss: 4.7649\n",
      "Epoch [2/2], Step [14570/64305], Loss: 4.6494\n",
      "Epoch [2/2], Step [14580/64305], Loss: 4.8803\n",
      "Epoch [2/2], Step [14590/64305], Loss: 4.9752\n",
      "Epoch [2/2], Step [14600/64305], Loss: 4.8026\n",
      "Epoch [2/2], Step [14610/64305], Loss: 5.0482\n",
      "Epoch [2/2], Step [14620/64305], Loss: 4.8953\n",
      "Epoch [2/2], Step [14630/64305], Loss: 4.7759\n",
      "Epoch [2/2], Step [14640/64305], Loss: 4.7578\n",
      "Epoch [2/2], Step [14650/64305], Loss: 4.7434\n",
      "Epoch [2/2], Step [14660/64305], Loss: 4.8089\n",
      "Epoch [2/2], Step [14670/64305], Loss: 4.8049\n",
      "Epoch [2/2], Step [14680/64305], Loss: 4.5450\n",
      "Epoch [2/2], Step [14690/64305], Loss: 4.7462\n",
      "Epoch [2/2], Step [14700/64305], Loss: 4.7614\n",
      "Epoch [2/2], Step [14710/64305], Loss: 4.7890\n",
      "Epoch [2/2], Step [14720/64305], Loss: 4.7591\n",
      "Epoch [2/2], Step [14730/64305], Loss: 4.5597\n",
      "Epoch [2/2], Step [14740/64305], Loss: 4.8077\n",
      "Epoch [2/2], Step [14750/64305], Loss: 5.0553\n",
      "Epoch [2/2], Step [14760/64305], Loss: 4.7944\n",
      "Epoch [2/2], Step [14770/64305], Loss: 4.7500\n",
      "Epoch [2/2], Step [14780/64305], Loss: 4.8925\n",
      "Epoch [2/2], Step [14790/64305], Loss: 4.6596\n",
      "Epoch [2/2], Step [14800/64305], Loss: 4.8033\n",
      "Epoch [2/2], Step [14810/64305], Loss: 4.9344\n",
      "Epoch [2/2], Step [14820/64305], Loss: 4.7088\n",
      "Epoch [2/2], Step [14830/64305], Loss: 4.9201\n",
      "Epoch [2/2], Step [14840/64305], Loss: 4.7389\n",
      "Epoch [2/2], Step [14850/64305], Loss: 4.7099\n",
      "Epoch [2/2], Step [14860/64305], Loss: 4.9428\n",
      "Epoch [2/2], Step [14870/64305], Loss: 4.9217\n",
      "Epoch [2/2], Step [14880/64305], Loss: 4.7491\n",
      "Epoch [2/2], Step [14890/64305], Loss: 4.8830\n",
      "Epoch [2/2], Step [14900/64305], Loss: 4.8171\n",
      "Epoch [2/2], Step [14910/64305], Loss: 4.7301\n",
      "Epoch [2/2], Step [14920/64305], Loss: 4.7469\n",
      "Epoch [2/2], Step [14930/64305], Loss: 4.7235\n",
      "Epoch [2/2], Step [14940/64305], Loss: 4.9684\n",
      "Epoch [2/2], Step [14950/64305], Loss: 4.8022\n",
      "Epoch [2/2], Step [14960/64305], Loss: 4.7252\n",
      "Epoch [2/2], Step [14970/64305], Loss: 5.0068\n",
      "Epoch [2/2], Step [14980/64305], Loss: 4.8269\n",
      "Epoch [2/2], Step [14990/64305], Loss: 4.8463\n",
      "Epoch [2/2], Step [15000/64305], Loss: 4.8237\n",
      "Epoch [2/2], Step [15010/64305], Loss: 4.8439\n",
      "Epoch [2/2], Step [15020/64305], Loss: 4.6518\n",
      "Epoch [2/2], Step [15030/64305], Loss: 4.8318\n",
      "Epoch [2/2], Step [15040/64305], Loss: 4.7190\n",
      "Epoch [2/2], Step [15050/64305], Loss: 4.9684\n",
      "Epoch [2/2], Step [15060/64305], Loss: 4.6439\n",
      "Epoch [2/2], Step [15070/64305], Loss: 4.6189\n",
      "Epoch [2/2], Step [15080/64305], Loss: 4.7955\n",
      "Epoch [2/2], Step [15090/64305], Loss: 4.8223\n",
      "Epoch [2/2], Step [15100/64305], Loss: 4.9219\n",
      "Epoch [2/2], Step [15110/64305], Loss: 4.9631\n",
      "Epoch [2/2], Step [15120/64305], Loss: 4.6693\n",
      "Epoch [2/2], Step [15130/64305], Loss: 4.8425\n",
      "Epoch [2/2], Step [15140/64305], Loss: 4.7018\n",
      "Epoch [2/2], Step [15150/64305], Loss: 4.6887\n",
      "Epoch [2/2], Step [15160/64305], Loss: 4.8187\n",
      "Epoch [2/2], Step [15170/64305], Loss: 4.8495\n",
      "Epoch [2/2], Step [15180/64305], Loss: 4.8495\n",
      "Epoch [2/2], Step [15190/64305], Loss: 4.8385\n",
      "Epoch [2/2], Step [15200/64305], Loss: 4.6293\n",
      "Epoch [2/2], Step [15210/64305], Loss: 4.7618\n",
      "Epoch [2/2], Step [15220/64305], Loss: 4.7221\n",
      "Epoch [2/2], Step [15230/64305], Loss: 4.7528\n",
      "Epoch [2/2], Step [15240/64305], Loss: 4.6075\n",
      "Epoch [2/2], Step [15250/64305], Loss: 4.7062\n",
      "Epoch [2/2], Step [15260/64305], Loss: 4.9077\n",
      "Epoch [2/2], Step [15270/64305], Loss: 4.7119\n",
      "Epoch [2/2], Step [15280/64305], Loss: 4.8517\n",
      "Epoch [2/2], Step [15290/64305], Loss: 4.8406\n",
      "Epoch [2/2], Step [15300/64305], Loss: 4.5520\n",
      "Epoch [2/2], Step [15310/64305], Loss: 4.8746\n",
      "Epoch [2/2], Step [15320/64305], Loss: 4.7794\n",
      "Epoch [2/2], Step [15330/64305], Loss: 4.8921\n",
      "Epoch [2/2], Step [15340/64305], Loss: 4.8657\n",
      "Epoch [2/2], Step [15350/64305], Loss: 4.7430\n",
      "Epoch [2/2], Step [15360/64305], Loss: 4.7603\n",
      "Epoch [2/2], Step [15370/64305], Loss: 4.9079\n",
      "Epoch [2/2], Step [15380/64305], Loss: 4.6114\n",
      "Epoch [2/2], Step [15390/64305], Loss: 4.7176\n",
      "Epoch [2/2], Step [15400/64305], Loss: 4.8342\n",
      "Epoch [2/2], Step [15410/64305], Loss: 4.7312\n",
      "Epoch [2/2], Step [15420/64305], Loss: 4.7217\n",
      "Epoch [2/2], Step [15430/64305], Loss: 4.8536\n",
      "Epoch [2/2], Step [15440/64305], Loss: 4.7822\n",
      "Epoch [2/2], Step [15450/64305], Loss: 4.6522\n",
      "Epoch [2/2], Step [15460/64305], Loss: 4.7384\n",
      "Epoch [2/2], Step [15470/64305], Loss: 4.5862\n",
      "Epoch [2/2], Step [15480/64305], Loss: 4.7287\n",
      "Epoch [2/2], Step [15490/64305], Loss: 4.6605\n",
      "Epoch [2/2], Step [15500/64305], Loss: 4.9293\n",
      "Epoch [2/2], Step [15510/64305], Loss: 4.6376\n",
      "Epoch [2/2], Step [15520/64305], Loss: 4.7440\n",
      "Epoch [2/2], Step [15530/64305], Loss: 4.7391\n",
      "Epoch [2/2], Step [15540/64305], Loss: 4.7503\n",
      "Epoch [2/2], Step [15550/64305], Loss: 4.9790\n",
      "Epoch [2/2], Step [15560/64305], Loss: 4.6893\n",
      "Epoch [2/2], Step [15570/64305], Loss: 4.8822\n",
      "Epoch [2/2], Step [15580/64305], Loss: 4.7289\n",
      "Epoch [2/2], Step [15590/64305], Loss: 4.8925\n",
      "Epoch [2/2], Step [15600/64305], Loss: 4.7528\n",
      "Epoch [2/2], Step [15610/64305], Loss: 4.7224\n",
      "Epoch [2/2], Step [15620/64305], Loss: 4.6396\n",
      "Epoch [2/2], Step [15630/64305], Loss: 4.6584\n",
      "Epoch [2/2], Step [15640/64305], Loss: 4.8782\n",
      "Epoch [2/2], Step [15650/64305], Loss: 4.8026\n",
      "Epoch [2/2], Step [15660/64305], Loss: 4.9874\n",
      "Epoch [2/2], Step [15670/64305], Loss: 4.6232\n",
      "Epoch [2/2], Step [15680/64305], Loss: 4.5126\n",
      "Epoch [2/2], Step [15690/64305], Loss: 4.9976\n",
      "Epoch [2/2], Step [15700/64305], Loss: 4.6752\n",
      "Epoch [2/2], Step [15710/64305], Loss: 4.8526\n",
      "Epoch [2/2], Step [15720/64305], Loss: 4.7735\n",
      "Epoch [2/2], Step [15730/64305], Loss: 4.7901\n",
      "Epoch [2/2], Step [15740/64305], Loss: 4.7952\n",
      "Epoch [2/2], Step [15750/64305], Loss: 4.8816\n",
      "Epoch [2/2], Step [15760/64305], Loss: 4.8894\n",
      "Epoch [2/2], Step [15770/64305], Loss: 4.8074\n",
      "Epoch [2/2], Step [15780/64305], Loss: 4.8440\n",
      "Epoch [2/2], Step [15790/64305], Loss: 4.7976\n",
      "Epoch [2/2], Step [15800/64305], Loss: 4.8963\n",
      "Epoch [2/2], Step [15810/64305], Loss: 4.8108\n",
      "Epoch [2/2], Step [15820/64305], Loss: 4.8204\n",
      "Epoch [2/2], Step [15830/64305], Loss: 4.7003\n",
      "Epoch [2/2], Step [15840/64305], Loss: 4.8570\n",
      "Epoch [2/2], Step [15850/64305], Loss: 4.7797\n",
      "Epoch [2/2], Step [15860/64305], Loss: 4.7345\n",
      "Epoch [2/2], Step [15870/64305], Loss: 4.5235\n",
      "Epoch [2/2], Step [15880/64305], Loss: 4.7872\n",
      "Epoch [2/2], Step [15890/64305], Loss: 4.7768\n",
      "Epoch [2/2], Step [15900/64305], Loss: 4.8142\n",
      "Epoch [2/2], Step [15910/64305], Loss: 4.4606\n",
      "Epoch [2/2], Step [15920/64305], Loss: 4.9212\n",
      "Epoch [2/2], Step [15930/64305], Loss: 4.7548\n",
      "Epoch [2/2], Step [15940/64305], Loss: 4.7360\n",
      "Epoch [2/2], Step [15950/64305], Loss: 5.0370\n",
      "Epoch [2/2], Step [15960/64305], Loss: 4.7034\n",
      "Epoch [2/2], Step [15970/64305], Loss: 4.7678\n",
      "Epoch [2/2], Step [15980/64305], Loss: 4.8609\n",
      "Epoch [2/2], Step [15990/64305], Loss: 4.8326\n",
      "Epoch [2/2], Step [16000/64305], Loss: 4.8620\n",
      "Epoch [2/2], Step [16010/64305], Loss: 4.8247\n",
      "Epoch [2/2], Step [16020/64305], Loss: 4.6813\n",
      "Epoch [2/2], Step [16030/64305], Loss: 4.9585\n",
      "Epoch [2/2], Step [16040/64305], Loss: 4.7928\n",
      "Epoch [2/2], Step [16050/64305], Loss: 4.8099\n",
      "Epoch [2/2], Step [16060/64305], Loss: 4.7523\n",
      "Epoch [2/2], Step [16070/64305], Loss: 4.6904\n",
      "Epoch [2/2], Step [16080/64305], Loss: 4.7185\n",
      "Epoch [2/2], Step [16090/64305], Loss: 4.7617\n",
      "Epoch [2/2], Step [16100/64305], Loss: 4.6915\n",
      "Epoch [2/2], Step [16110/64305], Loss: 4.6515\n",
      "Epoch [2/2], Step [16120/64305], Loss: 4.6454\n",
      "Epoch [2/2], Step [16130/64305], Loss: 4.8108\n",
      "Epoch [2/2], Step [16140/64305], Loss: 4.8131\n",
      "Epoch [2/2], Step [16150/64305], Loss: 4.6340\n",
      "Epoch [2/2], Step [16160/64305], Loss: 4.7676\n",
      "Epoch [2/2], Step [16170/64305], Loss: 4.8454\n",
      "Epoch [2/2], Step [16180/64305], Loss: 4.9647\n",
      "Epoch [2/2], Step [16190/64305], Loss: 4.8571\n",
      "Epoch [2/2], Step [16200/64305], Loss: 4.8125\n",
      "Epoch [2/2], Step [16210/64305], Loss: 4.9590\n",
      "Epoch [2/2], Step [16220/64305], Loss: 4.6432\n",
      "Epoch [2/2], Step [16230/64305], Loss: 4.7134\n",
      "Epoch [2/2], Step [16240/64305], Loss: 4.9970\n",
      "Epoch [2/2], Step [16250/64305], Loss: 4.7183\n",
      "Epoch [2/2], Step [16260/64305], Loss: 4.8694\n",
      "Epoch [2/2], Step [16270/64305], Loss: 4.9440\n",
      "Epoch [2/2], Step [16280/64305], Loss: 4.6632\n",
      "Epoch [2/2], Step [16290/64305], Loss: 4.6756\n",
      "Epoch [2/2], Step [16300/64305], Loss: 4.8699\n",
      "Epoch [2/2], Step [16310/64305], Loss: 4.6830\n",
      "Epoch [2/2], Step [16320/64305], Loss: 4.7701\n",
      "Epoch [2/2], Step [16330/64305], Loss: 4.7820\n",
      "Epoch [2/2], Step [16340/64305], Loss: 4.8160\n",
      "Epoch [2/2], Step [16350/64305], Loss: 4.8049\n",
      "Epoch [2/2], Step [16360/64305], Loss: 4.6917\n",
      "Epoch [2/2], Step [16370/64305], Loss: 4.7796\n",
      "Epoch [2/2], Step [16380/64305], Loss: 4.7690\n",
      "Epoch [2/2], Step [16390/64305], Loss: 4.7829\n",
      "Epoch [2/2], Step [16400/64305], Loss: 4.7731\n",
      "Epoch [2/2], Step [16410/64305], Loss: 4.7552\n",
      "Epoch [2/2], Step [16420/64305], Loss: 4.8248\n",
      "Epoch [2/2], Step [16430/64305], Loss: 4.7721\n",
      "Epoch [2/2], Step [16440/64305], Loss: 4.8371\n",
      "Epoch [2/2], Step [16450/64305], Loss: 4.8060\n",
      "Epoch [2/2], Step [16460/64305], Loss: 4.6811\n",
      "Epoch [2/2], Step [16470/64305], Loss: 4.8614\n",
      "Epoch [2/2], Step [16480/64305], Loss: 5.0600\n",
      "Epoch [2/2], Step [16490/64305], Loss: 4.6728\n",
      "Epoch [2/2], Step [16500/64305], Loss: 4.6482\n",
      "Epoch [2/2], Step [16510/64305], Loss: 4.7935\n",
      "Epoch [2/2], Step [16520/64305], Loss: 4.9657\n",
      "Epoch [2/2], Step [16530/64305], Loss: 4.6935\n",
      "Epoch [2/2], Step [16540/64305], Loss: 4.7708\n",
      "Epoch [2/2], Step [16550/64305], Loss: 4.6347\n",
      "Epoch [2/2], Step [16560/64305], Loss: 4.8427\n",
      "Epoch [2/2], Step [16570/64305], Loss: 4.7424\n",
      "Epoch [2/2], Step [16580/64305], Loss: 4.6698\n",
      "Epoch [2/2], Step [16590/64305], Loss: 4.6703\n",
      "Epoch [2/2], Step [16600/64305], Loss: 4.6978\n",
      "Epoch [2/2], Step [16610/64305], Loss: 4.8820\n",
      "Epoch [2/2], Step [16620/64305], Loss: 4.6811\n",
      "Epoch [2/2], Step [16630/64305], Loss: 4.7873\n",
      "Epoch [2/2], Step [16640/64305], Loss: 4.8452\n",
      "Epoch [2/2], Step [16650/64305], Loss: 4.8036\n",
      "Epoch [2/2], Step [16660/64305], Loss: 4.8051\n",
      "Epoch [2/2], Step [16670/64305], Loss: 4.8362\n",
      "Epoch [2/2], Step [16680/64305], Loss: 4.8839\n",
      "Epoch [2/2], Step [16690/64305], Loss: 4.6496\n",
      "Epoch [2/2], Step [16700/64305], Loss: 4.7954\n",
      "Epoch [2/2], Step [16710/64305], Loss: 4.6830\n",
      "Epoch [2/2], Step [16720/64305], Loss: 4.7717\n",
      "Epoch [2/2], Step [16730/64305], Loss: 4.8070\n",
      "Epoch [2/2], Step [16740/64305], Loss: 4.7368\n",
      "Epoch [2/2], Step [16750/64305], Loss: 4.8835\n",
      "Epoch [2/2], Step [16760/64305], Loss: 4.8703\n",
      "Epoch [2/2], Step [16770/64305], Loss: 4.6229\n",
      "Epoch [2/2], Step [16780/64305], Loss: 4.7802\n",
      "Epoch [2/2], Step [16790/64305], Loss: 4.8767\n",
      "Epoch [2/2], Step [16800/64305], Loss: 4.9860\n",
      "Epoch [2/2], Step [16810/64305], Loss: 4.9229\n",
      "Epoch [2/2], Step [16820/64305], Loss: 4.8932\n",
      "Epoch [2/2], Step [16830/64305], Loss: 4.9314\n",
      "Epoch [2/2], Step [16840/64305], Loss: 4.9502\n",
      "Epoch [2/2], Step [16850/64305], Loss: 4.5984\n",
      "Epoch [2/2], Step [16860/64305], Loss: 4.6582\n",
      "Epoch [2/2], Step [16870/64305], Loss: 4.7667\n",
      "Epoch [2/2], Step [16880/64305], Loss: 4.8745\n",
      "Epoch [2/2], Step [16890/64305], Loss: 5.0306\n",
      "Epoch [2/2], Step [16900/64305], Loss: 4.7351\n",
      "Epoch [2/2], Step [16910/64305], Loss: 4.8615\n",
      "Epoch [2/2], Step [16920/64305], Loss: 4.6835\n",
      "Epoch [2/2], Step [16930/64305], Loss: 4.9993\n",
      "Epoch [2/2], Step [16940/64305], Loss: 4.6984\n",
      "Epoch [2/2], Step [16950/64305], Loss: 4.8394\n",
      "Epoch [2/2], Step [16960/64305], Loss: 4.6447\n",
      "Epoch [2/2], Step [16970/64305], Loss: 4.8517\n",
      "Epoch [2/2], Step [16980/64305], Loss: 4.7174\n",
      "Epoch [2/2], Step [16990/64305], Loss: 4.8470\n",
      "Epoch [2/2], Step [17000/64305], Loss: 4.8501\n",
      "Epoch [2/2], Step [17010/64305], Loss: 4.8350\n",
      "Epoch [2/2], Step [17020/64305], Loss: 5.0201\n",
      "Epoch [2/2], Step [17030/64305], Loss: 4.7589\n",
      "Epoch [2/2], Step [17040/64305], Loss: 4.6805\n",
      "Epoch [2/2], Step [17050/64305], Loss: 4.6484\n",
      "Epoch [2/2], Step [17060/64305], Loss: 4.8988\n",
      "Epoch [2/2], Step [17070/64305], Loss: 4.7134\n",
      "Epoch [2/2], Step [17080/64305], Loss: 4.7457\n",
      "Epoch [2/2], Step [17090/64305], Loss: 4.7708\n",
      "Epoch [2/2], Step [17100/64305], Loss: 4.7186\n",
      "Epoch [2/2], Step [17110/64305], Loss: 4.5345\n",
      "Epoch [2/2], Step [17120/64305], Loss: 4.8181\n",
      "Epoch [2/2], Step [17130/64305], Loss: 4.6477\n",
      "Epoch [2/2], Step [17140/64305], Loss: 4.7712\n",
      "Epoch [2/2], Step [17150/64305], Loss: 4.9201\n",
      "Epoch [2/2], Step [17160/64305], Loss: 4.7857\n",
      "Epoch [2/2], Step [17170/64305], Loss: 5.1390\n",
      "Epoch [2/2], Step [17180/64305], Loss: 4.7739\n",
      "Epoch [2/2], Step [17190/64305], Loss: 4.8509\n",
      "Epoch [2/2], Step [17200/64305], Loss: 4.7993\n",
      "Epoch [2/2], Step [17210/64305], Loss: 4.9192\n",
      "Epoch [2/2], Step [17220/64305], Loss: 4.6214\n",
      "Epoch [2/2], Step [17230/64305], Loss: 4.9058\n",
      "Epoch [2/2], Step [17240/64305], Loss: 4.9327\n",
      "Epoch [2/2], Step [17250/64305], Loss: 4.9970\n",
      "Epoch [2/2], Step [17260/64305], Loss: 4.7798\n",
      "Epoch [2/2], Step [17270/64305], Loss: 4.5426\n",
      "Epoch [2/2], Step [17280/64305], Loss: 4.7425\n",
      "Epoch [2/2], Step [17290/64305], Loss: 4.5826\n",
      "Epoch [2/2], Step [17300/64305], Loss: 4.7562\n",
      "Epoch [2/2], Step [17310/64305], Loss: 4.6698\n",
      "Epoch [2/2], Step [17320/64305], Loss: 4.7127\n",
      "Epoch [2/2], Step [17330/64305], Loss: 4.8773\n",
      "Epoch [2/2], Step [17340/64305], Loss: 4.7898\n",
      "Epoch [2/2], Step [17350/64305], Loss: 4.8140\n",
      "Epoch [2/2], Step [17360/64305], Loss: 4.8275\n",
      "Epoch [2/2], Step [17370/64305], Loss: 4.7037\n",
      "Epoch [2/2], Step [17380/64305], Loss: 4.9440\n",
      "Epoch [2/2], Step [17390/64305], Loss: 4.7401\n",
      "Epoch [2/2], Step [17400/64305], Loss: 4.6362\n",
      "Epoch [2/2], Step [17410/64305], Loss: 4.8490\n",
      "Epoch [2/2], Step [17420/64305], Loss: 4.8562\n",
      "Epoch [2/2], Step [17430/64305], Loss: 4.6495\n",
      "Epoch [2/2], Step [17440/64305], Loss: 4.7966\n",
      "Epoch [2/2], Step [17450/64305], Loss: 4.7144\n",
      "Epoch [2/2], Step [17460/64305], Loss: 4.8291\n",
      "Epoch [2/2], Step [17470/64305], Loss: 4.7448\n",
      "Epoch [2/2], Step [17480/64305], Loss: 4.9441\n",
      "Epoch [2/2], Step [17490/64305], Loss: 4.7398\n",
      "Epoch [2/2], Step [17500/64305], Loss: 5.0463\n",
      "Epoch [2/2], Step [17510/64305], Loss: 4.8355\n",
      "Epoch [2/2], Step [17520/64305], Loss: 4.7922\n",
      "Epoch [2/2], Step [17530/64305], Loss: 4.8324\n",
      "Epoch [2/2], Step [17540/64305], Loss: 4.6909\n",
      "Epoch [2/2], Step [17550/64305], Loss: 4.9535\n",
      "Epoch [2/2], Step [17560/64305], Loss: 4.7438\n",
      "Epoch [2/2], Step [17570/64305], Loss: 4.7087\n",
      "Epoch [2/2], Step [17580/64305], Loss: 4.6659\n",
      "Epoch [2/2], Step [17590/64305], Loss: 5.0419\n",
      "Epoch [2/2], Step [17600/64305], Loss: 4.5698\n",
      "Epoch [2/2], Step [17610/64305], Loss: 4.8379\n",
      "Epoch [2/2], Step [17620/64305], Loss: 4.7669\n",
      "Epoch [2/2], Step [17630/64305], Loss: 4.7006\n",
      "Epoch [2/2], Step [17640/64305], Loss: 4.7897\n",
      "Epoch [2/2], Step [17650/64305], Loss: 4.6896\n",
      "Epoch [2/2], Step [17660/64305], Loss: 4.6313\n",
      "Epoch [2/2], Step [17670/64305], Loss: 4.7436\n",
      "Epoch [2/2], Step [17680/64305], Loss: 4.4977\n",
      "Epoch [2/2], Step [17690/64305], Loss: 4.7828\n",
      "Epoch [2/2], Step [17700/64305], Loss: 4.8264\n",
      "Epoch [2/2], Step [17710/64305], Loss: 4.8209\n",
      "Epoch [2/2], Step [17720/64305], Loss: 4.7858\n",
      "Epoch [2/2], Step [17730/64305], Loss: 5.1007\n",
      "Epoch [2/2], Step [17740/64305], Loss: 4.7766\n",
      "Epoch [2/2], Step [17750/64305], Loss: 4.6794\n",
      "Epoch [2/2], Step [17760/64305], Loss: 4.8781\n",
      "Epoch [2/2], Step [17770/64305], Loss: 4.9328\n",
      "Epoch [2/2], Step [17780/64305], Loss: 4.5939\n",
      "Epoch [2/2], Step [17790/64305], Loss: 4.9907\n",
      "Epoch [2/2], Step [17800/64305], Loss: 4.8453\n",
      "Epoch [2/2], Step [17810/64305], Loss: 4.7324\n",
      "Epoch [2/2], Step [17820/64305], Loss: 4.8382\n",
      "Epoch [2/2], Step [17830/64305], Loss: 4.7462\n",
      "Epoch [2/2], Step [17840/64305], Loss: 4.8220\n",
      "Epoch [2/2], Step [17850/64305], Loss: 4.7249\n",
      "Epoch [2/2], Step [17860/64305], Loss: 4.8614\n",
      "Epoch [2/2], Step [17870/64305], Loss: 4.8483\n",
      "Epoch [2/2], Step [17880/64305], Loss: 4.9220\n",
      "Epoch [2/2], Step [17890/64305], Loss: 4.8264\n",
      "Epoch [2/2], Step [17900/64305], Loss: 4.6499\n",
      "Epoch [2/2], Step [17910/64305], Loss: 4.5326\n",
      "Epoch [2/2], Step [17920/64305], Loss: 4.6668\n",
      "Epoch [2/2], Step [17930/64305], Loss: 4.6772\n",
      "Epoch [2/2], Step [17940/64305], Loss: 4.6592\n",
      "Epoch [2/2], Step [17950/64305], Loss: 4.7362\n",
      "Epoch [2/2], Step [17960/64305], Loss: 4.8438\n",
      "Epoch [2/2], Step [17970/64305], Loss: 4.9986\n",
      "Epoch [2/2], Step [17980/64305], Loss: 4.6935\n",
      "Epoch [2/2], Step [17990/64305], Loss: 4.7534\n",
      "Epoch [2/2], Step [18000/64305], Loss: 4.7179\n",
      "Epoch [2/2], Step [18010/64305], Loss: 4.9154\n",
      "Epoch [2/2], Step [18020/64305], Loss: 4.9575\n",
      "Epoch [2/2], Step [18030/64305], Loss: 4.7441\n",
      "Epoch [2/2], Step [18040/64305], Loss: 4.8144\n",
      "Epoch [2/2], Step [18050/64305], Loss: 4.8865\n",
      "Epoch [2/2], Step [18060/64305], Loss: 4.8401\n",
      "Epoch [2/2], Step [18070/64305], Loss: 4.8780\n",
      "Epoch [2/2], Step [18080/64305], Loss: 4.5579\n",
      "Epoch [2/2], Step [18090/64305], Loss: 4.8254\n",
      "Epoch [2/2], Step [18100/64305], Loss: 4.8329\n",
      "Epoch [2/2], Step [18110/64305], Loss: 5.0937\n",
      "Epoch [2/2], Step [18120/64305], Loss: 4.7026\n",
      "Epoch [2/2], Step [18130/64305], Loss: 5.0260\n",
      "Epoch [2/2], Step [18140/64305], Loss: 4.9189\n",
      "Epoch [2/2], Step [18150/64305], Loss: 4.7374\n",
      "Epoch [2/2], Step [18160/64305], Loss: 4.9082\n",
      "Epoch [2/2], Step [18170/64305], Loss: 4.8090\n",
      "Epoch [2/2], Step [18180/64305], Loss: 4.8823\n",
      "Epoch [2/2], Step [18190/64305], Loss: 4.8007\n",
      "Epoch [2/2], Step [18200/64305], Loss: 4.9343\n",
      "Epoch [2/2], Step [18210/64305], Loss: 4.5092\n",
      "Epoch [2/2], Step [18220/64305], Loss: 4.8408\n",
      "Epoch [2/2], Step [18230/64305], Loss: 4.7474\n",
      "Epoch [2/2], Step [18240/64305], Loss: 4.8362\n",
      "Epoch [2/2], Step [18250/64305], Loss: 4.9421\n",
      "Epoch [2/2], Step [18260/64305], Loss: 4.7146\n",
      "Epoch [2/2], Step [18270/64305], Loss: 4.7306\n",
      "Epoch [2/2], Step [18280/64305], Loss: 4.6682\n",
      "Epoch [2/2], Step [18290/64305], Loss: 4.8036\n",
      "Epoch [2/2], Step [18300/64305], Loss: 4.7952\n",
      "Epoch [2/2], Step [18310/64305], Loss: 4.6722\n",
      "Epoch [2/2], Step [18320/64305], Loss: 4.7720\n",
      "Epoch [2/2], Step [18330/64305], Loss: 4.9077\n",
      "Epoch [2/2], Step [18340/64305], Loss: 4.8532\n",
      "Epoch [2/2], Step [18350/64305], Loss: 4.7877\n",
      "Epoch [2/2], Step [18360/64305], Loss: 4.8186\n",
      "Epoch [2/2], Step [18370/64305], Loss: 4.7456\n",
      "Epoch [2/2], Step [18380/64305], Loss: 4.7998\n",
      "Epoch [2/2], Step [18390/64305], Loss: 4.7945\n",
      "Epoch [2/2], Step [18400/64305], Loss: 4.7066\n",
      "Epoch [2/2], Step [18410/64305], Loss: 4.6539\n",
      "Epoch [2/2], Step [18420/64305], Loss: 4.6234\n",
      "Epoch [2/2], Step [18430/64305], Loss: 4.6416\n",
      "Epoch [2/2], Step [18440/64305], Loss: 4.8447\n",
      "Epoch [2/2], Step [18450/64305], Loss: 4.6690\n",
      "Epoch [2/2], Step [18460/64305], Loss: 4.5272\n",
      "Epoch [2/2], Step [18470/64305], Loss: 4.7781\n",
      "Epoch [2/2], Step [18480/64305], Loss: 4.8762\n",
      "Epoch [2/2], Step [18490/64305], Loss: 4.7688\n",
      "Epoch [2/2], Step [18500/64305], Loss: 4.8669\n",
      "Epoch [2/2], Step [18510/64305], Loss: 4.7278\n",
      "Epoch [2/2], Step [18520/64305], Loss: 4.9175\n",
      "Epoch [2/2], Step [18530/64305], Loss: 4.6602\n",
      "Epoch [2/2], Step [18540/64305], Loss: 4.7846\n",
      "Epoch [2/2], Step [18550/64305], Loss: 4.7017\n",
      "Epoch [2/2], Step [18560/64305], Loss: 4.7644\n",
      "Epoch [2/2], Step [18570/64305], Loss: 4.6353\n",
      "Epoch [2/2], Step [18580/64305], Loss: 4.7521\n",
      "Epoch [2/2], Step [18590/64305], Loss: 4.7715\n",
      "Epoch [2/2], Step [18600/64305], Loss: 4.8825\n",
      "Epoch [2/2], Step [18610/64305], Loss: 4.5386\n",
      "Epoch [2/2], Step [18620/64305], Loss: 4.6975\n",
      "Epoch [2/2], Step [18630/64305], Loss: 4.6351\n",
      "Epoch [2/2], Step [18640/64305], Loss: 4.7990\n",
      "Epoch [2/2], Step [18650/64305], Loss: 4.8184\n",
      "Epoch [2/2], Step [18660/64305], Loss: 4.8265\n",
      "Epoch [2/2], Step [18670/64305], Loss: 4.8602\n",
      "Epoch [2/2], Step [18680/64305], Loss: 4.7728\n",
      "Epoch [2/2], Step [18690/64305], Loss: 4.9683\n",
      "Epoch [2/2], Step [18700/64305], Loss: 4.6855\n",
      "Epoch [2/2], Step [18710/64305], Loss: 4.7619\n",
      "Epoch [2/2], Step [18720/64305], Loss: 4.8220\n",
      "Epoch [2/2], Step [18730/64305], Loss: 4.7142\n",
      "Epoch [2/2], Step [18740/64305], Loss: 4.6777\n",
      "Epoch [2/2], Step [18750/64305], Loss: 4.9002\n",
      "Epoch [2/2], Step [18760/64305], Loss: 4.8956\n",
      "Epoch [2/2], Step [18770/64305], Loss: 4.8031\n",
      "Epoch [2/2], Step [18780/64305], Loss: 4.9124\n",
      "Epoch [2/2], Step [18790/64305], Loss: 4.7611\n",
      "Epoch [2/2], Step [18800/64305], Loss: 4.6027\n",
      "Epoch [2/2], Step [18810/64305], Loss: 4.5724\n",
      "Epoch [2/2], Step [18820/64305], Loss: 4.7385\n",
      "Epoch [2/2], Step [18830/64305], Loss: 4.6151\n",
      "Epoch [2/2], Step [18840/64305], Loss: 4.6702\n",
      "Epoch [2/2], Step [18850/64305], Loss: 4.7670\n",
      "Epoch [2/2], Step [18860/64305], Loss: 4.6626\n",
      "Epoch [2/2], Step [18870/64305], Loss: 4.7660\n",
      "Epoch [2/2], Step [18880/64305], Loss: 4.7787\n",
      "Epoch [2/2], Step [18890/64305], Loss: 4.9060\n",
      "Epoch [2/2], Step [18900/64305], Loss: 4.7652\n",
      "Epoch [2/2], Step [18910/64305], Loss: 4.7147\n",
      "Epoch [2/2], Step [18920/64305], Loss: 4.6060\n",
      "Epoch [2/2], Step [18930/64305], Loss: 4.7775\n",
      "Epoch [2/2], Step [18940/64305], Loss: 4.8703\n",
      "Epoch [2/2], Step [18950/64305], Loss: 4.9635\n",
      "Epoch [2/2], Step [18960/64305], Loss: 4.9114\n",
      "Epoch [2/2], Step [18970/64305], Loss: 4.8444\n",
      "Epoch [2/2], Step [18980/64305], Loss: 4.7231\n",
      "Epoch [2/2], Step [18990/64305], Loss: 4.9039\n",
      "Epoch [2/2], Step [19000/64305], Loss: 4.8836\n",
      "Epoch [2/2], Step [19010/64305], Loss: 4.8976\n",
      "Epoch [2/2], Step [19020/64305], Loss: 4.6205\n",
      "Epoch [2/2], Step [19030/64305], Loss: 4.8255\n",
      "Epoch [2/2], Step [19040/64305], Loss: 4.8384\n",
      "Epoch [2/2], Step [19050/64305], Loss: 4.7363\n",
      "Epoch [2/2], Step [19060/64305], Loss: 4.7616\n",
      "Epoch [2/2], Step [19070/64305], Loss: 4.5323\n",
      "Epoch [2/2], Step [19080/64305], Loss: 4.6884\n",
      "Epoch [2/2], Step [19090/64305], Loss: 4.8217\n",
      "Epoch [2/2], Step [19100/64305], Loss: 4.7717\n",
      "Epoch [2/2], Step [19110/64305], Loss: 4.7779\n",
      "Epoch [2/2], Step [19120/64305], Loss: 4.9444\n",
      "Epoch [2/2], Step [19130/64305], Loss: 4.6596\n",
      "Epoch [2/2], Step [19140/64305], Loss: 5.0160\n",
      "Epoch [2/2], Step [19150/64305], Loss: 4.9581\n",
      "Epoch [2/2], Step [19160/64305], Loss: 4.8618\n",
      "Epoch [2/2], Step [19170/64305], Loss: 4.6642\n",
      "Epoch [2/2], Step [19180/64305], Loss: 4.5439\n",
      "Epoch [2/2], Step [19190/64305], Loss: 4.8826\n",
      "Epoch [2/2], Step [19200/64305], Loss: 4.7382\n",
      "Epoch [2/2], Step [19210/64305], Loss: 4.8941\n",
      "Epoch [2/2], Step [19220/64305], Loss: 4.6682\n",
      "Epoch [2/2], Step [19230/64305], Loss: 4.7633\n",
      "Epoch [2/2], Step [19240/64305], Loss: 4.5705\n",
      "Epoch [2/2], Step [19250/64305], Loss: 4.7033\n",
      "Epoch [2/2], Step [19260/64305], Loss: 4.9411\n",
      "Epoch [2/2], Step [19270/64305], Loss: 4.8927\n",
      "Epoch [2/2], Step [19280/64305], Loss: 4.8995\n",
      "Epoch [2/2], Step [19290/64305], Loss: 4.5927\n",
      "Epoch [2/2], Step [19300/64305], Loss: 4.7311\n",
      "Epoch [2/2], Step [19310/64305], Loss: 4.7767\n",
      "Epoch [2/2], Step [19320/64305], Loss: 4.7520\n",
      "Epoch [2/2], Step [19330/64305], Loss: 5.0659\n",
      "Epoch [2/2], Step [19340/64305], Loss: 4.7977\n",
      "Epoch [2/2], Step [19350/64305], Loss: 4.9112\n",
      "Epoch [2/2], Step [19360/64305], Loss: 4.8647\n",
      "Epoch [2/2], Step [19370/64305], Loss: 4.8388\n",
      "Epoch [2/2], Step [19380/64305], Loss: 4.8791\n",
      "Epoch [2/2], Step [19390/64305], Loss: 4.7849\n",
      "Epoch [2/2], Step [19400/64305], Loss: 5.0845\n",
      "Epoch [2/2], Step [19410/64305], Loss: 4.5867\n",
      "Epoch [2/2], Step [19420/64305], Loss: 4.7372\n",
      "Epoch [2/2], Step [19430/64305], Loss: 4.7040\n",
      "Epoch [2/2], Step [19440/64305], Loss: 4.8874\n",
      "Epoch [2/2], Step [19450/64305], Loss: 4.6228\n",
      "Epoch [2/2], Step [19460/64305], Loss: 4.8522\n",
      "Epoch [2/2], Step [19470/64305], Loss: 5.0575\n",
      "Epoch [2/2], Step [19480/64305], Loss: 4.6911\n",
      "Epoch [2/2], Step [19490/64305], Loss: 4.9018\n",
      "Epoch [2/2], Step [19500/64305], Loss: 4.7981\n",
      "Epoch [2/2], Step [19510/64305], Loss: 4.7927\n",
      "Epoch [2/2], Step [19520/64305], Loss: 4.6224\n",
      "Epoch [2/2], Step [19530/64305], Loss: 4.8872\n",
      "Epoch [2/2], Step [19540/64305], Loss: 4.9740\n",
      "Epoch [2/2], Step [19550/64305], Loss: 4.8057\n",
      "Epoch [2/2], Step [19560/64305], Loss: 4.8331\n",
      "Epoch [2/2], Step [19570/64305], Loss: 4.6183\n",
      "Epoch [2/2], Step [19580/64305], Loss: 4.8281\n",
      "Epoch [2/2], Step [19590/64305], Loss: 4.8673\n",
      "Epoch [2/2], Step [19600/64305], Loss: 5.0866\n",
      "Epoch [2/2], Step [19610/64305], Loss: 4.6532\n",
      "Epoch [2/2], Step [19620/64305], Loss: 4.6643\n",
      "Epoch [2/2], Step [19630/64305], Loss: 4.9155\n",
      "Epoch [2/2], Step [19640/64305], Loss: 4.7563\n",
      "Epoch [2/2], Step [19650/64305], Loss: 4.7681\n",
      "Epoch [2/2], Step [19660/64305], Loss: 4.5565\n",
      "Epoch [2/2], Step [19670/64305], Loss: 4.7718\n",
      "Epoch [2/2], Step [19680/64305], Loss: 4.7930\n",
      "Epoch [2/2], Step [19690/64305], Loss: 4.8318\n",
      "Epoch [2/2], Step [19700/64305], Loss: 4.7223\n",
      "Epoch [2/2], Step [19710/64305], Loss: 4.7802\n",
      "Epoch [2/2], Step [19720/64305], Loss: 4.7767\n",
      "Epoch [2/2], Step [19730/64305], Loss: 4.8339\n",
      "Epoch [2/2], Step [19740/64305], Loss: 4.5834\n",
      "Epoch [2/2], Step [19750/64305], Loss: 4.9229\n",
      "Epoch [2/2], Step [19760/64305], Loss: 4.6925\n",
      "Epoch [2/2], Step [19770/64305], Loss: 4.7076\n",
      "Epoch [2/2], Step [19780/64305], Loss: 4.7106\n",
      "Epoch [2/2], Step [19790/64305], Loss: 4.7492\n",
      "Epoch [2/2], Step [19800/64305], Loss: 4.7323\n",
      "Epoch [2/2], Step [19810/64305], Loss: 4.7986\n",
      "Epoch [2/2], Step [19820/64305], Loss: 4.8439\n",
      "Epoch [2/2], Step [19830/64305], Loss: 4.9869\n",
      "Epoch [2/2], Step [19840/64305], Loss: 4.7754\n",
      "Epoch [2/2], Step [19850/64305], Loss: 4.6320\n",
      "Epoch [2/2], Step [19860/64305], Loss: 4.8603\n",
      "Epoch [2/2], Step [19870/64305], Loss: 4.9312\n",
      "Epoch [2/2], Step [19880/64305], Loss: 4.5712\n",
      "Epoch [2/2], Step [19890/64305], Loss: 4.8518\n",
      "Epoch [2/2], Step [19900/64305], Loss: 5.0216\n",
      "Epoch [2/2], Step [19910/64305], Loss: 4.8997\n",
      "Epoch [2/2], Step [19920/64305], Loss: 4.7684\n",
      "Epoch [2/2], Step [19930/64305], Loss: 4.7150\n",
      "Epoch [2/2], Step [19940/64305], Loss: 4.7790\n",
      "Epoch [2/2], Step [19950/64305], Loss: 4.7035\n",
      "Epoch [2/2], Step [19960/64305], Loss: 4.5905\n",
      "Epoch [2/2], Step [19970/64305], Loss: 4.8493\n",
      "Epoch [2/2], Step [19980/64305], Loss: 4.8065\n",
      "Epoch [2/2], Step [19990/64305], Loss: 4.9020\n",
      "Epoch [2/2], Step [20000/64305], Loss: 4.8664\n",
      "Epoch [2/2], Step [20010/64305], Loss: 4.7267\n",
      "Epoch [2/2], Step [20020/64305], Loss: 4.9448\n",
      "Epoch [2/2], Step [20030/64305], Loss: 4.8527\n",
      "Epoch [2/2], Step [20040/64305], Loss: 4.7165\n",
      "Epoch [2/2], Step [20050/64305], Loss: 4.6827\n",
      "Epoch [2/2], Step [20060/64305], Loss: 4.5372\n",
      "Epoch [2/2], Step [20070/64305], Loss: 4.9316\n",
      "Epoch [2/2], Step [20080/64305], Loss: 4.6389\n",
      "Epoch [2/2], Step [20090/64305], Loss: 4.9224\n",
      "Epoch [2/2], Step [20100/64305], Loss: 5.0529\n",
      "Epoch [2/2], Step [20110/64305], Loss: 4.6279\n",
      "Epoch [2/2], Step [20120/64305], Loss: 4.7973\n",
      "Epoch [2/2], Step [20130/64305], Loss: 4.6938\n",
      "Epoch [2/2], Step [20140/64305], Loss: 4.8280\n",
      "Epoch [2/2], Step [20150/64305], Loss: 4.5691\n",
      "Epoch [2/2], Step [20160/64305], Loss: 4.6933\n",
      "Epoch [2/2], Step [20170/64305], Loss: 4.9270\n",
      "Epoch [2/2], Step [20180/64305], Loss: 4.8097\n",
      "Epoch [2/2], Step [20190/64305], Loss: 4.9905\n",
      "Epoch [2/2], Step [20200/64305], Loss: 4.9852\n",
      "Epoch [2/2], Step [20210/64305], Loss: 4.7350\n",
      "Epoch [2/2], Step [20220/64305], Loss: 4.7740\n",
      "Epoch [2/2], Step [20230/64305], Loss: 4.5930\n",
      "Epoch [2/2], Step [20240/64305], Loss: 4.7765\n",
      "Epoch [2/2], Step [20250/64305], Loss: 4.7137\n",
      "Epoch [2/2], Step [20260/64305], Loss: 4.7777\n",
      "Epoch [2/2], Step [20270/64305], Loss: 4.7231\n",
      "Epoch [2/2], Step [20280/64305], Loss: 4.6761\n",
      "Epoch [2/2], Step [20290/64305], Loss: 4.7534\n",
      "Epoch [2/2], Step [20300/64305], Loss: 4.7402\n",
      "Epoch [2/2], Step [20310/64305], Loss: 4.6958\n",
      "Epoch [2/2], Step [20320/64305], Loss: 5.0190\n",
      "Epoch [2/2], Step [20330/64305], Loss: 4.6442\n",
      "Epoch [2/2], Step [20340/64305], Loss: 4.6732\n",
      "Epoch [2/2], Step [20350/64305], Loss: 4.7493\n",
      "Epoch [2/2], Step [20360/64305], Loss: 4.8310\n",
      "Epoch [2/2], Step [20370/64305], Loss: 4.7333\n",
      "Epoch [2/2], Step [20380/64305], Loss: 4.7310\n",
      "Epoch [2/2], Step [20390/64305], Loss: 4.8590\n",
      "Epoch [2/2], Step [20400/64305], Loss: 4.7563\n",
      "Epoch [2/2], Step [20410/64305], Loss: 4.7975\n",
      "Epoch [2/2], Step [20420/64305], Loss: 4.7062\n",
      "Epoch [2/2], Step [20430/64305], Loss: 4.9160\n",
      "Epoch [2/2], Step [20440/64305], Loss: 4.7244\n",
      "Epoch [2/2], Step [20450/64305], Loss: 4.8739\n",
      "Epoch [2/2], Step [20460/64305], Loss: 4.8796\n",
      "Epoch [2/2], Step [20470/64305], Loss: 4.5652\n",
      "Epoch [2/2], Step [20480/64305], Loss: 4.6752\n",
      "Epoch [2/2], Step [20490/64305], Loss: 4.5306\n",
      "Epoch [2/2], Step [20500/64305], Loss: 4.9481\n",
      "Epoch [2/2], Step [20510/64305], Loss: 4.6720\n",
      "Epoch [2/2], Step [20520/64305], Loss: 4.6462\n",
      "Epoch [2/2], Step [20530/64305], Loss: 4.9304\n",
      "Epoch [2/2], Step [20540/64305], Loss: 4.8464\n",
      "Epoch [2/2], Step [20550/64305], Loss: 4.9176\n",
      "Epoch [2/2], Step [20560/64305], Loss: 4.9507\n",
      "Epoch [2/2], Step [20570/64305], Loss: 4.8185\n",
      "Epoch [2/2], Step [20580/64305], Loss: 4.9800\n",
      "Epoch [2/2], Step [20590/64305], Loss: 4.7717\n",
      "Epoch [2/2], Step [20600/64305], Loss: 4.9064\n",
      "Epoch [2/2], Step [20610/64305], Loss: 4.7027\n",
      "Epoch [2/2], Step [20620/64305], Loss: 4.8022\n",
      "Epoch [2/2], Step [20630/64305], Loss: 4.6864\n",
      "Epoch [2/2], Step [20640/64305], Loss: 4.8216\n",
      "Epoch [2/2], Step [20650/64305], Loss: 4.8702\n",
      "Epoch [2/2], Step [20660/64305], Loss: 4.8371\n",
      "Epoch [2/2], Step [20670/64305], Loss: 4.7350\n",
      "Epoch [2/2], Step [20680/64305], Loss: 4.6646\n",
      "Epoch [2/2], Step [20690/64305], Loss: 4.5960\n",
      "Epoch [2/2], Step [20700/64305], Loss: 4.9761\n",
      "Epoch [2/2], Step [20710/64305], Loss: 4.7755\n",
      "Epoch [2/2], Step [20720/64305], Loss: 4.7019\n",
      "Epoch [2/2], Step [20730/64305], Loss: 4.9550\n",
      "Epoch [2/2], Step [20740/64305], Loss: 4.8658\n",
      "Epoch [2/2], Step [20750/64305], Loss: 4.7921\n",
      "Epoch [2/2], Step [20760/64305], Loss: 4.8902\n",
      "Epoch [2/2], Step [20770/64305], Loss: 4.6545\n",
      "Epoch [2/2], Step [20780/64305], Loss: 4.8965\n",
      "Epoch [2/2], Step [20790/64305], Loss: 4.6747\n",
      "Epoch [2/2], Step [20800/64305], Loss: 4.7510\n",
      "Epoch [2/2], Step [20810/64305], Loss: 4.7833\n",
      "Epoch [2/2], Step [20820/64305], Loss: 4.8274\n",
      "Epoch [2/2], Step [20830/64305], Loss: 4.5044\n",
      "Epoch [2/2], Step [20840/64305], Loss: 4.9044\n",
      "Epoch [2/2], Step [20850/64305], Loss: 4.8117\n",
      "Epoch [2/2], Step [20860/64305], Loss: 4.8261\n",
      "Epoch [2/2], Step [20870/64305], Loss: 4.6847\n",
      "Epoch [2/2], Step [20880/64305], Loss: 5.0211\n",
      "Epoch [2/2], Step [20890/64305], Loss: 4.9713\n",
      "Epoch [2/2], Step [20900/64305], Loss: 4.7200\n",
      "Epoch [2/2], Step [20910/64305], Loss: 4.6826\n",
      "Epoch [2/2], Step [20920/64305], Loss: 4.8698\n",
      "Epoch [2/2], Step [20930/64305], Loss: 5.0167\n",
      "Epoch [2/2], Step [20940/64305], Loss: 4.8662\n",
      "Epoch [2/2], Step [20950/64305], Loss: 4.7738\n",
      "Epoch [2/2], Step [20960/64305], Loss: 4.6401\n",
      "Epoch [2/2], Step [20970/64305], Loss: 4.6228\n",
      "Epoch [2/2], Step [20980/64305], Loss: 4.7844\n",
      "Epoch [2/2], Step [20990/64305], Loss: 4.8020\n",
      "Epoch [2/2], Step [21000/64305], Loss: 4.8078\n",
      "Epoch [2/2], Step [21010/64305], Loss: 4.8348\n",
      "Epoch [2/2], Step [21020/64305], Loss: 4.9141\n",
      "Epoch [2/2], Step [21030/64305], Loss: 4.8592\n",
      "Epoch [2/2], Step [21040/64305], Loss: 4.8865\n",
      "Epoch [2/2], Step [21050/64305], Loss: 4.8523\n",
      "Epoch [2/2], Step [21060/64305], Loss: 4.8289\n",
      "Epoch [2/2], Step [21070/64305], Loss: 4.8370\n",
      "Epoch [2/2], Step [21080/64305], Loss: 4.6602\n",
      "Epoch [2/2], Step [21090/64305], Loss: 4.6599\n",
      "Epoch [2/2], Step [21100/64305], Loss: 4.7271\n",
      "Epoch [2/2], Step [21110/64305], Loss: 4.6950\n",
      "Epoch [2/2], Step [21120/64305], Loss: 4.7591\n",
      "Epoch [2/2], Step [21130/64305], Loss: 4.6141\n",
      "Epoch [2/2], Step [21140/64305], Loss: 4.6465\n",
      "Epoch [2/2], Step [21150/64305], Loss: 4.8517\n",
      "Epoch [2/2], Step [21160/64305], Loss: 4.7366\n",
      "Epoch [2/2], Step [21170/64305], Loss: 4.7842\n",
      "Epoch [2/2], Step [21180/64305], Loss: 4.7457\n",
      "Epoch [2/2], Step [21190/64305], Loss: 4.8455\n",
      "Epoch [2/2], Step [21200/64305], Loss: 4.8793\n",
      "Epoch [2/2], Step [21210/64305], Loss: 4.6498\n",
      "Epoch [2/2], Step [21220/64305], Loss: 4.6765\n",
      "Epoch [2/2], Step [21230/64305], Loss: 4.7041\n",
      "Epoch [2/2], Step [21240/64305], Loss: 4.7259\n",
      "Epoch [2/2], Step [21250/64305], Loss: 4.8281\n",
      "Epoch [2/2], Step [21260/64305], Loss: 4.7925\n",
      "Epoch [2/2], Step [21270/64305], Loss: 4.7303\n",
      "Epoch [2/2], Step [21280/64305], Loss: 4.6361\n",
      "Epoch [2/2], Step [21290/64305], Loss: 4.8665\n",
      "Epoch [2/2], Step [21300/64305], Loss: 4.7672\n",
      "Epoch [2/2], Step [21310/64305], Loss: 5.0027\n",
      "Epoch [2/2], Step [21320/64305], Loss: 4.6818\n",
      "Epoch [2/2], Step [21330/64305], Loss: 4.7377\n",
      "Epoch [2/2], Step [21340/64305], Loss: 4.7621\n",
      "Epoch [2/2], Step [21350/64305], Loss: 4.9110\n",
      "Epoch [2/2], Step [21360/64305], Loss: 4.8615\n",
      "Epoch [2/2], Step [21370/64305], Loss: 4.6641\n",
      "Epoch [2/2], Step [21380/64305], Loss: 4.9193\n",
      "Epoch [2/2], Step [21390/64305], Loss: 4.8249\n",
      "Epoch [2/2], Step [21400/64305], Loss: 4.6209\n",
      "Epoch [2/2], Step [21410/64305], Loss: 4.6606\n",
      "Epoch [2/2], Step [21420/64305], Loss: 4.7395\n",
      "Epoch [2/2], Step [21430/64305], Loss: 4.7788\n",
      "Epoch [2/2], Step [21440/64305], Loss: 4.6820\n",
      "Epoch [2/2], Step [21450/64305], Loss: 4.6891\n",
      "Epoch [2/2], Step [21460/64305], Loss: 4.7968\n",
      "Epoch [2/2], Step [21470/64305], Loss: 4.6611\n",
      "Epoch [2/2], Step [21480/64305], Loss: 4.9052\n",
      "Epoch [2/2], Step [21490/64305], Loss: 4.6756\n",
      "Epoch [2/2], Step [21500/64305], Loss: 4.8895\n",
      "Epoch [2/2], Step [21510/64305], Loss: 4.5435\n",
      "Epoch [2/2], Step [21520/64305], Loss: 4.9185\n",
      "Epoch [2/2], Step [21530/64305], Loss: 4.6672\n",
      "Epoch [2/2], Step [21540/64305], Loss: 4.6418\n",
      "Epoch [2/2], Step [21550/64305], Loss: 4.9444\n",
      "Epoch [2/2], Step [21560/64305], Loss: 4.7385\n",
      "Epoch [2/2], Step [21570/64305], Loss: 4.8966\n",
      "Epoch [2/2], Step [21580/64305], Loss: 4.8617\n",
      "Epoch [2/2], Step [21590/64305], Loss: 4.7128\n",
      "Epoch [2/2], Step [21600/64305], Loss: 4.7406\n",
      "Epoch [2/2], Step [21610/64305], Loss: 4.8269\n",
      "Epoch [2/2], Step [21620/64305], Loss: 4.7651\n",
      "Epoch [2/2], Step [21630/64305], Loss: 4.9337\n",
      "Epoch [2/2], Step [21640/64305], Loss: 4.5972\n",
      "Epoch [2/2], Step [21650/64305], Loss: 4.9669\n",
      "Epoch [2/2], Step [21660/64305], Loss: 4.7604\n",
      "Epoch [2/2], Step [21670/64305], Loss: 4.8726\n",
      "Epoch [2/2], Step [21680/64305], Loss: 4.7062\n",
      "Epoch [2/2], Step [21690/64305], Loss: 4.6932\n",
      "Epoch [2/2], Step [21700/64305], Loss: 4.8438\n",
      "Epoch [2/2], Step [21710/64305], Loss: 4.6850\n",
      "Epoch [2/2], Step [21720/64305], Loss: 4.8538\n",
      "Epoch [2/2], Step [21730/64305], Loss: 4.7765\n",
      "Epoch [2/2], Step [21740/64305], Loss: 4.5749\n",
      "Epoch [2/2], Step [21750/64305], Loss: 4.7300\n",
      "Epoch [2/2], Step [21760/64305], Loss: 4.8256\n",
      "Epoch [2/2], Step [21770/64305], Loss: 4.8760\n",
      "Epoch [2/2], Step [21780/64305], Loss: 4.7702\n",
      "Epoch [2/2], Step [21790/64305], Loss: 4.7905\n",
      "Epoch [2/2], Step [21800/64305], Loss: 4.7551\n",
      "Epoch [2/2], Step [21810/64305], Loss: 4.8716\n",
      "Epoch [2/2], Step [21820/64305], Loss: 4.7146\n",
      "Epoch [2/2], Step [21830/64305], Loss: 5.0484\n",
      "Epoch [2/2], Step [21840/64305], Loss: 4.8511\n",
      "Epoch [2/2], Step [21850/64305], Loss: 4.8491\n",
      "Epoch [2/2], Step [21860/64305], Loss: 4.6878\n",
      "Epoch [2/2], Step [21870/64305], Loss: 5.0182\n",
      "Epoch [2/2], Step [21880/64305], Loss: 4.7450\n",
      "Epoch [2/2], Step [21890/64305], Loss: 4.7676\n",
      "Epoch [2/2], Step [21900/64305], Loss: 4.5769\n",
      "Epoch [2/2], Step [21910/64305], Loss: 4.8577\n",
      "Epoch [2/2], Step [21920/64305], Loss: 4.8508\n",
      "Epoch [2/2], Step [21930/64305], Loss: 4.7782\n",
      "Epoch [2/2], Step [21940/64305], Loss: 4.6906\n",
      "Epoch [2/2], Step [21950/64305], Loss: 4.8001\n",
      "Epoch [2/2], Step [21960/64305], Loss: 4.8386\n",
      "Epoch [2/2], Step [21970/64305], Loss: 4.8558\n",
      "Epoch [2/2], Step [21980/64305], Loss: 4.7768\n",
      "Epoch [2/2], Step [21990/64305], Loss: 4.9248\n",
      "Epoch [2/2], Step [22000/64305], Loss: 4.8097\n",
      "Epoch [2/2], Step [22010/64305], Loss: 4.7699\n",
      "Epoch [2/2], Step [22020/64305], Loss: 4.8801\n",
      "Epoch [2/2], Step [22030/64305], Loss: 5.0028\n",
      "Epoch [2/2], Step [22040/64305], Loss: 4.6204\n",
      "Epoch [2/2], Step [22050/64305], Loss: 4.9102\n",
      "Epoch [2/2], Step [22060/64305], Loss: 4.8203\n",
      "Epoch [2/2], Step [22070/64305], Loss: 4.5597\n",
      "Epoch [2/2], Step [22080/64305], Loss: 4.6990\n",
      "Epoch [2/2], Step [22090/64305], Loss: 4.6800\n",
      "Epoch [2/2], Step [22100/64305], Loss: 4.9461\n",
      "Epoch [2/2], Step [22110/64305], Loss: 4.8102\n",
      "Epoch [2/2], Step [22120/64305], Loss: 4.6645\n",
      "Epoch [2/2], Step [22130/64305], Loss: 4.8360\n",
      "Epoch [2/2], Step [22140/64305], Loss: 4.8658\n",
      "Epoch [2/2], Step [22150/64305], Loss: 4.5461\n",
      "Epoch [2/2], Step [22160/64305], Loss: 4.8738\n",
      "Epoch [2/2], Step [22170/64305], Loss: 4.7960\n",
      "Epoch [2/2], Step [22180/64305], Loss: 4.5920\n",
      "Epoch [2/2], Step [22190/64305], Loss: 4.7079\n",
      "Epoch [2/2], Step [22200/64305], Loss: 4.7925\n",
      "Epoch [2/2], Step [22210/64305], Loss: 4.8076\n",
      "Epoch [2/2], Step [22220/64305], Loss: 4.9423\n",
      "Epoch [2/2], Step [22230/64305], Loss: 4.8672\n",
      "Epoch [2/2], Step [22240/64305], Loss: 4.7093\n",
      "Epoch [2/2], Step [22250/64305], Loss: 4.8401\n",
      "Epoch [2/2], Step [22260/64305], Loss: 4.8865\n",
      "Epoch [2/2], Step [22270/64305], Loss: 4.8607\n",
      "Epoch [2/2], Step [22280/64305], Loss: 4.6629\n",
      "Epoch [2/2], Step [22290/64305], Loss: 4.8719\n",
      "Epoch [2/2], Step [22300/64305], Loss: 4.8245\n",
      "Epoch [2/2], Step [22310/64305], Loss: 4.8073\n",
      "Epoch [2/2], Step [22320/64305], Loss: 4.6277\n",
      "Epoch [2/2], Step [22330/64305], Loss: 4.6644\n",
      "Epoch [2/2], Step [22340/64305], Loss: 4.8364\n",
      "Epoch [2/2], Step [22350/64305], Loss: 4.6324\n",
      "Epoch [2/2], Step [22360/64305], Loss: 4.7662\n",
      "Epoch [2/2], Step [22370/64305], Loss: 4.8404\n",
      "Epoch [2/2], Step [22380/64305], Loss: 4.7898\n",
      "Epoch [2/2], Step [22390/64305], Loss: 4.7738\n",
      "Epoch [2/2], Step [22400/64305], Loss: 4.7349\n",
      "Epoch [2/2], Step [22410/64305], Loss: 4.7689\n",
      "Epoch [2/2], Step [22420/64305], Loss: 4.7508\n",
      "Epoch [2/2], Step [22430/64305], Loss: 5.0033\n",
      "Epoch [2/2], Step [22440/64305], Loss: 4.8807\n",
      "Epoch [2/2], Step [22450/64305], Loss: 4.9446\n",
      "Epoch [2/2], Step [22460/64305], Loss: 5.1412\n",
      "Epoch [2/2], Step [22470/64305], Loss: 4.5313\n",
      "Epoch [2/2], Step [22480/64305], Loss: 4.6932\n",
      "Epoch [2/2], Step [22490/64305], Loss: 4.6435\n",
      "Epoch [2/2], Step [22500/64305], Loss: 4.8221\n",
      "Epoch [2/2], Step [22510/64305], Loss: 4.8532\n",
      "Epoch [2/2], Step [22520/64305], Loss: 4.8517\n",
      "Epoch [2/2], Step [22530/64305], Loss: 4.6966\n",
      "Epoch [2/2], Step [22540/64305], Loss: 4.6356\n",
      "Epoch [2/2], Step [22550/64305], Loss: 4.7430\n",
      "Epoch [2/2], Step [22560/64305], Loss: 4.7089\n",
      "Epoch [2/2], Step [22570/64305], Loss: 4.7503\n",
      "Epoch [2/2], Step [22580/64305], Loss: 4.7593\n",
      "Epoch [2/2], Step [22590/64305], Loss: 4.6166\n",
      "Epoch [2/2], Step [22600/64305], Loss: 4.9254\n",
      "Epoch [2/2], Step [22610/64305], Loss: 4.9393\n",
      "Epoch [2/2], Step [22620/64305], Loss: 4.8336\n",
      "Epoch [2/2], Step [22630/64305], Loss: 4.6403\n",
      "Epoch [2/2], Step [22640/64305], Loss: 5.0198\n",
      "Epoch [2/2], Step [22650/64305], Loss: 4.7053\n",
      "Epoch [2/2], Step [22660/64305], Loss: 4.8336\n",
      "Epoch [2/2], Step [22670/64305], Loss: 4.8409\n",
      "Epoch [2/2], Step [22680/64305], Loss: 4.8907\n",
      "Epoch [2/2], Step [22690/64305], Loss: 4.8022\n",
      "Epoch [2/2], Step [22700/64305], Loss: 4.9239\n",
      "Epoch [2/2], Step [22710/64305], Loss: 4.7139\n",
      "Epoch [2/2], Step [22720/64305], Loss: 4.7764\n",
      "Epoch [2/2], Step [22730/64305], Loss: 4.7108\n",
      "Epoch [2/2], Step [22740/64305], Loss: 4.8906\n",
      "Epoch [2/2], Step [22750/64305], Loss: 4.8526\n",
      "Epoch [2/2], Step [22760/64305], Loss: 5.0530\n",
      "Epoch [2/2], Step [22770/64305], Loss: 4.6550\n",
      "Epoch [2/2], Step [22780/64305], Loss: 4.6669\n",
      "Epoch [2/2], Step [22790/64305], Loss: 4.8828\n",
      "Epoch [2/2], Step [22800/64305], Loss: 4.9427\n",
      "Epoch [2/2], Step [22810/64305], Loss: 4.6174\n",
      "Epoch [2/2], Step [22820/64305], Loss: 4.7379\n",
      "Epoch [2/2], Step [22830/64305], Loss: 4.6857\n",
      "Epoch [2/2], Step [22840/64305], Loss: 4.7471\n",
      "Epoch [2/2], Step [22850/64305], Loss: 4.6633\n",
      "Epoch [2/2], Step [22860/64305], Loss: 4.7562\n",
      "Epoch [2/2], Step [22870/64305], Loss: 4.8118\n",
      "Epoch [2/2], Step [22880/64305], Loss: 4.8400\n",
      "Epoch [2/2], Step [22890/64305], Loss: 4.6747\n",
      "Epoch [2/2], Step [22900/64305], Loss: 4.7834\n",
      "Epoch [2/2], Step [22910/64305], Loss: 4.6981\n",
      "Epoch [2/2], Step [22920/64305], Loss: 4.6907\n",
      "Epoch [2/2], Step [22930/64305], Loss: 4.6095\n",
      "Epoch [2/2], Step [22940/64305], Loss: 4.7121\n",
      "Epoch [2/2], Step [22950/64305], Loss: 4.8949\n",
      "Epoch [2/2], Step [22960/64305], Loss: 4.6360\n",
      "Epoch [2/2], Step [22970/64305], Loss: 4.7075\n",
      "Epoch [2/2], Step [22980/64305], Loss: 4.8502\n",
      "Epoch [2/2], Step [22990/64305], Loss: 4.6361\n",
      "Epoch [2/2], Step [23000/64305], Loss: 4.7414\n",
      "Epoch [2/2], Step [23010/64305], Loss: 4.7084\n",
      "Epoch [2/2], Step [23020/64305], Loss: 4.6766\n",
      "Epoch [2/2], Step [23030/64305], Loss: 4.7079\n",
      "Epoch [2/2], Step [23040/64305], Loss: 4.7891\n",
      "Epoch [2/2], Step [23050/64305], Loss: 4.5355\n",
      "Epoch [2/2], Step [23060/64305], Loss: 4.8308\n",
      "Epoch [2/2], Step [23070/64305], Loss: 4.8360\n",
      "Epoch [2/2], Step [23080/64305], Loss: 4.8106\n",
      "Epoch [2/2], Step [23090/64305], Loss: 4.7372\n",
      "Epoch [2/2], Step [23100/64305], Loss: 4.6175\n",
      "Epoch [2/2], Step [23110/64305], Loss: 4.5771\n",
      "Epoch [2/2], Step [23120/64305], Loss: 4.6197\n",
      "Epoch [2/2], Step [23130/64305], Loss: 4.8868\n",
      "Epoch [2/2], Step [23140/64305], Loss: 4.9190\n",
      "Epoch [2/2], Step [23150/64305], Loss: 4.8294\n",
      "Epoch [2/2], Step [23160/64305], Loss: 4.7652\n",
      "Epoch [2/2], Step [23170/64305], Loss: 4.6819\n",
      "Epoch [2/2], Step [23180/64305], Loss: 4.9041\n",
      "Epoch [2/2], Step [23190/64305], Loss: 4.8527\n",
      "Epoch [2/2], Step [23200/64305], Loss: 4.6750\n",
      "Epoch [2/2], Step [23210/64305], Loss: 4.6757\n",
      "Epoch [2/2], Step [23220/64305], Loss: 4.8878\n",
      "Epoch [2/2], Step [23230/64305], Loss: 4.7269\n",
      "Epoch [2/2], Step [23240/64305], Loss: 4.5656\n",
      "Epoch [2/2], Step [23250/64305], Loss: 4.9875\n",
      "Epoch [2/2], Step [23260/64305], Loss: 4.8742\n",
      "Epoch [2/2], Step [23270/64305], Loss: 4.6098\n",
      "Epoch [2/2], Step [23280/64305], Loss: 4.6841\n",
      "Epoch [2/2], Step [23290/64305], Loss: 4.6881\n",
      "Epoch [2/2], Step [23300/64305], Loss: 4.6278\n",
      "Epoch [2/2], Step [23310/64305], Loss: 4.8514\n",
      "Epoch [2/2], Step [23320/64305], Loss: 4.7053\n",
      "Epoch [2/2], Step [23330/64305], Loss: 4.9257\n",
      "Epoch [2/2], Step [23340/64305], Loss: 4.7384\n",
      "Epoch [2/2], Step [23350/64305], Loss: 4.7420\n",
      "Epoch [2/2], Step [23360/64305], Loss: 4.8370\n",
      "Epoch [2/2], Step [23370/64305], Loss: 4.5820\n",
      "Epoch [2/2], Step [23380/64305], Loss: 4.6964\n",
      "Epoch [2/2], Step [23390/64305], Loss: 4.7407\n",
      "Epoch [2/2], Step [23400/64305], Loss: 4.9594\n",
      "Epoch [2/2], Step [23410/64305], Loss: 4.6092\n",
      "Epoch [2/2], Step [23420/64305], Loss: 4.5669\n",
      "Epoch [2/2], Step [23430/64305], Loss: 4.6529\n",
      "Epoch [2/2], Step [23440/64305], Loss: 4.8055\n",
      "Epoch [2/2], Step [23450/64305], Loss: 4.7077\n",
      "Epoch [2/2], Step [23460/64305], Loss: 4.8349\n",
      "Epoch [2/2], Step [23470/64305], Loss: 4.8685\n",
      "Epoch [2/2], Step [23480/64305], Loss: 4.8072\n",
      "Epoch [2/2], Step [23490/64305], Loss: 4.8652\n",
      "Epoch [2/2], Step [23500/64305], Loss: 4.6082\n",
      "Epoch [2/2], Step [23510/64305], Loss: 4.8013\n",
      "Epoch [2/2], Step [23520/64305], Loss: 4.6982\n",
      "Epoch [2/2], Step [23530/64305], Loss: 4.7782\n",
      "Epoch [2/2], Step [23540/64305], Loss: 4.7937\n",
      "Epoch [2/2], Step [23550/64305], Loss: 4.7876\n",
      "Epoch [2/2], Step [23560/64305], Loss: 4.7510\n",
      "Epoch [2/2], Step [23570/64305], Loss: 4.8513\n",
      "Epoch [2/2], Step [23580/64305], Loss: 4.7834\n",
      "Epoch [2/2], Step [23590/64305], Loss: 4.8672\n",
      "Epoch [2/2], Step [23600/64305], Loss: 4.6860\n",
      "Epoch [2/2], Step [23610/64305], Loss: 4.8491\n",
      "Epoch [2/2], Step [23620/64305], Loss: 4.9687\n",
      "Epoch [2/2], Step [23630/64305], Loss: 4.5097\n",
      "Epoch [2/2], Step [23640/64305], Loss: 4.7567\n",
      "Epoch [2/2], Step [23650/64305], Loss: 4.8128\n",
      "Epoch [2/2], Step [23660/64305], Loss: 4.7392\n",
      "Epoch [2/2], Step [23670/64305], Loss: 4.7592\n",
      "Epoch [2/2], Step [23680/64305], Loss: 4.8655\n",
      "Epoch [2/2], Step [23690/64305], Loss: 4.7140\n",
      "Epoch [2/2], Step [23700/64305], Loss: 4.7788\n",
      "Epoch [2/2], Step [23710/64305], Loss: 4.6457\n",
      "Epoch [2/2], Step [23720/64305], Loss: 4.6673\n",
      "Epoch [2/2], Step [23730/64305], Loss: 4.9150\n",
      "Epoch [2/2], Step [23740/64305], Loss: 4.6550\n",
      "Epoch [2/2], Step [23750/64305], Loss: 4.6587\n",
      "Epoch [2/2], Step [23760/64305], Loss: 4.8710\n",
      "Epoch [2/2], Step [23770/64305], Loss: 4.8760\n",
      "Epoch [2/2], Step [23780/64305], Loss: 4.7121\n",
      "Epoch [2/2], Step [23790/64305], Loss: 4.9065\n",
      "Epoch [2/2], Step [23800/64305], Loss: 4.6233\n",
      "Epoch [2/2], Step [23810/64305], Loss: 4.7690\n",
      "Epoch [2/2], Step [23820/64305], Loss: 4.7647\n",
      "Epoch [2/2], Step [23830/64305], Loss: 4.8481\n",
      "Epoch [2/2], Step [23840/64305], Loss: 4.8529\n",
      "Epoch [2/2], Step [23850/64305], Loss: 4.7566\n",
      "Epoch [2/2], Step [23860/64305], Loss: 4.7835\n",
      "Epoch [2/2], Step [23870/64305], Loss: 5.2006\n",
      "Epoch [2/2], Step [23880/64305], Loss: 4.7883\n",
      "Epoch [2/2], Step [23890/64305], Loss: 4.9084\n",
      "Epoch [2/2], Step [23900/64305], Loss: 4.7293\n",
      "Epoch [2/2], Step [23910/64305], Loss: 4.7872\n",
      "Epoch [2/2], Step [23920/64305], Loss: 4.7165\n",
      "Epoch [2/2], Step [23930/64305], Loss: 4.8911\n",
      "Epoch [2/2], Step [23940/64305], Loss: 4.7772\n",
      "Epoch [2/2], Step [23950/64305], Loss: 4.6105\n",
      "Epoch [2/2], Step [23960/64305], Loss: 4.7222\n",
      "Epoch [2/2], Step [23970/64305], Loss: 4.7375\n",
      "Epoch [2/2], Step [23980/64305], Loss: 5.0548\n",
      "Epoch [2/2], Step [23990/64305], Loss: 4.8337\n",
      "Epoch [2/2], Step [24000/64305], Loss: 4.8490\n",
      "Epoch [2/2], Step [24010/64305], Loss: 4.7057\n",
      "Epoch [2/2], Step [24020/64305], Loss: 4.6101\n",
      "Epoch [2/2], Step [24030/64305], Loss: 4.6563\n",
      "Epoch [2/2], Step [24040/64305], Loss: 4.8416\n",
      "Epoch [2/2], Step [24050/64305], Loss: 4.8717\n",
      "Epoch [2/2], Step [24060/64305], Loss: 4.7595\n",
      "Epoch [2/2], Step [24070/64305], Loss: 4.9570\n",
      "Epoch [2/2], Step [24080/64305], Loss: 4.5809\n",
      "Epoch [2/2], Step [24090/64305], Loss: 4.6983\n",
      "Epoch [2/2], Step [24100/64305], Loss: 4.8891\n",
      "Epoch [2/2], Step [24110/64305], Loss: 4.6649\n",
      "Epoch [2/2], Step [24120/64305], Loss: 4.7603\n",
      "Epoch [2/2], Step [24130/64305], Loss: 4.8314\n",
      "Epoch [2/2], Step [24140/64305], Loss: 4.9401\n",
      "Epoch [2/2], Step [24150/64305], Loss: 4.7803\n",
      "Epoch [2/2], Step [24160/64305], Loss: 4.6068\n",
      "Epoch [2/2], Step [24170/64305], Loss: 4.7791\n",
      "Epoch [2/2], Step [24180/64305], Loss: 4.7323\n",
      "Epoch [2/2], Step [24190/64305], Loss: 4.6574\n",
      "Epoch [2/2], Step [24200/64305], Loss: 4.7307\n",
      "Epoch [2/2], Step [24210/64305], Loss: 4.5229\n",
      "Epoch [2/2], Step [24220/64305], Loss: 4.7675\n",
      "Epoch [2/2], Step [24230/64305], Loss: 4.8634\n",
      "Epoch [2/2], Step [24240/64305], Loss: 4.7574\n",
      "Epoch [2/2], Step [24250/64305], Loss: 4.8701\n",
      "Epoch [2/2], Step [24260/64305], Loss: 4.6255\n",
      "Epoch [2/2], Step [24270/64305], Loss: 4.7636\n",
      "Epoch [2/2], Step [24280/64305], Loss: 4.7686\n",
      "Epoch [2/2], Step [24290/64305], Loss: 4.7266\n",
      "Epoch [2/2], Step [24300/64305], Loss: 4.6468\n",
      "Epoch [2/2], Step [24310/64305], Loss: 4.9132\n",
      "Epoch [2/2], Step [24320/64305], Loss: 4.7839\n",
      "Epoch [2/2], Step [24330/64305], Loss: 4.6757\n",
      "Epoch [2/2], Step [24340/64305], Loss: 4.7220\n",
      "Epoch [2/2], Step [24350/64305], Loss: 4.8755\n",
      "Epoch [2/2], Step [24360/64305], Loss: 4.9851\n",
      "Epoch [2/2], Step [24370/64305], Loss: 4.6556\n",
      "Epoch [2/2], Step [24380/64305], Loss: 4.4922\n",
      "Epoch [2/2], Step [24390/64305], Loss: 4.9010\n",
      "Epoch [2/2], Step [24400/64305], Loss: 4.7483\n",
      "Epoch [2/2], Step [24410/64305], Loss: 4.8678\n",
      "Epoch [2/2], Step [24420/64305], Loss: 4.7600\n",
      "Epoch [2/2], Step [24430/64305], Loss: 4.7124\n",
      "Epoch [2/2], Step [24440/64305], Loss: 4.7842\n",
      "Epoch [2/2], Step [24450/64305], Loss: 4.6928\n",
      "Epoch [2/2], Step [24460/64305], Loss: 4.6744\n",
      "Epoch [2/2], Step [24470/64305], Loss: 4.7162\n",
      "Epoch [2/2], Step [24480/64305], Loss: 4.8443\n",
      "Epoch [2/2], Step [24490/64305], Loss: 4.7243\n",
      "Epoch [2/2], Step [24500/64305], Loss: 4.5896\n",
      "Epoch [2/2], Step [24510/64305], Loss: 4.7421\n",
      "Epoch [2/2], Step [24520/64305], Loss: 4.8798\n",
      "Epoch [2/2], Step [24530/64305], Loss: 4.6970\n",
      "Epoch [2/2], Step [24540/64305], Loss: 4.8664\n",
      "Epoch [2/2], Step [24550/64305], Loss: 4.8464\n",
      "Epoch [2/2], Step [24560/64305], Loss: 4.8394\n",
      "Epoch [2/2], Step [24570/64305], Loss: 4.7745\n",
      "Epoch [2/2], Step [24580/64305], Loss: 4.8542\n",
      "Epoch [2/2], Step [24590/64305], Loss: 4.6377\n",
      "Epoch [2/2], Step [24600/64305], Loss: 4.8735\n",
      "Epoch [2/2], Step [24610/64305], Loss: 5.0605\n",
      "Epoch [2/2], Step [24620/64305], Loss: 4.8646\n",
      "Epoch [2/2], Step [24630/64305], Loss: 4.8358\n",
      "Epoch [2/2], Step [24640/64305], Loss: 4.8424\n",
      "Epoch [2/2], Step [24650/64305], Loss: 4.7709\n",
      "Epoch [2/2], Step [24660/64305], Loss: 4.8254\n",
      "Epoch [2/2], Step [24670/64305], Loss: 4.7613\n",
      "Epoch [2/2], Step [24680/64305], Loss: 4.7271\n",
      "Epoch [2/2], Step [24690/64305], Loss: 4.8335\n",
      "Epoch [2/2], Step [24700/64305], Loss: 4.8423\n",
      "Epoch [2/2], Step [24710/64305], Loss: 4.6440\n",
      "Epoch [2/2], Step [24720/64305], Loss: 4.9180\n",
      "Epoch [2/2], Step [24730/64305], Loss: 4.7415\n",
      "Epoch [2/2], Step [24740/64305], Loss: 4.7549\n",
      "Epoch [2/2], Step [24750/64305], Loss: 4.7727\n",
      "Epoch [2/2], Step [24760/64305], Loss: 4.7859\n",
      "Epoch [2/2], Step [24770/64305], Loss: 4.9028\n",
      "Epoch [2/2], Step [24780/64305], Loss: 4.8314\n",
      "Epoch [2/2], Step [24790/64305], Loss: 4.7140\n",
      "Epoch [2/2], Step [24800/64305], Loss: 4.7151\n",
      "Epoch [2/2], Step [24810/64305], Loss: 4.8161\n",
      "Epoch [2/2], Step [24820/64305], Loss: 4.6865\n",
      "Epoch [2/2], Step [24830/64305], Loss: 4.6072\n",
      "Epoch [2/2], Step [24840/64305], Loss: 4.7226\n",
      "Epoch [2/2], Step [24850/64305], Loss: 4.8903\n",
      "Epoch [2/2], Step [24860/64305], Loss: 4.9405\n",
      "Epoch [2/2], Step [24870/64305], Loss: 4.6720\n",
      "Epoch [2/2], Step [24880/64305], Loss: 4.7849\n",
      "Epoch [2/2], Step [24890/64305], Loss: 4.8362\n",
      "Epoch [2/2], Step [24900/64305], Loss: 4.8725\n",
      "Epoch [2/2], Step [24910/64305], Loss: 4.8822\n",
      "Epoch [2/2], Step [24920/64305], Loss: 4.6153\n",
      "Epoch [2/2], Step [24930/64305], Loss: 4.7802\n",
      "Epoch [2/2], Step [24940/64305], Loss: 4.7509\n",
      "Epoch [2/2], Step [24950/64305], Loss: 4.7586\n",
      "Epoch [2/2], Step [24960/64305], Loss: 4.7599\n",
      "Epoch [2/2], Step [24970/64305], Loss: 4.9090\n",
      "Epoch [2/2], Step [24980/64305], Loss: 4.6858\n",
      "Epoch [2/2], Step [24990/64305], Loss: 4.7592\n",
      "Epoch [2/2], Step [25000/64305], Loss: 4.8713\n",
      "Epoch [2/2], Step [25010/64305], Loss: 4.7575\n",
      "Epoch [2/2], Step [25020/64305], Loss: 4.7758\n",
      "Epoch [2/2], Step [25030/64305], Loss: 4.7773\n",
      "Epoch [2/2], Step [25040/64305], Loss: 4.9319\n",
      "Epoch [2/2], Step [25050/64305], Loss: 4.7552\n",
      "Epoch [2/2], Step [25060/64305], Loss: 4.7673\n",
      "Epoch [2/2], Step [25070/64305], Loss: 4.5749\n",
      "Epoch [2/2], Step [25080/64305], Loss: 4.6278\n",
      "Epoch [2/2], Step [25090/64305], Loss: 4.6806\n",
      "Epoch [2/2], Step [25100/64305], Loss: 4.6917\n",
      "Epoch [2/2], Step [25110/64305], Loss: 4.8040\n",
      "Epoch [2/2], Step [25120/64305], Loss: 4.8241\n",
      "Epoch [2/2], Step [25130/64305], Loss: 4.7054\n",
      "Epoch [2/2], Step [25140/64305], Loss: 4.6382\n",
      "Epoch [2/2], Step [25150/64305], Loss: 4.6006\n",
      "Epoch [2/2], Step [25160/64305], Loss: 4.8187\n",
      "Epoch [2/2], Step [25170/64305], Loss: 4.8371\n",
      "Epoch [2/2], Step [25180/64305], Loss: 4.8372\n",
      "Epoch [2/2], Step [25190/64305], Loss: 4.7348\n",
      "Epoch [2/2], Step [25200/64305], Loss: 4.9364\n",
      "Epoch [2/2], Step [25210/64305], Loss: 4.7219\n",
      "Epoch [2/2], Step [25220/64305], Loss: 4.9315\n",
      "Epoch [2/2], Step [25230/64305], Loss: 4.7953\n",
      "Epoch [2/2], Step [25240/64305], Loss: 4.9234\n",
      "Epoch [2/2], Step [25250/64305], Loss: 4.7390\n",
      "Epoch [2/2], Step [25260/64305], Loss: 4.7056\n",
      "Epoch [2/2], Step [25270/64305], Loss: 5.0045\n",
      "Epoch [2/2], Step [25280/64305], Loss: 4.9586\n",
      "Epoch [2/2], Step [25290/64305], Loss: 4.6531\n",
      "Epoch [2/2], Step [25300/64305], Loss: 4.7737\n",
      "Epoch [2/2], Step [25310/64305], Loss: 4.6601\n",
      "Epoch [2/2], Step [25320/64305], Loss: 4.8349\n",
      "Epoch [2/2], Step [25330/64305], Loss: 4.8282\n",
      "Epoch [2/2], Step [25340/64305], Loss: 4.8321\n",
      "Epoch [2/2], Step [25350/64305], Loss: 4.5762\n",
      "Epoch [2/2], Step [25360/64305], Loss: 4.6626\n",
      "Epoch [2/2], Step [25370/64305], Loss: 4.8133\n",
      "Epoch [2/2], Step [25380/64305], Loss: 4.9157\n",
      "Epoch [2/2], Step [25390/64305], Loss: 4.9431\n",
      "Epoch [2/2], Step [25400/64305], Loss: 4.8465\n",
      "Epoch [2/2], Step [25410/64305], Loss: 4.9173\n",
      "Epoch [2/2], Step [25420/64305], Loss: 4.6497\n",
      "Epoch [2/2], Step [25430/64305], Loss: 4.6621\n",
      "Epoch [2/2], Step [25440/64305], Loss: 4.5917\n",
      "Epoch [2/2], Step [25450/64305], Loss: 5.0409\n",
      "Epoch [2/2], Step [25460/64305], Loss: 4.6309\n",
      "Epoch [2/2], Step [25470/64305], Loss: 4.6283\n",
      "Epoch [2/2], Step [25480/64305], Loss: 4.7303\n",
      "Epoch [2/2], Step [25490/64305], Loss: 4.6942\n",
      "Epoch [2/2], Step [25500/64305], Loss: 4.7079\n",
      "Epoch [2/2], Step [25510/64305], Loss: 4.8094\n",
      "Epoch [2/2], Step [25520/64305], Loss: 4.7779\n",
      "Epoch [2/2], Step [25530/64305], Loss: 4.6564\n",
      "Epoch [2/2], Step [25540/64305], Loss: 4.7540\n",
      "Epoch [2/2], Step [25550/64305], Loss: 4.7084\n",
      "Epoch [2/2], Step [25560/64305], Loss: 4.5894\n",
      "Epoch [2/2], Step [25570/64305], Loss: 4.7662\n",
      "Epoch [2/2], Step [25580/64305], Loss: 4.9146\n",
      "Epoch [2/2], Step [25590/64305], Loss: 4.8464\n",
      "Epoch [2/2], Step [25600/64305], Loss: 4.8786\n",
      "Epoch [2/2], Step [25610/64305], Loss: 4.7469\n",
      "Epoch [2/2], Step [25620/64305], Loss: 4.8397\n",
      "Epoch [2/2], Step [25630/64305], Loss: 4.6808\n",
      "Epoch [2/2], Step [25640/64305], Loss: 4.7460\n",
      "Epoch [2/2], Step [25650/64305], Loss: 4.9571\n",
      "Epoch [2/2], Step [25660/64305], Loss: 4.6846\n",
      "Epoch [2/2], Step [25670/64305], Loss: 4.8178\n",
      "Epoch [2/2], Step [25680/64305], Loss: 4.6654\n",
      "Epoch [2/2], Step [25690/64305], Loss: 4.8630\n",
      "Epoch [2/2], Step [25700/64305], Loss: 4.6596\n",
      "Epoch [2/2], Step [25710/64305], Loss: 4.6458\n",
      "Epoch [2/2], Step [25720/64305], Loss: 4.6364\n",
      "Epoch [2/2], Step [25730/64305], Loss: 4.8771\n",
      "Epoch [2/2], Step [25740/64305], Loss: 4.7096\n",
      "Epoch [2/2], Step [25750/64305], Loss: 4.7902\n",
      "Epoch [2/2], Step [25760/64305], Loss: 4.6534\n",
      "Epoch [2/2], Step [25770/64305], Loss: 4.8755\n",
      "Epoch [2/2], Step [25780/64305], Loss: 4.6540\n",
      "Epoch [2/2], Step [25790/64305], Loss: 4.8237\n",
      "Epoch [2/2], Step [25800/64305], Loss: 4.6904\n",
      "Epoch [2/2], Step [25810/64305], Loss: 4.6503\n",
      "Epoch [2/2], Step [25820/64305], Loss: 4.8210\n",
      "Epoch [2/2], Step [25830/64305], Loss: 4.9917\n",
      "Epoch [2/2], Step [25840/64305], Loss: 4.7276\n",
      "Epoch [2/2], Step [25850/64305], Loss: 4.6298\n",
      "Epoch [2/2], Step [25860/64305], Loss: 4.7577\n",
      "Epoch [2/2], Step [25870/64305], Loss: 4.8610\n",
      "Epoch [2/2], Step [25880/64305], Loss: 4.6630\n",
      "Epoch [2/2], Step [25890/64305], Loss: 4.9778\n",
      "Epoch [2/2], Step [25900/64305], Loss: 4.4764\n",
      "Epoch [2/2], Step [25910/64305], Loss: 4.7484\n",
      "Epoch [2/2], Step [25920/64305], Loss: 4.7237\n",
      "Epoch [2/2], Step [25930/64305], Loss: 4.5922\n",
      "Epoch [2/2], Step [25940/64305], Loss: 4.7192\n",
      "Epoch [2/2], Step [25950/64305], Loss: 4.7898\n",
      "Epoch [2/2], Step [25960/64305], Loss: 4.8004\n",
      "Epoch [2/2], Step [25970/64305], Loss: 4.6944\n",
      "Epoch [2/2], Step [25980/64305], Loss: 4.5556\n",
      "Epoch [2/2], Step [25990/64305], Loss: 4.7310\n",
      "Epoch [2/2], Step [26000/64305], Loss: 4.6301\n",
      "Epoch [2/2], Step [26010/64305], Loss: 4.8573\n",
      "Epoch [2/2], Step [26020/64305], Loss: 4.5807\n",
      "Epoch [2/2], Step [26030/64305], Loss: 4.7795\n",
      "Epoch [2/2], Step [26040/64305], Loss: 4.8089\n",
      "Epoch [2/2], Step [26050/64305], Loss: 4.6008\n",
      "Epoch [2/2], Step [26060/64305], Loss: 4.6442\n",
      "Epoch [2/2], Step [26070/64305], Loss: 4.7695\n",
      "Epoch [2/2], Step [26080/64305], Loss: 4.7775\n",
      "Epoch [2/2], Step [26090/64305], Loss: 4.7935\n",
      "Epoch [2/2], Step [26100/64305], Loss: 4.7104\n",
      "Epoch [2/2], Step [26110/64305], Loss: 4.7726\n",
      "Epoch [2/2], Step [26120/64305], Loss: 4.8607\n",
      "Epoch [2/2], Step [26130/64305], Loss: 4.8793\n",
      "Epoch [2/2], Step [26140/64305], Loss: 4.6346\n",
      "Epoch [2/2], Step [26150/64305], Loss: 4.7009\n",
      "Epoch [2/2], Step [26160/64305], Loss: 4.7553\n",
      "Epoch [2/2], Step [26170/64305], Loss: 4.8191\n",
      "Epoch [2/2], Step [26180/64305], Loss: 4.7663\n",
      "Epoch [2/2], Step [26190/64305], Loss: 4.9100\n",
      "Epoch [2/2], Step [26200/64305], Loss: 4.6106\n",
      "Epoch [2/2], Step [26210/64305], Loss: 4.7810\n",
      "Epoch [2/2], Step [26220/64305], Loss: 4.6380\n",
      "Epoch [2/2], Step [26230/64305], Loss: 4.9280\n",
      "Epoch [2/2], Step [26240/64305], Loss: 4.7462\n",
      "Epoch [2/2], Step [26250/64305], Loss: 4.8625\n",
      "Epoch [2/2], Step [26260/64305], Loss: 4.7707\n",
      "Epoch [2/2], Step [26270/64305], Loss: 4.8986\n",
      "Epoch [2/2], Step [26280/64305], Loss: 4.6879\n",
      "Epoch [2/2], Step [26290/64305], Loss: 4.7909\n",
      "Epoch [2/2], Step [26300/64305], Loss: 4.8976\n",
      "Epoch [2/2], Step [26310/64305], Loss: 4.4829\n",
      "Epoch [2/2], Step [26320/64305], Loss: 4.6288\n",
      "Epoch [2/2], Step [26330/64305], Loss: 4.6895\n",
      "Epoch [2/2], Step [26340/64305], Loss: 4.7496\n",
      "Epoch [2/2], Step [26350/64305], Loss: 4.8078\n",
      "Epoch [2/2], Step [26360/64305], Loss: 4.8152\n",
      "Epoch [2/2], Step [26370/64305], Loss: 4.7154\n",
      "Epoch [2/2], Step [26380/64305], Loss: 4.7333\n",
      "Epoch [2/2], Step [26390/64305], Loss: 4.8025\n",
      "Epoch [2/2], Step [26400/64305], Loss: 4.8392\n",
      "Epoch [2/2], Step [26410/64305], Loss: 4.5195\n",
      "Epoch [2/2], Step [26420/64305], Loss: 4.9643\n",
      "Epoch [2/2], Step [26430/64305], Loss: 4.5543\n",
      "Epoch [2/2], Step [26440/64305], Loss: 4.9791\n",
      "Epoch [2/2], Step [26450/64305], Loss: 4.7055\n",
      "Epoch [2/2], Step [26460/64305], Loss: 4.6882\n",
      "Epoch [2/2], Step [26470/64305], Loss: 4.6998\n",
      "Epoch [2/2], Step [26480/64305], Loss: 4.8450\n",
      "Epoch [2/2], Step [26490/64305], Loss: 4.7919\n",
      "Epoch [2/2], Step [26500/64305], Loss: 4.9123\n",
      "Epoch [2/2], Step [26510/64305], Loss: 4.8898\n",
      "Epoch [2/2], Step [26520/64305], Loss: 4.7063\n",
      "Epoch [2/2], Step [26530/64305], Loss: 4.6849\n",
      "Epoch [2/2], Step [26540/64305], Loss: 4.7550\n",
      "Epoch [2/2], Step [26550/64305], Loss: 4.5809\n",
      "Epoch [2/2], Step [26560/64305], Loss: 4.6155\n",
      "Epoch [2/2], Step [26570/64305], Loss: 4.9122\n",
      "Epoch [2/2], Step [26580/64305], Loss: 4.8959\n",
      "Epoch [2/2], Step [26590/64305], Loss: 4.9343\n",
      "Epoch [2/2], Step [26600/64305], Loss: 4.8796\n",
      "Epoch [2/2], Step [26610/64305], Loss: 4.7480\n",
      "Epoch [2/2], Step [26620/64305], Loss: 4.7769\n",
      "Epoch [2/2], Step [26630/64305], Loss: 4.8648\n",
      "Epoch [2/2], Step [26640/64305], Loss: 4.4889\n",
      "Epoch [2/2], Step [26650/64305], Loss: 4.8488\n",
      "Epoch [2/2], Step [26660/64305], Loss: 4.8974\n",
      "Epoch [2/2], Step [26670/64305], Loss: 4.6385\n",
      "Epoch [2/2], Step [26680/64305], Loss: 4.7065\n",
      "Epoch [2/2], Step [26690/64305], Loss: 4.5256\n",
      "Epoch [2/2], Step [26700/64305], Loss: 4.8347\n",
      "Epoch [2/2], Step [26710/64305], Loss: 4.9741\n",
      "Epoch [2/2], Step [26720/64305], Loss: 4.4106\n",
      "Epoch [2/2], Step [26730/64305], Loss: 4.8509\n",
      "Epoch [2/2], Step [26740/64305], Loss: 4.7050\n",
      "Epoch [2/2], Step [26750/64305], Loss: 4.6330\n",
      "Epoch [2/2], Step [26760/64305], Loss: 4.7229\n",
      "Epoch [2/2], Step [26770/64305], Loss: 4.8082\n",
      "Epoch [2/2], Step [26780/64305], Loss: 4.8153\n",
      "Epoch [2/2], Step [26790/64305], Loss: 4.6551\n",
      "Epoch [2/2], Step [26800/64305], Loss: 4.9345\n",
      "Epoch [2/2], Step [26810/64305], Loss: 4.6818\n",
      "Epoch [2/2], Step [26820/64305], Loss: 4.7350\n",
      "Epoch [2/2], Step [26830/64305], Loss: 4.5962\n",
      "Epoch [2/2], Step [26840/64305], Loss: 4.7780\n",
      "Epoch [2/2], Step [26850/64305], Loss: 4.5878\n",
      "Epoch [2/2], Step [26860/64305], Loss: 4.6851\n",
      "Epoch [2/2], Step [26870/64305], Loss: 4.6824\n",
      "Epoch [2/2], Step [26880/64305], Loss: 4.7323\n",
      "Epoch [2/2], Step [26890/64305], Loss: 4.6970\n",
      "Epoch [2/2], Step [26900/64305], Loss: 4.6364\n",
      "Epoch [2/2], Step [26910/64305], Loss: 4.6043\n",
      "Epoch [2/2], Step [26920/64305], Loss: 4.8084\n",
      "Epoch [2/2], Step [26930/64305], Loss: 4.7127\n",
      "Epoch [2/2], Step [26940/64305], Loss: 4.7234\n",
      "Epoch [2/2], Step [26950/64305], Loss: 4.6831\n",
      "Epoch [2/2], Step [26960/64305], Loss: 4.6548\n",
      "Epoch [2/2], Step [26970/64305], Loss: 4.6059\n",
      "Epoch [2/2], Step [26980/64305], Loss: 4.8289\n",
      "Epoch [2/2], Step [26990/64305], Loss: 4.8864\n",
      "Epoch [2/2], Step [27000/64305], Loss: 4.5782\n",
      "Epoch [2/2], Step [27010/64305], Loss: 4.9698\n",
      "Epoch [2/2], Step [27020/64305], Loss: 4.7953\n",
      "Epoch [2/2], Step [27030/64305], Loss: 4.8345\n",
      "Epoch [2/2], Step [27040/64305], Loss: 4.7578\n",
      "Epoch [2/2], Step [27050/64305], Loss: 4.9602\n",
      "Epoch [2/2], Step [27060/64305], Loss: 4.9806\n",
      "Epoch [2/2], Step [27070/64305], Loss: 4.8536\n",
      "Epoch [2/2], Step [27080/64305], Loss: 4.6985\n",
      "Epoch [2/2], Step [27090/64305], Loss: 4.6058\n",
      "Epoch [2/2], Step [27100/64305], Loss: 4.8097\n",
      "Epoch [2/2], Step [27110/64305], Loss: 4.7879\n",
      "Epoch [2/2], Step [27120/64305], Loss: 4.7626\n",
      "Epoch [2/2], Step [27130/64305], Loss: 4.5288\n",
      "Epoch [2/2], Step [27140/64305], Loss: 4.7440\n",
      "Epoch [2/2], Step [27150/64305], Loss: 4.8960\n",
      "Epoch [2/2], Step [27160/64305], Loss: 4.8777\n",
      "Epoch [2/2], Step [27170/64305], Loss: 4.8056\n",
      "Epoch [2/2], Step [27180/64305], Loss: 5.0197\n",
      "Epoch [2/2], Step [27190/64305], Loss: 4.8964\n",
      "Epoch [2/2], Step [27200/64305], Loss: 4.7233\n",
      "Epoch [2/2], Step [27210/64305], Loss: 4.5804\n",
      "Epoch [2/2], Step [27220/64305], Loss: 4.6093\n",
      "Epoch [2/2], Step [27230/64305], Loss: 4.8866\n",
      "Epoch [2/2], Step [27240/64305], Loss: 4.8265\n",
      "Epoch [2/2], Step [27250/64305], Loss: 4.4888\n",
      "Epoch [2/2], Step [27260/64305], Loss: 4.8159\n",
      "Epoch [2/2], Step [27270/64305], Loss: 4.7976\n",
      "Epoch [2/2], Step [27280/64305], Loss: 4.7717\n",
      "Epoch [2/2], Step [27290/64305], Loss: 4.8883\n",
      "Epoch [2/2], Step [27300/64305], Loss: 4.7895\n",
      "Epoch [2/2], Step [27310/64305], Loss: 4.7351\n",
      "Epoch [2/2], Step [27320/64305], Loss: 4.7960\n",
      "Epoch [2/2], Step [27330/64305], Loss: 4.6838\n",
      "Epoch [2/2], Step [27340/64305], Loss: 4.7787\n",
      "Epoch [2/2], Step [27350/64305], Loss: 4.9103\n",
      "Epoch [2/2], Step [27360/64305], Loss: 4.7427\n",
      "Epoch [2/2], Step [27370/64305], Loss: 4.8807\n",
      "Epoch [2/2], Step [27380/64305], Loss: 4.7848\n",
      "Epoch [2/2], Step [27390/64305], Loss: 4.6710\n",
      "Epoch [2/2], Step [27400/64305], Loss: 5.0588\n",
      "Epoch [2/2], Step [27410/64305], Loss: 4.5839\n",
      "Epoch [2/2], Step [27420/64305], Loss: 4.7640\n",
      "Epoch [2/2], Step [27430/64305], Loss: 4.6868\n",
      "Epoch [2/2], Step [27440/64305], Loss: 4.8625\n",
      "Epoch [2/2], Step [27450/64305], Loss: 4.9190\n",
      "Epoch [2/2], Step [27460/64305], Loss: 4.7348\n",
      "Epoch [2/2], Step [27470/64305], Loss: 4.6982\n",
      "Epoch [2/2], Step [27480/64305], Loss: 4.5631\n",
      "Epoch [2/2], Step [27490/64305], Loss: 4.8531\n",
      "Epoch [2/2], Step [27500/64305], Loss: 4.9059\n",
      "Epoch [2/2], Step [27510/64305], Loss: 4.9167\n",
      "Epoch [2/2], Step [27520/64305], Loss: 4.8428\n",
      "Epoch [2/2], Step [27530/64305], Loss: 4.9264\n",
      "Epoch [2/2], Step [27540/64305], Loss: 4.7737\n",
      "Epoch [2/2], Step [27550/64305], Loss: 4.6978\n",
      "Epoch [2/2], Step [27560/64305], Loss: 4.9619\n",
      "Epoch [2/2], Step [27570/64305], Loss: 4.6387\n",
      "Epoch [2/2], Step [27580/64305], Loss: 4.8793\n",
      "Epoch [2/2], Step [27590/64305], Loss: 4.7497\n",
      "Epoch [2/2], Step [27600/64305], Loss: 4.6091\n",
      "Epoch [2/2], Step [27610/64305], Loss: 4.5913\n",
      "Epoch [2/2], Step [27620/64305], Loss: 4.8134\n",
      "Epoch [2/2], Step [27630/64305], Loss: 4.7855\n",
      "Epoch [2/2], Step [27640/64305], Loss: 4.8724\n",
      "Epoch [2/2], Step [27650/64305], Loss: 4.8425\n",
      "Epoch [2/2], Step [27660/64305], Loss: 4.9270\n",
      "Epoch [2/2], Step [27670/64305], Loss: 4.5920\n",
      "Epoch [2/2], Step [27680/64305], Loss: 4.7739\n",
      "Epoch [2/2], Step [27690/64305], Loss: 4.8256\n",
      "Epoch [2/2], Step [27700/64305], Loss: 4.9922\n",
      "Epoch [2/2], Step [27710/64305], Loss: 4.8419\n",
      "Epoch [2/2], Step [27720/64305], Loss: 4.8754\n",
      "Epoch [2/2], Step [27730/64305], Loss: 4.7368\n",
      "Epoch [2/2], Step [27740/64305], Loss: 4.7070\n",
      "Epoch [2/2], Step [27750/64305], Loss: 4.7997\n",
      "Epoch [2/2], Step [27760/64305], Loss: 4.6075\n",
      "Epoch [2/2], Step [27770/64305], Loss: 4.7107\n",
      "Epoch [2/2], Step [27780/64305], Loss: 4.8997\n",
      "Epoch [2/2], Step [27790/64305], Loss: 4.8084\n",
      "Epoch [2/2], Step [27800/64305], Loss: 4.8803\n",
      "Epoch [2/2], Step [27810/64305], Loss: 4.7473\n",
      "Epoch [2/2], Step [27820/64305], Loss: 4.8878\n",
      "Epoch [2/2], Step [27830/64305], Loss: 4.8613\n",
      "Epoch [2/2], Step [27840/64305], Loss: 4.8172\n",
      "Epoch [2/2], Step [27850/64305], Loss: 4.9420\n",
      "Epoch [2/2], Step [27860/64305], Loss: 4.6405\n",
      "Epoch [2/2], Step [27870/64305], Loss: 4.8367\n",
      "Epoch [2/2], Step [27880/64305], Loss: 4.8320\n",
      "Epoch [2/2], Step [27890/64305], Loss: 4.7473\n",
      "Epoch [2/2], Step [27900/64305], Loss: 4.5607\n",
      "Epoch [2/2], Step [27910/64305], Loss: 5.0091\n",
      "Epoch [2/2], Step [27920/64305], Loss: 4.8929\n",
      "Epoch [2/2], Step [27930/64305], Loss: 4.8059\n",
      "Epoch [2/2], Step [27940/64305], Loss: 4.8312\n",
      "Epoch [2/2], Step [27950/64305], Loss: 4.8565\n",
      "Epoch [2/2], Step [27960/64305], Loss: 4.7508\n",
      "Epoch [2/2], Step [27970/64305], Loss: 4.6598\n",
      "Epoch [2/2], Step [27980/64305], Loss: 4.5938\n",
      "Epoch [2/2], Step [27990/64305], Loss: 4.7528\n",
      "Epoch [2/2], Step [28000/64305], Loss: 4.7958\n",
      "Epoch [2/2], Step [28010/64305], Loss: 4.7468\n",
      "Epoch [2/2], Step [28020/64305], Loss: 4.6886\n",
      "Epoch [2/2], Step [28030/64305], Loss: 4.9297\n",
      "Epoch [2/2], Step [28040/64305], Loss: 4.6955\n",
      "Epoch [2/2], Step [28050/64305], Loss: 4.7622\n",
      "Epoch [2/2], Step [28060/64305], Loss: 4.6882\n",
      "Epoch [2/2], Step [28070/64305], Loss: 4.7112\n",
      "Epoch [2/2], Step [28080/64305], Loss: 4.5986\n",
      "Epoch [2/2], Step [28090/64305], Loss: 4.6407\n",
      "Epoch [2/2], Step [28100/64305], Loss: 4.6087\n",
      "Epoch [2/2], Step [28110/64305], Loss: 4.7695\n",
      "Epoch [2/2], Step [28120/64305], Loss: 4.8825\n",
      "Epoch [2/2], Step [28130/64305], Loss: 4.6451\n",
      "Epoch [2/2], Step [28140/64305], Loss: 4.6622\n",
      "Epoch [2/2], Step [28150/64305], Loss: 4.7077\n",
      "Epoch [2/2], Step [28160/64305], Loss: 4.6568\n",
      "Epoch [2/2], Step [28170/64305], Loss: 4.7222\n",
      "Epoch [2/2], Step [28180/64305], Loss: 4.5963\n",
      "Epoch [2/2], Step [28190/64305], Loss: 4.8893\n",
      "Epoch [2/2], Step [28200/64305], Loss: 4.6421\n",
      "Epoch [2/2], Step [28210/64305], Loss: 4.9373\n",
      "Epoch [2/2], Step [28220/64305], Loss: 4.8318\n",
      "Epoch [2/2], Step [28230/64305], Loss: 4.7693\n",
      "Epoch [2/2], Step [28240/64305], Loss: 4.8481\n",
      "Epoch [2/2], Step [28250/64305], Loss: 4.7859\n",
      "Epoch [2/2], Step [28260/64305], Loss: 4.6154\n",
      "Epoch [2/2], Step [28270/64305], Loss: 4.5389\n",
      "Epoch [2/2], Step [28280/64305], Loss: 4.7222\n",
      "Epoch [2/2], Step [28290/64305], Loss: 4.9618\n",
      "Epoch [2/2], Step [28300/64305], Loss: 4.6359\n",
      "Epoch [2/2], Step [28310/64305], Loss: 4.8664\n",
      "Epoch [2/2], Step [28320/64305], Loss: 4.8805\n",
      "Epoch [2/2], Step [28330/64305], Loss: 4.6406\n",
      "Epoch [2/2], Step [28340/64305], Loss: 4.8903\n",
      "Epoch [2/2], Step [28350/64305], Loss: 4.7348\n",
      "Epoch [2/2], Step [28360/64305], Loss: 4.8625\n",
      "Epoch [2/2], Step [28370/64305], Loss: 4.9772\n",
      "Epoch [2/2], Step [28380/64305], Loss: 4.8944\n",
      "Epoch [2/2], Step [28390/64305], Loss: 4.6249\n",
      "Epoch [2/2], Step [28400/64305], Loss: 4.7199\n",
      "Epoch [2/2], Step [28410/64305], Loss: 4.7116\n",
      "Epoch [2/2], Step [28420/64305], Loss: 4.7607\n",
      "Epoch [2/2], Step [28430/64305], Loss: 4.8502\n",
      "Epoch [2/2], Step [28440/64305], Loss: 4.6947\n",
      "Epoch [2/2], Step [28450/64305], Loss: 4.7996\n",
      "Epoch [2/2], Step [28460/64305], Loss: 4.7817\n",
      "Epoch [2/2], Step [28470/64305], Loss: 4.7929\n",
      "Epoch [2/2], Step [28480/64305], Loss: 4.7885\n",
      "Epoch [2/2], Step [28490/64305], Loss: 4.6144\n",
      "Epoch [2/2], Step [28500/64305], Loss: 4.6377\n",
      "Epoch [2/2], Step [28510/64305], Loss: 4.6744\n",
      "Epoch [2/2], Step [28520/64305], Loss: 4.6147\n",
      "Epoch [2/2], Step [28530/64305], Loss: 4.6492\n",
      "Epoch [2/2], Step [28540/64305], Loss: 4.6898\n",
      "Epoch [2/2], Step [28550/64305], Loss: 4.6757\n",
      "Epoch [2/2], Step [28560/64305], Loss: 4.7108\n",
      "Epoch [2/2], Step [28570/64305], Loss: 4.8541\n",
      "Epoch [2/2], Step [28580/64305], Loss: 4.6200\n",
      "Epoch [2/2], Step [28590/64305], Loss: 5.0417\n",
      "Epoch [2/2], Step [28600/64305], Loss: 4.7994\n",
      "Epoch [2/2], Step [28610/64305], Loss: 4.7112\n",
      "Epoch [2/2], Step [28620/64305], Loss: 4.7690\n",
      "Epoch [2/2], Step [28630/64305], Loss: 4.7889\n",
      "Epoch [2/2], Step [28640/64305], Loss: 4.9086\n",
      "Epoch [2/2], Step [28650/64305], Loss: 4.6952\n",
      "Epoch [2/2], Step [28660/64305], Loss: 4.6738\n",
      "Epoch [2/2], Step [28670/64305], Loss: 4.8693\n",
      "Epoch [2/2], Step [28680/64305], Loss: 4.8791\n",
      "Epoch [2/2], Step [28690/64305], Loss: 4.9655\n",
      "Epoch [2/2], Step [28700/64305], Loss: 4.7515\n",
      "Epoch [2/2], Step [28710/64305], Loss: 4.8713\n",
      "Epoch [2/2], Step [28720/64305], Loss: 4.7892\n",
      "Epoch [2/2], Step [28730/64305], Loss: 4.7504\n",
      "Epoch [2/2], Step [28740/64305], Loss: 4.7119\n",
      "Epoch [2/2], Step [28750/64305], Loss: 4.7015\n",
      "Epoch [2/2], Step [28760/64305], Loss: 4.6928\n",
      "Epoch [2/2], Step [28770/64305], Loss: 4.6521\n",
      "Epoch [2/2], Step [28780/64305], Loss: 4.6804\n",
      "Epoch [2/2], Step [28790/64305], Loss: 4.9332\n",
      "Epoch [2/2], Step [28800/64305], Loss: 4.6096\n",
      "Epoch [2/2], Step [28810/64305], Loss: 4.7489\n",
      "Epoch [2/2], Step [28820/64305], Loss: 4.8592\n",
      "Epoch [2/2], Step [28830/64305], Loss: 4.8529\n",
      "Epoch [2/2], Step [28840/64305], Loss: 4.9510\n",
      "Epoch [2/2], Step [28850/64305], Loss: 4.8435\n",
      "Epoch [2/2], Step [28860/64305], Loss: 4.6703\n",
      "Epoch [2/2], Step [28870/64305], Loss: 4.5855\n",
      "Epoch [2/2], Step [28880/64305], Loss: 4.6408\n",
      "Epoch [2/2], Step [28890/64305], Loss: 4.7350\n",
      "Epoch [2/2], Step [28900/64305], Loss: 4.6992\n",
      "Epoch [2/2], Step [28910/64305], Loss: 4.7853\n",
      "Epoch [2/2], Step [28920/64305], Loss: 4.9283\n",
      "Epoch [2/2], Step [28930/64305], Loss: 4.6963\n",
      "Epoch [2/2], Step [28940/64305], Loss: 4.8853\n",
      "Epoch [2/2], Step [28950/64305], Loss: 4.7182\n",
      "Epoch [2/2], Step [28960/64305], Loss: 4.6813\n",
      "Epoch [2/2], Step [28970/64305], Loss: 4.6341\n",
      "Epoch [2/2], Step [28980/64305], Loss: 4.6460\n",
      "Epoch [2/2], Step [28990/64305], Loss: 4.7506\n",
      "Epoch [2/2], Step [29000/64305], Loss: 4.6215\n",
      "Epoch [2/2], Step [29010/64305], Loss: 4.7383\n",
      "Epoch [2/2], Step [29020/64305], Loss: 4.7111\n",
      "Epoch [2/2], Step [29030/64305], Loss: 4.8065\n",
      "Epoch [2/2], Step [29040/64305], Loss: 4.5566\n",
      "Epoch [2/2], Step [29050/64305], Loss: 4.9434\n",
      "Epoch [2/2], Step [29060/64305], Loss: 4.8122\n",
      "Epoch [2/2], Step [29070/64305], Loss: 4.8270\n",
      "Epoch [2/2], Step [29080/64305], Loss: 4.5818\n",
      "Epoch [2/2], Step [29090/64305], Loss: 4.8470\n",
      "Epoch [2/2], Step [29100/64305], Loss: 4.7632\n",
      "Epoch [2/2], Step [29110/64305], Loss: 4.7255\n",
      "Epoch [2/2], Step [29120/64305], Loss: 4.7264\n",
      "Epoch [2/2], Step [29130/64305], Loss: 4.5936\n",
      "Epoch [2/2], Step [29140/64305], Loss: 4.7986\n",
      "Epoch [2/2], Step [29150/64305], Loss: 4.8905\n",
      "Epoch [2/2], Step [29160/64305], Loss: 4.7898\n",
      "Epoch [2/2], Step [29170/64305], Loss: 4.7625\n",
      "Epoch [2/2], Step [29180/64305], Loss: 4.7577\n",
      "Epoch [2/2], Step [29190/64305], Loss: 4.7527\n",
      "Epoch [2/2], Step [29200/64305], Loss: 4.9248\n",
      "Epoch [2/2], Step [29210/64305], Loss: 4.9503\n",
      "Epoch [2/2], Step [29220/64305], Loss: 4.9464\n",
      "Epoch [2/2], Step [29230/64305], Loss: 4.6656\n",
      "Epoch [2/2], Step [29240/64305], Loss: 4.9089\n",
      "Epoch [2/2], Step [29250/64305], Loss: 4.9160\n",
      "Epoch [2/2], Step [29260/64305], Loss: 4.7072\n",
      "Epoch [2/2], Step [29270/64305], Loss: 4.8811\n",
      "Epoch [2/2], Step [29280/64305], Loss: 4.6291\n",
      "Epoch [2/2], Step [29290/64305], Loss: 4.6425\n",
      "Epoch [2/2], Step [29300/64305], Loss: 4.7433\n",
      "Epoch [2/2], Step [29310/64305], Loss: 4.7212\n",
      "Epoch [2/2], Step [29320/64305], Loss: 4.9246\n",
      "Epoch [2/2], Step [29330/64305], Loss: 4.8510\n",
      "Epoch [2/2], Step [29340/64305], Loss: 4.8499\n",
      "Epoch [2/2], Step [29350/64305], Loss: 4.8905\n",
      "Epoch [2/2], Step [29360/64305], Loss: 4.6771\n",
      "Epoch [2/2], Step [29370/64305], Loss: 4.9261\n",
      "Epoch [2/2], Step [29380/64305], Loss: 4.6810\n",
      "Epoch [2/2], Step [29390/64305], Loss: 4.7732\n",
      "Epoch [2/2], Step [29400/64305], Loss: 4.6999\n",
      "Epoch [2/2], Step [29410/64305], Loss: 4.5782\n",
      "Epoch [2/2], Step [29420/64305], Loss: 4.6063\n",
      "Epoch [2/2], Step [29430/64305], Loss: 4.6903\n",
      "Epoch [2/2], Step [29440/64305], Loss: 4.7667\n",
      "Epoch [2/2], Step [29450/64305], Loss: 4.7196\n",
      "Epoch [2/2], Step [29460/64305], Loss: 4.7417\n",
      "Epoch [2/2], Step [29470/64305], Loss: 4.4740\n",
      "Epoch [2/2], Step [29480/64305], Loss: 4.7133\n",
      "Epoch [2/2], Step [29490/64305], Loss: 4.7692\n",
      "Epoch [2/2], Step [29500/64305], Loss: 4.7755\n",
      "Epoch [2/2], Step [29510/64305], Loss: 4.7969\n",
      "Epoch [2/2], Step [29520/64305], Loss: 4.6265\n",
      "Epoch [2/2], Step [29530/64305], Loss: 4.8078\n",
      "Epoch [2/2], Step [29540/64305], Loss: 4.9733\n",
      "Epoch [2/2], Step [29550/64305], Loss: 4.7715\n",
      "Epoch [2/2], Step [29560/64305], Loss: 4.7945\n",
      "Epoch [2/2], Step [29570/64305], Loss: 4.8899\n",
      "Epoch [2/2], Step [29580/64305], Loss: 4.7240\n",
      "Epoch [2/2], Step [29590/64305], Loss: 4.7470\n",
      "Epoch [2/2], Step [29600/64305], Loss: 4.8885\n",
      "Epoch [2/2], Step [29610/64305], Loss: 4.5871\n",
      "Epoch [2/2], Step [29620/64305], Loss: 4.8471\n",
      "Epoch [2/2], Step [29630/64305], Loss: 4.6281\n",
      "Epoch [2/2], Step [29640/64305], Loss: 4.7710\n",
      "Epoch [2/2], Step [29650/64305], Loss: 4.7736\n",
      "Epoch [2/2], Step [29660/64305], Loss: 4.7342\n",
      "Epoch [2/2], Step [29670/64305], Loss: 4.8465\n",
      "Epoch [2/2], Step [29680/64305], Loss: 4.8009\n",
      "Epoch [2/2], Step [29690/64305], Loss: 4.6205\n",
      "Epoch [2/2], Step [29700/64305], Loss: 5.0264\n",
      "Epoch [2/2], Step [29710/64305], Loss: 4.7500\n",
      "Epoch [2/2], Step [29720/64305], Loss: 5.0609\n",
      "Epoch [2/2], Step [29730/64305], Loss: 4.7057\n",
      "Epoch [2/2], Step [29740/64305], Loss: 4.7133\n",
      "Epoch [2/2], Step [29750/64305], Loss: 4.4434\n",
      "Epoch [2/2], Step [29760/64305], Loss: 4.6932\n",
      "Epoch [2/2], Step [29770/64305], Loss: 4.8460\n",
      "Epoch [2/2], Step [29780/64305], Loss: 4.7383\n",
      "Epoch [2/2], Step [29790/64305], Loss: 4.8082\n",
      "Epoch [2/2], Step [29800/64305], Loss: 4.7994\n",
      "Epoch [2/2], Step [29810/64305], Loss: 4.5875\n",
      "Epoch [2/2], Step [29820/64305], Loss: 4.8173\n",
      "Epoch [2/2], Step [29830/64305], Loss: 5.0178\n",
      "Epoch [2/2], Step [29840/64305], Loss: 4.7708\n",
      "Epoch [2/2], Step [29850/64305], Loss: 4.9450\n",
      "Epoch [2/2], Step [29860/64305], Loss: 4.7150\n",
      "Epoch [2/2], Step [29870/64305], Loss: 4.5361\n",
      "Epoch [2/2], Step [29880/64305], Loss: 4.6005\n",
      "Epoch [2/2], Step [29890/64305], Loss: 4.6789\n",
      "Epoch [2/2], Step [29900/64305], Loss: 4.6207\n",
      "Epoch [2/2], Step [29910/64305], Loss: 4.7237\n",
      "Epoch [2/2], Step [29920/64305], Loss: 4.7324\n",
      "Epoch [2/2], Step [29930/64305], Loss: 4.8109\n",
      "Epoch [2/2], Step [29940/64305], Loss: 4.7438\n",
      "Epoch [2/2], Step [29950/64305], Loss: 4.8208\n",
      "Epoch [2/2], Step [29960/64305], Loss: 4.7100\n",
      "Epoch [2/2], Step [29970/64305], Loss: 4.6823\n",
      "Epoch [2/2], Step [29980/64305], Loss: 4.8367\n",
      "Epoch [2/2], Step [29990/64305], Loss: 4.7373\n",
      "Epoch [2/2], Step [30000/64305], Loss: 4.7853\n",
      "Epoch [2/2], Step [30010/64305], Loss: 4.7855\n",
      "Epoch [2/2], Step [30020/64305], Loss: 4.7022\n",
      "Epoch [2/2], Step [30030/64305], Loss: 4.7825\n",
      "Epoch [2/2], Step [30040/64305], Loss: 4.6028\n",
      "Epoch [2/2], Step [30050/64305], Loss: 4.4688\n",
      "Epoch [2/2], Step [30060/64305], Loss: 4.6866\n",
      "Epoch [2/2], Step [30070/64305], Loss: 4.7569\n",
      "Epoch [2/2], Step [30080/64305], Loss: 4.6675\n",
      "Epoch [2/2], Step [30090/64305], Loss: 4.5322\n",
      "Epoch [2/2], Step [30100/64305], Loss: 4.8863\n",
      "Epoch [2/2], Step [30110/64305], Loss: 4.9550\n",
      "Epoch [2/2], Step [30120/64305], Loss: 4.7636\n",
      "Epoch [2/2], Step [30130/64305], Loss: 4.7318\n",
      "Epoch [2/2], Step [30140/64305], Loss: 4.8867\n",
      "Epoch [2/2], Step [30150/64305], Loss: 4.8009\n",
      "Epoch [2/2], Step [30160/64305], Loss: 4.9304\n",
      "Epoch [2/2], Step [30170/64305], Loss: 4.8249\n",
      "Epoch [2/2], Step [30180/64305], Loss: 4.6081\n",
      "Epoch [2/2], Step [30190/64305], Loss: 4.6607\n",
      "Epoch [2/2], Step [30200/64305], Loss: 4.7657\n",
      "Epoch [2/2], Step [30210/64305], Loss: 4.8133\n",
      "Epoch [2/2], Step [30220/64305], Loss: 4.7140\n",
      "Epoch [2/2], Step [30230/64305], Loss: 4.7327\n",
      "Epoch [2/2], Step [30240/64305], Loss: 4.5440\n",
      "Epoch [2/2], Step [30250/64305], Loss: 4.6353\n",
      "Epoch [2/2], Step [30260/64305], Loss: 4.5082\n",
      "Epoch [2/2], Step [30270/64305], Loss: 4.8259\n",
      "Epoch [2/2], Step [30280/64305], Loss: 4.8665\n",
      "Epoch [2/2], Step [30290/64305], Loss: 4.9586\n",
      "Epoch [2/2], Step [30300/64305], Loss: 4.8037\n",
      "Epoch [2/2], Step [30310/64305], Loss: 4.8249\n",
      "Epoch [2/2], Step [30320/64305], Loss: 4.6976\n",
      "Epoch [2/2], Step [30330/64305], Loss: 4.7203\n",
      "Epoch [2/2], Step [30340/64305], Loss: 4.6651\n",
      "Epoch [2/2], Step [30350/64305], Loss: 4.9155\n",
      "Epoch [2/2], Step [30360/64305], Loss: 4.6747\n",
      "Epoch [2/2], Step [30370/64305], Loss: 4.7090\n",
      "Epoch [2/2], Step [30380/64305], Loss: 4.7382\n",
      "Epoch [2/2], Step [30390/64305], Loss: 4.6033\n",
      "Epoch [2/2], Step [30400/64305], Loss: 4.6642\n",
      "Epoch [2/2], Step [30410/64305], Loss: 4.6977\n",
      "Epoch [2/2], Step [30420/64305], Loss: 4.5644\n",
      "Epoch [2/2], Step [30430/64305], Loss: 4.8552\n",
      "Epoch [2/2], Step [30440/64305], Loss: 4.7569\n",
      "Epoch [2/2], Step [30450/64305], Loss: 4.9388\n",
      "Epoch [2/2], Step [30460/64305], Loss: 4.9740\n",
      "Epoch [2/2], Step [30470/64305], Loss: 4.6668\n",
      "Epoch [2/2], Step [30480/64305], Loss: 4.7280\n",
      "Epoch [2/2], Step [30490/64305], Loss: 4.9821\n",
      "Epoch [2/2], Step [30500/64305], Loss: 4.7427\n",
      "Epoch [2/2], Step [30510/64305], Loss: 4.7908\n",
      "Epoch [2/2], Step [30520/64305], Loss: 4.5903\n",
      "Epoch [2/2], Step [30530/64305], Loss: 4.9252\n",
      "Epoch [2/2], Step [30540/64305], Loss: 4.7587\n",
      "Epoch [2/2], Step [30550/64305], Loss: 4.8330\n",
      "Epoch [2/2], Step [30560/64305], Loss: 4.7347\n",
      "Epoch [2/2], Step [30570/64305], Loss: 4.5141\n",
      "Epoch [2/2], Step [30580/64305], Loss: 4.8717\n",
      "Epoch [2/2], Step [30590/64305], Loss: 4.7617\n",
      "Epoch [2/2], Step [30600/64305], Loss: 4.7135\n",
      "Epoch [2/2], Step [30610/64305], Loss: 4.8274\n",
      "Epoch [2/2], Step [30620/64305], Loss: 4.6462\n",
      "Epoch [2/2], Step [30630/64305], Loss: 4.7915\n",
      "Epoch [2/2], Step [30640/64305], Loss: 4.7935\n",
      "Epoch [2/2], Step [30650/64305], Loss: 4.8095\n",
      "Epoch [2/2], Step [30660/64305], Loss: 4.7317\n",
      "Epoch [2/2], Step [30670/64305], Loss: 4.6422\n",
      "Epoch [2/2], Step [30680/64305], Loss: 4.6817\n",
      "Epoch [2/2], Step [30690/64305], Loss: 4.9702\n",
      "Epoch [2/2], Step [30700/64305], Loss: 4.5971\n",
      "Epoch [2/2], Step [30710/64305], Loss: 4.7460\n",
      "Epoch [2/2], Step [30720/64305], Loss: 4.7138\n",
      "Epoch [2/2], Step [30730/64305], Loss: 4.7666\n",
      "Epoch [2/2], Step [30740/64305], Loss: 4.9500\n",
      "Epoch [2/2], Step [30750/64305], Loss: 4.7768\n",
      "Epoch [2/2], Step [30760/64305], Loss: 4.7345\n",
      "Epoch [2/2], Step [30770/64305], Loss: 4.8961\n",
      "Epoch [2/2], Step [30780/64305], Loss: 4.7328\n",
      "Epoch [2/2], Step [30790/64305], Loss: 4.8072\n",
      "Epoch [2/2], Step [30800/64305], Loss: 4.5334\n",
      "Epoch [2/2], Step [30810/64305], Loss: 4.8265\n",
      "Epoch [2/2], Step [30820/64305], Loss: 4.8283\n",
      "Epoch [2/2], Step [30830/64305], Loss: 4.7576\n",
      "Epoch [2/2], Step [30840/64305], Loss: 4.8334\n",
      "Epoch [2/2], Step [30850/64305], Loss: 4.5469\n",
      "Epoch [2/2], Step [30860/64305], Loss: 4.7470\n",
      "Epoch [2/2], Step [30870/64305], Loss: 4.7003\n",
      "Epoch [2/2], Step [30880/64305], Loss: 4.5894\n",
      "Epoch [2/2], Step [30890/64305], Loss: 4.8919\n",
      "Epoch [2/2], Step [30900/64305], Loss: 4.7843\n",
      "Epoch [2/2], Step [30910/64305], Loss: 4.8295\n",
      "Epoch [2/2], Step [30920/64305], Loss: 4.7296\n",
      "Epoch [2/2], Step [30930/64305], Loss: 4.9209\n",
      "Epoch [2/2], Step [30940/64305], Loss: 4.7506\n",
      "Epoch [2/2], Step [30950/64305], Loss: 4.6436\n",
      "Epoch [2/2], Step [30960/64305], Loss: 4.7393\n",
      "Epoch [2/2], Step [30970/64305], Loss: 4.7668\n",
      "Epoch [2/2], Step [30980/64305], Loss: 4.6860\n",
      "Epoch [2/2], Step [30990/64305], Loss: 4.6709\n",
      "Epoch [2/2], Step [31000/64305], Loss: 4.8434\n",
      "Epoch [2/2], Step [31010/64305], Loss: 4.7457\n",
      "Epoch [2/2], Step [31020/64305], Loss: 4.7615\n",
      "Epoch [2/2], Step [31030/64305], Loss: 4.8993\n",
      "Epoch [2/2], Step [31040/64305], Loss: 4.8603\n",
      "Epoch [2/2], Step [31050/64305], Loss: 4.7386\n",
      "Epoch [2/2], Step [31060/64305], Loss: 4.7283\n",
      "Epoch [2/2], Step [31070/64305], Loss: 5.0200\n",
      "Epoch [2/2], Step [31080/64305], Loss: 4.6760\n",
      "Epoch [2/2], Step [31090/64305], Loss: 4.7050\n",
      "Epoch [2/2], Step [31100/64305], Loss: 4.6830\n",
      "Epoch [2/2], Step [31110/64305], Loss: 4.7725\n",
      "Epoch [2/2], Step [31120/64305], Loss: 4.8316\n",
      "Epoch [2/2], Step [31130/64305], Loss: 4.4376\n",
      "Epoch [2/2], Step [31140/64305], Loss: 4.6739\n",
      "Epoch [2/2], Step [31150/64305], Loss: 4.6260\n",
      "Epoch [2/2], Step [31160/64305], Loss: 4.7830\n",
      "Epoch [2/2], Step [31170/64305], Loss: 4.6790\n",
      "Epoch [2/2], Step [31180/64305], Loss: 4.7514\n",
      "Epoch [2/2], Step [31190/64305], Loss: 4.8846\n",
      "Epoch [2/2], Step [31200/64305], Loss: 4.7043\n",
      "Epoch [2/2], Step [31210/64305], Loss: 4.7662\n",
      "Epoch [2/2], Step [31220/64305], Loss: 4.5854\n",
      "Epoch [2/2], Step [31230/64305], Loss: 4.8956\n",
      "Epoch [2/2], Step [31240/64305], Loss: 4.8096\n",
      "Epoch [2/2], Step [31250/64305], Loss: 4.8302\n",
      "Epoch [2/2], Step [31260/64305], Loss: 4.5301\n",
      "Epoch [2/2], Step [31270/64305], Loss: 4.5583\n",
      "Epoch [2/2], Step [31280/64305], Loss: 4.7232\n",
      "Epoch [2/2], Step [31290/64305], Loss: 4.7034\n",
      "Epoch [2/2], Step [31300/64305], Loss: 4.8804\n",
      "Epoch [2/2], Step [31310/64305], Loss: 4.7773\n",
      "Epoch [2/2], Step [31320/64305], Loss: 4.7846\n",
      "Epoch [2/2], Step [31330/64305], Loss: 4.7156\n",
      "Epoch [2/2], Step [31340/64305], Loss: 4.6883\n",
      "Epoch [2/2], Step [31350/64305], Loss: 5.0267\n",
      "Epoch [2/2], Step [31360/64305], Loss: 4.7176\n",
      "Epoch [2/2], Step [31370/64305], Loss: 4.8566\n",
      "Epoch [2/2], Step [31380/64305], Loss: 4.7306\n",
      "Epoch [2/2], Step [31390/64305], Loss: 4.8039\n",
      "Epoch [2/2], Step [31400/64305], Loss: 4.7641\n",
      "Epoch [2/2], Step [31410/64305], Loss: 4.9138\n",
      "Epoch [2/2], Step [31420/64305], Loss: 4.7281\n",
      "Epoch [2/2], Step [31430/64305], Loss: 4.8112\n",
      "Epoch [2/2], Step [31440/64305], Loss: 4.5303\n",
      "Epoch [2/2], Step [31450/64305], Loss: 4.4918\n",
      "Epoch [2/2], Step [31460/64305], Loss: 4.5989\n",
      "Epoch [2/2], Step [31470/64305], Loss: 4.7812\n",
      "Epoch [2/2], Step [31480/64305], Loss: 4.7661\n",
      "Epoch [2/2], Step [31490/64305], Loss: 4.7678\n",
      "Epoch [2/2], Step [31500/64305], Loss: 4.6972\n",
      "Epoch [2/2], Step [31510/64305], Loss: 4.9333\n",
      "Epoch [2/2], Step [31520/64305], Loss: 4.9029\n",
      "Epoch [2/2], Step [31530/64305], Loss: 4.7307\n",
      "Epoch [2/2], Step [31540/64305], Loss: 4.6906\n",
      "Epoch [2/2], Step [31550/64305], Loss: 4.8933\n",
      "Epoch [2/2], Step [31560/64305], Loss: 4.7516\n",
      "Epoch [2/2], Step [31570/64305], Loss: 4.7667\n",
      "Epoch [2/2], Step [31580/64305], Loss: 4.7606\n",
      "Epoch [2/2], Step [31590/64305], Loss: 4.8263\n",
      "Epoch [2/2], Step [31600/64305], Loss: 4.6473\n",
      "Epoch [2/2], Step [31610/64305], Loss: 4.6795\n",
      "Epoch [2/2], Step [31620/64305], Loss: 4.9008\n",
      "Epoch [2/2], Step [31630/64305], Loss: 4.8122\n",
      "Epoch [2/2], Step [31640/64305], Loss: 4.8001\n",
      "Epoch [2/2], Step [31650/64305], Loss: 4.5329\n",
      "Epoch [2/2], Step [31660/64305], Loss: 4.8811\n",
      "Epoch [2/2], Step [31670/64305], Loss: 4.6539\n",
      "Epoch [2/2], Step [31680/64305], Loss: 4.6455\n",
      "Epoch [2/2], Step [31690/64305], Loss: 4.5066\n",
      "Epoch [2/2], Step [31700/64305], Loss: 4.7351\n",
      "Epoch [2/2], Step [31710/64305], Loss: 4.8001\n",
      "Epoch [2/2], Step [31720/64305], Loss: 4.7193\n",
      "Epoch [2/2], Step [31730/64305], Loss: 4.8072\n",
      "Epoch [2/2], Step [31740/64305], Loss: 4.9165\n",
      "Epoch [2/2], Step [31750/64305], Loss: 4.6579\n",
      "Epoch [2/2], Step [31760/64305], Loss: 4.8350\n",
      "Epoch [2/2], Step [31770/64305], Loss: 4.8148\n",
      "Epoch [2/2], Step [31780/64305], Loss: 4.7632\n",
      "Epoch [2/2], Step [31790/64305], Loss: 4.7536\n",
      "Epoch [2/2], Step [31800/64305], Loss: 4.8610\n",
      "Epoch [2/2], Step [31810/64305], Loss: 4.8747\n",
      "Epoch [2/2], Step [31820/64305], Loss: 4.6726\n",
      "Epoch [2/2], Step [31830/64305], Loss: 4.6720\n",
      "Epoch [2/2], Step [31840/64305], Loss: 4.7598\n",
      "Epoch [2/2], Step [31850/64305], Loss: 4.5781\n",
      "Epoch [2/2], Step [31860/64305], Loss: 4.7431\n",
      "Epoch [2/2], Step [31870/64305], Loss: 4.8147\n",
      "Epoch [2/2], Step [31880/64305], Loss: 4.8583\n",
      "Epoch [2/2], Step [31890/64305], Loss: 4.8164\n",
      "Epoch [2/2], Step [31900/64305], Loss: 4.8346\n",
      "Epoch [2/2], Step [31910/64305], Loss: 4.7209\n",
      "Epoch [2/2], Step [31920/64305], Loss: 4.7318\n",
      "Epoch [2/2], Step [31930/64305], Loss: 4.7748\n",
      "Epoch [2/2], Step [31940/64305], Loss: 4.8950\n",
      "Epoch [2/2], Step [31950/64305], Loss: 4.6279\n",
      "Epoch [2/2], Step [31960/64305], Loss: 4.7327\n",
      "Epoch [2/2], Step [31970/64305], Loss: 4.8077\n",
      "Epoch [2/2], Step [31980/64305], Loss: 4.9137\n",
      "Epoch [2/2], Step [31990/64305], Loss: 4.7019\n",
      "Epoch [2/2], Step [32000/64305], Loss: 4.9631\n",
      "Epoch [2/2], Step [32010/64305], Loss: 4.8704\n",
      "Epoch [2/2], Step [32020/64305], Loss: 4.5236\n",
      "Epoch [2/2], Step [32030/64305], Loss: 4.9134\n",
      "Epoch [2/2], Step [32040/64305], Loss: 4.6477\n",
      "Epoch [2/2], Step [32050/64305], Loss: 4.8166\n",
      "Epoch [2/2], Step [32060/64305], Loss: 4.7033\n",
      "Epoch [2/2], Step [32070/64305], Loss: 4.7530\n",
      "Epoch [2/2], Step [32080/64305], Loss: 4.6408\n",
      "Epoch [2/2], Step [32090/64305], Loss: 4.7991\n",
      "Epoch [2/2], Step [32100/64305], Loss: 4.5366\n",
      "Epoch [2/2], Step [32110/64305], Loss: 4.6892\n",
      "Epoch [2/2], Step [32120/64305], Loss: 4.8076\n",
      "Epoch [2/2], Step [32130/64305], Loss: 4.8243\n",
      "Epoch [2/2], Step [32140/64305], Loss: 4.4966\n",
      "Epoch [2/2], Step [32150/64305], Loss: 4.9311\n",
      "Epoch [2/2], Step [32160/64305], Loss: 4.7286\n",
      "Epoch [2/2], Step [32170/64305], Loss: 4.7990\n",
      "Epoch [2/2], Step [32180/64305], Loss: 4.6897\n",
      "Epoch [2/2], Step [32190/64305], Loss: 4.7675\n",
      "Epoch [2/2], Step [32200/64305], Loss: 4.8884\n",
      "Epoch [2/2], Step [32210/64305], Loss: 4.8463\n",
      "Epoch [2/2], Step [32220/64305], Loss: 4.5898\n",
      "Epoch [2/2], Step [32230/64305], Loss: 4.8764\n",
      "Epoch [2/2], Step [32240/64305], Loss: 4.6567\n",
      "Epoch [2/2], Step [32250/64305], Loss: 4.7872\n",
      "Epoch [2/2], Step [32260/64305], Loss: 4.5966\n",
      "Epoch [2/2], Step [32270/64305], Loss: 4.8530\n",
      "Epoch [2/2], Step [32280/64305], Loss: 4.7892\n",
      "Epoch [2/2], Step [32290/64305], Loss: 4.7987\n",
      "Epoch [2/2], Step [32300/64305], Loss: 4.8486\n",
      "Epoch [2/2], Step [32310/64305], Loss: 4.8844\n",
      "Epoch [2/2], Step [32320/64305], Loss: 4.7722\n",
      "Epoch [2/2], Step [32330/64305], Loss: 4.5269\n",
      "Epoch [2/2], Step [32340/64305], Loss: 4.7729\n",
      "Epoch [2/2], Step [32350/64305], Loss: 4.7280\n",
      "Epoch [2/2], Step [32360/64305], Loss: 4.8029\n",
      "Epoch [2/2], Step [32370/64305], Loss: 4.5879\n",
      "Epoch [2/2], Step [32380/64305], Loss: 4.7513\n",
      "Epoch [2/2], Step [32390/64305], Loss: 4.7071\n",
      "Epoch [2/2], Step [32400/64305], Loss: 4.6733\n",
      "Epoch [2/2], Step [32410/64305], Loss: 4.7895\n",
      "Epoch [2/2], Step [32420/64305], Loss: 4.8762\n",
      "Epoch [2/2], Step [32430/64305], Loss: 4.6155\n",
      "Epoch [2/2], Step [32440/64305], Loss: 4.8299\n",
      "Epoch [2/2], Step [32450/64305], Loss: 4.9797\n",
      "Epoch [2/2], Step [32460/64305], Loss: 4.6441\n",
      "Epoch [2/2], Step [32470/64305], Loss: 4.5618\n",
      "Epoch [2/2], Step [32480/64305], Loss: 4.8830\n",
      "Epoch [2/2], Step [32490/64305], Loss: 4.9067\n",
      "Epoch [2/2], Step [32500/64305], Loss: 4.7156\n",
      "Epoch [2/2], Step [32510/64305], Loss: 4.7512\n",
      "Epoch [2/2], Step [32520/64305], Loss: 4.9394\n",
      "Epoch [2/2], Step [32530/64305], Loss: 4.8213\n",
      "Epoch [2/2], Step [32540/64305], Loss: 4.8717\n",
      "Epoch [2/2], Step [32550/64305], Loss: 4.9089\n",
      "Epoch [2/2], Step [32560/64305], Loss: 4.7255\n",
      "Epoch [2/2], Step [32570/64305], Loss: 4.6843\n",
      "Epoch [2/2], Step [32580/64305], Loss: 4.8914\n",
      "Epoch [2/2], Step [32590/64305], Loss: 4.7763\n",
      "Epoch [2/2], Step [32600/64305], Loss: 4.9961\n",
      "Epoch [2/2], Step [32610/64305], Loss: 4.8474\n",
      "Epoch [2/2], Step [32620/64305], Loss: 4.4881\n",
      "Epoch [2/2], Step [32630/64305], Loss: 4.6279\n",
      "Epoch [2/2], Step [32640/64305], Loss: 4.9557\n",
      "Epoch [2/2], Step [32650/64305], Loss: 4.7000\n",
      "Epoch [2/2], Step [32660/64305], Loss: 5.0379\n",
      "Epoch [2/2], Step [32670/64305], Loss: 4.8507\n",
      "Epoch [2/2], Step [32680/64305], Loss: 4.7802\n",
      "Epoch [2/2], Step [32690/64305], Loss: 5.0401\n",
      "Epoch [2/2], Step [32700/64305], Loss: 4.8033\n",
      "Epoch [2/2], Step [32710/64305], Loss: 4.9040\n",
      "Epoch [2/2], Step [32720/64305], Loss: 4.6577\n",
      "Epoch [2/2], Step [32730/64305], Loss: 4.9650\n",
      "Epoch [2/2], Step [32740/64305], Loss: 4.6988\n",
      "Epoch [2/2], Step [32750/64305], Loss: 4.7932\n",
      "Epoch [2/2], Step [32760/64305], Loss: 4.9039\n",
      "Epoch [2/2], Step [32770/64305], Loss: 4.7539\n",
      "Epoch [2/2], Step [32780/64305], Loss: 4.5188\n",
      "Epoch [2/2], Step [32790/64305], Loss: 4.8155\n",
      "Epoch [2/2], Step [32800/64305], Loss: 4.6327\n",
      "Epoch [2/2], Step [32810/64305], Loss: 4.6915\n",
      "Epoch [2/2], Step [32820/64305], Loss: 4.6929\n",
      "Epoch [2/2], Step [32830/64305], Loss: 4.6229\n",
      "Epoch [2/2], Step [32840/64305], Loss: 4.7147\n",
      "Epoch [2/2], Step [32850/64305], Loss: 4.8247\n",
      "Epoch [2/2], Step [32860/64305], Loss: 4.6530\n",
      "Epoch [2/2], Step [32870/64305], Loss: 4.8662\n",
      "Epoch [2/2], Step [32880/64305], Loss: 4.9290\n",
      "Epoch [2/2], Step [32890/64305], Loss: 4.7260\n",
      "Epoch [2/2], Step [32900/64305], Loss: 4.8307\n",
      "Epoch [2/2], Step [32910/64305], Loss: 4.8303\n",
      "Epoch [2/2], Step [32920/64305], Loss: 4.7194\n",
      "Epoch [2/2], Step [32930/64305], Loss: 4.7638\n",
      "Epoch [2/2], Step [32940/64305], Loss: 4.7739\n",
      "Epoch [2/2], Step [32950/64305], Loss: 4.9645\n",
      "Epoch [2/2], Step [32960/64305], Loss: 4.6881\n",
      "Epoch [2/2], Step [32970/64305], Loss: 4.7094\n",
      "Epoch [2/2], Step [32980/64305], Loss: 4.7760\n",
      "Epoch [2/2], Step [32990/64305], Loss: 4.7921\n",
      "Epoch [2/2], Step [33000/64305], Loss: 4.8440\n",
      "Epoch [2/2], Step [33010/64305], Loss: 4.7844\n",
      "Epoch [2/2], Step [33020/64305], Loss: 4.8812\n",
      "Epoch [2/2], Step [33030/64305], Loss: 4.8182\n",
      "Epoch [2/2], Step [33040/64305], Loss: 4.7870\n",
      "Epoch [2/2], Step [33050/64305], Loss: 4.7524\n",
      "Epoch [2/2], Step [33060/64305], Loss: 4.7936\n",
      "Epoch [2/2], Step [33070/64305], Loss: 4.6700\n",
      "Epoch [2/2], Step [33080/64305], Loss: 4.7157\n",
      "Epoch [2/2], Step [33090/64305], Loss: 4.8669\n",
      "Epoch [2/2], Step [33100/64305], Loss: 4.6889\n",
      "Epoch [2/2], Step [33110/64305], Loss: 4.5170\n",
      "Epoch [2/2], Step [33120/64305], Loss: 4.6936\n",
      "Epoch [2/2], Step [33130/64305], Loss: 4.8389\n",
      "Epoch [2/2], Step [33140/64305], Loss: 4.6439\n",
      "Epoch [2/2], Step [33150/64305], Loss: 4.8600\n",
      "Epoch [2/2], Step [33160/64305], Loss: 4.6931\n",
      "Epoch [2/2], Step [33170/64305], Loss: 4.8658\n",
      "Epoch [2/2], Step [33180/64305], Loss: 4.8030\n",
      "Epoch [2/2], Step [33190/64305], Loss: 4.6781\n",
      "Epoch [2/2], Step [33200/64305], Loss: 4.7821\n",
      "Epoch [2/2], Step [33210/64305], Loss: 4.8422\n",
      "Epoch [2/2], Step [33220/64305], Loss: 4.8371\n",
      "Epoch [2/2], Step [33230/64305], Loss: 4.6458\n",
      "Epoch [2/2], Step [33240/64305], Loss: 4.8057\n",
      "Epoch [2/2], Step [33250/64305], Loss: 4.8690\n",
      "Epoch [2/2], Step [33260/64305], Loss: 4.5991\n",
      "Epoch [2/2], Step [33270/64305], Loss: 4.7909\n",
      "Epoch [2/2], Step [33280/64305], Loss: 4.7530\n",
      "Epoch [2/2], Step [33290/64305], Loss: 4.7505\n",
      "Epoch [2/2], Step [33300/64305], Loss: 4.7252\n",
      "Epoch [2/2], Step [33310/64305], Loss: 4.7699\n",
      "Epoch [2/2], Step [33320/64305], Loss: 4.8227\n",
      "Epoch [2/2], Step [33330/64305], Loss: 4.5581\n",
      "Epoch [2/2], Step [33340/64305], Loss: 4.7439\n",
      "Epoch [2/2], Step [33350/64305], Loss: 4.9654\n",
      "Epoch [2/2], Step [33360/64305], Loss: 4.6072\n",
      "Epoch [2/2], Step [33370/64305], Loss: 4.6853\n",
      "Epoch [2/2], Step [33380/64305], Loss: 4.7166\n",
      "Epoch [2/2], Step [33390/64305], Loss: 4.8008\n",
      "Epoch [2/2], Step [33400/64305], Loss: 4.7694\n",
      "Epoch [2/2], Step [33410/64305], Loss: 4.8603\n",
      "Epoch [2/2], Step [33420/64305], Loss: 4.8584\n",
      "Epoch [2/2], Step [33430/64305], Loss: 4.7505\n",
      "Epoch [2/2], Step [33440/64305], Loss: 4.7077\n",
      "Epoch [2/2], Step [33450/64305], Loss: 4.7776\n",
      "Epoch [2/2], Step [33460/64305], Loss: 4.8684\n",
      "Epoch [2/2], Step [33470/64305], Loss: 4.7418\n",
      "Epoch [2/2], Step [33480/64305], Loss: 4.8351\n",
      "Epoch [2/2], Step [33490/64305], Loss: 4.6008\n",
      "Epoch [2/2], Step [33500/64305], Loss: 4.6381\n",
      "Epoch [2/2], Step [33510/64305], Loss: 4.7913\n",
      "Epoch [2/2], Step [33520/64305], Loss: 4.8517\n",
      "Epoch [2/2], Step [33530/64305], Loss: 4.7931\n",
      "Epoch [2/2], Step [33540/64305], Loss: 5.0172\n",
      "Epoch [2/2], Step [33550/64305], Loss: 4.8225\n",
      "Epoch [2/2], Step [33560/64305], Loss: 4.7088\n",
      "Epoch [2/2], Step [33570/64305], Loss: 4.6792\n",
      "Epoch [2/2], Step [33580/64305], Loss: 4.6616\n",
      "Epoch [2/2], Step [33590/64305], Loss: 4.9739\n",
      "Epoch [2/2], Step [33600/64305], Loss: 4.8370\n",
      "Epoch [2/2], Step [33610/64305], Loss: 4.7794\n",
      "Epoch [2/2], Step [33620/64305], Loss: 4.6158\n",
      "Epoch [2/2], Step [33630/64305], Loss: 4.7160\n",
      "Epoch [2/2], Step [33640/64305], Loss: 4.8336\n",
      "Epoch [2/2], Step [33650/64305], Loss: 4.7217\n",
      "Epoch [2/2], Step [33660/64305], Loss: 4.6833\n",
      "Epoch [2/2], Step [33670/64305], Loss: 4.6565\n",
      "Epoch [2/2], Step [33680/64305], Loss: 4.7800\n",
      "Epoch [2/2], Step [33690/64305], Loss: 4.7027\n",
      "Epoch [2/2], Step [33700/64305], Loss: 4.7388\n",
      "Epoch [2/2], Step [33710/64305], Loss: 4.5734\n",
      "Epoch [2/2], Step [33720/64305], Loss: 4.9200\n",
      "Epoch [2/2], Step [33730/64305], Loss: 4.8660\n",
      "Epoch [2/2], Step [33740/64305], Loss: 4.8478\n",
      "Epoch [2/2], Step [33750/64305], Loss: 4.8979\n",
      "Epoch [2/2], Step [33760/64305], Loss: 4.9318\n",
      "Epoch [2/2], Step [33770/64305], Loss: 4.9921\n",
      "Epoch [2/2], Step [33780/64305], Loss: 4.8414\n",
      "Epoch [2/2], Step [33790/64305], Loss: 4.8503\n",
      "Epoch [2/2], Step [33800/64305], Loss: 4.7337\n",
      "Epoch [2/2], Step [33810/64305], Loss: 4.7783\n",
      "Epoch [2/2], Step [33820/64305], Loss: 4.7952\n",
      "Epoch [2/2], Step [33830/64305], Loss: 4.8105\n",
      "Epoch [2/2], Step [33840/64305], Loss: 4.8816\n",
      "Epoch [2/2], Step [33850/64305], Loss: 4.7134\n",
      "Epoch [2/2], Step [33860/64305], Loss: 4.6903\n",
      "Epoch [2/2], Step [33870/64305], Loss: 4.8532\n",
      "Epoch [2/2], Step [33880/64305], Loss: 4.7920\n",
      "Epoch [2/2], Step [33890/64305], Loss: 4.5435\n",
      "Epoch [2/2], Step [33900/64305], Loss: 4.8333\n",
      "Epoch [2/2], Step [33910/64305], Loss: 4.9375\n",
      "Epoch [2/2], Step [33920/64305], Loss: 4.6597\n",
      "Epoch [2/2], Step [33930/64305], Loss: 4.7251\n",
      "Epoch [2/2], Step [33940/64305], Loss: 4.6285\n",
      "Epoch [2/2], Step [33950/64305], Loss: 4.8109\n",
      "Epoch [2/2], Step [33960/64305], Loss: 4.8334\n",
      "Epoch [2/2], Step [33970/64305], Loss: 4.8779\n",
      "Epoch [2/2], Step [33980/64305], Loss: 4.6390\n",
      "Epoch [2/2], Step [33990/64305], Loss: 4.7302\n",
      "Epoch [2/2], Step [34000/64305], Loss: 4.7308\n",
      "Epoch [2/2], Step [34010/64305], Loss: 4.7663\n",
      "Epoch [2/2], Step [34020/64305], Loss: 4.8389\n",
      "Epoch [2/2], Step [34030/64305], Loss: 4.8130\n",
      "Epoch [2/2], Step [34040/64305], Loss: 4.9282\n",
      "Epoch [2/2], Step [34050/64305], Loss: 4.7328\n",
      "Epoch [2/2], Step [34060/64305], Loss: 4.8439\n",
      "Epoch [2/2], Step [34070/64305], Loss: 4.9147\n",
      "Epoch [2/2], Step [34080/64305], Loss: 4.8366\n",
      "Epoch [2/2], Step [34090/64305], Loss: 4.7305\n",
      "Epoch [2/2], Step [34100/64305], Loss: 4.7702\n",
      "Epoch [2/2], Step [34110/64305], Loss: 4.6585\n",
      "Epoch [2/2], Step [34120/64305], Loss: 4.8175\n",
      "Epoch [2/2], Step [34130/64305], Loss: 4.7507\n",
      "Epoch [2/2], Step [34140/64305], Loss: 4.8125\n",
      "Epoch [2/2], Step [34150/64305], Loss: 4.7447\n",
      "Epoch [2/2], Step [34160/64305], Loss: 4.6559\n",
      "Epoch [2/2], Step [34170/64305], Loss: 4.7886\n",
      "Epoch [2/2], Step [34180/64305], Loss: 4.8990\n",
      "Epoch [2/2], Step [34190/64305], Loss: 4.8221\n",
      "Epoch [2/2], Step [34200/64305], Loss: 4.7142\n",
      "Epoch [2/2], Step [34210/64305], Loss: 4.9440\n",
      "Epoch [2/2], Step [34220/64305], Loss: 4.6421\n",
      "Epoch [2/2], Step [34230/64305], Loss: 4.8435\n",
      "Epoch [2/2], Step [34240/64305], Loss: 4.7611\n",
      "Epoch [2/2], Step [34250/64305], Loss: 4.6572\n",
      "Epoch [2/2], Step [34260/64305], Loss: 4.7234\n",
      "Epoch [2/2], Step [34270/64305], Loss: 4.6593\n",
      "Epoch [2/2], Step [34280/64305], Loss: 4.8251\n",
      "Epoch [2/2], Step [34290/64305], Loss: 4.9064\n",
      "Epoch [2/2], Step [34300/64305], Loss: 4.8529\n",
      "Epoch [2/2], Step [34310/64305], Loss: 4.6834\n",
      "Epoch [2/2], Step [34320/64305], Loss: 4.7117\n",
      "Epoch [2/2], Step [34330/64305], Loss: 4.7007\n",
      "Epoch [2/2], Step [34340/64305], Loss: 4.6654\n",
      "Epoch [2/2], Step [34350/64305], Loss: 4.7436\n",
      "Epoch [2/2], Step [34360/64305], Loss: 4.6938\n",
      "Epoch [2/2], Step [34370/64305], Loss: 4.6055\n",
      "Epoch [2/2], Step [34380/64305], Loss: 4.7960\n",
      "Epoch [2/2], Step [34390/64305], Loss: 4.7100\n",
      "Epoch [2/2], Step [34400/64305], Loss: 4.8128\n",
      "Epoch [2/2], Step [34410/64305], Loss: 4.7882\n",
      "Epoch [2/2], Step [34420/64305], Loss: 4.7678\n",
      "Epoch [2/2], Step [34430/64305], Loss: 4.8037\n",
      "Epoch [2/2], Step [34440/64305], Loss: 4.9606\n",
      "Epoch [2/2], Step [34450/64305], Loss: 4.7641\n",
      "Epoch [2/2], Step [34460/64305], Loss: 4.5771\n",
      "Epoch [2/2], Step [34470/64305], Loss: 4.8119\n",
      "Epoch [2/2], Step [34480/64305], Loss: 4.9017\n",
      "Epoch [2/2], Step [34490/64305], Loss: 4.7355\n",
      "Epoch [2/2], Step [34500/64305], Loss: 4.8177\n",
      "Epoch [2/2], Step [34510/64305], Loss: 4.8475\n",
      "Epoch [2/2], Step [34520/64305], Loss: 4.6955\n",
      "Epoch [2/2], Step [34530/64305], Loss: 4.7187\n",
      "Epoch [2/2], Step [34540/64305], Loss: 4.6607\n",
      "Epoch [2/2], Step [34550/64305], Loss: 4.7088\n",
      "Epoch [2/2], Step [34560/64305], Loss: 4.9394\n",
      "Epoch [2/2], Step [34570/64305], Loss: 4.6891\n",
      "Epoch [2/2], Step [34580/64305], Loss: 4.8123\n",
      "Epoch [2/2], Step [34590/64305], Loss: 4.6113\n",
      "Epoch [2/2], Step [34600/64305], Loss: 4.8174\n",
      "Epoch [2/2], Step [34610/64305], Loss: 4.5097\n",
      "Epoch [2/2], Step [34620/64305], Loss: 4.9104\n",
      "Epoch [2/2], Step [34630/64305], Loss: 4.9729\n",
      "Epoch [2/2], Step [34640/64305], Loss: 4.7512\n",
      "Epoch [2/2], Step [34650/64305], Loss: 4.6525\n",
      "Epoch [2/2], Step [34660/64305], Loss: 4.6705\n",
      "Epoch [2/2], Step [34670/64305], Loss: 4.6489\n",
      "Epoch [2/2], Step [34680/64305], Loss: 4.5778\n",
      "Epoch [2/2], Step [34690/64305], Loss: 4.6560\n",
      "Epoch [2/2], Step [34700/64305], Loss: 4.8060\n",
      "Epoch [2/2], Step [34710/64305], Loss: 4.6007\n",
      "Epoch [2/2], Step [34720/64305], Loss: 4.7763\n",
      "Epoch [2/2], Step [34730/64305], Loss: 4.7986\n",
      "Epoch [2/2], Step [34740/64305], Loss: 4.9273\n",
      "Epoch [2/2], Step [34750/64305], Loss: 4.6487\n",
      "Epoch [2/2], Step [34760/64305], Loss: 4.6902\n",
      "Epoch [2/2], Step [34770/64305], Loss: 4.7440\n",
      "Epoch [2/2], Step [34780/64305], Loss: 4.6743\n",
      "Epoch [2/2], Step [34790/64305], Loss: 4.7670\n",
      "Epoch [2/2], Step [34800/64305], Loss: 4.6911\n",
      "Epoch [2/2], Step [34810/64305], Loss: 4.6217\n",
      "Epoch [2/2], Step [34820/64305], Loss: 4.6501\n",
      "Epoch [2/2], Step [34830/64305], Loss: 4.6475\n",
      "Epoch [2/2], Step [34840/64305], Loss: 4.8269\n",
      "Epoch [2/2], Step [34850/64305], Loss: 4.8600\n",
      "Epoch [2/2], Step [34860/64305], Loss: 4.8513\n",
      "Epoch [2/2], Step [34870/64305], Loss: 4.6452\n",
      "Epoch [2/2], Step [34880/64305], Loss: 4.6911\n",
      "Epoch [2/2], Step [34890/64305], Loss: 4.8477\n",
      "Epoch [2/2], Step [34900/64305], Loss: 4.4753\n",
      "Epoch [2/2], Step [34910/64305], Loss: 4.7533\n",
      "Epoch [2/2], Step [34920/64305], Loss: 4.6044\n",
      "Epoch [2/2], Step [34930/64305], Loss: 4.8932\n",
      "Epoch [2/2], Step [34940/64305], Loss: 4.7420\n",
      "Epoch [2/2], Step [34950/64305], Loss: 4.8741\n",
      "Epoch [2/2], Step [34960/64305], Loss: 4.6996\n",
      "Epoch [2/2], Step [34970/64305], Loss: 4.7382\n",
      "Epoch [2/2], Step [34980/64305], Loss: 4.7921\n",
      "Epoch [2/2], Step [34990/64305], Loss: 4.9155\n",
      "Epoch [2/2], Step [35000/64305], Loss: 4.8350\n",
      "Epoch [2/2], Step [35010/64305], Loss: 4.6839\n",
      "Epoch [2/2], Step [35020/64305], Loss: 4.8624\n",
      "Epoch [2/2], Step [35030/64305], Loss: 4.6634\n",
      "Epoch [2/2], Step [35040/64305], Loss: 4.7036\n",
      "Epoch [2/2], Step [35050/64305], Loss: 4.7314\n",
      "Epoch [2/2], Step [35060/64305], Loss: 4.7547\n",
      "Epoch [2/2], Step [35070/64305], Loss: 4.7137\n",
      "Epoch [2/2], Step [35080/64305], Loss: 4.7882\n",
      "Epoch [2/2], Step [35090/64305], Loss: 4.7073\n",
      "Epoch [2/2], Step [35100/64305], Loss: 4.7675\n",
      "Epoch [2/2], Step [35110/64305], Loss: 4.8404\n",
      "Epoch [2/2], Step [35120/64305], Loss: 4.7563\n",
      "Epoch [2/2], Step [35130/64305], Loss: 4.8855\n",
      "Epoch [2/2], Step [35140/64305], Loss: 4.8008\n",
      "Epoch [2/2], Step [35150/64305], Loss: 4.6453\n",
      "Epoch [2/2], Step [35160/64305], Loss: 4.8121\n",
      "Epoch [2/2], Step [35170/64305], Loss: 4.6570\n",
      "Epoch [2/2], Step [35180/64305], Loss: 4.6820\n",
      "Epoch [2/2], Step [35190/64305], Loss: 4.6060\n",
      "Epoch [2/2], Step [35200/64305], Loss: 4.7547\n",
      "Epoch [2/2], Step [35210/64305], Loss: 4.8780\n",
      "Epoch [2/2], Step [35220/64305], Loss: 4.8296\n",
      "Epoch [2/2], Step [35230/64305], Loss: 4.7982\n",
      "Epoch [2/2], Step [35240/64305], Loss: 4.5731\n",
      "Epoch [2/2], Step [35250/64305], Loss: 4.9178\n",
      "Epoch [2/2], Step [35260/64305], Loss: 4.6768\n",
      "Epoch [2/2], Step [35270/64305], Loss: 4.7854\n",
      "Epoch [2/2], Step [35280/64305], Loss: 4.7860\n",
      "Epoch [2/2], Step [35290/64305], Loss: 4.7228\n",
      "Epoch [2/2], Step [35300/64305], Loss: 4.6915\n",
      "Epoch [2/2], Step [35310/64305], Loss: 4.6225\n",
      "Epoch [2/2], Step [35320/64305], Loss: 4.6009\n",
      "Epoch [2/2], Step [35330/64305], Loss: 4.8695\n",
      "Epoch [2/2], Step [35340/64305], Loss: 4.7448\n",
      "Epoch [2/2], Step [35350/64305], Loss: 4.6896\n",
      "Epoch [2/2], Step [35360/64305], Loss: 4.7039\n",
      "Epoch [2/2], Step [35370/64305], Loss: 4.7278\n",
      "Epoch [2/2], Step [35380/64305], Loss: 4.6697\n",
      "Epoch [2/2], Step [35390/64305], Loss: 4.6971\n",
      "Epoch [2/2], Step [35400/64305], Loss: 4.8024\n",
      "Epoch [2/2], Step [35410/64305], Loss: 4.7400\n",
      "Epoch [2/2], Step [35420/64305], Loss: 4.7740\n",
      "Epoch [2/2], Step [35430/64305], Loss: 4.6418\n",
      "Epoch [2/2], Step [35440/64305], Loss: 4.7217\n",
      "Epoch [2/2], Step [35450/64305], Loss: 4.8379\n",
      "Epoch [2/2], Step [35460/64305], Loss: 4.8061\n",
      "Epoch [2/2], Step [35470/64305], Loss: 4.7609\n",
      "Epoch [2/2], Step [35480/64305], Loss: 4.8285\n",
      "Epoch [2/2], Step [35490/64305], Loss: 4.7265\n",
      "Epoch [2/2], Step [35500/64305], Loss: 4.7858\n",
      "Epoch [2/2], Step [35510/64305], Loss: 4.8354\n",
      "Epoch [2/2], Step [35520/64305], Loss: 4.6582\n",
      "Epoch [2/2], Step [35530/64305], Loss: 4.9791\n",
      "Epoch [2/2], Step [35540/64305], Loss: 4.5987\n",
      "Epoch [2/2], Step [35550/64305], Loss: 4.7616\n",
      "Epoch [2/2], Step [35560/64305], Loss: 4.6578\n",
      "Epoch [2/2], Step [35570/64305], Loss: 4.7975\n",
      "Epoch [2/2], Step [35580/64305], Loss: 4.7075\n",
      "Epoch [2/2], Step [35590/64305], Loss: 4.7995\n",
      "Epoch [2/2], Step [35600/64305], Loss: 4.6987\n",
      "Epoch [2/2], Step [35610/64305], Loss: 4.7420\n",
      "Epoch [2/2], Step [35620/64305], Loss: 4.4663\n",
      "Epoch [2/2], Step [35630/64305], Loss: 4.7263\n",
      "Epoch [2/2], Step [35640/64305], Loss: 4.5450\n",
      "Epoch [2/2], Step [35650/64305], Loss: 4.7783\n",
      "Epoch [2/2], Step [35660/64305], Loss: 4.8539\n",
      "Epoch [2/2], Step [35670/64305], Loss: 4.8375\n",
      "Epoch [2/2], Step [35680/64305], Loss: 4.8879\n",
      "Epoch [2/2], Step [35690/64305], Loss: 4.8444\n",
      "Epoch [2/2], Step [35700/64305], Loss: 4.6837\n",
      "Epoch [2/2], Step [35710/64305], Loss: 4.6600\n",
      "Epoch [2/2], Step [35720/64305], Loss: 4.7585\n",
      "Epoch [2/2], Step [35730/64305], Loss: 4.7724\n",
      "Epoch [2/2], Step [35740/64305], Loss: 4.7450\n",
      "Epoch [2/2], Step [35750/64305], Loss: 4.9469\n",
      "Epoch [2/2], Step [35760/64305], Loss: 4.9453\n",
      "Epoch [2/2], Step [35770/64305], Loss: 4.5097\n",
      "Epoch [2/2], Step [35780/64305], Loss: 4.9627\n",
      "Epoch [2/2], Step [35790/64305], Loss: 4.6466\n",
      "Epoch [2/2], Step [35800/64305], Loss: 4.5955\n",
      "Epoch [2/2], Step [35810/64305], Loss: 4.7267\n",
      "Epoch [2/2], Step [35820/64305], Loss: 4.7847\n",
      "Epoch [2/2], Step [35830/64305], Loss: 4.6796\n",
      "Epoch [2/2], Step [35840/64305], Loss: 4.6547\n",
      "Epoch [2/2], Step [35850/64305], Loss: 4.9076\n",
      "Epoch [2/2], Step [35860/64305], Loss: 4.6629\n",
      "Epoch [2/2], Step [35870/64305], Loss: 4.9866\n",
      "Epoch [2/2], Step [35880/64305], Loss: 4.6857\n",
      "Epoch [2/2], Step [35890/64305], Loss: 4.7736\n",
      "Epoch [2/2], Step [35900/64305], Loss: 4.6426\n",
      "Epoch [2/2], Step [35910/64305], Loss: 4.6941\n",
      "Epoch [2/2], Step [35920/64305], Loss: 4.8110\n",
      "Epoch [2/2], Step [35930/64305], Loss: 4.8604\n",
      "Epoch [2/2], Step [35940/64305], Loss: 4.7085\n",
      "Epoch [2/2], Step [35950/64305], Loss: 4.8399\n",
      "Epoch [2/2], Step [35960/64305], Loss: 4.8360\n",
      "Epoch [2/2], Step [35970/64305], Loss: 4.7939\n",
      "Epoch [2/2], Step [35980/64305], Loss: 4.8389\n",
      "Epoch [2/2], Step [35990/64305], Loss: 4.5836\n",
      "Epoch [2/2], Step [36000/64305], Loss: 4.8582\n",
      "Epoch [2/2], Step [36010/64305], Loss: 4.9113\n",
      "Epoch [2/2], Step [36020/64305], Loss: 4.7370\n",
      "Epoch [2/2], Step [36030/64305], Loss: 4.8337\n",
      "Epoch [2/2], Step [36040/64305], Loss: 4.8088\n",
      "Epoch [2/2], Step [36050/64305], Loss: 4.6658\n",
      "Epoch [2/2], Step [36060/64305], Loss: 4.9140\n",
      "Epoch [2/2], Step [36070/64305], Loss: 4.5460\n",
      "Epoch [2/2], Step [36080/64305], Loss: 4.9077\n",
      "Epoch [2/2], Step [36090/64305], Loss: 4.9757\n",
      "Epoch [2/2], Step [36100/64305], Loss: 4.8016\n",
      "Epoch [2/2], Step [36110/64305], Loss: 4.9751\n",
      "Epoch [2/2], Step [36120/64305], Loss: 4.6395\n",
      "Epoch [2/2], Step [36130/64305], Loss: 4.5389\n",
      "Epoch [2/2], Step [36140/64305], Loss: 4.7584\n",
      "Epoch [2/2], Step [36150/64305], Loss: 4.5992\n",
      "Epoch [2/2], Step [36160/64305], Loss: 5.0227\n",
      "Epoch [2/2], Step [36170/64305], Loss: 4.8886\n",
      "Epoch [2/2], Step [36180/64305], Loss: 4.8339\n",
      "Epoch [2/2], Step [36190/64305], Loss: 4.7855\n",
      "Epoch [2/2], Step [36200/64305], Loss: 4.6458\n",
      "Epoch [2/2], Step [36210/64305], Loss: 4.8082\n",
      "Epoch [2/2], Step [36220/64305], Loss: 4.7472\n",
      "Epoch [2/2], Step [36230/64305], Loss: 4.7076\n",
      "Epoch [2/2], Step [36240/64305], Loss: 4.7167\n",
      "Epoch [2/2], Step [36250/64305], Loss: 4.7723\n",
      "Epoch [2/2], Step [36260/64305], Loss: 4.8472\n",
      "Epoch [2/2], Step [36270/64305], Loss: 4.6565\n",
      "Epoch [2/2], Step [36280/64305], Loss: 4.8121\n",
      "Epoch [2/2], Step [36290/64305], Loss: 4.8546\n",
      "Epoch [2/2], Step [36300/64305], Loss: 4.7527\n",
      "Epoch [2/2], Step [36310/64305], Loss: 4.6059\n",
      "Epoch [2/2], Step [36320/64305], Loss: 4.7950\n",
      "Epoch [2/2], Step [36330/64305], Loss: 4.7478\n",
      "Epoch [2/2], Step [36340/64305], Loss: 4.6321\n",
      "Epoch [2/2], Step [36350/64305], Loss: 4.6633\n",
      "Epoch [2/2], Step [36360/64305], Loss: 4.8245\n",
      "Epoch [2/2], Step [36370/64305], Loss: 4.6871\n",
      "Epoch [2/2], Step [36380/64305], Loss: 4.6044\n",
      "Epoch [2/2], Step [36390/64305], Loss: 4.7528\n",
      "Epoch [2/2], Step [36400/64305], Loss: 4.4954\n",
      "Epoch [2/2], Step [36410/64305], Loss: 4.7242\n",
      "Epoch [2/2], Step [36420/64305], Loss: 4.9002\n",
      "Epoch [2/2], Step [36430/64305], Loss: 4.8322\n",
      "Epoch [2/2], Step [36440/64305], Loss: 4.7609\n",
      "Epoch [2/2], Step [36450/64305], Loss: 4.4943\n",
      "Epoch [2/2], Step [36460/64305], Loss: 4.9736\n",
      "Epoch [2/2], Step [36470/64305], Loss: 5.0210\n",
      "Epoch [2/2], Step [36480/64305], Loss: 4.5929\n",
      "Epoch [2/2], Step [36490/64305], Loss: 4.6682\n",
      "Epoch [2/2], Step [36500/64305], Loss: 4.9099\n",
      "Epoch [2/2], Step [36510/64305], Loss: 4.5389\n",
      "Epoch [2/2], Step [36520/64305], Loss: 5.0400\n",
      "Epoch [2/2], Step [36530/64305], Loss: 4.7859\n",
      "Epoch [2/2], Step [36540/64305], Loss: 4.6404\n",
      "Epoch [2/2], Step [36550/64305], Loss: 4.7673\n",
      "Epoch [2/2], Step [36560/64305], Loss: 4.5395\n",
      "Epoch [2/2], Step [36570/64305], Loss: 4.8365\n",
      "Epoch [2/2], Step [36580/64305], Loss: 4.6430\n",
      "Epoch [2/2], Step [36590/64305], Loss: 4.8994\n",
      "Epoch [2/2], Step [36600/64305], Loss: 4.7747\n",
      "Epoch [2/2], Step [36610/64305], Loss: 4.8701\n",
      "Epoch [2/2], Step [36620/64305], Loss: 4.6009\n",
      "Epoch [2/2], Step [36630/64305], Loss: 4.7572\n",
      "Epoch [2/2], Step [36640/64305], Loss: 4.7092\n",
      "Epoch [2/2], Step [36650/64305], Loss: 4.8760\n",
      "Epoch [2/2], Step [36660/64305], Loss: 4.7055\n",
      "Epoch [2/2], Step [36670/64305], Loss: 4.6909\n",
      "Epoch [2/2], Step [36680/64305], Loss: 4.5466\n",
      "Epoch [2/2], Step [36690/64305], Loss: 4.9137\n",
      "Epoch [2/2], Step [36700/64305], Loss: 4.5278\n",
      "Epoch [2/2], Step [36710/64305], Loss: 4.8247\n",
      "Epoch [2/2], Step [36720/64305], Loss: 4.8053\n",
      "Epoch [2/2], Step [36730/64305], Loss: 4.7146\n",
      "Epoch [2/2], Step [36740/64305], Loss: 4.5071\n",
      "Epoch [2/2], Step [36750/64305], Loss: 4.7089\n",
      "Epoch [2/2], Step [36760/64305], Loss: 4.7015\n",
      "Epoch [2/2], Step [36770/64305], Loss: 4.7445\n",
      "Epoch [2/2], Step [36780/64305], Loss: 4.8800\n",
      "Epoch [2/2], Step [36790/64305], Loss: 4.8534\n",
      "Epoch [2/2], Step [36800/64305], Loss: 4.5016\n",
      "Epoch [2/2], Step [36810/64305], Loss: 4.7048\n",
      "Epoch [2/2], Step [36820/64305], Loss: 4.7628\n",
      "Epoch [2/2], Step [36830/64305], Loss: 4.9015\n",
      "Epoch [2/2], Step [36840/64305], Loss: 4.6451\n",
      "Epoch [2/2], Step [36850/64305], Loss: 4.7307\n",
      "Epoch [2/2], Step [36860/64305], Loss: 4.8403\n",
      "Epoch [2/2], Step [36870/64305], Loss: 4.5392\n",
      "Epoch [2/2], Step [36880/64305], Loss: 4.7803\n",
      "Epoch [2/2], Step [36890/64305], Loss: 4.6013\n",
      "Epoch [2/2], Step [36900/64305], Loss: 4.6535\n",
      "Epoch [2/2], Step [36910/64305], Loss: 4.7393\n",
      "Epoch [2/2], Step [36920/64305], Loss: 4.8241\n",
      "Epoch [2/2], Step [36930/64305], Loss: 4.7795\n",
      "Epoch [2/2], Step [36940/64305], Loss: 4.9173\n",
      "Epoch [2/2], Step [36950/64305], Loss: 4.6382\n",
      "Epoch [2/2], Step [36960/64305], Loss: 4.7776\n",
      "Epoch [2/2], Step [36970/64305], Loss: 4.5357\n",
      "Epoch [2/2], Step [36980/64305], Loss: 4.7573\n",
      "Epoch [2/2], Step [36990/64305], Loss: 4.8717\n",
      "Epoch [2/2], Step [37000/64305], Loss: 4.7356\n",
      "Epoch [2/2], Step [37010/64305], Loss: 4.6584\n",
      "Epoch [2/2], Step [37020/64305], Loss: 4.7964\n",
      "Epoch [2/2], Step [37030/64305], Loss: 4.9469\n",
      "Epoch [2/2], Step [37040/64305], Loss: 4.9648\n",
      "Epoch [2/2], Step [37050/64305], Loss: 4.8779\n",
      "Epoch [2/2], Step [37060/64305], Loss: 4.7395\n",
      "Epoch [2/2], Step [37070/64305], Loss: 4.7995\n",
      "Epoch [2/2], Step [37080/64305], Loss: 4.9509\n",
      "Epoch [2/2], Step [37090/64305], Loss: 4.6330\n",
      "Epoch [2/2], Step [37100/64305], Loss: 4.6996\n",
      "Epoch [2/2], Step [37110/64305], Loss: 4.8241\n",
      "Epoch [2/2], Step [37120/64305], Loss: 4.4959\n",
      "Epoch [2/2], Step [37130/64305], Loss: 4.6132\n",
      "Epoch [2/2], Step [37140/64305], Loss: 4.8655\n",
      "Epoch [2/2], Step [37150/64305], Loss: 4.7038\n",
      "Epoch [2/2], Step [37160/64305], Loss: 4.6023\n",
      "Epoch [2/2], Step [37170/64305], Loss: 4.5118\n",
      "Epoch [2/2], Step [37180/64305], Loss: 4.9560\n",
      "Epoch [2/2], Step [37190/64305], Loss: 4.7928\n",
      "Epoch [2/2], Step [37200/64305], Loss: 4.7270\n",
      "Epoch [2/2], Step [37210/64305], Loss: 4.7557\n",
      "Epoch [2/2], Step [37220/64305], Loss: 4.8196\n",
      "Epoch [2/2], Step [37230/64305], Loss: 4.6492\n",
      "Epoch [2/2], Step [37240/64305], Loss: 4.7249\n",
      "Epoch [2/2], Step [37250/64305], Loss: 4.7405\n",
      "Epoch [2/2], Step [37260/64305], Loss: 4.4376\n",
      "Epoch [2/2], Step [37270/64305], Loss: 4.7519\n",
      "Epoch [2/2], Step [37280/64305], Loss: 4.8713\n",
      "Epoch [2/2], Step [37290/64305], Loss: 4.7714\n",
      "Epoch [2/2], Step [37300/64305], Loss: 4.9413\n",
      "Epoch [2/2], Step [37310/64305], Loss: 4.7467\n",
      "Epoch [2/2], Step [37320/64305], Loss: 4.6032\n",
      "Epoch [2/2], Step [37330/64305], Loss: 4.7222\n",
      "Epoch [2/2], Step [37340/64305], Loss: 4.8476\n",
      "Epoch [2/2], Step [37350/64305], Loss: 4.8170\n",
      "Epoch [2/2], Step [37360/64305], Loss: 4.6543\n",
      "Epoch [2/2], Step [37370/64305], Loss: 4.7366\n",
      "Epoch [2/2], Step [37380/64305], Loss: 4.8083\n",
      "Epoch [2/2], Step [37390/64305], Loss: 4.7294\n",
      "Epoch [2/2], Step [37400/64305], Loss: 4.7267\n",
      "Epoch [2/2], Step [37410/64305], Loss: 4.6803\n",
      "Epoch [2/2], Step [37420/64305], Loss: 4.6944\n",
      "Epoch [2/2], Step [37430/64305], Loss: 4.7077\n",
      "Epoch [2/2], Step [37440/64305], Loss: 4.8200\n",
      "Epoch [2/2], Step [37450/64305], Loss: 4.7534\n",
      "Epoch [2/2], Step [37460/64305], Loss: 4.7528\n",
      "Epoch [2/2], Step [37470/64305], Loss: 4.7170\n",
      "Epoch [2/2], Step [37480/64305], Loss: 4.8030\n",
      "Epoch [2/2], Step [37490/64305], Loss: 4.7297\n",
      "Epoch [2/2], Step [37500/64305], Loss: 4.8016\n",
      "Epoch [2/2], Step [37510/64305], Loss: 4.7284\n",
      "Epoch [2/2], Step [37520/64305], Loss: 4.7368\n",
      "Epoch [2/2], Step [37530/64305], Loss: 4.7201\n",
      "Epoch [2/2], Step [37540/64305], Loss: 4.7856\n",
      "Epoch [2/2], Step [37550/64305], Loss: 4.8438\n",
      "Epoch [2/2], Step [37560/64305], Loss: 4.8592\n",
      "Epoch [2/2], Step [37570/64305], Loss: 4.7914\n",
      "Epoch [2/2], Step [37580/64305], Loss: 4.9703\n",
      "Epoch [2/2], Step [37590/64305], Loss: 4.6727\n",
      "Epoch [2/2], Step [37600/64305], Loss: 4.8741\n",
      "Epoch [2/2], Step [37610/64305], Loss: 4.4809\n",
      "Epoch [2/2], Step [37620/64305], Loss: 4.6718\n",
      "Epoch [2/2], Step [37630/64305], Loss: 4.8030\n",
      "Epoch [2/2], Step [37640/64305], Loss: 4.9253\n",
      "Epoch [2/2], Step [37650/64305], Loss: 4.9067\n",
      "Epoch [2/2], Step [37660/64305], Loss: 4.7319\n",
      "Epoch [2/2], Step [37670/64305], Loss: 4.7426\n",
      "Epoch [2/2], Step [37680/64305], Loss: 4.6905\n",
      "Epoch [2/2], Step [37690/64305], Loss: 4.5274\n",
      "Epoch [2/2], Step [37700/64305], Loss: 4.6815\n",
      "Epoch [2/2], Step [37710/64305], Loss: 4.6526\n",
      "Epoch [2/2], Step [37720/64305], Loss: 4.8213\n",
      "Epoch [2/2], Step [37730/64305], Loss: 4.6998\n",
      "Epoch [2/2], Step [37740/64305], Loss: 4.7118\n",
      "Epoch [2/2], Step [37750/64305], Loss: 4.8443\n",
      "Epoch [2/2], Step [37760/64305], Loss: 4.9380\n",
      "Epoch [2/2], Step [37770/64305], Loss: 4.7513\n",
      "Epoch [2/2], Step [37780/64305], Loss: 4.7525\n",
      "Epoch [2/2], Step [37790/64305], Loss: 4.6947\n",
      "Epoch [2/2], Step [37800/64305], Loss: 4.6513\n",
      "Epoch [2/2], Step [37810/64305], Loss: 4.6681\n",
      "Epoch [2/2], Step [37820/64305], Loss: 4.9359\n",
      "Epoch [2/2], Step [37830/64305], Loss: 4.9147\n",
      "Epoch [2/2], Step [37840/64305], Loss: 4.8441\n",
      "Epoch [2/2], Step [37850/64305], Loss: 4.3969\n",
      "Epoch [2/2], Step [37860/64305], Loss: 4.9183\n",
      "Epoch [2/2], Step [37870/64305], Loss: 4.7498\n",
      "Epoch [2/2], Step [37880/64305], Loss: 4.7964\n",
      "Epoch [2/2], Step [37890/64305], Loss: 4.7014\n",
      "Epoch [2/2], Step [37900/64305], Loss: 4.8019\n",
      "Epoch [2/2], Step [37910/64305], Loss: 4.7670\n",
      "Epoch [2/2], Step [37920/64305], Loss: 4.7145\n",
      "Epoch [2/2], Step [37930/64305], Loss: 4.8059\n",
      "Epoch [2/2], Step [37940/64305], Loss: 4.7555\n",
      "Epoch [2/2], Step [37950/64305], Loss: 4.6774\n",
      "Epoch [2/2], Step [37960/64305], Loss: 4.6642\n",
      "Epoch [2/2], Step [37970/64305], Loss: 4.6989\n",
      "Epoch [2/2], Step [37980/64305], Loss: 4.6142\n",
      "Epoch [2/2], Step [37990/64305], Loss: 4.8774\n",
      "Epoch [2/2], Step [38000/64305], Loss: 4.8089\n",
      "Epoch [2/2], Step [38010/64305], Loss: 4.8753\n",
      "Epoch [2/2], Step [38020/64305], Loss: 4.6832\n",
      "Epoch [2/2], Step [38030/64305], Loss: 4.7226\n",
      "Epoch [2/2], Step [38040/64305], Loss: 4.6882\n",
      "Epoch [2/2], Step [38050/64305], Loss: 4.7373\n",
      "Epoch [2/2], Step [38060/64305], Loss: 4.7140\n",
      "Epoch [2/2], Step [38070/64305], Loss: 4.6596\n",
      "Epoch [2/2], Step [38080/64305], Loss: 4.7809\n",
      "Epoch [2/2], Step [38090/64305], Loss: 4.8563\n",
      "Epoch [2/2], Step [38100/64305], Loss: 4.7451\n",
      "Epoch [2/2], Step [38110/64305], Loss: 4.7181\n",
      "Epoch [2/2], Step [38120/64305], Loss: 4.8510\n",
      "Epoch [2/2], Step [38130/64305], Loss: 4.9243\n",
      "Epoch [2/2], Step [38140/64305], Loss: 4.5824\n",
      "Epoch [2/2], Step [38150/64305], Loss: 4.5498\n",
      "Epoch [2/2], Step [38160/64305], Loss: 4.6116\n",
      "Epoch [2/2], Step [38170/64305], Loss: 4.9159\n",
      "Epoch [2/2], Step [38180/64305], Loss: 4.6719\n",
      "Epoch [2/2], Step [38190/64305], Loss: 4.6790\n",
      "Epoch [2/2], Step [38200/64305], Loss: 4.6282\n",
      "Epoch [2/2], Step [38210/64305], Loss: 4.7962\n",
      "Epoch [2/2], Step [38220/64305], Loss: 4.7218\n",
      "Epoch [2/2], Step [38230/64305], Loss: 4.8119\n",
      "Epoch [2/2], Step [38240/64305], Loss: 4.6470\n",
      "Epoch [2/2], Step [38250/64305], Loss: 4.8585\n",
      "Epoch [2/2], Step [38260/64305], Loss: 4.7701\n",
      "Epoch [2/2], Step [38270/64305], Loss: 4.7561\n",
      "Epoch [2/2], Step [38280/64305], Loss: 4.8277\n",
      "Epoch [2/2], Step [38290/64305], Loss: 4.9052\n",
      "Epoch [2/2], Step [38300/64305], Loss: 4.7567\n",
      "Epoch [2/2], Step [38310/64305], Loss: 4.6352\n",
      "Epoch [2/2], Step [38320/64305], Loss: 4.8092\n",
      "Epoch [2/2], Step [38330/64305], Loss: 4.8239\n",
      "Epoch [2/2], Step [38340/64305], Loss: 4.6211\n",
      "Epoch [2/2], Step [38350/64305], Loss: 4.7808\n",
      "Epoch [2/2], Step [38360/64305], Loss: 4.5348\n",
      "Epoch [2/2], Step [38370/64305], Loss: 4.8313\n",
      "Epoch [2/2], Step [38380/64305], Loss: 4.8467\n",
      "Epoch [2/2], Step [38390/64305], Loss: 4.6448\n",
      "Epoch [2/2], Step [38400/64305], Loss: 4.9740\n",
      "Epoch [2/2], Step [38410/64305], Loss: 4.8067\n",
      "Epoch [2/2], Step [38420/64305], Loss: 4.7559\n",
      "Epoch [2/2], Step [38430/64305], Loss: 4.7664\n",
      "Epoch [2/2], Step [38440/64305], Loss: 4.8001\n",
      "Epoch [2/2], Step [38450/64305], Loss: 4.8739\n",
      "Epoch [2/2], Step [38460/64305], Loss: 4.8487\n",
      "Epoch [2/2], Step [38470/64305], Loss: 4.7807\n",
      "Epoch [2/2], Step [38480/64305], Loss: 4.7687\n",
      "Epoch [2/2], Step [38490/64305], Loss: 4.8735\n",
      "Epoch [2/2], Step [38500/64305], Loss: 4.7523\n",
      "Epoch [2/2], Step [38510/64305], Loss: 4.8355\n",
      "Epoch [2/2], Step [38520/64305], Loss: 4.8217\n",
      "Epoch [2/2], Step [38530/64305], Loss: 4.7032\n",
      "Epoch [2/2], Step [38540/64305], Loss: 4.7393\n",
      "Epoch [2/2], Step [38550/64305], Loss: 4.9209\n",
      "Epoch [2/2], Step [38560/64305], Loss: 4.7024\n",
      "Epoch [2/2], Step [38570/64305], Loss: 4.7019\n",
      "Epoch [2/2], Step [38580/64305], Loss: 4.5326\n",
      "Epoch [2/2], Step [38590/64305], Loss: 4.8365\n",
      "Epoch [2/2], Step [38600/64305], Loss: 4.7400\n",
      "Epoch [2/2], Step [38610/64305], Loss: 4.7855\n",
      "Epoch [2/2], Step [38620/64305], Loss: 4.6616\n",
      "Epoch [2/2], Step [38630/64305], Loss: 4.9100\n",
      "Epoch [2/2], Step [38640/64305], Loss: 4.8906\n",
      "Epoch [2/2], Step [38650/64305], Loss: 4.8188\n",
      "Epoch [2/2], Step [38660/64305], Loss: 4.7459\n",
      "Epoch [2/2], Step [38670/64305], Loss: 4.8540\n",
      "Epoch [2/2], Step [38680/64305], Loss: 4.7692\n",
      "Epoch [2/2], Step [38690/64305], Loss: 4.7522\n",
      "Epoch [2/2], Step [38700/64305], Loss: 4.8294\n",
      "Epoch [2/2], Step [38710/64305], Loss: 4.6460\n",
      "Epoch [2/2], Step [38720/64305], Loss: 4.8021\n",
      "Epoch [2/2], Step [38730/64305], Loss: 4.6031\n",
      "Epoch [2/2], Step [38740/64305], Loss: 4.5779\n",
      "Epoch [2/2], Step [38750/64305], Loss: 4.6623\n",
      "Epoch [2/2], Step [38760/64305], Loss: 4.8006\n",
      "Epoch [2/2], Step [38770/64305], Loss: 4.8780\n",
      "Epoch [2/2], Step [38780/64305], Loss: 4.8033\n",
      "Epoch [2/2], Step [38790/64305], Loss: 4.4756\n",
      "Epoch [2/2], Step [38800/64305], Loss: 4.7493\n",
      "Epoch [2/2], Step [38810/64305], Loss: 4.8334\n",
      "Epoch [2/2], Step [38820/64305], Loss: 4.7386\n",
      "Epoch [2/2], Step [38830/64305], Loss: 4.7914\n",
      "Epoch [2/2], Step [38840/64305], Loss: 4.7170\n",
      "Epoch [2/2], Step [38850/64305], Loss: 4.7212\n",
      "Epoch [2/2], Step [38860/64305], Loss: 4.6994\n",
      "Epoch [2/2], Step [38870/64305], Loss: 4.8225\n",
      "Epoch [2/2], Step [38880/64305], Loss: 4.9033\n",
      "Epoch [2/2], Step [38890/64305], Loss: 4.8331\n",
      "Epoch [2/2], Step [38900/64305], Loss: 4.7110\n",
      "Epoch [2/2], Step [38910/64305], Loss: 4.6611\n",
      "Epoch [2/2], Step [38920/64305], Loss: 4.6710\n",
      "Epoch [2/2], Step [38930/64305], Loss: 4.8609\n",
      "Epoch [2/2], Step [38940/64305], Loss: 4.7300\n",
      "Epoch [2/2], Step [38950/64305], Loss: 4.8327\n",
      "Epoch [2/2], Step [38960/64305], Loss: 4.5222\n",
      "Epoch [2/2], Step [38970/64305], Loss: 4.6866\n",
      "Epoch [2/2], Step [38980/64305], Loss: 4.9226\n",
      "Epoch [2/2], Step [38990/64305], Loss: 4.6953\n",
      "Epoch [2/2], Step [39000/64305], Loss: 4.5386\n",
      "Epoch [2/2], Step [39010/64305], Loss: 4.8478\n",
      "Epoch [2/2], Step [39020/64305], Loss: 4.6425\n",
      "Epoch [2/2], Step [39030/64305], Loss: 4.7668\n",
      "Epoch [2/2], Step [39040/64305], Loss: 4.7723\n",
      "Epoch [2/2], Step [39050/64305], Loss: 4.8313\n",
      "Epoch [2/2], Step [39060/64305], Loss: 4.7426\n",
      "Epoch [2/2], Step [39070/64305], Loss: 4.7419\n",
      "Epoch [2/2], Step [39080/64305], Loss: 4.8601\n",
      "Epoch [2/2], Step [39090/64305], Loss: 4.7794\n",
      "Epoch [2/2], Step [39100/64305], Loss: 4.6211\n",
      "Epoch [2/2], Step [39110/64305], Loss: 4.8340\n",
      "Epoch [2/2], Step [39120/64305], Loss: 4.6900\n",
      "Epoch [2/2], Step [39130/64305], Loss: 4.7644\n",
      "Epoch [2/2], Step [39140/64305], Loss: 4.8026\n",
      "Epoch [2/2], Step [39150/64305], Loss: 4.7889\n",
      "Epoch [2/2], Step [39160/64305], Loss: 4.8892\n",
      "Epoch [2/2], Step [39170/64305], Loss: 4.7309\n",
      "Epoch [2/2], Step [39180/64305], Loss: 4.7964\n",
      "Epoch [2/2], Step [39190/64305], Loss: 4.5673\n",
      "Epoch [2/2], Step [39200/64305], Loss: 4.8690\n",
      "Epoch [2/2], Step [39210/64305], Loss: 4.7297\n",
      "Epoch [2/2], Step [39220/64305], Loss: 4.7095\n",
      "Epoch [2/2], Step [39230/64305], Loss: 4.6991\n",
      "Epoch [2/2], Step [39240/64305], Loss: 4.7652\n",
      "Epoch [2/2], Step [39250/64305], Loss: 4.7875\n",
      "Epoch [2/2], Step [39260/64305], Loss: 4.8088\n",
      "Epoch [2/2], Step [39270/64305], Loss: 4.7849\n",
      "Epoch [2/2], Step [39280/64305], Loss: 4.7576\n",
      "Epoch [2/2], Step [39290/64305], Loss: 4.9529\n",
      "Epoch [2/2], Step [39300/64305], Loss: 4.8521\n",
      "Epoch [2/2], Step [39310/64305], Loss: 4.5731\n",
      "Epoch [2/2], Step [39320/64305], Loss: 4.8352\n",
      "Epoch [2/2], Step [39330/64305], Loss: 4.8469\n",
      "Epoch [2/2], Step [39340/64305], Loss: 4.6758\n",
      "Epoch [2/2], Step [39350/64305], Loss: 4.6191\n",
      "Epoch [2/2], Step [39360/64305], Loss: 4.8713\n",
      "Epoch [2/2], Step [39370/64305], Loss: 4.7120\n",
      "Epoch [2/2], Step [39380/64305], Loss: 4.8734\n",
      "Epoch [2/2], Step [39390/64305], Loss: 4.7695\n",
      "Epoch [2/2], Step [39400/64305], Loss: 4.8109\n",
      "Epoch [2/2], Step [39410/64305], Loss: 4.8170\n",
      "Epoch [2/2], Step [39420/64305], Loss: 4.8687\n",
      "Epoch [2/2], Step [39430/64305], Loss: 4.8010\n",
      "Epoch [2/2], Step [39440/64305], Loss: 4.8557\n",
      "Epoch [2/2], Step [39450/64305], Loss: 4.8188\n",
      "Epoch [2/2], Step [39460/64305], Loss: 4.5579\n",
      "Epoch [2/2], Step [39470/64305], Loss: 4.7407\n",
      "Epoch [2/2], Step [39480/64305], Loss: 4.7426\n",
      "Epoch [2/2], Step [39490/64305], Loss: 4.6708\n",
      "Epoch [2/2], Step [39500/64305], Loss: 4.6313\n",
      "Epoch [2/2], Step [39510/64305], Loss: 4.8773\n",
      "Epoch [2/2], Step [39520/64305], Loss: 5.0286\n",
      "Epoch [2/2], Step [39530/64305], Loss: 4.7025\n",
      "Epoch [2/2], Step [39540/64305], Loss: 4.4957\n",
      "Epoch [2/2], Step [39550/64305], Loss: 4.6067\n",
      "Epoch [2/2], Step [39560/64305], Loss: 4.8124\n",
      "Epoch [2/2], Step [39570/64305], Loss: 4.7088\n",
      "Epoch [2/2], Step [39580/64305], Loss: 4.8856\n",
      "Epoch [2/2], Step [39590/64305], Loss: 4.6568\n",
      "Epoch [2/2], Step [39600/64305], Loss: 4.6282\n",
      "Epoch [2/2], Step [39610/64305], Loss: 4.6345\n",
      "Epoch [2/2], Step [39620/64305], Loss: 4.5459\n",
      "Epoch [2/2], Step [39630/64305], Loss: 4.7280\n",
      "Epoch [2/2], Step [39640/64305], Loss: 4.9361\n",
      "Epoch [2/2], Step [39650/64305], Loss: 4.8433\n",
      "Epoch [2/2], Step [39660/64305], Loss: 4.8647\n",
      "Epoch [2/2], Step [39670/64305], Loss: 4.7746\n",
      "Epoch [2/2], Step [39680/64305], Loss: 4.7003\n",
      "Epoch [2/2], Step [39690/64305], Loss: 4.7989\n",
      "Epoch [2/2], Step [39700/64305], Loss: 4.7705\n",
      "Epoch [2/2], Step [39710/64305], Loss: 4.7176\n",
      "Epoch [2/2], Step [39720/64305], Loss: 4.8419\n",
      "Epoch [2/2], Step [39730/64305], Loss: 4.6887\n",
      "Epoch [2/2], Step [39740/64305], Loss: 4.7454\n",
      "Epoch [2/2], Step [39750/64305], Loss: 4.8442\n",
      "Epoch [2/2], Step [39760/64305], Loss: 4.7578\n",
      "Epoch [2/2], Step [39770/64305], Loss: 4.9194\n",
      "Epoch [2/2], Step [39780/64305], Loss: 4.8305\n",
      "Epoch [2/2], Step [39790/64305], Loss: 4.5775\n",
      "Epoch [2/2], Step [39800/64305], Loss: 4.6512\n",
      "Epoch [2/2], Step [39810/64305], Loss: 4.8077\n",
      "Epoch [2/2], Step [39820/64305], Loss: 4.8552\n",
      "Epoch [2/2], Step [39830/64305], Loss: 4.8230\n",
      "Epoch [2/2], Step [39840/64305], Loss: 4.8550\n",
      "Epoch [2/2], Step [39850/64305], Loss: 4.7929\n",
      "Epoch [2/2], Step [39860/64305], Loss: 4.8128\n",
      "Epoch [2/2], Step [39870/64305], Loss: 4.4585\n",
      "Epoch [2/2], Step [39880/64305], Loss: 4.6408\n",
      "Epoch [2/2], Step [39890/64305], Loss: 4.5781\n",
      "Epoch [2/2], Step [39900/64305], Loss: 4.8009\n",
      "Epoch [2/2], Step [39910/64305], Loss: 4.6063\n",
      "Epoch [2/2], Step [39920/64305], Loss: 4.7782\n",
      "Epoch [2/2], Step [39930/64305], Loss: 4.9690\n",
      "Epoch [2/2], Step [39940/64305], Loss: 4.7446\n",
      "Epoch [2/2], Step [39950/64305], Loss: 4.8117\n",
      "Epoch [2/2], Step [39960/64305], Loss: 4.4908\n",
      "Epoch [2/2], Step [39970/64305], Loss: 4.8804\n",
      "Epoch [2/2], Step [39980/64305], Loss: 4.6535\n",
      "Epoch [2/2], Step [39990/64305], Loss: 4.6301\n",
      "Epoch [2/2], Step [40000/64305], Loss: 4.5637\n",
      "Epoch [2/2], Step [40010/64305], Loss: 4.6950\n",
      "Epoch [2/2], Step [40020/64305], Loss: 4.8516\n",
      "Epoch [2/2], Step [40030/64305], Loss: 4.7144\n",
      "Epoch [2/2], Step [40040/64305], Loss: 4.7803\n",
      "Epoch [2/2], Step [40050/64305], Loss: 4.9631\n",
      "Epoch [2/2], Step [40060/64305], Loss: 4.7779\n",
      "Epoch [2/2], Step [40070/64305], Loss: 4.6907\n",
      "Epoch [2/2], Step [40080/64305], Loss: 4.6749\n",
      "Epoch [2/2], Step [40090/64305], Loss: 4.8137\n",
      "Epoch [2/2], Step [40100/64305], Loss: 4.7066\n",
      "Epoch [2/2], Step [40110/64305], Loss: 4.7341\n",
      "Epoch [2/2], Step [40120/64305], Loss: 4.5990\n",
      "Epoch [2/2], Step [40130/64305], Loss: 4.7256\n",
      "Epoch [2/2], Step [40140/64305], Loss: 4.8801\n",
      "Epoch [2/2], Step [40150/64305], Loss: 4.7852\n",
      "Epoch [2/2], Step [40160/64305], Loss: 4.9161\n",
      "Epoch [2/2], Step [40170/64305], Loss: 4.5940\n",
      "Epoch [2/2], Step [40180/64305], Loss: 4.8058\n",
      "Epoch [2/2], Step [40190/64305], Loss: 4.7627\n",
      "Epoch [2/2], Step [40200/64305], Loss: 4.5777\n",
      "Epoch [2/2], Step [40210/64305], Loss: 4.7762\n",
      "Epoch [2/2], Step [40220/64305], Loss: 4.8117\n",
      "Epoch [2/2], Step [40230/64305], Loss: 4.8846\n",
      "Epoch [2/2], Step [40240/64305], Loss: 4.6888\n",
      "Epoch [2/2], Step [40250/64305], Loss: 4.9956\n",
      "Epoch [2/2], Step [40260/64305], Loss: 4.7964\n",
      "Epoch [2/2], Step [40270/64305], Loss: 4.8039\n",
      "Epoch [2/2], Step [40280/64305], Loss: 5.0329\n",
      "Epoch [2/2], Step [40290/64305], Loss: 4.7065\n",
      "Epoch [2/2], Step [40300/64305], Loss: 4.7072\n",
      "Epoch [2/2], Step [40310/64305], Loss: 4.6823\n",
      "Epoch [2/2], Step [40320/64305], Loss: 4.6908\n",
      "Epoch [2/2], Step [40330/64305], Loss: 4.9213\n",
      "Epoch [2/2], Step [40340/64305], Loss: 4.8912\n",
      "Epoch [2/2], Step [40350/64305], Loss: 4.6143\n",
      "Epoch [2/2], Step [40360/64305], Loss: 4.7549\n",
      "Epoch [2/2], Step [40370/64305], Loss: 4.7070\n",
      "Epoch [2/2], Step [40380/64305], Loss: 4.7904\n",
      "Epoch [2/2], Step [40390/64305], Loss: 4.9099\n",
      "Epoch [2/2], Step [40400/64305], Loss: 4.6230\n",
      "Epoch [2/2], Step [40410/64305], Loss: 4.6763\n",
      "Epoch [2/2], Step [40420/64305], Loss: 4.7597\n",
      "Epoch [2/2], Step [40430/64305], Loss: 4.7050\n",
      "Epoch [2/2], Step [40440/64305], Loss: 4.8613\n",
      "Epoch [2/2], Step [40450/64305], Loss: 4.8810\n",
      "Epoch [2/2], Step [40460/64305], Loss: 4.7062\n",
      "Epoch [2/2], Step [40470/64305], Loss: 4.6176\n",
      "Epoch [2/2], Step [40480/64305], Loss: 4.6518\n",
      "Epoch [2/2], Step [40490/64305], Loss: 4.7259\n",
      "Epoch [2/2], Step [40500/64305], Loss: 4.7793\n",
      "Epoch [2/2], Step [40510/64305], Loss: 4.7028\n",
      "Epoch [2/2], Step [40520/64305], Loss: 4.7272\n",
      "Epoch [2/2], Step [40530/64305], Loss: 4.7426\n",
      "Epoch [2/2], Step [40540/64305], Loss: 4.9472\n",
      "Epoch [2/2], Step [40550/64305], Loss: 4.7978\n",
      "Epoch [2/2], Step [40560/64305], Loss: 4.7450\n",
      "Epoch [2/2], Step [40570/64305], Loss: 4.8855\n",
      "Epoch [2/2], Step [40580/64305], Loss: 4.7113\n",
      "Epoch [2/2], Step [40590/64305], Loss: 4.6594\n",
      "Epoch [2/2], Step [40600/64305], Loss: 4.8978\n",
      "Epoch [2/2], Step [40610/64305], Loss: 4.6597\n",
      "Epoch [2/2], Step [40620/64305], Loss: 4.9553\n",
      "Epoch [2/2], Step [40630/64305], Loss: 4.7566\n",
      "Epoch [2/2], Step [40640/64305], Loss: 4.6770\n",
      "Epoch [2/2], Step [40650/64305], Loss: 4.6795\n",
      "Epoch [2/2], Step [40660/64305], Loss: 4.7806\n",
      "Epoch [2/2], Step [40670/64305], Loss: 4.7712\n",
      "Epoch [2/2], Step [40680/64305], Loss: 4.6739\n",
      "Epoch [2/2], Step [40690/64305], Loss: 4.5248\n",
      "Epoch [2/2], Step [40700/64305], Loss: 4.6238\n",
      "Epoch [2/2], Step [40710/64305], Loss: 4.7540\n",
      "Epoch [2/2], Step [40720/64305], Loss: 4.7366\n",
      "Epoch [2/2], Step [40730/64305], Loss: 4.8178\n",
      "Epoch [2/2], Step [40740/64305], Loss: 4.7986\n",
      "Epoch [2/2], Step [40750/64305], Loss: 4.8253\n",
      "Epoch [2/2], Step [40760/64305], Loss: 4.6855\n",
      "Epoch [2/2], Step [40770/64305], Loss: 4.7020\n",
      "Epoch [2/2], Step [40780/64305], Loss: 4.6857\n",
      "Epoch [2/2], Step [40790/64305], Loss: 4.8832\n",
      "Epoch [2/2], Step [40800/64305], Loss: 4.6675\n",
      "Epoch [2/2], Step [40810/64305], Loss: 4.6861\n",
      "Epoch [2/2], Step [40820/64305], Loss: 4.6828\n",
      "Epoch [2/2], Step [40830/64305], Loss: 4.9598\n",
      "Epoch [2/2], Step [40840/64305], Loss: 4.9516\n",
      "Epoch [2/2], Step [40850/64305], Loss: 4.4998\n",
      "Epoch [2/2], Step [40860/64305], Loss: 4.8739\n",
      "Epoch [2/2], Step [40870/64305], Loss: 4.7145\n",
      "Epoch [2/2], Step [40880/64305], Loss: 4.9617\n",
      "Epoch [2/2], Step [40890/64305], Loss: 4.7047\n",
      "Epoch [2/2], Step [40900/64305], Loss: 4.5618\n",
      "Epoch [2/2], Step [40910/64305], Loss: 4.7465\n",
      "Epoch [2/2], Step [40920/64305], Loss: 4.8093\n",
      "Epoch [2/2], Step [40930/64305], Loss: 4.8426\n",
      "Epoch [2/2], Step [40940/64305], Loss: 4.8315\n",
      "Epoch [2/2], Step [40950/64305], Loss: 4.7083\n",
      "Epoch [2/2], Step [40960/64305], Loss: 4.6907\n",
      "Epoch [2/2], Step [40970/64305], Loss: 4.8177\n",
      "Epoch [2/2], Step [40980/64305], Loss: 4.7712\n",
      "Epoch [2/2], Step [40990/64305], Loss: 4.6323\n",
      "Epoch [2/2], Step [41000/64305], Loss: 4.7730\n",
      "Epoch [2/2], Step [41010/64305], Loss: 4.7913\n",
      "Epoch [2/2], Step [41020/64305], Loss: 4.7108\n",
      "Epoch [2/2], Step [41030/64305], Loss: 4.8536\n",
      "Epoch [2/2], Step [41040/64305], Loss: 4.6856\n",
      "Epoch [2/2], Step [41050/64305], Loss: 4.8552\n",
      "Epoch [2/2], Step [41060/64305], Loss: 4.5648\n",
      "Epoch [2/2], Step [41070/64305], Loss: 4.6349\n",
      "Epoch [2/2], Step [41080/64305], Loss: 4.7056\n",
      "Epoch [2/2], Step [41090/64305], Loss: 4.6654\n",
      "Epoch [2/2], Step [41100/64305], Loss: 4.6481\n",
      "Epoch [2/2], Step [41110/64305], Loss: 4.7231\n",
      "Epoch [2/2], Step [41120/64305], Loss: 4.5416\n",
      "Epoch [2/2], Step [41130/64305], Loss: 4.7940\n",
      "Epoch [2/2], Step [41140/64305], Loss: 4.7806\n",
      "Epoch [2/2], Step [41150/64305], Loss: 4.7617\n",
      "Epoch [2/2], Step [41160/64305], Loss: 4.7009\n",
      "Epoch [2/2], Step [41170/64305], Loss: 4.8918\n",
      "Epoch [2/2], Step [41180/64305], Loss: 4.7733\n",
      "Epoch [2/2], Step [41190/64305], Loss: 4.6935\n",
      "Epoch [2/2], Step [41200/64305], Loss: 4.6481\n",
      "Epoch [2/2], Step [41210/64305], Loss: 4.7775\n",
      "Epoch [2/2], Step [41220/64305], Loss: 4.7712\n",
      "Epoch [2/2], Step [41230/64305], Loss: 4.6287\n",
      "Epoch [2/2], Step [41240/64305], Loss: 4.7036\n",
      "Epoch [2/2], Step [41250/64305], Loss: 4.7778\n",
      "Epoch [2/2], Step [41260/64305], Loss: 4.5218\n",
      "Epoch [2/2], Step [41270/64305], Loss: 4.6875\n",
      "Epoch [2/2], Step [41280/64305], Loss: 4.7155\n",
      "Epoch [2/2], Step [41290/64305], Loss: 4.4732\n",
      "Epoch [2/2], Step [41300/64305], Loss: 4.7805\n",
      "Epoch [2/2], Step [41310/64305], Loss: 4.8460\n",
      "Epoch [2/2], Step [41320/64305], Loss: 4.4983\n",
      "Epoch [2/2], Step [41330/64305], Loss: 4.6869\n",
      "Epoch [2/2], Step [41340/64305], Loss: 4.7430\n",
      "Epoch [2/2], Step [41350/64305], Loss: 4.6370\n",
      "Epoch [2/2], Step [41360/64305], Loss: 4.6730\n",
      "Epoch [2/2], Step [41370/64305], Loss: 4.6716\n",
      "Epoch [2/2], Step [41380/64305], Loss: 4.6208\n",
      "Epoch [2/2], Step [41390/64305], Loss: 4.9192\n",
      "Epoch [2/2], Step [41400/64305], Loss: 4.7541\n",
      "Epoch [2/2], Step [41410/64305], Loss: 4.9348\n",
      "Epoch [2/2], Step [41420/64305], Loss: 4.6074\n",
      "Epoch [2/2], Step [41430/64305], Loss: 4.5973\n",
      "Epoch [2/2], Step [41440/64305], Loss: 4.6699\n",
      "Epoch [2/2], Step [41450/64305], Loss: 4.6235\n",
      "Epoch [2/2], Step [41460/64305], Loss: 4.7149\n",
      "Epoch [2/2], Step [41470/64305], Loss: 4.7970\n",
      "Epoch [2/2], Step [41480/64305], Loss: 4.8173\n",
      "Epoch [2/2], Step [41490/64305], Loss: 4.7915\n",
      "Epoch [2/2], Step [41500/64305], Loss: 4.6880\n",
      "Epoch [2/2], Step [41510/64305], Loss: 4.7118\n",
      "Epoch [2/2], Step [41520/64305], Loss: 4.6977\n",
      "Epoch [2/2], Step [41530/64305], Loss: 4.6677\n",
      "Epoch [2/2], Step [41540/64305], Loss: 4.4724\n",
      "Epoch [2/2], Step [41550/64305], Loss: 4.6536\n",
      "Epoch [2/2], Step [41560/64305], Loss: 4.4983\n",
      "Epoch [2/2], Step [41570/64305], Loss: 4.8764\n",
      "Epoch [2/2], Step [41580/64305], Loss: 4.7714\n",
      "Epoch [2/2], Step [41590/64305], Loss: 4.8424\n",
      "Epoch [2/2], Step [41600/64305], Loss: 4.9460\n",
      "Epoch [2/2], Step [41610/64305], Loss: 4.8991\n",
      "Epoch [2/2], Step [41620/64305], Loss: 4.8762\n",
      "Epoch [2/2], Step [41630/64305], Loss: 4.8647\n",
      "Epoch [2/2], Step [41640/64305], Loss: 4.8525\n",
      "Epoch [2/2], Step [41650/64305], Loss: 4.6987\n",
      "Epoch [2/2], Step [41660/64305], Loss: 4.8117\n",
      "Epoch [2/2], Step [41670/64305], Loss: 4.7084\n",
      "Epoch [2/2], Step [41680/64305], Loss: 4.9303\n",
      "Epoch [2/2], Step [41690/64305], Loss: 4.8800\n",
      "Epoch [2/2], Step [41700/64305], Loss: 4.7520\n",
      "Epoch [2/2], Step [41710/64305], Loss: 4.8276\n",
      "Epoch [2/2], Step [41720/64305], Loss: 4.8608\n",
      "Epoch [2/2], Step [41730/64305], Loss: 4.6811\n",
      "Epoch [2/2], Step [41740/64305], Loss: 4.7537\n",
      "Epoch [2/2], Step [41750/64305], Loss: 4.8016\n",
      "Epoch [2/2], Step [41760/64305], Loss: 4.5933\n",
      "Epoch [2/2], Step [41770/64305], Loss: 4.8843\n",
      "Epoch [2/2], Step [41780/64305], Loss: 4.6758\n",
      "Epoch [2/2], Step [41790/64305], Loss: 4.6364\n",
      "Epoch [2/2], Step [41800/64305], Loss: 4.7206\n",
      "Epoch [2/2], Step [41810/64305], Loss: 4.5124\n",
      "Epoch [2/2], Step [41820/64305], Loss: 4.8373\n",
      "Epoch [2/2], Step [41830/64305], Loss: 4.9701\n",
      "Epoch [2/2], Step [41840/64305], Loss: 4.6983\n",
      "Epoch [2/2], Step [41850/64305], Loss: 4.8413\n",
      "Epoch [2/2], Step [41860/64305], Loss: 4.6958\n",
      "Epoch [2/2], Step [41870/64305], Loss: 4.8725\n",
      "Epoch [2/2], Step [41880/64305], Loss: 4.9347\n",
      "Epoch [2/2], Step [41890/64305], Loss: 4.7063\n",
      "Epoch [2/2], Step [41900/64305], Loss: 4.7090\n",
      "Epoch [2/2], Step [41910/64305], Loss: 4.5375\n",
      "Epoch [2/2], Step [41920/64305], Loss: 4.7774\n",
      "Epoch [2/2], Step [41930/64305], Loss: 4.6548\n",
      "Epoch [2/2], Step [41940/64305], Loss: 4.6473\n",
      "Epoch [2/2], Step [41950/64305], Loss: 4.8632\n",
      "Epoch [2/2], Step [41960/64305], Loss: 4.7823\n",
      "Epoch [2/2], Step [41970/64305], Loss: 4.9448\n",
      "Epoch [2/2], Step [41980/64305], Loss: 4.7362\n",
      "Epoch [2/2], Step [41990/64305], Loss: 4.8111\n",
      "Epoch [2/2], Step [42000/64305], Loss: 4.6615\n",
      "Epoch [2/2], Step [42010/64305], Loss: 4.8961\n",
      "Epoch [2/2], Step [42020/64305], Loss: 4.6632\n",
      "Epoch [2/2], Step [42030/64305], Loss: 4.6853\n",
      "Epoch [2/2], Step [42040/64305], Loss: 4.8362\n",
      "Epoch [2/2], Step [42050/64305], Loss: 4.8311\n",
      "Epoch [2/2], Step [42060/64305], Loss: 4.7144\n",
      "Epoch [2/2], Step [42070/64305], Loss: 4.7926\n",
      "Epoch [2/2], Step [42080/64305], Loss: 4.8572\n",
      "Epoch [2/2], Step [42090/64305], Loss: 4.7892\n",
      "Epoch [2/2], Step [42100/64305], Loss: 4.7467\n",
      "Epoch [2/2], Step [42110/64305], Loss: 4.8732\n",
      "Epoch [2/2], Step [42120/64305], Loss: 4.7598\n",
      "Epoch [2/2], Step [42130/64305], Loss: 4.5271\n",
      "Epoch [2/2], Step [42140/64305], Loss: 4.6328\n",
      "Epoch [2/2], Step [42150/64305], Loss: 4.6503\n",
      "Epoch [2/2], Step [42160/64305], Loss: 4.7720\n",
      "Epoch [2/2], Step [42170/64305], Loss: 4.7509\n",
      "Epoch [2/2], Step [42180/64305], Loss: 4.6170\n",
      "Epoch [2/2], Step [42190/64305], Loss: 4.7467\n",
      "Epoch [2/2], Step [42200/64305], Loss: 4.6703\n",
      "Epoch [2/2], Step [42210/64305], Loss: 4.8237\n",
      "Epoch [2/2], Step [42220/64305], Loss: 4.6330\n",
      "Epoch [2/2], Step [42230/64305], Loss: 4.7562\n",
      "Epoch [2/2], Step [42240/64305], Loss: 4.4722\n",
      "Epoch [2/2], Step [42250/64305], Loss: 4.7655\n",
      "Epoch [2/2], Step [42260/64305], Loss: 4.8539\n",
      "Epoch [2/2], Step [42270/64305], Loss: 4.8623\n",
      "Epoch [2/2], Step [42280/64305], Loss: 4.6120\n",
      "Epoch [2/2], Step [42290/64305], Loss: 4.7347\n",
      "Epoch [2/2], Step [42300/64305], Loss: 4.6798\n",
      "Epoch [2/2], Step [42310/64305], Loss: 4.8796\n",
      "Epoch [2/2], Step [42320/64305], Loss: 4.7068\n",
      "Epoch [2/2], Step [42330/64305], Loss: 4.7184\n",
      "Epoch [2/2], Step [42340/64305], Loss: 4.8536\n",
      "Epoch [2/2], Step [42350/64305], Loss: 4.8968\n",
      "Epoch [2/2], Step [42360/64305], Loss: 4.7320\n",
      "Epoch [2/2], Step [42370/64305], Loss: 4.6893\n",
      "Epoch [2/2], Step [42380/64305], Loss: 4.8869\n",
      "Epoch [2/2], Step [42390/64305], Loss: 4.9449\n",
      "Epoch [2/2], Step [42400/64305], Loss: 4.7435\n",
      "Epoch [2/2], Step [42410/64305], Loss: 4.8104\n",
      "Epoch [2/2], Step [42420/64305], Loss: 4.7407\n",
      "Epoch [2/2], Step [42430/64305], Loss: 4.7801\n",
      "Epoch [2/2], Step [42440/64305], Loss: 4.7936\n",
      "Epoch [2/2], Step [42450/64305], Loss: 4.8600\n",
      "Epoch [2/2], Step [42460/64305], Loss: 4.6438\n",
      "Epoch [2/2], Step [42470/64305], Loss: 4.8522\n",
      "Epoch [2/2], Step [42480/64305], Loss: 4.6678\n",
      "Epoch [2/2], Step [42490/64305], Loss: 4.7224\n",
      "Epoch [2/2], Step [42500/64305], Loss: 4.6180\n",
      "Epoch [2/2], Step [42510/64305], Loss: 4.6430\n",
      "Epoch [2/2], Step [42520/64305], Loss: 4.7444\n",
      "Epoch [2/2], Step [42530/64305], Loss: 4.8265\n",
      "Epoch [2/2], Step [42540/64305], Loss: 4.6190\n",
      "Epoch [2/2], Step [42550/64305], Loss: 4.7817\n",
      "Epoch [2/2], Step [42560/64305], Loss: 4.8077\n",
      "Epoch [2/2], Step [42570/64305], Loss: 4.7815\n",
      "Epoch [2/2], Step [42580/64305], Loss: 4.3315\n",
      "Epoch [2/2], Step [42590/64305], Loss: 4.6167\n",
      "Epoch [2/2], Step [42600/64305], Loss: 4.7388\n",
      "Epoch [2/2], Step [42610/64305], Loss: 4.5546\n",
      "Epoch [2/2], Step [42620/64305], Loss: 4.7012\n",
      "Epoch [2/2], Step [42630/64305], Loss: 4.6988\n",
      "Epoch [2/2], Step [42640/64305], Loss: 4.7460\n",
      "Epoch [2/2], Step [42650/64305], Loss: 4.6762\n",
      "Epoch [2/2], Step [42660/64305], Loss: 4.7450\n",
      "Epoch [2/2], Step [42670/64305], Loss: 4.9297\n",
      "Epoch [2/2], Step [42680/64305], Loss: 4.8206\n",
      "Epoch [2/2], Step [42690/64305], Loss: 4.7597\n",
      "Epoch [2/2], Step [42700/64305], Loss: 4.8457\n",
      "Epoch [2/2], Step [42710/64305], Loss: 4.8312\n",
      "Epoch [2/2], Step [42720/64305], Loss: 4.5520\n",
      "Epoch [2/2], Step [42730/64305], Loss: 4.6790\n",
      "Epoch [2/2], Step [42740/64305], Loss: 4.8457\n",
      "Epoch [2/2], Step [42750/64305], Loss: 4.6886\n",
      "Epoch [2/2], Step [42760/64305], Loss: 4.8563\n",
      "Epoch [2/2], Step [42770/64305], Loss: 4.7267\n",
      "Epoch [2/2], Step [42780/64305], Loss: 4.6425\n",
      "Epoch [2/2], Step [42790/64305], Loss: 4.4937\n",
      "Epoch [2/2], Step [42800/64305], Loss: 4.6824\n",
      "Epoch [2/2], Step [42810/64305], Loss: 4.5655\n",
      "Epoch [2/2], Step [42820/64305], Loss: 4.5453\n",
      "Epoch [2/2], Step [42830/64305], Loss: 4.4017\n",
      "Epoch [2/2], Step [42840/64305], Loss: 4.7937\n",
      "Epoch [2/2], Step [42850/64305], Loss: 4.9479\n",
      "Epoch [2/2], Step [42860/64305], Loss: 4.6263\n",
      "Epoch [2/2], Step [42870/64305], Loss: 4.8028\n",
      "Epoch [2/2], Step [42880/64305], Loss: 4.8464\n",
      "Epoch [2/2], Step [42890/64305], Loss: 4.8225\n",
      "Epoch [2/2], Step [42900/64305], Loss: 4.8311\n",
      "Epoch [2/2], Step [42910/64305], Loss: 4.8814\n",
      "Epoch [2/2], Step [42920/64305], Loss: 4.7287\n",
      "Epoch [2/2], Step [42930/64305], Loss: 4.7106\n",
      "Epoch [2/2], Step [42940/64305], Loss: 4.8939\n",
      "Epoch [2/2], Step [42950/64305], Loss: 4.8747\n",
      "Epoch [2/2], Step [42960/64305], Loss: 4.7324\n",
      "Epoch [2/2], Step [42970/64305], Loss: 4.7040\n",
      "Epoch [2/2], Step [42980/64305], Loss: 4.7221\n",
      "Epoch [2/2], Step [42990/64305], Loss: 4.8646\n",
      "Epoch [2/2], Step [43000/64305], Loss: 4.8416\n",
      "Epoch [2/2], Step [43010/64305], Loss: 4.8513\n",
      "Epoch [2/2], Step [43020/64305], Loss: 4.7992\n",
      "Epoch [2/2], Step [43030/64305], Loss: 4.6704\n",
      "Epoch [2/2], Step [43040/64305], Loss: 4.6020\n",
      "Epoch [2/2], Step [43050/64305], Loss: 4.6844\n",
      "Epoch [2/2], Step [43060/64305], Loss: 4.4651\n",
      "Epoch [2/2], Step [43070/64305], Loss: 5.0455\n",
      "Epoch [2/2], Step [43080/64305], Loss: 4.8014\n",
      "Epoch [2/2], Step [43090/64305], Loss: 4.5003\n",
      "Epoch [2/2], Step [43100/64305], Loss: 4.6461\n",
      "Epoch [2/2], Step [43110/64305], Loss: 4.5854\n",
      "Epoch [2/2], Step [43120/64305], Loss: 4.5456\n",
      "Epoch [2/2], Step [43130/64305], Loss: 4.8999\n",
      "Epoch [2/2], Step [43140/64305], Loss: 4.6101\n",
      "Epoch [2/2], Step [43150/64305], Loss: 4.7242\n",
      "Epoch [2/2], Step [43160/64305], Loss: 4.7768\n",
      "Epoch [2/2], Step [43170/64305], Loss: 4.6975\n",
      "Epoch [2/2], Step [43180/64305], Loss: 4.3725\n",
      "Epoch [2/2], Step [43190/64305], Loss: 4.7170\n",
      "Epoch [2/2], Step [43200/64305], Loss: 4.9047\n",
      "Epoch [2/2], Step [43210/64305], Loss: 4.6474\n",
      "Epoch [2/2], Step [43220/64305], Loss: 4.7043\n",
      "Epoch [2/2], Step [43230/64305], Loss: 4.7216\n",
      "Epoch [2/2], Step [43240/64305], Loss: 4.6532\n",
      "Epoch [2/2], Step [43250/64305], Loss: 4.6714\n",
      "Epoch [2/2], Step [43260/64305], Loss: 4.6770\n",
      "Epoch [2/2], Step [43270/64305], Loss: 4.7500\n",
      "Epoch [2/2], Step [43280/64305], Loss: 4.8867\n",
      "Epoch [2/2], Step [43290/64305], Loss: 4.6122\n",
      "Epoch [2/2], Step [43300/64305], Loss: 4.8767\n",
      "Epoch [2/2], Step [43310/64305], Loss: 4.7338\n",
      "Epoch [2/2], Step [43320/64305], Loss: 4.8405\n",
      "Epoch [2/2], Step [43330/64305], Loss: 4.8689\n",
      "Epoch [2/2], Step [43340/64305], Loss: 4.7760\n",
      "Epoch [2/2], Step [43350/64305], Loss: 4.6226\n",
      "Epoch [2/2], Step [43360/64305], Loss: 4.6655\n",
      "Epoch [2/2], Step [43370/64305], Loss: 4.9398\n",
      "Epoch [2/2], Step [43380/64305], Loss: 4.8426\n",
      "Epoch [2/2], Step [43390/64305], Loss: 4.7536\n",
      "Epoch [2/2], Step [43400/64305], Loss: 4.7266\n",
      "Epoch [2/2], Step [43410/64305], Loss: 4.8086\n",
      "Epoch [2/2], Step [43420/64305], Loss: 4.6958\n",
      "Epoch [2/2], Step [43430/64305], Loss: 4.7331\n",
      "Epoch [2/2], Step [43440/64305], Loss: 4.6002\n",
      "Epoch [2/2], Step [43450/64305], Loss: 4.5498\n",
      "Epoch [2/2], Step [43460/64305], Loss: 4.8140\n",
      "Epoch [2/2], Step [43470/64305], Loss: 4.7602\n",
      "Epoch [2/2], Step [43480/64305], Loss: 4.6901\n",
      "Epoch [2/2], Step [43490/64305], Loss: 4.6761\n",
      "Epoch [2/2], Step [43500/64305], Loss: 4.6937\n",
      "Epoch [2/2], Step [43510/64305], Loss: 4.7811\n",
      "Epoch [2/2], Step [43520/64305], Loss: 4.9016\n",
      "Epoch [2/2], Step [43530/64305], Loss: 4.8280\n",
      "Epoch [2/2], Step [43540/64305], Loss: 4.7194\n",
      "Epoch [2/2], Step [43550/64305], Loss: 4.8049\n",
      "Epoch [2/2], Step [43560/64305], Loss: 4.7257\n",
      "Epoch [2/2], Step [43570/64305], Loss: 4.7986\n",
      "Epoch [2/2], Step [43580/64305], Loss: 4.7757\n",
      "Epoch [2/2], Step [43590/64305], Loss: 4.6461\n",
      "Epoch [2/2], Step [43600/64305], Loss: 4.9388\n",
      "Epoch [2/2], Step [43610/64305], Loss: 4.7062\n",
      "Epoch [2/2], Step [43620/64305], Loss: 4.7986\n",
      "Epoch [2/2], Step [43630/64305], Loss: 4.5739\n",
      "Epoch [2/2], Step [43640/64305], Loss: 4.9199\n",
      "Epoch [2/2], Step [43650/64305], Loss: 4.6764\n",
      "Epoch [2/2], Step [43660/64305], Loss: 4.7277\n",
      "Epoch [2/2], Step [43670/64305], Loss: 4.8395\n",
      "Epoch [2/2], Step [43680/64305], Loss: 4.6384\n",
      "Epoch [2/2], Step [43690/64305], Loss: 4.7542\n",
      "Epoch [2/2], Step [43700/64305], Loss: 4.7080\n",
      "Epoch [2/2], Step [43710/64305], Loss: 4.6722\n",
      "Epoch [2/2], Step [43720/64305], Loss: 4.8401\n",
      "Epoch [2/2], Step [43730/64305], Loss: 4.8954\n",
      "Epoch [2/2], Step [43740/64305], Loss: 4.7909\n",
      "Epoch [2/2], Step [43750/64305], Loss: 4.5343\n",
      "Epoch [2/2], Step [43760/64305], Loss: 4.8842\n",
      "Epoch [2/2], Step [43770/64305], Loss: 4.7378\n",
      "Epoch [2/2], Step [43780/64305], Loss: 4.6084\n",
      "Epoch [2/2], Step [43790/64305], Loss: 4.8391\n",
      "Epoch [2/2], Step [43800/64305], Loss: 4.6702\n",
      "Epoch [2/2], Step [43810/64305], Loss: 4.7236\n",
      "Epoch [2/2], Step [43820/64305], Loss: 4.5938\n",
      "Epoch [2/2], Step [43830/64305], Loss: 4.5605\n",
      "Epoch [2/2], Step [43840/64305], Loss: 4.6175\n",
      "Epoch [2/2], Step [43850/64305], Loss: 4.7826\n",
      "Epoch [2/2], Step [43860/64305], Loss: 4.8114\n",
      "Epoch [2/2], Step [43870/64305], Loss: 4.6636\n",
      "Epoch [2/2], Step [43880/64305], Loss: 4.7703\n",
      "Epoch [2/2], Step [43890/64305], Loss: 4.6782\n",
      "Epoch [2/2], Step [43900/64305], Loss: 4.7755\n",
      "Epoch [2/2], Step [43910/64305], Loss: 4.5304\n",
      "Epoch [2/2], Step [43920/64305], Loss: 4.6403\n",
      "Epoch [2/2], Step [43930/64305], Loss: 4.8081\n",
      "Epoch [2/2], Step [43940/64305], Loss: 4.8019\n",
      "Epoch [2/2], Step [43950/64305], Loss: 4.9393\n",
      "Epoch [2/2], Step [43960/64305], Loss: 4.6203\n",
      "Epoch [2/2], Step [43970/64305], Loss: 4.6062\n",
      "Epoch [2/2], Step [43980/64305], Loss: 4.7227\n",
      "Epoch [2/2], Step [43990/64305], Loss: 4.6360\n",
      "Epoch [2/2], Step [44000/64305], Loss: 4.8662\n",
      "Epoch [2/2], Step [44010/64305], Loss: 4.7393\n",
      "Epoch [2/2], Step [44020/64305], Loss: 4.7157\n",
      "Epoch [2/2], Step [44030/64305], Loss: 4.6577\n",
      "Epoch [2/2], Step [44040/64305], Loss: 4.8541\n",
      "Epoch [2/2], Step [44050/64305], Loss: 4.7210\n",
      "Epoch [2/2], Step [44060/64305], Loss: 4.8705\n",
      "Epoch [2/2], Step [44070/64305], Loss: 4.7344\n",
      "Epoch [2/2], Step [44080/64305], Loss: 4.9559\n",
      "Epoch [2/2], Step [44090/64305], Loss: 4.9222\n",
      "Epoch [2/2], Step [44100/64305], Loss: 4.5169\n",
      "Epoch [2/2], Step [44110/64305], Loss: 4.7445\n",
      "Epoch [2/2], Step [44120/64305], Loss: 4.8083\n",
      "Epoch [2/2], Step [44130/64305], Loss: 4.7215\n",
      "Epoch [2/2], Step [44140/64305], Loss: 4.7173\n",
      "Epoch [2/2], Step [44150/64305], Loss: 4.6558\n",
      "Epoch [2/2], Step [44160/64305], Loss: 4.8544\n",
      "Epoch [2/2], Step [44170/64305], Loss: 4.6061\n",
      "Epoch [2/2], Step [44180/64305], Loss: 4.7325\n",
      "Epoch [2/2], Step [44190/64305], Loss: 4.5867\n",
      "Epoch [2/2], Step [44200/64305], Loss: 4.4723\n",
      "Epoch [2/2], Step [44210/64305], Loss: 4.7710\n",
      "Epoch [2/2], Step [44220/64305], Loss: 4.7153\n",
      "Epoch [2/2], Step [44230/64305], Loss: 4.6163\n",
      "Epoch [2/2], Step [44240/64305], Loss: 4.8608\n",
      "Epoch [2/2], Step [44250/64305], Loss: 4.6340\n",
      "Epoch [2/2], Step [44260/64305], Loss: 4.5604\n",
      "Epoch [2/2], Step [44270/64305], Loss: 4.9197\n",
      "Epoch [2/2], Step [44280/64305], Loss: 4.8308\n",
      "Epoch [2/2], Step [44290/64305], Loss: 4.6785\n",
      "Epoch [2/2], Step [44300/64305], Loss: 4.8290\n",
      "Epoch [2/2], Step [44310/64305], Loss: 4.8749\n",
      "Epoch [2/2], Step [44320/64305], Loss: 4.8244\n",
      "Epoch [2/2], Step [44330/64305], Loss: 4.6508\n",
      "Epoch [2/2], Step [44340/64305], Loss: 4.7991\n",
      "Epoch [2/2], Step [44350/64305], Loss: 4.8314\n",
      "Epoch [2/2], Step [44360/64305], Loss: 4.6245\n",
      "Epoch [2/2], Step [44370/64305], Loss: 4.8508\n",
      "Epoch [2/2], Step [44380/64305], Loss: 4.7325\n",
      "Epoch [2/2], Step [44390/64305], Loss: 4.9547\n",
      "Epoch [2/2], Step [44400/64305], Loss: 4.5040\n",
      "Epoch [2/2], Step [44410/64305], Loss: 4.9756\n",
      "Epoch [2/2], Step [44420/64305], Loss: 4.7909\n",
      "Epoch [2/2], Step [44430/64305], Loss: 4.8409\n",
      "Epoch [2/2], Step [44440/64305], Loss: 4.7822\n",
      "Epoch [2/2], Step [44450/64305], Loss: 4.6856\n",
      "Epoch [2/2], Step [44460/64305], Loss: 4.6520\n",
      "Epoch [2/2], Step [44470/64305], Loss: 4.7621\n",
      "Epoch [2/2], Step [44480/64305], Loss: 4.9072\n",
      "Epoch [2/2], Step [44490/64305], Loss: 4.6972\n",
      "Epoch [2/2], Step [44500/64305], Loss: 4.8824\n",
      "Epoch [2/2], Step [44510/64305], Loss: 4.9492\n",
      "Epoch [2/2], Step [44520/64305], Loss: 4.7718\n",
      "Epoch [2/2], Step [44530/64305], Loss: 4.7781\n",
      "Epoch [2/2], Step [44540/64305], Loss: 4.6846\n",
      "Epoch [2/2], Step [44550/64305], Loss: 4.8962\n",
      "Epoch [2/2], Step [44560/64305], Loss: 4.8228\n",
      "Epoch [2/2], Step [44570/64305], Loss: 4.6936\n",
      "Epoch [2/2], Step [44580/64305], Loss: 4.7230\n",
      "Epoch [2/2], Step [44590/64305], Loss: 4.7105\n",
      "Epoch [2/2], Step [44600/64305], Loss: 4.8438\n",
      "Epoch [2/2], Step [44610/64305], Loss: 4.6424\n",
      "Epoch [2/2], Step [44620/64305], Loss: 4.5948\n",
      "Epoch [2/2], Step [44630/64305], Loss: 4.9769\n",
      "Epoch [2/2], Step [44640/64305], Loss: 4.6365\n",
      "Epoch [2/2], Step [44650/64305], Loss: 4.6116\n",
      "Epoch [2/2], Step [44660/64305], Loss: 4.7027\n",
      "Epoch [2/2], Step [44670/64305], Loss: 4.7141\n",
      "Epoch [2/2], Step [44680/64305], Loss: 4.8365\n",
      "Epoch [2/2], Step [44690/64305], Loss: 4.6876\n",
      "Epoch [2/2], Step [44700/64305], Loss: 4.7010\n",
      "Epoch [2/2], Step [44710/64305], Loss: 4.6456\n",
      "Epoch [2/2], Step [44720/64305], Loss: 4.6595\n",
      "Epoch [2/2], Step [44730/64305], Loss: 4.9041\n",
      "Epoch [2/2], Step [44740/64305], Loss: 4.6615\n",
      "Epoch [2/2], Step [44750/64305], Loss: 4.6524\n",
      "Epoch [2/2], Step [44760/64305], Loss: 4.6479\n",
      "Epoch [2/2], Step [44770/64305], Loss: 5.0976\n",
      "Epoch [2/2], Step [44780/64305], Loss: 4.7846\n",
      "Epoch [2/2], Step [44790/64305], Loss: 4.6061\n",
      "Epoch [2/2], Step [44800/64305], Loss: 4.4613\n",
      "Epoch [2/2], Step [44810/64305], Loss: 4.6683\n",
      "Epoch [2/2], Step [44820/64305], Loss: 4.8757\n",
      "Epoch [2/2], Step [44830/64305], Loss: 4.9082\n",
      "Epoch [2/2], Step [44840/64305], Loss: 4.7220\n",
      "Epoch [2/2], Step [44850/64305], Loss: 4.8850\n",
      "Epoch [2/2], Step [44860/64305], Loss: 4.7139\n",
      "Epoch [2/2], Step [44870/64305], Loss: 4.9959\n",
      "Epoch [2/2], Step [44880/64305], Loss: 4.6435\n",
      "Epoch [2/2], Step [44890/64305], Loss: 4.8022\n",
      "Epoch [2/2], Step [44900/64305], Loss: 4.9526\n",
      "Epoch [2/2], Step [44910/64305], Loss: 4.7814\n",
      "Epoch [2/2], Step [44920/64305], Loss: 4.7794\n",
      "Epoch [2/2], Step [44930/64305], Loss: 4.5609\n",
      "Epoch [2/2], Step [44940/64305], Loss: 4.6568\n",
      "Epoch [2/2], Step [44950/64305], Loss: 4.6563\n",
      "Epoch [2/2], Step [44960/64305], Loss: 4.7905\n",
      "Epoch [2/2], Step [44970/64305], Loss: 4.7577\n",
      "Epoch [2/2], Step [44980/64305], Loss: 4.8036\n",
      "Epoch [2/2], Step [44990/64305], Loss: 4.8155\n",
      "Epoch [2/2], Step [45000/64305], Loss: 4.8637\n",
      "Epoch [2/2], Step [45010/64305], Loss: 4.6448\n",
      "Epoch [2/2], Step [45020/64305], Loss: 4.6688\n",
      "Epoch [2/2], Step [45030/64305], Loss: 4.8335\n",
      "Epoch [2/2], Step [45040/64305], Loss: 4.6933\n",
      "Epoch [2/2], Step [45050/64305], Loss: 4.7081\n",
      "Epoch [2/2], Step [45060/64305], Loss: 4.6406\n",
      "Epoch [2/2], Step [45070/64305], Loss: 4.9032\n",
      "Epoch [2/2], Step [45080/64305], Loss: 4.7145\n",
      "Epoch [2/2], Step [45090/64305], Loss: 4.9613\n",
      "Epoch [2/2], Step [45100/64305], Loss: 4.8896\n",
      "Epoch [2/2], Step [45110/64305], Loss: 4.6989\n",
      "Epoch [2/2], Step [45120/64305], Loss: 4.6835\n",
      "Epoch [2/2], Step [45130/64305], Loss: 4.6156\n",
      "Epoch [2/2], Step [45140/64305], Loss: 4.9181\n",
      "Epoch [2/2], Step [45150/64305], Loss: 4.8453\n",
      "Epoch [2/2], Step [45160/64305], Loss: 4.6508\n",
      "Epoch [2/2], Step [45170/64305], Loss: 4.7220\n",
      "Epoch [2/2], Step [45180/64305], Loss: 4.7529\n",
      "Epoch [2/2], Step [45190/64305], Loss: 4.6212\n",
      "Epoch [2/2], Step [45200/64305], Loss: 4.6179\n",
      "Epoch [2/2], Step [45210/64305], Loss: 4.6239\n",
      "Epoch [2/2], Step [45220/64305], Loss: 4.5993\n",
      "Epoch [2/2], Step [45230/64305], Loss: 4.8133\n",
      "Epoch [2/2], Step [45240/64305], Loss: 4.9352\n",
      "Epoch [2/2], Step [45250/64305], Loss: 4.7006\n",
      "Epoch [2/2], Step [45260/64305], Loss: 4.7192\n",
      "Epoch [2/2], Step [45270/64305], Loss: 4.7410\n",
      "Epoch [2/2], Step [45280/64305], Loss: 4.7932\n",
      "Epoch [2/2], Step [45290/64305], Loss: 4.7815\n",
      "Epoch [2/2], Step [45300/64305], Loss: 4.6953\n",
      "Epoch [2/2], Step [45310/64305], Loss: 4.7433\n",
      "Epoch [2/2], Step [45320/64305], Loss: 4.6733\n",
      "Epoch [2/2], Step [45330/64305], Loss: 4.9779\n",
      "Epoch [2/2], Step [45340/64305], Loss: 4.7550\n",
      "Epoch [2/2], Step [45350/64305], Loss: 4.7055\n",
      "Epoch [2/2], Step [45360/64305], Loss: 4.7072\n",
      "Epoch [2/2], Step [45370/64305], Loss: 4.7514\n",
      "Epoch [2/2], Step [45380/64305], Loss: 4.6023\n",
      "Epoch [2/2], Step [45390/64305], Loss: 4.7240\n",
      "Epoch [2/2], Step [45400/64305], Loss: 4.7209\n",
      "Epoch [2/2], Step [45410/64305], Loss: 4.5888\n",
      "Epoch [2/2], Step [45420/64305], Loss: 4.7347\n",
      "Epoch [2/2], Step [45430/64305], Loss: 4.5452\n",
      "Epoch [2/2], Step [45440/64305], Loss: 4.6878\n",
      "Epoch [2/2], Step [45450/64305], Loss: 4.6304\n",
      "Epoch [2/2], Step [45460/64305], Loss: 4.8950\n",
      "Epoch [2/2], Step [45470/64305], Loss: 4.8705\n",
      "Epoch [2/2], Step [45480/64305], Loss: 4.6741\n",
      "Epoch [2/2], Step [45490/64305], Loss: 4.8490\n",
      "Epoch [2/2], Step [45500/64305], Loss: 4.6602\n",
      "Epoch [2/2], Step [45510/64305], Loss: 4.8403\n",
      "Epoch [2/2], Step [45520/64305], Loss: 4.7427\n",
      "Epoch [2/2], Step [45530/64305], Loss: 4.7495\n",
      "Epoch [2/2], Step [45540/64305], Loss: 4.7338\n",
      "Epoch [2/2], Step [45550/64305], Loss: 4.8422\n",
      "Epoch [2/2], Step [45560/64305], Loss: 4.9092\n",
      "Epoch [2/2], Step [45570/64305], Loss: 4.7296\n",
      "Epoch [2/2], Step [45580/64305], Loss: 4.8514\n",
      "Epoch [2/2], Step [45590/64305], Loss: 4.6524\n",
      "Epoch [2/2], Step [45600/64305], Loss: 4.7866\n",
      "Epoch [2/2], Step [45610/64305], Loss: 4.4826\n",
      "Epoch [2/2], Step [45620/64305], Loss: 4.8290\n",
      "Epoch [2/2], Step [45630/64305], Loss: 4.4536\n",
      "Epoch [2/2], Step [45640/64305], Loss: 4.7070\n",
      "Epoch [2/2], Step [45650/64305], Loss: 4.7456\n",
      "Epoch [2/2], Step [45660/64305], Loss: 4.8039\n",
      "Epoch [2/2], Step [45670/64305], Loss: 4.9279\n",
      "Epoch [2/2], Step [45680/64305], Loss: 4.6902\n",
      "Epoch [2/2], Step [45690/64305], Loss: 4.7925\n",
      "Epoch [2/2], Step [45700/64305], Loss: 4.6594\n",
      "Epoch [2/2], Step [45710/64305], Loss: 4.5829\n",
      "Epoch [2/2], Step [45720/64305], Loss: 4.8311\n",
      "Epoch [2/2], Step [45730/64305], Loss: 4.6790\n",
      "Epoch [2/2], Step [45740/64305], Loss: 4.8312\n",
      "Epoch [2/2], Step [45750/64305], Loss: 5.0075\n",
      "Epoch [2/2], Step [45760/64305], Loss: 4.9028\n",
      "Epoch [2/2], Step [45770/64305], Loss: 4.7141\n",
      "Epoch [2/2], Step [45780/64305], Loss: 4.6853\n",
      "Epoch [2/2], Step [45790/64305], Loss: 4.7197\n",
      "Epoch [2/2], Step [45800/64305], Loss: 4.9218\n",
      "Epoch [2/2], Step [45810/64305], Loss: 4.5333\n",
      "Epoch [2/2], Step [45820/64305], Loss: 4.6976\n",
      "Epoch [2/2], Step [45830/64305], Loss: 4.6694\n",
      "Epoch [2/2], Step [45840/64305], Loss: 4.7994\n",
      "Epoch [2/2], Step [45850/64305], Loss: 4.5659\n",
      "Epoch [2/2], Step [45860/64305], Loss: 4.8775\n",
      "Epoch [2/2], Step [45870/64305], Loss: 4.7719\n",
      "Epoch [2/2], Step [45880/64305], Loss: 4.7717\n",
      "Epoch [2/2], Step [45890/64305], Loss: 4.7722\n",
      "Epoch [2/2], Step [45900/64305], Loss: 4.6134\n",
      "Epoch [2/2], Step [45910/64305], Loss: 4.8468\n",
      "Epoch [2/2], Step [45920/64305], Loss: 4.4935\n",
      "Epoch [2/2], Step [45930/64305], Loss: 4.8221\n",
      "Epoch [2/2], Step [45940/64305], Loss: 4.7538\n",
      "Epoch [2/2], Step [45950/64305], Loss: 4.7612\n",
      "Epoch [2/2], Step [45960/64305], Loss: 4.7448\n",
      "Epoch [2/2], Step [45970/64305], Loss: 4.7875\n",
      "Epoch [2/2], Step [45980/64305], Loss: 4.9820\n",
      "Epoch [2/2], Step [45990/64305], Loss: 4.7888\n",
      "Epoch [2/2], Step [46000/64305], Loss: 4.7004\n",
      "Epoch [2/2], Step [46010/64305], Loss: 4.8736\n",
      "Epoch [2/2], Step [46020/64305], Loss: 4.6052\n",
      "Epoch [2/2], Step [46030/64305], Loss: 4.8124\n",
      "Epoch [2/2], Step [46040/64305], Loss: 4.6634\n",
      "Epoch [2/2], Step [46050/64305], Loss: 4.5642\n",
      "Epoch [2/2], Step [46060/64305], Loss: 4.6856\n",
      "Epoch [2/2], Step [46070/64305], Loss: 4.6689\n",
      "Epoch [2/2], Step [46080/64305], Loss: 4.7328\n",
      "Epoch [2/2], Step [46090/64305], Loss: 4.7764\n",
      "Epoch [2/2], Step [46100/64305], Loss: 4.7364\n",
      "Epoch [2/2], Step [46110/64305], Loss: 5.0121\n",
      "Epoch [2/2], Step [46120/64305], Loss: 4.8649\n",
      "Epoch [2/2], Step [46130/64305], Loss: 4.7337\n",
      "Epoch [2/2], Step [46140/64305], Loss: 4.8015\n",
      "Epoch [2/2], Step [46150/64305], Loss: 4.8238\n",
      "Epoch [2/2], Step [46160/64305], Loss: 4.7136\n",
      "Epoch [2/2], Step [46170/64305], Loss: 4.9543\n",
      "Epoch [2/2], Step [46180/64305], Loss: 4.7617\n",
      "Epoch [2/2], Step [46190/64305], Loss: 4.7231\n",
      "Epoch [2/2], Step [46200/64305], Loss: 4.6137\n",
      "Epoch [2/2], Step [46210/64305], Loss: 4.6921\n",
      "Epoch [2/2], Step [46220/64305], Loss: 4.7456\n",
      "Epoch [2/2], Step [46230/64305], Loss: 4.6396\n",
      "Epoch [2/2], Step [46240/64305], Loss: 4.7461\n",
      "Epoch [2/2], Step [46250/64305], Loss: 4.6915\n",
      "Epoch [2/2], Step [46260/64305], Loss: 4.7971\n",
      "Epoch [2/2], Step [46270/64305], Loss: 4.9246\n",
      "Epoch [2/2], Step [46280/64305], Loss: 4.5967\n",
      "Epoch [2/2], Step [46290/64305], Loss: 4.8031\n",
      "Epoch [2/2], Step [46300/64305], Loss: 4.7144\n",
      "Epoch [2/2], Step [46310/64305], Loss: 4.6739\n",
      "Epoch [2/2], Step [46320/64305], Loss: 4.8265\n",
      "Epoch [2/2], Step [46330/64305], Loss: 4.8089\n",
      "Epoch [2/2], Step [46340/64305], Loss: 4.7958\n",
      "Epoch [2/2], Step [46350/64305], Loss: 4.7986\n",
      "Epoch [2/2], Step [46360/64305], Loss: 4.8061\n",
      "Epoch [2/2], Step [46370/64305], Loss: 4.6862\n",
      "Epoch [2/2], Step [46380/64305], Loss: 4.9107\n",
      "Epoch [2/2], Step [46390/64305], Loss: 4.6254\n",
      "Epoch [2/2], Step [46400/64305], Loss: 4.8813\n",
      "Epoch [2/2], Step [46410/64305], Loss: 4.8093\n",
      "Epoch [2/2], Step [46420/64305], Loss: 4.8564\n",
      "Epoch [2/2], Step [46430/64305], Loss: 4.6624\n",
      "Epoch [2/2], Step [46440/64305], Loss: 4.6225\n",
      "Epoch [2/2], Step [46450/64305], Loss: 4.7138\n",
      "Epoch [2/2], Step [46460/64305], Loss: 4.7695\n",
      "Epoch [2/2], Step [46470/64305], Loss: 4.6160\n",
      "Epoch [2/2], Step [46480/64305], Loss: 4.6661\n",
      "Epoch [2/2], Step [46490/64305], Loss: 4.7031\n",
      "Epoch [2/2], Step [46500/64305], Loss: 4.6169\n",
      "Epoch [2/2], Step [46510/64305], Loss: 4.7117\n",
      "Epoch [2/2], Step [46520/64305], Loss: 4.6822\n",
      "Epoch [2/2], Step [46530/64305], Loss: 4.7340\n",
      "Epoch [2/2], Step [46540/64305], Loss: 4.9768\n",
      "Epoch [2/2], Step [46550/64305], Loss: 4.6853\n",
      "Epoch [2/2], Step [46560/64305], Loss: 4.9441\n",
      "Epoch [2/2], Step [46570/64305], Loss: 4.7502\n",
      "Epoch [2/2], Step [46580/64305], Loss: 4.8396\n",
      "Epoch [2/2], Step [46590/64305], Loss: 4.5802\n",
      "Epoch [2/2], Step [46600/64305], Loss: 4.9163\n",
      "Epoch [2/2], Step [46610/64305], Loss: 4.6402\n",
      "Epoch [2/2], Step [46620/64305], Loss: 4.8101\n",
      "Epoch [2/2], Step [46630/64305], Loss: 4.6595\n",
      "Epoch [2/2], Step [46640/64305], Loss: 4.7034\n",
      "Epoch [2/2], Step [46650/64305], Loss: 4.8207\n",
      "Epoch [2/2], Step [46660/64305], Loss: 4.7624\n",
      "Epoch [2/2], Step [46670/64305], Loss: 4.5859\n",
      "Epoch [2/2], Step [46680/64305], Loss: 4.7117\n",
      "Epoch [2/2], Step [46690/64305], Loss: 4.7727\n",
      "Epoch [2/2], Step [46700/64305], Loss: 4.8778\n",
      "Epoch [2/2], Step [46710/64305], Loss: 4.7268\n",
      "Epoch [2/2], Step [46720/64305], Loss: 4.7996\n",
      "Epoch [2/2], Step [46730/64305], Loss: 4.6180\n",
      "Epoch [2/2], Step [46740/64305], Loss: 4.7411\n",
      "Epoch [2/2], Step [46750/64305], Loss: 4.7781\n",
      "Epoch [2/2], Step [46760/64305], Loss: 4.6778\n",
      "Epoch [2/2], Step [46770/64305], Loss: 4.8313\n",
      "Epoch [2/2], Step [46780/64305], Loss: 4.8082\n",
      "Epoch [2/2], Step [46790/64305], Loss: 4.7218\n",
      "Epoch [2/2], Step [46800/64305], Loss: 4.7451\n",
      "Epoch [2/2], Step [46810/64305], Loss: 4.8071\n",
      "Epoch [2/2], Step [46820/64305], Loss: 4.5702\n",
      "Epoch [2/2], Step [46830/64305], Loss: 4.7308\n",
      "Epoch [2/2], Step [46840/64305], Loss: 4.6530\n",
      "Epoch [2/2], Step [46850/64305], Loss: 4.6124\n",
      "Epoch [2/2], Step [46860/64305], Loss: 4.9097\n",
      "Epoch [2/2], Step [46870/64305], Loss: 4.6916\n",
      "Epoch [2/2], Step [46880/64305], Loss: 4.7293\n",
      "Epoch [2/2], Step [46890/64305], Loss: 4.9605\n",
      "Epoch [2/2], Step [46900/64305], Loss: 4.5226\n",
      "Epoch [2/2], Step [46910/64305], Loss: 4.9431\n",
      "Epoch [2/2], Step [46920/64305], Loss: 4.8006\n",
      "Epoch [2/2], Step [46930/64305], Loss: 4.8386\n",
      "Epoch [2/2], Step [46940/64305], Loss: 4.7188\n",
      "Epoch [2/2], Step [46950/64305], Loss: 4.7148\n",
      "Epoch [2/2], Step [46960/64305], Loss: 4.9938\n",
      "Epoch [2/2], Step [46970/64305], Loss: 4.8295\n",
      "Epoch [2/2], Step [46980/64305], Loss: 4.9432\n",
      "Epoch [2/2], Step [46990/64305], Loss: 4.7358\n",
      "Epoch [2/2], Step [47000/64305], Loss: 4.7253\n",
      "Epoch [2/2], Step [47010/64305], Loss: 4.7574\n",
      "Epoch [2/2], Step [47020/64305], Loss: 4.7666\n",
      "Epoch [2/2], Step [47030/64305], Loss: 4.7243\n",
      "Epoch [2/2], Step [47040/64305], Loss: 4.7949\n",
      "Epoch [2/2], Step [47050/64305], Loss: 4.6404\n",
      "Epoch [2/2], Step [47060/64305], Loss: 4.5563\n",
      "Epoch [2/2], Step [47070/64305], Loss: 4.7100\n",
      "Epoch [2/2], Step [47080/64305], Loss: 4.6780\n",
      "Epoch [2/2], Step [47090/64305], Loss: 4.5960\n",
      "Epoch [2/2], Step [47100/64305], Loss: 4.6106\n",
      "Epoch [2/2], Step [47110/64305], Loss: 4.8130\n",
      "Epoch [2/2], Step [47120/64305], Loss: 4.5801\n",
      "Epoch [2/2], Step [47130/64305], Loss: 4.7962\n",
      "Epoch [2/2], Step [47140/64305], Loss: 4.7608\n",
      "Epoch [2/2], Step [47150/64305], Loss: 4.9725\n",
      "Epoch [2/2], Step [47160/64305], Loss: 4.6196\n",
      "Epoch [2/2], Step [47170/64305], Loss: 4.6281\n",
      "Epoch [2/2], Step [47180/64305], Loss: 4.8516\n",
      "Epoch [2/2], Step [47190/64305], Loss: 4.7060\n",
      "Epoch [2/2], Step [47200/64305], Loss: 4.7055\n",
      "Epoch [2/2], Step [47210/64305], Loss: 4.5396\n",
      "Epoch [2/2], Step [47220/64305], Loss: 4.9358\n",
      "Epoch [2/2], Step [47230/64305], Loss: 4.9406\n",
      "Epoch [2/2], Step [47240/64305], Loss: 4.6890\n",
      "Epoch [2/2], Step [47250/64305], Loss: 4.7064\n",
      "Epoch [2/2], Step [47260/64305], Loss: 4.7320\n",
      "Epoch [2/2], Step [47270/64305], Loss: 4.8119\n",
      "Epoch [2/2], Step [47280/64305], Loss: 4.6929\n",
      "Epoch [2/2], Step [47290/64305], Loss: 4.7136\n",
      "Epoch [2/2], Step [47300/64305], Loss: 4.7384\n",
      "Epoch [2/2], Step [47310/64305], Loss: 4.6639\n",
      "Epoch [2/2], Step [47320/64305], Loss: 4.9613\n",
      "Epoch [2/2], Step [47330/64305], Loss: 4.6147\n",
      "Epoch [2/2], Step [47340/64305], Loss: 4.8665\n",
      "Epoch [2/2], Step [47350/64305], Loss: 4.7912\n",
      "Epoch [2/2], Step [47360/64305], Loss: 4.6630\n",
      "Epoch [2/2], Step [47370/64305], Loss: 4.7627\n",
      "Epoch [2/2], Step [47380/64305], Loss: 4.8141\n",
      "Epoch [2/2], Step [47390/64305], Loss: 4.9332\n",
      "Epoch [2/2], Step [47400/64305], Loss: 4.8970\n",
      "Epoch [2/2], Step [47410/64305], Loss: 4.8767\n",
      "Epoch [2/2], Step [47420/64305], Loss: 4.8068\n",
      "Epoch [2/2], Step [47430/64305], Loss: 4.7762\n",
      "Epoch [2/2], Step [47440/64305], Loss: 4.8159\n",
      "Epoch [2/2], Step [47450/64305], Loss: 4.7364\n",
      "Epoch [2/2], Step [47460/64305], Loss: 4.7742\n",
      "Epoch [2/2], Step [47470/64305], Loss: 4.8556\n",
      "Epoch [2/2], Step [47480/64305], Loss: 4.6733\n",
      "Epoch [2/2], Step [47490/64305], Loss: 4.8298\n",
      "Epoch [2/2], Step [47500/64305], Loss: 4.7861\n",
      "Epoch [2/2], Step [47510/64305], Loss: 4.6618\n",
      "Epoch [2/2], Step [47520/64305], Loss: 4.7448\n",
      "Epoch [2/2], Step [47530/64305], Loss: 4.6554\n",
      "Epoch [2/2], Step [47540/64305], Loss: 4.6791\n",
      "Epoch [2/2], Step [47550/64305], Loss: 4.7379\n",
      "Epoch [2/2], Step [47560/64305], Loss: 4.8254\n",
      "Epoch [2/2], Step [47570/64305], Loss: 4.7849\n",
      "Epoch [2/2], Step [47580/64305], Loss: 4.7099\n",
      "Epoch [2/2], Step [47590/64305], Loss: 4.8016\n",
      "Epoch [2/2], Step [47600/64305], Loss: 4.7379\n",
      "Epoch [2/2], Step [47610/64305], Loss: 4.9115\n",
      "Epoch [2/2], Step [47620/64305], Loss: 4.7412\n",
      "Epoch [2/2], Step [47630/64305], Loss: 4.7838\n",
      "Epoch [2/2], Step [47640/64305], Loss: 4.7081\n",
      "Epoch [2/2], Step [47650/64305], Loss: 4.5663\n",
      "Epoch [2/2], Step [47660/64305], Loss: 4.6735\n",
      "Epoch [2/2], Step [47670/64305], Loss: 4.7425\n",
      "Epoch [2/2], Step [47680/64305], Loss: 4.8626\n",
      "Epoch [2/2], Step [47690/64305], Loss: 4.7346\n",
      "Epoch [2/2], Step [47700/64305], Loss: 4.5687\n",
      "Epoch [2/2], Step [47710/64305], Loss: 4.8162\n",
      "Epoch [2/2], Step [47720/64305], Loss: 4.8542\n",
      "Epoch [2/2], Step [47730/64305], Loss: 4.8476\n",
      "Epoch [2/2], Step [47740/64305], Loss: 4.8526\n",
      "Epoch [2/2], Step [47750/64305], Loss: 4.5013\n",
      "Epoch [2/2], Step [47760/64305], Loss: 4.7848\n",
      "Epoch [2/2], Step [47770/64305], Loss: 4.7996\n",
      "Epoch [2/2], Step [47780/64305], Loss: 4.7606\n",
      "Epoch [2/2], Step [47790/64305], Loss: 4.8673\n",
      "Epoch [2/2], Step [47800/64305], Loss: 4.8118\n",
      "Epoch [2/2], Step [47810/64305], Loss: 4.5975\n",
      "Epoch [2/2], Step [47820/64305], Loss: 4.8770\n",
      "Epoch [2/2], Step [47830/64305], Loss: 4.7549\n",
      "Epoch [2/2], Step [47840/64305], Loss: 4.5683\n",
      "Epoch [2/2], Step [47850/64305], Loss: 4.5931\n",
      "Epoch [2/2], Step [47860/64305], Loss: 4.6660\n",
      "Epoch [2/2], Step [47870/64305], Loss: 4.8848\n",
      "Epoch [2/2], Step [47880/64305], Loss: 4.7755\n",
      "Epoch [2/2], Step [47890/64305], Loss: 4.7746\n",
      "Epoch [2/2], Step [47900/64305], Loss: 4.6575\n",
      "Epoch [2/2], Step [47910/64305], Loss: 5.0058\n",
      "Epoch [2/2], Step [47920/64305], Loss: 4.9469\n",
      "Epoch [2/2], Step [47930/64305], Loss: 4.5465\n",
      "Epoch [2/2], Step [47940/64305], Loss: 4.7318\n",
      "Epoch [2/2], Step [47950/64305], Loss: 4.6761\n",
      "Epoch [2/2], Step [47960/64305], Loss: 4.7839\n",
      "Epoch [2/2], Step [47970/64305], Loss: 4.7656\n",
      "Epoch [2/2], Step [47980/64305], Loss: 4.6825\n",
      "Epoch [2/2], Step [47990/64305], Loss: 4.7091\n",
      "Epoch [2/2], Step [48000/64305], Loss: 4.7459\n",
      "Epoch [2/2], Step [48010/64305], Loss: 4.7093\n",
      "Epoch [2/2], Step [48020/64305], Loss: 5.0191\n",
      "Epoch [2/2], Step [48030/64305], Loss: 4.9563\n",
      "Epoch [2/2], Step [48040/64305], Loss: 4.8122\n",
      "Epoch [2/2], Step [48050/64305], Loss: 4.8738\n",
      "Epoch [2/2], Step [48060/64305], Loss: 4.5646\n",
      "Epoch [2/2], Step [48070/64305], Loss: 4.6122\n",
      "Epoch [2/2], Step [48080/64305], Loss: 4.6437\n",
      "Epoch [2/2], Step [48090/64305], Loss: 4.6622\n",
      "Epoch [2/2], Step [48100/64305], Loss: 4.7532\n",
      "Epoch [2/2], Step [48110/64305], Loss: 4.6811\n",
      "Epoch [2/2], Step [48120/64305], Loss: 4.9173\n",
      "Epoch [2/2], Step [48130/64305], Loss: 4.8167\n",
      "Epoch [2/2], Step [48140/64305], Loss: 4.6976\n",
      "Epoch [2/2], Step [48150/64305], Loss: 4.7601\n",
      "Epoch [2/2], Step [48160/64305], Loss: 4.6364\n",
      "Epoch [2/2], Step [48170/64305], Loss: 4.7237\n",
      "Epoch [2/2], Step [48180/64305], Loss: 4.6885\n",
      "Epoch [2/2], Step [48190/64305], Loss: 5.0052\n",
      "Epoch [2/2], Step [48200/64305], Loss: 4.6469\n",
      "Epoch [2/2], Step [48210/64305], Loss: 4.6323\n",
      "Epoch [2/2], Step [48220/64305], Loss: 4.5648\n",
      "Epoch [2/2], Step [48230/64305], Loss: 4.8247\n",
      "Epoch [2/2], Step [48240/64305], Loss: 4.4477\n",
      "Epoch [2/2], Step [48250/64305], Loss: 4.6259\n",
      "Epoch [2/2], Step [48260/64305], Loss: 4.8419\n",
      "Epoch [2/2], Step [48270/64305], Loss: 4.6761\n",
      "Epoch [2/2], Step [48280/64305], Loss: 4.6047\n",
      "Epoch [2/2], Step [48290/64305], Loss: 4.7372\n",
      "Epoch [2/2], Step [48300/64305], Loss: 4.7725\n",
      "Epoch [2/2], Step [48310/64305], Loss: 4.7440\n",
      "Epoch [2/2], Step [48320/64305], Loss: 4.9418\n",
      "Epoch [2/2], Step [48330/64305], Loss: 4.6947\n",
      "Epoch [2/2], Step [48340/64305], Loss: 4.5836\n",
      "Epoch [2/2], Step [48350/64305], Loss: 4.7657\n",
      "Epoch [2/2], Step [48360/64305], Loss: 4.8020\n",
      "Epoch [2/2], Step [48370/64305], Loss: 4.7505\n",
      "Epoch [2/2], Step [48380/64305], Loss: 4.7228\n",
      "Epoch [2/2], Step [48390/64305], Loss: 4.7446\n",
      "Epoch [2/2], Step [48400/64305], Loss: 4.7814\n",
      "Epoch [2/2], Step [48410/64305], Loss: 4.8124\n",
      "Epoch [2/2], Step [48420/64305], Loss: 4.8272\n",
      "Epoch [2/2], Step [48430/64305], Loss: 4.7828\n",
      "Epoch [2/2], Step [48440/64305], Loss: 4.7098\n",
      "Epoch [2/2], Step [48450/64305], Loss: 4.6148\n",
      "Epoch [2/2], Step [48460/64305], Loss: 4.8211\n",
      "Epoch [2/2], Step [48470/64305], Loss: 4.7368\n",
      "Epoch [2/2], Step [48480/64305], Loss: 4.9955\n",
      "Epoch [2/2], Step [48490/64305], Loss: 4.5142\n",
      "Epoch [2/2], Step [48500/64305], Loss: 4.8475\n",
      "Epoch [2/2], Step [48510/64305], Loss: 4.7229\n",
      "Epoch [2/2], Step [48520/64305], Loss: 4.8620\n",
      "Epoch [2/2], Step [48530/64305], Loss: 4.7336\n",
      "Epoch [2/2], Step [48540/64305], Loss: 4.8038\n",
      "Epoch [2/2], Step [48550/64305], Loss: 4.5824\n",
      "Epoch [2/2], Step [48560/64305], Loss: 4.8683\n",
      "Epoch [2/2], Step [48570/64305], Loss: 4.7710\n",
      "Epoch [2/2], Step [48580/64305], Loss: 4.6899\n",
      "Epoch [2/2], Step [48590/64305], Loss: 4.8900\n",
      "Epoch [2/2], Step [48600/64305], Loss: 4.8661\n",
      "Epoch [2/2], Step [48610/64305], Loss: 4.6935\n",
      "Epoch [2/2], Step [48620/64305], Loss: 4.6404\n",
      "Epoch [2/2], Step [48630/64305], Loss: 4.7817\n",
      "Epoch [2/2], Step [48640/64305], Loss: 4.7636\n",
      "Epoch [2/2], Step [48650/64305], Loss: 4.5663\n",
      "Epoch [2/2], Step [48660/64305], Loss: 4.8511\n",
      "Epoch [2/2], Step [48670/64305], Loss: 5.0394\n",
      "Epoch [2/2], Step [48680/64305], Loss: 4.7261\n",
      "Epoch [2/2], Step [48690/64305], Loss: 4.5574\n",
      "Epoch [2/2], Step [48700/64305], Loss: 4.6758\n",
      "Epoch [2/2], Step [48710/64305], Loss: 4.5998\n",
      "Epoch [2/2], Step [48720/64305], Loss: 4.7437\n",
      "Epoch [2/2], Step [48730/64305], Loss: 4.6740\n",
      "Epoch [2/2], Step [48740/64305], Loss: 4.7742\n",
      "Epoch [2/2], Step [48750/64305], Loss: 4.7123\n",
      "Epoch [2/2], Step [48760/64305], Loss: 4.8154\n",
      "Epoch [2/2], Step [48770/64305], Loss: 4.6980\n",
      "Epoch [2/2], Step [48780/64305], Loss: 4.7230\n",
      "Epoch [2/2], Step [48790/64305], Loss: 4.6558\n",
      "Epoch [2/2], Step [48800/64305], Loss: 4.6533\n",
      "Epoch [2/2], Step [48810/64305], Loss: 4.7681\n",
      "Epoch [2/2], Step [48820/64305], Loss: 4.9366\n",
      "Epoch [2/2], Step [48830/64305], Loss: 4.7606\n",
      "Epoch [2/2], Step [48840/64305], Loss: 4.8175\n",
      "Epoch [2/2], Step [48850/64305], Loss: 4.6471\n",
      "Epoch [2/2], Step [48860/64305], Loss: 4.7511\n",
      "Epoch [2/2], Step [48870/64305], Loss: 4.7452\n",
      "Epoch [2/2], Step [48880/64305], Loss: 4.8329\n",
      "Epoch [2/2], Step [48890/64305], Loss: 4.7273\n",
      "Epoch [2/2], Step [48900/64305], Loss: 4.6883\n",
      "Epoch [2/2], Step [48910/64305], Loss: 4.7190\n",
      "Epoch [2/2], Step [48920/64305], Loss: 4.9951\n",
      "Epoch [2/2], Step [48930/64305], Loss: 4.8876\n",
      "Epoch [2/2], Step [48940/64305], Loss: 4.5710\n",
      "Epoch [2/2], Step [48950/64305], Loss: 4.6215\n",
      "Epoch [2/2], Step [48960/64305], Loss: 4.7635\n",
      "Epoch [2/2], Step [48970/64305], Loss: 4.6580\n",
      "Epoch [2/2], Step [48980/64305], Loss: 4.7992\n",
      "Epoch [2/2], Step [48990/64305], Loss: 4.5969\n",
      "Epoch [2/2], Step [49000/64305], Loss: 4.7221\n",
      "Epoch [2/2], Step [49010/64305], Loss: 4.6057\n",
      "Epoch [2/2], Step [49020/64305], Loss: 4.6857\n",
      "Epoch [2/2], Step [49030/64305], Loss: 4.5743\n",
      "Epoch [2/2], Step [49040/64305], Loss: 4.7481\n",
      "Epoch [2/2], Step [49050/64305], Loss: 4.5134\n",
      "Epoch [2/2], Step [49060/64305], Loss: 4.7965\n",
      "Epoch [2/2], Step [49070/64305], Loss: 4.6357\n",
      "Epoch [2/2], Step [49080/64305], Loss: 4.7683\n",
      "Epoch [2/2], Step [49090/64305], Loss: 4.8708\n",
      "Epoch [2/2], Step [49100/64305], Loss: 4.8465\n",
      "Epoch [2/2], Step [49110/64305], Loss: 4.8284\n",
      "Epoch [2/2], Step [49120/64305], Loss: 4.8277\n",
      "Epoch [2/2], Step [49130/64305], Loss: 4.6948\n",
      "Epoch [2/2], Step [49140/64305], Loss: 4.9606\n",
      "Epoch [2/2], Step [49150/64305], Loss: 4.8058\n",
      "Epoch [2/2], Step [49160/64305], Loss: 4.7132\n",
      "Epoch [2/2], Step [49170/64305], Loss: 4.7595\n",
      "Epoch [2/2], Step [49180/64305], Loss: 4.6563\n",
      "Epoch [2/2], Step [49190/64305], Loss: 4.7836\n",
      "Epoch [2/2], Step [49200/64305], Loss: 4.8414\n",
      "Epoch [2/2], Step [49210/64305], Loss: 4.8603\n",
      "Epoch [2/2], Step [49220/64305], Loss: 4.7365\n",
      "Epoch [2/2], Step [49230/64305], Loss: 4.8824\n",
      "Epoch [2/2], Step [49240/64305], Loss: 4.9680\n",
      "Epoch [2/2], Step [49250/64305], Loss: 4.8219\n",
      "Epoch [2/2], Step [49260/64305], Loss: 4.7585\n",
      "Epoch [2/2], Step [49270/64305], Loss: 4.7567\n",
      "Epoch [2/2], Step [49280/64305], Loss: 4.8785\n",
      "Epoch [2/2], Step [49290/64305], Loss: 4.5790\n",
      "Epoch [2/2], Step [49300/64305], Loss: 4.8839\n",
      "Epoch [2/2], Step [49310/64305], Loss: 4.8410\n",
      "Epoch [2/2], Step [49320/64305], Loss: 5.0267\n",
      "Epoch [2/2], Step [49330/64305], Loss: 4.8980\n",
      "Epoch [2/2], Step [49340/64305], Loss: 4.6641\n",
      "Epoch [2/2], Step [49350/64305], Loss: 4.6359\n",
      "Epoch [2/2], Step [49360/64305], Loss: 4.7482\n",
      "Epoch [2/2], Step [49370/64305], Loss: 4.7063\n",
      "Epoch [2/2], Step [49380/64305], Loss: 4.5954\n",
      "Epoch [2/2], Step [49390/64305], Loss: 4.6914\n",
      "Epoch [2/2], Step [49400/64305], Loss: 4.8204\n",
      "Epoch [2/2], Step [49410/64305], Loss: 4.7445\n",
      "Epoch [2/2], Step [49420/64305], Loss: 4.8367\n",
      "Epoch [2/2], Step [49430/64305], Loss: 4.7248\n",
      "Epoch [2/2], Step [49440/64305], Loss: 4.7219\n",
      "Epoch [2/2], Step [49450/64305], Loss: 4.6819\n",
      "Epoch [2/2], Step [49460/64305], Loss: 4.6369\n",
      "Epoch [2/2], Step [49470/64305], Loss: 4.7772\n",
      "Epoch [2/2], Step [49480/64305], Loss: 4.9706\n",
      "Epoch [2/2], Step [49490/64305], Loss: 4.5287\n",
      "Epoch [2/2], Step [49500/64305], Loss: 4.6186\n",
      "Epoch [2/2], Step [49510/64305], Loss: 4.6373\n",
      "Epoch [2/2], Step [49520/64305], Loss: 4.7394\n",
      "Epoch [2/2], Step [49530/64305], Loss: 4.6679\n",
      "Epoch [2/2], Step [49540/64305], Loss: 4.7577\n",
      "Epoch [2/2], Step [49550/64305], Loss: 4.8916\n",
      "Epoch [2/2], Step [49560/64305], Loss: 4.9055\n",
      "Epoch [2/2], Step [49570/64305], Loss: 4.8605\n",
      "Epoch [2/2], Step [49580/64305], Loss: 4.9984\n",
      "Epoch [2/2], Step [49590/64305], Loss: 4.6595\n",
      "Epoch [2/2], Step [49600/64305], Loss: 4.6494\n",
      "Epoch [2/2], Step [49610/64305], Loss: 4.5640\n",
      "Epoch [2/2], Step [49620/64305], Loss: 4.7299\n",
      "Epoch [2/2], Step [49630/64305], Loss: 4.6984\n",
      "Epoch [2/2], Step [49640/64305], Loss: 4.7650\n",
      "Epoch [2/2], Step [49650/64305], Loss: 4.8057\n",
      "Epoch [2/2], Step [49660/64305], Loss: 4.7223\n",
      "Epoch [2/2], Step [49670/64305], Loss: 4.6621\n",
      "Epoch [2/2], Step [49680/64305], Loss: 4.7530\n",
      "Epoch [2/2], Step [49690/64305], Loss: 4.7535\n",
      "Epoch [2/2], Step [49700/64305], Loss: 4.7955\n",
      "Epoch [2/2], Step [49710/64305], Loss: 4.7877\n",
      "Epoch [2/2], Step [49720/64305], Loss: 4.7214\n",
      "Epoch [2/2], Step [49730/64305], Loss: 4.7503\n",
      "Epoch [2/2], Step [49740/64305], Loss: 4.7416\n",
      "Epoch [2/2], Step [49750/64305], Loss: 4.8901\n",
      "Epoch [2/2], Step [49760/64305], Loss: 4.5861\n",
      "Epoch [2/2], Step [49770/64305], Loss: 4.9649\n",
      "Epoch [2/2], Step [49780/64305], Loss: 4.7683\n",
      "Epoch [2/2], Step [49790/64305], Loss: 4.7029\n",
      "Epoch [2/2], Step [49800/64305], Loss: 4.8575\n",
      "Epoch [2/2], Step [49810/64305], Loss: 4.8406\n",
      "Epoch [2/2], Step [49820/64305], Loss: 4.9301\n",
      "Epoch [2/2], Step [49830/64305], Loss: 4.8704\n",
      "Epoch [2/2], Step [49840/64305], Loss: 4.9969\n",
      "Epoch [2/2], Step [49850/64305], Loss: 4.6618\n",
      "Epoch [2/2], Step [49860/64305], Loss: 4.8205\n",
      "Epoch [2/2], Step [49870/64305], Loss: 4.7096\n",
      "Epoch [2/2], Step [49880/64305], Loss: 4.7324\n",
      "Epoch [2/2], Step [49890/64305], Loss: 4.7580\n",
      "Epoch [2/2], Step [49900/64305], Loss: 4.5911\n",
      "Epoch [2/2], Step [49910/64305], Loss: 4.7380\n",
      "Epoch [2/2], Step [49920/64305], Loss: 4.6715\n",
      "Epoch [2/2], Step [49930/64305], Loss: 4.7365\n",
      "Epoch [2/2], Step [49940/64305], Loss: 4.6701\n",
      "Epoch [2/2], Step [49950/64305], Loss: 4.5992\n",
      "Epoch [2/2], Step [49960/64305], Loss: 4.4331\n",
      "Epoch [2/2], Step [49970/64305], Loss: 4.6830\n",
      "Epoch [2/2], Step [49980/64305], Loss: 4.6988\n",
      "Epoch [2/2], Step [49990/64305], Loss: 4.6580\n",
      "Epoch [2/2], Step [50000/64305], Loss: 4.8906\n",
      "Epoch [2/2], Step [50010/64305], Loss: 4.7171\n",
      "Epoch [2/2], Step [50020/64305], Loss: 4.5731\n",
      "Epoch [2/2], Step [50030/64305], Loss: 4.6887\n",
      "Epoch [2/2], Step [50040/64305], Loss: 4.6866\n",
      "Epoch [2/2], Step [50050/64305], Loss: 4.4823\n",
      "Epoch [2/2], Step [50060/64305], Loss: 4.7860\n",
      "Epoch [2/2], Step [50070/64305], Loss: 4.6264\n",
      "Epoch [2/2], Step [50080/64305], Loss: 4.5089\n",
      "Epoch [2/2], Step [50090/64305], Loss: 4.7786\n",
      "Epoch [2/2], Step [50100/64305], Loss: 4.6490\n",
      "Epoch [2/2], Step [50110/64305], Loss: 4.6654\n",
      "Epoch [2/2], Step [50120/64305], Loss: 4.7304\n",
      "Epoch [2/2], Step [50130/64305], Loss: 5.0278\n",
      "Epoch [2/2], Step [50140/64305], Loss: 4.8932\n",
      "Epoch [2/2], Step [50150/64305], Loss: 4.8918\n",
      "Epoch [2/2], Step [50160/64305], Loss: 4.9831\n",
      "Epoch [2/2], Step [50170/64305], Loss: 4.6235\n",
      "Epoch [2/2], Step [50180/64305], Loss: 4.6233\n",
      "Epoch [2/2], Step [50190/64305], Loss: 4.6429\n",
      "Epoch [2/2], Step [50200/64305], Loss: 4.9586\n",
      "Epoch [2/2], Step [50210/64305], Loss: 4.6948\n",
      "Epoch [2/2], Step [50220/64305], Loss: 4.5431\n",
      "Epoch [2/2], Step [50230/64305], Loss: 4.7898\n",
      "Epoch [2/2], Step [50240/64305], Loss: 4.6318\n",
      "Epoch [2/2], Step [50250/64305], Loss: 4.6985\n",
      "Epoch [2/2], Step [50260/64305], Loss: 4.7681\n",
      "Epoch [2/2], Step [50270/64305], Loss: 4.5990\n",
      "Epoch [2/2], Step [50280/64305], Loss: 4.4747\n",
      "Epoch [2/2], Step [50290/64305], Loss: 4.8019\n",
      "Epoch [2/2], Step [50300/64305], Loss: 4.6219\n",
      "Epoch [2/2], Step [50310/64305], Loss: 4.8269\n",
      "Epoch [2/2], Step [50320/64305], Loss: 4.9540\n",
      "Epoch [2/2], Step [50330/64305], Loss: 4.8156\n",
      "Epoch [2/2], Step [50340/64305], Loss: 4.8496\n",
      "Epoch [2/2], Step [50350/64305], Loss: 4.5814\n",
      "Epoch [2/2], Step [50360/64305], Loss: 4.7135\n",
      "Epoch [2/2], Step [50370/64305], Loss: 4.6890\n",
      "Epoch [2/2], Step [50380/64305], Loss: 4.9064\n",
      "Epoch [2/2], Step [50390/64305], Loss: 4.5891\n",
      "Epoch [2/2], Step [50400/64305], Loss: 4.7088\n",
      "Epoch [2/2], Step [50410/64305], Loss: 4.7033\n",
      "Epoch [2/2], Step [50420/64305], Loss: 4.6245\n",
      "Epoch [2/2], Step [50430/64305], Loss: 4.7354\n",
      "Epoch [2/2], Step [50440/64305], Loss: 4.6591\n",
      "Epoch [2/2], Step [50450/64305], Loss: 4.7607\n",
      "Epoch [2/2], Step [50460/64305], Loss: 4.6641\n",
      "Epoch [2/2], Step [50470/64305], Loss: 4.8538\n",
      "Epoch [2/2], Step [50480/64305], Loss: 4.8067\n",
      "Epoch [2/2], Step [50490/64305], Loss: 4.5080\n",
      "Epoch [2/2], Step [50500/64305], Loss: 4.7503\n",
      "Epoch [2/2], Step [50510/64305], Loss: 4.7303\n",
      "Epoch [2/2], Step [50520/64305], Loss: 4.7260\n",
      "Epoch [2/2], Step [50530/64305], Loss: 4.6878\n",
      "Epoch [2/2], Step [50540/64305], Loss: 4.7315\n",
      "Epoch [2/2], Step [50550/64305], Loss: 4.7104\n",
      "Epoch [2/2], Step [50560/64305], Loss: 4.5894\n",
      "Epoch [2/2], Step [50570/64305], Loss: 4.8047\n",
      "Epoch [2/2], Step [50580/64305], Loss: 4.6777\n",
      "Epoch [2/2], Step [50590/64305], Loss: 4.8242\n",
      "Epoch [2/2], Step [50600/64305], Loss: 4.8343\n",
      "Epoch [2/2], Step [50610/64305], Loss: 4.6797\n",
      "Epoch [2/2], Step [50620/64305], Loss: 4.5458\n",
      "Epoch [2/2], Step [50630/64305], Loss: 4.8891\n",
      "Epoch [2/2], Step [50640/64305], Loss: 4.7350\n",
      "Epoch [2/2], Step [50650/64305], Loss: 4.6828\n",
      "Epoch [2/2], Step [50660/64305], Loss: 4.6031\n",
      "Epoch [2/2], Step [50670/64305], Loss: 4.6114\n",
      "Epoch [2/2], Step [50680/64305], Loss: 4.6748\n",
      "Epoch [2/2], Step [50690/64305], Loss: 4.6531\n",
      "Epoch [2/2], Step [50700/64305], Loss: 4.8170\n",
      "Epoch [2/2], Step [50710/64305], Loss: 4.6205\n",
      "Epoch [2/2], Step [50720/64305], Loss: 4.7030\n",
      "Epoch [2/2], Step [50730/64305], Loss: 4.8899\n",
      "Epoch [2/2], Step [50740/64305], Loss: 4.6423\n",
      "Epoch [2/2], Step [50750/64305], Loss: 4.7316\n",
      "Epoch [2/2], Step [50760/64305], Loss: 4.5792\n",
      "Epoch [2/2], Step [50770/64305], Loss: 4.8197\n",
      "Epoch [2/2], Step [50780/64305], Loss: 4.6534\n",
      "Epoch [2/2], Step [50790/64305], Loss: 4.6970\n",
      "Epoch [2/2], Step [50800/64305], Loss: 4.7794\n",
      "Epoch [2/2], Step [50810/64305], Loss: 4.6751\n",
      "Epoch [2/2], Step [50820/64305], Loss: 4.6983\n",
      "Epoch [2/2], Step [50830/64305], Loss: 4.7723\n",
      "Epoch [2/2], Step [50840/64305], Loss: 4.9763\n",
      "Epoch [2/2], Step [50850/64305], Loss: 4.5978\n",
      "Epoch [2/2], Step [50860/64305], Loss: 5.0171\n",
      "Epoch [2/2], Step [50870/64305], Loss: 4.9141\n",
      "Epoch [2/2], Step [50880/64305], Loss: 4.8714\n",
      "Epoch [2/2], Step [50890/64305], Loss: 4.9417\n",
      "Epoch [2/2], Step [50900/64305], Loss: 4.6696\n",
      "Epoch [2/2], Step [50910/64305], Loss: 4.7190\n",
      "Epoch [2/2], Step [50920/64305], Loss: 4.7716\n",
      "Epoch [2/2], Step [50930/64305], Loss: 4.6718\n",
      "Epoch [2/2], Step [50940/64305], Loss: 4.8237\n",
      "Epoch [2/2], Step [50950/64305], Loss: 4.7806\n",
      "Epoch [2/2], Step [50960/64305], Loss: 4.7217\n",
      "Epoch [2/2], Step [50970/64305], Loss: 4.7498\n",
      "Epoch [2/2], Step [50980/64305], Loss: 4.7881\n",
      "Epoch [2/2], Step [50990/64305], Loss: 4.8094\n",
      "Epoch [2/2], Step [51000/64305], Loss: 4.8820\n",
      "Epoch [2/2], Step [51010/64305], Loss: 4.7812\n",
      "Epoch [2/2], Step [51020/64305], Loss: 4.8810\n",
      "Epoch [2/2], Step [51030/64305], Loss: 4.6799\n",
      "Epoch [2/2], Step [51040/64305], Loss: 4.6423\n",
      "Epoch [2/2], Step [51050/64305], Loss: 4.7273\n",
      "Epoch [2/2], Step [51060/64305], Loss: 4.5161\n",
      "Epoch [2/2], Step [51070/64305], Loss: 4.7982\n",
      "Epoch [2/2], Step [51080/64305], Loss: 4.7251\n",
      "Epoch [2/2], Step [51090/64305], Loss: 4.8183\n",
      "Epoch [2/2], Step [51100/64305], Loss: 4.5743\n",
      "Epoch [2/2], Step [51110/64305], Loss: 4.7921\n",
      "Epoch [2/2], Step [51120/64305], Loss: 4.6950\n",
      "Epoch [2/2], Step [51130/64305], Loss: 4.7891\n",
      "Epoch [2/2], Step [51140/64305], Loss: 4.9408\n",
      "Epoch [2/2], Step [51150/64305], Loss: 4.8045\n",
      "Epoch [2/2], Step [51160/64305], Loss: 4.7890\n",
      "Epoch [2/2], Step [51170/64305], Loss: 4.7617\n",
      "Epoch [2/2], Step [51180/64305], Loss: 4.6637\n",
      "Epoch [2/2], Step [51190/64305], Loss: 4.7284\n",
      "Epoch [2/2], Step [51200/64305], Loss: 4.8849\n",
      "Epoch [2/2], Step [51210/64305], Loss: 4.6009\n",
      "Epoch [2/2], Step [51220/64305], Loss: 4.8814\n",
      "Epoch [2/2], Step [51230/64305], Loss: 4.8763\n",
      "Epoch [2/2], Step [51240/64305], Loss: 4.6437\n",
      "Epoch [2/2], Step [51250/64305], Loss: 4.8197\n",
      "Epoch [2/2], Step [51260/64305], Loss: 4.8689\n",
      "Epoch [2/2], Step [51270/64305], Loss: 4.7835\n",
      "Epoch [2/2], Step [51280/64305], Loss: 4.8679\n",
      "Epoch [2/2], Step [51290/64305], Loss: 4.7632\n",
      "Epoch [2/2], Step [51300/64305], Loss: 4.7118\n",
      "Epoch [2/2], Step [51310/64305], Loss: 4.6749\n",
      "Epoch [2/2], Step [51320/64305], Loss: 4.7906\n",
      "Epoch [2/2], Step [51330/64305], Loss: 4.7143\n",
      "Epoch [2/2], Step [51340/64305], Loss: 4.6774\n",
      "Epoch [2/2], Step [51350/64305], Loss: 4.7613\n",
      "Epoch [2/2], Step [51360/64305], Loss: 4.8104\n",
      "Epoch [2/2], Step [51370/64305], Loss: 4.8926\n",
      "Epoch [2/2], Step [51380/64305], Loss: 4.9826\n",
      "Epoch [2/2], Step [51390/64305], Loss: 4.4370\n",
      "Epoch [2/2], Step [51400/64305], Loss: 4.5818\n",
      "Epoch [2/2], Step [51410/64305], Loss: 4.6147\n",
      "Epoch [2/2], Step [51420/64305], Loss: 4.6633\n",
      "Epoch [2/2], Step [51430/64305], Loss: 4.9359\n",
      "Epoch [2/2], Step [51440/64305], Loss: 4.7599\n",
      "Epoch [2/2], Step [51450/64305], Loss: 4.7936\n",
      "Epoch [2/2], Step [51460/64305], Loss: 4.8278\n",
      "Epoch [2/2], Step [51470/64305], Loss: 4.8439\n",
      "Epoch [2/2], Step [51480/64305], Loss: 4.6652\n",
      "Epoch [2/2], Step [51490/64305], Loss: 4.5578\n",
      "Epoch [2/2], Step [51500/64305], Loss: 4.7824\n",
      "Epoch [2/2], Step [51510/64305], Loss: 4.6611\n",
      "Epoch [2/2], Step [51520/64305], Loss: 4.8918\n",
      "Epoch [2/2], Step [51530/64305], Loss: 4.6166\n",
      "Epoch [2/2], Step [51540/64305], Loss: 4.5836\n",
      "Epoch [2/2], Step [51550/64305], Loss: 4.6640\n",
      "Epoch [2/2], Step [51560/64305], Loss: 4.6818\n",
      "Epoch [2/2], Step [51570/64305], Loss: 4.7044\n",
      "Epoch [2/2], Step [51580/64305], Loss: 4.7871\n",
      "Epoch [2/2], Step [51590/64305], Loss: 4.7678\n",
      "Epoch [2/2], Step [51600/64305], Loss: 5.1539\n",
      "Epoch [2/2], Step [51610/64305], Loss: 4.7294\n",
      "Epoch [2/2], Step [51620/64305], Loss: 4.6013\n",
      "Epoch [2/2], Step [51630/64305], Loss: 4.6786\n",
      "Epoch [2/2], Step [51640/64305], Loss: 4.9361\n",
      "Epoch [2/2], Step [51650/64305], Loss: 4.8233\n",
      "Epoch [2/2], Step [51660/64305], Loss: 4.6956\n",
      "Epoch [2/2], Step [51670/64305], Loss: 4.5116\n",
      "Epoch [2/2], Step [51680/64305], Loss: 4.8092\n",
      "Epoch [2/2], Step [51690/64305], Loss: 4.7880\n",
      "Epoch [2/2], Step [51700/64305], Loss: 4.7736\n",
      "Epoch [2/2], Step [51710/64305], Loss: 4.7990\n",
      "Epoch [2/2], Step [51720/64305], Loss: 4.7173\n",
      "Epoch [2/2], Step [51730/64305], Loss: 4.8648\n",
      "Epoch [2/2], Step [51740/64305], Loss: 4.6330\n",
      "Epoch [2/2], Step [51750/64305], Loss: 4.6682\n",
      "Epoch [2/2], Step [51760/64305], Loss: 4.6930\n",
      "Epoch [2/2], Step [51770/64305], Loss: 4.5555\n",
      "Epoch [2/2], Step [51780/64305], Loss: 4.6528\n",
      "Epoch [2/2], Step [51790/64305], Loss: 4.7120\n",
      "Epoch [2/2], Step [51800/64305], Loss: 4.7330\n",
      "Epoch [2/2], Step [51810/64305], Loss: 4.7162\n",
      "Epoch [2/2], Step [51820/64305], Loss: 4.7232\n",
      "Epoch [2/2], Step [51830/64305], Loss: 4.7592\n",
      "Epoch [2/2], Step [51840/64305], Loss: 4.9579\n",
      "Epoch [2/2], Step [51850/64305], Loss: 4.7444\n",
      "Epoch [2/2], Step [51860/64305], Loss: 4.7949\n",
      "Epoch [2/2], Step [51870/64305], Loss: 4.5885\n",
      "Epoch [2/2], Step [51880/64305], Loss: 4.7705\n",
      "Epoch [2/2], Step [51890/64305], Loss: 4.7103\n",
      "Epoch [2/2], Step [51900/64305], Loss: 4.4508\n",
      "Epoch [2/2], Step [51910/64305], Loss: 4.6143\n",
      "Epoch [2/2], Step [51920/64305], Loss: 4.7295\n",
      "Epoch [2/2], Step [51930/64305], Loss: 4.6782\n",
      "Epoch [2/2], Step [51940/64305], Loss: 4.5653\n",
      "Epoch [2/2], Step [51950/64305], Loss: 4.6112\n",
      "Epoch [2/2], Step [51960/64305], Loss: 4.7178\n",
      "Epoch [2/2], Step [51970/64305], Loss: 4.9812\n",
      "Epoch [2/2], Step [51980/64305], Loss: 4.5825\n",
      "Epoch [2/2], Step [51990/64305], Loss: 4.8528\n",
      "Epoch [2/2], Step [52000/64305], Loss: 4.7000\n",
      "Epoch [2/2], Step [52010/64305], Loss: 4.6952\n",
      "Epoch [2/2], Step [52020/64305], Loss: 4.6093\n",
      "Epoch [2/2], Step [52030/64305], Loss: 4.7362\n",
      "Epoch [2/2], Step [52040/64305], Loss: 4.7006\n",
      "Epoch [2/2], Step [52050/64305], Loss: 4.5970\n",
      "Epoch [2/2], Step [52060/64305], Loss: 4.7985\n",
      "Epoch [2/2], Step [52070/64305], Loss: 4.8320\n",
      "Epoch [2/2], Step [52080/64305], Loss: 4.9042\n",
      "Epoch [2/2], Step [52090/64305], Loss: 4.7744\n",
      "Epoch [2/2], Step [52100/64305], Loss: 4.8124\n",
      "Epoch [2/2], Step [52110/64305], Loss: 4.7904\n",
      "Epoch [2/2], Step [52120/64305], Loss: 4.9784\n",
      "Epoch [2/2], Step [52130/64305], Loss: 4.7423\n",
      "Epoch [2/2], Step [52140/64305], Loss: 4.5474\n",
      "Epoch [2/2], Step [52150/64305], Loss: 4.6319\n",
      "Epoch [2/2], Step [52160/64305], Loss: 4.7598\n",
      "Epoch [2/2], Step [52170/64305], Loss: 4.7781\n",
      "Epoch [2/2], Step [52180/64305], Loss: 4.6692\n",
      "Epoch [2/2], Step [52190/64305], Loss: 4.7658\n",
      "Epoch [2/2], Step [52200/64305], Loss: 4.6361\n",
      "Epoch [2/2], Step [52210/64305], Loss: 4.6727\n",
      "Epoch [2/2], Step [52220/64305], Loss: 4.6229\n",
      "Epoch [2/2], Step [52230/64305], Loss: 4.6313\n",
      "Epoch [2/2], Step [52240/64305], Loss: 4.7465\n",
      "Epoch [2/2], Step [52250/64305], Loss: 4.7691\n",
      "Epoch [2/2], Step [52260/64305], Loss: 4.8892\n",
      "Epoch [2/2], Step [52270/64305], Loss: 4.9140\n",
      "Epoch [2/2], Step [52280/64305], Loss: 4.7206\n",
      "Epoch [2/2], Step [52290/64305], Loss: 4.7515\n",
      "Epoch [2/2], Step [52300/64305], Loss: 4.7662\n",
      "Epoch [2/2], Step [52310/64305], Loss: 4.7113\n",
      "Epoch [2/2], Step [52320/64305], Loss: 4.7552\n",
      "Epoch [2/2], Step [52330/64305], Loss: 4.7654\n",
      "Epoch [2/2], Step [52340/64305], Loss: 4.6140\n",
      "Epoch [2/2], Step [52350/64305], Loss: 4.8945\n",
      "Epoch [2/2], Step [52360/64305], Loss: 4.6505\n",
      "Epoch [2/2], Step [52370/64305], Loss: 4.5853\n",
      "Epoch [2/2], Step [52380/64305], Loss: 4.7877\n",
      "Epoch [2/2], Step [52390/64305], Loss: 4.8140\n",
      "Epoch [2/2], Step [52400/64305], Loss: 4.6330\n",
      "Epoch [2/2], Step [52410/64305], Loss: 4.6029\n",
      "Epoch [2/2], Step [52420/64305], Loss: 4.6199\n",
      "Epoch [2/2], Step [52430/64305], Loss: 4.6781\n",
      "Epoch [2/2], Step [52440/64305], Loss: 4.7678\n",
      "Epoch [2/2], Step [52450/64305], Loss: 4.7306\n",
      "Epoch [2/2], Step [52460/64305], Loss: 4.7163\n",
      "Epoch [2/2], Step [52470/64305], Loss: 4.5821\n",
      "Epoch [2/2], Step [52480/64305], Loss: 4.9558\n",
      "Epoch [2/2], Step [52490/64305], Loss: 4.6499\n",
      "Epoch [2/2], Step [52500/64305], Loss: 4.7797\n",
      "Epoch [2/2], Step [52510/64305], Loss: 4.7668\n",
      "Epoch [2/2], Step [52520/64305], Loss: 4.7722\n",
      "Epoch [2/2], Step [52530/64305], Loss: 4.6164\n",
      "Epoch [2/2], Step [52540/64305], Loss: 4.6093\n",
      "Epoch [2/2], Step [52550/64305], Loss: 4.5668\n",
      "Epoch [2/2], Step [52560/64305], Loss: 4.8621\n",
      "Epoch [2/2], Step [52570/64305], Loss: 4.6687\n",
      "Epoch [2/2], Step [52580/64305], Loss: 4.7084\n",
      "Epoch [2/2], Step [52590/64305], Loss: 4.7728\n",
      "Epoch [2/2], Step [52600/64305], Loss: 4.9291\n",
      "Epoch [2/2], Step [52610/64305], Loss: 4.6451\n",
      "Epoch [2/2], Step [52620/64305], Loss: 4.8451\n",
      "Epoch [2/2], Step [52630/64305], Loss: 4.8709\n",
      "Epoch [2/2], Step [52640/64305], Loss: 4.6954\n",
      "Epoch [2/2], Step [52650/64305], Loss: 4.7394\n",
      "Epoch [2/2], Step [52660/64305], Loss: 4.7581\n",
      "Epoch [2/2], Step [52670/64305], Loss: 4.5340\n",
      "Epoch [2/2], Step [52680/64305], Loss: 4.8225\n",
      "Epoch [2/2], Step [52690/64305], Loss: 4.9225\n",
      "Epoch [2/2], Step [52700/64305], Loss: 4.6001\n",
      "Epoch [2/2], Step [52710/64305], Loss: 4.6528\n",
      "Epoch [2/2], Step [52720/64305], Loss: 4.7144\n",
      "Epoch [2/2], Step [52730/64305], Loss: 4.7303\n",
      "Epoch [2/2], Step [52740/64305], Loss: 4.7141\n",
      "Epoch [2/2], Step [52750/64305], Loss: 4.4908\n",
      "Epoch [2/2], Step [52760/64305], Loss: 4.7104\n",
      "Epoch [2/2], Step [52770/64305], Loss: 4.5902\n",
      "Epoch [2/2], Step [52780/64305], Loss: 4.7399\n",
      "Epoch [2/2], Step [52790/64305], Loss: 4.7045\n",
      "Epoch [2/2], Step [52800/64305], Loss: 4.7995\n",
      "Epoch [2/2], Step [52810/64305], Loss: 4.6259\n",
      "Epoch [2/2], Step [52820/64305], Loss: 4.6236\n",
      "Epoch [2/2], Step [52830/64305], Loss: 4.6225\n",
      "Epoch [2/2], Step [52840/64305], Loss: 4.8136\n",
      "Epoch [2/2], Step [52850/64305], Loss: 4.9617\n",
      "Epoch [2/2], Step [52860/64305], Loss: 4.5603\n",
      "Epoch [2/2], Step [52870/64305], Loss: 4.9341\n",
      "Epoch [2/2], Step [52880/64305], Loss: 4.4927\n",
      "Epoch [2/2], Step [52890/64305], Loss: 4.7382\n",
      "Epoch [2/2], Step [52900/64305], Loss: 4.7188\n",
      "Epoch [2/2], Step [52910/64305], Loss: 4.7023\n",
      "Epoch [2/2], Step [52920/64305], Loss: 4.7368\n",
      "Epoch [2/2], Step [52930/64305], Loss: 4.7126\n",
      "Epoch [2/2], Step [52940/64305], Loss: 4.7458\n",
      "Epoch [2/2], Step [52950/64305], Loss: 4.5773\n",
      "Epoch [2/2], Step [52960/64305], Loss: 4.7830\n",
      "Epoch [2/2], Step [52970/64305], Loss: 5.0790\n",
      "Epoch [2/2], Step [52980/64305], Loss: 4.6857\n",
      "Epoch [2/2], Step [52990/64305], Loss: 4.8043\n",
      "Epoch [2/2], Step [53000/64305], Loss: 4.5812\n",
      "Epoch [2/2], Step [53010/64305], Loss: 4.6871\n",
      "Epoch [2/2], Step [53020/64305], Loss: 4.6518\n",
      "Epoch [2/2], Step [53030/64305], Loss: 4.7284\n",
      "Epoch [2/2], Step [53040/64305], Loss: 4.7397\n",
      "Epoch [2/2], Step [53050/64305], Loss: 4.9913\n",
      "Epoch [2/2], Step [53060/64305], Loss: 4.8431\n",
      "Epoch [2/2], Step [53070/64305], Loss: 4.6004\n",
      "Epoch [2/2], Step [53080/64305], Loss: 4.7728\n",
      "Epoch [2/2], Step [53090/64305], Loss: 4.5485\n",
      "Epoch [2/2], Step [53100/64305], Loss: 4.6753\n",
      "Epoch [2/2], Step [53110/64305], Loss: 4.6553\n",
      "Epoch [2/2], Step [53120/64305], Loss: 4.7190\n",
      "Epoch [2/2], Step [53130/64305], Loss: 4.8016\n",
      "Epoch [2/2], Step [53140/64305], Loss: 4.8746\n",
      "Epoch [2/2], Step [53150/64305], Loss: 4.9000\n",
      "Epoch [2/2], Step [53160/64305], Loss: 4.5700\n",
      "Epoch [2/2], Step [53170/64305], Loss: 4.7495\n",
      "Epoch [2/2], Step [53180/64305], Loss: 4.6619\n",
      "Epoch [2/2], Step [53190/64305], Loss: 4.5625\n",
      "Epoch [2/2], Step [53200/64305], Loss: 4.6199\n",
      "Epoch [2/2], Step [53210/64305], Loss: 4.8461\n",
      "Epoch [2/2], Step [53220/64305], Loss: 4.8311\n",
      "Epoch [2/2], Step [53230/64305], Loss: 4.7512\n",
      "Epoch [2/2], Step [53240/64305], Loss: 4.9467\n",
      "Epoch [2/2], Step [53250/64305], Loss: 4.7885\n",
      "Epoch [2/2], Step [53260/64305], Loss: 4.5966\n",
      "Epoch [2/2], Step [53270/64305], Loss: 4.6433\n",
      "Epoch [2/2], Step [53280/64305], Loss: 4.6881\n",
      "Epoch [2/2], Step [53290/64305], Loss: 4.8196\n",
      "Epoch [2/2], Step [53300/64305], Loss: 4.8211\n",
      "Epoch [2/2], Step [53310/64305], Loss: 4.7323\n",
      "Epoch [2/2], Step [53320/64305], Loss: 4.6163\n",
      "Epoch [2/2], Step [53330/64305], Loss: 4.5975\n",
      "Epoch [2/2], Step [53340/64305], Loss: 4.5465\n",
      "Epoch [2/2], Step [53350/64305], Loss: 4.6938\n",
      "Epoch [2/2], Step [53360/64305], Loss: 4.8232\n",
      "Epoch [2/2], Step [53370/64305], Loss: 4.7763\n",
      "Epoch [2/2], Step [53380/64305], Loss: 4.6824\n",
      "Epoch [2/2], Step [53390/64305], Loss: 4.7528\n",
      "Epoch [2/2], Step [53400/64305], Loss: 4.6680\n",
      "Epoch [2/2], Step [53410/64305], Loss: 4.9334\n",
      "Epoch [2/2], Step [53420/64305], Loss: 4.8139\n",
      "Epoch [2/2], Step [53430/64305], Loss: 4.7162\n",
      "Epoch [2/2], Step [53440/64305], Loss: 4.6995\n",
      "Epoch [2/2], Step [53450/64305], Loss: 4.5527\n",
      "Epoch [2/2], Step [53460/64305], Loss: 4.7134\n",
      "Epoch [2/2], Step [53470/64305], Loss: 4.6836\n",
      "Epoch [2/2], Step [53480/64305], Loss: 4.7325\n",
      "Epoch [2/2], Step [53490/64305], Loss: 4.8130\n",
      "Epoch [2/2], Step [53500/64305], Loss: 4.5710\n",
      "Epoch [2/2], Step [53510/64305], Loss: 4.7822\n",
      "Epoch [2/2], Step [53520/64305], Loss: 4.6971\n",
      "Epoch [2/2], Step [53530/64305], Loss: 4.7221\n",
      "Epoch [2/2], Step [53540/64305], Loss: 4.5718\n",
      "Epoch [2/2], Step [53550/64305], Loss: 4.6460\n",
      "Epoch [2/2], Step [53560/64305], Loss: 4.7864\n",
      "Epoch [2/2], Step [53570/64305], Loss: 4.5312\n",
      "Epoch [2/2], Step [53580/64305], Loss: 4.7214\n",
      "Epoch [2/2], Step [53590/64305], Loss: 4.7621\n",
      "Epoch [2/2], Step [53600/64305], Loss: 4.6939\n",
      "Epoch [2/2], Step [53610/64305], Loss: 4.8515\n",
      "Epoch [2/2], Step [53620/64305], Loss: 4.7376\n",
      "Epoch [2/2], Step [53630/64305], Loss: 4.7722\n",
      "Epoch [2/2], Step [53640/64305], Loss: 4.6530\n",
      "Epoch [2/2], Step [53650/64305], Loss: 4.6874\n",
      "Epoch [2/2], Step [53660/64305], Loss: 4.8071\n",
      "Epoch [2/2], Step [53670/64305], Loss: 4.5793\n",
      "Epoch [2/2], Step [53680/64305], Loss: 4.7547\n",
      "Epoch [2/2], Step [53690/64305], Loss: 4.9456\n",
      "Epoch [2/2], Step [53700/64305], Loss: 4.7855\n",
      "Epoch [2/2], Step [53710/64305], Loss: 4.7013\n",
      "Epoch [2/2], Step [53720/64305], Loss: 4.6863\n",
      "Epoch [2/2], Step [53730/64305], Loss: 4.8208\n",
      "Epoch [2/2], Step [53740/64305], Loss: 4.6626\n",
      "Epoch [2/2], Step [53750/64305], Loss: 4.8651\n",
      "Epoch [2/2], Step [53760/64305], Loss: 4.7211\n",
      "Epoch [2/2], Step [53770/64305], Loss: 4.6968\n",
      "Epoch [2/2], Step [53780/64305], Loss: 4.7614\n",
      "Epoch [2/2], Step [53790/64305], Loss: 4.6758\n",
      "Epoch [2/2], Step [53800/64305], Loss: 4.9828\n",
      "Epoch [2/2], Step [53810/64305], Loss: 4.6834\n",
      "Epoch [2/2], Step [53820/64305], Loss: 4.7114\n",
      "Epoch [2/2], Step [53830/64305], Loss: 4.8065\n",
      "Epoch [2/2], Step [53840/64305], Loss: 4.4935\n",
      "Epoch [2/2], Step [53850/64305], Loss: 4.6757\n",
      "Epoch [2/2], Step [53860/64305], Loss: 4.8094\n",
      "Epoch [2/2], Step [53870/64305], Loss: 4.6925\n",
      "Epoch [2/2], Step [53880/64305], Loss: 4.8543\n",
      "Epoch [2/2], Step [53890/64305], Loss: 5.0628\n",
      "Epoch [2/2], Step [53900/64305], Loss: 4.9601\n",
      "Epoch [2/2], Step [53910/64305], Loss: 4.6109\n",
      "Epoch [2/2], Step [53920/64305], Loss: 4.6046\n",
      "Epoch [2/2], Step [53930/64305], Loss: 4.6166\n",
      "Epoch [2/2], Step [53940/64305], Loss: 4.6801\n",
      "Epoch [2/2], Step [53950/64305], Loss: 4.8249\n",
      "Epoch [2/2], Step [53960/64305], Loss: 4.7604\n",
      "Epoch [2/2], Step [53970/64305], Loss: 4.7714\n",
      "Epoch [2/2], Step [53980/64305], Loss: 4.5792\n",
      "Epoch [2/2], Step [53990/64305], Loss: 4.7988\n",
      "Epoch [2/2], Step [54000/64305], Loss: 4.6337\n",
      "Epoch [2/2], Step [54010/64305], Loss: 4.6079\n",
      "Epoch [2/2], Step [54020/64305], Loss: 4.8688\n",
      "Epoch [2/2], Step [54030/64305], Loss: 4.6540\n",
      "Epoch [2/2], Step [54040/64305], Loss: 4.6468\n",
      "Epoch [2/2], Step [54050/64305], Loss: 4.7137\n",
      "Epoch [2/2], Step [54060/64305], Loss: 4.6487\n",
      "Epoch [2/2], Step [54070/64305], Loss: 4.6568\n",
      "Epoch [2/2], Step [54080/64305], Loss: 4.7312\n",
      "Epoch [2/2], Step [54090/64305], Loss: 4.6467\n",
      "Epoch [2/2], Step [54100/64305], Loss: 4.7431\n",
      "Epoch [2/2], Step [54110/64305], Loss: 4.6399\n",
      "Epoch [2/2], Step [54120/64305], Loss: 4.6137\n",
      "Epoch [2/2], Step [54130/64305], Loss: 4.6180\n",
      "Epoch [2/2], Step [54140/64305], Loss: 4.8615\n",
      "Epoch [2/2], Step [54150/64305], Loss: 4.7051\n",
      "Epoch [2/2], Step [54160/64305], Loss: 4.7017\n",
      "Epoch [2/2], Step [54170/64305], Loss: 4.7342\n",
      "Epoch [2/2], Step [54180/64305], Loss: 4.8696\n",
      "Epoch [2/2], Step [54190/64305], Loss: 4.6792\n",
      "Epoch [2/2], Step [54200/64305], Loss: 4.8536\n",
      "Epoch [2/2], Step [54210/64305], Loss: 4.6438\n",
      "Epoch [2/2], Step [54220/64305], Loss: 4.6548\n",
      "Epoch [2/2], Step [54230/64305], Loss: 4.5456\n",
      "Epoch [2/2], Step [54240/64305], Loss: 4.7273\n",
      "Epoch [2/2], Step [54250/64305], Loss: 4.7729\n",
      "Epoch [2/2], Step [54260/64305], Loss: 4.5780\n",
      "Epoch [2/2], Step [54270/64305], Loss: 4.6375\n",
      "Epoch [2/2], Step [54280/64305], Loss: 4.7789\n",
      "Epoch [2/2], Step [54290/64305], Loss: 4.6601\n",
      "Epoch [2/2], Step [54300/64305], Loss: 4.9127\n",
      "Epoch [2/2], Step [54310/64305], Loss: 4.6297\n",
      "Epoch [2/2], Step [54320/64305], Loss: 4.5072\n",
      "Epoch [2/2], Step [54330/64305], Loss: 4.8708\n",
      "Epoch [2/2], Step [54340/64305], Loss: 4.6807\n",
      "Epoch [2/2], Step [54350/64305], Loss: 4.6679\n",
      "Epoch [2/2], Step [54360/64305], Loss: 4.7552\n",
      "Epoch [2/2], Step [54370/64305], Loss: 4.7423\n",
      "Epoch [2/2], Step [54380/64305], Loss: 4.5777\n",
      "Epoch [2/2], Step [54390/64305], Loss: 4.5386\n",
      "Epoch [2/2], Step [54400/64305], Loss: 4.6400\n",
      "Epoch [2/2], Step [54410/64305], Loss: 4.8075\n",
      "Epoch [2/2], Step [54420/64305], Loss: 4.7539\n",
      "Epoch [2/2], Step [54430/64305], Loss: 4.7813\n",
      "Epoch [2/2], Step [54440/64305], Loss: 4.5218\n",
      "Epoch [2/2], Step [54450/64305], Loss: 4.4979\n",
      "Epoch [2/2], Step [54460/64305], Loss: 4.7058\n",
      "Epoch [2/2], Step [54470/64305], Loss: 4.6882\n",
      "Epoch [2/2], Step [54480/64305], Loss: 4.6057\n",
      "Epoch [2/2], Step [54490/64305], Loss: 4.6273\n",
      "Epoch [2/2], Step [54500/64305], Loss: 4.8647\n",
      "Epoch [2/2], Step [54510/64305], Loss: 4.7765\n",
      "Epoch [2/2], Step [54520/64305], Loss: 4.6788\n",
      "Epoch [2/2], Step [54530/64305], Loss: 4.5835\n",
      "Epoch [2/2], Step [54540/64305], Loss: 4.5837\n",
      "Epoch [2/2], Step [54550/64305], Loss: 4.7463\n",
      "Epoch [2/2], Step [54560/64305], Loss: 4.6234\n",
      "Epoch [2/2], Step [54570/64305], Loss: 4.8099\n",
      "Epoch [2/2], Step [54580/64305], Loss: 4.9075\n",
      "Epoch [2/2], Step [54590/64305], Loss: 4.6909\n",
      "Epoch [2/2], Step [54600/64305], Loss: 4.7098\n",
      "Epoch [2/2], Step [54610/64305], Loss: 4.7678\n",
      "Epoch [2/2], Step [54620/64305], Loss: 4.6825\n",
      "Epoch [2/2], Step [54630/64305], Loss: 4.7006\n",
      "Epoch [2/2], Step [54640/64305], Loss: 4.6119\n",
      "Epoch [2/2], Step [54650/64305], Loss: 4.6891\n",
      "Epoch [2/2], Step [54660/64305], Loss: 4.7682\n",
      "Epoch [2/2], Step [54670/64305], Loss: 4.5527\n",
      "Epoch [2/2], Step [54680/64305], Loss: 4.6278\n",
      "Epoch [2/2], Step [54690/64305], Loss: 4.8925\n",
      "Epoch [2/2], Step [54700/64305], Loss: 4.8570\n",
      "Epoch [2/2], Step [54710/64305], Loss: 4.7226\n",
      "Epoch [2/2], Step [54720/64305], Loss: 4.6558\n",
      "Epoch [2/2], Step [54730/64305], Loss: 4.6871\n",
      "Epoch [2/2], Step [54740/64305], Loss: 5.0111\n",
      "Epoch [2/2], Step [54750/64305], Loss: 4.7001\n",
      "Epoch [2/2], Step [54760/64305], Loss: 4.9646\n",
      "Epoch [2/2], Step [54770/64305], Loss: 4.7662\n",
      "Epoch [2/2], Step [54780/64305], Loss: 4.5896\n",
      "Epoch [2/2], Step [54790/64305], Loss: 4.7244\n",
      "Epoch [2/2], Step [54800/64305], Loss: 4.7396\n",
      "Epoch [2/2], Step [54810/64305], Loss: 4.7852\n",
      "Epoch [2/2], Step [54820/64305], Loss: 4.6719\n",
      "Epoch [2/2], Step [54830/64305], Loss: 4.7999\n",
      "Epoch [2/2], Step [54840/64305], Loss: 4.7328\n",
      "Epoch [2/2], Step [54850/64305], Loss: 4.6648\n",
      "Epoch [2/2], Step [54860/64305], Loss: 4.5724\n",
      "Epoch [2/2], Step [54870/64305], Loss: 4.7723\n",
      "Epoch [2/2], Step [54880/64305], Loss: 4.5940\n",
      "Epoch [2/2], Step [54890/64305], Loss: 4.6917\n",
      "Epoch [2/2], Step [54900/64305], Loss: 4.6946\n",
      "Epoch [2/2], Step [54910/64305], Loss: 4.7267\n",
      "Epoch [2/2], Step [54920/64305], Loss: 4.7819\n",
      "Epoch [2/2], Step [54930/64305], Loss: 4.6919\n",
      "Epoch [2/2], Step [54940/64305], Loss: 4.7292\n",
      "Epoch [2/2], Step [54950/64305], Loss: 4.6368\n",
      "Epoch [2/2], Step [54960/64305], Loss: 4.7444\n",
      "Epoch [2/2], Step [54970/64305], Loss: 4.7655\n",
      "Epoch [2/2], Step [54980/64305], Loss: 4.7063\n",
      "Epoch [2/2], Step [54990/64305], Loss: 4.6154\n",
      "Epoch [2/2], Step [55000/64305], Loss: 4.8621\n",
      "Epoch [2/2], Step [55010/64305], Loss: 4.6110\n",
      "Epoch [2/2], Step [55020/64305], Loss: 4.8868\n",
      "Epoch [2/2], Step [55030/64305], Loss: 4.8460\n",
      "Epoch [2/2], Step [55040/64305], Loss: 4.7964\n",
      "Epoch [2/2], Step [55050/64305], Loss: 4.5518\n",
      "Epoch [2/2], Step [55060/64305], Loss: 4.7582\n",
      "Epoch [2/2], Step [55070/64305], Loss: 4.7259\n",
      "Epoch [2/2], Step [55080/64305], Loss: 4.6710\n",
      "Epoch [2/2], Step [55090/64305], Loss: 4.6053\n",
      "Epoch [2/2], Step [55100/64305], Loss: 4.8401\n",
      "Epoch [2/2], Step [55110/64305], Loss: 4.6295\n",
      "Epoch [2/2], Step [55120/64305], Loss: 4.7367\n",
      "Epoch [2/2], Step [55130/64305], Loss: 4.6017\n",
      "Epoch [2/2], Step [55140/64305], Loss: 4.9381\n",
      "Epoch [2/2], Step [55150/64305], Loss: 4.5901\n",
      "Epoch [2/2], Step [55160/64305], Loss: 4.6049\n",
      "Epoch [2/2], Step [55170/64305], Loss: 4.7135\n",
      "Epoch [2/2], Step [55180/64305], Loss: 4.7208\n",
      "Epoch [2/2], Step [55190/64305], Loss: 4.6100\n",
      "Epoch [2/2], Step [55200/64305], Loss: 4.6324\n",
      "Epoch [2/2], Step [55210/64305], Loss: 4.7653\n",
      "Epoch [2/2], Step [55220/64305], Loss: 4.6548\n",
      "Epoch [2/2], Step [55230/64305], Loss: 4.7330\n",
      "Epoch [2/2], Step [55240/64305], Loss: 4.6310\n",
      "Epoch [2/2], Step [55250/64305], Loss: 4.7768\n",
      "Epoch [2/2], Step [55260/64305], Loss: 4.5724\n",
      "Epoch [2/2], Step [55270/64305], Loss: 4.6515\n",
      "Epoch [2/2], Step [55280/64305], Loss: 4.7147\n",
      "Epoch [2/2], Step [55290/64305], Loss: 4.6437\n",
      "Epoch [2/2], Step [55300/64305], Loss: 4.9323\n",
      "Epoch [2/2], Step [55310/64305], Loss: 4.6179\n",
      "Epoch [2/2], Step [55320/64305], Loss: 4.8281\n",
      "Epoch [2/2], Step [55330/64305], Loss: 4.5430\n",
      "Epoch [2/2], Step [55340/64305], Loss: 4.9180\n",
      "Epoch [2/2], Step [55350/64305], Loss: 4.7083\n",
      "Epoch [2/2], Step [55360/64305], Loss: 4.6246\n",
      "Epoch [2/2], Step [55370/64305], Loss: 4.6021\n",
      "Epoch [2/2], Step [55380/64305], Loss: 4.7580\n",
      "Epoch [2/2], Step [55390/64305], Loss: 4.5112\n",
      "Epoch [2/2], Step [55400/64305], Loss: 4.5619\n",
      "Epoch [2/2], Step [55410/64305], Loss: 4.6511\n",
      "Epoch [2/2], Step [55420/64305], Loss: 4.5747\n",
      "Epoch [2/2], Step [55430/64305], Loss: 4.8679\n",
      "Epoch [2/2], Step [55440/64305], Loss: 4.6722\n",
      "Epoch [2/2], Step [55450/64305], Loss: 4.8385\n",
      "Epoch [2/2], Step [55460/64305], Loss: 4.5820\n",
      "Epoch [2/2], Step [55470/64305], Loss: 4.6385\n",
      "Epoch [2/2], Step [55480/64305], Loss: 4.7764\n",
      "Epoch [2/2], Step [55490/64305], Loss: 4.5856\n",
      "Epoch [2/2], Step [55500/64305], Loss: 4.6961\n",
      "Epoch [2/2], Step [55510/64305], Loss: 4.6342\n",
      "Epoch [2/2], Step [55520/64305], Loss: 4.7053\n",
      "Epoch [2/2], Step [55530/64305], Loss: 4.8233\n",
      "Epoch [2/2], Step [55540/64305], Loss: 4.7582\n",
      "Epoch [2/2], Step [55550/64305], Loss: 4.8381\n",
      "Epoch [2/2], Step [55560/64305], Loss: 4.8445\n",
      "Epoch [2/2], Step [55570/64305], Loss: 4.7086\n",
      "Epoch [2/2], Step [55580/64305], Loss: 4.7446\n",
      "Epoch [2/2], Step [55590/64305], Loss: 4.8550\n",
      "Epoch [2/2], Step [55600/64305], Loss: 4.8179\n",
      "Epoch [2/2], Step [55610/64305], Loss: 4.5895\n",
      "Epoch [2/2], Step [55620/64305], Loss: 4.7211\n",
      "Epoch [2/2], Step [55630/64305], Loss: 4.7864\n",
      "Epoch [2/2], Step [55640/64305], Loss: 4.6807\n",
      "Epoch [2/2], Step [55650/64305], Loss: 4.6505\n",
      "Epoch [2/2], Step [55660/64305], Loss: 4.6751\n",
      "Epoch [2/2], Step [55670/64305], Loss: 4.8093\n",
      "Epoch [2/2], Step [55680/64305], Loss: 4.7510\n",
      "Epoch [2/2], Step [55690/64305], Loss: 4.6589\n",
      "Epoch [2/2], Step [55700/64305], Loss: 4.8033\n",
      "Epoch [2/2], Step [55710/64305], Loss: 4.7670\n",
      "Epoch [2/2], Step [55720/64305], Loss: 4.6788\n",
      "Epoch [2/2], Step [55730/64305], Loss: 4.7198\n",
      "Epoch [2/2], Step [55740/64305], Loss: 4.8205\n",
      "Epoch [2/2], Step [55750/64305], Loss: 4.5807\n",
      "Epoch [2/2], Step [55760/64305], Loss: 4.8322\n",
      "Epoch [2/2], Step [55770/64305], Loss: 4.6117\n",
      "Epoch [2/2], Step [55780/64305], Loss: 4.7304\n",
      "Epoch [2/2], Step [55790/64305], Loss: 4.7684\n",
      "Epoch [2/2], Step [55800/64305], Loss: 4.4437\n",
      "Epoch [2/2], Step [55810/64305], Loss: 4.7144\n",
      "Epoch [2/2], Step [55820/64305], Loss: 4.7579\n",
      "Epoch [2/2], Step [55830/64305], Loss: 4.6347\n",
      "Epoch [2/2], Step [55840/64305], Loss: 4.7592\n",
      "Epoch [2/2], Step [55850/64305], Loss: 4.7555\n",
      "Epoch [2/2], Step [55860/64305], Loss: 4.6725\n",
      "Epoch [2/2], Step [55870/64305], Loss: 4.6869\n",
      "Epoch [2/2], Step [55880/64305], Loss: 4.6374\n",
      "Epoch [2/2], Step [55890/64305], Loss: 4.7754\n",
      "Epoch [2/2], Step [55900/64305], Loss: 4.8572\n",
      "Epoch [2/2], Step [55910/64305], Loss: 5.0571\n",
      "Epoch [2/2], Step [55920/64305], Loss: 4.6797\n",
      "Epoch [2/2], Step [55930/64305], Loss: 4.8392\n",
      "Epoch [2/2], Step [55940/64305], Loss: 5.0304\n",
      "Epoch [2/2], Step [55950/64305], Loss: 4.7928\n",
      "Epoch [2/2], Step [55960/64305], Loss: 4.7546\n",
      "Epoch [2/2], Step [55970/64305], Loss: 4.6981\n",
      "Epoch [2/2], Step [55980/64305], Loss: 4.6342\n",
      "Epoch [2/2], Step [55990/64305], Loss: 4.8891\n",
      "Epoch [2/2], Step [56000/64305], Loss: 4.5118\n",
      "Epoch [2/2], Step [56010/64305], Loss: 4.6963\n",
      "Epoch [2/2], Step [56020/64305], Loss: 4.7139\n",
      "Epoch [2/2], Step [56030/64305], Loss: 4.6843\n",
      "Epoch [2/2], Step [56040/64305], Loss: 4.7741\n",
      "Epoch [2/2], Step [56050/64305], Loss: 4.8529\n",
      "Epoch [2/2], Step [56060/64305], Loss: 4.8425\n",
      "Epoch [2/2], Step [56070/64305], Loss: 4.6941\n",
      "Epoch [2/2], Step [56080/64305], Loss: 4.5866\n",
      "Epoch [2/2], Step [56090/64305], Loss: 4.8291\n",
      "Epoch [2/2], Step [56100/64305], Loss: 4.6867\n",
      "Epoch [2/2], Step [56110/64305], Loss: 4.6999\n",
      "Epoch [2/2], Step [56120/64305], Loss: 4.7442\n",
      "Epoch [2/2], Step [56130/64305], Loss: 4.8740\n",
      "Epoch [2/2], Step [56140/64305], Loss: 4.6239\n",
      "Epoch [2/2], Step [56150/64305], Loss: 4.6619\n",
      "Epoch [2/2], Step [56160/64305], Loss: 4.8302\n",
      "Epoch [2/2], Step [56170/64305], Loss: 4.6791\n",
      "Epoch [2/2], Step [56180/64305], Loss: 4.7367\n",
      "Epoch [2/2], Step [56190/64305], Loss: 4.7849\n",
      "Epoch [2/2], Step [56200/64305], Loss: 4.5184\n",
      "Epoch [2/2], Step [56210/64305], Loss: 4.8727\n",
      "Epoch [2/2], Step [56220/64305], Loss: 4.6736\n",
      "Epoch [2/2], Step [56230/64305], Loss: 4.7865\n",
      "Epoch [2/2], Step [56240/64305], Loss: 4.6904\n",
      "Epoch [2/2], Step [56250/64305], Loss: 4.7212\n",
      "Epoch [2/2], Step [56260/64305], Loss: 4.7701\n",
      "Epoch [2/2], Step [56270/64305], Loss: 4.8585\n",
      "Epoch [2/2], Step [56280/64305], Loss: 4.5356\n",
      "Epoch [2/2], Step [56290/64305], Loss: 4.8519\n",
      "Epoch [2/2], Step [56300/64305], Loss: 4.4596\n",
      "Epoch [2/2], Step [56310/64305], Loss: 4.7552\n",
      "Epoch [2/2], Step [56320/64305], Loss: 4.7219\n",
      "Epoch [2/2], Step [56330/64305], Loss: 4.6523\n",
      "Epoch [2/2], Step [56340/64305], Loss: 4.6658\n",
      "Epoch [2/2], Step [56350/64305], Loss: 4.7250\n",
      "Epoch [2/2], Step [56360/64305], Loss: 4.7187\n",
      "Epoch [2/2], Step [56370/64305], Loss: 4.6838\n",
      "Epoch [2/2], Step [56380/64305], Loss: 4.7487\n",
      "Epoch [2/2], Step [56390/64305], Loss: 4.7888\n",
      "Epoch [2/2], Step [56400/64305], Loss: 4.5754\n",
      "Epoch [2/2], Step [56410/64305], Loss: 4.7795\n",
      "Epoch [2/2], Step [56420/64305], Loss: 4.8762\n",
      "Epoch [2/2], Step [56430/64305], Loss: 4.7199\n",
      "Epoch [2/2], Step [56440/64305], Loss: 4.5550\n",
      "Epoch [2/2], Step [56450/64305], Loss: 4.6872\n",
      "Epoch [2/2], Step [56460/64305], Loss: 4.8918\n",
      "Epoch [2/2], Step [56470/64305], Loss: 4.6159\n",
      "Epoch [2/2], Step [56480/64305], Loss: 4.6931\n",
      "Epoch [2/2], Step [56490/64305], Loss: 4.6980\n",
      "Epoch [2/2], Step [56500/64305], Loss: 4.7236\n",
      "Epoch [2/2], Step [56510/64305], Loss: 4.5789\n",
      "Epoch [2/2], Step [56520/64305], Loss: 4.7616\n",
      "Epoch [2/2], Step [56530/64305], Loss: 4.8229\n",
      "Epoch [2/2], Step [56540/64305], Loss: 4.7051\n",
      "Epoch [2/2], Step [56550/64305], Loss: 4.7202\n",
      "Epoch [2/2], Step [56560/64305], Loss: 4.5898\n",
      "Epoch [2/2], Step [56570/64305], Loss: 4.6789\n",
      "Epoch [2/2], Step [56580/64305], Loss: 4.9270\n",
      "Epoch [2/2], Step [56590/64305], Loss: 4.8408\n",
      "Epoch [2/2], Step [56600/64305], Loss: 4.6589\n",
      "Epoch [2/2], Step [56610/64305], Loss: 4.7160\n",
      "Epoch [2/2], Step [56620/64305], Loss: 4.7898\n",
      "Epoch [2/2], Step [56630/64305], Loss: 4.6604\n",
      "Epoch [2/2], Step [56640/64305], Loss: 4.7338\n",
      "Epoch [2/2], Step [56650/64305], Loss: 4.6774\n",
      "Epoch [2/2], Step [56660/64305], Loss: 4.8692\n",
      "Epoch [2/2], Step [56670/64305], Loss: 4.6907\n",
      "Epoch [2/2], Step [56680/64305], Loss: 4.6108\n",
      "Epoch [2/2], Step [56690/64305], Loss: 4.6758\n",
      "Epoch [2/2], Step [56700/64305], Loss: 4.7979\n",
      "Epoch [2/2], Step [56710/64305], Loss: 4.8017\n",
      "Epoch [2/2], Step [56720/64305], Loss: 4.6169\n",
      "Epoch [2/2], Step [56730/64305], Loss: 4.6724\n",
      "Epoch [2/2], Step [56740/64305], Loss: 4.8054\n",
      "Epoch [2/2], Step [56750/64305], Loss: 4.8333\n",
      "Epoch [2/2], Step [56760/64305], Loss: 4.6589\n",
      "Epoch [2/2], Step [56770/64305], Loss: 4.7750\n",
      "Epoch [2/2], Step [56780/64305], Loss: 4.7188\n",
      "Epoch [2/2], Step [56790/64305], Loss: 4.8327\n",
      "Epoch [2/2], Step [56800/64305], Loss: 4.7093\n",
      "Epoch [2/2], Step [56810/64305], Loss: 4.7859\n",
      "Epoch [2/2], Step [56820/64305], Loss: 4.6631\n",
      "Epoch [2/2], Step [56830/64305], Loss: 4.7240\n",
      "Epoch [2/2], Step [56840/64305], Loss: 4.7917\n",
      "Epoch [2/2], Step [56850/64305], Loss: 4.5808\n",
      "Epoch [2/2], Step [56860/64305], Loss: 4.8072\n",
      "Epoch [2/2], Step [56870/64305], Loss: 4.9141\n",
      "Epoch [2/2], Step [56880/64305], Loss: 4.4949\n",
      "Epoch [2/2], Step [56890/64305], Loss: 4.9782\n",
      "Epoch [2/2], Step [56900/64305], Loss: 4.6234\n",
      "Epoch [2/2], Step [56910/64305], Loss: 4.6199\n",
      "Epoch [2/2], Step [56920/64305], Loss: 4.5352\n",
      "Epoch [2/2], Step [56930/64305], Loss: 4.8185\n",
      "Epoch [2/2], Step [56940/64305], Loss: 4.7151\n",
      "Epoch [2/2], Step [56950/64305], Loss: 4.7853\n",
      "Epoch [2/2], Step [56960/64305], Loss: 4.8133\n",
      "Epoch [2/2], Step [56970/64305], Loss: 4.7308\n",
      "Epoch [2/2], Step [56980/64305], Loss: 4.5478\n",
      "Epoch [2/2], Step [56990/64305], Loss: 4.5885\n",
      "Epoch [2/2], Step [57000/64305], Loss: 4.5057\n",
      "Epoch [2/2], Step [57010/64305], Loss: 4.5624\n",
      "Epoch [2/2], Step [57020/64305], Loss: 4.7187\n",
      "Epoch [2/2], Step [57030/64305], Loss: 4.8787\n",
      "Epoch [2/2], Step [57040/64305], Loss: 4.7262\n",
      "Epoch [2/2], Step [57050/64305], Loss: 4.7612\n",
      "Epoch [2/2], Step [57060/64305], Loss: 4.8856\n",
      "Epoch [2/2], Step [57070/64305], Loss: 4.6123\n",
      "Epoch [2/2], Step [57080/64305], Loss: 4.5110\n",
      "Epoch [2/2], Step [57090/64305], Loss: 4.6578\n",
      "Epoch [2/2], Step [57100/64305], Loss: 4.7755\n",
      "Epoch [2/2], Step [57110/64305], Loss: 4.7630\n",
      "Epoch [2/2], Step [57120/64305], Loss: 4.7264\n",
      "Epoch [2/2], Step [57130/64305], Loss: 4.8170\n",
      "Epoch [2/2], Step [57140/64305], Loss: 4.8258\n",
      "Epoch [2/2], Step [57150/64305], Loss: 4.7883\n",
      "Epoch [2/2], Step [57160/64305], Loss: 4.8785\n",
      "Epoch [2/2], Step [57170/64305], Loss: 4.5761\n",
      "Epoch [2/2], Step [57180/64305], Loss: 4.8940\n",
      "Epoch [2/2], Step [57190/64305], Loss: 4.7308\n",
      "Epoch [2/2], Step [57200/64305], Loss: 4.8710\n",
      "Epoch [2/2], Step [57210/64305], Loss: 4.5956\n",
      "Epoch [2/2], Step [57220/64305], Loss: 4.6379\n",
      "Epoch [2/2], Step [57230/64305], Loss: 4.7353\n",
      "Epoch [2/2], Step [57240/64305], Loss: 4.7556\n",
      "Epoch [2/2], Step [57250/64305], Loss: 4.7173\n",
      "Epoch [2/2], Step [57260/64305], Loss: 4.7690\n",
      "Epoch [2/2], Step [57270/64305], Loss: 4.6499\n",
      "Epoch [2/2], Step [57280/64305], Loss: 4.6471\n",
      "Epoch [2/2], Step [57290/64305], Loss: 4.5705\n",
      "Epoch [2/2], Step [57300/64305], Loss: 4.9045\n",
      "Epoch [2/2], Step [57310/64305], Loss: 4.6110\n",
      "Epoch [2/2], Step [57320/64305], Loss: 4.7369\n",
      "Epoch [2/2], Step [57330/64305], Loss: 4.7052\n",
      "Epoch [2/2], Step [57340/64305], Loss: 4.7106\n",
      "Epoch [2/2], Step [57350/64305], Loss: 4.6372\n",
      "Epoch [2/2], Step [57360/64305], Loss: 4.8180\n",
      "Epoch [2/2], Step [57370/64305], Loss: 4.4679\n",
      "Epoch [2/2], Step [57380/64305], Loss: 4.6455\n",
      "Epoch [2/2], Step [57390/64305], Loss: 4.7942\n",
      "Epoch [2/2], Step [57400/64305], Loss: 4.7103\n",
      "Epoch [2/2], Step [57410/64305], Loss: 4.8464\n",
      "Epoch [2/2], Step [57420/64305], Loss: 4.7954\n",
      "Epoch [2/2], Step [57430/64305], Loss: 4.7003\n",
      "Epoch [2/2], Step [57440/64305], Loss: 4.5456\n",
      "Epoch [2/2], Step [57450/64305], Loss: 4.5934\n",
      "Epoch [2/2], Step [57460/64305], Loss: 4.7054\n",
      "Epoch [2/2], Step [57470/64305], Loss: 4.7601\n",
      "Epoch [2/2], Step [57480/64305], Loss: 4.8671\n",
      "Epoch [2/2], Step [57490/64305], Loss: 4.9049\n",
      "Epoch [2/2], Step [57500/64305], Loss: 4.7446\n",
      "Epoch [2/2], Step [57510/64305], Loss: 4.6837\n",
      "Epoch [2/2], Step [57520/64305], Loss: 4.6818\n",
      "Epoch [2/2], Step [57530/64305], Loss: 4.8423\n",
      "Epoch [2/2], Step [57540/64305], Loss: 4.8755\n",
      "Epoch [2/2], Step [57550/64305], Loss: 4.6506\n",
      "Epoch [2/2], Step [57560/64305], Loss: 4.7115\n",
      "Epoch [2/2], Step [57570/64305], Loss: 4.5814\n",
      "Epoch [2/2], Step [57580/64305], Loss: 4.7600\n",
      "Epoch [2/2], Step [57590/64305], Loss: 5.0284\n",
      "Epoch [2/2], Step [57600/64305], Loss: 4.5161\n",
      "Epoch [2/2], Step [57610/64305], Loss: 4.5284\n",
      "Epoch [2/2], Step [57620/64305], Loss: 4.5815\n",
      "Epoch [2/2], Step [57630/64305], Loss: 4.8042\n",
      "Epoch [2/2], Step [57640/64305], Loss: 4.8582\n",
      "Epoch [2/2], Step [57650/64305], Loss: 4.7929\n",
      "Epoch [2/2], Step [57660/64305], Loss: 4.5931\n",
      "Epoch [2/2], Step [57670/64305], Loss: 4.8324\n",
      "Epoch [2/2], Step [57680/64305], Loss: 4.7318\n",
      "Epoch [2/2], Step [57690/64305], Loss: 4.6892\n",
      "Epoch [2/2], Step [57700/64305], Loss: 4.5941\n",
      "Epoch [2/2], Step [57710/64305], Loss: 4.5185\n",
      "Epoch [2/2], Step [57720/64305], Loss: 4.6029\n",
      "Epoch [2/2], Step [57730/64305], Loss: 4.6743\n",
      "Epoch [2/2], Step [57740/64305], Loss: 4.5959\n",
      "Epoch [2/2], Step [57750/64305], Loss: 4.9494\n",
      "Epoch [2/2], Step [57760/64305], Loss: 4.6683\n",
      "Epoch [2/2], Step [57770/64305], Loss: 4.7622\n",
      "Epoch [2/2], Step [57780/64305], Loss: 4.7325\n",
      "Epoch [2/2], Step [57790/64305], Loss: 4.8146\n",
      "Epoch [2/2], Step [57800/64305], Loss: 4.4679\n",
      "Epoch [2/2], Step [57810/64305], Loss: 4.7990\n",
      "Epoch [2/2], Step [57820/64305], Loss: 4.6721\n",
      "Epoch [2/2], Step [57830/64305], Loss: 4.4771\n",
      "Epoch [2/2], Step [57840/64305], Loss: 4.7591\n",
      "Epoch [2/2], Step [57850/64305], Loss: 4.5389\n",
      "Epoch [2/2], Step [57860/64305], Loss: 4.6890\n",
      "Epoch [2/2], Step [57870/64305], Loss: 4.8120\n",
      "Epoch [2/2], Step [57880/64305], Loss: 4.9558\n",
      "Epoch [2/2], Step [57890/64305], Loss: 4.6634\n",
      "Epoch [2/2], Step [57900/64305], Loss: 4.6091\n",
      "Epoch [2/2], Step [57910/64305], Loss: 4.8314\n",
      "Epoch [2/2], Step [57920/64305], Loss: 4.7784\n",
      "Epoch [2/2], Step [57930/64305], Loss: 4.6018\n",
      "Epoch [2/2], Step [57940/64305], Loss: 4.9009\n",
      "Epoch [2/2], Step [57950/64305], Loss: 4.6630\n",
      "Epoch [2/2], Step [57960/64305], Loss: 4.6345\n",
      "Epoch [2/2], Step [57970/64305], Loss: 4.8295\n",
      "Epoch [2/2], Step [57980/64305], Loss: 4.5952\n",
      "Epoch [2/2], Step [57990/64305], Loss: 4.6889\n",
      "Epoch [2/2], Step [58000/64305], Loss: 4.6976\n",
      "Epoch [2/2], Step [58010/64305], Loss: 4.6563\n",
      "Epoch [2/2], Step [58020/64305], Loss: 4.6394\n",
      "Epoch [2/2], Step [58030/64305], Loss: 4.5694\n",
      "Epoch [2/2], Step [58040/64305], Loss: 4.8331\n",
      "Epoch [2/2], Step [58050/64305], Loss: 4.8962\n",
      "Epoch [2/2], Step [58060/64305], Loss: 4.7235\n",
      "Epoch [2/2], Step [58070/64305], Loss: 4.7772\n",
      "Epoch [2/2], Step [58080/64305], Loss: 4.6998\n",
      "Epoch [2/2], Step [58090/64305], Loss: 4.8420\n",
      "Epoch [2/2], Step [58100/64305], Loss: 4.8744\n",
      "Epoch [2/2], Step [58110/64305], Loss: 4.6472\n",
      "Epoch [2/2], Step [58120/64305], Loss: 4.7358\n",
      "Epoch [2/2], Step [58130/64305], Loss: 4.5651\n",
      "Epoch [2/2], Step [58140/64305], Loss: 4.6646\n",
      "Epoch [2/2], Step [58150/64305], Loss: 4.7026\n",
      "Epoch [2/2], Step [58160/64305], Loss: 4.7493\n",
      "Epoch [2/2], Step [58170/64305], Loss: 4.7528\n",
      "Epoch [2/2], Step [58180/64305], Loss: 4.7068\n",
      "Epoch [2/2], Step [58190/64305], Loss: 4.6179\n",
      "Epoch [2/2], Step [58200/64305], Loss: 4.5657\n",
      "Epoch [2/2], Step [58210/64305], Loss: 4.4982\n",
      "Epoch [2/2], Step [58220/64305], Loss: 4.6779\n",
      "Epoch [2/2], Step [58230/64305], Loss: 4.7433\n",
      "Epoch [2/2], Step [58240/64305], Loss: 4.6051\n",
      "Epoch [2/2], Step [58250/64305], Loss: 4.6807\n",
      "Epoch [2/2], Step [58260/64305], Loss: 4.7653\n",
      "Epoch [2/2], Step [58270/64305], Loss: 4.8454\n",
      "Epoch [2/2], Step [58280/64305], Loss: 4.5918\n",
      "Epoch [2/2], Step [58290/64305], Loss: 4.7199\n",
      "Epoch [2/2], Step [58300/64305], Loss: 4.6761\n",
      "Epoch [2/2], Step [58310/64305], Loss: 4.5919\n",
      "Epoch [2/2], Step [58320/64305], Loss: 4.7712\n",
      "Epoch [2/2], Step [58330/64305], Loss: 4.8279\n",
      "Epoch [2/2], Step [58340/64305], Loss: 4.6510\n",
      "Epoch [2/2], Step [58350/64305], Loss: 4.5879\n",
      "Epoch [2/2], Step [58360/64305], Loss: 4.7534\n",
      "Epoch [2/2], Step [58370/64305], Loss: 4.7061\n",
      "Epoch [2/2], Step [58380/64305], Loss: 4.7874\n",
      "Epoch [2/2], Step [58390/64305], Loss: 4.7734\n",
      "Epoch [2/2], Step [58400/64305], Loss: 4.6477\n",
      "Epoch [2/2], Step [58410/64305], Loss: 4.8503\n",
      "Epoch [2/2], Step [58420/64305], Loss: 4.5745\n",
      "Epoch [2/2], Step [58430/64305], Loss: 4.8158\n",
      "Epoch [2/2], Step [58440/64305], Loss: 4.5932\n",
      "Epoch [2/2], Step [58450/64305], Loss: 4.8044\n",
      "Epoch [2/2], Step [58460/64305], Loss: 4.8361\n",
      "Epoch [2/2], Step [58470/64305], Loss: 4.9151\n",
      "Epoch [2/2], Step [58480/64305], Loss: 4.7218\n",
      "Epoch [2/2], Step [58490/64305], Loss: 4.7972\n",
      "Epoch [2/2], Step [58500/64305], Loss: 4.7106\n",
      "Epoch [2/2], Step [58510/64305], Loss: 4.7167\n",
      "Epoch [2/2], Step [58520/64305], Loss: 4.9138\n",
      "Epoch [2/2], Step [58530/64305], Loss: 4.5039\n",
      "Epoch [2/2], Step [58540/64305], Loss: 4.6566\n",
      "Epoch [2/2], Step [58550/64305], Loss: 4.8793\n",
      "Epoch [2/2], Step [58560/64305], Loss: 4.6361\n",
      "Epoch [2/2], Step [58570/64305], Loss: 4.6330\n",
      "Epoch [2/2], Step [58580/64305], Loss: 4.8391\n",
      "Epoch [2/2], Step [58590/64305], Loss: 4.6150\n",
      "Epoch [2/2], Step [58600/64305], Loss: 4.7614\n",
      "Epoch [2/2], Step [58610/64305], Loss: 4.6938\n",
      "Epoch [2/2], Step [58620/64305], Loss: 4.8171\n",
      "Epoch [2/2], Step [58630/64305], Loss: 4.9135\n",
      "Epoch [2/2], Step [58640/64305], Loss: 4.6675\n",
      "Epoch [2/2], Step [58650/64305], Loss: 4.5842\n",
      "Epoch [2/2], Step [58660/64305], Loss: 4.5300\n",
      "Epoch [2/2], Step [58670/64305], Loss: 4.8013\n",
      "Epoch [2/2], Step [58680/64305], Loss: 4.6825\n",
      "Epoch [2/2], Step [58690/64305], Loss: 4.5007\n",
      "Epoch [2/2], Step [58700/64305], Loss: 4.7344\n",
      "Epoch [2/2], Step [58710/64305], Loss: 4.9155\n",
      "Epoch [2/2], Step [58720/64305], Loss: 4.6896\n",
      "Epoch [2/2], Step [58730/64305], Loss: 4.7338\n",
      "Epoch [2/2], Step [58740/64305], Loss: 4.7391\n",
      "Epoch [2/2], Step [58750/64305], Loss: 5.0532\n",
      "Epoch [2/2], Step [58760/64305], Loss: 4.6108\n",
      "Epoch [2/2], Step [58770/64305], Loss: 4.5943\n",
      "Epoch [2/2], Step [58780/64305], Loss: 4.6784\n",
      "Epoch [2/2], Step [58790/64305], Loss: 4.7459\n",
      "Epoch [2/2], Step [58800/64305], Loss: 4.7498\n",
      "Epoch [2/2], Step [58810/64305], Loss: 4.6650\n",
      "Epoch [2/2], Step [58820/64305], Loss: 4.7220\n",
      "Epoch [2/2], Step [58830/64305], Loss: 4.7815\n",
      "Epoch [2/2], Step [58840/64305], Loss: 4.6661\n",
      "Epoch [2/2], Step [58850/64305], Loss: 4.8110\n",
      "Epoch [2/2], Step [58860/64305], Loss: 4.5669\n",
      "Epoch [2/2], Step [58870/64305], Loss: 4.5003\n",
      "Epoch [2/2], Step [58880/64305], Loss: 4.8249\n",
      "Epoch [2/2], Step [58890/64305], Loss: 4.6441\n",
      "Epoch [2/2], Step [58900/64305], Loss: 4.8007\n",
      "Epoch [2/2], Step [58910/64305], Loss: 4.6879\n",
      "Epoch [2/2], Step [58920/64305], Loss: 4.7726\n",
      "Epoch [2/2], Step [58930/64305], Loss: 4.7014\n",
      "Epoch [2/2], Step [58940/64305], Loss: 4.8885\n",
      "Epoch [2/2], Step [58950/64305], Loss: 4.6828\n",
      "Epoch [2/2], Step [58960/64305], Loss: 4.8740\n",
      "Epoch [2/2], Step [58970/64305], Loss: 4.7056\n",
      "Epoch [2/2], Step [58980/64305], Loss: 4.8402\n",
      "Epoch [2/2], Step [58990/64305], Loss: 4.9194\n",
      "Epoch [2/2], Step [59000/64305], Loss: 4.6695\n",
      "Epoch [2/2], Step [59010/64305], Loss: 4.8071\n",
      "Epoch [2/2], Step [59020/64305], Loss: 4.6315\n",
      "Epoch [2/2], Step [59030/64305], Loss: 4.7297\n",
      "Epoch [2/2], Step [59040/64305], Loss: 4.8394\n",
      "Epoch [2/2], Step [59050/64305], Loss: 4.8477\n",
      "Epoch [2/2], Step [59060/64305], Loss: 4.8996\n",
      "Epoch [2/2], Step [59070/64305], Loss: 4.8545\n",
      "Epoch [2/2], Step [59080/64305], Loss: 4.7000\n",
      "Epoch [2/2], Step [59090/64305], Loss: 4.6253\n",
      "Epoch [2/2], Step [59100/64305], Loss: 4.7269\n",
      "Epoch [2/2], Step [59110/64305], Loss: 4.6770\n",
      "Epoch [2/2], Step [59120/64305], Loss: 4.9210\n",
      "Epoch [2/2], Step [59130/64305], Loss: 4.6334\n",
      "Epoch [2/2], Step [59140/64305], Loss: 4.8411\n",
      "Epoch [2/2], Step [59150/64305], Loss: 4.6763\n",
      "Epoch [2/2], Step [59160/64305], Loss: 4.4914\n",
      "Epoch [2/2], Step [59170/64305], Loss: 4.7972\n",
      "Epoch [2/2], Step [59180/64305], Loss: 4.7745\n",
      "Epoch [2/2], Step [59190/64305], Loss: 4.6103\n",
      "Epoch [2/2], Step [59200/64305], Loss: 4.7689\n",
      "Epoch [2/2], Step [59210/64305], Loss: 4.7865\n",
      "Epoch [2/2], Step [59220/64305], Loss: 4.7770\n",
      "Epoch [2/2], Step [59230/64305], Loss: 4.7267\n",
      "Epoch [2/2], Step [59240/64305], Loss: 4.5921\n",
      "Epoch [2/2], Step [59250/64305], Loss: 4.7198\n",
      "Epoch [2/2], Step [59260/64305], Loss: 4.6681\n",
      "Epoch [2/2], Step [59270/64305], Loss: 4.7126\n",
      "Epoch [2/2], Step [59280/64305], Loss: 4.8119\n",
      "Epoch [2/2], Step [59290/64305], Loss: 4.7408\n",
      "Epoch [2/2], Step [59300/64305], Loss: 4.8527\n",
      "Epoch [2/2], Step [59310/64305], Loss: 4.4287\n",
      "Epoch [2/2], Step [59320/64305], Loss: 4.6075\n",
      "Epoch [2/2], Step [59330/64305], Loss: 4.6708\n",
      "Epoch [2/2], Step [59340/64305], Loss: 4.6645\n",
      "Epoch [2/2], Step [59350/64305], Loss: 4.7576\n",
      "Epoch [2/2], Step [59360/64305], Loss: 4.7519\n",
      "Epoch [2/2], Step [59370/64305], Loss: 4.8631\n",
      "Epoch [2/2], Step [59380/64305], Loss: 4.7465\n",
      "Epoch [2/2], Step [59390/64305], Loss: 4.8296\n",
      "Epoch [2/2], Step [59400/64305], Loss: 4.7213\n",
      "Epoch [2/2], Step [59410/64305], Loss: 4.8828\n",
      "Epoch [2/2], Step [59420/64305], Loss: 4.7575\n",
      "Epoch [2/2], Step [59430/64305], Loss: 4.7870\n",
      "Epoch [2/2], Step [59440/64305], Loss: 4.5113\n",
      "Epoch [2/2], Step [59450/64305], Loss: 4.4059\n",
      "Epoch [2/2], Step [59460/64305], Loss: 4.7106\n",
      "Epoch [2/2], Step [59470/64305], Loss: 4.5640\n",
      "Epoch [2/2], Step [59480/64305], Loss: 4.6146\n",
      "Epoch [2/2], Step [59490/64305], Loss: 4.8986\n",
      "Epoch [2/2], Step [59500/64305], Loss: 4.9456\n",
      "Epoch [2/2], Step [59510/64305], Loss: 4.6658\n",
      "Epoch [2/2], Step [59520/64305], Loss: 4.7971\n",
      "Epoch [2/2], Step [59530/64305], Loss: 4.6861\n",
      "Epoch [2/2], Step [59540/64305], Loss: 4.8201\n",
      "Epoch [2/2], Step [59550/64305], Loss: 4.8184\n",
      "Epoch [2/2], Step [59560/64305], Loss: 4.5715\n",
      "Epoch [2/2], Step [59570/64305], Loss: 4.7637\n",
      "Epoch [2/2], Step [59580/64305], Loss: 4.6856\n",
      "Epoch [2/2], Step [59590/64305], Loss: 4.6277\n",
      "Epoch [2/2], Step [59600/64305], Loss: 4.7237\n",
      "Epoch [2/2], Step [59610/64305], Loss: 4.7414\n",
      "Epoch [2/2], Step [59620/64305], Loss: 4.9482\n",
      "Epoch [2/2], Step [59630/64305], Loss: 4.8423\n",
      "Epoch [2/2], Step [59640/64305], Loss: 4.5896\n",
      "Epoch [2/2], Step [59650/64305], Loss: 4.8245\n",
      "Epoch [2/2], Step [59660/64305], Loss: 4.7817\n",
      "Epoch [2/2], Step [59670/64305], Loss: 4.6748\n",
      "Epoch [2/2], Step [59680/64305], Loss: 4.9013\n",
      "Epoch [2/2], Step [59690/64305], Loss: 4.7105\n",
      "Epoch [2/2], Step [59700/64305], Loss: 4.8684\n",
      "Epoch [2/2], Step [59710/64305], Loss: 4.7977\n",
      "Epoch [2/2], Step [59720/64305], Loss: 4.5897\n",
      "Epoch [2/2], Step [59730/64305], Loss: 4.6565\n",
      "Epoch [2/2], Step [59740/64305], Loss: 4.7148\n",
      "Epoch [2/2], Step [59750/64305], Loss: 4.6034\n",
      "Epoch [2/2], Step [59760/64305], Loss: 4.7361\n",
      "Epoch [2/2], Step [59770/64305], Loss: 4.4669\n",
      "Epoch [2/2], Step [59780/64305], Loss: 4.7868\n",
      "Epoch [2/2], Step [59790/64305], Loss: 4.8766\n",
      "Epoch [2/2], Step [59800/64305], Loss: 4.7505\n",
      "Epoch [2/2], Step [59810/64305], Loss: 4.7642\n",
      "Epoch [2/2], Step [59820/64305], Loss: 4.8448\n",
      "Epoch [2/2], Step [59830/64305], Loss: 4.8013\n",
      "Epoch [2/2], Step [59840/64305], Loss: 4.7596\n",
      "Epoch [2/2], Step [59850/64305], Loss: 4.6898\n",
      "Epoch [2/2], Step [59860/64305], Loss: 4.5979\n",
      "Epoch [2/2], Step [59870/64305], Loss: 4.7435\n",
      "Epoch [2/2], Step [59880/64305], Loss: 4.5550\n",
      "Epoch [2/2], Step [59890/64305], Loss: 4.6438\n",
      "Epoch [2/2], Step [59900/64305], Loss: 4.7233\n",
      "Epoch [2/2], Step [59910/64305], Loss: 4.6467\n",
      "Epoch [2/2], Step [59920/64305], Loss: 4.7707\n",
      "Epoch [2/2], Step [59930/64305], Loss: 4.8885\n",
      "Epoch [2/2], Step [59940/64305], Loss: 4.7591\n",
      "Epoch [2/2], Step [59950/64305], Loss: 4.6687\n",
      "Epoch [2/2], Step [59960/64305], Loss: 4.6810\n",
      "Epoch [2/2], Step [59970/64305], Loss: 4.7190\n",
      "Epoch [2/2], Step [59980/64305], Loss: 4.5492\n",
      "Epoch [2/2], Step [59990/64305], Loss: 4.8540\n",
      "Epoch [2/2], Step [60000/64305], Loss: 4.8329\n",
      "Epoch [2/2], Step [60010/64305], Loss: 4.9286\n",
      "Epoch [2/2], Step [60020/64305], Loss: 4.7514\n",
      "Epoch [2/2], Step [60030/64305], Loss: 4.6156\n",
      "Epoch [2/2], Step [60040/64305], Loss: 4.7194\n",
      "Epoch [2/2], Step [60050/64305], Loss: 4.7893\n",
      "Epoch [2/2], Step [60060/64305], Loss: 4.7625\n",
      "Epoch [2/2], Step [60070/64305], Loss: 4.6844\n",
      "Epoch [2/2], Step [60080/64305], Loss: 4.9796\n",
      "Epoch [2/2], Step [60090/64305], Loss: 4.6179\n",
      "Epoch [2/2], Step [60100/64305], Loss: 4.5147\n",
      "Epoch [2/2], Step [60110/64305], Loss: 4.7194\n",
      "Epoch [2/2], Step [60120/64305], Loss: 4.6386\n",
      "Epoch [2/2], Step [60130/64305], Loss: 4.8445\n",
      "Epoch [2/2], Step [60140/64305], Loss: 4.7718\n",
      "Epoch [2/2], Step [60150/64305], Loss: 4.9854\n",
      "Epoch [2/2], Step [60160/64305], Loss: 4.7875\n",
      "Epoch [2/2], Step [60170/64305], Loss: 4.4958\n",
      "Epoch [2/2], Step [60180/64305], Loss: 4.7173\n",
      "Epoch [2/2], Step [60190/64305], Loss: 4.8358\n",
      "Epoch [2/2], Step [60200/64305], Loss: 4.8016\n",
      "Epoch [2/2], Step [60210/64305], Loss: 4.8148\n",
      "Epoch [2/2], Step [60220/64305], Loss: 4.6429\n",
      "Epoch [2/2], Step [60230/64305], Loss: 4.7208\n",
      "Epoch [2/2], Step [60240/64305], Loss: 4.6515\n",
      "Epoch [2/2], Step [60250/64305], Loss: 4.7085\n",
      "Epoch [2/2], Step [60260/64305], Loss: 4.3722\n",
      "Epoch [2/2], Step [60270/64305], Loss: 4.7511\n",
      "Epoch [2/2], Step [60280/64305], Loss: 4.7379\n",
      "Epoch [2/2], Step [60290/64305], Loss: 5.0174\n",
      "Epoch [2/2], Step [60300/64305], Loss: 4.6173\n",
      "Epoch [2/2], Step [60310/64305], Loss: 4.8476\n",
      "Epoch [2/2], Step [60320/64305], Loss: 4.7213\n",
      "Epoch [2/2], Step [60330/64305], Loss: 4.6978\n",
      "Epoch [2/2], Step [60340/64305], Loss: 5.0401\n",
      "Epoch [2/2], Step [60350/64305], Loss: 4.5840\n",
      "Epoch [2/2], Step [60360/64305], Loss: 4.8118\n",
      "Epoch [2/2], Step [60370/64305], Loss: 4.6706\n",
      "Epoch [2/2], Step [60380/64305], Loss: 4.6501\n",
      "Epoch [2/2], Step [60390/64305], Loss: 4.5523\n",
      "Epoch [2/2], Step [60400/64305], Loss: 4.6242\n",
      "Epoch [2/2], Step [60410/64305], Loss: 4.7813\n",
      "Epoch [2/2], Step [60420/64305], Loss: 4.6294\n",
      "Epoch [2/2], Step [60430/64305], Loss: 4.7016\n",
      "Epoch [2/2], Step [60440/64305], Loss: 4.5307\n",
      "Epoch [2/2], Step [60450/64305], Loss: 4.9339\n",
      "Epoch [2/2], Step [60460/64305], Loss: 4.8935\n",
      "Epoch [2/2], Step [60470/64305], Loss: 4.8669\n",
      "Epoch [2/2], Step [60480/64305], Loss: 4.7154\n",
      "Epoch [2/2], Step [60490/64305], Loss: 4.5402\n",
      "Epoch [2/2], Step [60500/64305], Loss: 4.6314\n",
      "Epoch [2/2], Step [60510/64305], Loss: 4.8090\n",
      "Epoch [2/2], Step [60520/64305], Loss: 4.4314\n",
      "Epoch [2/2], Step [60530/64305], Loss: 4.8744\n",
      "Epoch [2/2], Step [60540/64305], Loss: 4.6242\n",
      "Epoch [2/2], Step [60550/64305], Loss: 4.5983\n",
      "Epoch [2/2], Step [60560/64305], Loss: 4.6935\n",
      "Epoch [2/2], Step [60570/64305], Loss: 4.7465\n",
      "Epoch [2/2], Step [60580/64305], Loss: 4.6657\n",
      "Epoch [2/2], Step [60590/64305], Loss: 4.5877\n",
      "Epoch [2/2], Step [60600/64305], Loss: 4.5912\n",
      "Epoch [2/2], Step [60610/64305], Loss: 4.6883\n",
      "Epoch [2/2], Step [60620/64305], Loss: 4.6587\n",
      "Epoch [2/2], Step [60630/64305], Loss: 4.8434\n",
      "Epoch [2/2], Step [60640/64305], Loss: 4.7428\n",
      "Epoch [2/2], Step [60650/64305], Loss: 4.9089\n",
      "Epoch [2/2], Step [60660/64305], Loss: 4.6550\n",
      "Epoch [2/2], Step [60670/64305], Loss: 4.5345\n",
      "Epoch [2/2], Step [60680/64305], Loss: 4.7221\n",
      "Epoch [2/2], Step [60690/64305], Loss: 4.6630\n",
      "Epoch [2/2], Step [60700/64305], Loss: 4.6582\n",
      "Epoch [2/2], Step [60710/64305], Loss: 4.5858\n",
      "Epoch [2/2], Step [60720/64305], Loss: 4.5947\n",
      "Epoch [2/2], Step [60730/64305], Loss: 4.7164\n",
      "Epoch [2/2], Step [60740/64305], Loss: 4.7684\n",
      "Epoch [2/2], Step [60750/64305], Loss: 4.5609\n",
      "Epoch [2/2], Step [60760/64305], Loss: 4.5632\n",
      "Epoch [2/2], Step [60770/64305], Loss: 4.7898\n",
      "Epoch [2/2], Step [60780/64305], Loss: 4.9198\n",
      "Epoch [2/2], Step [60790/64305], Loss: 4.8710\n",
      "Epoch [2/2], Step [60800/64305], Loss: 4.7584\n",
      "Epoch [2/2], Step [60810/64305], Loss: 4.7075\n",
      "Epoch [2/2], Step [60820/64305], Loss: 4.7618\n",
      "Epoch [2/2], Step [60830/64305], Loss: 4.9275\n",
      "Epoch [2/2], Step [60840/64305], Loss: 4.7553\n",
      "Epoch [2/2], Step [60850/64305], Loss: 4.8875\n",
      "Epoch [2/2], Step [60860/64305], Loss: 4.7537\n",
      "Epoch [2/2], Step [60870/64305], Loss: 4.7136\n",
      "Epoch [2/2], Step [60880/64305], Loss: 4.8270\n",
      "Epoch [2/2], Step [60890/64305], Loss: 4.5080\n",
      "Epoch [2/2], Step [60900/64305], Loss: 4.8302\n",
      "Epoch [2/2], Step [60910/64305], Loss: 4.7794\n",
      "Epoch [2/2], Step [60920/64305], Loss: 4.7055\n",
      "Epoch [2/2], Step [60930/64305], Loss: 4.8644\n",
      "Epoch [2/2], Step [60940/64305], Loss: 4.8722\n",
      "Epoch [2/2], Step [60950/64305], Loss: 4.6366\n",
      "Epoch [2/2], Step [60960/64305], Loss: 4.7409\n",
      "Epoch [2/2], Step [60970/64305], Loss: 4.7558\n",
      "Epoch [2/2], Step [60980/64305], Loss: 4.7246\n",
      "Epoch [2/2], Step [60990/64305], Loss: 4.6033\n",
      "Epoch [2/2], Step [61000/64305], Loss: 4.5753\n",
      "Epoch [2/2], Step [61010/64305], Loss: 4.5573\n",
      "Epoch [2/2], Step [61020/64305], Loss: 4.7283\n",
      "Epoch [2/2], Step [61030/64305], Loss: 4.5318\n",
      "Epoch [2/2], Step [61040/64305], Loss: 4.7493\n",
      "Epoch [2/2], Step [61050/64305], Loss: 4.6547\n",
      "Epoch [2/2], Step [61060/64305], Loss: 4.6768\n",
      "Epoch [2/2], Step [61070/64305], Loss: 4.7068\n",
      "Epoch [2/2], Step [61080/64305], Loss: 4.7455\n",
      "Epoch [2/2], Step [61090/64305], Loss: 4.7350\n",
      "Epoch [2/2], Step [61100/64305], Loss: 4.6815\n",
      "Epoch [2/2], Step [61110/64305], Loss: 4.6324\n",
      "Epoch [2/2], Step [61120/64305], Loss: 4.6557\n",
      "Epoch [2/2], Step [61130/64305], Loss: 4.9072\n",
      "Epoch [2/2], Step [61140/64305], Loss: 4.9062\n",
      "Epoch [2/2], Step [61150/64305], Loss: 4.9024\n",
      "Epoch [2/2], Step [61160/64305], Loss: 4.6744\n",
      "Epoch [2/2], Step [61170/64305], Loss: 4.6793\n",
      "Epoch [2/2], Step [61180/64305], Loss: 4.8289\n",
      "Epoch [2/2], Step [61190/64305], Loss: 4.8330\n",
      "Epoch [2/2], Step [61200/64305], Loss: 4.6269\n",
      "Epoch [2/2], Step [61210/64305], Loss: 4.7280\n",
      "Epoch [2/2], Step [61220/64305], Loss: 4.6516\n",
      "Epoch [2/2], Step [61230/64305], Loss: 4.3870\n",
      "Epoch [2/2], Step [61240/64305], Loss: 4.7784\n",
      "Epoch [2/2], Step [61250/64305], Loss: 4.6548\n",
      "Epoch [2/2], Step [61260/64305], Loss: 4.7953\n",
      "Epoch [2/2], Step [61270/64305], Loss: 4.5403\n",
      "Epoch [2/2], Step [61280/64305], Loss: 4.8136\n",
      "Epoch [2/2], Step [61290/64305], Loss: 4.8984\n",
      "Epoch [2/2], Step [61300/64305], Loss: 4.8371\n",
      "Epoch [2/2], Step [61310/64305], Loss: 4.6620\n",
      "Epoch [2/2], Step [61320/64305], Loss: 4.8879\n",
      "Epoch [2/2], Step [61330/64305], Loss: 4.7372\n",
      "Epoch [2/2], Step [61340/64305], Loss: 4.7634\n",
      "Epoch [2/2], Step [61350/64305], Loss: 4.7291\n",
      "Epoch [2/2], Step [61360/64305], Loss: 4.6383\n",
      "Epoch [2/2], Step [61370/64305], Loss: 4.6079\n",
      "Epoch [2/2], Step [61380/64305], Loss: 4.5420\n",
      "Epoch [2/2], Step [61390/64305], Loss: 4.8028\n",
      "Epoch [2/2], Step [61400/64305], Loss: 4.7568\n",
      "Epoch [2/2], Step [61410/64305], Loss: 4.6582\n",
      "Epoch [2/2], Step [61420/64305], Loss: 4.9346\n",
      "Epoch [2/2], Step [61430/64305], Loss: 4.6284\n",
      "Epoch [2/2], Step [61440/64305], Loss: 4.6648\n",
      "Epoch [2/2], Step [61450/64305], Loss: 4.5843\n",
      "Epoch [2/2], Step [61460/64305], Loss: 4.7094\n",
      "Epoch [2/2], Step [61470/64305], Loss: 4.7847\n",
      "Epoch [2/2], Step [61480/64305], Loss: 4.7984\n",
      "Epoch [2/2], Step [61490/64305], Loss: 4.7559\n",
      "Epoch [2/2], Step [61500/64305], Loss: 4.8612\n",
      "Epoch [2/2], Step [61510/64305], Loss: 4.5752\n",
      "Epoch [2/2], Step [61520/64305], Loss: 4.8385\n",
      "Epoch [2/2], Step [61530/64305], Loss: 4.9244\n",
      "Epoch [2/2], Step [61540/64305], Loss: 4.6387\n",
      "Epoch [2/2], Step [61550/64305], Loss: 4.7626\n",
      "Epoch [2/2], Step [61560/64305], Loss: 4.8982\n",
      "Epoch [2/2], Step [61570/64305], Loss: 4.8030\n",
      "Epoch [2/2], Step [61580/64305], Loss: 4.5919\n",
      "Epoch [2/2], Step [61590/64305], Loss: 4.7053\n",
      "Epoch [2/2], Step [61600/64305], Loss: 4.9046\n",
      "Epoch [2/2], Step [61610/64305], Loss: 4.8985\n",
      "Epoch [2/2], Step [61620/64305], Loss: 4.6883\n",
      "Epoch [2/2], Step [61630/64305], Loss: 4.6905\n",
      "Epoch [2/2], Step [61640/64305], Loss: 4.9428\n",
      "Epoch [2/2], Step [61650/64305], Loss: 4.7921\n",
      "Epoch [2/2], Step [61660/64305], Loss: 4.7866\n",
      "Epoch [2/2], Step [61670/64305], Loss: 4.6694\n",
      "Epoch [2/2], Step [61680/64305], Loss: 4.6000\n",
      "Epoch [2/2], Step [61690/64305], Loss: 4.7410\n",
      "Epoch [2/2], Step [61700/64305], Loss: 4.6537\n",
      "Epoch [2/2], Step [61710/64305], Loss: 4.8042\n",
      "Epoch [2/2], Step [61720/64305], Loss: 4.6216\n",
      "Epoch [2/2], Step [61730/64305], Loss: 4.7087\n",
      "Epoch [2/2], Step [61740/64305], Loss: 4.6769\n",
      "Epoch [2/2], Step [61750/64305], Loss: 4.7060\n",
      "Epoch [2/2], Step [61760/64305], Loss: 4.8383\n",
      "Epoch [2/2], Step [61770/64305], Loss: 4.5216\n",
      "Epoch [2/2], Step [61780/64305], Loss: 4.9670\n",
      "Epoch [2/2], Step [61790/64305], Loss: 4.7469\n",
      "Epoch [2/2], Step [61800/64305], Loss: 4.8125\n",
      "Epoch [2/2], Step [61810/64305], Loss: 4.7567\n",
      "Epoch [2/2], Step [61820/64305], Loss: 4.6747\n",
      "Epoch [2/2], Step [61830/64305], Loss: 4.6408\n",
      "Epoch [2/2], Step [61840/64305], Loss: 4.5256\n",
      "Epoch [2/2], Step [61850/64305], Loss: 4.7426\n",
      "Epoch [2/2], Step [61860/64305], Loss: 4.7013\n",
      "Epoch [2/2], Step [61870/64305], Loss: 4.7150\n",
      "Epoch [2/2], Step [61880/64305], Loss: 4.7072\n",
      "Epoch [2/2], Step [61890/64305], Loss: 4.5266\n",
      "Epoch [2/2], Step [61900/64305], Loss: 4.8341\n",
      "Epoch [2/2], Step [61910/64305], Loss: 4.8093\n",
      "Epoch [2/2], Step [61920/64305], Loss: 4.7031\n",
      "Epoch [2/2], Step [61930/64305], Loss: 4.8737\n",
      "Epoch [2/2], Step [61940/64305], Loss: 4.7847\n",
      "Epoch [2/2], Step [61950/64305], Loss: 4.7671\n",
      "Epoch [2/2], Step [61960/64305], Loss: 4.6436\n",
      "Epoch [2/2], Step [61970/64305], Loss: 4.8438\n",
      "Epoch [2/2], Step [61980/64305], Loss: 4.6687\n",
      "Epoch [2/2], Step [61990/64305], Loss: 4.5394\n",
      "Epoch [2/2], Step [62000/64305], Loss: 4.5629\n",
      "Epoch [2/2], Step [62010/64305], Loss: 5.0184\n",
      "Epoch [2/2], Step [62020/64305], Loss: 4.6226\n",
      "Epoch [2/2], Step [62030/64305], Loss: 4.6162\n",
      "Epoch [2/2], Step [62040/64305], Loss: 4.8163\n",
      "Epoch [2/2], Step [62050/64305], Loss: 4.6577\n",
      "Epoch [2/2], Step [62060/64305], Loss: 4.5581\n",
      "Epoch [2/2], Step [62070/64305], Loss: 4.5753\n",
      "Epoch [2/2], Step [62080/64305], Loss: 4.7027\n",
      "Epoch [2/2], Step [62090/64305], Loss: 4.7226\n",
      "Epoch [2/2], Step [62100/64305], Loss: 4.5924\n",
      "Epoch [2/2], Step [62110/64305], Loss: 4.8910\n",
      "Epoch [2/2], Step [62120/64305], Loss: 4.9833\n",
      "Epoch [2/2], Step [62130/64305], Loss: 4.8524\n",
      "Epoch [2/2], Step [62140/64305], Loss: 4.7255\n",
      "Epoch [2/2], Step [62150/64305], Loss: 4.6186\n",
      "Epoch [2/2], Step [62160/64305], Loss: 4.8268\n",
      "Epoch [2/2], Step [62170/64305], Loss: 4.4931\n",
      "Epoch [2/2], Step [62180/64305], Loss: 4.6459\n",
      "Epoch [2/2], Step [62190/64305], Loss: 4.6150\n",
      "Epoch [2/2], Step [62200/64305], Loss: 4.5938\n",
      "Epoch [2/2], Step [62210/64305], Loss: 4.6581\n",
      "Epoch [2/2], Step [62220/64305], Loss: 4.6769\n",
      "Epoch [2/2], Step [62230/64305], Loss: 4.8319\n",
      "Epoch [2/2], Step [62240/64305], Loss: 4.6509\n",
      "Epoch [2/2], Step [62250/64305], Loss: 4.6080\n",
      "Epoch [2/2], Step [62260/64305], Loss: 4.6440\n",
      "Epoch [2/2], Step [62270/64305], Loss: 4.6083\n",
      "Epoch [2/2], Step [62280/64305], Loss: 4.6388\n",
      "Epoch [2/2], Step [62290/64305], Loss: 4.6077\n",
      "Epoch [2/2], Step [62300/64305], Loss: 4.7479\n",
      "Epoch [2/2], Step [62310/64305], Loss: 4.8137\n",
      "Epoch [2/2], Step [62320/64305], Loss: 4.6950\n",
      "Epoch [2/2], Step [62330/64305], Loss: 4.6610\n",
      "Epoch [2/2], Step [62340/64305], Loss: 4.8433\n",
      "Epoch [2/2], Step [62350/64305], Loss: 4.7444\n",
      "Epoch [2/2], Step [62360/64305], Loss: 4.7385\n",
      "Epoch [2/2], Step [62370/64305], Loss: 4.7584\n",
      "Epoch [2/2], Step [62380/64305], Loss: 4.7451\n",
      "Epoch [2/2], Step [62390/64305], Loss: 4.5985\n",
      "Epoch [2/2], Step [62400/64305], Loss: 4.8583\n",
      "Epoch [2/2], Step [62410/64305], Loss: 4.7187\n",
      "Epoch [2/2], Step [62420/64305], Loss: 4.5450\n",
      "Epoch [2/2], Step [62430/64305], Loss: 4.7772\n",
      "Epoch [2/2], Step [62440/64305], Loss: 4.7506\n",
      "Epoch [2/2], Step [62450/64305], Loss: 4.9347\n",
      "Epoch [2/2], Step [62460/64305], Loss: 4.7957\n",
      "Epoch [2/2], Step [62470/64305], Loss: 4.5883\n",
      "Epoch [2/2], Step [62480/64305], Loss: 4.6728\n",
      "Epoch [2/2], Step [62490/64305], Loss: 4.5846\n",
      "Epoch [2/2], Step [62500/64305], Loss: 4.5316\n",
      "Epoch [2/2], Step [62510/64305], Loss: 4.7856\n",
      "Epoch [2/2], Step [62520/64305], Loss: 4.7805\n",
      "Epoch [2/2], Step [62530/64305], Loss: 4.6959\n",
      "Epoch [2/2], Step [62540/64305], Loss: 4.8135\n",
      "Epoch [2/2], Step [62550/64305], Loss: 4.6880\n",
      "Epoch [2/2], Step [62560/64305], Loss: 4.7747\n",
      "Epoch [2/2], Step [62570/64305], Loss: 4.8711\n",
      "Epoch [2/2], Step [62580/64305], Loss: 4.7781\n",
      "Epoch [2/2], Step [62590/64305], Loss: 4.8884\n",
      "Epoch [2/2], Step [62600/64305], Loss: 4.6934\n",
      "Epoch [2/2], Step [62610/64305], Loss: 4.5829\n",
      "Epoch [2/2], Step [62620/64305], Loss: 4.7041\n",
      "Epoch [2/2], Step [62630/64305], Loss: 4.6342\n",
      "Epoch [2/2], Step [62640/64305], Loss: 4.6397\n",
      "Epoch [2/2], Step [62650/64305], Loss: 4.8486\n",
      "Epoch [2/2], Step [62660/64305], Loss: 4.5192\n",
      "Epoch [2/2], Step [62670/64305], Loss: 4.7557\n",
      "Epoch [2/2], Step [62680/64305], Loss: 4.8856\n",
      "Epoch [2/2], Step [62690/64305], Loss: 4.5600\n",
      "Epoch [2/2], Step [62700/64305], Loss: 4.7757\n",
      "Epoch [2/2], Step [62710/64305], Loss: 4.7905\n",
      "Epoch [2/2], Step [62720/64305], Loss: 4.7291\n",
      "Epoch [2/2], Step [62730/64305], Loss: 4.7873\n",
      "Epoch [2/2], Step [62740/64305], Loss: 4.6704\n",
      "Epoch [2/2], Step [62750/64305], Loss: 4.8494\n",
      "Epoch [2/2], Step [62760/64305], Loss: 4.7574\n",
      "Epoch [2/2], Step [62770/64305], Loss: 4.7946\n",
      "Epoch [2/2], Step [62780/64305], Loss: 4.8449\n",
      "Epoch [2/2], Step [62790/64305], Loss: 4.8172\n",
      "Epoch [2/2], Step [62800/64305], Loss: 4.5937\n",
      "Epoch [2/2], Step [62810/64305], Loss: 4.7298\n",
      "Epoch [2/2], Step [62820/64305], Loss: 4.7916\n",
      "Epoch [2/2], Step [62830/64305], Loss: 4.8155\n",
      "Epoch [2/2], Step [62840/64305], Loss: 4.7241\n",
      "Epoch [2/2], Step [62850/64305], Loss: 4.6624\n",
      "Epoch [2/2], Step [62860/64305], Loss: 4.6636\n",
      "Epoch [2/2], Step [62870/64305], Loss: 4.5505\n",
      "Epoch [2/2], Step [62880/64305], Loss: 4.7561\n",
      "Epoch [2/2], Step [62890/64305], Loss: 4.7869\n",
      "Epoch [2/2], Step [62900/64305], Loss: 4.7246\n",
      "Epoch [2/2], Step [62910/64305], Loss: 4.6917\n",
      "Epoch [2/2], Step [62920/64305], Loss: 4.8195\n",
      "Epoch [2/2], Step [62930/64305], Loss: 4.7594\n",
      "Epoch [2/2], Step [62940/64305], Loss: 4.7951\n",
      "Epoch [2/2], Step [62950/64305], Loss: 4.8108\n",
      "Epoch [2/2], Step [62960/64305], Loss: 4.7882\n",
      "Epoch [2/2], Step [62970/64305], Loss: 4.7660\n",
      "Epoch [2/2], Step [62980/64305], Loss: 4.7356\n",
      "Epoch [2/2], Step [62990/64305], Loss: 4.7041\n",
      "Epoch [2/2], Step [63000/64305], Loss: 4.6996\n",
      "Epoch [2/2], Step [63010/64305], Loss: 4.6775\n",
      "Epoch [2/2], Step [63020/64305], Loss: 4.6794\n",
      "Epoch [2/2], Step [63030/64305], Loss: 4.9075\n",
      "Epoch [2/2], Step [63040/64305], Loss: 4.7881\n",
      "Epoch [2/2], Step [63050/64305], Loss: 4.7666\n",
      "Epoch [2/2], Step [63060/64305], Loss: 4.6311\n",
      "Epoch [2/2], Step [63070/64305], Loss: 4.7851\n",
      "Epoch [2/2], Step [63080/64305], Loss: 4.7421\n",
      "Epoch [2/2], Step [63090/64305], Loss: 4.7989\n",
      "Epoch [2/2], Step [63100/64305], Loss: 4.5885\n",
      "Epoch [2/2], Step [63110/64305], Loss: 4.6079\n",
      "Epoch [2/2], Step [63120/64305], Loss: 4.8645\n",
      "Epoch [2/2], Step [63130/64305], Loss: 4.9321\n",
      "Epoch [2/2], Step [63140/64305], Loss: 4.6952\n",
      "Epoch [2/2], Step [63150/64305], Loss: 4.7616\n",
      "Epoch [2/2], Step [63160/64305], Loss: 4.7881\n",
      "Epoch [2/2], Step [63170/64305], Loss: 4.9029\n",
      "Epoch [2/2], Step [63180/64305], Loss: 4.7530\n",
      "Epoch [2/2], Step [63190/64305], Loss: 4.7373\n",
      "Epoch [2/2], Step [63200/64305], Loss: 4.7283\n",
      "Epoch [2/2], Step [63210/64305], Loss: 4.8768\n",
      "Epoch [2/2], Step [63220/64305], Loss: 4.6094\n",
      "Epoch [2/2], Step [63230/64305], Loss: 4.6087\n",
      "Epoch [2/2], Step [63240/64305], Loss: 4.8230\n",
      "Epoch [2/2], Step [63250/64305], Loss: 4.6606\n",
      "Epoch [2/2], Step [63260/64305], Loss: 4.8193\n",
      "Epoch [2/2], Step [63270/64305], Loss: 4.9032\n",
      "Epoch [2/2], Step [63280/64305], Loss: 4.6851\n",
      "Epoch [2/2], Step [63290/64305], Loss: 4.6616\n",
      "Epoch [2/2], Step [63300/64305], Loss: 4.7787\n",
      "Epoch [2/2], Step [63310/64305], Loss: 4.8230\n",
      "Epoch [2/2], Step [63320/64305], Loss: 4.5869\n",
      "Epoch [2/2], Step [63330/64305], Loss: 4.6472\n",
      "Epoch [2/2], Step [63340/64305], Loss: 4.5568\n",
      "Epoch [2/2], Step [63350/64305], Loss: 4.5371\n",
      "Epoch [2/2], Step [63360/64305], Loss: 4.7135\n",
      "Epoch [2/2], Step [63370/64305], Loss: 4.7648\n",
      "Epoch [2/2], Step [63380/64305], Loss: 4.6682\n",
      "Epoch [2/2], Step [63390/64305], Loss: 4.6966\n",
      "Epoch [2/2], Step [63400/64305], Loss: 4.6456\n",
      "Epoch [2/2], Step [63410/64305], Loss: 4.8213\n",
      "Epoch [2/2], Step [63420/64305], Loss: 4.6564\n",
      "Epoch [2/2], Step [63430/64305], Loss: 4.4847\n",
      "Epoch [2/2], Step [63440/64305], Loss: 4.5445\n",
      "Epoch [2/2], Step [63450/64305], Loss: 4.8432\n",
      "Epoch [2/2], Step [63460/64305], Loss: 4.7761\n",
      "Epoch [2/2], Step [63470/64305], Loss: 4.8705\n",
      "Epoch [2/2], Step [63480/64305], Loss: 4.7587\n",
      "Epoch [2/2], Step [63490/64305], Loss: 4.6772\n",
      "Epoch [2/2], Step [63500/64305], Loss: 4.7640\n",
      "Epoch [2/2], Step [63510/64305], Loss: 4.7131\n",
      "Epoch [2/2], Step [63520/64305], Loss: 4.7488\n",
      "Epoch [2/2], Step [63530/64305], Loss: 4.7927\n",
      "Epoch [2/2], Step [63540/64305], Loss: 4.7816\n",
      "Epoch [2/2], Step [63550/64305], Loss: 4.6658\n",
      "Epoch [2/2], Step [63560/64305], Loss: 4.9408\n",
      "Epoch [2/2], Step [63570/64305], Loss: 4.7824\n",
      "Epoch [2/2], Step [63580/64305], Loss: 4.6804\n",
      "Epoch [2/2], Step [63590/64305], Loss: 4.7002\n",
      "Epoch [2/2], Step [63600/64305], Loss: 4.7986\n",
      "Epoch [2/2], Step [63610/64305], Loss: 4.6849\n",
      "Epoch [2/2], Step [63620/64305], Loss: 4.5945\n",
      "Epoch [2/2], Step [63630/64305], Loss: 4.7242\n",
      "Epoch [2/2], Step [63640/64305], Loss: 4.6911\n",
      "Epoch [2/2], Step [63650/64305], Loss: 4.7115\n",
      "Epoch [2/2], Step [63660/64305], Loss: 4.7458\n",
      "Epoch [2/2], Step [63670/64305], Loss: 4.7959\n",
      "Epoch [2/2], Step [63680/64305], Loss: 4.5911\n",
      "Epoch [2/2], Step [63690/64305], Loss: 4.6689\n",
      "Epoch [2/2], Step [63700/64305], Loss: 4.7852\n",
      "Epoch [2/2], Step [63710/64305], Loss: 4.6282\n",
      "Epoch [2/2], Step [63720/64305], Loss: 4.7077\n",
      "Epoch [2/2], Step [63730/64305], Loss: 4.8214\n",
      "Epoch [2/2], Step [63740/64305], Loss: 4.5648\n",
      "Epoch [2/2], Step [63750/64305], Loss: 4.7344\n",
      "Epoch [2/2], Step [63760/64305], Loss: 4.8225\n",
      "Epoch [2/2], Step [63770/64305], Loss: 4.7350\n",
      "Epoch [2/2], Step [63780/64305], Loss: 4.4973\n",
      "Epoch [2/2], Step [63790/64305], Loss: 4.7314\n",
      "Epoch [2/2], Step [63800/64305], Loss: 4.8671\n",
      "Epoch [2/2], Step [63810/64305], Loss: 4.8139\n",
      "Epoch [2/2], Step [63820/64305], Loss: 4.5683\n",
      "Epoch [2/2], Step [63830/64305], Loss: 4.8988\n",
      "Epoch [2/2], Step [63840/64305], Loss: 4.9203\n",
      "Epoch [2/2], Step [63850/64305], Loss: 4.7414\n",
      "Epoch [2/2], Step [63860/64305], Loss: 4.9544\n",
      "Epoch [2/2], Step [63870/64305], Loss: 4.7350\n",
      "Epoch [2/2], Step [63880/64305], Loss: 4.7359\n",
      "Epoch [2/2], Step [63890/64305], Loss: 4.8785\n",
      "Epoch [2/2], Step [63900/64305], Loss: 4.8085\n",
      "Epoch [2/2], Step [63910/64305], Loss: 4.8191\n",
      "Epoch [2/2], Step [63920/64305], Loss: 4.4679\n",
      "Epoch [2/2], Step [63930/64305], Loss: 4.6063\n",
      "Epoch [2/2], Step [63940/64305], Loss: 4.9114\n",
      "Epoch [2/2], Step [63950/64305], Loss: 4.9852\n",
      "Epoch [2/2], Step [63960/64305], Loss: 4.4220\n",
      "Epoch [2/2], Step [63970/64305], Loss: 4.5704\n",
      "Epoch [2/2], Step [63980/64305], Loss: 4.5771\n",
      "Epoch [2/2], Step [63990/64305], Loss: 4.5499\n",
      "Epoch [2/2], Step [64000/64305], Loss: 4.4689\n",
      "Epoch [2/2], Step [64010/64305], Loss: 4.8695\n",
      "Epoch [2/2], Step [64020/64305], Loss: 4.8550\n",
      "Epoch [2/2], Step [64030/64305], Loss: 4.6736\n",
      "Epoch [2/2], Step [64040/64305], Loss: 4.7006\n",
      "Epoch [2/2], Step [64050/64305], Loss: 4.5617\n",
      "Epoch [2/2], Step [64060/64305], Loss: 4.7323\n",
      "Epoch [2/2], Step [64070/64305], Loss: 4.6201\n",
      "Epoch [2/2], Step [64080/64305], Loss: 4.8020\n",
      "Epoch [2/2], Step [64090/64305], Loss: 4.8144\n",
      "Epoch [2/2], Step [64100/64305], Loss: 4.5891\n",
      "Epoch [2/2], Step [64110/64305], Loss: 4.8595\n",
      "Epoch [2/2], Step [64120/64305], Loss: 4.6914\n",
      "Epoch [2/2], Step [64130/64305], Loss: 4.8186\n",
      "Epoch [2/2], Step [64140/64305], Loss: 4.5901\n",
      "Epoch [2/2], Step [64150/64305], Loss: 4.6877\n",
      "Epoch [2/2], Step [64160/64305], Loss: 4.7807\n",
      "Epoch [2/2], Step [64170/64305], Loss: 4.9166\n",
      "Epoch [2/2], Step [64180/64305], Loss: 4.7028\n",
      "Epoch [2/2], Step [64190/64305], Loss: 4.9376\n",
      "Epoch [2/2], Step [64200/64305], Loss: 4.6893\n",
      "Epoch [2/2], Step [64210/64305], Loss: 4.6114\n",
      "Epoch [2/2], Step [64220/64305], Loss: 4.6876\n",
      "Epoch [2/2], Step [64230/64305], Loss: 4.5268\n",
      "Epoch [2/2], Step [64240/64305], Loss: 4.7331\n",
      "Epoch [2/2], Step [64250/64305], Loss: 4.9041\n",
      "Epoch [2/2], Step [64260/64305], Loss: 4.6930\n",
      "Epoch [2/2], Step [64270/64305], Loss: 4.5955\n",
      "Epoch [2/2], Step [64280/64305], Loss: 4.6827\n",
      "Epoch [2/2], Step [64290/64305], Loss: 4.6908\n",
      "Epoch [2/2], Step [64300/64305], Loss: 4.8523\n",
      "Epoch [2/2] Average Loss: 4.7639, Perplexity: 117.21\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for the model with attention: 89.81\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "context_length = 32  # Increased context size\n",
    "# vocab_size = tokenizer.n_vocab\n",
    "vocab_size = len(tokenizer)\n",
    "embedding_dim = 128\n",
    "attention_dim = 64\n",
    "hidden_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_dataloader(\n",
    "    formatted_text[:9999297], batch_size=batch_size,\n",
    "    context_length=context_length, shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = LanguageModelWithAttention(\n",
    "    vocab_size, embedding_dim, attention_dim, context_length, hidden_dim, num_heads, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters'\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses_attention = []\n",
    "perplexities_attention = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(x)\n",
    "        \n",
    "        # Reshape logits and targets for loss computation\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step \"\n",
    "                  f\"[{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    train_losses_attention.append(avg_loss)\n",
    "    perplexities_attention.append(perplexity)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T16:27:44.853301Z",
     "iopub.status.busy": "2024-12-14T16:27:44.852948Z",
     "iopub.status.idle": "2024-12-14T16:28:05.358829Z",
     "shell.execute_reply": "2024-12-14T16:28:05.357760Z",
     "shell.execute_reply.started": "2024-12-14T16:27:44.853270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for the model with attention: 89.81\n",
      "4.497745903370599\n",
      "36152.88157129288\n"
     ]
    }
   ],
   "source": [
    "steps = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(steps, train_losses_attention, label='Training Loss')\n",
    "plt.title('Model with attention: Training loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps, perplexities_attention, label='Perplexity')\n",
    "plt.title('Model with attention: Perplexity over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in dev_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits, _ = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(dev_dataloader)\n",
    "perplexity_attention = math.exp(avg_loss)\n",
    "print(f\"Perplexity for the model with attention: {perplexity_attention:.2f}\")\n",
    "print(avg_loss)\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T16:00:04.025000Z",
     "iopub.status.busy": "2024-12-14T16:00:04.024608Z",
     "iopub.status.idle": "2024-12-14T16:00:25.835522Z",
     "shell.execute_reply": "2024-12-14T16:00:25.834694Z",
     "shell.execute_reply.started": "2024-12-14T16:00:04.024956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "\n",
      "[CLS] من در راه[SEP] اتفاق بیافتد و نیست جای حرکتکاران راه و اعلام کرد و پیش رو واگذار شده است 57184 حتی خوشبختانه آسمان\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, start_text, context_length=32, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = tokenizer.encode(start_text)\n",
    "    context = torch.tensor(generated, dtype=torch.long,\n",
    "                          device=device).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(context_length):\n",
    "            if context.size(1) >= context_length:\n",
    "                break\n",
    "            logits, _ = model(context)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "            context = torch.cat(\n",
    "                [context, next_token_id.unsqueeze(0)], dim=1\n",
    "            )\n",
    "    \n",
    "    generated_text = tokenizer.decode(context[0].tolist())\n",
    "    return generated_text\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "token_ids = tokenizer.encode(formatted_text)\n",
    "\n",
    "start_text = \" من در راه\"\n",
    "generated_text = generate_text(model, tokenizer, start_text, context_length=32)\n",
    "print(\"Generated Text:\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "؟؟؟  من در راه ؟؟؟ اتفاق بیافتد و نیست جای حرکتکاران راه و اعلام کرد و پیش رو واگذار شده است 57184 حتی خوشبختانه آسمان"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add instead of concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T17:30:03.218693Z",
     "iopub.status.busy": "2024-12-14T17:30:03.218061Z",
     "iopub.status.idle": "2024-12-14T17:30:03.228365Z",
     "shell.execute_reply": "2024-12-14T17:30:03.227381Z",
     "shell.execute_reply.started": "2024-12-14T17:30:03.218661Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadCausalAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, attention_dim, num_heads, context_length, dropout=0.1):\n",
    "        super(MultiHeadCausalAttention, self).__init__()\n",
    "        assert attention_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = attention_dim // num_heads\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.W_q = nn.Linear(embedding_dim, attention_dim)\n",
    "        self.W_k = nn.Linear(embedding_dim, attention_dim)\n",
    "        self.W_v = nn.Linear(embedding_dim, attention_dim)\n",
    "        self.fc_out = nn.Linear(attention_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer( 'mask',torch.tril(torch.ones(context_length, context_length)).unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # Split the embedding into self.num_heads different pieces\n",
    "        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        #print(\"Q,K,V: \", Q.shape, K.shape, V.shape)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        #print(\"scores: \", scores.shape)\n",
    "        # Apply the causal mask\n",
    "        mask = self.mask[:, :seq_length, :seq_length]\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        #print(\"attention weights, values: \", attention_weights.shape, V.shape)\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        #print(\"context: \", context.shape)\n",
    "\n",
    "        context = context.sum(dim=1)\n",
    "        context = context.repeat_interleave(self.num_heads, dim=-1)\n",
    "\n",
    "        out = self.fc_out(context)\n",
    "        return out, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-14T17:30:12.997343Z",
     "iopub.status.busy": "2024-12-14T17:30:12.996751Z",
     "iopub.status.idle": "2024-12-14T17:46:57.589676Z",
     "shell.execute_reply": "2024-12-14T17:46:57.588572Z",
     "shell.execute_reply.started": "2024-12-14T17:30:12.997311Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/2], Step [0/64305], Loss: 10.1423\n",
      "Epoch [1/2], Step [10/64305], Loss: 9.7549\n",
      "Epoch [1/2], Step [20/64305], Loss: 8.3209\n",
      "Epoch [1/2], Step [30/64305], Loss: 7.4312\n",
      "Epoch [1/2], Step [40/64305], Loss: 7.2445\n",
      "Epoch [1/2], Step [50/64305], Loss: 7.2170\n",
      "Epoch [1/2], Step [60/64305], Loss: 6.8157\n",
      "Epoch [1/2], Step [70/64305], Loss: 7.0283\n",
      "Epoch [1/2], Step [80/64305], Loss: 7.0572\n",
      "Epoch [1/2], Step [90/64305], Loss: 6.8303\n",
      "Epoch [1/2], Step [100/64305], Loss: 6.8476\n",
      "Epoch [1/2], Step [110/64305], Loss: 6.7023\n",
      "Epoch [1/2], Step [120/64305], Loss: 6.8422\n",
      "Epoch [1/2], Step [130/64305], Loss: 6.7851\n",
      "Epoch [1/2], Step [140/64305], Loss: 6.8684\n",
      "Epoch [1/2], Step [150/64305], Loss: 6.6249\n",
      "Epoch [1/2], Step [160/64305], Loss: 6.6894\n",
      "Epoch [1/2], Step [170/64305], Loss: 6.4854\n",
      "Epoch [1/2], Step [180/64305], Loss: 6.7716\n",
      "Epoch [1/2], Step [190/64305], Loss: 6.6230\n",
      "Epoch [1/2], Step [200/64305], Loss: 6.4857\n",
      "Epoch [1/2], Step [210/64305], Loss: 6.5177\n",
      "Epoch [1/2], Step [220/64305], Loss: 6.4944\n",
      "Epoch [1/2], Step [230/64305], Loss: 6.5303\n",
      "Epoch [1/2], Step [240/64305], Loss: 6.5862\n",
      "Epoch [1/2], Step [250/64305], Loss: 6.6547\n",
      "Epoch [1/2], Step [260/64305], Loss: 6.3280\n",
      "Epoch [1/2], Step [270/64305], Loss: 6.4833\n",
      "Epoch [1/2], Step [280/64305], Loss: 6.5045\n",
      "Epoch [1/2], Step [290/64305], Loss: 6.5081\n",
      "Epoch [1/2], Step [300/64305], Loss: 6.3475\n",
      "Epoch [1/2], Step [310/64305], Loss: 6.3530\n",
      "Epoch [1/2], Step [320/64305], Loss: 6.3889\n",
      "Epoch [1/2], Step [330/64305], Loss: 6.6093\n",
      "Epoch [1/2], Step [340/64305], Loss: 6.1387\n",
      "Epoch [1/2], Step [350/64305], Loss: 6.2009\n",
      "Epoch [1/2], Step [360/64305], Loss: 6.3408\n",
      "Epoch [1/2], Step [370/64305], Loss: 6.4618\n",
      "Epoch [1/2], Step [380/64305], Loss: 6.3841\n",
      "Epoch [1/2], Step [390/64305], Loss: 6.3951\n",
      "Epoch [1/2], Step [400/64305], Loss: 6.3964\n",
      "Epoch [1/2], Step [410/64305], Loss: 6.3939\n",
      "Epoch [1/2], Step [420/64305], Loss: 6.0640\n",
      "Epoch [1/2], Step [430/64305], Loss: 6.2882\n",
      "Epoch [1/2], Step [440/64305], Loss: 6.3831\n",
      "Epoch [1/2], Step [450/64305], Loss: 6.3136\n",
      "Epoch [1/2], Step [460/64305], Loss: 6.4602\n",
      "Epoch [1/2], Step [470/64305], Loss: 6.2204\n",
      "Epoch [1/2], Step [480/64305], Loss: 6.3439\n",
      "Epoch [1/2], Step [490/64305], Loss: 6.2119\n",
      "Epoch [1/2], Step [500/64305], Loss: 6.2454\n",
      "Epoch [1/2], Step [510/64305], Loss: 6.1745\n",
      "Epoch [1/2], Step [520/64305], Loss: 6.3510\n",
      "Epoch [1/2], Step [530/64305], Loss: 6.3809\n",
      "Epoch [1/2], Step [540/64305], Loss: 6.4424\n",
      "Epoch [1/2], Step [550/64305], Loss: 6.3709\n",
      "Epoch [1/2], Step [560/64305], Loss: 6.1971\n",
      "Epoch [1/2], Step [570/64305], Loss: 6.3268\n",
      "Epoch [1/2], Step [580/64305], Loss: 6.0974\n",
      "Epoch [1/2], Step [590/64305], Loss: 6.2862\n",
      "Epoch [1/2], Step [600/64305], Loss: 6.5057\n",
      "Epoch [1/2], Step [610/64305], Loss: 6.4291\n",
      "Epoch [1/2], Step [620/64305], Loss: 6.3096\n",
      "Epoch [1/2], Step [630/64305], Loss: 6.1243\n",
      "Epoch [1/2], Step [640/64305], Loss: 6.4428\n",
      "Epoch [1/2], Step [650/64305], Loss: 6.2522\n",
      "Epoch [1/2], Step [660/64305], Loss: 6.0829\n",
      "Epoch [1/2], Step [670/64305], Loss: 6.2995\n",
      "Epoch [1/2], Step [680/64305], Loss: 6.3597\n",
      "Epoch [1/2], Step [690/64305], Loss: 6.0877\n",
      "Epoch [1/2], Step [700/64305], Loss: 6.3631\n",
      "Epoch [1/2], Step [710/64305], Loss: 6.1326\n",
      "Epoch [1/2], Step [720/64305], Loss: 6.2094\n",
      "Epoch [1/2], Step [730/64305], Loss: 6.2513\n",
      "Epoch [1/2], Step [740/64305], Loss: 6.2897\n",
      "Epoch [1/2], Step [750/64305], Loss: 6.2509\n",
      "Epoch [1/2], Step [760/64305], Loss: 6.3705\n",
      "Epoch [1/2], Step [770/64305], Loss: 6.0422\n",
      "Epoch [1/2], Step [780/64305], Loss: 6.1286\n",
      "Epoch [1/2], Step [790/64305], Loss: 6.1919\n",
      "Epoch [1/2], Step [800/64305], Loss: 6.3422\n",
      "Epoch [1/2], Step [810/64305], Loss: 6.2411\n",
      "Epoch [1/2], Step [820/64305], Loss: 6.2570\n",
      "Epoch [1/2], Step [830/64305], Loss: 5.9705\n",
      "Epoch [1/2], Step [840/64305], Loss: 6.0787\n",
      "Epoch [1/2], Step [850/64305], Loss: 6.0961\n",
      "Epoch [1/2], Step [860/64305], Loss: 6.0647\n",
      "Epoch [1/2], Step [870/64305], Loss: 6.2940\n",
      "Epoch [1/2], Step [880/64305], Loss: 5.9289\n",
      "Epoch [1/2], Step [890/64305], Loss: 5.8801\n",
      "Epoch [1/2], Step [900/64305], Loss: 6.0358\n",
      "Epoch [1/2], Step [910/64305], Loss: 5.9510\n",
      "Epoch [1/2], Step [920/64305], Loss: 6.1387\n",
      "Epoch [1/2], Step [930/64305], Loss: 6.0625\n",
      "Epoch [1/2], Step [940/64305], Loss: 6.3252\n",
      "Epoch [1/2], Step [950/64305], Loss: 6.3534\n",
      "Epoch [1/2], Step [960/64305], Loss: 6.2839\n",
      "Epoch [1/2], Step [970/64305], Loss: 6.1437\n",
      "Epoch [1/2], Step [980/64305], Loss: 6.0160\n",
      "Epoch [1/2], Step [990/64305], Loss: 6.0054\n",
      "Epoch [1/2], Step [1000/64305], Loss: 6.0318\n",
      "Epoch [1/2], Step [1010/64305], Loss: 6.1162\n",
      "Epoch [1/2], Step [1020/64305], Loss: 6.0753\n",
      "Epoch [1/2], Step [1030/64305], Loss: 6.2353\n",
      "Epoch [1/2], Step [1040/64305], Loss: 5.9485\n",
      "Epoch [1/2], Step [1050/64305], Loss: 6.0666\n",
      "Epoch [1/2], Step [1060/64305], Loss: 6.0744\n",
      "Epoch [1/2], Step [1070/64305], Loss: 6.0436\n",
      "Epoch [1/2], Step [1080/64305], Loss: 6.1852\n",
      "Epoch [1/2], Step [1090/64305], Loss: 6.0929\n",
      "Epoch [1/2], Step [1100/64305], Loss: 5.8068\n",
      "Epoch [1/2], Step [1110/64305], Loss: 6.0316\n",
      "Epoch [1/2], Step [1120/64305], Loss: 6.0000\n",
      "Epoch [1/2], Step [1130/64305], Loss: 6.3305\n",
      "Epoch [1/2], Step [1140/64305], Loss: 6.0031\n",
      "Epoch [1/2], Step [1150/64305], Loss: 6.0170\n",
      "Epoch [1/2], Step [1160/64305], Loss: 6.1549\n",
      "Epoch [1/2], Step [1170/64305], Loss: 6.0004\n",
      "Epoch [1/2], Step [1180/64305], Loss: 5.9864\n",
      "Epoch [1/2], Step [1190/64305], Loss: 6.0358\n",
      "Epoch [1/2], Step [1200/64305], Loss: 5.9428\n",
      "Epoch [1/2], Step [1210/64305], Loss: 5.8457\n",
      "Epoch [1/2], Step [1220/64305], Loss: 6.0153\n",
      "Epoch [1/2], Step [1230/64305], Loss: 6.3817\n",
      "Epoch [1/2], Step [1240/64305], Loss: 5.8384\n",
      "Epoch [1/2], Step [1250/64305], Loss: 5.8442\n",
      "Epoch [1/2], Step [1260/64305], Loss: 6.1087\n",
      "Epoch [1/2], Step [1270/64305], Loss: 6.1955\n",
      "Epoch [1/2], Step [1280/64305], Loss: 6.1014\n",
      "Epoch [1/2], Step [1290/64305], Loss: 6.0445\n",
      "Epoch [1/2], Step [1300/64305], Loss: 6.2390\n",
      "Epoch [1/2], Step [1310/64305], Loss: 5.8632\n",
      "Epoch [1/2], Step [1320/64305], Loss: 5.9817\n",
      "Epoch [1/2], Step [1330/64305], Loss: 6.1711\n",
      "Epoch [1/2], Step [1340/64305], Loss: 5.9043\n",
      "Epoch [1/2], Step [1350/64305], Loss: 6.0281\n",
      "Epoch [1/2], Step [1360/64305], Loss: 6.1655\n",
      "Epoch [1/2], Step [1370/64305], Loss: 6.4001\n",
      "Epoch [1/2], Step [1380/64305], Loss: 5.9278\n",
      "Epoch [1/2], Step [1390/64305], Loss: 5.8851\n",
      "Epoch [1/2], Step [1400/64305], Loss: 6.1883\n",
      "Epoch [1/2], Step [1410/64305], Loss: 6.0254\n",
      "Epoch [1/2], Step [1420/64305], Loss: 5.9982\n",
      "Epoch [1/2], Step [1430/64305], Loss: 5.8483\n",
      "Epoch [1/2], Step [1440/64305], Loss: 6.0350\n",
      "Epoch [1/2], Step [1450/64305], Loss: 6.1656\n",
      "Epoch [1/2], Step [1460/64305], Loss: 6.0972\n",
      "Epoch [1/2], Step [1470/64305], Loss: 5.9609\n",
      "Epoch [1/2], Step [1480/64305], Loss: 5.7754\n",
      "Epoch [1/2], Step [1490/64305], Loss: 6.0040\n",
      "Epoch [1/2], Step [1500/64305], Loss: 6.1361\n",
      "Epoch [1/2], Step [1510/64305], Loss: 6.0788\n",
      "Epoch [1/2], Step [1520/64305], Loss: 5.8635\n",
      "Epoch [1/2], Step [1530/64305], Loss: 6.1409\n",
      "Epoch [1/2], Step [1540/64305], Loss: 6.0934\n",
      "Epoch [1/2], Step [1550/64305], Loss: 6.0553\n",
      "Epoch [1/2], Step [1560/64305], Loss: 5.6462\n",
      "Epoch [1/2], Step [1570/64305], Loss: 6.0475\n",
      "Epoch [1/2], Step [1580/64305], Loss: 5.8658\n",
      "Epoch [1/2], Step [1590/64305], Loss: 6.1333\n",
      "Epoch [1/2], Step [1600/64305], Loss: 5.9651\n",
      "Epoch [1/2], Step [1610/64305], Loss: 6.0075\n",
      "Epoch [1/2], Step [1620/64305], Loss: 5.9817\n",
      "Epoch [1/2], Step [1630/64305], Loss: 5.9496\n",
      "Epoch [1/2], Step [1640/64305], Loss: 5.9820\n",
      "Epoch [1/2], Step [1650/64305], Loss: 5.6783\n",
      "Epoch [1/2], Step [1660/64305], Loss: 5.9198\n",
      "Epoch [1/2], Step [1670/64305], Loss: 5.7608\n",
      "Epoch [1/2], Step [1680/64305], Loss: 5.7828\n",
      "Epoch [1/2], Step [1690/64305], Loss: 6.0083\n",
      "Epoch [1/2], Step [1700/64305], Loss: 6.0669\n",
      "Epoch [1/2], Step [1710/64305], Loss: 5.9947\n",
      "Epoch [1/2], Step [1720/64305], Loss: 5.9855\n",
      "Epoch [1/2], Step [1730/64305], Loss: 5.9715\n",
      "Epoch [1/2], Step [1740/64305], Loss: 6.1107\n",
      "Epoch [1/2], Step [1750/64305], Loss: 5.8905\n",
      "Epoch [1/2], Step [1760/64305], Loss: 6.0025\n",
      "Epoch [1/2], Step [1770/64305], Loss: 6.0441\n",
      "Epoch [1/2], Step [1780/64305], Loss: 5.9861\n",
      "Epoch [1/2], Step [1790/64305], Loss: 5.9125\n",
      "Epoch [1/2], Step [1800/64305], Loss: 5.7171\n",
      "Epoch [1/2], Step [1810/64305], Loss: 5.7164\n",
      "Epoch [1/2], Step [1820/64305], Loss: 5.8718\n",
      "Epoch [1/2], Step [1830/64305], Loss: 5.9591\n",
      "Epoch [1/2], Step [1840/64305], Loss: 5.8113\n",
      "Epoch [1/2], Step [1850/64305], Loss: 6.0486\n",
      "Epoch [1/2], Step [1860/64305], Loss: 5.7079\n",
      "Epoch [1/2], Step [1870/64305], Loss: 6.0720\n",
      "Epoch [1/2], Step [1880/64305], Loss: 5.8954\n",
      "Epoch [1/2], Step [1890/64305], Loss: 5.9338\n",
      "Epoch [1/2], Step [1900/64305], Loss: 5.9888\n",
      "Epoch [1/2], Step [1910/64305], Loss: 5.8330\n",
      "Epoch [1/2], Step [1920/64305], Loss: 6.1264\n",
      "Epoch [1/2], Step [1930/64305], Loss: 5.9068\n",
      "Epoch [1/2], Step [1940/64305], Loss: 5.8039\n",
      "Epoch [1/2], Step [1950/64305], Loss: 5.6403\n",
      "Epoch [1/2], Step [1960/64305], Loss: 6.1224\n",
      "Epoch [1/2], Step [1970/64305], Loss: 5.9217\n",
      "Epoch [1/2], Step [1980/64305], Loss: 5.8877\n",
      "Epoch [1/2], Step [1990/64305], Loss: 5.9333\n",
      "Epoch [1/2], Step [2000/64305], Loss: 5.9048\n",
      "Epoch [1/2], Step [2010/64305], Loss: 5.9615\n",
      "Epoch [1/2], Step [2020/64305], Loss: 5.8173\n",
      "Epoch [1/2], Step [2030/64305], Loss: 5.9696\n",
      "Epoch [1/2], Step [2040/64305], Loss: 5.9933\n",
      "Epoch [1/2], Step [2050/64305], Loss: 5.9897\n",
      "Epoch [1/2], Step [2060/64305], Loss: 5.9274\n",
      "Epoch [1/2], Step [2070/64305], Loss: 5.8782\n",
      "Epoch [1/2], Step [2080/64305], Loss: 5.9034\n",
      "Epoch [1/2], Step [2090/64305], Loss: 5.7735\n",
      "Epoch [1/2], Step [2100/64305], Loss: 5.7750\n",
      "Epoch [1/2], Step [2110/64305], Loss: 5.7116\n",
      "Epoch [1/2], Step [2120/64305], Loss: 5.9436\n",
      "Epoch [1/2], Step [2130/64305], Loss: 5.8816\n",
      "Epoch [1/2], Step [2140/64305], Loss: 5.8630\n",
      "Epoch [1/2], Step [2150/64305], Loss: 6.0597\n",
      "Epoch [1/2], Step [2160/64305], Loss: 6.0706\n",
      "Epoch [1/2], Step [2170/64305], Loss: 5.8465\n",
      "Epoch [1/2], Step [2180/64305], Loss: 6.0723\n",
      "Epoch [1/2], Step [2190/64305], Loss: 5.7503\n",
      "Epoch [1/2], Step [2200/64305], Loss: 5.9423\n",
      "Epoch [1/2], Step [2210/64305], Loss: 5.7960\n",
      "Epoch [1/2], Step [2220/64305], Loss: 5.9475\n",
      "Epoch [1/2], Step [2230/64305], Loss: 5.9377\n",
      "Epoch [1/2], Step [2240/64305], Loss: 5.9054\n",
      "Epoch [1/2], Step [2250/64305], Loss: 5.9284\n",
      "Epoch [1/2], Step [2260/64305], Loss: 5.6596\n",
      "Epoch [1/2], Step [2270/64305], Loss: 5.7490\n",
      "Epoch [1/2], Step [2280/64305], Loss: 5.9394\n",
      "Epoch [1/2], Step [2290/64305], Loss: 6.0518\n",
      "Epoch [1/2], Step [2300/64305], Loss: 6.0653\n",
      "Epoch [1/2], Step [2310/64305], Loss: 6.0509\n",
      "Epoch [1/2], Step [2320/64305], Loss: 6.0746\n",
      "Epoch [1/2], Step [2330/64305], Loss: 6.1497\n",
      "Epoch [1/2], Step [2340/64305], Loss: 5.8983\n",
      "Epoch [1/2], Step [2350/64305], Loss: 5.9517\n",
      "Epoch [1/2], Step [2360/64305], Loss: 6.1378\n",
      "Epoch [1/2], Step [2370/64305], Loss: 5.8106\n",
      "Epoch [1/2], Step [2380/64305], Loss: 5.8820\n",
      "Epoch [1/2], Step [2390/64305], Loss: 5.8361\n",
      "Epoch [1/2], Step [2400/64305], Loss: 5.9217\n",
      "Epoch [1/2], Step [2410/64305], Loss: 6.0285\n",
      "Epoch [1/2], Step [2420/64305], Loss: 5.8824\n",
      "Epoch [1/2], Step [2430/64305], Loss: 5.7367\n",
      "Epoch [1/2], Step [2440/64305], Loss: 5.7837\n",
      "Epoch [1/2], Step [2450/64305], Loss: 5.8126\n",
      "Epoch [1/2], Step [2460/64305], Loss: 5.8066\n",
      "Epoch [1/2], Step [2470/64305], Loss: 5.8074\n",
      "Epoch [1/2], Step [2480/64305], Loss: 5.7144\n",
      "Epoch [1/2], Step [2490/64305], Loss: 5.9784\n",
      "Epoch [1/2], Step [2500/64305], Loss: 5.7793\n",
      "Epoch [1/2], Step [2510/64305], Loss: 5.7423\n",
      "Epoch [1/2], Step [2520/64305], Loss: 5.8901\n",
      "Epoch [1/2], Step [2530/64305], Loss: 5.6819\n",
      "Epoch [1/2], Step [2540/64305], Loss: 5.7555\n",
      "Epoch [1/2], Step [2550/64305], Loss: 5.6628\n",
      "Epoch [1/2], Step [2560/64305], Loss: 5.7917\n",
      "Epoch [1/2], Step [2570/64305], Loss: 6.0309\n",
      "Epoch [1/2], Step [2580/64305], Loss: 6.0492\n",
      "Epoch [1/2], Step [2590/64305], Loss: 5.8119\n",
      "Epoch [1/2], Step [2600/64305], Loss: 5.8602\n",
      "Epoch [1/2], Step [2610/64305], Loss: 5.7700\n",
      "Epoch [1/2], Step [2620/64305], Loss: 5.8110\n",
      "Epoch [1/2], Step [2630/64305], Loss: 6.0008\n",
      "Epoch [1/2], Step [2640/64305], Loss: 5.6541\n",
      "Epoch [1/2], Step [2650/64305], Loss: 5.8783\n",
      "Epoch [1/2], Step [2660/64305], Loss: 5.8039\n",
      "Epoch [1/2], Step [2670/64305], Loss: 5.7462\n",
      "Epoch [1/2], Step [2680/64305], Loss: 5.7396\n",
      "Epoch [1/2], Step [2690/64305], Loss: 5.7698\n",
      "Epoch [1/2], Step [2700/64305], Loss: 5.9078\n",
      "Epoch [1/2], Step [2710/64305], Loss: 6.1107\n",
      "Epoch [1/2], Step [2720/64305], Loss: 5.6975\n",
      "Epoch [1/2], Step [2730/64305], Loss: 5.7808\n",
      "Epoch [1/2], Step [2740/64305], Loss: 5.7457\n",
      "Epoch [1/2], Step [2750/64305], Loss: 5.8869\n",
      "Epoch [1/2], Step [2760/64305], Loss: 5.8311\n",
      "Epoch [1/2], Step [2770/64305], Loss: 5.8729\n",
      "Epoch [1/2], Step [2780/64305], Loss: 5.7793\n",
      "Epoch [1/2], Step [2790/64305], Loss: 5.9272\n",
      "Epoch [1/2], Step [2800/64305], Loss: 6.0140\n",
      "Epoch [1/2], Step [2810/64305], Loss: 5.9159\n",
      "Epoch [1/2], Step [2820/64305], Loss: 5.9541\n",
      "Epoch [1/2], Step [2830/64305], Loss: 5.8350\n",
      "Epoch [1/2], Step [2840/64305], Loss: 5.8129\n",
      "Epoch [1/2], Step [2850/64305], Loss: 5.7611\n",
      "Epoch [1/2], Step [2860/64305], Loss: 5.7417\n",
      "Epoch [1/2], Step [2870/64305], Loss: 5.6437\n",
      "Epoch [1/2], Step [2880/64305], Loss: 5.8356\n",
      "Epoch [1/2], Step [2890/64305], Loss: 5.9008\n",
      "Epoch [1/2], Step [2900/64305], Loss: 5.9172\n",
      "Epoch [1/2], Step [2910/64305], Loss: 5.9799\n",
      "Epoch [1/2], Step [2920/64305], Loss: 5.8144\n",
      "Epoch [1/2], Step [2930/64305], Loss: 5.8163\n",
      "Epoch [1/2], Step [2940/64305], Loss: 6.0129\n",
      "Epoch [1/2], Step [2950/64305], Loss: 5.6587\n",
      "Epoch [1/2], Step [2960/64305], Loss: 5.6628\n",
      "Epoch [1/2], Step [2970/64305], Loss: 5.7124\n",
      "Epoch [1/2], Step [2980/64305], Loss: 5.7686\n",
      "Epoch [1/2], Step [2990/64305], Loss: 5.8323\n",
      "Epoch [1/2], Step [3000/64305], Loss: 5.8249\n",
      "Epoch [1/2], Step [3010/64305], Loss: 5.7759\n",
      "Epoch [1/2], Step [3020/64305], Loss: 5.8094\n",
      "Epoch [1/2], Step [3030/64305], Loss: 5.9025\n",
      "Epoch [1/2], Step [3040/64305], Loss: 5.4552\n",
      "Epoch [1/2], Step [3050/64305], Loss: 5.8657\n",
      "Epoch [1/2], Step [3060/64305], Loss: 5.8307\n",
      "Epoch [1/2], Step [3070/64305], Loss: 5.8246\n",
      "Epoch [1/2], Step [3080/64305], Loss: 5.7049\n",
      "Epoch [1/2], Step [3090/64305], Loss: 5.7125\n",
      "Epoch [1/2], Step [3100/64305], Loss: 5.8027\n",
      "Epoch [1/2], Step [3110/64305], Loss: 5.9392\n",
      "Epoch [1/2], Step [3120/64305], Loss: 5.6751\n",
      "Epoch [1/2], Step [3130/64305], Loss: 5.8264\n",
      "Epoch [1/2], Step [3140/64305], Loss: 5.8964\n",
      "Epoch [1/2], Step [3150/64305], Loss: 5.8092\n",
      "Epoch [1/2], Step [3160/64305], Loss: 5.7561\n",
      "Epoch [1/2], Step [3170/64305], Loss: 5.7402\n",
      "Epoch [1/2], Step [3180/64305], Loss: 5.7202\n",
      "Epoch [1/2], Step [3190/64305], Loss: 5.7092\n",
      "Epoch [1/2], Step [3200/64305], Loss: 5.7165\n",
      "Epoch [1/2], Step [3210/64305], Loss: 5.9263\n",
      "Epoch [1/2], Step [3220/64305], Loss: 5.8171\n",
      "Epoch [1/2], Step [3230/64305], Loss: 5.8052\n",
      "Epoch [1/2], Step [3240/64305], Loss: 5.8351\n",
      "Epoch [1/2], Step [3250/64305], Loss: 5.6490\n",
      "Epoch [1/2], Step [3260/64305], Loss: 5.8557\n",
      "Epoch [1/2], Step [3270/64305], Loss: 5.7986\n",
      "Epoch [1/2], Step [3280/64305], Loss: 5.6033\n",
      "Epoch [1/2], Step [3290/64305], Loss: 5.6010\n",
      "Epoch [1/2], Step [3300/64305], Loss: 5.6652\n",
      "Epoch [1/2], Step [3310/64305], Loss: 5.4575\n",
      "Epoch [1/2], Step [3320/64305], Loss: 5.8467\n",
      "Epoch [1/2], Step [3330/64305], Loss: 5.9621\n",
      "Epoch [1/2], Step [3340/64305], Loss: 5.8070\n",
      "Epoch [1/2], Step [3350/64305], Loss: 5.7099\n",
      "Epoch [1/2], Step [3360/64305], Loss: 5.8247\n",
      "Epoch [1/2], Step [3370/64305], Loss: 5.6667\n",
      "Epoch [1/2], Step [3380/64305], Loss: 5.6516\n",
      "Epoch [1/2], Step [3390/64305], Loss: 5.7235\n",
      "Epoch [1/2], Step [3400/64305], Loss: 5.7154\n",
      "Epoch [1/2], Step [3410/64305], Loss: 5.6326\n",
      "Epoch [1/2], Step [3420/64305], Loss: 5.6512\n",
      "Epoch [1/2], Step [3430/64305], Loss: 5.5158\n",
      "Epoch [1/2], Step [3440/64305], Loss: 5.8179\n",
      "Epoch [1/2], Step [3450/64305], Loss: 5.6510\n",
      "Epoch [1/2], Step [3460/64305], Loss: 5.7268\n",
      "Epoch [1/2], Step [3470/64305], Loss: 5.6565\n",
      "Epoch [1/2], Step [3480/64305], Loss: 5.6205\n",
      "Epoch [1/2], Step [3490/64305], Loss: 5.8543\n",
      "Epoch [1/2], Step [3500/64305], Loss: 5.7222\n",
      "Epoch [1/2], Step [3510/64305], Loss: 5.6843\n",
      "Epoch [1/2], Step [3520/64305], Loss: 5.8167\n",
      "Epoch [1/2], Step [3530/64305], Loss: 5.5193\n",
      "Epoch [1/2], Step [3540/64305], Loss: 5.7393\n",
      "Epoch [1/2], Step [3550/64305], Loss: 5.5830\n",
      "Epoch [1/2], Step [3560/64305], Loss: 5.6083\n",
      "Epoch [1/2], Step [3570/64305], Loss: 5.8007\n",
      "Epoch [1/2], Step [3580/64305], Loss: 5.7738\n",
      "Epoch [1/2], Step [3590/64305], Loss: 5.6516\n",
      "Epoch [1/2], Step [3600/64305], Loss: 5.7000\n",
      "Epoch [1/2], Step [3610/64305], Loss: 5.8065\n",
      "Epoch [1/2], Step [3620/64305], Loss: 5.8027\n",
      "Epoch [1/2], Step [3630/64305], Loss: 5.7515\n",
      "Epoch [1/2], Step [3640/64305], Loss: 5.8039\n",
      "Epoch [1/2], Step [3650/64305], Loss: 5.5917\n",
      "Epoch [1/2], Step [3660/64305], Loss: 5.5601\n",
      "Epoch [1/2], Step [3670/64305], Loss: 5.5745\n",
      "Epoch [1/2], Step [3680/64305], Loss: 5.7333\n",
      "Epoch [1/2], Step [3690/64305], Loss: 5.8588\n",
      "Epoch [1/2], Step [3700/64305], Loss: 5.6841\n",
      "Epoch [1/2], Step [3710/64305], Loss: 5.5253\n",
      "Epoch [1/2], Step [3720/64305], Loss: 5.5896\n",
      "Epoch [1/2], Step [3730/64305], Loss: 5.6683\n",
      "Epoch [1/2], Step [3740/64305], Loss: 5.5614\n",
      "Epoch [1/2], Step [3750/64305], Loss: 5.5771\n",
      "Epoch [1/2], Step [3760/64305], Loss: 5.4931\n",
      "Epoch [1/2], Step [3770/64305], Loss: 5.4695\n",
      "Epoch [1/2], Step [3780/64305], Loss: 5.7350\n",
      "Epoch [1/2], Step [3790/64305], Loss: 5.6397\n",
      "Epoch [1/2], Step [3800/64305], Loss: 5.7907\n",
      "Epoch [1/2], Step [3810/64305], Loss: 5.6472\n",
      "Epoch [1/2], Step [3820/64305], Loss: 5.7498\n",
      "Epoch [1/2], Step [3830/64305], Loss: 5.4864\n",
      "Epoch [1/2], Step [3840/64305], Loss: 5.6563\n",
      "Epoch [1/2], Step [3850/64305], Loss: 5.6062\n",
      "Epoch [1/2], Step [3860/64305], Loss: 5.7149\n",
      "Epoch [1/2], Step [3870/64305], Loss: 5.6935\n",
      "Epoch [1/2], Step [3880/64305], Loss: 5.7393\n",
      "Epoch [1/2], Step [3890/64305], Loss: 5.4277\n",
      "Epoch [1/2], Step [3900/64305], Loss: 5.6060\n",
      "Epoch [1/2], Step [3910/64305], Loss: 5.8245\n",
      "Epoch [1/2], Step [3920/64305], Loss: 5.8843\n",
      "Epoch [1/2], Step [3930/64305], Loss: 5.6199\n",
      "Epoch [1/2], Step [3940/64305], Loss: 5.5306\n",
      "Epoch [1/2], Step [3950/64305], Loss: 5.8558\n",
      "Epoch [1/2], Step [3960/64305], Loss: 5.5612\n",
      "Epoch [1/2], Step [3970/64305], Loss: 5.7923\n",
      "Epoch [1/2], Step [3980/64305], Loss: 5.7018\n",
      "Epoch [1/2], Step [3990/64305], Loss: 5.4717\n",
      "Epoch [1/2], Step [4000/64305], Loss: 5.6412\n",
      "Epoch [1/2], Step [4010/64305], Loss: 5.6496\n",
      "Epoch [1/2], Step [4020/64305], Loss: 5.7309\n",
      "Epoch [1/2], Step [4030/64305], Loss: 5.6956\n",
      "Epoch [1/2], Step [4040/64305], Loss: 5.3760\n",
      "Epoch [1/2], Step [4050/64305], Loss: 5.5933\n",
      "Epoch [1/2], Step [4060/64305], Loss: 5.6334\n",
      "Epoch [1/2], Step [4070/64305], Loss: 5.5596\n",
      "Epoch [1/2], Step [4080/64305], Loss: 5.8044\n",
      "Epoch [1/2], Step [4090/64305], Loss: 5.6672\n",
      "Epoch [1/2], Step [4100/64305], Loss: 5.6776\n",
      "Epoch [1/2], Step [4110/64305], Loss: 5.5704\n",
      "Epoch [1/2], Step [4120/64305], Loss: 5.8709\n",
      "Epoch [1/2], Step [4130/64305], Loss: 5.5465\n",
      "Epoch [1/2], Step [4140/64305], Loss: 5.5655\n",
      "Epoch [1/2], Step [4150/64305], Loss: 5.6623\n",
      "Epoch [1/2], Step [4160/64305], Loss: 5.5433\n",
      "Epoch [1/2], Step [4170/64305], Loss: 5.7529\n",
      "Epoch [1/2], Step [4180/64305], Loss: 5.7885\n",
      "Epoch [1/2], Step [4190/64305], Loss: 5.5233\n",
      "Epoch [1/2], Step [4200/64305], Loss: 5.7433\n",
      "Epoch [1/2], Step [4210/64305], Loss: 5.6606\n",
      "Epoch [1/2], Step [4220/64305], Loss: 5.5263\n",
      "Epoch [1/2], Step [4230/64305], Loss: 5.6950\n",
      "Epoch [1/2], Step [4240/64305], Loss: 5.6797\n",
      "Epoch [1/2], Step [4250/64305], Loss: 5.7062\n",
      "Epoch [1/2], Step [4260/64305], Loss: 5.6399\n",
      "Epoch [1/2], Step [4270/64305], Loss: 5.6225\n",
      "Epoch [1/2], Step [4280/64305], Loss: 5.5078\n",
      "Epoch [1/2], Step [4290/64305], Loss: 5.5126\n",
      "Epoch [1/2], Step [4300/64305], Loss: 5.6395\n",
      "Epoch [1/2], Step [4310/64305], Loss: 5.5733\n",
      "Epoch [1/2], Step [4320/64305], Loss: 5.4195\n",
      "Epoch [1/2], Step [4330/64305], Loss: 5.5906\n",
      "Epoch [1/2], Step [4340/64305], Loss: 5.6822\n",
      "Epoch [1/2], Step [4350/64305], Loss: 5.7139\n",
      "Epoch [1/2], Step [4360/64305], Loss: 5.8582\n",
      "Epoch [1/2], Step [4370/64305], Loss: 5.3742\n",
      "Epoch [1/2], Step [4380/64305], Loss: 5.6798\n",
      "Epoch [1/2], Step [4390/64305], Loss: 5.8935\n",
      "Epoch [1/2], Step [4400/64305], Loss: 5.6282\n",
      "Epoch [1/2], Step [4410/64305], Loss: 5.5819\n",
      "Epoch [1/2], Step [4420/64305], Loss: 5.7547\n",
      "Epoch [1/2], Step [4430/64305], Loss: 5.5558\n",
      "Epoch [1/2], Step [4440/64305], Loss: 5.6956\n",
      "Epoch [1/2], Step [4450/64305], Loss: 5.5665\n",
      "Epoch [1/2], Step [4460/64305], Loss: 5.5913\n",
      "Epoch [1/2], Step [4470/64305], Loss: 5.4916\n",
      "Epoch [1/2], Step [4480/64305], Loss: 5.6041\n",
      "Epoch [1/2], Step [4490/64305], Loss: 5.6097\n",
      "Epoch [1/2], Step [4500/64305], Loss: 5.5308\n",
      "Epoch [1/2], Step [4510/64305], Loss: 5.7249\n",
      "Epoch [1/2], Step [4520/64305], Loss: 5.6388\n",
      "Epoch [1/2], Step [4530/64305], Loss: 5.9451\n",
      "Epoch [1/2], Step [4540/64305], Loss: 5.5367\n",
      "Epoch [1/2], Step [4550/64305], Loss: 5.5847\n",
      "Epoch [1/2], Step [4560/64305], Loss: 5.6075\n",
      "Epoch [1/2], Step [4570/64305], Loss: 5.7341\n",
      "Epoch [1/2], Step [4580/64305], Loss: 5.6352\n",
      "Epoch [1/2], Step [4590/64305], Loss: 5.7111\n",
      "Epoch [1/2], Step [4600/64305], Loss: 5.9420\n",
      "Epoch [1/2], Step [4610/64305], Loss: 5.7187\n",
      "Epoch [1/2], Step [4620/64305], Loss: 5.7550\n",
      "Epoch [1/2], Step [4630/64305], Loss: 5.7216\n",
      "Epoch [1/2], Step [4640/64305], Loss: 5.5616\n",
      "Epoch [1/2], Step [4650/64305], Loss: 5.4091\n",
      "Epoch [1/2], Step [4660/64305], Loss: 5.8294\n",
      "Epoch [1/2], Step [4670/64305], Loss: 5.5733\n",
      "Epoch [1/2], Step [4680/64305], Loss: 5.6252\n",
      "Epoch [1/2], Step [4690/64305], Loss: 5.6751\n",
      "Epoch [1/2], Step [4700/64305], Loss: 5.7321\n",
      "Epoch [1/2], Step [4710/64305], Loss: 5.5335\n",
      "Epoch [1/2], Step [4720/64305], Loss: 5.5051\n",
      "Epoch [1/2], Step [4730/64305], Loss: 5.7161\n",
      "Epoch [1/2], Step [4740/64305], Loss: 5.6251\n",
      "Epoch [1/2], Step [4750/64305], Loss: 5.5367\n",
      "Epoch [1/2], Step [4760/64305], Loss: 5.6235\n",
      "Epoch [1/2], Step [4770/64305], Loss: 5.8251\n",
      "Epoch [1/2], Step [4780/64305], Loss: 5.6391\n",
      "Epoch [1/2], Step [4790/64305], Loss: 5.5651\n",
      "Epoch [1/2], Step [4800/64305], Loss: 5.7391\n",
      "Epoch [1/2], Step [4810/64305], Loss: 5.5134\n",
      "Epoch [1/2], Step [4820/64305], Loss: 5.5499\n",
      "Epoch [1/2], Step [4830/64305], Loss: 5.7109\n",
      "Epoch [1/2], Step [4840/64305], Loss: 5.7122\n",
      "Epoch [1/2], Step [4850/64305], Loss: 5.6681\n",
      "Epoch [1/2], Step [4860/64305], Loss: 5.7786\n",
      "Epoch [1/2], Step [4870/64305], Loss: 5.7112\n",
      "Epoch [1/2], Step [4880/64305], Loss: 5.6915\n",
      "Epoch [1/2], Step [4890/64305], Loss: 5.8712\n",
      "Epoch [1/2], Step [4900/64305], Loss: 5.6376\n",
      "Epoch [1/2], Step [4910/64305], Loss: 5.3668\n",
      "Epoch [1/2], Step [4920/64305], Loss: 5.4351\n",
      "Epoch [1/2], Step [4930/64305], Loss: 5.5627\n",
      "Epoch [1/2], Step [4940/64305], Loss: 5.5688\n",
      "Epoch [1/2], Step [4950/64305], Loss: 5.7098\n",
      "Epoch [1/2], Step [4960/64305], Loss: 5.3044\n",
      "Epoch [1/2], Step [4970/64305], Loss: 5.5547\n",
      "Epoch [1/2], Step [4980/64305], Loss: 5.4652\n",
      "Epoch [1/2], Step [4990/64305], Loss: 5.5789\n",
      "Epoch [1/2], Step [5000/64305], Loss: 5.4378\n",
      "Epoch [1/2], Step [5010/64305], Loss: 5.6680\n",
      "Epoch [1/2], Step [5020/64305], Loss: 5.6765\n",
      "Epoch [1/2], Step [5030/64305], Loss: 5.7799\n",
      "Epoch [1/2], Step [5040/64305], Loss: 5.4624\n",
      "Epoch [1/2], Step [5050/64305], Loss: 5.5301\n",
      "Epoch [1/2], Step [5060/64305], Loss: 5.5314\n",
      "Epoch [1/2], Step [5070/64305], Loss: 5.6939\n",
      "Epoch [1/2], Step [5080/64305], Loss: 5.4898\n",
      "Epoch [1/2], Step [5090/64305], Loss: 5.5055\n",
      "Epoch [1/2], Step [5100/64305], Loss: 5.5354\n",
      "Epoch [1/2], Step [5110/64305], Loss: 5.4681\n",
      "Epoch [1/2], Step [5120/64305], Loss: 5.4209\n",
      "Epoch [1/2], Step [5130/64305], Loss: 5.4216\n",
      "Epoch [1/2], Step [5140/64305], Loss: 5.3925\n",
      "Epoch [1/2], Step [5150/64305], Loss: 5.5749\n",
      "Epoch [1/2], Step [5160/64305], Loss: 5.5635\n",
      "Epoch [1/2], Step [5170/64305], Loss: 5.4553\n",
      "Epoch [1/2], Step [5180/64305], Loss: 5.6396\n",
      "Epoch [1/2], Step [5190/64305], Loss: 5.5352\n",
      "Epoch [1/2], Step [5200/64305], Loss: 5.4749\n",
      "Epoch [1/2], Step [5210/64305], Loss: 5.4735\n",
      "Epoch [1/2], Step [5220/64305], Loss: 5.5029\n",
      "Epoch [1/2], Step [5230/64305], Loss: 5.6410\n",
      "Epoch [1/2], Step [5240/64305], Loss: 5.5086\n",
      "Epoch [1/2], Step [5250/64305], Loss: 5.3007\n",
      "Epoch [1/2], Step [5260/64305], Loss: 5.5200\n",
      "Epoch [1/2], Step [5270/64305], Loss: 5.6501\n",
      "Epoch [1/2], Step [5280/64305], Loss: 5.6466\n",
      "Epoch [1/2], Step [5290/64305], Loss: 5.6883\n",
      "Epoch [1/2], Step [5300/64305], Loss: 5.6363\n",
      "Epoch [1/2], Step [5310/64305], Loss: 5.3120\n",
      "Epoch [1/2], Step [5320/64305], Loss: 5.4775\n",
      "Epoch [1/2], Step [5330/64305], Loss: 5.6263\n",
      "Epoch [1/2], Step [5340/64305], Loss: 5.5899\n",
      "Epoch [1/2], Step [5350/64305], Loss: 5.4987\n",
      "Epoch [1/2], Step [5360/64305], Loss: 5.5686\n",
      "Epoch [1/2], Step [5370/64305], Loss: 5.4895\n",
      "Epoch [1/2], Step [5380/64305], Loss: 5.6935\n",
      "Epoch [1/2], Step [5390/64305], Loss: 5.3993\n",
      "Epoch [1/2], Step [5400/64305], Loss: 5.5390\n",
      "Epoch [1/2], Step [5410/64305], Loss: 5.6974\n",
      "Epoch [1/2], Step [5420/64305], Loss: 5.7140\n",
      "Epoch [1/2], Step [5430/64305], Loss: 5.6708\n",
      "Epoch [1/2], Step [5440/64305], Loss: 5.6505\n",
      "Epoch [1/2], Step [5450/64305], Loss: 5.4735\n",
      "Epoch [1/2], Step [5460/64305], Loss: 5.4526\n",
      "Epoch [1/2], Step [5470/64305], Loss: 5.7950\n",
      "Epoch [1/2], Step [5480/64305], Loss: 5.5410\n",
      "Epoch [1/2], Step [5490/64305], Loss: 5.5313\n",
      "Epoch [1/2], Step [5500/64305], Loss: 5.6296\n",
      "Epoch [1/2], Step [5510/64305], Loss: 5.6146\n",
      "Epoch [1/2], Step [5520/64305], Loss: 5.5092\n",
      "Epoch [1/2], Step [5530/64305], Loss: 5.6550\n",
      "Epoch [1/2], Step [5540/64305], Loss: 5.4885\n",
      "Epoch [1/2], Step [5550/64305], Loss: 5.9827\n",
      "Epoch [1/2], Step [5560/64305], Loss: 5.5958\n",
      "Epoch [1/2], Step [5570/64305], Loss: 5.3367\n",
      "Epoch [1/2], Step [5580/64305], Loss: 5.6617\n",
      "Epoch [1/2], Step [5590/64305], Loss: 5.5513\n",
      "Epoch [1/2], Step [5600/64305], Loss: 5.6460\n",
      "Epoch [1/2], Step [5610/64305], Loss: 5.5178\n",
      "Epoch [1/2], Step [5620/64305], Loss: 5.4999\n",
      "Epoch [1/2], Step [5630/64305], Loss: 5.5489\n",
      "Epoch [1/2], Step [5640/64305], Loss: 5.6889\n",
      "Epoch [1/2], Step [5650/64305], Loss: 5.5854\n",
      "Epoch [1/2], Step [5660/64305], Loss: 5.5503\n",
      "Epoch [1/2], Step [5670/64305], Loss: 5.3205\n",
      "Epoch [1/2], Step [5680/64305], Loss: 5.6401\n",
      "Epoch [1/2], Step [5690/64305], Loss: 5.4489\n",
      "Epoch [1/2], Step [5700/64305], Loss: 5.4584\n",
      "Epoch [1/2], Step [5710/64305], Loss: 5.6247\n",
      "Epoch [1/2], Step [5720/64305], Loss: 5.4136\n",
      "Epoch [1/2], Step [5730/64305], Loss: 5.5520\n",
      "Epoch [1/2], Step [5740/64305], Loss: 5.5323\n",
      "Epoch [1/2], Step [5750/64305], Loss: 5.4805\n",
      "Epoch [1/2], Step [5760/64305], Loss: 5.6522\n",
      "Epoch [1/2], Step [5770/64305], Loss: 5.5784\n",
      "Epoch [1/2], Step [5780/64305], Loss: 5.4846\n",
      "Epoch [1/2], Step [5790/64305], Loss: 5.5631\n",
      "Epoch [1/2], Step [5800/64305], Loss: 5.6775\n",
      "Epoch [1/2], Step [5810/64305], Loss: 5.1933\n",
      "Epoch [1/2], Step [5820/64305], Loss: 5.4902\n",
      "Epoch [1/2], Step [5830/64305], Loss: 5.5363\n",
      "Epoch [1/2], Step [5840/64305], Loss: 5.3635\n",
      "Epoch [1/2], Step [5850/64305], Loss: 5.3080\n",
      "Epoch [1/2], Step [5860/64305], Loss: 5.3790\n",
      "Epoch [1/2], Step [5870/64305], Loss: 5.6886\n",
      "Epoch [1/2], Step [5880/64305], Loss: 5.5732\n",
      "Epoch [1/2], Step [5890/64305], Loss: 5.5477\n",
      "Epoch [1/2], Step [5900/64305], Loss: 5.4153\n",
      "Epoch [1/2], Step [5910/64305], Loss: 5.4207\n",
      "Epoch [1/2], Step [5920/64305], Loss: 5.5475\n",
      "Epoch [1/2], Step [5930/64305], Loss: 5.3203\n",
      "Epoch [1/2], Step [5940/64305], Loss: 5.4981\n",
      "Epoch [1/2], Step [5950/64305], Loss: 5.5873\n",
      "Epoch [1/2], Step [5960/64305], Loss: 5.4872\n",
      "Epoch [1/2], Step [5970/64305], Loss: 5.6508\n",
      "Epoch [1/2], Step [5980/64305], Loss: 5.6893\n",
      "Epoch [1/2], Step [5990/64305], Loss: 5.6267\n",
      "Epoch [1/2], Step [6000/64305], Loss: 5.7785\n",
      "Epoch [1/2], Step [6010/64305], Loss: 5.4030\n",
      "Epoch [1/2], Step [6020/64305], Loss: 5.4810\n",
      "Epoch [1/2], Step [6030/64305], Loss: 5.3302\n",
      "Epoch [1/2], Step [6040/64305], Loss: 5.9217\n",
      "Epoch [1/2], Step [6050/64305], Loss: 5.4429\n",
      "Epoch [1/2], Step [6060/64305], Loss: 5.6944\n",
      "Epoch [1/2], Step [6070/64305], Loss: 5.5339\n",
      "Epoch [1/2], Step [6080/64305], Loss: 5.4095\n",
      "Epoch [1/2], Step [6090/64305], Loss: 5.5180\n",
      "Epoch [1/2], Step [6100/64305], Loss: 5.4803\n",
      "Epoch [1/2], Step [6110/64305], Loss: 5.2892\n",
      "Epoch [1/2], Step [6120/64305], Loss: 5.7900\n",
      "Epoch [1/2], Step [6130/64305], Loss: 5.6327\n",
      "Epoch [1/2], Step [6140/64305], Loss: 5.5573\n",
      "Epoch [1/2], Step [6150/64305], Loss: 5.7247\n",
      "Epoch [1/2], Step [6160/64305], Loss: 5.5464\n",
      "Epoch [1/2], Step [6170/64305], Loss: 5.5147\n",
      "Epoch [1/2], Step [6180/64305], Loss: 5.6221\n",
      "Epoch [1/2], Step [6190/64305], Loss: 5.6254\n",
      "Epoch [1/2], Step [6200/64305], Loss: 5.6104\n",
      "Epoch [1/2], Step [6210/64305], Loss: 5.5069\n",
      "Epoch [1/2], Step [6220/64305], Loss: 5.6496\n",
      "Epoch [1/2], Step [6230/64305], Loss: 5.2735\n",
      "Epoch [1/2], Step [6240/64305], Loss: 5.5732\n",
      "Epoch [1/2], Step [6250/64305], Loss: 5.7243\n",
      "Epoch [1/2], Step [6260/64305], Loss: 5.5251\n",
      "Epoch [1/2], Step [6270/64305], Loss: 5.4750\n",
      "Epoch [1/2], Step [6280/64305], Loss: 5.5639\n",
      "Epoch [1/2], Step [6290/64305], Loss: 5.5499\n",
      "Epoch [1/2], Step [6300/64305], Loss: 5.4484\n",
      "Epoch [1/2], Step [6310/64305], Loss: 5.4754\n",
      "Epoch [1/2], Step [6320/64305], Loss: 5.5085\n",
      "Epoch [1/2], Step [6330/64305], Loss: 5.4108\n",
      "Epoch [1/2], Step [6340/64305], Loss: 5.5733\n",
      "Epoch [1/2], Step [6350/64305], Loss: 5.5932\n",
      "Epoch [1/2], Step [6360/64305], Loss: 5.3440\n",
      "Epoch [1/2], Step [6370/64305], Loss: 5.4772\n",
      "Epoch [1/2], Step [6380/64305], Loss: 5.4084\n",
      "Epoch [1/2], Step [6390/64305], Loss: 5.4396\n",
      "Epoch [1/2], Step [6400/64305], Loss: 5.3081\n",
      "Epoch [1/2], Step [6410/64305], Loss: 5.4612\n",
      "Epoch [1/2], Step [6420/64305], Loss: 5.4437\n",
      "Epoch [1/2], Step [6430/64305], Loss: 5.5385\n",
      "Epoch [1/2], Step [6440/64305], Loss: 5.6015\n",
      "Epoch [1/2], Step [6450/64305], Loss: 5.5153\n",
      "Epoch [1/2], Step [6460/64305], Loss: 5.4677\n",
      "Epoch [1/2], Step [6470/64305], Loss: 5.5628\n",
      "Epoch [1/2], Step [6480/64305], Loss: 5.6325\n",
      "Epoch [1/2], Step [6490/64305], Loss: 5.2420\n",
      "Epoch [1/2], Step [6500/64305], Loss: 5.3908\n",
      "Epoch [1/2], Step [6510/64305], Loss: 5.7244\n",
      "Epoch [1/2], Step [6520/64305], Loss: 5.4310\n",
      "Epoch [1/2], Step [6530/64305], Loss: 5.5760\n",
      "Epoch [1/2], Step [6540/64305], Loss: 5.3866\n",
      "Epoch [1/2], Step [6550/64305], Loss: 5.5730\n",
      "Epoch [1/2], Step [6560/64305], Loss: 5.6683\n",
      "Epoch [1/2], Step [6570/64305], Loss: 5.6792\n",
      "Epoch [1/2], Step [6580/64305], Loss: 5.5531\n",
      "Epoch [1/2], Step [6590/64305], Loss: 5.6384\n",
      "Epoch [1/2], Step [6600/64305], Loss: 5.5158\n",
      "Epoch [1/2], Step [6610/64305], Loss: 5.5549\n",
      "Epoch [1/2], Step [6620/64305], Loss: 5.4562\n",
      "Epoch [1/2], Step [6630/64305], Loss: 5.5679\n",
      "Epoch [1/2], Step [6640/64305], Loss: 5.4371\n",
      "Epoch [1/2], Step [6650/64305], Loss: 5.6426\n",
      "Epoch [1/2], Step [6660/64305], Loss: 5.2843\n",
      "Epoch [1/2], Step [6670/64305], Loss: 5.5891\n",
      "Epoch [1/2], Step [6680/64305], Loss: 5.4624\n",
      "Epoch [1/2], Step [6690/64305], Loss: 5.4380\n",
      "Epoch [1/2], Step [6700/64305], Loss: 5.4359\n",
      "Epoch [1/2], Step [6710/64305], Loss: 5.5768\n",
      "Epoch [1/2], Step [6720/64305], Loss: 5.2476\n",
      "Epoch [1/2], Step [6730/64305], Loss: 5.3790\n",
      "Epoch [1/2], Step [6740/64305], Loss: 5.6189\n",
      "Epoch [1/2], Step [6750/64305], Loss: 5.4070\n",
      "Epoch [1/2], Step [6760/64305], Loss: 5.5983\n",
      "Epoch [1/2], Step [6770/64305], Loss: 5.4744\n",
      "Epoch [1/2], Step [6780/64305], Loss: 5.5020\n",
      "Epoch [1/2], Step [6790/64305], Loss: 5.5299\n",
      "Epoch [1/2], Step [6800/64305], Loss: 5.3617\n",
      "Epoch [1/2], Step [6810/64305], Loss: 5.2928\n",
      "Epoch [1/2], Step [6820/64305], Loss: 5.3421\n",
      "Epoch [1/2], Step [6830/64305], Loss: 5.6120\n",
      "Epoch [1/2], Step [6840/64305], Loss: 5.6939\n",
      "Epoch [1/2], Step [6850/64305], Loss: 5.5248\n",
      "Epoch [1/2], Step [6860/64305], Loss: 5.6749\n",
      "Epoch [1/2], Step [6870/64305], Loss: 5.5817\n",
      "Epoch [1/2], Step [6880/64305], Loss: 5.4469\n",
      "Epoch [1/2], Step [6890/64305], Loss: 5.5274\n",
      "Epoch [1/2], Step [6900/64305], Loss: 5.4379\n",
      "Epoch [1/2], Step [6910/64305], Loss: 5.3278\n",
      "Epoch [1/2], Step [6920/64305], Loss: 5.2731\n",
      "Epoch [1/2], Step [6930/64305], Loss: 5.5295\n",
      "Epoch [1/2], Step [6940/64305], Loss: 5.3941\n",
      "Epoch [1/2], Step [6950/64305], Loss: 5.5369\n",
      "Epoch [1/2], Step [6960/64305], Loss: 5.5487\n",
      "Epoch [1/2], Step [6970/64305], Loss: 5.1076\n",
      "Epoch [1/2], Step [6980/64305], Loss: 5.3426\n",
      "Epoch [1/2], Step [6990/64305], Loss: 5.7188\n",
      "Epoch [1/2], Step [7000/64305], Loss: 5.3256\n",
      "Epoch [1/2], Step [7010/64305], Loss: 5.3335\n",
      "Epoch [1/2], Step [7020/64305], Loss: 5.4734\n",
      "Epoch [1/2], Step [7030/64305], Loss: 5.4245\n",
      "Epoch [1/2], Step [7040/64305], Loss: 5.3842\n",
      "Epoch [1/2], Step [7050/64305], Loss: 5.3384\n",
      "Epoch [1/2], Step [7060/64305], Loss: 5.5811\n",
      "Epoch [1/2], Step [7070/64305], Loss: 5.3381\n",
      "Epoch [1/2], Step [7080/64305], Loss: 5.3351\n",
      "Epoch [1/2], Step [7090/64305], Loss: 5.6197\n",
      "Epoch [1/2], Step [7100/64305], Loss: 5.2341\n",
      "Epoch [1/2], Step [7110/64305], Loss: 5.4104\n",
      "Epoch [1/2], Step [7120/64305], Loss: 5.5685\n",
      "Epoch [1/2], Step [7130/64305], Loss: 5.6849\n",
      "Epoch [1/2], Step [7140/64305], Loss: 5.5798\n",
      "Epoch [1/2], Step [7150/64305], Loss: 5.2765\n",
      "Epoch [1/2], Step [7160/64305], Loss: 5.5091\n",
      "Epoch [1/2], Step [7170/64305], Loss: 5.5815\n",
      "Epoch [1/2], Step [7180/64305], Loss: 5.5878\n",
      "Epoch [1/2], Step [7190/64305], Loss: 5.3936\n",
      "Epoch [1/2], Step [7200/64305], Loss: 5.3660\n",
      "Epoch [1/2], Step [7210/64305], Loss: 5.4261\n",
      "Epoch [1/2], Step [7220/64305], Loss: 5.4445\n",
      "Epoch [1/2], Step [7230/64305], Loss: 5.1370\n",
      "Epoch [1/2], Step [7240/64305], Loss: 5.5814\n",
      "Epoch [1/2], Step [7250/64305], Loss: 5.6528\n",
      "Epoch [1/2], Step [7260/64305], Loss: 5.6402\n",
      "Epoch [1/2], Step [7270/64305], Loss: 5.5765\n",
      "Epoch [1/2], Step [7280/64305], Loss: 5.5338\n",
      "Epoch [1/2], Step [7290/64305], Loss: 5.3565\n",
      "Epoch [1/2], Step [7300/64305], Loss: 5.2052\n",
      "Epoch [1/2], Step [7310/64305], Loss: 5.2760\n",
      "Epoch [1/2], Step [7320/64305], Loss: 5.5082\n",
      "Epoch [1/2], Step [7330/64305], Loss: 5.3406\n",
      "Epoch [1/2], Step [7340/64305], Loss: 5.2754\n",
      "Epoch [1/2], Step [7350/64305], Loss: 5.3393\n",
      "Epoch [1/2], Step [7360/64305], Loss: 5.3718\n",
      "Epoch [1/2], Step [7370/64305], Loss: 5.5427\n",
      "Epoch [1/2], Step [7380/64305], Loss: 5.7415\n",
      "Epoch [1/2], Step [7390/64305], Loss: 5.4900\n",
      "Epoch [1/2], Step [7400/64305], Loss: 5.4954\n",
      "Epoch [1/2], Step [7410/64305], Loss: 5.7193\n",
      "Epoch [1/2], Step [7420/64305], Loss: 5.2716\n",
      "Epoch [1/2], Step [7430/64305], Loss: 5.4987\n",
      "Epoch [1/2], Step [7440/64305], Loss: 5.4607\n",
      "Epoch [1/2], Step [7450/64305], Loss: 5.3477\n",
      "Epoch [1/2], Step [7460/64305], Loss: 5.3301\n",
      "Epoch [1/2], Step [7470/64305], Loss: 5.3486\n",
      "Epoch [1/2], Step [7480/64305], Loss: 5.3595\n",
      "Epoch [1/2], Step [7490/64305], Loss: 5.4806\n",
      "Epoch [1/2], Step [7500/64305], Loss: 5.6249\n",
      "Epoch [1/2], Step [7510/64305], Loss: 5.5663\n",
      "Epoch [1/2], Step [7520/64305], Loss: 5.3259\n",
      "Epoch [1/2], Step [7530/64305], Loss: 5.5503\n",
      "Epoch [1/2], Step [7540/64305], Loss: 5.2988\n",
      "Epoch [1/2], Step [7550/64305], Loss: 5.6139\n",
      "Epoch [1/2], Step [7560/64305], Loss: 5.5968\n",
      "Epoch [1/2], Step [7570/64305], Loss: 5.4721\n",
      "Epoch [1/2], Step [7580/64305], Loss: 5.6362\n",
      "Epoch [1/2], Step [7590/64305], Loss: 5.4358\n",
      "Epoch [1/2], Step [7600/64305], Loss: 5.6907\n",
      "Epoch [1/2], Step [7610/64305], Loss: 5.6965\n",
      "Epoch [1/2], Step [7620/64305], Loss: 5.5913\n",
      "Epoch [1/2], Step [7630/64305], Loss: 5.4444\n",
      "Epoch [1/2], Step [7640/64305], Loss: 5.3927\n",
      "Epoch [1/2], Step [7650/64305], Loss: 5.2979\n",
      "Epoch [1/2], Step [7660/64305], Loss: 5.4186\n",
      "Epoch [1/2], Step [7670/64305], Loss: 5.4265\n",
      "Epoch [1/2], Step [7680/64305], Loss: 5.4078\n",
      "Epoch [1/2], Step [7690/64305], Loss: 5.2887\n",
      "Epoch [1/2], Step [7700/64305], Loss: 5.5790\n",
      "Epoch [1/2], Step [7710/64305], Loss: 5.4726\n",
      "Epoch [1/2], Step [7720/64305], Loss: 5.4383\n",
      "Epoch [1/2], Step [7730/64305], Loss: 5.4099\n",
      "Epoch [1/2], Step [7740/64305], Loss: 5.3833\n",
      "Epoch [1/2], Step [7750/64305], Loss: 5.7827\n",
      "Epoch [1/2], Step [7760/64305], Loss: 5.3379\n",
      "Epoch [1/2], Step [7770/64305], Loss: 5.3025\n",
      "Epoch [1/2], Step [7780/64305], Loss: 5.5941\n",
      "Epoch [1/2], Step [7790/64305], Loss: 5.1713\n",
      "Epoch [1/2], Step [7800/64305], Loss: 5.6312\n",
      "Epoch [1/2], Step [7810/64305], Loss: 5.2927\n",
      "Epoch [1/2], Step [7820/64305], Loss: 5.3033\n",
      "Epoch [1/2], Step [7830/64305], Loss: 5.2447\n",
      "Epoch [1/2], Step [7840/64305], Loss: 5.4857\n",
      "Epoch [1/2], Step [7850/64305], Loss: 5.3874\n",
      "Epoch [1/2], Step [7860/64305], Loss: 5.4384\n",
      "Epoch [1/2], Step [7870/64305], Loss: 5.4837\n",
      "Epoch [1/2], Step [7880/64305], Loss: 5.2434\n",
      "Epoch [1/2], Step [7890/64305], Loss: 5.2263\n",
      "Epoch [1/2], Step [7900/64305], Loss: 5.5101\n",
      "Epoch [1/2], Step [7910/64305], Loss: 5.3197\n",
      "Epoch [1/2], Step [7920/64305], Loss: 5.3067\n",
      "Epoch [1/2], Step [7930/64305], Loss: 5.3949\n",
      "Epoch [1/2], Step [7940/64305], Loss: 5.2860\n",
      "Epoch [1/2], Step [7950/64305], Loss: 5.3794\n",
      "Epoch [1/2], Step [7960/64305], Loss: 5.4655\n",
      "Epoch [1/2], Step [7970/64305], Loss: 5.2287\n",
      "Epoch [1/2], Step [7980/64305], Loss: 5.3671\n",
      "Epoch [1/2], Step [7990/64305], Loss: 5.5539\n",
      "Epoch [1/2], Step [8000/64305], Loss: 5.3106\n",
      "Epoch [1/2], Step [8010/64305], Loss: 5.5284\n",
      "Epoch [1/2], Step [8020/64305], Loss: 5.2381\n",
      "Epoch [1/2], Step [8030/64305], Loss: 5.4737\n",
      "Epoch [1/2], Step [8040/64305], Loss: 5.5455\n",
      "Epoch [1/2], Step [8050/64305], Loss: 5.1554\n",
      "Epoch [1/2], Step [8060/64305], Loss: 5.5076\n",
      "Epoch [1/2], Step [8070/64305], Loss: 5.4827\n",
      "Epoch [1/2], Step [8080/64305], Loss: 5.2587\n",
      "Epoch [1/2], Step [8090/64305], Loss: 5.3646\n",
      "Epoch [1/2], Step [8100/64305], Loss: 5.4949\n",
      "Epoch [1/2], Step [8110/64305], Loss: 5.3745\n",
      "Epoch [1/2], Step [8120/64305], Loss: 5.4568\n",
      "Epoch [1/2], Step [8130/64305], Loss: 5.3528\n",
      "Epoch [1/2], Step [8140/64305], Loss: 5.4666\n",
      "Epoch [1/2], Step [8150/64305], Loss: 5.3992\n",
      "Epoch [1/2], Step [8160/64305], Loss: 5.4159\n",
      "Epoch [1/2], Step [8170/64305], Loss: 5.2231\n",
      "Epoch [1/2], Step [8180/64305], Loss: 5.4881\n",
      "Epoch [1/2], Step [8190/64305], Loss: 5.4043\n",
      "Epoch [1/2], Step [8200/64305], Loss: 5.3169\n",
      "Epoch [1/2], Step [8210/64305], Loss: 5.4699\n",
      "Epoch [1/2], Step [8220/64305], Loss: 5.3312\n",
      "Epoch [1/2], Step [8230/64305], Loss: 5.5069\n",
      "Epoch [1/2], Step [8240/64305], Loss: 5.2738\n",
      "Epoch [1/2], Step [8250/64305], Loss: 5.2716\n",
      "Epoch [1/2], Step [8260/64305], Loss: 5.5959\n",
      "Epoch [1/2], Step [8270/64305], Loss: 5.4909\n",
      "Epoch [1/2], Step [8280/64305], Loss: 5.2468\n",
      "Epoch [1/2], Step [8290/64305], Loss: 5.3055\n",
      "Epoch [1/2], Step [8300/64305], Loss: 5.4540\n",
      "Epoch [1/2], Step [8310/64305], Loss: 5.4545\n",
      "Epoch [1/2], Step [8320/64305], Loss: 5.6268\n",
      "Epoch [1/2], Step [8330/64305], Loss: 5.2555\n",
      "Epoch [1/2], Step [8340/64305], Loss: 5.3926\n",
      "Epoch [1/2], Step [8350/64305], Loss: 5.5652\n",
      "Epoch [1/2], Step [8360/64305], Loss: 5.4913\n",
      "Epoch [1/2], Step [8370/64305], Loss: 5.3342\n",
      "Epoch [1/2], Step [8380/64305], Loss: 5.5179\n",
      "Epoch [1/2], Step [8390/64305], Loss: 5.4251\n",
      "Epoch [1/2], Step [8400/64305], Loss: 5.6035\n",
      "Epoch [1/2], Step [8410/64305], Loss: 5.3724\n",
      "Epoch [1/2], Step [8420/64305], Loss: 5.6084\n",
      "Epoch [1/2], Step [8430/64305], Loss: 5.1696\n",
      "Epoch [1/2], Step [8440/64305], Loss: 5.3469\n",
      "Epoch [1/2], Step [8450/64305], Loss: 5.2976\n",
      "Epoch [1/2], Step [8460/64305], Loss: 5.2813\n",
      "Epoch [1/2], Step [8470/64305], Loss: 5.4904\n",
      "Epoch [1/2], Step [8480/64305], Loss: 5.2133\n",
      "Epoch [1/2], Step [8490/64305], Loss: 5.5601\n",
      "Epoch [1/2], Step [8500/64305], Loss: 5.5081\n",
      "Epoch [1/2], Step [8510/64305], Loss: 5.4693\n",
      "Epoch [1/2], Step [8520/64305], Loss: 5.4803\n",
      "Epoch [1/2], Step [8530/64305], Loss: 5.6471\n",
      "Epoch [1/2], Step [8540/64305], Loss: 5.4062\n",
      "Epoch [1/2], Step [8550/64305], Loss: 5.2447\n",
      "Epoch [1/2], Step [8560/64305], Loss: 5.3997\n",
      "Epoch [1/2], Step [8570/64305], Loss: 5.3742\n",
      "Epoch [1/2], Step [8580/64305], Loss: 5.4597\n",
      "Epoch [1/2], Step [8590/64305], Loss: 5.3925\n",
      "Epoch [1/2], Step [8600/64305], Loss: 5.5514\n",
      "Epoch [1/2], Step [8610/64305], Loss: 5.2915\n",
      "Epoch [1/2], Step [8620/64305], Loss: 5.3604\n",
      "Epoch [1/2], Step [8630/64305], Loss: 5.4110\n",
      "Epoch [1/2], Step [8640/64305], Loss: 5.2839\n",
      "Epoch [1/2], Step [8650/64305], Loss: 5.3759\n",
      "Epoch [1/2], Step [8660/64305], Loss: 5.4233\n",
      "Epoch [1/2], Step [8670/64305], Loss: 5.3289\n",
      "Epoch [1/2], Step [8680/64305], Loss: 5.5673\n",
      "Epoch [1/2], Step [8690/64305], Loss: 5.4160\n",
      "Epoch [1/2], Step [8700/64305], Loss: 5.4418\n",
      "Epoch [1/2], Step [8710/64305], Loss: 5.4565\n",
      "Epoch [1/2], Step [8720/64305], Loss: 5.5894\n",
      "Epoch [1/2], Step [8730/64305], Loss: 5.5404\n",
      "Epoch [1/2], Step [8740/64305], Loss: 5.3675\n",
      "Epoch [1/2], Step [8750/64305], Loss: 5.4705\n",
      "Epoch [1/2], Step [8760/64305], Loss: 5.4451\n",
      "Epoch [1/2], Step [8770/64305], Loss: 5.3949\n",
      "Epoch [1/2], Step [8780/64305], Loss: 5.2257\n",
      "Epoch [1/2], Step [8790/64305], Loss: 5.4935\n",
      "Epoch [1/2], Step [8800/64305], Loss: 5.3366\n",
      "Epoch [1/2], Step [8810/64305], Loss: 5.4562\n",
      "Epoch [1/2], Step [8820/64305], Loss: 5.2356\n",
      "Epoch [1/2], Step [8830/64305], Loss: 5.2767\n",
      "Epoch [1/2], Step [8840/64305], Loss: 5.4542\n",
      "Epoch [1/2], Step [8850/64305], Loss: 5.4486\n",
      "Epoch [1/2], Step [8860/64305], Loss: 5.5462\n",
      "Epoch [1/2], Step [8870/64305], Loss: 5.4340\n",
      "Epoch [1/2], Step [8880/64305], Loss: 5.4727\n",
      "Epoch [1/2], Step [8890/64305], Loss: 5.2606\n",
      "Epoch [1/2], Step [8900/64305], Loss: 5.5447\n",
      "Epoch [1/2], Step [8910/64305], Loss: 5.4179\n",
      "Epoch [1/2], Step [8920/64305], Loss: 5.3389\n",
      "Epoch [1/2], Step [8930/64305], Loss: 5.3070\n",
      "Epoch [1/2], Step [8940/64305], Loss: 5.3262\n",
      "Epoch [1/2], Step [8950/64305], Loss: 5.2493\n",
      "Epoch [1/2], Step [8960/64305], Loss: 5.7392\n",
      "Epoch [1/2], Step [8970/64305], Loss: 5.4006\n",
      "Epoch [1/2], Step [8980/64305], Loss: 5.4563\n",
      "Epoch [1/2], Step [8990/64305], Loss: 5.4313\n",
      "Epoch [1/2], Step [9000/64305], Loss: 5.0493\n",
      "Epoch [1/2], Step [9010/64305], Loss: 5.5950\n",
      "Epoch [1/2], Step [9020/64305], Loss: 5.2492\n",
      "Epoch [1/2], Step [9030/64305], Loss: 5.2603\n",
      "Epoch [1/2], Step [9040/64305], Loss: 5.4314\n",
      "Epoch [1/2], Step [9050/64305], Loss: 5.3841\n",
      "Epoch [1/2], Step [9060/64305], Loss: 5.3537\n",
      "Epoch [1/2], Step [9070/64305], Loss: 5.3998\n",
      "Epoch [1/2], Step [9080/64305], Loss: 5.6981\n",
      "Epoch [1/2], Step [9090/64305], Loss: 5.1984\n",
      "Epoch [1/2], Step [9100/64305], Loss: 5.4079\n",
      "Epoch [1/2], Step [9110/64305], Loss: 5.4317\n",
      "Epoch [1/2], Step [9120/64305], Loss: 5.4028\n",
      "Epoch [1/2], Step [9130/64305], Loss: 5.3000\n",
      "Epoch [1/2], Step [9140/64305], Loss: 5.2554\n",
      "Epoch [1/2], Step [9150/64305], Loss: 5.2730\n",
      "Epoch [1/2], Step [9160/64305], Loss: 5.3992\n",
      "Epoch [1/2], Step [9170/64305], Loss: 5.4004\n",
      "Epoch [1/2], Step [9180/64305], Loss: 5.4796\n",
      "Epoch [1/2], Step [9190/64305], Loss: 5.3297\n",
      "Epoch [1/2], Step [9200/64305], Loss: 5.4773\n",
      "Epoch [1/2], Step [9210/64305], Loss: 5.3536\n",
      "Epoch [1/2], Step [9220/64305], Loss: 5.5851\n",
      "Epoch [1/2], Step [9230/64305], Loss: 5.5510\n",
      "Epoch [1/2], Step [9240/64305], Loss: 5.5332\n",
      "Epoch [1/2], Step [9250/64305], Loss: 5.2219\n",
      "Epoch [1/2], Step [9260/64305], Loss: 5.1693\n",
      "Epoch [1/2], Step [9270/64305], Loss: 5.3626\n",
      "Epoch [1/2], Step [9280/64305], Loss: 5.3280\n",
      "Epoch [1/2], Step [9290/64305], Loss: 5.4498\n",
      "Epoch [1/2], Step [9300/64305], Loss: 5.2720\n",
      "Epoch [1/2], Step [9310/64305], Loss: 5.3329\n",
      "Epoch [1/2], Step [9320/64305], Loss: 5.2227\n",
      "Epoch [1/2], Step [9330/64305], Loss: 5.5588\n",
      "Epoch [1/2], Step [9340/64305], Loss: 5.4861\n",
      "Epoch [1/2], Step [9350/64305], Loss: 5.1481\n",
      "Epoch [1/2], Step [9360/64305], Loss: 5.2914\n",
      "Epoch [1/2], Step [9370/64305], Loss: 5.3903\n",
      "Epoch [1/2], Step [9380/64305], Loss: 5.4975\n",
      "Epoch [1/2], Step [9390/64305], Loss: 5.2206\n",
      "Epoch [1/2], Step [9400/64305], Loss: 5.3561\n",
      "Epoch [1/2], Step [9410/64305], Loss: 5.2334\n",
      "Epoch [1/2], Step [9420/64305], Loss: 5.4309\n",
      "Epoch [1/2], Step [9430/64305], Loss: 5.2123\n",
      "Epoch [1/2], Step [9440/64305], Loss: 5.2427\n",
      "Epoch [1/2], Step [9450/64305], Loss: 5.3237\n",
      "Epoch [1/2], Step [9460/64305], Loss: 5.3312\n",
      "Epoch [1/2], Step [9470/64305], Loss: 5.1765\n",
      "Epoch [1/2], Step [9480/64305], Loss: 5.4343\n",
      "Epoch [1/2], Step [9490/64305], Loss: 5.1336\n",
      "Epoch [1/2], Step [9500/64305], Loss: 5.4721\n",
      "Epoch [1/2], Step [9510/64305], Loss: 5.3372\n",
      "Epoch [1/2], Step [9520/64305], Loss: 5.5703\n",
      "Epoch [1/2], Step [9530/64305], Loss: 5.2913\n",
      "Epoch [1/2], Step [9540/64305], Loss: 5.3593\n",
      "Epoch [1/2], Step [9550/64305], Loss: 5.4989\n",
      "Epoch [1/2], Step [9560/64305], Loss: 5.2870\n",
      "Epoch [1/2], Step [9570/64305], Loss: 5.4439\n",
      "Epoch [1/2], Step [9580/64305], Loss: 5.1783\n",
      "Epoch [1/2], Step [9590/64305], Loss: 5.4705\n",
      "Epoch [1/2], Step [9600/64305], Loss: 5.1424\n",
      "Epoch [1/2], Step [9610/64305], Loss: 5.3339\n",
      "Epoch [1/2], Step [9620/64305], Loss: 5.3951\n",
      "Epoch [1/2], Step [9630/64305], Loss: 5.2811\n",
      "Epoch [1/2], Step [9640/64305], Loss: 5.1447\n",
      "Epoch [1/2], Step [9650/64305], Loss: 5.2902\n",
      "Epoch [1/2], Step [9660/64305], Loss: 5.2503\n",
      "Epoch [1/2], Step [9670/64305], Loss: 5.2996\n",
      "Epoch [1/2], Step [9680/64305], Loss: 5.3568\n",
      "Epoch [1/2], Step [9690/64305], Loss: 5.5297\n",
      "Epoch [1/2], Step [9700/64305], Loss: 5.5224\n",
      "Epoch [1/2], Step [9710/64305], Loss: 5.2979\n",
      "Epoch [1/2], Step [9720/64305], Loss: 5.4654\n",
      "Epoch [1/2], Step [9730/64305], Loss: 5.3289\n",
      "Epoch [1/2], Step [9740/64305], Loss: 5.3311\n",
      "Epoch [1/2], Step [9750/64305], Loss: 5.4230\n",
      "Epoch [1/2], Step [9760/64305], Loss: 5.3283\n",
      "Epoch [1/2], Step [9770/64305], Loss: 5.7100\n",
      "Epoch [1/2], Step [9780/64305], Loss: 5.2524\n",
      "Epoch [1/2], Step [9790/64305], Loss: 5.4753\n",
      "Epoch [1/2], Step [9800/64305], Loss: 5.1578\n",
      "Epoch [1/2], Step [9810/64305], Loss: 5.2846\n",
      "Epoch [1/2], Step [9820/64305], Loss: 5.4700\n",
      "Epoch [1/2], Step [9830/64305], Loss: 5.0986\n",
      "Epoch [1/2], Step [9840/64305], Loss: 5.3513\n",
      "Epoch [1/2], Step [9850/64305], Loss: 5.4531\n",
      "Epoch [1/2], Step [9860/64305], Loss: 5.3458\n",
      "Epoch [1/2], Step [9870/64305], Loss: 5.4136\n",
      "Epoch [1/2], Step [9880/64305], Loss: 5.4161\n",
      "Epoch [1/2], Step [9890/64305], Loss: 5.4440\n",
      "Epoch [1/2], Step [9900/64305], Loss: 5.5170\n",
      "Epoch [1/2], Step [9910/64305], Loss: 5.4700\n",
      "Epoch [1/2], Step [9920/64305], Loss: 5.3219\n",
      "Epoch [1/2], Step [9930/64305], Loss: 5.1808\n",
      "Epoch [1/2], Step [9940/64305], Loss: 5.5081\n",
      "Epoch [1/2], Step [9950/64305], Loss: 5.3050\n",
      "Epoch [1/2], Step [9960/64305], Loss: 5.3519\n",
      "Epoch [1/2], Step [9970/64305], Loss: 5.4182\n",
      "Epoch [1/2], Step [9980/64305], Loss: 5.2330\n",
      "Epoch [1/2], Step [9990/64305], Loss: 5.2657\n",
      "Epoch [1/2], Step [10000/64305], Loss: 5.4566\n",
      "Epoch [1/2], Step [10010/64305], Loss: 5.3225\n",
      "Epoch [1/2], Step [10020/64305], Loss: 5.3376\n",
      "Epoch [1/2], Step [10030/64305], Loss: 5.5610\n",
      "Epoch [1/2], Step [10040/64305], Loss: 5.4120\n",
      "Epoch [1/2], Step [10050/64305], Loss: 5.2699\n",
      "Epoch [1/2], Step [10060/64305], Loss: 5.2621\n",
      "Epoch [1/2], Step [10070/64305], Loss: 5.2097\n",
      "Epoch [1/2], Step [10080/64305], Loss: 5.1505\n",
      "Epoch [1/2], Step [10090/64305], Loss: 5.2992\n",
      "Epoch [1/2], Step [10100/64305], Loss: 5.1732\n",
      "Epoch [1/2], Step [10110/64305], Loss: 5.3015\n",
      "Epoch [1/2], Step [10120/64305], Loss: 5.3959\n",
      "Epoch [1/2], Step [10130/64305], Loss: 5.3493\n",
      "Epoch [1/2], Step [10140/64305], Loss: 5.2753\n",
      "Epoch [1/2], Step [10150/64305], Loss: 5.5404\n",
      "Epoch [1/2], Step [10160/64305], Loss: 5.1921\n",
      "Epoch [1/2], Step [10170/64305], Loss: 5.4679\n",
      "Epoch [1/2], Step [10180/64305], Loss: 5.1787\n",
      "Epoch [1/2], Step [10190/64305], Loss: 5.1452\n",
      "Epoch [1/2], Step [10200/64305], Loss: 5.6567\n",
      "Epoch [1/2], Step [10210/64305], Loss: 5.3244\n",
      "Epoch [1/2], Step [10220/64305], Loss: 5.5273\n",
      "Epoch [1/2], Step [10230/64305], Loss: 5.4206\n",
      "Epoch [1/2], Step [10240/64305], Loss: 5.2548\n",
      "Epoch [1/2], Step [10250/64305], Loss: 5.0862\n",
      "Epoch [1/2], Step [10260/64305], Loss: 5.1961\n",
      "Epoch [1/2], Step [10270/64305], Loss: 5.4726\n",
      "Epoch [1/2], Step [10280/64305], Loss: 5.4256\n",
      "Epoch [1/2], Step [10290/64305], Loss: 5.2989\n",
      "Epoch [1/2], Step [10300/64305], Loss: 5.2546\n",
      "Epoch [1/2], Step [10310/64305], Loss: 5.3503\n",
      "Epoch [1/2], Step [10320/64305], Loss: 5.3023\n",
      "Epoch [1/2], Step [10330/64305], Loss: 5.1278\n",
      "Epoch [1/2], Step [10340/64305], Loss: 5.1958\n",
      "Epoch [1/2], Step [10350/64305], Loss: 5.4016\n",
      "Epoch [1/2], Step [10360/64305], Loss: 5.3730\n",
      "Epoch [1/2], Step [10370/64305], Loss: 5.2126\n",
      "Epoch [1/2], Step [10380/64305], Loss: 5.3510\n",
      "Epoch [1/2], Step [10390/64305], Loss: 5.3972\n",
      "Epoch [1/2], Step [10400/64305], Loss: 5.2306\n",
      "Epoch [1/2], Step [10410/64305], Loss: 5.2134\n",
      "Epoch [1/2], Step [10420/64305], Loss: 5.2928\n",
      "Epoch [1/2], Step [10430/64305], Loss: 5.3823\n",
      "Epoch [1/2], Step [10440/64305], Loss: 5.4173\n",
      "Epoch [1/2], Step [10450/64305], Loss: 5.2769\n",
      "Epoch [1/2], Step [10460/64305], Loss: 5.1266\n",
      "Epoch [1/2], Step [10470/64305], Loss: 5.3175\n",
      "Epoch [1/2], Step [10480/64305], Loss: 5.3815\n",
      "Epoch [1/2], Step [10490/64305], Loss: 5.2196\n",
      "Epoch [1/2], Step [10500/64305], Loss: 5.1763\n",
      "Epoch [1/2], Step [10510/64305], Loss: 5.2887\n",
      "Epoch [1/2], Step [10520/64305], Loss: 5.2391\n",
      "Epoch [1/2], Step [10530/64305], Loss: 5.5270\n",
      "Epoch [1/2], Step [10540/64305], Loss: 5.1995\n",
      "Epoch [1/2], Step [10550/64305], Loss: 5.2796\n",
      "Epoch [1/2], Step [10560/64305], Loss: 5.3286\n",
      "Epoch [1/2], Step [10570/64305], Loss: 5.5475\n",
      "Epoch [1/2], Step [10580/64305], Loss: 5.3206\n",
      "Epoch [1/2], Step [10590/64305], Loss: 5.5414\n",
      "Epoch [1/2], Step [10600/64305], Loss: 5.2905\n",
      "Epoch [1/2], Step [10610/64305], Loss: 5.1721\n",
      "Epoch [1/2], Step [10620/64305], Loss: 5.3877\n",
      "Epoch [1/2], Step [10630/64305], Loss: 5.4691\n",
      "Epoch [1/2], Step [10640/64305], Loss: 5.1438\n",
      "Epoch [1/2], Step [10650/64305], Loss: 5.2531\n",
      "Epoch [1/2], Step [10660/64305], Loss: 5.3567\n",
      "Epoch [1/2], Step [10670/64305], Loss: 5.6163\n",
      "Epoch [1/2], Step [10680/64305], Loss: 5.3343\n",
      "Epoch [1/2], Step [10690/64305], Loss: 5.2704\n",
      "Epoch [1/2], Step [10700/64305], Loss: 5.4310\n",
      "Epoch [1/2], Step [10710/64305], Loss: 5.3735\n",
      "Epoch [1/2], Step [10720/64305], Loss: 5.1829\n",
      "Epoch [1/2], Step [10730/64305], Loss: 5.3903\n",
      "Epoch [1/2], Step [10740/64305], Loss: 5.2773\n",
      "Epoch [1/2], Step [10750/64305], Loss: 5.5203\n",
      "Epoch [1/2], Step [10760/64305], Loss: 5.3775\n",
      "Epoch [1/2], Step [10770/64305], Loss: 5.0909\n",
      "Epoch [1/2], Step [10780/64305], Loss: 5.2890\n",
      "Epoch [1/2], Step [10790/64305], Loss: 5.4817\n",
      "Epoch [1/2], Step [10800/64305], Loss: 5.1904\n",
      "Epoch [1/2], Step [10810/64305], Loss: 5.5049\n",
      "Epoch [1/2], Step [10820/64305], Loss: 5.1112\n",
      "Epoch [1/2], Step [10830/64305], Loss: 5.2050\n",
      "Epoch [1/2], Step [10840/64305], Loss: 5.2560\n",
      "Epoch [1/2], Step [10850/64305], Loss: 5.1240\n",
      "Epoch [1/2], Step [10860/64305], Loss: 5.1554\n",
      "Epoch [1/2], Step [10870/64305], Loss: 5.2776\n",
      "Epoch [1/2], Step [10880/64305], Loss: 5.3247\n",
      "Epoch [1/2], Step [10890/64305], Loss: 5.1997\n",
      "Epoch [1/2], Step [10900/64305], Loss: 5.2586\n",
      "Epoch [1/2], Step [10910/64305], Loss: 5.2457\n",
      "Epoch [1/2], Step [10920/64305], Loss: 5.4683\n",
      "Epoch [1/2], Step [10930/64305], Loss: 5.2118\n",
      "Epoch [1/2], Step [10940/64305], Loss: 5.1534\n",
      "Epoch [1/2], Step [10950/64305], Loss: 5.4144\n",
      "Epoch [1/2], Step [10960/64305], Loss: 5.3965\n",
      "Epoch [1/2], Step [10970/64305], Loss: 5.2726\n",
      "Epoch [1/2], Step [10980/64305], Loss: 5.3818\n",
      "Epoch [1/2], Step [10990/64305], Loss: 5.5103\n",
      "Epoch [1/2], Step [11000/64305], Loss: 5.2639\n",
      "Epoch [1/2], Step [11010/64305], Loss: 5.5197\n",
      "Epoch [1/2], Step [11020/64305], Loss: 5.4493\n",
      "Epoch [1/2], Step [11030/64305], Loss: 5.6032\n",
      "Epoch [1/2], Step [11040/64305], Loss: 5.4347\n",
      "Epoch [1/2], Step [11050/64305], Loss: 5.2044\n",
      "Epoch [1/2], Step [11060/64305], Loss: 5.1964\n",
      "Epoch [1/2], Step [11070/64305], Loss: 5.4430\n",
      "Epoch [1/2], Step [11080/64305], Loss: 5.2141\n",
      "Epoch [1/2], Step [11090/64305], Loss: 5.3967\n",
      "Epoch [1/2], Step [11100/64305], Loss: 5.1753\n",
      "Epoch [1/2], Step [11110/64305], Loss: 5.3443\n",
      "Epoch [1/2], Step [11120/64305], Loss: 5.2428\n",
      "Epoch [1/2], Step [11130/64305], Loss: 5.6081\n",
      "Epoch [1/2], Step [11140/64305], Loss: 5.3881\n",
      "Epoch [1/2], Step [11150/64305], Loss: 5.1062\n",
      "Epoch [1/2], Step [11160/64305], Loss: 5.1818\n",
      "Epoch [1/2], Step [11170/64305], Loss: 5.2663\n",
      "Epoch [1/2], Step [11180/64305], Loss: 5.3079\n",
      "Epoch [1/2], Step [11190/64305], Loss: 5.2996\n",
      "Epoch [1/2], Step [11200/64305], Loss: 5.2698\n",
      "Epoch [1/2], Step [11210/64305], Loss: 5.4070\n",
      "Epoch [1/2], Step [11220/64305], Loss: 5.0251\n",
      "Epoch [1/2], Step [11230/64305], Loss: 5.0953\n",
      "Epoch [1/2], Step [11240/64305], Loss: 5.2625\n",
      "Epoch [1/2], Step [11250/64305], Loss: 5.3918\n",
      "Epoch [1/2], Step [11260/64305], Loss: 5.5073\n",
      "Epoch [1/2], Step [11270/64305], Loss: 5.1252\n",
      "Epoch [1/2], Step [11280/64305], Loss: 5.3326\n",
      "Epoch [1/2], Step [11290/64305], Loss: 5.3533\n",
      "Epoch [1/2], Step [11300/64305], Loss: 5.4072\n",
      "Epoch [1/2], Step [11310/64305], Loss: 5.4789\n",
      "Epoch [1/2], Step [11320/64305], Loss: 5.5854\n",
      "Epoch [1/2], Step [11330/64305], Loss: 5.4620\n",
      "Epoch [1/2], Step [11340/64305], Loss: 5.3983\n",
      "Epoch [1/2], Step [11350/64305], Loss: 5.1424\n",
      "Epoch [1/2], Step [11360/64305], Loss: 5.3870\n",
      "Epoch [1/2], Step [11370/64305], Loss: 5.3269\n",
      "Epoch [1/2], Step [11380/64305], Loss: 5.6016\n",
      "Epoch [1/2], Step [11390/64305], Loss: 5.0809\n",
      "Epoch [1/2], Step [11400/64305], Loss: 5.2519\n",
      "Epoch [1/2], Step [11410/64305], Loss: 5.1410\n",
      "Epoch [1/2], Step [11420/64305], Loss: 5.3602\n",
      "Epoch [1/2], Step [11430/64305], Loss: 5.1445\n",
      "Epoch [1/2], Step [11440/64305], Loss: 5.3333\n",
      "Epoch [1/2], Step [11450/64305], Loss: 5.4118\n",
      "Epoch [1/2], Step [11460/64305], Loss: 5.3229\n",
      "Epoch [1/2], Step [11470/64305], Loss: 5.1809\n",
      "Epoch [1/2], Step [11480/64305], Loss: 5.2828\n",
      "Epoch [1/2], Step [11490/64305], Loss: 5.4233\n",
      "Epoch [1/2], Step [11500/64305], Loss: 5.2848\n",
      "Epoch [1/2], Step [11510/64305], Loss: 5.1758\n",
      "Epoch [1/2], Step [11520/64305], Loss: 5.5133\n",
      "Epoch [1/2], Step [11530/64305], Loss: 5.4132\n",
      "Epoch [1/2], Step [11540/64305], Loss: 5.3261\n",
      "Epoch [1/2], Step [11550/64305], Loss: 5.4196\n",
      "Epoch [1/2], Step [11560/64305], Loss: 5.3566\n",
      "Epoch [1/2], Step [11570/64305], Loss: 5.2922\n",
      "Epoch [1/2], Step [11580/64305], Loss: 5.3268\n",
      "Epoch [1/2], Step [11590/64305], Loss: 5.2147\n",
      "Epoch [1/2], Step [11600/64305], Loss: 5.2655\n",
      "Epoch [1/2], Step [11610/64305], Loss: 5.1258\n",
      "Epoch [1/2], Step [11620/64305], Loss: 5.4012\n",
      "Epoch [1/2], Step [11630/64305], Loss: 5.2188\n",
      "Epoch [1/2], Step [11640/64305], Loss: 5.2883\n",
      "Epoch [1/2], Step [11650/64305], Loss: 5.2327\n",
      "Epoch [1/2], Step [11660/64305], Loss: 5.4428\n",
      "Epoch [1/2], Step [11670/64305], Loss: 5.3153\n",
      "Epoch [1/2], Step [11680/64305], Loss: 5.3529\n",
      "Epoch [1/2], Step [11690/64305], Loss: 5.5276\n",
      "Epoch [1/2], Step [11700/64305], Loss: 5.3632\n",
      "Epoch [1/2], Step [11710/64305], Loss: 5.2669\n",
      "Epoch [1/2], Step [11720/64305], Loss: 5.3747\n",
      "Epoch [1/2], Step [11730/64305], Loss: 5.2255\n",
      "Epoch [1/2], Step [11740/64305], Loss: 5.1301\n",
      "Epoch [1/2], Step [11750/64305], Loss: 5.3160\n",
      "Epoch [1/2], Step [11760/64305], Loss: 5.2692\n",
      "Epoch [1/2], Step [11770/64305], Loss: 5.5050\n",
      "Epoch [1/2], Step [11780/64305], Loss: 5.3167\n",
      "Epoch [1/2], Step [11790/64305], Loss: 5.4128\n",
      "Epoch [1/2], Step [11800/64305], Loss: 5.0391\n",
      "Epoch [1/2], Step [11810/64305], Loss: 5.2081\n",
      "Epoch [1/2], Step [11820/64305], Loss: 5.3453\n",
      "Epoch [1/2], Step [11830/64305], Loss: 5.3471\n",
      "Epoch [1/2], Step [11840/64305], Loss: 5.3096\n",
      "Epoch [1/2], Step [11850/64305], Loss: 5.3486\n",
      "Epoch [1/2], Step [11860/64305], Loss: 5.2477\n",
      "Epoch [1/2], Step [11870/64305], Loss: 5.4241\n",
      "Epoch [1/2], Step [11880/64305], Loss: 5.2201\n",
      "Epoch [1/2], Step [11890/64305], Loss: 5.3456\n",
      "Epoch [1/2], Step [11900/64305], Loss: 5.4202\n",
      "Epoch [1/2], Step [11910/64305], Loss: 5.1879\n",
      "Epoch [1/2], Step [11920/64305], Loss: 5.2435\n",
      "Epoch [1/2], Step [11930/64305], Loss: 5.3484\n",
      "Epoch [1/2], Step [11940/64305], Loss: 5.2584\n",
      "Epoch [1/2], Step [11950/64305], Loss: 5.0148\n",
      "Epoch [1/2], Step [11960/64305], Loss: 5.0515\n",
      "Epoch [1/2], Step [11970/64305], Loss: 5.2543\n",
      "Epoch [1/2], Step [11980/64305], Loss: 5.1898\n",
      "Epoch [1/2], Step [11990/64305], Loss: 5.2883\n",
      "Epoch [1/2], Step [12000/64305], Loss: 5.4108\n",
      "Epoch [1/2], Step [12010/64305], Loss: 5.5154\n",
      "Epoch [1/2], Step [12020/64305], Loss: 5.4464\n",
      "Epoch [1/2], Step [12030/64305], Loss: 4.9711\n",
      "Epoch [1/2], Step [12040/64305], Loss: 5.1066\n",
      "Epoch [1/2], Step [12050/64305], Loss: 5.2619\n",
      "Epoch [1/2], Step [12060/64305], Loss: 5.1243\n",
      "Epoch [1/2], Step [12070/64305], Loss: 5.3928\n",
      "Epoch [1/2], Step [12080/64305], Loss: 5.0572\n",
      "Epoch [1/2], Step [12090/64305], Loss: 5.3737\n",
      "Epoch [1/2], Step [12100/64305], Loss: 5.3322\n",
      "Epoch [1/2], Step [12110/64305], Loss: 5.2270\n",
      "Epoch [1/2], Step [12120/64305], Loss: 5.1376\n",
      "Epoch [1/2], Step [12130/64305], Loss: 5.3808\n",
      "Epoch [1/2], Step [12140/64305], Loss: 5.1895\n",
      "Epoch [1/2], Step [12150/64305], Loss: 5.2712\n",
      "Epoch [1/2], Step [12160/64305], Loss: 5.2936\n",
      "Epoch [1/2], Step [12170/64305], Loss: 5.2393\n",
      "Epoch [1/2], Step [12180/64305], Loss: 5.1518\n",
      "Epoch [1/2], Step [12190/64305], Loss: 5.3471\n",
      "Epoch [1/2], Step [12200/64305], Loss: 5.3123\n",
      "Epoch [1/2], Step [12210/64305], Loss: 5.1012\n",
      "Epoch [1/2], Step [12220/64305], Loss: 5.1703\n",
      "Epoch [1/2], Step [12230/64305], Loss: 5.2255\n",
      "Epoch [1/2], Step [12240/64305], Loss: 5.3016\n",
      "Epoch [1/2], Step [12250/64305], Loss: 5.2733\n",
      "Epoch [1/2], Step [12260/64305], Loss: 5.3217\n",
      "Epoch [1/2], Step [12270/64305], Loss: 5.1663\n",
      "Epoch [1/2], Step [12280/64305], Loss: 5.2813\n",
      "Epoch [1/2], Step [12290/64305], Loss: 5.2490\n",
      "Epoch [1/2], Step [12300/64305], Loss: 5.3150\n",
      "Epoch [1/2], Step [12310/64305], Loss: 5.1804\n",
      "Epoch [1/2], Step [12320/64305], Loss: 5.3312\n",
      "Epoch [1/2], Step [12330/64305], Loss: 5.3351\n",
      "Epoch [1/2], Step [12340/64305], Loss: 5.3613\n",
      "Epoch [1/2], Step [12350/64305], Loss: 5.1988\n",
      "Epoch [1/2], Step [12360/64305], Loss: 5.2836\n",
      "Epoch [1/2], Step [12370/64305], Loss: 5.3033\n",
      "Epoch [1/2], Step [12380/64305], Loss: 5.2298\n",
      "Epoch [1/2], Step [12390/64305], Loss: 5.3353\n",
      "Epoch [1/2], Step [12400/64305], Loss: 5.3722\n",
      "Epoch [1/2], Step [12410/64305], Loss: 5.3276\n",
      "Epoch [1/2], Step [12420/64305], Loss: 5.2237\n",
      "Epoch [1/2], Step [12430/64305], Loss: 5.3281\n",
      "Epoch [1/2], Step [12440/64305], Loss: 5.4975\n",
      "Epoch [1/2], Step [12450/64305], Loss: 5.3357\n",
      "Epoch [1/2], Step [12460/64305], Loss: 5.3305\n",
      "Epoch [1/2], Step [12470/64305], Loss: 5.1799\n",
      "Epoch [1/2], Step [12480/64305], Loss: 5.3904\n",
      "Epoch [1/2], Step [12490/64305], Loss: 5.3959\n",
      "Epoch [1/2], Step [12500/64305], Loss: 5.2294\n",
      "Epoch [1/2], Step [12510/64305], Loss: 5.1132\n",
      "Epoch [1/2], Step [12520/64305], Loss: 5.4708\n",
      "Epoch [1/2], Step [12530/64305], Loss: 5.1512\n",
      "Epoch [1/2], Step [12540/64305], Loss: 5.3485\n",
      "Epoch [1/2], Step [12550/64305], Loss: 5.0403\n",
      "Epoch [1/2], Step [12560/64305], Loss: 5.2377\n",
      "Epoch [1/2], Step [12570/64305], Loss: 5.0694\n",
      "Epoch [1/2], Step [12580/64305], Loss: 5.2540\n",
      "Epoch [1/2], Step [12590/64305], Loss: 5.3161\n",
      "Epoch [1/2], Step [12600/64305], Loss: 5.2739\n",
      "Epoch [1/2], Step [12610/64305], Loss: 5.1886\n",
      "Epoch [1/2], Step [12620/64305], Loss: 5.2640\n",
      "Epoch [1/2], Step [12630/64305], Loss: 5.2413\n",
      "Epoch [1/2], Step [12640/64305], Loss: 5.4138\n",
      "Epoch [1/2], Step [12650/64305], Loss: 5.3570\n",
      "Epoch [1/2], Step [12660/64305], Loss: 5.1916\n",
      "Epoch [1/2], Step [12670/64305], Loss: 5.4181\n",
      "Epoch [1/2], Step [12680/64305], Loss: 5.3834\n",
      "Epoch [1/2], Step [12690/64305], Loss: 5.4575\n",
      "Epoch [1/2], Step [12700/64305], Loss: 5.4745\n",
      "Epoch [1/2], Step [12710/64305], Loss: 5.2447\n",
      "Epoch [1/2], Step [12720/64305], Loss: 5.4663\n",
      "Epoch [1/2], Step [12730/64305], Loss: 5.1886\n",
      "Epoch [1/2], Step [12740/64305], Loss: 5.2404\n",
      "Epoch [1/2], Step [12750/64305], Loss: 5.2007\n",
      "Epoch [1/2], Step [12760/64305], Loss: 5.0953\n",
      "Epoch [1/2], Step [12770/64305], Loss: 5.3663\n",
      "Epoch [1/2], Step [12780/64305], Loss: 5.1810\n",
      "Epoch [1/2], Step [12790/64305], Loss: 5.2011\n",
      "Epoch [1/2], Step [12800/64305], Loss: 5.1164\n",
      "Epoch [1/2], Step [12810/64305], Loss: 5.2655\n",
      "Epoch [1/2], Step [12820/64305], Loss: 5.0684\n",
      "Epoch [1/2], Step [12830/64305], Loss: 5.1541\n",
      "Epoch [1/2], Step [12840/64305], Loss: 5.1598\n",
      "Epoch [1/2], Step [12850/64305], Loss: 5.5897\n",
      "Epoch [1/2], Step [12860/64305], Loss: 5.2390\n",
      "Epoch [1/2], Step [12870/64305], Loss: 5.1891\n",
      "Epoch [1/2], Step [12880/64305], Loss: 5.0848\n",
      "Epoch [1/2], Step [12890/64305], Loss: 5.1176\n",
      "Epoch [1/2], Step [12900/64305], Loss: 5.2592\n",
      "Epoch [1/2], Step [12910/64305], Loss: 5.3451\n",
      "Epoch [1/2], Step [12920/64305], Loss: 5.2121\n",
      "Epoch [1/2], Step [12930/64305], Loss: 5.1856\n",
      "Epoch [1/2], Step [12940/64305], Loss: 5.0344\n",
      "Epoch [1/2], Step [12950/64305], Loss: 5.4753\n",
      "Epoch [1/2], Step [12960/64305], Loss: 5.3717\n",
      "Epoch [1/2], Step [12970/64305], Loss: 5.0396\n",
      "Epoch [1/2], Step [12980/64305], Loss: 5.1953\n",
      "Epoch [1/2], Step [12990/64305], Loss: 5.3788\n",
      "Epoch [1/2], Step [13000/64305], Loss: 5.2421\n",
      "Epoch [1/2], Step [13010/64305], Loss: 5.5026\n",
      "Epoch [1/2], Step [13020/64305], Loss: 5.0171\n",
      "Epoch [1/2], Step [13030/64305], Loss: 5.1095\n",
      "Epoch [1/2], Step [13040/64305], Loss: 5.2933\n",
      "Epoch [1/2], Step [13050/64305], Loss: 5.0981\n",
      "Epoch [1/2], Step [13060/64305], Loss: 5.1725\n",
      "Epoch [1/2], Step [13070/64305], Loss: 5.2622\n",
      "Epoch [1/2], Step [13080/64305], Loss: 5.2896\n",
      "Epoch [1/2], Step [13090/64305], Loss: 5.4478\n",
      "Epoch [1/2], Step [13100/64305], Loss: 5.1469\n",
      "Epoch [1/2], Step [13110/64305], Loss: 5.4957\n",
      "Epoch [1/2], Step [13120/64305], Loss: 5.1220\n",
      "Epoch [1/2], Step [13130/64305], Loss: 5.4027\n",
      "Epoch [1/2], Step [13140/64305], Loss: 5.3242\n",
      "Epoch [1/2], Step [13150/64305], Loss: 5.4791\n",
      "Epoch [1/2], Step [13160/64305], Loss: 5.1994\n",
      "Epoch [1/2], Step [13170/64305], Loss: 5.3259\n",
      "Epoch [1/2], Step [13180/64305], Loss: 5.4206\n",
      "Epoch [1/2], Step [13190/64305], Loss: 5.0870\n",
      "Epoch [1/2], Step [13200/64305], Loss: 5.0038\n",
      "Epoch [1/2], Step [13210/64305], Loss: 5.1586\n",
      "Epoch [1/2], Step [13220/64305], Loss: 5.3357\n",
      "Epoch [1/2], Step [13230/64305], Loss: 5.2085\n",
      "Epoch [1/2], Step [13240/64305], Loss: 5.5555\n",
      "Epoch [1/2], Step [13250/64305], Loss: 5.2836\n",
      "Epoch [1/2], Step [13260/64305], Loss: 5.3114\n",
      "Epoch [1/2], Step [13270/64305], Loss: 5.3647\n",
      "Epoch [1/2], Step [13280/64305], Loss: 5.3766\n",
      "Epoch [1/2], Step [13290/64305], Loss: 5.3345\n",
      "Epoch [1/2], Step [13300/64305], Loss: 5.0414\n",
      "Epoch [1/2], Step [13310/64305], Loss: 5.3455\n",
      "Epoch [1/2], Step [13320/64305], Loss: 5.2360\n",
      "Epoch [1/2], Step [13330/64305], Loss: 5.4859\n",
      "Epoch [1/2], Step [13340/64305], Loss: 5.2766\n",
      "Epoch [1/2], Step [13350/64305], Loss: 5.2857\n",
      "Epoch [1/2], Step [13360/64305], Loss: 5.2768\n",
      "Epoch [1/2], Step [13370/64305], Loss: 5.2439\n",
      "Epoch [1/2], Step [13380/64305], Loss: 5.0795\n",
      "Epoch [1/2], Step [13390/64305], Loss: 5.3220\n",
      "Epoch [1/2], Step [13400/64305], Loss: 5.1754\n",
      "Epoch [1/2], Step [13410/64305], Loss: 5.2371\n",
      "Epoch [1/2], Step [13420/64305], Loss: 5.2380\n",
      "Epoch [1/2], Step [13430/64305], Loss: 5.1641\n",
      "Epoch [1/2], Step [13440/64305], Loss: 5.2074\n",
      "Epoch [1/2], Step [13450/64305], Loss: 5.3222\n",
      "Epoch [1/2], Step [13460/64305], Loss: 5.2855\n",
      "Epoch [1/2], Step [13470/64305], Loss: 5.2338\n",
      "Epoch [1/2], Step [13480/64305], Loss: 5.1440\n",
      "Epoch [1/2], Step [13490/64305], Loss: 5.3946\n",
      "Epoch [1/2], Step [13500/64305], Loss: 5.0650\n",
      "Epoch [1/2], Step [13510/64305], Loss: 5.4272\n",
      "Epoch [1/2], Step [13520/64305], Loss: 5.2530\n",
      "Epoch [1/2], Step [13530/64305], Loss: 5.0271\n",
      "Epoch [1/2], Step [13540/64305], Loss: 5.1091\n",
      "Epoch [1/2], Step [13550/64305], Loss: 5.2526\n",
      "Epoch [1/2], Step [13560/64305], Loss: 5.3195\n",
      "Epoch [1/2], Step [13570/64305], Loss: 5.2306\n",
      "Epoch [1/2], Step [13580/64305], Loss: 5.2846\n",
      "Epoch [1/2], Step [13590/64305], Loss: 5.3508\n",
      "Epoch [1/2], Step [13600/64305], Loss: 5.3816\n",
      "Epoch [1/2], Step [13610/64305], Loss: 5.2026\n",
      "Epoch [1/2], Step [13620/64305], Loss: 5.2136\n",
      "Epoch [1/2], Step [13630/64305], Loss: 5.2864\n",
      "Epoch [1/2], Step [13640/64305], Loss: 5.0540\n",
      "Epoch [1/2], Step [13650/64305], Loss: 5.2815\n",
      "Epoch [1/2], Step [13660/64305], Loss: 5.3350\n",
      "Epoch [1/2], Step [13670/64305], Loss: 5.2855\n",
      "Epoch [1/2], Step [13680/64305], Loss: 5.1950\n",
      "Epoch [1/2], Step [13690/64305], Loss: 5.5007\n",
      "Epoch [1/2], Step [13700/64305], Loss: 5.2830\n",
      "Epoch [1/2], Step [13710/64305], Loss: 5.2452\n",
      "Epoch [1/2], Step [13720/64305], Loss: 5.1298\n",
      "Epoch [1/2], Step [13730/64305], Loss: 5.1341\n",
      "Epoch [1/2], Step [13740/64305], Loss: 5.3471\n",
      "Epoch [1/2], Step [13750/64305], Loss: 5.1971\n",
      "Epoch [1/2], Step [13760/64305], Loss: 5.2390\n",
      "Epoch [1/2], Step [13770/64305], Loss: 5.1857\n",
      "Epoch [1/2], Step [13780/64305], Loss: 5.2432\n",
      "Epoch [1/2], Step [13790/64305], Loss: 5.1161\n",
      "Epoch [1/2], Step [13800/64305], Loss: 5.1179\n",
      "Epoch [1/2], Step [13810/64305], Loss: 5.1361\n",
      "Epoch [1/2], Step [13820/64305], Loss: 5.2909\n",
      "Epoch [1/2], Step [13830/64305], Loss: 5.3833\n",
      "Epoch [1/2], Step [13840/64305], Loss: 5.3152\n",
      "Epoch [1/2], Step [13850/64305], Loss: 5.5179\n",
      "Epoch [1/2], Step [13860/64305], Loss: 5.1888\n",
      "Epoch [1/2], Step [13870/64305], Loss: 5.3411\n",
      "Epoch [1/2], Step [13880/64305], Loss: 5.4920\n",
      "Epoch [1/2], Step [13890/64305], Loss: 5.1190\n",
      "Epoch [1/2], Step [13900/64305], Loss: 5.1880\n",
      "Epoch [1/2], Step [13910/64305], Loss: 5.2333\n",
      "Epoch [1/2], Step [13920/64305], Loss: 5.1674\n",
      "Epoch [1/2], Step [13930/64305], Loss: 5.3804\n",
      "Epoch [1/2], Step [13940/64305], Loss: 5.1389\n",
      "Epoch [1/2], Step [13950/64305], Loss: 5.2612\n",
      "Epoch [1/2], Step [13960/64305], Loss: 4.9936\n",
      "Epoch [1/2], Step [13970/64305], Loss: 5.1126\n",
      "Epoch [1/2], Step [13980/64305], Loss: 5.1876\n",
      "Epoch [1/2], Step [13990/64305], Loss: 5.1793\n",
      "Epoch [1/2], Step [14000/64305], Loss: 5.1584\n",
      "Epoch [1/2], Step [14010/64305], Loss: 5.4886\n",
      "Epoch [1/2], Step [14020/64305], Loss: 5.3223\n",
      "Epoch [1/2], Step [14030/64305], Loss: 5.2029\n",
      "Epoch [1/2], Step [14040/64305], Loss: 5.1291\n",
      "Epoch [1/2], Step [14050/64305], Loss: 5.1961\n",
      "Epoch [1/2], Step [14060/64305], Loss: 5.1073\n",
      "Epoch [1/2], Step [14070/64305], Loss: 5.1258\n",
      "Epoch [1/2], Step [14080/64305], Loss: 5.1956\n",
      "Epoch [1/2], Step [14090/64305], Loss: 5.1016\n",
      "Epoch [1/2], Step [14100/64305], Loss: 5.1582\n",
      "Epoch [1/2], Step [14110/64305], Loss: 5.4967\n",
      "Epoch [1/2], Step [14120/64305], Loss: 5.1805\n",
      "Epoch [1/2], Step [14130/64305], Loss: 5.1786\n",
      "Epoch [1/2], Step [14140/64305], Loss: 5.3398\n",
      "Epoch [1/2], Step [14150/64305], Loss: 5.2458\n",
      "Epoch [1/2], Step [14160/64305], Loss: 5.1926\n",
      "Epoch [1/2], Step [14170/64305], Loss: 5.2708\n",
      "Epoch [1/2], Step [14180/64305], Loss: 5.3477\n",
      "Epoch [1/2], Step [14190/64305], Loss: 5.2287\n",
      "Epoch [1/2], Step [14200/64305], Loss: 5.0360\n",
      "Epoch [1/2], Step [14210/64305], Loss: 5.0751\n",
      "Epoch [1/2], Step [14220/64305], Loss: 5.2326\n",
      "Epoch [1/2], Step [14230/64305], Loss: 5.2637\n",
      "Epoch [1/2], Step [14240/64305], Loss: 5.1578\n",
      "Epoch [1/2], Step [14250/64305], Loss: 5.2287\n",
      "Epoch [1/2], Step [14260/64305], Loss: 5.2742\n",
      "Epoch [1/2], Step [14270/64305], Loss: 5.2383\n",
      "Epoch [1/2], Step [14280/64305], Loss: 5.2478\n",
      "Epoch [1/2], Step [14290/64305], Loss: 5.0856\n",
      "Epoch [1/2], Step [14300/64305], Loss: 5.0396\n",
      "Epoch [1/2], Step [14310/64305], Loss: 5.1118\n",
      "Epoch [1/2], Step [14320/64305], Loss: 5.1731\n",
      "Epoch [1/2], Step [14330/64305], Loss: 5.3067\n",
      "Epoch [1/2], Step [14340/64305], Loss: 5.3890\n",
      "Epoch [1/2], Step [14350/64305], Loss: 5.2658\n",
      "Epoch [1/2], Step [14360/64305], Loss: 5.2390\n",
      "Epoch [1/2], Step [14370/64305], Loss: 5.4533\n",
      "Epoch [1/2], Step [14380/64305], Loss: 5.1295\n",
      "Epoch [1/2], Step [14390/64305], Loss: 5.2006\n",
      "Epoch [1/2], Step [14400/64305], Loss: 5.1710\n",
      "Epoch [1/2], Step [14410/64305], Loss: 5.3116\n",
      "Epoch [1/2], Step [14420/64305], Loss: 5.3208\n",
      "Epoch [1/2], Step [14430/64305], Loss: 5.1352\n",
      "Epoch [1/2], Step [14440/64305], Loss: 5.2179\n",
      "Epoch [1/2], Step [14450/64305], Loss: 5.1486\n",
      "Epoch [1/2], Step [14460/64305], Loss: 5.1804\n",
      "Epoch [1/2], Step [14470/64305], Loss: 5.3793\n",
      "Epoch [1/2], Step [14480/64305], Loss: 5.3030\n",
      "Epoch [1/2], Step [14490/64305], Loss: 5.3883\n",
      "Epoch [1/2], Step [14500/64305], Loss: 5.0052\n",
      "Epoch [1/2], Step [14510/64305], Loss: 5.0632\n",
      "Epoch [1/2], Step [14520/64305], Loss: 5.4047\n",
      "Epoch [1/2], Step [14530/64305], Loss: 5.4455\n",
      "Epoch [1/2], Step [14540/64305], Loss: 5.4179\n",
      "Epoch [1/2], Step [14550/64305], Loss: 5.2153\n",
      "Epoch [1/2], Step [14560/64305], Loss: 5.2588\n",
      "Epoch [1/2], Step [14570/64305], Loss: 5.2035\n",
      "Epoch [1/2], Step [14580/64305], Loss: 5.3170\n",
      "Epoch [1/2], Step [14590/64305], Loss: 5.2972\n",
      "Epoch [1/2], Step [14600/64305], Loss: 5.3847\n",
      "Epoch [1/2], Step [14610/64305], Loss: 5.2570\n",
      "Epoch [1/2], Step [14620/64305], Loss: 5.1209\n",
      "Epoch [1/2], Step [14630/64305], Loss: 5.3266\n",
      "Epoch [1/2], Step [14640/64305], Loss: 5.1295\n",
      "Epoch [1/2], Step [14650/64305], Loss: 5.3351\n",
      "Epoch [1/2], Step [14660/64305], Loss: 5.4376\n",
      "Epoch [1/2], Step [14670/64305], Loss: 5.2153\n",
      "Epoch [1/2], Step [14680/64305], Loss: 5.0347\n",
      "Epoch [1/2], Step [14690/64305], Loss: 5.2761\n",
      "Epoch [1/2], Step [14700/64305], Loss: 5.3169\n",
      "Epoch [1/2], Step [14710/64305], Loss: 5.2930\n",
      "Epoch [1/2], Step [14720/64305], Loss: 5.1806\n",
      "Epoch [1/2], Step [14730/64305], Loss: 5.1294\n",
      "Epoch [1/2], Step [14740/64305], Loss: 5.1560\n",
      "Epoch [1/2], Step [14750/64305], Loss: 5.2307\n",
      "Epoch [1/2], Step [14760/64305], Loss: 5.3011\n",
      "Epoch [1/2], Step [14770/64305], Loss: 5.1872\n",
      "Epoch [1/2], Step [14780/64305], Loss: 5.0334\n",
      "Epoch [1/2], Step [14790/64305], Loss: 5.0114\n",
      "Epoch [1/2], Step [14800/64305], Loss: 5.1176\n",
      "Epoch [1/2], Step [14810/64305], Loss: 5.1062\n",
      "Epoch [1/2], Step [14820/64305], Loss: 5.0087\n",
      "Epoch [1/2], Step [14830/64305], Loss: 5.4720\n",
      "Epoch [1/2], Step [14840/64305], Loss: 5.2243\n",
      "Epoch [1/2], Step [14850/64305], Loss: 5.0953\n",
      "Epoch [1/2], Step [14860/64305], Loss: 5.2223\n",
      "Epoch [1/2], Step [14870/64305], Loss: 5.1557\n",
      "Epoch [1/2], Step [14880/64305], Loss: 5.1921\n",
      "Epoch [1/2], Step [14890/64305], Loss: 5.1773\n",
      "Epoch [1/2], Step [14900/64305], Loss: 5.0603\n",
      "Epoch [1/2], Step [14910/64305], Loss: 5.1433\n",
      "Epoch [1/2], Step [14920/64305], Loss: 5.2390\n",
      "Epoch [1/2], Step [14930/64305], Loss: 5.2201\n",
      "Epoch [1/2], Step [14940/64305], Loss: 5.1738\n",
      "Epoch [1/2], Step [14950/64305], Loss: 5.2790\n",
      "Epoch [1/2], Step [14960/64305], Loss: 5.2823\n",
      "Epoch [1/2], Step [14970/64305], Loss: 5.4577\n",
      "Epoch [1/2], Step [14980/64305], Loss: 5.2164\n",
      "Epoch [1/2], Step [14990/64305], Loss: 5.3673\n",
      "Epoch [1/2], Step [15000/64305], Loss: 5.1780\n",
      "Epoch [1/2], Step [15010/64305], Loss: 5.1796\n",
      "Epoch [1/2], Step [15020/64305], Loss: 5.3424\n",
      "Epoch [1/2], Step [15030/64305], Loss: 5.1097\n",
      "Epoch [1/2], Step [15040/64305], Loss: 5.2106\n",
      "Epoch [1/2], Step [15050/64305], Loss: 5.3513\n",
      "Epoch [1/2], Step [15060/64305], Loss: 5.3573\n",
      "Epoch [1/2], Step [15070/64305], Loss: 5.3897\n",
      "Epoch [1/2], Step [15080/64305], Loss: 5.3594\n",
      "Epoch [1/2], Step [15090/64305], Loss: 5.1254\n",
      "Epoch [1/2], Step [15100/64305], Loss: 5.2107\n",
      "Epoch [1/2], Step [15110/64305], Loss: 5.2397\n",
      "Epoch [1/2], Step [15120/64305], Loss: 5.1655\n",
      "Epoch [1/2], Step [15130/64305], Loss: 5.4065\n",
      "Epoch [1/2], Step [15140/64305], Loss: 5.5353\n",
      "Epoch [1/2], Step [15150/64305], Loss: 5.3023\n",
      "Epoch [1/2], Step [15160/64305], Loss: 5.2723\n",
      "Epoch [1/2], Step [15170/64305], Loss: 5.3163\n",
      "Epoch [1/2], Step [15180/64305], Loss: 5.1567\n",
      "Epoch [1/2], Step [15190/64305], Loss: 5.1756\n",
      "Epoch [1/2], Step [15200/64305], Loss: 4.9370\n",
      "Epoch [1/2], Step [15210/64305], Loss: 5.1783\n",
      "Epoch [1/2], Step [15220/64305], Loss: 5.3099\n",
      "Epoch [1/2], Step [15230/64305], Loss: 5.0775\n",
      "Epoch [1/2], Step [15240/64305], Loss: 5.3091\n",
      "Epoch [1/2], Step [15250/64305], Loss: 4.9496\n",
      "Epoch [1/2], Step [15260/64305], Loss: 5.2394\n",
      "Epoch [1/2], Step [15270/64305], Loss: 5.3128\n",
      "Epoch [1/2], Step [15280/64305], Loss: 5.2865\n",
      "Epoch [1/2], Step [15290/64305], Loss: 5.2311\n",
      "Epoch [1/2], Step [15300/64305], Loss: 5.3203\n",
      "Epoch [1/2], Step [15310/64305], Loss: 5.1215\n",
      "Epoch [1/2], Step [15320/64305], Loss: 5.2083\n",
      "Epoch [1/2], Step [15330/64305], Loss: 5.2027\n",
      "Epoch [1/2], Step [15340/64305], Loss: 5.1069\n",
      "Epoch [1/2], Step [15350/64305], Loss: 5.1600\n",
      "Epoch [1/2], Step [15360/64305], Loss: 4.9825\n",
      "Epoch [1/2], Step [15370/64305], Loss: 5.0743\n",
      "Epoch [1/2], Step [15380/64305], Loss: 5.1082\n",
      "Epoch [1/2], Step [15390/64305], Loss: 5.0811\n",
      "Epoch [1/2], Step [15400/64305], Loss: 5.3882\n",
      "Epoch [1/2], Step [15410/64305], Loss: 5.3918\n",
      "Epoch [1/2], Step [15420/64305], Loss: 5.3682\n",
      "Epoch [1/2], Step [15430/64305], Loss: 5.2337\n",
      "Epoch [1/2], Step [15440/64305], Loss: 5.0537\n",
      "Epoch [1/2], Step [15450/64305], Loss: 5.1161\n",
      "Epoch [1/2], Step [15460/64305], Loss: 5.0910\n",
      "Epoch [1/2], Step [15470/64305], Loss: 5.3270\n",
      "Epoch [1/2], Step [15480/64305], Loss: 5.1873\n",
      "Epoch [1/2], Step [15490/64305], Loss: 5.4125\n",
      "Epoch [1/2], Step [15500/64305], Loss: 5.2253\n",
      "Epoch [1/2], Step [15510/64305], Loss: 5.2949\n",
      "Epoch [1/2], Step [15520/64305], Loss: 5.1736\n",
      "Epoch [1/2], Step [15530/64305], Loss: 5.3595\n",
      "Epoch [1/2], Step [15540/64305], Loss: 5.1124\n",
      "Epoch [1/2], Step [15550/64305], Loss: 5.3674\n",
      "Epoch [1/2], Step [15560/64305], Loss: 5.3469\n",
      "Epoch [1/2], Step [15570/64305], Loss: 5.2093\n",
      "Epoch [1/2], Step [15580/64305], Loss: 5.3496\n",
      "Epoch [1/2], Step [15590/64305], Loss: 5.5208\n",
      "Epoch [1/2], Step [15600/64305], Loss: 5.1703\n",
      "Epoch [1/2], Step [15610/64305], Loss: 5.1773\n",
      "Epoch [1/2], Step [15620/64305], Loss: 5.4230\n",
      "Epoch [1/2], Step [15630/64305], Loss: 5.2037\n",
      "Epoch [1/2], Step [15640/64305], Loss: 5.2450\n",
      "Epoch [1/2], Step [15650/64305], Loss: 5.0114\n",
      "Epoch [1/2], Step [15660/64305], Loss: 5.2417\n",
      "Epoch [1/2], Step [15670/64305], Loss: 5.2159\n",
      "Epoch [1/2], Step [15680/64305], Loss: 5.0673\n",
      "Epoch [1/2], Step [15690/64305], Loss: 5.1176\n",
      "Epoch [1/2], Step [15700/64305], Loss: 5.0845\n",
      "Epoch [1/2], Step [15710/64305], Loss: 5.0366\n",
      "Epoch [1/2], Step [15720/64305], Loss: 5.3477\n",
      "Epoch [1/2], Step [15730/64305], Loss: 5.2348\n",
      "Epoch [1/2], Step [15740/64305], Loss: 5.2260\n",
      "Epoch [1/2], Step [15750/64305], Loss: 5.2439\n",
      "Epoch [1/2], Step [15760/64305], Loss: 5.3039\n",
      "Epoch [1/2], Step [15770/64305], Loss: 5.2604\n",
      "Epoch [1/2], Step [15780/64305], Loss: 5.2171\n",
      "Epoch [1/2], Step [15790/64305], Loss: 5.3858\n",
      "Epoch [1/2], Step [15800/64305], Loss: 5.1866\n",
      "Epoch [1/2], Step [15810/64305], Loss: 5.4438\n",
      "Epoch [1/2], Step [15820/64305], Loss: 5.1412\n",
      "Epoch [1/2], Step [15830/64305], Loss: 5.1905\n",
      "Epoch [1/2], Step [15840/64305], Loss: 5.2665\n",
      "Epoch [1/2], Step [15850/64305], Loss: 5.2592\n",
      "Epoch [1/2], Step [15860/64305], Loss: 5.0989\n",
      "Epoch [1/2], Step [15870/64305], Loss: 5.4296\n",
      "Epoch [1/2], Step [15880/64305], Loss: 5.3408\n",
      "Epoch [1/2], Step [15890/64305], Loss: 5.1710\n",
      "Epoch [1/2], Step [15900/64305], Loss: 5.2922\n",
      "Epoch [1/2], Step [15910/64305], Loss: 5.1593\n",
      "Epoch [1/2], Step [15920/64305], Loss: 5.1260\n",
      "Epoch [1/2], Step [15930/64305], Loss: 5.1245\n",
      "Epoch [1/2], Step [15940/64305], Loss: 5.1370\n",
      "Epoch [1/2], Step [15950/64305], Loss: 5.0330\n",
      "Epoch [1/2], Step [15960/64305], Loss: 5.2004\n",
      "Epoch [1/2], Step [15970/64305], Loss: 5.1268\n",
      "Epoch [1/2], Step [15980/64305], Loss: 5.0611\n",
      "Epoch [1/2], Step [15990/64305], Loss: 5.0523\n",
      "Epoch [1/2], Step [16000/64305], Loss: 5.2478\n",
      "Epoch [1/2], Step [16010/64305], Loss: 5.4763\n",
      "Epoch [1/2], Step [16020/64305], Loss: 5.1598\n",
      "Epoch [1/2], Step [16030/64305], Loss: 5.2841\n",
      "Epoch [1/2], Step [16040/64305], Loss: 5.0429\n",
      "Epoch [1/2], Step [16050/64305], Loss: 5.0928\n",
      "Epoch [1/2], Step [16060/64305], Loss: 5.0761\n",
      "Epoch [1/2], Step [16070/64305], Loss: 5.0921\n",
      "Epoch [1/2], Step [16080/64305], Loss: 5.1898\n",
      "Epoch [1/2], Step [16090/64305], Loss: 5.1199\n",
      "Epoch [1/2], Step [16100/64305], Loss: 5.2690\n",
      "Epoch [1/2], Step [16110/64305], Loss: 5.3232\n",
      "Epoch [1/2], Step [16120/64305], Loss: 5.1822\n",
      "Epoch [1/2], Step [16130/64305], Loss: 5.1916\n",
      "Epoch [1/2], Step [16140/64305], Loss: 5.2877\n",
      "Epoch [1/2], Step [16150/64305], Loss: 5.2509\n",
      "Epoch [1/2], Step [16160/64305], Loss: 5.2758\n",
      "Epoch [1/2], Step [16170/64305], Loss: 5.0905\n",
      "Epoch [1/2], Step [16180/64305], Loss: 5.1643\n",
      "Epoch [1/2], Step [16190/64305], Loss: 5.2625\n",
      "Epoch [1/2], Step [16200/64305], Loss: 5.3058\n",
      "Epoch [1/2], Step [16210/64305], Loss: 5.3252\n",
      "Epoch [1/2], Step [16220/64305], Loss: 4.9763\n",
      "Epoch [1/2], Step [16230/64305], Loss: 5.1699\n",
      "Epoch [1/2], Step [16240/64305], Loss: 5.1796\n",
      "Epoch [1/2], Step [16250/64305], Loss: 5.0328\n",
      "Epoch [1/2], Step [16260/64305], Loss: 5.2736\n",
      "Epoch [1/2], Step [16270/64305], Loss: 5.4933\n",
      "Epoch [1/2], Step [16280/64305], Loss: 5.1758\n",
      "Epoch [1/2], Step [16290/64305], Loss: 5.2172\n",
      "Epoch [1/2], Step [16300/64305], Loss: 5.1482\n",
      "Epoch [1/2], Step [16310/64305], Loss: 5.0691\n",
      "Epoch [1/2], Step [16320/64305], Loss: 5.3067\n",
      "Epoch [1/2], Step [16330/64305], Loss: 5.4186\n",
      "Epoch [1/2], Step [16340/64305], Loss: 5.2712\n",
      "Epoch [1/2], Step [16350/64305], Loss: 5.3965\n",
      "Epoch [1/2], Step [16360/64305], Loss: 5.0971\n",
      "Epoch [1/2], Step [16370/64305], Loss: 5.2678\n",
      "Epoch [1/2], Step [16380/64305], Loss: 5.3091\n",
      "Epoch [1/2], Step [16390/64305], Loss: 5.4197\n",
      "Epoch [1/2], Step [16400/64305], Loss: 5.3962\n",
      "Epoch [1/2], Step [16410/64305], Loss: 5.3202\n",
      "Epoch [1/2], Step [16420/64305], Loss: 5.1014\n",
      "Epoch [1/2], Step [16430/64305], Loss: 5.1087\n",
      "Epoch [1/2], Step [16440/64305], Loss: 5.0852\n",
      "Epoch [1/2], Step [16450/64305], Loss: 4.9966\n",
      "Epoch [1/2], Step [16460/64305], Loss: 5.2314\n",
      "Epoch [1/2], Step [16470/64305], Loss: 5.3186\n",
      "Epoch [1/2], Step [16480/64305], Loss: 5.2346\n",
      "Epoch [1/2], Step [16490/64305], Loss: 5.0658\n",
      "Epoch [1/2], Step [16500/64305], Loss: 5.0280\n",
      "Epoch [1/2], Step [16510/64305], Loss: 5.3237\n",
      "Epoch [1/2], Step [16520/64305], Loss: 5.1049\n",
      "Epoch [1/2], Step [16530/64305], Loss: 5.0050\n",
      "Epoch [1/2], Step [16540/64305], Loss: 5.2277\n",
      "Epoch [1/2], Step [16550/64305], Loss: 5.0459\n",
      "Epoch [1/2], Step [16560/64305], Loss: 5.3079\n",
      "Epoch [1/2], Step [16570/64305], Loss: 5.4458\n",
      "Epoch [1/2], Step [16580/64305], Loss: 5.1065\n",
      "Epoch [1/2], Step [16590/64305], Loss: 5.2417\n",
      "Epoch [1/2], Step [16600/64305], Loss: 5.0189\n",
      "Epoch [1/2], Step [16610/64305], Loss: 5.1645\n",
      "Epoch [1/2], Step [16620/64305], Loss: 5.2201\n",
      "Epoch [1/2], Step [16630/64305], Loss: 5.1504\n",
      "Epoch [1/2], Step [16640/64305], Loss: 5.1072\n",
      "Epoch [1/2], Step [16650/64305], Loss: 4.9528\n",
      "Epoch [1/2], Step [16660/64305], Loss: 5.0132\n",
      "Epoch [1/2], Step [16670/64305], Loss: 5.0249\n",
      "Epoch [1/2], Step [16680/64305], Loss: 5.2492\n",
      "Epoch [1/2], Step [16690/64305], Loss: 5.2482\n",
      "Epoch [1/2], Step [16700/64305], Loss: 5.2782\n",
      "Epoch [1/2], Step [16710/64305], Loss: 5.1397\n",
      "Epoch [1/2], Step [16720/64305], Loss: 5.1482\n",
      "Epoch [1/2], Step [16730/64305], Loss: 5.1362\n",
      "Epoch [1/2], Step [16740/64305], Loss: 4.9724\n",
      "Epoch [1/2], Step [16750/64305], Loss: 5.1584\n",
      "Epoch [1/2], Step [16760/64305], Loss: 5.1815\n",
      "Epoch [1/2], Step [16770/64305], Loss: 5.2753\n",
      "Epoch [1/2], Step [16780/64305], Loss: 5.3832\n",
      "Epoch [1/2], Step [16790/64305], Loss: 5.2639\n",
      "Epoch [1/2], Step [16800/64305], Loss: 5.2442\n",
      "Epoch [1/2], Step [16810/64305], Loss: 5.1536\n",
      "Epoch [1/2], Step [16820/64305], Loss: 5.0742\n",
      "Epoch [1/2], Step [16830/64305], Loss: 5.0324\n",
      "Epoch [1/2], Step [16840/64305], Loss: 5.1318\n",
      "Epoch [1/2], Step [16850/64305], Loss: 5.0874\n",
      "Epoch [1/2], Step [16860/64305], Loss: 5.1448\n",
      "Epoch [1/2], Step [16870/64305], Loss: 5.1161\n",
      "Epoch [1/2], Step [16880/64305], Loss: 5.2711\n",
      "Epoch [1/2], Step [16890/64305], Loss: 5.2555\n",
      "Epoch [1/2], Step [16900/64305], Loss: 5.1656\n",
      "Epoch [1/2], Step [16910/64305], Loss: 5.1240\n",
      "Epoch [1/2], Step [16920/64305], Loss: 5.1517\n",
      "Epoch [1/2], Step [16930/64305], Loss: 5.3281\n",
      "Epoch [1/2], Step [16940/64305], Loss: 5.1080\n",
      "Epoch [1/2], Step [16950/64305], Loss: 5.1875\n",
      "Epoch [1/2], Step [16960/64305], Loss: 5.3442\n",
      "Epoch [1/2], Step [16970/64305], Loss: 4.8615\n",
      "Epoch [1/2], Step [16980/64305], Loss: 5.3154\n",
      "Epoch [1/2], Step [16990/64305], Loss: 5.0323\n",
      "Epoch [1/2], Step [17000/64305], Loss: 5.2575\n",
      "Epoch [1/2], Step [17010/64305], Loss: 5.2668\n",
      "Epoch [1/2], Step [17020/64305], Loss: 5.1272\n",
      "Epoch [1/2], Step [17030/64305], Loss: 4.9741\n",
      "Epoch [1/2], Step [17040/64305], Loss: 4.9714\n",
      "Epoch [1/2], Step [17050/64305], Loss: 5.2770\n",
      "Epoch [1/2], Step [17060/64305], Loss: 5.2438\n",
      "Epoch [1/2], Step [17070/64305], Loss: 5.1560\n",
      "Epoch [1/2], Step [17080/64305], Loss: 5.3239\n",
      "Epoch [1/2], Step [17090/64305], Loss: 5.3158\n",
      "Epoch [1/2], Step [17100/64305], Loss: 5.0416\n",
      "Epoch [1/2], Step [17110/64305], Loss: 5.1765\n",
      "Epoch [1/2], Step [17120/64305], Loss: 5.2940\n",
      "Epoch [1/2], Step [17130/64305], Loss: 5.0324\n",
      "Epoch [1/2], Step [17140/64305], Loss: 4.9961\n",
      "Epoch [1/2], Step [17150/64305], Loss: 5.2277\n",
      "Epoch [1/2], Step [17160/64305], Loss: 5.0677\n",
      "Epoch [1/2], Step [17170/64305], Loss: 5.1904\n",
      "Epoch [1/2], Step [17180/64305], Loss: 5.2206\n",
      "Epoch [1/2], Step [17190/64305], Loss: 5.5893\n",
      "Epoch [1/2], Step [17200/64305], Loss: 5.1662\n",
      "Epoch [1/2], Step [17210/64305], Loss: 5.2551\n",
      "Epoch [1/2], Step [17220/64305], Loss: 5.0741\n",
      "Epoch [1/2], Step [17230/64305], Loss: 5.0230\n",
      "Epoch [1/2], Step [17240/64305], Loss: 5.0381\n",
      "Epoch [1/2], Step [17250/64305], Loss: 5.0366\n",
      "Epoch [1/2], Step [17260/64305], Loss: 5.1141\n",
      "Epoch [1/2], Step [17270/64305], Loss: 5.2497\n",
      "Epoch [1/2], Step [17280/64305], Loss: 5.1552\n",
      "Epoch [1/2], Step [17290/64305], Loss: 5.4782\n",
      "Epoch [1/2], Step [17300/64305], Loss: 5.3993\n",
      "Epoch [1/2], Step [17310/64305], Loss: 5.1299\n",
      "Epoch [1/2], Step [17320/64305], Loss: 4.8638\n",
      "Epoch [1/2], Step [17330/64305], Loss: 5.1602\n",
      "Epoch [1/2], Step [17340/64305], Loss: 5.4512\n",
      "Epoch [1/2], Step [17350/64305], Loss: 5.1351\n",
      "Epoch [1/2], Step [17360/64305], Loss: 5.1581\n",
      "Epoch [1/2], Step [17370/64305], Loss: 5.1892\n",
      "Epoch [1/2], Step [17380/64305], Loss: 5.2360\n",
      "Epoch [1/2], Step [17390/64305], Loss: 5.2147\n",
      "Epoch [1/2], Step [17400/64305], Loss: 5.4212\n",
      "Epoch [1/2], Step [17410/64305], Loss: 5.3737\n",
      "Epoch [1/2], Step [17420/64305], Loss: 5.2223\n",
      "Epoch [1/2], Step [17430/64305], Loss: 5.0091\n",
      "Epoch [1/2], Step [17440/64305], Loss: 5.1256\n",
      "Epoch [1/2], Step [17450/64305], Loss: 5.4018\n",
      "Epoch [1/2], Step [17460/64305], Loss: 4.9858\n",
      "Epoch [1/2], Step [17470/64305], Loss: 5.1147\n",
      "Epoch [1/2], Step [17480/64305], Loss: 5.2084\n",
      "Epoch [1/2], Step [17490/64305], Loss: 5.2505\n",
      "Epoch [1/2], Step [17500/64305], Loss: 5.2887\n",
      "Epoch [1/2], Step [17510/64305], Loss: 5.2027\n",
      "Epoch [1/2], Step [17520/64305], Loss: 5.3300\n",
      "Epoch [1/2], Step [17530/64305], Loss: 5.0988\n",
      "Epoch [1/2], Step [17540/64305], Loss: 5.2046\n",
      "Epoch [1/2], Step [17550/64305], Loss: 5.1892\n",
      "Epoch [1/2], Step [17560/64305], Loss: 5.2764\n",
      "Epoch [1/2], Step [17570/64305], Loss: 5.2867\n",
      "Epoch [1/2], Step [17580/64305], Loss: 5.2929\n",
      "Epoch [1/2], Step [17590/64305], Loss: 5.0928\n",
      "Epoch [1/2], Step [17600/64305], Loss: 5.1504\n",
      "Epoch [1/2], Step [17610/64305], Loss: 5.1639\n",
      "Epoch [1/2], Step [17620/64305], Loss: 5.0370\n",
      "Epoch [1/2], Step [17630/64305], Loss: 5.4527\n",
      "Epoch [1/2], Step [17640/64305], Loss: 5.3996\n",
      "Epoch [1/2], Step [17650/64305], Loss: 5.1236\n",
      "Epoch [1/2], Step [17660/64305], Loss: 5.0231\n",
      "Epoch [1/2], Step [17670/64305], Loss: 5.2338\n",
      "Epoch [1/2], Step [17680/64305], Loss: 5.0267\n",
      "Epoch [1/2], Step [17690/64305], Loss: 5.2720\n",
      "Epoch [1/2], Step [17700/64305], Loss: 5.2038\n",
      "Epoch [1/2], Step [17710/64305], Loss: 5.3037\n",
      "Epoch [1/2], Step [17720/64305], Loss: 5.1599\n",
      "Epoch [1/2], Step [17730/64305], Loss: 5.0832\n",
      "Epoch [1/2], Step [17740/64305], Loss: 5.2557\n",
      "Epoch [1/2], Step [17750/64305], Loss: 5.2836\n",
      "Epoch [1/2], Step [17760/64305], Loss: 5.4091\n",
      "Epoch [1/2], Step [17770/64305], Loss: 5.1751\n",
      "Epoch [1/2], Step [17780/64305], Loss: 5.1309\n",
      "Epoch [1/2], Step [17790/64305], Loss: 5.2988\n",
      "Epoch [1/2], Step [17800/64305], Loss: 5.2061\n",
      "Epoch [1/2], Step [17810/64305], Loss: 5.1576\n",
      "Epoch [1/2], Step [17820/64305], Loss: 5.1247\n",
      "Epoch [1/2], Step [17830/64305], Loss: 5.1866\n",
      "Epoch [1/2], Step [17840/64305], Loss: 5.0557\n",
      "Epoch [1/2], Step [17850/64305], Loss: 5.0491\n",
      "Epoch [1/2], Step [17860/64305], Loss: 5.2951\n",
      "Epoch [1/2], Step [17870/64305], Loss: 5.2702\n",
      "Epoch [1/2], Step [17880/64305], Loss: 5.0725\n",
      "Epoch [1/2], Step [17890/64305], Loss: 5.2162\n",
      "Epoch [1/2], Step [17900/64305], Loss: 5.1245\n",
      "Epoch [1/2], Step [17910/64305], Loss: 5.2729\n",
      "Epoch [1/2], Step [17920/64305], Loss: 5.2197\n",
      "Epoch [1/2], Step [17930/64305], Loss: 5.3541\n",
      "Epoch [1/2], Step [17940/64305], Loss: 5.2052\n",
      "Epoch [1/2], Step [17950/64305], Loss: 5.2500\n",
      "Epoch [1/2], Step [17960/64305], Loss: 5.1855\n",
      "Epoch [1/2], Step [17970/64305], Loss: 5.0489\n",
      "Epoch [1/2], Step [17980/64305], Loss: 5.2886\n",
      "Epoch [1/2], Step [17990/64305], Loss: 5.2532\n",
      "Epoch [1/2], Step [18000/64305], Loss: 5.2232\n",
      "Epoch [1/2], Step [18010/64305], Loss: 5.3851\n",
      "Epoch [1/2], Step [18020/64305], Loss: 5.1290\n",
      "Epoch [1/2], Step [18030/64305], Loss: 5.2447\n",
      "Epoch [1/2], Step [18040/64305], Loss: 5.2888\n",
      "Epoch [1/2], Step [18050/64305], Loss: 5.3346\n",
      "Epoch [1/2], Step [18060/64305], Loss: 5.1839\n",
      "Epoch [1/2], Step [18070/64305], Loss: 5.2501\n",
      "Epoch [1/2], Step [18080/64305], Loss: 4.9075\n",
      "Epoch [1/2], Step [18090/64305], Loss: 4.9913\n",
      "Epoch [1/2], Step [18100/64305], Loss: 4.9927\n",
      "Epoch [1/2], Step [18110/64305], Loss: 5.0956\n",
      "Epoch [1/2], Step [18120/64305], Loss: 4.8401\n",
      "Epoch [1/2], Step [18130/64305], Loss: 5.2520\n",
      "Epoch [1/2], Step [18140/64305], Loss: 5.1325\n",
      "Epoch [1/2], Step [18150/64305], Loss: 4.9757\n",
      "Epoch [1/2], Step [18160/64305], Loss: 5.3958\n",
      "Epoch [1/2], Step [18170/64305], Loss: 5.1218\n",
      "Epoch [1/2], Step [18180/64305], Loss: 5.1066\n",
      "Epoch [1/2], Step [18190/64305], Loss: 4.8800\n",
      "Epoch [1/2], Step [18200/64305], Loss: 5.2260\n",
      "Epoch [1/2], Step [18210/64305], Loss: 5.0861\n",
      "Epoch [1/2], Step [18220/64305], Loss: 5.1103\n",
      "Epoch [1/2], Step [18230/64305], Loss: 5.1411\n",
      "Epoch [1/2], Step [18240/64305], Loss: 5.0767\n",
      "Epoch [1/2], Step [18250/64305], Loss: 5.1731\n",
      "Epoch [1/2], Step [18260/64305], Loss: 5.1014\n",
      "Epoch [1/2], Step [18270/64305], Loss: 5.1500\n",
      "Epoch [1/2], Step [18280/64305], Loss: 5.2253\n",
      "Epoch [1/2], Step [18290/64305], Loss: 5.0927\n",
      "Epoch [1/2], Step [18300/64305], Loss: 5.1260\n",
      "Epoch [1/2], Step [18310/64305], Loss: 5.3690\n",
      "Epoch [1/2], Step [18320/64305], Loss: 5.2293\n",
      "Epoch [1/2], Step [18330/64305], Loss: 5.3132\n",
      "Epoch [1/2], Step [18340/64305], Loss: 5.1396\n",
      "Epoch [1/2], Step [18350/64305], Loss: 5.1509\n",
      "Epoch [1/2], Step [18360/64305], Loss: 5.3594\n",
      "Epoch [1/2], Step [18370/64305], Loss: 5.0804\n",
      "Epoch [1/2], Step [18380/64305], Loss: 5.2906\n",
      "Epoch [1/2], Step [18390/64305], Loss: 5.0012\n",
      "Epoch [1/2], Step [18400/64305], Loss: 5.2587\n",
      "Epoch [1/2], Step [18410/64305], Loss: 5.2803\n",
      "Epoch [1/2], Step [18420/64305], Loss: 5.3757\n",
      "Epoch [1/2], Step [18430/64305], Loss: 5.2523\n",
      "Epoch [1/2], Step [18440/64305], Loss: 5.3688\n",
      "Epoch [1/2], Step [18450/64305], Loss: 5.2131\n",
      "Epoch [1/2], Step [18460/64305], Loss: 5.0805\n",
      "Epoch [1/2], Step [18470/64305], Loss: 5.5745\n",
      "Epoch [1/2], Step [18480/64305], Loss: 5.2491\n",
      "Epoch [1/2], Step [18490/64305], Loss: 5.2093\n",
      "Epoch [1/2], Step [18500/64305], Loss: 5.0927\n",
      "Epoch [1/2], Step [18510/64305], Loss: 5.1563\n",
      "Epoch [1/2], Step [18520/64305], Loss: 5.1005\n",
      "Epoch [1/2], Step [18530/64305], Loss: 5.1411\n",
      "Epoch [1/2], Step [18540/64305], Loss: 5.1471\n",
      "Epoch [1/2], Step [18550/64305], Loss: 5.2300\n",
      "Epoch [1/2], Step [18560/64305], Loss: 4.9594\n",
      "Epoch [1/2], Step [18570/64305], Loss: 5.2424\n",
      "Epoch [1/2], Step [18580/64305], Loss: 5.0496\n",
      "Epoch [1/2], Step [18590/64305], Loss: 5.1942\n",
      "Epoch [1/2], Step [18600/64305], Loss: 5.2759\n",
      "Epoch [1/2], Step [18610/64305], Loss: 5.0597\n",
      "Epoch [1/2], Step [18620/64305], Loss: 5.1494\n",
      "Epoch [1/2], Step [18630/64305], Loss: 5.0440\n",
      "Epoch [1/2], Step [18640/64305], Loss: 4.9743\n",
      "Epoch [1/2], Step [18650/64305], Loss: 5.0910\n",
      "Epoch [1/2], Step [18660/64305], Loss: 5.1932\n",
      "Epoch [1/2], Step [18670/64305], Loss: 5.3009\n",
      "Epoch [1/2], Step [18680/64305], Loss: 5.1959\n",
      "Epoch [1/2], Step [18690/64305], Loss: 5.2735\n",
      "Epoch [1/2], Step [18700/64305], Loss: 5.1755\n",
      "Epoch [1/2], Step [18710/64305], Loss: 5.2361\n",
      "Epoch [1/2], Step [18720/64305], Loss: 5.1789\n",
      "Epoch [1/2], Step [18730/64305], Loss: 5.3364\n",
      "Epoch [1/2], Step [18740/64305], Loss: 5.2513\n",
      "Epoch [1/2], Step [18750/64305], Loss: 4.9020\n",
      "Epoch [1/2], Step [18760/64305], Loss: 4.9739\n",
      "Epoch [1/2], Step [18770/64305], Loss: 5.2576\n",
      "Epoch [1/2], Step [18780/64305], Loss: 5.2374\n",
      "Epoch [1/2], Step [18790/64305], Loss: 5.2010\n",
      "Epoch [1/2], Step [18800/64305], Loss: 4.9792\n",
      "Epoch [1/2], Step [18810/64305], Loss: 5.0601\n",
      "Epoch [1/2], Step [18820/64305], Loss: 4.8538\n",
      "Epoch [1/2], Step [18830/64305], Loss: 5.1780\n",
      "Epoch [1/2], Step [18840/64305], Loss: 5.4984\n",
      "Epoch [1/2], Step [18850/64305], Loss: 5.3224\n",
      "Epoch [1/2], Step [18860/64305], Loss: 5.1065\n",
      "Epoch [1/2], Step [18870/64305], Loss: 5.1418\n",
      "Epoch [1/2], Step [18880/64305], Loss: 5.4153\n",
      "Epoch [1/2], Step [18890/64305], Loss: 5.2010\n",
      "Epoch [1/2], Step [18900/64305], Loss: 5.0816\n",
      "Epoch [1/2], Step [18910/64305], Loss: 5.2652\n",
      "Epoch [1/2], Step [18920/64305], Loss: 5.0829\n",
      "Epoch [1/2], Step [18930/64305], Loss: 4.9208\n",
      "Epoch [1/2], Step [18940/64305], Loss: 5.4226\n",
      "Epoch [1/2], Step [18950/64305], Loss: 4.9986\n",
      "Epoch [1/2], Step [18960/64305], Loss: 5.0229\n",
      "Epoch [1/2], Step [18970/64305], Loss: 5.1404\n",
      "Epoch [1/2], Step [18980/64305], Loss: 4.8885\n",
      "Epoch [1/2], Step [18990/64305], Loss: 5.3200\n",
      "Epoch [1/2], Step [19000/64305], Loss: 5.1381\n",
      "Epoch [1/2], Step [19010/64305], Loss: 4.9308\n",
      "Epoch [1/2], Step [19020/64305], Loss: 5.0672\n",
      "Epoch [1/2], Step [19030/64305], Loss: 5.0391\n",
      "Epoch [1/2], Step [19040/64305], Loss: 5.2551\n",
      "Epoch [1/2], Step [19050/64305], Loss: 5.0917\n",
      "Epoch [1/2], Step [19060/64305], Loss: 5.2668\n",
      "Epoch [1/2], Step [19070/64305], Loss: 5.2496\n",
      "Epoch [1/2], Step [19080/64305], Loss: 5.1430\n",
      "Epoch [1/2], Step [19090/64305], Loss: 5.2006\n",
      "Epoch [1/2], Step [19100/64305], Loss: 5.1815\n",
      "Epoch [1/2], Step [19110/64305], Loss: 5.2246\n",
      "Epoch [1/2], Step [19120/64305], Loss: 5.2818\n",
      "Epoch [1/2], Step [19130/64305], Loss: 4.9312\n",
      "Epoch [1/2], Step [19140/64305], Loss: 5.1375\n",
      "Epoch [1/2], Step [19150/64305], Loss: 5.0614\n",
      "Epoch [1/2], Step [19160/64305], Loss: 5.1461\n",
      "Epoch [1/2], Step [19170/64305], Loss: 5.1424\n",
      "Epoch [1/2], Step [19180/64305], Loss: 5.0391\n",
      "Epoch [1/2], Step [19190/64305], Loss: 5.1725\n",
      "Epoch [1/2], Step [19200/64305], Loss: 5.3249\n",
      "Epoch [1/2], Step [19210/64305], Loss: 5.0274\n",
      "Epoch [1/2], Step [19220/64305], Loss: 5.0186\n",
      "Epoch [1/2], Step [19230/64305], Loss: 5.0660\n",
      "Epoch [1/2], Step [19240/64305], Loss: 4.8537\n",
      "Epoch [1/2], Step [19250/64305], Loss: 5.0812\n",
      "Epoch [1/2], Step [19260/64305], Loss: 5.0979\n",
      "Epoch [1/2], Step [19270/64305], Loss: 4.9689\n",
      "Epoch [1/2], Step [19280/64305], Loss: 4.9513\n",
      "Epoch [1/2], Step [19290/64305], Loss: 5.0464\n",
      "Epoch [1/2], Step [19300/64305], Loss: 4.9883\n",
      "Epoch [1/2], Step [19310/64305], Loss: 5.0651\n",
      "Epoch [1/2], Step [19320/64305], Loss: 5.1088\n",
      "Epoch [1/2], Step [19330/64305], Loss: 4.9656\n",
      "Epoch [1/2], Step [19340/64305], Loss: 5.3539\n",
      "Epoch [1/2], Step [19350/64305], Loss: 5.0661\n",
      "Epoch [1/2], Step [19360/64305], Loss: 5.2094\n",
      "Epoch [1/2], Step [19370/64305], Loss: 5.0643\n",
      "Epoch [1/2], Step [19380/64305], Loss: 5.1221\n",
      "Epoch [1/2], Step [19390/64305], Loss: 5.0258\n",
      "Epoch [1/2], Step [19400/64305], Loss: 5.1428\n",
      "Epoch [1/2], Step [19410/64305], Loss: 5.0424\n",
      "Epoch [1/2], Step [19420/64305], Loss: 5.0021\n",
      "Epoch [1/2], Step [19430/64305], Loss: 5.0450\n",
      "Epoch [1/2], Step [19440/64305], Loss: 5.2890\n",
      "Epoch [1/2], Step [19450/64305], Loss: 5.0447\n",
      "Epoch [1/2], Step [19460/64305], Loss: 5.1415\n",
      "Epoch [1/2], Step [19470/64305], Loss: 5.3599\n",
      "Epoch [1/2], Step [19480/64305], Loss: 5.0301\n",
      "Epoch [1/2], Step [19490/64305], Loss: 5.0189\n",
      "Epoch [1/2], Step [19500/64305], Loss: 4.7546\n",
      "Epoch [1/2], Step [19510/64305], Loss: 5.3367\n",
      "Epoch [1/2], Step [19520/64305], Loss: 5.2391\n",
      "Epoch [1/2], Step [19530/64305], Loss: 5.1876\n",
      "Epoch [1/2], Step [19540/64305], Loss: 5.1182\n",
      "Epoch [1/2], Step [19550/64305], Loss: 5.2423\n",
      "Epoch [1/2], Step [19560/64305], Loss: 5.0290\n",
      "Epoch [1/2], Step [19570/64305], Loss: 5.0728\n",
      "Epoch [1/2], Step [19580/64305], Loss: 5.2306\n",
      "Epoch [1/2], Step [19590/64305], Loss: 5.1802\n",
      "Epoch [1/2], Step [19600/64305], Loss: 5.1812\n",
      "Epoch [1/2], Step [19610/64305], Loss: 5.1484\n",
      "Epoch [1/2], Step [19620/64305], Loss: 5.3070\n",
      "Epoch [1/2], Step [19630/64305], Loss: 5.2340\n",
      "Epoch [1/2], Step [19640/64305], Loss: 4.9083\n",
      "Epoch [1/2], Step [19650/64305], Loss: 4.9088\n",
      "Epoch [1/2], Step [19660/64305], Loss: 5.3044\n",
      "Epoch [1/2], Step [19670/64305], Loss: 5.2221\n",
      "Epoch [1/2], Step [19680/64305], Loss: 5.2290\n",
      "Epoch [1/2], Step [19690/64305], Loss: 5.0883\n",
      "Epoch [1/2], Step [19700/64305], Loss: 4.9953\n",
      "Epoch [1/2], Step [19710/64305], Loss: 5.0836\n",
      "Epoch [1/2], Step [19720/64305], Loss: 5.1495\n",
      "Epoch [1/2], Step [19730/64305], Loss: 5.2885\n",
      "Epoch [1/2], Step [19740/64305], Loss: 5.1682\n",
      "Epoch [1/2], Step [19750/64305], Loss: 5.1646\n",
      "Epoch [1/2], Step [19760/64305], Loss: 5.1965\n",
      "Epoch [1/2], Step [19770/64305], Loss: 5.0922\n",
      "Epoch [1/2], Step [19780/64305], Loss: 5.0740\n",
      "Epoch [1/2], Step [19790/64305], Loss: 5.0597\n",
      "Epoch [1/2], Step [19800/64305], Loss: 4.8938\n",
      "Epoch [1/2], Step [19810/64305], Loss: 4.9978\n",
      "Epoch [1/2], Step [19820/64305], Loss: 5.1572\n",
      "Epoch [1/2], Step [19830/64305], Loss: 4.9829\n",
      "Epoch [1/2], Step [19840/64305], Loss: 5.1678\n",
      "Epoch [1/2], Step [19850/64305], Loss: 5.1075\n",
      "Epoch [1/2], Step [19860/64305], Loss: 5.2751\n",
      "Epoch [1/2], Step [19870/64305], Loss: 5.1909\n",
      "Epoch [1/2], Step [19880/64305], Loss: 5.0574\n",
      "Epoch [1/2], Step [19890/64305], Loss: 4.9926\n",
      "Epoch [1/2], Step [19900/64305], Loss: 5.2174\n",
      "Epoch [1/2], Step [19910/64305], Loss: 5.0407\n",
      "Epoch [1/2], Step [19920/64305], Loss: 5.0986\n",
      "Epoch [1/2], Step [19930/64305], Loss: 5.2346\n",
      "Epoch [1/2], Step [19940/64305], Loss: 5.0105\n",
      "Epoch [1/2], Step [19950/64305], Loss: 5.1880\n",
      "Epoch [1/2], Step [19960/64305], Loss: 5.0436\n",
      "Epoch [1/2], Step [19970/64305], Loss: 5.3549\n",
      "Epoch [1/2], Step [19980/64305], Loss: 5.1754\n",
      "Epoch [1/2], Step [19990/64305], Loss: 5.1818\n",
      "Epoch [1/2], Step [20000/64305], Loss: 5.1286\n",
      "Epoch [1/2], Step [20010/64305], Loss: 5.0424\n",
      "Epoch [1/2], Step [20020/64305], Loss: 5.1023\n",
      "Epoch [1/2], Step [20030/64305], Loss: 5.0778\n",
      "Epoch [1/2], Step [20040/64305], Loss: 5.1867\n",
      "Epoch [1/2], Step [20050/64305], Loss: 5.2152\n",
      "Epoch [1/2], Step [20060/64305], Loss: 5.0922\n",
      "Epoch [1/2], Step [20070/64305], Loss: 5.2422\n",
      "Epoch [1/2], Step [20080/64305], Loss: 5.2349\n",
      "Epoch [1/2], Step [20090/64305], Loss: 5.4306\n",
      "Epoch [1/2], Step [20100/64305], Loss: 5.2962\n",
      "Epoch [1/2], Step [20110/64305], Loss: 4.9262\n",
      "Epoch [1/2], Step [20120/64305], Loss: 5.0363\n",
      "Epoch [1/2], Step [20130/64305], Loss: 4.9302\n",
      "Epoch [1/2], Step [20140/64305], Loss: 5.1760\n",
      "Epoch [1/2], Step [20150/64305], Loss: 4.9243\n",
      "Epoch [1/2], Step [20160/64305], Loss: 4.9365\n",
      "Epoch [1/2], Step [20170/64305], Loss: 5.1550\n",
      "Epoch [1/2], Step [20180/64305], Loss: 5.0852\n",
      "Epoch [1/2], Step [20190/64305], Loss: 5.1470\n",
      "Epoch [1/2], Step [20200/64305], Loss: 5.1561\n",
      "Epoch [1/2], Step [20210/64305], Loss: 5.2076\n",
      "Epoch [1/2], Step [20220/64305], Loss: 5.3646\n",
      "Epoch [1/2], Step [20230/64305], Loss: 5.1172\n",
      "Epoch [1/2], Step [20240/64305], Loss: 4.9871\n",
      "Epoch [1/2], Step [20250/64305], Loss: 5.0466\n",
      "Epoch [1/2], Step [20260/64305], Loss: 4.9859\n",
      "Epoch [1/2], Step [20270/64305], Loss: 5.0052\n",
      "Epoch [1/2], Step [20280/64305], Loss: 5.0630\n",
      "Epoch [1/2], Step [20290/64305], Loss: 4.9807\n",
      "Epoch [1/2], Step [20300/64305], Loss: 5.0444\n",
      "Epoch [1/2], Step [20310/64305], Loss: 5.1042\n",
      "Epoch [1/2], Step [20320/64305], Loss: 5.2392\n",
      "Epoch [1/2], Step [20330/64305], Loss: 5.2568\n",
      "Epoch [1/2], Step [20340/64305], Loss: 5.2509\n",
      "Epoch [1/2], Step [20350/64305], Loss: 5.1698\n",
      "Epoch [1/2], Step [20360/64305], Loss: 5.2567\n",
      "Epoch [1/2], Step [20370/64305], Loss: 5.2747\n",
      "Epoch [1/2], Step [20380/64305], Loss: 5.2235\n",
      "Epoch [1/2], Step [20390/64305], Loss: 4.9907\n",
      "Epoch [1/2], Step [20400/64305], Loss: 5.2438\n",
      "Epoch [1/2], Step [20410/64305], Loss: 5.2661\n",
      "Epoch [1/2], Step [20420/64305], Loss: 5.1406\n",
      "Epoch [1/2], Step [20430/64305], Loss: 5.3749\n",
      "Epoch [1/2], Step [20440/64305], Loss: 4.9059\n",
      "Epoch [1/2], Step [20450/64305], Loss: 5.2451\n",
      "Epoch [1/2], Step [20460/64305], Loss: 5.0995\n",
      "Epoch [1/2], Step [20470/64305], Loss: 4.9910\n",
      "Epoch [1/2], Step [20480/64305], Loss: 5.0582\n",
      "Epoch [1/2], Step [20490/64305], Loss: 4.9797\n",
      "Epoch [1/2], Step [20500/64305], Loss: 5.1949\n",
      "Epoch [1/2], Step [20510/64305], Loss: 5.0702\n",
      "Epoch [1/2], Step [20520/64305], Loss: 5.0106\n",
      "Epoch [1/2], Step [20530/64305], Loss: 5.0515\n",
      "Epoch [1/2], Step [20540/64305], Loss: 4.9118\n",
      "Epoch [1/2], Step [20550/64305], Loss: 5.0361\n",
      "Epoch [1/2], Step [20560/64305], Loss: 5.1252\n",
      "Epoch [1/2], Step [20570/64305], Loss: 4.9632\n",
      "Epoch [1/2], Step [20580/64305], Loss: 4.9318\n",
      "Epoch [1/2], Step [20590/64305], Loss: 5.2528\n",
      "Epoch [1/2], Step [20600/64305], Loss: 5.1127\n",
      "Epoch [1/2], Step [20610/64305], Loss: 5.2194\n",
      "Epoch [1/2], Step [20620/64305], Loss: 5.1013\n",
      "Epoch [1/2], Step [20630/64305], Loss: 5.2111\n",
      "Epoch [1/2], Step [20640/64305], Loss: 5.2665\n",
      "Epoch [1/2], Step [20650/64305], Loss: 5.0901\n",
      "Epoch [1/2], Step [20660/64305], Loss: 5.2764\n",
      "Epoch [1/2], Step [20670/64305], Loss: 5.0038\n",
      "Epoch [1/2], Step [20680/64305], Loss: 5.1652\n",
      "Epoch [1/2], Step [20690/64305], Loss: 5.2544\n",
      "Epoch [1/2], Step [20700/64305], Loss: 5.0489\n",
      "Epoch [1/2], Step [20710/64305], Loss: 5.0321\n",
      "Epoch [1/2], Step [20720/64305], Loss: 4.9053\n",
      "Epoch [1/2], Step [20730/64305], Loss: 5.2022\n",
      "Epoch [1/2], Step [20740/64305], Loss: 5.1058\n",
      "Epoch [1/2], Step [20750/64305], Loss: 5.0988\n",
      "Epoch [1/2], Step [20760/64305], Loss: 5.2317\n",
      "Epoch [1/2], Step [20770/64305], Loss: 5.1212\n",
      "Epoch [1/2], Step [20780/64305], Loss: 5.1277\n",
      "Epoch [1/2], Step [20790/64305], Loss: 5.0951\n",
      "Epoch [1/2], Step [20800/64305], Loss: 5.0713\n",
      "Epoch [1/2], Step [20810/64305], Loss: 5.0219\n",
      "Epoch [1/2], Step [20820/64305], Loss: 4.9290\n",
      "Epoch [1/2], Step [20830/64305], Loss: 4.9514\n",
      "Epoch [1/2], Step [20840/64305], Loss: 5.4080\n",
      "Epoch [1/2], Step [20850/64305], Loss: 5.1125\n",
      "Epoch [1/2], Step [20860/64305], Loss: 5.3344\n",
      "Epoch [1/2], Step [20870/64305], Loss: 5.1721\n",
      "Epoch [1/2], Step [20880/64305], Loss: 5.2633\n",
      "Epoch [1/2], Step [20890/64305], Loss: 5.2059\n",
      "Epoch [1/2], Step [20900/64305], Loss: 5.1756\n",
      "Epoch [1/2], Step [20910/64305], Loss: 5.0851\n",
      "Epoch [1/2], Step [20920/64305], Loss: 4.9228\n",
      "Epoch [1/2], Step [20930/64305], Loss: 5.2008\n",
      "Epoch [1/2], Step [20940/64305], Loss: 4.9552\n",
      "Epoch [1/2], Step [20950/64305], Loss: 5.0885\n",
      "Epoch [1/2], Step [20960/64305], Loss: 5.1513\n",
      "Epoch [1/2], Step [20970/64305], Loss: 5.1944\n",
      "Epoch [1/2], Step [20980/64305], Loss: 5.2305\n",
      "Epoch [1/2], Step [20990/64305], Loss: 5.1413\n",
      "Epoch [1/2], Step [21000/64305], Loss: 5.2298\n",
      "Epoch [1/2], Step [21010/64305], Loss: 5.1964\n",
      "Epoch [1/2], Step [21020/64305], Loss: 5.1004\n",
      "Epoch [1/2], Step [21030/64305], Loss: 5.0855\n",
      "Epoch [1/2], Step [21040/64305], Loss: 5.2227\n",
      "Epoch [1/2], Step [21050/64305], Loss: 5.1671\n",
      "Epoch [1/2], Step [21060/64305], Loss: 5.0382\n",
      "Epoch [1/2], Step [21070/64305], Loss: 5.0335\n",
      "Epoch [1/2], Step [21080/64305], Loss: 5.3454\n",
      "Epoch [1/2], Step [21090/64305], Loss: 5.2159\n",
      "Epoch [1/2], Step [21100/64305], Loss: 5.0497\n",
      "Epoch [1/2], Step [21110/64305], Loss: 5.0439\n",
      "Epoch [1/2], Step [21120/64305], Loss: 5.3648\n",
      "Epoch [1/2], Step [21130/64305], Loss: 5.1593\n",
      "Epoch [1/2], Step [21140/64305], Loss: 5.0198\n",
      "Epoch [1/2], Step [21150/64305], Loss: 5.1754\n",
      "Epoch [1/2], Step [21160/64305], Loss: 5.1585\n",
      "Epoch [1/2], Step [21170/64305], Loss: 5.1538\n",
      "Epoch [1/2], Step [21180/64305], Loss: 5.0104\n",
      "Epoch [1/2], Step [21190/64305], Loss: 4.9600\n",
      "Epoch [1/2], Step [21200/64305], Loss: 5.1797\n",
      "Epoch [1/2], Step [21210/64305], Loss: 5.1391\n",
      "Epoch [1/2], Step [21220/64305], Loss: 5.1355\n",
      "Epoch [1/2], Step [21230/64305], Loss: 5.0567\n",
      "Epoch [1/2], Step [21240/64305], Loss: 5.0964\n",
      "Epoch [1/2], Step [21250/64305], Loss: 5.0943\n",
      "Epoch [1/2], Step [21260/64305], Loss: 4.9835\n",
      "Epoch [1/2], Step [21270/64305], Loss: 5.2399\n",
      "Epoch [1/2], Step [21280/64305], Loss: 5.2378\n",
      "Epoch [1/2], Step [21290/64305], Loss: 5.1397\n",
      "Epoch [1/2], Step [21300/64305], Loss: 4.9349\n",
      "Epoch [1/2], Step [21310/64305], Loss: 5.4133\n",
      "Epoch [1/2], Step [21320/64305], Loss: 5.2674\n",
      "Epoch [1/2], Step [21330/64305], Loss: 4.9405\n",
      "Epoch [1/2], Step [21340/64305], Loss: 5.1916\n",
      "Epoch [1/2], Step [21350/64305], Loss: 5.2671\n",
      "Epoch [1/2], Step [21360/64305], Loss: 5.2062\n",
      "Epoch [1/2], Step [21370/64305], Loss: 5.0483\n",
      "Epoch [1/2], Step [21380/64305], Loss: 5.0504\n",
      "Epoch [1/2], Step [21390/64305], Loss: 5.0159\n",
      "Epoch [1/2], Step [21400/64305], Loss: 5.0329\n",
      "Epoch [1/2], Step [21410/64305], Loss: 5.2878\n",
      "Epoch [1/2], Step [21420/64305], Loss: 4.9308\n",
      "Epoch [1/2], Step [21430/64305], Loss: 5.0458\n",
      "Epoch [1/2], Step [21440/64305], Loss: 5.3015\n",
      "Epoch [1/2], Step [21450/64305], Loss: 5.0743\n",
      "Epoch [1/2], Step [21460/64305], Loss: 5.1993\n",
      "Epoch [1/2], Step [21470/64305], Loss: 5.1844\n",
      "Epoch [1/2], Step [21480/64305], Loss: 5.3058\n",
      "Epoch [1/2], Step [21490/64305], Loss: 5.0288\n",
      "Epoch [1/2], Step [21500/64305], Loss: 5.0202\n",
      "Epoch [1/2], Step [21510/64305], Loss: 5.2665\n",
      "Epoch [1/2], Step [21520/64305], Loss: 5.0485\n",
      "Epoch [1/2], Step [21530/64305], Loss: 5.3182\n",
      "Epoch [1/2], Step [21540/64305], Loss: 5.0594\n",
      "Epoch [1/2], Step [21550/64305], Loss: 5.2548\n",
      "Epoch [1/2], Step [21560/64305], Loss: 5.2948\n",
      "Epoch [1/2], Step [21570/64305], Loss: 5.0398\n",
      "Epoch [1/2], Step [21580/64305], Loss: 5.1485\n",
      "Epoch [1/2], Step [21590/64305], Loss: 5.1277\n",
      "Epoch [1/2], Step [21600/64305], Loss: 5.2134\n",
      "Epoch [1/2], Step [21610/64305], Loss: 5.2358\n",
      "Epoch [1/2], Step [21620/64305], Loss: 5.0458\n",
      "Epoch [1/2], Step [21630/64305], Loss: 4.9574\n",
      "Epoch [1/2], Step [21640/64305], Loss: 5.1059\n",
      "Epoch [1/2], Step [21650/64305], Loss: 5.2025\n",
      "Epoch [1/2], Step [21660/64305], Loss: 5.2514\n",
      "Epoch [1/2], Step [21670/64305], Loss: 5.0350\n",
      "Epoch [1/2], Step [21680/64305], Loss: 5.0686\n",
      "Epoch [1/2], Step [21690/64305], Loss: 5.1024\n",
      "Epoch [1/2], Step [21700/64305], Loss: 5.3365\n",
      "Epoch [1/2], Step [21710/64305], Loss: 5.0654\n",
      "Epoch [1/2], Step [21720/64305], Loss: 5.2145\n",
      "Epoch [1/2], Step [21730/64305], Loss: 5.1603\n",
      "Epoch [1/2], Step [21740/64305], Loss: 5.1474\n",
      "Epoch [1/2], Step [21750/64305], Loss: 5.1958\n",
      "Epoch [1/2], Step [21760/64305], Loss: 4.8495\n",
      "Epoch [1/2], Step [21770/64305], Loss: 5.3682\n",
      "Epoch [1/2], Step [21780/64305], Loss: 4.8530\n",
      "Epoch [1/2], Step [21790/64305], Loss: 5.1108\n",
      "Epoch [1/2], Step [21800/64305], Loss: 5.2123\n",
      "Epoch [1/2], Step [21810/64305], Loss: 5.0392\n",
      "Epoch [1/2], Step [21820/64305], Loss: 4.9722\n",
      "Epoch [1/2], Step [21830/64305], Loss: 5.0964\n",
      "Epoch [1/2], Step [21840/64305], Loss: 5.0871\n",
      "Epoch [1/2], Step [21850/64305], Loss: 5.2295\n",
      "Epoch [1/2], Step [21860/64305], Loss: 5.2076\n",
      "Epoch [1/2], Step [21870/64305], Loss: 5.0263\n",
      "Epoch [1/2], Step [21880/64305], Loss: 5.0130\n",
      "Epoch [1/2], Step [21890/64305], Loss: 5.1528\n",
      "Epoch [1/2], Step [21900/64305], Loss: 5.1909\n",
      "Epoch [1/2], Step [21910/64305], Loss: 5.0535\n",
      "Epoch [1/2], Step [21920/64305], Loss: 5.0423\n",
      "Epoch [1/2], Step [21930/64305], Loss: 5.2773\n",
      "Epoch [1/2], Step [21940/64305], Loss: 5.1406\n",
      "Epoch [1/2], Step [21950/64305], Loss: 5.2467\n",
      "Epoch [1/2], Step [21960/64305], Loss: 5.1755\n",
      "Epoch [1/2], Step [21970/64305], Loss: 5.0929\n",
      "Epoch [1/2], Step [21980/64305], Loss: 5.1680\n",
      "Epoch [1/2], Step [21990/64305], Loss: 5.3084\n",
      "Epoch [1/2], Step [22000/64305], Loss: 4.9427\n",
      "Epoch [1/2], Step [22010/64305], Loss: 4.9511\n",
      "Epoch [1/2], Step [22020/64305], Loss: 5.2471\n",
      "Epoch [1/2], Step [22030/64305], Loss: 5.4392\n",
      "Epoch [1/2], Step [22040/64305], Loss: 4.9596\n",
      "Epoch [1/2], Step [22050/64305], Loss: 5.3207\n",
      "Epoch [1/2], Step [22060/64305], Loss: 5.0868\n",
      "Epoch [1/2], Step [22070/64305], Loss: 5.1459\n",
      "Epoch [1/2], Step [22080/64305], Loss: 5.2622\n",
      "Epoch [1/2], Step [22090/64305], Loss: 5.0971\n",
      "Epoch [1/2], Step [22100/64305], Loss: 5.1655\n",
      "Epoch [1/2], Step [22110/64305], Loss: 4.8621\n",
      "Epoch [1/2], Step [22120/64305], Loss: 5.1067\n",
      "Epoch [1/2], Step [22130/64305], Loss: 5.1734\n",
      "Epoch [1/2], Step [22140/64305], Loss: 5.1349\n",
      "Epoch [1/2], Step [22150/64305], Loss: 5.1929\n",
      "Epoch [1/2], Step [22160/64305], Loss: 5.1424\n",
      "Epoch [1/2], Step [22170/64305], Loss: 5.0405\n",
      "Epoch [1/2], Step [22180/64305], Loss: 5.2843\n",
      "Epoch [1/2], Step [22190/64305], Loss: 5.0967\n",
      "Epoch [1/2], Step [22200/64305], Loss: 5.1649\n",
      "Epoch [1/2], Step [22210/64305], Loss: 5.0941\n",
      "Epoch [1/2], Step [22220/64305], Loss: 5.0921\n",
      "Epoch [1/2], Step [22230/64305], Loss: 5.2894\n",
      "Epoch [1/2], Step [22240/64305], Loss: 5.1357\n",
      "Epoch [1/2], Step [22250/64305], Loss: 5.1201\n",
      "Epoch [1/2], Step [22260/64305], Loss: 5.0973\n",
      "Epoch [1/2], Step [22270/64305], Loss: 5.0959\n",
      "Epoch [1/2], Step [22280/64305], Loss: 5.0968\n",
      "Epoch [1/2], Step [22290/64305], Loss: 5.0419\n",
      "Epoch [1/2], Step [22300/64305], Loss: 5.1123\n",
      "Epoch [1/2], Step [22310/64305], Loss: 5.1363\n",
      "Epoch [1/2], Step [22320/64305], Loss: 4.9808\n",
      "Epoch [1/2], Step [22330/64305], Loss: 5.1565\n",
      "Epoch [1/2], Step [22340/64305], Loss: 5.0767\n",
      "Epoch [1/2], Step [22350/64305], Loss: 4.9575\n",
      "Epoch [1/2], Step [22360/64305], Loss: 4.9460\n",
      "Epoch [1/2], Step [22370/64305], Loss: 5.1002\n",
      "Epoch [1/2], Step [22380/64305], Loss: 5.3003\n",
      "Epoch [1/2], Step [22390/64305], Loss: 5.1615\n",
      "Epoch [1/2], Step [22400/64305], Loss: 5.1508\n",
      "Epoch [1/2], Step [22410/64305], Loss: 5.1451\n",
      "Epoch [1/2], Step [22420/64305], Loss: 5.1599\n",
      "Epoch [1/2], Step [22430/64305], Loss: 4.9838\n",
      "Epoch [1/2], Step [22440/64305], Loss: 5.1024\n",
      "Epoch [1/2], Step [22450/64305], Loss: 4.8807\n",
      "Epoch [1/2], Step [22460/64305], Loss: 4.9591\n",
      "Epoch [1/2], Step [22470/64305], Loss: 5.2757\n",
      "Epoch [1/2], Step [22480/64305], Loss: 5.1103\n",
      "Epoch [1/2], Step [22490/64305], Loss: 4.9199\n",
      "Epoch [1/2], Step [22500/64305], Loss: 4.8363\n",
      "Epoch [1/2], Step [22510/64305], Loss: 4.9477\n",
      "Epoch [1/2], Step [22520/64305], Loss: 5.1574\n",
      "Epoch [1/2], Step [22530/64305], Loss: 5.1372\n",
      "Epoch [1/2], Step [22540/64305], Loss: 5.3533\n",
      "Epoch [1/2], Step [22550/64305], Loss: 4.8867\n",
      "Epoch [1/2], Step [22560/64305], Loss: 5.0712\n",
      "Epoch [1/2], Step [22570/64305], Loss: 5.2619\n",
      "Epoch [1/2], Step [22580/64305], Loss: 4.8990\n",
      "Epoch [1/2], Step [22590/64305], Loss: 5.1684\n",
      "Epoch [1/2], Step [22600/64305], Loss: 4.9878\n",
      "Epoch [1/2], Step [22610/64305], Loss: 5.2914\n",
      "Epoch [1/2], Step [22620/64305], Loss: 5.1071\n",
      "Epoch [1/2], Step [22630/64305], Loss: 5.1452\n",
      "Epoch [1/2], Step [22640/64305], Loss: 5.2490\n",
      "Epoch [1/2], Step [22650/64305], Loss: 5.2229\n",
      "Epoch [1/2], Step [22660/64305], Loss: 5.0336\n",
      "Epoch [1/2], Step [22670/64305], Loss: 5.2450\n",
      "Epoch [1/2], Step [22680/64305], Loss: 5.0361\n",
      "Epoch [1/2], Step [22690/64305], Loss: 5.2209\n",
      "Epoch [1/2], Step [22700/64305], Loss: 5.1170\n",
      "Epoch [1/2], Step [22710/64305], Loss: 4.9929\n",
      "Epoch [1/2], Step [22720/64305], Loss: 5.0445\n",
      "Epoch [1/2], Step [22730/64305], Loss: 5.1329\n",
      "Epoch [1/2], Step [22740/64305], Loss: 5.1625\n",
      "Epoch [1/2], Step [22750/64305], Loss: 4.9230\n",
      "Epoch [1/2], Step [22760/64305], Loss: 5.2301\n",
      "Epoch [1/2], Step [22770/64305], Loss: 4.9624\n",
      "Epoch [1/2], Step [22780/64305], Loss: 5.0327\n",
      "Epoch [1/2], Step [22790/64305], Loss: 5.3373\n",
      "Epoch [1/2], Step [22800/64305], Loss: 5.2071\n",
      "Epoch [1/2], Step [22810/64305], Loss: 5.3551\n",
      "Epoch [1/2], Step [22820/64305], Loss: 5.1574\n",
      "Epoch [1/2], Step [22830/64305], Loss: 5.0144\n",
      "Epoch [1/2], Step [22840/64305], Loss: 4.9296\n",
      "Epoch [1/2], Step [22850/64305], Loss: 5.2928\n",
      "Epoch [1/2], Step [22860/64305], Loss: 5.1502\n",
      "Epoch [1/2], Step [22870/64305], Loss: 4.9149\n",
      "Epoch [1/2], Step [22880/64305], Loss: 5.1999\n",
      "Epoch [1/2], Step [22890/64305], Loss: 5.0135\n",
      "Epoch [1/2], Step [22900/64305], Loss: 5.2417\n",
      "Epoch [1/2], Step [22910/64305], Loss: 5.1599\n",
      "Epoch [1/2], Step [22920/64305], Loss: 5.1169\n",
      "Epoch [1/2], Step [22930/64305], Loss: 4.8879\n",
      "Epoch [1/2], Step [22940/64305], Loss: 5.1403\n",
      "Epoch [1/2], Step [22950/64305], Loss: 5.0516\n",
      "Epoch [1/2], Step [22960/64305], Loss: 5.1239\n",
      "Epoch [1/2], Step [22970/64305], Loss: 5.1322\n",
      "Epoch [1/2], Step [22980/64305], Loss: 5.3158\n",
      "Epoch [1/2], Step [22990/64305], Loss: 5.0426\n",
      "Epoch [1/2], Step [23000/64305], Loss: 5.2512\n",
      "Epoch [1/2], Step [23010/64305], Loss: 5.0788\n",
      "Epoch [1/2], Step [23020/64305], Loss: 5.1538\n",
      "Epoch [1/2], Step [23030/64305], Loss: 4.9623\n",
      "Epoch [1/2], Step [23040/64305], Loss: 4.9988\n",
      "Epoch [1/2], Step [23050/64305], Loss: 5.1351\n",
      "Epoch [1/2], Step [23060/64305], Loss: 5.1604\n",
      "Epoch [1/2], Step [23070/64305], Loss: 5.0897\n",
      "Epoch [1/2], Step [23080/64305], Loss: 4.9491\n",
      "Epoch [1/2], Step [23090/64305], Loss: 5.2582\n",
      "Epoch [1/2], Step [23100/64305], Loss: 5.0210\n",
      "Epoch [1/2], Step [23110/64305], Loss: 5.0053\n",
      "Epoch [1/2], Step [23120/64305], Loss: 5.0442\n",
      "Epoch [1/2], Step [23130/64305], Loss: 5.1918\n",
      "Epoch [1/2], Step [23140/64305], Loss: 4.9389\n",
      "Epoch [1/2], Step [23150/64305], Loss: 5.4324\n",
      "Epoch [1/2], Step [23160/64305], Loss: 4.9011\n",
      "Epoch [1/2], Step [23170/64305], Loss: 5.2603\n",
      "Epoch [1/2], Step [23180/64305], Loss: 5.1882\n",
      "Epoch [1/2], Step [23190/64305], Loss: 5.1174\n",
      "Epoch [1/2], Step [23200/64305], Loss: 5.2490\n",
      "Epoch [1/2], Step [23210/64305], Loss: 5.1685\n",
      "Epoch [1/2], Step [23220/64305], Loss: 5.0848\n",
      "Epoch [1/2], Step [23230/64305], Loss: 5.1065\n",
      "Epoch [1/2], Step [23240/64305], Loss: 5.0673\n",
      "Epoch [1/2], Step [23250/64305], Loss: 5.0211\n",
      "Epoch [1/2], Step [23260/64305], Loss: 4.9006\n",
      "Epoch [1/2], Step [23270/64305], Loss: 5.3210\n",
      "Epoch [1/2], Step [23280/64305], Loss: 5.2087\n",
      "Epoch [1/2], Step [23290/64305], Loss: 5.3965\n",
      "Epoch [1/2], Step [23300/64305], Loss: 4.9380\n",
      "Epoch [1/2], Step [23310/64305], Loss: 5.0269\n",
      "Epoch [1/2], Step [23320/64305], Loss: 5.0806\n",
      "Epoch [1/2], Step [23330/64305], Loss: 5.0683\n",
      "Epoch [1/2], Step [23340/64305], Loss: 5.2227\n",
      "Epoch [1/2], Step [23350/64305], Loss: 5.1327\n",
      "Epoch [1/2], Step [23360/64305], Loss: 5.1224\n",
      "Epoch [1/2], Step [23370/64305], Loss: 4.9711\n",
      "Epoch [1/2], Step [23380/64305], Loss: 5.0715\n",
      "Epoch [1/2], Step [23390/64305], Loss: 5.1219\n",
      "Epoch [1/2], Step [23400/64305], Loss: 5.1847\n",
      "Epoch [1/2], Step [23410/64305], Loss: 5.1455\n",
      "Epoch [1/2], Step [23420/64305], Loss: 4.9488\n",
      "Epoch [1/2], Step [23430/64305], Loss: 5.1666\n",
      "Epoch [1/2], Step [23440/64305], Loss: 5.0431\n",
      "Epoch [1/2], Step [23450/64305], Loss: 5.0593\n",
      "Epoch [1/2], Step [23460/64305], Loss: 5.1073\n",
      "Epoch [1/2], Step [23470/64305], Loss: 5.0044\n",
      "Epoch [1/2], Step [23480/64305], Loss: 5.0236\n",
      "Epoch [1/2], Step [23490/64305], Loss: 5.1951\n",
      "Epoch [1/2], Step [23500/64305], Loss: 5.1951\n",
      "Epoch [1/2], Step [23510/64305], Loss: 5.1625\n",
      "Epoch [1/2], Step [23520/64305], Loss: 5.0591\n",
      "Epoch [1/2], Step [23530/64305], Loss: 5.1078\n",
      "Epoch [1/2], Step [23540/64305], Loss: 5.2132\n",
      "Epoch [1/2], Step [23550/64305], Loss: 4.9896\n",
      "Epoch [1/2], Step [23560/64305], Loss: 4.9259\n",
      "Epoch [1/2], Step [23570/64305], Loss: 5.0508\n",
      "Epoch [1/2], Step [23580/64305], Loss: 5.1075\n",
      "Epoch [1/2], Step [23590/64305], Loss: 5.1761\n",
      "Epoch [1/2], Step [23600/64305], Loss: 5.1312\n",
      "Epoch [1/2], Step [23610/64305], Loss: 5.1578\n",
      "Epoch [1/2], Step [23620/64305], Loss: 5.1354\n",
      "Epoch [1/2], Step [23630/64305], Loss: 5.0840\n",
      "Epoch [1/2], Step [23640/64305], Loss: 5.1864\n",
      "Epoch [1/2], Step [23650/64305], Loss: 5.1593\n",
      "Epoch [1/2], Step [23660/64305], Loss: 5.2394\n",
      "Epoch [1/2], Step [23670/64305], Loss: 5.2101\n",
      "Epoch [1/2], Step [23680/64305], Loss: 5.1790\n",
      "Epoch [1/2], Step [23690/64305], Loss: 5.1228\n",
      "Epoch [1/2], Step [23700/64305], Loss: 4.9935\n",
      "Epoch [1/2], Step [23710/64305], Loss: 5.2373\n",
      "Epoch [1/2], Step [23720/64305], Loss: 5.0844\n",
      "Epoch [1/2], Step [23730/64305], Loss: 5.1344\n",
      "Epoch [1/2], Step [23740/64305], Loss: 5.0666\n",
      "Epoch [1/2], Step [23750/64305], Loss: 5.4378\n",
      "Epoch [1/2], Step [23760/64305], Loss: 5.1158\n",
      "Epoch [1/2], Step [23770/64305], Loss: 5.2984\n",
      "Epoch [1/2], Step [23780/64305], Loss: 4.7770\n",
      "Epoch [1/2], Step [23790/64305], Loss: 4.9574\n",
      "Epoch [1/2], Step [23800/64305], Loss: 5.2162\n",
      "Epoch [1/2], Step [23810/64305], Loss: 5.1073\n",
      "Epoch [1/2], Step [23820/64305], Loss: 5.2479\n",
      "Epoch [1/2], Step [23830/64305], Loss: 5.1101\n",
      "Epoch [1/2], Step [23840/64305], Loss: 5.0210\n",
      "Epoch [1/2], Step [23850/64305], Loss: 5.0182\n",
      "Epoch [1/2], Step [23860/64305], Loss: 5.1282\n",
      "Epoch [1/2], Step [23870/64305], Loss: 4.9794\n",
      "Epoch [1/2], Step [23880/64305], Loss: 4.8454\n",
      "Epoch [1/2], Step [23890/64305], Loss: 5.1822\n",
      "Epoch [1/2], Step [23900/64305], Loss: 5.0875\n",
      "Epoch [1/2], Step [23910/64305], Loss: 4.9346\n",
      "Epoch [1/2], Step [23920/64305], Loss: 5.0193\n",
      "Epoch [1/2], Step [23930/64305], Loss: 5.0364\n",
      "Epoch [1/2], Step [23940/64305], Loss: 5.1538\n",
      "Epoch [1/2], Step [23950/64305], Loss: 5.2004\n",
      "Epoch [1/2], Step [23960/64305], Loss: 5.2106\n",
      "Epoch [1/2], Step [23970/64305], Loss: 4.9254\n",
      "Epoch [1/2], Step [23980/64305], Loss: 4.9168\n",
      "Epoch [1/2], Step [23990/64305], Loss: 5.0619\n",
      "Epoch [1/2], Step [24000/64305], Loss: 5.1095\n",
      "Epoch [1/2], Step [24010/64305], Loss: 5.0374\n",
      "Epoch [1/2], Step [24020/64305], Loss: 5.0197\n",
      "Epoch [1/2], Step [24030/64305], Loss: 5.1627\n",
      "Epoch [1/2], Step [24040/64305], Loss: 5.2452\n",
      "Epoch [1/2], Step [24050/64305], Loss: 4.9941\n",
      "Epoch [1/2], Step [24060/64305], Loss: 5.0632\n",
      "Epoch [1/2], Step [24070/64305], Loss: 4.9330\n",
      "Epoch [1/2], Step [24080/64305], Loss: 5.0231\n",
      "Epoch [1/2], Step [24090/64305], Loss: 5.2230\n",
      "Epoch [1/2], Step [24100/64305], Loss: 5.0076\n",
      "Epoch [1/2], Step [24110/64305], Loss: 5.1026\n",
      "Epoch [1/2], Step [24120/64305], Loss: 5.0311\n",
      "Epoch [1/2], Step [24130/64305], Loss: 4.8761\n",
      "Epoch [1/2], Step [24140/64305], Loss: 5.1905\n",
      "Epoch [1/2], Step [24150/64305], Loss: 5.0220\n",
      "Epoch [1/2], Step [24160/64305], Loss: 5.0657\n",
      "Epoch [1/2], Step [24170/64305], Loss: 4.9820\n",
      "Epoch [1/2], Step [24180/64305], Loss: 5.1986\n",
      "Epoch [1/2], Step [24190/64305], Loss: 5.0053\n",
      "Epoch [1/2], Step [24200/64305], Loss: 5.1907\n",
      "Epoch [1/2], Step [24210/64305], Loss: 5.1193\n",
      "Epoch [1/2], Step [24220/64305], Loss: 5.1499\n",
      "Epoch [1/2], Step [24230/64305], Loss: 5.1181\n",
      "Epoch [1/2], Step [24240/64305], Loss: 5.1803\n",
      "Epoch [1/2], Step [24250/64305], Loss: 4.8906\n",
      "Epoch [1/2], Step [24260/64305], Loss: 5.0587\n",
      "Epoch [1/2], Step [24270/64305], Loss: 5.0655\n",
      "Epoch [1/2], Step [24280/64305], Loss: 4.8175\n",
      "Epoch [1/2], Step [24290/64305], Loss: 4.9497\n",
      "Epoch [1/2], Step [24300/64305], Loss: 4.9915\n",
      "Epoch [1/2], Step [24310/64305], Loss: 4.9654\n",
      "Epoch [1/2], Step [24320/64305], Loss: 5.1488\n",
      "Epoch [1/2], Step [24330/64305], Loss: 5.0059\n",
      "Epoch [1/2], Step [24340/64305], Loss: 5.1816\n",
      "Epoch [1/2], Step [24350/64305], Loss: 5.2826\n",
      "Epoch [1/2], Step [24360/64305], Loss: 4.9718\n",
      "Epoch [1/2], Step [24370/64305], Loss: 5.0463\n",
      "Epoch [1/2], Step [24380/64305], Loss: 5.0969\n",
      "Epoch [1/2], Step [24390/64305], Loss: 5.2577\n",
      "Epoch [1/2], Step [24400/64305], Loss: 4.9063\n",
      "Epoch [1/2], Step [24410/64305], Loss: 5.1310\n",
      "Epoch [1/2], Step [24420/64305], Loss: 5.2197\n",
      "Epoch [1/2], Step [24430/64305], Loss: 5.1290\n",
      "Epoch [1/2], Step [24440/64305], Loss: 5.1197\n",
      "Epoch [1/2], Step [24450/64305], Loss: 5.2760\n",
      "Epoch [1/2], Step [24460/64305], Loss: 5.3104\n",
      "Epoch [1/2], Step [24470/64305], Loss: 5.0475\n",
      "Epoch [1/2], Step [24480/64305], Loss: 4.9960\n",
      "Epoch [1/2], Step [24490/64305], Loss: 5.2959\n",
      "Epoch [1/2], Step [24500/64305], Loss: 5.2478\n",
      "Epoch [1/2], Step [24510/64305], Loss: 5.0633\n",
      "Epoch [1/2], Step [24520/64305], Loss: 5.1473\n",
      "Epoch [1/2], Step [24530/64305], Loss: 5.2901\n",
      "Epoch [1/2], Step [24540/64305], Loss: 5.1878\n",
      "Epoch [1/2], Step [24550/64305], Loss: 5.3984\n",
      "Epoch [1/2], Step [24560/64305], Loss: 5.1677\n",
      "Epoch [1/2], Step [24570/64305], Loss: 5.0022\n",
      "Epoch [1/2], Step [24580/64305], Loss: 4.9923\n",
      "Epoch [1/2], Step [24590/64305], Loss: 5.1272\n",
      "Epoch [1/2], Step [24600/64305], Loss: 5.1199\n",
      "Epoch [1/2], Step [24610/64305], Loss: 4.9076\n",
      "Epoch [1/2], Step [24620/64305], Loss: 5.0430\n",
      "Epoch [1/2], Step [24630/64305], Loss: 4.9390\n",
      "Epoch [1/2], Step [24640/64305], Loss: 5.2640\n",
      "Epoch [1/2], Step [24650/64305], Loss: 5.1074\n",
      "Epoch [1/2], Step [24660/64305], Loss: 4.8784\n",
      "Epoch [1/2], Step [24670/64305], Loss: 5.2032\n",
      "Epoch [1/2], Step [24680/64305], Loss: 5.1326\n",
      "Epoch [1/2], Step [24690/64305], Loss: 5.1066\n",
      "Epoch [1/2], Step [24700/64305], Loss: 5.1974\n",
      "Epoch [1/2], Step [24710/64305], Loss: 5.2491\n",
      "Epoch [1/2], Step [24720/64305], Loss: 5.0152\n",
      "Epoch [1/2], Step [24730/64305], Loss: 5.1495\n",
      "Epoch [1/2], Step [24740/64305], Loss: 5.3235\n",
      "Epoch [1/2], Step [24750/64305], Loss: 5.2210\n",
      "Epoch [1/2], Step [24760/64305], Loss: 4.8411\n",
      "Epoch [1/2], Step [24770/64305], Loss: 5.1557\n",
      "Epoch [1/2], Step [24780/64305], Loss: 5.1987\n",
      "Epoch [1/2], Step [24790/64305], Loss: 5.0474\n",
      "Epoch [1/2], Step [24800/64305], Loss: 5.2044\n",
      "Epoch [1/2], Step [24810/64305], Loss: 5.1151\n",
      "Epoch [1/2], Step [24820/64305], Loss: 5.0244\n",
      "Epoch [1/2], Step [24830/64305], Loss: 4.9290\n",
      "Epoch [1/2], Step [24840/64305], Loss: 5.1377\n",
      "Epoch [1/2], Step [24850/64305], Loss: 5.1620\n",
      "Epoch [1/2], Step [24860/64305], Loss: 5.1477\n",
      "Epoch [1/2], Step [24870/64305], Loss: 4.9088\n",
      "Epoch [1/2], Step [24880/64305], Loss: 5.0606\n",
      "Epoch [1/2], Step [24890/64305], Loss: 5.0690\n",
      "Epoch [1/2], Step [24900/64305], Loss: 4.9823\n",
      "Epoch [1/2], Step [24910/64305], Loss: 5.0904\n",
      "Epoch [1/2], Step [24920/64305], Loss: 4.9556\n",
      "Epoch [1/2], Step [24930/64305], Loss: 5.1497\n",
      "Epoch [1/2], Step [24940/64305], Loss: 5.1820\n",
      "Epoch [1/2], Step [24950/64305], Loss: 5.0250\n",
      "Epoch [1/2], Step [24960/64305], Loss: 5.1168\n",
      "Epoch [1/2], Step [24970/64305], Loss: 5.1792\n",
      "Epoch [1/2], Step [24980/64305], Loss: 5.0439\n",
      "Epoch [1/2], Step [24990/64305], Loss: 5.3173\n",
      "Epoch [1/2], Step [25000/64305], Loss: 5.1050\n",
      "Epoch [1/2], Step [25010/64305], Loss: 5.1654\n",
      "Epoch [1/2], Step [25020/64305], Loss: 5.1178\n",
      "Epoch [1/2], Step [25030/64305], Loss: 5.2098\n",
      "Epoch [1/2], Step [25040/64305], Loss: 5.1938\n",
      "Epoch [1/2], Step [25050/64305], Loss: 5.3276\n",
      "Epoch [1/2], Step [25060/64305], Loss: 4.9254\n",
      "Epoch [1/2], Step [25070/64305], Loss: 5.1012\n",
      "Epoch [1/2], Step [25080/64305], Loss: 4.9176\n",
      "Epoch [1/2], Step [25090/64305], Loss: 5.0275\n",
      "Epoch [1/2], Step [25100/64305], Loss: 4.9252\n",
      "Epoch [1/2], Step [25110/64305], Loss: 4.9052\n",
      "Epoch [1/2], Step [25120/64305], Loss: 5.0383\n",
      "Epoch [1/2], Step [25130/64305], Loss: 4.7127\n",
      "Epoch [1/2], Step [25140/64305], Loss: 5.2440\n",
      "Epoch [1/2], Step [25150/64305], Loss: 5.2359\n",
      "Epoch [1/2], Step [25160/64305], Loss: 4.9234\n",
      "Epoch [1/2], Step [25170/64305], Loss: 5.1256\n",
      "Epoch [1/2], Step [25180/64305], Loss: 5.1159\n",
      "Epoch [1/2], Step [25190/64305], Loss: 5.1050\n",
      "Epoch [1/2], Step [25200/64305], Loss: 5.0537\n",
      "Epoch [1/2], Step [25210/64305], Loss: 5.0811\n",
      "Epoch [1/2], Step [25220/64305], Loss: 5.0616\n",
      "Epoch [1/2], Step [25230/64305], Loss: 5.1724\n",
      "Epoch [1/2], Step [25240/64305], Loss: 5.1337\n",
      "Epoch [1/2], Step [25250/64305], Loss: 4.9626\n",
      "Epoch [1/2], Step [25260/64305], Loss: 5.1714\n",
      "Epoch [1/2], Step [25270/64305], Loss: 4.8559\n",
      "Epoch [1/2], Step [25280/64305], Loss: 5.1328\n",
      "Epoch [1/2], Step [25290/64305], Loss: 5.1788\n",
      "Epoch [1/2], Step [25300/64305], Loss: 5.0951\n",
      "Epoch [1/2], Step [25310/64305], Loss: 5.0740\n",
      "Epoch [1/2], Step [25320/64305], Loss: 5.2082\n",
      "Epoch [1/2], Step [25330/64305], Loss: 5.1478\n",
      "Epoch [1/2], Step [25340/64305], Loss: 5.0023\n",
      "Epoch [1/2], Step [25350/64305], Loss: 5.2287\n",
      "Epoch [1/2], Step [25360/64305], Loss: 4.9853\n",
      "Epoch [1/2], Step [25370/64305], Loss: 4.9249\n",
      "Epoch [1/2], Step [25380/64305], Loss: 5.1854\n",
      "Epoch [1/2], Step [25390/64305], Loss: 5.1731\n",
      "Epoch [1/2], Step [25400/64305], Loss: 4.9384\n",
      "Epoch [1/2], Step [25410/64305], Loss: 5.2406\n",
      "Epoch [1/2], Step [25420/64305], Loss: 5.0428\n",
      "Epoch [1/2], Step [25430/64305], Loss: 5.0653\n",
      "Epoch [1/2], Step [25440/64305], Loss: 5.2342\n",
      "Epoch [1/2], Step [25450/64305], Loss: 4.9849\n",
      "Epoch [1/2], Step [25460/64305], Loss: 5.1835\n",
      "Epoch [1/2], Step [25470/64305], Loss: 4.9918\n",
      "Epoch [1/2], Step [25480/64305], Loss: 5.1418\n",
      "Epoch [1/2], Step [25490/64305], Loss: 5.2483\n",
      "Epoch [1/2], Step [25500/64305], Loss: 4.9627\n",
      "Epoch [1/2], Step [25510/64305], Loss: 5.3574\n",
      "Epoch [1/2], Step [25520/64305], Loss: 5.0827\n",
      "Epoch [1/2], Step [25530/64305], Loss: 4.8150\n",
      "Epoch [1/2], Step [25540/64305], Loss: 5.2013\n",
      "Epoch [1/2], Step [25550/64305], Loss: 4.9214\n",
      "Epoch [1/2], Step [25560/64305], Loss: 5.0774\n",
      "Epoch [1/2], Step [25570/64305], Loss: 4.7885\n",
      "Epoch [1/2], Step [25580/64305], Loss: 5.0685\n",
      "Epoch [1/2], Step [25590/64305], Loss: 5.1113\n",
      "Epoch [1/2], Step [25600/64305], Loss: 5.2388\n",
      "Epoch [1/2], Step [25610/64305], Loss: 5.1634\n",
      "Epoch [1/2], Step [25620/64305], Loss: 4.9932\n",
      "Epoch [1/2], Step [25630/64305], Loss: 4.9188\n",
      "Epoch [1/2], Step [25640/64305], Loss: 5.0463\n",
      "Epoch [1/2], Step [25650/64305], Loss: 5.1412\n",
      "Epoch [1/2], Step [25660/64305], Loss: 5.1736\n",
      "Epoch [1/2], Step [25670/64305], Loss: 5.0973\n",
      "Epoch [1/2], Step [25680/64305], Loss: 4.8931\n",
      "Epoch [1/2], Step [25690/64305], Loss: 5.1303\n",
      "Epoch [1/2], Step [25700/64305], Loss: 5.0079\n",
      "Epoch [1/2], Step [25710/64305], Loss: 4.8717\n",
      "Epoch [1/2], Step [25720/64305], Loss: 5.0013\n",
      "Epoch [1/2], Step [25730/64305], Loss: 4.8971\n",
      "Epoch [1/2], Step [25740/64305], Loss: 5.2375\n",
      "Epoch [1/2], Step [25750/64305], Loss: 4.8871\n",
      "Epoch [1/2], Step [25760/64305], Loss: 4.9839\n",
      "Epoch [1/2], Step [25770/64305], Loss: 5.0641\n",
      "Epoch [1/2], Step [25780/64305], Loss: 5.0159\n",
      "Epoch [1/2], Step [25790/64305], Loss: 4.8815\n",
      "Epoch [1/2], Step [25800/64305], Loss: 4.9334\n",
      "Epoch [1/2], Step [25810/64305], Loss: 5.0053\n",
      "Epoch [1/2], Step [25820/64305], Loss: 5.1040\n",
      "Epoch [1/2], Step [25830/64305], Loss: 4.9404\n",
      "Epoch [1/2], Step [25840/64305], Loss: 5.1209\n",
      "Epoch [1/2], Step [25850/64305], Loss: 5.1923\n",
      "Epoch [1/2], Step [25860/64305], Loss: 5.1584\n",
      "Epoch [1/2], Step [25870/64305], Loss: 4.9664\n",
      "Epoch [1/2], Step [25880/64305], Loss: 5.0403\n",
      "Epoch [1/2], Step [25890/64305], Loss: 5.2976\n",
      "Epoch [1/2], Step [25900/64305], Loss: 5.1268\n",
      "Epoch [1/2], Step [25910/64305], Loss: 5.0093\n",
      "Epoch [1/2], Step [25920/64305], Loss: 5.1649\n",
      "Epoch [1/2], Step [25930/64305], Loss: 5.1771\n",
      "Epoch [1/2], Step [25940/64305], Loss: 4.9621\n",
      "Epoch [1/2], Step [25950/64305], Loss: 4.9837\n",
      "Epoch [1/2], Step [25960/64305], Loss: 5.4618\n",
      "Epoch [1/2], Step [25970/64305], Loss: 5.1051\n",
      "Epoch [1/2], Step [25980/64305], Loss: 5.0854\n",
      "Epoch [1/2], Step [25990/64305], Loss: 5.0998\n",
      "Epoch [1/2], Step [26000/64305], Loss: 5.0050\n",
      "Epoch [1/2], Step [26010/64305], Loss: 4.9384\n",
      "Epoch [1/2], Step [26020/64305], Loss: 5.0972\n",
      "Epoch [1/2], Step [26030/64305], Loss: 4.9040\n",
      "Epoch [1/2], Step [26040/64305], Loss: 5.0164\n",
      "Epoch [1/2], Step [26050/64305], Loss: 5.1681\n",
      "Epoch [1/2], Step [26060/64305], Loss: 5.1449\n",
      "Epoch [1/2], Step [26070/64305], Loss: 5.1698\n",
      "Epoch [1/2], Step [26080/64305], Loss: 4.9967\n",
      "Epoch [1/2], Step [26090/64305], Loss: 5.0538\n",
      "Epoch [1/2], Step [26100/64305], Loss: 4.9599\n",
      "Epoch [1/2], Step [26110/64305], Loss: 5.0703\n",
      "Epoch [1/2], Step [26120/64305], Loss: 5.0550\n",
      "Epoch [1/2], Step [26130/64305], Loss: 5.0775\n",
      "Epoch [1/2], Step [26140/64305], Loss: 4.8482\n",
      "Epoch [1/2], Step [26150/64305], Loss: 5.0068\n",
      "Epoch [1/2], Step [26160/64305], Loss: 5.2694\n",
      "Epoch [1/2], Step [26170/64305], Loss: 5.0025\n",
      "Epoch [1/2], Step [26180/64305], Loss: 4.9923\n",
      "Epoch [1/2], Step [26190/64305], Loss: 5.0279\n",
      "Epoch [1/2], Step [26200/64305], Loss: 5.0579\n",
      "Epoch [1/2], Step [26210/64305], Loss: 5.0205\n",
      "Epoch [1/2], Step [26220/64305], Loss: 4.8571\n",
      "Epoch [1/2], Step [26230/64305], Loss: 5.0688\n",
      "Epoch [1/2], Step [26240/64305], Loss: 5.0586\n",
      "Epoch [1/2], Step [26250/64305], Loss: 5.0663\n",
      "Epoch [1/2], Step [26260/64305], Loss: 5.1139\n",
      "Epoch [1/2], Step [26270/64305], Loss: 5.0036\n",
      "Epoch [1/2], Step [26280/64305], Loss: 5.1750\n",
      "Epoch [1/2], Step [26290/64305], Loss: 5.0072\n",
      "Epoch [1/2], Step [26300/64305], Loss: 5.1041\n",
      "Epoch [1/2], Step [26310/64305], Loss: 5.0714\n",
      "Epoch [1/2], Step [26320/64305], Loss: 4.8896\n",
      "Epoch [1/2], Step [26330/64305], Loss: 5.0204\n",
      "Epoch [1/2], Step [26340/64305], Loss: 4.8803\n",
      "Epoch [1/2], Step [26350/64305], Loss: 5.0895\n",
      "Epoch [1/2], Step [26360/64305], Loss: 5.0519\n",
      "Epoch [1/2], Step [26370/64305], Loss: 5.1982\n",
      "Epoch [1/2], Step [26380/64305], Loss: 4.9003\n",
      "Epoch [1/2], Step [26390/64305], Loss: 4.9919\n",
      "Epoch [1/2], Step [26400/64305], Loss: 5.2736\n",
      "Epoch [1/2], Step [26410/64305], Loss: 5.2578\n",
      "Epoch [1/2], Step [26420/64305], Loss: 5.1594\n",
      "Epoch [1/2], Step [26430/64305], Loss: 5.1437\n",
      "Epoch [1/2], Step [26440/64305], Loss: 5.0762\n",
      "Epoch [1/2], Step [26450/64305], Loss: 5.1271\n",
      "Epoch [1/2], Step [26460/64305], Loss: 5.0364\n",
      "Epoch [1/2], Step [26470/64305], Loss: 4.9472\n",
      "Epoch [1/2], Step [26480/64305], Loss: 5.2771\n",
      "Epoch [1/2], Step [26490/64305], Loss: 5.0784\n",
      "Epoch [1/2], Step [26500/64305], Loss: 4.8939\n",
      "Epoch [1/2], Step [26510/64305], Loss: 5.0769\n",
      "Epoch [1/2], Step [26520/64305], Loss: 4.9901\n",
      "Epoch [1/2], Step [26530/64305], Loss: 5.0664\n",
      "Epoch [1/2], Step [26540/64305], Loss: 5.1131\n",
      "Epoch [1/2], Step [26550/64305], Loss: 5.1084\n",
      "Epoch [1/2], Step [26560/64305], Loss: 5.0524\n",
      "Epoch [1/2], Step [26570/64305], Loss: 5.0968\n",
      "Epoch [1/2], Step [26580/64305], Loss: 5.0596\n",
      "Epoch [1/2], Step [26590/64305], Loss: 5.0350\n",
      "Epoch [1/2], Step [26600/64305], Loss: 4.9777\n",
      "Epoch [1/2], Step [26610/64305], Loss: 5.0233\n",
      "Epoch [1/2], Step [26620/64305], Loss: 4.8429\n",
      "Epoch [1/2], Step [26630/64305], Loss: 5.1035\n",
      "Epoch [1/2], Step [26640/64305], Loss: 4.8779\n",
      "Epoch [1/2], Step [26650/64305], Loss: 5.2147\n",
      "Epoch [1/2], Step [26660/64305], Loss: 5.0495\n",
      "Epoch [1/2], Step [26670/64305], Loss: 5.1490\n",
      "Epoch [1/2], Step [26680/64305], Loss: 5.0988\n",
      "Epoch [1/2], Step [26690/64305], Loss: 4.8929\n",
      "Epoch [1/2], Step [26700/64305], Loss: 4.8479\n",
      "Epoch [1/2], Step [26710/64305], Loss: 5.0644\n",
      "Epoch [1/2], Step [26720/64305], Loss: 5.1116\n",
      "Epoch [1/2], Step [26730/64305], Loss: 5.2658\n",
      "Epoch [1/2], Step [26740/64305], Loss: 5.1146\n",
      "Epoch [1/2], Step [26750/64305], Loss: 5.3056\n",
      "Epoch [1/2], Step [26760/64305], Loss: 4.8374\n",
      "Epoch [1/2], Step [26770/64305], Loss: 4.8613\n",
      "Epoch [1/2], Step [26780/64305], Loss: 5.0441\n",
      "Epoch [1/2], Step [26790/64305], Loss: 5.0056\n",
      "Epoch [1/2], Step [26800/64305], Loss: 5.1663\n",
      "Epoch [1/2], Step [26810/64305], Loss: 5.1326\n",
      "Epoch [1/2], Step [26820/64305], Loss: 5.3607\n",
      "Epoch [1/2], Step [26830/64305], Loss: 5.0441\n",
      "Epoch [1/2], Step [26840/64305], Loss: 4.9648\n",
      "Epoch [1/2], Step [26850/64305], Loss: 5.2231\n",
      "Epoch [1/2], Step [26860/64305], Loss: 5.0347\n",
      "Epoch [1/2], Step [26870/64305], Loss: 4.8036\n",
      "Epoch [1/2], Step [26880/64305], Loss: 5.0016\n",
      "Epoch [1/2], Step [26890/64305], Loss: 5.0949\n",
      "Epoch [1/2], Step [26900/64305], Loss: 5.2952\n",
      "Epoch [1/2], Step [26910/64305], Loss: 5.1135\n",
      "Epoch [1/2], Step [26920/64305], Loss: 5.0145\n",
      "Epoch [1/2], Step [26930/64305], Loss: 4.8035\n",
      "Epoch [1/2], Step [26940/64305], Loss: 4.9400\n",
      "Epoch [1/2], Step [26950/64305], Loss: 5.0871\n",
      "Epoch [1/2], Step [26960/64305], Loss: 5.4205\n",
      "Epoch [1/2], Step [26970/64305], Loss: 5.1346\n",
      "Epoch [1/2], Step [26980/64305], Loss: 4.9955\n",
      "Epoch [1/2], Step [26990/64305], Loss: 5.0464\n",
      "Epoch [1/2], Step [27000/64305], Loss: 4.9883\n",
      "Epoch [1/2], Step [27010/64305], Loss: 5.0186\n",
      "Epoch [1/2], Step [27020/64305], Loss: 5.3141\n",
      "Epoch [1/2], Step [27030/64305], Loss: 4.9834\n",
      "Epoch [1/2], Step [27040/64305], Loss: 5.1523\n",
      "Epoch [1/2], Step [27050/64305], Loss: 5.1956\n",
      "Epoch [1/2], Step [27060/64305], Loss: 5.0935\n",
      "Epoch [1/2], Step [27070/64305], Loss: 4.9937\n",
      "Epoch [1/2], Step [27080/64305], Loss: 5.0158\n",
      "Epoch [1/2], Step [27090/64305], Loss: 5.1608\n",
      "Epoch [1/2], Step [27100/64305], Loss: 5.1653\n",
      "Epoch [1/2], Step [27110/64305], Loss: 5.1528\n",
      "Epoch [1/2], Step [27120/64305], Loss: 4.9779\n",
      "Epoch [1/2], Step [27130/64305], Loss: 4.5876\n",
      "Epoch [1/2], Step [27140/64305], Loss: 5.3020\n",
      "Epoch [1/2], Step [27150/64305], Loss: 5.1408\n",
      "Epoch [1/2], Step [27160/64305], Loss: 4.9283\n",
      "Epoch [1/2], Step [27170/64305], Loss: 5.1385\n",
      "Epoch [1/2], Step [27180/64305], Loss: 5.1943\n",
      "Epoch [1/2], Step [27190/64305], Loss: 5.1153\n",
      "Epoch [1/2], Step [27200/64305], Loss: 4.9661\n",
      "Epoch [1/2], Step [27210/64305], Loss: 5.0835\n",
      "Epoch [1/2], Step [27220/64305], Loss: 4.9087\n",
      "Epoch [1/2], Step [27230/64305], Loss: 5.1364\n",
      "Epoch [1/2], Step [27240/64305], Loss: 4.9251\n",
      "Epoch [1/2], Step [27250/64305], Loss: 5.1207\n",
      "Epoch [1/2], Step [27260/64305], Loss: 4.9498\n",
      "Epoch [1/2], Step [27270/64305], Loss: 5.1562\n",
      "Epoch [1/2], Step [27280/64305], Loss: 5.0765\n",
      "Epoch [1/2], Step [27290/64305], Loss: 5.0944\n",
      "Epoch [1/2], Step [27300/64305], Loss: 5.0839\n",
      "Epoch [1/2], Step [27310/64305], Loss: 5.2169\n",
      "Epoch [1/2], Step [27320/64305], Loss: 5.0158\n",
      "Epoch [1/2], Step [27330/64305], Loss: 5.0282\n",
      "Epoch [1/2], Step [27340/64305], Loss: 4.9453\n",
      "Epoch [1/2], Step [27350/64305], Loss: 5.1066\n",
      "Epoch [1/2], Step [27360/64305], Loss: 5.1059\n",
      "Epoch [1/2], Step [27370/64305], Loss: 5.0454\n",
      "Epoch [1/2], Step [27380/64305], Loss: 5.1755\n",
      "Epoch [1/2], Step [27390/64305], Loss: 5.2787\n",
      "Epoch [1/2], Step [27400/64305], Loss: 5.2894\n",
      "Epoch [1/2], Step [27410/64305], Loss: 5.2002\n",
      "Epoch [1/2], Step [27420/64305], Loss: 5.2288\n",
      "Epoch [1/2], Step [27430/64305], Loss: 4.9469\n",
      "Epoch [1/2], Step [27440/64305], Loss: 5.0010\n",
      "Epoch [1/2], Step [27450/64305], Loss: 5.2185\n",
      "Epoch [1/2], Step [27460/64305], Loss: 4.9509\n",
      "Epoch [1/2], Step [27470/64305], Loss: 5.0337\n",
      "Epoch [1/2], Step [27480/64305], Loss: 5.0355\n",
      "Epoch [1/2], Step [27490/64305], Loss: 5.0095\n",
      "Epoch [1/2], Step [27500/64305], Loss: 5.2451\n",
      "Epoch [1/2], Step [27510/64305], Loss: 5.0940\n",
      "Epoch [1/2], Step [27520/64305], Loss: 5.0804\n",
      "Epoch [1/2], Step [27530/64305], Loss: 5.1171\n",
      "Epoch [1/2], Step [27540/64305], Loss: 5.0143\n",
      "Epoch [1/2], Step [27550/64305], Loss: 4.8359\n",
      "Epoch [1/2], Step [27560/64305], Loss: 4.9794\n",
      "Epoch [1/2], Step [27570/64305], Loss: 5.1111\n",
      "Epoch [1/2], Step [27580/64305], Loss: 4.9883\n",
      "Epoch [1/2], Step [27590/64305], Loss: 4.9852\n",
      "Epoch [1/2], Step [27600/64305], Loss: 4.8280\n",
      "Epoch [1/2], Step [27610/64305], Loss: 5.0792\n",
      "Epoch [1/2], Step [27620/64305], Loss: 5.1414\n",
      "Epoch [1/2], Step [27630/64305], Loss: 5.3062\n",
      "Epoch [1/2], Step [27640/64305], Loss: 5.1816\n",
      "Epoch [1/2], Step [27650/64305], Loss: 5.0208\n",
      "Epoch [1/2], Step [27660/64305], Loss: 5.1806\n",
      "Epoch [1/2], Step [27670/64305], Loss: 4.8634\n",
      "Epoch [1/2], Step [27680/64305], Loss: 4.9350\n",
      "Epoch [1/2], Step [27690/64305], Loss: 4.8880\n",
      "Epoch [1/2], Step [27700/64305], Loss: 5.1879\n",
      "Epoch [1/2], Step [27710/64305], Loss: 5.0255\n",
      "Epoch [1/2], Step [27720/64305], Loss: 4.8676\n",
      "Epoch [1/2], Step [27730/64305], Loss: 5.2158\n",
      "Epoch [1/2], Step [27740/64305], Loss: 5.0052\n",
      "Epoch [1/2], Step [27750/64305], Loss: 4.7927\n",
      "Epoch [1/2], Step [27760/64305], Loss: 4.9173\n",
      "Epoch [1/2], Step [27770/64305], Loss: 5.0834\n",
      "Epoch [1/2], Step [27780/64305], Loss: 5.1525\n",
      "Epoch [1/2], Step [27790/64305], Loss: 4.9556\n",
      "Epoch [1/2], Step [27800/64305], Loss: 4.9909\n",
      "Epoch [1/2], Step [27810/64305], Loss: 5.0543\n",
      "Epoch [1/2], Step [27820/64305], Loss: 5.2269\n",
      "Epoch [1/2], Step [27830/64305], Loss: 5.3619\n",
      "Epoch [1/2], Step [27840/64305], Loss: 5.0955\n",
      "Epoch [1/2], Step [27850/64305], Loss: 5.1872\n",
      "Epoch [1/2], Step [27860/64305], Loss: 5.0229\n",
      "Epoch [1/2], Step [27870/64305], Loss: 4.9646\n",
      "Epoch [1/2], Step [27880/64305], Loss: 5.1618\n",
      "Epoch [1/2], Step [27890/64305], Loss: 5.0227\n",
      "Epoch [1/2], Step [27900/64305], Loss: 5.0333\n",
      "Epoch [1/2], Step [27910/64305], Loss: 4.9895\n",
      "Epoch [1/2], Step [27920/64305], Loss: 4.9661\n",
      "Epoch [1/2], Step [27930/64305], Loss: 5.0171\n",
      "Epoch [1/2], Step [27940/64305], Loss: 5.1937\n",
      "Epoch [1/2], Step [27950/64305], Loss: 5.1726\n",
      "Epoch [1/2], Step [27960/64305], Loss: 5.1024\n",
      "Epoch [1/2], Step [27970/64305], Loss: 4.9078\n",
      "Epoch [1/2], Step [27980/64305], Loss: 4.9196\n",
      "Epoch [1/2], Step [27990/64305], Loss: 5.1271\n",
      "Epoch [1/2], Step [28000/64305], Loss: 4.9898\n",
      "Epoch [1/2], Step [28010/64305], Loss: 5.1141\n",
      "Epoch [1/2], Step [28020/64305], Loss: 5.1822\n",
      "Epoch [1/2], Step [28030/64305], Loss: 4.9070\n",
      "Epoch [1/2], Step [28040/64305], Loss: 5.1648\n",
      "Epoch [1/2], Step [28050/64305], Loss: 5.0345\n",
      "Epoch [1/2], Step [28060/64305], Loss: 5.1109\n",
      "Epoch [1/2], Step [28070/64305], Loss: 4.9946\n",
      "Epoch [1/2], Step [28080/64305], Loss: 4.9290\n",
      "Epoch [1/2], Step [28090/64305], Loss: 5.0651\n",
      "Epoch [1/2], Step [28100/64305], Loss: 5.0941\n",
      "Epoch [1/2], Step [28110/64305], Loss: 5.1705\n",
      "Epoch [1/2], Step [28120/64305], Loss: 4.9310\n",
      "Epoch [1/2], Step [28130/64305], Loss: 5.0013\n",
      "Epoch [1/2], Step [28140/64305], Loss: 4.9460\n",
      "Epoch [1/2], Step [28150/64305], Loss: 5.1227\n",
      "Epoch [1/2], Step [28160/64305], Loss: 5.0722\n",
      "Epoch [1/2], Step [28170/64305], Loss: 4.8571\n",
      "Epoch [1/2], Step [28180/64305], Loss: 4.9598\n",
      "Epoch [1/2], Step [28190/64305], Loss: 4.9750\n",
      "Epoch [1/2], Step [28200/64305], Loss: 4.9884\n",
      "Epoch [1/2], Step [28210/64305], Loss: 5.1455\n",
      "Epoch [1/2], Step [28220/64305], Loss: 5.1372\n",
      "Epoch [1/2], Step [28230/64305], Loss: 4.9667\n",
      "Epoch [1/2], Step [28240/64305], Loss: 5.0607\n",
      "Epoch [1/2], Step [28250/64305], Loss: 5.0985\n",
      "Epoch [1/2], Step [28260/64305], Loss: 4.9093\n",
      "Epoch [1/2], Step [28270/64305], Loss: 5.0893\n",
      "Epoch [1/2], Step [28280/64305], Loss: 5.0608\n",
      "Epoch [1/2], Step [28290/64305], Loss: 4.8314\n",
      "Epoch [1/2], Step [28300/64305], Loss: 5.1890\n",
      "Epoch [1/2], Step [28310/64305], Loss: 4.8855\n",
      "Epoch [1/2], Step [28320/64305], Loss: 5.0285\n",
      "Epoch [1/2], Step [28330/64305], Loss: 5.2492\n",
      "Epoch [1/2], Step [28340/64305], Loss: 4.9991\n",
      "Epoch [1/2], Step [28350/64305], Loss: 5.0854\n",
      "Epoch [1/2], Step [28360/64305], Loss: 5.2738\n",
      "Epoch [1/2], Step [28370/64305], Loss: 5.0491\n",
      "Epoch [1/2], Step [28380/64305], Loss: 4.9259\n",
      "Epoch [1/2], Step [28390/64305], Loss: 4.9876\n",
      "Epoch [1/2], Step [28400/64305], Loss: 5.0673\n",
      "Epoch [1/2], Step [28410/64305], Loss: 5.0414\n",
      "Epoch [1/2], Step [28420/64305], Loss: 5.2121\n",
      "Epoch [1/2], Step [28430/64305], Loss: 5.1839\n",
      "Epoch [1/2], Step [28440/64305], Loss: 5.2504\n",
      "Epoch [1/2], Step [28450/64305], Loss: 4.8691\n",
      "Epoch [1/2], Step [28460/64305], Loss: 4.9547\n",
      "Epoch [1/2], Step [28470/64305], Loss: 5.1053\n",
      "Epoch [1/2], Step [28480/64305], Loss: 5.0939\n",
      "Epoch [1/2], Step [28490/64305], Loss: 5.2961\n",
      "Epoch [1/2], Step [28500/64305], Loss: 5.0039\n",
      "Epoch [1/2], Step [28510/64305], Loss: 5.0712\n",
      "Epoch [1/2], Step [28520/64305], Loss: 5.0368\n",
      "Epoch [1/2], Step [28530/64305], Loss: 4.9920\n",
      "Epoch [1/2], Step [28540/64305], Loss: 5.0514\n",
      "Epoch [1/2], Step [28550/64305], Loss: 4.9492\n",
      "Epoch [1/2], Step [28560/64305], Loss: 5.0831\n",
      "Epoch [1/2], Step [28570/64305], Loss: 5.0258\n",
      "Epoch [1/2], Step [28580/64305], Loss: 4.8570\n",
      "Epoch [1/2], Step [28590/64305], Loss: 5.1319\n",
      "Epoch [1/2], Step [28600/64305], Loss: 5.2845\n",
      "Epoch [1/2], Step [28610/64305], Loss: 4.9519\n",
      "Epoch [1/2], Step [28620/64305], Loss: 5.0813\n",
      "Epoch [1/2], Step [28630/64305], Loss: 4.9288\n",
      "Epoch [1/2], Step [28640/64305], Loss: 5.0860\n",
      "Epoch [1/2], Step [28650/64305], Loss: 5.0889\n",
      "Epoch [1/2], Step [28660/64305], Loss: 5.0162\n",
      "Epoch [1/2], Step [28670/64305], Loss: 4.9153\n",
      "Epoch [1/2], Step [28680/64305], Loss: 5.0585\n",
      "Epoch [1/2], Step [28690/64305], Loss: 4.9624\n",
      "Epoch [1/2], Step [28700/64305], Loss: 5.3175\n",
      "Epoch [1/2], Step [28710/64305], Loss: 5.0408\n",
      "Epoch [1/2], Step [28720/64305], Loss: 5.0980\n",
      "Epoch [1/2], Step [28730/64305], Loss: 5.1537\n",
      "Epoch [1/2], Step [28740/64305], Loss: 4.9367\n",
      "Epoch [1/2], Step [28750/64305], Loss: 5.0877\n",
      "Epoch [1/2], Step [28760/64305], Loss: 5.1797\n",
      "Epoch [1/2], Step [28770/64305], Loss: 4.8246\n",
      "Epoch [1/2], Step [28780/64305], Loss: 5.1374\n",
      "Epoch [1/2], Step [28790/64305], Loss: 5.2055\n",
      "Epoch [1/2], Step [28800/64305], Loss: 5.1708\n",
      "Epoch [1/2], Step [28810/64305], Loss: 5.1569\n",
      "Epoch [1/2], Step [28820/64305], Loss: 5.0942\n",
      "Epoch [1/2], Step [28830/64305], Loss: 4.9827\n",
      "Epoch [1/2], Step [28840/64305], Loss: 5.1775\n",
      "Epoch [1/2], Step [28850/64305], Loss: 5.1034\n",
      "Epoch [1/2], Step [28860/64305], Loss: 5.0714\n",
      "Epoch [1/2], Step [28870/64305], Loss: 4.9280\n",
      "Epoch [1/2], Step [28880/64305], Loss: 5.0676\n",
      "Epoch [1/2], Step [28890/64305], Loss: 4.8814\n",
      "Epoch [1/2], Step [28900/64305], Loss: 5.3787\n",
      "Epoch [1/2], Step [28910/64305], Loss: 4.9885\n",
      "Epoch [1/2], Step [28920/64305], Loss: 5.1385\n",
      "Epoch [1/2], Step [28930/64305], Loss: 5.0506\n",
      "Epoch [1/2], Step [28940/64305], Loss: 5.2116\n",
      "Epoch [1/2], Step [28950/64305], Loss: 5.0104\n",
      "Epoch [1/2], Step [28960/64305], Loss: 5.0737\n",
      "Epoch [1/2], Step [28970/64305], Loss: 5.0767\n",
      "Epoch [1/2], Step [28980/64305], Loss: 5.1317\n",
      "Epoch [1/2], Step [28990/64305], Loss: 5.0532\n",
      "Epoch [1/2], Step [29000/64305], Loss: 5.0384\n",
      "Epoch [1/2], Step [29010/64305], Loss: 5.2177\n",
      "Epoch [1/2], Step [29020/64305], Loss: 5.1641\n",
      "Epoch [1/2], Step [29030/64305], Loss: 5.0201\n",
      "Epoch [1/2], Step [29040/64305], Loss: 4.8959\n",
      "Epoch [1/2], Step [29050/64305], Loss: 5.0221\n",
      "Epoch [1/2], Step [29060/64305], Loss: 5.2688\n",
      "Epoch [1/2], Step [29070/64305], Loss: 5.3023\n",
      "Epoch [1/2], Step [29080/64305], Loss: 5.0442\n",
      "Epoch [1/2], Step [29090/64305], Loss: 5.1016\n",
      "Epoch [1/2], Step [29100/64305], Loss: 4.9418\n",
      "Epoch [1/2], Step [29110/64305], Loss: 4.9830\n",
      "Epoch [1/2], Step [29120/64305], Loss: 5.0720\n",
      "Epoch [1/2], Step [29130/64305], Loss: 5.0092\n",
      "Epoch [1/2], Step [29140/64305], Loss: 5.2225\n",
      "Epoch [1/2], Step [29150/64305], Loss: 4.8824\n",
      "Epoch [1/2], Step [29160/64305], Loss: 5.0270\n",
      "Epoch [1/2], Step [29170/64305], Loss: 5.2564\n",
      "Epoch [1/2], Step [29180/64305], Loss: 5.1467\n",
      "Epoch [1/2], Step [29190/64305], Loss: 5.2370\n",
      "Epoch [1/2], Step [29200/64305], Loss: 4.9531\n",
      "Epoch [1/2], Step [29210/64305], Loss: 5.1996\n",
      "Epoch [1/2], Step [29220/64305], Loss: 5.0492\n",
      "Epoch [1/2], Step [29230/64305], Loss: 5.0667\n",
      "Epoch [1/2], Step [29240/64305], Loss: 5.0773\n",
      "Epoch [1/2], Step [29250/64305], Loss: 5.0818\n",
      "Epoch [1/2], Step [29260/64305], Loss: 5.4785\n",
      "Epoch [1/2], Step [29270/64305], Loss: 5.1337\n",
      "Epoch [1/2], Step [29280/64305], Loss: 5.0478\n",
      "Epoch [1/2], Step [29290/64305], Loss: 5.0364\n",
      "Epoch [1/2], Step [29300/64305], Loss: 5.0518\n",
      "Epoch [1/2], Step [29310/64305], Loss: 5.1433\n",
      "Epoch [1/2], Step [29320/64305], Loss: 5.0778\n",
      "Epoch [1/2], Step [29330/64305], Loss: 5.0948\n",
      "Epoch [1/2], Step [29340/64305], Loss: 5.0877\n",
      "Epoch [1/2], Step [29350/64305], Loss: 4.9174\n",
      "Epoch [1/2], Step [29360/64305], Loss: 5.2781\n",
      "Epoch [1/2], Step [29370/64305], Loss: 5.0766\n",
      "Epoch [1/2], Step [29380/64305], Loss: 5.1812\n",
      "Epoch [1/2], Step [29390/64305], Loss: 4.9618\n",
      "Epoch [1/2], Step [29400/64305], Loss: 5.1164\n",
      "Epoch [1/2], Step [29410/64305], Loss: 4.9996\n",
      "Epoch [1/2], Step [29420/64305], Loss: 5.0563\n",
      "Epoch [1/2], Step [29430/64305], Loss: 4.8832\n",
      "Epoch [1/2], Step [29440/64305], Loss: 4.9999\n",
      "Epoch [1/2], Step [29450/64305], Loss: 5.0760\n",
      "Epoch [1/2], Step [29460/64305], Loss: 4.9413\n",
      "Epoch [1/2], Step [29470/64305], Loss: 4.9918\n",
      "Epoch [1/2], Step [29480/64305], Loss: 5.0058\n",
      "Epoch [1/2], Step [29490/64305], Loss: 4.9712\n",
      "Epoch [1/2], Step [29500/64305], Loss: 4.9936\n",
      "Epoch [1/2], Step [29510/64305], Loss: 4.9450\n",
      "Epoch [1/2], Step [29520/64305], Loss: 5.1006\n",
      "Epoch [1/2], Step [29530/64305], Loss: 4.9737\n",
      "Epoch [1/2], Step [29540/64305], Loss: 4.9158\n",
      "Epoch [1/2], Step [29550/64305], Loss: 5.0365\n",
      "Epoch [1/2], Step [29560/64305], Loss: 5.0521\n",
      "Epoch [1/2], Step [29570/64305], Loss: 5.0979\n",
      "Epoch [1/2], Step [29580/64305], Loss: 5.2959\n",
      "Epoch [1/2], Step [29590/64305], Loss: 4.9491\n",
      "Epoch [1/2], Step [29600/64305], Loss: 5.2436\n",
      "Epoch [1/2], Step [29610/64305], Loss: 4.9653\n",
      "Epoch [1/2], Step [29620/64305], Loss: 4.9608\n",
      "Epoch [1/2], Step [29630/64305], Loss: 4.8998\n",
      "Epoch [1/2], Step [29640/64305], Loss: 5.1713\n",
      "Epoch [1/2], Step [29650/64305], Loss: 4.9211\n",
      "Epoch [1/2], Step [29660/64305], Loss: 4.9974\n",
      "Epoch [1/2], Step [29670/64305], Loss: 5.0873\n",
      "Epoch [1/2], Step [29680/64305], Loss: 5.0053\n",
      "Epoch [1/2], Step [29690/64305], Loss: 5.0446\n",
      "Epoch [1/2], Step [29700/64305], Loss: 4.8683\n",
      "Epoch [1/2], Step [29710/64305], Loss: 5.0529\n",
      "Epoch [1/2], Step [29720/64305], Loss: 5.0127\n",
      "Epoch [1/2], Step [29730/64305], Loss: 4.9209\n",
      "Epoch [1/2], Step [29740/64305], Loss: 5.0171\n",
      "Epoch [1/2], Step [29750/64305], Loss: 4.9338\n",
      "Epoch [1/2], Step [29760/64305], Loss: 4.9595\n",
      "Epoch [1/2], Step [29770/64305], Loss: 5.0292\n",
      "Epoch [1/2], Step [29780/64305], Loss: 5.1907\n",
      "Epoch [1/2], Step [29790/64305], Loss: 5.0287\n",
      "Epoch [1/2], Step [29800/64305], Loss: 4.9372\n",
      "Epoch [1/2], Step [29810/64305], Loss: 4.9262\n",
      "Epoch [1/2], Step [29820/64305], Loss: 5.1097\n",
      "Epoch [1/2], Step [29830/64305], Loss: 5.0132\n",
      "Epoch [1/2], Step [29840/64305], Loss: 5.3608\n",
      "Epoch [1/2], Step [29850/64305], Loss: 5.1451\n",
      "Epoch [1/2], Step [29860/64305], Loss: 5.0262\n",
      "Epoch [1/2], Step [29870/64305], Loss: 5.0719\n",
      "Epoch [1/2], Step [29880/64305], Loss: 5.0937\n",
      "Epoch [1/2], Step [29890/64305], Loss: 5.1932\n",
      "Epoch [1/2], Step [29900/64305], Loss: 4.9458\n",
      "Epoch [1/2], Step [29910/64305], Loss: 5.0896\n",
      "Epoch [1/2], Step [29920/64305], Loss: 5.1963\n",
      "Epoch [1/2], Step [29930/64305], Loss: 4.9812\n",
      "Epoch [1/2], Step [29940/64305], Loss: 4.9517\n",
      "Epoch [1/2], Step [29950/64305], Loss: 4.8540\n",
      "Epoch [1/2], Step [29960/64305], Loss: 4.9446\n",
      "Epoch [1/2], Step [29970/64305], Loss: 5.0003\n",
      "Epoch [1/2], Step [29980/64305], Loss: 5.3107\n",
      "Epoch [1/2], Step [29990/64305], Loss: 4.9163\n",
      "Epoch [1/2], Step [30000/64305], Loss: 4.7390\n",
      "Epoch [1/2], Step [30010/64305], Loss: 5.1442\n",
      "Epoch [1/2], Step [30020/64305], Loss: 4.9619\n",
      "Epoch [1/2], Step [30030/64305], Loss: 4.8912\n",
      "Epoch [1/2], Step [30040/64305], Loss: 5.1463\n",
      "Epoch [1/2], Step [30050/64305], Loss: 4.9247\n",
      "Epoch [1/2], Step [30060/64305], Loss: 4.9257\n",
      "Epoch [1/2], Step [30070/64305], Loss: 5.1604\n",
      "Epoch [1/2], Step [30080/64305], Loss: 5.0369\n",
      "Epoch [1/2], Step [30090/64305], Loss: 4.7884\n",
      "Epoch [1/2], Step [30100/64305], Loss: 5.1868\n",
      "Epoch [1/2], Step [30110/64305], Loss: 5.0840\n",
      "Epoch [1/2], Step [30120/64305], Loss: 5.1232\n",
      "Epoch [1/2], Step [30130/64305], Loss: 5.1627\n",
      "Epoch [1/2], Step [30140/64305], Loss: 5.1275\n",
      "Epoch [1/2], Step [30150/64305], Loss: 5.0249\n",
      "Epoch [1/2], Step [30160/64305], Loss: 5.2021\n",
      "Epoch [1/2], Step [30170/64305], Loss: 5.1002\n",
      "Epoch [1/2], Step [30180/64305], Loss: 5.0583\n",
      "Epoch [1/2], Step [30190/64305], Loss: 5.1709\n",
      "Epoch [1/2], Step [30200/64305], Loss: 5.2045\n",
      "Epoch [1/2], Step [30210/64305], Loss: 5.1235\n",
      "Epoch [1/2], Step [30220/64305], Loss: 5.1258\n",
      "Epoch [1/2], Step [30230/64305], Loss: 4.9427\n",
      "Epoch [1/2], Step [30240/64305], Loss: 5.1039\n",
      "Epoch [1/2], Step [30250/64305], Loss: 4.9440\n",
      "Epoch [1/2], Step [30260/64305], Loss: 5.1631\n",
      "Epoch [1/2], Step [30270/64305], Loss: 5.0418\n",
      "Epoch [1/2], Step [30280/64305], Loss: 5.1864\n",
      "Epoch [1/2], Step [30290/64305], Loss: 5.1050\n",
      "Epoch [1/2], Step [30300/64305], Loss: 4.9553\n",
      "Epoch [1/2], Step [30310/64305], Loss: 4.9267\n",
      "Epoch [1/2], Step [30320/64305], Loss: 4.9763\n",
      "Epoch [1/2], Step [30330/64305], Loss: 4.9951\n",
      "Epoch [1/2], Step [30340/64305], Loss: 5.1373\n",
      "Epoch [1/2], Step [30350/64305], Loss: 5.0110\n",
      "Epoch [1/2], Step [30360/64305], Loss: 4.8809\n",
      "Epoch [1/2], Step [30370/64305], Loss: 5.0143\n",
      "Epoch [1/2], Step [30380/64305], Loss: 4.8065\n",
      "Epoch [1/2], Step [30390/64305], Loss: 5.0230\n",
      "Epoch [1/2], Step [30400/64305], Loss: 5.0086\n",
      "Epoch [1/2], Step [30410/64305], Loss: 4.9903\n",
      "Epoch [1/2], Step [30420/64305], Loss: 4.8144\n",
      "Epoch [1/2], Step [30430/64305], Loss: 5.1795\n",
      "Epoch [1/2], Step [30440/64305], Loss: 4.9385\n",
      "Epoch [1/2], Step [30450/64305], Loss: 5.0530\n",
      "Epoch [1/2], Step [30460/64305], Loss: 5.2141\n",
      "Epoch [1/2], Step [30470/64305], Loss: 4.8131\n",
      "Epoch [1/2], Step [30480/64305], Loss: 4.8921\n",
      "Epoch [1/2], Step [30490/64305], Loss: 4.9495\n",
      "Epoch [1/2], Step [30500/64305], Loss: 4.8686\n",
      "Epoch [1/2], Step [30510/64305], Loss: 5.0523\n",
      "Epoch [1/2], Step [30520/64305], Loss: 4.9182\n",
      "Epoch [1/2], Step [30530/64305], Loss: 5.0003\n",
      "Epoch [1/2], Step [30540/64305], Loss: 4.9156\n",
      "Epoch [1/2], Step [30550/64305], Loss: 4.9068\n",
      "Epoch [1/2], Step [30560/64305], Loss: 5.1638\n",
      "Epoch [1/2], Step [30570/64305], Loss: 5.2218\n",
      "Epoch [1/2], Step [30580/64305], Loss: 4.9961\n",
      "Epoch [1/2], Step [30590/64305], Loss: 4.9887\n",
      "Epoch [1/2], Step [30600/64305], Loss: 5.0377\n",
      "Epoch [1/2], Step [30610/64305], Loss: 4.9707\n",
      "Epoch [1/2], Step [30620/64305], Loss: 5.2984\n",
      "Epoch [1/2], Step [30630/64305], Loss: 5.0090\n",
      "Epoch [1/2], Step [30640/64305], Loss: 4.9973\n",
      "Epoch [1/2], Step [30650/64305], Loss: 4.8745\n",
      "Epoch [1/2], Step [30660/64305], Loss: 5.0086\n",
      "Epoch [1/2], Step [30670/64305], Loss: 5.0314\n",
      "Epoch [1/2], Step [30680/64305], Loss: 5.1468\n",
      "Epoch [1/2], Step [30690/64305], Loss: 4.9370\n",
      "Epoch [1/2], Step [30700/64305], Loss: 5.0078\n",
      "Epoch [1/2], Step [30710/64305], Loss: 5.0871\n",
      "Epoch [1/2], Step [30720/64305], Loss: 4.9268\n",
      "Epoch [1/2], Step [30730/64305], Loss: 4.9214\n",
      "Epoch [1/2], Step [30740/64305], Loss: 5.0096\n",
      "Epoch [1/2], Step [30750/64305], Loss: 4.8901\n",
      "Epoch [1/2], Step [30760/64305], Loss: 5.0456\n",
      "Epoch [1/2], Step [30770/64305], Loss: 5.0986\n",
      "Epoch [1/2], Step [30780/64305], Loss: 5.1199\n",
      "Epoch [1/2], Step [30790/64305], Loss: 4.9256\n",
      "Epoch [1/2], Step [30800/64305], Loss: 5.0089\n",
      "Epoch [1/2], Step [30810/64305], Loss: 4.7918\n",
      "Epoch [1/2], Step [30820/64305], Loss: 4.8733\n",
      "Epoch [1/2], Step [30830/64305], Loss: 5.1259\n",
      "Epoch [1/2], Step [30840/64305], Loss: 5.0931\n",
      "Epoch [1/2], Step [30850/64305], Loss: 4.9341\n",
      "Epoch [1/2], Step [30860/64305], Loss: 4.9467\n",
      "Epoch [1/2], Step [30870/64305], Loss: 4.9894\n",
      "Epoch [1/2], Step [30880/64305], Loss: 4.9431\n",
      "Epoch [1/2], Step [30890/64305], Loss: 5.0963\n",
      "Epoch [1/2], Step [30900/64305], Loss: 5.1146\n",
      "Epoch [1/2], Step [30910/64305], Loss: 4.9021\n",
      "Epoch [1/2], Step [30920/64305], Loss: 5.1035\n",
      "Epoch [1/2], Step [30930/64305], Loss: 4.9674\n",
      "Epoch [1/2], Step [30940/64305], Loss: 5.1389\n",
      "Epoch [1/2], Step [30950/64305], Loss: 4.9496\n",
      "Epoch [1/2], Step [30960/64305], Loss: 5.3638\n",
      "Epoch [1/2], Step [30970/64305], Loss: 5.0373\n",
      "Epoch [1/2], Step [30980/64305], Loss: 4.9900\n",
      "Epoch [1/2], Step [30990/64305], Loss: 4.8888\n",
      "Epoch [1/2], Step [31000/64305], Loss: 5.1635\n",
      "Epoch [1/2], Step [31010/64305], Loss: 4.9272\n",
      "Epoch [1/2], Step [31020/64305], Loss: 5.1073\n",
      "Epoch [1/2], Step [31030/64305], Loss: 4.8445\n",
      "Epoch [1/2], Step [31040/64305], Loss: 5.1346\n",
      "Epoch [1/2], Step [31050/64305], Loss: 4.9736\n",
      "Epoch [1/2], Step [31060/64305], Loss: 4.9566\n",
      "Epoch [1/2], Step [31070/64305], Loss: 5.0491\n",
      "Epoch [1/2], Step [31080/64305], Loss: 5.0097\n",
      "Epoch [1/2], Step [31090/64305], Loss: 4.8465\n",
      "Epoch [1/2], Step [31100/64305], Loss: 5.0964\n",
      "Epoch [1/2], Step [31110/64305], Loss: 5.0485\n",
      "Epoch [1/2], Step [31120/64305], Loss: 5.1523\n",
      "Epoch [1/2], Step [31130/64305], Loss: 4.9230\n",
      "Epoch [1/2], Step [31140/64305], Loss: 4.8594\n",
      "Epoch [1/2], Step [31150/64305], Loss: 5.0441\n",
      "Epoch [1/2], Step [31160/64305], Loss: 5.2066\n",
      "Epoch [1/2], Step [31170/64305], Loss: 5.0736\n",
      "Epoch [1/2], Step [31180/64305], Loss: 5.3155\n",
      "Epoch [1/2], Step [31190/64305], Loss: 5.1534\n",
      "Epoch [1/2], Step [31200/64305], Loss: 4.9767\n",
      "Epoch [1/2], Step [31210/64305], Loss: 4.9102\n",
      "Epoch [1/2], Step [31220/64305], Loss: 5.0216\n",
      "Epoch [1/2], Step [31230/64305], Loss: 5.1712\n",
      "Epoch [1/2], Step [31240/64305], Loss: 5.1186\n",
      "Epoch [1/2], Step [31250/64305], Loss: 5.1171\n",
      "Epoch [1/2], Step [31260/64305], Loss: 5.0827\n",
      "Epoch [1/2], Step [31270/64305], Loss: 5.1986\n",
      "Epoch [1/2], Step [31280/64305], Loss: 4.9576\n",
      "Epoch [1/2], Step [31290/64305], Loss: 4.9889\n",
      "Epoch [1/2], Step [31300/64305], Loss: 5.2433\n",
      "Epoch [1/2], Step [31310/64305], Loss: 5.1119\n",
      "Epoch [1/2], Step [31320/64305], Loss: 5.1446\n",
      "Epoch [1/2], Step [31330/64305], Loss: 5.1171\n",
      "Epoch [1/2], Step [31340/64305], Loss: 5.1135\n",
      "Epoch [1/2], Step [31350/64305], Loss: 5.0571\n",
      "Epoch [1/2], Step [31360/64305], Loss: 4.9583\n",
      "Epoch [1/2], Step [31370/64305], Loss: 5.0063\n",
      "Epoch [1/2], Step [31380/64305], Loss: 5.1270\n",
      "Epoch [1/2], Step [31390/64305], Loss: 5.0224\n",
      "Epoch [1/2], Step [31400/64305], Loss: 4.9962\n",
      "Epoch [1/2], Step [31410/64305], Loss: 4.8786\n",
      "Epoch [1/2], Step [31420/64305], Loss: 5.1556\n",
      "Epoch [1/2], Step [31430/64305], Loss: 4.9833\n",
      "Epoch [1/2], Step [31440/64305], Loss: 5.1019\n",
      "Epoch [1/2], Step [31450/64305], Loss: 5.0541\n",
      "Epoch [1/2], Step [31460/64305], Loss: 5.2397\n",
      "Epoch [1/2], Step [31470/64305], Loss: 5.1064\n",
      "Epoch [1/2], Step [31480/64305], Loss: 5.0483\n",
      "Epoch [1/2], Step [31490/64305], Loss: 4.9564\n",
      "Epoch [1/2], Step [31500/64305], Loss: 4.9850\n",
      "Epoch [1/2], Step [31510/64305], Loss: 5.0381\n",
      "Epoch [1/2], Step [31520/64305], Loss: 5.1694\n",
      "Epoch [1/2], Step [31530/64305], Loss: 5.0711\n",
      "Epoch [1/2], Step [31540/64305], Loss: 5.1310\n",
      "Epoch [1/2], Step [31550/64305], Loss: 4.8875\n",
      "Epoch [1/2], Step [31560/64305], Loss: 5.1183\n",
      "Epoch [1/2], Step [31570/64305], Loss: 5.1636\n",
      "Epoch [1/2], Step [31580/64305], Loss: 5.0592\n",
      "Epoch [1/2], Step [31590/64305], Loss: 5.0475\n",
      "Epoch [1/2], Step [31600/64305], Loss: 4.9171\n",
      "Epoch [1/2], Step [31610/64305], Loss: 5.0437\n",
      "Epoch [1/2], Step [31620/64305], Loss: 5.1177\n",
      "Epoch [1/2], Step [31630/64305], Loss: 5.0297\n",
      "Epoch [1/2], Step [31640/64305], Loss: 4.8540\n",
      "Epoch [1/2], Step [31650/64305], Loss: 4.7455\n",
      "Epoch [1/2], Step [31660/64305], Loss: 4.8952\n",
      "Epoch [1/2], Step [31670/64305], Loss: 5.0539\n",
      "Epoch [1/2], Step [31680/64305], Loss: 4.9274\n",
      "Epoch [1/2], Step [31690/64305], Loss: 5.2002\n",
      "Epoch [1/2], Step [31700/64305], Loss: 4.8798\n",
      "Epoch [1/2], Step [31710/64305], Loss: 5.0626\n",
      "Epoch [1/2], Step [31720/64305], Loss: 5.1152\n",
      "Epoch [1/2], Step [31730/64305], Loss: 5.0895\n",
      "Epoch [1/2], Step [31740/64305], Loss: 4.9562\n",
      "Epoch [1/2], Step [31750/64305], Loss: 4.9765\n",
      "Epoch [1/2], Step [31760/64305], Loss: 4.9540\n",
      "Epoch [1/2], Step [31770/64305], Loss: 4.8622\n",
      "Epoch [1/2], Step [31780/64305], Loss: 4.9160\n",
      "Epoch [1/2], Step [31790/64305], Loss: 5.0333\n",
      "Epoch [1/2], Step [31800/64305], Loss: 5.1256\n",
      "Epoch [1/2], Step [31810/64305], Loss: 4.8438\n",
      "Epoch [1/2], Step [31820/64305], Loss: 4.9014\n",
      "Epoch [1/2], Step [31830/64305], Loss: 5.1392\n",
      "Epoch [1/2], Step [31840/64305], Loss: 5.1646\n",
      "Epoch [1/2], Step [31850/64305], Loss: 5.0816\n",
      "Epoch [1/2], Step [31860/64305], Loss: 5.0945\n",
      "Epoch [1/2], Step [31870/64305], Loss: 4.8708\n",
      "Epoch [1/2], Step [31880/64305], Loss: 5.1160\n",
      "Epoch [1/2], Step [31890/64305], Loss: 4.8889\n",
      "Epoch [1/2], Step [31900/64305], Loss: 5.1830\n",
      "Epoch [1/2], Step [31910/64305], Loss: 4.9405\n",
      "Epoch [1/2], Step [31920/64305], Loss: 5.0144\n",
      "Epoch [1/2], Step [31930/64305], Loss: 5.0200\n",
      "Epoch [1/2], Step [31940/64305], Loss: 5.0456\n",
      "Epoch [1/2], Step [31950/64305], Loss: 4.9697\n",
      "Epoch [1/2], Step [31960/64305], Loss: 5.0103\n",
      "Epoch [1/2], Step [31970/64305], Loss: 4.9753\n",
      "Epoch [1/2], Step [31980/64305], Loss: 4.8797\n",
      "Epoch [1/2], Step [31990/64305], Loss: 5.1385\n",
      "Epoch [1/2], Step [32000/64305], Loss: 5.1481\n",
      "Epoch [1/2], Step [32010/64305], Loss: 5.0126\n",
      "Epoch [1/2], Step [32020/64305], Loss: 4.9711\n",
      "Epoch [1/2], Step [32030/64305], Loss: 4.9465\n",
      "Epoch [1/2], Step [32040/64305], Loss: 5.1991\n",
      "Epoch [1/2], Step [32050/64305], Loss: 5.0333\n",
      "Epoch [1/2], Step [32060/64305], Loss: 4.9930\n",
      "Epoch [1/2], Step [32070/64305], Loss: 5.2732\n",
      "Epoch [1/2], Step [32080/64305], Loss: 5.1180\n",
      "Epoch [1/2], Step [32090/64305], Loss: 4.8924\n",
      "Epoch [1/2], Step [32100/64305], Loss: 5.2037\n",
      "Epoch [1/2], Step [32110/64305], Loss: 5.0567\n",
      "Epoch [1/2], Step [32120/64305], Loss: 5.1024\n",
      "Epoch [1/2], Step [32130/64305], Loss: 4.8936\n",
      "Epoch [1/2], Step [32140/64305], Loss: 4.8716\n",
      "Epoch [1/2], Step [32150/64305], Loss: 4.9930\n",
      "Epoch [1/2], Step [32160/64305], Loss: 5.1420\n",
      "Epoch [1/2], Step [32170/64305], Loss: 5.1636\n",
      "Epoch [1/2], Step [32180/64305], Loss: 5.0192\n",
      "Epoch [1/2], Step [32190/64305], Loss: 4.9853\n",
      "Epoch [1/2], Step [32200/64305], Loss: 5.1640\n",
      "Epoch [1/2], Step [32210/64305], Loss: 4.9598\n",
      "Epoch [1/2], Step [32220/64305], Loss: 4.8816\n",
      "Epoch [1/2], Step [32230/64305], Loss: 5.0235\n",
      "Epoch [1/2], Step [32240/64305], Loss: 5.1178\n",
      "Epoch [1/2], Step [32250/64305], Loss: 4.8610\n",
      "Epoch [1/2], Step [32260/64305], Loss: 5.0656\n",
      "Epoch [1/2], Step [32270/64305], Loss: 5.1180\n",
      "Epoch [1/2], Step [32280/64305], Loss: 5.1337\n",
      "Epoch [1/2], Step [32290/64305], Loss: 4.9835\n",
      "Epoch [1/2], Step [32300/64305], Loss: 5.0955\n",
      "Epoch [1/2], Step [32310/64305], Loss: 4.9222\n",
      "Epoch [1/2], Step [32320/64305], Loss: 4.9018\n",
      "Epoch [1/2], Step [32330/64305], Loss: 5.2373\n",
      "Epoch [1/2], Step [32340/64305], Loss: 4.9595\n",
      "Epoch [1/2], Step [32350/64305], Loss: 4.9857\n",
      "Epoch [1/2], Step [32360/64305], Loss: 4.9901\n",
      "Epoch [1/2], Step [32370/64305], Loss: 5.1159\n",
      "Epoch [1/2], Step [32380/64305], Loss: 5.0965\n",
      "Epoch [1/2], Step [32390/64305], Loss: 5.0412\n",
      "Epoch [1/2], Step [32400/64305], Loss: 4.9221\n",
      "Epoch [1/2], Step [32410/64305], Loss: 5.2104\n",
      "Epoch [1/2], Step [32420/64305], Loss: 4.8869\n",
      "Epoch [1/2], Step [32430/64305], Loss: 5.0082\n",
      "Epoch [1/2], Step [32440/64305], Loss: 5.1367\n",
      "Epoch [1/2], Step [32450/64305], Loss: 5.0562\n",
      "Epoch [1/2], Step [32460/64305], Loss: 5.0548\n",
      "Epoch [1/2], Step [32470/64305], Loss: 5.0411\n",
      "Epoch [1/2], Step [32480/64305], Loss: 4.9207\n",
      "Epoch [1/2], Step [32490/64305], Loss: 5.0365\n",
      "Epoch [1/2], Step [32500/64305], Loss: 5.0128\n",
      "Epoch [1/2], Step [32510/64305], Loss: 5.1190\n",
      "Epoch [1/2], Step [32520/64305], Loss: 5.1526\n",
      "Epoch [1/2], Step [32530/64305], Loss: 5.1460\n",
      "Epoch [1/2], Step [32540/64305], Loss: 5.1196\n",
      "Epoch [1/2], Step [32550/64305], Loss: 5.0822\n",
      "Epoch [1/2], Step [32560/64305], Loss: 5.0834\n",
      "Epoch [1/2], Step [32570/64305], Loss: 4.9980\n",
      "Epoch [1/2], Step [32580/64305], Loss: 4.9664\n",
      "Epoch [1/2], Step [32590/64305], Loss: 5.0847\n",
      "Epoch [1/2], Step [32600/64305], Loss: 4.9371\n",
      "Epoch [1/2], Step [32610/64305], Loss: 4.8474\n",
      "Epoch [1/2], Step [32620/64305], Loss: 5.1347\n",
      "Epoch [1/2], Step [32630/64305], Loss: 5.0655\n",
      "Epoch [1/2], Step [32640/64305], Loss: 5.0286\n",
      "Epoch [1/2], Step [32650/64305], Loss: 5.3198\n",
      "Epoch [1/2], Step [32660/64305], Loss: 5.2594\n",
      "Epoch [1/2], Step [32670/64305], Loss: 4.9359\n",
      "Epoch [1/2], Step [32680/64305], Loss: 5.0816\n",
      "Epoch [1/2], Step [32690/64305], Loss: 4.9906\n",
      "Epoch [1/2], Step [32700/64305], Loss: 4.9486\n",
      "Epoch [1/2], Step [32710/64305], Loss: 5.0720\n",
      "Epoch [1/2], Step [32720/64305], Loss: 5.2243\n",
      "Epoch [1/2], Step [32730/64305], Loss: 4.9667\n",
      "Epoch [1/2], Step [32740/64305], Loss: 5.0794\n",
      "Epoch [1/2], Step [32750/64305], Loss: 5.0226\n",
      "Epoch [1/2], Step [32760/64305], Loss: 5.2945\n",
      "Epoch [1/2], Step [32770/64305], Loss: 4.9717\n",
      "Epoch [1/2], Step [32780/64305], Loss: 4.8695\n",
      "Epoch [1/2], Step [32790/64305], Loss: 5.1042\n",
      "Epoch [1/2], Step [32800/64305], Loss: 4.9672\n",
      "Epoch [1/2], Step [32810/64305], Loss: 4.9710\n",
      "Epoch [1/2], Step [32820/64305], Loss: 4.9418\n",
      "Epoch [1/2], Step [32830/64305], Loss: 4.8088\n",
      "Epoch [1/2], Step [32840/64305], Loss: 4.8661\n",
      "Epoch [1/2], Step [32850/64305], Loss: 5.1665\n",
      "Epoch [1/2], Step [32860/64305], Loss: 5.0479\n",
      "Epoch [1/2], Step [32870/64305], Loss: 5.0002\n",
      "Epoch [1/2], Step [32880/64305], Loss: 5.1098\n",
      "Epoch [1/2], Step [32890/64305], Loss: 5.0751\n",
      "Epoch [1/2], Step [32900/64305], Loss: 4.9121\n",
      "Epoch [1/2], Step [32910/64305], Loss: 5.1393\n",
      "Epoch [1/2], Step [32920/64305], Loss: 4.9284\n",
      "Epoch [1/2], Step [32930/64305], Loss: 5.1790\n",
      "Epoch [1/2], Step [32940/64305], Loss: 5.0548\n",
      "Epoch [1/2], Step [32950/64305], Loss: 5.1503\n",
      "Epoch [1/2], Step [32960/64305], Loss: 5.0334\n",
      "Epoch [1/2], Step [32970/64305], Loss: 4.8652\n",
      "Epoch [1/2], Step [32980/64305], Loss: 5.1908\n",
      "Epoch [1/2], Step [32990/64305], Loss: 5.2412\n",
      "Epoch [1/2], Step [33000/64305], Loss: 5.0078\n",
      "Epoch [1/2], Step [33010/64305], Loss: 5.0487\n",
      "Epoch [1/2], Step [33020/64305], Loss: 4.9766\n",
      "Epoch [1/2], Step [33030/64305], Loss: 4.8683\n",
      "Epoch [1/2], Step [33040/64305], Loss: 5.0553\n",
      "Epoch [1/2], Step [33050/64305], Loss: 4.8240\n",
      "Epoch [1/2], Step [33060/64305], Loss: 5.3058\n",
      "Epoch [1/2], Step [33070/64305], Loss: 5.2277\n",
      "Epoch [1/2], Step [33080/64305], Loss: 4.9218\n",
      "Epoch [1/2], Step [33090/64305], Loss: 5.1102\n",
      "Epoch [1/2], Step [33100/64305], Loss: 4.9457\n",
      "Epoch [1/2], Step [33110/64305], Loss: 5.1159\n",
      "Epoch [1/2], Step [33120/64305], Loss: 4.9214\n",
      "Epoch [1/2], Step [33130/64305], Loss: 5.0801\n",
      "Epoch [1/2], Step [33140/64305], Loss: 4.9638\n",
      "Epoch [1/2], Step [33150/64305], Loss: 4.9506\n",
      "Epoch [1/2], Step [33160/64305], Loss: 5.0615\n",
      "Epoch [1/2], Step [33170/64305], Loss: 5.1661\n",
      "Epoch [1/2], Step [33180/64305], Loss: 5.1267\n",
      "Epoch [1/2], Step [33190/64305], Loss: 5.0799\n",
      "Epoch [1/2], Step [33200/64305], Loss: 4.9482\n",
      "Epoch [1/2], Step [33210/64305], Loss: 5.0661\n",
      "Epoch [1/2], Step [33220/64305], Loss: 5.1409\n",
      "Epoch [1/2], Step [33230/64305], Loss: 5.0167\n",
      "Epoch [1/2], Step [33240/64305], Loss: 5.1269\n",
      "Epoch [1/2], Step [33250/64305], Loss: 5.1310\n",
      "Epoch [1/2], Step [33260/64305], Loss: 5.1792\n",
      "Epoch [1/2], Step [33270/64305], Loss: 5.0681\n",
      "Epoch [1/2], Step [33280/64305], Loss: 5.2348\n",
      "Epoch [1/2], Step [33290/64305], Loss: 5.0672\n",
      "Epoch [1/2], Step [33300/64305], Loss: 5.0242\n",
      "Epoch [1/2], Step [33310/64305], Loss: 5.0067\n",
      "Epoch [1/2], Step [33320/64305], Loss: 5.2083\n",
      "Epoch [1/2], Step [33330/64305], Loss: 4.9872\n",
      "Epoch [1/2], Step [33340/64305], Loss: 5.0085\n",
      "Epoch [1/2], Step [33350/64305], Loss: 5.0560\n",
      "Epoch [1/2], Step [33360/64305], Loss: 5.0903\n",
      "Epoch [1/2], Step [33370/64305], Loss: 4.9886\n",
      "Epoch [1/2], Step [33380/64305], Loss: 5.0654\n",
      "Epoch [1/2], Step [33390/64305], Loss: 4.8482\n",
      "Epoch [1/2], Step [33400/64305], Loss: 5.0216\n",
      "Epoch [1/2], Step [33410/64305], Loss: 5.1069\n",
      "Epoch [1/2], Step [33420/64305], Loss: 5.0628\n",
      "Epoch [1/2], Step [33430/64305], Loss: 5.0341\n",
      "Epoch [1/2], Step [33440/64305], Loss: 4.9728\n",
      "Epoch [1/2], Step [33450/64305], Loss: 4.9473\n",
      "Epoch [1/2], Step [33460/64305], Loss: 5.0348\n",
      "Epoch [1/2], Step [33470/64305], Loss: 4.9435\n",
      "Epoch [1/2], Step [33480/64305], Loss: 5.0232\n",
      "Epoch [1/2], Step [33490/64305], Loss: 4.7562\n",
      "Epoch [1/2], Step [33500/64305], Loss: 5.2036\n",
      "Epoch [1/2], Step [33510/64305], Loss: 5.1088\n",
      "Epoch [1/2], Step [33520/64305], Loss: 5.0639\n",
      "Epoch [1/2], Step [33530/64305], Loss: 4.8940\n",
      "Epoch [1/2], Step [33540/64305], Loss: 4.8812\n",
      "Epoch [1/2], Step [33550/64305], Loss: 5.0642\n",
      "Epoch [1/2], Step [33560/64305], Loss: 5.1171\n",
      "Epoch [1/2], Step [33570/64305], Loss: 5.1965\n",
      "Epoch [1/2], Step [33580/64305], Loss: 5.0644\n",
      "Epoch [1/2], Step [33590/64305], Loss: 5.0985\n",
      "Epoch [1/2], Step [33600/64305], Loss: 4.8925\n",
      "Epoch [1/2], Step [33610/64305], Loss: 5.0174\n",
      "Epoch [1/2], Step [33620/64305], Loss: 4.8319\n",
      "Epoch [1/2], Step [33630/64305], Loss: 5.0427\n",
      "Epoch [1/2], Step [33640/64305], Loss: 4.9511\n",
      "Epoch [1/2], Step [33650/64305], Loss: 5.0491\n",
      "Epoch [1/2], Step [33660/64305], Loss: 4.9331\n",
      "Epoch [1/2], Step [33670/64305], Loss: 4.8156\n",
      "Epoch [1/2], Step [33680/64305], Loss: 5.0467\n",
      "Epoch [1/2], Step [33690/64305], Loss: 4.8196\n",
      "Epoch [1/2], Step [33700/64305], Loss: 5.0456\n",
      "Epoch [1/2], Step [33710/64305], Loss: 5.1335\n",
      "Epoch [1/2], Step [33720/64305], Loss: 5.0800\n",
      "Epoch [1/2], Step [33730/64305], Loss: 4.9784\n",
      "Epoch [1/2], Step [33740/64305], Loss: 4.9406\n",
      "Epoch [1/2], Step [33750/64305], Loss: 5.0099\n",
      "Epoch [1/2], Step [33760/64305], Loss: 5.0787\n",
      "Epoch [1/2], Step [33770/64305], Loss: 5.1969\n",
      "Epoch [1/2], Step [33780/64305], Loss: 4.8968\n",
      "Epoch [1/2], Step [33790/64305], Loss: 5.0457\n",
      "Epoch [1/2], Step [33800/64305], Loss: 4.9782\n",
      "Epoch [1/2], Step [33810/64305], Loss: 4.9412\n",
      "Epoch [1/2], Step [33820/64305], Loss: 5.2525\n",
      "Epoch [1/2], Step [33830/64305], Loss: 5.1540\n",
      "Epoch [1/2], Step [33840/64305], Loss: 5.2794\n",
      "Epoch [1/2], Step [33850/64305], Loss: 4.9323\n",
      "Epoch [1/2], Step [33860/64305], Loss: 4.8964\n",
      "Epoch [1/2], Step [33870/64305], Loss: 5.0841\n",
      "Epoch [1/2], Step [33880/64305], Loss: 5.1483\n",
      "Epoch [1/2], Step [33890/64305], Loss: 4.9160\n",
      "Epoch [1/2], Step [33900/64305], Loss: 4.9916\n",
      "Epoch [1/2], Step [33910/64305], Loss: 5.0629\n",
      "Epoch [1/2], Step [33920/64305], Loss: 5.1126\n",
      "Epoch [1/2], Step [33930/64305], Loss: 4.9450\n",
      "Epoch [1/2], Step [33940/64305], Loss: 5.0445\n",
      "Epoch [1/2], Step [33950/64305], Loss: 4.8808\n",
      "Epoch [1/2], Step [33960/64305], Loss: 4.8243\n",
      "Epoch [1/2], Step [33970/64305], Loss: 5.0162\n",
      "Epoch [1/2], Step [33980/64305], Loss: 4.9483\n",
      "Epoch [1/2], Step [33990/64305], Loss: 5.0524\n",
      "Epoch [1/2], Step [34000/64305], Loss: 4.9679\n",
      "Epoch [1/2], Step [34010/64305], Loss: 4.8884\n",
      "Epoch [1/2], Step [34020/64305], Loss: 5.0383\n",
      "Epoch [1/2], Step [34030/64305], Loss: 5.0415\n",
      "Epoch [1/2], Step [34040/64305], Loss: 5.0579\n",
      "Epoch [1/2], Step [34050/64305], Loss: 5.0329\n",
      "Epoch [1/2], Step [34060/64305], Loss: 4.8317\n",
      "Epoch [1/2], Step [34070/64305], Loss: 4.9688\n",
      "Epoch [1/2], Step [34080/64305], Loss: 5.0077\n",
      "Epoch [1/2], Step [34090/64305], Loss: 4.9502\n",
      "Epoch [1/2], Step [34100/64305], Loss: 4.9508\n",
      "Epoch [1/2], Step [34110/64305], Loss: 5.1327\n",
      "Epoch [1/2], Step [34120/64305], Loss: 4.8970\n",
      "Epoch [1/2], Step [34130/64305], Loss: 5.0120\n",
      "Epoch [1/2], Step [34140/64305], Loss: 5.0704\n",
      "Epoch [1/2], Step [34150/64305], Loss: 5.1099\n",
      "Epoch [1/2], Step [34160/64305], Loss: 5.0584\n",
      "Epoch [1/2], Step [34170/64305], Loss: 5.0779\n",
      "Epoch [1/2], Step [34180/64305], Loss: 5.0742\n",
      "Epoch [1/2], Step [34190/64305], Loss: 5.1480\n",
      "Epoch [1/2], Step [34200/64305], Loss: 5.2492\n",
      "Epoch [1/2], Step [34210/64305], Loss: 4.8724\n",
      "Epoch [1/2], Step [34220/64305], Loss: 5.0476\n",
      "Epoch [1/2], Step [34230/64305], Loss: 4.9485\n",
      "Epoch [1/2], Step [34240/64305], Loss: 5.0774\n",
      "Epoch [1/2], Step [34250/64305], Loss: 5.0463\n",
      "Epoch [1/2], Step [34260/64305], Loss: 4.9438\n",
      "Epoch [1/2], Step [34270/64305], Loss: 5.0844\n",
      "Epoch [1/2], Step [34280/64305], Loss: 5.0242\n",
      "Epoch [1/2], Step [34290/64305], Loss: 5.1436\n",
      "Epoch [1/2], Step [34300/64305], Loss: 5.1093\n",
      "Epoch [1/2], Step [34310/64305], Loss: 4.9140\n",
      "Epoch [1/2], Step [34320/64305], Loss: 4.9317\n",
      "Epoch [1/2], Step [34330/64305], Loss: 4.8845\n",
      "Epoch [1/2], Step [34340/64305], Loss: 4.9048\n",
      "Epoch [1/2], Step [34350/64305], Loss: 4.8859\n",
      "Epoch [1/2], Step [34360/64305], Loss: 4.9160\n",
      "Epoch [1/2], Step [34370/64305], Loss: 5.1694\n",
      "Epoch [1/2], Step [34380/64305], Loss: 4.9642\n",
      "Epoch [1/2], Step [34390/64305], Loss: 5.0878\n",
      "Epoch [1/2], Step [34400/64305], Loss: 5.0087\n",
      "Epoch [1/2], Step [34410/64305], Loss: 5.1855\n",
      "Epoch [1/2], Step [34420/64305], Loss: 4.7693\n",
      "Epoch [1/2], Step [34430/64305], Loss: 5.0787\n",
      "Epoch [1/2], Step [34440/64305], Loss: 4.9467\n",
      "Epoch [1/2], Step [34450/64305], Loss: 4.9920\n",
      "Epoch [1/2], Step [34460/64305], Loss: 4.9339\n",
      "Epoch [1/2], Step [34470/64305], Loss: 4.8389\n",
      "Epoch [1/2], Step [34480/64305], Loss: 4.9238\n",
      "Epoch [1/2], Step [34490/64305], Loss: 5.1798\n",
      "Epoch [1/2], Step [34500/64305], Loss: 5.0384\n",
      "Epoch [1/2], Step [34510/64305], Loss: 5.2152\n",
      "Epoch [1/2], Step [34520/64305], Loss: 4.9585\n",
      "Epoch [1/2], Step [34530/64305], Loss: 4.9063\n",
      "Epoch [1/2], Step [34540/64305], Loss: 4.9664\n",
      "Epoch [1/2], Step [34550/64305], Loss: 5.1270\n",
      "Epoch [1/2], Step [34560/64305], Loss: 5.0744\n",
      "Epoch [1/2], Step [34570/64305], Loss: 5.1732\n",
      "Epoch [1/2], Step [34580/64305], Loss: 5.0219\n",
      "Epoch [1/2], Step [34590/64305], Loss: 5.0112\n",
      "Epoch [1/2], Step [34600/64305], Loss: 5.0859\n",
      "Epoch [1/2], Step [34610/64305], Loss: 5.1448\n",
      "Epoch [1/2], Step [34620/64305], Loss: 4.8033\n",
      "Epoch [1/2], Step [34630/64305], Loss: 5.0565\n",
      "Epoch [1/2], Step [34640/64305], Loss: 5.1377\n",
      "Epoch [1/2], Step [34650/64305], Loss: 5.1218\n",
      "Epoch [1/2], Step [34660/64305], Loss: 5.0043\n",
      "Epoch [1/2], Step [34670/64305], Loss: 4.9502\n",
      "Epoch [1/2], Step [34680/64305], Loss: 4.9701\n",
      "Epoch [1/2], Step [34690/64305], Loss: 4.9417\n",
      "Epoch [1/2], Step [34700/64305], Loss: 5.1107\n",
      "Epoch [1/2], Step [34710/64305], Loss: 4.8993\n",
      "Epoch [1/2], Step [34720/64305], Loss: 5.2389\n",
      "Epoch [1/2], Step [34730/64305], Loss: 5.0744\n",
      "Epoch [1/2], Step [34740/64305], Loss: 4.9974\n",
      "Epoch [1/2], Step [34750/64305], Loss: 4.7472\n",
      "Epoch [1/2], Step [34760/64305], Loss: 5.0379\n",
      "Epoch [1/2], Step [34770/64305], Loss: 5.0031\n",
      "Epoch [1/2], Step [34780/64305], Loss: 4.8406\n",
      "Epoch [1/2], Step [34790/64305], Loss: 5.0568\n",
      "Epoch [1/2], Step [34800/64305], Loss: 5.0287\n",
      "Epoch [1/2], Step [34810/64305], Loss: 5.0265\n",
      "Epoch [1/2], Step [34820/64305], Loss: 4.9179\n",
      "Epoch [1/2], Step [34830/64305], Loss: 5.1865\n",
      "Epoch [1/2], Step [34840/64305], Loss: 5.0017\n",
      "Epoch [1/2], Step [34850/64305], Loss: 5.0361\n",
      "Epoch [1/2], Step [34860/64305], Loss: 4.8608\n",
      "Epoch [1/2], Step [34870/64305], Loss: 5.1034\n",
      "Epoch [1/2], Step [34880/64305], Loss: 4.7857\n",
      "Epoch [1/2], Step [34890/64305], Loss: 5.1351\n",
      "Epoch [1/2], Step [34900/64305], Loss: 4.9090\n",
      "Epoch [1/2], Step [34910/64305], Loss: 5.1328\n",
      "Epoch [1/2], Step [34920/64305], Loss: 5.1342\n",
      "Epoch [1/2], Step [34930/64305], Loss: 5.2209\n",
      "Epoch [1/2], Step [34940/64305], Loss: 5.0706\n",
      "Epoch [1/2], Step [34950/64305], Loss: 5.0114\n",
      "Epoch [1/2], Step [34960/64305], Loss: 5.1422\n",
      "Epoch [1/2], Step [34970/64305], Loss: 5.0645\n",
      "Epoch [1/2], Step [34980/64305], Loss: 4.9460\n",
      "Epoch [1/2], Step [34990/64305], Loss: 5.1414\n",
      "Epoch [1/2], Step [35000/64305], Loss: 4.9657\n",
      "Epoch [1/2], Step [35010/64305], Loss: 5.1544\n",
      "Epoch [1/2], Step [35020/64305], Loss: 5.0049\n",
      "Epoch [1/2], Step [35030/64305], Loss: 5.0160\n",
      "Epoch [1/2], Step [35040/64305], Loss: 5.0670\n",
      "Epoch [1/2], Step [35050/64305], Loss: 4.7557\n",
      "Epoch [1/2], Step [35060/64305], Loss: 5.1362\n",
      "Epoch [1/2], Step [35070/64305], Loss: 5.0205\n",
      "Epoch [1/2], Step [35080/64305], Loss: 4.9490\n",
      "Epoch [1/2], Step [35090/64305], Loss: 4.8079\n",
      "Epoch [1/2], Step [35100/64305], Loss: 4.9861\n",
      "Epoch [1/2], Step [35110/64305], Loss: 4.9381\n",
      "Epoch [1/2], Step [35120/64305], Loss: 5.2868\n",
      "Epoch [1/2], Step [35130/64305], Loss: 4.7639\n",
      "Epoch [1/2], Step [35140/64305], Loss: 5.1986\n",
      "Epoch [1/2], Step [35150/64305], Loss: 4.9946\n",
      "Epoch [1/2], Step [35160/64305], Loss: 5.1932\n",
      "Epoch [1/2], Step [35170/64305], Loss: 4.8497\n",
      "Epoch [1/2], Step [35180/64305], Loss: 5.1073\n",
      "Epoch [1/2], Step [35190/64305], Loss: 4.9465\n",
      "Epoch [1/2], Step [35200/64305], Loss: 4.8830\n",
      "Epoch [1/2], Step [35210/64305], Loss: 4.9641\n",
      "Epoch [1/2], Step [35220/64305], Loss: 5.0143\n",
      "Epoch [1/2], Step [35230/64305], Loss: 5.0034\n",
      "Epoch [1/2], Step [35240/64305], Loss: 5.0654\n",
      "Epoch [1/2], Step [35250/64305], Loss: 5.0986\n",
      "Epoch [1/2], Step [35260/64305], Loss: 4.9039\n",
      "Epoch [1/2], Step [35270/64305], Loss: 5.1317\n",
      "Epoch [1/2], Step [35280/64305], Loss: 5.0404\n",
      "Epoch [1/2], Step [35290/64305], Loss: 5.0871\n",
      "Epoch [1/2], Step [35300/64305], Loss: 5.0437\n",
      "Epoch [1/2], Step [35310/64305], Loss: 5.1063\n",
      "Epoch [1/2], Step [35320/64305], Loss: 4.8165\n",
      "Epoch [1/2], Step [35330/64305], Loss: 5.2553\n",
      "Epoch [1/2], Step [35340/64305], Loss: 4.9537\n",
      "Epoch [1/2], Step [35350/64305], Loss: 5.1545\n",
      "Epoch [1/2], Step [35360/64305], Loss: 4.9679\n",
      "Epoch [1/2], Step [35370/64305], Loss: 5.2208\n",
      "Epoch [1/2], Step [35380/64305], Loss: 4.8710\n",
      "Epoch [1/2], Step [35390/64305], Loss: 5.0346\n",
      "Epoch [1/2], Step [35400/64305], Loss: 4.8877\n",
      "Epoch [1/2], Step [35410/64305], Loss: 5.0279\n",
      "Epoch [1/2], Step [35420/64305], Loss: 5.0571\n",
      "Epoch [1/2], Step [35430/64305], Loss: 4.9870\n",
      "Epoch [1/2], Step [35440/64305], Loss: 4.6845\n",
      "Epoch [1/2], Step [35450/64305], Loss: 5.0971\n",
      "Epoch [1/2], Step [35460/64305], Loss: 5.2249\n",
      "Epoch [1/2], Step [35470/64305], Loss: 4.9053\n",
      "Epoch [1/2], Step [35480/64305], Loss: 4.9900\n",
      "Epoch [1/2], Step [35490/64305], Loss: 5.1048\n",
      "Epoch [1/2], Step [35500/64305], Loss: 5.1045\n",
      "Epoch [1/2], Step [35510/64305], Loss: 5.2211\n",
      "Epoch [1/2], Step [35520/64305], Loss: 5.0815\n",
      "Epoch [1/2], Step [35530/64305], Loss: 5.1886\n",
      "Epoch [1/2], Step [35540/64305], Loss: 4.9877\n",
      "Epoch [1/2], Step [35550/64305], Loss: 5.0926\n",
      "Epoch [1/2], Step [35560/64305], Loss: 4.9738\n",
      "Epoch [1/2], Step [35570/64305], Loss: 5.0562\n",
      "Epoch [1/2], Step [35580/64305], Loss: 5.2346\n",
      "Epoch [1/2], Step [35590/64305], Loss: 4.9533\n",
      "Epoch [1/2], Step [35600/64305], Loss: 4.8967\n",
      "Epoch [1/2], Step [35610/64305], Loss: 4.5880\n",
      "Epoch [1/2], Step [35620/64305], Loss: 5.1330\n",
      "Epoch [1/2], Step [35630/64305], Loss: 5.0980\n",
      "Epoch [1/2], Step [35640/64305], Loss: 4.9024\n",
      "Epoch [1/2], Step [35650/64305], Loss: 4.9162\n",
      "Epoch [1/2], Step [35660/64305], Loss: 4.9440\n",
      "Epoch [1/2], Step [35670/64305], Loss: 5.1326\n",
      "Epoch [1/2], Step [35680/64305], Loss: 5.0388\n",
      "Epoch [1/2], Step [35690/64305], Loss: 4.8819\n",
      "Epoch [1/2], Step [35700/64305], Loss: 5.0651\n",
      "Epoch [1/2], Step [35710/64305], Loss: 4.9958\n",
      "Epoch [1/2], Step [35720/64305], Loss: 5.0891\n",
      "Epoch [1/2], Step [35730/64305], Loss: 4.9122\n",
      "Epoch [1/2], Step [35740/64305], Loss: 4.8887\n",
      "Epoch [1/2], Step [35750/64305], Loss: 4.8752\n",
      "Epoch [1/2], Step [35760/64305], Loss: 4.8368\n",
      "Epoch [1/2], Step [35770/64305], Loss: 5.1395\n",
      "Epoch [1/2], Step [35780/64305], Loss: 5.1680\n",
      "Epoch [1/2], Step [35790/64305], Loss: 5.1541\n",
      "Epoch [1/2], Step [35800/64305], Loss: 4.9398\n",
      "Epoch [1/2], Step [35810/64305], Loss: 5.1531\n",
      "Epoch [1/2], Step [35820/64305], Loss: 5.1126\n",
      "Epoch [1/2], Step [35830/64305], Loss: 4.9141\n",
      "Epoch [1/2], Step [35840/64305], Loss: 4.8749\n",
      "Epoch [1/2], Step [35850/64305], Loss: 5.1601\n",
      "Epoch [1/2], Step [35860/64305], Loss: 4.9674\n",
      "Epoch [1/2], Step [35870/64305], Loss: 5.1888\n",
      "Epoch [1/2], Step [35880/64305], Loss: 4.9681\n",
      "Epoch [1/2], Step [35890/64305], Loss: 4.9306\n",
      "Epoch [1/2], Step [35900/64305], Loss: 5.0399\n",
      "Epoch [1/2], Step [35910/64305], Loss: 5.0221\n",
      "Epoch [1/2], Step [35920/64305], Loss: 5.0698\n",
      "Epoch [1/2], Step [35930/64305], Loss: 4.9036\n",
      "Epoch [1/2], Step [35940/64305], Loss: 4.9052\n",
      "Epoch [1/2], Step [35950/64305], Loss: 5.0387\n",
      "Epoch [1/2], Step [35960/64305], Loss: 4.8039\n",
      "Epoch [1/2], Step [35970/64305], Loss: 5.0379\n",
      "Epoch [1/2], Step [35980/64305], Loss: 4.8490\n",
      "Epoch [1/2], Step [35990/64305], Loss: 4.9719\n",
      "Epoch [1/2], Step [36000/64305], Loss: 4.8388\n",
      "Epoch [1/2], Step [36010/64305], Loss: 4.8277\n",
      "Epoch [1/2], Step [36020/64305], Loss: 4.9264\n",
      "Epoch [1/2], Step [36030/64305], Loss: 4.9688\n",
      "Epoch [1/2], Step [36040/64305], Loss: 5.1578\n",
      "Epoch [1/2], Step [36050/64305], Loss: 5.1089\n",
      "Epoch [1/2], Step [36060/64305], Loss: 4.7390\n",
      "Epoch [1/2], Step [36070/64305], Loss: 4.9827\n",
      "Epoch [1/2], Step [36080/64305], Loss: 4.8747\n",
      "Epoch [1/2], Step [36090/64305], Loss: 5.1258\n",
      "Epoch [1/2], Step [36100/64305], Loss: 5.0172\n",
      "Epoch [1/2], Step [36110/64305], Loss: 5.0219\n",
      "Epoch [1/2], Step [36120/64305], Loss: 4.9624\n",
      "Epoch [1/2], Step [36130/64305], Loss: 5.0102\n",
      "Epoch [1/2], Step [36140/64305], Loss: 4.9428\n",
      "Epoch [1/2], Step [36150/64305], Loss: 4.8558\n",
      "Epoch [1/2], Step [36160/64305], Loss: 5.1186\n",
      "Epoch [1/2], Step [36170/64305], Loss: 4.9890\n",
      "Epoch [1/2], Step [36180/64305], Loss: 4.9525\n",
      "Epoch [1/2], Step [36190/64305], Loss: 4.9193\n",
      "Epoch [1/2], Step [36200/64305], Loss: 5.0210\n",
      "Epoch [1/2], Step [36210/64305], Loss: 5.1415\n",
      "Epoch [1/2], Step [36220/64305], Loss: 4.8957\n",
      "Epoch [1/2], Step [36230/64305], Loss: 5.0508\n",
      "Epoch [1/2], Step [36240/64305], Loss: 5.0982\n",
      "Epoch [1/2], Step [36250/64305], Loss: 5.0574\n",
      "Epoch [1/2], Step [36260/64305], Loss: 4.9160\n",
      "Epoch [1/2], Step [36270/64305], Loss: 4.8956\n",
      "Epoch [1/2], Step [36280/64305], Loss: 5.0456\n",
      "Epoch [1/2], Step [36290/64305], Loss: 4.8387\n",
      "Epoch [1/2], Step [36300/64305], Loss: 5.0597\n",
      "Epoch [1/2], Step [36310/64305], Loss: 5.0270\n",
      "Epoch [1/2], Step [36320/64305], Loss: 4.9579\n",
      "Epoch [1/2], Step [36330/64305], Loss: 4.9651\n",
      "Epoch [1/2], Step [36340/64305], Loss: 5.3488\n",
      "Epoch [1/2], Step [36350/64305], Loss: 4.9003\n",
      "Epoch [1/2], Step [36360/64305], Loss: 4.9969\n",
      "Epoch [1/2], Step [36370/64305], Loss: 5.0920\n",
      "Epoch [1/2], Step [36380/64305], Loss: 4.8763\n",
      "Epoch [1/2], Step [36390/64305], Loss: 5.1118\n",
      "Epoch [1/2], Step [36400/64305], Loss: 4.8156\n",
      "Epoch [1/2], Step [36410/64305], Loss: 4.9467\n",
      "Epoch [1/2], Step [36420/64305], Loss: 5.0003\n",
      "Epoch [1/2], Step [36430/64305], Loss: 5.0638\n",
      "Epoch [1/2], Step [36440/64305], Loss: 4.8642\n",
      "Epoch [1/2], Step [36450/64305], Loss: 4.9213\n",
      "Epoch [1/2], Step [36460/64305], Loss: 4.8560\n",
      "Epoch [1/2], Step [36470/64305], Loss: 4.9500\n",
      "Epoch [1/2], Step [36480/64305], Loss: 5.1839\n",
      "Epoch [1/2], Step [36490/64305], Loss: 4.9328\n",
      "Epoch [1/2], Step [36500/64305], Loss: 5.1962\n",
      "Epoch [1/2], Step [36510/64305], Loss: 4.7959\n",
      "Epoch [1/2], Step [36520/64305], Loss: 5.1222\n",
      "Epoch [1/2], Step [36530/64305], Loss: 5.0409\n",
      "Epoch [1/2], Step [36540/64305], Loss: 5.1067\n",
      "Epoch [1/2], Step [36550/64305], Loss: 5.2702\n",
      "Epoch [1/2], Step [36560/64305], Loss: 4.9876\n",
      "Epoch [1/2], Step [36570/64305], Loss: 4.8228\n",
      "Epoch [1/2], Step [36580/64305], Loss: 4.9455\n",
      "Epoch [1/2], Step [36590/64305], Loss: 4.8465\n",
      "Epoch [1/2], Step [36600/64305], Loss: 5.1141\n",
      "Epoch [1/2], Step [36610/64305], Loss: 4.9697\n",
      "Epoch [1/2], Step [36620/64305], Loss: 4.9528\n",
      "Epoch [1/2], Step [36630/64305], Loss: 4.9839\n",
      "Epoch [1/2], Step [36640/64305], Loss: 4.9863\n",
      "Epoch [1/2], Step [36650/64305], Loss: 5.0519\n",
      "Epoch [1/2], Step [36660/64305], Loss: 5.0228\n",
      "Epoch [1/2], Step [36670/64305], Loss: 4.9703\n",
      "Epoch [1/2], Step [36680/64305], Loss: 4.7849\n",
      "Epoch [1/2], Step [36690/64305], Loss: 5.0571\n",
      "Epoch [1/2], Step [36700/64305], Loss: 4.9210\n",
      "Epoch [1/2], Step [36710/64305], Loss: 4.9735\n",
      "Epoch [1/2], Step [36720/64305], Loss: 5.0432\n",
      "Epoch [1/2], Step [36730/64305], Loss: 5.1639\n",
      "Epoch [1/2], Step [36740/64305], Loss: 4.8372\n",
      "Epoch [1/2], Step [36750/64305], Loss: 5.1385\n",
      "Epoch [1/2], Step [36760/64305], Loss: 4.7668\n",
      "Epoch [1/2], Step [36770/64305], Loss: 5.1462\n",
      "Epoch [1/2], Step [36780/64305], Loss: 5.1229\n",
      "Epoch [1/2], Step [36790/64305], Loss: 5.1638\n",
      "Epoch [1/2], Step [36800/64305], Loss: 5.0182\n",
      "Epoch [1/2], Step [36810/64305], Loss: 5.1763\n",
      "Epoch [1/2], Step [36820/64305], Loss: 4.8881\n",
      "Epoch [1/2], Step [36830/64305], Loss: 5.0414\n",
      "Epoch [1/2], Step [36840/64305], Loss: 4.8879\n",
      "Epoch [1/2], Step [36850/64305], Loss: 4.7806\n",
      "Epoch [1/2], Step [36860/64305], Loss: 4.8369\n",
      "Epoch [1/2], Step [36870/64305], Loss: 4.9253\n",
      "Epoch [1/2], Step [36880/64305], Loss: 4.9495\n",
      "Epoch [1/2], Step [36890/64305], Loss: 4.7953\n",
      "Epoch [1/2], Step [36900/64305], Loss: 5.0212\n",
      "Epoch [1/2], Step [36910/64305], Loss: 4.9327\n",
      "Epoch [1/2], Step [36920/64305], Loss: 4.8500\n",
      "Epoch [1/2], Step [36930/64305], Loss: 4.9316\n",
      "Epoch [1/2], Step [36940/64305], Loss: 5.0596\n",
      "Epoch [1/2], Step [36950/64305], Loss: 5.1036\n",
      "Epoch [1/2], Step [36960/64305], Loss: 4.8872\n",
      "Epoch [1/2], Step [36970/64305], Loss: 5.1009\n",
      "Epoch [1/2], Step [36980/64305], Loss: 4.9397\n",
      "Epoch [1/2], Step [36990/64305], Loss: 5.0252\n",
      "Epoch [1/2], Step [37000/64305], Loss: 4.8904\n",
      "Epoch [1/2], Step [37010/64305], Loss: 4.8723\n",
      "Epoch [1/2], Step [37020/64305], Loss: 4.9551\n",
      "Epoch [1/2], Step [37030/64305], Loss: 5.1889\n",
      "Epoch [1/2], Step [37040/64305], Loss: 5.0032\n",
      "Epoch [1/2], Step [37050/64305], Loss: 4.9637\n",
      "Epoch [1/2], Step [37060/64305], Loss: 4.8729\n",
      "Epoch [1/2], Step [37070/64305], Loss: 5.2480\n",
      "Epoch [1/2], Step [37080/64305], Loss: 4.8581\n",
      "Epoch [1/2], Step [37090/64305], Loss: 4.9202\n",
      "Epoch [1/2], Step [37100/64305], Loss: 4.9249\n",
      "Epoch [1/2], Step [37110/64305], Loss: 4.9586\n",
      "Epoch [1/2], Step [37120/64305], Loss: 5.0324\n",
      "Epoch [1/2], Step [37130/64305], Loss: 5.1365\n",
      "Epoch [1/2], Step [37140/64305], Loss: 4.7990\n",
      "Epoch [1/2], Step [37150/64305], Loss: 5.1491\n",
      "Epoch [1/2], Step [37160/64305], Loss: 5.0303\n",
      "Epoch [1/2], Step [37170/64305], Loss: 4.9538\n",
      "Epoch [1/2], Step [37180/64305], Loss: 5.0438\n",
      "Epoch [1/2], Step [37190/64305], Loss: 5.2182\n",
      "Epoch [1/2], Step [37200/64305], Loss: 5.1101\n",
      "Epoch [1/2], Step [37210/64305], Loss: 4.9031\n",
      "Epoch [1/2], Step [37220/64305], Loss: 5.0751\n",
      "Epoch [1/2], Step [37230/64305], Loss: 5.0125\n",
      "Epoch [1/2], Step [37240/64305], Loss: 4.8230\n",
      "Epoch [1/2], Step [37250/64305], Loss: 4.9290\n",
      "Epoch [1/2], Step [37260/64305], Loss: 5.0257\n",
      "Epoch [1/2], Step [37270/64305], Loss: 4.9957\n",
      "Epoch [1/2], Step [37280/64305], Loss: 4.8324\n",
      "Epoch [1/2], Step [37290/64305], Loss: 4.9610\n",
      "Epoch [1/2], Step [37300/64305], Loss: 4.9185\n",
      "Epoch [1/2], Step [37310/64305], Loss: 5.1240\n",
      "Epoch [1/2], Step [37320/64305], Loss: 5.0182\n",
      "Epoch [1/2], Step [37330/64305], Loss: 5.0410\n",
      "Epoch [1/2], Step [37340/64305], Loss: 5.0141\n",
      "Epoch [1/2], Step [37350/64305], Loss: 5.0484\n",
      "Epoch [1/2], Step [37360/64305], Loss: 4.9545\n",
      "Epoch [1/2], Step [37370/64305], Loss: 4.9760\n",
      "Epoch [1/2], Step [37380/64305], Loss: 5.0745\n",
      "Epoch [1/2], Step [37390/64305], Loss: 5.2695\n",
      "Epoch [1/2], Step [37400/64305], Loss: 5.1115\n",
      "Epoch [1/2], Step [37410/64305], Loss: 4.9985\n",
      "Epoch [1/2], Step [37420/64305], Loss: 4.9853\n",
      "Epoch [1/2], Step [37430/64305], Loss: 4.9268\n",
      "Epoch [1/2], Step [37440/64305], Loss: 5.0009\n",
      "Epoch [1/2], Step [37450/64305], Loss: 5.1982\n",
      "Epoch [1/2], Step [37460/64305], Loss: 5.0686\n",
      "Epoch [1/2], Step [37470/64305], Loss: 5.0372\n",
      "Epoch [1/2], Step [37480/64305], Loss: 5.1343\n",
      "Epoch [1/2], Step [37490/64305], Loss: 5.0143\n",
      "Epoch [1/2], Step [37500/64305], Loss: 5.0242\n",
      "Epoch [1/2], Step [37510/64305], Loss: 4.9784\n",
      "Epoch [1/2], Step [37520/64305], Loss: 4.9267\n",
      "Epoch [1/2], Step [37530/64305], Loss: 5.2461\n",
      "Epoch [1/2], Step [37540/64305], Loss: 5.2031\n",
      "Epoch [1/2], Step [37550/64305], Loss: 5.1978\n",
      "Epoch [1/2], Step [37560/64305], Loss: 4.8493\n",
      "Epoch [1/2], Step [37570/64305], Loss: 4.8667\n",
      "Epoch [1/2], Step [37580/64305], Loss: 4.9829\n",
      "Epoch [1/2], Step [37590/64305], Loss: 5.0949\n",
      "Epoch [1/2], Step [37600/64305], Loss: 4.7979\n",
      "Epoch [1/2], Step [37610/64305], Loss: 5.0631\n",
      "Epoch [1/2], Step [37620/64305], Loss: 4.9592\n",
      "Epoch [1/2], Step [37630/64305], Loss: 4.9227\n",
      "Epoch [1/2], Step [37640/64305], Loss: 4.8739\n",
      "Epoch [1/2], Step [37650/64305], Loss: 5.1913\n",
      "Epoch [1/2], Step [37660/64305], Loss: 4.9587\n",
      "Epoch [1/2], Step [37670/64305], Loss: 4.9940\n",
      "Epoch [1/2], Step [37680/64305], Loss: 4.9974\n",
      "Epoch [1/2], Step [37690/64305], Loss: 5.0313\n",
      "Epoch [1/2], Step [37700/64305], Loss: 4.7530\n",
      "Epoch [1/2], Step [37710/64305], Loss: 5.0817\n",
      "Epoch [1/2], Step [37720/64305], Loss: 5.0056\n",
      "Epoch [1/2], Step [37730/64305], Loss: 5.1689\n",
      "Epoch [1/2], Step [37740/64305], Loss: 4.9622\n",
      "Epoch [1/2], Step [37750/64305], Loss: 4.8940\n",
      "Epoch [1/2], Step [37760/64305], Loss: 5.1993\n",
      "Epoch [1/2], Step [37770/64305], Loss: 4.9120\n",
      "Epoch [1/2], Step [37780/64305], Loss: 4.8232\n",
      "Epoch [1/2], Step [37790/64305], Loss: 5.1296\n",
      "Epoch [1/2], Step [37800/64305], Loss: 4.9669\n",
      "Epoch [1/2], Step [37810/64305], Loss: 4.7517\n",
      "Epoch [1/2], Step [37820/64305], Loss: 4.9457\n",
      "Epoch [1/2], Step [37830/64305], Loss: 5.0081\n",
      "Epoch [1/2], Step [37840/64305], Loss: 4.7973\n",
      "Epoch [1/2], Step [37850/64305], Loss: 4.9682\n",
      "Epoch [1/2], Step [37860/64305], Loss: 5.0148\n",
      "Epoch [1/2], Step [37870/64305], Loss: 5.0902\n",
      "Epoch [1/2], Step [37880/64305], Loss: 4.8582\n",
      "Epoch [1/2], Step [37890/64305], Loss: 4.9795\n",
      "Epoch [1/2], Step [37900/64305], Loss: 4.8733\n",
      "Epoch [1/2], Step [37910/64305], Loss: 5.0242\n",
      "Epoch [1/2], Step [37920/64305], Loss: 4.9400\n",
      "Epoch [1/2], Step [37930/64305], Loss: 4.8809\n",
      "Epoch [1/2], Step [37940/64305], Loss: 5.1152\n",
      "Epoch [1/2], Step [37950/64305], Loss: 4.9709\n",
      "Epoch [1/2], Step [37960/64305], Loss: 4.8887\n",
      "Epoch [1/2], Step [37970/64305], Loss: 5.1111\n",
      "Epoch [1/2], Step [37980/64305], Loss: 5.0572\n",
      "Epoch [1/2], Step [37990/64305], Loss: 4.9361\n",
      "Epoch [1/2], Step [38000/64305], Loss: 4.9508\n",
      "Epoch [1/2], Step [38010/64305], Loss: 4.8216\n",
      "Epoch [1/2], Step [38020/64305], Loss: 5.0387\n",
      "Epoch [1/2], Step [38030/64305], Loss: 4.7819\n",
      "Epoch [1/2], Step [38040/64305], Loss: 4.9032\n",
      "Epoch [1/2], Step [38050/64305], Loss: 4.9472\n",
      "Epoch [1/2], Step [38060/64305], Loss: 5.0332\n",
      "Epoch [1/2], Step [38070/64305], Loss: 4.8187\n",
      "Epoch [1/2], Step [38080/64305], Loss: 5.0956\n",
      "Epoch [1/2], Step [38090/64305], Loss: 4.9319\n",
      "Epoch [1/2], Step [38100/64305], Loss: 4.8642\n",
      "Epoch [1/2], Step [38110/64305], Loss: 5.2661\n",
      "Epoch [1/2], Step [38120/64305], Loss: 4.8564\n",
      "Epoch [1/2], Step [38130/64305], Loss: 5.0696\n",
      "Epoch [1/2], Step [38140/64305], Loss: 4.9804\n",
      "Epoch [1/2], Step [38150/64305], Loss: 4.9789\n",
      "Epoch [1/2], Step [38160/64305], Loss: 5.1627\n",
      "Epoch [1/2], Step [38170/64305], Loss: 4.7898\n",
      "Epoch [1/2], Step [38180/64305], Loss: 5.0913\n",
      "Epoch [1/2], Step [38190/64305], Loss: 5.0443\n",
      "Epoch [1/2], Step [38200/64305], Loss: 4.8726\n",
      "Epoch [1/2], Step [38210/64305], Loss: 5.0637\n",
      "Epoch [1/2], Step [38220/64305], Loss: 4.8615\n",
      "Epoch [1/2], Step [38230/64305], Loss: 5.0340\n",
      "Epoch [1/2], Step [38240/64305], Loss: 4.9569\n",
      "Epoch [1/2], Step [38250/64305], Loss: 4.6998\n",
      "Epoch [1/2], Step [38260/64305], Loss: 5.0063\n",
      "Epoch [1/2], Step [38270/64305], Loss: 5.2031\n",
      "Epoch [1/2], Step [38280/64305], Loss: 5.0405\n",
      "Epoch [1/2], Step [38290/64305], Loss: 5.1325\n",
      "Epoch [1/2], Step [38300/64305], Loss: 5.0314\n",
      "Epoch [1/2], Step [38310/64305], Loss: 5.0367\n",
      "Epoch [1/2], Step [38320/64305], Loss: 4.8247\n",
      "Epoch [1/2], Step [38330/64305], Loss: 4.9616\n",
      "Epoch [1/2], Step [38340/64305], Loss: 4.9104\n",
      "Epoch [1/2], Step [38350/64305], Loss: 4.7187\n",
      "Epoch [1/2], Step [38360/64305], Loss: 5.2771\n",
      "Epoch [1/2], Step [38370/64305], Loss: 5.1280\n",
      "Epoch [1/2], Step [38380/64305], Loss: 5.1654\n",
      "Epoch [1/2], Step [38390/64305], Loss: 4.8323\n",
      "Epoch [1/2], Step [38400/64305], Loss: 4.8480\n",
      "Epoch [1/2], Step [38410/64305], Loss: 5.0495\n",
      "Epoch [1/2], Step [38420/64305], Loss: 4.8953\n",
      "Epoch [1/2], Step [38430/64305], Loss: 4.9562\n",
      "Epoch [1/2], Step [38440/64305], Loss: 4.8939\n",
      "Epoch [1/2], Step [38450/64305], Loss: 5.2640\n",
      "Epoch [1/2], Step [38460/64305], Loss: 4.9968\n",
      "Epoch [1/2], Step [38470/64305], Loss: 5.0616\n",
      "Epoch [1/2], Step [38480/64305], Loss: 5.2459\n",
      "Epoch [1/2], Step [38490/64305], Loss: 5.1120\n",
      "Epoch [1/2], Step [38500/64305], Loss: 5.0175\n",
      "Epoch [1/2], Step [38510/64305], Loss: 4.7692\n",
      "Epoch [1/2], Step [38520/64305], Loss: 4.8491\n",
      "Epoch [1/2], Step [38530/64305], Loss: 5.0534\n",
      "Epoch [1/2], Step [38540/64305], Loss: 4.9450\n",
      "Epoch [1/2], Step [38550/64305], Loss: 5.1827\n",
      "Epoch [1/2], Step [38560/64305], Loss: 5.0809\n",
      "Epoch [1/2], Step [38570/64305], Loss: 4.8253\n",
      "Epoch [1/2], Step [38580/64305], Loss: 4.9739\n",
      "Epoch [1/2], Step [38590/64305], Loss: 5.0522\n",
      "Epoch [1/2], Step [38600/64305], Loss: 4.8579\n",
      "Epoch [1/2], Step [38610/64305], Loss: 5.0072\n",
      "Epoch [1/2], Step [38620/64305], Loss: 5.0755\n",
      "Epoch [1/2], Step [38630/64305], Loss: 5.2588\n",
      "Epoch [1/2], Step [38640/64305], Loss: 4.9626\n",
      "Epoch [1/2], Step [38650/64305], Loss: 5.1298\n",
      "Epoch [1/2], Step [38660/64305], Loss: 5.0683\n",
      "Epoch [1/2], Step [38670/64305], Loss: 5.1160\n",
      "Epoch [1/2], Step [38680/64305], Loss: 5.1341\n",
      "Epoch [1/2], Step [38690/64305], Loss: 4.9742\n",
      "Epoch [1/2], Step [38700/64305], Loss: 5.0015\n",
      "Epoch [1/2], Step [38710/64305], Loss: 5.0381\n",
      "Epoch [1/2], Step [38720/64305], Loss: 4.8704\n",
      "Epoch [1/2], Step [38730/64305], Loss: 4.8425\n",
      "Epoch [1/2], Step [38740/64305], Loss: 5.0501\n",
      "Epoch [1/2], Step [38750/64305], Loss: 4.9768\n",
      "Epoch [1/2], Step [38760/64305], Loss: 5.0666\n",
      "Epoch [1/2], Step [38770/64305], Loss: 4.8378\n",
      "Epoch [1/2], Step [38780/64305], Loss: 4.9835\n",
      "Epoch [1/2], Step [38790/64305], Loss: 5.0809\n",
      "Epoch [1/2], Step [38800/64305], Loss: 4.9920\n",
      "Epoch [1/2], Step [38810/64305], Loss: 5.1133\n",
      "Epoch [1/2], Step [38820/64305], Loss: 5.2141\n",
      "Epoch [1/2], Step [38830/64305], Loss: 5.0650\n",
      "Epoch [1/2], Step [38840/64305], Loss: 5.1079\n",
      "Epoch [1/2], Step [38850/64305], Loss: 4.6856\n",
      "Epoch [1/2], Step [38860/64305], Loss: 4.9315\n",
      "Epoch [1/2], Step [38870/64305], Loss: 4.9353\n",
      "Epoch [1/2], Step [38880/64305], Loss: 4.9279\n",
      "Epoch [1/2], Step [38890/64305], Loss: 5.0207\n",
      "Epoch [1/2], Step [38900/64305], Loss: 5.0682\n",
      "Epoch [1/2], Step [38910/64305], Loss: 4.9709\n",
      "Epoch [1/2], Step [38920/64305], Loss: 5.0464\n",
      "Epoch [1/2], Step [38930/64305], Loss: 4.7608\n",
      "Epoch [1/2], Step [38940/64305], Loss: 5.1221\n",
      "Epoch [1/2], Step [38950/64305], Loss: 4.9234\n",
      "Epoch [1/2], Step [38960/64305], Loss: 5.0623\n",
      "Epoch [1/2], Step [38970/64305], Loss: 4.9174\n",
      "Epoch [1/2], Step [38980/64305], Loss: 4.8703\n",
      "Epoch [1/2], Step [38990/64305], Loss: 5.0872\n",
      "Epoch [1/2], Step [39000/64305], Loss: 4.9297\n",
      "Epoch [1/2], Step [39010/64305], Loss: 4.9176\n",
      "Epoch [1/2], Step [39020/64305], Loss: 4.9454\n",
      "Epoch [1/2], Step [39030/64305], Loss: 5.4407\n",
      "Epoch [1/2], Step [39040/64305], Loss: 4.9411\n",
      "Epoch [1/2], Step [39050/64305], Loss: 5.2460\n",
      "Epoch [1/2], Step [39060/64305], Loss: 5.0820\n",
      "Epoch [1/2], Step [39070/64305], Loss: 5.1608\n",
      "Epoch [1/2], Step [39080/64305], Loss: 4.9905\n",
      "Epoch [1/2], Step [39090/64305], Loss: 4.8766\n",
      "Epoch [1/2], Step [39100/64305], Loss: 4.9083\n",
      "Epoch [1/2], Step [39110/64305], Loss: 5.1455\n",
      "Epoch [1/2], Step [39120/64305], Loss: 5.1159\n",
      "Epoch [1/2], Step [39130/64305], Loss: 4.9755\n",
      "Epoch [1/2], Step [39140/64305], Loss: 5.0479\n",
      "Epoch [1/2], Step [39150/64305], Loss: 5.0999\n",
      "Epoch [1/2], Step [39160/64305], Loss: 5.0799\n",
      "Epoch [1/2], Step [39170/64305], Loss: 5.0311\n",
      "Epoch [1/2], Step [39180/64305], Loss: 5.2390\n",
      "Epoch [1/2], Step [39190/64305], Loss: 4.8435\n",
      "Epoch [1/2], Step [39200/64305], Loss: 5.0341\n",
      "Epoch [1/2], Step [39210/64305], Loss: 4.7985\n",
      "Epoch [1/2], Step [39220/64305], Loss: 4.8575\n",
      "Epoch [1/2], Step [39230/64305], Loss: 4.9537\n",
      "Epoch [1/2], Step [39240/64305], Loss: 4.8833\n",
      "Epoch [1/2], Step [39250/64305], Loss: 5.0809\n",
      "Epoch [1/2], Step [39260/64305], Loss: 4.8846\n",
      "Epoch [1/2], Step [39270/64305], Loss: 5.2133\n",
      "Epoch [1/2], Step [39280/64305], Loss: 5.1778\n",
      "Epoch [1/2], Step [39290/64305], Loss: 4.8996\n",
      "Epoch [1/2], Step [39300/64305], Loss: 4.9641\n",
      "Epoch [1/2], Step [39310/64305], Loss: 4.9546\n",
      "Epoch [1/2], Step [39320/64305], Loss: 5.0179\n",
      "Epoch [1/2], Step [39330/64305], Loss: 5.0864\n",
      "Epoch [1/2], Step [39340/64305], Loss: 5.0000\n",
      "Epoch [1/2], Step [39350/64305], Loss: 5.0178\n",
      "Epoch [1/2], Step [39360/64305], Loss: 5.0844\n",
      "Epoch [1/2], Step [39370/64305], Loss: 4.9668\n",
      "Epoch [1/2], Step [39380/64305], Loss: 5.0420\n",
      "Epoch [1/2], Step [39390/64305], Loss: 5.0691\n",
      "Epoch [1/2], Step [39400/64305], Loss: 4.8574\n",
      "Epoch [1/2], Step [39410/64305], Loss: 5.0041\n",
      "Epoch [1/2], Step [39420/64305], Loss: 5.1615\n",
      "Epoch [1/2], Step [39430/64305], Loss: 5.2095\n",
      "Epoch [1/2], Step [39440/64305], Loss: 4.9785\n",
      "Epoch [1/2], Step [39450/64305], Loss: 5.1958\n",
      "Epoch [1/2], Step [39460/64305], Loss: 4.9456\n",
      "Epoch [1/2], Step [39470/64305], Loss: 5.0461\n",
      "Epoch [1/2], Step [39480/64305], Loss: 4.8940\n",
      "Epoch [1/2], Step [39490/64305], Loss: 4.8437\n",
      "Epoch [1/2], Step [39500/64305], Loss: 4.9630\n",
      "Epoch [1/2], Step [39510/64305], Loss: 5.0028\n",
      "Epoch [1/2], Step [39520/64305], Loss: 4.9034\n",
      "Epoch [1/2], Step [39530/64305], Loss: 5.0010\n",
      "Epoch [1/2], Step [39540/64305], Loss: 4.9640\n",
      "Epoch [1/2], Step [39550/64305], Loss: 4.8931\n",
      "Epoch [1/2], Step [39560/64305], Loss: 4.7738\n",
      "Epoch [1/2], Step [39570/64305], Loss: 5.0035\n",
      "Epoch [1/2], Step [39580/64305], Loss: 5.0797\n",
      "Epoch [1/2], Step [39590/64305], Loss: 4.9394\n",
      "Epoch [1/2], Step [39600/64305], Loss: 5.0729\n",
      "Epoch [1/2], Step [39610/64305], Loss: 5.0492\n",
      "Epoch [1/2], Step [39620/64305], Loss: 4.9998\n",
      "Epoch [1/2], Step [39630/64305], Loss: 4.9679\n",
      "Epoch [1/2], Step [39640/64305], Loss: 4.7753\n",
      "Epoch [1/2], Step [39650/64305], Loss: 5.2032\n",
      "Epoch [1/2], Step [39660/64305], Loss: 4.9485\n",
      "Epoch [1/2], Step [39670/64305], Loss: 4.9765\n",
      "Epoch [1/2], Step [39680/64305], Loss: 4.8853\n",
      "Epoch [1/2], Step [39690/64305], Loss: 4.8387\n",
      "Epoch [1/2], Step [39700/64305], Loss: 5.0746\n",
      "Epoch [1/2], Step [39710/64305], Loss: 4.8337\n",
      "Epoch [1/2], Step [39720/64305], Loss: 4.8967\n",
      "Epoch [1/2], Step [39730/64305], Loss: 5.1143\n",
      "Epoch [1/2], Step [39740/64305], Loss: 4.9820\n",
      "Epoch [1/2], Step [39750/64305], Loss: 5.1427\n",
      "Epoch [1/2], Step [39760/64305], Loss: 5.2307\n",
      "Epoch [1/2], Step [39770/64305], Loss: 4.9977\n",
      "Epoch [1/2], Step [39780/64305], Loss: 5.0679\n",
      "Epoch [1/2], Step [39790/64305], Loss: 4.9757\n",
      "Epoch [1/2], Step [39800/64305], Loss: 5.0447\n",
      "Epoch [1/2], Step [39810/64305], Loss: 5.0036\n",
      "Epoch [1/2], Step [39820/64305], Loss: 4.8775\n",
      "Epoch [1/2], Step [39830/64305], Loss: 4.9601\n",
      "Epoch [1/2], Step [39840/64305], Loss: 5.1740\n",
      "Epoch [1/2], Step [39850/64305], Loss: 4.9266\n",
      "Epoch [1/2], Step [39860/64305], Loss: 5.1431\n",
      "Epoch [1/2], Step [39870/64305], Loss: 5.0237\n",
      "Epoch [1/2], Step [39880/64305], Loss: 5.0091\n",
      "Epoch [1/2], Step [39890/64305], Loss: 4.7256\n",
      "Epoch [1/2], Step [39900/64305], Loss: 5.0002\n",
      "Epoch [1/2], Step [39910/64305], Loss: 4.8762\n",
      "Epoch [1/2], Step [39920/64305], Loss: 5.0235\n",
      "Epoch [1/2], Step [39930/64305], Loss: 4.8889\n",
      "Epoch [1/2], Step [39940/64305], Loss: 5.0086\n",
      "Epoch [1/2], Step [39950/64305], Loss: 4.8646\n",
      "Epoch [1/2], Step [39960/64305], Loss: 5.1389\n",
      "Epoch [1/2], Step [39970/64305], Loss: 5.0475\n",
      "Epoch [1/2], Step [39980/64305], Loss: 5.0623\n",
      "Epoch [1/2], Step [39990/64305], Loss: 4.7542\n",
      "Epoch [1/2], Step [40000/64305], Loss: 5.0027\n",
      "Epoch [1/2], Step [40010/64305], Loss: 5.0442\n",
      "Epoch [1/2], Step [40020/64305], Loss: 4.7191\n",
      "Epoch [1/2], Step [40030/64305], Loss: 4.9778\n",
      "Epoch [1/2], Step [40040/64305], Loss: 5.0601\n",
      "Epoch [1/2], Step [40050/64305], Loss: 4.8739\n",
      "Epoch [1/2], Step [40060/64305], Loss: 4.8846\n",
      "Epoch [1/2], Step [40070/64305], Loss: 4.9043\n",
      "Epoch [1/2], Step [40080/64305], Loss: 4.9509\n",
      "Epoch [1/2], Step [40090/64305], Loss: 5.1246\n",
      "Epoch [1/2], Step [40100/64305], Loss: 4.9450\n",
      "Epoch [1/2], Step [40110/64305], Loss: 5.1583\n",
      "Epoch [1/2], Step [40120/64305], Loss: 5.0583\n",
      "Epoch [1/2], Step [40130/64305], Loss: 5.2456\n",
      "Epoch [1/2], Step [40140/64305], Loss: 4.9749\n",
      "Epoch [1/2], Step [40150/64305], Loss: 5.0813\n",
      "Epoch [1/2], Step [40160/64305], Loss: 5.0449\n",
      "Epoch [1/2], Step [40170/64305], Loss: 4.9059\n",
      "Epoch [1/2], Step [40180/64305], Loss: 4.8685\n",
      "Epoch [1/2], Step [40190/64305], Loss: 5.0248\n",
      "Epoch [1/2], Step [40200/64305], Loss: 4.9793\n",
      "Epoch [1/2], Step [40210/64305], Loss: 4.8773\n",
      "Epoch [1/2], Step [40220/64305], Loss: 5.2043\n",
      "Epoch [1/2], Step [40230/64305], Loss: 4.8541\n",
      "Epoch [1/2], Step [40240/64305], Loss: 4.8798\n",
      "Epoch [1/2], Step [40250/64305], Loss: 4.9282\n",
      "Epoch [1/2], Step [40260/64305], Loss: 4.9168\n",
      "Epoch [1/2], Step [40270/64305], Loss: 5.0283\n",
      "Epoch [1/2], Step [40280/64305], Loss: 4.9736\n",
      "Epoch [1/2], Step [40290/64305], Loss: 4.8823\n",
      "Epoch [1/2], Step [40300/64305], Loss: 4.8408\n",
      "Epoch [1/2], Step [40310/64305], Loss: 4.8234\n",
      "Epoch [1/2], Step [40320/64305], Loss: 4.9741\n",
      "Epoch [1/2], Step [40330/64305], Loss: 4.8171\n",
      "Epoch [1/2], Step [40340/64305], Loss: 4.8997\n",
      "Epoch [1/2], Step [40350/64305], Loss: 5.1205\n",
      "Epoch [1/2], Step [40360/64305], Loss: 5.0700\n",
      "Epoch [1/2], Step [40370/64305], Loss: 5.0367\n",
      "Epoch [1/2], Step [40380/64305], Loss: 4.8440\n",
      "Epoch [1/2], Step [40390/64305], Loss: 4.8858\n",
      "Epoch [1/2], Step [40400/64305], Loss: 5.0631\n",
      "Epoch [1/2], Step [40410/64305], Loss: 4.9843\n",
      "Epoch [1/2], Step [40420/64305], Loss: 4.8184\n",
      "Epoch [1/2], Step [40430/64305], Loss: 4.9994\n",
      "Epoch [1/2], Step [40440/64305], Loss: 4.9422\n",
      "Epoch [1/2], Step [40450/64305], Loss: 5.0208\n",
      "Epoch [1/2], Step [40460/64305], Loss: 4.9879\n",
      "Epoch [1/2], Step [40470/64305], Loss: 5.2379\n",
      "Epoch [1/2], Step [40480/64305], Loss: 5.0214\n",
      "Epoch [1/2], Step [40490/64305], Loss: 5.2260\n",
      "Epoch [1/2], Step [40500/64305], Loss: 4.9744\n",
      "Epoch [1/2], Step [40510/64305], Loss: 4.9538\n",
      "Epoch [1/2], Step [40520/64305], Loss: 4.9387\n",
      "Epoch [1/2], Step [40530/64305], Loss: 4.8616\n",
      "Epoch [1/2], Step [40540/64305], Loss: 5.0877\n",
      "Epoch [1/2], Step [40550/64305], Loss: 5.0317\n",
      "Epoch [1/2], Step [40560/64305], Loss: 5.0924\n",
      "Epoch [1/2], Step [40570/64305], Loss: 4.7867\n",
      "Epoch [1/2], Step [40580/64305], Loss: 5.0498\n",
      "Epoch [1/2], Step [40590/64305], Loss: 4.9801\n",
      "Epoch [1/2], Step [40600/64305], Loss: 4.9425\n",
      "Epoch [1/2], Step [40610/64305], Loss: 4.7977\n",
      "Epoch [1/2], Step [40620/64305], Loss: 4.9442\n",
      "Epoch [1/2], Step [40630/64305], Loss: 5.0554\n",
      "Epoch [1/2], Step [40640/64305], Loss: 4.9658\n",
      "Epoch [1/2], Step [40650/64305], Loss: 4.9049\n",
      "Epoch [1/2], Step [40660/64305], Loss: 4.9694\n",
      "Epoch [1/2], Step [40670/64305], Loss: 4.9843\n",
      "Epoch [1/2], Step [40680/64305], Loss: 4.8559\n",
      "Epoch [1/2], Step [40690/64305], Loss: 5.1027\n",
      "Epoch [1/2], Step [40700/64305], Loss: 5.1508\n",
      "Epoch [1/2], Step [40710/64305], Loss: 4.9877\n",
      "Epoch [1/2], Step [40720/64305], Loss: 4.7947\n",
      "Epoch [1/2], Step [40730/64305], Loss: 4.7850\n",
      "Epoch [1/2], Step [40740/64305], Loss: 5.1072\n",
      "Epoch [1/2], Step [40750/64305], Loss: 4.9293\n",
      "Epoch [1/2], Step [40760/64305], Loss: 5.2389\n",
      "Epoch [1/2], Step [40770/64305], Loss: 5.1349\n",
      "Epoch [1/2], Step [40780/64305], Loss: 4.9054\n",
      "Epoch [1/2], Step [40790/64305], Loss: 4.8682\n",
      "Epoch [1/2], Step [40800/64305], Loss: 4.7919\n",
      "Epoch [1/2], Step [40810/64305], Loss: 4.8080\n",
      "Epoch [1/2], Step [40820/64305], Loss: 4.9237\n",
      "Epoch [1/2], Step [40830/64305], Loss: 5.0445\n",
      "Epoch [1/2], Step [40840/64305], Loss: 4.8076\n",
      "Epoch [1/2], Step [40850/64305], Loss: 5.0245\n",
      "Epoch [1/2], Step [40860/64305], Loss: 4.9059\n",
      "Epoch [1/2], Step [40870/64305], Loss: 4.9185\n",
      "Epoch [1/2], Step [40880/64305], Loss: 4.9372\n",
      "Epoch [1/2], Step [40890/64305], Loss: 4.9417\n",
      "Epoch [1/2], Step [40900/64305], Loss: 5.1746\n",
      "Epoch [1/2], Step [40910/64305], Loss: 4.9018\n",
      "Epoch [1/2], Step [40920/64305], Loss: 5.1800\n",
      "Epoch [1/2], Step [40930/64305], Loss: 4.7559\n",
      "Epoch [1/2], Step [40940/64305], Loss: 4.9408\n",
      "Epoch [1/2], Step [40950/64305], Loss: 5.0327\n",
      "Epoch [1/2], Step [40960/64305], Loss: 4.9407\n",
      "Epoch [1/2], Step [40970/64305], Loss: 5.1243\n",
      "Epoch [1/2], Step [40980/64305], Loss: 5.0191\n",
      "Epoch [1/2], Step [40990/64305], Loss: 4.8607\n",
      "Epoch [1/2], Step [41000/64305], Loss: 5.1567\n",
      "Epoch [1/2], Step [41010/64305], Loss: 5.0550\n",
      "Epoch [1/2], Step [41020/64305], Loss: 4.8382\n",
      "Epoch [1/2], Step [41030/64305], Loss: 4.9368\n",
      "Epoch [1/2], Step [41040/64305], Loss: 4.8642\n",
      "Epoch [1/2], Step [41050/64305], Loss: 5.0084\n",
      "Epoch [1/2], Step [41060/64305], Loss: 5.0060\n",
      "Epoch [1/2], Step [41070/64305], Loss: 5.0153\n",
      "Epoch [1/2], Step [41080/64305], Loss: 5.0336\n",
      "Epoch [1/2], Step [41090/64305], Loss: 4.7724\n",
      "Epoch [1/2], Step [41100/64305], Loss: 4.8428\n",
      "Epoch [1/2], Step [41110/64305], Loss: 5.0478\n",
      "Epoch [1/2], Step [41120/64305], Loss: 4.9336\n",
      "Epoch [1/2], Step [41130/64305], Loss: 5.1106\n",
      "Epoch [1/2], Step [41140/64305], Loss: 4.7107\n",
      "Epoch [1/2], Step [41150/64305], Loss: 4.8364\n",
      "Epoch [1/2], Step [41160/64305], Loss: 5.0241\n",
      "Epoch [1/2], Step [41170/64305], Loss: 4.9968\n",
      "Epoch [1/2], Step [41180/64305], Loss: 5.0922\n",
      "Epoch [1/2], Step [41190/64305], Loss: 5.0458\n",
      "Epoch [1/2], Step [41200/64305], Loss: 4.8549\n",
      "Epoch [1/2], Step [41210/64305], Loss: 5.1514\n",
      "Epoch [1/2], Step [41220/64305], Loss: 4.8280\n",
      "Epoch [1/2], Step [41230/64305], Loss: 4.8858\n",
      "Epoch [1/2], Step [41240/64305], Loss: 4.6824\n",
      "Epoch [1/2], Step [41250/64305], Loss: 5.0174\n",
      "Epoch [1/2], Step [41260/64305], Loss: 4.8889\n",
      "Epoch [1/2], Step [41270/64305], Loss: 5.1159\n",
      "Epoch [1/2], Step [41280/64305], Loss: 4.7514\n",
      "Epoch [1/2], Step [41290/64305], Loss: 5.0891\n",
      "Epoch [1/2], Step [41300/64305], Loss: 4.9080\n",
      "Epoch [1/2], Step [41310/64305], Loss: 4.9645\n",
      "Epoch [1/2], Step [41320/64305], Loss: 4.8996\n",
      "Epoch [1/2], Step [41330/64305], Loss: 4.9280\n",
      "Epoch [1/2], Step [41340/64305], Loss: 4.9420\n",
      "Epoch [1/2], Step [41350/64305], Loss: 4.8256\n",
      "Epoch [1/2], Step [41360/64305], Loss: 4.8729\n",
      "Epoch [1/2], Step [41370/64305], Loss: 4.9577\n",
      "Epoch [1/2], Step [41380/64305], Loss: 5.0235\n",
      "Epoch [1/2], Step [41390/64305], Loss: 5.0321\n",
      "Epoch [1/2], Step [41400/64305], Loss: 4.9347\n",
      "Epoch [1/2], Step [41410/64305], Loss: 4.8976\n",
      "Epoch [1/2], Step [41420/64305], Loss: 5.0551\n",
      "Epoch [1/2], Step [41430/64305], Loss: 4.9584\n",
      "Epoch [1/2], Step [41440/64305], Loss: 5.1247\n",
      "Epoch [1/2], Step [41450/64305], Loss: 5.0343\n",
      "Epoch [1/2], Step [41460/64305], Loss: 5.0984\n",
      "Epoch [1/2], Step [41470/64305], Loss: 4.8436\n",
      "Epoch [1/2], Step [41480/64305], Loss: 4.7481\n",
      "Epoch [1/2], Step [41490/64305], Loss: 4.9216\n",
      "Epoch [1/2], Step [41500/64305], Loss: 4.9312\n",
      "Epoch [1/2], Step [41510/64305], Loss: 4.9165\n",
      "Epoch [1/2], Step [41520/64305], Loss: 4.8122\n",
      "Epoch [1/2], Step [41530/64305], Loss: 4.8670\n",
      "Epoch [1/2], Step [41540/64305], Loss: 5.2218\n",
      "Epoch [1/2], Step [41550/64305], Loss: 4.9879\n",
      "Epoch [1/2], Step [41560/64305], Loss: 4.8635\n",
      "Epoch [1/2], Step [41570/64305], Loss: 4.9737\n",
      "Epoch [1/2], Step [41580/64305], Loss: 4.7954\n",
      "Epoch [1/2], Step [41590/64305], Loss: 5.0563\n",
      "Epoch [1/2], Step [41600/64305], Loss: 5.0294\n",
      "Epoch [1/2], Step [41610/64305], Loss: 5.0013\n",
      "Epoch [1/2], Step [41620/64305], Loss: 5.0638\n",
      "Epoch [1/2], Step [41630/64305], Loss: 4.9130\n",
      "Epoch [1/2], Step [41640/64305], Loss: 4.8049\n",
      "Epoch [1/2], Step [41650/64305], Loss: 5.0218\n",
      "Epoch [1/2], Step [41660/64305], Loss: 4.9927\n",
      "Epoch [1/2], Step [41670/64305], Loss: 5.1953\n",
      "Epoch [1/2], Step [41680/64305], Loss: 5.0109\n",
      "Epoch [1/2], Step [41690/64305], Loss: 4.8670\n",
      "Epoch [1/2], Step [41700/64305], Loss: 4.8919\n",
      "Epoch [1/2], Step [41710/64305], Loss: 4.9001\n",
      "Epoch [1/2], Step [41720/64305], Loss: 4.8651\n",
      "Epoch [1/2], Step [41730/64305], Loss: 5.2243\n",
      "Epoch [1/2], Step [41740/64305], Loss: 4.9969\n",
      "Epoch [1/2], Step [41750/64305], Loss: 5.0416\n",
      "Epoch [1/2], Step [41760/64305], Loss: 5.0653\n",
      "Epoch [1/2], Step [41770/64305], Loss: 4.9788\n",
      "Epoch [1/2], Step [41780/64305], Loss: 5.0277\n",
      "Epoch [1/2], Step [41790/64305], Loss: 4.9986\n",
      "Epoch [1/2], Step [41800/64305], Loss: 4.6579\n",
      "Epoch [1/2], Step [41810/64305], Loss: 5.1625\n",
      "Epoch [1/2], Step [41820/64305], Loss: 4.9913\n",
      "Epoch [1/2], Step [41830/64305], Loss: 5.2421\n",
      "Epoch [1/2], Step [41840/64305], Loss: 5.0549\n",
      "Epoch [1/2], Step [41850/64305], Loss: 4.9393\n",
      "Epoch [1/2], Step [41860/64305], Loss: 5.0630\n",
      "Epoch [1/2], Step [41870/64305], Loss: 4.7981\n",
      "Epoch [1/2], Step [41880/64305], Loss: 4.9541\n",
      "Epoch [1/2], Step [41890/64305], Loss: 4.9605\n",
      "Epoch [1/2], Step [41900/64305], Loss: 4.8963\n",
      "Epoch [1/2], Step [41910/64305], Loss: 4.8461\n",
      "Epoch [1/2], Step [41920/64305], Loss: 4.8642\n",
      "Epoch [1/2], Step [41930/64305], Loss: 4.8687\n",
      "Epoch [1/2], Step [41940/64305], Loss: 5.0646\n",
      "Epoch [1/2], Step [41950/64305], Loss: 5.0136\n",
      "Epoch [1/2], Step [41960/64305], Loss: 5.0307\n",
      "Epoch [1/2], Step [41970/64305], Loss: 5.1161\n",
      "Epoch [1/2], Step [41980/64305], Loss: 5.0031\n",
      "Epoch [1/2], Step [41990/64305], Loss: 4.9160\n",
      "Epoch [1/2], Step [42000/64305], Loss: 4.9691\n",
      "Epoch [1/2], Step [42010/64305], Loss: 4.9289\n",
      "Epoch [1/2], Step [42020/64305], Loss: 5.1366\n",
      "Epoch [1/2], Step [42030/64305], Loss: 5.0186\n",
      "Epoch [1/2], Step [42040/64305], Loss: 4.9749\n",
      "Epoch [1/2], Step [42050/64305], Loss: 4.8926\n",
      "Epoch [1/2], Step [42060/64305], Loss: 4.8434\n",
      "Epoch [1/2], Step [42070/64305], Loss: 5.0091\n",
      "Epoch [1/2], Step [42080/64305], Loss: 4.9941\n",
      "Epoch [1/2], Step [42090/64305], Loss: 5.1779\n",
      "Epoch [1/2], Step [42100/64305], Loss: 4.9194\n",
      "Epoch [1/2], Step [42110/64305], Loss: 4.9876\n",
      "Epoch [1/2], Step [42120/64305], Loss: 5.0840\n",
      "Epoch [1/2], Step [42130/64305], Loss: 4.9333\n",
      "Epoch [1/2], Step [42140/64305], Loss: 4.9424\n",
      "Epoch [1/2], Step [42150/64305], Loss: 5.0312\n",
      "Epoch [1/2], Step [42160/64305], Loss: 5.0164\n",
      "Epoch [1/2], Step [42170/64305], Loss: 4.9594\n",
      "Epoch [1/2], Step [42180/64305], Loss: 4.8452\n",
      "Epoch [1/2], Step [42190/64305], Loss: 4.9550\n",
      "Epoch [1/2], Step [42200/64305], Loss: 4.9111\n",
      "Epoch [1/2], Step [42210/64305], Loss: 4.9027\n",
      "Epoch [1/2], Step [42220/64305], Loss: 4.9968\n",
      "Epoch [1/2], Step [42230/64305], Loss: 5.0350\n",
      "Epoch [1/2], Step [42240/64305], Loss: 5.0223\n",
      "Epoch [1/2], Step [42250/64305], Loss: 4.7734\n",
      "Epoch [1/2], Step [42260/64305], Loss: 5.0229\n",
      "Epoch [1/2], Step [42270/64305], Loss: 4.9179\n",
      "Epoch [1/2], Step [42280/64305], Loss: 5.1294\n",
      "Epoch [1/2], Step [42290/64305], Loss: 5.0741\n",
      "Epoch [1/2], Step [42300/64305], Loss: 5.1285\n",
      "Epoch [1/2], Step [42310/64305], Loss: 4.7954\n",
      "Epoch [1/2], Step [42320/64305], Loss: 4.8373\n",
      "Epoch [1/2], Step [42330/64305], Loss: 4.9356\n",
      "Epoch [1/2], Step [42340/64305], Loss: 5.1178\n",
      "Epoch [1/2], Step [42350/64305], Loss: 4.9357\n",
      "Epoch [1/2], Step [42360/64305], Loss: 4.8970\n",
      "Epoch [1/2], Step [42370/64305], Loss: 4.8520\n",
      "Epoch [1/2], Step [42380/64305], Loss: 5.1461\n",
      "Epoch [1/2], Step [42390/64305], Loss: 5.1773\n",
      "Epoch [1/2], Step [42400/64305], Loss: 4.9887\n",
      "Epoch [1/2], Step [42410/64305], Loss: 4.9718\n",
      "Epoch [1/2], Step [42420/64305], Loss: 5.0027\n",
      "Epoch [1/2], Step [42430/64305], Loss: 4.8492\n",
      "Epoch [1/2], Step [42440/64305], Loss: 5.0991\n",
      "Epoch [1/2], Step [42450/64305], Loss: 4.9500\n",
      "Epoch [1/2], Step [42460/64305], Loss: 5.1310\n",
      "Epoch [1/2], Step [42470/64305], Loss: 5.1654\n",
      "Epoch [1/2], Step [42480/64305], Loss: 4.8227\n",
      "Epoch [1/2], Step [42490/64305], Loss: 5.0313\n",
      "Epoch [1/2], Step [42500/64305], Loss: 4.9482\n",
      "Epoch [1/2], Step [42510/64305], Loss: 5.0738\n",
      "Epoch [1/2], Step [42520/64305], Loss: 5.0969\n",
      "Epoch [1/2], Step [42530/64305], Loss: 4.9753\n",
      "Epoch [1/2], Step [42540/64305], Loss: 4.8314\n",
      "Epoch [1/2], Step [42550/64305], Loss: 5.0802\n",
      "Epoch [1/2], Step [42560/64305], Loss: 5.0786\n",
      "Epoch [1/2], Step [42570/64305], Loss: 5.2942\n",
      "Epoch [1/2], Step [42580/64305], Loss: 5.0783\n",
      "Epoch [1/2], Step [42590/64305], Loss: 5.0708\n",
      "Epoch [1/2], Step [42600/64305], Loss: 5.1664\n",
      "Epoch [1/2], Step [42610/64305], Loss: 4.8662\n",
      "Epoch [1/2], Step [42620/64305], Loss: 5.1272\n",
      "Epoch [1/2], Step [42630/64305], Loss: 5.0297\n",
      "Epoch [1/2], Step [42640/64305], Loss: 4.7645\n",
      "Epoch [1/2], Step [42650/64305], Loss: 4.8131\n",
      "Epoch [1/2], Step [42660/64305], Loss: 4.9356\n",
      "Epoch [1/2], Step [42670/64305], Loss: 5.0916\n",
      "Epoch [1/2], Step [42680/64305], Loss: 5.0662\n",
      "Epoch [1/2], Step [42690/64305], Loss: 4.7931\n",
      "Epoch [1/2], Step [42700/64305], Loss: 5.0384\n",
      "Epoch [1/2], Step [42710/64305], Loss: 5.0679\n",
      "Epoch [1/2], Step [42720/64305], Loss: 5.0267\n",
      "Epoch [1/2], Step [42730/64305], Loss: 4.8654\n",
      "Epoch [1/2], Step [42740/64305], Loss: 4.9860\n",
      "Epoch [1/2], Step [42750/64305], Loss: 5.1165\n",
      "Epoch [1/2], Step [42760/64305], Loss: 4.9277\n",
      "Epoch [1/2], Step [42770/64305], Loss: 4.9450\n",
      "Epoch [1/2], Step [42780/64305], Loss: 4.9389\n",
      "Epoch [1/2], Step [42790/64305], Loss: 5.0028\n",
      "Epoch [1/2], Step [42800/64305], Loss: 5.0570\n",
      "Epoch [1/2], Step [42810/64305], Loss: 5.0909\n",
      "Epoch [1/2], Step [42820/64305], Loss: 4.9067\n",
      "Epoch [1/2], Step [42830/64305], Loss: 4.7890\n",
      "Epoch [1/2], Step [42840/64305], Loss: 5.0739\n",
      "Epoch [1/2], Step [42850/64305], Loss: 4.9572\n",
      "Epoch [1/2], Step [42860/64305], Loss: 4.8064\n",
      "Epoch [1/2], Step [42870/64305], Loss: 4.9537\n",
      "Epoch [1/2], Step [42880/64305], Loss: 5.1175\n",
      "Epoch [1/2], Step [42890/64305], Loss: 4.8556\n",
      "Epoch [1/2], Step [42900/64305], Loss: 5.1029\n",
      "Epoch [1/2], Step [42910/64305], Loss: 4.7622\n",
      "Epoch [1/2], Step [42920/64305], Loss: 4.7806\n",
      "Epoch [1/2], Step [42930/64305], Loss: 4.7838\n",
      "Epoch [1/2], Step [42940/64305], Loss: 5.1313\n",
      "Epoch [1/2], Step [42950/64305], Loss: 4.7988\n",
      "Epoch [1/2], Step [42960/64305], Loss: 4.9153\n",
      "Epoch [1/2], Step [42970/64305], Loss: 5.0198\n",
      "Epoch [1/2], Step [42980/64305], Loss: 4.9258\n",
      "Epoch [1/2], Step [42990/64305], Loss: 5.0314\n",
      "Epoch [1/2], Step [43000/64305], Loss: 4.8032\n",
      "Epoch [1/2], Step [43010/64305], Loss: 5.0002\n",
      "Epoch [1/2], Step [43020/64305], Loss: 4.8266\n",
      "Epoch [1/2], Step [43030/64305], Loss: 4.9588\n",
      "Epoch [1/2], Step [43040/64305], Loss: 4.9515\n",
      "Epoch [1/2], Step [43050/64305], Loss: 5.1202\n",
      "Epoch [1/2], Step [43060/64305], Loss: 4.8966\n",
      "Epoch [1/2], Step [43070/64305], Loss: 5.0920\n",
      "Epoch [1/2], Step [43080/64305], Loss: 5.0660\n",
      "Epoch [1/2], Step [43090/64305], Loss: 4.8761\n",
      "Epoch [1/2], Step [43100/64305], Loss: 4.9534\n",
      "Epoch [1/2], Step [43110/64305], Loss: 4.7493\n",
      "Epoch [1/2], Step [43120/64305], Loss: 4.7107\n",
      "Epoch [1/2], Step [43130/64305], Loss: 4.9284\n",
      "Epoch [1/2], Step [43140/64305], Loss: 4.9579\n",
      "Epoch [1/2], Step [43150/64305], Loss: 4.8799\n",
      "Epoch [1/2], Step [43160/64305], Loss: 4.8874\n",
      "Epoch [1/2], Step [43170/64305], Loss: 5.0284\n",
      "Epoch [1/2], Step [43180/64305], Loss: 4.8768\n",
      "Epoch [1/2], Step [43190/64305], Loss: 4.9917\n",
      "Epoch [1/2], Step [43200/64305], Loss: 4.7416\n",
      "Epoch [1/2], Step [43210/64305], Loss: 4.9765\n",
      "Epoch [1/2], Step [43220/64305], Loss: 5.0986\n",
      "Epoch [1/2], Step [43230/64305], Loss: 5.0250\n",
      "Epoch [1/2], Step [43240/64305], Loss: 4.9472\n",
      "Epoch [1/2], Step [43250/64305], Loss: 5.0433\n",
      "Epoch [1/2], Step [43260/64305], Loss: 4.8765\n",
      "Epoch [1/2], Step [43270/64305], Loss: 4.9364\n",
      "Epoch [1/2], Step [43280/64305], Loss: 5.0239\n",
      "Epoch [1/2], Step [43290/64305], Loss: 4.9077\n",
      "Epoch [1/2], Step [43300/64305], Loss: 5.0758\n",
      "Epoch [1/2], Step [43310/64305], Loss: 5.0188\n",
      "Epoch [1/2], Step [43320/64305], Loss: 4.8883\n",
      "Epoch [1/2], Step [43330/64305], Loss: 4.9652\n",
      "Epoch [1/2], Step [43340/64305], Loss: 4.9708\n",
      "Epoch [1/2], Step [43350/64305], Loss: 5.0335\n",
      "Epoch [1/2], Step [43360/64305], Loss: 5.0764\n",
      "Epoch [1/2], Step [43370/64305], Loss: 5.0006\n",
      "Epoch [1/2], Step [43380/64305], Loss: 4.9509\n",
      "Epoch [1/2], Step [43390/64305], Loss: 4.7647\n",
      "Epoch [1/2], Step [43400/64305], Loss: 5.0992\n",
      "Epoch [1/2], Step [43410/64305], Loss: 4.8748\n",
      "Epoch [1/2], Step [43420/64305], Loss: 5.0387\n",
      "Epoch [1/2], Step [43430/64305], Loss: 4.8820\n",
      "Epoch [1/2], Step [43440/64305], Loss: 5.1506\n",
      "Epoch [1/2], Step [43450/64305], Loss: 5.0656\n",
      "Epoch [1/2], Step [43460/64305], Loss: 5.0695\n",
      "Epoch [1/2], Step [43470/64305], Loss: 5.0295\n",
      "Epoch [1/2], Step [43480/64305], Loss: 5.1378\n",
      "Epoch [1/2], Step [43490/64305], Loss: 5.0065\n",
      "Epoch [1/2], Step [43500/64305], Loss: 5.1629\n",
      "Epoch [1/2], Step [43510/64305], Loss: 5.0230\n",
      "Epoch [1/2], Step [43520/64305], Loss: 5.1279\n",
      "Epoch [1/2], Step [43530/64305], Loss: 4.7930\n",
      "Epoch [1/2], Step [43540/64305], Loss: 4.9903\n",
      "Epoch [1/2], Step [43550/64305], Loss: 5.0001\n",
      "Epoch [1/2], Step [43560/64305], Loss: 4.9326\n",
      "Epoch [1/2], Step [43570/64305], Loss: 4.8568\n",
      "Epoch [1/2], Step [43580/64305], Loss: 5.0777\n",
      "Epoch [1/2], Step [43590/64305], Loss: 4.9844\n",
      "Epoch [1/2], Step [43600/64305], Loss: 4.9213\n",
      "Epoch [1/2], Step [43610/64305], Loss: 4.9254\n",
      "Epoch [1/2], Step [43620/64305], Loss: 5.0557\n",
      "Epoch [1/2], Step [43630/64305], Loss: 4.8419\n",
      "Epoch [1/2], Step [43640/64305], Loss: 4.8802\n",
      "Epoch [1/2], Step [43650/64305], Loss: 5.1566\n",
      "Epoch [1/2], Step [43660/64305], Loss: 5.0728\n",
      "Epoch [1/2], Step [43670/64305], Loss: 5.0596\n",
      "Epoch [1/2], Step [43680/64305], Loss: 4.9494\n",
      "Epoch [1/2], Step [43690/64305], Loss: 4.7776\n",
      "Epoch [1/2], Step [43700/64305], Loss: 4.8722\n",
      "Epoch [1/2], Step [43710/64305], Loss: 4.7515\n",
      "Epoch [1/2], Step [43720/64305], Loss: 4.9522\n",
      "Epoch [1/2], Step [43730/64305], Loss: 4.8519\n",
      "Epoch [1/2], Step [43740/64305], Loss: 5.0801\n",
      "Epoch [1/2], Step [43750/64305], Loss: 5.0060\n",
      "Epoch [1/2], Step [43760/64305], Loss: 5.1721\n",
      "Epoch [1/2], Step [43770/64305], Loss: 5.0659\n",
      "Epoch [1/2], Step [43780/64305], Loss: 5.0365\n",
      "Epoch [1/2], Step [43790/64305], Loss: 5.2270\n",
      "Epoch [1/2], Step [43800/64305], Loss: 4.9116\n",
      "Epoch [1/2], Step [43810/64305], Loss: 4.9337\n",
      "Epoch [1/2], Step [43820/64305], Loss: 4.9732\n",
      "Epoch [1/2], Step [43830/64305], Loss: 4.7980\n",
      "Epoch [1/2], Step [43840/64305], Loss: 5.0020\n",
      "Epoch [1/2], Step [43850/64305], Loss: 5.0109\n",
      "Epoch [1/2], Step [43860/64305], Loss: 4.8453\n",
      "Epoch [1/2], Step [43870/64305], Loss: 4.9891\n",
      "Epoch [1/2], Step [43880/64305], Loss: 4.9559\n",
      "Epoch [1/2], Step [43890/64305], Loss: 5.0155\n",
      "Epoch [1/2], Step [43900/64305], Loss: 5.1094\n",
      "Epoch [1/2], Step [43910/64305], Loss: 4.9511\n",
      "Epoch [1/2], Step [43920/64305], Loss: 4.7604\n",
      "Epoch [1/2], Step [43930/64305], Loss: 5.0129\n",
      "Epoch [1/2], Step [43940/64305], Loss: 4.9187\n",
      "Epoch [1/2], Step [43950/64305], Loss: 5.1380\n",
      "Epoch [1/2], Step [43960/64305], Loss: 4.8821\n",
      "Epoch [1/2], Step [43970/64305], Loss: 5.1770\n",
      "Epoch [1/2], Step [43980/64305], Loss: 4.9993\n",
      "Epoch [1/2], Step [43990/64305], Loss: 4.9896\n",
      "Epoch [1/2], Step [44000/64305], Loss: 4.8928\n",
      "Epoch [1/2], Step [44010/64305], Loss: 4.9313\n",
      "Epoch [1/2], Step [44020/64305], Loss: 5.0813\n",
      "Epoch [1/2], Step [44030/64305], Loss: 5.0140\n",
      "Epoch [1/2], Step [44040/64305], Loss: 4.8744\n",
      "Epoch [1/2], Step [44050/64305], Loss: 5.0505\n",
      "Epoch [1/2], Step [44060/64305], Loss: 5.0360\n",
      "Epoch [1/2], Step [44070/64305], Loss: 4.9388\n",
      "Epoch [1/2], Step [44080/64305], Loss: 4.9339\n",
      "Epoch [1/2], Step [44090/64305], Loss: 4.9676\n",
      "Epoch [1/2], Step [44100/64305], Loss: 4.8224\n",
      "Epoch [1/2], Step [44110/64305], Loss: 5.1678\n",
      "Epoch [1/2], Step [44120/64305], Loss: 5.0381\n",
      "Epoch [1/2], Step [44130/64305], Loss: 5.1395\n",
      "Epoch [1/2], Step [44140/64305], Loss: 5.0980\n",
      "Epoch [1/2], Step [44150/64305], Loss: 4.7691\n",
      "Epoch [1/2], Step [44160/64305], Loss: 5.1672\n",
      "Epoch [1/2], Step [44170/64305], Loss: 4.9656\n",
      "Epoch [1/2], Step [44180/64305], Loss: 5.1281\n",
      "Epoch [1/2], Step [44190/64305], Loss: 5.0565\n",
      "Epoch [1/2], Step [44200/64305], Loss: 4.9818\n",
      "Epoch [1/2], Step [44210/64305], Loss: 4.9525\n",
      "Epoch [1/2], Step [44220/64305], Loss: 5.2188\n",
      "Epoch [1/2], Step [44230/64305], Loss: 4.7939\n",
      "Epoch [1/2], Step [44240/64305], Loss: 4.8457\n",
      "Epoch [1/2], Step [44250/64305], Loss: 4.7979\n",
      "Epoch [1/2], Step [44260/64305], Loss: 4.8454\n",
      "Epoch [1/2], Step [44270/64305], Loss: 4.6428\n",
      "Epoch [1/2], Step [44280/64305], Loss: 4.8739\n",
      "Epoch [1/2], Step [44290/64305], Loss: 5.0953\n",
      "Epoch [1/2], Step [44300/64305], Loss: 4.9969\n",
      "Epoch [1/2], Step [44310/64305], Loss: 4.9381\n",
      "Epoch [1/2], Step [44320/64305], Loss: 4.7161\n",
      "Epoch [1/2], Step [44330/64305], Loss: 4.8172\n",
      "Epoch [1/2], Step [44340/64305], Loss: 4.9029\n",
      "Epoch [1/2], Step [44350/64305], Loss: 5.1033\n",
      "Epoch [1/2], Step [44360/64305], Loss: 4.9986\n",
      "Epoch [1/2], Step [44370/64305], Loss: 5.1490\n",
      "Epoch [1/2], Step [44380/64305], Loss: 4.9873\n",
      "Epoch [1/2], Step [44390/64305], Loss: 4.7588\n",
      "Epoch [1/2], Step [44400/64305], Loss: 5.2165\n",
      "Epoch [1/2], Step [44410/64305], Loss: 5.0396\n",
      "Epoch [1/2], Step [44420/64305], Loss: 4.9567\n",
      "Epoch [1/2], Step [44430/64305], Loss: 5.0858\n",
      "Epoch [1/2], Step [44440/64305], Loss: 4.8746\n",
      "Epoch [1/2], Step [44450/64305], Loss: 4.9073\n",
      "Epoch [1/2], Step [44460/64305], Loss: 4.7803\n",
      "Epoch [1/2], Step [44470/64305], Loss: 5.0908\n",
      "Epoch [1/2], Step [44480/64305], Loss: 5.2268\n",
      "Epoch [1/2], Step [44490/64305], Loss: 4.8426\n",
      "Epoch [1/2], Step [44500/64305], Loss: 5.0138\n",
      "Epoch [1/2], Step [44510/64305], Loss: 4.9656\n",
      "Epoch [1/2], Step [44520/64305], Loss: 4.9889\n",
      "Epoch [1/2], Step [44530/64305], Loss: 5.0236\n",
      "Epoch [1/2], Step [44540/64305], Loss: 4.9371\n",
      "Epoch [1/2], Step [44550/64305], Loss: 4.9566\n",
      "Epoch [1/2], Step [44560/64305], Loss: 5.0330\n",
      "Epoch [1/2], Step [44570/64305], Loss: 4.8154\n",
      "Epoch [1/2], Step [44580/64305], Loss: 4.8687\n",
      "Epoch [1/2], Step [44590/64305], Loss: 4.6814\n",
      "Epoch [1/2], Step [44600/64305], Loss: 4.9528\n",
      "Epoch [1/2], Step [44610/64305], Loss: 4.9068\n",
      "Epoch [1/2], Step [44620/64305], Loss: 4.9580\n",
      "Epoch [1/2], Step [44630/64305], Loss: 4.9542\n",
      "Epoch [1/2], Step [44640/64305], Loss: 4.7982\n",
      "Epoch [1/2], Step [44650/64305], Loss: 4.9879\n",
      "Epoch [1/2], Step [44660/64305], Loss: 4.9103\n",
      "Epoch [1/2], Step [44670/64305], Loss: 5.0650\n",
      "Epoch [1/2], Step [44680/64305], Loss: 4.9579\n",
      "Epoch [1/2], Step [44690/64305], Loss: 5.0018\n",
      "Epoch [1/2], Step [44700/64305], Loss: 5.0259\n",
      "Epoch [1/2], Step [44710/64305], Loss: 5.0004\n",
      "Epoch [1/2], Step [44720/64305], Loss: 4.9599\n",
      "Epoch [1/2], Step [44730/64305], Loss: 4.8389\n",
      "Epoch [1/2], Step [44740/64305], Loss: 4.9156\n",
      "Epoch [1/2], Step [44750/64305], Loss: 4.8864\n",
      "Epoch [1/2], Step [44760/64305], Loss: 4.8426\n",
      "Epoch [1/2], Step [44770/64305], Loss: 4.9700\n",
      "Epoch [1/2], Step [44780/64305], Loss: 4.8453\n",
      "Epoch [1/2], Step [44790/64305], Loss: 5.1238\n",
      "Epoch [1/2], Step [44800/64305], Loss: 4.8791\n",
      "Epoch [1/2], Step [44810/64305], Loss: 5.2138\n",
      "Epoch [1/2], Step [44820/64305], Loss: 5.0453\n",
      "Epoch [1/2], Step [44830/64305], Loss: 5.3051\n",
      "Epoch [1/2], Step [44840/64305], Loss: 5.0712\n",
      "Epoch [1/2], Step [44850/64305], Loss: 5.0608\n",
      "Epoch [1/2], Step [44860/64305], Loss: 4.9277\n",
      "Epoch [1/2], Step [44870/64305], Loss: 5.0247\n",
      "Epoch [1/2], Step [44880/64305], Loss: 4.8679\n",
      "Epoch [1/2], Step [44890/64305], Loss: 4.8556\n",
      "Epoch [1/2], Step [44900/64305], Loss: 4.8007\n",
      "Epoch [1/2], Step [44910/64305], Loss: 4.8883\n",
      "Epoch [1/2], Step [44920/64305], Loss: 5.0904\n",
      "Epoch [1/2], Step [44930/64305], Loss: 4.8964\n",
      "Epoch [1/2], Step [44940/64305], Loss: 4.8962\n",
      "Epoch [1/2], Step [44950/64305], Loss: 4.8839\n",
      "Epoch [1/2], Step [44960/64305], Loss: 4.8807\n",
      "Epoch [1/2], Step [44970/64305], Loss: 4.9843\n",
      "Epoch [1/2], Step [44980/64305], Loss: 5.0005\n",
      "Epoch [1/2], Step [44990/64305], Loss: 4.9721\n",
      "Epoch [1/2], Step [45000/64305], Loss: 4.9999\n",
      "Epoch [1/2], Step [45010/64305], Loss: 5.1471\n",
      "Epoch [1/2], Step [45020/64305], Loss: 5.2871\n",
      "Epoch [1/2], Step [45030/64305], Loss: 5.1238\n",
      "Epoch [1/2], Step [45040/64305], Loss: 4.9212\n",
      "Epoch [1/2], Step [45050/64305], Loss: 5.0785\n",
      "Epoch [1/2], Step [45060/64305], Loss: 5.1006\n",
      "Epoch [1/2], Step [45070/64305], Loss: 4.9106\n",
      "Epoch [1/2], Step [45080/64305], Loss: 4.9628\n",
      "Epoch [1/2], Step [45090/64305], Loss: 5.0278\n",
      "Epoch [1/2], Step [45100/64305], Loss: 4.9608\n",
      "Epoch [1/2], Step [45110/64305], Loss: 4.9372\n",
      "Epoch [1/2], Step [45120/64305], Loss: 5.0001\n",
      "Epoch [1/2], Step [45130/64305], Loss: 4.8921\n",
      "Epoch [1/2], Step [45140/64305], Loss: 5.0834\n",
      "Epoch [1/2], Step [45150/64305], Loss: 4.8921\n",
      "Epoch [1/2], Step [45160/64305], Loss: 4.8937\n",
      "Epoch [1/2], Step [45170/64305], Loss: 4.9774\n",
      "Epoch [1/2], Step [45180/64305], Loss: 4.9018\n",
      "Epoch [1/2], Step [45190/64305], Loss: 5.0244\n",
      "Epoch [1/2], Step [45200/64305], Loss: 4.7289\n",
      "Epoch [1/2], Step [45210/64305], Loss: 4.9854\n",
      "Epoch [1/2], Step [45220/64305], Loss: 5.0484\n",
      "Epoch [1/2], Step [45230/64305], Loss: 4.8300\n",
      "Epoch [1/2], Step [45240/64305], Loss: 4.9895\n",
      "Epoch [1/2], Step [45250/64305], Loss: 5.0649\n",
      "Epoch [1/2], Step [45260/64305], Loss: 4.8988\n",
      "Epoch [1/2], Step [45270/64305], Loss: 5.0451\n",
      "Epoch [1/2], Step [45280/64305], Loss: 4.9550\n",
      "Epoch [1/2], Step [45290/64305], Loss: 5.1389\n",
      "Epoch [1/2], Step [45300/64305], Loss: 4.8977\n",
      "Epoch [1/2], Step [45310/64305], Loss: 4.6143\n",
      "Epoch [1/2], Step [45320/64305], Loss: 4.9739\n",
      "Epoch [1/2], Step [45330/64305], Loss: 4.9434\n",
      "Epoch [1/2], Step [45340/64305], Loss: 4.8179\n",
      "Epoch [1/2], Step [45350/64305], Loss: 4.9198\n",
      "Epoch [1/2], Step [45360/64305], Loss: 4.8982\n",
      "Epoch [1/2], Step [45370/64305], Loss: 5.0160\n",
      "Epoch [1/2], Step [45380/64305], Loss: 4.9316\n",
      "Epoch [1/2], Step [45390/64305], Loss: 4.9313\n",
      "Epoch [1/2], Step [45400/64305], Loss: 4.9430\n",
      "Epoch [1/2], Step [45410/64305], Loss: 4.9968\n",
      "Epoch [1/2], Step [45420/64305], Loss: 4.9709\n",
      "Epoch [1/2], Step [45430/64305], Loss: 4.9239\n",
      "Epoch [1/2], Step [45440/64305], Loss: 4.9711\n",
      "Epoch [1/2], Step [45450/64305], Loss: 4.9171\n",
      "Epoch [1/2], Step [45460/64305], Loss: 5.0325\n",
      "Epoch [1/2], Step [45470/64305], Loss: 4.9731\n",
      "Epoch [1/2], Step [45480/64305], Loss: 4.6506\n",
      "Epoch [1/2], Step [45490/64305], Loss: 4.9468\n",
      "Epoch [1/2], Step [45500/64305], Loss: 4.9844\n",
      "Epoch [1/2], Step [45510/64305], Loss: 4.9405\n",
      "Epoch [1/2], Step [45520/64305], Loss: 5.0279\n",
      "Epoch [1/2], Step [45530/64305], Loss: 5.2244\n",
      "Epoch [1/2], Step [45540/64305], Loss: 4.7551\n",
      "Epoch [1/2], Step [45550/64305], Loss: 4.8511\n",
      "Epoch [1/2], Step [45560/64305], Loss: 5.1260\n",
      "Epoch [1/2], Step [45570/64305], Loss: 4.8858\n",
      "Epoch [1/2], Step [45580/64305], Loss: 4.9713\n",
      "Epoch [1/2], Step [45590/64305], Loss: 4.9418\n",
      "Epoch [1/2], Step [45600/64305], Loss: 5.0327\n",
      "Epoch [1/2], Step [45610/64305], Loss: 4.9914\n",
      "Epoch [1/2], Step [45620/64305], Loss: 5.2028\n",
      "Epoch [1/2], Step [45630/64305], Loss: 4.9046\n",
      "Epoch [1/2], Step [45640/64305], Loss: 4.9759\n",
      "Epoch [1/2], Step [45650/64305], Loss: 4.6983\n",
      "Epoch [1/2], Step [45660/64305], Loss: 5.1295\n",
      "Epoch [1/2], Step [45670/64305], Loss: 4.8504\n",
      "Epoch [1/2], Step [45680/64305], Loss: 4.9831\n",
      "Epoch [1/2], Step [45690/64305], Loss: 4.9029\n",
      "Epoch [1/2], Step [45700/64305], Loss: 5.0027\n",
      "Epoch [1/2], Step [45710/64305], Loss: 4.9087\n",
      "Epoch [1/2], Step [45720/64305], Loss: 4.9386\n",
      "Epoch [1/2], Step [45730/64305], Loss: 4.9214\n",
      "Epoch [1/2], Step [45740/64305], Loss: 4.7649\n",
      "Epoch [1/2], Step [45750/64305], Loss: 4.7808\n",
      "Epoch [1/2], Step [45760/64305], Loss: 5.0792\n",
      "Epoch [1/2], Step [45770/64305], Loss: 5.0349\n",
      "Epoch [1/2], Step [45780/64305], Loss: 4.9203\n",
      "Epoch [1/2], Step [45790/64305], Loss: 4.9880\n",
      "Epoch [1/2], Step [45800/64305], Loss: 5.0718\n",
      "Epoch [1/2], Step [45810/64305], Loss: 4.7931\n",
      "Epoch [1/2], Step [45820/64305], Loss: 5.2646\n",
      "Epoch [1/2], Step [45830/64305], Loss: 4.7941\n",
      "Epoch [1/2], Step [45840/64305], Loss: 4.8951\n",
      "Epoch [1/2], Step [45850/64305], Loss: 4.9032\n",
      "Epoch [1/2], Step [45860/64305], Loss: 5.1170\n",
      "Epoch [1/2], Step [45870/64305], Loss: 5.1580\n",
      "Epoch [1/2], Step [45880/64305], Loss: 5.0923\n",
      "Epoch [1/2], Step [45890/64305], Loss: 5.1550\n",
      "Epoch [1/2], Step [45900/64305], Loss: 5.1228\n",
      "Epoch [1/2], Step [45910/64305], Loss: 5.0495\n",
      "Epoch [1/2], Step [45920/64305], Loss: 4.9479\n",
      "Epoch [1/2], Step [45930/64305], Loss: 4.8799\n",
      "Epoch [1/2], Step [45940/64305], Loss: 4.8346\n",
      "Epoch [1/2], Step [45950/64305], Loss: 4.9735\n",
      "Epoch [1/2], Step [45960/64305], Loss: 4.9928\n",
      "Epoch [1/2], Step [45970/64305], Loss: 5.0245\n",
      "Epoch [1/2], Step [45980/64305], Loss: 4.9269\n",
      "Epoch [1/2], Step [45990/64305], Loss: 5.0209\n",
      "Epoch [1/2], Step [46000/64305], Loss: 5.0841\n",
      "Epoch [1/2], Step [46010/64305], Loss: 4.9444\n",
      "Epoch [1/2], Step [46020/64305], Loss: 4.8867\n",
      "Epoch [1/2], Step [46030/64305], Loss: 4.9843\n",
      "Epoch [1/2], Step [46040/64305], Loss: 5.0499\n",
      "Epoch [1/2], Step [46050/64305], Loss: 5.3252\n",
      "Epoch [1/2], Step [46060/64305], Loss: 4.8554\n",
      "Epoch [1/2], Step [46070/64305], Loss: 4.8253\n",
      "Epoch [1/2], Step [46080/64305], Loss: 5.0615\n",
      "Epoch [1/2], Step [46090/64305], Loss: 4.7952\n",
      "Epoch [1/2], Step [46100/64305], Loss: 5.0769\n",
      "Epoch [1/2], Step [46110/64305], Loss: 4.8969\n",
      "Epoch [1/2], Step [46120/64305], Loss: 4.9272\n",
      "Epoch [1/2], Step [46130/64305], Loss: 4.8967\n",
      "Epoch [1/2], Step [46140/64305], Loss: 5.0909\n",
      "Epoch [1/2], Step [46150/64305], Loss: 5.0796\n",
      "Epoch [1/2], Step [46160/64305], Loss: 4.7608\n",
      "Epoch [1/2], Step [46170/64305], Loss: 4.9752\n",
      "Epoch [1/2], Step [46180/64305], Loss: 4.8574\n",
      "Epoch [1/2], Step [46190/64305], Loss: 4.6419\n",
      "Epoch [1/2], Step [46200/64305], Loss: 5.0317\n",
      "Epoch [1/2], Step [46210/64305], Loss: 5.1104\n",
      "Epoch [1/2], Step [46220/64305], Loss: 4.8234\n",
      "Epoch [1/2], Step [46230/64305], Loss: 5.0493\n",
      "Epoch [1/2], Step [46240/64305], Loss: 4.9776\n",
      "Epoch [1/2], Step [46250/64305], Loss: 4.7880\n",
      "Epoch [1/2], Step [46260/64305], Loss: 5.0108\n",
      "Epoch [1/2], Step [46270/64305], Loss: 4.8912\n",
      "Epoch [1/2], Step [46280/64305], Loss: 5.0802\n",
      "Epoch [1/2], Step [46290/64305], Loss: 4.9594\n",
      "Epoch [1/2], Step [46300/64305], Loss: 4.9777\n",
      "Epoch [1/2], Step [46310/64305], Loss: 5.1130\n",
      "Epoch [1/2], Step [46320/64305], Loss: 4.8463\n",
      "Epoch [1/2], Step [46330/64305], Loss: 4.9996\n",
      "Epoch [1/2], Step [46340/64305], Loss: 4.8965\n",
      "Epoch [1/2], Step [46350/64305], Loss: 4.7566\n",
      "Epoch [1/2], Step [46360/64305], Loss: 5.0511\n",
      "Epoch [1/2], Step [46370/64305], Loss: 5.0890\n",
      "Epoch [1/2], Step [46380/64305], Loss: 4.8396\n",
      "Epoch [1/2], Step [46390/64305], Loss: 5.0253\n",
      "Epoch [1/2], Step [46400/64305], Loss: 5.0019\n",
      "Epoch [1/2], Step [46410/64305], Loss: 4.8270\n",
      "Epoch [1/2], Step [46420/64305], Loss: 5.1792\n",
      "Epoch [1/2], Step [46430/64305], Loss: 4.9192\n",
      "Epoch [1/2], Step [46440/64305], Loss: 5.0847\n",
      "Epoch [1/2], Step [46450/64305], Loss: 4.8932\n",
      "Epoch [1/2], Step [46460/64305], Loss: 5.0827\n",
      "Epoch [1/2], Step [46470/64305], Loss: 4.9705\n",
      "Epoch [1/2], Step [46480/64305], Loss: 5.0675\n",
      "Epoch [1/2], Step [46490/64305], Loss: 5.2703\n",
      "Epoch [1/2], Step [46500/64305], Loss: 5.2166\n",
      "Epoch [1/2], Step [46510/64305], Loss: 4.8436\n",
      "Epoch [1/2], Step [46520/64305], Loss: 4.8944\n",
      "Epoch [1/2], Step [46530/64305], Loss: 5.0331\n",
      "Epoch [1/2], Step [46540/64305], Loss: 4.9239\n",
      "Epoch [1/2], Step [46550/64305], Loss: 4.9412\n",
      "Epoch [1/2], Step [46560/64305], Loss: 4.9149\n",
      "Epoch [1/2], Step [46570/64305], Loss: 5.0783\n",
      "Epoch [1/2], Step [46580/64305], Loss: 5.0953\n",
      "Epoch [1/2], Step [46590/64305], Loss: 4.8171\n",
      "Epoch [1/2], Step [46600/64305], Loss: 4.7805\n",
      "Epoch [1/2], Step [46610/64305], Loss: 4.9502\n",
      "Epoch [1/2], Step [46620/64305], Loss: 5.0261\n",
      "Epoch [1/2], Step [46630/64305], Loss: 4.7411\n",
      "Epoch [1/2], Step [46640/64305], Loss: 4.9329\n",
      "Epoch [1/2], Step [46650/64305], Loss: 5.1143\n",
      "Epoch [1/2], Step [46660/64305], Loss: 4.9170\n",
      "Epoch [1/2], Step [46670/64305], Loss: 4.9058\n",
      "Epoch [1/2], Step [46680/64305], Loss: 5.0371\n",
      "Epoch [1/2], Step [46690/64305], Loss: 4.9369\n",
      "Epoch [1/2], Step [46700/64305], Loss: 4.7452\n",
      "Epoch [1/2], Step [46710/64305], Loss: 5.0089\n",
      "Epoch [1/2], Step [46720/64305], Loss: 4.9508\n",
      "Epoch [1/2], Step [46730/64305], Loss: 4.8731\n",
      "Epoch [1/2], Step [46740/64305], Loss: 4.9706\n",
      "Epoch [1/2], Step [46750/64305], Loss: 4.8976\n",
      "Epoch [1/2], Step [46760/64305], Loss: 4.9287\n",
      "Epoch [1/2], Step [46770/64305], Loss: 4.9419\n",
      "Epoch [1/2], Step [46780/64305], Loss: 5.0452\n",
      "Epoch [1/2], Step [46790/64305], Loss: 4.8687\n",
      "Epoch [1/2], Step [46800/64305], Loss: 5.1144\n",
      "Epoch [1/2], Step [46810/64305], Loss: 4.9251\n",
      "Epoch [1/2], Step [46820/64305], Loss: 5.0581\n",
      "Epoch [1/2], Step [46830/64305], Loss: 4.9696\n",
      "Epoch [1/2], Step [46840/64305], Loss: 4.9688\n",
      "Epoch [1/2], Step [46850/64305], Loss: 5.0933\n",
      "Epoch [1/2], Step [46860/64305], Loss: 4.7084\n",
      "Epoch [1/2], Step [46870/64305], Loss: 5.0522\n",
      "Epoch [1/2], Step [46880/64305], Loss: 4.8365\n",
      "Epoch [1/2], Step [46890/64305], Loss: 4.9426\n",
      "Epoch [1/2], Step [46900/64305], Loss: 4.8403\n",
      "Epoch [1/2], Step [46910/64305], Loss: 5.1840\n",
      "Epoch [1/2], Step [46920/64305], Loss: 4.6887\n",
      "Epoch [1/2], Step [46930/64305], Loss: 4.9257\n",
      "Epoch [1/2], Step [46940/64305], Loss: 5.1940\n",
      "Epoch [1/2], Step [46950/64305], Loss: 4.9141\n",
      "Epoch [1/2], Step [46960/64305], Loss: 4.9454\n",
      "Epoch [1/2], Step [46970/64305], Loss: 4.9217\n",
      "Epoch [1/2], Step [46980/64305], Loss: 4.9371\n",
      "Epoch [1/2], Step [46990/64305], Loss: 4.9028\n",
      "Epoch [1/2], Step [47000/64305], Loss: 4.9295\n",
      "Epoch [1/2], Step [47010/64305], Loss: 5.0390\n",
      "Epoch [1/2], Step [47020/64305], Loss: 5.0030\n",
      "Epoch [1/2], Step [47030/64305], Loss: 5.0623\n",
      "Epoch [1/2], Step [47040/64305], Loss: 5.0353\n",
      "Epoch [1/2], Step [47050/64305], Loss: 4.8379\n",
      "Epoch [1/2], Step [47060/64305], Loss: 5.0473\n",
      "Epoch [1/2], Step [47070/64305], Loss: 4.8785\n",
      "Epoch [1/2], Step [47080/64305], Loss: 4.9410\n",
      "Epoch [1/2], Step [47090/64305], Loss: 4.9108\n",
      "Epoch [1/2], Step [47100/64305], Loss: 5.0846\n",
      "Epoch [1/2], Step [47110/64305], Loss: 4.9479\n",
      "Epoch [1/2], Step [47120/64305], Loss: 5.0303\n",
      "Epoch [1/2], Step [47130/64305], Loss: 5.1727\n",
      "Epoch [1/2], Step [47140/64305], Loss: 4.8895\n",
      "Epoch [1/2], Step [47150/64305], Loss: 5.0391\n",
      "Epoch [1/2], Step [47160/64305], Loss: 5.0233\n",
      "Epoch [1/2], Step [47170/64305], Loss: 4.9798\n",
      "Epoch [1/2], Step [47180/64305], Loss: 4.7865\n",
      "Epoch [1/2], Step [47190/64305], Loss: 4.9441\n",
      "Epoch [1/2], Step [47200/64305], Loss: 5.0148\n",
      "Epoch [1/2], Step [47210/64305], Loss: 5.0539\n",
      "Epoch [1/2], Step [47220/64305], Loss: 5.0477\n",
      "Epoch [1/2], Step [47230/64305], Loss: 4.9880\n",
      "Epoch [1/2], Step [47240/64305], Loss: 5.0714\n",
      "Epoch [1/2], Step [47250/64305], Loss: 5.0290\n",
      "Epoch [1/2], Step [47260/64305], Loss: 5.0248\n",
      "Epoch [1/2], Step [47270/64305], Loss: 5.0396\n",
      "Epoch [1/2], Step [47280/64305], Loss: 4.7354\n",
      "Epoch [1/2], Step [47290/64305], Loss: 5.0667\n",
      "Epoch [1/2], Step [47300/64305], Loss: 5.0760\n",
      "Epoch [1/2], Step [47310/64305], Loss: 4.8955\n",
      "Epoch [1/2], Step [47320/64305], Loss: 5.1410\n",
      "Epoch [1/2], Step [47330/64305], Loss: 4.8324\n",
      "Epoch [1/2], Step [47340/64305], Loss: 4.9874\n",
      "Epoch [1/2], Step [47350/64305], Loss: 5.0245\n",
      "Epoch [1/2], Step [47360/64305], Loss: 5.1825\n",
      "Epoch [1/2], Step [47370/64305], Loss: 4.9250\n",
      "Epoch [1/2], Step [47380/64305], Loss: 4.9607\n",
      "Epoch [1/2], Step [47390/64305], Loss: 4.9835\n",
      "Epoch [1/2], Step [47400/64305], Loss: 5.0623\n",
      "Epoch [1/2], Step [47410/64305], Loss: 4.8270\n",
      "Epoch [1/2], Step [47420/64305], Loss: 4.7718\n",
      "Epoch [1/2], Step [47430/64305], Loss: 4.7526\n",
      "Epoch [1/2], Step [47440/64305], Loss: 5.0115\n",
      "Epoch [1/2], Step [47450/64305], Loss: 4.9863\n",
      "Epoch [1/2], Step [47460/64305], Loss: 5.0180\n",
      "Epoch [1/2], Step [47470/64305], Loss: 4.7980\n",
      "Epoch [1/2], Step [47480/64305], Loss: 5.1318\n",
      "Epoch [1/2], Step [47490/64305], Loss: 5.0864\n",
      "Epoch [1/2], Step [47500/64305], Loss: 4.9191\n",
      "Epoch [1/2], Step [47510/64305], Loss: 4.8113\n",
      "Epoch [1/2], Step [47520/64305], Loss: 4.9135\n",
      "Epoch [1/2], Step [47530/64305], Loss: 5.0787\n",
      "Epoch [1/2], Step [47540/64305], Loss: 4.9641\n",
      "Epoch [1/2], Step [47550/64305], Loss: 5.1952\n",
      "Epoch [1/2], Step [47560/64305], Loss: 4.8483\n",
      "Epoch [1/2], Step [47570/64305], Loss: 5.0285\n",
      "Epoch [1/2], Step [47580/64305], Loss: 4.9969\n",
      "Epoch [1/2], Step [47590/64305], Loss: 4.9547\n",
      "Epoch [1/2], Step [47600/64305], Loss: 4.8141\n",
      "Epoch [1/2], Step [47610/64305], Loss: 5.0314\n",
      "Epoch [1/2], Step [47620/64305], Loss: 4.8226\n",
      "Epoch [1/2], Step [47630/64305], Loss: 5.0255\n",
      "Epoch [1/2], Step [47640/64305], Loss: 5.0773\n",
      "Epoch [1/2], Step [47650/64305], Loss: 5.0010\n",
      "Epoch [1/2], Step [47660/64305], Loss: 4.9554\n",
      "Epoch [1/2], Step [47670/64305], Loss: 5.0521\n",
      "Epoch [1/2], Step [47680/64305], Loss: 5.1563\n",
      "Epoch [1/2], Step [47690/64305], Loss: 5.0991\n",
      "Epoch [1/2], Step [47700/64305], Loss: 4.9336\n",
      "Epoch [1/2], Step [47710/64305], Loss: 4.9045\n",
      "Epoch [1/2], Step [47720/64305], Loss: 4.9522\n",
      "Epoch [1/2], Step [47730/64305], Loss: 4.9742\n",
      "Epoch [1/2], Step [47740/64305], Loss: 4.9517\n",
      "Epoch [1/2], Step [47750/64305], Loss: 4.9095\n",
      "Epoch [1/2], Step [47760/64305], Loss: 5.0099\n",
      "Epoch [1/2], Step [47770/64305], Loss: 5.0208\n",
      "Epoch [1/2], Step [47780/64305], Loss: 5.0500\n",
      "Epoch [1/2], Step [47790/64305], Loss: 4.8117\n",
      "Epoch [1/2], Step [47800/64305], Loss: 4.8684\n",
      "Epoch [1/2], Step [47810/64305], Loss: 4.7725\n",
      "Epoch [1/2], Step [47820/64305], Loss: 4.9530\n",
      "Epoch [1/2], Step [47830/64305], Loss: 5.0525\n",
      "Epoch [1/2], Step [47840/64305], Loss: 5.0797\n",
      "Epoch [1/2], Step [47850/64305], Loss: 5.0288\n",
      "Epoch [1/2], Step [47860/64305], Loss: 4.8200\n",
      "Epoch [1/2], Step [47870/64305], Loss: 4.8207\n",
      "Epoch [1/2], Step [47880/64305], Loss: 5.1710\n",
      "Epoch [1/2], Step [47890/64305], Loss: 4.9684\n",
      "Epoch [1/2], Step [47900/64305], Loss: 5.0292\n",
      "Epoch [1/2], Step [47910/64305], Loss: 4.9457\n",
      "Epoch [1/2], Step [47920/64305], Loss: 5.2434\n",
      "Epoch [1/2], Step [47930/64305], Loss: 4.8777\n",
      "Epoch [1/2], Step [47940/64305], Loss: 5.1148\n",
      "Epoch [1/2], Step [47950/64305], Loss: 5.0068\n",
      "Epoch [1/2], Step [47960/64305], Loss: 4.9049\n",
      "Epoch [1/2], Step [47970/64305], Loss: 4.8310\n",
      "Epoch [1/2], Step [47980/64305], Loss: 4.9912\n",
      "Epoch [1/2], Step [47990/64305], Loss: 5.1190\n",
      "Epoch [1/2], Step [48000/64305], Loss: 4.9537\n",
      "Epoch [1/2], Step [48010/64305], Loss: 4.8088\n",
      "Epoch [1/2], Step [48020/64305], Loss: 4.9221\n",
      "Epoch [1/2], Step [48030/64305], Loss: 5.0275\n",
      "Epoch [1/2], Step [48040/64305], Loss: 4.8880\n",
      "Epoch [1/2], Step [48050/64305], Loss: 4.9236\n",
      "Epoch [1/2], Step [48060/64305], Loss: 4.9689\n",
      "Epoch [1/2], Step [48070/64305], Loss: 4.9077\n",
      "Epoch [1/2], Step [48080/64305], Loss: 4.7702\n",
      "Epoch [1/2], Step [48090/64305], Loss: 4.9874\n",
      "Epoch [1/2], Step [48100/64305], Loss: 4.8000\n",
      "Epoch [1/2], Step [48110/64305], Loss: 4.8932\n",
      "Epoch [1/2], Step [48120/64305], Loss: 4.9394\n",
      "Epoch [1/2], Step [48130/64305], Loss: 4.8471\n",
      "Epoch [1/2], Step [48140/64305], Loss: 4.9011\n",
      "Epoch [1/2], Step [48150/64305], Loss: 4.8678\n",
      "Epoch [1/2], Step [48160/64305], Loss: 4.9268\n",
      "Epoch [1/2], Step [48170/64305], Loss: 5.0332\n",
      "Epoch [1/2], Step [48180/64305], Loss: 5.0143\n",
      "Epoch [1/2], Step [48190/64305], Loss: 5.1556\n",
      "Epoch [1/2], Step [48200/64305], Loss: 4.9112\n",
      "Epoch [1/2], Step [48210/64305], Loss: 5.0914\n",
      "Epoch [1/2], Step [48220/64305], Loss: 5.1613\n",
      "Epoch [1/2], Step [48230/64305], Loss: 4.8304\n",
      "Epoch [1/2], Step [48240/64305], Loss: 4.8072\n",
      "Epoch [1/2], Step [48250/64305], Loss: 4.9373\n",
      "Epoch [1/2], Step [48260/64305], Loss: 4.9252\n",
      "Epoch [1/2], Step [48270/64305], Loss: 5.0682\n",
      "Epoch [1/2], Step [48280/64305], Loss: 5.1678\n",
      "Epoch [1/2], Step [48290/64305], Loss: 4.8988\n",
      "Epoch [1/2], Step [48300/64305], Loss: 4.7844\n",
      "Epoch [1/2], Step [48310/64305], Loss: 5.1239\n",
      "Epoch [1/2], Step [48320/64305], Loss: 5.0156\n",
      "Epoch [1/2], Step [48330/64305], Loss: 4.7832\n",
      "Epoch [1/2], Step [48340/64305], Loss: 5.1096\n",
      "Epoch [1/2], Step [48350/64305], Loss: 5.0430\n",
      "Epoch [1/2], Step [48360/64305], Loss: 4.9540\n",
      "Epoch [1/2], Step [48370/64305], Loss: 4.7197\n",
      "Epoch [1/2], Step [48380/64305], Loss: 4.9943\n",
      "Epoch [1/2], Step [48390/64305], Loss: 4.8821\n",
      "Epoch [1/2], Step [48400/64305], Loss: 4.8482\n",
      "Epoch [1/2], Step [48410/64305], Loss: 4.8413\n",
      "Epoch [1/2], Step [48420/64305], Loss: 4.8379\n",
      "Epoch [1/2], Step [48430/64305], Loss: 4.8531\n",
      "Epoch [1/2], Step [48440/64305], Loss: 5.2136\n",
      "Epoch [1/2], Step [48450/64305], Loss: 5.0924\n",
      "Epoch [1/2], Step [48460/64305], Loss: 4.7495\n",
      "Epoch [1/2], Step [48470/64305], Loss: 4.8269\n",
      "Epoch [1/2], Step [48480/64305], Loss: 4.7026\n",
      "Epoch [1/2], Step [48490/64305], Loss: 4.8827\n",
      "Epoch [1/2], Step [48500/64305], Loss: 5.0813\n",
      "Epoch [1/2], Step [48510/64305], Loss: 4.9786\n",
      "Epoch [1/2], Step [48520/64305], Loss: 5.0423\n",
      "Epoch [1/2], Step [48530/64305], Loss: 4.9597\n",
      "Epoch [1/2], Step [48540/64305], Loss: 5.0668\n",
      "Epoch [1/2], Step [48550/64305], Loss: 4.7143\n",
      "Epoch [1/2], Step [48560/64305], Loss: 4.8765\n",
      "Epoch [1/2], Step [48570/64305], Loss: 4.8384\n",
      "Epoch [1/2], Step [48580/64305], Loss: 4.9126\n",
      "Epoch [1/2], Step [48590/64305], Loss: 4.9970\n",
      "Epoch [1/2], Step [48600/64305], Loss: 4.9634\n",
      "Epoch [1/2], Step [48610/64305], Loss: 4.6959\n",
      "Epoch [1/2], Step [48620/64305], Loss: 4.9834\n",
      "Epoch [1/2], Step [48630/64305], Loss: 5.0358\n",
      "Epoch [1/2], Step [48640/64305], Loss: 4.7896\n",
      "Epoch [1/2], Step [48650/64305], Loss: 4.8564\n",
      "Epoch [1/2], Step [48660/64305], Loss: 4.8940\n",
      "Epoch [1/2], Step [48670/64305], Loss: 5.0637\n",
      "Epoch [1/2], Step [48680/64305], Loss: 4.9128\n",
      "Epoch [1/2], Step [48690/64305], Loss: 5.1425\n",
      "Epoch [1/2], Step [48700/64305], Loss: 4.9781\n",
      "Epoch [1/2], Step [48710/64305], Loss: 4.7339\n",
      "Epoch [1/2], Step [48720/64305], Loss: 5.0697\n",
      "Epoch [1/2], Step [48730/64305], Loss: 5.1053\n",
      "Epoch [1/2], Step [48740/64305], Loss: 4.8242\n",
      "Epoch [1/2], Step [48750/64305], Loss: 4.8263\n",
      "Epoch [1/2], Step [48760/64305], Loss: 4.8785\n",
      "Epoch [1/2], Step [48770/64305], Loss: 4.9426\n",
      "Epoch [1/2], Step [48780/64305], Loss: 4.8374\n",
      "Epoch [1/2], Step [48790/64305], Loss: 4.9503\n",
      "Epoch [1/2], Step [48800/64305], Loss: 4.6295\n",
      "Epoch [1/2], Step [48810/64305], Loss: 5.1693\n",
      "Epoch [1/2], Step [48820/64305], Loss: 5.0185\n",
      "Epoch [1/2], Step [48830/64305], Loss: 4.8822\n",
      "Epoch [1/2], Step [48840/64305], Loss: 4.8854\n",
      "Epoch [1/2], Step [48850/64305], Loss: 5.0324\n",
      "Epoch [1/2], Step [48860/64305], Loss: 4.8180\n",
      "Epoch [1/2], Step [48870/64305], Loss: 4.9899\n",
      "Epoch [1/2], Step [48880/64305], Loss: 4.8767\n",
      "Epoch [1/2], Step [48890/64305], Loss: 4.7124\n",
      "Epoch [1/2], Step [48900/64305], Loss: 4.9297\n",
      "Epoch [1/2], Step [48910/64305], Loss: 4.9640\n",
      "Epoch [1/2], Step [48920/64305], Loss: 4.7252\n",
      "Epoch [1/2], Step [48930/64305], Loss: 4.7109\n",
      "Epoch [1/2], Step [48940/64305], Loss: 5.0359\n",
      "Epoch [1/2], Step [48950/64305], Loss: 4.8966\n",
      "Epoch [1/2], Step [48960/64305], Loss: 5.0032\n",
      "Epoch [1/2], Step [48970/64305], Loss: 4.8679\n",
      "Epoch [1/2], Step [48980/64305], Loss: 4.9362\n",
      "Epoch [1/2], Step [48990/64305], Loss: 5.0842\n",
      "Epoch [1/2], Step [49000/64305], Loss: 5.0372\n",
      "Epoch [1/2], Step [49010/64305], Loss: 5.2072\n",
      "Epoch [1/2], Step [49020/64305], Loss: 4.9213\n",
      "Epoch [1/2], Step [49030/64305], Loss: 5.0897\n",
      "Epoch [1/2], Step [49040/64305], Loss: 5.0061\n",
      "Epoch [1/2], Step [49050/64305], Loss: 4.9836\n",
      "Epoch [1/2], Step [49060/64305], Loss: 4.9542\n",
      "Epoch [1/2], Step [49070/64305], Loss: 4.8543\n",
      "Epoch [1/2], Step [49080/64305], Loss: 5.0499\n",
      "Epoch [1/2], Step [49090/64305], Loss: 4.8868\n",
      "Epoch [1/2], Step [49100/64305], Loss: 4.7950\n",
      "Epoch [1/2], Step [49110/64305], Loss: 5.1319\n",
      "Epoch [1/2], Step [49120/64305], Loss: 5.0375\n",
      "Epoch [1/2], Step [49130/64305], Loss: 5.1188\n",
      "Epoch [1/2], Step [49140/64305], Loss: 4.9054\n",
      "Epoch [1/2], Step [49150/64305], Loss: 4.8977\n",
      "Epoch [1/2], Step [49160/64305], Loss: 5.2116\n",
      "Epoch [1/2], Step [49170/64305], Loss: 5.0146\n",
      "Epoch [1/2], Step [49180/64305], Loss: 4.6430\n",
      "Epoch [1/2], Step [49190/64305], Loss: 4.8536\n",
      "Epoch [1/2], Step [49200/64305], Loss: 5.1020\n",
      "Epoch [1/2], Step [49210/64305], Loss: 4.9681\n",
      "Epoch [1/2], Step [49220/64305], Loss: 5.0077\n",
      "Epoch [1/2], Step [49230/64305], Loss: 5.1327\n",
      "Epoch [1/2], Step [49240/64305], Loss: 4.9526\n",
      "Epoch [1/2], Step [49250/64305], Loss: 4.8127\n",
      "Epoch [1/2], Step [49260/64305], Loss: 4.9356\n",
      "Epoch [1/2], Step [49270/64305], Loss: 4.9104\n",
      "Epoch [1/2], Step [49280/64305], Loss: 5.0929\n",
      "Epoch [1/2], Step [49290/64305], Loss: 4.9627\n",
      "Epoch [1/2], Step [49300/64305], Loss: 5.1594\n",
      "Epoch [1/2], Step [49310/64305], Loss: 4.8758\n",
      "Epoch [1/2], Step [49320/64305], Loss: 5.0228\n",
      "Epoch [1/2], Step [49330/64305], Loss: 4.9567\n",
      "Epoch [1/2], Step [49340/64305], Loss: 4.9415\n",
      "Epoch [1/2], Step [49350/64305], Loss: 4.8844\n",
      "Epoch [1/2], Step [49360/64305], Loss: 5.0697\n",
      "Epoch [1/2], Step [49370/64305], Loss: 4.9812\n",
      "Epoch [1/2], Step [49380/64305], Loss: 5.0884\n",
      "Epoch [1/2], Step [49390/64305], Loss: 4.8701\n",
      "Epoch [1/2], Step [49400/64305], Loss: 4.9800\n",
      "Epoch [1/2], Step [49410/64305], Loss: 4.7789\n",
      "Epoch [1/2], Step [49420/64305], Loss: 5.1282\n",
      "Epoch [1/2], Step [49430/64305], Loss: 5.1623\n",
      "Epoch [1/2], Step [49440/64305], Loss: 5.0084\n",
      "Epoch [1/2], Step [49450/64305], Loss: 5.0713\n",
      "Epoch [1/2], Step [49460/64305], Loss: 4.8478\n",
      "Epoch [1/2], Step [49470/64305], Loss: 4.7378\n",
      "Epoch [1/2], Step [49480/64305], Loss: 4.7722\n",
      "Epoch [1/2], Step [49490/64305], Loss: 4.9750\n",
      "Epoch [1/2], Step [49500/64305], Loss: 4.7196\n",
      "Epoch [1/2], Step [49510/64305], Loss: 4.8862\n",
      "Epoch [1/2], Step [49520/64305], Loss: 4.9757\n",
      "Epoch [1/2], Step [49530/64305], Loss: 4.8949\n",
      "Epoch [1/2], Step [49540/64305], Loss: 5.2810\n",
      "Epoch [1/2], Step [49550/64305], Loss: 4.8772\n",
      "Epoch [1/2], Step [49560/64305], Loss: 4.9865\n",
      "Epoch [1/2], Step [49570/64305], Loss: 4.7406\n",
      "Epoch [1/2], Step [49580/64305], Loss: 4.7276\n",
      "Epoch [1/2], Step [49590/64305], Loss: 5.2060\n",
      "Epoch [1/2], Step [49600/64305], Loss: 5.1595\n",
      "Epoch [1/2], Step [49610/64305], Loss: 5.0455\n",
      "Epoch [1/2], Step [49620/64305], Loss: 4.9292\n",
      "Epoch [1/2], Step [49630/64305], Loss: 5.0539\n",
      "Epoch [1/2], Step [49640/64305], Loss: 4.9567\n",
      "Epoch [1/2], Step [49650/64305], Loss: 5.0683\n",
      "Epoch [1/2], Step [49660/64305], Loss: 4.8149\n",
      "Epoch [1/2], Step [49670/64305], Loss: 4.9283\n",
      "Epoch [1/2], Step [49680/64305], Loss: 4.8914\n",
      "Epoch [1/2], Step [49690/64305], Loss: 5.1178\n",
      "Epoch [1/2], Step [49700/64305], Loss: 5.0317\n",
      "Epoch [1/2], Step [49710/64305], Loss: 4.9211\n",
      "Epoch [1/2], Step [49720/64305], Loss: 4.9678\n",
      "Epoch [1/2], Step [49730/64305], Loss: 4.9162\n",
      "Epoch [1/2], Step [49740/64305], Loss: 5.1609\n",
      "Epoch [1/2], Step [49750/64305], Loss: 4.7632\n",
      "Epoch [1/2], Step [49760/64305], Loss: 5.0520\n",
      "Epoch [1/2], Step [49770/64305], Loss: 5.0978\n",
      "Epoch [1/2], Step [49780/64305], Loss: 4.8564\n",
      "Epoch [1/2], Step [49790/64305], Loss: 4.9616\n",
      "Epoch [1/2], Step [49800/64305], Loss: 4.6547\n",
      "Epoch [1/2], Step [49810/64305], Loss: 4.8872\n",
      "Epoch [1/2], Step [49820/64305], Loss: 5.0172\n",
      "Epoch [1/2], Step [49830/64305], Loss: 4.9798\n",
      "Epoch [1/2], Step [49840/64305], Loss: 4.8945\n",
      "Epoch [1/2], Step [49850/64305], Loss: 4.7979\n",
      "Epoch [1/2], Step [49860/64305], Loss: 5.0478\n",
      "Epoch [1/2], Step [49870/64305], Loss: 4.9218\n",
      "Epoch [1/2], Step [49880/64305], Loss: 5.0515\n",
      "Epoch [1/2], Step [49890/64305], Loss: 4.9583\n",
      "Epoch [1/2], Step [49900/64305], Loss: 5.0207\n",
      "Epoch [1/2], Step [49910/64305], Loss: 4.6674\n",
      "Epoch [1/2], Step [49920/64305], Loss: 5.0421\n",
      "Epoch [1/2], Step [49930/64305], Loss: 5.1048\n",
      "Epoch [1/2], Step [49940/64305], Loss: 5.0413\n",
      "Epoch [1/2], Step [49950/64305], Loss: 4.8014\n",
      "Epoch [1/2], Step [49960/64305], Loss: 4.7287\n",
      "Epoch [1/2], Step [49970/64305], Loss: 5.0221\n",
      "Epoch [1/2], Step [49980/64305], Loss: 4.9226\n",
      "Epoch [1/2], Step [49990/64305], Loss: 4.8888\n",
      "Epoch [1/2], Step [50000/64305], Loss: 4.7133\n",
      "Epoch [1/2], Step [50010/64305], Loss: 5.0840\n",
      "Epoch [1/2], Step [50020/64305], Loss: 5.0819\n",
      "Epoch [1/2], Step [50030/64305], Loss: 4.9359\n",
      "Epoch [1/2], Step [50040/64305], Loss: 4.7086\n",
      "Epoch [1/2], Step [50050/64305], Loss: 5.0588\n",
      "Epoch [1/2], Step [50060/64305], Loss: 4.8526\n",
      "Epoch [1/2], Step [50070/64305], Loss: 4.8871\n",
      "Epoch [1/2], Step [50080/64305], Loss: 4.9090\n",
      "Epoch [1/2], Step [50090/64305], Loss: 4.9460\n",
      "Epoch [1/2], Step [50100/64305], Loss: 5.0177\n",
      "Epoch [1/2], Step [50110/64305], Loss: 4.8983\n",
      "Epoch [1/2], Step [50120/64305], Loss: 4.9217\n",
      "Epoch [1/2], Step [50130/64305], Loss: 4.9197\n",
      "Epoch [1/2], Step [50140/64305], Loss: 4.9959\n",
      "Epoch [1/2], Step [50150/64305], Loss: 4.8045\n",
      "Epoch [1/2], Step [50160/64305], Loss: 5.0311\n",
      "Epoch [1/2], Step [50170/64305], Loss: 4.9712\n",
      "Epoch [1/2], Step [50180/64305], Loss: 4.8252\n",
      "Epoch [1/2], Step [50190/64305], Loss: 4.7909\n",
      "Epoch [1/2], Step [50200/64305], Loss: 4.7475\n",
      "Epoch [1/2], Step [50210/64305], Loss: 4.7624\n",
      "Epoch [1/2], Step [50220/64305], Loss: 5.0826\n",
      "Epoch [1/2], Step [50230/64305], Loss: 5.0278\n",
      "Epoch [1/2], Step [50240/64305], Loss: 4.9938\n",
      "Epoch [1/2], Step [50250/64305], Loss: 4.8959\n",
      "Epoch [1/2], Step [50260/64305], Loss: 5.0560\n",
      "Epoch [1/2], Step [50270/64305], Loss: 4.7855\n",
      "Epoch [1/2], Step [50280/64305], Loss: 4.7677\n",
      "Epoch [1/2], Step [50290/64305], Loss: 4.9604\n",
      "Epoch [1/2], Step [50300/64305], Loss: 4.8456\n",
      "Epoch [1/2], Step [50310/64305], Loss: 4.9130\n",
      "Epoch [1/2], Step [50320/64305], Loss: 4.9965\n",
      "Epoch [1/2], Step [50330/64305], Loss: 5.0059\n",
      "Epoch [1/2], Step [50340/64305], Loss: 4.8741\n",
      "Epoch [1/2], Step [50350/64305], Loss: 4.8905\n",
      "Epoch [1/2], Step [50360/64305], Loss: 4.9106\n",
      "Epoch [1/2], Step [50370/64305], Loss: 5.0778\n",
      "Epoch [1/2], Step [50380/64305], Loss: 4.8929\n",
      "Epoch [1/2], Step [50390/64305], Loss: 4.7421\n",
      "Epoch [1/2], Step [50400/64305], Loss: 5.0647\n",
      "Epoch [1/2], Step [50410/64305], Loss: 4.9738\n",
      "Epoch [1/2], Step [50420/64305], Loss: 4.8260\n",
      "Epoch [1/2], Step [50430/64305], Loss: 4.8855\n",
      "Epoch [1/2], Step [50440/64305], Loss: 4.9895\n",
      "Epoch [1/2], Step [50450/64305], Loss: 4.9265\n",
      "Epoch [1/2], Step [50460/64305], Loss: 5.0287\n",
      "Epoch [1/2], Step [50470/64305], Loss: 4.9225\n",
      "Epoch [1/2], Step [50480/64305], Loss: 5.0966\n",
      "Epoch [1/2], Step [50490/64305], Loss: 4.9132\n",
      "Epoch [1/2], Step [50500/64305], Loss: 5.1907\n",
      "Epoch [1/2], Step [50510/64305], Loss: 4.7664\n",
      "Epoch [1/2], Step [50520/64305], Loss: 5.0105\n",
      "Epoch [1/2], Step [50530/64305], Loss: 4.9945\n",
      "Epoch [1/2], Step [50540/64305], Loss: 4.8456\n",
      "Epoch [1/2], Step [50550/64305], Loss: 4.9730\n",
      "Epoch [1/2], Step [50560/64305], Loss: 4.9595\n",
      "Epoch [1/2], Step [50570/64305], Loss: 4.9938\n",
      "Epoch [1/2], Step [50580/64305], Loss: 5.0853\n",
      "Epoch [1/2], Step [50590/64305], Loss: 4.9746\n",
      "Epoch [1/2], Step [50600/64305], Loss: 5.2222\n",
      "Epoch [1/2], Step [50610/64305], Loss: 4.9668\n",
      "Epoch [1/2], Step [50620/64305], Loss: 5.0157\n",
      "Epoch [1/2], Step [50630/64305], Loss: 5.0699\n",
      "Epoch [1/2], Step [50640/64305], Loss: 4.8137\n",
      "Epoch [1/2], Step [50650/64305], Loss: 4.8958\n",
      "Epoch [1/2], Step [50660/64305], Loss: 5.0418\n",
      "Epoch [1/2], Step [50670/64305], Loss: 4.9760\n",
      "Epoch [1/2], Step [50680/64305], Loss: 4.9543\n",
      "Epoch [1/2], Step [50690/64305], Loss: 5.0819\n",
      "Epoch [1/2], Step [50700/64305], Loss: 5.0930\n",
      "Epoch [1/2], Step [50710/64305], Loss: 4.9115\n",
      "Epoch [1/2], Step [50720/64305], Loss: 4.7890\n",
      "Epoch [1/2], Step [50730/64305], Loss: 4.7290\n",
      "Epoch [1/2], Step [50740/64305], Loss: 4.9343\n",
      "Epoch [1/2], Step [50750/64305], Loss: 4.9659\n",
      "Epoch [1/2], Step [50760/64305], Loss: 4.7709\n",
      "Epoch [1/2], Step [50770/64305], Loss: 4.8697\n",
      "Epoch [1/2], Step [50780/64305], Loss: 5.0222\n",
      "Epoch [1/2], Step [50790/64305], Loss: 4.7436\n",
      "Epoch [1/2], Step [50800/64305], Loss: 4.7250\n",
      "Epoch [1/2], Step [50810/64305], Loss: 5.0148\n",
      "Epoch [1/2], Step [50820/64305], Loss: 4.8015\n",
      "Epoch [1/2], Step [50830/64305], Loss: 4.9671\n",
      "Epoch [1/2], Step [50840/64305], Loss: 4.8463\n",
      "Epoch [1/2], Step [50850/64305], Loss: 5.0396\n",
      "Epoch [1/2], Step [50860/64305], Loss: 4.9734\n",
      "Epoch [1/2], Step [50870/64305], Loss: 4.7909\n",
      "Epoch [1/2], Step [50880/64305], Loss: 5.0129\n",
      "Epoch [1/2], Step [50890/64305], Loss: 4.6708\n",
      "Epoch [1/2], Step [50900/64305], Loss: 4.9362\n",
      "Epoch [1/2], Step [50910/64305], Loss: 5.1613\n",
      "Epoch [1/2], Step [50920/64305], Loss: 4.8999\n",
      "Epoch [1/2], Step [50930/64305], Loss: 4.9985\n",
      "Epoch [1/2], Step [50940/64305], Loss: 4.8430\n",
      "Epoch [1/2], Step [50950/64305], Loss: 5.0195\n",
      "Epoch [1/2], Step [50960/64305], Loss: 4.9805\n",
      "Epoch [1/2], Step [50970/64305], Loss: 4.8355\n",
      "Epoch [1/2], Step [50980/64305], Loss: 4.8688\n",
      "Epoch [1/2], Step [50990/64305], Loss: 5.0022\n",
      "Epoch [1/2], Step [51000/64305], Loss: 4.9080\n",
      "Epoch [1/2], Step [51010/64305], Loss: 4.7719\n",
      "Epoch [1/2], Step [51020/64305], Loss: 4.9940\n",
      "Epoch [1/2], Step [51030/64305], Loss: 5.0982\n",
      "Epoch [1/2], Step [51040/64305], Loss: 4.8679\n",
      "Epoch [1/2], Step [51050/64305], Loss: 4.9240\n",
      "Epoch [1/2], Step [51060/64305], Loss: 4.8091\n",
      "Epoch [1/2], Step [51070/64305], Loss: 4.9757\n",
      "Epoch [1/2], Step [51080/64305], Loss: 4.9658\n",
      "Epoch [1/2], Step [51090/64305], Loss: 4.9581\n",
      "Epoch [1/2], Step [51100/64305], Loss: 4.9550\n",
      "Epoch [1/2], Step [51110/64305], Loss: 4.8327\n",
      "Epoch [1/2], Step [51120/64305], Loss: 4.8338\n",
      "Epoch [1/2], Step [51130/64305], Loss: 5.0355\n",
      "Epoch [1/2], Step [51140/64305], Loss: 4.7732\n",
      "Epoch [1/2], Step [51150/64305], Loss: 4.9138\n",
      "Epoch [1/2], Step [51160/64305], Loss: 4.9113\n",
      "Epoch [1/2], Step [51170/64305], Loss: 5.3330\n",
      "Epoch [1/2], Step [51180/64305], Loss: 4.9305\n",
      "Epoch [1/2], Step [51190/64305], Loss: 4.8279\n",
      "Epoch [1/2], Step [51200/64305], Loss: 4.7448\n",
      "Epoch [1/2], Step [51210/64305], Loss: 5.0080\n",
      "Epoch [1/2], Step [51220/64305], Loss: 4.8983\n",
      "Epoch [1/2], Step [51230/64305], Loss: 5.1950\n",
      "Epoch [1/2], Step [51240/64305], Loss: 4.9501\n",
      "Epoch [1/2], Step [51250/64305], Loss: 5.0182\n",
      "Epoch [1/2], Step [51260/64305], Loss: 4.9374\n",
      "Epoch [1/2], Step [51270/64305], Loss: 4.9889\n",
      "Epoch [1/2], Step [51280/64305], Loss: 4.7654\n",
      "Epoch [1/2], Step [51290/64305], Loss: 4.9869\n",
      "Epoch [1/2], Step [51300/64305], Loss: 4.8480\n",
      "Epoch [1/2], Step [51310/64305], Loss: 4.8530\n",
      "Epoch [1/2], Step [51320/64305], Loss: 5.2073\n",
      "Epoch [1/2], Step [51330/64305], Loss: 4.9281\n",
      "Epoch [1/2], Step [51340/64305], Loss: 4.9582\n",
      "Epoch [1/2], Step [51350/64305], Loss: 5.0713\n",
      "Epoch [1/2], Step [51360/64305], Loss: 5.0251\n",
      "Epoch [1/2], Step [51370/64305], Loss: 4.9898\n",
      "Epoch [1/2], Step [51380/64305], Loss: 5.1024\n",
      "Epoch [1/2], Step [51390/64305], Loss: 4.9098\n",
      "Epoch [1/2], Step [51400/64305], Loss: 4.9240\n",
      "Epoch [1/2], Step [51410/64305], Loss: 5.0137\n",
      "Epoch [1/2], Step [51420/64305], Loss: 4.9096\n",
      "Epoch [1/2], Step [51430/64305], Loss: 5.2895\n",
      "Epoch [1/2], Step [51440/64305], Loss: 4.9871\n",
      "Epoch [1/2], Step [51450/64305], Loss: 5.0287\n",
      "Epoch [1/2], Step [51460/64305], Loss: 4.7826\n",
      "Epoch [1/2], Step [51470/64305], Loss: 4.7962\n",
      "Epoch [1/2], Step [51480/64305], Loss: 5.0342\n",
      "Epoch [1/2], Step [51490/64305], Loss: 4.8861\n",
      "Epoch [1/2], Step [51500/64305], Loss: 5.1519\n",
      "Epoch [1/2], Step [51510/64305], Loss: 4.8659\n",
      "Epoch [1/2], Step [51520/64305], Loss: 4.9094\n",
      "Epoch [1/2], Step [51530/64305], Loss: 5.0304\n",
      "Epoch [1/2], Step [51540/64305], Loss: 4.8551\n",
      "Epoch [1/2], Step [51550/64305], Loss: 4.8384\n",
      "Epoch [1/2], Step [51560/64305], Loss: 5.0953\n",
      "Epoch [1/2], Step [51570/64305], Loss: 5.2026\n",
      "Epoch [1/2], Step [51580/64305], Loss: 4.7295\n",
      "Epoch [1/2], Step [51590/64305], Loss: 4.7758\n",
      "Epoch [1/2], Step [51600/64305], Loss: 4.9003\n",
      "Epoch [1/2], Step [51610/64305], Loss: 4.8806\n",
      "Epoch [1/2], Step [51620/64305], Loss: 5.0825\n",
      "Epoch [1/2], Step [51630/64305], Loss: 4.8037\n",
      "Epoch [1/2], Step [51640/64305], Loss: 4.9113\n",
      "Epoch [1/2], Step [51650/64305], Loss: 4.7335\n",
      "Epoch [1/2], Step [51660/64305], Loss: 4.9354\n",
      "Epoch [1/2], Step [51670/64305], Loss: 4.8714\n",
      "Epoch [1/2], Step [51680/64305], Loss: 5.0405\n",
      "Epoch [1/2], Step [51690/64305], Loss: 4.7491\n",
      "Epoch [1/2], Step [51700/64305], Loss: 4.7281\n",
      "Epoch [1/2], Step [51710/64305], Loss: 4.9096\n",
      "Epoch [1/2], Step [51720/64305], Loss: 4.5978\n",
      "Epoch [1/2], Step [51730/64305], Loss: 4.5858\n",
      "Epoch [1/2], Step [51740/64305], Loss: 4.8549\n",
      "Epoch [1/2], Step [51750/64305], Loss: 4.8429\n",
      "Epoch [1/2], Step [51760/64305], Loss: 4.7760\n",
      "Epoch [1/2], Step [51770/64305], Loss: 4.6758\n",
      "Epoch [1/2], Step [51780/64305], Loss: 4.9861\n",
      "Epoch [1/2], Step [51790/64305], Loss: 5.0243\n",
      "Epoch [1/2], Step [51800/64305], Loss: 4.8265\n",
      "Epoch [1/2], Step [51810/64305], Loss: 4.9186\n",
      "Epoch [1/2], Step [51820/64305], Loss: 4.7937\n",
      "Epoch [1/2], Step [51830/64305], Loss: 5.0408\n",
      "Epoch [1/2], Step [51840/64305], Loss: 4.8932\n",
      "Epoch [1/2], Step [51850/64305], Loss: 5.1031\n",
      "Epoch [1/2], Step [51860/64305], Loss: 4.9607\n",
      "Epoch [1/2], Step [51870/64305], Loss: 4.9190\n",
      "Epoch [1/2], Step [51880/64305], Loss: 5.0193\n",
      "Epoch [1/2], Step [51890/64305], Loss: 5.0570\n",
      "Epoch [1/2], Step [51900/64305], Loss: 5.2338\n",
      "Epoch [1/2], Step [51910/64305], Loss: 4.8345\n",
      "Epoch [1/2], Step [51920/64305], Loss: 4.8103\n",
      "Epoch [1/2], Step [51930/64305], Loss: 4.8896\n",
      "Epoch [1/2], Step [51940/64305], Loss: 4.9050\n",
      "Epoch [1/2], Step [51950/64305], Loss: 4.8946\n",
      "Epoch [1/2], Step [51960/64305], Loss: 5.0644\n",
      "Epoch [1/2], Step [51970/64305], Loss: 4.8961\n",
      "Epoch [1/2], Step [51980/64305], Loss: 4.8584\n",
      "Epoch [1/2], Step [51990/64305], Loss: 5.0713\n",
      "Epoch [1/2], Step [52000/64305], Loss: 4.8981\n",
      "Epoch [1/2], Step [52010/64305], Loss: 5.0169\n",
      "Epoch [1/2], Step [52020/64305], Loss: 5.0288\n",
      "Epoch [1/2], Step [52030/64305], Loss: 4.8868\n",
      "Epoch [1/2], Step [52040/64305], Loss: 4.9325\n",
      "Epoch [1/2], Step [52050/64305], Loss: 5.0458\n",
      "Epoch [1/2], Step [52060/64305], Loss: 4.7794\n",
      "Epoch [1/2], Step [52070/64305], Loss: 5.1990\n",
      "Epoch [1/2], Step [52080/64305], Loss: 4.6051\n",
      "Epoch [1/2], Step [52090/64305], Loss: 5.0352\n",
      "Epoch [1/2], Step [52100/64305], Loss: 4.7824\n",
      "Epoch [1/2], Step [52110/64305], Loss: 4.9563\n",
      "Epoch [1/2], Step [52120/64305], Loss: 4.9304\n",
      "Epoch [1/2], Step [52130/64305], Loss: 4.6749\n",
      "Epoch [1/2], Step [52140/64305], Loss: 4.9611\n",
      "Epoch [1/2], Step [52150/64305], Loss: 4.8513\n",
      "Epoch [1/2], Step [52160/64305], Loss: 5.0687\n",
      "Epoch [1/2], Step [52170/64305], Loss: 5.1361\n",
      "Epoch [1/2], Step [52180/64305], Loss: 4.9371\n",
      "Epoch [1/2], Step [52190/64305], Loss: 5.0029\n",
      "Epoch [1/2], Step [52200/64305], Loss: 4.9716\n",
      "Epoch [1/2], Step [52210/64305], Loss: 4.9266\n",
      "Epoch [1/2], Step [52220/64305], Loss: 5.0405\n",
      "Epoch [1/2], Step [52230/64305], Loss: 4.9883\n",
      "Epoch [1/2], Step [52240/64305], Loss: 4.8982\n",
      "Epoch [1/2], Step [52250/64305], Loss: 4.7583\n",
      "Epoch [1/2], Step [52260/64305], Loss: 4.9563\n",
      "Epoch [1/2], Step [52270/64305], Loss: 4.8766\n",
      "Epoch [1/2], Step [52280/64305], Loss: 4.9485\n",
      "Epoch [1/2], Step [52290/64305], Loss: 5.1932\n",
      "Epoch [1/2], Step [52300/64305], Loss: 4.8375\n",
      "Epoch [1/2], Step [52310/64305], Loss: 4.9247\n",
      "Epoch [1/2], Step [52320/64305], Loss: 4.7914\n",
      "Epoch [1/2], Step [52330/64305], Loss: 5.0951\n",
      "Epoch [1/2], Step [52340/64305], Loss: 4.9519\n",
      "Epoch [1/2], Step [52350/64305], Loss: 4.7695\n",
      "Epoch [1/2], Step [52360/64305], Loss: 4.9663\n",
      "Epoch [1/2], Step [52370/64305], Loss: 4.9008\n",
      "Epoch [1/2], Step [52380/64305], Loss: 4.7985\n",
      "Epoch [1/2], Step [52390/64305], Loss: 4.8280\n",
      "Epoch [1/2], Step [52400/64305], Loss: 5.0028\n",
      "Epoch [1/2], Step [52410/64305], Loss: 4.9227\n",
      "Epoch [1/2], Step [52420/64305], Loss: 4.6280\n",
      "Epoch [1/2], Step [52430/64305], Loss: 4.7835\n",
      "Epoch [1/2], Step [52440/64305], Loss: 4.9305\n",
      "Epoch [1/2], Step [52450/64305], Loss: 4.7420\n",
      "Epoch [1/2], Step [52460/64305], Loss: 4.8807\n",
      "Epoch [1/2], Step [52470/64305], Loss: 4.8709\n",
      "Epoch [1/2], Step [52480/64305], Loss: 4.8947\n",
      "Epoch [1/2], Step [52490/64305], Loss: 5.1365\n",
      "Epoch [1/2], Step [52500/64305], Loss: 4.8890\n",
      "Epoch [1/2], Step [52510/64305], Loss: 4.8139\n",
      "Epoch [1/2], Step [52520/64305], Loss: 5.0211\n",
      "Epoch [1/2], Step [52530/64305], Loss: 4.8972\n",
      "Epoch [1/2], Step [52540/64305], Loss: 5.1262\n",
      "Epoch [1/2], Step [52550/64305], Loss: 4.9684\n",
      "Epoch [1/2], Step [52560/64305], Loss: 5.0726\n",
      "Epoch [1/2], Step [52570/64305], Loss: 4.8783\n",
      "Epoch [1/2], Step [52580/64305], Loss: 5.0575\n",
      "Epoch [1/2], Step [52590/64305], Loss: 5.0108\n",
      "Epoch [1/2], Step [52600/64305], Loss: 4.6624\n",
      "Epoch [1/2], Step [52610/64305], Loss: 5.0197\n",
      "Epoch [1/2], Step [52620/64305], Loss: 4.9560\n",
      "Epoch [1/2], Step [52630/64305], Loss: 4.9980\n",
      "Epoch [1/2], Step [52640/64305], Loss: 4.8478\n",
      "Epoch [1/2], Step [52650/64305], Loss: 4.8271\n",
      "Epoch [1/2], Step [52660/64305], Loss: 5.0351\n",
      "Epoch [1/2], Step [52670/64305], Loss: 5.0432\n",
      "Epoch [1/2], Step [52680/64305], Loss: 4.9189\n",
      "Epoch [1/2], Step [52690/64305], Loss: 4.8487\n",
      "Epoch [1/2], Step [52700/64305], Loss: 4.9367\n",
      "Epoch [1/2], Step [52710/64305], Loss: 4.8907\n",
      "Epoch [1/2], Step [52720/64305], Loss: 4.9716\n",
      "Epoch [1/2], Step [52730/64305], Loss: 4.7545\n",
      "Epoch [1/2], Step [52740/64305], Loss: 5.1382\n",
      "Epoch [1/2], Step [52750/64305], Loss: 4.9731\n",
      "Epoch [1/2], Step [52760/64305], Loss: 4.9435\n",
      "Epoch [1/2], Step [52770/64305], Loss: 4.7429\n",
      "Epoch [1/2], Step [52780/64305], Loss: 5.0037\n",
      "Epoch [1/2], Step [52790/64305], Loss: 5.1379\n",
      "Epoch [1/2], Step [52800/64305], Loss: 4.9438\n",
      "Epoch [1/2], Step [52810/64305], Loss: 4.8443\n",
      "Epoch [1/2], Step [52820/64305], Loss: 4.9829\n",
      "Epoch [1/2], Step [52830/64305], Loss: 5.0581\n",
      "Epoch [1/2], Step [52840/64305], Loss: 5.0226\n",
      "Epoch [1/2], Step [52850/64305], Loss: 5.0082\n",
      "Epoch [1/2], Step [52860/64305], Loss: 4.6645\n",
      "Epoch [1/2], Step [52870/64305], Loss: 4.9685\n",
      "Epoch [1/2], Step [52880/64305], Loss: 5.0468\n",
      "Epoch [1/2], Step [52890/64305], Loss: 4.9459\n",
      "Epoch [1/2], Step [52900/64305], Loss: 4.8201\n",
      "Epoch [1/2], Step [52910/64305], Loss: 4.8208\n",
      "Epoch [1/2], Step [52920/64305], Loss: 4.8847\n",
      "Epoch [1/2], Step [52930/64305], Loss: 4.9970\n",
      "Epoch [1/2], Step [52940/64305], Loss: 4.8820\n",
      "Epoch [1/2], Step [52950/64305], Loss: 4.8888\n",
      "Epoch [1/2], Step [52960/64305], Loss: 4.9192\n",
      "Epoch [1/2], Step [52970/64305], Loss: 4.9263\n",
      "Epoch [1/2], Step [52980/64305], Loss: 4.8915\n",
      "Epoch [1/2], Step [52990/64305], Loss: 4.7842\n",
      "Epoch [1/2], Step [53000/64305], Loss: 4.9262\n",
      "Epoch [1/2], Step [53010/64305], Loss: 4.8284\n",
      "Epoch [1/2], Step [53020/64305], Loss: 4.9486\n",
      "Epoch [1/2], Step [53030/64305], Loss: 4.9485\n",
      "Epoch [1/2], Step [53040/64305], Loss: 4.8182\n",
      "Epoch [1/2], Step [53050/64305], Loss: 5.0112\n",
      "Epoch [1/2], Step [53060/64305], Loss: 5.0853\n",
      "Epoch [1/2], Step [53070/64305], Loss: 4.9337\n",
      "Epoch [1/2], Step [53080/64305], Loss: 4.8993\n",
      "Epoch [1/2], Step [53090/64305], Loss: 5.0425\n",
      "Epoch [1/2], Step [53100/64305], Loss: 4.8213\n",
      "Epoch [1/2], Step [53110/64305], Loss: 4.9344\n",
      "Epoch [1/2], Step [53120/64305], Loss: 4.9609\n",
      "Epoch [1/2], Step [53130/64305], Loss: 4.8871\n",
      "Epoch [1/2], Step [53140/64305], Loss: 4.9450\n",
      "Epoch [1/2], Step [53150/64305], Loss: 5.0328\n",
      "Epoch [1/2], Step [53160/64305], Loss: 4.8427\n",
      "Epoch [1/2], Step [53170/64305], Loss: 4.9292\n",
      "Epoch [1/2], Step [53180/64305], Loss: 5.1368\n",
      "Epoch [1/2], Step [53190/64305], Loss: 5.0422\n",
      "Epoch [1/2], Step [53200/64305], Loss: 4.9258\n",
      "Epoch [1/2], Step [53210/64305], Loss: 5.2103\n",
      "Epoch [1/2], Step [53220/64305], Loss: 4.7894\n",
      "Epoch [1/2], Step [53230/64305], Loss: 5.1549\n",
      "Epoch [1/2], Step [53240/64305], Loss: 4.8923\n",
      "Epoch [1/2], Step [53250/64305], Loss: 4.9404\n",
      "Epoch [1/2], Step [53260/64305], Loss: 4.8432\n",
      "Epoch [1/2], Step [53270/64305], Loss: 4.9346\n",
      "Epoch [1/2], Step [53280/64305], Loss: 4.8827\n",
      "Epoch [1/2], Step [53290/64305], Loss: 4.9326\n",
      "Epoch [1/2], Step [53300/64305], Loss: 4.9900\n",
      "Epoch [1/2], Step [53310/64305], Loss: 4.8781\n",
      "Epoch [1/2], Step [53320/64305], Loss: 4.8921\n",
      "Epoch [1/2], Step [53330/64305], Loss: 4.9214\n",
      "Epoch [1/2], Step [53340/64305], Loss: 4.8962\n",
      "Epoch [1/2], Step [53350/64305], Loss: 4.9620\n",
      "Epoch [1/2], Step [53360/64305], Loss: 4.8891\n",
      "Epoch [1/2], Step [53370/64305], Loss: 4.8824\n",
      "Epoch [1/2], Step [53380/64305], Loss: 4.8634\n",
      "Epoch [1/2], Step [53390/64305], Loss: 4.8845\n",
      "Epoch [1/2], Step [53400/64305], Loss: 4.8720\n",
      "Epoch [1/2], Step [53410/64305], Loss: 4.7842\n",
      "Epoch [1/2], Step [53420/64305], Loss: 5.1632\n",
      "Epoch [1/2], Step [53430/64305], Loss: 4.8881\n",
      "Epoch [1/2], Step [53440/64305], Loss: 4.8249\n",
      "Epoch [1/2], Step [53450/64305], Loss: 4.7465\n",
      "Epoch [1/2], Step [53460/64305], Loss: 4.9500\n",
      "Epoch [1/2], Step [53470/64305], Loss: 4.6195\n",
      "Epoch [1/2], Step [53480/64305], Loss: 4.7714\n",
      "Epoch [1/2], Step [53490/64305], Loss: 4.9200\n",
      "Epoch [1/2], Step [53500/64305], Loss: 5.0314\n",
      "Epoch [1/2], Step [53510/64305], Loss: 4.9647\n",
      "Epoch [1/2], Step [53520/64305], Loss: 4.9998\n",
      "Epoch [1/2], Step [53530/64305], Loss: 5.2179\n",
      "Epoch [1/2], Step [53540/64305], Loss: 4.7533\n",
      "Epoch [1/2], Step [53550/64305], Loss: 5.1251\n",
      "Epoch [1/2], Step [53560/64305], Loss: 4.9939\n",
      "Epoch [1/2], Step [53570/64305], Loss: 4.8662\n",
      "Epoch [1/2], Step [53580/64305], Loss: 4.7997\n",
      "Epoch [1/2], Step [53590/64305], Loss: 4.8880\n",
      "Epoch [1/2], Step [53600/64305], Loss: 5.0127\n",
      "Epoch [1/2], Step [53610/64305], Loss: 4.8794\n",
      "Epoch [1/2], Step [53620/64305], Loss: 5.0437\n",
      "Epoch [1/2], Step [53630/64305], Loss: 4.9272\n",
      "Epoch [1/2], Step [53640/64305], Loss: 5.0252\n",
      "Epoch [1/2], Step [53650/64305], Loss: 5.0764\n",
      "Epoch [1/2], Step [53660/64305], Loss: 4.8921\n",
      "Epoch [1/2], Step [53670/64305], Loss: 4.8307\n",
      "Epoch [1/2], Step [53680/64305], Loss: 4.7756\n",
      "Epoch [1/2], Step [53690/64305], Loss: 5.0054\n",
      "Epoch [1/2], Step [53700/64305], Loss: 4.8857\n",
      "Epoch [1/2], Step [53710/64305], Loss: 4.8259\n",
      "Epoch [1/2], Step [53720/64305], Loss: 4.8155\n",
      "Epoch [1/2], Step [53730/64305], Loss: 5.0320\n",
      "Epoch [1/2], Step [53740/64305], Loss: 5.0730\n",
      "Epoch [1/2], Step [53750/64305], Loss: 4.9305\n",
      "Epoch [1/2], Step [53760/64305], Loss: 5.0980\n",
      "Epoch [1/2], Step [53770/64305], Loss: 5.2495\n",
      "Epoch [1/2], Step [53780/64305], Loss: 5.1608\n",
      "Epoch [1/2], Step [53790/64305], Loss: 4.6986\n",
      "Epoch [1/2], Step [53800/64305], Loss: 4.9466\n",
      "Epoch [1/2], Step [53810/64305], Loss: 4.7497\n",
      "Epoch [1/2], Step [53820/64305], Loss: 5.0778\n",
      "Epoch [1/2], Step [53830/64305], Loss: 5.0602\n",
      "Epoch [1/2], Step [53840/64305], Loss: 4.7546\n",
      "Epoch [1/2], Step [53850/64305], Loss: 4.7675\n",
      "Epoch [1/2], Step [53860/64305], Loss: 4.8255\n",
      "Epoch [1/2], Step [53870/64305], Loss: 4.6901\n",
      "Epoch [1/2], Step [53880/64305], Loss: 4.9441\n",
      "Epoch [1/2], Step [53890/64305], Loss: 4.9454\n",
      "Epoch [1/2], Step [53900/64305], Loss: 5.1548\n",
      "Epoch [1/2], Step [53910/64305], Loss: 5.0533\n",
      "Epoch [1/2], Step [53920/64305], Loss: 4.9464\n",
      "Epoch [1/2], Step [53930/64305], Loss: 4.9413\n",
      "Epoch [1/2], Step [53940/64305], Loss: 4.9274\n",
      "Epoch [1/2], Step [53950/64305], Loss: 5.0178\n",
      "Epoch [1/2], Step [53960/64305], Loss: 4.8964\n",
      "Epoch [1/2], Step [53970/64305], Loss: 4.7623\n",
      "Epoch [1/2], Step [53980/64305], Loss: 4.9377\n",
      "Epoch [1/2], Step [53990/64305], Loss: 4.9257\n",
      "Epoch [1/2], Step [54000/64305], Loss: 5.1343\n",
      "Epoch [1/2], Step [54010/64305], Loss: 4.9220\n",
      "Epoch [1/2], Step [54020/64305], Loss: 5.0704\n",
      "Epoch [1/2], Step [54030/64305], Loss: 5.1239\n",
      "Epoch [1/2], Step [54040/64305], Loss: 4.8633\n",
      "Epoch [1/2], Step [54050/64305], Loss: 4.8655\n",
      "Epoch [1/2], Step [54060/64305], Loss: 5.0153\n",
      "Epoch [1/2], Step [54070/64305], Loss: 4.9124\n",
      "Epoch [1/2], Step [54080/64305], Loss: 4.9185\n",
      "Epoch [1/2], Step [54090/64305], Loss: 4.9954\n",
      "Epoch [1/2], Step [54100/64305], Loss: 5.0311\n",
      "Epoch [1/2], Step [54110/64305], Loss: 4.8702\n",
      "Epoch [1/2], Step [54120/64305], Loss: 4.8975\n",
      "Epoch [1/2], Step [54130/64305], Loss: 4.9547\n",
      "Epoch [1/2], Step [54140/64305], Loss: 4.8037\n",
      "Epoch [1/2], Step [54150/64305], Loss: 4.9131\n",
      "Epoch [1/2], Step [54160/64305], Loss: 4.9369\n",
      "Epoch [1/2], Step [54170/64305], Loss: 4.9169\n",
      "Epoch [1/2], Step [54180/64305], Loss: 4.9913\n",
      "Epoch [1/2], Step [54190/64305], Loss: 4.8810\n",
      "Epoch [1/2], Step [54200/64305], Loss: 5.0128\n",
      "Epoch [1/2], Step [54210/64305], Loss: 4.9832\n",
      "Epoch [1/2], Step [54220/64305], Loss: 4.9181\n",
      "Epoch [1/2], Step [54230/64305], Loss: 4.9429\n",
      "Epoch [1/2], Step [54240/64305], Loss: 5.0655\n",
      "Epoch [1/2], Step [54250/64305], Loss: 4.9923\n",
      "Epoch [1/2], Step [54260/64305], Loss: 4.8294\n",
      "Epoch [1/2], Step [54270/64305], Loss: 5.0040\n",
      "Epoch [1/2], Step [54280/64305], Loss: 4.9427\n",
      "Epoch [1/2], Step [54290/64305], Loss: 4.8804\n",
      "Epoch [1/2], Step [54300/64305], Loss: 5.0083\n",
      "Epoch [1/2], Step [54310/64305], Loss: 4.9283\n",
      "Epoch [1/2], Step [54320/64305], Loss: 4.9350\n",
      "Epoch [1/2], Step [54330/64305], Loss: 4.8549\n",
      "Epoch [1/2], Step [54340/64305], Loss: 4.8000\n",
      "Epoch [1/2], Step [54350/64305], Loss: 5.0971\n",
      "Epoch [1/2], Step [54360/64305], Loss: 5.0964\n",
      "Epoch [1/2], Step [54370/64305], Loss: 4.7916\n",
      "Epoch [1/2], Step [54380/64305], Loss: 5.1790\n",
      "Epoch [1/2], Step [54390/64305], Loss: 4.8442\n",
      "Epoch [1/2], Step [54400/64305], Loss: 5.0859\n",
      "Epoch [1/2], Step [54410/64305], Loss: 4.9932\n",
      "Epoch [1/2], Step [54420/64305], Loss: 5.1706\n",
      "Epoch [1/2], Step [54430/64305], Loss: 4.8906\n",
      "Epoch [1/2], Step [54440/64305], Loss: 4.9936\n",
      "Epoch [1/2], Step [54450/64305], Loss: 4.8556\n",
      "Epoch [1/2], Step [54460/64305], Loss: 4.7516\n",
      "Epoch [1/2], Step [54470/64305], Loss: 4.9865\n",
      "Epoch [1/2], Step [54480/64305], Loss: 4.9210\n",
      "Epoch [1/2], Step [54490/64305], Loss: 4.8464\n",
      "Epoch [1/2], Step [54500/64305], Loss: 4.8487\n",
      "Epoch [1/2], Step [54510/64305], Loss: 5.0737\n",
      "Epoch [1/2], Step [54520/64305], Loss: 5.0587\n",
      "Epoch [1/2], Step [54530/64305], Loss: 4.7112\n",
      "Epoch [1/2], Step [54540/64305], Loss: 4.8180\n",
      "Epoch [1/2], Step [54550/64305], Loss: 4.8508\n",
      "Epoch [1/2], Step [54560/64305], Loss: 4.8035\n",
      "Epoch [1/2], Step [54570/64305], Loss: 5.0947\n",
      "Epoch [1/2], Step [54580/64305], Loss: 4.8597\n",
      "Epoch [1/2], Step [54590/64305], Loss: 4.9571\n",
      "Epoch [1/2], Step [54600/64305], Loss: 4.9346\n",
      "Epoch [1/2], Step [54610/64305], Loss: 5.0590\n",
      "Epoch [1/2], Step [54620/64305], Loss: 5.0337\n",
      "Epoch [1/2], Step [54630/64305], Loss: 4.8518\n",
      "Epoch [1/2], Step [54640/64305], Loss: 5.0899\n",
      "Epoch [1/2], Step [54650/64305], Loss: 4.7166\n",
      "Epoch [1/2], Step [54660/64305], Loss: 5.0969\n",
      "Epoch [1/2], Step [54670/64305], Loss: 4.9248\n",
      "Epoch [1/2], Step [54680/64305], Loss: 4.9507\n",
      "Epoch [1/2], Step [54690/64305], Loss: 4.8322\n",
      "Epoch [1/2], Step [54700/64305], Loss: 4.9079\n",
      "Epoch [1/2], Step [54710/64305], Loss: 4.9262\n",
      "Epoch [1/2], Step [54720/64305], Loss: 4.9659\n",
      "Epoch [1/2], Step [54730/64305], Loss: 4.7435\n",
      "Epoch [1/2], Step [54740/64305], Loss: 5.0281\n",
      "Epoch [1/2], Step [54750/64305], Loss: 4.7619\n",
      "Epoch [1/2], Step [54760/64305], Loss: 4.9328\n",
      "Epoch [1/2], Step [54770/64305], Loss: 4.8528\n",
      "Epoch [1/2], Step [54780/64305], Loss: 4.7392\n",
      "Epoch [1/2], Step [54790/64305], Loss: 5.1890\n",
      "Epoch [1/2], Step [54800/64305], Loss: 4.9367\n",
      "Epoch [1/2], Step [54810/64305], Loss: 4.7126\n",
      "Epoch [1/2], Step [54820/64305], Loss: 5.0147\n",
      "Epoch [1/2], Step [54830/64305], Loss: 4.9073\n",
      "Epoch [1/2], Step [54840/64305], Loss: 4.8939\n",
      "Epoch [1/2], Step [54850/64305], Loss: 4.8671\n",
      "Epoch [1/2], Step [54860/64305], Loss: 4.8596\n",
      "Epoch [1/2], Step [54870/64305], Loss: 5.3253\n",
      "Epoch [1/2], Step [54880/64305], Loss: 4.6524\n",
      "Epoch [1/2], Step [54890/64305], Loss: 4.8809\n",
      "Epoch [1/2], Step [54900/64305], Loss: 4.7868\n",
      "Epoch [1/2], Step [54910/64305], Loss: 5.1125\n",
      "Epoch [1/2], Step [54920/64305], Loss: 4.8821\n",
      "Epoch [1/2], Step [54930/64305], Loss: 4.9138\n",
      "Epoch [1/2], Step [54940/64305], Loss: 4.9842\n",
      "Epoch [1/2], Step [54950/64305], Loss: 4.9602\n",
      "Epoch [1/2], Step [54960/64305], Loss: 4.8284\n",
      "Epoch [1/2], Step [54970/64305], Loss: 5.0388\n",
      "Epoch [1/2], Step [54980/64305], Loss: 4.9486\n",
      "Epoch [1/2], Step [54990/64305], Loss: 5.0184\n",
      "Epoch [1/2], Step [55000/64305], Loss: 5.0374\n",
      "Epoch [1/2], Step [55010/64305], Loss: 5.0186\n",
      "Epoch [1/2], Step [55020/64305], Loss: 5.0710\n",
      "Epoch [1/2], Step [55030/64305], Loss: 5.2544\n",
      "Epoch [1/2], Step [55040/64305], Loss: 4.9228\n",
      "Epoch [1/2], Step [55050/64305], Loss: 5.0677\n",
      "Epoch [1/2], Step [55060/64305], Loss: 4.9798\n",
      "Epoch [1/2], Step [55070/64305], Loss: 4.9619\n",
      "Epoch [1/2], Step [55080/64305], Loss: 5.0594\n",
      "Epoch [1/2], Step [55090/64305], Loss: 4.8220\n",
      "Epoch [1/2], Step [55100/64305], Loss: 4.7315\n",
      "Epoch [1/2], Step [55110/64305], Loss: 5.0749\n",
      "Epoch [1/2], Step [55120/64305], Loss: 4.9105\n",
      "Epoch [1/2], Step [55130/64305], Loss: 5.0820\n",
      "Epoch [1/2], Step [55140/64305], Loss: 4.9420\n",
      "Epoch [1/2], Step [55150/64305], Loss: 4.9529\n",
      "Epoch [1/2], Step [55160/64305], Loss: 4.9465\n",
      "Epoch [1/2], Step [55170/64305], Loss: 4.7736\n",
      "Epoch [1/2], Step [55180/64305], Loss: 4.6679\n",
      "Epoch [1/2], Step [55190/64305], Loss: 4.9762\n",
      "Epoch [1/2], Step [55200/64305], Loss: 4.8944\n",
      "Epoch [1/2], Step [55210/64305], Loss: 4.9770\n",
      "Epoch [1/2], Step [55220/64305], Loss: 4.8648\n",
      "Epoch [1/2], Step [55230/64305], Loss: 4.8502\n",
      "Epoch [1/2], Step [55240/64305], Loss: 5.0946\n",
      "Epoch [1/2], Step [55250/64305], Loss: 4.7827\n",
      "Epoch [1/2], Step [55260/64305], Loss: 4.9233\n",
      "Epoch [1/2], Step [55270/64305], Loss: 4.8425\n",
      "Epoch [1/2], Step [55280/64305], Loss: 5.0031\n",
      "Epoch [1/2], Step [55290/64305], Loss: 4.9332\n",
      "Epoch [1/2], Step [55300/64305], Loss: 4.5633\n",
      "Epoch [1/2], Step [55310/64305], Loss: 4.9534\n",
      "Epoch [1/2], Step [55320/64305], Loss: 5.0167\n",
      "Epoch [1/2], Step [55330/64305], Loss: 4.9563\n",
      "Epoch [1/2], Step [55340/64305], Loss: 4.8840\n",
      "Epoch [1/2], Step [55350/64305], Loss: 4.8717\n",
      "Epoch [1/2], Step [55360/64305], Loss: 4.9473\n",
      "Epoch [1/2], Step [55370/64305], Loss: 5.0330\n",
      "Epoch [1/2], Step [55380/64305], Loss: 4.9588\n",
      "Epoch [1/2], Step [55390/64305], Loss: 4.9659\n",
      "Epoch [1/2], Step [55400/64305], Loss: 5.1133\n",
      "Epoch [1/2], Step [55410/64305], Loss: 4.9133\n",
      "Epoch [1/2], Step [55420/64305], Loss: 4.9327\n",
      "Epoch [1/2], Step [55430/64305], Loss: 4.8053\n",
      "Epoch [1/2], Step [55440/64305], Loss: 4.9707\n",
      "Epoch [1/2], Step [55450/64305], Loss: 4.8801\n",
      "Epoch [1/2], Step [55460/64305], Loss: 4.8651\n",
      "Epoch [1/2], Step [55470/64305], Loss: 5.0169\n",
      "Epoch [1/2], Step [55480/64305], Loss: 5.1983\n",
      "Epoch [1/2], Step [55490/64305], Loss: 4.9630\n",
      "Epoch [1/2], Step [55500/64305], Loss: 4.9308\n",
      "Epoch [1/2], Step [55510/64305], Loss: 4.9522\n",
      "Epoch [1/2], Step [55520/64305], Loss: 4.8771\n",
      "Epoch [1/2], Step [55530/64305], Loss: 4.8228\n",
      "Epoch [1/2], Step [55540/64305], Loss: 5.1487\n",
      "Epoch [1/2], Step [55550/64305], Loss: 4.8883\n",
      "Epoch [1/2], Step [55560/64305], Loss: 4.8455\n",
      "Epoch [1/2], Step [55570/64305], Loss: 5.0194\n",
      "Epoch [1/2], Step [55580/64305], Loss: 4.8846\n",
      "Epoch [1/2], Step [55590/64305], Loss: 4.7902\n",
      "Epoch [1/2], Step [55600/64305], Loss: 4.9142\n",
      "Epoch [1/2], Step [55610/64305], Loss: 4.8574\n",
      "Epoch [1/2], Step [55620/64305], Loss: 4.8940\n",
      "Epoch [1/2], Step [55630/64305], Loss: 4.8423\n",
      "Epoch [1/2], Step [55640/64305], Loss: 4.9644\n",
      "Epoch [1/2], Step [55650/64305], Loss: 4.9463\n",
      "Epoch [1/2], Step [55660/64305], Loss: 4.8659\n",
      "Epoch [1/2], Step [55670/64305], Loss: 4.9208\n",
      "Epoch [1/2], Step [55680/64305], Loss: 4.9301\n",
      "Epoch [1/2], Step [55690/64305], Loss: 4.9204\n",
      "Epoch [1/2], Step [55700/64305], Loss: 4.6901\n",
      "Epoch [1/2], Step [55710/64305], Loss: 4.7963\n",
      "Epoch [1/2], Step [55720/64305], Loss: 5.0065\n",
      "Epoch [1/2], Step [55730/64305], Loss: 4.9818\n",
      "Epoch [1/2], Step [55740/64305], Loss: 4.8221\n",
      "Epoch [1/2], Step [55750/64305], Loss: 4.8732\n",
      "Epoch [1/2], Step [55760/64305], Loss: 4.7618\n",
      "Epoch [1/2], Step [55770/64305], Loss: 5.0166\n",
      "Epoch [1/2], Step [55780/64305], Loss: 4.9241\n",
      "Epoch [1/2], Step [55790/64305], Loss: 5.0716\n",
      "Epoch [1/2], Step [55800/64305], Loss: 5.0978\n",
      "Epoch [1/2], Step [55810/64305], Loss: 5.0456\n",
      "Epoch [1/2], Step [55820/64305], Loss: 4.9339\n",
      "Epoch [1/2], Step [55830/64305], Loss: 4.9358\n",
      "Epoch [1/2], Step [55840/64305], Loss: 4.8496\n",
      "Epoch [1/2], Step [55850/64305], Loss: 4.8436\n",
      "Epoch [1/2], Step [55860/64305], Loss: 5.0439\n",
      "Epoch [1/2], Step [55870/64305], Loss: 4.9927\n",
      "Epoch [1/2], Step [55880/64305], Loss: 5.0137\n",
      "Epoch [1/2], Step [55890/64305], Loss: 4.9187\n",
      "Epoch [1/2], Step [55900/64305], Loss: 4.8250\n",
      "Epoch [1/2], Step [55910/64305], Loss: 5.0208\n",
      "Epoch [1/2], Step [55920/64305], Loss: 4.6609\n",
      "Epoch [1/2], Step [55930/64305], Loss: 4.7233\n",
      "Epoch [1/2], Step [55940/64305], Loss: 4.8884\n",
      "Epoch [1/2], Step [55950/64305], Loss: 4.9148\n",
      "Epoch [1/2], Step [55960/64305], Loss: 4.9989\n",
      "Epoch [1/2], Step [55970/64305], Loss: 4.8450\n",
      "Epoch [1/2], Step [55980/64305], Loss: 5.0613\n",
      "Epoch [1/2], Step [55990/64305], Loss: 4.9104\n",
      "Epoch [1/2], Step [56000/64305], Loss: 5.0142\n",
      "Epoch [1/2], Step [56010/64305], Loss: 4.9227\n",
      "Epoch [1/2], Step [56020/64305], Loss: 4.8454\n",
      "Epoch [1/2], Step [56030/64305], Loss: 5.0284\n",
      "Epoch [1/2], Step [56040/64305], Loss: 4.9881\n",
      "Epoch [1/2], Step [56050/64305], Loss: 4.6931\n",
      "Epoch [1/2], Step [56060/64305], Loss: 4.8651\n",
      "Epoch [1/2], Step [56070/64305], Loss: 5.0188\n",
      "Epoch [1/2], Step [56080/64305], Loss: 4.7528\n",
      "Epoch [1/2], Step [56090/64305], Loss: 4.7763\n",
      "Epoch [1/2], Step [56100/64305], Loss: 4.9871\n",
      "Epoch [1/2], Step [56110/64305], Loss: 4.9620\n",
      "Epoch [1/2], Step [56120/64305], Loss: 4.9849\n",
      "Epoch [1/2], Step [56130/64305], Loss: 5.0599\n",
      "Epoch [1/2], Step [56140/64305], Loss: 5.0622\n",
      "Epoch [1/2], Step [56150/64305], Loss: 5.0013\n",
      "Epoch [1/2], Step [56160/64305], Loss: 4.8792\n",
      "Epoch [1/2], Step [56170/64305], Loss: 5.0560\n",
      "Epoch [1/2], Step [56180/64305], Loss: 4.7519\n",
      "Epoch [1/2], Step [56190/64305], Loss: 4.9300\n",
      "Epoch [1/2], Step [56200/64305], Loss: 4.8894\n",
      "Epoch [1/2], Step [56210/64305], Loss: 5.0918\n",
      "Epoch [1/2], Step [56220/64305], Loss: 4.9427\n",
      "Epoch [1/2], Step [56230/64305], Loss: 4.8463\n",
      "Epoch [1/2], Step [56240/64305], Loss: 4.8022\n",
      "Epoch [1/2], Step [56250/64305], Loss: 5.0336\n",
      "Epoch [1/2], Step [56260/64305], Loss: 4.9464\n",
      "Epoch [1/2], Step [56270/64305], Loss: 4.8104\n",
      "Epoch [1/2], Step [56280/64305], Loss: 4.8770\n",
      "Epoch [1/2], Step [56290/64305], Loss: 4.9226\n",
      "Epoch [1/2], Step [56300/64305], Loss: 5.1481\n",
      "Epoch [1/2], Step [56310/64305], Loss: 4.9639\n",
      "Epoch [1/2], Step [56320/64305], Loss: 4.9772\n",
      "Epoch [1/2], Step [56330/64305], Loss: 4.8060\n",
      "Epoch [1/2], Step [56340/64305], Loss: 5.0087\n",
      "Epoch [1/2], Step [56350/64305], Loss: 4.9391\n",
      "Epoch [1/2], Step [56360/64305], Loss: 5.0039\n",
      "Epoch [1/2], Step [56370/64305], Loss: 5.0808\n",
      "Epoch [1/2], Step [56380/64305], Loss: 5.0946\n",
      "Epoch [1/2], Step [56390/64305], Loss: 5.1364\n",
      "Epoch [1/2], Step [56400/64305], Loss: 4.8385\n",
      "Epoch [1/2], Step [56410/64305], Loss: 4.6783\n",
      "Epoch [1/2], Step [56420/64305], Loss: 4.8255\n",
      "Epoch [1/2], Step [56430/64305], Loss: 4.9935\n",
      "Epoch [1/2], Step [56440/64305], Loss: 4.8773\n",
      "Epoch [1/2], Step [56450/64305], Loss: 4.9077\n",
      "Epoch [1/2], Step [56460/64305], Loss: 4.9023\n",
      "Epoch [1/2], Step [56470/64305], Loss: 4.9534\n",
      "Epoch [1/2], Step [56480/64305], Loss: 4.8816\n",
      "Epoch [1/2], Step [56490/64305], Loss: 4.9564\n",
      "Epoch [1/2], Step [56500/64305], Loss: 4.7489\n",
      "Epoch [1/2], Step [56510/64305], Loss: 4.6767\n",
      "Epoch [1/2], Step [56520/64305], Loss: 5.0185\n",
      "Epoch [1/2], Step [56530/64305], Loss: 4.8444\n",
      "Epoch [1/2], Step [56540/64305], Loss: 5.0865\n",
      "Epoch [1/2], Step [56550/64305], Loss: 4.9763\n",
      "Epoch [1/2], Step [56560/64305], Loss: 4.8512\n",
      "Epoch [1/2], Step [56570/64305], Loss: 5.0232\n",
      "Epoch [1/2], Step [56580/64305], Loss: 5.0514\n",
      "Epoch [1/2], Step [56590/64305], Loss: 4.8868\n",
      "Epoch [1/2], Step [56600/64305], Loss: 4.6455\n",
      "Epoch [1/2], Step [56610/64305], Loss: 5.0066\n",
      "Epoch [1/2], Step [56620/64305], Loss: 4.9934\n",
      "Epoch [1/2], Step [56630/64305], Loss: 4.6858\n",
      "Epoch [1/2], Step [56640/64305], Loss: 4.7865\n",
      "Epoch [1/2], Step [56650/64305], Loss: 5.0663\n",
      "Epoch [1/2], Step [56660/64305], Loss: 4.9721\n",
      "Epoch [1/2], Step [56670/64305], Loss: 4.9390\n",
      "Epoch [1/2], Step [56680/64305], Loss: 4.9905\n",
      "Epoch [1/2], Step [56690/64305], Loss: 4.9793\n",
      "Epoch [1/2], Step [56700/64305], Loss: 5.0201\n",
      "Epoch [1/2], Step [56710/64305], Loss: 4.8171\n",
      "Epoch [1/2], Step [56720/64305], Loss: 4.8873\n",
      "Epoch [1/2], Step [56730/64305], Loss: 4.8074\n",
      "Epoch [1/2], Step [56740/64305], Loss: 4.8214\n",
      "Epoch [1/2], Step [56750/64305], Loss: 4.8900\n",
      "Epoch [1/2], Step [56760/64305], Loss: 4.9768\n",
      "Epoch [1/2], Step [56770/64305], Loss: 4.6702\n",
      "Epoch [1/2], Step [56780/64305], Loss: 4.9306\n",
      "Epoch [1/2], Step [56790/64305], Loss: 5.0579\n",
      "Epoch [1/2], Step [56800/64305], Loss: 4.8818\n",
      "Epoch [1/2], Step [56810/64305], Loss: 4.7768\n",
      "Epoch [1/2], Step [56820/64305], Loss: 4.9087\n",
      "Epoch [1/2], Step [56830/64305], Loss: 4.8993\n",
      "Epoch [1/2], Step [56840/64305], Loss: 5.0741\n",
      "Epoch [1/2], Step [56850/64305], Loss: 5.0784\n",
      "Epoch [1/2], Step [56860/64305], Loss: 4.8723\n",
      "Epoch [1/2], Step [56870/64305], Loss: 5.0150\n",
      "Epoch [1/2], Step [56880/64305], Loss: 4.8991\n",
      "Epoch [1/2], Step [56890/64305], Loss: 4.8146\n",
      "Epoch [1/2], Step [56900/64305], Loss: 4.8548\n",
      "Epoch [1/2], Step [56910/64305], Loss: 4.7615\n",
      "Epoch [1/2], Step [56920/64305], Loss: 4.8225\n",
      "Epoch [1/2], Step [56930/64305], Loss: 4.8688\n",
      "Epoch [1/2], Step [56940/64305], Loss: 4.9145\n",
      "Epoch [1/2], Step [56950/64305], Loss: 4.9449\n",
      "Epoch [1/2], Step [56960/64305], Loss: 4.9578\n",
      "Epoch [1/2], Step [56970/64305], Loss: 4.8158\n",
      "Epoch [1/2], Step [56980/64305], Loss: 4.8576\n",
      "Epoch [1/2], Step [56990/64305], Loss: 4.8234\n",
      "Epoch [1/2], Step [57000/64305], Loss: 4.9667\n",
      "Epoch [1/2], Step [57010/64305], Loss: 4.8561\n",
      "Epoch [1/2], Step [57020/64305], Loss: 4.9752\n",
      "Epoch [1/2], Step [57030/64305], Loss: 4.9125\n",
      "Epoch [1/2], Step [57040/64305], Loss: 4.9338\n",
      "Epoch [1/2], Step [57050/64305], Loss: 5.0266\n",
      "Epoch [1/2], Step [57060/64305], Loss: 5.1055\n",
      "Epoch [1/2], Step [57070/64305], Loss: 4.8280\n",
      "Epoch [1/2], Step [57080/64305], Loss: 4.8565\n",
      "Epoch [1/2], Step [57090/64305], Loss: 4.9558\n",
      "Epoch [1/2], Step [57100/64305], Loss: 4.8301\n",
      "Epoch [1/2], Step [57110/64305], Loss: 4.6497\n",
      "Epoch [1/2], Step [57120/64305], Loss: 5.0661\n",
      "Epoch [1/2], Step [57130/64305], Loss: 4.8609\n",
      "Epoch [1/2], Step [57140/64305], Loss: 4.9396\n",
      "Epoch [1/2], Step [57150/64305], Loss: 4.8832\n",
      "Epoch [1/2], Step [57160/64305], Loss: 4.9593\n",
      "Epoch [1/2], Step [57170/64305], Loss: 5.0662\n",
      "Epoch [1/2], Step [57180/64305], Loss: 4.9071\n",
      "Epoch [1/2], Step [57190/64305], Loss: 5.0459\n",
      "Epoch [1/2], Step [57200/64305], Loss: 5.0812\n",
      "Epoch [1/2], Step [57210/64305], Loss: 4.9659\n",
      "Epoch [1/2], Step [57220/64305], Loss: 4.8863\n",
      "Epoch [1/2], Step [57230/64305], Loss: 4.9910\n",
      "Epoch [1/2], Step [57240/64305], Loss: 4.9267\n",
      "Epoch [1/2], Step [57250/64305], Loss: 4.7130\n",
      "Epoch [1/2], Step [57260/64305], Loss: 4.8512\n",
      "Epoch [1/2], Step [57270/64305], Loss: 4.9091\n",
      "Epoch [1/2], Step [57280/64305], Loss: 4.8261\n",
      "Epoch [1/2], Step [57290/64305], Loss: 4.9310\n",
      "Epoch [1/2], Step [57300/64305], Loss: 5.0012\n",
      "Epoch [1/2], Step [57310/64305], Loss: 4.8011\n",
      "Epoch [1/2], Step [57320/64305], Loss: 5.0946\n",
      "Epoch [1/2], Step [57330/64305], Loss: 5.2204\n",
      "Epoch [1/2], Step [57340/64305], Loss: 4.9222\n",
      "Epoch [1/2], Step [57350/64305], Loss: 4.8325\n",
      "Epoch [1/2], Step [57360/64305], Loss: 4.8632\n",
      "Epoch [1/2], Step [57370/64305], Loss: 4.9025\n",
      "Epoch [1/2], Step [57380/64305], Loss: 4.9077\n",
      "Epoch [1/2], Step [57390/64305], Loss: 4.7921\n",
      "Epoch [1/2], Step [57400/64305], Loss: 5.0664\n",
      "Epoch [1/2], Step [57410/64305], Loss: 5.0505\n",
      "Epoch [1/2], Step [57420/64305], Loss: 4.9743\n",
      "Epoch [1/2], Step [57430/64305], Loss: 4.8993\n",
      "Epoch [1/2], Step [57440/64305], Loss: 4.9624\n",
      "Epoch [1/2], Step [57450/64305], Loss: 4.8256\n",
      "Epoch [1/2], Step [57460/64305], Loss: 4.9572\n",
      "Epoch [1/2], Step [57470/64305], Loss: 4.8585\n",
      "Epoch [1/2], Step [57480/64305], Loss: 4.9378\n",
      "Epoch [1/2], Step [57490/64305], Loss: 5.0354\n",
      "Epoch [1/2], Step [57500/64305], Loss: 4.7407\n",
      "Epoch [1/2], Step [57510/64305], Loss: 4.9474\n",
      "Epoch [1/2], Step [57520/64305], Loss: 4.8288\n",
      "Epoch [1/2], Step [57530/64305], Loss: 4.9823\n",
      "Epoch [1/2], Step [57540/64305], Loss: 5.0350\n",
      "Epoch [1/2], Step [57550/64305], Loss: 5.0497\n",
      "Epoch [1/2], Step [57560/64305], Loss: 4.7565\n",
      "Epoch [1/2], Step [57570/64305], Loss: 4.7038\n",
      "Epoch [1/2], Step [57580/64305], Loss: 4.7931\n",
      "Epoch [1/2], Step [57590/64305], Loss: 4.7626\n",
      "Epoch [1/2], Step [57600/64305], Loss: 4.9556\n",
      "Epoch [1/2], Step [57610/64305], Loss: 4.8872\n",
      "Epoch [1/2], Step [57620/64305], Loss: 5.0829\n",
      "Epoch [1/2], Step [57630/64305], Loss: 4.7554\n",
      "Epoch [1/2], Step [57640/64305], Loss: 4.8340\n",
      "Epoch [1/2], Step [57650/64305], Loss: 4.8824\n",
      "Epoch [1/2], Step [57660/64305], Loss: 5.0635\n",
      "Epoch [1/2], Step [57670/64305], Loss: 4.8844\n",
      "Epoch [1/2], Step [57680/64305], Loss: 5.1084\n",
      "Epoch [1/2], Step [57690/64305], Loss: 4.9710\n",
      "Epoch [1/2], Step [57700/64305], Loss: 4.9031\n",
      "Epoch [1/2], Step [57710/64305], Loss: 4.7983\n",
      "Epoch [1/2], Step [57720/64305], Loss: 4.9260\n",
      "Epoch [1/2], Step [57730/64305], Loss: 4.9475\n",
      "Epoch [1/2], Step [57740/64305], Loss: 4.5280\n",
      "Epoch [1/2], Step [57750/64305], Loss: 4.9823\n",
      "Epoch [1/2], Step [57760/64305], Loss: 4.9494\n",
      "Epoch [1/2], Step [57770/64305], Loss: 4.9932\n",
      "Epoch [1/2], Step [57780/64305], Loss: 4.8924\n",
      "Epoch [1/2], Step [57790/64305], Loss: 5.0884\n",
      "Epoch [1/2], Step [57800/64305], Loss: 4.8489\n",
      "Epoch [1/2], Step [57810/64305], Loss: 4.7941\n",
      "Epoch [1/2], Step [57820/64305], Loss: 4.6927\n",
      "Epoch [1/2], Step [57830/64305], Loss: 5.0093\n",
      "Epoch [1/2], Step [57840/64305], Loss: 4.7586\n",
      "Epoch [1/2], Step [57850/64305], Loss: 4.7521\n",
      "Epoch [1/2], Step [57860/64305], Loss: 5.0665\n",
      "Epoch [1/2], Step [57870/64305], Loss: 5.0684\n",
      "Epoch [1/2], Step [57880/64305], Loss: 4.8634\n",
      "Epoch [1/2], Step [57890/64305], Loss: 4.9108\n",
      "Epoch [1/2], Step [57900/64305], Loss: 4.8785\n",
      "Epoch [1/2], Step [57910/64305], Loss: 4.8337\n",
      "Epoch [1/2], Step [57920/64305], Loss: 4.9936\n",
      "Epoch [1/2], Step [57930/64305], Loss: 4.8860\n",
      "Epoch [1/2], Step [57940/64305], Loss: 4.6909\n",
      "Epoch [1/2], Step [57950/64305], Loss: 4.7460\n",
      "Epoch [1/2], Step [57960/64305], Loss: 4.9486\n",
      "Epoch [1/2], Step [57970/64305], Loss: 5.0874\n",
      "Epoch [1/2], Step [57980/64305], Loss: 5.0432\n",
      "Epoch [1/2], Step [57990/64305], Loss: 4.9669\n",
      "Epoch [1/2], Step [58000/64305], Loss: 5.0937\n",
      "Epoch [1/2], Step [58010/64305], Loss: 4.5834\n",
      "Epoch [1/2], Step [58020/64305], Loss: 5.1069\n",
      "Epoch [1/2], Step [58030/64305], Loss: 5.0600\n",
      "Epoch [1/2], Step [58040/64305], Loss: 5.0545\n",
      "Epoch [1/2], Step [58050/64305], Loss: 4.9447\n",
      "Epoch [1/2], Step [58060/64305], Loss: 5.0877\n",
      "Epoch [1/2], Step [58070/64305], Loss: 4.8698\n",
      "Epoch [1/2], Step [58080/64305], Loss: 4.9365\n",
      "Epoch [1/2], Step [58090/64305], Loss: 5.0605\n",
      "Epoch [1/2], Step [58100/64305], Loss: 4.8361\n",
      "Epoch [1/2], Step [58110/64305], Loss: 4.7865\n",
      "Epoch [1/2], Step [58120/64305], Loss: 4.9198\n",
      "Epoch [1/2], Step [58130/64305], Loss: 5.0323\n",
      "Epoch [1/2], Step [58140/64305], Loss: 4.8178\n",
      "Epoch [1/2], Step [58150/64305], Loss: 4.9350\n",
      "Epoch [1/2], Step [58160/64305], Loss: 4.8124\n",
      "Epoch [1/2], Step [58170/64305], Loss: 4.9816\n",
      "Epoch [1/2], Step [58180/64305], Loss: 4.6603\n",
      "Epoch [1/2], Step [58190/64305], Loss: 5.0114\n",
      "Epoch [1/2], Step [58200/64305], Loss: 4.8529\n",
      "Epoch [1/2], Step [58210/64305], Loss: 4.7457\n",
      "Epoch [1/2], Step [58220/64305], Loss: 5.2173\n",
      "Epoch [1/2], Step [58230/64305], Loss: 4.9510\n",
      "Epoch [1/2], Step [58240/64305], Loss: 4.9028\n",
      "Epoch [1/2], Step [58250/64305], Loss: 5.0551\n",
      "Epoch [1/2], Step [58260/64305], Loss: 4.8310\n",
      "Epoch [1/2], Step [58270/64305], Loss: 4.9143\n",
      "Epoch [1/2], Step [58280/64305], Loss: 4.7836\n",
      "Epoch [1/2], Step [58290/64305], Loss: 4.8021\n",
      "Epoch [1/2], Step [58300/64305], Loss: 4.8925\n",
      "Epoch [1/2], Step [58310/64305], Loss: 4.8792\n",
      "Epoch [1/2], Step [58320/64305], Loss: 4.7521\n",
      "Epoch [1/2], Step [58330/64305], Loss: 4.9732\n",
      "Epoch [1/2], Step [58340/64305], Loss: 5.0158\n",
      "Epoch [1/2], Step [58350/64305], Loss: 4.7233\n",
      "Epoch [1/2], Step [58360/64305], Loss: 4.9476\n",
      "Epoch [1/2], Step [58370/64305], Loss: 4.6227\n",
      "Epoch [1/2], Step [58380/64305], Loss: 5.0240\n",
      "Epoch [1/2], Step [58390/64305], Loss: 4.9865\n",
      "Epoch [1/2], Step [58400/64305], Loss: 4.9289\n",
      "Epoch [1/2], Step [58410/64305], Loss: 5.0821\n",
      "Epoch [1/2], Step [58420/64305], Loss: 4.9431\n",
      "Epoch [1/2], Step [58430/64305], Loss: 4.9795\n",
      "Epoch [1/2], Step [58440/64305], Loss: 4.9931\n",
      "Epoch [1/2], Step [58450/64305], Loss: 4.9317\n",
      "Epoch [1/2], Step [58460/64305], Loss: 4.7250\n",
      "Epoch [1/2], Step [58470/64305], Loss: 4.9655\n",
      "Epoch [1/2], Step [58480/64305], Loss: 5.1215\n",
      "Epoch [1/2], Step [58490/64305], Loss: 4.8268\n",
      "Epoch [1/2], Step [58500/64305], Loss: 4.8410\n",
      "Epoch [1/2], Step [58510/64305], Loss: 4.7699\n",
      "Epoch [1/2], Step [58520/64305], Loss: 5.0546\n",
      "Epoch [1/2], Step [58530/64305], Loss: 5.2410\n",
      "Epoch [1/2], Step [58540/64305], Loss: 4.8032\n",
      "Epoch [1/2], Step [58550/64305], Loss: 4.8581\n",
      "Epoch [1/2], Step [58560/64305], Loss: 4.8048\n",
      "Epoch [1/2], Step [58570/64305], Loss: 4.8744\n",
      "Epoch [1/2], Step [58580/64305], Loss: 4.7289\n",
      "Epoch [1/2], Step [58590/64305], Loss: 4.8948\n",
      "Epoch [1/2], Step [58600/64305], Loss: 4.8507\n",
      "Epoch [1/2], Step [58610/64305], Loss: 4.7114\n",
      "Epoch [1/2], Step [58620/64305], Loss: 4.8827\n",
      "Epoch [1/2], Step [58630/64305], Loss: 4.8875\n",
      "Epoch [1/2], Step [58640/64305], Loss: 4.7935\n",
      "Epoch [1/2], Step [58650/64305], Loss: 5.1541\n",
      "Epoch [1/2], Step [58660/64305], Loss: 4.9840\n",
      "Epoch [1/2], Step [58670/64305], Loss: 5.0646\n",
      "Epoch [1/2], Step [58680/64305], Loss: 4.9681\n",
      "Epoch [1/2], Step [58690/64305], Loss: 4.9128\n",
      "Epoch [1/2], Step [58700/64305], Loss: 4.7847\n",
      "Epoch [1/2], Step [58710/64305], Loss: 4.7016\n",
      "Epoch [1/2], Step [58720/64305], Loss: 4.7106\n",
      "Epoch [1/2], Step [58730/64305], Loss: 5.0960\n",
      "Epoch [1/2], Step [58740/64305], Loss: 4.8790\n",
      "Epoch [1/2], Step [58750/64305], Loss: 5.0332\n",
      "Epoch [1/2], Step [58760/64305], Loss: 4.8972\n",
      "Epoch [1/2], Step [58770/64305], Loss: 4.7913\n",
      "Epoch [1/2], Step [58780/64305], Loss: 4.9483\n",
      "Epoch [1/2], Step [58790/64305], Loss: 5.0616\n",
      "Epoch [1/2], Step [58800/64305], Loss: 4.7836\n",
      "Epoch [1/2], Step [58810/64305], Loss: 4.9600\n",
      "Epoch [1/2], Step [58820/64305], Loss: 5.0981\n",
      "Epoch [1/2], Step [58830/64305], Loss: 4.9102\n",
      "Epoch [1/2], Step [58840/64305], Loss: 5.0314\n",
      "Epoch [1/2], Step [58850/64305], Loss: 5.0797\n",
      "Epoch [1/2], Step [58860/64305], Loss: 4.8919\n",
      "Epoch [1/2], Step [58870/64305], Loss: 4.8033\n",
      "Epoch [1/2], Step [58880/64305], Loss: 4.7910\n",
      "Epoch [1/2], Step [58890/64305], Loss: 4.8320\n",
      "Epoch [1/2], Step [58900/64305], Loss: 5.1221\n",
      "Epoch [1/2], Step [58910/64305], Loss: 4.8504\n",
      "Epoch [1/2], Step [58920/64305], Loss: 4.6877\n",
      "Epoch [1/2], Step [58930/64305], Loss: 4.8633\n",
      "Epoch [1/2], Step [58940/64305], Loss: 4.6902\n",
      "Epoch [1/2], Step [58950/64305], Loss: 4.8326\n",
      "Epoch [1/2], Step [58960/64305], Loss: 4.9424\n",
      "Epoch [1/2], Step [58970/64305], Loss: 5.2074\n",
      "Epoch [1/2], Step [58980/64305], Loss: 4.7090\n",
      "Epoch [1/2], Step [58990/64305], Loss: 4.7182\n",
      "Epoch [1/2], Step [59000/64305], Loss: 4.8816\n",
      "Epoch [1/2], Step [59010/64305], Loss: 4.9733\n",
      "Epoch [1/2], Step [59020/64305], Loss: 5.0075\n",
      "Epoch [1/2], Step [59030/64305], Loss: 4.7960\n",
      "Epoch [1/2], Step [59040/64305], Loss: 4.8271\n",
      "Epoch [1/2], Step [59050/64305], Loss: 4.8596\n",
      "Epoch [1/2], Step [59060/64305], Loss: 4.8300\n",
      "Epoch [1/2], Step [59070/64305], Loss: 5.1413\n",
      "Epoch [1/2], Step [59080/64305], Loss: 4.8684\n",
      "Epoch [1/2], Step [59090/64305], Loss: 4.8048\n",
      "Epoch [1/2], Step [59100/64305], Loss: 4.9574\n",
      "Epoch [1/2], Step [59110/64305], Loss: 5.0134\n",
      "Epoch [1/2], Step [59120/64305], Loss: 4.8669\n",
      "Epoch [1/2], Step [59130/64305], Loss: 4.8562\n",
      "Epoch [1/2], Step [59140/64305], Loss: 4.8043\n",
      "Epoch [1/2], Step [59150/64305], Loss: 5.0452\n",
      "Epoch [1/2], Step [59160/64305], Loss: 4.8204\n",
      "Epoch [1/2], Step [59170/64305], Loss: 4.9046\n",
      "Epoch [1/2], Step [59180/64305], Loss: 4.8716\n",
      "Epoch [1/2], Step [59190/64305], Loss: 4.8132\n",
      "Epoch [1/2], Step [59200/64305], Loss: 4.8627\n",
      "Epoch [1/2], Step [59210/64305], Loss: 4.7834\n",
      "Epoch [1/2], Step [59220/64305], Loss: 4.7687\n",
      "Epoch [1/2], Step [59230/64305], Loss: 5.0387\n",
      "Epoch [1/2], Step [59240/64305], Loss: 5.0461\n",
      "Epoch [1/2], Step [59250/64305], Loss: 5.0450\n",
      "Epoch [1/2], Step [59260/64305], Loss: 4.9119\n",
      "Epoch [1/2], Step [59270/64305], Loss: 4.7888\n",
      "Epoch [1/2], Step [59280/64305], Loss: 4.9976\n",
      "Epoch [1/2], Step [59290/64305], Loss: 4.8856\n",
      "Epoch [1/2], Step [59300/64305], Loss: 4.8434\n",
      "Epoch [1/2], Step [59310/64305], Loss: 4.8366\n",
      "Epoch [1/2], Step [59320/64305], Loss: 5.0132\n",
      "Epoch [1/2], Step [59330/64305], Loss: 4.7373\n",
      "Epoch [1/2], Step [59340/64305], Loss: 4.9003\n",
      "Epoch [1/2], Step [59350/64305], Loss: 4.9797\n",
      "Epoch [1/2], Step [59360/64305], Loss: 4.9083\n",
      "Epoch [1/2], Step [59370/64305], Loss: 4.9517\n",
      "Epoch [1/2], Step [59380/64305], Loss: 5.0117\n",
      "Epoch [1/2], Step [59390/64305], Loss: 5.1031\n",
      "Epoch [1/2], Step [59400/64305], Loss: 4.7377\n",
      "Epoch [1/2], Step [59410/64305], Loss: 4.7119\n",
      "Epoch [1/2], Step [59420/64305], Loss: 4.9181\n",
      "Epoch [1/2], Step [59430/64305], Loss: 4.9215\n",
      "Epoch [1/2], Step [59440/64305], Loss: 4.8025\n",
      "Epoch [1/2], Step [59450/64305], Loss: 4.8162\n",
      "Epoch [1/2], Step [59460/64305], Loss: 4.9672\n",
      "Epoch [1/2], Step [59470/64305], Loss: 4.9254\n",
      "Epoch [1/2], Step [59480/64305], Loss: 4.8265\n",
      "Epoch [1/2], Step [59490/64305], Loss: 4.9937\n",
      "Epoch [1/2], Step [59500/64305], Loss: 5.0059\n",
      "Epoch [1/2], Step [59510/64305], Loss: 4.9471\n",
      "Epoch [1/2], Step [59520/64305], Loss: 4.9338\n",
      "Epoch [1/2], Step [59530/64305], Loss: 4.9283\n",
      "Epoch [1/2], Step [59540/64305], Loss: 5.0989\n",
      "Epoch [1/2], Step [59550/64305], Loss: 5.0338\n",
      "Epoch [1/2], Step [59560/64305], Loss: 4.7758\n",
      "Epoch [1/2], Step [59570/64305], Loss: 4.8980\n",
      "Epoch [1/2], Step [59580/64305], Loss: 4.8843\n",
      "Epoch [1/2], Step [59590/64305], Loss: 4.6744\n",
      "Epoch [1/2], Step [59600/64305], Loss: 5.0798\n",
      "Epoch [1/2], Step [59610/64305], Loss: 4.9569\n",
      "Epoch [1/2], Step [59620/64305], Loss: 4.7930\n",
      "Epoch [1/2], Step [59630/64305], Loss: 4.9461\n",
      "Epoch [1/2], Step [59640/64305], Loss: 4.8271\n",
      "Epoch [1/2], Step [59650/64305], Loss: 5.0471\n",
      "Epoch [1/2], Step [59660/64305], Loss: 4.7234\n",
      "Epoch [1/2], Step [59670/64305], Loss: 4.6905\n",
      "Epoch [1/2], Step [59680/64305], Loss: 4.7466\n",
      "Epoch [1/2], Step [59690/64305], Loss: 4.8580\n",
      "Epoch [1/2], Step [59700/64305], Loss: 4.8838\n",
      "Epoch [1/2], Step [59710/64305], Loss: 5.0491\n",
      "Epoch [1/2], Step [59720/64305], Loss: 5.0006\n",
      "Epoch [1/2], Step [59730/64305], Loss: 5.2172\n",
      "Epoch [1/2], Step [59740/64305], Loss: 4.9500\n",
      "Epoch [1/2], Step [59750/64305], Loss: 4.9754\n",
      "Epoch [1/2], Step [59760/64305], Loss: 4.9720\n",
      "Epoch [1/2], Step [59770/64305], Loss: 4.8269\n",
      "Epoch [1/2], Step [59780/64305], Loss: 4.9135\n",
      "Epoch [1/2], Step [59790/64305], Loss: 4.9280\n",
      "Epoch [1/2], Step [59800/64305], Loss: 4.8723\n",
      "Epoch [1/2], Step [59810/64305], Loss: 5.0560\n",
      "Epoch [1/2], Step [59820/64305], Loss: 4.9251\n",
      "Epoch [1/2], Step [59830/64305], Loss: 4.9356\n",
      "Epoch [1/2], Step [59840/64305], Loss: 4.9267\n",
      "Epoch [1/2], Step [59850/64305], Loss: 4.9766\n",
      "Epoch [1/2], Step [59860/64305], Loss: 5.0068\n",
      "Epoch [1/2], Step [59870/64305], Loss: 4.9862\n",
      "Epoch [1/2], Step [59880/64305], Loss: 4.8517\n",
      "Epoch [1/2], Step [59890/64305], Loss: 4.7344\n",
      "Epoch [1/2], Step [59900/64305], Loss: 5.0325\n",
      "Epoch [1/2], Step [59910/64305], Loss: 4.8108\n",
      "Epoch [1/2], Step [59920/64305], Loss: 4.9696\n",
      "Epoch [1/2], Step [59930/64305], Loss: 4.9119\n",
      "Epoch [1/2], Step [59940/64305], Loss: 4.9399\n",
      "Epoch [1/2], Step [59950/64305], Loss: 4.9126\n",
      "Epoch [1/2], Step [59960/64305], Loss: 4.9335\n",
      "Epoch [1/2], Step [59970/64305], Loss: 4.8443\n",
      "Epoch [1/2], Step [59980/64305], Loss: 5.1721\n",
      "Epoch [1/2], Step [59990/64305], Loss: 4.8527\n",
      "Epoch [1/2], Step [60000/64305], Loss: 4.7183\n",
      "Epoch [1/2], Step [60010/64305], Loss: 4.8289\n",
      "Epoch [1/2], Step [60020/64305], Loss: 4.8597\n",
      "Epoch [1/2], Step [60030/64305], Loss: 5.0999\n",
      "Epoch [1/2], Step [60040/64305], Loss: 4.8352\n",
      "Epoch [1/2], Step [60050/64305], Loss: 5.0111\n",
      "Epoch [1/2], Step [60060/64305], Loss: 5.0312\n",
      "Epoch [1/2], Step [60070/64305], Loss: 4.8416\n",
      "Epoch [1/2], Step [60080/64305], Loss: 4.8234\n",
      "Epoch [1/2], Step [60090/64305], Loss: 5.1187\n",
      "Epoch [1/2], Step [60100/64305], Loss: 5.0425\n",
      "Epoch [1/2], Step [60110/64305], Loss: 5.0525\n",
      "Epoch [1/2], Step [60120/64305], Loss: 5.0913\n",
      "Epoch [1/2], Step [60130/64305], Loss: 5.0860\n",
      "Epoch [1/2], Step [60140/64305], Loss: 4.8002\n",
      "Epoch [1/2], Step [60150/64305], Loss: 4.7958\n",
      "Epoch [1/2], Step [60160/64305], Loss: 4.9835\n",
      "Epoch [1/2], Step [60170/64305], Loss: 4.8907\n",
      "Epoch [1/2], Step [60180/64305], Loss: 4.8079\n",
      "Epoch [1/2], Step [60190/64305], Loss: 4.8620\n",
      "Epoch [1/2], Step [60200/64305], Loss: 4.9429\n",
      "Epoch [1/2], Step [60210/64305], Loss: 4.8903\n",
      "Epoch [1/2], Step [60220/64305], Loss: 4.8739\n",
      "Epoch [1/2], Step [60230/64305], Loss: 4.9533\n",
      "Epoch [1/2], Step [60240/64305], Loss: 4.9561\n",
      "Epoch [1/2], Step [60250/64305], Loss: 4.8983\n",
      "Epoch [1/2], Step [60260/64305], Loss: 4.9345\n",
      "Epoch [1/2], Step [60270/64305], Loss: 4.8299\n",
      "Epoch [1/2], Step [60280/64305], Loss: 4.9323\n",
      "Epoch [1/2], Step [60290/64305], Loss: 4.8643\n",
      "Epoch [1/2], Step [60300/64305], Loss: 4.9222\n",
      "Epoch [1/2], Step [60310/64305], Loss: 4.9961\n",
      "Epoch [1/2], Step [60320/64305], Loss: 5.0471\n",
      "Epoch [1/2], Step [60330/64305], Loss: 4.5359\n",
      "Epoch [1/2], Step [60340/64305], Loss: 4.9537\n",
      "Epoch [1/2], Step [60350/64305], Loss: 4.9117\n",
      "Epoch [1/2], Step [60360/64305], Loss: 4.9579\n",
      "Epoch [1/2], Step [60370/64305], Loss: 4.9516\n",
      "Epoch [1/2], Step [60380/64305], Loss: 5.0444\n",
      "Epoch [1/2], Step [60390/64305], Loss: 5.0579\n",
      "Epoch [1/2], Step [60400/64305], Loss: 5.0223\n",
      "Epoch [1/2], Step [60410/64305], Loss: 5.0438\n",
      "Epoch [1/2], Step [60420/64305], Loss: 4.9590\n",
      "Epoch [1/2], Step [60430/64305], Loss: 4.8449\n",
      "Epoch [1/2], Step [60440/64305], Loss: 4.8856\n",
      "Epoch [1/2], Step [60450/64305], Loss: 4.8251\n",
      "Epoch [1/2], Step [60460/64305], Loss: 4.8301\n",
      "Epoch [1/2], Step [60470/64305], Loss: 4.9692\n",
      "Epoch [1/2], Step [60480/64305], Loss: 4.8128\n",
      "Epoch [1/2], Step [60490/64305], Loss: 4.7849\n",
      "Epoch [1/2], Step [60500/64305], Loss: 4.8692\n",
      "Epoch [1/2], Step [60510/64305], Loss: 4.7145\n",
      "Epoch [1/2], Step [60520/64305], Loss: 4.7233\n",
      "Epoch [1/2], Step [60530/64305], Loss: 4.8272\n",
      "Epoch [1/2], Step [60540/64305], Loss: 4.8412\n",
      "Epoch [1/2], Step [60550/64305], Loss: 4.7719\n",
      "Epoch [1/2], Step [60560/64305], Loss: 5.0126\n",
      "Epoch [1/2], Step [60570/64305], Loss: 4.9401\n",
      "Epoch [1/2], Step [60580/64305], Loss: 4.8701\n",
      "Epoch [1/2], Step [60590/64305], Loss: 4.9509\n",
      "Epoch [1/2], Step [60600/64305], Loss: 5.0890\n",
      "Epoch [1/2], Step [60610/64305], Loss: 4.9081\n",
      "Epoch [1/2], Step [60620/64305], Loss: 5.1235\n",
      "Epoch [1/2], Step [60630/64305], Loss: 4.8889\n",
      "Epoch [1/2], Step [60640/64305], Loss: 4.9292\n",
      "Epoch [1/2], Step [60650/64305], Loss: 5.1762\n",
      "Epoch [1/2], Step [60660/64305], Loss: 5.0238\n",
      "Epoch [1/2], Step [60670/64305], Loss: 4.7107\n",
      "Epoch [1/2], Step [60680/64305], Loss: 4.8297\n",
      "Epoch [1/2], Step [60690/64305], Loss: 4.8662\n",
      "Epoch [1/2], Step [60700/64305], Loss: 4.8805\n",
      "Epoch [1/2], Step [60710/64305], Loss: 4.9231\n",
      "Epoch [1/2], Step [60720/64305], Loss: 4.8662\n",
      "Epoch [1/2], Step [60730/64305], Loss: 4.8934\n",
      "Epoch [1/2], Step [60740/64305], Loss: 4.9136\n",
      "Epoch [1/2], Step [60750/64305], Loss: 5.0670\n",
      "Epoch [1/2], Step [60760/64305], Loss: 4.9717\n",
      "Epoch [1/2], Step [60770/64305], Loss: 4.9133\n",
      "Epoch [1/2], Step [60780/64305], Loss: 4.9960\n",
      "Epoch [1/2], Step [60790/64305], Loss: 4.7882\n",
      "Epoch [1/2], Step [60800/64305], Loss: 4.7300\n",
      "Epoch [1/2], Step [60810/64305], Loss: 4.8145\n",
      "Epoch [1/2], Step [60820/64305], Loss: 5.0163\n",
      "Epoch [1/2], Step [60830/64305], Loss: 4.8065\n",
      "Epoch [1/2], Step [60840/64305], Loss: 4.8297\n",
      "Epoch [1/2], Step [60850/64305], Loss: 4.9713\n",
      "Epoch [1/2], Step [60860/64305], Loss: 4.8411\n",
      "Epoch [1/2], Step [60870/64305], Loss: 5.0176\n",
      "Epoch [1/2], Step [60880/64305], Loss: 5.0371\n",
      "Epoch [1/2], Step [60890/64305], Loss: 4.7756\n",
      "Epoch [1/2], Step [60900/64305], Loss: 4.7316\n",
      "Epoch [1/2], Step [60910/64305], Loss: 5.0300\n",
      "Epoch [1/2], Step [60920/64305], Loss: 4.9513\n",
      "Epoch [1/2], Step [60930/64305], Loss: 4.8187\n",
      "Epoch [1/2], Step [60940/64305], Loss: 4.8677\n",
      "Epoch [1/2], Step [60950/64305], Loss: 4.9709\n",
      "Epoch [1/2], Step [60960/64305], Loss: 4.8337\n",
      "Epoch [1/2], Step [60970/64305], Loss: 4.8273\n",
      "Epoch [1/2], Step [60980/64305], Loss: 4.8393\n",
      "Epoch [1/2], Step [60990/64305], Loss: 4.9898\n",
      "Epoch [1/2], Step [61000/64305], Loss: 5.0015\n",
      "Epoch [1/2], Step [61010/64305], Loss: 4.9389\n",
      "Epoch [1/2], Step [61020/64305], Loss: 4.8992\n",
      "Epoch [1/2], Step [61030/64305], Loss: 5.0591\n",
      "Epoch [1/2], Step [61040/64305], Loss: 4.8643\n",
      "Epoch [1/2], Step [61050/64305], Loss: 4.8407\n",
      "Epoch [1/2], Step [61060/64305], Loss: 4.8236\n",
      "Epoch [1/2], Step [61070/64305], Loss: 4.8052\n",
      "Epoch [1/2], Step [61080/64305], Loss: 4.9624\n",
      "Epoch [1/2], Step [61090/64305], Loss: 4.8961\n",
      "Epoch [1/2], Step [61100/64305], Loss: 4.8307\n",
      "Epoch [1/2], Step [61110/64305], Loss: 4.9753\n",
      "Epoch [1/2], Step [61120/64305], Loss: 5.0675\n",
      "Epoch [1/2], Step [61130/64305], Loss: 5.0713\n",
      "Epoch [1/2], Step [61140/64305], Loss: 4.7647\n",
      "Epoch [1/2], Step [61150/64305], Loss: 4.9377\n",
      "Epoch [1/2], Step [61160/64305], Loss: 4.8364\n",
      "Epoch [1/2], Step [61170/64305], Loss: 4.9625\n",
      "Epoch [1/2], Step [61180/64305], Loss: 5.1674\n",
      "Epoch [1/2], Step [61190/64305], Loss: 4.6903\n",
      "Epoch [1/2], Step [61200/64305], Loss: 4.7609\n",
      "Epoch [1/2], Step [61210/64305], Loss: 4.9229\n",
      "Epoch [1/2], Step [61220/64305], Loss: 5.0122\n",
      "Epoch [1/2], Step [61230/64305], Loss: 5.1718\n",
      "Epoch [1/2], Step [61240/64305], Loss: 4.9638\n",
      "Epoch [1/2], Step [61250/64305], Loss: 4.9937\n",
      "Epoch [1/2], Step [61260/64305], Loss: 5.1136\n",
      "Epoch [1/2], Step [61270/64305], Loss: 4.9782\n",
      "Epoch [1/2], Step [61280/64305], Loss: 4.6762\n",
      "Epoch [1/2], Step [61290/64305], Loss: 5.0398\n",
      "Epoch [1/2], Step [61300/64305], Loss: 4.9359\n",
      "Epoch [1/2], Step [61310/64305], Loss: 4.9075\n",
      "Epoch [1/2], Step [61320/64305], Loss: 4.8835\n",
      "Epoch [1/2], Step [61330/64305], Loss: 4.8162\n",
      "Epoch [1/2], Step [61340/64305], Loss: 4.9073\n",
      "Epoch [1/2], Step [61350/64305], Loss: 4.8953\n",
      "Epoch [1/2], Step [61360/64305], Loss: 4.8251\n",
      "Epoch [1/2], Step [61370/64305], Loss: 4.8920\n",
      "Epoch [1/2], Step [61380/64305], Loss: 5.0140\n",
      "Epoch [1/2], Step [61390/64305], Loss: 4.7986\n",
      "Epoch [1/2], Step [61400/64305], Loss: 4.9983\n",
      "Epoch [1/2], Step [61410/64305], Loss: 5.0102\n",
      "Epoch [1/2], Step [61420/64305], Loss: 4.9990\n",
      "Epoch [1/2], Step [61430/64305], Loss: 5.0464\n",
      "Epoch [1/2], Step [61440/64305], Loss: 4.9016\n",
      "Epoch [1/2], Step [61450/64305], Loss: 4.8322\n",
      "Epoch [1/2], Step [61460/64305], Loss: 4.8733\n",
      "Epoch [1/2], Step [61470/64305], Loss: 4.7610\n",
      "Epoch [1/2], Step [61480/64305], Loss: 4.7966\n",
      "Epoch [1/2], Step [61490/64305], Loss: 4.7845\n",
      "Epoch [1/2], Step [61500/64305], Loss: 4.8989\n",
      "Epoch [1/2], Step [61510/64305], Loss: 4.7526\n",
      "Epoch [1/2], Step [61520/64305], Loss: 4.9078\n",
      "Epoch [1/2], Step [61530/64305], Loss: 4.9632\n",
      "Epoch [1/2], Step [61540/64305], Loss: 4.9816\n",
      "Epoch [1/2], Step [61550/64305], Loss: 4.8726\n",
      "Epoch [1/2], Step [61560/64305], Loss: 5.0973\n",
      "Epoch [1/2], Step [61570/64305], Loss: 4.9674\n",
      "Epoch [1/2], Step [61580/64305], Loss: 4.9836\n",
      "Epoch [1/2], Step [61590/64305], Loss: 4.8781\n",
      "Epoch [1/2], Step [61600/64305], Loss: 4.7605\n",
      "Epoch [1/2], Step [61610/64305], Loss: 4.9820\n",
      "Epoch [1/2], Step [61620/64305], Loss: 4.9480\n",
      "Epoch [1/2], Step [61630/64305], Loss: 5.0824\n",
      "Epoch [1/2], Step [61640/64305], Loss: 5.0317\n",
      "Epoch [1/2], Step [61650/64305], Loss: 4.9215\n",
      "Epoch [1/2], Step [61660/64305], Loss: 4.7981\n",
      "Epoch [1/2], Step [61670/64305], Loss: 4.7650\n",
      "Epoch [1/2], Step [61680/64305], Loss: 4.8639\n",
      "Epoch [1/2], Step [61690/64305], Loss: 4.9730\n",
      "Epoch [1/2], Step [61700/64305], Loss: 4.8087\n",
      "Epoch [1/2], Step [61710/64305], Loss: 4.8551\n",
      "Epoch [1/2], Step [61720/64305], Loss: 4.8653\n",
      "Epoch [1/2], Step [61730/64305], Loss: 4.9049\n",
      "Epoch [1/2], Step [61740/64305], Loss: 4.8609\n",
      "Epoch [1/2], Step [61750/64305], Loss: 4.8346\n",
      "Epoch [1/2], Step [61760/64305], Loss: 4.9952\n",
      "Epoch [1/2], Step [61770/64305], Loss: 4.9083\n",
      "Epoch [1/2], Step [61780/64305], Loss: 4.8720\n",
      "Epoch [1/2], Step [61790/64305], Loss: 5.0290\n",
      "Epoch [1/2], Step [61800/64305], Loss: 4.8414\n",
      "Epoch [1/2], Step [61810/64305], Loss: 4.7645\n",
      "Epoch [1/2], Step [61820/64305], Loss: 4.7815\n",
      "Epoch [1/2], Step [61830/64305], Loss: 5.2273\n",
      "Epoch [1/2], Step [61840/64305], Loss: 4.7653\n",
      "Epoch [1/2], Step [61850/64305], Loss: 4.9112\n",
      "Epoch [1/2], Step [61860/64305], Loss: 4.8735\n",
      "Epoch [1/2], Step [61870/64305], Loss: 4.8724\n",
      "Epoch [1/2], Step [61880/64305], Loss: 5.0028\n",
      "Epoch [1/2], Step [61890/64305], Loss: 4.8702\n",
      "Epoch [1/2], Step [61900/64305], Loss: 4.7875\n",
      "Epoch [1/2], Step [61910/64305], Loss: 5.0943\n",
      "Epoch [1/2], Step [61920/64305], Loss: 4.9633\n",
      "Epoch [1/2], Step [61930/64305], Loss: 4.8007\n",
      "Epoch [1/2], Step [61940/64305], Loss: 4.8276\n",
      "Epoch [1/2], Step [61950/64305], Loss: 4.8881\n",
      "Epoch [1/2], Step [61960/64305], Loss: 4.9887\n",
      "Epoch [1/2], Step [61970/64305], Loss: 4.9080\n",
      "Epoch [1/2], Step [61980/64305], Loss: 4.8110\n",
      "Epoch [1/2], Step [61990/64305], Loss: 4.9903\n",
      "Epoch [1/2], Step [62000/64305], Loss: 4.9008\n",
      "Epoch [1/2], Step [62010/64305], Loss: 4.8155\n",
      "Epoch [1/2], Step [62020/64305], Loss: 5.1294\n",
      "Epoch [1/2], Step [62030/64305], Loss: 4.7890\n",
      "Epoch [1/2], Step [62040/64305], Loss: 4.9538\n",
      "Epoch [1/2], Step [62050/64305], Loss: 5.0471\n",
      "Epoch [1/2], Step [62060/64305], Loss: 4.9463\n",
      "Epoch [1/2], Step [62070/64305], Loss: 4.9250\n",
      "Epoch [1/2], Step [62080/64305], Loss: 4.8016\n",
      "Epoch [1/2], Step [62090/64305], Loss: 4.5759\n",
      "Epoch [1/2], Step [62100/64305], Loss: 4.9078\n",
      "Epoch [1/2], Step [62110/64305], Loss: 5.1053\n",
      "Epoch [1/2], Step [62120/64305], Loss: 4.8608\n",
      "Epoch [1/2], Step [62130/64305], Loss: 5.0418\n",
      "Epoch [1/2], Step [62140/64305], Loss: 4.9060\n",
      "Epoch [1/2], Step [62150/64305], Loss: 4.8539\n",
      "Epoch [1/2], Step [62160/64305], Loss: 4.9032\n",
      "Epoch [1/2], Step [62170/64305], Loss: 5.0141\n",
      "Epoch [1/2], Step [62180/64305], Loss: 4.9574\n",
      "Epoch [1/2], Step [62190/64305], Loss: 5.0249\n",
      "Epoch [1/2], Step [62200/64305], Loss: 4.7287\n",
      "Epoch [1/2], Step [62210/64305], Loss: 4.8559\n",
      "Epoch [1/2], Step [62220/64305], Loss: 5.0427\n",
      "Epoch [1/2], Step [62230/64305], Loss: 4.8022\n",
      "Epoch [1/2], Step [62240/64305], Loss: 5.1138\n",
      "Epoch [1/2], Step [62250/64305], Loss: 4.9943\n",
      "Epoch [1/2], Step [62260/64305], Loss: 4.8497\n",
      "Epoch [1/2], Step [62270/64305], Loss: 4.7604\n",
      "Epoch [1/2], Step [62280/64305], Loss: 4.8442\n",
      "Epoch [1/2], Step [62290/64305], Loss: 4.7229\n",
      "Epoch [1/2], Step [62300/64305], Loss: 5.0339\n",
      "Epoch [1/2], Step [62310/64305], Loss: 4.8111\n",
      "Epoch [1/2], Step [62320/64305], Loss: 5.0826\n",
      "Epoch [1/2], Step [62330/64305], Loss: 4.9954\n",
      "Epoch [1/2], Step [62340/64305], Loss: 5.0517\n",
      "Epoch [1/2], Step [62350/64305], Loss: 4.8623\n",
      "Epoch [1/2], Step [62360/64305], Loss: 4.9695\n",
      "Epoch [1/2], Step [62370/64305], Loss: 5.0223\n",
      "Epoch [1/2], Step [62380/64305], Loss: 4.8495\n",
      "Epoch [1/2], Step [62390/64305], Loss: 4.9532\n",
      "Epoch [1/2], Step [62400/64305], Loss: 5.0235\n",
      "Epoch [1/2], Step [62410/64305], Loss: 4.8419\n",
      "Epoch [1/2], Step [62420/64305], Loss: 4.7717\n",
      "Epoch [1/2], Step [62430/64305], Loss: 4.7909\n",
      "Epoch [1/2], Step [62440/64305], Loss: 5.0263\n",
      "Epoch [1/2], Step [62450/64305], Loss: 4.7145\n",
      "Epoch [1/2], Step [62460/64305], Loss: 4.9215\n",
      "Epoch [1/2], Step [62470/64305], Loss: 4.9049\n",
      "Epoch [1/2], Step [62480/64305], Loss: 4.9970\n",
      "Epoch [1/2], Step [62490/64305], Loss: 5.1877\n",
      "Epoch [1/2], Step [62500/64305], Loss: 5.0686\n",
      "Epoch [1/2], Step [62510/64305], Loss: 4.8317\n",
      "Epoch [1/2], Step [62520/64305], Loss: 4.8626\n",
      "Epoch [1/2], Step [62530/64305], Loss: 4.9748\n",
      "Epoch [1/2], Step [62540/64305], Loss: 4.8460\n",
      "Epoch [1/2], Step [62550/64305], Loss: 4.9741\n",
      "Epoch [1/2], Step [62560/64305], Loss: 4.7651\n",
      "Epoch [1/2], Step [62570/64305], Loss: 4.7812\n",
      "Epoch [1/2], Step [62580/64305], Loss: 5.1457\n",
      "Epoch [1/2], Step [62590/64305], Loss: 4.9353\n",
      "Epoch [1/2], Step [62600/64305], Loss: 4.8892\n",
      "Epoch [1/2], Step [62610/64305], Loss: 4.9011\n",
      "Epoch [1/2], Step [62620/64305], Loss: 4.9032\n",
      "Epoch [1/2], Step [62630/64305], Loss: 4.9329\n",
      "Epoch [1/2], Step [62640/64305], Loss: 4.9020\n",
      "Epoch [1/2], Step [62650/64305], Loss: 4.9008\n",
      "Epoch [1/2], Step [62660/64305], Loss: 4.9427\n",
      "Epoch [1/2], Step [62670/64305], Loss: 5.0123\n",
      "Epoch [1/2], Step [62680/64305], Loss: 4.7363\n",
      "Epoch [1/2], Step [62690/64305], Loss: 4.8062\n",
      "Epoch [1/2], Step [62700/64305], Loss: 4.8417\n",
      "Epoch [1/2], Step [62710/64305], Loss: 4.9932\n",
      "Epoch [1/2], Step [62720/64305], Loss: 5.0138\n",
      "Epoch [1/2], Step [62730/64305], Loss: 4.8842\n",
      "Epoch [1/2], Step [62740/64305], Loss: 4.9470\n",
      "Epoch [1/2], Step [62750/64305], Loss: 4.9627\n",
      "Epoch [1/2], Step [62760/64305], Loss: 4.8924\n",
      "Epoch [1/2], Step [62770/64305], Loss: 4.7347\n",
      "Epoch [1/2], Step [62780/64305], Loss: 4.8579\n",
      "Epoch [1/2], Step [62790/64305], Loss: 4.9604\n",
      "Epoch [1/2], Step [62800/64305], Loss: 4.9394\n",
      "Epoch [1/2], Step [62810/64305], Loss: 4.8291\n",
      "Epoch [1/2], Step [62820/64305], Loss: 4.8360\n",
      "Epoch [1/2], Step [62830/64305], Loss: 5.0085\n",
      "Epoch [1/2], Step [62840/64305], Loss: 5.0548\n",
      "Epoch [1/2], Step [62850/64305], Loss: 4.7876\n",
      "Epoch [1/2], Step [62860/64305], Loss: 4.9754\n",
      "Epoch [1/2], Step [62870/64305], Loss: 4.9626\n",
      "Epoch [1/2], Step [62880/64305], Loss: 5.1731\n",
      "Epoch [1/2], Step [62890/64305], Loss: 4.9038\n",
      "Epoch [1/2], Step [62900/64305], Loss: 4.9022\n",
      "Epoch [1/2], Step [62910/64305], Loss: 5.0704\n",
      "Epoch [1/2], Step [62920/64305], Loss: 4.8902\n",
      "Epoch [1/2], Step [62930/64305], Loss: 4.9238\n",
      "Epoch [1/2], Step [62940/64305], Loss: 4.9322\n",
      "Epoch [1/2], Step [62950/64305], Loss: 4.8649\n",
      "Epoch [1/2], Step [62960/64305], Loss: 4.8868\n",
      "Epoch [1/2], Step [62970/64305], Loss: 4.9846\n",
      "Epoch [1/2], Step [62980/64305], Loss: 4.8264\n",
      "Epoch [1/2], Step [62990/64305], Loss: 4.8301\n",
      "Epoch [1/2], Step [63000/64305], Loss: 4.8157\n",
      "Epoch [1/2], Step [63010/64305], Loss: 5.0864\n",
      "Epoch [1/2], Step [63020/64305], Loss: 5.0021\n",
      "Epoch [1/2], Step [63030/64305], Loss: 4.8851\n",
      "Epoch [1/2], Step [63040/64305], Loss: 4.9363\n",
      "Epoch [1/2], Step [63050/64305], Loss: 4.9140\n",
      "Epoch [1/2], Step [63060/64305], Loss: 4.8964\n",
      "Epoch [1/2], Step [63070/64305], Loss: 4.9773\n",
      "Epoch [1/2], Step [63080/64305], Loss: 5.0084\n",
      "Epoch [1/2], Step [63090/64305], Loss: 4.9557\n",
      "Epoch [1/2], Step [63100/64305], Loss: 5.0532\n",
      "Epoch [1/2], Step [63110/64305], Loss: 5.1011\n",
      "Epoch [1/2], Step [63120/64305], Loss: 4.8779\n",
      "Epoch [1/2], Step [63130/64305], Loss: 4.9486\n",
      "Epoch [1/2], Step [63140/64305], Loss: 4.8564\n",
      "Epoch [1/2], Step [63150/64305], Loss: 5.0625\n",
      "Epoch [1/2], Step [63160/64305], Loss: 4.9095\n",
      "Epoch [1/2], Step [63170/64305], Loss: 4.8667\n",
      "Epoch [1/2], Step [63180/64305], Loss: 4.8895\n",
      "Epoch [1/2], Step [63190/64305], Loss: 4.8143\n",
      "Epoch [1/2], Step [63200/64305], Loss: 4.7893\n",
      "Epoch [1/2], Step [63210/64305], Loss: 4.9503\n",
      "Epoch [1/2], Step [63220/64305], Loss: 4.8281\n",
      "Epoch [1/2], Step [63230/64305], Loss: 5.1037\n",
      "Epoch [1/2], Step [63240/64305], Loss: 5.0381\n",
      "Epoch [1/2], Step [63250/64305], Loss: 4.9676\n",
      "Epoch [1/2], Step [63260/64305], Loss: 4.9704\n",
      "Epoch [1/2], Step [63270/64305], Loss: 5.0122\n",
      "Epoch [1/2], Step [63280/64305], Loss: 4.9053\n",
      "Epoch [1/2], Step [63290/64305], Loss: 4.8921\n",
      "Epoch [1/2], Step [63300/64305], Loss: 5.0064\n",
      "Epoch [1/2], Step [63310/64305], Loss: 4.9096\n",
      "Epoch [1/2], Step [63320/64305], Loss: 4.9248\n",
      "Epoch [1/2], Step [63330/64305], Loss: 4.9791\n",
      "Epoch [1/2], Step [63340/64305], Loss: 4.9893\n",
      "Epoch [1/2], Step [63350/64305], Loss: 4.9437\n",
      "Epoch [1/2], Step [63360/64305], Loss: 5.0218\n",
      "Epoch [1/2], Step [63370/64305], Loss: 4.7122\n",
      "Epoch [1/2], Step [63380/64305], Loss: 4.8893\n",
      "Epoch [1/2], Step [63390/64305], Loss: 4.8314\n",
      "Epoch [1/2], Step [63400/64305], Loss: 4.6851\n",
      "Epoch [1/2], Step [63410/64305], Loss: 4.9170\n",
      "Epoch [1/2], Step [63420/64305], Loss: 4.8657\n",
      "Epoch [1/2], Step [63430/64305], Loss: 4.8396\n",
      "Epoch [1/2], Step [63440/64305], Loss: 4.9198\n",
      "Epoch [1/2], Step [63450/64305], Loss: 5.0059\n",
      "Epoch [1/2], Step [63460/64305], Loss: 5.0506\n",
      "Epoch [1/2], Step [63470/64305], Loss: 4.8823\n",
      "Epoch [1/2], Step [63480/64305], Loss: 4.9452\n",
      "Epoch [1/2], Step [63490/64305], Loss: 4.6892\n",
      "Epoch [1/2], Step [63500/64305], Loss: 4.8659\n",
      "Epoch [1/2], Step [63510/64305], Loss: 4.9503\n",
      "Epoch [1/2], Step [63520/64305], Loss: 5.0292\n",
      "Epoch [1/2], Step [63530/64305], Loss: 4.8121\n",
      "Epoch [1/2], Step [63540/64305], Loss: 4.9018\n",
      "Epoch [1/2], Step [63550/64305], Loss: 4.8407\n",
      "Epoch [1/2], Step [63560/64305], Loss: 4.8787\n",
      "Epoch [1/2], Step [63570/64305], Loss: 4.7979\n",
      "Epoch [1/2], Step [63580/64305], Loss: 4.9092\n",
      "Epoch [1/2], Step [63590/64305], Loss: 4.9065\n",
      "Epoch [1/2], Step [63600/64305], Loss: 5.0587\n",
      "Epoch [1/2], Step [63610/64305], Loss: 4.6892\n",
      "Epoch [1/2], Step [63620/64305], Loss: 5.0217\n",
      "Epoch [1/2], Step [63630/64305], Loss: 4.8366\n",
      "Epoch [1/2], Step [63640/64305], Loss: 4.9727\n",
      "Epoch [1/2], Step [63650/64305], Loss: 4.8732\n",
      "Epoch [1/2], Step [63660/64305], Loss: 4.9024\n",
      "Epoch [1/2], Step [63670/64305], Loss: 5.0270\n",
      "Epoch [1/2], Step [63680/64305], Loss: 5.0831\n",
      "Epoch [1/2], Step [63690/64305], Loss: 4.8872\n",
      "Epoch [1/2], Step [63700/64305], Loss: 5.0644\n",
      "Epoch [1/2], Step [63710/64305], Loss: 4.9290\n",
      "Epoch [1/2], Step [63720/64305], Loss: 4.9416\n",
      "Epoch [1/2], Step [63730/64305], Loss: 4.8862\n",
      "Epoch [1/2], Step [63740/64305], Loss: 5.0156\n",
      "Epoch [1/2], Step [63750/64305], Loss: 4.9972\n",
      "Epoch [1/2], Step [63760/64305], Loss: 4.8853\n",
      "Epoch [1/2], Step [63770/64305], Loss: 4.9272\n",
      "Epoch [1/2], Step [63780/64305], Loss: 4.8292\n",
      "Epoch [1/2], Step [63790/64305], Loss: 4.8308\n",
      "Epoch [1/2], Step [63800/64305], Loss: 4.8704\n",
      "Epoch [1/2], Step [63810/64305], Loss: 4.9568\n",
      "Epoch [1/2], Step [63820/64305], Loss: 4.8348\n",
      "Epoch [1/2], Step [63830/64305], Loss: 5.0276\n",
      "Epoch [1/2], Step [63840/64305], Loss: 4.8523\n",
      "Epoch [1/2], Step [63850/64305], Loss: 4.8587\n",
      "Epoch [1/2], Step [63860/64305], Loss: 4.8861\n",
      "Epoch [1/2], Step [63870/64305], Loss: 4.8783\n",
      "Epoch [1/2], Step [63880/64305], Loss: 4.9986\n",
      "Epoch [1/2], Step [63890/64305], Loss: 5.0039\n",
      "Epoch [1/2], Step [63900/64305], Loss: 4.7981\n",
      "Epoch [1/2], Step [63910/64305], Loss: 4.7243\n",
      "Epoch [1/2], Step [63920/64305], Loss: 4.9280\n",
      "Epoch [1/2], Step [63930/64305], Loss: 4.7538\n",
      "Epoch [1/2], Step [63940/64305], Loss: 4.9384\n",
      "Epoch [1/2], Step [63950/64305], Loss: 4.8414\n",
      "Epoch [1/2], Step [63960/64305], Loss: 5.0508\n",
      "Epoch [1/2], Step [63970/64305], Loss: 4.9728\n",
      "Epoch [1/2], Step [63980/64305], Loss: 4.7269\n",
      "Epoch [1/2], Step [63990/64305], Loss: 4.9051\n",
      "Epoch [1/2], Step [64000/64305], Loss: 4.7449\n",
      "Epoch [1/2], Step [64010/64305], Loss: 4.8652\n",
      "Epoch [1/2], Step [64020/64305], Loss: 4.9426\n",
      "Epoch [1/2], Step [64030/64305], Loss: 4.9168\n",
      "Epoch [1/2], Step [64040/64305], Loss: 4.7280\n",
      "Epoch [1/2], Step [64050/64305], Loss: 4.9596\n",
      "Epoch [1/2], Step [64060/64305], Loss: 4.7829\n",
      "Epoch [1/2], Step [64070/64305], Loss: 4.9412\n",
      "Epoch [1/2], Step [64080/64305], Loss: 4.8544\n",
      "Epoch [1/2], Step [64090/64305], Loss: 4.8434\n",
      "Epoch [1/2], Step [64100/64305], Loss: 4.8658\n",
      "Epoch [1/2], Step [64110/64305], Loss: 4.8057\n",
      "Epoch [1/2], Step [64120/64305], Loss: 4.8024\n",
      "Epoch [1/2], Step [64130/64305], Loss: 4.9895\n",
      "Epoch [1/2], Step [64140/64305], Loss: 4.9941\n",
      "Epoch [1/2], Step [64150/64305], Loss: 4.7893\n",
      "Epoch [1/2], Step [64160/64305], Loss: 4.8563\n",
      "Epoch [1/2], Step [64170/64305], Loss: 5.1264\n",
      "Epoch [1/2], Step [64180/64305], Loss: 4.9459\n",
      "Epoch [1/2], Step [64190/64305], Loss: 4.8819\n",
      "Epoch [1/2], Step [64200/64305], Loss: 4.8221\n",
      "Epoch [1/2], Step [64210/64305], Loss: 4.8868\n",
      "Epoch [1/2], Step [64220/64305], Loss: 4.9212\n",
      "Epoch [1/2], Step [64230/64305], Loss: 4.8800\n",
      "Epoch [1/2], Step [64240/64305], Loss: 4.7563\n",
      "Epoch [1/2], Step [64250/64305], Loss: 4.8162\n",
      "Epoch [1/2], Step [64260/64305], Loss: 5.0136\n",
      "Epoch [1/2], Step [64270/64305], Loss: 5.0610\n",
      "Epoch [1/2], Step [64280/64305], Loss: 5.0678\n",
      "Epoch [1/2], Step [64290/64305], Loss: 5.2037\n",
      "Epoch [1/2], Step [64300/64305], Loss: 4.8807\n",
      "Epoch [1/2] Average Loss: 5.1356, Perplexity: 169.96\n",
      "Epoch [2/2], Step [0/64305], Loss: 4.9289\n",
      "Epoch [2/2], Step [10/64305], Loss: 4.8627\n",
      "Epoch [2/2], Step [20/64305], Loss: 4.8059\n",
      "Epoch [2/2], Step [30/64305], Loss: 4.9221\n",
      "Epoch [2/2], Step [40/64305], Loss: 4.9269\n",
      "Epoch [2/2], Step [50/64305], Loss: 4.8230\n",
      "Epoch [2/2], Step [60/64305], Loss: 4.9520\n",
      "Epoch [2/2], Step [70/64305], Loss: 4.9992\n",
      "Epoch [2/2], Step [80/64305], Loss: 5.1129\n",
      "Epoch [2/2], Step [90/64305], Loss: 4.8699\n",
      "Epoch [2/2], Step [100/64305], Loss: 4.8415\n",
      "Epoch [2/2], Step [110/64305], Loss: 4.7025\n",
      "Epoch [2/2], Step [120/64305], Loss: 5.0288\n",
      "Epoch [2/2], Step [130/64305], Loss: 4.8730\n",
      "Epoch [2/2], Step [140/64305], Loss: 4.6209\n",
      "Epoch [2/2], Step [150/64305], Loss: 4.9488\n",
      "Epoch [2/2], Step [160/64305], Loss: 4.8983\n",
      "Epoch [2/2], Step [170/64305], Loss: 4.8996\n",
      "Epoch [2/2], Step [180/64305], Loss: 4.8811\n",
      "Epoch [2/2], Step [190/64305], Loss: 5.0854\n",
      "Epoch [2/2], Step [200/64305], Loss: 4.7688\n",
      "Epoch [2/2], Step [210/64305], Loss: 4.8911\n",
      "Epoch [2/2], Step [220/64305], Loss: 5.1313\n",
      "Epoch [2/2], Step [230/64305], Loss: 5.0702\n",
      "Epoch [2/2], Step [240/64305], Loss: 4.7444\n",
      "Epoch [2/2], Step [250/64305], Loss: 4.7223\n",
      "Epoch [2/2], Step [260/64305], Loss: 4.7920\n",
      "Epoch [2/2], Step [270/64305], Loss: 5.0550\n",
      "Epoch [2/2], Step [280/64305], Loss: 4.6941\n",
      "Epoch [2/2], Step [290/64305], Loss: 4.8792\n",
      "Epoch [2/2], Step [300/64305], Loss: 4.6093\n",
      "Epoch [2/2], Step [310/64305], Loss: 4.9293\n",
      "Epoch [2/2], Step [320/64305], Loss: 4.7794\n",
      "Epoch [2/2], Step [330/64305], Loss: 4.9491\n",
      "Epoch [2/2], Step [340/64305], Loss: 5.0337\n",
      "Epoch [2/2], Step [350/64305], Loss: 5.0311\n",
      "Epoch [2/2], Step [360/64305], Loss: 4.8365\n",
      "Epoch [2/2], Step [370/64305], Loss: 4.9977\n",
      "Epoch [2/2], Step [380/64305], Loss: 5.1147\n",
      "Epoch [2/2], Step [390/64305], Loss: 4.8696\n",
      "Epoch [2/2], Step [400/64305], Loss: 4.8483\n",
      "Epoch [2/2], Step [410/64305], Loss: 4.7665\n",
      "Epoch [2/2], Step [420/64305], Loss: 4.8435\n",
      "Epoch [2/2], Step [430/64305], Loss: 4.9278\n",
      "Epoch [2/2], Step [440/64305], Loss: 4.7692\n",
      "Epoch [2/2], Step [450/64305], Loss: 4.8758\n",
      "Epoch [2/2], Step [460/64305], Loss: 4.9689\n",
      "Epoch [2/2], Step [470/64305], Loss: 5.0513\n",
      "Epoch [2/2], Step [480/64305], Loss: 4.9705\n",
      "Epoch [2/2], Step [490/64305], Loss: 4.8012\n",
      "Epoch [2/2], Step [500/64305], Loss: 4.7364\n",
      "Epoch [2/2], Step [510/64305], Loss: 4.9160\n",
      "Epoch [2/2], Step [520/64305], Loss: 4.9843\n",
      "Epoch [2/2], Step [530/64305], Loss: 4.7813\n",
      "Epoch [2/2], Step [540/64305], Loss: 4.8755\n",
      "Epoch [2/2], Step [550/64305], Loss: 4.8534\n",
      "Epoch [2/2], Step [560/64305], Loss: 4.5775\n",
      "Epoch [2/2], Step [570/64305], Loss: 4.8413\n",
      "Epoch [2/2], Step [580/64305], Loss: 4.9485\n",
      "Epoch [2/2], Step [590/64305], Loss: 4.8196\n",
      "Epoch [2/2], Step [600/64305], Loss: 4.9751\n",
      "Epoch [2/2], Step [610/64305], Loss: 5.0566\n",
      "Epoch [2/2], Step [620/64305], Loss: 5.0727\n",
      "Epoch [2/2], Step [630/64305], Loss: 4.9099\n",
      "Epoch [2/2], Step [640/64305], Loss: 4.9095\n",
      "Epoch [2/2], Step [650/64305], Loss: 4.9983\n",
      "Epoch [2/2], Step [660/64305], Loss: 5.0584\n",
      "Epoch [2/2], Step [670/64305], Loss: 4.9283\n",
      "Epoch [2/2], Step [680/64305], Loss: 5.0040\n",
      "Epoch [2/2], Step [690/64305], Loss: 4.8643\n",
      "Epoch [2/2], Step [700/64305], Loss: 4.9166\n",
      "Epoch [2/2], Step [710/64305], Loss: 4.8270\n",
      "Epoch [2/2], Step [720/64305], Loss: 4.9936\n",
      "Epoch [2/2], Step [730/64305], Loss: 4.7538\n",
      "Epoch [2/2], Step [740/64305], Loss: 4.9438\n",
      "Epoch [2/2], Step [750/64305], Loss: 4.9118\n",
      "Epoch [2/2], Step [760/64305], Loss: 4.7995\n",
      "Epoch [2/2], Step [770/64305], Loss: 4.9352\n",
      "Epoch [2/2], Step [780/64305], Loss: 4.8540\n",
      "Epoch [2/2], Step [790/64305], Loss: 4.8048\n",
      "Epoch [2/2], Step [800/64305], Loss: 4.9576\n",
      "Epoch [2/2], Step [810/64305], Loss: 4.9016\n",
      "Epoch [2/2], Step [820/64305], Loss: 5.0025\n",
      "Epoch [2/2], Step [830/64305], Loss: 4.9947\n",
      "Epoch [2/2], Step [840/64305], Loss: 4.8411\n",
      "Epoch [2/2], Step [850/64305], Loss: 4.9754\n",
      "Epoch [2/2], Step [860/64305], Loss: 4.8474\n",
      "Epoch [2/2], Step [870/64305], Loss: 4.9225\n",
      "Epoch [2/2], Step [880/64305], Loss: 4.8438\n",
      "Epoch [2/2], Step [890/64305], Loss: 4.6656\n",
      "Epoch [2/2], Step [900/64305], Loss: 4.9383\n",
      "Epoch [2/2], Step [910/64305], Loss: 4.9242\n",
      "Epoch [2/2], Step [920/64305], Loss: 4.9432\n",
      "Epoch [2/2], Step [930/64305], Loss: 4.7434\n",
      "Epoch [2/2], Step [940/64305], Loss: 4.9316\n",
      "Epoch [2/2], Step [950/64305], Loss: 4.9263\n",
      "Epoch [2/2], Step [960/64305], Loss: 4.9114\n",
      "Epoch [2/2], Step [970/64305], Loss: 4.9128\n",
      "Epoch [2/2], Step [980/64305], Loss: 4.7723\n",
      "Epoch [2/2], Step [990/64305], Loss: 4.8436\n",
      "Epoch [2/2], Step [1000/64305], Loss: 4.8628\n",
      "Epoch [2/2], Step [1010/64305], Loss: 4.8633\n",
      "Epoch [2/2], Step [1020/64305], Loss: 4.5976\n",
      "Epoch [2/2], Step [1030/64305], Loss: 5.0277\n",
      "Epoch [2/2], Step [1040/64305], Loss: 4.7475\n",
      "Epoch [2/2], Step [1050/64305], Loss: 4.9798\n",
      "Epoch [2/2], Step [1060/64305], Loss: 4.7837\n",
      "Epoch [2/2], Step [1070/64305], Loss: 4.8535\n",
      "Epoch [2/2], Step [1080/64305], Loss: 4.8283\n",
      "Epoch [2/2], Step [1090/64305], Loss: 4.9632\n",
      "Epoch [2/2], Step [1100/64305], Loss: 4.8151\n",
      "Epoch [2/2], Step [1110/64305], Loss: 4.9806\n",
      "Epoch [2/2], Step [1120/64305], Loss: 4.7600\n",
      "Epoch [2/2], Step [1130/64305], Loss: 4.9264\n",
      "Epoch [2/2], Step [1140/64305], Loss: 4.8435\n",
      "Epoch [2/2], Step [1150/64305], Loss: 4.8578\n",
      "Epoch [2/2], Step [1160/64305], Loss: 4.9080\n",
      "Epoch [2/2], Step [1170/64305], Loss: 4.8465\n",
      "Epoch [2/2], Step [1180/64305], Loss: 4.8614\n",
      "Epoch [2/2], Step [1190/64305], Loss: 5.1347\n",
      "Epoch [2/2], Step [1200/64305], Loss: 5.0125\n",
      "Epoch [2/2], Step [1210/64305], Loss: 4.9639\n",
      "Epoch [2/2], Step [1220/64305], Loss: 5.0145\n",
      "Epoch [2/2], Step [1230/64305], Loss: 4.9194\n",
      "Epoch [2/2], Step [1240/64305], Loss: 5.0586\n",
      "Epoch [2/2], Step [1250/64305], Loss: 4.9570\n",
      "Epoch [2/2], Step [1260/64305], Loss: 4.7561\n",
      "Epoch [2/2], Step [1270/64305], Loss: 4.8955\n",
      "Epoch [2/2], Step [1280/64305], Loss: 4.9074\n",
      "Epoch [2/2], Step [1290/64305], Loss: 4.9705\n",
      "Epoch [2/2], Step [1300/64305], Loss: 4.9931\n",
      "Epoch [2/2], Step [1310/64305], Loss: 4.9486\n",
      "Epoch [2/2], Step [1320/64305], Loss: 4.9230\n",
      "Epoch [2/2], Step [1330/64305], Loss: 4.8295\n",
      "Epoch [2/2], Step [1340/64305], Loss: 5.0411\n",
      "Epoch [2/2], Step [1350/64305], Loss: 4.8812\n",
      "Epoch [2/2], Step [1360/64305], Loss: 4.9811\n",
      "Epoch [2/2], Step [1370/64305], Loss: 4.8265\n",
      "Epoch [2/2], Step [1380/64305], Loss: 4.8922\n",
      "Epoch [2/2], Step [1390/64305], Loss: 4.8918\n",
      "Epoch [2/2], Step [1400/64305], Loss: 4.7239\n",
      "Epoch [2/2], Step [1410/64305], Loss: 4.7829\n",
      "Epoch [2/2], Step [1420/64305], Loss: 5.1230\n",
      "Epoch [2/2], Step [1430/64305], Loss: 4.9424\n",
      "Epoch [2/2], Step [1440/64305], Loss: 4.7759\n",
      "Epoch [2/2], Step [1450/64305], Loss: 4.9201\n",
      "Epoch [2/2], Step [1460/64305], Loss: 5.0063\n",
      "Epoch [2/2], Step [1470/64305], Loss: 4.7833\n",
      "Epoch [2/2], Step [1480/64305], Loss: 4.9910\n",
      "Epoch [2/2], Step [1490/64305], Loss: 4.9977\n",
      "Epoch [2/2], Step [1500/64305], Loss: 5.0384\n",
      "Epoch [2/2], Step [1510/64305], Loss: 4.6868\n",
      "Epoch [2/2], Step [1520/64305], Loss: 4.8465\n",
      "Epoch [2/2], Step [1530/64305], Loss: 4.9089\n",
      "Epoch [2/2], Step [1540/64305], Loss: 5.1014\n",
      "Epoch [2/2], Step [1550/64305], Loss: 4.9063\n",
      "Epoch [2/2], Step [1560/64305], Loss: 4.8241\n",
      "Epoch [2/2], Step [1570/64305], Loss: 5.0079\n",
      "Epoch [2/2], Step [1580/64305], Loss: 5.0720\n",
      "Epoch [2/2], Step [1590/64305], Loss: 4.7759\n",
      "Epoch [2/2], Step [1600/64305], Loss: 4.7043\n",
      "Epoch [2/2], Step [1610/64305], Loss: 4.9101\n",
      "Epoch [2/2], Step [1620/64305], Loss: 4.7117\n",
      "Epoch [2/2], Step [1630/64305], Loss: 4.6111\n",
      "Epoch [2/2], Step [1640/64305], Loss: 4.7219\n",
      "Epoch [2/2], Step [1650/64305], Loss: 4.9794\n",
      "Epoch [2/2], Step [1660/64305], Loss: 4.8124\n",
      "Epoch [2/2], Step [1670/64305], Loss: 4.8916\n",
      "Epoch [2/2], Step [1680/64305], Loss: 4.6896\n",
      "Epoch [2/2], Step [1690/64305], Loss: 4.8427\n",
      "Epoch [2/2], Step [1700/64305], Loss: 4.7999\n",
      "Epoch [2/2], Step [1710/64305], Loss: 4.9316\n",
      "Epoch [2/2], Step [1720/64305], Loss: 4.9947\n",
      "Epoch [2/2], Step [1730/64305], Loss: 4.9215\n",
      "Epoch [2/2], Step [1740/64305], Loss: 4.9070\n",
      "Epoch [2/2], Step [1750/64305], Loss: 4.9581\n",
      "Epoch [2/2], Step [1760/64305], Loss: 5.0457\n",
      "Epoch [2/2], Step [1770/64305], Loss: 4.8583\n",
      "Epoch [2/2], Step [1780/64305], Loss: 4.7127\n",
      "Epoch [2/2], Step [1790/64305], Loss: 4.9817\n",
      "Epoch [2/2], Step [1800/64305], Loss: 4.8055\n",
      "Epoch [2/2], Step [1810/64305], Loss: 4.9839\n",
      "Epoch [2/2], Step [1820/64305], Loss: 4.9129\n",
      "Epoch [2/2], Step [1830/64305], Loss: 4.7971\n",
      "Epoch [2/2], Step [1840/64305], Loss: 4.7898\n",
      "Epoch [2/2], Step [1850/64305], Loss: 4.8432\n",
      "Epoch [2/2], Step [1860/64305], Loss: 5.0333\n",
      "Epoch [2/2], Step [1870/64305], Loss: 4.8872\n",
      "Epoch [2/2], Step [1880/64305], Loss: 4.6417\n",
      "Epoch [2/2], Step [1890/64305], Loss: 5.0347\n",
      "Epoch [2/2], Step [1900/64305], Loss: 4.7734\n",
      "Epoch [2/2], Step [1910/64305], Loss: 4.7738\n",
      "Epoch [2/2], Step [1920/64305], Loss: 4.8644\n",
      "Epoch [2/2], Step [1930/64305], Loss: 4.7567\n",
      "Epoch [2/2], Step [1940/64305], Loss: 4.9657\n",
      "Epoch [2/2], Step [1950/64305], Loss: 4.6784\n",
      "Epoch [2/2], Step [1960/64305], Loss: 5.0581\n",
      "Epoch [2/2], Step [1970/64305], Loss: 4.8547\n",
      "Epoch [2/2], Step [1980/64305], Loss: 4.9842\n",
      "Epoch [2/2], Step [1990/64305], Loss: 4.8035\n",
      "Epoch [2/2], Step [2000/64305], Loss: 4.8873\n",
      "Epoch [2/2], Step [2010/64305], Loss: 4.8879\n",
      "Epoch [2/2], Step [2020/64305], Loss: 4.9470\n",
      "Epoch [2/2], Step [2030/64305], Loss: 4.9753\n",
      "Epoch [2/2], Step [2040/64305], Loss: 4.8836\n",
      "Epoch [2/2], Step [2050/64305], Loss: 4.7947\n",
      "Epoch [2/2], Step [2060/64305], Loss: 4.8457\n",
      "Epoch [2/2], Step [2070/64305], Loss: 4.7881\n",
      "Epoch [2/2], Step [2080/64305], Loss: 4.8948\n",
      "Epoch [2/2], Step [2090/64305], Loss: 4.9837\n",
      "Epoch [2/2], Step [2100/64305], Loss: 5.1264\n",
      "Epoch [2/2], Step [2110/64305], Loss: 4.7835\n",
      "Epoch [2/2], Step [2120/64305], Loss: 4.9629\n",
      "Epoch [2/2], Step [2130/64305], Loss: 4.9312\n",
      "Epoch [2/2], Step [2140/64305], Loss: 4.8402\n",
      "Epoch [2/2], Step [2150/64305], Loss: 4.9594\n",
      "Epoch [2/2], Step [2160/64305], Loss: 4.8522\n",
      "Epoch [2/2], Step [2170/64305], Loss: 4.8428\n",
      "Epoch [2/2], Step [2180/64305], Loss: 5.0405\n",
      "Epoch [2/2], Step [2190/64305], Loss: 5.0298\n",
      "Epoch [2/2], Step [2200/64305], Loss: 4.6969\n",
      "Epoch [2/2], Step [2210/64305], Loss: 4.9413\n",
      "Epoch [2/2], Step [2220/64305], Loss: 4.8785\n",
      "Epoch [2/2], Step [2230/64305], Loss: 4.7073\n",
      "Epoch [2/2], Step [2240/64305], Loss: 4.9948\n",
      "Epoch [2/2], Step [2250/64305], Loss: 4.9215\n",
      "Epoch [2/2], Step [2260/64305], Loss: 4.6869\n",
      "Epoch [2/2], Step [2270/64305], Loss: 4.8540\n",
      "Epoch [2/2], Step [2280/64305], Loss: 4.8888\n",
      "Epoch [2/2], Step [2290/64305], Loss: 4.9961\n",
      "Epoch [2/2], Step [2300/64305], Loss: 5.1406\n",
      "Epoch [2/2], Step [2310/64305], Loss: 4.9332\n",
      "Epoch [2/2], Step [2320/64305], Loss: 4.7579\n",
      "Epoch [2/2], Step [2330/64305], Loss: 4.8874\n",
      "Epoch [2/2], Step [2340/64305], Loss: 4.9261\n",
      "Epoch [2/2], Step [2350/64305], Loss: 4.7010\n",
      "Epoch [2/2], Step [2360/64305], Loss: 4.8837\n",
      "Epoch [2/2], Step [2370/64305], Loss: 4.7654\n",
      "Epoch [2/2], Step [2380/64305], Loss: 4.6564\n",
      "Epoch [2/2], Step [2390/64305], Loss: 4.8610\n",
      "Epoch [2/2], Step [2400/64305], Loss: 4.7361\n",
      "Epoch [2/2], Step [2410/64305], Loss: 5.0362\n",
      "Epoch [2/2], Step [2420/64305], Loss: 4.7416\n",
      "Epoch [2/2], Step [2430/64305], Loss: 4.8986\n",
      "Epoch [2/2], Step [2440/64305], Loss: 4.7776\n",
      "Epoch [2/2], Step [2450/64305], Loss: 4.8602\n",
      "Epoch [2/2], Step [2460/64305], Loss: 4.8110\n",
      "Epoch [2/2], Step [2470/64305], Loss: 4.7785\n",
      "Epoch [2/2], Step [2480/64305], Loss: 5.0023\n",
      "Epoch [2/2], Step [2490/64305], Loss: 5.0137\n",
      "Epoch [2/2], Step [2500/64305], Loss: 4.9622\n",
      "Epoch [2/2], Step [2510/64305], Loss: 4.9139\n",
      "Epoch [2/2], Step [2520/64305], Loss: 4.7563\n",
      "Epoch [2/2], Step [2530/64305], Loss: 4.5785\n",
      "Epoch [2/2], Step [2540/64305], Loss: 4.8094\n",
      "Epoch [2/2], Step [2550/64305], Loss: 4.9420\n",
      "Epoch [2/2], Step [2560/64305], Loss: 4.8356\n",
      "Epoch [2/2], Step [2570/64305], Loss: 4.8291\n",
      "Epoch [2/2], Step [2580/64305], Loss: 4.9455\n",
      "Epoch [2/2], Step [2590/64305], Loss: 4.9419\n",
      "Epoch [2/2], Step [2600/64305], Loss: 4.8811\n",
      "Epoch [2/2], Step [2610/64305], Loss: 4.7410\n",
      "Epoch [2/2], Step [2620/64305], Loss: 4.8454\n",
      "Epoch [2/2], Step [2630/64305], Loss: 4.6847\n",
      "Epoch [2/2], Step [2640/64305], Loss: 4.9208\n",
      "Epoch [2/2], Step [2650/64305], Loss: 4.8787\n",
      "Epoch [2/2], Step [2660/64305], Loss: 5.0361\n",
      "Epoch [2/2], Step [2670/64305], Loss: 4.8943\n",
      "Epoch [2/2], Step [2680/64305], Loss: 5.0632\n",
      "Epoch [2/2], Step [2690/64305], Loss: 5.0565\n",
      "Epoch [2/2], Step [2700/64305], Loss: 4.9155\n",
      "Epoch [2/2], Step [2710/64305], Loss: 4.6868\n",
      "Epoch [2/2], Step [2720/64305], Loss: 4.9508\n",
      "Epoch [2/2], Step [2730/64305], Loss: 5.1028\n",
      "Epoch [2/2], Step [2740/64305], Loss: 4.9971\n",
      "Epoch [2/2], Step [2750/64305], Loss: 4.7045\n",
      "Epoch [2/2], Step [2760/64305], Loss: 4.9281\n",
      "Epoch [2/2], Step [2770/64305], Loss: 5.0264\n",
      "Epoch [2/2], Step [2780/64305], Loss: 4.8957\n",
      "Epoch [2/2], Step [2790/64305], Loss: 4.6907\n",
      "Epoch [2/2], Step [2800/64305], Loss: 4.9135\n",
      "Epoch [2/2], Step [2810/64305], Loss: 4.8859\n",
      "Epoch [2/2], Step [2820/64305], Loss: 4.9790\n",
      "Epoch [2/2], Step [2830/64305], Loss: 4.9036\n",
      "Epoch [2/2], Step [2840/64305], Loss: 5.0430\n",
      "Epoch [2/2], Step [2850/64305], Loss: 5.0640\n",
      "Epoch [2/2], Step [2860/64305], Loss: 5.0277\n",
      "Epoch [2/2], Step [2870/64305], Loss: 4.8118\n",
      "Epoch [2/2], Step [2880/64305], Loss: 4.7216\n",
      "Epoch [2/2], Step [2890/64305], Loss: 4.8195\n",
      "Epoch [2/2], Step [2900/64305], Loss: 5.0647\n",
      "Epoch [2/2], Step [2910/64305], Loss: 4.8386\n",
      "Epoch [2/2], Step [2920/64305], Loss: 4.8309\n",
      "Epoch [2/2], Step [2930/64305], Loss: 5.0324\n",
      "Epoch [2/2], Step [2940/64305], Loss: 4.8567\n",
      "Epoch [2/2], Step [2950/64305], Loss: 4.9502\n",
      "Epoch [2/2], Step [2960/64305], Loss: 5.1059\n",
      "Epoch [2/2], Step [2970/64305], Loss: 4.8664\n",
      "Epoch [2/2], Step [2980/64305], Loss: 4.9188\n",
      "Epoch [2/2], Step [2990/64305], Loss: 4.8293\n",
      "Epoch [2/2], Step [3000/64305], Loss: 4.8939\n",
      "Epoch [2/2], Step [3010/64305], Loss: 4.8150\n",
      "Epoch [2/2], Step [3020/64305], Loss: 4.6732\n",
      "Epoch [2/2], Step [3030/64305], Loss: 4.9273\n",
      "Epoch [2/2], Step [3040/64305], Loss: 4.8424\n",
      "Epoch [2/2], Step [3050/64305], Loss: 4.7956\n",
      "Epoch [2/2], Step [3060/64305], Loss: 4.8954\n",
      "Epoch [2/2], Step [3070/64305], Loss: 4.9147\n",
      "Epoch [2/2], Step [3080/64305], Loss: 4.9332\n",
      "Epoch [2/2], Step [3090/64305], Loss: 4.8752\n",
      "Epoch [2/2], Step [3100/64305], Loss: 4.9751\n",
      "Epoch [2/2], Step [3110/64305], Loss: 4.9265\n",
      "Epoch [2/2], Step [3120/64305], Loss: 4.7747\n",
      "Epoch [2/2], Step [3130/64305], Loss: 5.1170\n",
      "Epoch [2/2], Step [3140/64305], Loss: 5.0640\n",
      "Epoch [2/2], Step [3150/64305], Loss: 4.7753\n",
      "Epoch [2/2], Step [3160/64305], Loss: 4.9227\n",
      "Epoch [2/2], Step [3170/64305], Loss: 5.0182\n",
      "Epoch [2/2], Step [3180/64305], Loss: 4.9723\n",
      "Epoch [2/2], Step [3190/64305], Loss: 4.7603\n",
      "Epoch [2/2], Step [3200/64305], Loss: 4.7266\n",
      "Epoch [2/2], Step [3210/64305], Loss: 5.2264\n",
      "Epoch [2/2], Step [3220/64305], Loss: 5.0389\n",
      "Epoch [2/2], Step [3230/64305], Loss: 4.5714\n",
      "Epoch [2/2], Step [3240/64305], Loss: 5.0225\n",
      "Epoch [2/2], Step [3250/64305], Loss: 5.0221\n",
      "Epoch [2/2], Step [3260/64305], Loss: 4.7858\n",
      "Epoch [2/2], Step [3270/64305], Loss: 4.8571\n",
      "Epoch [2/2], Step [3280/64305], Loss: 5.0237\n",
      "Epoch [2/2], Step [3290/64305], Loss: 4.7604\n",
      "Epoch [2/2], Step [3300/64305], Loss: 5.0172\n",
      "Epoch [2/2], Step [3310/64305], Loss: 4.9386\n",
      "Epoch [2/2], Step [3320/64305], Loss: 5.1179\n",
      "Epoch [2/2], Step [3330/64305], Loss: 4.8778\n",
      "Epoch [2/2], Step [3340/64305], Loss: 5.0138\n",
      "Epoch [2/2], Step [3350/64305], Loss: 5.1065\n",
      "Epoch [2/2], Step [3360/64305], Loss: 4.9036\n",
      "Epoch [2/2], Step [3370/64305], Loss: 4.9854\n",
      "Epoch [2/2], Step [3380/64305], Loss: 4.7915\n",
      "Epoch [2/2], Step [3390/64305], Loss: 4.9166\n",
      "Epoch [2/2], Step [3400/64305], Loss: 4.9998\n",
      "Epoch [2/2], Step [3410/64305], Loss: 4.8623\n",
      "Epoch [2/2], Step [3420/64305], Loss: 4.7501\n",
      "Epoch [2/2], Step [3430/64305], Loss: 5.1628\n",
      "Epoch [2/2], Step [3440/64305], Loss: 4.9064\n",
      "Epoch [2/2], Step [3450/64305], Loss: 5.0385\n",
      "Epoch [2/2], Step [3460/64305], Loss: 4.9942\n",
      "Epoch [2/2], Step [3470/64305], Loss: 5.0420\n",
      "Epoch [2/2], Step [3480/64305], Loss: 4.8558\n",
      "Epoch [2/2], Step [3490/64305], Loss: 4.8233\n",
      "Epoch [2/2], Step [3500/64305], Loss: 4.8012\n",
      "Epoch [2/2], Step [3510/64305], Loss: 4.7270\n",
      "Epoch [2/2], Step [3520/64305], Loss: 4.9237\n",
      "Epoch [2/2], Step [3530/64305], Loss: 4.9762\n",
      "Epoch [2/2], Step [3540/64305], Loss: 4.9313\n",
      "Epoch [2/2], Step [3550/64305], Loss: 4.5984\n",
      "Epoch [2/2], Step [3560/64305], Loss: 4.9965\n",
      "Epoch [2/2], Step [3570/64305], Loss: 4.8817\n",
      "Epoch [2/2], Step [3580/64305], Loss: 4.9084\n",
      "Epoch [2/2], Step [3590/64305], Loss: 4.8641\n",
      "Epoch [2/2], Step [3600/64305], Loss: 4.8342\n",
      "Epoch [2/2], Step [3610/64305], Loss: 4.9006\n",
      "Epoch [2/2], Step [3620/64305], Loss: 4.6181\n",
      "Epoch [2/2], Step [3630/64305], Loss: 4.5729\n",
      "Epoch [2/2], Step [3640/64305], Loss: 4.9340\n",
      "Epoch [2/2], Step [3650/64305], Loss: 4.8363\n",
      "Epoch [2/2], Step [3660/64305], Loss: 4.8835\n",
      "Epoch [2/2], Step [3670/64305], Loss: 4.8295\n",
      "Epoch [2/2], Step [3680/64305], Loss: 5.1147\n",
      "Epoch [2/2], Step [3690/64305], Loss: 4.8170\n",
      "Epoch [2/2], Step [3700/64305], Loss: 4.8080\n",
      "Epoch [2/2], Step [3710/64305], Loss: 4.8568\n",
      "Epoch [2/2], Step [3720/64305], Loss: 4.7946\n",
      "Epoch [2/2], Step [3730/64305], Loss: 4.8939\n",
      "Epoch [2/2], Step [3740/64305], Loss: 4.8030\n",
      "Epoch [2/2], Step [3750/64305], Loss: 4.7461\n",
      "Epoch [2/2], Step [3760/64305], Loss: 4.9104\n",
      "Epoch [2/2], Step [3770/64305], Loss: 4.8775\n",
      "Epoch [2/2], Step [3780/64305], Loss: 4.9506\n",
      "Epoch [2/2], Step [3790/64305], Loss: 4.8622\n",
      "Epoch [2/2], Step [3800/64305], Loss: 4.7885\n",
      "Epoch [2/2], Step [3810/64305], Loss: 4.6813\n",
      "Epoch [2/2], Step [3820/64305], Loss: 4.8102\n",
      "Epoch [2/2], Step [3830/64305], Loss: 4.8975\n",
      "Epoch [2/2], Step [3840/64305], Loss: 5.0069\n",
      "Epoch [2/2], Step [3850/64305], Loss: 4.9586\n",
      "Epoch [2/2], Step [3860/64305], Loss: 4.8239\n",
      "Epoch [2/2], Step [3870/64305], Loss: 4.9413\n",
      "Epoch [2/2], Step [3880/64305], Loss: 4.9913\n",
      "Epoch [2/2], Step [3890/64305], Loss: 4.9406\n",
      "Epoch [2/2], Step [3900/64305], Loss: 4.9193\n",
      "Epoch [2/2], Step [3910/64305], Loss: 5.0150\n",
      "Epoch [2/2], Step [3920/64305], Loss: 4.9546\n",
      "Epoch [2/2], Step [3930/64305], Loss: 4.6585\n",
      "Epoch [2/2], Step [3940/64305], Loss: 5.0266\n",
      "Epoch [2/2], Step [3950/64305], Loss: 4.9260\n",
      "Epoch [2/2], Step [3960/64305], Loss: 4.9151\n",
      "Epoch [2/2], Step [3970/64305], Loss: 4.7461\n",
      "Epoch [2/2], Step [3980/64305], Loss: 4.8860\n",
      "Epoch [2/2], Step [3990/64305], Loss: 4.9077\n",
      "Epoch [2/2], Step [4000/64305], Loss: 4.9388\n",
      "Epoch [2/2], Step [4010/64305], Loss: 4.9032\n",
      "Epoch [2/2], Step [4020/64305], Loss: 4.8067\n",
      "Epoch [2/2], Step [4030/64305], Loss: 4.7381\n",
      "Epoch [2/2], Step [4040/64305], Loss: 4.8091\n",
      "Epoch [2/2], Step [4050/64305], Loss: 4.8486\n",
      "Epoch [2/2], Step [4060/64305], Loss: 4.7845\n",
      "Epoch [2/2], Step [4070/64305], Loss: 4.9282\n",
      "Epoch [2/2], Step [4080/64305], Loss: 4.9100\n",
      "Epoch [2/2], Step [4090/64305], Loss: 4.9212\n",
      "Epoch [2/2], Step [4100/64305], Loss: 4.8556\n",
      "Epoch [2/2], Step [4110/64305], Loss: 4.9290\n",
      "Epoch [2/2], Step [4120/64305], Loss: 5.0504\n",
      "Epoch [2/2], Step [4130/64305], Loss: 4.5654\n",
      "Epoch [2/2], Step [4140/64305], Loss: 4.9812\n",
      "Epoch [2/2], Step [4150/64305], Loss: 4.8303\n",
      "Epoch [2/2], Step [4160/64305], Loss: 4.8115\n",
      "Epoch [2/2], Step [4170/64305], Loss: 4.8638\n",
      "Epoch [2/2], Step [4180/64305], Loss: 4.7799\n",
      "Epoch [2/2], Step [4190/64305], Loss: 4.8693\n",
      "Epoch [2/2], Step [4200/64305], Loss: 4.8635\n",
      "Epoch [2/2], Step [4210/64305], Loss: 4.7874\n",
      "Epoch [2/2], Step [4220/64305], Loss: 4.7754\n",
      "Epoch [2/2], Step [4230/64305], Loss: 5.0330\n",
      "Epoch [2/2], Step [4240/64305], Loss: 4.9102\n",
      "Epoch [2/2], Step [4250/64305], Loss: 5.0135\n",
      "Epoch [2/2], Step [4260/64305], Loss: 4.7725\n",
      "Epoch [2/2], Step [4270/64305], Loss: 4.9606\n",
      "Epoch [2/2], Step [4280/64305], Loss: 4.8850\n",
      "Epoch [2/2], Step [4290/64305], Loss: 4.8159\n",
      "Epoch [2/2], Step [4300/64305], Loss: 5.1934\n",
      "Epoch [2/2], Step [4310/64305], Loss: 4.8378\n",
      "Epoch [2/2], Step [4320/64305], Loss: 4.8745\n",
      "Epoch [2/2], Step [4330/64305], Loss: 4.8351\n",
      "Epoch [2/2], Step [4340/64305], Loss: 4.7238\n",
      "Epoch [2/2], Step [4350/64305], Loss: 4.7337\n",
      "Epoch [2/2], Step [4360/64305], Loss: 4.9047\n",
      "Epoch [2/2], Step [4370/64305], Loss: 5.1250\n",
      "Epoch [2/2], Step [4380/64305], Loss: 4.9468\n",
      "Epoch [2/2], Step [4390/64305], Loss: 4.9287\n",
      "Epoch [2/2], Step [4400/64305], Loss: 5.0222\n",
      "Epoch [2/2], Step [4410/64305], Loss: 4.9387\n",
      "Epoch [2/2], Step [4420/64305], Loss: 4.9611\n",
      "Epoch [2/2], Step [4430/64305], Loss: 5.0548\n",
      "Epoch [2/2], Step [4440/64305], Loss: 4.9571\n",
      "Epoch [2/2], Step [4450/64305], Loss: 4.6050\n",
      "Epoch [2/2], Step [4460/64305], Loss: 4.8384\n",
      "Epoch [2/2], Step [4470/64305], Loss: 4.8812\n",
      "Epoch [2/2], Step [4480/64305], Loss: 5.0609\n",
      "Epoch [2/2], Step [4490/64305], Loss: 4.7226\n",
      "Epoch [2/2], Step [4500/64305], Loss: 4.8491\n",
      "Epoch [2/2], Step [4510/64305], Loss: 4.7242\n",
      "Epoch [2/2], Step [4520/64305], Loss: 5.1188\n",
      "Epoch [2/2], Step [4530/64305], Loss: 4.6902\n",
      "Epoch [2/2], Step [4540/64305], Loss: 4.6250\n",
      "Epoch [2/2], Step [4550/64305], Loss: 4.8888\n",
      "Epoch [2/2], Step [4560/64305], Loss: 4.7630\n",
      "Epoch [2/2], Step [4570/64305], Loss: 4.8448\n",
      "Epoch [2/2], Step [4580/64305], Loss: 4.8936\n",
      "Epoch [2/2], Step [4590/64305], Loss: 4.7611\n",
      "Epoch [2/2], Step [4600/64305], Loss: 4.8733\n",
      "Epoch [2/2], Step [4610/64305], Loss: 4.7854\n",
      "Epoch [2/2], Step [4620/64305], Loss: 4.9629\n",
      "Epoch [2/2], Step [4630/64305], Loss: 5.0526\n",
      "Epoch [2/2], Step [4640/64305], Loss: 4.6757\n",
      "Epoch [2/2], Step [4650/64305], Loss: 4.9792\n",
      "Epoch [2/2], Step [4660/64305], Loss: 5.1690\n",
      "Epoch [2/2], Step [4670/64305], Loss: 4.8253\n",
      "Epoch [2/2], Step [4680/64305], Loss: 4.9227\n",
      "Epoch [2/2], Step [4690/64305], Loss: 4.9516\n",
      "Epoch [2/2], Step [4700/64305], Loss: 4.8560\n",
      "Epoch [2/2], Step [4710/64305], Loss: 4.8635\n",
      "Epoch [2/2], Step [4720/64305], Loss: 4.6938\n",
      "Epoch [2/2], Step [4730/64305], Loss: 4.8142\n",
      "Epoch [2/2], Step [4740/64305], Loss: 4.9569\n",
      "Epoch [2/2], Step [4750/64305], Loss: 4.7544\n",
      "Epoch [2/2], Step [4760/64305], Loss: 4.8950\n",
      "Epoch [2/2], Step [4770/64305], Loss: 4.7547\n",
      "Epoch [2/2], Step [4780/64305], Loss: 4.6120\n",
      "Epoch [2/2], Step [4790/64305], Loss: 4.7480\n",
      "Epoch [2/2], Step [4800/64305], Loss: 5.0466\n",
      "Epoch [2/2], Step [4810/64305], Loss: 4.8495\n",
      "Epoch [2/2], Step [4820/64305], Loss: 4.9217\n",
      "Epoch [2/2], Step [4830/64305], Loss: 4.9105\n",
      "Epoch [2/2], Step [4840/64305], Loss: 4.9071\n",
      "Epoch [2/2], Step [4850/64305], Loss: 5.0172\n",
      "Epoch [2/2], Step [4860/64305], Loss: 4.8082\n",
      "Epoch [2/2], Step [4870/64305], Loss: 4.9019\n",
      "Epoch [2/2], Step [4880/64305], Loss: 4.6456\n",
      "Epoch [2/2], Step [4890/64305], Loss: 5.1232\n",
      "Epoch [2/2], Step [4900/64305], Loss: 4.8430\n",
      "Epoch [2/2], Step [4910/64305], Loss: 4.6781\n",
      "Epoch [2/2], Step [4920/64305], Loss: 5.0843\n",
      "Epoch [2/2], Step [4930/64305], Loss: 4.7951\n",
      "Epoch [2/2], Step [4940/64305], Loss: 4.9171\n",
      "Epoch [2/2], Step [4950/64305], Loss: 4.9275\n",
      "Epoch [2/2], Step [4960/64305], Loss: 4.9734\n",
      "Epoch [2/2], Step [4970/64305], Loss: 4.8513\n",
      "Epoch [2/2], Step [4980/64305], Loss: 4.7707\n",
      "Epoch [2/2], Step [4990/64305], Loss: 4.9984\n",
      "Epoch [2/2], Step [5000/64305], Loss: 4.8847\n",
      "Epoch [2/2], Step [5010/64305], Loss: 4.8231\n",
      "Epoch [2/2], Step [5020/64305], Loss: 4.9138\n",
      "Epoch [2/2], Step [5030/64305], Loss: 4.7453\n",
      "Epoch [2/2], Step [5040/64305], Loss: 4.9276\n",
      "Epoch [2/2], Step [5050/64305], Loss: 4.9538\n",
      "Epoch [2/2], Step [5060/64305], Loss: 5.0390\n",
      "Epoch [2/2], Step [5070/64305], Loss: 4.8056\n",
      "Epoch [2/2], Step [5080/64305], Loss: 4.9835\n",
      "Epoch [2/2], Step [5090/64305], Loss: 4.9527\n",
      "Epoch [2/2], Step [5100/64305], Loss: 4.9619\n",
      "Epoch [2/2], Step [5110/64305], Loss: 4.9576\n",
      "Epoch [2/2], Step [5120/64305], Loss: 4.9202\n",
      "Epoch [2/2], Step [5130/64305], Loss: 5.0868\n",
      "Epoch [2/2], Step [5140/64305], Loss: 4.7290\n",
      "Epoch [2/2], Step [5150/64305], Loss: 5.1700\n",
      "Epoch [2/2], Step [5160/64305], Loss: 4.8256\n",
      "Epoch [2/2], Step [5170/64305], Loss: 4.6349\n",
      "Epoch [2/2], Step [5180/64305], Loss: 4.9424\n",
      "Epoch [2/2], Step [5190/64305], Loss: 4.8088\n",
      "Epoch [2/2], Step [5200/64305], Loss: 5.1565\n",
      "Epoch [2/2], Step [5210/64305], Loss: 4.5742\n",
      "Epoch [2/2], Step [5220/64305], Loss: 5.0809\n",
      "Epoch [2/2], Step [5230/64305], Loss: 4.9942\n",
      "Epoch [2/2], Step [5240/64305], Loss: 4.7094\n",
      "Epoch [2/2], Step [5250/64305], Loss: 4.9846\n",
      "Epoch [2/2], Step [5260/64305], Loss: 5.0640\n",
      "Epoch [2/2], Step [5270/64305], Loss: 4.8322\n",
      "Epoch [2/2], Step [5280/64305], Loss: 4.8585\n",
      "Epoch [2/2], Step [5290/64305], Loss: 4.8841\n",
      "Epoch [2/2], Step [5300/64305], Loss: 4.9007\n",
      "Epoch [2/2], Step [5310/64305], Loss: 4.8616\n",
      "Epoch [2/2], Step [5320/64305], Loss: 4.7208\n",
      "Epoch [2/2], Step [5330/64305], Loss: 4.9672\n",
      "Epoch [2/2], Step [5340/64305], Loss: 4.9976\n",
      "Epoch [2/2], Step [5350/64305], Loss: 4.9584\n",
      "Epoch [2/2], Step [5360/64305], Loss: 4.9887\n",
      "Epoch [2/2], Step [5370/64305], Loss: 5.0020\n",
      "Epoch [2/2], Step [5380/64305], Loss: 4.8334\n",
      "Epoch [2/2], Step [5390/64305], Loss: 4.7887\n",
      "Epoch [2/2], Step [5400/64305], Loss: 4.8652\n",
      "Epoch [2/2], Step [5410/64305], Loss: 4.7469\n",
      "Epoch [2/2], Step [5420/64305], Loss: 4.8472\n",
      "Epoch [2/2], Step [5430/64305], Loss: 5.1220\n",
      "Epoch [2/2], Step [5440/64305], Loss: 4.8516\n",
      "Epoch [2/2], Step [5450/64305], Loss: 5.0158\n",
      "Epoch [2/2], Step [5460/64305], Loss: 4.9125\n",
      "Epoch [2/2], Step [5470/64305], Loss: 4.9870\n",
      "Epoch [2/2], Step [5480/64305], Loss: 4.7800\n",
      "Epoch [2/2], Step [5490/64305], Loss: 4.8027\n",
      "Epoch [2/2], Step [5500/64305], Loss: 4.8299\n",
      "Epoch [2/2], Step [5510/64305], Loss: 4.8520\n",
      "Epoch [2/2], Step [5520/64305], Loss: 4.9363\n",
      "Epoch [2/2], Step [5530/64305], Loss: 4.8067\n",
      "Epoch [2/2], Step [5540/64305], Loss: 5.0710\n",
      "Epoch [2/2], Step [5550/64305], Loss: 4.8631\n",
      "Epoch [2/2], Step [5560/64305], Loss: 4.9789\n",
      "Epoch [2/2], Step [5570/64305], Loss: 4.9397\n",
      "Epoch [2/2], Step [5580/64305], Loss: 5.0771\n",
      "Epoch [2/2], Step [5590/64305], Loss: 5.1525\n",
      "Epoch [2/2], Step [5600/64305], Loss: 4.8409\n",
      "Epoch [2/2], Step [5610/64305], Loss: 4.9138\n",
      "Epoch [2/2], Step [5620/64305], Loss: 4.8256\n",
      "Epoch [2/2], Step [5630/64305], Loss: 5.0056\n",
      "Epoch [2/2], Step [5640/64305], Loss: 5.1184\n",
      "Epoch [2/2], Step [5650/64305], Loss: 4.8380\n",
      "Epoch [2/2], Step [5660/64305], Loss: 4.8661\n",
      "Epoch [2/2], Step [5670/64305], Loss: 4.9821\n",
      "Epoch [2/2], Step [5680/64305], Loss: 4.8766\n",
      "Epoch [2/2], Step [5690/64305], Loss: 4.9967\n",
      "Epoch [2/2], Step [5700/64305], Loss: 4.6990\n",
      "Epoch [2/2], Step [5710/64305], Loss: 5.1174\n",
      "Epoch [2/2], Step [5720/64305], Loss: 4.7958\n",
      "Epoch [2/2], Step [5730/64305], Loss: 4.7754\n",
      "Epoch [2/2], Step [5740/64305], Loss: 5.0513\n",
      "Epoch [2/2], Step [5750/64305], Loss: 4.8269\n",
      "Epoch [2/2], Step [5760/64305], Loss: 4.6619\n",
      "Epoch [2/2], Step [5770/64305], Loss: 4.6185\n",
      "Epoch [2/2], Step [5780/64305], Loss: 4.8560\n",
      "Epoch [2/2], Step [5790/64305], Loss: 4.8379\n",
      "Epoch [2/2], Step [5800/64305], Loss: 4.6527\n",
      "Epoch [2/2], Step [5810/64305], Loss: 4.9160\n",
      "Epoch [2/2], Step [5820/64305], Loss: 4.7020\n",
      "Epoch [2/2], Step [5830/64305], Loss: 5.0327\n",
      "Epoch [2/2], Step [5840/64305], Loss: 4.6252\n",
      "Epoch [2/2], Step [5850/64305], Loss: 4.8616\n",
      "Epoch [2/2], Step [5860/64305], Loss: 4.8524\n",
      "Epoch [2/2], Step [5870/64305], Loss: 4.8955\n",
      "Epoch [2/2], Step [5880/64305], Loss: 4.9395\n",
      "Epoch [2/2], Step [5890/64305], Loss: 4.7864\n",
      "Epoch [2/2], Step [5900/64305], Loss: 4.8471\n",
      "Epoch [2/2], Step [5910/64305], Loss: 4.9191\n",
      "Epoch [2/2], Step [5920/64305], Loss: 4.8636\n",
      "Epoch [2/2], Step [5930/64305], Loss: 5.0964\n",
      "Epoch [2/2], Step [5940/64305], Loss: 5.1628\n",
      "Epoch [2/2], Step [5950/64305], Loss: 4.9206\n",
      "Epoch [2/2], Step [5960/64305], Loss: 4.8126\n",
      "Epoch [2/2], Step [5970/64305], Loss: 4.8707\n",
      "Epoch [2/2], Step [5980/64305], Loss: 4.9920\n",
      "Epoch [2/2], Step [5990/64305], Loss: 4.7421\n",
      "Epoch [2/2], Step [6000/64305], Loss: 5.0690\n",
      "Epoch [2/2], Step [6010/64305], Loss: 4.7521\n",
      "Epoch [2/2], Step [6020/64305], Loss: 4.6962\n",
      "Epoch [2/2], Step [6030/64305], Loss: 4.7124\n",
      "Epoch [2/2], Step [6040/64305], Loss: 4.7954\n",
      "Epoch [2/2], Step [6050/64305], Loss: 4.8458\n",
      "Epoch [2/2], Step [6060/64305], Loss: 4.8584\n",
      "Epoch [2/2], Step [6070/64305], Loss: 4.7069\n",
      "Epoch [2/2], Step [6080/64305], Loss: 4.8103\n",
      "Epoch [2/2], Step [6090/64305], Loss: 4.8622\n",
      "Epoch [2/2], Step [6100/64305], Loss: 4.7506\n",
      "Epoch [2/2], Step [6110/64305], Loss: 5.0575\n",
      "Epoch [2/2], Step [6120/64305], Loss: 4.9162\n",
      "Epoch [2/2], Step [6130/64305], Loss: 4.8870\n",
      "Epoch [2/2], Step [6140/64305], Loss: 4.9101\n",
      "Epoch [2/2], Step [6150/64305], Loss: 4.8175\n",
      "Epoch [2/2], Step [6160/64305], Loss: 4.7868\n",
      "Epoch [2/2], Step [6170/64305], Loss: 4.7948\n",
      "Epoch [2/2], Step [6180/64305], Loss: 5.0795\n",
      "Epoch [2/2], Step [6190/64305], Loss: 4.8046\n",
      "Epoch [2/2], Step [6200/64305], Loss: 4.8276\n",
      "Epoch [2/2], Step [6210/64305], Loss: 5.0035\n",
      "Epoch [2/2], Step [6220/64305], Loss: 5.1236\n",
      "Epoch [2/2], Step [6230/64305], Loss: 4.8629\n",
      "Epoch [2/2], Step [6240/64305], Loss: 4.8381\n",
      "Epoch [2/2], Step [6250/64305], Loss: 4.8510\n",
      "Epoch [2/2], Step [6260/64305], Loss: 4.8537\n",
      "Epoch [2/2], Step [6270/64305], Loss: 4.8210\n",
      "Epoch [2/2], Step [6280/64305], Loss: 4.8968\n",
      "Epoch [2/2], Step [6290/64305], Loss: 4.9846\n",
      "Epoch [2/2], Step [6300/64305], Loss: 4.8430\n",
      "Epoch [2/2], Step [6310/64305], Loss: 4.7707\n",
      "Epoch [2/2], Step [6320/64305], Loss: 4.9146\n",
      "Epoch [2/2], Step [6330/64305], Loss: 4.8572\n",
      "Epoch [2/2], Step [6340/64305], Loss: 5.0205\n",
      "Epoch [2/2], Step [6350/64305], Loss: 4.9193\n",
      "Epoch [2/2], Step [6360/64305], Loss: 4.9438\n",
      "Epoch [2/2], Step [6370/64305], Loss: 4.7813\n",
      "Epoch [2/2], Step [6380/64305], Loss: 5.0419\n",
      "Epoch [2/2], Step [6390/64305], Loss: 5.0660\n",
      "Epoch [2/2], Step [6400/64305], Loss: 5.0366\n",
      "Epoch [2/2], Step [6410/64305], Loss: 4.8305\n",
      "Epoch [2/2], Step [6420/64305], Loss: 4.8002\n",
      "Epoch [2/2], Step [6430/64305], Loss: 5.0354\n",
      "Epoch [2/2], Step [6440/64305], Loss: 4.8402\n",
      "Epoch [2/2], Step [6450/64305], Loss: 4.9216\n",
      "Epoch [2/2], Step [6460/64305], Loss: 4.9507\n",
      "Epoch [2/2], Step [6470/64305], Loss: 4.8814\n",
      "Epoch [2/2], Step [6480/64305], Loss: 4.8535\n",
      "Epoch [2/2], Step [6490/64305], Loss: 4.9120\n",
      "Epoch [2/2], Step [6500/64305], Loss: 5.0776\n",
      "Epoch [2/2], Step [6510/64305], Loss: 5.1239\n",
      "Epoch [2/2], Step [6520/64305], Loss: 4.8798\n",
      "Epoch [2/2], Step [6530/64305], Loss: 4.8278\n",
      "Epoch [2/2], Step [6540/64305], Loss: 4.7204\n",
      "Epoch [2/2], Step [6550/64305], Loss: 4.9459\n",
      "Epoch [2/2], Step [6560/64305], Loss: 5.0036\n",
      "Epoch [2/2], Step [6570/64305], Loss: 5.0540\n",
      "Epoch [2/2], Step [6580/64305], Loss: 4.8698\n",
      "Epoch [2/2], Step [6590/64305], Loss: 4.7989\n",
      "Epoch [2/2], Step [6600/64305], Loss: 4.9765\n",
      "Epoch [2/2], Step [6610/64305], Loss: 4.8281\n",
      "Epoch [2/2], Step [6620/64305], Loss: 4.9850\n",
      "Epoch [2/2], Step [6630/64305], Loss: 5.0312\n",
      "Epoch [2/2], Step [6640/64305], Loss: 4.9229\n",
      "Epoch [2/2], Step [6650/64305], Loss: 5.0340\n",
      "Epoch [2/2], Step [6660/64305], Loss: 4.8882\n",
      "Epoch [2/2], Step [6670/64305], Loss: 5.0205\n",
      "Epoch [2/2], Step [6680/64305], Loss: 4.9111\n",
      "Epoch [2/2], Step [6690/64305], Loss: 4.7602\n",
      "Epoch [2/2], Step [6700/64305], Loss: 4.8848\n",
      "Epoch [2/2], Step [6710/64305], Loss: 4.8379\n",
      "Epoch [2/2], Step [6720/64305], Loss: 4.8353\n",
      "Epoch [2/2], Step [6730/64305], Loss: 4.9351\n",
      "Epoch [2/2], Step [6740/64305], Loss: 4.6711\n",
      "Epoch [2/2], Step [6750/64305], Loss: 4.6743\n",
      "Epoch [2/2], Step [6760/64305], Loss: 4.8028\n",
      "Epoch [2/2], Step [6770/64305], Loss: 4.7830\n",
      "Epoch [2/2], Step [6780/64305], Loss: 4.8088\n",
      "Epoch [2/2], Step [6790/64305], Loss: 4.9846\n",
      "Epoch [2/2], Step [6800/64305], Loss: 4.6405\n",
      "Epoch [2/2], Step [6810/64305], Loss: 4.6911\n",
      "Epoch [2/2], Step [6820/64305], Loss: 4.9593\n",
      "Epoch [2/2], Step [6830/64305], Loss: 4.8352\n",
      "Epoch [2/2], Step [6840/64305], Loss: 5.1072\n",
      "Epoch [2/2], Step [6850/64305], Loss: 5.0149\n",
      "Epoch [2/2], Step [6860/64305], Loss: 4.9207\n",
      "Epoch [2/2], Step [6870/64305], Loss: 5.0666\n",
      "Epoch [2/2], Step [6880/64305], Loss: 4.7979\n",
      "Epoch [2/2], Step [6890/64305], Loss: 4.7896\n",
      "Epoch [2/2], Step [6900/64305], Loss: 5.1074\n",
      "Epoch [2/2], Step [6910/64305], Loss: 4.9237\n",
      "Epoch [2/2], Step [6920/64305], Loss: 4.9617\n",
      "Epoch [2/2], Step [6930/64305], Loss: 4.9699\n",
      "Epoch [2/2], Step [6940/64305], Loss: 4.8700\n",
      "Epoch [2/2], Step [6950/64305], Loss: 4.8830\n",
      "Epoch [2/2], Step [6960/64305], Loss: 4.7705\n",
      "Epoch [2/2], Step [6970/64305], Loss: 4.9038\n",
      "Epoch [2/2], Step [6980/64305], Loss: 4.7373\n",
      "Epoch [2/2], Step [6990/64305], Loss: 4.9509\n",
      "Epoch [2/2], Step [7000/64305], Loss: 4.9387\n",
      "Epoch [2/2], Step [7010/64305], Loss: 4.9581\n",
      "Epoch [2/2], Step [7020/64305], Loss: 5.1113\n",
      "Epoch [2/2], Step [7030/64305], Loss: 5.0387\n",
      "Epoch [2/2], Step [7040/64305], Loss: 4.9502\n",
      "Epoch [2/2], Step [7050/64305], Loss: 4.9037\n",
      "Epoch [2/2], Step [7060/64305], Loss: 4.8089\n",
      "Epoch [2/2], Step [7070/64305], Loss: 4.8652\n",
      "Epoch [2/2], Step [7080/64305], Loss: 4.9296\n",
      "Epoch [2/2], Step [7090/64305], Loss: 4.5288\n",
      "Epoch [2/2], Step [7100/64305], Loss: 4.8820\n",
      "Epoch [2/2], Step [7110/64305], Loss: 5.1008\n",
      "Epoch [2/2], Step [7120/64305], Loss: 5.2111\n",
      "Epoch [2/2], Step [7130/64305], Loss: 4.8882\n",
      "Epoch [2/2], Step [7140/64305], Loss: 4.8555\n",
      "Epoch [2/2], Step [7150/64305], Loss: 4.8920\n",
      "Epoch [2/2], Step [7160/64305], Loss: 4.8451\n",
      "Epoch [2/2], Step [7170/64305], Loss: 5.1126\n",
      "Epoch [2/2], Step [7180/64305], Loss: 5.1858\n",
      "Epoch [2/2], Step [7190/64305], Loss: 4.8583\n",
      "Epoch [2/2], Step [7200/64305], Loss: 5.0093\n",
      "Epoch [2/2], Step [7210/64305], Loss: 4.8680\n",
      "Epoch [2/2], Step [7220/64305], Loss: 4.8518\n",
      "Epoch [2/2], Step [7230/64305], Loss: 4.9320\n",
      "Epoch [2/2], Step [7240/64305], Loss: 5.1464\n",
      "Epoch [2/2], Step [7250/64305], Loss: 4.8744\n",
      "Epoch [2/2], Step [7260/64305], Loss: 4.8170\n",
      "Epoch [2/2], Step [7270/64305], Loss: 4.9228\n",
      "Epoch [2/2], Step [7280/64305], Loss: 4.8973\n",
      "Epoch [2/2], Step [7290/64305], Loss: 4.9735\n",
      "Epoch [2/2], Step [7300/64305], Loss: 4.8718\n",
      "Epoch [2/2], Step [7310/64305], Loss: 4.8477\n",
      "Epoch [2/2], Step [7320/64305], Loss: 4.9839\n",
      "Epoch [2/2], Step [7330/64305], Loss: 4.8777\n",
      "Epoch [2/2], Step [7340/64305], Loss: 4.6576\n",
      "Epoch [2/2], Step [7350/64305], Loss: 4.9717\n",
      "Epoch [2/2], Step [7360/64305], Loss: 4.8316\n",
      "Epoch [2/2], Step [7370/64305], Loss: 4.8309\n",
      "Epoch [2/2], Step [7380/64305], Loss: 4.7697\n",
      "Epoch [2/2], Step [7390/64305], Loss: 4.9133\n",
      "Epoch [2/2], Step [7400/64305], Loss: 4.8108\n",
      "Epoch [2/2], Step [7410/64305], Loss: 4.7990\n",
      "Epoch [2/2], Step [7420/64305], Loss: 4.8594\n",
      "Epoch [2/2], Step [7430/64305], Loss: 4.8877\n",
      "Epoch [2/2], Step [7440/64305], Loss: 4.7053\n",
      "Epoch [2/2], Step [7450/64305], Loss: 4.8066\n",
      "Epoch [2/2], Step [7460/64305], Loss: 4.8684\n",
      "Epoch [2/2], Step [7470/64305], Loss: 4.9431\n",
      "Epoch [2/2], Step [7480/64305], Loss: 4.8352\n",
      "Epoch [2/2], Step [7490/64305], Loss: 4.9982\n",
      "Epoch [2/2], Step [7500/64305], Loss: 4.6576\n",
      "Epoch [2/2], Step [7510/64305], Loss: 5.1104\n",
      "Epoch [2/2], Step [7520/64305], Loss: 4.8982\n",
      "Epoch [2/2], Step [7530/64305], Loss: 4.8678\n",
      "Epoch [2/2], Step [7540/64305], Loss: 4.6901\n",
      "Epoch [2/2], Step [7550/64305], Loss: 4.9406\n",
      "Epoch [2/2], Step [7560/64305], Loss: 5.0534\n",
      "Epoch [2/2], Step [7570/64305], Loss: 4.8114\n",
      "Epoch [2/2], Step [7580/64305], Loss: 5.0646\n",
      "Epoch [2/2], Step [7590/64305], Loss: 4.9256\n",
      "Epoch [2/2], Step [7600/64305], Loss: 5.0607\n",
      "Epoch [2/2], Step [7610/64305], Loss: 4.8568\n",
      "Epoch [2/2], Step [7620/64305], Loss: 5.1866\n",
      "Epoch [2/2], Step [7630/64305], Loss: 5.0519\n",
      "Epoch [2/2], Step [7640/64305], Loss: 5.0032\n",
      "Epoch [2/2], Step [7650/64305], Loss: 4.7561\n",
      "Epoch [2/2], Step [7660/64305], Loss: 5.0504\n",
      "Epoch [2/2], Step [7670/64305], Loss: 4.9014\n",
      "Epoch [2/2], Step [7680/64305], Loss: 4.9876\n",
      "Epoch [2/2], Step [7690/64305], Loss: 5.0083\n",
      "Epoch [2/2], Step [7700/64305], Loss: 4.9086\n",
      "Epoch [2/2], Step [7710/64305], Loss: 4.7782\n",
      "Epoch [2/2], Step [7720/64305], Loss: 5.0537\n",
      "Epoch [2/2], Step [7730/64305], Loss: 4.9393\n",
      "Epoch [2/2], Step [7740/64305], Loss: 4.6737\n",
      "Epoch [2/2], Step [7750/64305], Loss: 4.9448\n",
      "Epoch [2/2], Step [7760/64305], Loss: 4.7879\n",
      "Epoch [2/2], Step [7770/64305], Loss: 5.0359\n",
      "Epoch [2/2], Step [7780/64305], Loss: 4.8975\n",
      "Epoch [2/2], Step [7790/64305], Loss: 5.1432\n",
      "Epoch [2/2], Step [7800/64305], Loss: 4.8198\n",
      "Epoch [2/2], Step [7810/64305], Loss: 4.8560\n",
      "Epoch [2/2], Step [7820/64305], Loss: 4.7566\n",
      "Epoch [2/2], Step [7830/64305], Loss: 4.7875\n",
      "Epoch [2/2], Step [7840/64305], Loss: 4.7965\n",
      "Epoch [2/2], Step [7850/64305], Loss: 4.9060\n",
      "Epoch [2/2], Step [7860/64305], Loss: 4.7416\n",
      "Epoch [2/2], Step [7870/64305], Loss: 4.7823\n",
      "Epoch [2/2], Step [7880/64305], Loss: 4.8747\n",
      "Epoch [2/2], Step [7890/64305], Loss: 4.7482\n",
      "Epoch [2/2], Step [7900/64305], Loss: 5.0737\n",
      "Epoch [2/2], Step [7910/64305], Loss: 5.1454\n",
      "Epoch [2/2], Step [7920/64305], Loss: 4.6716\n",
      "Epoch [2/2], Step [7930/64305], Loss: 4.7817\n",
      "Epoch [2/2], Step [7940/64305], Loss: 4.7418\n",
      "Epoch [2/2], Step [7950/64305], Loss: 4.7008\n",
      "Epoch [2/2], Step [7960/64305], Loss: 4.8813\n",
      "Epoch [2/2], Step [7970/64305], Loss: 5.0712\n",
      "Epoch [2/2], Step [7980/64305], Loss: 4.8103\n",
      "Epoch [2/2], Step [7990/64305], Loss: 4.8077\n",
      "Epoch [2/2], Step [8000/64305], Loss: 4.7983\n",
      "Epoch [2/2], Step [8010/64305], Loss: 4.7457\n",
      "Epoch [2/2], Step [8020/64305], Loss: 4.7649\n",
      "Epoch [2/2], Step [8030/64305], Loss: 5.0500\n",
      "Epoch [2/2], Step [8040/64305], Loss: 4.9287\n",
      "Epoch [2/2], Step [8050/64305], Loss: 4.6196\n",
      "Epoch [2/2], Step [8060/64305], Loss: 4.9162\n",
      "Epoch [2/2], Step [8070/64305], Loss: 4.6971\n",
      "Epoch [2/2], Step [8080/64305], Loss: 4.9767\n",
      "Epoch [2/2], Step [8090/64305], Loss: 5.0274\n",
      "Epoch [2/2], Step [8100/64305], Loss: 4.9343\n",
      "Epoch [2/2], Step [8110/64305], Loss: 5.1488\n",
      "Epoch [2/2], Step [8120/64305], Loss: 4.8147\n",
      "Epoch [2/2], Step [8130/64305], Loss: 4.7380\n",
      "Epoch [2/2], Step [8140/64305], Loss: 4.8376\n",
      "Epoch [2/2], Step [8150/64305], Loss: 5.0983\n",
      "Epoch [2/2], Step [8160/64305], Loss: 4.8656\n",
      "Epoch [2/2], Step [8170/64305], Loss: 4.8498\n",
      "Epoch [2/2], Step [8180/64305], Loss: 4.8781\n",
      "Epoch [2/2], Step [8190/64305], Loss: 4.6379\n",
      "Epoch [2/2], Step [8200/64305], Loss: 4.8407\n",
      "Epoch [2/2], Step [8210/64305], Loss: 4.7914\n",
      "Epoch [2/2], Step [8220/64305], Loss: 4.9715\n",
      "Epoch [2/2], Step [8230/64305], Loss: 4.9063\n",
      "Epoch [2/2], Step [8240/64305], Loss: 4.8940\n",
      "Epoch [2/2], Step [8250/64305], Loss: 4.9408\n",
      "Epoch [2/2], Step [8260/64305], Loss: 4.7504\n",
      "Epoch [2/2], Step [8270/64305], Loss: 5.0012\n",
      "Epoch [2/2], Step [8280/64305], Loss: 4.8862\n",
      "Epoch [2/2], Step [8290/64305], Loss: 4.9921\n",
      "Epoch [2/2], Step [8300/64305], Loss: 4.9491\n",
      "Epoch [2/2], Step [8310/64305], Loss: 4.8833\n",
      "Epoch [2/2], Step [8320/64305], Loss: 4.6866\n",
      "Epoch [2/2], Step [8330/64305], Loss: 4.9619\n",
      "Epoch [2/2], Step [8340/64305], Loss: 5.0712\n",
      "Epoch [2/2], Step [8350/64305], Loss: 4.8649\n",
      "Epoch [2/2], Step [8360/64305], Loss: 4.8883\n",
      "Epoch [2/2], Step [8370/64305], Loss: 4.7794\n",
      "Epoch [2/2], Step [8380/64305], Loss: 5.1285\n",
      "Epoch [2/2], Step [8390/64305], Loss: 4.9677\n",
      "Epoch [2/2], Step [8400/64305], Loss: 4.7156\n",
      "Epoch [2/2], Step [8410/64305], Loss: 4.9305\n",
      "Epoch [2/2], Step [8420/64305], Loss: 4.7093\n",
      "Epoch [2/2], Step [8430/64305], Loss: 4.7857\n",
      "Epoch [2/2], Step [8440/64305], Loss: 4.8731\n",
      "Epoch [2/2], Step [8450/64305], Loss: 4.9500\n",
      "Epoch [2/2], Step [8460/64305], Loss: 4.8074\n",
      "Epoch [2/2], Step [8470/64305], Loss: 4.9085\n",
      "Epoch [2/2], Step [8480/64305], Loss: 4.9445\n",
      "Epoch [2/2], Step [8490/64305], Loss: 4.7226\n",
      "Epoch [2/2], Step [8500/64305], Loss: 5.0047\n",
      "Epoch [2/2], Step [8510/64305], Loss: 4.8221\n",
      "Epoch [2/2], Step [8520/64305], Loss: 4.9265\n",
      "Epoch [2/2], Step [8530/64305], Loss: 4.7827\n",
      "Epoch [2/2], Step [8540/64305], Loss: 4.8384\n",
      "Epoch [2/2], Step [8550/64305], Loss: 4.8468\n",
      "Epoch [2/2], Step [8560/64305], Loss: 4.8803\n",
      "Epoch [2/2], Step [8570/64305], Loss: 4.7520\n",
      "Epoch [2/2], Step [8580/64305], Loss: 4.8941\n",
      "Epoch [2/2], Step [8590/64305], Loss: 4.8270\n",
      "Epoch [2/2], Step [8600/64305], Loss: 4.8105\n",
      "Epoch [2/2], Step [8610/64305], Loss: 4.9737\n",
      "Epoch [2/2], Step [8620/64305], Loss: 4.7782\n",
      "Epoch [2/2], Step [8630/64305], Loss: 5.1033\n",
      "Epoch [2/2], Step [8640/64305], Loss: 4.8051\n",
      "Epoch [2/2], Step [8650/64305], Loss: 5.0299\n",
      "Epoch [2/2], Step [8660/64305], Loss: 4.7662\n",
      "Epoch [2/2], Step [8670/64305], Loss: 4.8596\n",
      "Epoch [2/2], Step [8680/64305], Loss: 4.8596\n",
      "Epoch [2/2], Step [8690/64305], Loss: 5.0838\n",
      "Epoch [2/2], Step [8700/64305], Loss: 4.9594\n",
      "Epoch [2/2], Step [8710/64305], Loss: 4.9165\n",
      "Epoch [2/2], Step [8720/64305], Loss: 4.6713\n",
      "Epoch [2/2], Step [8730/64305], Loss: 4.8279\n",
      "Epoch [2/2], Step [8740/64305], Loss: 4.9501\n",
      "Epoch [2/2], Step [8750/64305], Loss: 4.7972\n",
      "Epoch [2/2], Step [8760/64305], Loss: 4.9583\n",
      "Epoch [2/2], Step [8770/64305], Loss: 4.8547\n",
      "Epoch [2/2], Step [8780/64305], Loss: 5.0620\n",
      "Epoch [2/2], Step [8790/64305], Loss: 4.9485\n",
      "Epoch [2/2], Step [8800/64305], Loss: 4.7376\n",
      "Epoch [2/2], Step [8810/64305], Loss: 4.8223\n",
      "Epoch [2/2], Step [8820/64305], Loss: 4.8733\n",
      "Epoch [2/2], Step [8830/64305], Loss: 4.8433\n",
      "Epoch [2/2], Step [8840/64305], Loss: 4.9271\n",
      "Epoch [2/2], Step [8850/64305], Loss: 4.6885\n",
      "Epoch [2/2], Step [8860/64305], Loss: 4.9270\n",
      "Epoch [2/2], Step [8870/64305], Loss: 4.8138\n",
      "Epoch [2/2], Step [8880/64305], Loss: 4.9014\n",
      "Epoch [2/2], Step [8890/64305], Loss: 4.7002\n",
      "Epoch [2/2], Step [8900/64305], Loss: 4.9238\n",
      "Epoch [2/2], Step [8910/64305], Loss: 4.9790\n",
      "Epoch [2/2], Step [8920/64305], Loss: 4.8843\n",
      "Epoch [2/2], Step [8930/64305], Loss: 4.6864\n",
      "Epoch [2/2], Step [8940/64305], Loss: 4.8805\n",
      "Epoch [2/2], Step [8950/64305], Loss: 4.8269\n",
      "Epoch [2/2], Step [8960/64305], Loss: 4.7861\n",
      "Epoch [2/2], Step [8970/64305], Loss: 4.8773\n",
      "Epoch [2/2], Step [8980/64305], Loss: 4.7405\n",
      "Epoch [2/2], Step [8990/64305], Loss: 4.9711\n",
      "Epoch [2/2], Step [9000/64305], Loss: 4.7237\n",
      "Epoch [2/2], Step [9010/64305], Loss: 4.9856\n",
      "Epoch [2/2], Step [9020/64305], Loss: 4.8895\n",
      "Epoch [2/2], Step [9030/64305], Loss: 5.0966\n",
      "Epoch [2/2], Step [9040/64305], Loss: 4.8799\n",
      "Epoch [2/2], Step [9050/64305], Loss: 4.7866\n",
      "Epoch [2/2], Step [9060/64305], Loss: 4.9667\n",
      "Epoch [2/2], Step [9070/64305], Loss: 4.7451\n",
      "Epoch [2/2], Step [9080/64305], Loss: 4.9091\n",
      "Epoch [2/2], Step [9090/64305], Loss: 4.9683\n",
      "Epoch [2/2], Step [9100/64305], Loss: 4.8912\n",
      "Epoch [2/2], Step [9110/64305], Loss: 5.0366\n",
      "Epoch [2/2], Step [9120/64305], Loss: 4.9265\n",
      "Epoch [2/2], Step [9130/64305], Loss: 4.9572\n",
      "Epoch [2/2], Step [9140/64305], Loss: 4.8321\n",
      "Epoch [2/2], Step [9150/64305], Loss: 4.6635\n",
      "Epoch [2/2], Step [9160/64305], Loss: 4.8515\n",
      "Epoch [2/2], Step [9170/64305], Loss: 4.9541\n",
      "Epoch [2/2], Step [9180/64305], Loss: 4.8140\n",
      "Epoch [2/2], Step [9190/64305], Loss: 4.8240\n",
      "Epoch [2/2], Step [9200/64305], Loss: 4.8735\n",
      "Epoch [2/2], Step [9210/64305], Loss: 5.0588\n",
      "Epoch [2/2], Step [9220/64305], Loss: 4.7989\n",
      "Epoch [2/2], Step [9230/64305], Loss: 4.9029\n",
      "Epoch [2/2], Step [9240/64305], Loss: 5.0167\n",
      "Epoch [2/2], Step [9250/64305], Loss: 4.7638\n",
      "Epoch [2/2], Step [9260/64305], Loss: 4.7222\n",
      "Epoch [2/2], Step [9270/64305], Loss: 4.9132\n",
      "Epoch [2/2], Step [9280/64305], Loss: 5.0220\n",
      "Epoch [2/2], Step [9290/64305], Loss: 4.7674\n",
      "Epoch [2/2], Step [9300/64305], Loss: 5.0042\n",
      "Epoch [2/2], Step [9310/64305], Loss: 4.8959\n",
      "Epoch [2/2], Step [9320/64305], Loss: 4.7741\n",
      "Epoch [2/2], Step [9330/64305], Loss: 4.8699\n",
      "Epoch [2/2], Step [9340/64305], Loss: 4.7406\n",
      "Epoch [2/2], Step [9350/64305], Loss: 4.8779\n",
      "Epoch [2/2], Step [9360/64305], Loss: 4.8623\n",
      "Epoch [2/2], Step [9370/64305], Loss: 5.0104\n",
      "Epoch [2/2], Step [9380/64305], Loss: 4.7587\n",
      "Epoch [2/2], Step [9390/64305], Loss: 4.7074\n",
      "Epoch [2/2], Step [9400/64305], Loss: 4.6980\n",
      "Epoch [2/2], Step [9410/64305], Loss: 4.8707\n",
      "Epoch [2/2], Step [9420/64305], Loss: 4.7521\n",
      "Epoch [2/2], Step [9430/64305], Loss: 4.8845\n",
      "Epoch [2/2], Step [9440/64305], Loss: 4.9691\n",
      "Epoch [2/2], Step [9450/64305], Loss: 4.7079\n",
      "Epoch [2/2], Step [9460/64305], Loss: 4.9395\n",
      "Epoch [2/2], Step [9470/64305], Loss: 4.9414\n",
      "Epoch [2/2], Step [9480/64305], Loss: 4.8903\n",
      "Epoch [2/2], Step [9490/64305], Loss: 5.1020\n",
      "Epoch [2/2], Step [9500/64305], Loss: 4.6080\n",
      "Epoch [2/2], Step [9510/64305], Loss: 4.9870\n",
      "Epoch [2/2], Step [9520/64305], Loss: 5.0202\n",
      "Epoch [2/2], Step [9530/64305], Loss: 4.8013\n",
      "Epoch [2/2], Step [9540/64305], Loss: 5.0221\n",
      "Epoch [2/2], Step [9550/64305], Loss: 4.7611\n",
      "Epoch [2/2], Step [9560/64305], Loss: 4.8229\n",
      "Epoch [2/2], Step [9570/64305], Loss: 4.8551\n",
      "Epoch [2/2], Step [9580/64305], Loss: 4.9850\n",
      "Epoch [2/2], Step [9590/64305], Loss: 5.0502\n",
      "Epoch [2/2], Step [9600/64305], Loss: 4.6561\n",
      "Epoch [2/2], Step [9610/64305], Loss: 5.0423\n",
      "Epoch [2/2], Step [9620/64305], Loss: 4.9940\n",
      "Epoch [2/2], Step [9630/64305], Loss: 4.7701\n",
      "Epoch [2/2], Step [9640/64305], Loss: 4.8680\n",
      "Epoch [2/2], Step [9650/64305], Loss: 4.9872\n",
      "Epoch [2/2], Step [9660/64305], Loss: 4.9854\n",
      "Epoch [2/2], Step [9670/64305], Loss: 5.1098\n",
      "Epoch [2/2], Step [9680/64305], Loss: 4.7871\n",
      "Epoch [2/2], Step [9690/64305], Loss: 5.1107\n",
      "Epoch [2/2], Step [9700/64305], Loss: 4.7260\n",
      "Epoch [2/2], Step [9710/64305], Loss: 4.9744\n",
      "Epoch [2/2], Step [9720/64305], Loss: 4.8063\n",
      "Epoch [2/2], Step [9730/64305], Loss: 4.9927\n",
      "Epoch [2/2], Step [9740/64305], Loss: 4.9390\n",
      "Epoch [2/2], Step [9750/64305], Loss: 4.8376\n",
      "Epoch [2/2], Step [9760/64305], Loss: 4.7751\n",
      "Epoch [2/2], Step [9770/64305], Loss: 5.1950\n",
      "Epoch [2/2], Step [9780/64305], Loss: 4.9584\n",
      "Epoch [2/2], Step [9790/64305], Loss: 4.8027\n",
      "Epoch [2/2], Step [9800/64305], Loss: 5.0022\n",
      "Epoch [2/2], Step [9810/64305], Loss: 4.6807\n",
      "Epoch [2/2], Step [9820/64305], Loss: 4.8091\n",
      "Epoch [2/2], Step [9830/64305], Loss: 4.6545\n",
      "Epoch [2/2], Step [9840/64305], Loss: 4.8404\n",
      "Epoch [2/2], Step [9850/64305], Loss: 4.7845\n",
      "Epoch [2/2], Step [9860/64305], Loss: 4.7580\n",
      "Epoch [2/2], Step [9870/64305], Loss: 4.7109\n",
      "Epoch [2/2], Step [9880/64305], Loss: 4.8028\n",
      "Epoch [2/2], Step [9890/64305], Loss: 4.9636\n",
      "Epoch [2/2], Step [9900/64305], Loss: 4.6514\n",
      "Epoch [2/2], Step [9910/64305], Loss: 5.0317\n",
      "Epoch [2/2], Step [9920/64305], Loss: 5.0041\n",
      "Epoch [2/2], Step [9930/64305], Loss: 4.6429\n",
      "Epoch [2/2], Step [9940/64305], Loss: 4.9721\n",
      "Epoch [2/2], Step [9950/64305], Loss: 4.9347\n",
      "Epoch [2/2], Step [9960/64305], Loss: 5.0270\n",
      "Epoch [2/2], Step [9970/64305], Loss: 4.9545\n",
      "Epoch [2/2], Step [9980/64305], Loss: 5.0150\n",
      "Epoch [2/2], Step [9990/64305], Loss: 4.9729\n",
      "Epoch [2/2], Step [10000/64305], Loss: 4.9172\n",
      "Epoch [2/2], Step [10010/64305], Loss: 5.0910\n",
      "Epoch [2/2], Step [10020/64305], Loss: 4.8080\n",
      "Epoch [2/2], Step [10030/64305], Loss: 4.7753\n",
      "Epoch [2/2], Step [10040/64305], Loss: 5.1083\n",
      "Epoch [2/2], Step [10050/64305], Loss: 4.8792\n",
      "Epoch [2/2], Step [10060/64305], Loss: 4.7975\n",
      "Epoch [2/2], Step [10070/64305], Loss: 4.6914\n",
      "Epoch [2/2], Step [10080/64305], Loss: 4.8956\n",
      "Epoch [2/2], Step [10090/64305], Loss: 4.8386\n",
      "Epoch [2/2], Step [10100/64305], Loss: 5.0509\n",
      "Epoch [2/2], Step [10110/64305], Loss: 5.1176\n",
      "Epoch [2/2], Step [10120/64305], Loss: 4.8776\n",
      "Epoch [2/2], Step [10130/64305], Loss: 4.8287\n",
      "Epoch [2/2], Step [10140/64305], Loss: 4.8164\n",
      "Epoch [2/2], Step [10150/64305], Loss: 4.8643\n",
      "Epoch [2/2], Step [10160/64305], Loss: 4.7093\n",
      "Epoch [2/2], Step [10170/64305], Loss: 4.9372\n",
      "Epoch [2/2], Step [10180/64305], Loss: 4.9199\n",
      "Epoch [2/2], Step [10190/64305], Loss: 5.0484\n",
      "Epoch [2/2], Step [10200/64305], Loss: 4.8416\n",
      "Epoch [2/2], Step [10210/64305], Loss: 4.6981\n",
      "Epoch [2/2], Step [10220/64305], Loss: 4.7630\n",
      "Epoch [2/2], Step [10230/64305], Loss: 4.8693\n",
      "Epoch [2/2], Step [10240/64305], Loss: 4.8373\n",
      "Epoch [2/2], Step [10250/64305], Loss: 4.7454\n",
      "Epoch [2/2], Step [10260/64305], Loss: 5.0158\n",
      "Epoch [2/2], Step [10270/64305], Loss: 4.8745\n",
      "Epoch [2/2], Step [10280/64305], Loss: 4.8176\n",
      "Epoch [2/2], Step [10290/64305], Loss: 4.6343\n",
      "Epoch [2/2], Step [10300/64305], Loss: 4.7408\n",
      "Epoch [2/2], Step [10310/64305], Loss: 4.8853\n",
      "Epoch [2/2], Step [10320/64305], Loss: 4.6944\n",
      "Epoch [2/2], Step [10330/64305], Loss: 5.1850\n",
      "Epoch [2/2], Step [10340/64305], Loss: 4.9923\n",
      "Epoch [2/2], Step [10350/64305], Loss: 4.8304\n",
      "Epoch [2/2], Step [10360/64305], Loss: 4.8217\n",
      "Epoch [2/2], Step [10370/64305], Loss: 4.8712\n",
      "Epoch [2/2], Step [10380/64305], Loss: 4.8135\n",
      "Epoch [2/2], Step [10390/64305], Loss: 4.7506\n",
      "Epoch [2/2], Step [10400/64305], Loss: 4.7683\n",
      "Epoch [2/2], Step [10410/64305], Loss: 4.9033\n",
      "Epoch [2/2], Step [10420/64305], Loss: 4.7578\n",
      "Epoch [2/2], Step [10430/64305], Loss: 4.9123\n",
      "Epoch [2/2], Step [10440/64305], Loss: 4.8770\n",
      "Epoch [2/2], Step [10450/64305], Loss: 4.8469\n",
      "Epoch [2/2], Step [10460/64305], Loss: 4.8809\n",
      "Epoch [2/2], Step [10470/64305], Loss: 4.8231\n",
      "Epoch [2/2], Step [10480/64305], Loss: 4.7540\n",
      "Epoch [2/2], Step [10490/64305], Loss: 4.8145\n",
      "Epoch [2/2], Step [10500/64305], Loss: 4.9055\n",
      "Epoch [2/2], Step [10510/64305], Loss: 5.0215\n",
      "Epoch [2/2], Step [10520/64305], Loss: 4.9108\n",
      "Epoch [2/2], Step [10530/64305], Loss: 4.8925\n",
      "Epoch [2/2], Step [10540/64305], Loss: 4.7090\n",
      "Epoch [2/2], Step [10550/64305], Loss: 4.8917\n",
      "Epoch [2/2], Step [10560/64305], Loss: 4.6929\n",
      "Epoch [2/2], Step [10570/64305], Loss: 4.9242\n",
      "Epoch [2/2], Step [10580/64305], Loss: 4.9911\n",
      "Epoch [2/2], Step [10590/64305], Loss: 4.9442\n",
      "Epoch [2/2], Step [10600/64305], Loss: 4.9158\n",
      "Epoch [2/2], Step [10610/64305], Loss: 4.8553\n",
      "Epoch [2/2], Step [10620/64305], Loss: 4.8852\n",
      "Epoch [2/2], Step [10630/64305], Loss: 4.9877\n",
      "Epoch [2/2], Step [10640/64305], Loss: 4.9238\n",
      "Epoch [2/2], Step [10650/64305], Loss: 4.9473\n",
      "Epoch [2/2], Step [10660/64305], Loss: 4.7939\n",
      "Epoch [2/2], Step [10670/64305], Loss: 5.0295\n",
      "Epoch [2/2], Step [10680/64305], Loss: 4.9075\n",
      "Epoch [2/2], Step [10690/64305], Loss: 4.8171\n",
      "Epoch [2/2], Step [10700/64305], Loss: 4.7932\n",
      "Epoch [2/2], Step [10710/64305], Loss: 4.9666\n",
      "Epoch [2/2], Step [10720/64305], Loss: 4.9683\n",
      "Epoch [2/2], Step [10730/64305], Loss: 4.8006\n",
      "Epoch [2/2], Step [10740/64305], Loss: 5.0335\n",
      "Epoch [2/2], Step [10750/64305], Loss: 4.9624\n",
      "Epoch [2/2], Step [10760/64305], Loss: 4.5932\n",
      "Epoch [2/2], Step [10770/64305], Loss: 5.0832\n",
      "Epoch [2/2], Step [10780/64305], Loss: 4.6221\n",
      "Epoch [2/2], Step [10790/64305], Loss: 4.9401\n",
      "Epoch [2/2], Step [10800/64305], Loss: 4.8084\n",
      "Epoch [2/2], Step [10810/64305], Loss: 5.1053\n",
      "Epoch [2/2], Step [10820/64305], Loss: 4.8766\n",
      "Epoch [2/2], Step [10830/64305], Loss: 4.8822\n",
      "Epoch [2/2], Step [10840/64305], Loss: 4.8233\n",
      "Epoch [2/2], Step [10850/64305], Loss: 4.8648\n",
      "Epoch [2/2], Step [10860/64305], Loss: 4.8621\n",
      "Epoch [2/2], Step [10870/64305], Loss: 4.8924\n",
      "Epoch [2/2], Step [10880/64305], Loss: 4.5598\n",
      "Epoch [2/2], Step [10890/64305], Loss: 4.7675\n",
      "Epoch [2/2], Step [10900/64305], Loss: 5.0782\n",
      "Epoch [2/2], Step [10910/64305], Loss: 4.6759\n",
      "Epoch [2/2], Step [10920/64305], Loss: 5.1165\n",
      "Epoch [2/2], Step [10930/64305], Loss: 4.8433\n",
      "Epoch [2/2], Step [10940/64305], Loss: 5.1015\n",
      "Epoch [2/2], Step [10950/64305], Loss: 4.7072\n",
      "Epoch [2/2], Step [10960/64305], Loss: 4.6359\n",
      "Epoch [2/2], Step [10970/64305], Loss: 5.0493\n",
      "Epoch [2/2], Step [10980/64305], Loss: 4.9873\n",
      "Epoch [2/2], Step [10990/64305], Loss: 4.8454\n",
      "Epoch [2/2], Step [11000/64305], Loss: 4.6481\n",
      "Epoch [2/2], Step [11010/64305], Loss: 4.8819\n",
      "Epoch [2/2], Step [11020/64305], Loss: 4.8917\n",
      "Epoch [2/2], Step [11030/64305], Loss: 4.9787\n",
      "Epoch [2/2], Step [11040/64305], Loss: 4.9068\n",
      "Epoch [2/2], Step [11050/64305], Loss: 5.0305\n",
      "Epoch [2/2], Step [11060/64305], Loss: 4.8694\n",
      "Epoch [2/2], Step [11070/64305], Loss: 4.8009\n",
      "Epoch [2/2], Step [11080/64305], Loss: 4.9495\n",
      "Epoch [2/2], Step [11090/64305], Loss: 4.7785\n",
      "Epoch [2/2], Step [11100/64305], Loss: 4.9378\n",
      "Epoch [2/2], Step [11110/64305], Loss: 4.7606\n",
      "Epoch [2/2], Step [11120/64305], Loss: 4.9718\n",
      "Epoch [2/2], Step [11130/64305], Loss: 4.7409\n",
      "Epoch [2/2], Step [11140/64305], Loss: 4.7786\n",
      "Epoch [2/2], Step [11150/64305], Loss: 4.8793\n",
      "Epoch [2/2], Step [11160/64305], Loss: 4.6522\n",
      "Epoch [2/2], Step [11170/64305], Loss: 4.8297\n",
      "Epoch [2/2], Step [11180/64305], Loss: 4.7910\n",
      "Epoch [2/2], Step [11190/64305], Loss: 4.8958\n",
      "Epoch [2/2], Step [11200/64305], Loss: 5.0501\n",
      "Epoch [2/2], Step [11210/64305], Loss: 4.8769\n",
      "Epoch [2/2], Step [11220/64305], Loss: 4.8528\n",
      "Epoch [2/2], Step [11230/64305], Loss: 4.6137\n",
      "Epoch [2/2], Step [11240/64305], Loss: 4.7809\n",
      "Epoch [2/2], Step [11250/64305], Loss: 4.8840\n",
      "Epoch [2/2], Step [11260/64305], Loss: 4.6878\n",
      "Epoch [2/2], Step [11270/64305], Loss: 5.0535\n",
      "Epoch [2/2], Step [11280/64305], Loss: 4.9664\n",
      "Epoch [2/2], Step [11290/64305], Loss: 4.8219\n",
      "Epoch [2/2], Step [11300/64305], Loss: 4.6644\n",
      "Epoch [2/2], Step [11310/64305], Loss: 4.7421\n",
      "Epoch [2/2], Step [11320/64305], Loss: 4.8197\n",
      "Epoch [2/2], Step [11330/64305], Loss: 4.6738\n",
      "Epoch [2/2], Step [11340/64305], Loss: 4.7758\n",
      "Epoch [2/2], Step [11350/64305], Loss: 5.0745\n",
      "Epoch [2/2], Step [11360/64305], Loss: 4.7514\n",
      "Epoch [2/2], Step [11370/64305], Loss: 5.0041\n",
      "Epoch [2/2], Step [11380/64305], Loss: 4.8508\n",
      "Epoch [2/2], Step [11390/64305], Loss: 5.2399\n",
      "Epoch [2/2], Step [11400/64305], Loss: 4.8618\n",
      "Epoch [2/2], Step [11410/64305], Loss: 4.8361\n",
      "Epoch [2/2], Step [11420/64305], Loss: 5.0055\n",
      "Epoch [2/2], Step [11430/64305], Loss: 4.9609\n",
      "Epoch [2/2], Step [11440/64305], Loss: 4.9427\n",
      "Epoch [2/2], Step [11450/64305], Loss: 5.0443\n",
      "Epoch [2/2], Step [11460/64305], Loss: 4.8814\n",
      "Epoch [2/2], Step [11470/64305], Loss: 4.7894\n",
      "Epoch [2/2], Step [11480/64305], Loss: 4.9086\n",
      "Epoch [2/2], Step [11490/64305], Loss: 4.8015\n",
      "Epoch [2/2], Step [11500/64305], Loss: 4.7143\n",
      "Epoch [2/2], Step [11510/64305], Loss: 4.8810\n",
      "Epoch [2/2], Step [11520/64305], Loss: 4.9198\n",
      "Epoch [2/2], Step [11530/64305], Loss: 4.7523\n",
      "Epoch [2/2], Step [11540/64305], Loss: 5.0566\n",
      "Epoch [2/2], Step [11550/64305], Loss: 4.8921\n",
      "Epoch [2/2], Step [11560/64305], Loss: 4.8746\n",
      "Epoch [2/2], Step [11570/64305], Loss: 4.9340\n",
      "Epoch [2/2], Step [11580/64305], Loss: 5.0297\n",
      "Epoch [2/2], Step [11590/64305], Loss: 5.0488\n",
      "Epoch [2/2], Step [11600/64305], Loss: 4.7507\n",
      "Epoch [2/2], Step [11610/64305], Loss: 4.7954\n",
      "Epoch [2/2], Step [11620/64305], Loss: 5.0107\n",
      "Epoch [2/2], Step [11630/64305], Loss: 4.8058\n",
      "Epoch [2/2], Step [11640/64305], Loss: 4.9443\n",
      "Epoch [2/2], Step [11650/64305], Loss: 4.7671\n",
      "Epoch [2/2], Step [11660/64305], Loss: 4.8935\n",
      "Epoch [2/2], Step [11670/64305], Loss: 4.6619\n",
      "Epoch [2/2], Step [11680/64305], Loss: 4.8423\n",
      "Epoch [2/2], Step [11690/64305], Loss: 4.8595\n",
      "Epoch [2/2], Step [11700/64305], Loss: 4.9228\n",
      "Epoch [2/2], Step [11710/64305], Loss: 4.8505\n",
      "Epoch [2/2], Step [11720/64305], Loss: 4.9006\n",
      "Epoch [2/2], Step [11730/64305], Loss: 4.9432\n",
      "Epoch [2/2], Step [11740/64305], Loss: 4.9330\n",
      "Epoch [2/2], Step [11750/64305], Loss: 4.8879\n",
      "Epoch [2/2], Step [11760/64305], Loss: 4.9682\n",
      "Epoch [2/2], Step [11770/64305], Loss: 5.0545\n",
      "Epoch [2/2], Step [11780/64305], Loss: 4.7476\n",
      "Epoch [2/2], Step [11790/64305], Loss: 4.7283\n",
      "Epoch [2/2], Step [11800/64305], Loss: 4.8193\n",
      "Epoch [2/2], Step [11810/64305], Loss: 4.9257\n",
      "Epoch [2/2], Step [11820/64305], Loss: 4.7330\n",
      "Epoch [2/2], Step [11830/64305], Loss: 4.8960\n",
      "Epoch [2/2], Step [11840/64305], Loss: 4.8064\n",
      "Epoch [2/2], Step [11850/64305], Loss: 4.9638\n",
      "Epoch [2/2], Step [11860/64305], Loss: 4.7562\n",
      "Epoch [2/2], Step [11870/64305], Loss: 4.9110\n",
      "Epoch [2/2], Step [11880/64305], Loss: 5.0042\n",
      "Epoch [2/2], Step [11890/64305], Loss: 4.8207\n",
      "Epoch [2/2], Step [11900/64305], Loss: 4.8731\n",
      "Epoch [2/2], Step [11910/64305], Loss: 4.8661\n",
      "Epoch [2/2], Step [11920/64305], Loss: 4.8406\n",
      "Epoch [2/2], Step [11930/64305], Loss: 4.9353\n",
      "Epoch [2/2], Step [11940/64305], Loss: 4.7779\n",
      "Epoch [2/2], Step [11950/64305], Loss: 4.8614\n",
      "Epoch [2/2], Step [11960/64305], Loss: 5.0239\n",
      "Epoch [2/2], Step [11970/64305], Loss: 4.9512\n",
      "Epoch [2/2], Step [11980/64305], Loss: 4.8853\n",
      "Epoch [2/2], Step [11990/64305], Loss: 4.8649\n",
      "Epoch [2/2], Step [12000/64305], Loss: 4.8695\n",
      "Epoch [2/2], Step [12010/64305], Loss: 5.0081\n",
      "Epoch [2/2], Step [12020/64305], Loss: 4.8231\n",
      "Epoch [2/2], Step [12030/64305], Loss: 4.9368\n",
      "Epoch [2/2], Step [12040/64305], Loss: 4.7678\n",
      "Epoch [2/2], Step [12050/64305], Loss: 4.7584\n",
      "Epoch [2/2], Step [12060/64305], Loss: 4.6866\n",
      "Epoch [2/2], Step [12070/64305], Loss: 4.8462\n",
      "Epoch [2/2], Step [12080/64305], Loss: 4.7223\n",
      "Epoch [2/2], Step [12090/64305], Loss: 4.7686\n",
      "Epoch [2/2], Step [12100/64305], Loss: 4.9398\n",
      "Epoch [2/2], Step [12110/64305], Loss: 4.9227\n",
      "Epoch [2/2], Step [12120/64305], Loss: 4.8431\n",
      "Epoch [2/2], Step [12130/64305], Loss: 4.7220\n",
      "Epoch [2/2], Step [12140/64305], Loss: 4.8649\n",
      "Epoch [2/2], Step [12150/64305], Loss: 4.8970\n",
      "Epoch [2/2], Step [12160/64305], Loss: 4.8052\n",
      "Epoch [2/2], Step [12170/64305], Loss: 4.9176\n",
      "Epoch [2/2], Step [12180/64305], Loss: 4.9526\n",
      "Epoch [2/2], Step [12190/64305], Loss: 5.0170\n",
      "Epoch [2/2], Step [12200/64305], Loss: 4.9190\n",
      "Epoch [2/2], Step [12210/64305], Loss: 4.8451\n",
      "Epoch [2/2], Step [12220/64305], Loss: 4.7462\n",
      "Epoch [2/2], Step [12230/64305], Loss: 4.8371\n",
      "Epoch [2/2], Step [12240/64305], Loss: 4.9276\n",
      "Epoch [2/2], Step [12250/64305], Loss: 4.9414\n",
      "Epoch [2/2], Step [12260/64305], Loss: 4.8544\n",
      "Epoch [2/2], Step [12270/64305], Loss: 4.8100\n",
      "Epoch [2/2], Step [12280/64305], Loss: 4.8163\n",
      "Epoch [2/2], Step [12290/64305], Loss: 5.1186\n",
      "Epoch [2/2], Step [12300/64305], Loss: 4.7036\n",
      "Epoch [2/2], Step [12310/64305], Loss: 4.6896\n",
      "Epoch [2/2], Step [12320/64305], Loss: 4.9480\n",
      "Epoch [2/2], Step [12330/64305], Loss: 4.9482\n",
      "Epoch [2/2], Step [12340/64305], Loss: 4.6501\n",
      "Epoch [2/2], Step [12350/64305], Loss: 4.8275\n",
      "Epoch [2/2], Step [12360/64305], Loss: 5.0190\n",
      "Epoch [2/2], Step [12370/64305], Loss: 4.7726\n",
      "Epoch [2/2], Step [12380/64305], Loss: 4.8467\n",
      "Epoch [2/2], Step [12390/64305], Loss: 4.9538\n",
      "Epoch [2/2], Step [12400/64305], Loss: 4.7729\n",
      "Epoch [2/2], Step [12410/64305], Loss: 4.9164\n",
      "Epoch [2/2], Step [12420/64305], Loss: 5.0797\n",
      "Epoch [2/2], Step [12430/64305], Loss: 4.9691\n",
      "Epoch [2/2], Step [12440/64305], Loss: 4.7775\n",
      "Epoch [2/2], Step [12450/64305], Loss: 4.8927\n",
      "Epoch [2/2], Step [12460/64305], Loss: 4.9907\n",
      "Epoch [2/2], Step [12470/64305], Loss: 5.0859\n",
      "Epoch [2/2], Step [12480/64305], Loss: 4.8452\n",
      "Epoch [2/2], Step [12490/64305], Loss: 4.4932\n",
      "Epoch [2/2], Step [12500/64305], Loss: 4.8459\n",
      "Epoch [2/2], Step [12510/64305], Loss: 4.9196\n",
      "Epoch [2/2], Step [12520/64305], Loss: 4.8609\n",
      "Epoch [2/2], Step [12530/64305], Loss: 4.9909\n",
      "Epoch [2/2], Step [12540/64305], Loss: 4.8826\n",
      "Epoch [2/2], Step [12550/64305], Loss: 4.8321\n",
      "Epoch [2/2], Step [12560/64305], Loss: 4.8490\n",
      "Epoch [2/2], Step [12570/64305], Loss: 4.8908\n",
      "Epoch [2/2], Step [12580/64305], Loss: 4.7218\n",
      "Epoch [2/2], Step [12590/64305], Loss: 4.8309\n",
      "Epoch [2/2], Step [12600/64305], Loss: 4.7617\n",
      "Epoch [2/2], Step [12610/64305], Loss: 5.0214\n",
      "Epoch [2/2], Step [12620/64305], Loss: 4.9340\n",
      "Epoch [2/2], Step [12630/64305], Loss: 4.8917\n",
      "Epoch [2/2], Step [12640/64305], Loss: 4.8226\n",
      "Epoch [2/2], Step [12650/64305], Loss: 5.0140\n",
      "Epoch [2/2], Step [12660/64305], Loss: 4.9998\n",
      "Epoch [2/2], Step [12670/64305], Loss: 4.7421\n",
      "Epoch [2/2], Step [12680/64305], Loss: 4.8751\n",
      "Epoch [2/2], Step [12690/64305], Loss: 4.7284\n",
      "Epoch [2/2], Step [12700/64305], Loss: 5.0276\n",
      "Epoch [2/2], Step [12710/64305], Loss: 4.7419\n",
      "Epoch [2/2], Step [12720/64305], Loss: 4.9316\n",
      "Epoch [2/2], Step [12730/64305], Loss: 4.9169\n",
      "Epoch [2/2], Step [12740/64305], Loss: 4.8915\n",
      "Epoch [2/2], Step [12750/64305], Loss: 4.8084\n",
      "Epoch [2/2], Step [12760/64305], Loss: 4.8165\n",
      "Epoch [2/2], Step [12770/64305], Loss: 4.7606\n",
      "Epoch [2/2], Step [12780/64305], Loss: 4.8872\n",
      "Epoch [2/2], Step [12790/64305], Loss: 4.7781\n",
      "Epoch [2/2], Step [12800/64305], Loss: 4.9574\n",
      "Epoch [2/2], Step [12810/64305], Loss: 4.9119\n",
      "Epoch [2/2], Step [12820/64305], Loss: 4.7862\n",
      "Epoch [2/2], Step [12830/64305], Loss: 4.7668\n",
      "Epoch [2/2], Step [12840/64305], Loss: 4.8812\n",
      "Epoch [2/2], Step [12850/64305], Loss: 4.8664\n",
      "Epoch [2/2], Step [12860/64305], Loss: 4.9111\n",
      "Epoch [2/2], Step [12870/64305], Loss: 4.8596\n",
      "Epoch [2/2], Step [12880/64305], Loss: 4.9737\n",
      "Epoch [2/2], Step [12890/64305], Loss: 5.1273\n",
      "Epoch [2/2], Step [12900/64305], Loss: 4.7361\n",
      "Epoch [2/2], Step [12910/64305], Loss: 4.8703\n",
      "Epoch [2/2], Step [12920/64305], Loss: 4.8786\n",
      "Epoch [2/2], Step [12930/64305], Loss: 4.9168\n",
      "Epoch [2/2], Step [12940/64305], Loss: 4.8526\n",
      "Epoch [2/2], Step [12950/64305], Loss: 4.6292\n",
      "Epoch [2/2], Step [12960/64305], Loss: 4.7687\n",
      "Epoch [2/2], Step [12970/64305], Loss: 4.6898\n",
      "Epoch [2/2], Step [12980/64305], Loss: 4.9273\n",
      "Epoch [2/2], Step [12990/64305], Loss: 4.6230\n",
      "Epoch [2/2], Step [13000/64305], Loss: 4.8754\n",
      "Epoch [2/2], Step [13010/64305], Loss: 5.0164\n",
      "Epoch [2/2], Step [13020/64305], Loss: 4.9598\n",
      "Epoch [2/2], Step [13030/64305], Loss: 4.9328\n",
      "Epoch [2/2], Step [13040/64305], Loss: 4.5971\n",
      "Epoch [2/2], Step [13050/64305], Loss: 4.9467\n",
      "Epoch [2/2], Step [13060/64305], Loss: 4.7342\n",
      "Epoch [2/2], Step [13070/64305], Loss: 4.7332\n",
      "Epoch [2/2], Step [13080/64305], Loss: 4.6233\n",
      "Epoch [2/2], Step [13090/64305], Loss: 4.8683\n",
      "Epoch [2/2], Step [13100/64305], Loss: 5.0050\n",
      "Epoch [2/2], Step [13110/64305], Loss: 4.9980\n",
      "Epoch [2/2], Step [13120/64305], Loss: 4.8505\n",
      "Epoch [2/2], Step [13130/64305], Loss: 4.8870\n",
      "Epoch [2/2], Step [13140/64305], Loss: 4.8768\n",
      "Epoch [2/2], Step [13150/64305], Loss: 4.7197\n",
      "Epoch [2/2], Step [13160/64305], Loss: 4.9513\n",
      "Epoch [2/2], Step [13170/64305], Loss: 4.7770\n",
      "Epoch [2/2], Step [13180/64305], Loss: 4.6656\n",
      "Epoch [2/2], Step [13190/64305], Loss: 4.8191\n",
      "Epoch [2/2], Step [13200/64305], Loss: 5.0751\n",
      "Epoch [2/2], Step [13210/64305], Loss: 4.7935\n",
      "Epoch [2/2], Step [13220/64305], Loss: 4.7735\n",
      "Epoch [2/2], Step [13230/64305], Loss: 4.9659\n",
      "Epoch [2/2], Step [13240/64305], Loss: 4.7878\n",
      "Epoch [2/2], Step [13250/64305], Loss: 5.0587\n",
      "Epoch [2/2], Step [13260/64305], Loss: 4.8164\n",
      "Epoch [2/2], Step [13270/64305], Loss: 4.7915\n",
      "Epoch [2/2], Step [13280/64305], Loss: 4.6842\n",
      "Epoch [2/2], Step [13290/64305], Loss: 4.8563\n",
      "Epoch [2/2], Step [13300/64305], Loss: 4.7160\n",
      "Epoch [2/2], Step [13310/64305], Loss: 5.0125\n",
      "Epoch [2/2], Step [13320/64305], Loss: 4.6570\n",
      "Epoch [2/2], Step [13330/64305], Loss: 4.8359\n",
      "Epoch [2/2], Step [13340/64305], Loss: 4.9746\n",
      "Epoch [2/2], Step [13350/64305], Loss: 4.8069\n",
      "Epoch [2/2], Step [13360/64305], Loss: 4.7096\n",
      "Epoch [2/2], Step [13370/64305], Loss: 5.0048\n",
      "Epoch [2/2], Step [13380/64305], Loss: 5.0651\n",
      "Epoch [2/2], Step [13390/64305], Loss: 4.8822\n",
      "Epoch [2/2], Step [13400/64305], Loss: 4.7992\n",
      "Epoch [2/2], Step [13410/64305], Loss: 4.9655\n",
      "Epoch [2/2], Step [13420/64305], Loss: 4.9565\n",
      "Epoch [2/2], Step [13430/64305], Loss: 4.9699\n",
      "Epoch [2/2], Step [13440/64305], Loss: 4.8238\n",
      "Epoch [2/2], Step [13450/64305], Loss: 4.7533\n",
      "Epoch [2/2], Step [13460/64305], Loss: 4.8775\n",
      "Epoch [2/2], Step [13470/64305], Loss: 4.8846\n",
      "Epoch [2/2], Step [13480/64305], Loss: 4.8943\n",
      "Epoch [2/2], Step [13490/64305], Loss: 4.9458\n",
      "Epoch [2/2], Step [13500/64305], Loss: 4.5773\n",
      "Epoch [2/2], Step [13510/64305], Loss: 4.9462\n",
      "Epoch [2/2], Step [13520/64305], Loss: 4.8707\n",
      "Epoch [2/2], Step [13530/64305], Loss: 4.8595\n",
      "Epoch [2/2], Step [13540/64305], Loss: 4.8889\n",
      "Epoch [2/2], Step [13550/64305], Loss: 4.7969\n",
      "Epoch [2/2], Step [13560/64305], Loss: 4.7579\n",
      "Epoch [2/2], Step [13570/64305], Loss: 4.7491\n",
      "Epoch [2/2], Step [13580/64305], Loss: 4.9977\n",
      "Epoch [2/2], Step [13590/64305], Loss: 5.0353\n",
      "Epoch [2/2], Step [13600/64305], Loss: 4.9617\n",
      "Epoch [2/2], Step [13610/64305], Loss: 4.8609\n",
      "Epoch [2/2], Step [13620/64305], Loss: 4.9191\n",
      "Epoch [2/2], Step [13630/64305], Loss: 4.7711\n",
      "Epoch [2/2], Step [13640/64305], Loss: 4.7508\n",
      "Epoch [2/2], Step [13650/64305], Loss: 4.8768\n",
      "Epoch [2/2], Step [13660/64305], Loss: 4.5472\n",
      "Epoch [2/2], Step [13670/64305], Loss: 4.8221\n",
      "Epoch [2/2], Step [13680/64305], Loss: 4.6777\n",
      "Epoch [2/2], Step [13690/64305], Loss: 4.8023\n",
      "Epoch [2/2], Step [13700/64305], Loss: 4.8436\n",
      "Epoch [2/2], Step [13710/64305], Loss: 4.9081\n",
      "Epoch [2/2], Step [13720/64305], Loss: 4.9789\n",
      "Epoch [2/2], Step [13730/64305], Loss: 4.7745\n",
      "Epoch [2/2], Step [13740/64305], Loss: 5.2167\n",
      "Epoch [2/2], Step [13750/64305], Loss: 4.9732\n",
      "Epoch [2/2], Step [13760/64305], Loss: 4.7517\n",
      "Epoch [2/2], Step [13770/64305], Loss: 4.8497\n",
      "Epoch [2/2], Step [13780/64305], Loss: 4.9761\n",
      "Epoch [2/2], Step [13790/64305], Loss: 4.9385\n",
      "Epoch [2/2], Step [13800/64305], Loss: 4.8809\n",
      "Epoch [2/2], Step [13810/64305], Loss: 4.7944\n",
      "Epoch [2/2], Step [13820/64305], Loss: 4.9353\n",
      "Epoch [2/2], Step [13830/64305], Loss: 4.6308\n",
      "Epoch [2/2], Step [13840/64305], Loss: 4.8159\n",
      "Epoch [2/2], Step [13850/64305], Loss: 4.8718\n",
      "Epoch [2/2], Step [13860/64305], Loss: 4.9382\n",
      "Epoch [2/2], Step [13870/64305], Loss: 4.8562\n",
      "Epoch [2/2], Step [13880/64305], Loss: 4.7836\n",
      "Epoch [2/2], Step [13890/64305], Loss: 5.0266\n",
      "Epoch [2/2], Step [13900/64305], Loss: 5.0087\n",
      "Epoch [2/2], Step [13910/64305], Loss: 5.0165\n",
      "Epoch [2/2], Step [13920/64305], Loss: 4.8730\n",
      "Epoch [2/2], Step [13930/64305], Loss: 4.7306\n",
      "Epoch [2/2], Step [13940/64305], Loss: 5.0020\n",
      "Epoch [2/2], Step [13950/64305], Loss: 4.7522\n",
      "Epoch [2/2], Step [13960/64305], Loss: 4.9891\n",
      "Epoch [2/2], Step [13970/64305], Loss: 4.7729\n",
      "Epoch [2/2], Step [13980/64305], Loss: 4.8870\n",
      "Epoch [2/2], Step [13990/64305], Loss: 4.9504\n",
      "Epoch [2/2], Step [14000/64305], Loss: 5.1697\n",
      "Epoch [2/2], Step [14010/64305], Loss: 4.9522\n",
      "Epoch [2/2], Step [14020/64305], Loss: 4.9283\n",
      "Epoch [2/2], Step [14030/64305], Loss: 4.7549\n",
      "Epoch [2/2], Step [14040/64305], Loss: 4.7985\n",
      "Epoch [2/2], Step [14050/64305], Loss: 4.9345\n",
      "Epoch [2/2], Step [14060/64305], Loss: 4.8295\n",
      "Epoch [2/2], Step [14070/64305], Loss: 5.0285\n",
      "Epoch [2/2], Step [14080/64305], Loss: 4.8468\n",
      "Epoch [2/2], Step [14090/64305], Loss: 4.8773\n",
      "Epoch [2/2], Step [14100/64305], Loss: 4.9765\n",
      "Epoch [2/2], Step [14110/64305], Loss: 4.7520\n",
      "Epoch [2/2], Step [14120/64305], Loss: 4.8392\n",
      "Epoch [2/2], Step [14130/64305], Loss: 4.9385\n",
      "Epoch [2/2], Step [14140/64305], Loss: 4.7540\n",
      "Epoch [2/2], Step [14150/64305], Loss: 4.9917\n",
      "Epoch [2/2], Step [14160/64305], Loss: 4.8630\n",
      "Epoch [2/2], Step [14170/64305], Loss: 4.7584\n",
      "Epoch [2/2], Step [14180/64305], Loss: 4.8362\n",
      "Epoch [2/2], Step [14190/64305], Loss: 4.7938\n",
      "Epoch [2/2], Step [14200/64305], Loss: 4.8546\n",
      "Epoch [2/2], Step [14210/64305], Loss: 4.9589\n",
      "Epoch [2/2], Step [14220/64305], Loss: 4.9143\n",
      "Epoch [2/2], Step [14230/64305], Loss: 4.7491\n",
      "Epoch [2/2], Step [14240/64305], Loss: 4.8754\n",
      "Epoch [2/2], Step [14250/64305], Loss: 4.8364\n",
      "Epoch [2/2], Step [14260/64305], Loss: 4.8629\n",
      "Epoch [2/2], Step [14270/64305], Loss: 4.7977\n",
      "Epoch [2/2], Step [14280/64305], Loss: 4.8844\n",
      "Epoch [2/2], Step [14290/64305], Loss: 4.8859\n",
      "Epoch [2/2], Step [14300/64305], Loss: 4.8856\n",
      "Epoch [2/2], Step [14310/64305], Loss: 4.5480\n",
      "Epoch [2/2], Step [14320/64305], Loss: 4.6997\n",
      "Epoch [2/2], Step [14330/64305], Loss: 4.9405\n",
      "Epoch [2/2], Step [14340/64305], Loss: 4.8581\n",
      "Epoch [2/2], Step [14350/64305], Loss: 4.7946\n",
      "Epoch [2/2], Step [14360/64305], Loss: 4.9179\n",
      "Epoch [2/2], Step [14370/64305], Loss: 4.9627\n",
      "Epoch [2/2], Step [14380/64305], Loss: 5.0484\n",
      "Epoch [2/2], Step [14390/64305], Loss: 4.9236\n",
      "Epoch [2/2], Step [14400/64305], Loss: 4.8195\n",
      "Epoch [2/2], Step [14410/64305], Loss: 4.7601\n",
      "Epoch [2/2], Step [14420/64305], Loss: 4.8241\n",
      "Epoch [2/2], Step [14430/64305], Loss: 5.0250\n",
      "Epoch [2/2], Step [14440/64305], Loss: 4.8554\n",
      "Epoch [2/2], Step [14450/64305], Loss: 5.1236\n",
      "Epoch [2/2], Step [14460/64305], Loss: 4.8729\n",
      "Epoch [2/2], Step [14470/64305], Loss: 5.0241\n",
      "Epoch [2/2], Step [14480/64305], Loss: 4.8973\n",
      "Epoch [2/2], Step [14490/64305], Loss: 4.7873\n",
      "Epoch [2/2], Step [14500/64305], Loss: 4.6323\n",
      "Epoch [2/2], Step [14510/64305], Loss: 4.8560\n",
      "Epoch [2/2], Step [14520/64305], Loss: 4.7723\n",
      "Epoch [2/2], Step [14530/64305], Loss: 4.8353\n",
      "Epoch [2/2], Step [14540/64305], Loss: 4.9354\n",
      "Epoch [2/2], Step [14550/64305], Loss: 5.2230\n",
      "Epoch [2/2], Step [14560/64305], Loss: 5.0251\n",
      "Epoch [2/2], Step [14570/64305], Loss: 4.8868\n",
      "Epoch [2/2], Step [14580/64305], Loss: 5.0517\n",
      "Epoch [2/2], Step [14590/64305], Loss: 4.9509\n",
      "Epoch [2/2], Step [14600/64305], Loss: 4.8373\n",
      "Epoch [2/2], Step [14610/64305], Loss: 4.7896\n",
      "Epoch [2/2], Step [14620/64305], Loss: 4.8996\n",
      "Epoch [2/2], Step [14630/64305], Loss: 4.8214\n",
      "Epoch [2/2], Step [14640/64305], Loss: 4.8974\n",
      "Epoch [2/2], Step [14650/64305], Loss: 4.7472\n",
      "Epoch [2/2], Step [14660/64305], Loss: 4.8417\n",
      "Epoch [2/2], Step [14670/64305], Loss: 4.8195\n",
      "Epoch [2/2], Step [14680/64305], Loss: 4.7844\n",
      "Epoch [2/2], Step [14690/64305], Loss: 4.7998\n",
      "Epoch [2/2], Step [14700/64305], Loss: 5.0212\n",
      "Epoch [2/2], Step [14710/64305], Loss: 4.9361\n",
      "Epoch [2/2], Step [14720/64305], Loss: 4.7715\n",
      "Epoch [2/2], Step [14730/64305], Loss: 4.7854\n",
      "Epoch [2/2], Step [14740/64305], Loss: 4.8497\n",
      "Epoch [2/2], Step [14750/64305], Loss: 4.8207\n",
      "Epoch [2/2], Step [14760/64305], Loss: 4.9314\n",
      "Epoch [2/2], Step [14770/64305], Loss: 4.9548\n",
      "Epoch [2/2], Step [14780/64305], Loss: 4.9085\n",
      "Epoch [2/2], Step [14790/64305], Loss: 4.9514\n",
      "Epoch [2/2], Step [14800/64305], Loss: 5.0217\n",
      "Epoch [2/2], Step [14810/64305], Loss: 4.6991\n",
      "Epoch [2/2], Step [14820/64305], Loss: 5.0543\n",
      "Epoch [2/2], Step [14830/64305], Loss: 4.7305\n",
      "Epoch [2/2], Step [14840/64305], Loss: 5.0591\n",
      "Epoch [2/2], Step [14850/64305], Loss: 4.8759\n",
      "Epoch [2/2], Step [14860/64305], Loss: 4.8550\n",
      "Epoch [2/2], Step [14870/64305], Loss: 4.8202\n",
      "Epoch [2/2], Step [14880/64305], Loss: 4.9547\n",
      "Epoch [2/2], Step [14890/64305], Loss: 4.8472\n",
      "Epoch [2/2], Step [14900/64305], Loss: 4.9749\n",
      "Epoch [2/2], Step [14910/64305], Loss: 4.8649\n",
      "Epoch [2/2], Step [14920/64305], Loss: 4.8886\n",
      "Epoch [2/2], Step [14930/64305], Loss: 5.0072\n",
      "Epoch [2/2], Step [14940/64305], Loss: 4.6837\n",
      "Epoch [2/2], Step [14950/64305], Loss: 4.7982\n",
      "Epoch [2/2], Step [14960/64305], Loss: 4.8767\n",
      "Epoch [2/2], Step [14970/64305], Loss: 4.7853\n",
      "Epoch [2/2], Step [14980/64305], Loss: 4.8285\n",
      "Epoch [2/2], Step [14990/64305], Loss: 4.7695\n",
      "Epoch [2/2], Step [15000/64305], Loss: 4.7825\n",
      "Epoch [2/2], Step [15010/64305], Loss: 4.9074\n",
      "Epoch [2/2], Step [15020/64305], Loss: 4.8673\n",
      "Epoch [2/2], Step [15030/64305], Loss: 4.9260\n",
      "Epoch [2/2], Step [15040/64305], Loss: 4.7219\n",
      "Epoch [2/2], Step [15050/64305], Loss: 5.0665\n",
      "Epoch [2/2], Step [15060/64305], Loss: 4.7778\n",
      "Epoch [2/2], Step [15070/64305], Loss: 5.0915\n",
      "Epoch [2/2], Step [15080/64305], Loss: 4.8491\n",
      "Epoch [2/2], Step [15090/64305], Loss: 5.0307\n",
      "Epoch [2/2], Step [15100/64305], Loss: 5.0054\n",
      "Epoch [2/2], Step [15110/64305], Loss: 4.8996\n",
      "Epoch [2/2], Step [15120/64305], Loss: 4.8428\n",
      "Epoch [2/2], Step [15130/64305], Loss: 4.9542\n",
      "Epoch [2/2], Step [15140/64305], Loss: 4.9266\n",
      "Epoch [2/2], Step [15150/64305], Loss: 4.9160\n",
      "Epoch [2/2], Step [15160/64305], Loss: 4.9370\n",
      "Epoch [2/2], Step [15170/64305], Loss: 4.8752\n",
      "Epoch [2/2], Step [15180/64305], Loss: 4.7374\n",
      "Epoch [2/2], Step [15190/64305], Loss: 4.7901\n",
      "Epoch [2/2], Step [15200/64305], Loss: 5.0261\n",
      "Epoch [2/2], Step [15210/64305], Loss: 4.9307\n",
      "Epoch [2/2], Step [15220/64305], Loss: 5.0051\n",
      "Epoch [2/2], Step [15230/64305], Loss: 4.7293\n",
      "Epoch [2/2], Step [15240/64305], Loss: 4.5922\n",
      "Epoch [2/2], Step [15250/64305], Loss: 5.0079\n",
      "Epoch [2/2], Step [15260/64305], Loss: 4.7221\n",
      "Epoch [2/2], Step [15270/64305], Loss: 4.7997\n",
      "Epoch [2/2], Step [15280/64305], Loss: 4.9154\n",
      "Epoch [2/2], Step [15290/64305], Loss: 4.8357\n",
      "Epoch [2/2], Step [15300/64305], Loss: 4.8704\n",
      "Epoch [2/2], Step [15310/64305], Loss: 4.5746\n",
      "Epoch [2/2], Step [15320/64305], Loss: 4.8760\n",
      "Epoch [2/2], Step [15330/64305], Loss: 5.0522\n",
      "Epoch [2/2], Step [15340/64305], Loss: 4.8780\n",
      "Epoch [2/2], Step [15350/64305], Loss: 4.9695\n",
      "Epoch [2/2], Step [15360/64305], Loss: 4.8130\n",
      "Epoch [2/2], Step [15370/64305], Loss: 4.7087\n",
      "Epoch [2/2], Step [15380/64305], Loss: 4.9440\n",
      "Epoch [2/2], Step [15390/64305], Loss: 4.6013\n",
      "Epoch [2/2], Step [15400/64305], Loss: 4.8250\n",
      "Epoch [2/2], Step [15410/64305], Loss: 4.9595\n",
      "Epoch [2/2], Step [15420/64305], Loss: 4.9304\n",
      "Epoch [2/2], Step [15430/64305], Loss: 4.9743\n",
      "Epoch [2/2], Step [15440/64305], Loss: 4.8800\n",
      "Epoch [2/2], Step [15450/64305], Loss: 4.8532\n",
      "Epoch [2/2], Step [15460/64305], Loss: 4.8435\n",
      "Epoch [2/2], Step [15470/64305], Loss: 4.9158\n",
      "Epoch [2/2], Step [15480/64305], Loss: 4.7881\n",
      "Epoch [2/2], Step [15490/64305], Loss: 4.9406\n",
      "Epoch [2/2], Step [15500/64305], Loss: 5.0708\n",
      "Epoch [2/2], Step [15510/64305], Loss: 4.8132\n",
      "Epoch [2/2], Step [15520/64305], Loss: 4.7354\n",
      "Epoch [2/2], Step [15530/64305], Loss: 4.9933\n",
      "Epoch [2/2], Step [15540/64305], Loss: 4.9700\n",
      "Epoch [2/2], Step [15550/64305], Loss: 4.9262\n",
      "Epoch [2/2], Step [15560/64305], Loss: 4.8391\n",
      "Epoch [2/2], Step [15570/64305], Loss: 4.8580\n",
      "Epoch [2/2], Step [15580/64305], Loss: 4.7581\n",
      "Epoch [2/2], Step [15590/64305], Loss: 5.0355\n",
      "Epoch [2/2], Step [15600/64305], Loss: 4.7833\n",
      "Epoch [2/2], Step [15610/64305], Loss: 4.7916\n",
      "Epoch [2/2], Step [15620/64305], Loss: 4.9207\n",
      "Epoch [2/2], Step [15630/64305], Loss: 4.8246\n",
      "Epoch [2/2], Step [15640/64305], Loss: 4.9040\n",
      "Epoch [2/2], Step [15650/64305], Loss: 4.9071\n",
      "Epoch [2/2], Step [15660/64305], Loss: 4.8559\n",
      "Epoch [2/2], Step [15670/64305], Loss: 5.1827\n",
      "Epoch [2/2], Step [15680/64305], Loss: 4.9856\n",
      "Epoch [2/2], Step [15690/64305], Loss: 4.7702\n",
      "Epoch [2/2], Step [15700/64305], Loss: 4.9135\n",
      "Epoch [2/2], Step [15710/64305], Loss: 5.0043\n",
      "Epoch [2/2], Step [15720/64305], Loss: 4.7957\n",
      "Epoch [2/2], Step [15730/64305], Loss: 4.8883\n",
      "Epoch [2/2], Step [15740/64305], Loss: 5.1236\n",
      "Epoch [2/2], Step [15750/64305], Loss: 4.9375\n",
      "Epoch [2/2], Step [15760/64305], Loss: 4.8329\n",
      "Epoch [2/2], Step [15770/64305], Loss: 4.7478\n",
      "Epoch [2/2], Step [15780/64305], Loss: 4.9877\n",
      "Epoch [2/2], Step [15790/64305], Loss: 4.9457\n",
      "Epoch [2/2], Step [15800/64305], Loss: 4.6911\n",
      "Epoch [2/2], Step [15810/64305], Loss: 5.0313\n",
      "Epoch [2/2], Step [15820/64305], Loss: 5.0648\n",
      "Epoch [2/2], Step [15830/64305], Loss: 4.9206\n",
      "Epoch [2/2], Step [15840/64305], Loss: 4.9191\n",
      "Epoch [2/2], Step [15850/64305], Loss: 4.7589\n",
      "Epoch [2/2], Step [15860/64305], Loss: 4.8717\n",
      "Epoch [2/2], Step [15870/64305], Loss: 4.7444\n",
      "Epoch [2/2], Step [15880/64305], Loss: 4.8797\n",
      "Epoch [2/2], Step [15890/64305], Loss: 4.9112\n",
      "Epoch [2/2], Step [15900/64305], Loss: 4.7475\n",
      "Epoch [2/2], Step [15910/64305], Loss: 4.7049\n",
      "Epoch [2/2], Step [15920/64305], Loss: 4.9544\n",
      "Epoch [2/2], Step [15930/64305], Loss: 4.7761\n",
      "Epoch [2/2], Step [15940/64305], Loss: 4.9834\n",
      "Epoch [2/2], Step [15950/64305], Loss: 4.8956\n",
      "Epoch [2/2], Step [15960/64305], Loss: 4.9637\n",
      "Epoch [2/2], Step [15970/64305], Loss: 4.8743\n",
      "Epoch [2/2], Step [15980/64305], Loss: 4.7061\n",
      "Epoch [2/2], Step [15990/64305], Loss: 4.9158\n",
      "Epoch [2/2], Step [16000/64305], Loss: 4.7113\n",
      "Epoch [2/2], Step [16010/64305], Loss: 5.0691\n",
      "Epoch [2/2], Step [16020/64305], Loss: 4.9315\n",
      "Epoch [2/2], Step [16030/64305], Loss: 4.8115\n",
      "Epoch [2/2], Step [16040/64305], Loss: 4.9308\n",
      "Epoch [2/2], Step [16050/64305], Loss: 4.9665\n",
      "Epoch [2/2], Step [16060/64305], Loss: 4.8158\n",
      "Epoch [2/2], Step [16070/64305], Loss: 4.7795\n",
      "Epoch [2/2], Step [16080/64305], Loss: 4.7043\n",
      "Epoch [2/2], Step [16090/64305], Loss: 4.7662\n",
      "Epoch [2/2], Step [16100/64305], Loss: 4.7481\n",
      "Epoch [2/2], Step [16110/64305], Loss: 4.6241\n",
      "Epoch [2/2], Step [16120/64305], Loss: 4.5978\n",
      "Epoch [2/2], Step [16130/64305], Loss: 4.8048\n",
      "Epoch [2/2], Step [16140/64305], Loss: 4.9950\n",
      "Epoch [2/2], Step [16150/64305], Loss: 4.8910\n",
      "Epoch [2/2], Step [16160/64305], Loss: 4.7719\n",
      "Epoch [2/2], Step [16170/64305], Loss: 5.0185\n",
      "Epoch [2/2], Step [16180/64305], Loss: 4.8109\n",
      "Epoch [2/2], Step [16190/64305], Loss: 4.9421\n",
      "Epoch [2/2], Step [16200/64305], Loss: 4.9048\n",
      "Epoch [2/2], Step [16210/64305], Loss: 4.7763\n",
      "Epoch [2/2], Step [16220/64305], Loss: 5.0075\n",
      "Epoch [2/2], Step [16230/64305], Loss: 4.7341\n",
      "Epoch [2/2], Step [16240/64305], Loss: 4.7854\n",
      "Epoch [2/2], Step [16250/64305], Loss: 4.9178\n",
      "Epoch [2/2], Step [16260/64305], Loss: 4.7240\n",
      "Epoch [2/2], Step [16270/64305], Loss: 4.9152\n",
      "Epoch [2/2], Step [16280/64305], Loss: 4.8975\n",
      "Epoch [2/2], Step [16290/64305], Loss: 5.0287\n",
      "Epoch [2/2], Step [16300/64305], Loss: 4.6143\n",
      "Epoch [2/2], Step [16310/64305], Loss: 4.9224\n",
      "Epoch [2/2], Step [16320/64305], Loss: 4.8746\n",
      "Epoch [2/2], Step [16330/64305], Loss: 4.8988\n",
      "Epoch [2/2], Step [16340/64305], Loss: 4.8697\n",
      "Epoch [2/2], Step [16350/64305], Loss: 4.6964\n",
      "Epoch [2/2], Step [16360/64305], Loss: 4.7423\n",
      "Epoch [2/2], Step [16370/64305], Loss: 4.9437\n",
      "Epoch [2/2], Step [16380/64305], Loss: 4.7056\n",
      "Epoch [2/2], Step [16390/64305], Loss: 4.7991\n",
      "Epoch [2/2], Step [16400/64305], Loss: 5.0081\n",
      "Epoch [2/2], Step [16410/64305], Loss: 4.8083\n",
      "Epoch [2/2], Step [16420/64305], Loss: 4.9321\n",
      "Epoch [2/2], Step [16430/64305], Loss: 4.9500\n",
      "Epoch [2/2], Step [16440/64305], Loss: 4.8948\n",
      "Epoch [2/2], Step [16450/64305], Loss: 4.9489\n",
      "Epoch [2/2], Step [16460/64305], Loss: 4.9663\n",
      "Epoch [2/2], Step [16470/64305], Loss: 4.9054\n",
      "Epoch [2/2], Step [16480/64305], Loss: 4.6969\n",
      "Epoch [2/2], Step [16490/64305], Loss: 4.9719\n",
      "Epoch [2/2], Step [16500/64305], Loss: 4.9151\n",
      "Epoch [2/2], Step [16510/64305], Loss: 4.9621\n",
      "Epoch [2/2], Step [16520/64305], Loss: 4.7768\n",
      "Epoch [2/2], Step [16530/64305], Loss: 4.8590\n",
      "Epoch [2/2], Step [16540/64305], Loss: 5.0309\n",
      "Epoch [2/2], Step [16550/64305], Loss: 4.8545\n",
      "Epoch [2/2], Step [16560/64305], Loss: 4.9381\n",
      "Epoch [2/2], Step [16570/64305], Loss: 4.8067\n",
      "Epoch [2/2], Step [16580/64305], Loss: 4.9077\n",
      "Epoch [2/2], Step [16590/64305], Loss: 4.8060\n",
      "Epoch [2/2], Step [16600/64305], Loss: 4.7707\n",
      "Epoch [2/2], Step [16610/64305], Loss: 4.8481\n",
      "Epoch [2/2], Step [16620/64305], Loss: 4.9525\n",
      "Epoch [2/2], Step [16630/64305], Loss: 4.8419\n",
      "Epoch [2/2], Step [16640/64305], Loss: 4.8627\n",
      "Epoch [2/2], Step [16650/64305], Loss: 4.7868\n",
      "Epoch [2/2], Step [16660/64305], Loss: 5.0022\n",
      "Epoch [2/2], Step [16670/64305], Loss: 4.7076\n",
      "Epoch [2/2], Step [16680/64305], Loss: 4.8775\n",
      "Epoch [2/2], Step [16690/64305], Loss: 4.9695\n",
      "Epoch [2/2], Step [16700/64305], Loss: 4.6416\n",
      "Epoch [2/2], Step [16710/64305], Loss: 4.8400\n",
      "Epoch [2/2], Step [16720/64305], Loss: 4.8788\n",
      "Epoch [2/2], Step [16730/64305], Loss: 4.9086\n",
      "Epoch [2/2], Step [16740/64305], Loss: 4.8393\n",
      "Epoch [2/2], Step [16750/64305], Loss: 4.9499\n",
      "Epoch [2/2], Step [16760/64305], Loss: 4.8975\n",
      "Epoch [2/2], Step [16770/64305], Loss: 4.8076\n",
      "Epoch [2/2], Step [16780/64305], Loss: 4.8926\n",
      "Epoch [2/2], Step [16790/64305], Loss: 4.8323\n",
      "Epoch [2/2], Step [16800/64305], Loss: 4.7590\n",
      "Epoch [2/2], Step [16810/64305], Loss: 4.8712\n",
      "Epoch [2/2], Step [16820/64305], Loss: 4.7706\n",
      "Epoch [2/2], Step [16830/64305], Loss: 4.7537\n",
      "Epoch [2/2], Step [16840/64305], Loss: 4.7027\n",
      "Epoch [2/2], Step [16850/64305], Loss: 4.9580\n",
      "Epoch [2/2], Step [16860/64305], Loss: 4.7641\n",
      "Epoch [2/2], Step [16870/64305], Loss: 4.6278\n",
      "Epoch [2/2], Step [16880/64305], Loss: 4.7747\n",
      "Epoch [2/2], Step [16890/64305], Loss: 4.9717\n",
      "Epoch [2/2], Step [16900/64305], Loss: 4.9955\n",
      "Epoch [2/2], Step [16910/64305], Loss: 4.9703\n",
      "Epoch [2/2], Step [16920/64305], Loss: 4.7289\n",
      "Epoch [2/2], Step [16930/64305], Loss: 4.6449\n",
      "Epoch [2/2], Step [16940/64305], Loss: 5.0276\n",
      "Epoch [2/2], Step [16950/64305], Loss: 4.6226\n",
      "Epoch [2/2], Step [16960/64305], Loss: 4.9224\n",
      "Epoch [2/2], Step [16970/64305], Loss: 4.7295\n",
      "Epoch [2/2], Step [16980/64305], Loss: 4.7253\n",
      "Epoch [2/2], Step [16990/64305], Loss: 5.0122\n",
      "Epoch [2/2], Step [17000/64305], Loss: 4.7590\n",
      "Epoch [2/2], Step [17010/64305], Loss: 4.8380\n",
      "Epoch [2/2], Step [17020/64305], Loss: 4.8317\n",
      "Epoch [2/2], Step [17030/64305], Loss: 4.6390\n",
      "Epoch [2/2], Step [17040/64305], Loss: 4.9529\n",
      "Epoch [2/2], Step [17050/64305], Loss: 4.9170\n",
      "Epoch [2/2], Step [17060/64305], Loss: 4.8908\n",
      "Epoch [2/2], Step [17070/64305], Loss: 4.7673\n",
      "Epoch [2/2], Step [17080/64305], Loss: 4.9457\n",
      "Epoch [2/2], Step [17090/64305], Loss: 4.8929\n",
      "Epoch [2/2], Step [17100/64305], Loss: 4.8261\n",
      "Epoch [2/2], Step [17110/64305], Loss: 4.6082\n",
      "Epoch [2/2], Step [17120/64305], Loss: 4.7998\n",
      "Epoch [2/2], Step [17130/64305], Loss: 4.8067\n",
      "Epoch [2/2], Step [17140/64305], Loss: 4.9430\n",
      "Epoch [2/2], Step [17150/64305], Loss: 4.9101\n",
      "Epoch [2/2], Step [17160/64305], Loss: 4.6351\n",
      "Epoch [2/2], Step [17170/64305], Loss: 5.0140\n",
      "Epoch [2/2], Step [17180/64305], Loss: 4.8450\n",
      "Epoch [2/2], Step [17190/64305], Loss: 4.7134\n",
      "Epoch [2/2], Step [17200/64305], Loss: 4.8293\n",
      "Epoch [2/2], Step [17210/64305], Loss: 4.7850\n",
      "Epoch [2/2], Step [17220/64305], Loss: 4.8388\n",
      "Epoch [2/2], Step [17230/64305], Loss: 5.0772\n",
      "Epoch [2/2], Step [17240/64305], Loss: 4.8350\n",
      "Epoch [2/2], Step [17250/64305], Loss: 4.8987\n",
      "Epoch [2/2], Step [17260/64305], Loss: 4.8941\n",
      "Epoch [2/2], Step [17270/64305], Loss: 4.8644\n",
      "Epoch [2/2], Step [17280/64305], Loss: 5.1193\n",
      "Epoch [2/2], Step [17290/64305], Loss: 4.9089\n",
      "Epoch [2/2], Step [17300/64305], Loss: 4.9595\n",
      "Epoch [2/2], Step [17310/64305], Loss: 4.7115\n",
      "Epoch [2/2], Step [17320/64305], Loss: 4.9052\n",
      "Epoch [2/2], Step [17330/64305], Loss: 4.8464\n",
      "Epoch [2/2], Step [17340/64305], Loss: 5.0243\n",
      "Epoch [2/2], Step [17350/64305], Loss: 4.9200\n",
      "Epoch [2/2], Step [17360/64305], Loss: 4.8194\n",
      "Epoch [2/2], Step [17370/64305], Loss: 4.6607\n",
      "Epoch [2/2], Step [17380/64305], Loss: 4.9307\n",
      "Epoch [2/2], Step [17390/64305], Loss: 5.0390\n",
      "Epoch [2/2], Step [17400/64305], Loss: 4.8453\n",
      "Epoch [2/2], Step [17410/64305], Loss: 4.8563\n",
      "Epoch [2/2], Step [17420/64305], Loss: 4.8618\n",
      "Epoch [2/2], Step [17430/64305], Loss: 4.7422\n",
      "Epoch [2/2], Step [17440/64305], Loss: 4.6091\n",
      "Epoch [2/2], Step [17450/64305], Loss: 4.8958\n",
      "Epoch [2/2], Step [17460/64305], Loss: 4.8207\n",
      "Epoch [2/2], Step [17470/64305], Loss: 5.0078\n",
      "Epoch [2/2], Step [17480/64305], Loss: 4.9735\n",
      "Epoch [2/2], Step [17490/64305], Loss: 4.7902\n",
      "Epoch [2/2], Step [17500/64305], Loss: 4.6648\n",
      "Epoch [2/2], Step [17510/64305], Loss: 4.8335\n",
      "Epoch [2/2], Step [17520/64305], Loss: 4.9346\n",
      "Epoch [2/2], Step [17530/64305], Loss: 4.9183\n",
      "Epoch [2/2], Step [17540/64305], Loss: 5.0348\n",
      "Epoch [2/2], Step [17550/64305], Loss: 4.9458\n",
      "Epoch [2/2], Step [17560/64305], Loss: 4.8769\n",
      "Epoch [2/2], Step [17570/64305], Loss: 4.9670\n",
      "Epoch [2/2], Step [17580/64305], Loss: 4.8526\n",
      "Epoch [2/2], Step [17590/64305], Loss: 4.6425\n",
      "Epoch [2/2], Step [17600/64305], Loss: 4.7830\n",
      "Epoch [2/2], Step [17610/64305], Loss: 4.7983\n",
      "Epoch [2/2], Step [17620/64305], Loss: 4.8422\n",
      "Epoch [2/2], Step [17630/64305], Loss: 4.9845\n",
      "Epoch [2/2], Step [17640/64305], Loss: 4.8836\n",
      "Epoch [2/2], Step [17650/64305], Loss: 4.7406\n",
      "Epoch [2/2], Step [17660/64305], Loss: 4.8078\n",
      "Epoch [2/2], Step [17670/64305], Loss: 4.8489\n",
      "Epoch [2/2], Step [17680/64305], Loss: 4.9542\n",
      "Epoch [2/2], Step [17690/64305], Loss: 4.8664\n",
      "Epoch [2/2], Step [17700/64305], Loss: 5.1333\n",
      "Epoch [2/2], Step [17710/64305], Loss: 4.8971\n",
      "Epoch [2/2], Step [17720/64305], Loss: 4.9558\n",
      "Epoch [2/2], Step [17730/64305], Loss: 4.9527\n",
      "Epoch [2/2], Step [17740/64305], Loss: 4.9598\n",
      "Epoch [2/2], Step [17750/64305], Loss: 4.8642\n",
      "Epoch [2/2], Step [17760/64305], Loss: 4.5355\n",
      "Epoch [2/2], Step [17770/64305], Loss: 4.7994\n",
      "Epoch [2/2], Step [17780/64305], Loss: 4.9248\n",
      "Epoch [2/2], Step [17790/64305], Loss: 4.9757\n",
      "Epoch [2/2], Step [17800/64305], Loss: 4.8219\n",
      "Epoch [2/2], Step [17810/64305], Loss: 4.9479\n",
      "Epoch [2/2], Step [17820/64305], Loss: 4.8543\n",
      "Epoch [2/2], Step [17830/64305], Loss: 4.9371\n",
      "Epoch [2/2], Step [17840/64305], Loss: 4.7682\n",
      "Epoch [2/2], Step [17850/64305], Loss: 4.9265\n",
      "Epoch [2/2], Step [17860/64305], Loss: 4.6657\n",
      "Epoch [2/2], Step [17870/64305], Loss: 5.0502\n",
      "Epoch [2/2], Step [17880/64305], Loss: 5.0655\n",
      "Epoch [2/2], Step [17890/64305], Loss: 4.9880\n",
      "Epoch [2/2], Step [17900/64305], Loss: 4.8714\n",
      "Epoch [2/2], Step [17910/64305], Loss: 4.9778\n",
      "Epoch [2/2], Step [17920/64305], Loss: 4.5764\n",
      "Epoch [2/2], Step [17930/64305], Loss: 4.5831\n",
      "Epoch [2/2], Step [17940/64305], Loss: 4.6961\n",
      "Epoch [2/2], Step [17950/64305], Loss: 4.8448\n",
      "Epoch [2/2], Step [17960/64305], Loss: 5.0003\n",
      "Epoch [2/2], Step [17970/64305], Loss: 4.9368\n",
      "Epoch [2/2], Step [17980/64305], Loss: 5.0205\n",
      "Epoch [2/2], Step [17990/64305], Loss: 4.9491\n",
      "Epoch [2/2], Step [18000/64305], Loss: 4.8260\n",
      "Epoch [2/2], Step [18010/64305], Loss: 4.7086\n",
      "Epoch [2/2], Step [18020/64305], Loss: 4.9088\n",
      "Epoch [2/2], Step [18030/64305], Loss: 4.7804\n",
      "Epoch [2/2], Step [18040/64305], Loss: 4.6545\n",
      "Epoch [2/2], Step [18050/64305], Loss: 4.8201\n",
      "Epoch [2/2], Step [18060/64305], Loss: 4.7763\n",
      "Epoch [2/2], Step [18070/64305], Loss: 4.9698\n",
      "Epoch [2/2], Step [18080/64305], Loss: 4.8593\n",
      "Epoch [2/2], Step [18090/64305], Loss: 4.9596\n",
      "Epoch [2/2], Step [18100/64305], Loss: 4.8880\n",
      "Epoch [2/2], Step [18110/64305], Loss: 4.9884\n",
      "Epoch [2/2], Step [18120/64305], Loss: 4.8107\n",
      "Epoch [2/2], Step [18130/64305], Loss: 4.7684\n",
      "Epoch [2/2], Step [18140/64305], Loss: 4.9465\n",
      "Epoch [2/2], Step [18150/64305], Loss: 4.9132\n",
      "Epoch [2/2], Step [18160/64305], Loss: 4.7962\n",
      "Epoch [2/2], Step [18170/64305], Loss: 4.8886\n",
      "Epoch [2/2], Step [18180/64305], Loss: 4.8429\n",
      "Epoch [2/2], Step [18190/64305], Loss: 4.8392\n",
      "Epoch [2/2], Step [18200/64305], Loss: 5.1059\n",
      "Epoch [2/2], Step [18210/64305], Loss: 5.0356\n",
      "Epoch [2/2], Step [18220/64305], Loss: 4.8589\n",
      "Epoch [2/2], Step [18230/64305], Loss: 4.8255\n",
      "Epoch [2/2], Step [18240/64305], Loss: 4.8170\n",
      "Epoch [2/2], Step [18250/64305], Loss: 4.9000\n",
      "Epoch [2/2], Step [18260/64305], Loss: 5.0243\n",
      "Epoch [2/2], Step [18270/64305], Loss: 4.8015\n",
      "Epoch [2/2], Step [18280/64305], Loss: 4.9726\n",
      "Epoch [2/2], Step [18290/64305], Loss: 4.7772\n",
      "Epoch [2/2], Step [18300/64305], Loss: 4.8221\n",
      "Epoch [2/2], Step [18310/64305], Loss: 5.0359\n",
      "Epoch [2/2], Step [18320/64305], Loss: 4.9619\n",
      "Epoch [2/2], Step [18330/64305], Loss: 4.8519\n",
      "Epoch [2/2], Step [18340/64305], Loss: 5.0300\n",
      "Epoch [2/2], Step [18350/64305], Loss: 4.6892\n",
      "Epoch [2/2], Step [18360/64305], Loss: 4.8927\n",
      "Epoch [2/2], Step [18370/64305], Loss: 4.6651\n",
      "Epoch [2/2], Step [18380/64305], Loss: 4.8962\n",
      "Epoch [2/2], Step [18390/64305], Loss: 5.0208\n",
      "Epoch [2/2], Step [18400/64305], Loss: 4.6492\n",
      "Epoch [2/2], Step [18410/64305], Loss: 5.0021\n",
      "Epoch [2/2], Step [18420/64305], Loss: 4.8172\n",
      "Epoch [2/2], Step [18430/64305], Loss: 4.8893\n",
      "Epoch [2/2], Step [18440/64305], Loss: 5.0222\n",
      "Epoch [2/2], Step [18450/64305], Loss: 4.7967\n",
      "Epoch [2/2], Step [18460/64305], Loss: 4.8557\n",
      "Epoch [2/2], Step [18470/64305], Loss: 4.7013\n",
      "Epoch [2/2], Step [18480/64305], Loss: 4.7359\n",
      "Epoch [2/2], Step [18490/64305], Loss: 4.7589\n",
      "Epoch [2/2], Step [18500/64305], Loss: 4.9041\n",
      "Epoch [2/2], Step [18510/64305], Loss: 4.7624\n",
      "Epoch [2/2], Step [18520/64305], Loss: 4.8462\n",
      "Epoch [2/2], Step [18530/64305], Loss: 4.9273\n",
      "Epoch [2/2], Step [18540/64305], Loss: 4.6463\n",
      "Epoch [2/2], Step [18550/64305], Loss: 4.8486\n",
      "Epoch [2/2], Step [18560/64305], Loss: 4.8847\n",
      "Epoch [2/2], Step [18570/64305], Loss: 4.8703\n",
      "Epoch [2/2], Step [18580/64305], Loss: 4.8596\n",
      "Epoch [2/2], Step [18590/64305], Loss: 4.9245\n",
      "Epoch [2/2], Step [18600/64305], Loss: 5.0476\n",
      "Epoch [2/2], Step [18610/64305], Loss: 4.8620\n",
      "Epoch [2/2], Step [18620/64305], Loss: 4.9402\n",
      "Epoch [2/2], Step [18630/64305], Loss: 4.7807\n",
      "Epoch [2/2], Step [18640/64305], Loss: 4.8230\n",
      "Epoch [2/2], Step [18650/64305], Loss: 4.7184\n",
      "Epoch [2/2], Step [18660/64305], Loss: 4.8320\n",
      "Epoch [2/2], Step [18670/64305], Loss: 4.7563\n",
      "Epoch [2/2], Step [18680/64305], Loss: 4.6222\n",
      "Epoch [2/2], Step [18690/64305], Loss: 5.0005\n",
      "Epoch [2/2], Step [18700/64305], Loss: 4.7450\n",
      "Epoch [2/2], Step [18710/64305], Loss: 5.0551\n",
      "Epoch [2/2], Step [18720/64305], Loss: 4.8826\n",
      "Epoch [2/2], Step [18730/64305], Loss: 4.7902\n",
      "Epoch [2/2], Step [18740/64305], Loss: 4.8110\n",
      "Epoch [2/2], Step [18750/64305], Loss: 4.8525\n",
      "Epoch [2/2], Step [18760/64305], Loss: 4.7076\n",
      "Epoch [2/2], Step [18770/64305], Loss: 4.9277\n",
      "Epoch [2/2], Step [18780/64305], Loss: 4.9123\n",
      "Epoch [2/2], Step [18790/64305], Loss: 4.7518\n",
      "Epoch [2/2], Step [18800/64305], Loss: 5.1983\n",
      "Epoch [2/2], Step [18810/64305], Loss: 4.7635\n",
      "Epoch [2/2], Step [18820/64305], Loss: 4.9637\n",
      "Epoch [2/2], Step [18830/64305], Loss: 4.9549\n",
      "Epoch [2/2], Step [18840/64305], Loss: 4.9173\n",
      "Epoch [2/2], Step [18850/64305], Loss: 4.6508\n",
      "Epoch [2/2], Step [18860/64305], Loss: 4.9234\n",
      "Epoch [2/2], Step [18870/64305], Loss: 4.9316\n",
      "Epoch [2/2], Step [18880/64305], Loss: 5.0244\n",
      "Epoch [2/2], Step [18890/64305], Loss: 4.8096\n",
      "Epoch [2/2], Step [18900/64305], Loss: 4.7656\n",
      "Epoch [2/2], Step [18910/64305], Loss: 4.7586\n",
      "Epoch [2/2], Step [18920/64305], Loss: 4.9839\n",
      "Epoch [2/2], Step [18930/64305], Loss: 4.9980\n",
      "Epoch [2/2], Step [18940/64305], Loss: 5.1599\n",
      "Epoch [2/2], Step [18950/64305], Loss: 4.8743\n",
      "Epoch [2/2], Step [18960/64305], Loss: 5.0088\n",
      "Epoch [2/2], Step [18970/64305], Loss: 5.0387\n",
      "Epoch [2/2], Step [18980/64305], Loss: 4.8794\n",
      "Epoch [2/2], Step [18990/64305], Loss: 4.8364\n",
      "Epoch [2/2], Step [19000/64305], Loss: 5.0231\n",
      "Epoch [2/2], Step [19010/64305], Loss: 4.9057\n",
      "Epoch [2/2], Step [19020/64305], Loss: 4.8743\n",
      "Epoch [2/2], Step [19030/64305], Loss: 4.7088\n",
      "Epoch [2/2], Step [19040/64305], Loss: 4.8152\n",
      "Epoch [2/2], Step [19050/64305], Loss: 4.9601\n",
      "Epoch [2/2], Step [19060/64305], Loss: 4.7355\n",
      "Epoch [2/2], Step [19070/64305], Loss: 5.0481\n",
      "Epoch [2/2], Step [19080/64305], Loss: 4.7547\n",
      "Epoch [2/2], Step [19090/64305], Loss: 4.7796\n",
      "Epoch [2/2], Step [19100/64305], Loss: 4.8869\n",
      "Epoch [2/2], Step [19110/64305], Loss: 4.9157\n",
      "Epoch [2/2], Step [19120/64305], Loss: 4.9076\n",
      "Epoch [2/2], Step [19130/64305], Loss: 4.8549\n",
      "Epoch [2/2], Step [19140/64305], Loss: 4.8803\n",
      "Epoch [2/2], Step [19150/64305], Loss: 4.7234\n",
      "Epoch [2/2], Step [19160/64305], Loss: 4.6025\n",
      "Epoch [2/2], Step [19170/64305], Loss: 4.8051\n",
      "Epoch [2/2], Step [19180/64305], Loss: 4.7125\n",
      "Epoch [2/2], Step [19190/64305], Loss: 4.9746\n",
      "Epoch [2/2], Step [19200/64305], Loss: 5.0157\n",
      "Epoch [2/2], Step [19210/64305], Loss: 4.8234\n",
      "Epoch [2/2], Step [19220/64305], Loss: 5.1187\n",
      "Epoch [2/2], Step [19230/64305], Loss: 4.9834\n",
      "Epoch [2/2], Step [19240/64305], Loss: 4.9538\n",
      "Epoch [2/2], Step [19250/64305], Loss: 4.7961\n",
      "Epoch [2/2], Step [19260/64305], Loss: 4.5811\n",
      "Epoch [2/2], Step [19270/64305], Loss: 4.9409\n",
      "Epoch [2/2], Step [19280/64305], Loss: 4.7895\n",
      "Epoch [2/2], Step [19290/64305], Loss: 4.9117\n",
      "Epoch [2/2], Step [19300/64305], Loss: 4.8692\n",
      "Epoch [2/2], Step [19310/64305], Loss: 5.0432\n",
      "Epoch [2/2], Step [19320/64305], Loss: 4.8752\n",
      "Epoch [2/2], Step [19330/64305], Loss: 4.9596\n",
      "Epoch [2/2], Step [19340/64305], Loss: 4.7760\n",
      "Epoch [2/2], Step [19350/64305], Loss: 4.8038\n",
      "Epoch [2/2], Step [19360/64305], Loss: 4.9424\n",
      "Epoch [2/2], Step [19370/64305], Loss: 4.7897\n",
      "Epoch [2/2], Step [19380/64305], Loss: 4.6778\n",
      "Epoch [2/2], Step [19390/64305], Loss: 4.8975\n",
      "Epoch [2/2], Step [19400/64305], Loss: 4.9425\n",
      "Epoch [2/2], Step [19410/64305], Loss: 5.0214\n",
      "Epoch [2/2], Step [19420/64305], Loss: 4.8505\n",
      "Epoch [2/2], Step [19430/64305], Loss: 4.8750\n",
      "Epoch [2/2], Step [19440/64305], Loss: 4.8489\n",
      "Epoch [2/2], Step [19450/64305], Loss: 4.8251\n",
      "Epoch [2/2], Step [19460/64305], Loss: 4.8926\n",
      "Epoch [2/2], Step [19470/64305], Loss: 4.9430\n",
      "Epoch [2/2], Step [19480/64305], Loss: 4.8920\n",
      "Epoch [2/2], Step [19490/64305], Loss: 4.7499\n",
      "Epoch [2/2], Step [19500/64305], Loss: 5.0361\n",
      "Epoch [2/2], Step [19510/64305], Loss: 5.0175\n",
      "Epoch [2/2], Step [19520/64305], Loss: 4.7456\n",
      "Epoch [2/2], Step [19530/64305], Loss: 4.6660\n",
      "Epoch [2/2], Step [19540/64305], Loss: 5.0021\n",
      "Epoch [2/2], Step [19550/64305], Loss: 4.9335\n",
      "Epoch [2/2], Step [19560/64305], Loss: 4.7455\n",
      "Epoch [2/2], Step [19570/64305], Loss: 4.9710\n",
      "Epoch [2/2], Step [19580/64305], Loss: 4.9082\n",
      "Epoch [2/2], Step [19590/64305], Loss: 4.8579\n",
      "Epoch [2/2], Step [19600/64305], Loss: 4.8243\n",
      "Epoch [2/2], Step [19610/64305], Loss: 4.8710\n",
      "Epoch [2/2], Step [19620/64305], Loss: 4.9911\n",
      "Epoch [2/2], Step [19630/64305], Loss: 4.7772\n",
      "Epoch [2/2], Step [19640/64305], Loss: 4.8920\n",
      "Epoch [2/2], Step [19650/64305], Loss: 4.9615\n",
      "Epoch [2/2], Step [19660/64305], Loss: 4.8447\n",
      "Epoch [2/2], Step [19670/64305], Loss: 4.7868\n",
      "Epoch [2/2], Step [19680/64305], Loss: 4.7862\n",
      "Epoch [2/2], Step [19690/64305], Loss: 5.0109\n",
      "Epoch [2/2], Step [19700/64305], Loss: 5.0201\n",
      "Epoch [2/2], Step [19710/64305], Loss: 4.6952\n",
      "Epoch [2/2], Step [19720/64305], Loss: 4.8654\n",
      "Epoch [2/2], Step [19730/64305], Loss: 4.9539\n",
      "Epoch [2/2], Step [19740/64305], Loss: 4.8696\n",
      "Epoch [2/2], Step [19750/64305], Loss: 4.7951\n",
      "Epoch [2/2], Step [19760/64305], Loss: 4.8306\n",
      "Epoch [2/2], Step [19770/64305], Loss: 4.9354\n",
      "Epoch [2/2], Step [19780/64305], Loss: 4.8081\n",
      "Epoch [2/2], Step [19790/64305], Loss: 4.8451\n",
      "Epoch [2/2], Step [19800/64305], Loss: 4.9014\n",
      "Epoch [2/2], Step [19810/64305], Loss: 4.8613\n",
      "Epoch [2/2], Step [19820/64305], Loss: 4.8115\n",
      "Epoch [2/2], Step [19830/64305], Loss: 4.5773\n",
      "Epoch [2/2], Step [19840/64305], Loss: 4.7123\n",
      "Epoch [2/2], Step [19850/64305], Loss: 4.8869\n",
      "Epoch [2/2], Step [19860/64305], Loss: 4.7918\n",
      "Epoch [2/2], Step [19870/64305], Loss: 4.9317\n",
      "Epoch [2/2], Step [19880/64305], Loss: 4.7465\n",
      "Epoch [2/2], Step [19890/64305], Loss: 4.5446\n",
      "Epoch [2/2], Step [19900/64305], Loss: 5.1008\n",
      "Epoch [2/2], Step [19910/64305], Loss: 4.9662\n",
      "Epoch [2/2], Step [19920/64305], Loss: 4.8896\n",
      "Epoch [2/2], Step [19930/64305], Loss: 4.8950\n",
      "Epoch [2/2], Step [19940/64305], Loss: 4.8238\n",
      "Epoch [2/2], Step [19950/64305], Loss: 4.8579\n",
      "Epoch [2/2], Step [19960/64305], Loss: 4.9157\n",
      "Epoch [2/2], Step [19970/64305], Loss: 4.7726\n",
      "Epoch [2/2], Step [19980/64305], Loss: 4.7333\n",
      "Epoch [2/2], Step [19990/64305], Loss: 4.7856\n",
      "Epoch [2/2], Step [20000/64305], Loss: 4.9789\n",
      "Epoch [2/2], Step [20010/64305], Loss: 4.7066\n",
      "Epoch [2/2], Step [20020/64305], Loss: 4.8406\n",
      "Epoch [2/2], Step [20030/64305], Loss: 4.8527\n",
      "Epoch [2/2], Step [20040/64305], Loss: 4.9475\n",
      "Epoch [2/2], Step [20050/64305], Loss: 4.8231\n",
      "Epoch [2/2], Step [20060/64305], Loss: 4.7711\n",
      "Epoch [2/2], Step [20070/64305], Loss: 4.7981\n",
      "Epoch [2/2], Step [20080/64305], Loss: 4.6931\n",
      "Epoch [2/2], Step [20090/64305], Loss: 4.8487\n",
      "Epoch [2/2], Step [20100/64305], Loss: 4.7724\n",
      "Epoch [2/2], Step [20110/64305], Loss: 4.9886\n",
      "Epoch [2/2], Step [20120/64305], Loss: 4.9515\n",
      "Epoch [2/2], Step [20130/64305], Loss: 4.9078\n",
      "Epoch [2/2], Step [20140/64305], Loss: 4.8624\n",
      "Epoch [2/2], Step [20150/64305], Loss: 4.8478\n",
      "Epoch [2/2], Step [20160/64305], Loss: 4.8815\n",
      "Epoch [2/2], Step [20170/64305], Loss: 4.6841\n",
      "Epoch [2/2], Step [20180/64305], Loss: 4.9160\n",
      "Epoch [2/2], Step [20190/64305], Loss: 4.7904\n",
      "Epoch [2/2], Step [20200/64305], Loss: 4.9498\n",
      "Epoch [2/2], Step [20210/64305], Loss: 4.8610\n",
      "Epoch [2/2], Step [20220/64305], Loss: 4.9774\n",
      "Epoch [2/2], Step [20230/64305], Loss: 5.1155\n",
      "Epoch [2/2], Step [20240/64305], Loss: 4.5765\n",
      "Epoch [2/2], Step [20250/64305], Loss: 4.7240\n",
      "Epoch [2/2], Step [20260/64305], Loss: 4.8033\n",
      "Epoch [2/2], Step [20270/64305], Loss: 4.6563\n",
      "Epoch [2/2], Step [20280/64305], Loss: 4.8648\n",
      "Epoch [2/2], Step [20290/64305], Loss: 4.6403\n",
      "Epoch [2/2], Step [20300/64305], Loss: 4.9578\n",
      "Epoch [2/2], Step [20310/64305], Loss: 4.8960\n",
      "Epoch [2/2], Step [20320/64305], Loss: 5.0854\n",
      "Epoch [2/2], Step [20330/64305], Loss: 5.0086\n",
      "Epoch [2/2], Step [20340/64305], Loss: 4.7921\n",
      "Epoch [2/2], Step [20350/64305], Loss: 4.9746\n",
      "Epoch [2/2], Step [20360/64305], Loss: 4.7605\n",
      "Epoch [2/2], Step [20370/64305], Loss: 4.8856\n",
      "Epoch [2/2], Step [20380/64305], Loss: 4.9797\n",
      "Epoch [2/2], Step [20390/64305], Loss: 4.9692\n",
      "Epoch [2/2], Step [20400/64305], Loss: 5.0417\n",
      "Epoch [2/2], Step [20410/64305], Loss: 4.8149\n",
      "Epoch [2/2], Step [20420/64305], Loss: 4.8877\n",
      "Epoch [2/2], Step [20430/64305], Loss: 4.9167\n",
      "Epoch [2/2], Step [20440/64305], Loss: 4.7442\n",
      "Epoch [2/2], Step [20450/64305], Loss: 4.9370\n",
      "Epoch [2/2], Step [20460/64305], Loss: 4.8193\n",
      "Epoch [2/2], Step [20470/64305], Loss: 4.6667\n",
      "Epoch [2/2], Step [20480/64305], Loss: 4.7960\n",
      "Epoch [2/2], Step [20490/64305], Loss: 4.9056\n",
      "Epoch [2/2], Step [20500/64305], Loss: 5.0129\n",
      "Epoch [2/2], Step [20510/64305], Loss: 4.8890\n",
      "Epoch [2/2], Step [20520/64305], Loss: 4.6957\n",
      "Epoch [2/2], Step [20530/64305], Loss: 4.8053\n",
      "Epoch [2/2], Step [20540/64305], Loss: 4.8217\n",
      "Epoch [2/2], Step [20550/64305], Loss: 4.8939\n",
      "Epoch [2/2], Step [20560/64305], Loss: 4.9412\n",
      "Epoch [2/2], Step [20570/64305], Loss: 4.9222\n",
      "Epoch [2/2], Step [20580/64305], Loss: 4.7293\n",
      "Epoch [2/2], Step [20590/64305], Loss: 4.8366\n",
      "Epoch [2/2], Step [20600/64305], Loss: 4.8011\n",
      "Epoch [2/2], Step [20610/64305], Loss: 4.8998\n",
      "Epoch [2/2], Step [20620/64305], Loss: 5.0839\n",
      "Epoch [2/2], Step [20630/64305], Loss: 4.8285\n",
      "Epoch [2/2], Step [20640/64305], Loss: 4.9262\n",
      "Epoch [2/2], Step [20650/64305], Loss: 4.7977\n",
      "Epoch [2/2], Step [20660/64305], Loss: 4.8494\n",
      "Epoch [2/2], Step [20670/64305], Loss: 4.9073\n",
      "Epoch [2/2], Step [20680/64305], Loss: 4.9322\n",
      "Epoch [2/2], Step [20690/64305], Loss: 4.8424\n",
      "Epoch [2/2], Step [20700/64305], Loss: 4.8671\n",
      "Epoch [2/2], Step [20710/64305], Loss: 4.8535\n",
      "Epoch [2/2], Step [20720/64305], Loss: 4.8341\n",
      "Epoch [2/2], Step [20730/64305], Loss: 4.8351\n",
      "Epoch [2/2], Step [20740/64305], Loss: 4.8827\n",
      "Epoch [2/2], Step [20750/64305], Loss: 4.8133\n",
      "Epoch [2/2], Step [20760/64305], Loss: 5.1687\n",
      "Epoch [2/2], Step [20770/64305], Loss: 4.8145\n",
      "Epoch [2/2], Step [20780/64305], Loss: 4.8923\n",
      "Epoch [2/2], Step [20790/64305], Loss: 5.0213\n",
      "Epoch [2/2], Step [20800/64305], Loss: 4.9133\n",
      "Epoch [2/2], Step [20810/64305], Loss: 4.8309\n",
      "Epoch [2/2], Step [20820/64305], Loss: 4.8887\n",
      "Epoch [2/2], Step [20830/64305], Loss: 5.1191\n",
      "Epoch [2/2], Step [20840/64305], Loss: 4.6998\n",
      "Epoch [2/2], Step [20850/64305], Loss: 5.0398\n",
      "Epoch [2/2], Step [20860/64305], Loss: 4.8162\n",
      "Epoch [2/2], Step [20870/64305], Loss: 4.7328\n",
      "Epoch [2/2], Step [20880/64305], Loss: 4.8590\n",
      "Epoch [2/2], Step [20890/64305], Loss: 4.6027\n",
      "Epoch [2/2], Step [20900/64305], Loss: 4.9286\n",
      "Epoch [2/2], Step [20910/64305], Loss: 4.7939\n",
      "Epoch [2/2], Step [20920/64305], Loss: 4.7190\n",
      "Epoch [2/2], Step [20930/64305], Loss: 4.8869\n",
      "Epoch [2/2], Step [20940/64305], Loss: 4.8423\n",
      "Epoch [2/2], Step [20950/64305], Loss: 4.7013\n",
      "Epoch [2/2], Step [20960/64305], Loss: 4.8980\n",
      "Epoch [2/2], Step [20970/64305], Loss: 4.8841\n",
      "Epoch [2/2], Step [20980/64305], Loss: 4.8489\n",
      "Epoch [2/2], Step [20990/64305], Loss: 4.8929\n",
      "Epoch [2/2], Step [21000/64305], Loss: 5.0193\n",
      "Epoch [2/2], Step [21010/64305], Loss: 4.7006\n",
      "Epoch [2/2], Step [21020/64305], Loss: 4.9514\n",
      "Epoch [2/2], Step [21030/64305], Loss: 4.9624\n",
      "Epoch [2/2], Step [21040/64305], Loss: 4.9252\n",
      "Epoch [2/2], Step [21050/64305], Loss: 4.8404\n",
      "Epoch [2/2], Step [21060/64305], Loss: 4.6506\n",
      "Epoch [2/2], Step [21070/64305], Loss: 4.8153\n",
      "Epoch [2/2], Step [21080/64305], Loss: 5.0002\n",
      "Epoch [2/2], Step [21090/64305], Loss: 4.8364\n",
      "Epoch [2/2], Step [21100/64305], Loss: 5.0191\n",
      "Epoch [2/2], Step [21110/64305], Loss: 4.8390\n",
      "Epoch [2/2], Step [21120/64305], Loss: 4.9236\n",
      "Epoch [2/2], Step [21130/64305], Loss: 4.8456\n",
      "Epoch [2/2], Step [21140/64305], Loss: 4.7433\n",
      "Epoch [2/2], Step [21150/64305], Loss: 4.8795\n",
      "Epoch [2/2], Step [21160/64305], Loss: 4.7474\n",
      "Epoch [2/2], Step [21170/64305], Loss: 4.8277\n",
      "Epoch [2/2], Step [21180/64305], Loss: 4.8933\n",
      "Epoch [2/2], Step [21190/64305], Loss: 4.8399\n",
      "Epoch [2/2], Step [21200/64305], Loss: 4.7398\n",
      "Epoch [2/2], Step [21210/64305], Loss: 4.6894\n",
      "Epoch [2/2], Step [21220/64305], Loss: 4.9105\n",
      "Epoch [2/2], Step [21230/64305], Loss: 4.7157\n",
      "Epoch [2/2], Step [21240/64305], Loss: 4.8752\n",
      "Epoch [2/2], Step [21250/64305], Loss: 4.9701\n",
      "Epoch [2/2], Step [21260/64305], Loss: 4.9097\n",
      "Epoch [2/2], Step [21270/64305], Loss: 4.7622\n",
      "Epoch [2/2], Step [21280/64305], Loss: 4.8748\n",
      "Epoch [2/2], Step [21290/64305], Loss: 4.9206\n",
      "Epoch [2/2], Step [21300/64305], Loss: 4.9131\n",
      "Epoch [2/2], Step [21310/64305], Loss: 4.9798\n",
      "Epoch [2/2], Step [21320/64305], Loss: 4.9372\n",
      "Epoch [2/2], Step [21330/64305], Loss: 4.7801\n",
      "Epoch [2/2], Step [21340/64305], Loss: 4.8057\n",
      "Epoch [2/2], Step [21350/64305], Loss: 4.7002\n",
      "Epoch [2/2], Step [21360/64305], Loss: 4.5743\n",
      "Epoch [2/2], Step [21370/64305], Loss: 4.9488\n",
      "Epoch [2/2], Step [21380/64305], Loss: 4.7512\n",
      "Epoch [2/2], Step [21390/64305], Loss: 4.9571\n",
      "Epoch [2/2], Step [21400/64305], Loss: 4.9182\n",
      "Epoch [2/2], Step [21410/64305], Loss: 4.8749\n",
      "Epoch [2/2], Step [21420/64305], Loss: 4.7554\n",
      "Epoch [2/2], Step [21430/64305], Loss: 4.7476\n",
      "Epoch [2/2], Step [21440/64305], Loss: 4.9723\n",
      "Epoch [2/2], Step [21450/64305], Loss: 4.9875\n",
      "Epoch [2/2], Step [21460/64305], Loss: 4.8060\n",
      "Epoch [2/2], Step [21470/64305], Loss: 4.9142\n",
      "Epoch [2/2], Step [21480/64305], Loss: 4.8508\n",
      "Epoch [2/2], Step [21490/64305], Loss: 4.9928\n",
      "Epoch [2/2], Step [21500/64305], Loss: 4.9772\n",
      "Epoch [2/2], Step [21510/64305], Loss: 4.9982\n",
      "Epoch [2/2], Step [21520/64305], Loss: 4.8403\n",
      "Epoch [2/2], Step [21530/64305], Loss: 4.7202\n",
      "Epoch [2/2], Step [21540/64305], Loss: 4.6065\n",
      "Epoch [2/2], Step [21550/64305], Loss: 4.9717\n",
      "Epoch [2/2], Step [21560/64305], Loss: 4.9118\n",
      "Epoch [2/2], Step [21570/64305], Loss: 4.9231\n",
      "Epoch [2/2], Step [21580/64305], Loss: 4.8730\n",
      "Epoch [2/2], Step [21590/64305], Loss: 4.7940\n",
      "Epoch [2/2], Step [21600/64305], Loss: 4.9869\n",
      "Epoch [2/2], Step [21610/64305], Loss: 5.0083\n",
      "Epoch [2/2], Step [21620/64305], Loss: 4.8620\n",
      "Epoch [2/2], Step [21630/64305], Loss: 4.8836\n",
      "Epoch [2/2], Step [21640/64305], Loss: 4.8551\n",
      "Epoch [2/2], Step [21650/64305], Loss: 4.9774\n",
      "Epoch [2/2], Step [21660/64305], Loss: 4.9772\n",
      "Epoch [2/2], Step [21670/64305], Loss: 4.7469\n",
      "Epoch [2/2], Step [21680/64305], Loss: 4.7749\n",
      "Epoch [2/2], Step [21690/64305], Loss: 4.7198\n",
      "Epoch [2/2], Step [21700/64305], Loss: 5.0627\n",
      "Epoch [2/2], Step [21710/64305], Loss: 4.8070\n",
      "Epoch [2/2], Step [21720/64305], Loss: 4.7773\n",
      "Epoch [2/2], Step [21730/64305], Loss: 4.8806\n",
      "Epoch [2/2], Step [21740/64305], Loss: 4.8373\n",
      "Epoch [2/2], Step [21750/64305], Loss: 4.6599\n",
      "Epoch [2/2], Step [21760/64305], Loss: 4.8069\n",
      "Epoch [2/2], Step [21770/64305], Loss: 4.5207\n",
      "Epoch [2/2], Step [21780/64305], Loss: 4.8002\n",
      "Epoch [2/2], Step [21790/64305], Loss: 4.9313\n",
      "Epoch [2/2], Step [21800/64305], Loss: 4.7601\n",
      "Epoch [2/2], Step [21810/64305], Loss: 4.9610\n",
      "Epoch [2/2], Step [21820/64305], Loss: 4.8548\n",
      "Epoch [2/2], Step [21830/64305], Loss: 4.8181\n",
      "Epoch [2/2], Step [21840/64305], Loss: 4.7018\n",
      "Epoch [2/2], Step [21850/64305], Loss: 4.6854\n",
      "Epoch [2/2], Step [21860/64305], Loss: 4.9115\n",
      "Epoch [2/2], Step [21870/64305], Loss: 4.8750\n",
      "Epoch [2/2], Step [21880/64305], Loss: 4.7727\n",
      "Epoch [2/2], Step [21890/64305], Loss: 4.9146\n",
      "Epoch [2/2], Step [21900/64305], Loss: 4.7710\n",
      "Epoch [2/2], Step [21910/64305], Loss: 4.8858\n",
      "Epoch [2/2], Step [21920/64305], Loss: 4.7515\n",
      "Epoch [2/2], Step [21930/64305], Loss: 4.5773\n",
      "Epoch [2/2], Step [21940/64305], Loss: 4.7534\n",
      "Epoch [2/2], Step [21950/64305], Loss: 4.7836\n",
      "Epoch [2/2], Step [21960/64305], Loss: 4.9655\n",
      "Epoch [2/2], Step [21970/64305], Loss: 4.8862\n",
      "Epoch [2/2], Step [21980/64305], Loss: 4.6910\n",
      "Epoch [2/2], Step [21990/64305], Loss: 4.6886\n",
      "Epoch [2/2], Step [22000/64305], Loss: 4.8255\n",
      "Epoch [2/2], Step [22010/64305], Loss: 4.8155\n",
      "Epoch [2/2], Step [22020/64305], Loss: 5.0603\n",
      "Epoch [2/2], Step [22030/64305], Loss: 4.7276\n",
      "Epoch [2/2], Step [22040/64305], Loss: 4.8111\n",
      "Epoch [2/2], Step [22050/64305], Loss: 4.7833\n",
      "Epoch [2/2], Step [22060/64305], Loss: 4.8075\n",
      "Epoch [2/2], Step [22070/64305], Loss: 4.8491\n",
      "Epoch [2/2], Step [22080/64305], Loss: 4.7817\n",
      "Epoch [2/2], Step [22090/64305], Loss: 4.9511\n",
      "Epoch [2/2], Step [22100/64305], Loss: 5.0061\n",
      "Epoch [2/2], Step [22110/64305], Loss: 5.0675\n",
      "Epoch [2/2], Step [22120/64305], Loss: 5.0402\n",
      "Epoch [2/2], Step [22130/64305], Loss: 4.7975\n",
      "Epoch [2/2], Step [22140/64305], Loss: 4.8313\n",
      "Epoch [2/2], Step [22150/64305], Loss: 4.8528\n",
      "Epoch [2/2], Step [22160/64305], Loss: 4.9990\n",
      "Epoch [2/2], Step [22170/64305], Loss: 4.9744\n",
      "Epoch [2/2], Step [22180/64305], Loss: 4.8828\n",
      "Epoch [2/2], Step [22190/64305], Loss: 4.8422\n",
      "Epoch [2/2], Step [22200/64305], Loss: 5.0536\n",
      "Epoch [2/2], Step [22210/64305], Loss: 4.8794\n",
      "Epoch [2/2], Step [22220/64305], Loss: 4.7534\n",
      "Epoch [2/2], Step [22230/64305], Loss: 4.8906\n",
      "Epoch [2/2], Step [22240/64305], Loss: 4.8413\n",
      "Epoch [2/2], Step [22250/64305], Loss: 4.6640\n",
      "Epoch [2/2], Step [22260/64305], Loss: 4.9029\n",
      "Epoch [2/2], Step [22270/64305], Loss: 4.6666\n",
      "Epoch [2/2], Step [22280/64305], Loss: 4.7485\n",
      "Epoch [2/2], Step [22290/64305], Loss: 4.8877\n",
      "Epoch [2/2], Step [22300/64305], Loss: 4.6257\n",
      "Epoch [2/2], Step [22310/64305], Loss: 4.6577\n",
      "Epoch [2/2], Step [22320/64305], Loss: 4.8275\n",
      "Epoch [2/2], Step [22330/64305], Loss: 5.0761\n",
      "Epoch [2/2], Step [22340/64305], Loss: 5.2417\n",
      "Epoch [2/2], Step [22350/64305], Loss: 4.9153\n",
      "Epoch [2/2], Step [22360/64305], Loss: 4.7261\n",
      "Epoch [2/2], Step [22370/64305], Loss: 5.1034\n",
      "Epoch [2/2], Step [22380/64305], Loss: 4.7912\n",
      "Epoch [2/2], Step [22390/64305], Loss: 4.7030\n",
      "Epoch [2/2], Step [22400/64305], Loss: 4.7592\n",
      "Epoch [2/2], Step [22410/64305], Loss: 4.9051\n",
      "Epoch [2/2], Step [22420/64305], Loss: 4.7697\n",
      "Epoch [2/2], Step [22430/64305], Loss: 4.8654\n",
      "Epoch [2/2], Step [22440/64305], Loss: 4.8870\n",
      "Epoch [2/2], Step [22450/64305], Loss: 4.9401\n",
      "Epoch [2/2], Step [22460/64305], Loss: 5.0201\n",
      "Epoch [2/2], Step [22470/64305], Loss: 4.8608\n",
      "Epoch [2/2], Step [22480/64305], Loss: 4.8404\n",
      "Epoch [2/2], Step [22490/64305], Loss: 4.9698\n",
      "Epoch [2/2], Step [22500/64305], Loss: 4.9933\n",
      "Epoch [2/2], Step [22510/64305], Loss: 4.7998\n",
      "Epoch [2/2], Step [22520/64305], Loss: 4.9909\n",
      "Epoch [2/2], Step [22530/64305], Loss: 5.0479\n",
      "Epoch [2/2], Step [22540/64305], Loss: 4.8330\n",
      "Epoch [2/2], Step [22550/64305], Loss: 4.7217\n",
      "Epoch [2/2], Step [22560/64305], Loss: 5.0653\n",
      "Epoch [2/2], Step [22570/64305], Loss: 4.8274\n",
      "Epoch [2/2], Step [22580/64305], Loss: 5.0379\n",
      "Epoch [2/2], Step [22590/64305], Loss: 5.0502\n",
      "Epoch [2/2], Step [22600/64305], Loss: 4.9121\n",
      "Epoch [2/2], Step [22610/64305], Loss: 4.7065\n",
      "Epoch [2/2], Step [22620/64305], Loss: 4.7786\n",
      "Epoch [2/2], Step [22630/64305], Loss: 4.8932\n",
      "Epoch [2/2], Step [22640/64305], Loss: 4.8645\n",
      "Epoch [2/2], Step [22650/64305], Loss: 4.7014\n",
      "Epoch [2/2], Step [22660/64305], Loss: 4.8354\n",
      "Epoch [2/2], Step [22670/64305], Loss: 4.9494\n",
      "Epoch [2/2], Step [22680/64305], Loss: 4.7970\n",
      "Epoch [2/2], Step [22690/64305], Loss: 4.9127\n",
      "Epoch [2/2], Step [22700/64305], Loss: 4.9009\n",
      "Epoch [2/2], Step [22710/64305], Loss: 4.7696\n",
      "Epoch [2/2], Step [22720/64305], Loss: 4.9257\n",
      "Epoch [2/2], Step [22730/64305], Loss: 4.8258\n",
      "Epoch [2/2], Step [22740/64305], Loss: 4.8123\n",
      "Epoch [2/2], Step [22750/64305], Loss: 4.9452\n",
      "Epoch [2/2], Step [22760/64305], Loss: 4.8161\n",
      "Epoch [2/2], Step [22770/64305], Loss: 4.8268\n",
      "Epoch [2/2], Step [22780/64305], Loss: 4.9879\n",
      "Epoch [2/2], Step [22790/64305], Loss: 4.8927\n",
      "Epoch [2/2], Step [22800/64305], Loss: 4.7641\n",
      "Epoch [2/2], Step [22810/64305], Loss: 4.9816\n",
      "Epoch [2/2], Step [22820/64305], Loss: 4.7836\n",
      "Epoch [2/2], Step [22830/64305], Loss: 4.9571\n",
      "Epoch [2/2], Step [22840/64305], Loss: 5.1237\n",
      "Epoch [2/2], Step [22850/64305], Loss: 4.9612\n",
      "Epoch [2/2], Step [22860/64305], Loss: 4.7076\n",
      "Epoch [2/2], Step [22870/64305], Loss: 4.8725\n",
      "Epoch [2/2], Step [22880/64305], Loss: 4.9179\n",
      "Epoch [2/2], Step [22890/64305], Loss: 4.8184\n",
      "Epoch [2/2], Step [22900/64305], Loss: 4.7886\n",
      "Epoch [2/2], Step [22910/64305], Loss: 4.9286\n",
      "Epoch [2/2], Step [22920/64305], Loss: 4.8682\n",
      "Epoch [2/2], Step [22930/64305], Loss: 4.9328\n",
      "Epoch [2/2], Step [22940/64305], Loss: 4.9047\n",
      "Epoch [2/2], Step [22950/64305], Loss: 4.9285\n",
      "Epoch [2/2], Step [22960/64305], Loss: 4.6632\n",
      "Epoch [2/2], Step [22970/64305], Loss: 4.6711\n",
      "Epoch [2/2], Step [22980/64305], Loss: 4.9830\n",
      "Epoch [2/2], Step [22990/64305], Loss: 4.9853\n",
      "Epoch [2/2], Step [23000/64305], Loss: 4.8041\n",
      "Epoch [2/2], Step [23010/64305], Loss: 4.8099\n",
      "Epoch [2/2], Step [23020/64305], Loss: 4.8411\n",
      "Epoch [2/2], Step [23030/64305], Loss: 4.8649\n",
      "Epoch [2/2], Step [23040/64305], Loss: 4.7902\n",
      "Epoch [2/2], Step [23050/64305], Loss: 4.8522\n",
      "Epoch [2/2], Step [23060/64305], Loss: 5.1178\n",
      "Epoch [2/2], Step [23070/64305], Loss: 4.7531\n",
      "Epoch [2/2], Step [23080/64305], Loss: 4.8915\n",
      "Epoch [2/2], Step [23090/64305], Loss: 4.8562\n",
      "Epoch [2/2], Step [23100/64305], Loss: 4.8070\n",
      "Epoch [2/2], Step [23110/64305], Loss: 4.9283\n",
      "Epoch [2/2], Step [23120/64305], Loss: 4.9258\n",
      "Epoch [2/2], Step [23130/64305], Loss: 4.6912\n",
      "Epoch [2/2], Step [23140/64305], Loss: 4.7947\n",
      "Epoch [2/2], Step [23150/64305], Loss: 4.8673\n",
      "Epoch [2/2], Step [23160/64305], Loss: 5.1296\n",
      "Epoch [2/2], Step [23170/64305], Loss: 4.7951\n",
      "Epoch [2/2], Step [23180/64305], Loss: 4.9124\n",
      "Epoch [2/2], Step [23190/64305], Loss: 4.9587\n",
      "Epoch [2/2], Step [23200/64305], Loss: 4.8502\n",
      "Epoch [2/2], Step [23210/64305], Loss: 4.8218\n",
      "Epoch [2/2], Step [23220/64305], Loss: 4.7559\n",
      "Epoch [2/2], Step [23230/64305], Loss: 4.8220\n",
      "Epoch [2/2], Step [23240/64305], Loss: 4.8424\n",
      "Epoch [2/2], Step [23250/64305], Loss: 4.7724\n",
      "Epoch [2/2], Step [23260/64305], Loss: 4.9593\n",
      "Epoch [2/2], Step [23270/64305], Loss: 4.8892\n",
      "Epoch [2/2], Step [23280/64305], Loss: 4.8307\n",
      "Epoch [2/2], Step [23290/64305], Loss: 4.9092\n",
      "Epoch [2/2], Step [23300/64305], Loss: 4.8112\n",
      "Epoch [2/2], Step [23310/64305], Loss: 4.7601\n",
      "Epoch [2/2], Step [23320/64305], Loss: 4.8515\n",
      "Epoch [2/2], Step [23330/64305], Loss: 4.6291\n",
      "Epoch [2/2], Step [23340/64305], Loss: 4.7309\n",
      "Epoch [2/2], Step [23350/64305], Loss: 4.8430\n",
      "Epoch [2/2], Step [23360/64305], Loss: 4.9349\n",
      "Epoch [2/2], Step [23370/64305], Loss: 4.6929\n",
      "Epoch [2/2], Step [23380/64305], Loss: 4.9254\n",
      "Epoch [2/2], Step [23390/64305], Loss: 4.9757\n",
      "Epoch [2/2], Step [23400/64305], Loss: 4.7298\n",
      "Epoch [2/2], Step [23410/64305], Loss: 4.8344\n",
      "Epoch [2/2], Step [23420/64305], Loss: 4.8713\n",
      "Epoch [2/2], Step [23430/64305], Loss: 4.4950\n",
      "Epoch [2/2], Step [23440/64305], Loss: 4.8227\n",
      "Epoch [2/2], Step [23450/64305], Loss: 4.7854\n",
      "Epoch [2/2], Step [23460/64305], Loss: 4.7555\n",
      "Epoch [2/2], Step [23470/64305], Loss: 4.9262\n",
      "Epoch [2/2], Step [23480/64305], Loss: 5.0606\n",
      "Epoch [2/2], Step [23490/64305], Loss: 5.0062\n",
      "Epoch [2/2], Step [23500/64305], Loss: 4.7272\n",
      "Epoch [2/2], Step [23510/64305], Loss: 4.9216\n",
      "Epoch [2/2], Step [23520/64305], Loss: 4.6717\n",
      "Epoch [2/2], Step [23530/64305], Loss: 5.0744\n",
      "Epoch [2/2], Step [23540/64305], Loss: 5.0667\n",
      "Epoch [2/2], Step [23550/64305], Loss: 4.8365\n",
      "Epoch [2/2], Step [23560/64305], Loss: 4.8589\n",
      "Epoch [2/2], Step [23570/64305], Loss: 4.8081\n",
      "Epoch [2/2], Step [23580/64305], Loss: 4.7431\n",
      "Epoch [2/2], Step [23590/64305], Loss: 4.9058\n",
      "Epoch [2/2], Step [23600/64305], Loss: 4.9082\n",
      "Epoch [2/2], Step [23610/64305], Loss: 4.7952\n",
      "Epoch [2/2], Step [23620/64305], Loss: 4.6115\n",
      "Epoch [2/2], Step [23630/64305], Loss: 4.8132\n",
      "Epoch [2/2], Step [23640/64305], Loss: 4.7521\n",
      "Epoch [2/2], Step [23650/64305], Loss: 4.8055\n",
      "Epoch [2/2], Step [23660/64305], Loss: 4.7346\n",
      "Epoch [2/2], Step [23670/64305], Loss: 4.8268\n",
      "Epoch [2/2], Step [23680/64305], Loss: 4.8741\n",
      "Epoch [2/2], Step [23690/64305], Loss: 4.9449\n",
      "Epoch [2/2], Step [23700/64305], Loss: 4.9717\n",
      "Epoch [2/2], Step [23710/64305], Loss: 4.9562\n",
      "Epoch [2/2], Step [23720/64305], Loss: 4.6657\n",
      "Epoch [2/2], Step [23730/64305], Loss: 4.8337\n",
      "Epoch [2/2], Step [23740/64305], Loss: 5.0431\n",
      "Epoch [2/2], Step [23750/64305], Loss: 4.7557\n",
      "Epoch [2/2], Step [23760/64305], Loss: 4.7346\n",
      "Epoch [2/2], Step [23770/64305], Loss: 5.0020\n",
      "Epoch [2/2], Step [23780/64305], Loss: 4.8814\n",
      "Epoch [2/2], Step [23790/64305], Loss: 5.0081\n",
      "Epoch [2/2], Step [23800/64305], Loss: 4.8887\n",
      "Epoch [2/2], Step [23810/64305], Loss: 4.9498\n",
      "Epoch [2/2], Step [23820/64305], Loss: 4.8227\n",
      "Epoch [2/2], Step [23830/64305], Loss: 4.8054\n",
      "Epoch [2/2], Step [23840/64305], Loss: 4.8554\n",
      "Epoch [2/2], Step [23850/64305], Loss: 4.9176\n",
      "Epoch [2/2], Step [23860/64305], Loss: 4.9645\n",
      "Epoch [2/2], Step [23870/64305], Loss: 4.7835\n",
      "Epoch [2/2], Step [23880/64305], Loss: 4.8628\n",
      "Epoch [2/2], Step [23890/64305], Loss: 4.7643\n",
      "Epoch [2/2], Step [23900/64305], Loss: 5.0076\n",
      "Epoch [2/2], Step [23910/64305], Loss: 4.9543\n",
      "Epoch [2/2], Step [23920/64305], Loss: 4.6411\n",
      "Epoch [2/2], Step [23930/64305], Loss: 4.7919\n",
      "Epoch [2/2], Step [23940/64305], Loss: 4.7673\n",
      "Epoch [2/2], Step [23950/64305], Loss: 4.6161\n",
      "Epoch [2/2], Step [23960/64305], Loss: 4.8775\n",
      "Epoch [2/2], Step [23970/64305], Loss: 4.8520\n",
      "Epoch [2/2], Step [23980/64305], Loss: 4.9151\n",
      "Epoch [2/2], Step [23990/64305], Loss: 4.7899\n",
      "Epoch [2/2], Step [24000/64305], Loss: 4.8098\n",
      "Epoch [2/2], Step [24010/64305], Loss: 4.7900\n",
      "Epoch [2/2], Step [24020/64305], Loss: 4.9018\n",
      "Epoch [2/2], Step [24030/64305], Loss: 4.8323\n",
      "Epoch [2/2], Step [24040/64305], Loss: 4.7588\n",
      "Epoch [2/2], Step [24050/64305], Loss: 4.7646\n",
      "Epoch [2/2], Step [24060/64305], Loss: 4.9627\n",
      "Epoch [2/2], Step [24070/64305], Loss: 5.0629\n",
      "Epoch [2/2], Step [24080/64305], Loss: 4.7392\n",
      "Epoch [2/2], Step [24090/64305], Loss: 5.1109\n",
      "Epoch [2/2], Step [24100/64305], Loss: 4.7956\n",
      "Epoch [2/2], Step [24110/64305], Loss: 4.8783\n",
      "Epoch [2/2], Step [24120/64305], Loss: 5.0913\n",
      "Epoch [2/2], Step [24130/64305], Loss: 4.6647\n",
      "Epoch [2/2], Step [24140/64305], Loss: 5.0057\n",
      "Epoch [2/2], Step [24150/64305], Loss: 4.7560\n",
      "Epoch [2/2], Step [24160/64305], Loss: 4.9409\n",
      "Epoch [2/2], Step [24170/64305], Loss: 4.8719\n",
      "Epoch [2/2], Step [24180/64305], Loss: 4.8488\n",
      "Epoch [2/2], Step [24190/64305], Loss: 4.9431\n",
      "Epoch [2/2], Step [24200/64305], Loss: 4.6698\n",
      "Epoch [2/2], Step [24210/64305], Loss: 5.0984\n",
      "Epoch [2/2], Step [24220/64305], Loss: 4.9552\n",
      "Epoch [2/2], Step [24230/64305], Loss: 4.8120\n",
      "Epoch [2/2], Step [24240/64305], Loss: 4.9774\n",
      "Epoch [2/2], Step [24250/64305], Loss: 4.9246\n",
      "Epoch [2/2], Step [24260/64305], Loss: 4.8332\n",
      "Epoch [2/2], Step [24270/64305], Loss: 4.8778\n",
      "Epoch [2/2], Step [24280/64305], Loss: 5.1348\n",
      "Epoch [2/2], Step [24290/64305], Loss: 4.7441\n",
      "Epoch [2/2], Step [24300/64305], Loss: 4.8848\n",
      "Epoch [2/2], Step [24310/64305], Loss: 4.8074\n",
      "Epoch [2/2], Step [24320/64305], Loss: 4.9855\n",
      "Epoch [2/2], Step [24330/64305], Loss: 4.8671\n",
      "Epoch [2/2], Step [24340/64305], Loss: 4.8261\n",
      "Epoch [2/2], Step [24350/64305], Loss: 4.8394\n",
      "Epoch [2/2], Step [24360/64305], Loss: 4.7924\n",
      "Epoch [2/2], Step [24370/64305], Loss: 4.8564\n",
      "Epoch [2/2], Step [24380/64305], Loss: 4.6777\n",
      "Epoch [2/2], Step [24390/64305], Loss: 4.7987\n",
      "Epoch [2/2], Step [24400/64305], Loss: 4.8739\n",
      "Epoch [2/2], Step [24410/64305], Loss: 4.8565\n",
      "Epoch [2/2], Step [24420/64305], Loss: 4.7835\n",
      "Epoch [2/2], Step [24430/64305], Loss: 5.1137\n",
      "Epoch [2/2], Step [24440/64305], Loss: 4.6720\n",
      "Epoch [2/2], Step [24450/64305], Loss: 4.8062\n",
      "Epoch [2/2], Step [24460/64305], Loss: 4.6873\n",
      "Epoch [2/2], Step [24470/64305], Loss: 5.0349\n",
      "Epoch [2/2], Step [24480/64305], Loss: 4.9827\n",
      "Epoch [2/2], Step [24490/64305], Loss: 4.7904\n",
      "Epoch [2/2], Step [24500/64305], Loss: 4.6311\n",
      "Epoch [2/2], Step [24510/64305], Loss: 4.7676\n",
      "Epoch [2/2], Step [24520/64305], Loss: 4.7315\n",
      "Epoch [2/2], Step [24530/64305], Loss: 4.6607\n",
      "Epoch [2/2], Step [24540/64305], Loss: 4.7277\n",
      "Epoch [2/2], Step [24550/64305], Loss: 4.7762\n",
      "Epoch [2/2], Step [24560/64305], Loss: 4.7897\n",
      "Epoch [2/2], Step [24570/64305], Loss: 4.9293\n",
      "Epoch [2/2], Step [24580/64305], Loss: 5.0303\n",
      "Epoch [2/2], Step [24590/64305], Loss: 4.9301\n",
      "Epoch [2/2], Step [24600/64305], Loss: 4.7652\n",
      "Epoch [2/2], Step [24610/64305], Loss: 4.8326\n",
      "Epoch [2/2], Step [24620/64305], Loss: 4.8195\n",
      "Epoch [2/2], Step [24630/64305], Loss: 4.8114\n",
      "Epoch [2/2], Step [24640/64305], Loss: 4.8209\n",
      "Epoch [2/2], Step [24650/64305], Loss: 4.7357\n",
      "Epoch [2/2], Step [24660/64305], Loss: 4.8592\n",
      "Epoch [2/2], Step [24670/64305], Loss: 4.7422\n",
      "Epoch [2/2], Step [24680/64305], Loss: 4.7084\n",
      "Epoch [2/2], Step [24690/64305], Loss: 4.8368\n",
      "Epoch [2/2], Step [24700/64305], Loss: 4.8923\n",
      "Epoch [2/2], Step [24710/64305], Loss: 4.7623\n",
      "Epoch [2/2], Step [24720/64305], Loss: 4.7974\n",
      "Epoch [2/2], Step [24730/64305], Loss: 5.0043\n",
      "Epoch [2/2], Step [24740/64305], Loss: 5.0432\n",
      "Epoch [2/2], Step [24750/64305], Loss: 4.8363\n",
      "Epoch [2/2], Step [24760/64305], Loss: 4.9352\n",
      "Epoch [2/2], Step [24770/64305], Loss: 4.8326\n",
      "Epoch [2/2], Step [24780/64305], Loss: 4.8261\n",
      "Epoch [2/2], Step [24790/64305], Loss: 4.8568\n",
      "Epoch [2/2], Step [24800/64305], Loss: 4.9044\n",
      "Epoch [2/2], Step [24810/64305], Loss: 4.7524\n",
      "Epoch [2/2], Step [24820/64305], Loss: 4.8849\n",
      "Epoch [2/2], Step [24830/64305], Loss: 4.7142\n",
      "Epoch [2/2], Step [24840/64305], Loss: 4.5611\n",
      "Epoch [2/2], Step [24850/64305], Loss: 4.6398\n",
      "Epoch [2/2], Step [24860/64305], Loss: 4.9814\n",
      "Epoch [2/2], Step [24870/64305], Loss: 4.9679\n",
      "Epoch [2/2], Step [24880/64305], Loss: 4.8798\n",
      "Epoch [2/2], Step [24890/64305], Loss: 4.9452\n",
      "Epoch [2/2], Step [24900/64305], Loss: 4.7076\n",
      "Epoch [2/2], Step [24910/64305], Loss: 4.9484\n",
      "Epoch [2/2], Step [24920/64305], Loss: 4.8382\n",
      "Epoch [2/2], Step [24930/64305], Loss: 4.7599\n",
      "Epoch [2/2], Step [24940/64305], Loss: 4.9329\n",
      "Epoch [2/2], Step [24950/64305], Loss: 4.6234\n",
      "Epoch [2/2], Step [24960/64305], Loss: 4.7461\n",
      "Epoch [2/2], Step [24970/64305], Loss: 4.7845\n",
      "Epoch [2/2], Step [24980/64305], Loss: 4.6509\n",
      "Epoch [2/2], Step [24990/64305], Loss: 4.9306\n",
      "Epoch [2/2], Step [25000/64305], Loss: 4.8611\n",
      "Epoch [2/2], Step [25010/64305], Loss: 4.7396\n",
      "Epoch [2/2], Step [25020/64305], Loss: 4.7214\n",
      "Epoch [2/2], Step [25030/64305], Loss: 4.8752\n",
      "Epoch [2/2], Step [25040/64305], Loss: 4.9416\n",
      "Epoch [2/2], Step [25050/64305], Loss: 5.0037\n",
      "Epoch [2/2], Step [25060/64305], Loss: 4.9137\n",
      "Epoch [2/2], Step [25070/64305], Loss: 4.5713\n",
      "Epoch [2/2], Step [25080/64305], Loss: 4.6383\n",
      "Epoch [2/2], Step [25090/64305], Loss: 4.5825\n",
      "Epoch [2/2], Step [25100/64305], Loss: 4.7722\n",
      "Epoch [2/2], Step [25110/64305], Loss: 4.8122\n",
      "Epoch [2/2], Step [25120/64305], Loss: 5.1452\n",
      "Epoch [2/2], Step [25130/64305], Loss: 4.7012\n",
      "Epoch [2/2], Step [25140/64305], Loss: 4.7135\n",
      "Epoch [2/2], Step [25150/64305], Loss: 4.6995\n",
      "Epoch [2/2], Step [25160/64305], Loss: 4.8514\n",
      "Epoch [2/2], Step [25170/64305], Loss: 4.9093\n",
      "Epoch [2/2], Step [25180/64305], Loss: 5.0211\n",
      "Epoch [2/2], Step [25190/64305], Loss: 4.7951\n",
      "Epoch [2/2], Step [25200/64305], Loss: 4.7254\n",
      "Epoch [2/2], Step [25210/64305], Loss: 4.8901\n",
      "Epoch [2/2], Step [25220/64305], Loss: 4.9077\n",
      "Epoch [2/2], Step [25230/64305], Loss: 4.7657\n",
      "Epoch [2/2], Step [25240/64305], Loss: 4.9267\n",
      "Epoch [2/2], Step [25250/64305], Loss: 4.8761\n",
      "Epoch [2/2], Step [25260/64305], Loss: 4.6871\n",
      "Epoch [2/2], Step [25270/64305], Loss: 4.8586\n",
      "Epoch [2/2], Step [25280/64305], Loss: 5.1241\n",
      "Epoch [2/2], Step [25290/64305], Loss: 4.8236\n",
      "Epoch [2/2], Step [25300/64305], Loss: 4.7274\n",
      "Epoch [2/2], Step [25310/64305], Loss: 4.9074\n",
      "Epoch [2/2], Step [25320/64305], Loss: 4.7391\n",
      "Epoch [2/2], Step [25330/64305], Loss: 4.6503\n",
      "Epoch [2/2], Step [25340/64305], Loss: 4.7947\n",
      "Epoch [2/2], Step [25350/64305], Loss: 4.7046\n",
      "Epoch [2/2], Step [25360/64305], Loss: 4.7188\n",
      "Epoch [2/2], Step [25370/64305], Loss: 5.0706\n",
      "Epoch [2/2], Step [25380/64305], Loss: 4.8074\n",
      "Epoch [2/2], Step [25390/64305], Loss: 4.9057\n",
      "Epoch [2/2], Step [25400/64305], Loss: 4.6594\n",
      "Epoch [2/2], Step [25410/64305], Loss: 4.9100\n",
      "Epoch [2/2], Step [25420/64305], Loss: 4.7808\n",
      "Epoch [2/2], Step [25430/64305], Loss: 4.8725\n",
      "Epoch [2/2], Step [25440/64305], Loss: 4.8259\n",
      "Epoch [2/2], Step [25450/64305], Loss: 4.7708\n",
      "Epoch [2/2], Step [25460/64305], Loss: 4.9087\n",
      "Epoch [2/2], Step [25470/64305], Loss: 4.7679\n",
      "Epoch [2/2], Step [25480/64305], Loss: 4.7321\n",
      "Epoch [2/2], Step [25490/64305], Loss: 4.7916\n",
      "Epoch [2/2], Step [25500/64305], Loss: 5.0243\n",
      "Epoch [2/2], Step [25510/64305], Loss: 4.8721\n",
      "Epoch [2/2], Step [25520/64305], Loss: 4.8993\n",
      "Epoch [2/2], Step [25530/64305], Loss: 4.7776\n",
      "Epoch [2/2], Step [25540/64305], Loss: 4.7484\n",
      "Epoch [2/2], Step [25550/64305], Loss: 4.7353\n",
      "Epoch [2/2], Step [25560/64305], Loss: 4.9451\n",
      "Epoch [2/2], Step [25570/64305], Loss: 4.8458\n",
      "Epoch [2/2], Step [25580/64305], Loss: 4.8701\n",
      "Epoch [2/2], Step [25590/64305], Loss: 4.8479\n",
      "Epoch [2/2], Step [25600/64305], Loss: 5.0223\n",
      "Epoch [2/2], Step [25610/64305], Loss: 4.8523\n",
      "Epoch [2/2], Step [25620/64305], Loss: 4.9922\n",
      "Epoch [2/2], Step [25630/64305], Loss: 4.6288\n",
      "Epoch [2/2], Step [25640/64305], Loss: 4.6358\n",
      "Epoch [2/2], Step [25650/64305], Loss: 4.7886\n",
      "Epoch [2/2], Step [25660/64305], Loss: 4.9137\n",
      "Epoch [2/2], Step [25670/64305], Loss: 4.7308\n",
      "Epoch [2/2], Step [25680/64305], Loss: 4.6391\n",
      "Epoch [2/2], Step [25690/64305], Loss: 5.0627\n",
      "Epoch [2/2], Step [25700/64305], Loss: 4.9159\n",
      "Epoch [2/2], Step [25710/64305], Loss: 4.8228\n",
      "Epoch [2/2], Step [25720/64305], Loss: 4.8775\n",
      "Epoch [2/2], Step [25730/64305], Loss: 4.6789\n",
      "Epoch [2/2], Step [25740/64305], Loss: 4.7791\n",
      "Epoch [2/2], Step [25750/64305], Loss: 4.8310\n",
      "Epoch [2/2], Step [25760/64305], Loss: 4.7688\n",
      "Epoch [2/2], Step [25770/64305], Loss: 4.6246\n",
      "Epoch [2/2], Step [25780/64305], Loss: 4.7976\n",
      "Epoch [2/2], Step [25790/64305], Loss: 4.9235\n",
      "Epoch [2/2], Step [25800/64305], Loss: 4.8115\n",
      "Epoch [2/2], Step [25810/64305], Loss: 4.7559\n",
      "Epoch [2/2], Step [25820/64305], Loss: 4.9034\n",
      "Epoch [2/2], Step [25830/64305], Loss: 4.7395\n",
      "Epoch [2/2], Step [25840/64305], Loss: 4.7504\n",
      "Epoch [2/2], Step [25850/64305], Loss: 4.9747\n",
      "Epoch [2/2], Step [25860/64305], Loss: 4.8179\n",
      "Epoch [2/2], Step [25870/64305], Loss: 4.7526\n",
      "Epoch [2/2], Step [25880/64305], Loss: 4.8699\n",
      "Epoch [2/2], Step [25890/64305], Loss: 4.8640\n",
      "Epoch [2/2], Step [25900/64305], Loss: 4.9204\n",
      "Epoch [2/2], Step [25910/64305], Loss: 4.7868\n",
      "Epoch [2/2], Step [25920/64305], Loss: 4.9276\n",
      "Epoch [2/2], Step [25930/64305], Loss: 5.0059\n",
      "Epoch [2/2], Step [25940/64305], Loss: 4.8871\n",
      "Epoch [2/2], Step [25950/64305], Loss: 4.8188\n",
      "Epoch [2/2], Step [25960/64305], Loss: 4.7372\n",
      "Epoch [2/2], Step [25970/64305], Loss: 4.7787\n",
      "Epoch [2/2], Step [25980/64305], Loss: 5.0213\n",
      "Epoch [2/2], Step [25990/64305], Loss: 4.6363\n",
      "Epoch [2/2], Step [26000/64305], Loss: 4.8580\n",
      "Epoch [2/2], Step [26010/64305], Loss: 4.7971\n",
      "Epoch [2/2], Step [26020/64305], Loss: 4.9545\n",
      "Epoch [2/2], Step [26030/64305], Loss: 4.8713\n",
      "Epoch [2/2], Step [26040/64305], Loss: 5.0946\n",
      "Epoch [2/2], Step [26050/64305], Loss: 4.7973\n",
      "Epoch [2/2], Step [26060/64305], Loss: 4.9237\n",
      "Epoch [2/2], Step [26070/64305], Loss: 4.5811\n",
      "Epoch [2/2], Step [26080/64305], Loss: 4.8547\n",
      "Epoch [2/2], Step [26090/64305], Loss: 4.8540\n",
      "Epoch [2/2], Step [26100/64305], Loss: 4.6489\n",
      "Epoch [2/2], Step [26110/64305], Loss: 4.8213\n",
      "Epoch [2/2], Step [26120/64305], Loss: 4.9483\n",
      "Epoch [2/2], Step [26130/64305], Loss: 4.7918\n",
      "Epoch [2/2], Step [26140/64305], Loss: 5.0691\n",
      "Epoch [2/2], Step [26150/64305], Loss: 5.0983\n",
      "Epoch [2/2], Step [26160/64305], Loss: 4.8942\n",
      "Epoch [2/2], Step [26170/64305], Loss: 4.7253\n",
      "Epoch [2/2], Step [26180/64305], Loss: 5.0484\n",
      "Epoch [2/2], Step [26190/64305], Loss: 4.8461\n",
      "Epoch [2/2], Step [26200/64305], Loss: 4.7912\n",
      "Epoch [2/2], Step [26210/64305], Loss: 5.0919\n",
      "Epoch [2/2], Step [26220/64305], Loss: 4.7011\n",
      "Epoch [2/2], Step [26230/64305], Loss: 4.7910\n",
      "Epoch [2/2], Step [26240/64305], Loss: 4.7864\n",
      "Epoch [2/2], Step [26250/64305], Loss: 4.8102\n",
      "Epoch [2/2], Step [26260/64305], Loss: 4.9339\n",
      "Epoch [2/2], Step [26270/64305], Loss: 4.9708\n",
      "Epoch [2/2], Step [26280/64305], Loss: 4.8744\n",
      "Epoch [2/2], Step [26290/64305], Loss: 4.7998\n",
      "Epoch [2/2], Step [26300/64305], Loss: 4.8791\n",
      "Epoch [2/2], Step [26310/64305], Loss: 4.9051\n",
      "Epoch [2/2], Step [26320/64305], Loss: 4.7314\n",
      "Epoch [2/2], Step [26330/64305], Loss: 4.6353\n",
      "Epoch [2/2], Step [26340/64305], Loss: 4.5419\n",
      "Epoch [2/2], Step [26350/64305], Loss: 4.9736\n",
      "Epoch [2/2], Step [26360/64305], Loss: 4.9245\n",
      "Epoch [2/2], Step [26370/64305], Loss: 5.0170\n",
      "Epoch [2/2], Step [26380/64305], Loss: 4.9941\n",
      "Epoch [2/2], Step [26390/64305], Loss: 4.6914\n",
      "Epoch [2/2], Step [26400/64305], Loss: 4.8188\n",
      "Epoch [2/2], Step [26410/64305], Loss: 4.6434\n",
      "Epoch [2/2], Step [26420/64305], Loss: 4.7664\n",
      "Epoch [2/2], Step [26430/64305], Loss: 4.6492\n",
      "Epoch [2/2], Step [26440/64305], Loss: 5.0640\n",
      "Epoch [2/2], Step [26450/64305], Loss: 4.8964\n",
      "Epoch [2/2], Step [26460/64305], Loss: 4.7092\n",
      "Epoch [2/2], Step [26470/64305], Loss: 4.8550\n",
      "Epoch [2/2], Step [26480/64305], Loss: 5.0248\n",
      "Epoch [2/2], Step [26490/64305], Loss: 4.9047\n",
      "Epoch [2/2], Step [26500/64305], Loss: 4.5832\n",
      "Epoch [2/2], Step [26510/64305], Loss: 4.9143\n",
      "Epoch [2/2], Step [26520/64305], Loss: 4.8156\n",
      "Epoch [2/2], Step [26530/64305], Loss: 4.8406\n",
      "Epoch [2/2], Step [26540/64305], Loss: 4.9817\n",
      "Epoch [2/2], Step [26550/64305], Loss: 4.9107\n",
      "Epoch [2/2], Step [26560/64305], Loss: 4.9619\n",
      "Epoch [2/2], Step [26570/64305], Loss: 4.7643\n",
      "Epoch [2/2], Step [26580/64305], Loss: 4.8298\n",
      "Epoch [2/2], Step [26590/64305], Loss: 4.8153\n",
      "Epoch [2/2], Step [26600/64305], Loss: 5.1231\n",
      "Epoch [2/2], Step [26610/64305], Loss: 5.1073\n",
      "Epoch [2/2], Step [26620/64305], Loss: 4.9590\n",
      "Epoch [2/2], Step [26630/64305], Loss: 4.7066\n",
      "Epoch [2/2], Step [26640/64305], Loss: 4.7729\n",
      "Epoch [2/2], Step [26650/64305], Loss: 4.9505\n",
      "Epoch [2/2], Step [26660/64305], Loss: 4.6843\n",
      "Epoch [2/2], Step [26670/64305], Loss: 4.7783\n",
      "Epoch [2/2], Step [26680/64305], Loss: 4.6984\n",
      "Epoch [2/2], Step [26690/64305], Loss: 4.9364\n",
      "Epoch [2/2], Step [26700/64305], Loss: 4.7768\n",
      "Epoch [2/2], Step [26710/64305], Loss: 4.6398\n",
      "Epoch [2/2], Step [26720/64305], Loss: 4.9110\n",
      "Epoch [2/2], Step [26730/64305], Loss: 4.9002\n",
      "Epoch [2/2], Step [26740/64305], Loss: 4.9502\n",
      "Epoch [2/2], Step [26750/64305], Loss: 4.9296\n",
      "Epoch [2/2], Step [26760/64305], Loss: 4.9696\n",
      "Epoch [2/2], Step [26770/64305], Loss: 4.8142\n",
      "Epoch [2/2], Step [26780/64305], Loss: 4.8681\n",
      "Epoch [2/2], Step [26790/64305], Loss: 4.7824\n",
      "Epoch [2/2], Step [26800/64305], Loss: 4.8108\n",
      "Epoch [2/2], Step [26810/64305], Loss: 4.8266\n",
      "Epoch [2/2], Step [26820/64305], Loss: 4.9375\n",
      "Epoch [2/2], Step [26830/64305], Loss: 4.6937\n",
      "Epoch [2/2], Step [26840/64305], Loss: 4.7747\n",
      "Epoch [2/2], Step [26850/64305], Loss: 4.9555\n",
      "Epoch [2/2], Step [26860/64305], Loss: 4.8556\n",
      "Epoch [2/2], Step [26870/64305], Loss: 5.0303\n",
      "Epoch [2/2], Step [26880/64305], Loss: 4.6770\n",
      "Epoch [2/2], Step [26890/64305], Loss: 4.8778\n",
      "Epoch [2/2], Step [26900/64305], Loss: 4.8787\n",
      "Epoch [2/2], Step [26910/64305], Loss: 4.8368\n",
      "Epoch [2/2], Step [26920/64305], Loss: 4.9224\n",
      "Epoch [2/2], Step [26930/64305], Loss: 4.8427\n",
      "Epoch [2/2], Step [26940/64305], Loss: 4.9468\n",
      "Epoch [2/2], Step [26950/64305], Loss: 4.8854\n",
      "Epoch [2/2], Step [26960/64305], Loss: 4.5799\n",
      "Epoch [2/2], Step [26970/64305], Loss: 4.7491\n",
      "Epoch [2/2], Step [26980/64305], Loss: 4.8388\n",
      "Epoch [2/2], Step [26990/64305], Loss: 4.7243\n",
      "Epoch [2/2], Step [27000/64305], Loss: 4.7847\n",
      "Epoch [2/2], Step [27010/64305], Loss: 4.9063\n",
      "Epoch [2/2], Step [27020/64305], Loss: 4.8499\n",
      "Epoch [2/2], Step [27030/64305], Loss: 4.8063\n",
      "Epoch [2/2], Step [27040/64305], Loss: 4.7874\n",
      "Epoch [2/2], Step [27050/64305], Loss: 4.7402\n",
      "Epoch [2/2], Step [27060/64305], Loss: 4.8157\n",
      "Epoch [2/2], Step [27070/64305], Loss: 4.9479\n",
      "Epoch [2/2], Step [27080/64305], Loss: 4.8042\n",
      "Epoch [2/2], Step [27090/64305], Loss: 4.8784\n",
      "Epoch [2/2], Step [27100/64305], Loss: 4.9553\n",
      "Epoch [2/2], Step [27110/64305], Loss: 5.1102\n",
      "Epoch [2/2], Step [27120/64305], Loss: 4.7741\n",
      "Epoch [2/2], Step [27130/64305], Loss: 4.9209\n",
      "Epoch [2/2], Step [27140/64305], Loss: 4.8884\n",
      "Epoch [2/2], Step [27150/64305], Loss: 4.7387\n",
      "Epoch [2/2], Step [27160/64305], Loss: 4.9930\n",
      "Epoch [2/2], Step [27170/64305], Loss: 4.7257\n",
      "Epoch [2/2], Step [27180/64305], Loss: 4.8628\n",
      "Epoch [2/2], Step [27190/64305], Loss: 4.9782\n",
      "Epoch [2/2], Step [27200/64305], Loss: 4.9172\n",
      "Epoch [2/2], Step [27210/64305], Loss: 5.0989\n",
      "Epoch [2/2], Step [27220/64305], Loss: 4.9250\n",
      "Epoch [2/2], Step [27230/64305], Loss: 4.7851\n",
      "Epoch [2/2], Step [27240/64305], Loss: 4.7889\n",
      "Epoch [2/2], Step [27250/64305], Loss: 5.0251\n",
      "Epoch [2/2], Step [27260/64305], Loss: 4.8699\n",
      "Epoch [2/2], Step [27270/64305], Loss: 4.7926\n",
      "Epoch [2/2], Step [27280/64305], Loss: 4.8538\n",
      "Epoch [2/2], Step [27290/64305], Loss: 4.8218\n",
      "Epoch [2/2], Step [27300/64305], Loss: 4.9771\n",
      "Epoch [2/2], Step [27310/64305], Loss: 5.0249\n",
      "Epoch [2/2], Step [27320/64305], Loss: 4.8100\n",
      "Epoch [2/2], Step [27330/64305], Loss: 4.8314\n",
      "Epoch [2/2], Step [27340/64305], Loss: 4.8859\n",
      "Epoch [2/2], Step [27350/64305], Loss: 4.9444\n",
      "Epoch [2/2], Step [27360/64305], Loss: 4.7041\n",
      "Epoch [2/2], Step [27370/64305], Loss: 4.8591\n",
      "Epoch [2/2], Step [27380/64305], Loss: 4.8241\n",
      "Epoch [2/2], Step [27390/64305], Loss: 4.7322\n",
      "Epoch [2/2], Step [27400/64305], Loss: 4.9017\n",
      "Epoch [2/2], Step [27410/64305], Loss: 4.7937\n",
      "Epoch [2/2], Step [27420/64305], Loss: 4.7867\n",
      "Epoch [2/2], Step [27430/64305], Loss: 4.8722\n",
      "Epoch [2/2], Step [27440/64305], Loss: 4.8925\n",
      "Epoch [2/2], Step [27450/64305], Loss: 4.7986\n",
      "Epoch [2/2], Step [27460/64305], Loss: 4.8482\n",
      "Epoch [2/2], Step [27470/64305], Loss: 4.9168\n",
      "Epoch [2/2], Step [27480/64305], Loss: 4.8573\n",
      "Epoch [2/2], Step [27490/64305], Loss: 4.8913\n",
      "Epoch [2/2], Step [27500/64305], Loss: 4.7899\n",
      "Epoch [2/2], Step [27510/64305], Loss: 4.6531\n",
      "Epoch [2/2], Step [27520/64305], Loss: 4.7527\n",
      "Epoch [2/2], Step [27530/64305], Loss: 4.8196\n",
      "Epoch [2/2], Step [27540/64305], Loss: 4.8227\n",
      "Epoch [2/2], Step [27550/64305], Loss: 4.7475\n",
      "Epoch [2/2], Step [27560/64305], Loss: 4.8370\n",
      "Epoch [2/2], Step [27570/64305], Loss: 4.8601\n",
      "Epoch [2/2], Step [27580/64305], Loss: 4.8780\n",
      "Epoch [2/2], Step [27590/64305], Loss: 4.9756\n",
      "Epoch [2/2], Step [27600/64305], Loss: 4.7147\n",
      "Epoch [2/2], Step [27610/64305], Loss: 4.7293\n",
      "Epoch [2/2], Step [27620/64305], Loss: 4.6216\n",
      "Epoch [2/2], Step [27630/64305], Loss: 4.7802\n",
      "Epoch [2/2], Step [27640/64305], Loss: 4.9212\n",
      "Epoch [2/2], Step [27650/64305], Loss: 4.8438\n",
      "Epoch [2/2], Step [27660/64305], Loss: 4.8173\n",
      "Epoch [2/2], Step [27670/64305], Loss: 4.7493\n",
      "Epoch [2/2], Step [27680/64305], Loss: 4.9278\n",
      "Epoch [2/2], Step [27690/64305], Loss: 4.7947\n",
      "Epoch [2/2], Step [27700/64305], Loss: 4.8163\n",
      "Epoch [2/2], Step [27710/64305], Loss: 4.9551\n",
      "Epoch [2/2], Step [27720/64305], Loss: 4.9415\n",
      "Epoch [2/2], Step [27730/64305], Loss: 4.7919\n",
      "Epoch [2/2], Step [27740/64305], Loss: 4.8287\n",
      "Epoch [2/2], Step [27750/64305], Loss: 4.8076\n",
      "Epoch [2/2], Step [27760/64305], Loss: 4.7517\n",
      "Epoch [2/2], Step [27770/64305], Loss: 4.7952\n",
      "Epoch [2/2], Step [27780/64305], Loss: 4.7858\n",
      "Epoch [2/2], Step [27790/64305], Loss: 4.8473\n",
      "Epoch [2/2], Step [27800/64305], Loss: 4.9235\n",
      "Epoch [2/2], Step [27810/64305], Loss: 4.8659\n",
      "Epoch [2/2], Step [27820/64305], Loss: 4.6477\n",
      "Epoch [2/2], Step [27830/64305], Loss: 4.8048\n",
      "Epoch [2/2], Step [27840/64305], Loss: 4.8153\n",
      "Epoch [2/2], Step [27850/64305], Loss: 4.8475\n",
      "Epoch [2/2], Step [27860/64305], Loss: 4.8933\n",
      "Epoch [2/2], Step [27870/64305], Loss: 4.9542\n",
      "Epoch [2/2], Step [27880/64305], Loss: 4.9285\n",
      "Epoch [2/2], Step [27890/64305], Loss: 4.5762\n",
      "Epoch [2/2], Step [27900/64305], Loss: 4.8101\n",
      "Epoch [2/2], Step [27910/64305], Loss: 4.8828\n",
      "Epoch [2/2], Step [27920/64305], Loss: 4.8338\n",
      "Epoch [2/2], Step [27930/64305], Loss: 4.7464\n",
      "Epoch [2/2], Step [27940/64305], Loss: 4.9296\n",
      "Epoch [2/2], Step [27950/64305], Loss: 4.9038\n",
      "Epoch [2/2], Step [27960/64305], Loss: 4.8878\n",
      "Epoch [2/2], Step [27970/64305], Loss: 5.1261\n",
      "Epoch [2/2], Step [27980/64305], Loss: 4.7870\n",
      "Epoch [2/2], Step [27990/64305], Loss: 4.8514\n",
      "Epoch [2/2], Step [28000/64305], Loss: 4.9055\n",
      "Epoch [2/2], Step [28010/64305], Loss: 4.8499\n",
      "Epoch [2/2], Step [28020/64305], Loss: 4.9052\n",
      "Epoch [2/2], Step [28030/64305], Loss: 4.9848\n",
      "Epoch [2/2], Step [28040/64305], Loss: 4.8211\n",
      "Epoch [2/2], Step [28050/64305], Loss: 4.7410\n",
      "Epoch [2/2], Step [28060/64305], Loss: 5.0462\n",
      "Epoch [2/2], Step [28070/64305], Loss: 4.7639\n",
      "Epoch [2/2], Step [28080/64305], Loss: 5.1387\n",
      "Epoch [2/2], Step [28090/64305], Loss: 4.8100\n",
      "Epoch [2/2], Step [28100/64305], Loss: 5.1516\n",
      "Epoch [2/2], Step [28110/64305], Loss: 5.0470\n",
      "Epoch [2/2], Step [28120/64305], Loss: 4.8916\n",
      "Epoch [2/2], Step [28130/64305], Loss: 4.7336\n",
      "Epoch [2/2], Step [28140/64305], Loss: 5.0719\n",
      "Epoch [2/2], Step [28150/64305], Loss: 4.8421\n",
      "Epoch [2/2], Step [28160/64305], Loss: 4.7753\n",
      "Epoch [2/2], Step [28170/64305], Loss: 4.8346\n",
      "Epoch [2/2], Step [28180/64305], Loss: 4.9404\n",
      "Epoch [2/2], Step [28190/64305], Loss: 4.8522\n",
      "Epoch [2/2], Step [28200/64305], Loss: 4.8946\n",
      "Epoch [2/2], Step [28210/64305], Loss: 4.8424\n",
      "Epoch [2/2], Step [28220/64305], Loss: 4.8402\n",
      "Epoch [2/2], Step [28230/64305], Loss: 4.7419\n",
      "Epoch [2/2], Step [28240/64305], Loss: 4.8287\n",
      "Epoch [2/2], Step [28250/64305], Loss: 4.8786\n",
      "Epoch [2/2], Step [28260/64305], Loss: 4.6890\n",
      "Epoch [2/2], Step [28270/64305], Loss: 4.9587\n",
      "Epoch [2/2], Step [28280/64305], Loss: 5.1496\n",
      "Epoch [2/2], Step [28290/64305], Loss: 4.8956\n",
      "Epoch [2/2], Step [28300/64305], Loss: 4.8122\n",
      "Epoch [2/2], Step [28310/64305], Loss: 4.8475\n",
      "Epoch [2/2], Step [28320/64305], Loss: 4.6849\n",
      "Epoch [2/2], Step [28330/64305], Loss: 4.9946\n",
      "Epoch [2/2], Step [28340/64305], Loss: 4.8846\n",
      "Epoch [2/2], Step [28350/64305], Loss: 4.7023\n",
      "Epoch [2/2], Step [28360/64305], Loss: 4.7320\n",
      "Epoch [2/2], Step [28370/64305], Loss: 4.7066\n",
      "Epoch [2/2], Step [28380/64305], Loss: 4.9935\n",
      "Epoch [2/2], Step [28390/64305], Loss: 4.7977\n",
      "Epoch [2/2], Step [28400/64305], Loss: 5.0025\n",
      "Epoch [2/2], Step [28410/64305], Loss: 4.8282\n",
      "Epoch [2/2], Step [28420/64305], Loss: 4.8445\n",
      "Epoch [2/2], Step [28430/64305], Loss: 5.0593\n",
      "Epoch [2/2], Step [28440/64305], Loss: 4.7868\n",
      "Epoch [2/2], Step [28450/64305], Loss: 4.8550\n",
      "Epoch [2/2], Step [28460/64305], Loss: 4.9046\n",
      "Epoch [2/2], Step [28470/64305], Loss: 4.9359\n",
      "Epoch [2/2], Step [28480/64305], Loss: 4.9648\n",
      "Epoch [2/2], Step [28490/64305], Loss: 4.8702\n",
      "Epoch [2/2], Step [28500/64305], Loss: 4.8106\n",
      "Epoch [2/2], Step [28510/64305], Loss: 4.9582\n",
      "Epoch [2/2], Step [28520/64305], Loss: 4.8138\n",
      "Epoch [2/2], Step [28530/64305], Loss: 4.9342\n",
      "Epoch [2/2], Step [28540/64305], Loss: 4.8741\n",
      "Epoch [2/2], Step [28550/64305], Loss: 5.1530\n",
      "Epoch [2/2], Step [28560/64305], Loss: 4.7831\n",
      "Epoch [2/2], Step [28570/64305], Loss: 4.8521\n",
      "Epoch [2/2], Step [28580/64305], Loss: 4.8811\n",
      "Epoch [2/2], Step [28590/64305], Loss: 4.8565\n",
      "Epoch [2/2], Step [28600/64305], Loss: 4.9217\n",
      "Epoch [2/2], Step [28610/64305], Loss: 5.0538\n",
      "Epoch [2/2], Step [28620/64305], Loss: 4.8409\n",
      "Epoch [2/2], Step [28630/64305], Loss: 4.7580\n",
      "Epoch [2/2], Step [28640/64305], Loss: 4.7097\n",
      "Epoch [2/2], Step [28650/64305], Loss: 4.8156\n",
      "Epoch [2/2], Step [28660/64305], Loss: 4.8633\n",
      "Epoch [2/2], Step [28670/64305], Loss: 4.7901\n",
      "Epoch [2/2], Step [28680/64305], Loss: 4.7269\n",
      "Epoch [2/2], Step [28690/64305], Loss: 4.8298\n",
      "Epoch [2/2], Step [28700/64305], Loss: 4.6145\n",
      "Epoch [2/2], Step [28710/64305], Loss: 4.7842\n",
      "Epoch [2/2], Step [28720/64305], Loss: 4.8274\n",
      "Epoch [2/2], Step [28730/64305], Loss: 4.9923\n",
      "Epoch [2/2], Step [28740/64305], Loss: 4.9759\n",
      "Epoch [2/2], Step [28750/64305], Loss: 4.8873\n",
      "Epoch [2/2], Step [28760/64305], Loss: 4.8421\n",
      "Epoch [2/2], Step [28770/64305], Loss: 4.8791\n",
      "Epoch [2/2], Step [28780/64305], Loss: 4.8987\n",
      "Epoch [2/2], Step [28790/64305], Loss: 5.0505\n",
      "Epoch [2/2], Step [28800/64305], Loss: 4.9242\n",
      "Epoch [2/2], Step [28810/64305], Loss: 4.8825\n",
      "Epoch [2/2], Step [28820/64305], Loss: 4.7180\n",
      "Epoch [2/2], Step [28830/64305], Loss: 4.8313\n",
      "Epoch [2/2], Step [28840/64305], Loss: 4.9315\n",
      "Epoch [2/2], Step [28850/64305], Loss: 4.9400\n",
      "Epoch [2/2], Step [28860/64305], Loss: 4.8306\n",
      "Epoch [2/2], Step [28870/64305], Loss: 4.7672\n",
      "Epoch [2/2], Step [28880/64305], Loss: 4.8336\n",
      "Epoch [2/2], Step [28890/64305], Loss: 4.8681\n",
      "Epoch [2/2], Step [28900/64305], Loss: 4.9733\n",
      "Epoch [2/2], Step [28910/64305], Loss: 4.7643\n",
      "Epoch [2/2], Step [28920/64305], Loss: 4.9618\n",
      "Epoch [2/2], Step [28930/64305], Loss: 4.9902\n",
      "Epoch [2/2], Step [28940/64305], Loss: 4.9091\n",
      "Epoch [2/2], Step [28950/64305], Loss: 4.7310\n",
      "Epoch [2/2], Step [28960/64305], Loss: 4.8156\n",
      "Epoch [2/2], Step [28970/64305], Loss: 4.9177\n",
      "Epoch [2/2], Step [28980/64305], Loss: 4.7985\n",
      "Epoch [2/2], Step [28990/64305], Loss: 4.7653\n",
      "Epoch [2/2], Step [29000/64305], Loss: 4.8462\n",
      "Epoch [2/2], Step [29010/64305], Loss: 4.8939\n",
      "Epoch [2/2], Step [29020/64305], Loss: 4.7182\n",
      "Epoch [2/2], Step [29030/64305], Loss: 4.9668\n",
      "Epoch [2/2], Step [29040/64305], Loss: 4.9542\n",
      "Epoch [2/2], Step [29050/64305], Loss: 4.8454\n",
      "Epoch [2/2], Step [29060/64305], Loss: 4.9388\n",
      "Epoch [2/2], Step [29070/64305], Loss: 4.7962\n",
      "Epoch [2/2], Step [29080/64305], Loss: 4.8502\n",
      "Epoch [2/2], Step [29090/64305], Loss: 4.9437\n",
      "Epoch [2/2], Step [29100/64305], Loss: 4.7247\n",
      "Epoch [2/2], Step [29110/64305], Loss: 4.9310\n",
      "Epoch [2/2], Step [29120/64305], Loss: 4.7755\n",
      "Epoch [2/2], Step [29130/64305], Loss: 4.7993\n",
      "Epoch [2/2], Step [29140/64305], Loss: 4.8665\n",
      "Epoch [2/2], Step [29150/64305], Loss: 4.6527\n",
      "Epoch [2/2], Step [29160/64305], Loss: 4.8510\n",
      "Epoch [2/2], Step [29170/64305], Loss: 4.8435\n",
      "Epoch [2/2], Step [29180/64305], Loss: 4.9234\n",
      "Epoch [2/2], Step [29190/64305], Loss: 4.8367\n",
      "Epoch [2/2], Step [29200/64305], Loss: 4.6157\n",
      "Epoch [2/2], Step [29210/64305], Loss: 4.9324\n",
      "Epoch [2/2], Step [29220/64305], Loss: 4.8484\n",
      "Epoch [2/2], Step [29230/64305], Loss: 4.7798\n",
      "Epoch [2/2], Step [29240/64305], Loss: 4.5277\n",
      "Epoch [2/2], Step [29250/64305], Loss: 4.7962\n",
      "Epoch [2/2], Step [29260/64305], Loss: 4.5797\n",
      "Epoch [2/2], Step [29270/64305], Loss: 4.6702\n",
      "Epoch [2/2], Step [29280/64305], Loss: 4.7839\n",
      "Epoch [2/2], Step [29290/64305], Loss: 4.7470\n",
      "Epoch [2/2], Step [29300/64305], Loss: 4.9750\n",
      "Epoch [2/2], Step [29310/64305], Loss: 4.8261\n",
      "Epoch [2/2], Step [29320/64305], Loss: 4.8707\n",
      "Epoch [2/2], Step [29330/64305], Loss: 4.9408\n",
      "Epoch [2/2], Step [29340/64305], Loss: 4.6919\n",
      "Epoch [2/2], Step [29350/64305], Loss: 4.7226\n",
      "Epoch [2/2], Step [29360/64305], Loss: 4.7890\n",
      "Epoch [2/2], Step [29370/64305], Loss: 4.5957\n",
      "Epoch [2/2], Step [29380/64305], Loss: 4.7513\n",
      "Epoch [2/2], Step [29390/64305], Loss: 4.7734\n",
      "Epoch [2/2], Step [29400/64305], Loss: 4.8092\n",
      "Epoch [2/2], Step [29410/64305], Loss: 4.6911\n",
      "Epoch [2/2], Step [29420/64305], Loss: 4.7958\n",
      "Epoch [2/2], Step [29430/64305], Loss: 4.9407\n",
      "Epoch [2/2], Step [29440/64305], Loss: 4.7942\n",
      "Epoch [2/2], Step [29450/64305], Loss: 4.9713\n",
      "Epoch [2/2], Step [29460/64305], Loss: 4.7807\n",
      "Epoch [2/2], Step [29470/64305], Loss: 4.8980\n",
      "Epoch [2/2], Step [29480/64305], Loss: 4.8670\n",
      "Epoch [2/2], Step [29490/64305], Loss: 4.8726\n",
      "Epoch [2/2], Step [29500/64305], Loss: 4.7059\n",
      "Epoch [2/2], Step [29510/64305], Loss: 4.8917\n",
      "Epoch [2/2], Step [29520/64305], Loss: 4.8855\n",
      "Epoch [2/2], Step [29530/64305], Loss: 4.7494\n",
      "Epoch [2/2], Step [29540/64305], Loss: 4.8630\n",
      "Epoch [2/2], Step [29550/64305], Loss: 4.7013\n",
      "Epoch [2/2], Step [29560/64305], Loss: 4.7046\n",
      "Epoch [2/2], Step [29570/64305], Loss: 4.4616\n",
      "Epoch [2/2], Step [29580/64305], Loss: 4.7237\n",
      "Epoch [2/2], Step [29590/64305], Loss: 4.8843\n",
      "Epoch [2/2], Step [29600/64305], Loss: 4.6407\n",
      "Epoch [2/2], Step [29610/64305], Loss: 4.9467\n",
      "Epoch [2/2], Step [29620/64305], Loss: 5.0451\n",
      "Epoch [2/2], Step [29630/64305], Loss: 4.9015\n",
      "Epoch [2/2], Step [29640/64305], Loss: 4.4979\n",
      "Epoch [2/2], Step [29650/64305], Loss: 4.9149\n",
      "Epoch [2/2], Step [29660/64305], Loss: 4.8195\n",
      "Epoch [2/2], Step [29670/64305], Loss: 4.8107\n",
      "Epoch [2/2], Step [29680/64305], Loss: 4.7739\n",
      "Epoch [2/2], Step [29690/64305], Loss: 4.8061\n",
      "Epoch [2/2], Step [29700/64305], Loss: 4.8182\n",
      "Epoch [2/2], Step [29710/64305], Loss: 4.7983\n",
      "Epoch [2/2], Step [29720/64305], Loss: 4.8103\n",
      "Epoch [2/2], Step [29730/64305], Loss: 4.7613\n",
      "Epoch [2/2], Step [29740/64305], Loss: 4.8901\n",
      "Epoch [2/2], Step [29750/64305], Loss: 4.6422\n",
      "Epoch [2/2], Step [29760/64305], Loss: 4.7237\n",
      "Epoch [2/2], Step [29770/64305], Loss: 5.0387\n",
      "Epoch [2/2], Step [29780/64305], Loss: 4.7986\n",
      "Epoch [2/2], Step [29790/64305], Loss: 4.4956\n",
      "Epoch [2/2], Step [29800/64305], Loss: 4.7815\n",
      "Epoch [2/2], Step [29810/64305], Loss: 4.9469\n",
      "Epoch [2/2], Step [29820/64305], Loss: 4.8484\n",
      "Epoch [2/2], Step [29830/64305], Loss: 4.9017\n",
      "Epoch [2/2], Step [29840/64305], Loss: 4.7163\n",
      "Epoch [2/2], Step [29850/64305], Loss: 4.7697\n",
      "Epoch [2/2], Step [29860/64305], Loss: 4.8098\n",
      "Epoch [2/2], Step [29870/64305], Loss: 4.9870\n",
      "Epoch [2/2], Step [29880/64305], Loss: 4.9077\n",
      "Epoch [2/2], Step [29890/64305], Loss: 4.6408\n",
      "Epoch [2/2], Step [29900/64305], Loss: 4.8141\n",
      "Epoch [2/2], Step [29910/64305], Loss: 4.7835\n",
      "Epoch [2/2], Step [29920/64305], Loss: 4.9322\n",
      "Epoch [2/2], Step [29930/64305], Loss: 4.9201\n",
      "Epoch [2/2], Step [29940/64305], Loss: 4.6302\n",
      "Epoch [2/2], Step [29950/64305], Loss: 4.7199\n",
      "Epoch [2/2], Step [29960/64305], Loss: 4.8921\n",
      "Epoch [2/2], Step [29970/64305], Loss: 4.9373\n",
      "Epoch [2/2], Step [29980/64305], Loss: 4.7333\n",
      "Epoch [2/2], Step [29990/64305], Loss: 4.8033\n",
      "Epoch [2/2], Step [30000/64305], Loss: 5.0060\n",
      "Epoch [2/2], Step [30010/64305], Loss: 4.8299\n",
      "Epoch [2/2], Step [30020/64305], Loss: 5.0744\n",
      "Epoch [2/2], Step [30030/64305], Loss: 4.9548\n",
      "Epoch [2/2], Step [30040/64305], Loss: 4.9550\n",
      "Epoch [2/2], Step [30050/64305], Loss: 4.9885\n",
      "Epoch [2/2], Step [30060/64305], Loss: 4.9424\n",
      "Epoch [2/2], Step [30070/64305], Loss: 4.9290\n",
      "Epoch [2/2], Step [30080/64305], Loss: 4.9661\n",
      "Epoch [2/2], Step [30090/64305], Loss: 4.8210\n",
      "Epoch [2/2], Step [30100/64305], Loss: 4.8906\n",
      "Epoch [2/2], Step [30110/64305], Loss: 4.8155\n",
      "Epoch [2/2], Step [30120/64305], Loss: 4.9096\n",
      "Epoch [2/2], Step [30130/64305], Loss: 4.7683\n",
      "Epoch [2/2], Step [30140/64305], Loss: 4.7335\n",
      "Epoch [2/2], Step [30150/64305], Loss: 4.7780\n",
      "Epoch [2/2], Step [30160/64305], Loss: 4.8035\n",
      "Epoch [2/2], Step [30170/64305], Loss: 4.7436\n",
      "Epoch [2/2], Step [30180/64305], Loss: 4.7202\n",
      "Epoch [2/2], Step [30190/64305], Loss: 4.9210\n",
      "Epoch [2/2], Step [30200/64305], Loss: 4.9015\n",
      "Epoch [2/2], Step [30210/64305], Loss: 4.8706\n",
      "Epoch [2/2], Step [30220/64305], Loss: 4.7437\n",
      "Epoch [2/2], Step [30230/64305], Loss: 4.7421\n",
      "Epoch [2/2], Step [30240/64305], Loss: 4.8792\n",
      "Epoch [2/2], Step [30250/64305], Loss: 4.9410\n",
      "Epoch [2/2], Step [30260/64305], Loss: 4.6158\n",
      "Epoch [2/2], Step [30270/64305], Loss: 4.7783\n",
      "Epoch [2/2], Step [30280/64305], Loss: 4.9255\n",
      "Epoch [2/2], Step [30290/64305], Loss: 4.6547\n",
      "Epoch [2/2], Step [30300/64305], Loss: 4.9545\n",
      "Epoch [2/2], Step [30310/64305], Loss: 4.8053\n",
      "Epoch [2/2], Step [30320/64305], Loss: 4.8471\n",
      "Epoch [2/2], Step [30330/64305], Loss: 4.8537\n",
      "Epoch [2/2], Step [30340/64305], Loss: 5.0098\n",
      "Epoch [2/2], Step [30350/64305], Loss: 4.9565\n",
      "Epoch [2/2], Step [30360/64305], Loss: 4.8995\n",
      "Epoch [2/2], Step [30370/64305], Loss: 4.8940\n",
      "Epoch [2/2], Step [30380/64305], Loss: 5.0260\n",
      "Epoch [2/2], Step [30390/64305], Loss: 4.7682\n",
      "Epoch [2/2], Step [30400/64305], Loss: 4.9544\n",
      "Epoch [2/2], Step [30410/64305], Loss: 4.8721\n",
      "Epoch [2/2], Step [30420/64305], Loss: 4.9079\n",
      "Epoch [2/2], Step [30430/64305], Loss: 4.9555\n",
      "Epoch [2/2], Step [30440/64305], Loss: 4.7639\n",
      "Epoch [2/2], Step [30450/64305], Loss: 4.9618\n",
      "Epoch [2/2], Step [30460/64305], Loss: 5.0113\n",
      "Epoch [2/2], Step [30470/64305], Loss: 4.9082\n",
      "Epoch [2/2], Step [30480/64305], Loss: 4.9719\n",
      "Epoch [2/2], Step [30490/64305], Loss: 4.7308\n",
      "Epoch [2/2], Step [30500/64305], Loss: 4.9814\n",
      "Epoch [2/2], Step [30510/64305], Loss: 4.8150\n",
      "Epoch [2/2], Step [30520/64305], Loss: 4.7502\n",
      "Epoch [2/2], Step [30530/64305], Loss: 4.7220\n",
      "Epoch [2/2], Step [30540/64305], Loss: 5.1084\n",
      "Epoch [2/2], Step [30550/64305], Loss: 4.6470\n",
      "Epoch [2/2], Step [30560/64305], Loss: 4.8985\n",
      "Epoch [2/2], Step [30570/64305], Loss: 4.7057\n",
      "Epoch [2/2], Step [30580/64305], Loss: 4.8080\n",
      "Epoch [2/2], Step [30590/64305], Loss: 4.9819\n",
      "Epoch [2/2], Step [30600/64305], Loss: 4.8559\n",
      "Epoch [2/2], Step [30610/64305], Loss: 4.9366\n",
      "Epoch [2/2], Step [30620/64305], Loss: 4.9026\n",
      "Epoch [2/2], Step [30630/64305], Loss: 4.7731\n",
      "Epoch [2/2], Step [30640/64305], Loss: 4.9690\n",
      "Epoch [2/2], Step [30650/64305], Loss: 4.7930\n",
      "Epoch [2/2], Step [30660/64305], Loss: 4.6022\n",
      "Epoch [2/2], Step [30670/64305], Loss: 4.9576\n",
      "Epoch [2/2], Step [30680/64305], Loss: 5.0034\n",
      "Epoch [2/2], Step [30690/64305], Loss: 4.9052\n",
      "Epoch [2/2], Step [30700/64305], Loss: 4.7816\n",
      "Epoch [2/2], Step [30710/64305], Loss: 4.6474\n",
      "Epoch [2/2], Step [30720/64305], Loss: 4.8661\n",
      "Epoch [2/2], Step [30730/64305], Loss: 4.9799\n",
      "Epoch [2/2], Step [30740/64305], Loss: 4.8704\n",
      "Epoch [2/2], Step [30750/64305], Loss: 5.0404\n",
      "Epoch [2/2], Step [30760/64305], Loss: 4.8634\n",
      "Epoch [2/2], Step [30770/64305], Loss: 4.8046\n",
      "Epoch [2/2], Step [30780/64305], Loss: 4.7840\n",
      "Epoch [2/2], Step [30790/64305], Loss: 4.7437\n",
      "Epoch [2/2], Step [30800/64305], Loss: 4.7157\n",
      "Epoch [2/2], Step [30810/64305], Loss: 4.7748\n",
      "Epoch [2/2], Step [30820/64305], Loss: 4.6616\n",
      "Epoch [2/2], Step [30830/64305], Loss: 4.7859\n",
      "Epoch [2/2], Step [30840/64305], Loss: 4.8160\n",
      "Epoch [2/2], Step [30850/64305], Loss: 4.8499\n",
      "Epoch [2/2], Step [30860/64305], Loss: 4.7646\n",
      "Epoch [2/2], Step [30870/64305], Loss: 4.7369\n",
      "Epoch [2/2], Step [30880/64305], Loss: 4.6566\n",
      "Epoch [2/2], Step [30890/64305], Loss: 5.1983\n",
      "Epoch [2/2], Step [30900/64305], Loss: 4.8232\n",
      "Epoch [2/2], Step [30910/64305], Loss: 4.8623\n",
      "Epoch [2/2], Step [30920/64305], Loss: 4.7512\n",
      "Epoch [2/2], Step [30930/64305], Loss: 4.8791\n",
      "Epoch [2/2], Step [30940/64305], Loss: 5.1271\n",
      "Epoch [2/2], Step [30950/64305], Loss: 4.8723\n",
      "Epoch [2/2], Step [30960/64305], Loss: 4.5718\n",
      "Epoch [2/2], Step [30970/64305], Loss: 4.8461\n",
      "Epoch [2/2], Step [30980/64305], Loss: 4.7482\n",
      "Epoch [2/2], Step [30990/64305], Loss: 4.8273\n",
      "Epoch [2/2], Step [31000/64305], Loss: 4.8296\n",
      "Epoch [2/2], Step [31010/64305], Loss: 4.9676\n",
      "Epoch [2/2], Step [31020/64305], Loss: 4.6795\n",
      "Epoch [2/2], Step [31030/64305], Loss: 4.8302\n",
      "Epoch [2/2], Step [31040/64305], Loss: 4.8824\n",
      "Epoch [2/2], Step [31050/64305], Loss: 4.9387\n",
      "Epoch [2/2], Step [31060/64305], Loss: 4.7559\n",
      "Epoch [2/2], Step [31070/64305], Loss: 4.7105\n",
      "Epoch [2/2], Step [31080/64305], Loss: 4.9289\n",
      "Epoch [2/2], Step [31090/64305], Loss: 4.7350\n",
      "Epoch [2/2], Step [31100/64305], Loss: 4.7454\n",
      "Epoch [2/2], Step [31110/64305], Loss: 4.7244\n",
      "Epoch [2/2], Step [31120/64305], Loss: 4.5737\n",
      "Epoch [2/2], Step [31130/64305], Loss: 4.7757\n",
      "Epoch [2/2], Step [31140/64305], Loss: 4.9474\n",
      "Epoch [2/2], Step [31150/64305], Loss: 4.9935\n",
      "Epoch [2/2], Step [31160/64305], Loss: 4.8662\n",
      "Epoch [2/2], Step [31170/64305], Loss: 5.0130\n",
      "Epoch [2/2], Step [31180/64305], Loss: 5.0340\n",
      "Epoch [2/2], Step [31190/64305], Loss: 4.9305\n",
      "Epoch [2/2], Step [31200/64305], Loss: 4.6559\n",
      "Epoch [2/2], Step [31210/64305], Loss: 4.6861\n",
      "Epoch [2/2], Step [31220/64305], Loss: 5.1108\n",
      "Epoch [2/2], Step [31230/64305], Loss: 4.8330\n",
      "Epoch [2/2], Step [31240/64305], Loss: 4.6948\n",
      "Epoch [2/2], Step [31250/64305], Loss: 4.7273\n",
      "Epoch [2/2], Step [31260/64305], Loss: 4.7557\n",
      "Epoch [2/2], Step [31270/64305], Loss: 4.9216\n",
      "Epoch [2/2], Step [31280/64305], Loss: 4.8638\n",
      "Epoch [2/2], Step [31290/64305], Loss: 4.7884\n",
      "Epoch [2/2], Step [31300/64305], Loss: 4.8310\n",
      "Epoch [2/2], Step [31310/64305], Loss: 5.0100\n",
      "Epoch [2/2], Step [31320/64305], Loss: 4.7307\n",
      "Epoch [2/2], Step [31330/64305], Loss: 4.7847\n",
      "Epoch [2/2], Step [31340/64305], Loss: 4.9160\n",
      "Epoch [2/2], Step [31350/64305], Loss: 4.8588\n",
      "Epoch [2/2], Step [31360/64305], Loss: 4.8350\n",
      "Epoch [2/2], Step [31370/64305], Loss: 4.7309\n",
      "Epoch [2/2], Step [31380/64305], Loss: 4.9009\n",
      "Epoch [2/2], Step [31390/64305], Loss: 4.8310\n",
      "Epoch [2/2], Step [31400/64305], Loss: 5.1128\n",
      "Epoch [2/2], Step [31410/64305], Loss: 4.7877\n",
      "Epoch [2/2], Step [31420/64305], Loss: 4.8425\n",
      "Epoch [2/2], Step [31430/64305], Loss: 4.7493\n",
      "Epoch [2/2], Step [31440/64305], Loss: 4.9596\n",
      "Epoch [2/2], Step [31450/64305], Loss: 4.6993\n",
      "Epoch [2/2], Step [31460/64305], Loss: 4.8921\n",
      "Epoch [2/2], Step [31470/64305], Loss: 4.8457\n",
      "Epoch [2/2], Step [31480/64305], Loss: 4.7125\n",
      "Epoch [2/2], Step [31490/64305], Loss: 4.6566\n",
      "Epoch [2/2], Step [31500/64305], Loss: 4.7338\n",
      "Epoch [2/2], Step [31510/64305], Loss: 4.7562\n",
      "Epoch [2/2], Step [31520/64305], Loss: 4.9242\n",
      "Epoch [2/2], Step [31530/64305], Loss: 4.8632\n",
      "Epoch [2/2], Step [31540/64305], Loss: 5.0242\n",
      "Epoch [2/2], Step [31550/64305], Loss: 5.0900\n",
      "Epoch [2/2], Step [31560/64305], Loss: 4.9310\n",
      "Epoch [2/2], Step [31570/64305], Loss: 4.7250\n",
      "Epoch [2/2], Step [31580/64305], Loss: 4.5711\n",
      "Epoch [2/2], Step [31590/64305], Loss: 4.7780\n",
      "Epoch [2/2], Step [31600/64305], Loss: 4.6177\n",
      "Epoch [2/2], Step [31610/64305], Loss: 4.8468\n",
      "Epoch [2/2], Step [31620/64305], Loss: 4.8159\n",
      "Epoch [2/2], Step [31630/64305], Loss: 4.8233\n",
      "Epoch [2/2], Step [31640/64305], Loss: 5.0854\n",
      "Epoch [2/2], Step [31650/64305], Loss: 4.6675\n",
      "Epoch [2/2], Step [31660/64305], Loss: 4.5953\n",
      "Epoch [2/2], Step [31670/64305], Loss: 5.0957\n",
      "Epoch [2/2], Step [31680/64305], Loss: 4.9396\n",
      "Epoch [2/2], Step [31690/64305], Loss: 4.7476\n",
      "Epoch [2/2], Step [31700/64305], Loss: 4.8560\n",
      "Epoch [2/2], Step [31710/64305], Loss: 4.6964\n",
      "Epoch [2/2], Step [31720/64305], Loss: 5.0126\n",
      "Epoch [2/2], Step [31730/64305], Loss: 5.0414\n",
      "Epoch [2/2], Step [31740/64305], Loss: 4.6246\n",
      "Epoch [2/2], Step [31750/64305], Loss: 4.8869\n",
      "Epoch [2/2], Step [31760/64305], Loss: 4.9400\n",
      "Epoch [2/2], Step [31770/64305], Loss: 4.8909\n",
      "Epoch [2/2], Step [31780/64305], Loss: 4.9397\n",
      "Epoch [2/2], Step [31790/64305], Loss: 4.8448\n",
      "Epoch [2/2], Step [31800/64305], Loss: 4.9115\n",
      "Epoch [2/2], Step [31810/64305], Loss: 4.7138\n",
      "Epoch [2/2], Step [31820/64305], Loss: 5.0228\n",
      "Epoch [2/2], Step [31830/64305], Loss: 4.6929\n",
      "Epoch [2/2], Step [31840/64305], Loss: 4.7733\n",
      "Epoch [2/2], Step [31850/64305], Loss: 4.8139\n",
      "Epoch [2/2], Step [31860/64305], Loss: 4.8319\n",
      "Epoch [2/2], Step [31870/64305], Loss: 5.0155\n",
      "Epoch [2/2], Step [31880/64305], Loss: 4.8386\n",
      "Epoch [2/2], Step [31890/64305], Loss: 4.7338\n",
      "Epoch [2/2], Step [31900/64305], Loss: 4.8395\n",
      "Epoch [2/2], Step [31910/64305], Loss: 4.7989\n",
      "Epoch [2/2], Step [31920/64305], Loss: 4.6609\n",
      "Epoch [2/2], Step [31930/64305], Loss: 4.8208\n",
      "Epoch [2/2], Step [31940/64305], Loss: 4.7109\n",
      "Epoch [2/2], Step [31950/64305], Loss: 4.6908\n",
      "Epoch [2/2], Step [31960/64305], Loss: 4.9527\n",
      "Epoch [2/2], Step [31970/64305], Loss: 5.0722\n",
      "Epoch [2/2], Step [31980/64305], Loss: 4.7956\n",
      "Epoch [2/2], Step [31990/64305], Loss: 4.6320\n",
      "Epoch [2/2], Step [32000/64305], Loss: 4.9017\n",
      "Epoch [2/2], Step [32010/64305], Loss: 4.8676\n",
      "Epoch [2/2], Step [32020/64305], Loss: 4.7783\n",
      "Epoch [2/2], Step [32030/64305], Loss: 4.7236\n",
      "Epoch [2/2], Step [32040/64305], Loss: 4.8279\n",
      "Epoch [2/2], Step [32050/64305], Loss: 4.7093\n",
      "Epoch [2/2], Step [32060/64305], Loss: 4.6868\n",
      "Epoch [2/2], Step [32070/64305], Loss: 4.8269\n",
      "Epoch [2/2], Step [32080/64305], Loss: 4.5410\n",
      "Epoch [2/2], Step [32090/64305], Loss: 4.7151\n",
      "Epoch [2/2], Step [32100/64305], Loss: 4.5874\n",
      "Epoch [2/2], Step [32110/64305], Loss: 4.8120\n",
      "Epoch [2/2], Step [32120/64305], Loss: 4.7720\n",
      "Epoch [2/2], Step [32130/64305], Loss: 5.0140\n",
      "Epoch [2/2], Step [32140/64305], Loss: 5.0354\n",
      "Epoch [2/2], Step [32150/64305], Loss: 4.8647\n",
      "Epoch [2/2], Step [32160/64305], Loss: 4.6647\n",
      "Epoch [2/2], Step [32170/64305], Loss: 4.8372\n",
      "Epoch [2/2], Step [32180/64305], Loss: 4.8079\n",
      "Epoch [2/2], Step [32190/64305], Loss: 4.9511\n",
      "Epoch [2/2], Step [32200/64305], Loss: 4.8244\n",
      "Epoch [2/2], Step [32210/64305], Loss: 4.7782\n",
      "Epoch [2/2], Step [32220/64305], Loss: 4.9943\n",
      "Epoch [2/2], Step [32230/64305], Loss: 5.0382\n",
      "Epoch [2/2], Step [32240/64305], Loss: 4.6735\n",
      "Epoch [2/2], Step [32250/64305], Loss: 4.8558\n",
      "Epoch [2/2], Step [32260/64305], Loss: 4.8955\n",
      "Epoch [2/2], Step [32270/64305], Loss: 4.9605\n",
      "Epoch [2/2], Step [32280/64305], Loss: 4.6733\n",
      "Epoch [2/2], Step [32290/64305], Loss: 4.5290\n",
      "Epoch [2/2], Step [32300/64305], Loss: 4.9303\n",
      "Epoch [2/2], Step [32310/64305], Loss: 4.8450\n",
      "Epoch [2/2], Step [32320/64305], Loss: 4.9656\n",
      "Epoch [2/2], Step [32330/64305], Loss: 4.9346\n",
      "Epoch [2/2], Step [32340/64305], Loss: 4.7959\n",
      "Epoch [2/2], Step [32350/64305], Loss: 4.8491\n",
      "Epoch [2/2], Step [32360/64305], Loss: 4.8448\n",
      "Epoch [2/2], Step [32370/64305], Loss: 4.9205\n",
      "Epoch [2/2], Step [32380/64305], Loss: 4.7150\n",
      "Epoch [2/2], Step [32390/64305], Loss: 4.7891\n",
      "Epoch [2/2], Step [32400/64305], Loss: 4.7614\n",
      "Epoch [2/2], Step [32410/64305], Loss: 5.0264\n",
      "Epoch [2/2], Step [32420/64305], Loss: 4.9711\n",
      "Epoch [2/2], Step [32430/64305], Loss: 4.7817\n",
      "Epoch [2/2], Step [32440/64305], Loss: 4.9210\n",
      "Epoch [2/2], Step [32450/64305], Loss: 4.9068\n",
      "Epoch [2/2], Step [32460/64305], Loss: 4.8423\n",
      "Epoch [2/2], Step [32470/64305], Loss: 4.6435\n",
      "Epoch [2/2], Step [32480/64305], Loss: 4.7200\n",
      "Epoch [2/2], Step [32490/64305], Loss: 5.0081\n",
      "Epoch [2/2], Step [32500/64305], Loss: 5.0331\n",
      "Epoch [2/2], Step [32510/64305], Loss: 4.7497\n",
      "Epoch [2/2], Step [32520/64305], Loss: 5.0467\n",
      "Epoch [2/2], Step [32530/64305], Loss: 4.8143\n",
      "Epoch [2/2], Step [32540/64305], Loss: 4.9531\n",
      "Epoch [2/2], Step [32550/64305], Loss: 4.8921\n",
      "Epoch [2/2], Step [32560/64305], Loss: 4.7486\n",
      "Epoch [2/2], Step [32570/64305], Loss: 4.7436\n",
      "Epoch [2/2], Step [32580/64305], Loss: 5.0474\n",
      "Epoch [2/2], Step [32590/64305], Loss: 4.6853\n",
      "Epoch [2/2], Step [32600/64305], Loss: 4.8205\n",
      "Epoch [2/2], Step [32610/64305], Loss: 4.8013\n",
      "Epoch [2/2], Step [32620/64305], Loss: 4.9361\n",
      "Epoch [2/2], Step [32630/64305], Loss: 4.9101\n",
      "Epoch [2/2], Step [32640/64305], Loss: 4.8617\n",
      "Epoch [2/2], Step [32650/64305], Loss: 4.7687\n",
      "Epoch [2/2], Step [32660/64305], Loss: 5.0554\n",
      "Epoch [2/2], Step [32670/64305], Loss: 4.9062\n",
      "Epoch [2/2], Step [32680/64305], Loss: 4.9977\n",
      "Epoch [2/2], Step [32690/64305], Loss: 4.9549\n",
      "Epoch [2/2], Step [32700/64305], Loss: 4.6097\n",
      "Epoch [2/2], Step [32710/64305], Loss: 4.6006\n",
      "Epoch [2/2], Step [32720/64305], Loss: 4.6928\n",
      "Epoch [2/2], Step [32730/64305], Loss: 4.6379\n",
      "Epoch [2/2], Step [32740/64305], Loss: 5.0667\n",
      "Epoch [2/2], Step [32750/64305], Loss: 4.8291\n",
      "Epoch [2/2], Step [32760/64305], Loss: 4.9896\n",
      "Epoch [2/2], Step [32770/64305], Loss: 4.6897\n",
      "Epoch [2/2], Step [32780/64305], Loss: 4.7702\n",
      "Epoch [2/2], Step [32790/64305], Loss: 4.9445\n",
      "Epoch [2/2], Step [32800/64305], Loss: 4.8374\n",
      "Epoch [2/2], Step [32810/64305], Loss: 4.7214\n",
      "Epoch [2/2], Step [32820/64305], Loss: 4.6838\n",
      "Epoch [2/2], Step [32830/64305], Loss: 5.0661\n",
      "Epoch [2/2], Step [32840/64305], Loss: 4.8426\n",
      "Epoch [2/2], Step [32850/64305], Loss: 4.9400\n",
      "Epoch [2/2], Step [32860/64305], Loss: 4.9158\n",
      "Epoch [2/2], Step [32870/64305], Loss: 4.9306\n",
      "Epoch [2/2], Step [32880/64305], Loss: 4.8888\n",
      "Epoch [2/2], Step [32890/64305], Loss: 4.7593\n",
      "Epoch [2/2], Step [32900/64305], Loss: 4.7800\n",
      "Epoch [2/2], Step [32910/64305], Loss: 4.8686\n",
      "Epoch [2/2], Step [32920/64305], Loss: 4.6862\n",
      "Epoch [2/2], Step [32930/64305], Loss: 4.7938\n",
      "Epoch [2/2], Step [32940/64305], Loss: 4.7874\n",
      "Epoch [2/2], Step [32950/64305], Loss: 4.8840\n",
      "Epoch [2/2], Step [32960/64305], Loss: 4.6682\n",
      "Epoch [2/2], Step [32970/64305], Loss: 4.8844\n",
      "Epoch [2/2], Step [32980/64305], Loss: 4.6718\n",
      "Epoch [2/2], Step [32990/64305], Loss: 4.9188\n",
      "Epoch [2/2], Step [33000/64305], Loss: 4.7630\n",
      "Epoch [2/2], Step [33010/64305], Loss: 5.0501\n",
      "Epoch [2/2], Step [33020/64305], Loss: 4.9467\n",
      "Epoch [2/2], Step [33030/64305], Loss: 5.0226\n",
      "Epoch [2/2], Step [33040/64305], Loss: 4.9595\n",
      "Epoch [2/2], Step [33050/64305], Loss: 4.7317\n",
      "Epoch [2/2], Step [33060/64305], Loss: 4.7059\n",
      "Epoch [2/2], Step [33070/64305], Loss: 4.8422\n",
      "Epoch [2/2], Step [33080/64305], Loss: 4.8713\n",
      "Epoch [2/2], Step [33090/64305], Loss: 4.9414\n",
      "Epoch [2/2], Step [33100/64305], Loss: 4.8533\n",
      "Epoch [2/2], Step [33110/64305], Loss: 4.7432\n",
      "Epoch [2/2], Step [33120/64305], Loss: 4.8167\n",
      "Epoch [2/2], Step [33130/64305], Loss: 4.9935\n",
      "Epoch [2/2], Step [33140/64305], Loss: 4.8201\n",
      "Epoch [2/2], Step [33150/64305], Loss: 4.9516\n",
      "Epoch [2/2], Step [33160/64305], Loss: 4.6514\n",
      "Epoch [2/2], Step [33170/64305], Loss: 4.7509\n",
      "Epoch [2/2], Step [33180/64305], Loss: 4.8077\n",
      "Epoch [2/2], Step [33190/64305], Loss: 4.8379\n",
      "Epoch [2/2], Step [33200/64305], Loss: 4.7519\n",
      "Epoch [2/2], Step [33210/64305], Loss: 4.8067\n",
      "Epoch [2/2], Step [33220/64305], Loss: 4.8926\n",
      "Epoch [2/2], Step [33230/64305], Loss: 4.9518\n",
      "Epoch [2/2], Step [33240/64305], Loss: 4.8448\n",
      "Epoch [2/2], Step [33250/64305], Loss: 4.6545\n",
      "Epoch [2/2], Step [33260/64305], Loss: 4.8396\n",
      "Epoch [2/2], Step [33270/64305], Loss: 4.8084\n",
      "Epoch [2/2], Step [33280/64305], Loss: 4.6187\n",
      "Epoch [2/2], Step [33290/64305], Loss: 4.9397\n",
      "Epoch [2/2], Step [33300/64305], Loss: 4.7999\n",
      "Epoch [2/2], Step [33310/64305], Loss: 4.6417\n",
      "Epoch [2/2], Step [33320/64305], Loss: 4.8315\n",
      "Epoch [2/2], Step [33330/64305], Loss: 4.8300\n",
      "Epoch [2/2], Step [33340/64305], Loss: 4.7437\n",
      "Epoch [2/2], Step [33350/64305], Loss: 4.3982\n",
      "Epoch [2/2], Step [33360/64305], Loss: 4.6701\n",
      "Epoch [2/2], Step [33370/64305], Loss: 4.7961\n",
      "Epoch [2/2], Step [33380/64305], Loss: 4.6004\n",
      "Epoch [2/2], Step [33390/64305], Loss: 4.8976\n",
      "Epoch [2/2], Step [33400/64305], Loss: 4.7869\n",
      "Epoch [2/2], Step [33410/64305], Loss: 4.6594\n",
      "Epoch [2/2], Step [33420/64305], Loss: 4.8467\n",
      "Epoch [2/2], Step [33430/64305], Loss: 4.8581\n",
      "Epoch [2/2], Step [33440/64305], Loss: 5.1050\n",
      "Epoch [2/2], Step [33450/64305], Loss: 4.8252\n",
      "Epoch [2/2], Step [33460/64305], Loss: 4.6836\n",
      "Epoch [2/2], Step [33470/64305], Loss: 4.9340\n",
      "Epoch [2/2], Step [33480/64305], Loss: 4.8776\n",
      "Epoch [2/2], Step [33490/64305], Loss: 4.8121\n",
      "Epoch [2/2], Step [33500/64305], Loss: 4.6442\n",
      "Epoch [2/2], Step [33510/64305], Loss: 4.6827\n",
      "Epoch [2/2], Step [33520/64305], Loss: 4.7912\n",
      "Epoch [2/2], Step [33530/64305], Loss: 4.8766\n",
      "Epoch [2/2], Step [33540/64305], Loss: 4.8683\n",
      "Epoch [2/2], Step [33550/64305], Loss: 4.9538\n",
      "Epoch [2/2], Step [33560/64305], Loss: 4.7122\n",
      "Epoch [2/2], Step [33570/64305], Loss: 4.8283\n",
      "Epoch [2/2], Step [33580/64305], Loss: 5.0346\n",
      "Epoch [2/2], Step [33590/64305], Loss: 4.7247\n",
      "Epoch [2/2], Step [33600/64305], Loss: 4.8507\n",
      "Epoch [2/2], Step [33610/64305], Loss: 4.8334\n",
      "Epoch [2/2], Step [33620/64305], Loss: 4.7482\n",
      "Epoch [2/2], Step [33630/64305], Loss: 4.8721\n",
      "Epoch [2/2], Step [33640/64305], Loss: 4.7181\n",
      "Epoch [2/2], Step [33650/64305], Loss: 4.6370\n",
      "Epoch [2/2], Step [33660/64305], Loss: 4.7574\n",
      "Epoch [2/2], Step [33670/64305], Loss: 4.8195\n",
      "Epoch [2/2], Step [33680/64305], Loss: 5.0487\n",
      "Epoch [2/2], Step [33690/64305], Loss: 4.6491\n",
      "Epoch [2/2], Step [33700/64305], Loss: 4.8109\n",
      "Epoch [2/2], Step [33710/64305], Loss: 4.8338\n",
      "Epoch [2/2], Step [33720/64305], Loss: 4.7624\n",
      "Epoch [2/2], Step [33730/64305], Loss: 4.9590\n",
      "Epoch [2/2], Step [33740/64305], Loss: 4.9392\n",
      "Epoch [2/2], Step [33750/64305], Loss: 4.7830\n",
      "Epoch [2/2], Step [33760/64305], Loss: 4.6514\n",
      "Epoch [2/2], Step [33770/64305], Loss: 5.1728\n",
      "Epoch [2/2], Step [33780/64305], Loss: 4.8476\n",
      "Epoch [2/2], Step [33790/64305], Loss: 4.7771\n",
      "Epoch [2/2], Step [33800/64305], Loss: 4.8253\n",
      "Epoch [2/2], Step [33810/64305], Loss: 4.9402\n",
      "Epoch [2/2], Step [33820/64305], Loss: 5.0436\n",
      "Epoch [2/2], Step [33830/64305], Loss: 4.8558\n",
      "Epoch [2/2], Step [33840/64305], Loss: 4.9906\n",
      "Epoch [2/2], Step [33850/64305], Loss: 4.9454\n",
      "Epoch [2/2], Step [33860/64305], Loss: 4.9540\n",
      "Epoch [2/2], Step [33870/64305], Loss: 4.9266\n",
      "Epoch [2/2], Step [33880/64305], Loss: 4.8778\n",
      "Epoch [2/2], Step [33890/64305], Loss: 4.9107\n",
      "Epoch [2/2], Step [33900/64305], Loss: 4.7973\n",
      "Epoch [2/2], Step [33910/64305], Loss: 5.0159\n",
      "Epoch [2/2], Step [33920/64305], Loss: 4.8698\n",
      "Epoch [2/2], Step [33930/64305], Loss: 4.6603\n",
      "Epoch [2/2], Step [33940/64305], Loss: 4.7830\n",
      "Epoch [2/2], Step [33950/64305], Loss: 4.8529\n",
      "Epoch [2/2], Step [33960/64305], Loss: 4.9346\n",
      "Epoch [2/2], Step [33970/64305], Loss: 4.8529\n",
      "Epoch [2/2], Step [33980/64305], Loss: 4.7262\n",
      "Epoch [2/2], Step [33990/64305], Loss: 4.9036\n",
      "Epoch [2/2], Step [34000/64305], Loss: 4.8427\n",
      "Epoch [2/2], Step [34010/64305], Loss: 4.8903\n",
      "Epoch [2/2], Step [34020/64305], Loss: 4.8490\n",
      "Epoch [2/2], Step [34030/64305], Loss: 4.7517\n",
      "Epoch [2/2], Step [34040/64305], Loss: 4.7860\n",
      "Epoch [2/2], Step [34050/64305], Loss: 4.5829\n",
      "Epoch [2/2], Step [34060/64305], Loss: 4.9613\n",
      "Epoch [2/2], Step [34070/64305], Loss: 4.8133\n",
      "Epoch [2/2], Step [34080/64305], Loss: 5.1933\n",
      "Epoch [2/2], Step [34090/64305], Loss: 4.9690\n",
      "Epoch [2/2], Step [34100/64305], Loss: 4.9189\n",
      "Epoch [2/2], Step [34110/64305], Loss: 4.6951\n",
      "Epoch [2/2], Step [34120/64305], Loss: 4.8461\n",
      "Epoch [2/2], Step [34130/64305], Loss: 4.8924\n",
      "Epoch [2/2], Step [34140/64305], Loss: 4.8389\n",
      "Epoch [2/2], Step [34150/64305], Loss: 4.8924\n",
      "Epoch [2/2], Step [34160/64305], Loss: 4.8365\n",
      "Epoch [2/2], Step [34170/64305], Loss: 4.8045\n",
      "Epoch [2/2], Step [34180/64305], Loss: 4.8224\n",
      "Epoch [2/2], Step [34190/64305], Loss: 4.6215\n",
      "Epoch [2/2], Step [34200/64305], Loss: 4.8477\n",
      "Epoch [2/2], Step [34210/64305], Loss: 4.7444\n",
      "Epoch [2/2], Step [34220/64305], Loss: 4.9216\n",
      "Epoch [2/2], Step [34230/64305], Loss: 4.8062\n",
      "Epoch [2/2], Step [34240/64305], Loss: 4.7499\n",
      "Epoch [2/2], Step [34250/64305], Loss: 4.6675\n",
      "Epoch [2/2], Step [34260/64305], Loss: 4.6841\n",
      "Epoch [2/2], Step [34270/64305], Loss: 4.9576\n",
      "Epoch [2/2], Step [34280/64305], Loss: 4.6530\n",
      "Epoch [2/2], Step [34290/64305], Loss: 4.7659\n",
      "Epoch [2/2], Step [34300/64305], Loss: 5.0981\n",
      "Epoch [2/2], Step [34310/64305], Loss: 4.5816\n",
      "Epoch [2/2], Step [34320/64305], Loss: 4.8263\n",
      "Epoch [2/2], Step [34330/64305], Loss: 4.8805\n",
      "Epoch [2/2], Step [34340/64305], Loss: 4.7878\n",
      "Epoch [2/2], Step [34350/64305], Loss: 4.4891\n",
      "Epoch [2/2], Step [34360/64305], Loss: 4.9677\n",
      "Epoch [2/2], Step [34370/64305], Loss: 5.0051\n",
      "Epoch [2/2], Step [34380/64305], Loss: 4.7132\n",
      "Epoch [2/2], Step [34390/64305], Loss: 4.7025\n",
      "Epoch [2/2], Step [34400/64305], Loss: 4.4747\n",
      "Epoch [2/2], Step [34410/64305], Loss: 5.0278\n",
      "Epoch [2/2], Step [34420/64305], Loss: 4.8020\n",
      "Epoch [2/2], Step [34430/64305], Loss: 4.6272\n",
      "Epoch [2/2], Step [34440/64305], Loss: 4.6248\n",
      "Epoch [2/2], Step [34450/64305], Loss: 4.7718\n",
      "Epoch [2/2], Step [34460/64305], Loss: 4.6799\n",
      "Epoch [2/2], Step [34470/64305], Loss: 4.8021\n",
      "Epoch [2/2], Step [34480/64305], Loss: 5.1565\n",
      "Epoch [2/2], Step [34490/64305], Loss: 4.7318\n",
      "Epoch [2/2], Step [34500/64305], Loss: 4.8589\n",
      "Epoch [2/2], Step [34510/64305], Loss: 4.9107\n",
      "Epoch [2/2], Step [34520/64305], Loss: 4.8132\n",
      "Epoch [2/2], Step [34530/64305], Loss: 4.6921\n",
      "Epoch [2/2], Step [34540/64305], Loss: 4.8608\n",
      "Epoch [2/2], Step [34550/64305], Loss: 4.9515\n",
      "Epoch [2/2], Step [34560/64305], Loss: 4.9444\n",
      "Epoch [2/2], Step [34570/64305], Loss: 4.7434\n",
      "Epoch [2/2], Step [34580/64305], Loss: 4.8142\n",
      "Epoch [2/2], Step [34590/64305], Loss: 4.9529\n",
      "Epoch [2/2], Step [34600/64305], Loss: 4.8507\n",
      "Epoch [2/2], Step [34610/64305], Loss: 4.8809\n",
      "Epoch [2/2], Step [34620/64305], Loss: 4.9349\n",
      "Epoch [2/2], Step [34630/64305], Loss: 4.8836\n",
      "Epoch [2/2], Step [34640/64305], Loss: 4.7550\n",
      "Epoch [2/2], Step [34650/64305], Loss: 4.7835\n",
      "Epoch [2/2], Step [34660/64305], Loss: 4.7885\n",
      "Epoch [2/2], Step [34670/64305], Loss: 4.9167\n",
      "Epoch [2/2], Step [34680/64305], Loss: 4.7019\n",
      "Epoch [2/2], Step [34690/64305], Loss: 4.8191\n",
      "Epoch [2/2], Step [34700/64305], Loss: 4.8595\n",
      "Epoch [2/2], Step [34710/64305], Loss: 4.8242\n",
      "Epoch [2/2], Step [34720/64305], Loss: 4.7655\n",
      "Epoch [2/2], Step [34730/64305], Loss: 5.0413\n",
      "Epoch [2/2], Step [34740/64305], Loss: 4.6493\n",
      "Epoch [2/2], Step [34750/64305], Loss: 4.6590\n",
      "Epoch [2/2], Step [34760/64305], Loss: 4.8363\n",
      "Epoch [2/2], Step [34770/64305], Loss: 4.9594\n",
      "Epoch [2/2], Step [34780/64305], Loss: 5.0513\n",
      "Epoch [2/2], Step [34790/64305], Loss: 4.8456\n",
      "Epoch [2/2], Step [34800/64305], Loss: 4.8995\n",
      "Epoch [2/2], Step [34810/64305], Loss: 4.9337\n",
      "Epoch [2/2], Step [34820/64305], Loss: 4.9265\n",
      "Epoch [2/2], Step [34830/64305], Loss: 4.9010\n",
      "Epoch [2/2], Step [34840/64305], Loss: 4.9534\n",
      "Epoch [2/2], Step [34850/64305], Loss: 4.7487\n",
      "Epoch [2/2], Step [34860/64305], Loss: 4.7146\n",
      "Epoch [2/2], Step [34870/64305], Loss: 4.9367\n",
      "Epoch [2/2], Step [34880/64305], Loss: 4.8302\n",
      "Epoch [2/2], Step [34890/64305], Loss: 4.8363\n",
      "Epoch [2/2], Step [34900/64305], Loss: 4.9302\n",
      "Epoch [2/2], Step [34910/64305], Loss: 4.7428\n",
      "Epoch [2/2], Step [34920/64305], Loss: 4.8179\n",
      "Epoch [2/2], Step [34930/64305], Loss: 4.8314\n",
      "Epoch [2/2], Step [34940/64305], Loss: 4.7835\n",
      "Epoch [2/2], Step [34950/64305], Loss: 4.7240\n",
      "Epoch [2/2], Step [34960/64305], Loss: 4.7960\n",
      "Epoch [2/2], Step [34970/64305], Loss: 4.8570\n",
      "Epoch [2/2], Step [34980/64305], Loss: 4.8339\n",
      "Epoch [2/2], Step [34990/64305], Loss: 4.7930\n",
      "Epoch [2/2], Step [35000/64305], Loss: 4.7507\n",
      "Epoch [2/2], Step [35010/64305], Loss: 5.0108\n",
      "Epoch [2/2], Step [35020/64305], Loss: 4.9140\n",
      "Epoch [2/2], Step [35030/64305], Loss: 4.9588\n",
      "Epoch [2/2], Step [35040/64305], Loss: 4.8854\n",
      "Epoch [2/2], Step [35050/64305], Loss: 4.7588\n",
      "Epoch [2/2], Step [35060/64305], Loss: 4.9270\n",
      "Epoch [2/2], Step [35070/64305], Loss: 4.5236\n",
      "Epoch [2/2], Step [35080/64305], Loss: 4.8839\n",
      "Epoch [2/2], Step [35090/64305], Loss: 4.7628\n",
      "Epoch [2/2], Step [35100/64305], Loss: 4.8474\n",
      "Epoch [2/2], Step [35110/64305], Loss: 4.9810\n",
      "Epoch [2/2], Step [35120/64305], Loss: 4.7327\n",
      "Epoch [2/2], Step [35130/64305], Loss: 4.7428\n",
      "Epoch [2/2], Step [35140/64305], Loss: 4.7814\n",
      "Epoch [2/2], Step [35150/64305], Loss: 4.8848\n",
      "Epoch [2/2], Step [35160/64305], Loss: 4.7208\n",
      "Epoch [2/2], Step [35170/64305], Loss: 4.8106\n",
      "Epoch [2/2], Step [35180/64305], Loss: 4.8178\n",
      "Epoch [2/2], Step [35190/64305], Loss: 4.8856\n",
      "Epoch [2/2], Step [35200/64305], Loss: 4.9924\n",
      "Epoch [2/2], Step [35210/64305], Loss: 4.8372\n",
      "Epoch [2/2], Step [35220/64305], Loss: 4.7832\n",
      "Epoch [2/2], Step [35230/64305], Loss: 4.8275\n",
      "Epoch [2/2], Step [35240/64305], Loss: 4.9557\n",
      "Epoch [2/2], Step [35250/64305], Loss: 4.9285\n",
      "Epoch [2/2], Step [35260/64305], Loss: 4.9152\n",
      "Epoch [2/2], Step [35270/64305], Loss: 4.9181\n",
      "Epoch [2/2], Step [35280/64305], Loss: 4.8461\n",
      "Epoch [2/2], Step [35290/64305], Loss: 4.7511\n",
      "Epoch [2/2], Step [35300/64305], Loss: 4.7663\n",
      "Epoch [2/2], Step [35310/64305], Loss: 4.6884\n",
      "Epoch [2/2], Step [35320/64305], Loss: 4.7506\n",
      "Epoch [2/2], Step [35330/64305], Loss: 4.9772\n",
      "Epoch [2/2], Step [35340/64305], Loss: 4.9029\n",
      "Epoch [2/2], Step [35350/64305], Loss: 4.7853\n",
      "Epoch [2/2], Step [35360/64305], Loss: 4.7074\n",
      "Epoch [2/2], Step [35370/64305], Loss: 4.9978\n",
      "Epoch [2/2], Step [35380/64305], Loss: 4.7244\n",
      "Epoch [2/2], Step [35390/64305], Loss: 4.8147\n",
      "Epoch [2/2], Step [35400/64305], Loss: 4.8094\n",
      "Epoch [2/2], Step [35410/64305], Loss: 4.9193\n",
      "Epoch [2/2], Step [35420/64305], Loss: 4.8772\n",
      "Epoch [2/2], Step [35430/64305], Loss: 4.9984\n",
      "Epoch [2/2], Step [35440/64305], Loss: 4.7292\n",
      "Epoch [2/2], Step [35450/64305], Loss: 4.7492\n",
      "Epoch [2/2], Step [35460/64305], Loss: 4.8641\n",
      "Epoch [2/2], Step [35470/64305], Loss: 4.7894\n",
      "Epoch [2/2], Step [35480/64305], Loss: 4.8103\n",
      "Epoch [2/2], Step [35490/64305], Loss: 4.8414\n",
      "Epoch [2/2], Step [35500/64305], Loss: 4.7878\n",
      "Epoch [2/2], Step [35510/64305], Loss: 4.8042\n",
      "Epoch [2/2], Step [35520/64305], Loss: 5.0132\n",
      "Epoch [2/2], Step [35530/64305], Loss: 5.0997\n",
      "Epoch [2/2], Step [35540/64305], Loss: 4.8196\n",
      "Epoch [2/2], Step [35550/64305], Loss: 4.9361\n",
      "Epoch [2/2], Step [35560/64305], Loss: 4.8103\n",
      "Epoch [2/2], Step [35570/64305], Loss: 4.6049\n",
      "Epoch [2/2], Step [35580/64305], Loss: 4.8721\n",
      "Epoch [2/2], Step [35590/64305], Loss: 4.6991\n",
      "Epoch [2/2], Step [35600/64305], Loss: 4.8467\n",
      "Epoch [2/2], Step [35610/64305], Loss: 5.0618\n",
      "Epoch [2/2], Step [35620/64305], Loss: 4.8227\n",
      "Epoch [2/2], Step [35630/64305], Loss: 4.8370\n",
      "Epoch [2/2], Step [35640/64305], Loss: 4.9402\n",
      "Epoch [2/2], Step [35650/64305], Loss: 4.5063\n",
      "Epoch [2/2], Step [35660/64305], Loss: 4.7915\n",
      "Epoch [2/2], Step [35670/64305], Loss: 4.8604\n",
      "Epoch [2/2], Step [35680/64305], Loss: 4.8723\n",
      "Epoch [2/2], Step [35690/64305], Loss: 4.9166\n",
      "Epoch [2/2], Step [35700/64305], Loss: 4.6157\n",
      "Epoch [2/2], Step [35710/64305], Loss: 4.9049\n",
      "Epoch [2/2], Step [35720/64305], Loss: 5.0095\n",
      "Epoch [2/2], Step [35730/64305], Loss: 4.7871\n",
      "Epoch [2/2], Step [35740/64305], Loss: 5.0376\n",
      "Epoch [2/2], Step [35750/64305], Loss: 4.7454\n",
      "Epoch [2/2], Step [35760/64305], Loss: 4.9044\n",
      "Epoch [2/2], Step [35770/64305], Loss: 4.8052\n",
      "Epoch [2/2], Step [35780/64305], Loss: 4.6044\n",
      "Epoch [2/2], Step [35790/64305], Loss: 4.9723\n",
      "Epoch [2/2], Step [35800/64305], Loss: 4.9439\n",
      "Epoch [2/2], Step [35810/64305], Loss: 4.8417\n",
      "Epoch [2/2], Step [35820/64305], Loss: 4.8245\n",
      "Epoch [2/2], Step [35830/64305], Loss: 5.0387\n",
      "Epoch [2/2], Step [35840/64305], Loss: 4.9157\n",
      "Epoch [2/2], Step [35850/64305], Loss: 4.8461\n",
      "Epoch [2/2], Step [35860/64305], Loss: 5.0051\n",
      "Epoch [2/2], Step [35870/64305], Loss: 4.9076\n",
      "Epoch [2/2], Step [35880/64305], Loss: 4.9368\n",
      "Epoch [2/2], Step [35890/64305], Loss: 4.9572\n",
      "Epoch [2/2], Step [35900/64305], Loss: 4.7769\n",
      "Epoch [2/2], Step [35910/64305], Loss: 4.9117\n",
      "Epoch [2/2], Step [35920/64305], Loss: 4.8865\n",
      "Epoch [2/2], Step [35930/64305], Loss: 5.0467\n",
      "Epoch [2/2], Step [35940/64305], Loss: 4.8869\n",
      "Epoch [2/2], Step [35950/64305], Loss: 4.8738\n",
      "Epoch [2/2], Step [35960/64305], Loss: 4.8433\n",
      "Epoch [2/2], Step [35970/64305], Loss: 4.9117\n",
      "Epoch [2/2], Step [35980/64305], Loss: 4.8896\n",
      "Epoch [2/2], Step [35990/64305], Loss: 5.0073\n",
      "Epoch [2/2], Step [36000/64305], Loss: 4.7915\n",
      "Epoch [2/2], Step [36010/64305], Loss: 4.7792\n",
      "Epoch [2/2], Step [36020/64305], Loss: 4.8448\n",
      "Epoch [2/2], Step [36030/64305], Loss: 4.7076\n",
      "Epoch [2/2], Step [36040/64305], Loss: 4.6460\n",
      "Epoch [2/2], Step [36050/64305], Loss: 4.9973\n",
      "Epoch [2/2], Step [36060/64305], Loss: 4.9251\n",
      "Epoch [2/2], Step [36070/64305], Loss: 4.8143\n",
      "Epoch [2/2], Step [36080/64305], Loss: 4.8522\n",
      "Epoch [2/2], Step [36090/64305], Loss: 4.7049\n",
      "Epoch [2/2], Step [36100/64305], Loss: 4.9554\n",
      "Epoch [2/2], Step [36110/64305], Loss: 4.8454\n",
      "Epoch [2/2], Step [36120/64305], Loss: 4.8046\n",
      "Epoch [2/2], Step [36130/64305], Loss: 4.9612\n",
      "Epoch [2/2], Step [36140/64305], Loss: 4.9947\n",
      "Epoch [2/2], Step [36150/64305], Loss: 4.8955\n",
      "Epoch [2/2], Step [36160/64305], Loss: 4.8757\n",
      "Epoch [2/2], Step [36170/64305], Loss: 4.8887\n",
      "Epoch [2/2], Step [36180/64305], Loss: 4.9686\n",
      "Epoch [2/2], Step [36190/64305], Loss: 4.5905\n",
      "Epoch [2/2], Step [36200/64305], Loss: 4.4401\n",
      "Epoch [2/2], Step [36210/64305], Loss: 5.0292\n",
      "Epoch [2/2], Step [36220/64305], Loss: 4.8687\n",
      "Epoch [2/2], Step [36230/64305], Loss: 4.8278\n",
      "Epoch [2/2], Step [36240/64305], Loss: 4.8179\n",
      "Epoch [2/2], Step [36250/64305], Loss: 4.7302\n",
      "Epoch [2/2], Step [36260/64305], Loss: 4.7148\n",
      "Epoch [2/2], Step [36270/64305], Loss: 4.7228\n",
      "Epoch [2/2], Step [36280/64305], Loss: 4.8320\n",
      "Epoch [2/2], Step [36290/64305], Loss: 4.9883\n",
      "Epoch [2/2], Step [36300/64305], Loss: 4.7620\n",
      "Epoch [2/2], Step [36310/64305], Loss: 4.7704\n",
      "Epoch [2/2], Step [36320/64305], Loss: 4.7539\n",
      "Epoch [2/2], Step [36330/64305], Loss: 5.0484\n",
      "Epoch [2/2], Step [36340/64305], Loss: 4.7329\n",
      "Epoch [2/2], Step [36350/64305], Loss: 4.9834\n",
      "Epoch [2/2], Step [36360/64305], Loss: 4.6937\n",
      "Epoch [2/2], Step [36370/64305], Loss: 4.6991\n",
      "Epoch [2/2], Step [36380/64305], Loss: 4.7563\n",
      "Epoch [2/2], Step [36390/64305], Loss: 4.8472\n",
      "Epoch [2/2], Step [36400/64305], Loss: 4.8336\n",
      "Epoch [2/2], Step [36410/64305], Loss: 4.7244\n",
      "Epoch [2/2], Step [36420/64305], Loss: 5.0826\n",
      "Epoch [2/2], Step [36430/64305], Loss: 4.7364\n",
      "Epoch [2/2], Step [36440/64305], Loss: 4.7993\n",
      "Epoch [2/2], Step [36450/64305], Loss: 4.7248\n",
      "Epoch [2/2], Step [36460/64305], Loss: 4.7373\n",
      "Epoch [2/2], Step [36470/64305], Loss: 4.9801\n",
      "Epoch [2/2], Step [36480/64305], Loss: 4.8006\n",
      "Epoch [2/2], Step [36490/64305], Loss: 4.9244\n",
      "Epoch [2/2], Step [36500/64305], Loss: 4.7197\n",
      "Epoch [2/2], Step [36510/64305], Loss: 4.8145\n",
      "Epoch [2/2], Step [36520/64305], Loss: 4.7105\n",
      "Epoch [2/2], Step [36530/64305], Loss: 4.8820\n",
      "Epoch [2/2], Step [36540/64305], Loss: 4.9884\n",
      "Epoch [2/2], Step [36550/64305], Loss: 4.8183\n",
      "Epoch [2/2], Step [36560/64305], Loss: 5.0125\n",
      "Epoch [2/2], Step [36570/64305], Loss: 4.8261\n",
      "Epoch [2/2], Step [36580/64305], Loss: 4.7413\n",
      "Epoch [2/2], Step [36590/64305], Loss: 4.8816\n",
      "Epoch [2/2], Step [36600/64305], Loss: 4.9353\n",
      "Epoch [2/2], Step [36610/64305], Loss: 4.8483\n",
      "Epoch [2/2], Step [36620/64305], Loss: 4.6475\n",
      "Epoch [2/2], Step [36630/64305], Loss: 4.7621\n",
      "Epoch [2/2], Step [36640/64305], Loss: 4.8173\n",
      "Epoch [2/2], Step [36650/64305], Loss: 4.7930\n",
      "Epoch [2/2], Step [36660/64305], Loss: 4.6759\n",
      "Epoch [2/2], Step [36670/64305], Loss: 4.7166\n",
      "Epoch [2/2], Step [36680/64305], Loss: 4.9306\n",
      "Epoch [2/2], Step [36690/64305], Loss: 4.7546\n",
      "Epoch [2/2], Step [36700/64305], Loss: 4.8707\n",
      "Epoch [2/2], Step [36710/64305], Loss: 4.8212\n",
      "Epoch [2/2], Step [36720/64305], Loss: 5.0049\n",
      "Epoch [2/2], Step [36730/64305], Loss: 4.7932\n",
      "Epoch [2/2], Step [36740/64305], Loss: 4.8155\n",
      "Epoch [2/2], Step [36750/64305], Loss: 4.8857\n",
      "Epoch [2/2], Step [36760/64305], Loss: 4.7626\n",
      "Epoch [2/2], Step [36770/64305], Loss: 4.9332\n",
      "Epoch [2/2], Step [36780/64305], Loss: 4.7970\n",
      "Epoch [2/2], Step [36790/64305], Loss: 4.9562\n",
      "Epoch [2/2], Step [36800/64305], Loss: 4.6471\n",
      "Epoch [2/2], Step [36810/64305], Loss: 4.8937\n",
      "Epoch [2/2], Step [36820/64305], Loss: 4.8932\n",
      "Epoch [2/2], Step [36830/64305], Loss: 4.9149\n",
      "Epoch [2/2], Step [36840/64305], Loss: 5.1542\n",
      "Epoch [2/2], Step [36850/64305], Loss: 4.7328\n",
      "Epoch [2/2], Step [36860/64305], Loss: 4.8370\n",
      "Epoch [2/2], Step [36870/64305], Loss: 4.7573\n",
      "Epoch [2/2], Step [36880/64305], Loss: 4.7867\n",
      "Epoch [2/2], Step [36890/64305], Loss: 4.8276\n",
      "Epoch [2/2], Step [36900/64305], Loss: 4.6059\n",
      "Epoch [2/2], Step [36910/64305], Loss: 4.7570\n",
      "Epoch [2/2], Step [36920/64305], Loss: 4.8406\n",
      "Epoch [2/2], Step [36930/64305], Loss: 4.8606\n",
      "Epoch [2/2], Step [36940/64305], Loss: 4.8859\n",
      "Epoch [2/2], Step [36950/64305], Loss: 4.7317\n",
      "Epoch [2/2], Step [36960/64305], Loss: 4.6770\n",
      "Epoch [2/2], Step [36970/64305], Loss: 4.8022\n",
      "Epoch [2/2], Step [36980/64305], Loss: 4.8431\n",
      "Epoch [2/2], Step [36990/64305], Loss: 4.7159\n",
      "Epoch [2/2], Step [37000/64305], Loss: 4.8499\n",
      "Epoch [2/2], Step [37010/64305], Loss: 4.8708\n",
      "Epoch [2/2], Step [37020/64305], Loss: 4.8835\n",
      "Epoch [2/2], Step [37030/64305], Loss: 4.7603\n",
      "Epoch [2/2], Step [37040/64305], Loss: 4.7737\n",
      "Epoch [2/2], Step [37050/64305], Loss: 4.7611\n",
      "Epoch [2/2], Step [37060/64305], Loss: 4.8157\n",
      "Epoch [2/2], Step [37070/64305], Loss: 5.0255\n",
      "Epoch [2/2], Step [37080/64305], Loss: 4.7909\n",
      "Epoch [2/2], Step [37090/64305], Loss: 5.0417\n",
      "Epoch [2/2], Step [37100/64305], Loss: 4.9081\n",
      "Epoch [2/2], Step [37110/64305], Loss: 4.9038\n",
      "Epoch [2/2], Step [37120/64305], Loss: 4.8990\n",
      "Epoch [2/2], Step [37130/64305], Loss: 4.6767\n",
      "Epoch [2/2], Step [37140/64305], Loss: 4.6310\n",
      "Epoch [2/2], Step [37150/64305], Loss: 4.8295\n",
      "Epoch [2/2], Step [37160/64305], Loss: 4.8530\n",
      "Epoch [2/2], Step [37170/64305], Loss: 4.8547\n",
      "Epoch [2/2], Step [37180/64305], Loss: 4.9312\n",
      "Epoch [2/2], Step [37190/64305], Loss: 4.6644\n",
      "Epoch [2/2], Step [37200/64305], Loss: 4.6525\n",
      "Epoch [2/2], Step [37210/64305], Loss: 4.9484\n",
      "Epoch [2/2], Step [37220/64305], Loss: 4.7721\n",
      "Epoch [2/2], Step [37230/64305], Loss: 4.7103\n",
      "Epoch [2/2], Step [37240/64305], Loss: 4.7456\n",
      "Epoch [2/2], Step [37250/64305], Loss: 4.6464\n",
      "Epoch [2/2], Step [37260/64305], Loss: 4.8834\n",
      "Epoch [2/2], Step [37270/64305], Loss: 4.8468\n",
      "Epoch [2/2], Step [37280/64305], Loss: 4.8114\n",
      "Epoch [2/2], Step [37290/64305], Loss: 4.9337\n",
      "Epoch [2/2], Step [37300/64305], Loss: 4.8969\n",
      "Epoch [2/2], Step [37310/64305], Loss: 4.8840\n",
      "Epoch [2/2], Step [37320/64305], Loss: 4.7930\n",
      "Epoch [2/2], Step [37330/64305], Loss: 4.6056\n",
      "Epoch [2/2], Step [37340/64305], Loss: 4.8429\n",
      "Epoch [2/2], Step [37350/64305], Loss: 4.7866\n",
      "Epoch [2/2], Step [37360/64305], Loss: 4.7615\n",
      "Epoch [2/2], Step [37370/64305], Loss: 4.8571\n",
      "Epoch [2/2], Step [37380/64305], Loss: 4.6184\n",
      "Epoch [2/2], Step [37390/64305], Loss: 4.7938\n",
      "Epoch [2/2], Step [37400/64305], Loss: 4.7514\n",
      "Epoch [2/2], Step [37410/64305], Loss: 5.0646\n",
      "Epoch [2/2], Step [37420/64305], Loss: 4.6573\n",
      "Epoch [2/2], Step [37430/64305], Loss: 4.7314\n",
      "Epoch [2/2], Step [37440/64305], Loss: 4.8500\n",
      "Epoch [2/2], Step [37450/64305], Loss: 4.7794\n",
      "Epoch [2/2], Step [37460/64305], Loss: 4.9385\n",
      "Epoch [2/2], Step [37470/64305], Loss: 4.7103\n",
      "Epoch [2/2], Step [37480/64305], Loss: 4.6103\n",
      "Epoch [2/2], Step [37490/64305], Loss: 4.9198\n",
      "Epoch [2/2], Step [37500/64305], Loss: 4.8256\n",
      "Epoch [2/2], Step [37510/64305], Loss: 5.0317\n",
      "Epoch [2/2], Step [37520/64305], Loss: 4.8333\n",
      "Epoch [2/2], Step [37530/64305], Loss: 4.8395\n",
      "Epoch [2/2], Step [37540/64305], Loss: 4.9128\n",
      "Epoch [2/2], Step [37550/64305], Loss: 4.8299\n",
      "Epoch [2/2], Step [37560/64305], Loss: 4.8635\n",
      "Epoch [2/2], Step [37570/64305], Loss: 4.7804\n",
      "Epoch [2/2], Step [37580/64305], Loss: 4.8202\n",
      "Epoch [2/2], Step [37590/64305], Loss: 4.8249\n",
      "Epoch [2/2], Step [37600/64305], Loss: 4.8860\n",
      "Epoch [2/2], Step [37610/64305], Loss: 4.9768\n",
      "Epoch [2/2], Step [37620/64305], Loss: 4.8152\n",
      "Epoch [2/2], Step [37630/64305], Loss: 4.9119\n",
      "Epoch [2/2], Step [37640/64305], Loss: 4.9307\n",
      "Epoch [2/2], Step [37650/64305], Loss: 4.9888\n",
      "Epoch [2/2], Step [37660/64305], Loss: 4.8149\n",
      "Epoch [2/2], Step [37670/64305], Loss: 4.8521\n",
      "Epoch [2/2], Step [37680/64305], Loss: 4.9140\n",
      "Epoch [2/2], Step [37690/64305], Loss: 4.9274\n",
      "Epoch [2/2], Step [37700/64305], Loss: 5.0129\n",
      "Epoch [2/2], Step [37710/64305], Loss: 4.8127\n",
      "Epoch [2/2], Step [37720/64305], Loss: 4.7729\n",
      "Epoch [2/2], Step [37730/64305], Loss: 4.7966\n",
      "Epoch [2/2], Step [37740/64305], Loss: 4.7844\n",
      "Epoch [2/2], Step [37750/64305], Loss: 4.7762\n",
      "Epoch [2/2], Step [37760/64305], Loss: 4.7188\n",
      "Epoch [2/2], Step [37770/64305], Loss: 4.8045\n",
      "Epoch [2/2], Step [37780/64305], Loss: 4.8577\n",
      "Epoch [2/2], Step [37790/64305], Loss: 4.9100\n",
      "Epoch [2/2], Step [37800/64305], Loss: 4.8396\n",
      "Epoch [2/2], Step [37810/64305], Loss: 4.7980\n",
      "Epoch [2/2], Step [37820/64305], Loss: 4.9406\n",
      "Epoch [2/2], Step [37830/64305], Loss: 4.9367\n",
      "Epoch [2/2], Step [37840/64305], Loss: 4.9059\n",
      "Epoch [2/2], Step [37850/64305], Loss: 4.9167\n",
      "Epoch [2/2], Step [37860/64305], Loss: 4.7888\n",
      "Epoch [2/2], Step [37870/64305], Loss: 5.0519\n",
      "Epoch [2/2], Step [37880/64305], Loss: 4.9567\n",
      "Epoch [2/2], Step [37890/64305], Loss: 4.9999\n",
      "Epoch [2/2], Step [37900/64305], Loss: 4.7587\n",
      "Epoch [2/2], Step [37910/64305], Loss: 4.7671\n",
      "Epoch [2/2], Step [37920/64305], Loss: 4.8122\n",
      "Epoch [2/2], Step [37930/64305], Loss: 4.7612\n",
      "Epoch [2/2], Step [37940/64305], Loss: 4.7737\n",
      "Epoch [2/2], Step [37950/64305], Loss: 4.6674\n",
      "Epoch [2/2], Step [37960/64305], Loss: 4.7836\n",
      "Epoch [2/2], Step [37970/64305], Loss: 4.5733\n",
      "Epoch [2/2], Step [37980/64305], Loss: 4.6545\n",
      "Epoch [2/2], Step [37990/64305], Loss: 4.8677\n",
      "Epoch [2/2], Step [38000/64305], Loss: 4.8502\n",
      "Epoch [2/2], Step [38010/64305], Loss: 4.8057\n",
      "Epoch [2/2], Step [38020/64305], Loss: 4.8364\n",
      "Epoch [2/2], Step [38030/64305], Loss: 4.8626\n",
      "Epoch [2/2], Step [38040/64305], Loss: 4.7773\n",
      "Epoch [2/2], Step [38050/64305], Loss: 4.6976\n",
      "Epoch [2/2], Step [38060/64305], Loss: 4.8479\n",
      "Epoch [2/2], Step [38070/64305], Loss: 4.9131\n",
      "Epoch [2/2], Step [38080/64305], Loss: 4.9893\n",
      "Epoch [2/2], Step [38090/64305], Loss: 4.8154\n",
      "Epoch [2/2], Step [38100/64305], Loss: 4.8084\n",
      "Epoch [2/2], Step [38110/64305], Loss: 4.7912\n",
      "Epoch [2/2], Step [38120/64305], Loss: 4.7992\n",
      "Epoch [2/2], Step [38130/64305], Loss: 4.8764\n",
      "Epoch [2/2], Step [38140/64305], Loss: 4.7770\n",
      "Epoch [2/2], Step [38150/64305], Loss: 5.0244\n",
      "Epoch [2/2], Step [38160/64305], Loss: 4.9751\n",
      "Epoch [2/2], Step [38170/64305], Loss: 4.8769\n",
      "Epoch [2/2], Step [38180/64305], Loss: 4.9243\n",
      "Epoch [2/2], Step [38190/64305], Loss: 4.8810\n",
      "Epoch [2/2], Step [38200/64305], Loss: 4.7584\n",
      "Epoch [2/2], Step [38210/64305], Loss: 4.7443\n",
      "Epoch [2/2], Step [38220/64305], Loss: 4.8136\n",
      "Epoch [2/2], Step [38230/64305], Loss: 4.6324\n",
      "Epoch [2/2], Step [38240/64305], Loss: 4.7571\n",
      "Epoch [2/2], Step [38250/64305], Loss: 4.7679\n",
      "Epoch [2/2], Step [38260/64305], Loss: 4.8042\n",
      "Epoch [2/2], Step [38270/64305], Loss: 4.7550\n",
      "Epoch [2/2], Step [38280/64305], Loss: 4.8795\n",
      "Epoch [2/2], Step [38290/64305], Loss: 4.7263\n",
      "Epoch [2/2], Step [38300/64305], Loss: 4.9486\n",
      "Epoch [2/2], Step [38310/64305], Loss: 4.8200\n",
      "Epoch [2/2], Step [38320/64305], Loss: 4.9372\n",
      "Epoch [2/2], Step [38330/64305], Loss: 4.7471\n",
      "Epoch [2/2], Step [38340/64305], Loss: 4.9047\n",
      "Epoch [2/2], Step [38350/64305], Loss: 4.7781\n",
      "Epoch [2/2], Step [38360/64305], Loss: 4.7578\n",
      "Epoch [2/2], Step [38370/64305], Loss: 4.8967\n",
      "Epoch [2/2], Step [38380/64305], Loss: 4.9450\n",
      "Epoch [2/2], Step [38390/64305], Loss: 4.8530\n",
      "Epoch [2/2], Step [38400/64305], Loss: 4.8580\n",
      "Epoch [2/2], Step [38410/64305], Loss: 4.8783\n",
      "Epoch [2/2], Step [38420/64305], Loss: 4.7303\n",
      "Epoch [2/2], Step [38430/64305], Loss: 4.9722\n",
      "Epoch [2/2], Step [38440/64305], Loss: 4.8830\n",
      "Epoch [2/2], Step [38450/64305], Loss: 4.8473\n",
      "Epoch [2/2], Step [38460/64305], Loss: 4.8800\n",
      "Epoch [2/2], Step [38470/64305], Loss: 4.8980\n",
      "Epoch [2/2], Step [38480/64305], Loss: 4.7338\n",
      "Epoch [2/2], Step [38490/64305], Loss: 4.7115\n",
      "Epoch [2/2], Step [38500/64305], Loss: 4.8278\n",
      "Epoch [2/2], Step [38510/64305], Loss: 4.9386\n",
      "Epoch [2/2], Step [38520/64305], Loss: 4.8000\n",
      "Epoch [2/2], Step [38530/64305], Loss: 4.6971\n",
      "Epoch [2/2], Step [38540/64305], Loss: 4.5185\n",
      "Epoch [2/2], Step [38550/64305], Loss: 4.7917\n",
      "Epoch [2/2], Step [38560/64305], Loss: 5.1022\n",
      "Epoch [2/2], Step [38570/64305], Loss: 5.0430\n",
      "Epoch [2/2], Step [38580/64305], Loss: 4.8246\n",
      "Epoch [2/2], Step [38590/64305], Loss: 4.7549\n",
      "Epoch [2/2], Step [38600/64305], Loss: 4.6879\n",
      "Epoch [2/2], Step [38610/64305], Loss: 4.6586\n",
      "Epoch [2/2], Step [38620/64305], Loss: 4.7887\n",
      "Epoch [2/2], Step [38630/64305], Loss: 4.8766\n",
      "Epoch [2/2], Step [38640/64305], Loss: 4.7711\n",
      "Epoch [2/2], Step [38650/64305], Loss: 4.9720\n",
      "Epoch [2/2], Step [38660/64305], Loss: 4.8668\n",
      "Epoch [2/2], Step [38670/64305], Loss: 4.9282\n",
      "Epoch [2/2], Step [38680/64305], Loss: 4.7949\n",
      "Epoch [2/2], Step [38690/64305], Loss: 4.9765\n",
      "Epoch [2/2], Step [38700/64305], Loss: 4.8957\n",
      "Epoch [2/2], Step [38710/64305], Loss: 4.7656\n",
      "Epoch [2/2], Step [38720/64305], Loss: 4.7446\n",
      "Epoch [2/2], Step [38730/64305], Loss: 4.8148\n",
      "Epoch [2/2], Step [38740/64305], Loss: 5.2222\n",
      "Epoch [2/2], Step [38750/64305], Loss: 4.8694\n",
      "Epoch [2/2], Step [38760/64305], Loss: 4.8137\n",
      "Epoch [2/2], Step [38770/64305], Loss: 4.8616\n",
      "Epoch [2/2], Step [38780/64305], Loss: 4.9606\n",
      "Epoch [2/2], Step [38790/64305], Loss: 4.7215\n",
      "Epoch [2/2], Step [38800/64305], Loss: 4.6945\n",
      "Epoch [2/2], Step [38810/64305], Loss: 4.8761\n",
      "Epoch [2/2], Step [38820/64305], Loss: 4.8192\n",
      "Epoch [2/2], Step [38830/64305], Loss: 4.8643\n",
      "Epoch [2/2], Step [38840/64305], Loss: 4.8630\n",
      "Epoch [2/2], Step [38850/64305], Loss: 4.8473\n",
      "Epoch [2/2], Step [38860/64305], Loss: 4.8205\n",
      "Epoch [2/2], Step [38870/64305], Loss: 4.8671\n",
      "Epoch [2/2], Step [38880/64305], Loss: 4.7242\n",
      "Epoch [2/2], Step [38890/64305], Loss: 4.9644\n",
      "Epoch [2/2], Step [38900/64305], Loss: 4.9282\n",
      "Epoch [2/2], Step [38910/64305], Loss: 4.8548\n",
      "Epoch [2/2], Step [38920/64305], Loss: 4.6660\n",
      "Epoch [2/2], Step [38930/64305], Loss: 4.6927\n",
      "Epoch [2/2], Step [38940/64305], Loss: 4.7395\n",
      "Epoch [2/2], Step [38950/64305], Loss: 4.8352\n",
      "Epoch [2/2], Step [38960/64305], Loss: 4.9234\n",
      "Epoch [2/2], Step [38970/64305], Loss: 4.9268\n",
      "Epoch [2/2], Step [38980/64305], Loss: 4.9099\n",
      "Epoch [2/2], Step [38990/64305], Loss: 4.6914\n",
      "Epoch [2/2], Step [39000/64305], Loss: 4.7899\n",
      "Epoch [2/2], Step [39010/64305], Loss: 4.6361\n",
      "Epoch [2/2], Step [39020/64305], Loss: 4.8900\n",
      "Epoch [2/2], Step [39030/64305], Loss: 4.9654\n",
      "Epoch [2/2], Step [39040/64305], Loss: 4.8369\n",
      "Epoch [2/2], Step [39050/64305], Loss: 5.0531\n",
      "Epoch [2/2], Step [39060/64305], Loss: 4.8488\n",
      "Epoch [2/2], Step [39070/64305], Loss: 4.9171\n",
      "Epoch [2/2], Step [39080/64305], Loss: 4.7921\n",
      "Epoch [2/2], Step [39090/64305], Loss: 4.9055\n",
      "Epoch [2/2], Step [39100/64305], Loss: 4.6489\n",
      "Epoch [2/2], Step [39110/64305], Loss: 5.0206\n",
      "Epoch [2/2], Step [39120/64305], Loss: 4.8411\n",
      "Epoch [2/2], Step [39130/64305], Loss: 4.7223\n",
      "Epoch [2/2], Step [39140/64305], Loss: 4.7149\n",
      "Epoch [2/2], Step [39150/64305], Loss: 4.7112\n",
      "Epoch [2/2], Step [39160/64305], Loss: 4.8617\n",
      "Epoch [2/2], Step [39170/64305], Loss: 4.5854\n",
      "Epoch [2/2], Step [39180/64305], Loss: 4.7734\n",
      "Epoch [2/2], Step [39190/64305], Loss: 5.1296\n",
      "Epoch [2/2], Step [39200/64305], Loss: 4.8180\n",
      "Epoch [2/2], Step [39210/64305], Loss: 4.8911\n",
      "Epoch [2/2], Step [39220/64305], Loss: 4.8730\n",
      "Epoch [2/2], Step [39230/64305], Loss: 4.6914\n",
      "Epoch [2/2], Step [39240/64305], Loss: 4.9126\n",
      "Epoch [2/2], Step [39250/64305], Loss: 4.7943\n",
      "Epoch [2/2], Step [39260/64305], Loss: 4.7547\n",
      "Epoch [2/2], Step [39270/64305], Loss: 4.9375\n",
      "Epoch [2/2], Step [39280/64305], Loss: 4.8594\n",
      "Epoch [2/2], Step [39290/64305], Loss: 5.0416\n",
      "Epoch [2/2], Step [39300/64305], Loss: 5.0621\n",
      "Epoch [2/2], Step [39310/64305], Loss: 4.6992\n",
      "Epoch [2/2], Step [39320/64305], Loss: 4.6349\n",
      "Epoch [2/2], Step [39330/64305], Loss: 4.6633\n",
      "Epoch [2/2], Step [39340/64305], Loss: 4.9282\n",
      "Epoch [2/2], Step [39350/64305], Loss: 4.6221\n",
      "Epoch [2/2], Step [39360/64305], Loss: 4.8532\n",
      "Epoch [2/2], Step [39370/64305], Loss: 4.5838\n",
      "Epoch [2/2], Step [39380/64305], Loss: 4.9397\n",
      "Epoch [2/2], Step [39390/64305], Loss: 4.6740\n",
      "Epoch [2/2], Step [39400/64305], Loss: 4.9473\n",
      "Epoch [2/2], Step [39410/64305], Loss: 4.9098\n",
      "Epoch [2/2], Step [39420/64305], Loss: 4.9446\n",
      "Epoch [2/2], Step [39430/64305], Loss: 4.8764\n",
      "Epoch [2/2], Step [39440/64305], Loss: 4.9772\n",
      "Epoch [2/2], Step [39450/64305], Loss: 4.7993\n",
      "Epoch [2/2], Step [39460/64305], Loss: 4.9010\n",
      "Epoch [2/2], Step [39470/64305], Loss: 4.7476\n",
      "Epoch [2/2], Step [39480/64305], Loss: 4.8450\n",
      "Epoch [2/2], Step [39490/64305], Loss: 4.8559\n",
      "Epoch [2/2], Step [39500/64305], Loss: 4.9617\n",
      "Epoch [2/2], Step [39510/64305], Loss: 4.7231\n",
      "Epoch [2/2], Step [39520/64305], Loss: 4.7608\n",
      "Epoch [2/2], Step [39530/64305], Loss: 4.9674\n",
      "Epoch [2/2], Step [39540/64305], Loss: 4.8410\n",
      "Epoch [2/2], Step [39550/64305], Loss: 4.8395\n",
      "Epoch [2/2], Step [39560/64305], Loss: 4.9944\n",
      "Epoch [2/2], Step [39570/64305], Loss: 4.9718\n",
      "Epoch [2/2], Step [39580/64305], Loss: 4.9796\n",
      "Epoch [2/2], Step [39590/64305], Loss: 4.7369\n",
      "Epoch [2/2], Step [39600/64305], Loss: 5.0192\n",
      "Epoch [2/2], Step [39610/64305], Loss: 4.7257\n",
      "Epoch [2/2], Step [39620/64305], Loss: 5.1061\n",
      "Epoch [2/2], Step [39630/64305], Loss: 4.7502\n",
      "Epoch [2/2], Step [39640/64305], Loss: 4.8727\n",
      "Epoch [2/2], Step [39650/64305], Loss: 4.9573\n",
      "Epoch [2/2], Step [39660/64305], Loss: 5.0134\n",
      "Epoch [2/2], Step [39670/64305], Loss: 4.9753\n",
      "Epoch [2/2], Step [39680/64305], Loss: 4.6870\n",
      "Epoch [2/2], Step [39690/64305], Loss: 4.7651\n",
      "Epoch [2/2], Step [39700/64305], Loss: 4.7874\n",
      "Epoch [2/2], Step [39710/64305], Loss: 4.8474\n",
      "Epoch [2/2], Step [39720/64305], Loss: 4.7945\n",
      "Epoch [2/2], Step [39730/64305], Loss: 4.9885\n",
      "Epoch [2/2], Step [39740/64305], Loss: 4.9225\n",
      "Epoch [2/2], Step [39750/64305], Loss: 4.9562\n",
      "Epoch [2/2], Step [39760/64305], Loss: 4.8264\n",
      "Epoch [2/2], Step [39770/64305], Loss: 4.6888\n",
      "Epoch [2/2], Step [39780/64305], Loss: 4.8994\n",
      "Epoch [2/2], Step [39790/64305], Loss: 4.7492\n",
      "Epoch [2/2], Step [39800/64305], Loss: 4.7654\n",
      "Epoch [2/2], Step [39810/64305], Loss: 4.8585\n",
      "Epoch [2/2], Step [39820/64305], Loss: 4.7512\n",
      "Epoch [2/2], Step [39830/64305], Loss: 4.7848\n",
      "Epoch [2/2], Step [39840/64305], Loss: 4.8152\n",
      "Epoch [2/2], Step [39850/64305], Loss: 4.9443\n",
      "Epoch [2/2], Step [39860/64305], Loss: 4.9265\n",
      "Epoch [2/2], Step [39870/64305], Loss: 4.7790\n",
      "Epoch [2/2], Step [39880/64305], Loss: 4.8590\n",
      "Epoch [2/2], Step [39890/64305], Loss: 4.8493\n",
      "Epoch [2/2], Step [39900/64305], Loss: 4.7947\n",
      "Epoch [2/2], Step [39910/64305], Loss: 4.7923\n",
      "Epoch [2/2], Step [39920/64305], Loss: 4.8481\n",
      "Epoch [2/2], Step [39930/64305], Loss: 4.9232\n",
      "Epoch [2/2], Step [39940/64305], Loss: 4.6222\n",
      "Epoch [2/2], Step [39950/64305], Loss: 4.6376\n",
      "Epoch [2/2], Step [39960/64305], Loss: 4.9755\n",
      "Epoch [2/2], Step [39970/64305], Loss: 4.8256\n",
      "Epoch [2/2], Step [39980/64305], Loss: 4.6415\n",
      "Epoch [2/2], Step [39990/64305], Loss: 4.8377\n",
      "Epoch [2/2], Step [40000/64305], Loss: 4.8056\n",
      "Epoch [2/2], Step [40010/64305], Loss: 4.9139\n",
      "Epoch [2/2], Step [40020/64305], Loss: 5.0538\n",
      "Epoch [2/2], Step [40030/64305], Loss: 4.9671\n",
      "Epoch [2/2], Step [40040/64305], Loss: 4.7109\n",
      "Epoch [2/2], Step [40050/64305], Loss: 4.7722\n",
      "Epoch [2/2], Step [40060/64305], Loss: 4.6943\n",
      "Epoch [2/2], Step [40070/64305], Loss: 4.6546\n",
      "Epoch [2/2], Step [40080/64305], Loss: 4.8425\n",
      "Epoch [2/2], Step [40090/64305], Loss: 4.5956\n",
      "Epoch [2/2], Step [40100/64305], Loss: 4.9369\n",
      "Epoch [2/2], Step [40110/64305], Loss: 4.7236\n",
      "Epoch [2/2], Step [40120/64305], Loss: 4.9732\n",
      "Epoch [2/2], Step [40130/64305], Loss: 4.7526\n",
      "Epoch [2/2], Step [40140/64305], Loss: 4.6853\n",
      "Epoch [2/2], Step [40150/64305], Loss: 4.9544\n",
      "Epoch [2/2], Step [40160/64305], Loss: 4.7679\n",
      "Epoch [2/2], Step [40170/64305], Loss: 4.8319\n",
      "Epoch [2/2], Step [40180/64305], Loss: 4.6603\n",
      "Epoch [2/2], Step [40190/64305], Loss: 4.7044\n",
      "Epoch [2/2], Step [40200/64305], Loss: 4.8440\n",
      "Epoch [2/2], Step [40210/64305], Loss: 4.7868\n",
      "Epoch [2/2], Step [40220/64305], Loss: 4.8561\n",
      "Epoch [2/2], Step [40230/64305], Loss: 4.9894\n",
      "Epoch [2/2], Step [40240/64305], Loss: 4.9092\n",
      "Epoch [2/2], Step [40250/64305], Loss: 4.9763\n",
      "Epoch [2/2], Step [40260/64305], Loss: 4.9690\n",
      "Epoch [2/2], Step [40270/64305], Loss: 5.0267\n",
      "Epoch [2/2], Step [40280/64305], Loss: 4.7357\n",
      "Epoch [2/2], Step [40290/64305], Loss: 4.7540\n",
      "Epoch [2/2], Step [40300/64305], Loss: 4.8590\n",
      "Epoch [2/2], Step [40310/64305], Loss: 4.8098\n",
      "Epoch [2/2], Step [40320/64305], Loss: 4.9573\n",
      "Epoch [2/2], Step [40330/64305], Loss: 4.9328\n",
      "Epoch [2/2], Step [40340/64305], Loss: 4.8710\n",
      "Epoch [2/2], Step [40350/64305], Loss: 4.8137\n",
      "Epoch [2/2], Step [40360/64305], Loss: 4.7600\n",
      "Epoch [2/2], Step [40370/64305], Loss: 4.8246\n",
      "Epoch [2/2], Step [40380/64305], Loss: 4.8672\n",
      "Epoch [2/2], Step [40390/64305], Loss: 4.7687\n",
      "Epoch [2/2], Step [40400/64305], Loss: 4.6431\n",
      "Epoch [2/2], Step [40410/64305], Loss: 5.0324\n",
      "Epoch [2/2], Step [40420/64305], Loss: 4.8545\n",
      "Epoch [2/2], Step [40430/64305], Loss: 4.9285\n",
      "Epoch [2/2], Step [40440/64305], Loss: 4.7148\n",
      "Epoch [2/2], Step [40450/64305], Loss: 4.9018\n",
      "Epoch [2/2], Step [40460/64305], Loss: 4.8067\n",
      "Epoch [2/2], Step [40470/64305], Loss: 4.7796\n",
      "Epoch [2/2], Step [40480/64305], Loss: 4.6583\n",
      "Epoch [2/2], Step [40490/64305], Loss: 4.8526\n",
      "Epoch [2/2], Step [40500/64305], Loss: 4.7940\n",
      "Epoch [2/2], Step [40510/64305], Loss: 4.9317\n",
      "Epoch [2/2], Step [40520/64305], Loss: 4.8226\n",
      "Epoch [2/2], Step [40530/64305], Loss: 4.7694\n",
      "Epoch [2/2], Step [40540/64305], Loss: 4.9159\n",
      "Epoch [2/2], Step [40550/64305], Loss: 5.0617\n",
      "Epoch [2/2], Step [40560/64305], Loss: 4.7132\n",
      "Epoch [2/2], Step [40570/64305], Loss: 4.8612\n",
      "Epoch [2/2], Step [40580/64305], Loss: 4.9160\n",
      "Epoch [2/2], Step [40590/64305], Loss: 4.8237\n",
      "Epoch [2/2], Step [40600/64305], Loss: 4.7377\n",
      "Epoch [2/2], Step [40610/64305], Loss: 4.9215\n",
      "Epoch [2/2], Step [40620/64305], Loss: 4.8367\n",
      "Epoch [2/2], Step [40630/64305], Loss: 4.5945\n",
      "Epoch [2/2], Step [40640/64305], Loss: 4.8894\n",
      "Epoch [2/2], Step [40650/64305], Loss: 4.8157\n",
      "Epoch [2/2], Step [40660/64305], Loss: 4.7999\n",
      "Epoch [2/2], Step [40670/64305], Loss: 4.8837\n",
      "Epoch [2/2], Step [40680/64305], Loss: 5.1080\n",
      "Epoch [2/2], Step [40690/64305], Loss: 4.8369\n",
      "Epoch [2/2], Step [40700/64305], Loss: 4.7887\n",
      "Epoch [2/2], Step [40710/64305], Loss: 4.7361\n",
      "Epoch [2/2], Step [40720/64305], Loss: 4.9240\n",
      "Epoch [2/2], Step [40730/64305], Loss: 4.9497\n",
      "Epoch [2/2], Step [40740/64305], Loss: 4.8262\n",
      "Epoch [2/2], Step [40750/64305], Loss: 4.8322\n",
      "Epoch [2/2], Step [40760/64305], Loss: 4.8550\n",
      "Epoch [2/2], Step [40770/64305], Loss: 4.7378\n",
      "Epoch [2/2], Step [40780/64305], Loss: 4.9389\n",
      "Epoch [2/2], Step [40790/64305], Loss: 4.8451\n",
      "Epoch [2/2], Step [40800/64305], Loss: 5.0290\n",
      "Epoch [2/2], Step [40810/64305], Loss: 4.7798\n",
      "Epoch [2/2], Step [40820/64305], Loss: 4.8851\n",
      "Epoch [2/2], Step [40830/64305], Loss: 4.7947\n",
      "Epoch [2/2], Step [40840/64305], Loss: 4.8562\n",
      "Epoch [2/2], Step [40850/64305], Loss: 5.0233\n",
      "Epoch [2/2], Step [40860/64305], Loss: 5.0055\n",
      "Epoch [2/2], Step [40870/64305], Loss: 4.9151\n",
      "Epoch [2/2], Step [40880/64305], Loss: 4.8586\n",
      "Epoch [2/2], Step [40890/64305], Loss: 4.7369\n",
      "Epoch [2/2], Step [40900/64305], Loss: 4.8534\n",
      "Epoch [2/2], Step [40910/64305], Loss: 4.9113\n",
      "Epoch [2/2], Step [40920/64305], Loss: 4.7929\n",
      "Epoch [2/2], Step [40930/64305], Loss: 4.8999\n",
      "Epoch [2/2], Step [40940/64305], Loss: 4.8366\n",
      "Epoch [2/2], Step [40950/64305], Loss: 4.9127\n",
      "Epoch [2/2], Step [40960/64305], Loss: 4.8730\n",
      "Epoch [2/2], Step [40970/64305], Loss: 4.8240\n",
      "Epoch [2/2], Step [40980/64305], Loss: 4.7375\n",
      "Epoch [2/2], Step [40990/64305], Loss: 4.9008\n",
      "Epoch [2/2], Step [41000/64305], Loss: 4.9243\n",
      "Epoch [2/2], Step [41010/64305], Loss: 4.9451\n",
      "Epoch [2/2], Step [41020/64305], Loss: 4.7879\n",
      "Epoch [2/2], Step [41030/64305], Loss: 4.8571\n",
      "Epoch [2/2], Step [41040/64305], Loss: 4.7348\n",
      "Epoch [2/2], Step [41050/64305], Loss: 4.9070\n",
      "Epoch [2/2], Step [41060/64305], Loss: 4.6769\n",
      "Epoch [2/2], Step [41070/64305], Loss: 4.6429\n",
      "Epoch [2/2], Step [41080/64305], Loss: 4.7235\n",
      "Epoch [2/2], Step [41090/64305], Loss: 4.7720\n",
      "Epoch [2/2], Step [41100/64305], Loss: 4.7965\n",
      "Epoch [2/2], Step [41110/64305], Loss: 4.9388\n",
      "Epoch [2/2], Step [41120/64305], Loss: 4.8534\n",
      "Epoch [2/2], Step [41130/64305], Loss: 4.9086\n",
      "Epoch [2/2], Step [41140/64305], Loss: 4.9872\n",
      "Epoch [2/2], Step [41150/64305], Loss: 4.8083\n",
      "Epoch [2/2], Step [41160/64305], Loss: 4.7280\n",
      "Epoch [2/2], Step [41170/64305], Loss: 5.0361\n",
      "Epoch [2/2], Step [41180/64305], Loss: 4.8723\n",
      "Epoch [2/2], Step [41190/64305], Loss: 4.8013\n",
      "Epoch [2/2], Step [41200/64305], Loss: 4.8206\n",
      "Epoch [2/2], Step [41210/64305], Loss: 4.8563\n",
      "Epoch [2/2], Step [41220/64305], Loss: 4.8572\n",
      "Epoch [2/2], Step [41230/64305], Loss: 4.8672\n",
      "Epoch [2/2], Step [41240/64305], Loss: 4.9485\n",
      "Epoch [2/2], Step [41250/64305], Loss: 4.8865\n",
      "Epoch [2/2], Step [41260/64305], Loss: 4.8591\n",
      "Epoch [2/2], Step [41270/64305], Loss: 4.9860\n",
      "Epoch [2/2], Step [41280/64305], Loss: 4.6641\n",
      "Epoch [2/2], Step [41290/64305], Loss: 5.0156\n",
      "Epoch [2/2], Step [41300/64305], Loss: 4.8355\n",
      "Epoch [2/2], Step [41310/64305], Loss: 5.0139\n",
      "Epoch [2/2], Step [41320/64305], Loss: 4.9090\n",
      "Epoch [2/2], Step [41330/64305], Loss: 4.7096\n",
      "Epoch [2/2], Step [41340/64305], Loss: 4.7645\n",
      "Epoch [2/2], Step [41350/64305], Loss: 4.5410\n",
      "Epoch [2/2], Step [41360/64305], Loss: 4.5696\n",
      "Epoch [2/2], Step [41370/64305], Loss: 5.0273\n",
      "Epoch [2/2], Step [41380/64305], Loss: 4.6690\n",
      "Epoch [2/2], Step [41390/64305], Loss: 4.7020\n",
      "Epoch [2/2], Step [41400/64305], Loss: 4.7603\n",
      "Epoch [2/2], Step [41410/64305], Loss: 4.8569\n",
      "Epoch [2/2], Step [41420/64305], Loss: 5.1670\n",
      "Epoch [2/2], Step [41430/64305], Loss: 4.9757\n",
      "Epoch [2/2], Step [41440/64305], Loss: 4.7875\n",
      "Epoch [2/2], Step [41450/64305], Loss: 5.0889\n",
      "Epoch [2/2], Step [41460/64305], Loss: 4.8368\n",
      "Epoch [2/2], Step [41470/64305], Loss: 4.5894\n",
      "Epoch [2/2], Step [41480/64305], Loss: 4.8742\n",
      "Epoch [2/2], Step [41490/64305], Loss: 4.8044\n",
      "Epoch [2/2], Step [41500/64305], Loss: 4.6698\n",
      "Epoch [2/2], Step [41510/64305], Loss: 4.8475\n",
      "Epoch [2/2], Step [41520/64305], Loss: 4.6552\n",
      "Epoch [2/2], Step [41530/64305], Loss: 4.8404\n",
      "Epoch [2/2], Step [41540/64305], Loss: 4.8451\n",
      "Epoch [2/2], Step [41550/64305], Loss: 4.8197\n",
      "Epoch [2/2], Step [41560/64305], Loss: 5.0757\n",
      "Epoch [2/2], Step [41570/64305], Loss: 4.8788\n",
      "Epoch [2/2], Step [41580/64305], Loss: 4.6889\n",
      "Epoch [2/2], Step [41590/64305], Loss: 4.8427\n",
      "Epoch [2/2], Step [41600/64305], Loss: 4.8868\n",
      "Epoch [2/2], Step [41610/64305], Loss: 4.8450\n",
      "Epoch [2/2], Step [41620/64305], Loss: 4.9009\n",
      "Epoch [2/2], Step [41630/64305], Loss: 4.8903\n",
      "Epoch [2/2], Step [41640/64305], Loss: 4.6898\n",
      "Epoch [2/2], Step [41650/64305], Loss: 4.6109\n",
      "Epoch [2/2], Step [41660/64305], Loss: 4.8528\n",
      "Epoch [2/2], Step [41670/64305], Loss: 4.8153\n",
      "Epoch [2/2], Step [41680/64305], Loss: 4.6482\n",
      "Epoch [2/2], Step [41690/64305], Loss: 4.7739\n",
      "Epoch [2/2], Step [41700/64305], Loss: 5.0454\n",
      "Epoch [2/2], Step [41710/64305], Loss: 4.8852\n",
      "Epoch [2/2], Step [41720/64305], Loss: 4.8438\n",
      "Epoch [2/2], Step [41730/64305], Loss: 4.7110\n",
      "Epoch [2/2], Step [41740/64305], Loss: 4.9387\n",
      "Epoch [2/2], Step [41750/64305], Loss: 5.1914\n",
      "Epoch [2/2], Step [41760/64305], Loss: 4.6686\n",
      "Epoch [2/2], Step [41770/64305], Loss: 4.7801\n",
      "Epoch [2/2], Step [41780/64305], Loss: 4.7986\n",
      "Epoch [2/2], Step [41790/64305], Loss: 4.9468\n",
      "Epoch [2/2], Step [41800/64305], Loss: 4.7895\n",
      "Epoch [2/2], Step [41810/64305], Loss: 4.5355\n",
      "Epoch [2/2], Step [41820/64305], Loss: 4.8035\n",
      "Epoch [2/2], Step [41830/64305], Loss: 4.7081\n",
      "Epoch [2/2], Step [41840/64305], Loss: 4.7117\n",
      "Epoch [2/2], Step [41850/64305], Loss: 4.7610\n",
      "Epoch [2/2], Step [41860/64305], Loss: 4.8414\n",
      "Epoch [2/2], Step [41870/64305], Loss: 4.7969\n",
      "Epoch [2/2], Step [41880/64305], Loss: 5.0255\n",
      "Epoch [2/2], Step [41890/64305], Loss: 4.8964\n",
      "Epoch [2/2], Step [41900/64305], Loss: 5.0038\n",
      "Epoch [2/2], Step [41910/64305], Loss: 4.7851\n",
      "Epoch [2/2], Step [41920/64305], Loss: 4.8523\n",
      "Epoch [2/2], Step [41930/64305], Loss: 4.6345\n",
      "Epoch [2/2], Step [41940/64305], Loss: 4.9337\n",
      "Epoch [2/2], Step [41950/64305], Loss: 4.8609\n",
      "Epoch [2/2], Step [41960/64305], Loss: 4.7761\n",
      "Epoch [2/2], Step [41970/64305], Loss: 4.8779\n",
      "Epoch [2/2], Step [41980/64305], Loss: 4.8961\n",
      "Epoch [2/2], Step [41990/64305], Loss: 4.8746\n",
      "Epoch [2/2], Step [42000/64305], Loss: 4.8987\n",
      "Epoch [2/2], Step [42010/64305], Loss: 4.7874\n",
      "Epoch [2/2], Step [42020/64305], Loss: 4.9198\n",
      "Epoch [2/2], Step [42030/64305], Loss: 4.8766\n",
      "Epoch [2/2], Step [42040/64305], Loss: 4.6821\n",
      "Epoch [2/2], Step [42050/64305], Loss: 5.0402\n",
      "Epoch [2/2], Step [42060/64305], Loss: 4.7165\n",
      "Epoch [2/2], Step [42070/64305], Loss: 5.0728\n",
      "Epoch [2/2], Step [42080/64305], Loss: 4.6955\n",
      "Epoch [2/2], Step [42090/64305], Loss: 4.6911\n",
      "Epoch [2/2], Step [42100/64305], Loss: 4.9081\n",
      "Epoch [2/2], Step [42110/64305], Loss: 4.7969\n",
      "Epoch [2/2], Step [42120/64305], Loss: 4.8036\n",
      "Epoch [2/2], Step [42130/64305], Loss: 4.7590\n",
      "Epoch [2/2], Step [42140/64305], Loss: 4.9480\n",
      "Epoch [2/2], Step [42150/64305], Loss: 4.9881\n",
      "Epoch [2/2], Step [42160/64305], Loss: 4.8394\n",
      "Epoch [2/2], Step [42170/64305], Loss: 4.8206\n",
      "Epoch [2/2], Step [42180/64305], Loss: 4.6123\n",
      "Epoch [2/2], Step [42190/64305], Loss: 4.6370\n",
      "Epoch [2/2], Step [42200/64305], Loss: 5.0184\n",
      "Epoch [2/2], Step [42210/64305], Loss: 4.9858\n",
      "Epoch [2/2], Step [42220/64305], Loss: 4.9998\n",
      "Epoch [2/2], Step [42230/64305], Loss: 4.7303\n",
      "Epoch [2/2], Step [42240/64305], Loss: 4.6826\n",
      "Epoch [2/2], Step [42250/64305], Loss: 4.8430\n",
      "Epoch [2/2], Step [42260/64305], Loss: 4.6950\n",
      "Epoch [2/2], Step [42270/64305], Loss: 4.7991\n",
      "Epoch [2/2], Step [42280/64305], Loss: 4.8214\n",
      "Epoch [2/2], Step [42290/64305], Loss: 5.0019\n",
      "Epoch [2/2], Step [42300/64305], Loss: 4.8143\n",
      "Epoch [2/2], Step [42310/64305], Loss: 5.0162\n",
      "Epoch [2/2], Step [42320/64305], Loss: 4.9026\n",
      "Epoch [2/2], Step [42330/64305], Loss: 4.7800\n",
      "Epoch [2/2], Step [42340/64305], Loss: 4.9370\n",
      "Epoch [2/2], Step [42350/64305], Loss: 4.7411\n",
      "Epoch [2/2], Step [42360/64305], Loss: 4.8779\n",
      "Epoch [2/2], Step [42370/64305], Loss: 4.9094\n",
      "Epoch [2/2], Step [42380/64305], Loss: 4.9264\n",
      "Epoch [2/2], Step [42390/64305], Loss: 4.9294\n",
      "Epoch [2/2], Step [42400/64305], Loss: 5.0016\n",
      "Epoch [2/2], Step [42410/64305], Loss: 4.7490\n",
      "Epoch [2/2], Step [42420/64305], Loss: 4.7987\n",
      "Epoch [2/2], Step [42430/64305], Loss: 4.9292\n",
      "Epoch [2/2], Step [42440/64305], Loss: 4.9260\n",
      "Epoch [2/2], Step [42450/64305], Loss: 4.7647\n",
      "Epoch [2/2], Step [42460/64305], Loss: 4.7146\n",
      "Epoch [2/2], Step [42470/64305], Loss: 4.6578\n",
      "Epoch [2/2], Step [42480/64305], Loss: 4.5688\n",
      "Epoch [2/2], Step [42490/64305], Loss: 4.7909\n",
      "Epoch [2/2], Step [42500/64305], Loss: 4.7942\n",
      "Epoch [2/2], Step [42510/64305], Loss: 4.6673\n",
      "Epoch [2/2], Step [42520/64305], Loss: 4.9768\n",
      "Epoch [2/2], Step [42530/64305], Loss: 4.7112\n",
      "Epoch [2/2], Step [42540/64305], Loss: 4.8949\n",
      "Epoch [2/2], Step [42550/64305], Loss: 4.9009\n",
      "Epoch [2/2], Step [42560/64305], Loss: 4.9684\n",
      "Epoch [2/2], Step [42570/64305], Loss: 4.7408\n",
      "Epoch [2/2], Step [42580/64305], Loss: 4.8678\n",
      "Epoch [2/2], Step [42590/64305], Loss: 4.8531\n",
      "Epoch [2/2], Step [42600/64305], Loss: 4.7184\n",
      "Epoch [2/2], Step [42610/64305], Loss: 4.7759\n",
      "Epoch [2/2], Step [42620/64305], Loss: 5.0714\n",
      "Epoch [2/2], Step [42630/64305], Loss: 4.8407\n",
      "Epoch [2/2], Step [42640/64305], Loss: 4.9728\n",
      "Epoch [2/2], Step [42650/64305], Loss: 4.9525\n",
      "Epoch [2/2], Step [42660/64305], Loss: 4.5592\n",
      "Epoch [2/2], Step [42670/64305], Loss: 4.8146\n",
      "Epoch [2/2], Step [42680/64305], Loss: 4.6935\n",
      "Epoch [2/2], Step [42690/64305], Loss: 5.0084\n",
      "Epoch [2/2], Step [42700/64305], Loss: 4.7623\n",
      "Epoch [2/2], Step [42710/64305], Loss: 4.6367\n",
      "Epoch [2/2], Step [42720/64305], Loss: 4.7605\n",
      "Epoch [2/2], Step [42730/64305], Loss: 4.7459\n",
      "Epoch [2/2], Step [42740/64305], Loss: 4.7680\n",
      "Epoch [2/2], Step [42750/64305], Loss: 4.9057\n",
      "Epoch [2/2], Step [42760/64305], Loss: 4.6435\n",
      "Epoch [2/2], Step [42770/64305], Loss: 4.9053\n",
      "Epoch [2/2], Step [42780/64305], Loss: 5.0099\n",
      "Epoch [2/2], Step [42790/64305], Loss: 4.7331\n",
      "Epoch [2/2], Step [42800/64305], Loss: 4.7397\n",
      "Epoch [2/2], Step [42810/64305], Loss: 4.6870\n",
      "Epoch [2/2], Step [42820/64305], Loss: 4.8993\n",
      "Epoch [2/2], Step [42830/64305], Loss: 4.7411\n",
      "Epoch [2/2], Step [42840/64305], Loss: 4.8092\n",
      "Epoch [2/2], Step [42850/64305], Loss: 4.7912\n",
      "Epoch [2/2], Step [42860/64305], Loss: 4.7631\n",
      "Epoch [2/2], Step [42870/64305], Loss: 4.8212\n",
      "Epoch [2/2], Step [42880/64305], Loss: 4.4503\n",
      "Epoch [2/2], Step [42890/64305], Loss: 4.9434\n",
      "Epoch [2/2], Step [42900/64305], Loss: 4.7392\n",
      "Epoch [2/2], Step [42910/64305], Loss: 4.8507\n",
      "Epoch [2/2], Step [42920/64305], Loss: 4.9383\n",
      "Epoch [2/2], Step [42930/64305], Loss: 4.6953\n",
      "Epoch [2/2], Step [42940/64305], Loss: 4.5793\n",
      "Epoch [2/2], Step [42950/64305], Loss: 4.9845\n",
      "Epoch [2/2], Step [42960/64305], Loss: 4.9574\n",
      "Epoch [2/2], Step [42970/64305], Loss: 4.9945\n",
      "Epoch [2/2], Step [42980/64305], Loss: 4.7527\n",
      "Epoch [2/2], Step [42990/64305], Loss: 4.9221\n",
      "Epoch [2/2], Step [43000/64305], Loss: 4.9630\n",
      "Epoch [2/2], Step [43010/64305], Loss: 5.0668\n",
      "Epoch [2/2], Step [43020/64305], Loss: 4.9546\n",
      "Epoch [2/2], Step [43030/64305], Loss: 4.8055\n",
      "Epoch [2/2], Step [43040/64305], Loss: 4.7359\n",
      "Epoch [2/2], Step [43050/64305], Loss: 4.9535\n",
      "Epoch [2/2], Step [43060/64305], Loss: 4.8414\n",
      "Epoch [2/2], Step [43070/64305], Loss: 4.7553\n",
      "Epoch [2/2], Step [43080/64305], Loss: 4.7921\n",
      "Epoch [2/2], Step [43090/64305], Loss: 4.6970\n",
      "Epoch [2/2], Step [43100/64305], Loss: 4.8067\n",
      "Epoch [2/2], Step [43110/64305], Loss: 4.6249\n",
      "Epoch [2/2], Step [43120/64305], Loss: 4.8692\n",
      "Epoch [2/2], Step [43130/64305], Loss: 4.7832\n",
      "Epoch [2/2], Step [43140/64305], Loss: 4.9456\n",
      "Epoch [2/2], Step [43150/64305], Loss: 4.8278\n",
      "Epoch [2/2], Step [43160/64305], Loss: 4.7152\n",
      "Epoch [2/2], Step [43170/64305], Loss: 4.8456\n",
      "Epoch [2/2], Step [43180/64305], Loss: 4.8538\n",
      "Epoch [2/2], Step [43190/64305], Loss: 4.6484\n",
      "Epoch [2/2], Step [43200/64305], Loss: 4.8687\n",
      "Epoch [2/2], Step [43210/64305], Loss: 4.7134\n",
      "Epoch [2/2], Step [43220/64305], Loss: 4.8948\n",
      "Epoch [2/2], Step [43230/64305], Loss: 4.9313\n",
      "Epoch [2/2], Step [43240/64305], Loss: 4.8394\n",
      "Epoch [2/2], Step [43250/64305], Loss: 4.9077\n",
      "Epoch [2/2], Step [43260/64305], Loss: 4.5697\n",
      "Epoch [2/2], Step [43270/64305], Loss: 4.7110\n",
      "Epoch [2/2], Step [43280/64305], Loss: 4.6367\n",
      "Epoch [2/2], Step [43290/64305], Loss: 4.9470\n",
      "Epoch [2/2], Step [43300/64305], Loss: 5.0048\n",
      "Epoch [2/2], Step [43310/64305], Loss: 4.8704\n",
      "Epoch [2/2], Step [43320/64305], Loss: 4.8434\n",
      "Epoch [2/2], Step [43330/64305], Loss: 4.8666\n",
      "Epoch [2/2], Step [43340/64305], Loss: 4.9874\n",
      "Epoch [2/2], Step [43350/64305], Loss: 4.7920\n",
      "Epoch [2/2], Step [43360/64305], Loss: 4.7412\n",
      "Epoch [2/2], Step [43370/64305], Loss: 4.7329\n",
      "Epoch [2/2], Step [43380/64305], Loss: 4.8913\n",
      "Epoch [2/2], Step [43390/64305], Loss: 4.9525\n",
      "Epoch [2/2], Step [43400/64305], Loss: 4.6371\n",
      "Epoch [2/2], Step [43410/64305], Loss: 4.9035\n",
      "Epoch [2/2], Step [43420/64305], Loss: 4.8891\n",
      "Epoch [2/2], Step [43430/64305], Loss: 4.7284\n",
      "Epoch [2/2], Step [43440/64305], Loss: 4.9080\n",
      "Epoch [2/2], Step [43450/64305], Loss: 4.9908\n",
      "Epoch [2/2], Step [43460/64305], Loss: 4.7947\n",
      "Epoch [2/2], Step [43470/64305], Loss: 4.6858\n",
      "Epoch [2/2], Step [43480/64305], Loss: 4.9839\n",
      "Epoch [2/2], Step [43490/64305], Loss: 4.9041\n",
      "Epoch [2/2], Step [43500/64305], Loss: 4.4969\n",
      "Epoch [2/2], Step [43510/64305], Loss: 4.7445\n",
      "Epoch [2/2], Step [43520/64305], Loss: 4.7842\n",
      "Epoch [2/2], Step [43530/64305], Loss: 4.7287\n",
      "Epoch [2/2], Step [43540/64305], Loss: 4.7644\n",
      "Epoch [2/2], Step [43550/64305], Loss: 4.8339\n",
      "Epoch [2/2], Step [43560/64305], Loss: 4.9060\n",
      "Epoch [2/2], Step [43570/64305], Loss: 4.9658\n",
      "Epoch [2/2], Step [43580/64305], Loss: 4.9451\n",
      "Epoch [2/2], Step [43590/64305], Loss: 4.9203\n",
      "Epoch [2/2], Step [43600/64305], Loss: 4.8191\n",
      "Epoch [2/2], Step [43610/64305], Loss: 4.9281\n",
      "Epoch [2/2], Step [43620/64305], Loss: 4.8404\n",
      "Epoch [2/2], Step [43630/64305], Loss: 5.1944\n",
      "Epoch [2/2], Step [43640/64305], Loss: 4.7778\n",
      "Epoch [2/2], Step [43650/64305], Loss: 4.9885\n",
      "Epoch [2/2], Step [43660/64305], Loss: 5.0233\n",
      "Epoch [2/2], Step [43670/64305], Loss: 4.7530\n",
      "Epoch [2/2], Step [43680/64305], Loss: 4.7709\n",
      "Epoch [2/2], Step [43690/64305], Loss: 4.8262\n",
      "Epoch [2/2], Step [43700/64305], Loss: 4.7448\n",
      "Epoch [2/2], Step [43710/64305], Loss: 4.8185\n",
      "Epoch [2/2], Step [43720/64305], Loss: 4.9217\n",
      "Epoch [2/2], Step [43730/64305], Loss: 4.7717\n",
      "Epoch [2/2], Step [43740/64305], Loss: 4.7005\n",
      "Epoch [2/2], Step [43750/64305], Loss: 4.5894\n",
      "Epoch [2/2], Step [43760/64305], Loss: 4.7634\n",
      "Epoch [2/2], Step [43770/64305], Loss: 4.8209\n",
      "Epoch [2/2], Step [43780/64305], Loss: 4.8139\n",
      "Epoch [2/2], Step [43790/64305], Loss: 4.9023\n",
      "Epoch [2/2], Step [43800/64305], Loss: 4.9151\n",
      "Epoch [2/2], Step [43810/64305], Loss: 4.6616\n",
      "Epoch [2/2], Step [43820/64305], Loss: 4.7328\n",
      "Epoch [2/2], Step [43830/64305], Loss: 4.7829\n",
      "Epoch [2/2], Step [43840/64305], Loss: 4.7569\n",
      "Epoch [2/2], Step [43850/64305], Loss: 4.6398\n",
      "Epoch [2/2], Step [43860/64305], Loss: 4.7882\n",
      "Epoch [2/2], Step [43870/64305], Loss: 4.7696\n",
      "Epoch [2/2], Step [43880/64305], Loss: 5.0698\n",
      "Epoch [2/2], Step [43890/64305], Loss: 4.6414\n",
      "Epoch [2/2], Step [43900/64305], Loss: 4.9115\n",
      "Epoch [2/2], Step [43910/64305], Loss: 4.8873\n",
      "Epoch [2/2], Step [43920/64305], Loss: 4.7158\n",
      "Epoch [2/2], Step [43930/64305], Loss: 4.8739\n",
      "Epoch [2/2], Step [43940/64305], Loss: 4.9894\n",
      "Epoch [2/2], Step [43950/64305], Loss: 4.8558\n",
      "Epoch [2/2], Step [43960/64305], Loss: 4.9139\n",
      "Epoch [2/2], Step [43970/64305], Loss: 5.0610\n",
      "Epoch [2/2], Step [43980/64305], Loss: 4.7416\n",
      "Epoch [2/2], Step [43990/64305], Loss: 4.9026\n",
      "Epoch [2/2], Step [44000/64305], Loss: 4.8226\n",
      "Epoch [2/2], Step [44010/64305], Loss: 4.7109\n",
      "Epoch [2/2], Step [44020/64305], Loss: 4.9764\n",
      "Epoch [2/2], Step [44030/64305], Loss: 4.9302\n",
      "Epoch [2/2], Step [44040/64305], Loss: 4.7646\n",
      "Epoch [2/2], Step [44050/64305], Loss: 5.0247\n",
      "Epoch [2/2], Step [44060/64305], Loss: 4.8714\n",
      "Epoch [2/2], Step [44070/64305], Loss: 4.9169\n",
      "Epoch [2/2], Step [44080/64305], Loss: 5.1092\n",
      "Epoch [2/2], Step [44090/64305], Loss: 4.6865\n",
      "Epoch [2/2], Step [44100/64305], Loss: 4.6974\n",
      "Epoch [2/2], Step [44110/64305], Loss: 4.7924\n",
      "Epoch [2/2], Step [44120/64305], Loss: 4.9155\n",
      "Epoch [2/2], Step [44130/64305], Loss: 4.7431\n",
      "Epoch [2/2], Step [44140/64305], Loss: 4.9240\n",
      "Epoch [2/2], Step [44150/64305], Loss: 4.8983\n",
      "Epoch [2/2], Step [44160/64305], Loss: 4.7998\n",
      "Epoch [2/2], Step [44170/64305], Loss: 4.6717\n",
      "Epoch [2/2], Step [44180/64305], Loss: 4.9405\n",
      "Epoch [2/2], Step [44190/64305], Loss: 4.5270\n",
      "Epoch [2/2], Step [44200/64305], Loss: 4.9019\n",
      "Epoch [2/2], Step [44210/64305], Loss: 4.9091\n",
      "Epoch [2/2], Step [44220/64305], Loss: 4.9551\n",
      "Epoch [2/2], Step [44230/64305], Loss: 4.9213\n",
      "Epoch [2/2], Step [44240/64305], Loss: 4.5966\n",
      "Epoch [2/2], Step [44250/64305], Loss: 4.6688\n",
      "Epoch [2/2], Step [44260/64305], Loss: 4.7671\n",
      "Epoch [2/2], Step [44270/64305], Loss: 5.0091\n",
      "Epoch [2/2], Step [44280/64305], Loss: 4.8200\n",
      "Epoch [2/2], Step [44290/64305], Loss: 4.6773\n",
      "Epoch [2/2], Step [44300/64305], Loss: 4.7773\n",
      "Epoch [2/2], Step [44310/64305], Loss: 4.9775\n",
      "Epoch [2/2], Step [44320/64305], Loss: 4.6946\n",
      "Epoch [2/2], Step [44330/64305], Loss: 4.9841\n",
      "Epoch [2/2], Step [44340/64305], Loss: 4.9623\n",
      "Epoch [2/2], Step [44350/64305], Loss: 4.7690\n",
      "Epoch [2/2], Step [44360/64305], Loss: 4.9391\n",
      "Epoch [2/2], Step [44370/64305], Loss: 4.6024\n",
      "Epoch [2/2], Step [44380/64305], Loss: 4.8075\n",
      "Epoch [2/2], Step [44390/64305], Loss: 4.6263\n",
      "Epoch [2/2], Step [44400/64305], Loss: 4.8323\n",
      "Epoch [2/2], Step [44410/64305], Loss: 4.7720\n",
      "Epoch [2/2], Step [44420/64305], Loss: 5.0698\n",
      "Epoch [2/2], Step [44430/64305], Loss: 4.8872\n",
      "Epoch [2/2], Step [44440/64305], Loss: 4.9558\n",
      "Epoch [2/2], Step [44450/64305], Loss: 4.8299\n",
      "Epoch [2/2], Step [44460/64305], Loss: 4.9349\n",
      "Epoch [2/2], Step [44470/64305], Loss: 4.9460\n",
      "Epoch [2/2], Step [44480/64305], Loss: 5.0538\n",
      "Epoch [2/2], Step [44490/64305], Loss: 5.1525\n",
      "Epoch [2/2], Step [44500/64305], Loss: 4.8564\n",
      "Epoch [2/2], Step [44510/64305], Loss: 4.8538\n",
      "Epoch [2/2], Step [44520/64305], Loss: 4.7846\n",
      "Epoch [2/2], Step [44530/64305], Loss: 4.4903\n",
      "Epoch [2/2], Step [44540/64305], Loss: 4.8247\n",
      "Epoch [2/2], Step [44550/64305], Loss: 4.7016\n",
      "Epoch [2/2], Step [44560/64305], Loss: 4.9384\n",
      "Epoch [2/2], Step [44570/64305], Loss: 4.9602\n",
      "Epoch [2/2], Step [44580/64305], Loss: 4.9517\n",
      "Epoch [2/2], Step [44590/64305], Loss: 4.6907\n",
      "Epoch [2/2], Step [44600/64305], Loss: 4.9851\n",
      "Epoch [2/2], Step [44610/64305], Loss: 5.0481\n",
      "Epoch [2/2], Step [44620/64305], Loss: 4.9670\n",
      "Epoch [2/2], Step [44630/64305], Loss: 4.6383\n",
      "Epoch [2/2], Step [44640/64305], Loss: 4.6965\n",
      "Epoch [2/2], Step [44650/64305], Loss: 4.9013\n",
      "Epoch [2/2], Step [44660/64305], Loss: 4.7263\n",
      "Epoch [2/2], Step [44670/64305], Loss: 5.1477\n",
      "Epoch [2/2], Step [44680/64305], Loss: 4.8497\n",
      "Epoch [2/2], Step [44690/64305], Loss: 4.8009\n",
      "Epoch [2/2], Step [44700/64305], Loss: 4.7285\n",
      "Epoch [2/2], Step [44710/64305], Loss: 4.7789\n",
      "Epoch [2/2], Step [44720/64305], Loss: 4.6446\n",
      "Epoch [2/2], Step [44730/64305], Loss: 4.7653\n",
      "Epoch [2/2], Step [44740/64305], Loss: 4.7598\n",
      "Epoch [2/2], Step [44750/64305], Loss: 4.8099\n",
      "Epoch [2/2], Step [44760/64305], Loss: 4.8837\n",
      "Epoch [2/2], Step [44770/64305], Loss: 5.0874\n",
      "Epoch [2/2], Step [44780/64305], Loss: 4.8787\n",
      "Epoch [2/2], Step [44790/64305], Loss: 4.8563\n",
      "Epoch [2/2], Step [44800/64305], Loss: 4.7982\n",
      "Epoch [2/2], Step [44810/64305], Loss: 5.0761\n",
      "Epoch [2/2], Step [44820/64305], Loss: 4.7368\n",
      "Epoch [2/2], Step [44830/64305], Loss: 4.6858\n",
      "Epoch [2/2], Step [44840/64305], Loss: 5.0253\n",
      "Epoch [2/2], Step [44850/64305], Loss: 4.7672\n",
      "Epoch [2/2], Step [44860/64305], Loss: 4.9783\n",
      "Epoch [2/2], Step [44870/64305], Loss: 4.7281\n",
      "Epoch [2/2], Step [44880/64305], Loss: 4.7567\n",
      "Epoch [2/2], Step [44890/64305], Loss: 4.8883\n",
      "Epoch [2/2], Step [44900/64305], Loss: 4.6542\n",
      "Epoch [2/2], Step [44910/64305], Loss: 4.8834\n",
      "Epoch [2/2], Step [44920/64305], Loss: 4.6211\n",
      "Epoch [2/2], Step [44930/64305], Loss: 4.7655\n",
      "Epoch [2/2], Step [44940/64305], Loss: 4.9298\n",
      "Epoch [2/2], Step [44950/64305], Loss: 4.8335\n",
      "Epoch [2/2], Step [44960/64305], Loss: 4.7279\n",
      "Epoch [2/2], Step [44970/64305], Loss: 4.8480\n",
      "Epoch [2/2], Step [44980/64305], Loss: 4.7845\n",
      "Epoch [2/2], Step [44990/64305], Loss: 4.9490\n",
      "Epoch [2/2], Step [45000/64305], Loss: 4.9037\n",
      "Epoch [2/2], Step [45010/64305], Loss: 4.8209\n",
      "Epoch [2/2], Step [45020/64305], Loss: 4.7978\n",
      "Epoch [2/2], Step [45030/64305], Loss: 4.9588\n",
      "Epoch [2/2], Step [45040/64305], Loss: 4.6567\n",
      "Epoch [2/2], Step [45050/64305], Loss: 4.8408\n",
      "Epoch [2/2], Step [45060/64305], Loss: 4.8717\n",
      "Epoch [2/2], Step [45070/64305], Loss: 4.8921\n",
      "Epoch [2/2], Step [45080/64305], Loss: 4.8297\n",
      "Epoch [2/2], Step [45090/64305], Loss: 4.8998\n",
      "Epoch [2/2], Step [45100/64305], Loss: 4.6710\n",
      "Epoch [2/2], Step [45110/64305], Loss: 4.9926\n",
      "Epoch [2/2], Step [45120/64305], Loss: 4.9268\n",
      "Epoch [2/2], Step [45130/64305], Loss: 4.7821\n",
      "Epoch [2/2], Step [45140/64305], Loss: 4.8517\n",
      "Epoch [2/2], Step [45150/64305], Loss: 4.6639\n",
      "Epoch [2/2], Step [45160/64305], Loss: 4.7729\n",
      "Epoch [2/2], Step [45170/64305], Loss: 4.6410\n",
      "Epoch [2/2], Step [45180/64305], Loss: 4.6856\n",
      "Epoch [2/2], Step [45190/64305], Loss: 5.0530\n",
      "Epoch [2/2], Step [45200/64305], Loss: 4.9245\n",
      "Epoch [2/2], Step [45210/64305], Loss: 4.6485\n",
      "Epoch [2/2], Step [45220/64305], Loss: 4.6361\n",
      "Epoch [2/2], Step [45230/64305], Loss: 4.9614\n",
      "Epoch [2/2], Step [45240/64305], Loss: 4.9391\n",
      "Epoch [2/2], Step [45250/64305], Loss: 4.8538\n",
      "Epoch [2/2], Step [45260/64305], Loss: 4.8781\n",
      "Epoch [2/2], Step [45270/64305], Loss: 4.8427\n",
      "Epoch [2/2], Step [45280/64305], Loss: 4.7766\n",
      "Epoch [2/2], Step [45290/64305], Loss: 4.9532\n",
      "Epoch [2/2], Step [45300/64305], Loss: 4.6867\n",
      "Epoch [2/2], Step [45310/64305], Loss: 4.6225\n",
      "Epoch [2/2], Step [45320/64305], Loss: 5.0483\n",
      "Epoch [2/2], Step [45330/64305], Loss: 4.6517\n",
      "Epoch [2/2], Step [45340/64305], Loss: 4.8316\n",
      "Epoch [2/2], Step [45350/64305], Loss: 5.0404\n",
      "Epoch [2/2], Step [45360/64305], Loss: 4.7669\n",
      "Epoch [2/2], Step [45370/64305], Loss: 4.7181\n",
      "Epoch [2/2], Step [45380/64305], Loss: 4.8026\n",
      "Epoch [2/2], Step [45390/64305], Loss: 4.8979\n",
      "Epoch [2/2], Step [45400/64305], Loss: 4.6919\n",
      "Epoch [2/2], Step [45410/64305], Loss: 4.9970\n",
      "Epoch [2/2], Step [45420/64305], Loss: 4.7256\n",
      "Epoch [2/2], Step [45430/64305], Loss: 4.7210\n",
      "Epoch [2/2], Step [45440/64305], Loss: 4.8792\n",
      "Epoch [2/2], Step [45450/64305], Loss: 4.8560\n",
      "Epoch [2/2], Step [45460/64305], Loss: 4.7616\n",
      "Epoch [2/2], Step [45470/64305], Loss: 4.7568\n",
      "Epoch [2/2], Step [45480/64305], Loss: 4.8651\n",
      "Epoch [2/2], Step [45490/64305], Loss: 4.9124\n",
      "Epoch [2/2], Step [45500/64305], Loss: 4.7997\n",
      "Epoch [2/2], Step [45510/64305], Loss: 4.8354\n",
      "Epoch [2/2], Step [45520/64305], Loss: 4.8361\n",
      "Epoch [2/2], Step [45530/64305], Loss: 4.8502\n",
      "Epoch [2/2], Step [45540/64305], Loss: 5.0738\n",
      "Epoch [2/2], Step [45550/64305], Loss: 4.9402\n",
      "Epoch [2/2], Step [45560/64305], Loss: 4.8206\n",
      "Epoch [2/2], Step [45570/64305], Loss: 4.7772\n",
      "Epoch [2/2], Step [45580/64305], Loss: 4.9553\n",
      "Epoch [2/2], Step [45590/64305], Loss: 4.6392\n",
      "Epoch [2/2], Step [45600/64305], Loss: 4.9419\n",
      "Epoch [2/2], Step [45610/64305], Loss: 4.8234\n",
      "Epoch [2/2], Step [45620/64305], Loss: 4.8612\n",
      "Epoch [2/2], Step [45630/64305], Loss: 4.8961\n",
      "Epoch [2/2], Step [45640/64305], Loss: 4.7394\n",
      "Epoch [2/2], Step [45650/64305], Loss: 4.6941\n",
      "Epoch [2/2], Step [45660/64305], Loss: 4.7850\n",
      "Epoch [2/2], Step [45670/64305], Loss: 4.7775\n",
      "Epoch [2/2], Step [45680/64305], Loss: 4.9066\n",
      "Epoch [2/2], Step [45690/64305], Loss: 4.9092\n",
      "Epoch [2/2], Step [45700/64305], Loss: 4.8709\n",
      "Epoch [2/2], Step [45710/64305], Loss: 4.7518\n",
      "Epoch [2/2], Step [45720/64305], Loss: 4.8214\n",
      "Epoch [2/2], Step [45730/64305], Loss: 5.0079\n",
      "Epoch [2/2], Step [45740/64305], Loss: 4.8519\n",
      "Epoch [2/2], Step [45750/64305], Loss: 4.7657\n",
      "Epoch [2/2], Step [45760/64305], Loss: 4.9569\n",
      "Epoch [2/2], Step [45770/64305], Loss: 4.8849\n",
      "Epoch [2/2], Step [45780/64305], Loss: 4.7788\n",
      "Epoch [2/2], Step [45790/64305], Loss: 4.7660\n",
      "Epoch [2/2], Step [45800/64305], Loss: 4.6891\n",
      "Epoch [2/2], Step [45810/64305], Loss: 4.5112\n",
      "Epoch [2/2], Step [45820/64305], Loss: 4.9459\n",
      "Epoch [2/2], Step [45830/64305], Loss: 4.9463\n",
      "Epoch [2/2], Step [45840/64305], Loss: 4.9096\n",
      "Epoch [2/2], Step [45850/64305], Loss: 4.7646\n",
      "Epoch [2/2], Step [45860/64305], Loss: 4.7700\n",
      "Epoch [2/2], Step [45870/64305], Loss: 4.6539\n",
      "Epoch [2/2], Step [45880/64305], Loss: 4.7393\n",
      "Epoch [2/2], Step [45890/64305], Loss: 4.9455\n",
      "Epoch [2/2], Step [45900/64305], Loss: 4.8036\n",
      "Epoch [2/2], Step [45910/64305], Loss: 4.7546\n",
      "Epoch [2/2], Step [45920/64305], Loss: 4.7369\n",
      "Epoch [2/2], Step [45930/64305], Loss: 4.9201\n",
      "Epoch [2/2], Step [45940/64305], Loss: 4.8657\n",
      "Epoch [2/2], Step [45950/64305], Loss: 4.6552\n",
      "Epoch [2/2], Step [45960/64305], Loss: 4.7443\n",
      "Epoch [2/2], Step [45970/64305], Loss: 4.8347\n",
      "Epoch [2/2], Step [45980/64305], Loss: 4.9125\n",
      "Epoch [2/2], Step [45990/64305], Loss: 4.9336\n",
      "Epoch [2/2], Step [46000/64305], Loss: 4.8391\n",
      "Epoch [2/2], Step [46010/64305], Loss: 4.5921\n",
      "Epoch [2/2], Step [46020/64305], Loss: 5.0314\n",
      "Epoch [2/2], Step [46030/64305], Loss: 4.7248\n",
      "Epoch [2/2], Step [46040/64305], Loss: 4.7482\n",
      "Epoch [2/2], Step [46050/64305], Loss: 5.0094\n",
      "Epoch [2/2], Step [46060/64305], Loss: 4.9166\n",
      "Epoch [2/2], Step [46070/64305], Loss: 4.9555\n",
      "Epoch [2/2], Step [46080/64305], Loss: 4.7720\n",
      "Epoch [2/2], Step [46090/64305], Loss: 4.8792\n",
      "Epoch [2/2], Step [46100/64305], Loss: 5.0097\n",
      "Epoch [2/2], Step [46110/64305], Loss: 4.8219\n",
      "Epoch [2/2], Step [46120/64305], Loss: 4.6488\n",
      "Epoch [2/2], Step [46130/64305], Loss: 4.7993\n",
      "Epoch [2/2], Step [46140/64305], Loss: 4.7650\n",
      "Epoch [2/2], Step [46150/64305], Loss: 4.5405\n",
      "Epoch [2/2], Step [46160/64305], Loss: 4.9183\n",
      "Epoch [2/2], Step [46170/64305], Loss: 4.6446\n",
      "Epoch [2/2], Step [46180/64305], Loss: 4.8950\n",
      "Epoch [2/2], Step [46190/64305], Loss: 4.8206\n",
      "Epoch [2/2], Step [46200/64305], Loss: 4.8314\n",
      "Epoch [2/2], Step [46210/64305], Loss: 4.9832\n",
      "Epoch [2/2], Step [46220/64305], Loss: 4.7385\n",
      "Epoch [2/2], Step [46230/64305], Loss: 4.6725\n",
      "Epoch [2/2], Step [46240/64305], Loss: 4.8350\n",
      "Epoch [2/2], Step [46250/64305], Loss: 5.0703\n",
      "Epoch [2/2], Step [46260/64305], Loss: 4.7559\n",
      "Epoch [2/2], Step [46270/64305], Loss: 4.7423\n",
      "Epoch [2/2], Step [46280/64305], Loss: 4.9781\n",
      "Epoch [2/2], Step [46290/64305], Loss: 4.6456\n",
      "Epoch [2/2], Step [46300/64305], Loss: 4.8959\n",
      "Epoch [2/2], Step [46310/64305], Loss: 4.7748\n",
      "Epoch [2/2], Step [46320/64305], Loss: 5.0513\n",
      "Epoch [2/2], Step [46330/64305], Loss: 4.9086\n",
      "Epoch [2/2], Step [46340/64305], Loss: 4.9971\n",
      "Epoch [2/2], Step [46350/64305], Loss: 4.8428\n",
      "Epoch [2/2], Step [46360/64305], Loss: 4.6336\n",
      "Epoch [2/2], Step [46370/64305], Loss: 4.8540\n",
      "Epoch [2/2], Step [46380/64305], Loss: 4.7631\n",
      "Epoch [2/2], Step [46390/64305], Loss: 4.7258\n",
      "Epoch [2/2], Step [46400/64305], Loss: 4.9105\n",
      "Epoch [2/2], Step [46410/64305], Loss: 4.8655\n",
      "Epoch [2/2], Step [46420/64305], Loss: 4.7924\n",
      "Epoch [2/2], Step [46430/64305], Loss: 4.9652\n",
      "Epoch [2/2], Step [46440/64305], Loss: 4.5621\n",
      "Epoch [2/2], Step [46450/64305], Loss: 4.8966\n",
      "Epoch [2/2], Step [46460/64305], Loss: 4.7367\n",
      "Epoch [2/2], Step [46470/64305], Loss: 4.6747\n",
      "Epoch [2/2], Step [46480/64305], Loss: 4.7311\n",
      "Epoch [2/2], Step [46490/64305], Loss: 4.8539\n",
      "Epoch [2/2], Step [46500/64305], Loss: 4.7922\n",
      "Epoch [2/2], Step [46510/64305], Loss: 4.6975\n",
      "Epoch [2/2], Step [46520/64305], Loss: 4.8900\n",
      "Epoch [2/2], Step [46530/64305], Loss: 4.8509\n",
      "Epoch [2/2], Step [46540/64305], Loss: 4.8197\n",
      "Epoch [2/2], Step [46550/64305], Loss: 4.6724\n",
      "Epoch [2/2], Step [46560/64305], Loss: 4.7411\n",
      "Epoch [2/2], Step [46570/64305], Loss: 4.8959\n",
      "Epoch [2/2], Step [46580/64305], Loss: 4.6630\n",
      "Epoch [2/2], Step [46590/64305], Loss: 4.8173\n",
      "Epoch [2/2], Step [46600/64305], Loss: 4.7595\n",
      "Epoch [2/2], Step [46610/64305], Loss: 4.6369\n",
      "Epoch [2/2], Step [46620/64305], Loss: 4.8763\n",
      "Epoch [2/2], Step [46630/64305], Loss: 5.1193\n",
      "Epoch [2/2], Step [46640/64305], Loss: 4.7527\n",
      "Epoch [2/2], Step [46650/64305], Loss: 4.9124\n",
      "Epoch [2/2], Step [46660/64305], Loss: 4.9340\n",
      "Epoch [2/2], Step [46670/64305], Loss: 4.9215\n",
      "Epoch [2/2], Step [46680/64305], Loss: 4.9181\n",
      "Epoch [2/2], Step [46690/64305], Loss: 4.9009\n",
      "Epoch [2/2], Step [46700/64305], Loss: 4.8186\n",
      "Epoch [2/2], Step [46710/64305], Loss: 4.5518\n",
      "Epoch [2/2], Step [46720/64305], Loss: 4.7032\n",
      "Epoch [2/2], Step [46730/64305], Loss: 4.9163\n",
      "Epoch [2/2], Step [46740/64305], Loss: 4.8452\n",
      "Epoch [2/2], Step [46750/64305], Loss: 4.6104\n",
      "Epoch [2/2], Step [46760/64305], Loss: 4.8485\n",
      "Epoch [2/2], Step [46770/64305], Loss: 4.7902\n",
      "Epoch [2/2], Step [46780/64305], Loss: 4.7749\n",
      "Epoch [2/2], Step [46790/64305], Loss: 4.8031\n",
      "Epoch [2/2], Step [46800/64305], Loss: 4.8742\n",
      "Epoch [2/2], Step [46810/64305], Loss: 4.6636\n",
      "Epoch [2/2], Step [46820/64305], Loss: 4.7762\n",
      "Epoch [2/2], Step [46830/64305], Loss: 4.8178\n",
      "Epoch [2/2], Step [46840/64305], Loss: 4.8482\n",
      "Epoch [2/2], Step [46850/64305], Loss: 4.6519\n",
      "Epoch [2/2], Step [46860/64305], Loss: 5.0720\n",
      "Epoch [2/2], Step [46870/64305], Loss: 4.8342\n",
      "Epoch [2/2], Step [46880/64305], Loss: 4.8617\n",
      "Epoch [2/2], Step [46890/64305], Loss: 4.7922\n",
      "Epoch [2/2], Step [46900/64305], Loss: 4.7950\n",
      "Epoch [2/2], Step [46910/64305], Loss: 4.9323\n",
      "Epoch [2/2], Step [46920/64305], Loss: 5.0170\n",
      "Epoch [2/2], Step [46930/64305], Loss: 4.8170\n",
      "Epoch [2/2], Step [46940/64305], Loss: 4.9051\n",
      "Epoch [2/2], Step [46950/64305], Loss: 4.7085\n",
      "Epoch [2/2], Step [46960/64305], Loss: 5.1802\n",
      "Epoch [2/2], Step [46970/64305], Loss: 4.9706\n",
      "Epoch [2/2], Step [46980/64305], Loss: 4.8536\n",
      "Epoch [2/2], Step [46990/64305], Loss: 4.7502\n",
      "Epoch [2/2], Step [47000/64305], Loss: 4.7440\n",
      "Epoch [2/2], Step [47010/64305], Loss: 4.7692\n",
      "Epoch [2/2], Step [47020/64305], Loss: 5.0207\n",
      "Epoch [2/2], Step [47030/64305], Loss: 4.9157\n",
      "Epoch [2/2], Step [47040/64305], Loss: 4.7302\n",
      "Epoch [2/2], Step [47050/64305], Loss: 4.7986\n",
      "Epoch [2/2], Step [47060/64305], Loss: 4.5335\n",
      "Epoch [2/2], Step [47070/64305], Loss: 4.7859\n",
      "Epoch [2/2], Step [47080/64305], Loss: 4.7031\n",
      "Epoch [2/2], Step [47090/64305], Loss: 4.7381\n",
      "Epoch [2/2], Step [47100/64305], Loss: 4.8896\n",
      "Epoch [2/2], Step [47110/64305], Loss: 4.9360\n",
      "Epoch [2/2], Step [47120/64305], Loss: 4.9653\n",
      "Epoch [2/2], Step [47130/64305], Loss: 4.8876\n",
      "Epoch [2/2], Step [47140/64305], Loss: 4.9297\n",
      "Epoch [2/2], Step [47150/64305], Loss: 4.8251\n",
      "Epoch [2/2], Step [47160/64305], Loss: 4.9237\n",
      "Epoch [2/2], Step [47170/64305], Loss: 4.6815\n",
      "Epoch [2/2], Step [47180/64305], Loss: 4.8665\n",
      "Epoch [2/2], Step [47190/64305], Loss: 4.9161\n",
      "Epoch [2/2], Step [47200/64305], Loss: 4.8373\n",
      "Epoch [2/2], Step [47210/64305], Loss: 4.9161\n",
      "Epoch [2/2], Step [47220/64305], Loss: 4.7695\n",
      "Epoch [2/2], Step [47230/64305], Loss: 4.8359\n",
      "Epoch [2/2], Step [47240/64305], Loss: 4.6670\n",
      "Epoch [2/2], Step [47250/64305], Loss: 4.6597\n",
      "Epoch [2/2], Step [47260/64305], Loss: 4.7403\n",
      "Epoch [2/2], Step [47270/64305], Loss: 4.9717\n",
      "Epoch [2/2], Step [47280/64305], Loss: 5.0167\n",
      "Epoch [2/2], Step [47290/64305], Loss: 4.4934\n",
      "Epoch [2/2], Step [47300/64305], Loss: 4.7504\n",
      "Epoch [2/2], Step [47310/64305], Loss: 4.7911\n",
      "Epoch [2/2], Step [47320/64305], Loss: 4.9824\n",
      "Epoch [2/2], Step [47330/64305], Loss: 4.7896\n",
      "Epoch [2/2], Step [47340/64305], Loss: 4.9297\n",
      "Epoch [2/2], Step [47350/64305], Loss: 4.8149\n",
      "Epoch [2/2], Step [47360/64305], Loss: 4.8327\n",
      "Epoch [2/2], Step [47370/64305], Loss: 4.8953\n",
      "Epoch [2/2], Step [47380/64305], Loss: 4.7939\n",
      "Epoch [2/2], Step [47390/64305], Loss: 4.9682\n",
      "Epoch [2/2], Step [47400/64305], Loss: 4.8931\n",
      "Epoch [2/2], Step [47410/64305], Loss: 4.6906\n",
      "Epoch [2/2], Step [47420/64305], Loss: 4.8765\n",
      "Epoch [2/2], Step [47430/64305], Loss: 4.7703\n",
      "Epoch [2/2], Step [47440/64305], Loss: 4.9532\n",
      "Epoch [2/2], Step [47450/64305], Loss: 4.6313\n",
      "Epoch [2/2], Step [47460/64305], Loss: 4.8568\n",
      "Epoch [2/2], Step [47470/64305], Loss: 4.7579\n",
      "Epoch [2/2], Step [47480/64305], Loss: 4.9277\n",
      "Epoch [2/2], Step [47490/64305], Loss: 4.8392\n",
      "Epoch [2/2], Step [47500/64305], Loss: 4.5726\n",
      "Epoch [2/2], Step [47510/64305], Loss: 4.5809\n",
      "Epoch [2/2], Step [47520/64305], Loss: 4.7826\n",
      "Epoch [2/2], Step [47530/64305], Loss: 4.6453\n",
      "Epoch [2/2], Step [47540/64305], Loss: 4.5604\n",
      "Epoch [2/2], Step [47550/64305], Loss: 4.7123\n",
      "Epoch [2/2], Step [47560/64305], Loss: 4.8714\n",
      "Epoch [2/2], Step [47570/64305], Loss: 4.9769\n",
      "Epoch [2/2], Step [47580/64305], Loss: 4.7696\n",
      "Epoch [2/2], Step [47590/64305], Loss: 4.7822\n",
      "Epoch [2/2], Step [47600/64305], Loss: 4.7499\n",
      "Epoch [2/2], Step [47610/64305], Loss: 4.9355\n",
      "Epoch [2/2], Step [47620/64305], Loss: 4.8732\n",
      "Epoch [2/2], Step [47630/64305], Loss: 4.7316\n",
      "Epoch [2/2], Step [47640/64305], Loss: 4.7846\n",
      "Epoch [2/2], Step [47650/64305], Loss: 4.9209\n",
      "Epoch [2/2], Step [47660/64305], Loss: 4.7226\n",
      "Epoch [2/2], Step [47670/64305], Loss: 4.7752\n",
      "Epoch [2/2], Step [47680/64305], Loss: 4.7176\n",
      "Epoch [2/2], Step [47690/64305], Loss: 4.8027\n",
      "Epoch [2/2], Step [47700/64305], Loss: 4.7936\n",
      "Epoch [2/2], Step [47710/64305], Loss: 4.9127\n",
      "Epoch [2/2], Step [47720/64305], Loss: 4.9593\n",
      "Epoch [2/2], Step [47730/64305], Loss: 4.7821\n",
      "Epoch [2/2], Step [47740/64305], Loss: 4.6310\n",
      "Epoch [2/2], Step [47750/64305], Loss: 5.0795\n",
      "Epoch [2/2], Step [47760/64305], Loss: 4.7177\n",
      "Epoch [2/2], Step [47770/64305], Loss: 4.8854\n",
      "Epoch [2/2], Step [47780/64305], Loss: 4.6203\n",
      "Epoch [2/2], Step [47790/64305], Loss: 4.8484\n",
      "Epoch [2/2], Step [47800/64305], Loss: 4.8859\n",
      "Epoch [2/2], Step [47810/64305], Loss: 4.6560\n",
      "Epoch [2/2], Step [47820/64305], Loss: 4.8456\n",
      "Epoch [2/2], Step [47830/64305], Loss: 4.8548\n",
      "Epoch [2/2], Step [47840/64305], Loss: 4.9563\n",
      "Epoch [2/2], Step [47850/64305], Loss: 4.7789\n",
      "Epoch [2/2], Step [47860/64305], Loss: 4.7213\n",
      "Epoch [2/2], Step [47870/64305], Loss: 4.9857\n",
      "Epoch [2/2], Step [47880/64305], Loss: 4.7993\n",
      "Epoch [2/2], Step [47890/64305], Loss: 4.9946\n",
      "Epoch [2/2], Step [47900/64305], Loss: 4.8338\n",
      "Epoch [2/2], Step [47910/64305], Loss: 5.0588\n",
      "Epoch [2/2], Step [47920/64305], Loss: 4.7465\n",
      "Epoch [2/2], Step [47930/64305], Loss: 4.8169\n",
      "Epoch [2/2], Step [47940/64305], Loss: 4.8275\n",
      "Epoch [2/2], Step [47950/64305], Loss: 4.8794\n",
      "Epoch [2/2], Step [47960/64305], Loss: 4.6755\n",
      "Epoch [2/2], Step [47970/64305], Loss: 4.7663\n",
      "Epoch [2/2], Step [47980/64305], Loss: 4.7996\n",
      "Epoch [2/2], Step [47990/64305], Loss: 4.8470\n",
      "Epoch [2/2], Step [48000/64305], Loss: 4.7379\n",
      "Epoch [2/2], Step [48010/64305], Loss: 4.9242\n",
      "Epoch [2/2], Step [48020/64305], Loss: 4.8533\n",
      "Epoch [2/2], Step [48030/64305], Loss: 4.6895\n",
      "Epoch [2/2], Step [48040/64305], Loss: 4.6920\n",
      "Epoch [2/2], Step [48050/64305], Loss: 4.7493\n",
      "Epoch [2/2], Step [48060/64305], Loss: 4.9169\n",
      "Epoch [2/2], Step [48070/64305], Loss: 4.6654\n",
      "Epoch [2/2], Step [48080/64305], Loss: 4.8447\n",
      "Epoch [2/2], Step [48090/64305], Loss: 4.8129\n",
      "Epoch [2/2], Step [48100/64305], Loss: 4.9597\n",
      "Epoch [2/2], Step [48110/64305], Loss: 4.7420\n",
      "Epoch [2/2], Step [48120/64305], Loss: 4.8391\n",
      "Epoch [2/2], Step [48130/64305], Loss: 5.1009\n",
      "Epoch [2/2], Step [48140/64305], Loss: 4.8350\n",
      "Epoch [2/2], Step [48150/64305], Loss: 4.7401\n",
      "Epoch [2/2], Step [48160/64305], Loss: 4.8938\n",
      "Epoch [2/2], Step [48170/64305], Loss: 4.8277\n",
      "Epoch [2/2], Step [48180/64305], Loss: 4.7377\n",
      "Epoch [2/2], Step [48190/64305], Loss: 4.9307\n",
      "Epoch [2/2], Step [48200/64305], Loss: 4.7578\n",
      "Epoch [2/2], Step [48210/64305], Loss: 4.7649\n",
      "Epoch [2/2], Step [48220/64305], Loss: 4.7832\n",
      "Epoch [2/2], Step [48230/64305], Loss: 4.7566\n",
      "Epoch [2/2], Step [48240/64305], Loss: 4.9005\n",
      "Epoch [2/2], Step [48250/64305], Loss: 4.8841\n",
      "Epoch [2/2], Step [48260/64305], Loss: 4.5689\n",
      "Epoch [2/2], Step [48270/64305], Loss: 4.8608\n",
      "Epoch [2/2], Step [48280/64305], Loss: 4.7242\n",
      "Epoch [2/2], Step [48290/64305], Loss: 4.7066\n",
      "Epoch [2/2], Step [48300/64305], Loss: 4.6581\n",
      "Epoch [2/2], Step [48310/64305], Loss: 4.9048\n",
      "Epoch [2/2], Step [48320/64305], Loss: 5.0582\n",
      "Epoch [2/2], Step [48330/64305], Loss: 4.5877\n",
      "Epoch [2/2], Step [48340/64305], Loss: 5.0876\n",
      "Epoch [2/2], Step [48350/64305], Loss: 4.6594\n",
      "Epoch [2/2], Step [48360/64305], Loss: 4.8847\n",
      "Epoch [2/2], Step [48370/64305], Loss: 4.7140\n",
      "Epoch [2/2], Step [48380/64305], Loss: 4.6332\n",
      "Epoch [2/2], Step [48390/64305], Loss: 4.7180\n",
      "Epoch [2/2], Step [48400/64305], Loss: 4.8869\n",
      "Epoch [2/2], Step [48410/64305], Loss: 4.8834\n",
      "Epoch [2/2], Step [48420/64305], Loss: 4.5983\n",
      "Epoch [2/2], Step [48430/64305], Loss: 5.0154\n",
      "Epoch [2/2], Step [48440/64305], Loss: 4.8183\n",
      "Epoch [2/2], Step [48450/64305], Loss: 4.7045\n",
      "Epoch [2/2], Step [48460/64305], Loss: 4.6515\n",
      "Epoch [2/2], Step [48470/64305], Loss: 4.7302\n",
      "Epoch [2/2], Step [48480/64305], Loss: 4.8814\n",
      "Epoch [2/2], Step [48490/64305], Loss: 4.7101\n",
      "Epoch [2/2], Step [48500/64305], Loss: 4.6606\n",
      "Epoch [2/2], Step [48510/64305], Loss: 4.8582\n",
      "Epoch [2/2], Step [48520/64305], Loss: 4.8434\n",
      "Epoch [2/2], Step [48530/64305], Loss: 4.9637\n",
      "Epoch [2/2], Step [48540/64305], Loss: 4.8374\n",
      "Epoch [2/2], Step [48550/64305], Loss: 4.6822\n",
      "Epoch [2/2], Step [48560/64305], Loss: 4.9029\n",
      "Epoch [2/2], Step [48570/64305], Loss: 4.7811\n",
      "Epoch [2/2], Step [48580/64305], Loss: 5.0141\n",
      "Epoch [2/2], Step [48590/64305], Loss: 4.8050\n",
      "Epoch [2/2], Step [48600/64305], Loss: 4.9226\n",
      "Epoch [2/2], Step [48610/64305], Loss: 4.8054\n",
      "Epoch [2/2], Step [48620/64305], Loss: 4.6722\n",
      "Epoch [2/2], Step [48630/64305], Loss: 4.9953\n",
      "Epoch [2/2], Step [48640/64305], Loss: 4.4314\n",
      "Epoch [2/2], Step [48650/64305], Loss: 4.8159\n",
      "Epoch [2/2], Step [48660/64305], Loss: 5.0066\n",
      "Epoch [2/2], Step [48670/64305], Loss: 4.9564\n",
      "Epoch [2/2], Step [48680/64305], Loss: 4.8230\n",
      "Epoch [2/2], Step [48690/64305], Loss: 4.9729\n",
      "Epoch [2/2], Step [48700/64305], Loss: 4.6335\n",
      "Epoch [2/2], Step [48710/64305], Loss: 4.5937\n",
      "Epoch [2/2], Step [48720/64305], Loss: 5.0396\n",
      "Epoch [2/2], Step [48730/64305], Loss: 4.6697\n",
      "Epoch [2/2], Step [48740/64305], Loss: 4.6254\n",
      "Epoch [2/2], Step [48750/64305], Loss: 4.7780\n",
      "Epoch [2/2], Step [48760/64305], Loss: 4.8038\n",
      "Epoch [2/2], Step [48770/64305], Loss: 4.7847\n",
      "Epoch [2/2], Step [48780/64305], Loss: 4.8141\n",
      "Epoch [2/2], Step [48790/64305], Loss: 4.8929\n",
      "Epoch [2/2], Step [48800/64305], Loss: 4.7643\n",
      "Epoch [2/2], Step [48810/64305], Loss: 4.8707\n",
      "Epoch [2/2], Step [48820/64305], Loss: 4.9082\n",
      "Epoch [2/2], Step [48830/64305], Loss: 4.8548\n",
      "Epoch [2/2], Step [48840/64305], Loss: 4.6455\n",
      "Epoch [2/2], Step [48850/64305], Loss: 4.8253\n",
      "Epoch [2/2], Step [48860/64305], Loss: 4.7560\n",
      "Epoch [2/2], Step [48870/64305], Loss: 4.6838\n",
      "Epoch [2/2], Step [48880/64305], Loss: 4.7059\n",
      "Epoch [2/2], Step [48890/64305], Loss: 4.9979\n",
      "Epoch [2/2], Step [48900/64305], Loss: 4.7488\n",
      "Epoch [2/2], Step [48910/64305], Loss: 4.8140\n",
      "Epoch [2/2], Step [48920/64305], Loss: 4.7542\n",
      "Epoch [2/2], Step [48930/64305], Loss: 4.8217\n",
      "Epoch [2/2], Step [48940/64305], Loss: 4.8077\n",
      "Epoch [2/2], Step [48950/64305], Loss: 4.8569\n",
      "Epoch [2/2], Step [48960/64305], Loss: 4.9214\n",
      "Epoch [2/2], Step [48970/64305], Loss: 4.8811\n",
      "Epoch [2/2], Step [48980/64305], Loss: 5.0090\n",
      "Epoch [2/2], Step [48990/64305], Loss: 4.8731\n",
      "Epoch [2/2], Step [49000/64305], Loss: 4.8079\n",
      "Epoch [2/2], Step [49010/64305], Loss: 4.8674\n",
      "Epoch [2/2], Step [49020/64305], Loss: 4.9551\n",
      "Epoch [2/2], Step [49030/64305], Loss: 4.9311\n",
      "Epoch [2/2], Step [49040/64305], Loss: 4.6469\n",
      "Epoch [2/2], Step [49050/64305], Loss: 4.6563\n",
      "Epoch [2/2], Step [49060/64305], Loss: 4.9574\n",
      "Epoch [2/2], Step [49070/64305], Loss: 4.7447\n",
      "Epoch [2/2], Step [49080/64305], Loss: 4.8577\n",
      "Epoch [2/2], Step [49090/64305], Loss: 4.8810\n",
      "Epoch [2/2], Step [49100/64305], Loss: 4.7461\n",
      "Epoch [2/2], Step [49110/64305], Loss: 4.8596\n",
      "Epoch [2/2], Step [49120/64305], Loss: 5.0535\n",
      "Epoch [2/2], Step [49130/64305], Loss: 4.9829\n",
      "Epoch [2/2], Step [49140/64305], Loss: 4.6701\n",
      "Epoch [2/2], Step [49150/64305], Loss: 4.9672\n",
      "Epoch [2/2], Step [49160/64305], Loss: 4.8874\n",
      "Epoch [2/2], Step [49170/64305], Loss: 4.7902\n",
      "Epoch [2/2], Step [49180/64305], Loss: 4.8412\n",
      "Epoch [2/2], Step [49190/64305], Loss: 4.8438\n",
      "Epoch [2/2], Step [49200/64305], Loss: 4.8290\n",
      "Epoch [2/2], Step [49210/64305], Loss: 4.6527\n",
      "Epoch [2/2], Step [49220/64305], Loss: 4.8846\n",
      "Epoch [2/2], Step [49230/64305], Loss: 4.7486\n",
      "Epoch [2/2], Step [49240/64305], Loss: 4.8753\n",
      "Epoch [2/2], Step [49250/64305], Loss: 4.9056\n",
      "Epoch [2/2], Step [49260/64305], Loss: 4.8247\n",
      "Epoch [2/2], Step [49270/64305], Loss: 4.6152\n",
      "Epoch [2/2], Step [49280/64305], Loss: 4.7722\n",
      "Epoch [2/2], Step [49290/64305], Loss: 4.8878\n",
      "Epoch [2/2], Step [49300/64305], Loss: 4.8485\n",
      "Epoch [2/2], Step [49310/64305], Loss: 4.7375\n",
      "Epoch [2/2], Step [49320/64305], Loss: 4.8873\n",
      "Epoch [2/2], Step [49330/64305], Loss: 4.7351\n",
      "Epoch [2/2], Step [49340/64305], Loss: 4.8058\n",
      "Epoch [2/2], Step [49350/64305], Loss: 4.6343\n",
      "Epoch [2/2], Step [49360/64305], Loss: 5.0037\n",
      "Epoch [2/2], Step [49370/64305], Loss: 4.8568\n",
      "Epoch [2/2], Step [49380/64305], Loss: 4.6628\n",
      "Epoch [2/2], Step [49390/64305], Loss: 4.8071\n",
      "Epoch [2/2], Step [49400/64305], Loss: 4.7047\n",
      "Epoch [2/2], Step [49410/64305], Loss: 4.6896\n",
      "Epoch [2/2], Step [49420/64305], Loss: 5.0572\n",
      "Epoch [2/2], Step [49430/64305], Loss: 4.6363\n",
      "Epoch [2/2], Step [49440/64305], Loss: 4.8839\n",
      "Epoch [2/2], Step [49450/64305], Loss: 4.7358\n",
      "Epoch [2/2], Step [49460/64305], Loss: 4.8579\n",
      "Epoch [2/2], Step [49470/64305], Loss: 5.1264\n",
      "Epoch [2/2], Step [49480/64305], Loss: 4.8946\n",
      "Epoch [2/2], Step [49490/64305], Loss: 4.6932\n",
      "Epoch [2/2], Step [49500/64305], Loss: 4.6706\n",
      "Epoch [2/2], Step [49510/64305], Loss: 4.6970\n",
      "Epoch [2/2], Step [49520/64305], Loss: 4.8517\n",
      "Epoch [2/2], Step [49530/64305], Loss: 5.0118\n",
      "Epoch [2/2], Step [49540/64305], Loss: 4.6723\n",
      "Epoch [2/2], Step [49550/64305], Loss: 4.7556\n",
      "Epoch [2/2], Step [49560/64305], Loss: 4.8490\n",
      "Epoch [2/2], Step [49570/64305], Loss: 5.0181\n",
      "Epoch [2/2], Step [49580/64305], Loss: 4.8815\n",
      "Epoch [2/2], Step [49590/64305], Loss: 4.9929\n",
      "Epoch [2/2], Step [49600/64305], Loss: 4.8355\n",
      "Epoch [2/2], Step [49610/64305], Loss: 4.8561\n",
      "Epoch [2/2], Step [49620/64305], Loss: 4.7229\n",
      "Epoch [2/2], Step [49630/64305], Loss: 4.7846\n",
      "Epoch [2/2], Step [49640/64305], Loss: 4.9193\n",
      "Epoch [2/2], Step [49650/64305], Loss: 4.6897\n",
      "Epoch [2/2], Step [49660/64305], Loss: 4.8269\n",
      "Epoch [2/2], Step [49670/64305], Loss: 4.8864\n",
      "Epoch [2/2], Step [49680/64305], Loss: 4.7839\n",
      "Epoch [2/2], Step [49690/64305], Loss: 4.7932\n",
      "Epoch [2/2], Step [49700/64305], Loss: 4.8083\n",
      "Epoch [2/2], Step [49710/64305], Loss: 4.8697\n",
      "Epoch [2/2], Step [49720/64305], Loss: 4.8183\n",
      "Epoch [2/2], Step [49730/64305], Loss: 4.6909\n",
      "Epoch [2/2], Step [49740/64305], Loss: 4.7841\n",
      "Epoch [2/2], Step [49750/64305], Loss: 4.6151\n",
      "Epoch [2/2], Step [49760/64305], Loss: 4.7715\n",
      "Epoch [2/2], Step [49770/64305], Loss: 4.8659\n",
      "Epoch [2/2], Step [49780/64305], Loss: 4.7108\n",
      "Epoch [2/2], Step [49790/64305], Loss: 4.8842\n",
      "Epoch [2/2], Step [49800/64305], Loss: 4.8562\n",
      "Epoch [2/2], Step [49810/64305], Loss: 4.8375\n",
      "Epoch [2/2], Step [49820/64305], Loss: 4.9864\n",
      "Epoch [2/2], Step [49830/64305], Loss: 4.9455\n",
      "Epoch [2/2], Step [49840/64305], Loss: 4.7344\n",
      "Epoch [2/2], Step [49850/64305], Loss: 4.8826\n",
      "Epoch [2/2], Step [49860/64305], Loss: 4.9738\n",
      "Epoch [2/2], Step [49870/64305], Loss: 4.8516\n",
      "Epoch [2/2], Step [49880/64305], Loss: 4.8287\n",
      "Epoch [2/2], Step [49890/64305], Loss: 4.7784\n",
      "Epoch [2/2], Step [49900/64305], Loss: 4.9282\n",
      "Epoch [2/2], Step [49910/64305], Loss: 4.9791\n",
      "Epoch [2/2], Step [49920/64305], Loss: 4.7732\n",
      "Epoch [2/2], Step [49930/64305], Loss: 4.8117\n",
      "Epoch [2/2], Step [49940/64305], Loss: 4.9467\n",
      "Epoch [2/2], Step [49950/64305], Loss: 4.8841\n",
      "Epoch [2/2], Step [49960/64305], Loss: 5.0325\n",
      "Epoch [2/2], Step [49970/64305], Loss: 4.7417\n",
      "Epoch [2/2], Step [49980/64305], Loss: 4.8090\n",
      "Epoch [2/2], Step [49990/64305], Loss: 4.8540\n",
      "Epoch [2/2], Step [50000/64305], Loss: 4.9006\n",
      "Epoch [2/2], Step [50010/64305], Loss: 4.6456\n",
      "Epoch [2/2], Step [50020/64305], Loss: 4.8477\n",
      "Epoch [2/2], Step [50030/64305], Loss: 4.7943\n",
      "Epoch [2/2], Step [50040/64305], Loss: 4.8322\n",
      "Epoch [2/2], Step [50050/64305], Loss: 4.8038\n",
      "Epoch [2/2], Step [50060/64305], Loss: 4.6757\n",
      "Epoch [2/2], Step [50070/64305], Loss: 5.0174\n",
      "Epoch [2/2], Step [50080/64305], Loss: 4.9042\n",
      "Epoch [2/2], Step [50090/64305], Loss: 5.0109\n",
      "Epoch [2/2], Step [50100/64305], Loss: 4.7349\n",
      "Epoch [2/2], Step [50110/64305], Loss: 4.8245\n",
      "Epoch [2/2], Step [50120/64305], Loss: 4.7478\n",
      "Epoch [2/2], Step [50130/64305], Loss: 5.0048\n",
      "Epoch [2/2], Step [50140/64305], Loss: 4.8593\n",
      "Epoch [2/2], Step [50150/64305], Loss: 4.6742\n",
      "Epoch [2/2], Step [50160/64305], Loss: 4.6388\n",
      "Epoch [2/2], Step [50170/64305], Loss: 4.9440\n",
      "Epoch [2/2], Step [50180/64305], Loss: 4.8431\n",
      "Epoch [2/2], Step [50190/64305], Loss: 4.9186\n",
      "Epoch [2/2], Step [50200/64305], Loss: 4.8719\n",
      "Epoch [2/2], Step [50210/64305], Loss: 4.9518\n",
      "Epoch [2/2], Step [50220/64305], Loss: 4.9585\n",
      "Epoch [2/2], Step [50230/64305], Loss: 4.7951\n",
      "Epoch [2/2], Step [50240/64305], Loss: 4.7717\n",
      "Epoch [2/2], Step [50250/64305], Loss: 4.6489\n",
      "Epoch [2/2], Step [50260/64305], Loss: 4.7636\n",
      "Epoch [2/2], Step [50270/64305], Loss: 4.8761\n",
      "Epoch [2/2], Step [50280/64305], Loss: 4.8316\n",
      "Epoch [2/2], Step [50290/64305], Loss: 4.7349\n",
      "Epoch [2/2], Step [50300/64305], Loss: 4.7134\n",
      "Epoch [2/2], Step [50310/64305], Loss: 4.7323\n",
      "Epoch [2/2], Step [50320/64305], Loss: 5.0867\n",
      "Epoch [2/2], Step [50330/64305], Loss: 5.0072\n",
      "Epoch [2/2], Step [50340/64305], Loss: 4.9705\n",
      "Epoch [2/2], Step [50350/64305], Loss: 4.7600\n",
      "Epoch [2/2], Step [50360/64305], Loss: 4.6615\n",
      "Epoch [2/2], Step [50370/64305], Loss: 4.8843\n",
      "Epoch [2/2], Step [50380/64305], Loss: 5.0120\n",
      "Epoch [2/2], Step [50390/64305], Loss: 4.9145\n",
      "Epoch [2/2], Step [50400/64305], Loss: 4.9734\n",
      "Epoch [2/2], Step [50410/64305], Loss: 4.9264\n",
      "Epoch [2/2], Step [50420/64305], Loss: 4.8436\n",
      "Epoch [2/2], Step [50430/64305], Loss: 5.2267\n",
      "Epoch [2/2], Step [50440/64305], Loss: 4.8826\n",
      "Epoch [2/2], Step [50450/64305], Loss: 4.6910\n",
      "Epoch [2/2], Step [50460/64305], Loss: 4.7246\n",
      "Epoch [2/2], Step [50470/64305], Loss: 4.8488\n",
      "Epoch [2/2], Step [50480/64305], Loss: 4.7503\n",
      "Epoch [2/2], Step [50490/64305], Loss: 5.0107\n",
      "Epoch [2/2], Step [50500/64305], Loss: 4.7947\n",
      "Epoch [2/2], Step [50510/64305], Loss: 4.8279\n",
      "Epoch [2/2], Step [50520/64305], Loss: 5.1238\n",
      "Epoch [2/2], Step [50530/64305], Loss: 4.8845\n",
      "Epoch [2/2], Step [50540/64305], Loss: 4.7900\n",
      "Epoch [2/2], Step [50550/64305], Loss: 4.8743\n",
      "Epoch [2/2], Step [50560/64305], Loss: 4.8553\n",
      "Epoch [2/2], Step [50570/64305], Loss: 4.7579\n",
      "Epoch [2/2], Step [50580/64305], Loss: 4.8050\n",
      "Epoch [2/2], Step [50590/64305], Loss: 4.7998\n",
      "Epoch [2/2], Step [50600/64305], Loss: 4.7626\n",
      "Epoch [2/2], Step [50610/64305], Loss: 4.6467\n",
      "Epoch [2/2], Step [50620/64305], Loss: 4.4859\n",
      "Epoch [2/2], Step [50630/64305], Loss: 4.9500\n",
      "Epoch [2/2], Step [50640/64305], Loss: 4.6264\n",
      "Epoch [2/2], Step [50650/64305], Loss: 5.0307\n",
      "Epoch [2/2], Step [50660/64305], Loss: 4.7269\n",
      "Epoch [2/2], Step [50670/64305], Loss: 4.8060\n",
      "Epoch [2/2], Step [50680/64305], Loss: 4.7814\n",
      "Epoch [2/2], Step [50690/64305], Loss: 4.7207\n",
      "Epoch [2/2], Step [50700/64305], Loss: 5.0740\n",
      "Epoch [2/2], Step [50710/64305], Loss: 4.5933\n",
      "Epoch [2/2], Step [50720/64305], Loss: 4.7663\n",
      "Epoch [2/2], Step [50730/64305], Loss: 4.9058\n",
      "Epoch [2/2], Step [50740/64305], Loss: 4.6823\n",
      "Epoch [2/2], Step [50750/64305], Loss: 4.8677\n",
      "Epoch [2/2], Step [50760/64305], Loss: 4.7729\n",
      "Epoch [2/2], Step [50770/64305], Loss: 4.8488\n",
      "Epoch [2/2], Step [50780/64305], Loss: 4.9144\n",
      "Epoch [2/2], Step [50790/64305], Loss: 4.8085\n",
      "Epoch [2/2], Step [50800/64305], Loss: 4.7852\n",
      "Epoch [2/2], Step [50810/64305], Loss: 4.9027\n",
      "Epoch [2/2], Step [50820/64305], Loss: 4.8904\n",
      "Epoch [2/2], Step [50830/64305], Loss: 4.6691\n",
      "Epoch [2/2], Step [50840/64305], Loss: 4.8388\n",
      "Epoch [2/2], Step [50850/64305], Loss: 4.7086\n",
      "Epoch [2/2], Step [50860/64305], Loss: 4.8441\n",
      "Epoch [2/2], Step [50870/64305], Loss: 4.5976\n",
      "Epoch [2/2], Step [50880/64305], Loss: 4.7193\n",
      "Epoch [2/2], Step [50890/64305], Loss: 4.8084\n",
      "Epoch [2/2], Step [50900/64305], Loss: 4.8189\n",
      "Epoch [2/2], Step [50910/64305], Loss: 4.9399\n",
      "Epoch [2/2], Step [50920/64305], Loss: 5.0307\n",
      "Epoch [2/2], Step [50930/64305], Loss: 4.8806\n",
      "Epoch [2/2], Step [50940/64305], Loss: 4.7918\n",
      "Epoch [2/2], Step [50950/64305], Loss: 5.0048\n",
      "Epoch [2/2], Step [50960/64305], Loss: 4.8394\n",
      "Epoch [2/2], Step [50970/64305], Loss: 4.7277\n",
      "Epoch [2/2], Step [50980/64305], Loss: 4.5904\n",
      "Epoch [2/2], Step [50990/64305], Loss: 4.8869\n",
      "Epoch [2/2], Step [51000/64305], Loss: 4.8365\n",
      "Epoch [2/2], Step [51010/64305], Loss: 4.7322\n",
      "Epoch [2/2], Step [51020/64305], Loss: 4.8721\n",
      "Epoch [2/2], Step [51030/64305], Loss: 4.8075\n",
      "Epoch [2/2], Step [51040/64305], Loss: 4.8087\n",
      "Epoch [2/2], Step [51050/64305], Loss: 4.7907\n",
      "Epoch [2/2], Step [51060/64305], Loss: 4.9875\n",
      "Epoch [2/2], Step [51070/64305], Loss: 4.9596\n",
      "Epoch [2/2], Step [51080/64305], Loss: 4.8131\n",
      "Epoch [2/2], Step [51090/64305], Loss: 4.8568\n",
      "Epoch [2/2], Step [51100/64305], Loss: 4.7105\n",
      "Epoch [2/2], Step [51110/64305], Loss: 4.8971\n",
      "Epoch [2/2], Step [51120/64305], Loss: 4.8176\n",
      "Epoch [2/2], Step [51130/64305], Loss: 4.8876\n",
      "Epoch [2/2], Step [51140/64305], Loss: 4.9128\n",
      "Epoch [2/2], Step [51150/64305], Loss: 4.8451\n",
      "Epoch [2/2], Step [51160/64305], Loss: 5.0343\n",
      "Epoch [2/2], Step [51170/64305], Loss: 4.9527\n",
      "Epoch [2/2], Step [51180/64305], Loss: 4.8206\n",
      "Epoch [2/2], Step [51190/64305], Loss: 4.8363\n",
      "Epoch [2/2], Step [51200/64305], Loss: 4.8534\n",
      "Epoch [2/2], Step [51210/64305], Loss: 4.8861\n",
      "Epoch [2/2], Step [51220/64305], Loss: 4.9901\n",
      "Epoch [2/2], Step [51230/64305], Loss: 4.6796\n",
      "Epoch [2/2], Step [51240/64305], Loss: 4.7163\n",
      "Epoch [2/2], Step [51250/64305], Loss: 4.7970\n",
      "Epoch [2/2], Step [51260/64305], Loss: 4.7664\n",
      "Epoch [2/2], Step [51270/64305], Loss: 4.8586\n",
      "Epoch [2/2], Step [51280/64305], Loss: 4.8162\n",
      "Epoch [2/2], Step [51290/64305], Loss: 4.8853\n",
      "Epoch [2/2], Step [51300/64305], Loss: 4.8696\n",
      "Epoch [2/2], Step [51310/64305], Loss: 4.8352\n",
      "Epoch [2/2], Step [51320/64305], Loss: 4.6963\n",
      "Epoch [2/2], Step [51330/64305], Loss: 4.5820\n",
      "Epoch [2/2], Step [51340/64305], Loss: 4.9598\n",
      "Epoch [2/2], Step [51350/64305], Loss: 4.8993\n",
      "Epoch [2/2], Step [51360/64305], Loss: 4.7759\n",
      "Epoch [2/2], Step [51370/64305], Loss: 5.0316\n",
      "Epoch [2/2], Step [51380/64305], Loss: 4.6212\n",
      "Epoch [2/2], Step [51390/64305], Loss: 4.6391\n",
      "Epoch [2/2], Step [51400/64305], Loss: 4.9145\n",
      "Epoch [2/2], Step [51410/64305], Loss: 4.6574\n",
      "Epoch [2/2], Step [51420/64305], Loss: 4.7019\n",
      "Epoch [2/2], Step [51430/64305], Loss: 4.7347\n",
      "Epoch [2/2], Step [51440/64305], Loss: 4.9311\n",
      "Epoch [2/2], Step [51450/64305], Loss: 4.6912\n",
      "Epoch [2/2], Step [51460/64305], Loss: 4.6297\n",
      "Epoch [2/2], Step [51470/64305], Loss: 5.0122\n",
      "Epoch [2/2], Step [51480/64305], Loss: 5.0343\n",
      "Epoch [2/2], Step [51490/64305], Loss: 4.6644\n",
      "Epoch [2/2], Step [51500/64305], Loss: 4.9501\n",
      "Epoch [2/2], Step [51510/64305], Loss: 4.6883\n",
      "Epoch [2/2], Step [51520/64305], Loss: 4.7376\n",
      "Epoch [2/2], Step [51530/64305], Loss: 4.9469\n",
      "Epoch [2/2], Step [51540/64305], Loss: 4.8825\n",
      "Epoch [2/2], Step [51550/64305], Loss: 4.8608\n",
      "Epoch [2/2], Step [51560/64305], Loss: 4.7454\n",
      "Epoch [2/2], Step [51570/64305], Loss: 4.9279\n",
      "Epoch [2/2], Step [51580/64305], Loss: 4.9653\n",
      "Epoch [2/2], Step [51590/64305], Loss: 4.6552\n",
      "Epoch [2/2], Step [51600/64305], Loss: 4.5659\n",
      "Epoch [2/2], Step [51610/64305], Loss: 4.8330\n",
      "Epoch [2/2], Step [51620/64305], Loss: 5.1086\n",
      "Epoch [2/2], Step [51630/64305], Loss: 5.0359\n",
      "Epoch [2/2], Step [51640/64305], Loss: 4.9926\n",
      "Epoch [2/2], Step [51650/64305], Loss: 4.7785\n",
      "Epoch [2/2], Step [51660/64305], Loss: 4.8056\n",
      "Epoch [2/2], Step [51670/64305], Loss: 4.8448\n",
      "Epoch [2/2], Step [51680/64305], Loss: 4.7198\n",
      "Epoch [2/2], Step [51690/64305], Loss: 4.6033\n",
      "Epoch [2/2], Step [51700/64305], Loss: 4.9474\n",
      "Epoch [2/2], Step [51710/64305], Loss: 4.8505\n",
      "Epoch [2/2], Step [51720/64305], Loss: 4.7964\n",
      "Epoch [2/2], Step [51730/64305], Loss: 4.7734\n",
      "Epoch [2/2], Step [51740/64305], Loss: 4.8201\n",
      "Epoch [2/2], Step [51750/64305], Loss: 4.9306\n",
      "Epoch [2/2], Step [51760/64305], Loss: 4.7397\n",
      "Epoch [2/2], Step [51770/64305], Loss: 4.7799\n",
      "Epoch [2/2], Step [51780/64305], Loss: 4.7966\n",
      "Epoch [2/2], Step [51790/64305], Loss: 4.6722\n",
      "Epoch [2/2], Step [51800/64305], Loss: 5.0231\n",
      "Epoch [2/2], Step [51810/64305], Loss: 4.6949\n",
      "Epoch [2/2], Step [51820/64305], Loss: 4.9495\n",
      "Epoch [2/2], Step [51830/64305], Loss: 4.7650\n",
      "Epoch [2/2], Step [51840/64305], Loss: 4.9569\n",
      "Epoch [2/2], Step [51850/64305], Loss: 4.7254\n",
      "Epoch [2/2], Step [51860/64305], Loss: 4.8053\n",
      "Epoch [2/2], Step [51870/64305], Loss: 4.8966\n",
      "Epoch [2/2], Step [51880/64305], Loss: 4.8385\n",
      "Epoch [2/2], Step [51890/64305], Loss: 5.0089\n",
      "Epoch [2/2], Step [51900/64305], Loss: 4.7192\n",
      "Epoch [2/2], Step [51910/64305], Loss: 4.5981\n",
      "Epoch [2/2], Step [51920/64305], Loss: 4.8952\n",
      "Epoch [2/2], Step [51930/64305], Loss: 4.7822\n",
      "Epoch [2/2], Step [51940/64305], Loss: 4.8686\n",
      "Epoch [2/2], Step [51950/64305], Loss: 4.8567\n",
      "Epoch [2/2], Step [51960/64305], Loss: 4.9681\n",
      "Epoch [2/2], Step [51970/64305], Loss: 4.6146\n",
      "Epoch [2/2], Step [51980/64305], Loss: 4.8306\n",
      "Epoch [2/2], Step [51990/64305], Loss: 4.7771\n",
      "Epoch [2/2], Step [52000/64305], Loss: 4.9865\n",
      "Epoch [2/2], Step [52010/64305], Loss: 4.6673\n",
      "Epoch [2/2], Step [52020/64305], Loss: 4.8178\n",
      "Epoch [2/2], Step [52030/64305], Loss: 4.8798\n",
      "Epoch [2/2], Step [52040/64305], Loss: 4.9099\n",
      "Epoch [2/2], Step [52050/64305], Loss: 4.7207\n",
      "Epoch [2/2], Step [52060/64305], Loss: 4.8650\n",
      "Epoch [2/2], Step [52070/64305], Loss: 4.7741\n",
      "Epoch [2/2], Step [52080/64305], Loss: 4.7799\n",
      "Epoch [2/2], Step [52090/64305], Loss: 4.8487\n",
      "Epoch [2/2], Step [52100/64305], Loss: 4.9251\n",
      "Epoch [2/2], Step [52110/64305], Loss: 4.8494\n",
      "Epoch [2/2], Step [52120/64305], Loss: 4.7193\n",
      "Epoch [2/2], Step [52130/64305], Loss: 4.9200\n",
      "Epoch [2/2], Step [52140/64305], Loss: 4.7766\n",
      "Epoch [2/2], Step [52150/64305], Loss: 4.9917\n",
      "Epoch [2/2], Step [52160/64305], Loss: 4.9126\n",
      "Epoch [2/2], Step [52170/64305], Loss: 4.8923\n",
      "Epoch [2/2], Step [52180/64305], Loss: 4.8033\n",
      "Epoch [2/2], Step [52190/64305], Loss: 4.8043\n",
      "Epoch [2/2], Step [52200/64305], Loss: 5.0414\n",
      "Epoch [2/2], Step [52210/64305], Loss: 4.6584\n",
      "Epoch [2/2], Step [52220/64305], Loss: 4.7914\n",
      "Epoch [2/2], Step [52230/64305], Loss: 5.0336\n",
      "Epoch [2/2], Step [52240/64305], Loss: 4.6921\n",
      "Epoch [2/2], Step [52250/64305], Loss: 4.7770\n",
      "Epoch [2/2], Step [52260/64305], Loss: 4.8253\n",
      "Epoch [2/2], Step [52270/64305], Loss: 4.7441\n",
      "Epoch [2/2], Step [52280/64305], Loss: 4.7580\n",
      "Epoch [2/2], Step [52290/64305], Loss: 4.8112\n",
      "Epoch [2/2], Step [52300/64305], Loss: 4.7606\n",
      "Epoch [2/2], Step [52310/64305], Loss: 4.7138\n",
      "Epoch [2/2], Step [52320/64305], Loss: 4.8394\n",
      "Epoch [2/2], Step [52330/64305], Loss: 4.7302\n",
      "Epoch [2/2], Step [52340/64305], Loss: 4.8258\n",
      "Epoch [2/2], Step [52350/64305], Loss: 4.7184\n",
      "Epoch [2/2], Step [52360/64305], Loss: 4.7559\n",
      "Epoch [2/2], Step [52370/64305], Loss: 5.1575\n",
      "Epoch [2/2], Step [52380/64305], Loss: 4.7726\n",
      "Epoch [2/2], Step [52390/64305], Loss: 4.9693\n",
      "Epoch [2/2], Step [52400/64305], Loss: 4.6468\n",
      "Epoch [2/2], Step [52410/64305], Loss: 4.7007\n",
      "Epoch [2/2], Step [52420/64305], Loss: 4.8936\n",
      "Epoch [2/2], Step [52430/64305], Loss: 4.5197\n",
      "Epoch [2/2], Step [52440/64305], Loss: 4.8438\n",
      "Epoch [2/2], Step [52450/64305], Loss: 4.6525\n",
      "Epoch [2/2], Step [52460/64305], Loss: 4.8689\n",
      "Epoch [2/2], Step [52470/64305], Loss: 4.7679\n",
      "Epoch [2/2], Step [52480/64305], Loss: 4.6799\n",
      "Epoch [2/2], Step [52490/64305], Loss: 4.9153\n",
      "Epoch [2/2], Step [52500/64305], Loss: 4.7292\n",
      "Epoch [2/2], Step [52510/64305], Loss: 4.8951\n",
      "Epoch [2/2], Step [52520/64305], Loss: 4.6898\n",
      "Epoch [2/2], Step [52530/64305], Loss: 4.9430\n",
      "Epoch [2/2], Step [52540/64305], Loss: 4.6929\n",
      "Epoch [2/2], Step [52550/64305], Loss: 4.7029\n",
      "Epoch [2/2], Step [52560/64305], Loss: 4.7351\n",
      "Epoch [2/2], Step [52570/64305], Loss: 4.7106\n",
      "Epoch [2/2], Step [52580/64305], Loss: 4.8683\n",
      "Epoch [2/2], Step [52590/64305], Loss: 4.9819\n",
      "Epoch [2/2], Step [52600/64305], Loss: 4.9004\n",
      "Epoch [2/2], Step [52610/64305], Loss: 4.7826\n",
      "Epoch [2/2], Step [52620/64305], Loss: 4.8347\n",
      "Epoch [2/2], Step [52630/64305], Loss: 4.8406\n",
      "Epoch [2/2], Step [52640/64305], Loss: 4.8317\n",
      "Epoch [2/2], Step [52650/64305], Loss: 4.9046\n",
      "Epoch [2/2], Step [52660/64305], Loss: 4.9551\n",
      "Epoch [2/2], Step [52670/64305], Loss: 4.7421\n",
      "Epoch [2/2], Step [52680/64305], Loss: 4.6818\n",
      "Epoch [2/2], Step [52690/64305], Loss: 4.7558\n",
      "Epoch [2/2], Step [52700/64305], Loss: 4.7013\n",
      "Epoch [2/2], Step [52710/64305], Loss: 4.8874\n",
      "Epoch [2/2], Step [52720/64305], Loss: 4.8634\n",
      "Epoch [2/2], Step [52730/64305], Loss: 4.7723\n",
      "Epoch [2/2], Step [52740/64305], Loss: 4.9474\n",
      "Epoch [2/2], Step [52750/64305], Loss: 4.9751\n",
      "Epoch [2/2], Step [52760/64305], Loss: 4.7769\n",
      "Epoch [2/2], Step [52770/64305], Loss: 4.8556\n",
      "Epoch [2/2], Step [52780/64305], Loss: 4.6675\n",
      "Epoch [2/2], Step [52790/64305], Loss: 4.8565\n",
      "Epoch [2/2], Step [52800/64305], Loss: 4.7975\n",
      "Epoch [2/2], Step [52810/64305], Loss: 4.7013\n",
      "Epoch [2/2], Step [52820/64305], Loss: 4.8777\n",
      "Epoch [2/2], Step [52830/64305], Loss: 4.6885\n",
      "Epoch [2/2], Step [52840/64305], Loss: 4.7445\n",
      "Epoch [2/2], Step [52850/64305], Loss: 4.7481\n",
      "Epoch [2/2], Step [52860/64305], Loss: 4.9977\n",
      "Epoch [2/2], Step [52870/64305], Loss: 4.9747\n",
      "Epoch [2/2], Step [52880/64305], Loss: 4.9097\n",
      "Epoch [2/2], Step [52890/64305], Loss: 4.9880\n",
      "Epoch [2/2], Step [52900/64305], Loss: 4.8835\n",
      "Epoch [2/2], Step [52910/64305], Loss: 5.0494\n",
      "Epoch [2/2], Step [52920/64305], Loss: 4.6449\n",
      "Epoch [2/2], Step [52930/64305], Loss: 4.8444\n",
      "Epoch [2/2], Step [52940/64305], Loss: 4.8772\n",
      "Epoch [2/2], Step [52950/64305], Loss: 5.0410\n",
      "Epoch [2/2], Step [52960/64305], Loss: 4.8417\n",
      "Epoch [2/2], Step [52970/64305], Loss: 5.0328\n",
      "Epoch [2/2], Step [52980/64305], Loss: 4.7202\n",
      "Epoch [2/2], Step [52990/64305], Loss: 4.7741\n",
      "Epoch [2/2], Step [53000/64305], Loss: 5.0591\n",
      "Epoch [2/2], Step [53010/64305], Loss: 4.9218\n",
      "Epoch [2/2], Step [53020/64305], Loss: 4.7820\n",
      "Epoch [2/2], Step [53030/64305], Loss: 4.7871\n",
      "Epoch [2/2], Step [53040/64305], Loss: 4.7831\n",
      "Epoch [2/2], Step [53050/64305], Loss: 4.7807\n",
      "Epoch [2/2], Step [53060/64305], Loss: 4.7397\n",
      "Epoch [2/2], Step [53070/64305], Loss: 4.8885\n",
      "Epoch [2/2], Step [53080/64305], Loss: 4.6731\n",
      "Epoch [2/2], Step [53090/64305], Loss: 4.7419\n",
      "Epoch [2/2], Step [53100/64305], Loss: 4.6805\n",
      "Epoch [2/2], Step [53110/64305], Loss: 4.9113\n",
      "Epoch [2/2], Step [53120/64305], Loss: 4.6747\n",
      "Epoch [2/2], Step [53130/64305], Loss: 5.1387\n",
      "Epoch [2/2], Step [53140/64305], Loss: 4.8363\n",
      "Epoch [2/2], Step [53150/64305], Loss: 4.9303\n",
      "Epoch [2/2], Step [53160/64305], Loss: 4.9531\n",
      "Epoch [2/2], Step [53170/64305], Loss: 4.8623\n",
      "Epoch [2/2], Step [53180/64305], Loss: 5.0132\n",
      "Epoch [2/2], Step [53190/64305], Loss: 4.5917\n",
      "Epoch [2/2], Step [53200/64305], Loss: 4.6762\n",
      "Epoch [2/2], Step [53210/64305], Loss: 5.0599\n",
      "Epoch [2/2], Step [53220/64305], Loss: 4.7259\n",
      "Epoch [2/2], Step [53230/64305], Loss: 4.8868\n",
      "Epoch [2/2], Step [53240/64305], Loss: 4.8760\n",
      "Epoch [2/2], Step [53250/64305], Loss: 4.5810\n",
      "Epoch [2/2], Step [53260/64305], Loss: 4.9421\n",
      "Epoch [2/2], Step [53270/64305], Loss: 4.8304\n",
      "Epoch [2/2], Step [53280/64305], Loss: 4.8713\n",
      "Epoch [2/2], Step [53290/64305], Loss: 4.8175\n",
      "Epoch [2/2], Step [53300/64305], Loss: 4.8188\n",
      "Epoch [2/2], Step [53310/64305], Loss: 4.9417\n",
      "Epoch [2/2], Step [53320/64305], Loss: 4.6958\n",
      "Epoch [2/2], Step [53330/64305], Loss: 4.7847\n",
      "Epoch [2/2], Step [53340/64305], Loss: 4.8255\n",
      "Epoch [2/2], Step [53350/64305], Loss: 4.8791\n",
      "Epoch [2/2], Step [53360/64305], Loss: 4.7137\n",
      "Epoch [2/2], Step [53370/64305], Loss: 4.7485\n",
      "Epoch [2/2], Step [53380/64305], Loss: 4.8466\n",
      "Epoch [2/2], Step [53390/64305], Loss: 4.7660\n",
      "Epoch [2/2], Step [53400/64305], Loss: 4.7462\n",
      "Epoch [2/2], Step [53410/64305], Loss: 4.8483\n",
      "Epoch [2/2], Step [53420/64305], Loss: 4.9546\n",
      "Epoch [2/2], Step [53430/64305], Loss: 4.6886\n",
      "Epoch [2/2], Step [53440/64305], Loss: 5.0184\n",
      "Epoch [2/2], Step [53450/64305], Loss: 4.9564\n",
      "Epoch [2/2], Step [53460/64305], Loss: 4.8683\n",
      "Epoch [2/2], Step [53470/64305], Loss: 4.7262\n",
      "Epoch [2/2], Step [53480/64305], Loss: 4.7372\n",
      "Epoch [2/2], Step [53490/64305], Loss: 4.8182\n",
      "Epoch [2/2], Step [53500/64305], Loss: 4.9240\n",
      "Epoch [2/2], Step [53510/64305], Loss: 4.7759\n",
      "Epoch [2/2], Step [53520/64305], Loss: 4.8919\n",
      "Epoch [2/2], Step [53530/64305], Loss: 4.8654\n",
      "Epoch [2/2], Step [53540/64305], Loss: 4.7621\n",
      "Epoch [2/2], Step [53550/64305], Loss: 4.8675\n",
      "Epoch [2/2], Step [53560/64305], Loss: 4.8683\n",
      "Epoch [2/2], Step [53570/64305], Loss: 4.9648\n",
      "Epoch [2/2], Step [53580/64305], Loss: 4.9207\n",
      "Epoch [2/2], Step [53590/64305], Loss: 4.7515\n",
      "Epoch [2/2], Step [53600/64305], Loss: 4.8649\n",
      "Epoch [2/2], Step [53610/64305], Loss: 4.8165\n",
      "Epoch [2/2], Step [53620/64305], Loss: 4.7460\n",
      "Epoch [2/2], Step [53630/64305], Loss: 4.8747\n",
      "Epoch [2/2], Step [53640/64305], Loss: 4.7374\n",
      "Epoch [2/2], Step [53650/64305], Loss: 4.6204\n",
      "Epoch [2/2], Step [53660/64305], Loss: 4.7130\n",
      "Epoch [2/2], Step [53670/64305], Loss: 4.8277\n",
      "Epoch [2/2], Step [53680/64305], Loss: 4.9126\n",
      "Epoch [2/2], Step [53690/64305], Loss: 4.8106\n",
      "Epoch [2/2], Step [53700/64305], Loss: 4.9879\n",
      "Epoch [2/2], Step [53710/64305], Loss: 4.8823\n",
      "Epoch [2/2], Step [53720/64305], Loss: 4.6384\n",
      "Epoch [2/2], Step [53730/64305], Loss: 5.0278\n",
      "Epoch [2/2], Step [53740/64305], Loss: 4.9482\n",
      "Epoch [2/2], Step [53750/64305], Loss: 4.8453\n",
      "Epoch [2/2], Step [53760/64305], Loss: 5.1228\n",
      "Epoch [2/2], Step [53770/64305], Loss: 4.7512\n",
      "Epoch [2/2], Step [53780/64305], Loss: 4.9661\n",
      "Epoch [2/2], Step [53790/64305], Loss: 4.9971\n",
      "Epoch [2/2], Step [53800/64305], Loss: 4.7770\n",
      "Epoch [2/2], Step [53810/64305], Loss: 4.6826\n",
      "Epoch [2/2], Step [53820/64305], Loss: 4.7514\n",
      "Epoch [2/2], Step [53830/64305], Loss: 5.0304\n",
      "Epoch [2/2], Step [53840/64305], Loss: 4.7972\n",
      "Epoch [2/2], Step [53850/64305], Loss: 4.8268\n",
      "Epoch [2/2], Step [53860/64305], Loss: 4.7580\n",
      "Epoch [2/2], Step [53870/64305], Loss: 4.8096\n",
      "Epoch [2/2], Step [53880/64305], Loss: 4.5440\n",
      "Epoch [2/2], Step [53890/64305], Loss: 4.6305\n",
      "Epoch [2/2], Step [53900/64305], Loss: 4.9683\n",
      "Epoch [2/2], Step [53910/64305], Loss: 4.8760\n",
      "Epoch [2/2], Step [53920/64305], Loss: 4.6453\n",
      "Epoch [2/2], Step [53930/64305], Loss: 4.8661\n",
      "Epoch [2/2], Step [53940/64305], Loss: 4.7375\n",
      "Epoch [2/2], Step [53950/64305], Loss: 4.9810\n",
      "Epoch [2/2], Step [53960/64305], Loss: 4.7939\n",
      "Epoch [2/2], Step [53970/64305], Loss: 4.7586\n",
      "Epoch [2/2], Step [53980/64305], Loss: 4.9095\n",
      "Epoch [2/2], Step [53990/64305], Loss: 4.8853\n",
      "Epoch [2/2], Step [54000/64305], Loss: 4.7654\n",
      "Epoch [2/2], Step [54010/64305], Loss: 4.9449\n",
      "Epoch [2/2], Step [54020/64305], Loss: 4.8412\n",
      "Epoch [2/2], Step [54030/64305], Loss: 4.7307\n",
      "Epoch [2/2], Step [54040/64305], Loss: 4.9265\n",
      "Epoch [2/2], Step [54050/64305], Loss: 4.7845\n",
      "Epoch [2/2], Step [54060/64305], Loss: 4.8644\n",
      "Epoch [2/2], Step [54070/64305], Loss: 5.0880\n",
      "Epoch [2/2], Step [54080/64305], Loss: 4.8642\n",
      "Epoch [2/2], Step [54090/64305], Loss: 4.8420\n",
      "Epoch [2/2], Step [54100/64305], Loss: 4.7097\n",
      "Epoch [2/2], Step [54110/64305], Loss: 4.5357\n",
      "Epoch [2/2], Step [54120/64305], Loss: 5.0361\n",
      "Epoch [2/2], Step [54130/64305], Loss: 4.7545\n",
      "Epoch [2/2], Step [54140/64305], Loss: 4.9231\n",
      "Epoch [2/2], Step [54150/64305], Loss: 4.8674\n",
      "Epoch [2/2], Step [54160/64305], Loss: 4.8367\n",
      "Epoch [2/2], Step [54170/64305], Loss: 4.9520\n",
      "Epoch [2/2], Step [54180/64305], Loss: 4.6167\n",
      "Epoch [2/2], Step [54190/64305], Loss: 4.7227\n",
      "Epoch [2/2], Step [54200/64305], Loss: 4.7834\n",
      "Epoch [2/2], Step [54210/64305], Loss: 4.9154\n",
      "Epoch [2/2], Step [54220/64305], Loss: 4.7265\n",
      "Epoch [2/2], Step [54230/64305], Loss: 4.8809\n",
      "Epoch [2/2], Step [54240/64305], Loss: 4.7786\n",
      "Epoch [2/2], Step [54250/64305], Loss: 4.9608\n",
      "Epoch [2/2], Step [54260/64305], Loss: 4.7698\n",
      "Epoch [2/2], Step [54270/64305], Loss: 4.8069\n",
      "Epoch [2/2], Step [54280/64305], Loss: 4.8992\n",
      "Epoch [2/2], Step [54290/64305], Loss: 4.8523\n",
      "Epoch [2/2], Step [54300/64305], Loss: 4.9716\n",
      "Epoch [2/2], Step [54310/64305], Loss: 4.8875\n",
      "Epoch [2/2], Step [54320/64305], Loss: 4.8018\n",
      "Epoch [2/2], Step [54330/64305], Loss: 4.8715\n",
      "Epoch [2/2], Step [54340/64305], Loss: 4.7447\n",
      "Epoch [2/2], Step [54350/64305], Loss: 4.8175\n",
      "Epoch [2/2], Step [54360/64305], Loss: 4.8595\n",
      "Epoch [2/2], Step [54370/64305], Loss: 4.6510\n",
      "Epoch [2/2], Step [54380/64305], Loss: 4.8952\n",
      "Epoch [2/2], Step [54390/64305], Loss: 4.9415\n",
      "Epoch [2/2], Step [54400/64305], Loss: 4.7198\n",
      "Epoch [2/2], Step [54410/64305], Loss: 4.9585\n",
      "Epoch [2/2], Step [54420/64305], Loss: 4.8025\n",
      "Epoch [2/2], Step [54430/64305], Loss: 4.9127\n",
      "Epoch [2/2], Step [54440/64305], Loss: 4.7325\n",
      "Epoch [2/2], Step [54450/64305], Loss: 4.7930\n",
      "Epoch [2/2], Step [54460/64305], Loss: 4.7823\n",
      "Epoch [2/2], Step [54470/64305], Loss: 4.7143\n",
      "Epoch [2/2], Step [54480/64305], Loss: 4.7710\n",
      "Epoch [2/2], Step [54490/64305], Loss: 4.7584\n",
      "Epoch [2/2], Step [54500/64305], Loss: 4.6547\n",
      "Epoch [2/2], Step [54510/64305], Loss: 4.7648\n",
      "Epoch [2/2], Step [54520/64305], Loss: 4.7873\n",
      "Epoch [2/2], Step [54530/64305], Loss: 4.7147\n",
      "Epoch [2/2], Step [54540/64305], Loss: 4.8726\n",
      "Epoch [2/2], Step [54550/64305], Loss: 4.8473\n",
      "Epoch [2/2], Step [54560/64305], Loss: 4.6196\n",
      "Epoch [2/2], Step [54570/64305], Loss: 4.8082\n",
      "Epoch [2/2], Step [54580/64305], Loss: 4.9234\n",
      "Epoch [2/2], Step [54590/64305], Loss: 4.7655\n",
      "Epoch [2/2], Step [54600/64305], Loss: 4.7597\n",
      "Epoch [2/2], Step [54610/64305], Loss: 4.6914\n",
      "Epoch [2/2], Step [54620/64305], Loss: 4.6728\n",
      "Epoch [2/2], Step [54630/64305], Loss: 4.9168\n",
      "Epoch [2/2], Step [54640/64305], Loss: 4.5426\n",
      "Epoch [2/2], Step [54650/64305], Loss: 4.8565\n",
      "Epoch [2/2], Step [54660/64305], Loss: 4.7809\n",
      "Epoch [2/2], Step [54670/64305], Loss: 4.9396\n",
      "Epoch [2/2], Step [54680/64305], Loss: 4.9247\n",
      "Epoch [2/2], Step [54690/64305], Loss: 4.7898\n",
      "Epoch [2/2], Step [54700/64305], Loss: 4.7940\n",
      "Epoch [2/2], Step [54710/64305], Loss: 4.7377\n",
      "Epoch [2/2], Step [54720/64305], Loss: 4.7743\n",
      "Epoch [2/2], Step [54730/64305], Loss: 4.7879\n",
      "Epoch [2/2], Step [54740/64305], Loss: 4.8686\n",
      "Epoch [2/2], Step [54750/64305], Loss: 5.0249\n",
      "Epoch [2/2], Step [54760/64305], Loss: 4.6288\n",
      "Epoch [2/2], Step [54770/64305], Loss: 4.7090\n",
      "Epoch [2/2], Step [54780/64305], Loss: 4.7920\n",
      "Epoch [2/2], Step [54790/64305], Loss: 4.8301\n",
      "Epoch [2/2], Step [54800/64305], Loss: 4.7750\n",
      "Epoch [2/2], Step [54810/64305], Loss: 4.8740\n",
      "Epoch [2/2], Step [54820/64305], Loss: 5.0052\n",
      "Epoch [2/2], Step [54830/64305], Loss: 4.9669\n",
      "Epoch [2/2], Step [54840/64305], Loss: 4.7457\n",
      "Epoch [2/2], Step [54850/64305], Loss: 4.7925\n",
      "Epoch [2/2], Step [54860/64305], Loss: 4.6688\n",
      "Epoch [2/2], Step [54870/64305], Loss: 4.8562\n",
      "Epoch [2/2], Step [54880/64305], Loss: 4.8191\n",
      "Epoch [2/2], Step [54890/64305], Loss: 4.9808\n",
      "Epoch [2/2], Step [54900/64305], Loss: 4.7509\n",
      "Epoch [2/2], Step [54910/64305], Loss: 4.7895\n",
      "Epoch [2/2], Step [54920/64305], Loss: 4.8436\n",
      "Epoch [2/2], Step [54930/64305], Loss: 4.7204\n",
      "Epoch [2/2], Step [54940/64305], Loss: 4.9634\n",
      "Epoch [2/2], Step [54950/64305], Loss: 4.6286\n",
      "Epoch [2/2], Step [54960/64305], Loss: 4.7123\n",
      "Epoch [2/2], Step [54970/64305], Loss: 4.8932\n",
      "Epoch [2/2], Step [54980/64305], Loss: 4.8224\n",
      "Epoch [2/2], Step [54990/64305], Loss: 4.5278\n",
      "Epoch [2/2], Step [55000/64305], Loss: 4.6042\n",
      "Epoch [2/2], Step [55010/64305], Loss: 4.9041\n",
      "Epoch [2/2], Step [55020/64305], Loss: 4.7004\n",
      "Epoch [2/2], Step [55030/64305], Loss: 4.7658\n",
      "Epoch [2/2], Step [55040/64305], Loss: 4.8865\n",
      "Epoch [2/2], Step [55050/64305], Loss: 4.7673\n",
      "Epoch [2/2], Step [55060/64305], Loss: 4.6110\n",
      "Epoch [2/2], Step [55070/64305], Loss: 4.8146\n",
      "Epoch [2/2], Step [55080/64305], Loss: 5.0321\n",
      "Epoch [2/2], Step [55090/64305], Loss: 4.6272\n",
      "Epoch [2/2], Step [55100/64305], Loss: 4.9519\n",
      "Epoch [2/2], Step [55110/64305], Loss: 4.8466\n",
      "Epoch [2/2], Step [55120/64305], Loss: 4.9084\n",
      "Epoch [2/2], Step [55130/64305], Loss: 4.7717\n",
      "Epoch [2/2], Step [55140/64305], Loss: 4.8166\n",
      "Epoch [2/2], Step [55150/64305], Loss: 4.7557\n",
      "Epoch [2/2], Step [55160/64305], Loss: 4.8299\n",
      "Epoch [2/2], Step [55170/64305], Loss: 4.6982\n",
      "Epoch [2/2], Step [55180/64305], Loss: 5.1218\n",
      "Epoch [2/2], Step [55190/64305], Loss: 4.9202\n",
      "Epoch [2/2], Step [55200/64305], Loss: 4.8674\n",
      "Epoch [2/2], Step [55210/64305], Loss: 4.7821\n",
      "Epoch [2/2], Step [55220/64305], Loss: 4.8699\n",
      "Epoch [2/2], Step [55230/64305], Loss: 4.8923\n",
      "Epoch [2/2], Step [55240/64305], Loss: 4.6080\n",
      "Epoch [2/2], Step [55250/64305], Loss: 4.6722\n",
      "Epoch [2/2], Step [55260/64305], Loss: 4.9970\n",
      "Epoch [2/2], Step [55270/64305], Loss: 4.9207\n",
      "Epoch [2/2], Step [55280/64305], Loss: 4.8639\n",
      "Epoch [2/2], Step [55290/64305], Loss: 4.7373\n",
      "Epoch [2/2], Step [55300/64305], Loss: 4.7452\n",
      "Epoch [2/2], Step [55310/64305], Loss: 4.9331\n",
      "Epoch [2/2], Step [55320/64305], Loss: 4.9099\n",
      "Epoch [2/2], Step [55330/64305], Loss: 4.5540\n",
      "Epoch [2/2], Step [55340/64305], Loss: 4.7072\n",
      "Epoch [2/2], Step [55350/64305], Loss: 4.7515\n",
      "Epoch [2/2], Step [55360/64305], Loss: 4.9462\n",
      "Epoch [2/2], Step [55370/64305], Loss: 4.7384\n",
      "Epoch [2/2], Step [55380/64305], Loss: 4.8389\n",
      "Epoch [2/2], Step [55390/64305], Loss: 4.7925\n",
      "Epoch [2/2], Step [55400/64305], Loss: 4.6285\n",
      "Epoch [2/2], Step [55410/64305], Loss: 5.0627\n",
      "Epoch [2/2], Step [55420/64305], Loss: 4.7516\n",
      "Epoch [2/2], Step [55430/64305], Loss: 4.8236\n",
      "Epoch [2/2], Step [55440/64305], Loss: 4.5971\n",
      "Epoch [2/2], Step [55450/64305], Loss: 4.6532\n",
      "Epoch [2/2], Step [55460/64305], Loss: 4.9526\n",
      "Epoch [2/2], Step [55470/64305], Loss: 4.6239\n",
      "Epoch [2/2], Step [55480/64305], Loss: 4.7782\n",
      "Epoch [2/2], Step [55490/64305], Loss: 4.8588\n",
      "Epoch [2/2], Step [55500/64305], Loss: 4.6342\n",
      "Epoch [2/2], Step [55510/64305], Loss: 4.7661\n",
      "Epoch [2/2], Step [55520/64305], Loss: 4.7063\n",
      "Epoch [2/2], Step [55530/64305], Loss: 4.7280\n",
      "Epoch [2/2], Step [55540/64305], Loss: 4.8182\n",
      "Epoch [2/2], Step [55550/64305], Loss: 4.8121\n",
      "Epoch [2/2], Step [55560/64305], Loss: 4.8959\n",
      "Epoch [2/2], Step [55570/64305], Loss: 4.8401\n",
      "Epoch [2/2], Step [55580/64305], Loss: 4.5874\n",
      "Epoch [2/2], Step [55590/64305], Loss: 4.8199\n",
      "Epoch [2/2], Step [55600/64305], Loss: 4.6626\n",
      "Epoch [2/2], Step [55610/64305], Loss: 4.9554\n",
      "Epoch [2/2], Step [55620/64305], Loss: 4.9014\n",
      "Epoch [2/2], Step [55630/64305], Loss: 4.8021\n",
      "Epoch [2/2], Step [55640/64305], Loss: 4.6940\n",
      "Epoch [2/2], Step [55650/64305], Loss: 4.6662\n",
      "Epoch [2/2], Step [55660/64305], Loss: 4.8563\n",
      "Epoch [2/2], Step [55670/64305], Loss: 4.6279\n",
      "Epoch [2/2], Step [55680/64305], Loss: 4.8375\n",
      "Epoch [2/2], Step [55690/64305], Loss: 4.5449\n",
      "Epoch [2/2], Step [55700/64305], Loss: 4.9536\n",
      "Epoch [2/2], Step [55710/64305], Loss: 4.7472\n",
      "Epoch [2/2], Step [55720/64305], Loss: 4.6533\n",
      "Epoch [2/2], Step [55730/64305], Loss: 4.7829\n",
      "Epoch [2/2], Step [55740/64305], Loss: 4.9526\n",
      "Epoch [2/2], Step [55750/64305], Loss: 4.9946\n",
      "Epoch [2/2], Step [55760/64305], Loss: 4.5867\n",
      "Epoch [2/2], Step [55770/64305], Loss: 4.6718\n",
      "Epoch [2/2], Step [55780/64305], Loss: 4.7807\n",
      "Epoch [2/2], Step [55790/64305], Loss: 4.8973\n",
      "Epoch [2/2], Step [55800/64305], Loss: 4.6085\n",
      "Epoch [2/2], Step [55810/64305], Loss: 4.7688\n",
      "Epoch [2/2], Step [55820/64305], Loss: 4.6220\n",
      "Epoch [2/2], Step [55830/64305], Loss: 4.6260\n",
      "Epoch [2/2], Step [55840/64305], Loss: 4.7983\n",
      "Epoch [2/2], Step [55850/64305], Loss: 5.0391\n",
      "Epoch [2/2], Step [55860/64305], Loss: 4.8642\n",
      "Epoch [2/2], Step [55870/64305], Loss: 4.7564\n",
      "Epoch [2/2], Step [55880/64305], Loss: 4.6947\n",
      "Epoch [2/2], Step [55890/64305], Loss: 4.6456\n",
      "Epoch [2/2], Step [55900/64305], Loss: 4.8897\n",
      "Epoch [2/2], Step [55910/64305], Loss: 4.8180\n",
      "Epoch [2/2], Step [55920/64305], Loss: 4.6936\n",
      "Epoch [2/2], Step [55930/64305], Loss: 4.8031\n",
      "Epoch [2/2], Step [55940/64305], Loss: 4.7256\n",
      "Epoch [2/2], Step [55950/64305], Loss: 4.7215\n",
      "Epoch [2/2], Step [55960/64305], Loss: 4.8758\n",
      "Epoch [2/2], Step [55970/64305], Loss: 4.6031\n",
      "Epoch [2/2], Step [55980/64305], Loss: 4.7324\n",
      "Epoch [2/2], Step [55990/64305], Loss: 4.8344\n",
      "Epoch [2/2], Step [56000/64305], Loss: 4.8044\n",
      "Epoch [2/2], Step [56010/64305], Loss: 4.9713\n",
      "Epoch [2/2], Step [56020/64305], Loss: 4.7782\n",
      "Epoch [2/2], Step [56030/64305], Loss: 4.7101\n",
      "Epoch [2/2], Step [56040/64305], Loss: 4.9228\n",
      "Epoch [2/2], Step [56050/64305], Loss: 4.7779\n",
      "Epoch [2/2], Step [56060/64305], Loss: 5.1108\n",
      "Epoch [2/2], Step [56070/64305], Loss: 4.7341\n",
      "Epoch [2/2], Step [56080/64305], Loss: 4.9084\n",
      "Epoch [2/2], Step [56090/64305], Loss: 4.7457\n",
      "Epoch [2/2], Step [56100/64305], Loss: 4.5792\n",
      "Epoch [2/2], Step [56110/64305], Loss: 5.0849\n",
      "Epoch [2/2], Step [56120/64305], Loss: 4.7479\n",
      "Epoch [2/2], Step [56130/64305], Loss: 4.7961\n",
      "Epoch [2/2], Step [56140/64305], Loss: 4.7715\n",
      "Epoch [2/2], Step [56150/64305], Loss: 4.7633\n",
      "Epoch [2/2], Step [56160/64305], Loss: 4.7296\n",
      "Epoch [2/2], Step [56170/64305], Loss: 4.9444\n",
      "Epoch [2/2], Step [56180/64305], Loss: 4.7299\n",
      "Epoch [2/2], Step [56190/64305], Loss: 4.7218\n",
      "Epoch [2/2], Step [56200/64305], Loss: 4.8138\n",
      "Epoch [2/2], Step [56210/64305], Loss: 4.9224\n",
      "Epoch [2/2], Step [56220/64305], Loss: 4.9027\n",
      "Epoch [2/2], Step [56230/64305], Loss: 4.7689\n",
      "Epoch [2/2], Step [56240/64305], Loss: 4.6902\n",
      "Epoch [2/2], Step [56250/64305], Loss: 4.5796\n",
      "Epoch [2/2], Step [56260/64305], Loss: 4.9575\n",
      "Epoch [2/2], Step [56270/64305], Loss: 4.9397\n",
      "Epoch [2/2], Step [56280/64305], Loss: 4.8114\n",
      "Epoch [2/2], Step [56290/64305], Loss: 4.7177\n",
      "Epoch [2/2], Step [56300/64305], Loss: 4.7770\n",
      "Epoch [2/2], Step [56310/64305], Loss: 4.9301\n",
      "Epoch [2/2], Step [56320/64305], Loss: 4.8543\n",
      "Epoch [2/2], Step [56330/64305], Loss: 5.0469\n",
      "Epoch [2/2], Step [56340/64305], Loss: 4.7602\n",
      "Epoch [2/2], Step [56350/64305], Loss: 4.7627\n",
      "Epoch [2/2], Step [56360/64305], Loss: 4.8397\n",
      "Epoch [2/2], Step [56370/64305], Loss: 5.1162\n",
      "Epoch [2/2], Step [56380/64305], Loss: 4.6969\n",
      "Epoch [2/2], Step [56390/64305], Loss: 4.7917\n",
      "Epoch [2/2], Step [56400/64305], Loss: 4.7517\n",
      "Epoch [2/2], Step [56410/64305], Loss: 4.4731\n",
      "Epoch [2/2], Step [56420/64305], Loss: 4.4873\n",
      "Epoch [2/2], Step [56430/64305], Loss: 4.7687\n",
      "Epoch [2/2], Step [56440/64305], Loss: 4.5513\n",
      "Epoch [2/2], Step [56450/64305], Loss: 4.9070\n",
      "Epoch [2/2], Step [56460/64305], Loss: 4.7831\n",
      "Epoch [2/2], Step [56470/64305], Loss: 5.0554\n",
      "Epoch [2/2], Step [56480/64305], Loss: 4.8762\n",
      "Epoch [2/2], Step [56490/64305], Loss: 4.6352\n",
      "Epoch [2/2], Step [56500/64305], Loss: 4.6728\n",
      "Epoch [2/2], Step [56510/64305], Loss: 4.7861\n",
      "Epoch [2/2], Step [56520/64305], Loss: 4.8275\n",
      "Epoch [2/2], Step [56530/64305], Loss: 4.7007\n",
      "Epoch [2/2], Step [56540/64305], Loss: 4.6410\n",
      "Epoch [2/2], Step [56550/64305], Loss: 4.9602\n",
      "Epoch [2/2], Step [56560/64305], Loss: 4.9118\n",
      "Epoch [2/2], Step [56570/64305], Loss: 4.7339\n",
      "Epoch [2/2], Step [56580/64305], Loss: 5.0292\n",
      "Epoch [2/2], Step [56590/64305], Loss: 4.8034\n",
      "Epoch [2/2], Step [56600/64305], Loss: 5.0033\n",
      "Epoch [2/2], Step [56610/64305], Loss: 4.6690\n",
      "Epoch [2/2], Step [56620/64305], Loss: 4.8593\n",
      "Epoch [2/2], Step [56630/64305], Loss: 4.9250\n",
      "Epoch [2/2], Step [56640/64305], Loss: 4.9081\n",
      "Epoch [2/2], Step [56650/64305], Loss: 4.9968\n",
      "Epoch [2/2], Step [56660/64305], Loss: 4.8559\n",
      "Epoch [2/2], Step [56670/64305], Loss: 4.6936\n",
      "Epoch [2/2], Step [56680/64305], Loss: 4.9911\n",
      "Epoch [2/2], Step [56690/64305], Loss: 4.5739\n",
      "Epoch [2/2], Step [56700/64305], Loss: 4.8573\n",
      "Epoch [2/2], Step [56710/64305], Loss: 4.7128\n",
      "Epoch [2/2], Step [56720/64305], Loss: 4.8343\n",
      "Epoch [2/2], Step [56730/64305], Loss: 4.8614\n",
      "Epoch [2/2], Step [56740/64305], Loss: 4.9393\n",
      "Epoch [2/2], Step [56750/64305], Loss: 4.7690\n",
      "Epoch [2/2], Step [56760/64305], Loss: 4.8919\n",
      "Epoch [2/2], Step [56770/64305], Loss: 4.8066\n",
      "Epoch [2/2], Step [56780/64305], Loss: 4.7606\n",
      "Epoch [2/2], Step [56790/64305], Loss: 4.7812\n",
      "Epoch [2/2], Step [56800/64305], Loss: 4.6558\n",
      "Epoch [2/2], Step [56810/64305], Loss: 4.8441\n",
      "Epoch [2/2], Step [56820/64305], Loss: 4.9513\n",
      "Epoch [2/2], Step [56830/64305], Loss: 4.8975\n",
      "Epoch [2/2], Step [56840/64305], Loss: 4.7345\n",
      "Epoch [2/2], Step [56850/64305], Loss: 5.0863\n",
      "Epoch [2/2], Step [56860/64305], Loss: 4.8337\n",
      "Epoch [2/2], Step [56870/64305], Loss: 4.9187\n",
      "Epoch [2/2], Step [56880/64305], Loss: 4.8343\n",
      "Epoch [2/2], Step [56890/64305], Loss: 4.9445\n",
      "Epoch [2/2], Step [56900/64305], Loss: 4.9864\n",
      "Epoch [2/2], Step [56910/64305], Loss: 4.7251\n",
      "Epoch [2/2], Step [56920/64305], Loss: 4.6915\n",
      "Epoch [2/2], Step [56930/64305], Loss: 4.5513\n",
      "Epoch [2/2], Step [56940/64305], Loss: 4.9208\n",
      "Epoch [2/2], Step [56950/64305], Loss: 4.8187\n",
      "Epoch [2/2], Step [56960/64305], Loss: 4.7299\n",
      "Epoch [2/2], Step [56970/64305], Loss: 4.8333\n",
      "Epoch [2/2], Step [56980/64305], Loss: 4.8444\n",
      "Epoch [2/2], Step [56990/64305], Loss: 4.9817\n",
      "Epoch [2/2], Step [57000/64305], Loss: 4.9157\n",
      "Epoch [2/2], Step [57010/64305], Loss: 4.7483\n",
      "Epoch [2/2], Step [57020/64305], Loss: 4.7678\n",
      "Epoch [2/2], Step [57030/64305], Loss: 4.8587\n",
      "Epoch [2/2], Step [57040/64305], Loss: 4.7251\n",
      "Epoch [2/2], Step [57050/64305], Loss: 4.9611\n",
      "Epoch [2/2], Step [57060/64305], Loss: 4.7966\n",
      "Epoch [2/2], Step [57070/64305], Loss: 4.8555\n",
      "Epoch [2/2], Step [57080/64305], Loss: 4.7616\n",
      "Epoch [2/2], Step [57090/64305], Loss: 5.0536\n",
      "Epoch [2/2], Step [57100/64305], Loss: 4.5883\n",
      "Epoch [2/2], Step [57110/64305], Loss: 4.8343\n",
      "Epoch [2/2], Step [57120/64305], Loss: 4.8882\n",
      "Epoch [2/2], Step [57130/64305], Loss: 4.9663\n",
      "Epoch [2/2], Step [57140/64305], Loss: 4.8389\n",
      "Epoch [2/2], Step [57150/64305], Loss: 4.8982\n",
      "Epoch [2/2], Step [57160/64305], Loss: 4.9432\n",
      "Epoch [2/2], Step [57170/64305], Loss: 4.7370\n",
      "Epoch [2/2], Step [57180/64305], Loss: 4.8420\n",
      "Epoch [2/2], Step [57190/64305], Loss: 4.9120\n",
      "Epoch [2/2], Step [57200/64305], Loss: 4.8880\n",
      "Epoch [2/2], Step [57210/64305], Loss: 4.8299\n",
      "Epoch [2/2], Step [57220/64305], Loss: 4.8547\n",
      "Epoch [2/2], Step [57230/64305], Loss: 4.9650\n",
      "Epoch [2/2], Step [57240/64305], Loss: 5.0449\n",
      "Epoch [2/2], Step [57250/64305], Loss: 4.8930\n",
      "Epoch [2/2], Step [57260/64305], Loss: 4.8168\n",
      "Epoch [2/2], Step [57270/64305], Loss: 5.0426\n",
      "Epoch [2/2], Step [57280/64305], Loss: 4.7522\n",
      "Epoch [2/2], Step [57290/64305], Loss: 4.7297\n",
      "Epoch [2/2], Step [57300/64305], Loss: 4.7555\n",
      "Epoch [2/2], Step [57310/64305], Loss: 4.9484\n",
      "Epoch [2/2], Step [57320/64305], Loss: 4.6838\n",
      "Epoch [2/2], Step [57330/64305], Loss: 4.5408\n",
      "Epoch [2/2], Step [57340/64305], Loss: 4.6887\n",
      "Epoch [2/2], Step [57350/64305], Loss: 5.0156\n",
      "Epoch [2/2], Step [57360/64305], Loss: 4.8642\n",
      "Epoch [2/2], Step [57370/64305], Loss: 4.8334\n",
      "Epoch [2/2], Step [57380/64305], Loss: 4.7522\n",
      "Epoch [2/2], Step [57390/64305], Loss: 5.0186\n",
      "Epoch [2/2], Step [57400/64305], Loss: 4.9080\n",
      "Epoch [2/2], Step [57410/64305], Loss: 4.9913\n",
      "Epoch [2/2], Step [57420/64305], Loss: 4.7382\n",
      "Epoch [2/2], Step [57430/64305], Loss: 4.5982\n",
      "Epoch [2/2], Step [57440/64305], Loss: 4.8121\n",
      "Epoch [2/2], Step [57450/64305], Loss: 4.7688\n",
      "Epoch [2/2], Step [57460/64305], Loss: 4.9871\n",
      "Epoch [2/2], Step [57470/64305], Loss: 4.9088\n",
      "Epoch [2/2], Step [57480/64305], Loss: 4.8236\n",
      "Epoch [2/2], Step [57490/64305], Loss: 4.8364\n",
      "Epoch [2/2], Step [57500/64305], Loss: 4.8227\n",
      "Epoch [2/2], Step [57510/64305], Loss: 4.9425\n",
      "Epoch [2/2], Step [57520/64305], Loss: 4.7738\n",
      "Epoch [2/2], Step [57530/64305], Loss: 4.7538\n",
      "Epoch [2/2], Step [57540/64305], Loss: 4.6700\n",
      "Epoch [2/2], Step [57550/64305], Loss: 4.7975\n",
      "Epoch [2/2], Step [57560/64305], Loss: 4.7063\n",
      "Epoch [2/2], Step [57570/64305], Loss: 4.9371\n",
      "Epoch [2/2], Step [57580/64305], Loss: 4.8467\n",
      "Epoch [2/2], Step [57590/64305], Loss: 4.8021\n",
      "Epoch [2/2], Step [57600/64305], Loss: 4.6194\n",
      "Epoch [2/2], Step [57610/64305], Loss: 4.7066\n",
      "Epoch [2/2], Step [57620/64305], Loss: 4.6897\n",
      "Epoch [2/2], Step [57630/64305], Loss: 4.9469\n",
      "Epoch [2/2], Step [57640/64305], Loss: 4.8698\n",
      "Epoch [2/2], Step [57650/64305], Loss: 4.8428\n",
      "Epoch [2/2], Step [57660/64305], Loss: 4.7932\n",
      "Epoch [2/2], Step [57670/64305], Loss: 4.7300\n",
      "Epoch [2/2], Step [57680/64305], Loss: 4.6665\n",
      "Epoch [2/2], Step [57690/64305], Loss: 4.9491\n",
      "Epoch [2/2], Step [57700/64305], Loss: 4.9137\n",
      "Epoch [2/2], Step [57710/64305], Loss: 4.7428\n",
      "Epoch [2/2], Step [57720/64305], Loss: 4.9922\n",
      "Epoch [2/2], Step [57730/64305], Loss: 4.9407\n",
      "Epoch [2/2], Step [57740/64305], Loss: 4.9874\n",
      "Epoch [2/2], Step [57750/64305], Loss: 4.9335\n",
      "Epoch [2/2], Step [57760/64305], Loss: 4.5609\n",
      "Epoch [2/2], Step [57770/64305], Loss: 4.7018\n",
      "Epoch [2/2], Step [57780/64305], Loss: 4.8912\n",
      "Epoch [2/2], Step [57790/64305], Loss: 4.8530\n",
      "Epoch [2/2], Step [57800/64305], Loss: 4.7219\n",
      "Epoch [2/2], Step [57810/64305], Loss: 4.7416\n",
      "Epoch [2/2], Step [57820/64305], Loss: 4.8405\n",
      "Epoch [2/2], Step [57830/64305], Loss: 4.7925\n",
      "Epoch [2/2], Step [57840/64305], Loss: 4.7562\n",
      "Epoch [2/2], Step [57850/64305], Loss: 4.9014\n",
      "Epoch [2/2], Step [57860/64305], Loss: 4.8014\n",
      "Epoch [2/2], Step [57870/64305], Loss: 4.6783\n",
      "Epoch [2/2], Step [57880/64305], Loss: 4.7208\n",
      "Epoch [2/2], Step [57890/64305], Loss: 4.8328\n",
      "Epoch [2/2], Step [57900/64305], Loss: 4.9621\n",
      "Epoch [2/2], Step [57910/64305], Loss: 5.0126\n",
      "Epoch [2/2], Step [57920/64305], Loss: 4.6547\n",
      "Epoch [2/2], Step [57930/64305], Loss: 5.0008\n",
      "Epoch [2/2], Step [57940/64305], Loss: 4.7809\n",
      "Epoch [2/2], Step [57950/64305], Loss: 4.9205\n",
      "Epoch [2/2], Step [57960/64305], Loss: 4.9974\n",
      "Epoch [2/2], Step [57970/64305], Loss: 4.7404\n",
      "Epoch [2/2], Step [57980/64305], Loss: 4.7697\n",
      "Epoch [2/2], Step [57990/64305], Loss: 4.7130\n",
      "Epoch [2/2], Step [58000/64305], Loss: 4.6795\n",
      "Epoch [2/2], Step [58010/64305], Loss: 4.8518\n",
      "Epoch [2/2], Step [58020/64305], Loss: 4.8343\n",
      "Epoch [2/2], Step [58030/64305], Loss: 4.7792\n",
      "Epoch [2/2], Step [58040/64305], Loss: 4.8832\n",
      "Epoch [2/2], Step [58050/64305], Loss: 4.7153\n",
      "Epoch [2/2], Step [58060/64305], Loss: 4.9661\n",
      "Epoch [2/2], Step [58070/64305], Loss: 4.8442\n",
      "Epoch [2/2], Step [58080/64305], Loss: 4.8497\n",
      "Epoch [2/2], Step [58090/64305], Loss: 4.7791\n",
      "Epoch [2/2], Step [58100/64305], Loss: 4.9143\n",
      "Epoch [2/2], Step [58110/64305], Loss: 4.9223\n",
      "Epoch [2/2], Step [58120/64305], Loss: 4.8698\n",
      "Epoch [2/2], Step [58130/64305], Loss: 4.8246\n",
      "Epoch [2/2], Step [58140/64305], Loss: 4.6524\n",
      "Epoch [2/2], Step [58150/64305], Loss: 4.7798\n",
      "Epoch [2/2], Step [58160/64305], Loss: 4.6568\n",
      "Epoch [2/2], Step [58170/64305], Loss: 5.0171\n",
      "Epoch [2/2], Step [58180/64305], Loss: 4.8737\n",
      "Epoch [2/2], Step [58190/64305], Loss: 4.9011\n",
      "Epoch [2/2], Step [58200/64305], Loss: 4.8089\n",
      "Epoch [2/2], Step [58210/64305], Loss: 4.8225\n",
      "Epoch [2/2], Step [58220/64305], Loss: 4.7977\n",
      "Epoch [2/2], Step [58230/64305], Loss: 4.6747\n",
      "Epoch [2/2], Step [58240/64305], Loss: 4.9257\n",
      "Epoch [2/2], Step [58250/64305], Loss: 4.7874\n",
      "Epoch [2/2], Step [58260/64305], Loss: 4.8565\n",
      "Epoch [2/2], Step [58270/64305], Loss: 4.7299\n",
      "Epoch [2/2], Step [58280/64305], Loss: 4.7498\n",
      "Epoch [2/2], Step [58290/64305], Loss: 4.7971\n",
      "Epoch [2/2], Step [58300/64305], Loss: 4.8494\n",
      "Epoch [2/2], Step [58310/64305], Loss: 4.7274\n",
      "Epoch [2/2], Step [58320/64305], Loss: 4.8108\n",
      "Epoch [2/2], Step [58330/64305], Loss: 4.7539\n",
      "Epoch [2/2], Step [58340/64305], Loss: 4.9774\n",
      "Epoch [2/2], Step [58350/64305], Loss: 4.7840\n",
      "Epoch [2/2], Step [58360/64305], Loss: 4.7863\n",
      "Epoch [2/2], Step [58370/64305], Loss: 4.7642\n",
      "Epoch [2/2], Step [58380/64305], Loss: 5.0295\n",
      "Epoch [2/2], Step [58390/64305], Loss: 4.7778\n",
      "Epoch [2/2], Step [58400/64305], Loss: 4.7477\n",
      "Epoch [2/2], Step [58410/64305], Loss: 4.5626\n",
      "Epoch [2/2], Step [58420/64305], Loss: 4.8647\n",
      "Epoch [2/2], Step [58430/64305], Loss: 4.9344\n",
      "Epoch [2/2], Step [58440/64305], Loss: 4.8996\n",
      "Epoch [2/2], Step [58450/64305], Loss: 4.7213\n",
      "Epoch [2/2], Step [58460/64305], Loss: 4.8095\n",
      "Epoch [2/2], Step [58470/64305], Loss: 4.7766\n",
      "Epoch [2/2], Step [58480/64305], Loss: 4.6645\n",
      "Epoch [2/2], Step [58490/64305], Loss: 5.0354\n",
      "Epoch [2/2], Step [58500/64305], Loss: 4.8732\n",
      "Epoch [2/2], Step [58510/64305], Loss: 4.8639\n",
      "Epoch [2/2], Step [58520/64305], Loss: 4.9018\n",
      "Epoch [2/2], Step [58530/64305], Loss: 4.7800\n",
      "Epoch [2/2], Step [58540/64305], Loss: 5.0034\n",
      "Epoch [2/2], Step [58550/64305], Loss: 4.8363\n",
      "Epoch [2/2], Step [58560/64305], Loss: 4.8079\n",
      "Epoch [2/2], Step [58570/64305], Loss: 4.7551\n",
      "Epoch [2/2], Step [58580/64305], Loss: 4.8514\n",
      "Epoch [2/2], Step [58590/64305], Loss: 4.8675\n",
      "Epoch [2/2], Step [58600/64305], Loss: 4.7237\n",
      "Epoch [2/2], Step [58610/64305], Loss: 4.8433\n",
      "Epoch [2/2], Step [58620/64305], Loss: 4.8589\n",
      "Epoch [2/2], Step [58630/64305], Loss: 4.5316\n",
      "Epoch [2/2], Step [58640/64305], Loss: 4.8510\n",
      "Epoch [2/2], Step [58650/64305], Loss: 4.8621\n",
      "Epoch [2/2], Step [58660/64305], Loss: 4.6947\n",
      "Epoch [2/2], Step [58670/64305], Loss: 4.7237\n",
      "Epoch [2/2], Step [58680/64305], Loss: 4.9004\n",
      "Epoch [2/2], Step [58690/64305], Loss: 4.9490\n",
      "Epoch [2/2], Step [58700/64305], Loss: 4.9159\n",
      "Epoch [2/2], Step [58710/64305], Loss: 5.0082\n",
      "Epoch [2/2], Step [58720/64305], Loss: 4.8338\n",
      "Epoch [2/2], Step [58730/64305], Loss: 4.6387\n",
      "Epoch [2/2], Step [58740/64305], Loss: 4.8613\n",
      "Epoch [2/2], Step [58750/64305], Loss: 4.8956\n",
      "Epoch [2/2], Step [58760/64305], Loss: 4.7463\n",
      "Epoch [2/2], Step [58770/64305], Loss: 4.7258\n",
      "Epoch [2/2], Step [58780/64305], Loss: 4.9398\n",
      "Epoch [2/2], Step [58790/64305], Loss: 4.7795\n",
      "Epoch [2/2], Step [58800/64305], Loss: 4.8524\n",
      "Epoch [2/2], Step [58810/64305], Loss: 4.6303\n",
      "Epoch [2/2], Step [58820/64305], Loss: 4.8566\n",
      "Epoch [2/2], Step [58830/64305], Loss: 4.9806\n",
      "Epoch [2/2], Step [58840/64305], Loss: 4.9321\n",
      "Epoch [2/2], Step [58850/64305], Loss: 4.8748\n",
      "Epoch [2/2], Step [58860/64305], Loss: 4.8684\n",
      "Epoch [2/2], Step [58870/64305], Loss: 4.7538\n",
      "Epoch [2/2], Step [58880/64305], Loss: 4.9154\n",
      "Epoch [2/2], Step [58890/64305], Loss: 4.8288\n",
      "Epoch [2/2], Step [58900/64305], Loss: 4.9888\n",
      "Epoch [2/2], Step [58910/64305], Loss: 4.7521\n",
      "Epoch [2/2], Step [58920/64305], Loss: 4.7964\n",
      "Epoch [2/2], Step [58930/64305], Loss: 4.6911\n",
      "Epoch [2/2], Step [58940/64305], Loss: 4.6294\n",
      "Epoch [2/2], Step [58950/64305], Loss: 4.8891\n",
      "Epoch [2/2], Step [58960/64305], Loss: 4.7218\n",
      "Epoch [2/2], Step [58970/64305], Loss: 5.0093\n",
      "Epoch [2/2], Step [58980/64305], Loss: 4.9939\n",
      "Epoch [2/2], Step [58990/64305], Loss: 4.8257\n",
      "Epoch [2/2], Step [59000/64305], Loss: 4.8206\n",
      "Epoch [2/2], Step [59010/64305], Loss: 4.8577\n",
      "Epoch [2/2], Step [59020/64305], Loss: 4.7411\n",
      "Epoch [2/2], Step [59030/64305], Loss: 4.6618\n",
      "Epoch [2/2], Step [59040/64305], Loss: 4.8734\n",
      "Epoch [2/2], Step [59050/64305], Loss: 4.7510\n",
      "Epoch [2/2], Step [59060/64305], Loss: 5.0342\n",
      "Epoch [2/2], Step [59070/64305], Loss: 4.8544\n",
      "Epoch [2/2], Step [59080/64305], Loss: 4.8532\n",
      "Epoch [2/2], Step [59090/64305], Loss: 5.0178\n",
      "Epoch [2/2], Step [59100/64305], Loss: 4.9589\n",
      "Epoch [2/2], Step [59110/64305], Loss: 4.8126\n",
      "Epoch [2/2], Step [59120/64305], Loss: 4.5055\n",
      "Epoch [2/2], Step [59130/64305], Loss: 4.5637\n",
      "Epoch [2/2], Step [59140/64305], Loss: 4.8590\n",
      "Epoch [2/2], Step [59150/64305], Loss: 4.7755\n",
      "Epoch [2/2], Step [59160/64305], Loss: 4.9589\n",
      "Epoch [2/2], Step [59170/64305], Loss: 4.7171\n",
      "Epoch [2/2], Step [59180/64305], Loss: 4.9113\n",
      "Epoch [2/2], Step [59190/64305], Loss: 4.7034\n",
      "Epoch [2/2], Step [59200/64305], Loss: 4.5829\n",
      "Epoch [2/2], Step [59210/64305], Loss: 4.7647\n",
      "Epoch [2/2], Step [59220/64305], Loss: 4.6534\n",
      "Epoch [2/2], Step [59230/64305], Loss: 4.8058\n",
      "Epoch [2/2], Step [59240/64305], Loss: 4.8177\n",
      "Epoch [2/2], Step [59250/64305], Loss: 4.9880\n",
      "Epoch [2/2], Step [59260/64305], Loss: 4.7729\n",
      "Epoch [2/2], Step [59270/64305], Loss: 4.8035\n",
      "Epoch [2/2], Step [59280/64305], Loss: 4.7201\n",
      "Epoch [2/2], Step [59290/64305], Loss: 4.7055\n",
      "Epoch [2/2], Step [59300/64305], Loss: 4.8319\n",
      "Epoch [2/2], Step [59310/64305], Loss: 4.7596\n",
      "Epoch [2/2], Step [59320/64305], Loss: 4.8806\n",
      "Epoch [2/2], Step [59330/64305], Loss: 4.8074\n",
      "Epoch [2/2], Step [59340/64305], Loss: 4.8621\n",
      "Epoch [2/2], Step [59350/64305], Loss: 4.7705\n",
      "Epoch [2/2], Step [59360/64305], Loss: 4.8424\n",
      "Epoch [2/2], Step [59370/64305], Loss: 5.0201\n",
      "Epoch [2/2], Step [59380/64305], Loss: 4.9917\n",
      "Epoch [2/2], Step [59390/64305], Loss: 4.7047\n",
      "Epoch [2/2], Step [59400/64305], Loss: 4.9943\n",
      "Epoch [2/2], Step [59410/64305], Loss: 4.6530\n",
      "Epoch [2/2], Step [59420/64305], Loss: 4.8567\n",
      "Epoch [2/2], Step [59430/64305], Loss: 4.9487\n",
      "Epoch [2/2], Step [59440/64305], Loss: 4.9665\n",
      "Epoch [2/2], Step [59450/64305], Loss: 5.0132\n",
      "Epoch [2/2], Step [59460/64305], Loss: 4.8226\n",
      "Epoch [2/2], Step [59470/64305], Loss: 4.7834\n",
      "Epoch [2/2], Step [59480/64305], Loss: 4.7156\n",
      "Epoch [2/2], Step [59490/64305], Loss: 4.8725\n",
      "Epoch [2/2], Step [59500/64305], Loss: 5.1823\n",
      "Epoch [2/2], Step [59510/64305], Loss: 4.8479\n",
      "Epoch [2/2], Step [59520/64305], Loss: 4.9338\n",
      "Epoch [2/2], Step [59530/64305], Loss: 4.7374\n",
      "Epoch [2/2], Step [59540/64305], Loss: 4.9415\n",
      "Epoch [2/2], Step [59550/64305], Loss: 4.6968\n",
      "Epoch [2/2], Step [59560/64305], Loss: 4.7778\n",
      "Epoch [2/2], Step [59570/64305], Loss: 4.6008\n",
      "Epoch [2/2], Step [59580/64305], Loss: 4.7242\n",
      "Epoch [2/2], Step [59590/64305], Loss: 4.9708\n",
      "Epoch [2/2], Step [59600/64305], Loss: 4.7605\n",
      "Epoch [2/2], Step [59610/64305], Loss: 4.8323\n",
      "Epoch [2/2], Step [59620/64305], Loss: 4.8091\n",
      "Epoch [2/2], Step [59630/64305], Loss: 4.9085\n",
      "Epoch [2/2], Step [59640/64305], Loss: 4.7780\n",
      "Epoch [2/2], Step [59650/64305], Loss: 4.8115\n",
      "Epoch [2/2], Step [59660/64305], Loss: 4.9242\n",
      "Epoch [2/2], Step [59670/64305], Loss: 4.8254\n",
      "Epoch [2/2], Step [59680/64305], Loss: 4.7620\n",
      "Epoch [2/2], Step [59690/64305], Loss: 4.6560\n",
      "Epoch [2/2], Step [59700/64305], Loss: 5.0110\n",
      "Epoch [2/2], Step [59710/64305], Loss: 4.8269\n",
      "Epoch [2/2], Step [59720/64305], Loss: 5.0190\n",
      "Epoch [2/2], Step [59730/64305], Loss: 4.8022\n",
      "Epoch [2/2], Step [59740/64305], Loss: 4.9293\n",
      "Epoch [2/2], Step [59750/64305], Loss: 4.6663\n",
      "Epoch [2/2], Step [59760/64305], Loss: 4.9501\n",
      "Epoch [2/2], Step [59770/64305], Loss: 4.8121\n",
      "Epoch [2/2], Step [59780/64305], Loss: 4.8019\n",
      "Epoch [2/2], Step [59790/64305], Loss: 4.9453\n",
      "Epoch [2/2], Step [59800/64305], Loss: 4.8356\n",
      "Epoch [2/2], Step [59810/64305], Loss: 4.8923\n",
      "Epoch [2/2], Step [59820/64305], Loss: 4.8235\n",
      "Epoch [2/2], Step [59830/64305], Loss: 4.9619\n",
      "Epoch [2/2], Step [59840/64305], Loss: 4.8313\n",
      "Epoch [2/2], Step [59850/64305], Loss: 4.8281\n",
      "Epoch [2/2], Step [59860/64305], Loss: 4.8471\n",
      "Epoch [2/2], Step [59870/64305], Loss: 4.9531\n",
      "Epoch [2/2], Step [59880/64305], Loss: 4.8284\n",
      "Epoch [2/2], Step [59890/64305], Loss: 4.8817\n",
      "Epoch [2/2], Step [59900/64305], Loss: 4.6193\n",
      "Epoch [2/2], Step [59910/64305], Loss: 4.9302\n",
      "Epoch [2/2], Step [59920/64305], Loss: 4.7481\n",
      "Epoch [2/2], Step [59930/64305], Loss: 4.8210\n",
      "Epoch [2/2], Step [59940/64305], Loss: 4.7527\n",
      "Epoch [2/2], Step [59950/64305], Loss: 4.9736\n",
      "Epoch [2/2], Step [59960/64305], Loss: 4.8461\n",
      "Epoch [2/2], Step [59970/64305], Loss: 5.0021\n",
      "Epoch [2/2], Step [59980/64305], Loss: 4.8152\n",
      "Epoch [2/2], Step [59990/64305], Loss: 4.6048\n",
      "Epoch [2/2], Step [60000/64305], Loss: 4.8038\n",
      "Epoch [2/2], Step [60010/64305], Loss: 4.7933\n",
      "Epoch [2/2], Step [60020/64305], Loss: 4.7774\n",
      "Epoch [2/2], Step [60030/64305], Loss: 4.7688\n",
      "Epoch [2/2], Step [60040/64305], Loss: 4.7585\n",
      "Epoch [2/2], Step [60050/64305], Loss: 4.8974\n",
      "Epoch [2/2], Step [60060/64305], Loss: 4.9408\n",
      "Epoch [2/2], Step [60070/64305], Loss: 4.7175\n",
      "Epoch [2/2], Step [60080/64305], Loss: 4.9581\n",
      "Epoch [2/2], Step [60090/64305], Loss: 4.9715\n",
      "Epoch [2/2], Step [60100/64305], Loss: 4.6530\n",
      "Epoch [2/2], Step [60110/64305], Loss: 4.9011\n",
      "Epoch [2/2], Step [60120/64305], Loss: 4.8576\n",
      "Epoch [2/2], Step [60130/64305], Loss: 4.7075\n",
      "Epoch [2/2], Step [60140/64305], Loss: 4.7576\n",
      "Epoch [2/2], Step [60150/64305], Loss: 4.7372\n",
      "Epoch [2/2], Step [60160/64305], Loss: 4.7600\n",
      "Epoch [2/2], Step [60170/64305], Loss: 4.8710\n",
      "Epoch [2/2], Step [60180/64305], Loss: 4.5989\n",
      "Epoch [2/2], Step [60190/64305], Loss: 5.0241\n",
      "Epoch [2/2], Step [60200/64305], Loss: 4.9904\n",
      "Epoch [2/2], Step [60210/64305], Loss: 4.8707\n",
      "Epoch [2/2], Step [60220/64305], Loss: 4.9418\n",
      "Epoch [2/2], Step [60230/64305], Loss: 4.7865\n",
      "Epoch [2/2], Step [60240/64305], Loss: 4.7730\n",
      "Epoch [2/2], Step [60250/64305], Loss: 4.8862\n",
      "Epoch [2/2], Step [60260/64305], Loss: 4.8115\n",
      "Epoch [2/2], Step [60270/64305], Loss: 4.8416\n",
      "Epoch [2/2], Step [60280/64305], Loss: 4.9675\n",
      "Epoch [2/2], Step [60290/64305], Loss: 4.7236\n",
      "Epoch [2/2], Step [60300/64305], Loss: 4.6422\n",
      "Epoch [2/2], Step [60310/64305], Loss: 4.8241\n",
      "Epoch [2/2], Step [60320/64305], Loss: 4.8895\n",
      "Epoch [2/2], Step [60330/64305], Loss: 4.8043\n",
      "Epoch [2/2], Step [60340/64305], Loss: 4.8740\n",
      "Epoch [2/2], Step [60350/64305], Loss: 5.0083\n",
      "Epoch [2/2], Step [60360/64305], Loss: 4.6890\n",
      "Epoch [2/2], Step [60370/64305], Loss: 4.7600\n",
      "Epoch [2/2], Step [60380/64305], Loss: 4.9367\n",
      "Epoch [2/2], Step [60390/64305], Loss: 4.6456\n",
      "Epoch [2/2], Step [60400/64305], Loss: 4.6987\n",
      "Epoch [2/2], Step [60410/64305], Loss: 4.7881\n",
      "Epoch [2/2], Step [60420/64305], Loss: 4.7032\n",
      "Epoch [2/2], Step [60430/64305], Loss: 4.6675\n",
      "Epoch [2/2], Step [60440/64305], Loss: 5.0238\n",
      "Epoch [2/2], Step [60450/64305], Loss: 4.9012\n",
      "Epoch [2/2], Step [60460/64305], Loss: 4.9094\n",
      "Epoch [2/2], Step [60470/64305], Loss: 4.8259\n",
      "Epoch [2/2], Step [60480/64305], Loss: 4.8869\n",
      "Epoch [2/2], Step [60490/64305], Loss: 4.8020\n",
      "Epoch [2/2], Step [60500/64305], Loss: 4.8819\n",
      "Epoch [2/2], Step [60510/64305], Loss: 4.8732\n",
      "Epoch [2/2], Step [60520/64305], Loss: 4.9238\n",
      "Epoch [2/2], Step [60530/64305], Loss: 5.0315\n",
      "Epoch [2/2], Step [60540/64305], Loss: 4.8868\n",
      "Epoch [2/2], Step [60550/64305], Loss: 5.0244\n",
      "Epoch [2/2], Step [60560/64305], Loss: 4.7879\n",
      "Epoch [2/2], Step [60570/64305], Loss: 4.8230\n",
      "Epoch [2/2], Step [60580/64305], Loss: 4.8208\n",
      "Epoch [2/2], Step [60590/64305], Loss: 4.9749\n",
      "Epoch [2/2], Step [60600/64305], Loss: 4.7947\n",
      "Epoch [2/2], Step [60610/64305], Loss: 4.7707\n",
      "Epoch [2/2], Step [60620/64305], Loss: 4.6713\n",
      "Epoch [2/2], Step [60630/64305], Loss: 4.8377\n",
      "Epoch [2/2], Step [60640/64305], Loss: 4.8232\n",
      "Epoch [2/2], Step [60650/64305], Loss: 4.7910\n",
      "Epoch [2/2], Step [60660/64305], Loss: 4.8317\n",
      "Epoch [2/2], Step [60670/64305], Loss: 4.8353\n",
      "Epoch [2/2], Step [60680/64305], Loss: 4.7334\n",
      "Epoch [2/2], Step [60690/64305], Loss: 4.7955\n",
      "Epoch [2/2], Step [60700/64305], Loss: 4.8218\n",
      "Epoch [2/2], Step [60710/64305], Loss: 4.7628\n",
      "Epoch [2/2], Step [60720/64305], Loss: 4.9671\n",
      "Epoch [2/2], Step [60730/64305], Loss: 4.8246\n",
      "Epoch [2/2], Step [60740/64305], Loss: 4.7398\n",
      "Epoch [2/2], Step [60750/64305], Loss: 4.6394\n",
      "Epoch [2/2], Step [60760/64305], Loss: 4.9397\n",
      "Epoch [2/2], Step [60770/64305], Loss: 4.9035\n",
      "Epoch [2/2], Step [60780/64305], Loss: 4.8184\n",
      "Epoch [2/2], Step [60790/64305], Loss: 4.7765\n",
      "Epoch [2/2], Step [60800/64305], Loss: 4.7406\n",
      "Epoch [2/2], Step [60810/64305], Loss: 4.9391\n",
      "Epoch [2/2], Step [60820/64305], Loss: 4.8531\n",
      "Epoch [2/2], Step [60830/64305], Loss: 4.8814\n",
      "Epoch [2/2], Step [60840/64305], Loss: 4.9257\n",
      "Epoch [2/2], Step [60850/64305], Loss: 4.8487\n",
      "Epoch [2/2], Step [60860/64305], Loss: 4.8700\n",
      "Epoch [2/2], Step [60870/64305], Loss: 4.8898\n",
      "Epoch [2/2], Step [60880/64305], Loss: 4.8154\n",
      "Epoch [2/2], Step [60890/64305], Loss: 4.8005\n",
      "Epoch [2/2], Step [60900/64305], Loss: 5.0070\n",
      "Epoch [2/2], Step [60910/64305], Loss: 4.7384\n",
      "Epoch [2/2], Step [60920/64305], Loss: 4.7037\n",
      "Epoch [2/2], Step [60930/64305], Loss: 4.9682\n",
      "Epoch [2/2], Step [60940/64305], Loss: 4.8072\n",
      "Epoch [2/2], Step [60950/64305], Loss: 4.8287\n",
      "Epoch [2/2], Step [60960/64305], Loss: 4.7932\n",
      "Epoch [2/2], Step [60970/64305], Loss: 4.7709\n",
      "Epoch [2/2], Step [60980/64305], Loss: 4.9095\n",
      "Epoch [2/2], Step [60990/64305], Loss: 4.7628\n",
      "Epoch [2/2], Step [61000/64305], Loss: 4.5810\n",
      "Epoch [2/2], Step [61010/64305], Loss: 4.9296\n",
      "Epoch [2/2], Step [61020/64305], Loss: 4.8376\n",
      "Epoch [2/2], Step [61030/64305], Loss: 4.9045\n",
      "Epoch [2/2], Step [61040/64305], Loss: 4.8981\n",
      "Epoch [2/2], Step [61050/64305], Loss: 4.8597\n",
      "Epoch [2/2], Step [61060/64305], Loss: 4.7832\n",
      "Epoch [2/2], Step [61070/64305], Loss: 4.8424\n",
      "Epoch [2/2], Step [61080/64305], Loss: 4.7429\n",
      "Epoch [2/2], Step [61090/64305], Loss: 4.7210\n",
      "Epoch [2/2], Step [61100/64305], Loss: 4.8789\n",
      "Epoch [2/2], Step [61110/64305], Loss: 4.9125\n",
      "Epoch [2/2], Step [61120/64305], Loss: 4.7250\n",
      "Epoch [2/2], Step [61130/64305], Loss: 4.9193\n",
      "Epoch [2/2], Step [61140/64305], Loss: 4.8433\n",
      "Epoch [2/2], Step [61150/64305], Loss: 4.7909\n",
      "Epoch [2/2], Step [61160/64305], Loss: 4.6641\n",
      "Epoch [2/2], Step [61170/64305], Loss: 4.8715\n",
      "Epoch [2/2], Step [61180/64305], Loss: 4.6182\n",
      "Epoch [2/2], Step [61190/64305], Loss: 4.7086\n",
      "Epoch [2/2], Step [61200/64305], Loss: 4.7510\n",
      "Epoch [2/2], Step [61210/64305], Loss: 4.8060\n",
      "Epoch [2/2], Step [61220/64305], Loss: 4.6245\n",
      "Epoch [2/2], Step [61230/64305], Loss: 4.9143\n",
      "Epoch [2/2], Step [61240/64305], Loss: 4.7913\n",
      "Epoch [2/2], Step [61250/64305], Loss: 4.7907\n",
      "Epoch [2/2], Step [61260/64305], Loss: 4.5879\n",
      "Epoch [2/2], Step [61270/64305], Loss: 4.7472\n",
      "Epoch [2/2], Step [61280/64305], Loss: 4.8831\n",
      "Epoch [2/2], Step [61290/64305], Loss: 4.5221\n",
      "Epoch [2/2], Step [61300/64305], Loss: 5.0605\n",
      "Epoch [2/2], Step [61310/64305], Loss: 4.7565\n",
      "Epoch [2/2], Step [61320/64305], Loss: 4.8302\n",
      "Epoch [2/2], Step [61330/64305], Loss: 4.6957\n",
      "Epoch [2/2], Step [61340/64305], Loss: 4.8919\n",
      "Epoch [2/2], Step [61350/64305], Loss: 4.7561\n",
      "Epoch [2/2], Step [61360/64305], Loss: 4.7858\n",
      "Epoch [2/2], Step [61370/64305], Loss: 4.8239\n",
      "Epoch [2/2], Step [61380/64305], Loss: 4.8031\n",
      "Epoch [2/2], Step [61390/64305], Loss: 4.8827\n",
      "Epoch [2/2], Step [61400/64305], Loss: 4.7881\n",
      "Epoch [2/2], Step [61410/64305], Loss: 4.8992\n",
      "Epoch [2/2], Step [61420/64305], Loss: 4.6017\n",
      "Epoch [2/2], Step [61430/64305], Loss: 5.0481\n",
      "Epoch [2/2], Step [61440/64305], Loss: 4.9881\n",
      "Epoch [2/2], Step [61450/64305], Loss: 4.7434\n",
      "Epoch [2/2], Step [61460/64305], Loss: 4.6342\n",
      "Epoch [2/2], Step [61470/64305], Loss: 4.7658\n",
      "Epoch [2/2], Step [61480/64305], Loss: 4.7652\n",
      "Epoch [2/2], Step [61490/64305], Loss: 4.8293\n",
      "Epoch [2/2], Step [61500/64305], Loss: 4.9643\n",
      "Epoch [2/2], Step [61510/64305], Loss: 4.7764\n",
      "Epoch [2/2], Step [61520/64305], Loss: 5.1008\n",
      "Epoch [2/2], Step [61530/64305], Loss: 4.8427\n",
      "Epoch [2/2], Step [61540/64305], Loss: 4.7676\n",
      "Epoch [2/2], Step [61550/64305], Loss: 4.9270\n",
      "Epoch [2/2], Step [61560/64305], Loss: 4.7919\n",
      "Epoch [2/2], Step [61570/64305], Loss: 4.5793\n",
      "Epoch [2/2], Step [61580/64305], Loss: 4.7825\n",
      "Epoch [2/2], Step [61590/64305], Loss: 4.8070\n",
      "Epoch [2/2], Step [61600/64305], Loss: 4.6601\n",
      "Epoch [2/2], Step [61610/64305], Loss: 4.8672\n",
      "Epoch [2/2], Step [61620/64305], Loss: 4.6884\n",
      "Epoch [2/2], Step [61630/64305], Loss: 4.6749\n",
      "Epoch [2/2], Step [61640/64305], Loss: 4.8406\n",
      "Epoch [2/2], Step [61650/64305], Loss: 4.8869\n",
      "Epoch [2/2], Step [61660/64305], Loss: 4.9239\n",
      "Epoch [2/2], Step [61670/64305], Loss: 4.8725\n",
      "Epoch [2/2], Step [61680/64305], Loss: 5.0511\n",
      "Epoch [2/2], Step [61690/64305], Loss: 4.9011\n",
      "Epoch [2/2], Step [61700/64305], Loss: 4.8058\n",
      "Epoch [2/2], Step [61710/64305], Loss: 4.9627\n",
      "Epoch [2/2], Step [61720/64305], Loss: 4.8913\n",
      "Epoch [2/2], Step [61730/64305], Loss: 4.6469\n",
      "Epoch [2/2], Step [61740/64305], Loss: 4.4029\n",
      "Epoch [2/2], Step [61750/64305], Loss: 4.6738\n",
      "Epoch [2/2], Step [61760/64305], Loss: 4.9061\n",
      "Epoch [2/2], Step [61770/64305], Loss: 4.7489\n",
      "Epoch [2/2], Step [61780/64305], Loss: 4.7489\n",
      "Epoch [2/2], Step [61790/64305], Loss: 4.8342\n",
      "Epoch [2/2], Step [61800/64305], Loss: 4.8556\n",
      "Epoch [2/2], Step [61810/64305], Loss: 4.8194\n",
      "Epoch [2/2], Step [61820/64305], Loss: 4.7656\n",
      "Epoch [2/2], Step [61830/64305], Loss: 5.0215\n",
      "Epoch [2/2], Step [61840/64305], Loss: 4.6449\n",
      "Epoch [2/2], Step [61850/64305], Loss: 4.8861\n",
      "Epoch [2/2], Step [61860/64305], Loss: 4.8568\n",
      "Epoch [2/2], Step [61870/64305], Loss: 4.7682\n",
      "Epoch [2/2], Step [61880/64305], Loss: 4.9209\n",
      "Epoch [2/2], Step [61890/64305], Loss: 4.7001\n",
      "Epoch [2/2], Step [61900/64305], Loss: 4.8349\n",
      "Epoch [2/2], Step [61910/64305], Loss: 4.7250\n",
      "Epoch [2/2], Step [61920/64305], Loss: 5.0005\n",
      "Epoch [2/2], Step [61930/64305], Loss: 4.8819\n",
      "Epoch [2/2], Step [61940/64305], Loss: 4.8884\n",
      "Epoch [2/2], Step [61950/64305], Loss: 4.7782\n",
      "Epoch [2/2], Step [61960/64305], Loss: 4.6620\n",
      "Epoch [2/2], Step [61970/64305], Loss: 4.8577\n",
      "Epoch [2/2], Step [61980/64305], Loss: 4.6745\n",
      "Epoch [2/2], Step [61990/64305], Loss: 4.8570\n",
      "Epoch [2/2], Step [62000/64305], Loss: 4.8201\n",
      "Epoch [2/2], Step [62010/64305], Loss: 4.7842\n",
      "Epoch [2/2], Step [62020/64305], Loss: 4.7714\n",
      "Epoch [2/2], Step [62030/64305], Loss: 5.0072\n",
      "Epoch [2/2], Step [62040/64305], Loss: 4.7388\n",
      "Epoch [2/2], Step [62050/64305], Loss: 4.9540\n",
      "Epoch [2/2], Step [62060/64305], Loss: 4.8666\n",
      "Epoch [2/2], Step [62070/64305], Loss: 4.9068\n",
      "Epoch [2/2], Step [62080/64305], Loss: 4.8715\n",
      "Epoch [2/2], Step [62090/64305], Loss: 4.7991\n",
      "Epoch [2/2], Step [62100/64305], Loss: 4.9463\n",
      "Epoch [2/2], Step [62110/64305], Loss: 4.8681\n",
      "Epoch [2/2], Step [62120/64305], Loss: 4.6777\n",
      "Epoch [2/2], Step [62130/64305], Loss: 4.7550\n",
      "Epoch [2/2], Step [62140/64305], Loss: 4.7700\n",
      "Epoch [2/2], Step [62150/64305], Loss: 4.9297\n",
      "Epoch [2/2], Step [62160/64305], Loss: 4.7330\n",
      "Epoch [2/2], Step [62170/64305], Loss: 4.9791\n",
      "Epoch [2/2], Step [62180/64305], Loss: 4.7929\n",
      "Epoch [2/2], Step [62190/64305], Loss: 4.7925\n",
      "Epoch [2/2], Step [62200/64305], Loss: 4.8878\n",
      "Epoch [2/2], Step [62210/64305], Loss: 4.8490\n",
      "Epoch [2/2], Step [62220/64305], Loss: 4.7124\n",
      "Epoch [2/2], Step [62230/64305], Loss: 4.8723\n",
      "Epoch [2/2], Step [62240/64305], Loss: 4.7476\n",
      "Epoch [2/2], Step [62250/64305], Loss: 4.9248\n",
      "Epoch [2/2], Step [62260/64305], Loss: 5.1476\n",
      "Epoch [2/2], Step [62270/64305], Loss: 4.8019\n",
      "Epoch [2/2], Step [62280/64305], Loss: 4.6612\n",
      "Epoch [2/2], Step [62290/64305], Loss: 4.8869\n",
      "Epoch [2/2], Step [62300/64305], Loss: 4.9056\n",
      "Epoch [2/2], Step [62310/64305], Loss: 4.9218\n",
      "Epoch [2/2], Step [62320/64305], Loss: 4.8746\n",
      "Epoch [2/2], Step [62330/64305], Loss: 4.8657\n",
      "Epoch [2/2], Step [62340/64305], Loss: 4.5757\n",
      "Epoch [2/2], Step [62350/64305], Loss: 4.8030\n",
      "Epoch [2/2], Step [62360/64305], Loss: 4.5800\n",
      "Epoch [2/2], Step [62370/64305], Loss: 4.7721\n",
      "Epoch [2/2], Step [62380/64305], Loss: 4.6738\n",
      "Epoch [2/2], Step [62390/64305], Loss: 4.8934\n",
      "Epoch [2/2], Step [62400/64305], Loss: 4.8147\n",
      "Epoch [2/2], Step [62410/64305], Loss: 4.8009\n",
      "Epoch [2/2], Step [62420/64305], Loss: 4.6091\n",
      "Epoch [2/2], Step [62430/64305], Loss: 4.6944\n",
      "Epoch [2/2], Step [62440/64305], Loss: 4.6901\n",
      "Epoch [2/2], Step [62450/64305], Loss: 5.0267\n",
      "Epoch [2/2], Step [62460/64305], Loss: 4.8684\n",
      "Epoch [2/2], Step [62470/64305], Loss: 4.6634\n",
      "Epoch [2/2], Step [62480/64305], Loss: 4.6824\n",
      "Epoch [2/2], Step [62490/64305], Loss: 4.8807\n",
      "Epoch [2/2], Step [62500/64305], Loss: 4.6826\n",
      "Epoch [2/2], Step [62510/64305], Loss: 4.8564\n",
      "Epoch [2/2], Step [62520/64305], Loss: 4.8050\n",
      "Epoch [2/2], Step [62530/64305], Loss: 4.7048\n",
      "Epoch [2/2], Step [62540/64305], Loss: 4.9332\n",
      "Epoch [2/2], Step [62550/64305], Loss: 4.7786\n",
      "Epoch [2/2], Step [62560/64305], Loss: 4.8280\n",
      "Epoch [2/2], Step [62570/64305], Loss: 5.0385\n",
      "Epoch [2/2], Step [62580/64305], Loss: 4.5515\n",
      "Epoch [2/2], Step [62590/64305], Loss: 4.7812\n",
      "Epoch [2/2], Step [62600/64305], Loss: 4.9857\n",
      "Epoch [2/2], Step [62610/64305], Loss: 4.7847\n",
      "Epoch [2/2], Step [62620/64305], Loss: 4.8483\n",
      "Epoch [2/2], Step [62630/64305], Loss: 4.7540\n",
      "Epoch [2/2], Step [62640/64305], Loss: 4.6314\n",
      "Epoch [2/2], Step [62650/64305], Loss: 4.7320\n",
      "Epoch [2/2], Step [62660/64305], Loss: 4.7972\n",
      "Epoch [2/2], Step [62670/64305], Loss: 4.8676\n",
      "Epoch [2/2], Step [62680/64305], Loss: 4.7690\n",
      "Epoch [2/2], Step [62690/64305], Loss: 4.9942\n",
      "Epoch [2/2], Step [62700/64305], Loss: 4.7351\n",
      "Epoch [2/2], Step [62710/64305], Loss: 4.8413\n",
      "Epoch [2/2], Step [62720/64305], Loss: 4.8826\n",
      "Epoch [2/2], Step [62730/64305], Loss: 4.7491\n",
      "Epoch [2/2], Step [62740/64305], Loss: 4.8545\n",
      "Epoch [2/2], Step [62750/64305], Loss: 4.7883\n",
      "Epoch [2/2], Step [62760/64305], Loss: 4.8530\n",
      "Epoch [2/2], Step [62770/64305], Loss: 4.7699\n",
      "Epoch [2/2], Step [62780/64305], Loss: 4.8008\n",
      "Epoch [2/2], Step [62790/64305], Loss: 4.7083\n",
      "Epoch [2/2], Step [62800/64305], Loss: 4.8473\n",
      "Epoch [2/2], Step [62810/64305], Loss: 5.1317\n",
      "Epoch [2/2], Step [62820/64305], Loss: 4.7394\n",
      "Epoch [2/2], Step [62830/64305], Loss: 4.8586\n",
      "Epoch [2/2], Step [62840/64305], Loss: 4.8959\n",
      "Epoch [2/2], Step [62850/64305], Loss: 4.8845\n",
      "Epoch [2/2], Step [62860/64305], Loss: 4.8554\n",
      "Epoch [2/2], Step [62870/64305], Loss: 4.9010\n",
      "Epoch [2/2], Step [62880/64305], Loss: 4.8367\n",
      "Epoch [2/2], Step [62890/64305], Loss: 4.7847\n",
      "Epoch [2/2], Step [62900/64305], Loss: 4.9099\n",
      "Epoch [2/2], Step [62910/64305], Loss: 4.9720\n",
      "Epoch [2/2], Step [62920/64305], Loss: 4.9198\n",
      "Epoch [2/2], Step [62930/64305], Loss: 4.8749\n",
      "Epoch [2/2], Step [62940/64305], Loss: 4.7189\n",
      "Epoch [2/2], Step [62950/64305], Loss: 5.0264\n",
      "Epoch [2/2], Step [62960/64305], Loss: 4.7561\n",
      "Epoch [2/2], Step [62970/64305], Loss: 4.7722\n",
      "Epoch [2/2], Step [62980/64305], Loss: 4.6456\n",
      "Epoch [2/2], Step [62990/64305], Loss: 4.7552\n",
      "Epoch [2/2], Step [63000/64305], Loss: 4.7302\n",
      "Epoch [2/2], Step [63010/64305], Loss: 4.8326\n",
      "Epoch [2/2], Step [63020/64305], Loss: 4.8590\n",
      "Epoch [2/2], Step [63030/64305], Loss: 4.8419\n",
      "Epoch [2/2], Step [63040/64305], Loss: 4.7941\n",
      "Epoch [2/2], Step [63050/64305], Loss: 4.9951\n",
      "Epoch [2/2], Step [63060/64305], Loss: 4.8156\n",
      "Epoch [2/2], Step [63070/64305], Loss: 4.7287\n",
      "Epoch [2/2], Step [63080/64305], Loss: 4.8731\n",
      "Epoch [2/2], Step [63090/64305], Loss: 4.9827\n",
      "Epoch [2/2], Step [63100/64305], Loss: 4.8083\n",
      "Epoch [2/2], Step [63110/64305], Loss: 4.8834\n",
      "Epoch [2/2], Step [63120/64305], Loss: 4.8942\n",
      "Epoch [2/2], Step [63130/64305], Loss: 4.5472\n",
      "Epoch [2/2], Step [63140/64305], Loss: 4.9941\n",
      "Epoch [2/2], Step [63150/64305], Loss: 4.8614\n",
      "Epoch [2/2], Step [63160/64305], Loss: 4.9343\n",
      "Epoch [2/2], Step [63170/64305], Loss: 4.7754\n",
      "Epoch [2/2], Step [63180/64305], Loss: 4.7691\n",
      "Epoch [2/2], Step [63190/64305], Loss: 4.7597\n",
      "Epoch [2/2], Step [63200/64305], Loss: 4.9966\n",
      "Epoch [2/2], Step [63210/64305], Loss: 4.7738\n",
      "Epoch [2/2], Step [63220/64305], Loss: 4.6879\n",
      "Epoch [2/2], Step [63230/64305], Loss: 4.7906\n",
      "Epoch [2/2], Step [63240/64305], Loss: 4.9160\n",
      "Epoch [2/2], Step [63250/64305], Loss: 4.8071\n",
      "Epoch [2/2], Step [63260/64305], Loss: 4.8320\n",
      "Epoch [2/2], Step [63270/64305], Loss: 4.6410\n",
      "Epoch [2/2], Step [63280/64305], Loss: 4.7561\n",
      "Epoch [2/2], Step [63290/64305], Loss: 4.5885\n",
      "Epoch [2/2], Step [63300/64305], Loss: 4.7273\n",
      "Epoch [2/2], Step [63310/64305], Loss: 4.8697\n",
      "Epoch [2/2], Step [63320/64305], Loss: 4.6712\n",
      "Epoch [2/2], Step [63330/64305], Loss: 4.6864\n",
      "Epoch [2/2], Step [63340/64305], Loss: 5.0532\n",
      "Epoch [2/2], Step [63350/64305], Loss: 4.7701\n",
      "Epoch [2/2], Step [63360/64305], Loss: 4.7812\n",
      "Epoch [2/2], Step [63370/64305], Loss: 5.0118\n",
      "Epoch [2/2], Step [63380/64305], Loss: 4.7718\n",
      "Epoch [2/2], Step [63390/64305], Loss: 4.7527\n",
      "Epoch [2/2], Step [63400/64305], Loss: 4.9866\n",
      "Epoch [2/2], Step [63410/64305], Loss: 4.7531\n",
      "Epoch [2/2], Step [63420/64305], Loss: 4.7099\n",
      "Epoch [2/2], Step [63430/64305], Loss: 4.7839\n",
      "Epoch [2/2], Step [63440/64305], Loss: 4.8228\n",
      "Epoch [2/2], Step [63450/64305], Loss: 4.8124\n",
      "Epoch [2/2], Step [63460/64305], Loss: 4.7232\n",
      "Epoch [2/2], Step [63470/64305], Loss: 4.6774\n",
      "Epoch [2/2], Step [63480/64305], Loss: 4.9845\n",
      "Epoch [2/2], Step [63490/64305], Loss: 4.6989\n",
      "Epoch [2/2], Step [63500/64305], Loss: 5.0829\n",
      "Epoch [2/2], Step [63510/64305], Loss: 4.8665\n",
      "Epoch [2/2], Step [63520/64305], Loss: 4.8532\n",
      "Epoch [2/2], Step [63530/64305], Loss: 4.7582\n",
      "Epoch [2/2], Step [63540/64305], Loss: 4.7557\n",
      "Epoch [2/2], Step [63550/64305], Loss: 4.7361\n",
      "Epoch [2/2], Step [63560/64305], Loss: 4.8962\n",
      "Epoch [2/2], Step [63570/64305], Loss: 4.8242\n",
      "Epoch [2/2], Step [63580/64305], Loss: 4.8812\n",
      "Epoch [2/2], Step [63590/64305], Loss: 4.8355\n",
      "Epoch [2/2], Step [63600/64305], Loss: 4.8283\n",
      "Epoch [2/2], Step [63610/64305], Loss: 4.9258\n",
      "Epoch [2/2], Step [63620/64305], Loss: 5.0864\n",
      "Epoch [2/2], Step [63630/64305], Loss: 4.8453\n",
      "Epoch [2/2], Step [63640/64305], Loss: 4.8643\n",
      "Epoch [2/2], Step [63650/64305], Loss: 4.8400\n",
      "Epoch [2/2], Step [63660/64305], Loss: 4.8021\n",
      "Epoch [2/2], Step [63670/64305], Loss: 4.6553\n",
      "Epoch [2/2], Step [63680/64305], Loss: 4.9316\n",
      "Epoch [2/2], Step [63690/64305], Loss: 4.6330\n",
      "Epoch [2/2], Step [63700/64305], Loss: 4.6650\n",
      "Epoch [2/2], Step [63710/64305], Loss: 4.8908\n",
      "Epoch [2/2], Step [63720/64305], Loss: 4.7484\n",
      "Epoch [2/2], Step [63730/64305], Loss: 5.0271\n",
      "Epoch [2/2], Step [63740/64305], Loss: 4.8543\n",
      "Epoch [2/2], Step [63750/64305], Loss: 4.6787\n",
      "Epoch [2/2], Step [63760/64305], Loss: 4.6645\n",
      "Epoch [2/2], Step [63770/64305], Loss: 4.8219\n",
      "Epoch [2/2], Step [63780/64305], Loss: 4.9625\n",
      "Epoch [2/2], Step [63790/64305], Loss: 4.8472\n",
      "Epoch [2/2], Step [63800/64305], Loss: 4.7454\n",
      "Epoch [2/2], Step [63810/64305], Loss: 4.6853\n",
      "Epoch [2/2], Step [63820/64305], Loss: 4.8408\n",
      "Epoch [2/2], Step [63830/64305], Loss: 4.9608\n",
      "Epoch [2/2], Step [63840/64305], Loss: 4.9348\n",
      "Epoch [2/2], Step [63850/64305], Loss: 4.7688\n",
      "Epoch [2/2], Step [63860/64305], Loss: 4.8807\n",
      "Epoch [2/2], Step [63870/64305], Loss: 4.7204\n",
      "Epoch [2/2], Step [63880/64305], Loss: 4.8011\n",
      "Epoch [2/2], Step [63890/64305], Loss: 5.0342\n",
      "Epoch [2/2], Step [63900/64305], Loss: 4.8339\n",
      "Epoch [2/2], Step [63910/64305], Loss: 4.9277\n",
      "Epoch [2/2], Step [63920/64305], Loss: 5.0241\n",
      "Epoch [2/2], Step [63930/64305], Loss: 4.7925\n",
      "Epoch [2/2], Step [63940/64305], Loss: 4.7241\n",
      "Epoch [2/2], Step [63950/64305], Loss: 4.8550\n",
      "Epoch [2/2], Step [63960/64305], Loss: 4.8345\n",
      "Epoch [2/2], Step [63970/64305], Loss: 4.6227\n",
      "Epoch [2/2], Step [63980/64305], Loss: 4.7517\n",
      "Epoch [2/2], Step [63990/64305], Loss: 4.7393\n",
      "Epoch [2/2], Step [64000/64305], Loss: 4.8339\n",
      "Epoch [2/2], Step [64010/64305], Loss: 4.5458\n",
      "Epoch [2/2], Step [64020/64305], Loss: 4.8367\n",
      "Epoch [2/2], Step [64030/64305], Loss: 4.7580\n",
      "Epoch [2/2], Step [64040/64305], Loss: 4.7535\n",
      "Epoch [2/2], Step [64050/64305], Loss: 4.8255\n",
      "Epoch [2/2], Step [64060/64305], Loss: 4.8255\n",
      "Epoch [2/2], Step [64070/64305], Loss: 4.6984\n",
      "Epoch [2/2], Step [64080/64305], Loss: 4.7353\n",
      "Epoch [2/2], Step [64090/64305], Loss: 4.7952\n",
      "Epoch [2/2], Step [64100/64305], Loss: 4.7480\n",
      "Epoch [2/2], Step [64110/64305], Loss: 4.6729\n",
      "Epoch [2/2], Step [64120/64305], Loss: 4.8479\n",
      "Epoch [2/2], Step [64130/64305], Loss: 4.9133\n",
      "Epoch [2/2], Step [64140/64305], Loss: 4.7051\n",
      "Epoch [2/2], Step [64150/64305], Loss: 4.7071\n",
      "Epoch [2/2], Step [64160/64305], Loss: 4.8441\n",
      "Epoch [2/2], Step [64170/64305], Loss: 4.8244\n",
      "Epoch [2/2], Step [64180/64305], Loss: 4.9973\n",
      "Epoch [2/2], Step [64190/64305], Loss: 4.7234\n",
      "Epoch [2/2], Step [64200/64305], Loss: 4.8512\n",
      "Epoch [2/2], Step [64210/64305], Loss: 4.6969\n",
      "Epoch [2/2], Step [64220/64305], Loss: 4.8308\n",
      "Epoch [2/2], Step [64230/64305], Loss: 4.8235\n",
      "Epoch [2/2], Step [64240/64305], Loss: 4.9171\n",
      "Epoch [2/2], Step [64250/64305], Loss: 4.7448\n",
      "Epoch [2/2], Step [64260/64305], Loss: 4.6369\n",
      "Epoch [2/2], Step [64270/64305], Loss: 4.8359\n",
      "Epoch [2/2], Step [64280/64305], Loss: 4.6673\n",
      "Epoch [2/2], Step [64290/64305], Loss: 4.7554\n",
      "Epoch [2/2], Step [64300/64305], Loss: 4.7744\n",
      "Epoch [2/2] Average Loss: 4.8453, Perplexity: 127.14\n"
     ]
    }
   ],
   "source": [
    "class LanguageModelWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, attention_dim, context_length, hidden_dim=256, num_heads=8, dropout=0.2):\n",
    "        super(LanguageModelWithAttention, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(context_length, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attention = MultiHeadCausalAttention(embedding_dim, attention_dim, num_heads, context_length, dropout)\n",
    "\n",
    "        # Adding more layers and non-linear activation functions\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0)\n",
    "        token_embeds = self.token_embedding(x)\n",
    "        position_embeds = self.position_embedding(positions)\n",
    "\n",
    "        embeddings = token_embeds + position_embeds\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        # Apply the attention layer\n",
    "        context, attention_weights = self.attention(embeddings)\n",
    "\n",
    "        # Apply the feedforward layers\n",
    "        out = self.fc1(context)\n",
    "        out = self.relu(out)\n",
    "        logits = self.fc2(out)\n",
    "        return logits, attention_weights\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "context_length = 32  # Increased context size\n",
    "# vocab_size = tokenizer.n_vocab\n",
    "vocab_size = len(tokenizer)\n",
    "embedding_dim = 128\n",
    "attention_dim = 64\n",
    "hidden_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_dataloader(\n",
    "    formatted_text[:9999297], batch_size=batch_size,\n",
    "    context_length=context_length, shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = LanguageModelWithAttention(\n",
    "    vocab_size, embedding_dim, attention_dim, context_length, hidden_dim, num_heads, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters'\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses_attention = []\n",
    "perplexities_attention = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(x)\n",
    "        \n",
    "        # Reshape logits and targets for loss computation\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step \"\n",
    "                  f\"[{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    train_losses_attention.append(avg_loss)\n",
    "    perplexities_attention.append(perplexity)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T17:49:52.145244Z",
     "iopub.status.busy": "2024-12-14T17:49:52.144608Z",
     "iopub.status.idle": "2024-12-14T17:50:12.938895Z",
     "shell.execute_reply": "2024-12-14T17:50:12.937947Z",
     "shell.execute_reply.started": "2024-12-14T17:49:52.145210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for the model with attention: 101.05\n",
      "4.615642619981313\n",
      "37100.53537940979\n"
     ]
    }
   ],
   "source": [
    "steps = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(steps, train_losses_attention, label='Training Loss')\n",
    "plt.title('Model with attention: Training loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps, perplexities_attention, label='Perplexity')\n",
    "plt.title('Model with attention: Perplexity over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in dev_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits, _ = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(dev_dataloader)\n",
    "perplexity_attention = math.exp(avg_loss)\n",
    "print(f\"Perplexity for the model with attention: {perplexity_attention:.2f}\")\n",
    "print(avg_loss)\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T17:50:21.337778Z",
     "iopub.status.busy": "2024-12-14T17:50:21.337416Z",
     "iopub.status.idle": "2024-12-14T17:50:43.692906Z",
     "shell.execute_reply": "2024-12-14T17:50:43.691967Z",
     "shell.execute_reply.started": "2024-12-14T17:50:21.337749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "\n",
      "[CLS] من در راه[SEP] طبیعی می داشت که دو برنامه تا در دقایق اولیه آلودهکاری های اساسی را اقدام کنند 70243 ما هدف یک\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, start_text, context_length=32, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = tokenizer.encode(start_text)\n",
    "    context = torch.tensor(generated, dtype=torch.long,\n",
    "                          device=device).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(context_length):\n",
    "            if context.size(1) >= context_length:\n",
    "                break\n",
    "            logits, _ = model(context)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "            context = torch.cat(\n",
    "                [context, next_token_id.unsqueeze(0)], dim=1\n",
    "            )\n",
    "    \n",
    "    generated_text = tokenizer.decode(context[0].tolist())\n",
    "    return generated_text\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "token_ids = tokenizer.encode(formatted_text)\n",
    "\n",
    "start_text = \" من در راه\"\n",
    "generated_text = generate_text(model, tokenizer, start_text, context_length=32)\n",
    "print(\"Generated Text:\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "؟؟  من در راه ؟؟ طبیعی می داشت که دو برنامه تا در دقایق اولیه آلودهکاری های اساسی را اقدام کنند 70243"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01bed83d6aa24f02b4e7db2bec55c4c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7cfe08cd17204325a054c2f91df50a71",
       "IPY_MODEL_86d4c87a6dbc4b8c9d6e556a3f2a1cb6",
       "IPY_MODEL_210269387ff0465abd4e0180e2f5aa9c"
      ],
      "layout": "IPY_MODEL_27eda8e4981d405fa68d93b7e9253ecb"
     }
    },
    "04b90e34c23748719c4e0701232c4768": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11016f9bfda04e03a540dbf7dadf0e83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12849537afda475babb0f262a020a6b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "164252500b5c4fadb4a71e374eb59ccd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19e205c527564c5893208d3042dbadfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adc332f5f7f9428988e874099f2aadb2",
      "placeholder": "​",
      "style": "IPY_MODEL_4b36b320618b4a95a6f83d5fa0ddb38a",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "1a34a113bff84f5bb2fd7a50f13f28a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "210269387ff0465abd4e0180e2f5aa9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12849537afda475babb0f262a020a6b7",
      "placeholder": "​",
      "style": "IPY_MODEL_d88feef5efa94b1aaea2cd93e14ae749",
      "value": " 350/350 [00:00&lt;00:00, 2.85kB/s]"
     }
    },
    "22a6dcf3b5664d97afa9d4ec886f9d44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25c99daad60a449ca3416381d78775dd",
      "max": 1331,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1a34a113bff84f5bb2fd7a50f13f28a9",
      "value": 1331
     }
    },
    "22d8698acb254230b9ff4c19ba5fa2fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25c99daad60a449ca3416381d78775dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27eda8e4981d405fa68d93b7e9253ecb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29243de39619472b899ece2f3fd07fae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ae3dcc70d836478baa062ec078d134f5",
       "IPY_MODEL_56f5c75b40c148fe96e2da20f74e5a28",
       "IPY_MODEL_5926d35075cf4f8ebce4f3e9a372e849"
      ],
      "layout": "IPY_MODEL_69f2eb3543174ac19ffeeefe5404502e"
     }
    },
    "2cf419a938734e58a2692aab2c3c8fbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "389fa9fd3f16427c9dc5352ee8d4de2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_504430e2c5354bfa919ad804eeb19033",
      "placeholder": "​",
      "style": "IPY_MODEL_2cf419a938734e58a2692aab2c3c8fbd",
      "value": "spiece.model: 100%"
     }
    },
    "3ac4e0a264ad408d80441b2a65efc2a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44159909c9454a89ae4f47d5108cc753": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b36b320618b4a95a6f83d5fa0ddb38a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "504430e2c5354bfa919ad804eeb19033": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52a9618414cc4f9a99bfff752afa9b26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "56f5c75b40c148fe96e2da20f74e5a28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11016f9bfda04e03a540dbf7dadf0e83",
      "max": 1127358,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a0deefa4eae84ce1b5c7dd494d0d2d82",
      "value": 1127358
     }
    },
    "5926d35075cf4f8ebce4f3e9a372e849": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df2ff6d447e64ef1ba57a48d89c52a3b",
      "placeholder": "​",
      "style": "IPY_MODEL_77f24dbcfa3d475b8cd2b207e922a3e3",
      "value": " 1.13M/1.13M [00:00&lt;00:00, 9.72MB/s]"
     }
    },
    "5c3568387cb64ea39f6cae2f9445d4d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbded0f35c904710b6f1dc03077ffeb2",
      "placeholder": "​",
      "style": "IPY_MODEL_db4c061e45b24ec2919d99942b1059c1",
      "value": " 537k/537k [00:00&lt;00:00, 3.89MB/s]"
     }
    },
    "5e05c683a6b540b69e5b2e9a9bff86ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "679bcc1607184edcba1541d44e83b086": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "69f2eb3543174ac19ffeeefe5404502e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76afda07719549e4aee9317bc8fa3414": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_19e205c527564c5893208d3042dbadfd",
       "IPY_MODEL_7ddb0b8085014e67a5eac5721edd18f3",
       "IPY_MODEL_dab3c6fdd5994f0ea90fb0925f81c4cd"
      ],
      "layout": "IPY_MODEL_164252500b5c4fadb4a71e374eb59ccd"
     }
    },
    "771bec1698054cd29c3249e29d0b763f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae81cf31445e4b799f152e9d22c6cafc",
      "placeholder": "​",
      "style": "IPY_MODEL_44159909c9454a89ae4f47d5108cc753",
      "value": "config.json: 100%"
     }
    },
    "77f24dbcfa3d475b8cd2b207e922a3e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7cfe08cd17204325a054c2f91df50a71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89b9acbf76af403f981cedf3b4cfcf52",
      "placeholder": "​",
      "style": "IPY_MODEL_83909cb2409b4fd098a047e13cb2f455",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "7ddb0b8085014e67a5eac5721edd18f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ac4e0a264ad408d80441b2a65efc2a7",
      "max": 399,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_52a9618414cc4f9a99bfff752afa9b26",
      "value": 399
     }
    },
    "8362237777d740e38e4b684bf8855ac5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83909cb2409b4fd098a047e13cb2f455": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8518d5962366481098a65e337956d3bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_771bec1698054cd29c3249e29d0b763f",
       "IPY_MODEL_22a6dcf3b5664d97afa9d4ec886f9d44",
       "IPY_MODEL_d0a416701cbf48b0aef255b2bf66e053"
      ],
      "layout": "IPY_MODEL_04b90e34c23748719c4e0701232c4768"
     }
    },
    "86d4c87a6dbc4b8c9d6e556a3f2a1cb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4ed39a07db24bed8f87a92f6f547ac9",
      "max": 350,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_679bcc1607184edcba1541d44e83b086",
      "value": 350
     }
    },
    "89b9acbf76af403f981cedf3b4cfcf52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c08d01af7d94448b15f6e8f58b930fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d370637fd8dc404dbe9f820040347eea",
      "max": 537052,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c99a918efed94df5a47365a354d63544",
      "value": 537052
     }
    },
    "a0deefa4eae84ce1b5c7dd494d0d2d82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a0e841ebaf2846398754e87c073c07d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a99954dd7ecb4fe3a00e4f0ab45ca91f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adc332f5f7f9428988e874099f2aadb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae3dcc70d836478baa062ec078d134f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e05c683a6b540b69e5b2e9a9bff86ce",
      "placeholder": "​",
      "style": "IPY_MODEL_ef1fde69336d4dc7b6c899c62ad9db71",
      "value": "tokenizer.json: 100%"
     }
    },
    "ae81cf31445e4b799f152e9d22c6cafc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c99a918efed94df5a47365a354d63544": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d0a416701cbf48b0aef255b2bf66e053": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a99954dd7ecb4fe3a00e4f0ab45ca91f",
      "placeholder": "​",
      "style": "IPY_MODEL_a0e841ebaf2846398754e87c073c07d1",
      "value": " 1.33k/1.33k [00:00&lt;00:00, 15.8kB/s]"
     }
    },
    "d370637fd8dc404dbe9f820040347eea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d88feef5efa94b1aaea2cd93e14ae749": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dab3c6fdd5994f0ea90fb0925f81c4cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_22d8698acb254230b9ff4c19ba5fa2fb",
      "placeholder": "​",
      "style": "IPY_MODEL_db612cd21e9a4ac385bc587c653ab51f",
      "value": " 399/399 [00:00&lt;00:00, 2.28kB/s]"
     }
    },
    "db4c061e45b24ec2919d99942b1059c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db612cd21e9a4ac385bc587c653ab51f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dbaf318333ec413f88da4fdc8e2be402": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_389fa9fd3f16427c9dc5352ee8d4de2d",
       "IPY_MODEL_9c08d01af7d94448b15f6e8f58b930fc",
       "IPY_MODEL_5c3568387cb64ea39f6cae2f9445d4d5"
      ],
      "layout": "IPY_MODEL_8362237777d740e38e4b684bf8855ac5"
     }
    },
    "dbded0f35c904710b6f1dc03077ffeb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df2ff6d447e64ef1ba57a48d89c52a3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4ed39a07db24bed8f87a92f6f547ac9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef1fde69336d4dc7b6c899c62ad9db71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
