{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e37dcc2-c3bc-4a74-a1bb-bc92eb07fb90",
   "metadata": {},
   "source": [
    "# Persian Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725a2d7-bfad-4627-8d78-e4a3260e349b",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4399cd45-4657-45db-8d2e-bc68114309af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#! pip install tokenizers scikit-learn --user \n",
    "#! pip install hazm --user \n",
    "#! pip install tiktoken --user \n",
    "#! pip install transformers --user\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import trainers\n",
    "from tokenizers.normalizers import StripAccents, Lowercase, Sequence\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer, UnigramTrainer\n",
    "from tokenizers.models import BPE, Unigram\n",
    "from transformers import  AutoTokenizer  #pipeline, GPT2LMHeadModel\n",
    "\n",
    "from hazm import * \n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b04b71-e698-4f30-aa08-3c6ebf89a799",
   "metadata": {},
   "source": [
    "## Main Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47420bce-2049-48ce-95bb-be757e2bcf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:  100000  lines seperators replaced\n",
      "Total lines replaced 95195\n",
      "Total lines replaced 40842\n",
      "Total lines replaced 1591\n"
     ]
    }
   ],
   "source": [
    "from src.helper  import clean_pers_text_replace, get_cleaned_text\n",
    "text_path = \"content/fas_news_2020_100K/fas_news_2020_100K-sentences.txt\"\n",
    "path_to_save_folder= \"model/train_data_pers\"\n",
    "\n",
    "raw_text = get_cleaned_text(text_path,clean_pers_text_replace)\n",
    "#enc_text = tokenizer.encode(raw_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82feb7f-6340-4cba-bf68-2b02c05fa169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import GPTDataset\n",
    "from src.dataset import create_dataloader\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "\n",
    "# Parameters\n",
    "batch_size = 128\n",
    "context_length = 32  # Context size for training\n",
    "vocab_size =  30000#tokenizer.n_vocab\n",
    "embedding_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea314dc7-1e1e-44b6-8629-c25e067e8b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create Dataset 2720000 / 2733504"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_dataloader(\n",
    "    raw_text,tokenizer = tokenizer,allowed_special=False, batch_size=batch_size, \n",
    "    context_length=context_length, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609d7700-93a5-4454-a67e-797ff70ea489",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f39c1374-f0b7-4bf1-9919-406e2302922c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create Dataset 2720000 / 2733504Epoch [1/5], Step [0/68337], Loss: 10.3052\n",
      "Validation perplexity: 28175.770958887126\n",
      "Epoch [1/5], Step [75/68337], Loss: 7.1304\n",
      "Epoch [1/5], Step [150/68337], Loss: 6.9182\n",
      "Epoch [1/5], Step [225/68337], Loss: 6.9815\n",
      "Epoch [1/5], Step [300/68337], Loss: 6.7851\n",
      "Epoch [1/5], Step [375/68337], Loss: 6.7210\n",
      "Epoch [1/5], Step [450/68337], Loss: 6.5990\n",
      "Epoch [1/5], Step [525/68337], Loss: 6.4429\n",
      "Epoch [1/5], Step [600/68337], Loss: 6.4764\n",
      "Epoch [1/5], Step [675/68337], Loss: 6.5710\n",
      "Epoch [1/5], Step [750/68337], Loss: 6.4239\n",
      "Epoch [1/5], Step [825/68337], Loss: 6.5475\n",
      "Epoch [1/5], Step [900/68337], Loss: 6.7137\n",
      "Epoch [1/5], Step [975/68337], Loss: 6.6781\n",
      "Epoch [1/5], Step [1050/68337], Loss: 6.3683\n",
      "Epoch [1/5], Step [1125/68337], Loss: 6.3483\n",
      "Epoch [1/5], Step [1200/68337], Loss: 6.4439\n",
      "Epoch [1/5], Step [1275/68337], Loss: 6.2980\n",
      "Epoch [1/5], Step [1350/68337], Loss: 6.3087\n",
      "Epoch [1/5], Step [1425/68337], Loss: 6.1882\n",
      "Epoch [1/5], Step [1500/68337], Loss: 6.0350\n",
      "Epoch [1/5], Step [1575/68337], Loss: 6.3959\n",
      "Epoch [1/5], Step [1650/68337], Loss: 6.3711\n",
      "Epoch [1/5], Step [1725/68337], Loss: 6.2595\n",
      "Epoch [1/5], Step [1800/68337], Loss: 6.1808\n",
      "Epoch [1/5], Step [1875/68337], Loss: 6.3170\n",
      "Epoch [1/5], Step [1950/68337], Loss: 6.0546\n",
      "Epoch [1/5], Step [2025/68337], Loss: 6.0996\n",
      "Epoch [1/5], Step [2100/68337], Loss: 5.9933\n",
      "Epoch [1/5], Step [2175/68337], Loss: 6.0513\n",
      "Epoch [1/5], Step [2250/68337], Loss: 6.1205\n",
      "Epoch [1/5], Step [2325/68337], Loss: 5.8498\n",
      "Epoch [1/5], Step [2400/68337], Loss: 5.9775\n",
      "Epoch [1/5], Step [2475/68337], Loss: 6.0362\n",
      "Epoch [1/5], Step [2550/68337], Loss: 6.1021\n",
      "Epoch [1/5], Step [2625/68337], Loss: 6.2195\n",
      "Epoch [1/5], Step [2700/68337], Loss: 5.9494\n",
      "Epoch [1/5], Step [2775/68337], Loss: 5.7279\n",
      "Epoch [1/5], Step [2850/68337], Loss: 5.9007\n",
      "Epoch [1/5], Step [2925/68337], Loss: 6.1501\n",
      "Epoch [1/5], Step [3000/68337], Loss: 5.9601\n",
      "Epoch [1/5], Step [3075/68337], Loss: 6.0501\n",
      "Epoch [1/5], Step [3150/68337], Loss: 6.0052\n",
      "Epoch [1/5], Step [3225/68337], Loss: 5.9225\n",
      "Epoch [1/5], Step [3300/68337], Loss: 6.0678\n",
      "Epoch [1/5], Step [3375/68337], Loss: 5.6787\n",
      "Epoch [1/5], Step [3450/68337], Loss: 5.9319\n",
      "Epoch [1/5], Step [3525/68337], Loss: 5.8475\n",
      "Epoch [1/5], Step [3600/68337], Loss: 5.7793\n",
      "Epoch [1/5], Step [3675/68337], Loss: 5.9475\n",
      "Epoch [1/5], Step [3750/68337], Loss: 5.5862\n",
      "Epoch [1/5], Step [3825/68337], Loss: 5.9647\n",
      "Epoch [1/5], Step [3900/68337], Loss: 5.6967\n",
      "Epoch [1/5], Step [3975/68337], Loss: 6.2198\n",
      "Epoch [1/5], Step [4050/68337], Loss: 5.8970\n",
      "Epoch [1/5], Step [4125/68337], Loss: 5.7011\n",
      "Epoch [1/5], Step [4200/68337], Loss: 5.8653\n",
      "Epoch [1/5], Step [4275/68337], Loss: 6.0432\n",
      "Epoch [1/5], Step [4350/68337], Loss: 5.8911\n",
      "Epoch [1/5], Step [4425/68337], Loss: 5.7919\n",
      "Epoch [1/5], Step [4500/68337], Loss: 5.9082\n",
      "Epoch [1/5], Step [4575/68337], Loss: 6.0426\n",
      "Epoch [1/5], Step [4650/68337], Loss: 5.7987\n",
      "Epoch [1/5], Step [4725/68337], Loss: 5.7488\n",
      "Epoch [1/5], Step [4800/68337], Loss: 5.8311\n",
      "Epoch [1/5], Step [4875/68337], Loss: 5.7575\n",
      "Epoch [1/5], Step [4950/68337], Loss: 5.8079\n",
      "Epoch [1/5], Step [5025/68337], Loss: 5.6567\n",
      "Epoch [1/5], Step [5100/68337], Loss: 5.8030\n",
      "Epoch [1/5], Step [5175/68337], Loss: 5.9807\n",
      "Epoch [1/5], Step [5250/68337], Loss: 5.8881\n",
      "Epoch [1/5], Step [5325/68337], Loss: 5.6709\n",
      "Epoch [1/5], Step [5400/68337], Loss: 5.8714\n",
      "Epoch [1/5], Step [5475/68337], Loss: 5.6765\n",
      "Epoch [1/5], Step [5550/68337], Loss: 5.6907\n",
      "Epoch [1/5], Step [5625/68337], Loss: 5.6542\n",
      "Epoch [1/5], Step [5700/68337], Loss: 5.6589\n",
      "Epoch [1/5], Step [5775/68337], Loss: 5.5855\n",
      "Epoch [1/5], Step [5850/68337], Loss: 5.6686\n",
      "Epoch [1/5], Step [5925/68337], Loss: 5.8650\n",
      "Epoch [1/5], Step [6000/68337], Loss: 5.6473\n",
      "Epoch [1/5], Step [6075/68337], Loss: 5.5544\n",
      "Epoch [1/5], Step [6150/68337], Loss: 5.8289\n",
      "Epoch [1/5], Step [6225/68337], Loss: 5.7614\n",
      "Epoch [1/5], Step [6300/68337], Loss: 5.6010\n",
      "Epoch [1/5], Step [6375/68337], Loss: 5.7463\n",
      "Epoch [1/5], Step [6450/68337], Loss: 5.7429\n",
      "Epoch [1/5], Step [6525/68337], Loss: 6.0052\n",
      "Epoch [1/5], Step [6600/68337], Loss: 5.6753\n",
      "Epoch [1/5], Step [6675/68337], Loss: 5.7847\n",
      "Epoch [1/5], Step [6750/68337], Loss: 5.9765\n",
      "Epoch [1/5], Step [6825/68337], Loss: 5.5319\n",
      "Epoch [1/5], Step [6900/68337], Loss: 5.6007\n",
      "Epoch [1/5], Step [6975/68337], Loss: 5.5446\n",
      "Epoch [1/5], Step [7050/68337], Loss: 5.7215\n",
      "Epoch [1/5], Step [7125/68337], Loss: 5.5543\n",
      "Epoch [1/5], Step [7200/68337], Loss: 5.9089\n",
      "Epoch [1/5], Step [7275/68337], Loss: 5.5534\n",
      "Epoch [1/5], Step [7350/68337], Loss: 5.5337\n",
      "Epoch [1/5], Step [7425/68337], Loss: 5.8444\n",
      "Epoch [1/5], Step [7500/68337], Loss: 5.6663\n",
      "Epoch [1/5], Step [7575/68337], Loss: 5.7571\n",
      "Epoch [1/5], Step [7650/68337], Loss: 5.9387\n",
      "Epoch [1/5], Step [7725/68337], Loss: 5.5562\n",
      "Epoch [1/5], Step [7800/68337], Loss: 5.4451\n",
      "Epoch [1/5], Step [7875/68337], Loss: 5.5989\n",
      "Epoch [1/5], Step [7950/68337], Loss: 5.8074\n",
      "Epoch [1/5], Step [8025/68337], Loss: 5.7166\n",
      "Epoch [1/5], Step [8100/68337], Loss: 5.7745\n",
      "Epoch [1/5], Step [8175/68337], Loss: 5.7631\n",
      "Epoch [1/5], Step [8250/68337], Loss: 5.6494\n",
      "Epoch [1/5], Step [8325/68337], Loss: 5.6870\n",
      "Epoch [1/5], Step [8400/68337], Loss: 5.6653\n",
      "Epoch [1/5], Step [8475/68337], Loss: 5.7098\n",
      "Epoch [1/5], Step [8550/68337], Loss: 5.7891\n",
      "Epoch [1/5], Step [8625/68337], Loss: 5.5088\n",
      "Epoch [1/5], Step [8700/68337], Loss: 5.4812\n",
      "Epoch [1/5], Step [8775/68337], Loss: 5.5464\n",
      "Epoch [1/5], Step [8850/68337], Loss: 5.5503\n",
      "Epoch [1/5], Step [8925/68337], Loss: 5.6917\n",
      "Epoch [1/5], Step [9000/68337], Loss: 5.6934\n",
      "Epoch [1/5], Step [9075/68337], Loss: 5.4693\n",
      "Epoch [1/5], Step [9150/68337], Loss: 5.5844\n",
      "Epoch [1/5], Step [9225/68337], Loss: 5.7600\n",
      "Epoch [1/5], Step [9300/68337], Loss: 5.7008\n",
      "Epoch [1/5], Step [9375/68337], Loss: 5.5413\n",
      "Epoch [1/5], Step [9450/68337], Loss: 5.3784\n",
      "Epoch [1/5], Step [9525/68337], Loss: 5.5505\n",
      "Epoch [1/5], Step [9600/68337], Loss: 5.5793\n",
      "Epoch [1/5], Step [9675/68337], Loss: 5.6482\n",
      "Epoch [1/5], Step [9750/68337], Loss: 5.2590\n",
      "Epoch [1/5], Step [9825/68337], Loss: 5.4429\n",
      "Epoch [1/5], Step [9900/68337], Loss: 5.6115\n",
      "Epoch [1/5], Step [9975/68337], Loss: 5.6105\n",
      "Validation perplexity: 237.57701627632886\n",
      "Epoch [1/5], Step [10050/68337], Loss: 5.8404\n",
      "Epoch [1/5], Step [10125/68337], Loss: 5.5248\n",
      "Epoch [1/5], Step [10200/68337], Loss: 5.4936\n",
      "Epoch [1/5], Step [10275/68337], Loss: 5.6395\n",
      "Epoch [1/5], Step [10350/68337], Loss: 5.5164\n",
      "Epoch [1/5], Step [10425/68337], Loss: 5.5526\n",
      "Epoch [1/5], Step [10500/68337], Loss: 5.4985\n",
      "Epoch [1/5], Step [10575/68337], Loss: 5.6192\n",
      "Epoch [1/5], Step [10650/68337], Loss: 5.3303\n",
      "Epoch [1/5], Step [10725/68337], Loss: 5.5066\n",
      "Epoch [1/5], Step [10800/68337], Loss: 5.3812\n",
      "Epoch [1/5], Step [10875/68337], Loss: 5.6219\n",
      "Epoch [1/5], Step [10950/68337], Loss: 5.5935\n",
      "Epoch [1/5], Step [11025/68337], Loss: 5.7227\n",
      "Epoch [1/5], Step [11100/68337], Loss: 5.3922\n",
      "Epoch [1/5], Step [11175/68337], Loss: 5.5150\n",
      "Epoch [1/5], Step [11250/68337], Loss: 5.4501\n",
      "Epoch [1/5], Step [11325/68337], Loss: 5.5138\n",
      "Epoch [1/5], Step [11400/68337], Loss: 5.5283\n",
      "Epoch [1/5], Step [11475/68337], Loss: 5.7575\n",
      "Epoch [1/5], Step [11550/68337], Loss: 5.6312\n",
      "Epoch [1/5], Step [11625/68337], Loss: 5.5062\n",
      "Epoch [1/5], Step [11700/68337], Loss: 5.7163\n",
      "Epoch [1/5], Step [11775/68337], Loss: 5.8408\n",
      "Epoch [1/5], Step [11850/68337], Loss: 5.5326\n",
      "Epoch [1/5], Step [11925/68337], Loss: 5.2405\n",
      "Epoch [1/5], Step [12000/68337], Loss: 5.4572\n",
      "Epoch [1/5], Step [12075/68337], Loss: 5.5154\n",
      "Epoch [1/5], Step [12150/68337], Loss: 5.5469\n",
      "Epoch [1/5], Step [12225/68337], Loss: 5.3003\n",
      "Epoch [1/5], Step [12300/68337], Loss: 5.3493\n",
      "Epoch [1/5], Step [12375/68337], Loss: 5.4913\n",
      "Epoch [1/5], Step [12450/68337], Loss: 5.6068\n",
      "Epoch [1/5], Step [12525/68337], Loss: 5.2847\n",
      "Epoch [1/5], Step [12600/68337], Loss: 5.2766\n",
      "Epoch [1/5], Step [12675/68337], Loss: 5.2311\n",
      "Epoch [1/5], Step [12750/68337], Loss: 5.4724\n",
      "Epoch [1/5], Step [12825/68337], Loss: 5.4125\n",
      "Epoch [1/5], Step [12900/68337], Loss: 5.6681\n",
      "Epoch [1/5], Step [12975/68337], Loss: 5.5626\n",
      "Epoch [1/5], Step [13050/68337], Loss: 5.5658\n",
      "Epoch [1/5], Step [13125/68337], Loss: 5.5020\n",
      "Epoch [1/5], Step [13200/68337], Loss: 5.3719\n",
      "Epoch [1/5], Step [13275/68337], Loss: 5.3920\n",
      "Epoch [1/5], Step [13350/68337], Loss: 5.4033\n",
      "Epoch [1/5], Step [13425/68337], Loss: 5.7058\n",
      "Epoch [1/5], Step [13500/68337], Loss: 5.4317\n",
      "Epoch [1/5], Step [13575/68337], Loss: 5.5954\n",
      "Epoch [1/5], Step [13650/68337], Loss: 5.6801\n",
      "Epoch [1/5], Step [13725/68337], Loss: 5.4963\n",
      "Epoch [1/5], Step [13800/68337], Loss: 5.5126\n",
      "Epoch [1/5], Step [13875/68337], Loss: 5.5090\n",
      "Epoch [1/5], Step [13950/68337], Loss: 5.2761\n",
      "Epoch [1/5], Step [14025/68337], Loss: 5.3662\n",
      "Epoch [1/5], Step [14100/68337], Loss: 5.5264\n",
      "Epoch [1/5], Step [14175/68337], Loss: 5.5232\n",
      "Epoch [1/5], Step [14250/68337], Loss: 5.4826\n",
      "Epoch [1/5], Step [14325/68337], Loss: 5.4942\n",
      "Epoch [1/5], Step [14400/68337], Loss: 5.3310\n",
      "Epoch [1/5], Step [14475/68337], Loss: 5.7001\n",
      "Epoch [1/5], Step [14550/68337], Loss: 5.5511\n",
      "Epoch [1/5], Step [14625/68337], Loss: 5.3402\n",
      "Epoch [1/5], Step [14700/68337], Loss: 5.5598\n",
      "Epoch [1/5], Step [14775/68337], Loss: 5.4499\n",
      "Epoch [1/5], Step [14850/68337], Loss: 5.4950\n",
      "Epoch [1/5], Step [14925/68337], Loss: 5.5357\n",
      "Epoch [1/5], Step [15000/68337], Loss: 5.4647\n",
      "Epoch [1/5], Step [15075/68337], Loss: 5.3682\n",
      "Epoch [1/5], Step [15150/68337], Loss: 5.5827\n",
      "Epoch [1/5], Step [15225/68337], Loss: 5.3312\n",
      "Epoch [1/5], Step [15300/68337], Loss: 5.5154\n",
      "Epoch [1/5], Step [15375/68337], Loss: 5.6022\n",
      "Epoch [1/5], Step [15450/68337], Loss: 5.4184\n",
      "Epoch [1/5], Step [15525/68337], Loss: 5.5899\n",
      "Epoch [1/5], Step [15600/68337], Loss: 5.3643\n",
      "Epoch [1/5], Step [15675/68337], Loss: 5.4280\n",
      "Epoch [1/5], Step [15750/68337], Loss: 5.4964\n",
      "Epoch [1/5], Step [15825/68337], Loss: 5.4506\n",
      "Epoch [1/5], Step [15900/68337], Loss: 5.3994\n",
      "Epoch [1/5], Step [15975/68337], Loss: 5.4225\n",
      "Epoch [1/5], Step [16050/68337], Loss: 5.5212\n",
      "Epoch [1/5], Step [16125/68337], Loss: 5.3303\n",
      "Epoch [1/5], Step [16200/68337], Loss: 5.3045\n",
      "Epoch [1/5], Step [16275/68337], Loss: 5.2133\n",
      "Epoch [1/5], Step [16350/68337], Loss: 5.4730\n",
      "Epoch [1/5], Step [16425/68337], Loss: 5.2555\n",
      "Epoch [1/5], Step [16500/68337], Loss: 5.4968\n",
      "Epoch [1/5], Step [16575/68337], Loss: 5.1605\n",
      "Epoch [1/5], Step [16650/68337], Loss: 5.3027\n",
      "Epoch [1/5], Step [16725/68337], Loss: 5.4854\n",
      "Epoch [1/5], Step [16800/68337], Loss: 5.4607\n",
      "Epoch [1/5], Step [16875/68337], Loss: 5.4129\n",
      "Epoch [1/5], Step [16950/68337], Loss: 5.3752\n",
      "Epoch [1/5], Step [17025/68337], Loss: 5.3197\n",
      "Epoch [1/5], Step [17100/68337], Loss: 5.5764\n",
      "Epoch [1/5], Step [17175/68337], Loss: 5.4642\n",
      "Epoch [1/5], Step [17250/68337], Loss: 5.4021\n",
      "Epoch [1/5], Step [17325/68337], Loss: 5.3574\n",
      "Epoch [1/5], Step [17400/68337], Loss: 5.3465\n",
      "Epoch [1/5], Step [17475/68337], Loss: 5.2824\n",
      "Epoch [1/5], Step [17550/68337], Loss: 5.2999\n",
      "Epoch [1/5], Step [17625/68337], Loss: 5.3274\n",
      "Epoch [1/5], Step [17700/68337], Loss: 5.4716\n",
      "Epoch [1/5], Step [17775/68337], Loss: 5.1498\n",
      "Epoch [1/5], Step [17850/68337], Loss: 5.3808\n",
      "Epoch [1/5], Step [17925/68337], Loss: 5.4098\n",
      "Epoch [1/5], Step [18000/68337], Loss: 5.3308\n",
      "Epoch [1/5], Step [18075/68337], Loss: 5.4025\n",
      "Epoch [1/5], Step [18150/68337], Loss: 5.4620\n",
      "Epoch [1/5], Step [18225/68337], Loss: 5.3958\n",
      "Epoch [1/5], Step [18300/68337], Loss: 5.5634\n",
      "Epoch [1/5], Step [18375/68337], Loss: 5.4562\n",
      "Epoch [1/5], Step [18450/68337], Loss: 5.4123\n",
      "Epoch [1/5], Step [18525/68337], Loss: 5.5216\n",
      "Epoch [1/5], Step [18600/68337], Loss: 5.3342\n",
      "Epoch [1/5], Step [18675/68337], Loss: 5.4488\n",
      "Epoch [1/5], Step [18750/68337], Loss: 5.3960\n",
      "Epoch [1/5], Step [18825/68337], Loss: 5.4495\n",
      "Epoch [1/5], Step [18900/68337], Loss: 5.3357\n",
      "Epoch [1/5], Step [18975/68337], Loss: 5.3414\n",
      "Epoch [1/5], Step [19050/68337], Loss: 5.3513\n",
      "Epoch [1/5], Step [19125/68337], Loss: 5.3612\n",
      "Epoch [1/5], Step [19200/68337], Loss: 5.3198\n",
      "Epoch [1/5], Step [19275/68337], Loss: 5.3168\n",
      "Epoch [1/5], Step [19350/68337], Loss: 5.3705\n",
      "Epoch [1/5], Step [19425/68337], Loss: 5.2686\n",
      "Epoch [1/5], Step [19500/68337], Loss: 5.4312\n",
      "Epoch [1/5], Step [19575/68337], Loss: 5.4288\n",
      "Epoch [1/5], Step [19650/68337], Loss: 5.2650\n",
      "Epoch [1/5], Step [19725/68337], Loss: 5.3710\n",
      "Epoch [1/5], Step [19800/68337], Loss: 5.4114\n",
      "Epoch [1/5], Step [19875/68337], Loss: 5.4589\n",
      "Epoch [1/5], Step [19950/68337], Loss: 5.3522\n",
      "Validation perplexity: 195.82100441280477\n",
      "Epoch [1/5], Step [20025/68337], Loss: 5.4727\n",
      "Epoch [1/5], Step [20100/68337], Loss: 5.2905\n",
      "Epoch [1/5], Step [20175/68337], Loss: 5.2700\n",
      "Epoch [1/5], Step [20250/68337], Loss: 5.4432\n",
      "Epoch [1/5], Step [20325/68337], Loss: 5.0723\n",
      "Epoch [1/5], Step [20400/68337], Loss: 5.2994\n",
      "Epoch [1/5], Step [20475/68337], Loss: 5.2855\n",
      "Epoch [1/5], Step [20550/68337], Loss: 5.4570\n",
      "Epoch [1/5], Step [20625/68337], Loss: 5.3980\n",
      "Epoch [1/5], Step [20700/68337], Loss: 5.3794\n",
      "Epoch [1/5], Step [20775/68337], Loss: 5.4965\n",
      "Epoch [1/5], Step [20850/68337], Loss: 5.3037\n",
      "Epoch [1/5], Step [20925/68337], Loss: 5.3329\n",
      "Epoch [1/5], Step [21000/68337], Loss: 5.5346\n",
      "Epoch [1/5], Step [21075/68337], Loss: 5.4139\n",
      "Epoch [1/5], Step [21150/68337], Loss: 5.3283\n",
      "Epoch [1/5], Step [21225/68337], Loss: 5.2521\n",
      "Epoch [1/5], Step [21300/68337], Loss: 5.4050\n",
      "Epoch [1/5], Step [21375/68337], Loss: 5.3433\n",
      "Epoch [1/5], Step [21450/68337], Loss: 5.5314\n",
      "Epoch [1/5], Step [21525/68337], Loss: 5.4015\n",
      "Epoch [1/5], Step [21600/68337], Loss: 5.3464\n",
      "Epoch [1/5], Step [21675/68337], Loss: 5.2646\n",
      "Epoch [1/5], Step [21750/68337], Loss: 5.3962\n",
      "Epoch [1/5], Step [21825/68337], Loss: 5.3560\n",
      "Epoch [1/5], Step [21900/68337], Loss: 5.3365\n",
      "Epoch [1/5], Step [21975/68337], Loss: 5.2276\n",
      "Epoch [1/5], Step [22050/68337], Loss: 5.4300\n",
      "Epoch [1/5], Step [22125/68337], Loss: 5.4440\n",
      "Epoch [1/5], Step [22200/68337], Loss: 5.2579\n",
      "Epoch [1/5], Step [22275/68337], Loss: 5.3374\n",
      "Epoch [1/5], Step [22350/68337], Loss: 5.4243\n",
      "Epoch [1/5], Step [22425/68337], Loss: 5.4261\n",
      "Epoch [1/5], Step [22500/68337], Loss: 5.4080\n",
      "Epoch [1/5], Step [22575/68337], Loss: 5.3883\n",
      "Epoch [1/5], Step [22650/68337], Loss: 5.3925\n",
      "Epoch [1/5], Step [22725/68337], Loss: 5.4284\n",
      "Epoch [1/5], Step [22800/68337], Loss: 5.3169\n",
      "Epoch [1/5], Step [22875/68337], Loss: 5.2305\n",
      "Epoch [1/5], Step [22950/68337], Loss: 5.3187\n",
      "Epoch [1/5], Step [23025/68337], Loss: 5.4637\n",
      "Epoch [1/5], Step [23100/68337], Loss: 5.2501\n",
      "Epoch [1/5], Step [23175/68337], Loss: 5.4132\n",
      "Epoch [1/5], Step [23250/68337], Loss: 5.3200\n",
      "Epoch [1/5], Step [23325/68337], Loss: 5.2508\n",
      "Epoch [1/5], Step [23400/68337], Loss: 5.3327\n",
      "Epoch [1/5], Step [23475/68337], Loss: 5.4130\n",
      "Epoch [1/5], Step [23550/68337], Loss: 5.4137\n",
      "Epoch [1/5], Step [23625/68337], Loss: 5.2378\n",
      "Epoch [1/5], Step [23700/68337], Loss: 5.3744\n",
      "Epoch [1/5], Step [23775/68337], Loss: 5.2552\n",
      "Epoch [1/5], Step [23850/68337], Loss: 5.1904\n",
      "Epoch [1/5], Step [23925/68337], Loss: 5.5481\n",
      "Epoch [1/5], Step [24000/68337], Loss: 5.1172\n",
      "Epoch [1/5], Step [24075/68337], Loss: 5.3843\n",
      "Epoch [1/5], Step [24150/68337], Loss: 5.3680\n",
      "Epoch [1/5], Step [24225/68337], Loss: 5.4396\n",
      "Epoch [1/5], Step [24300/68337], Loss: 5.2060\n",
      "Epoch [1/5], Step [24375/68337], Loss: 5.2524\n",
      "Epoch [1/5], Step [24450/68337], Loss: 5.2105\n",
      "Epoch [1/5], Step [24525/68337], Loss: 5.3908\n",
      "Epoch [1/5], Step [24600/68337], Loss: 5.3809\n",
      "Epoch [1/5], Step [24675/68337], Loss: 5.6182\n",
      "Epoch [1/5], Step [24750/68337], Loss: 5.2738\n",
      "Epoch [1/5], Step [24825/68337], Loss: 5.4130\n",
      "Epoch [1/5], Step [24900/68337], Loss: 5.3576\n",
      "Epoch [1/5], Step [24975/68337], Loss: 5.2605\n",
      "Epoch [1/5], Step [25050/68337], Loss: 5.2496\n",
      "Epoch [1/5], Step [25125/68337], Loss: 5.2899\n",
      "Epoch [1/5], Step [25200/68337], Loss: 5.2927\n",
      "Epoch [1/5], Step [25275/68337], Loss: 5.3036\n",
      "Epoch [1/5], Step [25350/68337], Loss: 5.3485\n",
      "Epoch [1/5], Step [25425/68337], Loss: 5.3445\n",
      "Epoch [1/5], Step [25500/68337], Loss: 5.4306\n",
      "Epoch [1/5], Step [25575/68337], Loss: 5.3836\n",
      "Epoch [1/5], Step [25650/68337], Loss: 5.2330\n",
      "Epoch [1/5], Step [25725/68337], Loss: 5.2572\n",
      "Epoch [1/5], Step [25800/68337], Loss: 5.2399\n",
      "Epoch [1/5], Step [25875/68337], Loss: 5.3523\n",
      "Epoch [1/5], Step [25950/68337], Loss: 5.3239\n",
      "Epoch [1/5], Step [26025/68337], Loss: 5.3289\n",
      "Epoch [1/5], Step [26100/68337], Loss: 5.3695\n",
      "Epoch [1/5], Step [26175/68337], Loss: 5.2714\n",
      "Epoch [1/5], Step [26250/68337], Loss: 5.3522\n",
      "Epoch [1/5], Step [26325/68337], Loss: 5.2737\n",
      "Epoch [1/5], Step [26400/68337], Loss: 5.3379\n",
      "Epoch [1/5], Step [26475/68337], Loss: 5.3307\n",
      "Epoch [1/5], Step [26550/68337], Loss: 5.3266\n",
      "Epoch [1/5], Step [26625/68337], Loss: 5.4944\n",
      "Epoch [1/5], Step [26700/68337], Loss: 5.3461\n",
      "Epoch [1/5], Step [26775/68337], Loss: 5.2659\n",
      "Epoch [1/5], Step [26850/68337], Loss: 5.3109\n",
      "Epoch [1/5], Step [26925/68337], Loss: 5.1846\n",
      "Epoch [1/5], Step [27000/68337], Loss: 5.0002\n",
      "Epoch [1/5], Step [27075/68337], Loss: 5.2654\n",
      "Epoch [1/5], Step [27150/68337], Loss: 5.3734\n",
      "Epoch [1/5], Step [27225/68337], Loss: 5.4804\n",
      "Epoch [1/5], Step [27300/68337], Loss: 5.1725\n",
      "Epoch [1/5], Step [27375/68337], Loss: 5.3465\n",
      "Epoch [1/5], Step [27450/68337], Loss: 5.3968\n",
      "Epoch [1/5], Step [27525/68337], Loss: 5.1804\n",
      "Epoch [1/5], Step [27600/68337], Loss: 5.3292\n",
      "Epoch [1/5], Step [27675/68337], Loss: 5.4175\n",
      "Epoch [1/5], Step [27750/68337], Loss: 5.2448\n",
      "Epoch [1/5], Step [27825/68337], Loss: 5.1463\n",
      "Epoch [1/5], Step [27900/68337], Loss: 5.4738\n",
      "Epoch [1/5], Step [27975/68337], Loss: 5.1884\n",
      "Epoch [1/5], Step [28050/68337], Loss: 5.4039\n",
      "Epoch [1/5], Step [28125/68337], Loss: 5.2351\n",
      "Epoch [1/5], Step [28200/68337], Loss: 5.4260\n",
      "Epoch [1/5], Step [28275/68337], Loss: 5.5047\n",
      "Epoch [1/5], Step [28350/68337], Loss: 5.4045\n",
      "Epoch [1/5], Step [28425/68337], Loss: 5.1122\n",
      "Epoch [1/5], Step [28500/68337], Loss: 5.2700\n",
      "Epoch [1/5], Step [28575/68337], Loss: 5.3458\n",
      "Epoch [1/5], Step [28650/68337], Loss: 5.1300\n",
      "Epoch [1/5], Step [28725/68337], Loss: 5.3969\n",
      "Epoch [1/5], Step [28800/68337], Loss: 5.2432\n",
      "Epoch [1/5], Step [28875/68337], Loss: 5.1697\n",
      "Epoch [1/5], Step [28950/68337], Loss: 5.3035\n",
      "Epoch [1/5], Step [29025/68337], Loss: 5.4054\n",
      "Epoch [1/5], Step [29100/68337], Loss: 5.2999\n",
      "Epoch [1/5], Step [29175/68337], Loss: 5.3166\n",
      "Epoch [1/5], Step [29250/68337], Loss: 5.3784\n",
      "Epoch [1/5], Step [29325/68337], Loss: 5.3110\n",
      "Epoch [1/5], Step [29400/68337], Loss: 5.3318\n",
      "Epoch [1/5], Step [29475/68337], Loss: 5.1749\n",
      "Epoch [1/5], Step [29550/68337], Loss: 5.3487\n",
      "Epoch [1/5], Step [29625/68337], Loss: 5.1957\n",
      "Epoch [1/5], Step [29700/68337], Loss: 5.2448\n",
      "Epoch [1/5], Step [29775/68337], Loss: 5.3156\n",
      "Epoch [1/5], Step [29850/68337], Loss: 5.1895\n",
      "Epoch [1/5], Step [29925/68337], Loss: 5.2246\n",
      "Epoch [1/5], Step [30000/68337], Loss: 5.1898\n",
      "Validation perplexity: 180.7078367888366\n",
      "Epoch [1/5], Step [30075/68337], Loss: 5.2458\n",
      "Epoch [1/5], Step [30150/68337], Loss: 5.4551\n",
      "Epoch [1/5], Step [30225/68337], Loss: 5.5151\n",
      "Epoch [1/5], Step [30300/68337], Loss: 5.3521\n",
      "Epoch [1/5], Step [30375/68337], Loss: 4.9965\n",
      "Epoch [1/5], Step [30450/68337], Loss: 5.2772\n",
      "Epoch [1/5], Step [30525/68337], Loss: 5.4997\n",
      "Epoch [1/5], Step [30600/68337], Loss: 5.1324\n",
      "Epoch [1/5], Step [30675/68337], Loss: 5.2359\n",
      "Epoch [1/5], Step [30750/68337], Loss: 5.4400\n",
      "Epoch [1/5], Step [30825/68337], Loss: 5.1718\n",
      "Epoch [1/5], Step [30900/68337], Loss: 5.3344\n",
      "Epoch [1/5], Step [30975/68337], Loss: 5.2185\n",
      "Epoch [1/5], Step [31050/68337], Loss: 5.2154\n",
      "Epoch [1/5], Step [31125/68337], Loss: 5.1525\n",
      "Epoch [1/5], Step [31200/68337], Loss: 5.3453\n",
      "Epoch [1/5], Step [31275/68337], Loss: 5.2795\n",
      "Epoch [1/5], Step [31350/68337], Loss: 5.3698\n",
      "Epoch [1/5], Step [31425/68337], Loss: 5.2574\n",
      "Epoch [1/5], Step [31500/68337], Loss: 5.2398\n",
      "Epoch [1/5], Step [31575/68337], Loss: 5.2717\n",
      "Epoch [1/5], Step [31650/68337], Loss: 5.3446\n",
      "Epoch [1/5], Step [31725/68337], Loss: 5.2663\n",
      "Epoch [1/5], Step [31800/68337], Loss: 5.2899\n",
      "Epoch [1/5], Step [31875/68337], Loss: 5.2420\n",
      "Epoch [1/5], Step [31950/68337], Loss: 5.1892\n",
      "Epoch [1/5], Step [32025/68337], Loss: 5.1459\n",
      "Epoch [1/5], Step [32100/68337], Loss: 5.2113\n",
      "Epoch [1/5], Step [32175/68337], Loss: 5.2333\n",
      "Epoch [1/5], Step [32250/68337], Loss: 5.2279\n",
      "Epoch [1/5], Step [32325/68337], Loss: 5.1734\n",
      "Epoch [1/5], Step [32400/68337], Loss: 5.3301\n",
      "Epoch [1/5], Step [32475/68337], Loss: 5.3501\n",
      "Epoch [1/5], Step [32550/68337], Loss: 5.2514\n",
      "Epoch [1/5], Step [32625/68337], Loss: 5.3892\n",
      "Epoch [1/5], Step [32700/68337], Loss: 5.4061\n",
      "Epoch [1/5], Step [32775/68337], Loss: 5.3150\n",
      "Epoch [1/5], Step [32850/68337], Loss: 5.2522\n",
      "Epoch [1/5], Step [32925/68337], Loss: 5.1829\n",
      "Epoch [1/5], Step [33000/68337], Loss: 5.2664\n",
      "Epoch [1/5], Step [33075/68337], Loss: 5.3448\n",
      "Epoch [1/5], Step [33150/68337], Loss: 5.3541\n",
      "Epoch [1/5], Step [33225/68337], Loss: 5.2501\n",
      "Epoch [1/5], Step [33300/68337], Loss: 5.0852\n",
      "Epoch [1/5], Step [33375/68337], Loss: 5.3553\n",
      "Epoch [1/5], Step [33450/68337], Loss: 5.2554\n",
      "Epoch [1/5], Step [33525/68337], Loss: 5.1776\n",
      "Epoch [1/5], Step [33600/68337], Loss: 5.1735\n",
      "Epoch [1/5], Step [33675/68337], Loss: 5.3656\n",
      "Epoch [1/5], Step [33750/68337], Loss: 5.2143\n",
      "Epoch [1/5], Step [33825/68337], Loss: 5.3152\n",
      "Epoch [1/5], Step [33900/68337], Loss: 5.3986\n",
      "Epoch [1/5], Step [33975/68337], Loss: 5.3107\n",
      "Epoch [1/5], Step [34050/68337], Loss: 5.1078\n",
      "Epoch [1/5], Step [34125/68337], Loss: 5.1663\n",
      "Epoch [1/5], Step [34200/68337], Loss: 5.2949\n",
      "Epoch [1/5], Step [34275/68337], Loss: 5.3789\n",
      "Epoch [1/5], Step [34350/68337], Loss: 5.2592\n",
      "Epoch [1/5], Step [34425/68337], Loss: 5.3144\n",
      "Epoch [1/5], Step [34500/68337], Loss: 5.2709\n",
      "Epoch [1/5], Step [34575/68337], Loss: 5.1449\n",
      "Epoch [1/5], Step [34650/68337], Loss: 5.3044\n",
      "Epoch [1/5], Step [34725/68337], Loss: 5.1102\n",
      "Epoch [1/5], Step [34800/68337], Loss: 5.5888\n",
      "Epoch [1/5], Step [34875/68337], Loss: 5.2949\n",
      "Epoch [1/5], Step [34950/68337], Loss: 5.2485\n",
      "Epoch [1/5], Step [35025/68337], Loss: 5.1519\n",
      "Epoch [1/5], Step [35100/68337], Loss: 5.1637\n",
      "Epoch [1/5], Step [35175/68337], Loss: 5.1743\n",
      "Epoch [1/5], Step [35250/68337], Loss: 5.1856\n",
      "Epoch [1/5], Step [35325/68337], Loss: 5.1873\n",
      "Epoch [1/5], Step [35400/68337], Loss: 5.1696\n",
      "Epoch [1/5], Step [35475/68337], Loss: 5.3763\n",
      "Epoch [1/5], Step [35550/68337], Loss: 5.2886\n",
      "Epoch [1/5], Step [35625/68337], Loss: 5.1659\n",
      "Epoch [1/5], Step [35700/68337], Loss: 5.0363\n",
      "Epoch [1/5], Step [35775/68337], Loss: 5.1403\n",
      "Epoch [1/5], Step [35850/68337], Loss: 5.3895\n",
      "Epoch [1/5], Step [35925/68337], Loss: 5.1433\n",
      "Epoch [1/5], Step [36000/68337], Loss: 5.1740\n",
      "Epoch [1/5], Step [36075/68337], Loss: 5.1572\n",
      "Epoch [1/5], Step [36150/68337], Loss: 5.3874\n",
      "Epoch [1/5], Step [36225/68337], Loss: 5.2178\n",
      "Epoch [1/5], Step [36300/68337], Loss: 5.2977\n",
      "Epoch [1/5], Step [36375/68337], Loss: 5.1784\n",
      "Epoch [1/5], Step [36450/68337], Loss: 5.2661\n",
      "Epoch [1/5], Step [36525/68337], Loss: 5.2362\n",
      "Epoch [1/5], Step [36600/68337], Loss: 5.3139\n",
      "Epoch [1/5], Step [36675/68337], Loss: 5.2071\n",
      "Epoch [1/5], Step [36750/68337], Loss: 5.2722\n",
      "Epoch [1/5], Step [36825/68337], Loss: 5.2173\n",
      "Epoch [1/5], Step [36900/68337], Loss: 5.3348\n",
      "Epoch [1/5], Step [36975/68337], Loss: 5.2776\n",
      "Epoch [1/5], Step [37050/68337], Loss: 5.4377\n",
      "Epoch [1/5], Step [37125/68337], Loss: 5.2845\n",
      "Epoch [1/5], Step [37200/68337], Loss: 5.2856\n",
      "Epoch [1/5], Step [37275/68337], Loss: 5.2874\n",
      "Epoch [1/5], Step [37350/68337], Loss: 5.4122\n",
      "Epoch [1/5], Step [37425/68337], Loss: 4.9838\n",
      "Epoch [1/5], Step [37500/68337], Loss: 5.2497\n",
      "Epoch [1/5], Step [37575/68337], Loss: 5.2282\n",
      "Epoch [1/5], Step [37650/68337], Loss: 5.3981\n",
      "Epoch [1/5], Step [37725/68337], Loss: 5.1971\n",
      "Epoch [1/5], Step [37800/68337], Loss: 5.1505\n",
      "Epoch [1/5], Step [37875/68337], Loss: 5.2373\n",
      "Epoch [1/5], Step [37950/68337], Loss: 5.4194\n",
      "Epoch [1/5], Step [38025/68337], Loss: 5.3294\n",
      "Epoch [1/5], Step [38100/68337], Loss: 5.3897\n",
      "Epoch [1/5], Step [38175/68337], Loss: 5.1814\n",
      "Epoch [1/5], Step [38250/68337], Loss: 5.2826\n",
      "Epoch [1/5], Step [38325/68337], Loss: 5.2621\n",
      "Epoch [1/5], Step [38400/68337], Loss: 5.2833\n",
      "Epoch [1/5], Step [38475/68337], Loss: 5.0778\n",
      "Epoch [1/5], Step [38550/68337], Loss: 5.0765\n",
      "Epoch [1/5], Step [38625/68337], Loss: 5.3204\n",
      "Epoch [1/5], Step [38700/68337], Loss: 5.3070\n",
      "Epoch [1/5], Step [38775/68337], Loss: 5.1764\n",
      "Epoch [1/5], Step [38850/68337], Loss: 5.1140\n",
      "Epoch [1/5], Step [38925/68337], Loss: 5.2072\n",
      "Epoch [1/5], Step [39000/68337], Loss: 5.2424\n",
      "Epoch [1/5], Step [39075/68337], Loss: 5.0830\n",
      "Epoch [1/5], Step [39150/68337], Loss: 5.2405\n",
      "Epoch [1/5], Step [39225/68337], Loss: 5.2034\n",
      "Epoch [1/5], Step [39300/68337], Loss: 5.2356\n",
      "Epoch [1/5], Step [39375/68337], Loss: 5.2343\n",
      "Epoch [1/5], Step [39450/68337], Loss: 5.1774\n",
      "Epoch [1/5], Step [39525/68337], Loss: 5.1679\n",
      "Epoch [1/5], Step [39600/68337], Loss: 5.3403\n",
      "Epoch [1/5], Step [39675/68337], Loss: 5.1993\n",
      "Epoch [1/5], Step [39750/68337], Loss: 4.9546\n",
      "Epoch [1/5], Step [39825/68337], Loss: 5.1649\n",
      "Epoch [1/5], Step [39900/68337], Loss: 5.1548\n",
      "Epoch [1/5], Step [39975/68337], Loss: 5.2294\n",
      "Validation perplexity: 172.42488256063933\n",
      "Epoch [1/5], Step [40050/68337], Loss: 5.1350\n",
      "Epoch [1/5], Step [40125/68337], Loss: 5.2110\n",
      "Epoch [1/5], Step [40200/68337], Loss: 5.0639\n",
      "Epoch [1/5], Step [40275/68337], Loss: 5.1529\n",
      "Epoch [1/5], Step [40350/68337], Loss: 5.2394\n",
      "Epoch [1/5], Step [40425/68337], Loss: 5.0735\n",
      "Epoch [1/5], Step [40500/68337], Loss: 5.2027\n",
      "Epoch [1/5], Step [40575/68337], Loss: 5.0711\n",
      "Epoch [1/5], Step [40650/68337], Loss: 5.3784\n",
      "Epoch [1/5], Step [40725/68337], Loss: 5.1505\n",
      "Epoch [1/5], Step [40800/68337], Loss: 5.1483\n",
      "Epoch [1/5], Step [40875/68337], Loss: 5.3496\n",
      "Epoch [1/5], Step [40950/68337], Loss: 5.3115\n",
      "Epoch [1/5], Step [41025/68337], Loss: 5.2405\n",
      "Epoch [1/5], Step [41100/68337], Loss: 5.1235\n",
      "Epoch [1/5], Step [41175/68337], Loss: 4.9791\n",
      "Epoch [1/5], Step [41250/68337], Loss: 5.2829\n",
      "Epoch [1/5], Step [41325/68337], Loss: 5.2043\n",
      "Epoch [1/5], Step [41400/68337], Loss: 5.2471\n",
      "Epoch [1/5], Step [41475/68337], Loss: 5.2285\n",
      "Epoch [1/5], Step [41550/68337], Loss: 5.1356\n",
      "Epoch [1/5], Step [41625/68337], Loss: 5.2364\n",
      "Epoch [1/5], Step [41700/68337], Loss: 5.2173\n",
      "Epoch [1/5], Step [41775/68337], Loss: 5.2895\n",
      "Epoch [1/5], Step [41850/68337], Loss: 4.9987\n",
      "Epoch [1/5], Step [41925/68337], Loss: 5.2290\n",
      "Epoch [1/5], Step [42000/68337], Loss: 5.2340\n",
      "Epoch [1/5], Step [42075/68337], Loss: 5.0373\n",
      "Epoch [1/5], Step [42150/68337], Loss: 5.2408\n",
      "Epoch [1/5], Step [42225/68337], Loss: 5.0575\n",
      "Epoch [1/5], Step [42300/68337], Loss: 5.3980\n",
      "Epoch [1/5], Step [42375/68337], Loss: 5.3084\n",
      "Epoch [1/5], Step [42450/68337], Loss: 5.0825\n",
      "Epoch [1/5], Step [42525/68337], Loss: 5.0858\n",
      "Epoch [1/5], Step [42600/68337], Loss: 5.1732\n",
      "Epoch [1/5], Step [42675/68337], Loss: 5.1610\n",
      "Epoch [1/5], Step [42750/68337], Loss: 5.2667\n",
      "Epoch [1/5], Step [42825/68337], Loss: 5.1991\n",
      "Epoch [1/5], Step [42900/68337], Loss: 5.1567\n",
      "Epoch [1/5], Step [42975/68337], Loss: 5.1257\n",
      "Epoch [1/5], Step [43050/68337], Loss: 5.2135\n",
      "Epoch [1/5], Step [43125/68337], Loss: 5.2389\n",
      "Epoch [1/5], Step [43200/68337], Loss: 5.2008\n",
      "Epoch [1/5], Step [43275/68337], Loss: 5.0642\n",
      "Epoch [1/5], Step [43350/68337], Loss: 5.1950\n",
      "Epoch [1/5], Step [43425/68337], Loss: 5.2497\n",
      "Epoch [1/5], Step [43500/68337], Loss: 5.1861\n",
      "Epoch [1/5], Step [43575/68337], Loss: 5.1651\n",
      "Epoch [1/5], Step [43650/68337], Loss: 5.1332\n",
      "Epoch [1/5], Step [43725/68337], Loss: 5.4289\n",
      "Epoch [1/5], Step [43800/68337], Loss: 5.2049\n",
      "Epoch [1/5], Step [43875/68337], Loss: 4.9930\n",
      "Epoch [1/5], Step [43950/68337], Loss: 5.2111\n",
      "Epoch [1/5], Step [44025/68337], Loss: 5.1431\n",
      "Epoch [1/5], Step [44100/68337], Loss: 5.3578\n",
      "Epoch [1/5], Step [44175/68337], Loss: 5.2210\n",
      "Epoch [1/5], Step [44250/68337], Loss: 5.1459\n",
      "Epoch [1/5], Step [44325/68337], Loss: 5.2660\n",
      "Epoch [1/5], Step [44400/68337], Loss: 5.0312\n",
      "Epoch [1/5], Step [44475/68337], Loss: 5.1299\n",
      "Epoch [1/5], Step [44550/68337], Loss: 5.1022\n",
      "Epoch [1/5], Step [44625/68337], Loss: 5.2080\n",
      "Epoch [1/5], Step [44700/68337], Loss: 4.9822\n",
      "Epoch [1/5], Step [44775/68337], Loss: 5.3295\n",
      "Epoch [1/5], Step [44850/68337], Loss: 5.0706\n",
      "Epoch [1/5], Step [44925/68337], Loss: 5.2316\n",
      "Epoch [1/5], Step [45000/68337], Loss: 5.1032\n",
      "Epoch [1/5], Step [45075/68337], Loss: 5.4142\n",
      "Epoch [1/5], Step [45150/68337], Loss: 5.0030\n",
      "Epoch [1/5], Step [45225/68337], Loss: 5.2910\n",
      "Epoch [1/5], Step [45300/68337], Loss: 5.1768\n",
      "Epoch [1/5], Step [45375/68337], Loss: 5.2375\n",
      "Epoch [1/5], Step [45450/68337], Loss: 5.3296\n",
      "Epoch [1/5], Step [45525/68337], Loss: 5.2317\n",
      "Epoch [1/5], Step [45600/68337], Loss: 5.3447\n",
      "Epoch [1/5], Step [45675/68337], Loss: 5.1213\n",
      "Epoch [1/5], Step [45750/68337], Loss: 5.3085\n",
      "Epoch [1/5], Step [45825/68337], Loss: 5.2747\n",
      "Epoch [1/5], Step [45900/68337], Loss: 5.2568\n",
      "Epoch [1/5], Step [45975/68337], Loss: 5.1946\n",
      "Epoch [1/5], Step [46050/68337], Loss: 5.2491\n",
      "Epoch [1/5], Step [46125/68337], Loss: 5.1251\n",
      "Epoch [1/5], Step [46200/68337], Loss: 5.2097\n",
      "Epoch [1/5], Step [46275/68337], Loss: 4.9954\n",
      "Epoch [1/5], Step [46350/68337], Loss: 5.3069\n",
      "Epoch [1/5], Step [46425/68337], Loss: 5.3894\n",
      "Epoch [1/5], Step [46500/68337], Loss: 5.2477\n",
      "Epoch [1/5], Step [46575/68337], Loss: 5.2476\n",
      "Epoch [1/5], Step [46650/68337], Loss: 5.2461\n",
      "Epoch [1/5], Step [46725/68337], Loss: 5.0825\n",
      "Epoch [1/5], Step [46800/68337], Loss: 5.0345\n",
      "Epoch [1/5], Step [46875/68337], Loss: 5.1097\n",
      "Epoch [1/5], Step [46950/68337], Loss: 5.1870\n",
      "Epoch [1/5], Step [47025/68337], Loss: 5.4098\n",
      "Epoch [1/5], Step [47100/68337], Loss: 5.2004\n",
      "Epoch [1/5], Step [47175/68337], Loss: 5.2877\n",
      "Epoch [1/5], Step [47250/68337], Loss: 5.1089\n",
      "Epoch [1/5], Step [47325/68337], Loss: 5.1545\n",
      "Epoch [1/5], Step [47400/68337], Loss: 5.2845\n",
      "Epoch [1/5], Step [47475/68337], Loss: 5.1108\n",
      "Epoch [1/5], Step [47550/68337], Loss: 5.1785\n",
      "Epoch [1/5], Step [47625/68337], Loss: 5.2485\n",
      "Epoch [1/5], Step [47700/68337], Loss: 5.2200\n",
      "Epoch [1/5], Step [47775/68337], Loss: 5.1204\n",
      "Epoch [1/5], Step [47850/68337], Loss: 5.2007\n",
      "Epoch [1/5], Step [47925/68337], Loss: 5.3122\n",
      "Epoch [1/5], Step [48000/68337], Loss: 5.2701\n",
      "Epoch [1/5], Step [48075/68337], Loss: 5.3914\n",
      "Epoch [1/5], Step [48150/68337], Loss: 5.2105\n",
      "Epoch [1/5], Step [48225/68337], Loss: 5.3340\n",
      "Epoch [1/5], Step [48300/68337], Loss: 5.2685\n",
      "Epoch [1/5], Step [48375/68337], Loss: 5.1384\n",
      "Epoch [1/5], Step [48450/68337], Loss: 5.2782\n",
      "Epoch [1/5], Step [48525/68337], Loss: 5.1648\n",
      "Epoch [1/5], Step [48600/68337], Loss: 5.1641\n",
      "Epoch [1/5], Step [48675/68337], Loss: 5.1442\n",
      "Epoch [1/5], Step [48750/68337], Loss: 5.2420\n",
      "Epoch [1/5], Step [48825/68337], Loss: 5.2994\n",
      "Epoch [1/5], Step [48900/68337], Loss: 5.2413\n",
      "Epoch [1/5], Step [48975/68337], Loss: 5.1359\n",
      "Epoch [1/5], Step [49050/68337], Loss: 5.0981\n",
      "Epoch [1/5], Step [49125/68337], Loss: 5.0309\n",
      "Epoch [1/5], Step [49200/68337], Loss: 5.2479\n",
      "Epoch [1/5], Step [49275/68337], Loss: 5.2613\n",
      "Epoch [1/5], Step [49350/68337], Loss: 5.1833\n",
      "Epoch [1/5], Step [49425/68337], Loss: 5.4132\n",
      "Epoch [1/5], Step [49500/68337], Loss: 4.9513\n",
      "Epoch [1/5], Step [49575/68337], Loss: 5.0216\n",
      "Epoch [1/5], Step [49650/68337], Loss: 5.2035\n",
      "Epoch [1/5], Step [49725/68337], Loss: 5.3644\n",
      "Epoch [1/5], Step [49800/68337], Loss: 5.1977\n",
      "Epoch [1/5], Step [49875/68337], Loss: 5.1975\n",
      "Epoch [1/5], Step [49950/68337], Loss: 5.0231\n",
      "Validation perplexity: 166.24268228093348\n",
      "Epoch [1/5], Step [50025/68337], Loss: 5.1551\n",
      "Epoch [1/5], Step [50100/68337], Loss: 5.1945\n",
      "Epoch [1/5], Step [50175/68337], Loss: 5.2766\n",
      "Epoch [1/5], Step [50250/68337], Loss: 5.3272\n",
      "Epoch [1/5], Step [50325/68337], Loss: 5.2438\n",
      "Epoch [1/5], Step [50400/68337], Loss: 5.1562\n",
      "Epoch [1/5], Step [50475/68337], Loss: 5.0956\n",
      "Epoch [1/5], Step [50550/68337], Loss: 5.1585\n",
      "Epoch [1/5], Step [50625/68337], Loss: 5.2238\n",
      "Epoch [1/5], Step [50700/68337], Loss: 5.1804\n",
      "Epoch [1/5], Step [50775/68337], Loss: 5.2933\n",
      "Epoch [1/5], Step [50850/68337], Loss: 5.2106\n",
      "Epoch [1/5], Step [50925/68337], Loss: 5.0331\n",
      "Epoch [1/5], Step [51000/68337], Loss: 5.2130\n",
      "Epoch [1/5], Step [51075/68337], Loss: 5.1576\n",
      "Epoch [1/5], Step [51150/68337], Loss: 5.1937\n",
      "Epoch [1/5], Step [51225/68337], Loss: 5.1034\n",
      "Epoch [1/5], Step [51300/68337], Loss: 5.1925\n",
      "Epoch [1/5], Step [51375/68337], Loss: 5.1944\n",
      "Epoch [1/5], Step [51450/68337], Loss: 5.1620\n",
      "Epoch [1/5], Step [51525/68337], Loss: 5.1142\n",
      "Epoch [1/5], Step [51600/68337], Loss: 5.1725\n",
      "Epoch [1/5], Step [51675/68337], Loss: 5.3995\n",
      "Epoch [1/5], Step [51750/68337], Loss: 5.3731\n",
      "Epoch [1/5], Step [51825/68337], Loss: 5.2231\n",
      "Epoch [1/5], Step [51900/68337], Loss: 5.1993\n",
      "Epoch [1/5], Step [51975/68337], Loss: 5.2430\n",
      "Epoch [1/5], Step [52050/68337], Loss: 4.9831\n",
      "Epoch [1/5], Step [52125/68337], Loss: 5.2268\n",
      "Epoch [1/5], Step [52200/68337], Loss: 5.2579\n",
      "Epoch [1/5], Step [52275/68337], Loss: 5.1701\n",
      "Epoch [1/5], Step [52350/68337], Loss: 5.0995\n",
      "Epoch [1/5], Step [52425/68337], Loss: 5.1578\n",
      "Epoch [1/5], Step [52500/68337], Loss: 5.2031\n",
      "Epoch [1/5], Step [52575/68337], Loss: 5.2200\n",
      "Epoch [1/5], Step [52650/68337], Loss: 5.1683\n",
      "Epoch [1/5], Step [52725/68337], Loss: 5.1035\n",
      "Epoch [1/5], Step [52800/68337], Loss: 5.2120\n",
      "Epoch [1/5], Step [52875/68337], Loss: 5.2389\n",
      "Epoch [1/5], Step [52950/68337], Loss: 5.1487\n",
      "Epoch [1/5], Step [53025/68337], Loss: 5.0399\n",
      "Epoch [1/5], Step [53100/68337], Loss: 5.1117\n",
      "Epoch [1/5], Step [53175/68337], Loss: 5.1600\n",
      "Epoch [1/5], Step [53250/68337], Loss: 5.1768\n",
      "Epoch [1/5], Step [53325/68337], Loss: 5.3057\n",
      "Epoch [1/5], Step [53400/68337], Loss: 5.1046\n",
      "Epoch [1/5], Step [53475/68337], Loss: 5.2252\n",
      "Epoch [1/5], Step [53550/68337], Loss: 5.3012\n",
      "Epoch [1/5], Step [53625/68337], Loss: 5.1811\n",
      "Epoch [1/5], Step [53700/68337], Loss: 5.0611\n",
      "Epoch [1/5], Step [53775/68337], Loss: 5.1014\n",
      "Epoch [1/5], Step [53850/68337], Loss: 5.1143\n",
      "Epoch [1/5], Step [53925/68337], Loss: 5.1827\n",
      "Epoch [1/5], Step [54000/68337], Loss: 5.3525\n",
      "Epoch [1/5], Step [54075/68337], Loss: 5.1688\n",
      "Epoch [1/5], Step [54150/68337], Loss: 5.0783\n",
      "Epoch [1/5], Step [54225/68337], Loss: 5.2347\n",
      "Epoch [1/5], Step [54300/68337], Loss: 5.1451\n",
      "Epoch [1/5], Step [54375/68337], Loss: 5.1887\n",
      "Epoch [1/5], Step [54450/68337], Loss: 5.1547\n",
      "Epoch [1/5], Step [54525/68337], Loss: 5.1888\n",
      "Epoch [1/5], Step [54600/68337], Loss: 4.9367\n",
      "Epoch [1/5], Step [54675/68337], Loss: 5.2258\n",
      "Epoch [1/5], Step [54750/68337], Loss: 5.3863\n",
      "Epoch [1/5], Step [54825/68337], Loss: 5.3317\n",
      "Epoch [1/5], Step [54900/68337], Loss: 5.2838\n",
      "Epoch [1/5], Step [54975/68337], Loss: 5.1879\n",
      "Epoch [1/5], Step [55050/68337], Loss: 5.1673\n",
      "Epoch [1/5], Step [55125/68337], Loss: 5.0102\n",
      "Epoch [1/5], Step [55200/68337], Loss: 5.2169\n",
      "Epoch [1/5], Step [55275/68337], Loss: 5.3692\n",
      "Epoch [1/5], Step [55350/68337], Loss: 5.0490\n",
      "Epoch [1/5], Step [55425/68337], Loss: 5.2474\n",
      "Epoch [1/5], Step [55500/68337], Loss: 5.2589\n",
      "Epoch [1/5], Step [55575/68337], Loss: 5.0357\n",
      "Epoch [1/5], Step [55650/68337], Loss: 5.0157\n",
      "Epoch [1/5], Step [55725/68337], Loss: 5.0707\n",
      "Epoch [1/5], Step [55800/68337], Loss: 5.2594\n",
      "Epoch [1/5], Step [55875/68337], Loss: 5.1086\n",
      "Epoch [1/5], Step [55950/68337], Loss: 5.0586\n",
      "Epoch [1/5], Step [56025/68337], Loss: 5.1843\n",
      "Epoch [1/5], Step [56100/68337], Loss: 5.1016\n",
      "Epoch [1/5], Step [56175/68337], Loss: 5.1955\n",
      "Epoch [1/5], Step [56250/68337], Loss: 5.1302\n",
      "Epoch [1/5], Step [56325/68337], Loss: 5.0852\n",
      "Epoch [1/5], Step [56400/68337], Loss: 5.3050\n",
      "Epoch [1/5], Step [56475/68337], Loss: 5.1788\n",
      "Epoch [1/5], Step [56550/68337], Loss: 5.1427\n",
      "Epoch [1/5], Step [56625/68337], Loss: 5.3398\n",
      "Epoch [1/5], Step [56700/68337], Loss: 4.9774\n",
      "Epoch [1/5], Step [56775/68337], Loss: 5.0918\n",
      "Epoch [1/5], Step [56850/68337], Loss: 5.2964\n",
      "Epoch [1/5], Step [56925/68337], Loss: 5.2047\n",
      "Epoch [1/5], Step [57000/68337], Loss: 5.2265\n",
      "Epoch [1/5], Step [57075/68337], Loss: 5.2932\n",
      "Epoch [1/5], Step [57150/68337], Loss: 5.1881\n",
      "Epoch [1/5], Step [57225/68337], Loss: 5.3016\n",
      "Epoch [1/5], Step [57300/68337], Loss: 5.1370\n",
      "Epoch [1/5], Step [57375/68337], Loss: 5.1533\n",
      "Epoch [1/5], Step [57450/68337], Loss: 5.2040\n",
      "Epoch [1/5], Step [57525/68337], Loss: 5.0342\n",
      "Epoch [1/5], Step [57600/68337], Loss: 5.1388\n",
      "Epoch [1/5], Step [57675/68337], Loss: 5.2062\n",
      "Epoch [1/5], Step [57750/68337], Loss: 5.0689\n",
      "Epoch [1/5], Step [57825/68337], Loss: 5.0165\n",
      "Epoch [1/5], Step [57900/68337], Loss: 5.2902\n",
      "Epoch [1/5], Step [57975/68337], Loss: 5.1200\n",
      "Epoch [1/5], Step [58050/68337], Loss: 5.0837\n",
      "Epoch [1/5], Step [58125/68337], Loss: 5.2852\n",
      "Epoch [1/5], Step [58200/68337], Loss: 5.2852\n",
      "Epoch [1/5], Step [58275/68337], Loss: 5.1829\n",
      "Epoch [1/5], Step [58350/68337], Loss: 5.3409\n",
      "Epoch [1/5], Step [58425/68337], Loss: 5.1772\n",
      "Epoch [1/5], Step [58500/68337], Loss: 5.1355\n",
      "Epoch [1/5], Step [58575/68337], Loss: 5.1973\n",
      "Epoch [1/5], Step [58650/68337], Loss: 5.2869\n",
      "Epoch [1/5], Step [58725/68337], Loss: 5.3516\n",
      "Epoch [1/5], Step [58800/68337], Loss: 5.0363\n",
      "Epoch [1/5], Step [58875/68337], Loss: 5.1180\n",
      "Epoch [1/5], Step [58950/68337], Loss: 5.2363\n",
      "Epoch [1/5], Step [59025/68337], Loss: 5.1589\n",
      "Epoch [1/5], Step [59100/68337], Loss: 5.0716\n",
      "Epoch [1/5], Step [59175/68337], Loss: 5.1850\n",
      "Epoch [1/5], Step [59250/68337], Loss: 5.2758\n",
      "Epoch [1/5], Step [59325/68337], Loss: 5.2065\n",
      "Epoch [1/5], Step [59400/68337], Loss: 5.1728\n",
      "Epoch [1/5], Step [59475/68337], Loss: 5.1527\n",
      "Epoch [1/5], Step [59550/68337], Loss: 5.2030\n",
      "Epoch [1/5], Step [59625/68337], Loss: 5.0579\n",
      "Epoch [1/5], Step [59700/68337], Loss: 5.2244\n",
      "Epoch [1/5], Step [59775/68337], Loss: 5.2341\n",
      "Epoch [1/5], Step [59850/68337], Loss: 5.2243\n",
      "Epoch [1/5], Step [59925/68337], Loss: 5.1222\n",
      "Epoch [1/5], Step [60000/68337], Loss: 5.0037\n",
      "Validation perplexity: 163.05925043059517\n",
      "Epoch [1/5], Step [60075/68337], Loss: 5.1498\n",
      "Epoch [1/5], Step [60150/68337], Loss: 5.0430\n",
      "Epoch [1/5], Step [60225/68337], Loss: 5.0683\n",
      "Epoch [1/5], Step [60300/68337], Loss: 5.3362\n",
      "Epoch [1/5], Step [60375/68337], Loss: 5.2317\n",
      "Epoch [1/5], Step [60450/68337], Loss: 5.1868\n",
      "Epoch [1/5], Step [60525/68337], Loss: 5.3288\n",
      "Epoch [1/5], Step [60600/68337], Loss: 5.2020\n",
      "Epoch [1/5], Step [60675/68337], Loss: 5.2495\n",
      "Epoch [1/5], Step [60750/68337], Loss: 5.1437\n",
      "Epoch [1/5], Step [60825/68337], Loss: 5.3075\n",
      "Epoch [1/5], Step [60900/68337], Loss: 4.9681\n",
      "Epoch [1/5], Step [60975/68337], Loss: 5.2028\n",
      "Epoch [1/5], Step [61050/68337], Loss: 5.3476\n",
      "Epoch [1/5], Step [61125/68337], Loss: 5.1996\n",
      "Epoch [1/5], Step [61200/68337], Loss: 5.1883\n",
      "Epoch [1/5], Step [61275/68337], Loss: 5.2266\n",
      "Epoch [1/5], Step [61350/68337], Loss: 5.1093\n",
      "Epoch [1/5], Step [61425/68337], Loss: 5.2246\n",
      "Epoch [1/5], Step [61500/68337], Loss: 5.2177\n",
      "Epoch [1/5], Step [61575/68337], Loss: 5.0235\n",
      "Epoch [1/5], Step [61650/68337], Loss: 4.9478\n",
      "Epoch [1/5], Step [61725/68337], Loss: 5.1597\n",
      "Epoch [1/5], Step [61800/68337], Loss: 5.2353\n",
      "Epoch [1/5], Step [61875/68337], Loss: 5.2036\n",
      "Epoch [1/5], Step [61950/68337], Loss: 5.1177\n",
      "Epoch [1/5], Step [62025/68337], Loss: 5.2187\n",
      "Epoch [1/5], Step [62100/68337], Loss: 5.2759\n",
      "Epoch [1/5], Step [62175/68337], Loss: 5.1463\n",
      "Epoch [1/5], Step [62250/68337], Loss: 5.1379\n",
      "Epoch [1/5], Step [62325/68337], Loss: 5.1158\n",
      "Epoch [1/5], Step [62400/68337], Loss: 5.2433\n",
      "Epoch [1/5], Step [62475/68337], Loss: 5.2361\n",
      "Epoch [1/5], Step [62550/68337], Loss: 5.3799\n",
      "Epoch [1/5], Step [62625/68337], Loss: 5.1653\n",
      "Epoch [1/5], Step [62700/68337], Loss: 5.2767\n",
      "Epoch [1/5], Step [62775/68337], Loss: 5.3440\n",
      "Epoch [1/5], Step [62850/68337], Loss: 5.1555\n",
      "Epoch [1/5], Step [62925/68337], Loss: 5.2733\n",
      "Epoch [1/5], Step [63000/68337], Loss: 5.2116\n",
      "Epoch [1/5], Step [63075/68337], Loss: 5.2497\n",
      "Epoch [1/5], Step [63150/68337], Loss: 5.1953\n",
      "Epoch [1/5], Step [63225/68337], Loss: 5.5421\n",
      "Epoch [1/5], Step [63300/68337], Loss: 4.9835\n",
      "Epoch [1/5], Step [63375/68337], Loss: 5.2874\n",
      "Epoch [1/5], Step [63450/68337], Loss: 5.1470\n",
      "Epoch [1/5], Step [63525/68337], Loss: 5.1040\n",
      "Epoch [1/5], Step [63600/68337], Loss: 5.0631\n",
      "Epoch [1/5], Step [63675/68337], Loss: 5.2007\n",
      "Epoch [1/5], Step [63750/68337], Loss: 5.2411\n",
      "Epoch [1/5], Step [63825/68337], Loss: 5.1533\n",
      "Epoch [1/5], Step [63900/68337], Loss: 5.1000\n",
      "Epoch [1/5], Step [63975/68337], Loss: 5.2679\n",
      "Epoch [1/5], Step [64050/68337], Loss: 5.2411\n",
      "Epoch [1/5], Step [64125/68337], Loss: 4.9380\n",
      "Epoch [1/5], Step [64200/68337], Loss: 5.2430\n",
      "Epoch [1/5], Step [64275/68337], Loss: 5.2668\n",
      "Epoch [1/5], Step [64350/68337], Loss: 5.2636\n",
      "Epoch [1/5], Step [64425/68337], Loss: 5.0292\n",
      "Epoch [1/5], Step [64500/68337], Loss: 5.0823\n",
      "Epoch [1/5], Step [64575/68337], Loss: 5.2236\n",
      "Epoch [1/5], Step [64650/68337], Loss: 4.9077\n",
      "Epoch [1/5], Step [64725/68337], Loss: 5.3171\n",
      "Epoch [1/5], Step [64800/68337], Loss: 5.1673\n",
      "Epoch [1/5], Step [64875/68337], Loss: 5.1578\n",
      "Epoch [1/5], Step [64950/68337], Loss: 5.1412\n",
      "Epoch [1/5], Step [65025/68337], Loss: 5.4247\n",
      "Epoch [1/5], Step [65100/68337], Loss: 5.1968\n",
      "Epoch [1/5], Step [65175/68337], Loss: 5.2752\n",
      "Epoch [1/5], Step [65250/68337], Loss: 5.2535\n",
      "Epoch [1/5], Step [65325/68337], Loss: 5.0343\n",
      "Epoch [1/5], Step [65400/68337], Loss: 5.2346\n",
      "Epoch [1/5], Step [65475/68337], Loss: 5.1003\n",
      "Epoch [1/5], Step [65550/68337], Loss: 5.1194\n",
      "Epoch [1/5], Step [65625/68337], Loss: 5.1648\n",
      "Epoch [1/5], Step [65700/68337], Loss: 5.1379\n",
      "Epoch [1/5], Step [65775/68337], Loss: 5.0900\n",
      "Epoch [1/5], Step [65850/68337], Loss: 5.2654\n",
      "Epoch [1/5], Step [65925/68337], Loss: 5.2557\n",
      "Epoch [1/5], Step [66000/68337], Loss: 4.9886\n",
      "Epoch [1/5], Step [66075/68337], Loss: 5.1308\n",
      "Epoch [1/5], Step [66150/68337], Loss: 4.9948\n",
      "Epoch [1/5], Step [66225/68337], Loss: 4.9905\n",
      "Epoch [1/5], Step [66300/68337], Loss: 5.0038\n",
      "Epoch [1/5], Step [66375/68337], Loss: 5.0299\n",
      "Epoch [1/5], Step [66450/68337], Loss: 5.2059\n",
      "Epoch [1/5], Step [66525/68337], Loss: 5.3024\n",
      "Epoch [1/5], Step [66600/68337], Loss: 5.2363\n",
      "Epoch [1/5], Step [66675/68337], Loss: 5.2536\n",
      "Epoch [1/5], Step [66750/68337], Loss: 5.2656\n",
      "Epoch [1/5], Step [66825/68337], Loss: 5.0945\n",
      "Epoch [1/5], Step [66900/68337], Loss: 5.1425\n",
      "Epoch [1/5], Step [66975/68337], Loss: 5.1282\n",
      "Epoch [1/5], Step [67050/68337], Loss: 5.2105\n",
      "Epoch [1/5], Step [67125/68337], Loss: 5.0024\n",
      "Epoch [1/5], Step [67200/68337], Loss: 5.1346\n",
      "Epoch [1/5], Step [67275/68337], Loss: 5.0546\n",
      "Epoch [1/5], Step [67350/68337], Loss: 5.1777\n",
      "Epoch [1/5], Step [67425/68337], Loss: 5.2602\n",
      "Epoch [1/5], Step [67500/68337], Loss: 5.2282\n",
      "Epoch [1/5], Step [67575/68337], Loss: 5.2242\n",
      "Epoch [1/5], Step [67650/68337], Loss: 5.3326\n",
      "Epoch [1/5], Step [67725/68337], Loss: 5.1280\n",
      "Epoch [1/5], Step [67800/68337], Loss: 5.1385\n",
      "Epoch [1/5], Step [67875/68337], Loss: 5.2495\n",
      "Epoch [1/5], Step [67950/68337], Loss: 5.0912\n",
      "Epoch [1/5], Step [68025/68337], Loss: 4.9929\n",
      "Epoch [1/5], Step [68100/68337], Loss: 5.0030\n",
      "Epoch [1/5], Step [68175/68337], Loss: 5.0191\n",
      "Epoch [1/5], Step [68250/68337], Loss: 5.2234\n",
      "Epoch [1/5], Step [68325/68337], Loss: 5.1321\n",
      "Epoch [1/5] Average Loss: 5.3602, Perplexity: 212.78\n",
      "Epoch [2/5], Step [0/68337], Loss: 5.0846\n",
      "Validation perplexity: 161.0612401553239\n",
      "Epoch [2/5], Step [75/68337], Loss: 5.3923\n",
      "Epoch [2/5], Step [150/68337], Loss: 5.1647\n",
      "Epoch [2/5], Step [225/68337], Loss: 5.1217\n",
      "Epoch [2/5], Step [300/68337], Loss: 5.1651\n",
      "Epoch [2/5], Step [375/68337], Loss: 5.1871\n",
      "Epoch [2/5], Step [450/68337], Loss: 5.0867\n",
      "Epoch [2/5], Step [525/68337], Loss: 4.9902\n",
      "Epoch [2/5], Step [600/68337], Loss: 5.1758\n",
      "Epoch [2/5], Step [675/68337], Loss: 5.1933\n",
      "Epoch [2/5], Step [750/68337], Loss: 5.2614\n",
      "Epoch [2/5], Step [825/68337], Loss: 5.0820\n",
      "Epoch [2/5], Step [900/68337], Loss: 4.9637\n",
      "Epoch [2/5], Step [975/68337], Loss: 5.0598\n",
      "Epoch [2/5], Step [1050/68337], Loss: 5.2357\n",
      "Epoch [2/5], Step [1125/68337], Loss: 5.1711\n",
      "Epoch [2/5], Step [1200/68337], Loss: 5.0958\n",
      "Epoch [2/5], Step [1275/68337], Loss: 5.0425\n",
      "Epoch [2/5], Step [1350/68337], Loss: 5.1960\n",
      "Epoch [2/5], Step [1425/68337], Loss: 5.1254\n",
      "Epoch [2/5], Step [1500/68337], Loss: 5.0954\n",
      "Epoch [2/5], Step [1575/68337], Loss: 5.1393\n",
      "Epoch [2/5], Step [1650/68337], Loss: 5.0999\n",
      "Epoch [2/5], Step [1725/68337], Loss: 4.9692\n",
      "Epoch [2/5], Step [1800/68337], Loss: 5.2564\n",
      "Epoch [2/5], Step [1875/68337], Loss: 5.2383\n",
      "Epoch [2/5], Step [1950/68337], Loss: 5.0283\n",
      "Epoch [2/5], Step [2025/68337], Loss: 5.2074\n",
      "Epoch [2/5], Step [2100/68337], Loss: 5.2377\n",
      "Epoch [2/5], Step [2175/68337], Loss: 5.0635\n",
      "Epoch [2/5], Step [2250/68337], Loss: 5.1344\n",
      "Epoch [2/5], Step [2325/68337], Loss: 5.1269\n",
      "Epoch [2/5], Step [2400/68337], Loss: 5.0561\n",
      "Epoch [2/5], Step [2475/68337], Loss: 5.1108\n",
      "Epoch [2/5], Step [2550/68337], Loss: 5.1541\n",
      "Epoch [2/5], Step [2625/68337], Loss: 5.1499\n",
      "Epoch [2/5], Step [2700/68337], Loss: 5.0947\n",
      "Epoch [2/5], Step [2775/68337], Loss: 5.1208\n",
      "Epoch [2/5], Step [2850/68337], Loss: 5.1577\n",
      "Epoch [2/5], Step [2925/68337], Loss: 5.3038\n",
      "Epoch [2/5], Step [3000/68337], Loss: 5.1943\n",
      "Epoch [2/5], Step [3075/68337], Loss: 5.0919\n",
      "Epoch [2/5], Step [3150/68337], Loss: 5.0311\n",
      "Epoch [2/5], Step [3225/68337], Loss: 4.9476\n",
      "Epoch [2/5], Step [3300/68337], Loss: 4.9707\n",
      "Epoch [2/5], Step [3375/68337], Loss: 5.1509\n",
      "Epoch [2/5], Step [3450/68337], Loss: 5.0595\n",
      "Epoch [2/5], Step [3525/68337], Loss: 5.0276\n",
      "Epoch [2/5], Step [3600/68337], Loss: 5.0820\n",
      "Epoch [2/5], Step [3675/68337], Loss: 5.2059\n",
      "Epoch [2/5], Step [3750/68337], Loss: 5.2197\n",
      "Epoch [2/5], Step [3825/68337], Loss: 5.1816\n",
      "Epoch [2/5], Step [3900/68337], Loss: 5.0254\n",
      "Epoch [2/5], Step [3975/68337], Loss: 5.1530\n",
      "Epoch [2/5], Step [4050/68337], Loss: 5.1492\n",
      "Epoch [2/5], Step [4125/68337], Loss: 5.1739\n",
      "Epoch [2/5], Step [4200/68337], Loss: 5.2356\n",
      "Epoch [2/5], Step [4275/68337], Loss: 5.2322\n",
      "Epoch [2/5], Step [4350/68337], Loss: 5.1951\n",
      "Epoch [2/5], Step [4425/68337], Loss: 5.0584\n",
      "Epoch [2/5], Step [4500/68337], Loss: 5.0468\n",
      "Epoch [2/5], Step [4575/68337], Loss: 5.1922\n",
      "Epoch [2/5], Step [4650/68337], Loss: 5.1324\n",
      "Epoch [2/5], Step [4725/68337], Loss: 5.2608\n",
      "Epoch [2/5], Step [4800/68337], Loss: 5.2065\n",
      "Epoch [2/5], Step [4875/68337], Loss: 5.2360\n",
      "Epoch [2/5], Step [4950/68337], Loss: 5.1644\n",
      "Epoch [2/5], Step [5025/68337], Loss: 5.1883\n",
      "Epoch [2/5], Step [5100/68337], Loss: 5.1606\n",
      "Epoch [2/5], Step [5175/68337], Loss: 5.1752\n",
      "Epoch [2/5], Step [5250/68337], Loss: 5.1278\n",
      "Epoch [2/5], Step [5325/68337], Loss: 5.1676\n",
      "Epoch [2/5], Step [5400/68337], Loss: 4.9731\n",
      "Epoch [2/5], Step [5475/68337], Loss: 5.1527\n",
      "Epoch [2/5], Step [5550/68337], Loss: 5.1821\n",
      "Epoch [2/5], Step [5625/68337], Loss: 5.2430\n",
      "Epoch [2/5], Step [5700/68337], Loss: 5.1476\n",
      "Epoch [2/5], Step [5775/68337], Loss: 5.0486\n",
      "Epoch [2/5], Step [5850/68337], Loss: 5.0788\n",
      "Epoch [2/5], Step [5925/68337], Loss: 5.0775\n",
      "Epoch [2/5], Step [6000/68337], Loss: 5.1949\n",
      "Epoch [2/5], Step [6075/68337], Loss: 5.0303\n",
      "Epoch [2/5], Step [6150/68337], Loss: 5.1829\n",
      "Epoch [2/5], Step [6225/68337], Loss: 5.0862\n",
      "Epoch [2/5], Step [6300/68337], Loss: 5.2332\n",
      "Epoch [2/5], Step [6375/68337], Loss: 5.1655\n",
      "Epoch [2/5], Step [6450/68337], Loss: 5.0096\n",
      "Epoch [2/5], Step [6525/68337], Loss: 5.1456\n",
      "Epoch [2/5], Step [6600/68337], Loss: 5.2792\n",
      "Epoch [2/5], Step [6675/68337], Loss: 5.1783\n",
      "Epoch [2/5], Step [6750/68337], Loss: 5.1087\n",
      "Epoch [2/5], Step [6825/68337], Loss: 4.9954\n",
      "Epoch [2/5], Step [6900/68337], Loss: 5.0397\n",
      "Epoch [2/5], Step [6975/68337], Loss: 5.1814\n",
      "Epoch [2/5], Step [7050/68337], Loss: 5.0458\n",
      "Epoch [2/5], Step [7125/68337], Loss: 5.2324\n",
      "Epoch [2/5], Step [7200/68337], Loss: 5.0488\n",
      "Epoch [2/5], Step [7275/68337], Loss: 5.1663\n",
      "Epoch [2/5], Step [7350/68337], Loss: 4.9795\n",
      "Epoch [2/5], Step [7425/68337], Loss: 5.3065\n",
      "Epoch [2/5], Step [7500/68337], Loss: 5.0078\n",
      "Epoch [2/5], Step [7575/68337], Loss: 5.1773\n",
      "Epoch [2/5], Step [7650/68337], Loss: 5.0729\n",
      "Epoch [2/5], Step [7725/68337], Loss: 5.0988\n",
      "Epoch [2/5], Step [7800/68337], Loss: 5.0367\n",
      "Epoch [2/5], Step [7875/68337], Loss: 5.1343\n",
      "Epoch [2/5], Step [7950/68337], Loss: 5.1682\n",
      "Epoch [2/5], Step [8025/68337], Loss: 4.9958\n",
      "Epoch [2/5], Step [8100/68337], Loss: 5.0890\n",
      "Epoch [2/5], Step [8175/68337], Loss: 5.2018\n",
      "Epoch [2/5], Step [8250/68337], Loss: 5.1782\n",
      "Epoch [2/5], Step [8325/68337], Loss: 5.1273\n",
      "Epoch [2/5], Step [8400/68337], Loss: 5.0411\n",
      "Epoch [2/5], Step [8475/68337], Loss: 4.7876\n",
      "Epoch [2/5], Step [8550/68337], Loss: 5.0686\n",
      "Epoch [2/5], Step [8625/68337], Loss: 5.0227\n",
      "Epoch [2/5], Step [8700/68337], Loss: 5.1092\n",
      "Epoch [2/5], Step [8775/68337], Loss: 5.2173\n",
      "Epoch [2/5], Step [8850/68337], Loss: 4.9924\n",
      "Epoch [2/5], Step [8925/68337], Loss: 5.1037\n",
      "Epoch [2/5], Step [9000/68337], Loss: 5.2516\n",
      "Epoch [2/5], Step [9075/68337], Loss: 4.9782\n",
      "Epoch [2/5], Step [9150/68337], Loss: 5.1138\n",
      "Epoch [2/5], Step [9225/68337], Loss: 5.1112\n",
      "Epoch [2/5], Step [9300/68337], Loss: 5.1005\n",
      "Epoch [2/5], Step [9375/68337], Loss: 5.0696\n",
      "Epoch [2/5], Step [9450/68337], Loss: 5.3526\n",
      "Epoch [2/5], Step [9525/68337], Loss: 4.9801\n",
      "Epoch [2/5], Step [9600/68337], Loss: 5.0091\n",
      "Epoch [2/5], Step [9675/68337], Loss: 5.0399\n",
      "Epoch [2/5], Step [9750/68337], Loss: 5.2018\n",
      "Epoch [2/5], Step [9825/68337], Loss: 4.9610\n",
      "Epoch [2/5], Step [9900/68337], Loss: 5.2577\n",
      "Epoch [2/5], Step [9975/68337], Loss: 5.2132\n",
      "Validation perplexity: 159.07406488821388\n",
      "Epoch [2/5], Step [10050/68337], Loss: 5.2125\n",
      "Epoch [2/5], Step [10125/68337], Loss: 5.1687\n",
      "Epoch [2/5], Step [10200/68337], Loss: 5.1514\n",
      "Epoch [2/5], Step [10275/68337], Loss: 5.0422\n",
      "Epoch [2/5], Step [10350/68337], Loss: 5.1120\n",
      "Epoch [2/5], Step [10425/68337], Loss: 5.0714\n",
      "Epoch [2/5], Step [10500/68337], Loss: 4.9866\n",
      "Epoch [2/5], Step [10575/68337], Loss: 4.9346\n",
      "Epoch [2/5], Step [10650/68337], Loss: 5.1573\n",
      "Epoch [2/5], Step [10725/68337], Loss: 5.3330\n",
      "Epoch [2/5], Step [10800/68337], Loss: 5.0297\n",
      "Epoch [2/5], Step [10875/68337], Loss: 5.0280\n",
      "Epoch [2/5], Step [10950/68337], Loss: 5.2117\n",
      "Epoch [2/5], Step [11025/68337], Loss: 5.2111\n",
      "Epoch [2/5], Step [11100/68337], Loss: 5.0547\n",
      "Epoch [2/5], Step [11175/68337], Loss: 4.9999\n",
      "Epoch [2/5], Step [11250/68337], Loss: 5.1466\n",
      "Epoch [2/5], Step [11325/68337], Loss: 5.0791\n",
      "Epoch [2/5], Step [11400/68337], Loss: 5.0942\n",
      "Epoch [2/5], Step [11475/68337], Loss: 5.1383\n",
      "Epoch [2/5], Step [11550/68337], Loss: 5.1642\n",
      "Epoch [2/5], Step [11625/68337], Loss: 5.2120\n",
      "Epoch [2/5], Step [11700/68337], Loss: 5.1624\n",
      "Epoch [2/5], Step [11775/68337], Loss: 4.9360\n",
      "Epoch [2/5], Step [11850/68337], Loss: 5.0430\n",
      "Epoch [2/5], Step [11925/68337], Loss: 5.1318\n",
      "Epoch [2/5], Step [12000/68337], Loss: 5.1214\n",
      "Epoch [2/5], Step [12075/68337], Loss: 5.1926\n",
      "Epoch [2/5], Step [12150/68337], Loss: 5.1448\n",
      "Epoch [2/5], Step [12225/68337], Loss: 5.0584\n",
      "Epoch [2/5], Step [12300/68337], Loss: 5.2742\n",
      "Epoch [2/5], Step [12375/68337], Loss: 5.1428\n",
      "Epoch [2/5], Step [12450/68337], Loss: 5.0075\n",
      "Epoch [2/5], Step [12525/68337], Loss: 4.9715\n",
      "Epoch [2/5], Step [12600/68337], Loss: 5.2516\n",
      "Epoch [2/5], Step [12675/68337], Loss: 5.2212\n",
      "Epoch [2/5], Step [12750/68337], Loss: 4.9917\n",
      "Epoch [2/5], Step [12825/68337], Loss: 4.9848\n",
      "Epoch [2/5], Step [12900/68337], Loss: 5.0732\n",
      "Epoch [2/5], Step [12975/68337], Loss: 4.9650\n",
      "Epoch [2/5], Step [13050/68337], Loss: 5.1174\n",
      "Epoch [2/5], Step [13125/68337], Loss: 5.0763\n",
      "Epoch [2/5], Step [13200/68337], Loss: 5.1225\n",
      "Epoch [2/5], Step [13275/68337], Loss: 4.9302\n",
      "Epoch [2/5], Step [13350/68337], Loss: 5.0466\n",
      "Epoch [2/5], Step [13425/68337], Loss: 4.9871\n",
      "Epoch [2/5], Step [13500/68337], Loss: 4.9957\n",
      "Epoch [2/5], Step [13575/68337], Loss: 5.1767\n",
      "Epoch [2/5], Step [13650/68337], Loss: 5.1814\n",
      "Epoch [2/5], Step [13725/68337], Loss: 5.0888\n",
      "Epoch [2/5], Step [13800/68337], Loss: 5.0934\n",
      "Epoch [2/5], Step [13875/68337], Loss: 5.2624\n",
      "Epoch [2/5], Step [13950/68337], Loss: 5.1416\n",
      "Epoch [2/5], Step [14025/68337], Loss: 5.2078\n",
      "Epoch [2/5], Step [14100/68337], Loss: 5.0727\n",
      "Epoch [2/5], Step [14175/68337], Loss: 5.2617\n",
      "Epoch [2/5], Step [14250/68337], Loss: 5.1433\n",
      "Epoch [2/5], Step [14325/68337], Loss: 4.9144\n",
      "Epoch [2/5], Step [14400/68337], Loss: 5.2253\n",
      "Epoch [2/5], Step [14475/68337], Loss: 5.0317\n",
      "Epoch [2/5], Step [14550/68337], Loss: 5.2471\n",
      "Epoch [2/5], Step [14625/68337], Loss: 5.1262\n",
      "Epoch [2/5], Step [14700/68337], Loss: 5.1544\n",
      "Epoch [2/5], Step [14775/68337], Loss: 5.1307\n",
      "Epoch [2/5], Step [14850/68337], Loss: 5.0602\n",
      "Epoch [2/5], Step [14925/68337], Loss: 4.9251\n",
      "Epoch [2/5], Step [15000/68337], Loss: 5.2603\n",
      "Epoch [2/5], Step [15075/68337], Loss: 5.1605\n",
      "Epoch [2/5], Step [15150/68337], Loss: 5.2419\n",
      "Epoch [2/5], Step [15225/68337], Loss: 5.0948\n",
      "Epoch [2/5], Step [15300/68337], Loss: 5.0394\n",
      "Epoch [2/5], Step [15375/68337], Loss: 5.3250\n",
      "Epoch [2/5], Step [15450/68337], Loss: 5.0408\n",
      "Epoch [2/5], Step [15525/68337], Loss: 5.0463\n",
      "Epoch [2/5], Step [15600/68337], Loss: 5.1581\n",
      "Epoch [2/5], Step [15675/68337], Loss: 5.1119\n",
      "Epoch [2/5], Step [15750/68337], Loss: 5.2619\n",
      "Epoch [2/5], Step [15825/68337], Loss: 5.2349\n",
      "Epoch [2/5], Step [15900/68337], Loss: 5.2228\n",
      "Epoch [2/5], Step [15975/68337], Loss: 5.0170\n",
      "Epoch [2/5], Step [16050/68337], Loss: 5.1369\n",
      "Epoch [2/5], Step [16125/68337], Loss: 5.2690\n",
      "Epoch [2/5], Step [16200/68337], Loss: 5.0910\n",
      "Epoch [2/5], Step [16275/68337], Loss: 5.0609\n",
      "Epoch [2/5], Step [16350/68337], Loss: 5.1365\n",
      "Epoch [2/5], Step [16425/68337], Loss: 4.8887\n",
      "Epoch [2/5], Step [16500/68337], Loss: 5.0541\n",
      "Epoch [2/5], Step [16575/68337], Loss: 5.1670\n",
      "Epoch [2/5], Step [16650/68337], Loss: 5.1541\n",
      "Epoch [2/5], Step [16725/68337], Loss: 5.1180\n",
      "Epoch [2/5], Step [16800/68337], Loss: 5.0616\n",
      "Epoch [2/5], Step [16875/68337], Loss: 5.0866\n",
      "Epoch [2/5], Step [16950/68337], Loss: 5.0867\n",
      "Epoch [2/5], Step [17025/68337], Loss: 5.1590\n",
      "Epoch [2/5], Step [17100/68337], Loss: 5.1954\n",
      "Epoch [2/5], Step [17175/68337], Loss: 4.9879\n",
      "Epoch [2/5], Step [17250/68337], Loss: 5.1544\n",
      "Epoch [2/5], Step [17325/68337], Loss: 5.0428\n",
      "Epoch [2/5], Step [17400/68337], Loss: 5.0788\n",
      "Epoch [2/5], Step [17475/68337], Loss: 5.1811\n",
      "Epoch [2/5], Step [17550/68337], Loss: 5.1343\n",
      "Epoch [2/5], Step [17625/68337], Loss: 5.1430\n",
      "Epoch [2/5], Step [17700/68337], Loss: 5.1729\n",
      "Epoch [2/5], Step [17775/68337], Loss: 4.9824\n",
      "Epoch [2/5], Step [17850/68337], Loss: 5.2169\n",
      "Epoch [2/5], Step [17925/68337], Loss: 5.1866\n",
      "Epoch [2/5], Step [18000/68337], Loss: 5.1726\n",
      "Epoch [2/5], Step [18075/68337], Loss: 5.0084\n",
      "Epoch [2/5], Step [18150/68337], Loss: 5.0677\n",
      "Epoch [2/5], Step [18225/68337], Loss: 5.0266\n",
      "Epoch [2/5], Step [18300/68337], Loss: 5.0896\n",
      "Epoch [2/5], Step [18375/68337], Loss: 5.0492\n",
      "Epoch [2/5], Step [18450/68337], Loss: 5.2312\n",
      "Epoch [2/5], Step [18525/68337], Loss: 5.1913\n",
      "Epoch [2/5], Step [18600/68337], Loss: 5.1998\n",
      "Epoch [2/5], Step [18675/68337], Loss: 5.2575\n",
      "Epoch [2/5], Step [18750/68337], Loss: 5.2009\n",
      "Epoch [2/5], Step [18825/68337], Loss: 4.9817\n",
      "Epoch [2/5], Step [18900/68337], Loss: 5.1350\n",
      "Epoch [2/5], Step [18975/68337], Loss: 4.9783\n",
      "Epoch [2/5], Step [19050/68337], Loss: 5.2677\n",
      "Epoch [2/5], Step [19125/68337], Loss: 5.1120\n",
      "Epoch [2/5], Step [19200/68337], Loss: 5.0554\n",
      "Epoch [2/5], Step [19275/68337], Loss: 5.1501\n",
      "Epoch [2/5], Step [19350/68337], Loss: 5.1031\n",
      "Epoch [2/5], Step [19425/68337], Loss: 5.1631\n",
      "Epoch [2/5], Step [19500/68337], Loss: 5.1916\n",
      "Epoch [2/5], Step [19575/68337], Loss: 5.2240\n",
      "Epoch [2/5], Step [19650/68337], Loss: 5.1332\n",
      "Epoch [2/5], Step [19725/68337], Loss: 5.0500\n",
      "Epoch [2/5], Step [19800/68337], Loss: 5.1932\n",
      "Epoch [2/5], Step [19875/68337], Loss: 5.0087\n",
      "Epoch [2/5], Step [19950/68337], Loss: 5.1534\n",
      "Validation perplexity: 157.47652625087525\n",
      "Epoch [2/5], Step [20025/68337], Loss: 5.2547\n",
      "Epoch [2/5], Step [20100/68337], Loss: 5.2107\n",
      "Epoch [2/5], Step [20175/68337], Loss: 5.0771\n",
      "Epoch [2/5], Step [20250/68337], Loss: 5.1792\n",
      "Epoch [2/5], Step [20325/68337], Loss: 5.2853\n",
      "Epoch [2/5], Step [20400/68337], Loss: 5.1023\n",
      "Epoch [2/5], Step [20475/68337], Loss: 5.0569\n",
      "Epoch [2/5], Step [20550/68337], Loss: 5.2810\n",
      "Epoch [2/5], Step [20625/68337], Loss: 5.2268\n",
      "Epoch [2/5], Step [20700/68337], Loss: 5.2358\n",
      "Epoch [2/5], Step [20775/68337], Loss: 4.8977\n",
      "Epoch [2/5], Step [20850/68337], Loss: 5.2351\n",
      "Epoch [2/5], Step [20925/68337], Loss: 5.1409\n",
      "Epoch [2/5], Step [21000/68337], Loss: 5.1784\n",
      "Epoch [2/5], Step [21075/68337], Loss: 5.1746\n",
      "Epoch [2/5], Step [21150/68337], Loss: 5.0548\n",
      "Epoch [2/5], Step [21225/68337], Loss: 4.8881\n",
      "Epoch [2/5], Step [21300/68337], Loss: 4.9797\n",
      "Epoch [2/5], Step [21375/68337], Loss: 5.2274\n",
      "Epoch [2/5], Step [21450/68337], Loss: 5.2269\n",
      "Epoch [2/5], Step [21525/68337], Loss: 5.2056\n",
      "Epoch [2/5], Step [21600/68337], Loss: 5.1446\n",
      "Epoch [2/5], Step [21675/68337], Loss: 5.0667\n",
      "Epoch [2/5], Step [21750/68337], Loss: 5.0242\n",
      "Epoch [2/5], Step [21825/68337], Loss: 5.3983\n",
      "Epoch [2/5], Step [21900/68337], Loss: 5.1669\n",
      "Epoch [2/5], Step [21975/68337], Loss: 5.0268\n",
      "Epoch [2/5], Step [22050/68337], Loss: 5.0235\n",
      "Epoch [2/5], Step [22125/68337], Loss: 5.1971\n",
      "Epoch [2/5], Step [22200/68337], Loss: 5.2197\n",
      "Epoch [2/5], Step [22275/68337], Loss: 5.1768\n",
      "Epoch [2/5], Step [22350/68337], Loss: 5.0655\n",
      "Epoch [2/5], Step [22425/68337], Loss: 5.1930\n",
      "Epoch [2/5], Step [22500/68337], Loss: 5.1772\n",
      "Epoch [2/5], Step [22575/68337], Loss: 5.1893\n",
      "Epoch [2/5], Step [22650/68337], Loss: 5.2929\n",
      "Epoch [2/5], Step [22725/68337], Loss: 5.0615\n",
      "Epoch [2/5], Step [22800/68337], Loss: 5.1279\n",
      "Epoch [2/5], Step [22875/68337], Loss: 5.0991\n",
      "Epoch [2/5], Step [22950/68337], Loss: 5.1733\n",
      "Epoch [2/5], Step [23025/68337], Loss: 5.0127\n",
      "Epoch [2/5], Step [23100/68337], Loss: 5.1360\n",
      "Epoch [2/5], Step [23175/68337], Loss: 5.0062\n",
      "Epoch [2/5], Step [23250/68337], Loss: 5.2419\n",
      "Epoch [2/5], Step [23325/68337], Loss: 5.0324\n",
      "Epoch [2/5], Step [23400/68337], Loss: 5.0764\n",
      "Epoch [2/5], Step [23475/68337], Loss: 4.9439\n",
      "Epoch [2/5], Step [23550/68337], Loss: 4.8569\n",
      "Epoch [2/5], Step [23625/68337], Loss: 5.1591\n",
      "Epoch [2/5], Step [23700/68337], Loss: 5.1250\n",
      "Epoch [2/5], Step [23775/68337], Loss: 5.0751\n",
      "Epoch [2/5], Step [23850/68337], Loss: 5.0385\n",
      "Epoch [2/5], Step [23925/68337], Loss: 5.0911\n",
      "Epoch [2/5], Step [24000/68337], Loss: 5.0757\n",
      "Epoch [2/5], Step [24075/68337], Loss: 5.2897\n",
      "Epoch [2/5], Step [24150/68337], Loss: 5.0182\n",
      "Epoch [2/5], Step [24225/68337], Loss: 5.2188\n",
      "Epoch [2/5], Step [24300/68337], Loss: 5.2955\n",
      "Epoch [2/5], Step [24375/68337], Loss: 5.2245\n",
      "Epoch [2/5], Step [24450/68337], Loss: 5.1400\n",
      "Epoch [2/5], Step [24525/68337], Loss: 5.1190\n",
      "Epoch [2/5], Step [24600/68337], Loss: 5.0257\n",
      "Epoch [2/5], Step [24675/68337], Loss: 5.0671\n",
      "Epoch [2/5], Step [24750/68337], Loss: 5.1173\n",
      "Epoch [2/5], Step [24825/68337], Loss: 5.0600\n",
      "Epoch [2/5], Step [24900/68337], Loss: 5.1072\n",
      "Epoch [2/5], Step [24975/68337], Loss: 5.1295\n",
      "Epoch [2/5], Step [25050/68337], Loss: 5.1326\n",
      "Epoch [2/5], Step [25125/68337], Loss: 5.2558\n",
      "Epoch [2/5], Step [25200/68337], Loss: 5.1236\n",
      "Epoch [2/5], Step [25275/68337], Loss: 5.1391\n",
      "Epoch [2/5], Step [25350/68337], Loss: 5.0957\n",
      "Epoch [2/5], Step [25425/68337], Loss: 5.2921\n",
      "Epoch [2/5], Step [25500/68337], Loss: 5.1322\n",
      "Epoch [2/5], Step [25575/68337], Loss: 5.1684\n",
      "Epoch [2/5], Step [25650/68337], Loss: 5.1729\n",
      "Epoch [2/5], Step [25725/68337], Loss: 4.9706\n",
      "Epoch [2/5], Step [25800/68337], Loss: 5.1075\n",
      "Epoch [2/5], Step [25875/68337], Loss: 4.9678\n",
      "Epoch [2/5], Step [25950/68337], Loss: 5.1249\n",
      "Epoch [2/5], Step [26025/68337], Loss: 5.1315\n",
      "Epoch [2/5], Step [26100/68337], Loss: 4.9172\n",
      "Epoch [2/5], Step [26175/68337], Loss: 5.1060\n",
      "Epoch [2/5], Step [26250/68337], Loss: 5.1299\n",
      "Epoch [2/5], Step [26325/68337], Loss: 5.0647\n",
      "Epoch [2/5], Step [26400/68337], Loss: 5.1573\n",
      "Epoch [2/5], Step [26475/68337], Loss: 5.0907\n",
      "Epoch [2/5], Step [26550/68337], Loss: 5.1791\n",
      "Epoch [2/5], Step [26625/68337], Loss: 5.0687\n",
      "Epoch [2/5], Step [26700/68337], Loss: 5.1771\n",
      "Epoch [2/5], Step [26775/68337], Loss: 5.0180\n",
      "Epoch [2/5], Step [26850/68337], Loss: 4.9606\n",
      "Epoch [2/5], Step [26925/68337], Loss: 5.0778\n",
      "Epoch [2/5], Step [27000/68337], Loss: 5.0498\n",
      "Epoch [2/5], Step [27075/68337], Loss: 5.3028\n",
      "Epoch [2/5], Step [27150/68337], Loss: 5.1155\n",
      "Epoch [2/5], Step [27225/68337], Loss: 5.1819\n",
      "Epoch [2/5], Step [27300/68337], Loss: 5.1840\n",
      "Epoch [2/5], Step [27375/68337], Loss: 5.1286\n",
      "Epoch [2/5], Step [27450/68337], Loss: 5.0885\n",
      "Epoch [2/5], Step [27525/68337], Loss: 5.0273\n",
      "Epoch [2/5], Step [27600/68337], Loss: 5.1349\n",
      "Epoch [2/5], Step [27675/68337], Loss: 4.9861\n",
      "Epoch [2/5], Step [27750/68337], Loss: 5.0319\n",
      "Epoch [2/5], Step [27825/68337], Loss: 5.2741\n",
      "Epoch [2/5], Step [27900/68337], Loss: 4.8914\n",
      "Epoch [2/5], Step [27975/68337], Loss: 5.0610\n",
      "Epoch [2/5], Step [28050/68337], Loss: 5.0149\n",
      "Epoch [2/5], Step [28125/68337], Loss: 5.1311\n",
      "Epoch [2/5], Step [28200/68337], Loss: 5.1218\n",
      "Epoch [2/5], Step [28275/68337], Loss: 5.0265\n",
      "Epoch [2/5], Step [28350/68337], Loss: 5.1987\n",
      "Epoch [2/5], Step [28425/68337], Loss: 5.1121\n",
      "Epoch [2/5], Step [28500/68337], Loss: 4.9737\n",
      "Epoch [2/5], Step [28575/68337], Loss: 5.1802\n",
      "Epoch [2/5], Step [28650/68337], Loss: 5.0367\n",
      "Epoch [2/5], Step [28725/68337], Loss: 5.2000\n",
      "Epoch [2/5], Step [28800/68337], Loss: 5.1191\n",
      "Epoch [2/5], Step [28875/68337], Loss: 4.9720\n",
      "Epoch [2/5], Step [28950/68337], Loss: 5.0497\n",
      "Epoch [2/5], Step [29025/68337], Loss: 5.0578\n",
      "Epoch [2/5], Step [29100/68337], Loss: 5.2559\n",
      "Epoch [2/5], Step [29175/68337], Loss: 4.9592\n",
      "Epoch [2/5], Step [29250/68337], Loss: 5.0245\n",
      "Epoch [2/5], Step [29325/68337], Loss: 5.1164\n",
      "Epoch [2/5], Step [29400/68337], Loss: 5.2056\n",
      "Epoch [2/5], Step [29475/68337], Loss: 5.1354\n",
      "Epoch [2/5], Step [29550/68337], Loss: 5.0995\n",
      "Epoch [2/5], Step [29625/68337], Loss: 5.1376\n",
      "Epoch [2/5], Step [29700/68337], Loss: 4.9514\n",
      "Epoch [2/5], Step [29775/68337], Loss: 5.1088\n",
      "Epoch [2/5], Step [29850/68337], Loss: 5.1618\n",
      "Epoch [2/5], Step [29925/68337], Loss: 5.1440\n",
      "Epoch [2/5], Step [30000/68337], Loss: 5.1134\n",
      "Validation perplexity: 155.48649432935346\n",
      "Epoch [2/5], Step [30075/68337], Loss: 5.1128\n",
      "Epoch [2/5], Step [30150/68337], Loss: 5.0597\n",
      "Epoch [2/5], Step [30225/68337], Loss: 5.0455\n",
      "Epoch [2/5], Step [30300/68337], Loss: 5.0273\n",
      "Epoch [2/5], Step [30375/68337], Loss: 5.1050\n",
      "Epoch [2/5], Step [30450/68337], Loss: 5.0249\n",
      "Epoch [2/5], Step [30525/68337], Loss: 5.0313\n",
      "Epoch [2/5], Step [30600/68337], Loss: 5.1865\n",
      "Epoch [2/5], Step [30675/68337], Loss: 5.0424\n",
      "Epoch [2/5], Step [30750/68337], Loss: 5.1237\n",
      "Epoch [2/5], Step [30825/68337], Loss: 5.0834\n",
      "Epoch [2/5], Step [30900/68337], Loss: 4.8707\n",
      "Epoch [2/5], Step [30975/68337], Loss: 5.0722\n",
      "Epoch [2/5], Step [31050/68337], Loss: 5.2516\n",
      "Epoch [2/5], Step [31125/68337], Loss: 5.0463\n",
      "Epoch [2/5], Step [31200/68337], Loss: 5.2701\n",
      "Epoch [2/5], Step [31275/68337], Loss: 5.1933\n",
      "Epoch [2/5], Step [31350/68337], Loss: 4.9067\n",
      "Epoch [2/5], Step [31425/68337], Loss: 5.0269\n",
      "Epoch [2/5], Step [31500/68337], Loss: 5.2031\n",
      "Epoch [2/5], Step [31575/68337], Loss: 4.9505\n",
      "Epoch [2/5], Step [31650/68337], Loss: 5.1904\n",
      "Epoch [2/5], Step [31725/68337], Loss: 5.0812\n",
      "Epoch [2/5], Step [31800/68337], Loss: 5.1433\n",
      "Epoch [2/5], Step [31875/68337], Loss: 5.1320\n",
      "Epoch [2/5], Step [31950/68337], Loss: 5.2127\n",
      "Epoch [2/5], Step [32025/68337], Loss: 5.0724\n",
      "Epoch [2/5], Step [32100/68337], Loss: 5.3001\n",
      "Epoch [2/5], Step [32175/68337], Loss: 5.0980\n",
      "Epoch [2/5], Step [32250/68337], Loss: 4.9679\n",
      "Epoch [2/5], Step [32325/68337], Loss: 5.1487\n",
      "Epoch [2/5], Step [32400/68337], Loss: 4.9129\n",
      "Epoch [2/5], Step [32475/68337], Loss: 5.0855\n",
      "Epoch [2/5], Step [32550/68337], Loss: 5.0700\n",
      "Epoch [2/5], Step [32625/68337], Loss: 5.1547\n",
      "Epoch [2/5], Step [32700/68337], Loss: 5.2132\n",
      "Epoch [2/5], Step [32775/68337], Loss: 5.1992\n",
      "Epoch [2/5], Step [32850/68337], Loss: 5.2171\n",
      "Epoch [2/5], Step [32925/68337], Loss: 5.1309\n",
      "Epoch [2/5], Step [33000/68337], Loss: 4.9558\n",
      "Epoch [2/5], Step [33075/68337], Loss: 5.1029\n",
      "Epoch [2/5], Step [33150/68337], Loss: 5.1782\n",
      "Epoch [2/5], Step [33225/68337], Loss: 4.9903\n",
      "Epoch [2/5], Step [33300/68337], Loss: 5.1735\n",
      "Epoch [2/5], Step [33375/68337], Loss: 5.0625\n",
      "Epoch [2/5], Step [33450/68337], Loss: 5.1388\n",
      "Epoch [2/5], Step [33525/68337], Loss: 5.1185\n",
      "Epoch [2/5], Step [33600/68337], Loss: 5.0146\n",
      "Epoch [2/5], Step [33675/68337], Loss: 5.0897\n",
      "Epoch [2/5], Step [33750/68337], Loss: 5.0673\n",
      "Epoch [2/5], Step [33825/68337], Loss: 5.0491\n",
      "Epoch [2/5], Step [33900/68337], Loss: 5.2072\n",
      "Epoch [2/5], Step [33975/68337], Loss: 5.1575\n",
      "Epoch [2/5], Step [34050/68337], Loss: 5.0801\n",
      "Epoch [2/5], Step [34125/68337], Loss: 5.1666\n",
      "Epoch [2/5], Step [34200/68337], Loss: 4.9916\n",
      "Epoch [2/5], Step [34275/68337], Loss: 5.1203\n",
      "Epoch [2/5], Step [34350/68337], Loss: 5.0557\n",
      "Epoch [2/5], Step [34425/68337], Loss: 5.0945\n",
      "Epoch [2/5], Step [34500/68337], Loss: 5.2099\n",
      "Epoch [2/5], Step [34575/68337], Loss: 5.0656\n",
      "Epoch [2/5], Step [34650/68337], Loss: 5.1650\n",
      "Epoch [2/5], Step [34725/68337], Loss: 5.1203\n",
      "Epoch [2/5], Step [34800/68337], Loss: 4.9690\n",
      "Epoch [2/5], Step [34875/68337], Loss: 5.0356\n",
      "Epoch [2/5], Step [34950/68337], Loss: 4.8547\n",
      "Epoch [2/5], Step [35025/68337], Loss: 5.1385\n",
      "Epoch [2/5], Step [35100/68337], Loss: 5.0565\n",
      "Epoch [2/5], Step [35175/68337], Loss: 5.1769\n",
      "Epoch [2/5], Step [35250/68337], Loss: 5.0696\n",
      "Epoch [2/5], Step [35325/68337], Loss: 5.0793\n",
      "Epoch [2/5], Step [35400/68337], Loss: 5.0445\n",
      "Epoch [2/5], Step [35475/68337], Loss: 4.9294\n",
      "Epoch [2/5], Step [35550/68337], Loss: 5.2351\n",
      "Epoch [2/5], Step [35625/68337], Loss: 5.1220\n",
      "Epoch [2/5], Step [35700/68337], Loss: 5.2777\n",
      "Epoch [2/5], Step [35775/68337], Loss: 5.0608\n",
      "Epoch [2/5], Step [35850/68337], Loss: 5.1052\n",
      "Epoch [2/5], Step [35925/68337], Loss: 5.0186\n",
      "Epoch [2/5], Step [36000/68337], Loss: 5.1341\n",
      "Epoch [2/5], Step [36075/68337], Loss: 5.2590\n",
      "Epoch [2/5], Step [36150/68337], Loss: 5.1238\n",
      "Epoch [2/5], Step [36225/68337], Loss: 5.0890\n",
      "Epoch [2/5], Step [36300/68337], Loss: 5.0602\n",
      "Epoch [2/5], Step [36375/68337], Loss: 4.9841\n",
      "Epoch [2/5], Step [36450/68337], Loss: 5.1308\n",
      "Epoch [2/5], Step [36525/68337], Loss: 5.0213\n",
      "Epoch [2/5], Step [36600/68337], Loss: 5.0006\n",
      "Epoch [2/5], Step [36675/68337], Loss: 5.0677\n",
      "Epoch [2/5], Step [36750/68337], Loss: 5.0454\n",
      "Epoch [2/5], Step [36825/68337], Loss: 5.1977\n",
      "Epoch [2/5], Step [36900/68337], Loss: 5.1053\n",
      "Epoch [2/5], Step [36975/68337], Loss: 5.0226\n",
      "Epoch [2/5], Step [37050/68337], Loss: 5.0768\n",
      "Epoch [2/5], Step [37125/68337], Loss: 4.9804\n",
      "Epoch [2/5], Step [37200/68337], Loss: 5.0239\n",
      "Epoch [2/5], Step [37275/68337], Loss: 5.0215\n",
      "Epoch [2/5], Step [37350/68337], Loss: 5.0768\n",
      "Epoch [2/5], Step [37425/68337], Loss: 5.0185\n",
      "Epoch [2/5], Step [37500/68337], Loss: 5.0439\n",
      "Epoch [2/5], Step [37575/68337], Loss: 5.0117\n",
      "Epoch [2/5], Step [37650/68337], Loss: 5.1370\n",
      "Epoch [2/5], Step [37725/68337], Loss: 5.0895\n",
      "Epoch [2/5], Step [37800/68337], Loss: 5.0708\n",
      "Epoch [2/5], Step [37875/68337], Loss: 5.0630\n",
      "Epoch [2/5], Step [37950/68337], Loss: 5.0549\n",
      "Epoch [2/5], Step [38025/68337], Loss: 5.1842\n",
      "Epoch [2/5], Step [38100/68337], Loss: 5.2359\n",
      "Epoch [2/5], Step [38175/68337], Loss: 5.0189\n",
      "Epoch [2/5], Step [38250/68337], Loss: 5.0198\n",
      "Epoch [2/5], Step [38325/68337], Loss: 5.2119\n",
      "Epoch [2/5], Step [38400/68337], Loss: 5.1188\n",
      "Epoch [2/5], Step [38475/68337], Loss: 5.0418\n",
      "Epoch [2/5], Step [38550/68337], Loss: 5.1278\n",
      "Epoch [2/5], Step [38625/68337], Loss: 5.2014\n",
      "Epoch [2/5], Step [38700/68337], Loss: 5.2002\n",
      "Epoch [2/5], Step [38775/68337], Loss: 5.2462\n",
      "Epoch [2/5], Step [38850/68337], Loss: 5.1985\n",
      "Epoch [2/5], Step [38925/68337], Loss: 5.1267\n",
      "Epoch [2/5], Step [39000/68337], Loss: 5.2547\n",
      "Epoch [2/5], Step [39075/68337], Loss: 5.0493\n",
      "Epoch [2/5], Step [39150/68337], Loss: 5.0790\n",
      "Epoch [2/5], Step [39225/68337], Loss: 5.1528\n",
      "Epoch [2/5], Step [39300/68337], Loss: 5.0985\n",
      "Epoch [2/5], Step [39375/68337], Loss: 5.0816\n",
      "Epoch [2/5], Step [39450/68337], Loss: 5.0901\n",
      "Epoch [2/5], Step [39525/68337], Loss: 5.1215\n",
      "Epoch [2/5], Step [39600/68337], Loss: 4.9914\n",
      "Epoch [2/5], Step [39675/68337], Loss: 4.9633\n",
      "Epoch [2/5], Step [39750/68337], Loss: 5.0300\n",
      "Epoch [2/5], Step [39825/68337], Loss: 5.1866\n",
      "Epoch [2/5], Step [39900/68337], Loss: 5.0978\n",
      "Epoch [2/5], Step [39975/68337], Loss: 4.9911\n",
      "Validation perplexity: 154.40358642379266\n",
      "Epoch [2/5], Step [40050/68337], Loss: 5.0201\n",
      "Epoch [2/5], Step [40125/68337], Loss: 5.0051\n",
      "Epoch [2/5], Step [40200/68337], Loss: 5.1255\n",
      "Epoch [2/5], Step [40275/68337], Loss: 5.1740\n",
      "Epoch [2/5], Step [40350/68337], Loss: 5.0734\n",
      "Epoch [2/5], Step [40425/68337], Loss: 5.0529\n",
      "Epoch [2/5], Step [40500/68337], Loss: 4.9610\n",
      "Epoch [2/5], Step [40575/68337], Loss: 5.1789\n",
      "Epoch [2/5], Step [40650/68337], Loss: 5.0938\n",
      "Epoch [2/5], Step [40725/68337], Loss: 5.0731\n",
      "Epoch [2/5], Step [40800/68337], Loss: 5.1182\n",
      "Epoch [2/5], Step [40875/68337], Loss: 4.9875\n",
      "Epoch [2/5], Step [40950/68337], Loss: 5.1178\n",
      "Epoch [2/5], Step [41025/68337], Loss: 4.9698\n",
      "Epoch [2/5], Step [41100/68337], Loss: 5.0009\n",
      "Epoch [2/5], Step [41175/68337], Loss: 5.1091\n",
      "Epoch [2/5], Step [41250/68337], Loss: 4.9064\n",
      "Epoch [2/5], Step [41325/68337], Loss: 5.0988\n",
      "Epoch [2/5], Step [41400/68337], Loss: 4.9960\n",
      "Epoch [2/5], Step [41475/68337], Loss: 5.1142\n",
      "Epoch [2/5], Step [41550/68337], Loss: 5.0159\n",
      "Epoch [2/5], Step [41625/68337], Loss: 5.2677\n",
      "Epoch [2/5], Step [41700/68337], Loss: 5.0840\n",
      "Epoch [2/5], Step [41775/68337], Loss: 5.0569\n",
      "Epoch [2/5], Step [41850/68337], Loss: 5.0980\n",
      "Epoch [2/5], Step [41925/68337], Loss: 5.2031\n",
      "Epoch [2/5], Step [42000/68337], Loss: 4.9673\n",
      "Epoch [2/5], Step [42075/68337], Loss: 5.0765\n",
      "Epoch [2/5], Step [42150/68337], Loss: 5.1394\n",
      "Epoch [2/5], Step [42225/68337], Loss: 5.1030\n",
      "Epoch [2/5], Step [42300/68337], Loss: 5.0406\n",
      "Epoch [2/5], Step [42375/68337], Loss: 5.0117\n",
      "Epoch [2/5], Step [42450/68337], Loss: 5.1930\n",
      "Epoch [2/5], Step [42525/68337], Loss: 5.1820\n",
      "Epoch [2/5], Step [42600/68337], Loss: 5.1106\n",
      "Epoch [2/5], Step [42675/68337], Loss: 4.9907\n",
      "Epoch [2/5], Step [42750/68337], Loss: 5.0196\n",
      "Epoch [2/5], Step [42825/68337], Loss: 5.0727\n",
      "Epoch [2/5], Step [42900/68337], Loss: 5.1275\n",
      "Epoch [2/5], Step [42975/68337], Loss: 5.3316\n",
      "Epoch [2/5], Step [43050/68337], Loss: 5.2916\n",
      "Epoch [2/5], Step [43125/68337], Loss: 5.0516\n",
      "Epoch [2/5], Step [43200/68337], Loss: 5.0714\n",
      "Epoch [2/5], Step [43275/68337], Loss: 5.1872\n",
      "Epoch [2/5], Step [43350/68337], Loss: 5.0697\n",
      "Epoch [2/5], Step [43425/68337], Loss: 5.1342\n",
      "Epoch [2/5], Step [43500/68337], Loss: 4.9777\n",
      "Epoch [2/5], Step [43575/68337], Loss: 5.0123\n",
      "Epoch [2/5], Step [43650/68337], Loss: 4.9738\n",
      "Epoch [2/5], Step [43725/68337], Loss: 5.0824\n",
      "Epoch [2/5], Step [43800/68337], Loss: 5.1595\n",
      "Epoch [2/5], Step [43875/68337], Loss: 5.1221\n",
      "Epoch [2/5], Step [43950/68337], Loss: 5.1324\n",
      "Epoch [2/5], Step [44025/68337], Loss: 5.2754\n",
      "Epoch [2/5], Step [44100/68337], Loss: 5.1989\n",
      "Epoch [2/5], Step [44175/68337], Loss: 4.9665\n",
      "Epoch [2/5], Step [44250/68337], Loss: 5.1158\n",
      "Epoch [2/5], Step [44325/68337], Loss: 5.0535\n",
      "Epoch [2/5], Step [44400/68337], Loss: 5.2205\n",
      "Epoch [2/5], Step [44475/68337], Loss: 5.0198\n",
      "Epoch [2/5], Step [44550/68337], Loss: 5.1157\n",
      "Epoch [2/5], Step [44625/68337], Loss: 5.1889\n",
      "Epoch [2/5], Step [44700/68337], Loss: 5.0491\n",
      "Epoch [2/5], Step [44775/68337], Loss: 5.0610\n",
      "Epoch [2/5], Step [44850/68337], Loss: 4.8620\n",
      "Epoch [2/5], Step [44925/68337], Loss: 5.0732\n",
      "Epoch [2/5], Step [45000/68337], Loss: 5.1547\n",
      "Epoch [2/5], Step [45075/68337], Loss: 5.1235\n",
      "Epoch [2/5], Step [45150/68337], Loss: 5.0126\n",
      "Epoch [2/5], Step [45225/68337], Loss: 5.0388\n",
      "Epoch [2/5], Step [45300/68337], Loss: 5.1327\n",
      "Epoch [2/5], Step [45375/68337], Loss: 4.9246\n",
      "Epoch [2/5], Step [45450/68337], Loss: 5.1151\n",
      "Epoch [2/5], Step [45525/68337], Loss: 5.0401\n",
      "Epoch [2/5], Step [45600/68337], Loss: 5.1164\n",
      "Epoch [2/5], Step [45675/68337], Loss: 5.0036\n",
      "Epoch [2/5], Step [45750/68337], Loss: 4.9880\n",
      "Epoch [2/5], Step [45825/68337], Loss: 4.8471\n",
      "Epoch [2/5], Step [45900/68337], Loss: 5.1033\n",
      "Epoch [2/5], Step [45975/68337], Loss: 5.0536\n",
      "Epoch [2/5], Step [46050/68337], Loss: 5.0599\n",
      "Epoch [2/5], Step [46125/68337], Loss: 4.9102\n",
      "Epoch [2/5], Step [46200/68337], Loss: 4.9208\n",
      "Epoch [2/5], Step [46275/68337], Loss: 5.0541\n",
      "Epoch [2/5], Step [46350/68337], Loss: 5.1365\n",
      "Epoch [2/5], Step [46425/68337], Loss: 5.0184\n",
      "Epoch [2/5], Step [46500/68337], Loss: 5.0791\n",
      "Epoch [2/5], Step [46575/68337], Loss: 5.1445\n",
      "Epoch [2/5], Step [46650/68337], Loss: 5.0101\n",
      "Epoch [2/5], Step [46725/68337], Loss: 5.0313\n",
      "Epoch [2/5], Step [46800/68337], Loss: 5.1498\n",
      "Epoch [2/5], Step [46875/68337], Loss: 5.0465\n",
      "Epoch [2/5], Step [46950/68337], Loss: 5.0467\n",
      "Epoch [2/5], Step [47025/68337], Loss: 5.2287\n",
      "Epoch [2/5], Step [47100/68337], Loss: 5.0506\n",
      "Epoch [2/5], Step [47175/68337], Loss: 4.9600\n",
      "Epoch [2/5], Step [47250/68337], Loss: 5.2229\n",
      "Epoch [2/5], Step [47325/68337], Loss: 5.1616\n",
      "Epoch [2/5], Step [47400/68337], Loss: 5.0631\n",
      "Epoch [2/5], Step [47475/68337], Loss: 5.0633\n",
      "Epoch [2/5], Step [47550/68337], Loss: 5.0594\n",
      "Epoch [2/5], Step [47625/68337], Loss: 5.1001\n",
      "Epoch [2/5], Step [47700/68337], Loss: 4.9204\n",
      "Epoch [2/5], Step [47775/68337], Loss: 5.0911\n",
      "Epoch [2/5], Step [47850/68337], Loss: 5.0270\n",
      "Epoch [2/5], Step [47925/68337], Loss: 4.9976\n",
      "Epoch [2/5], Step [48000/68337], Loss: 5.0625\n",
      "Epoch [2/5], Step [48075/68337], Loss: 4.9600\n",
      "Epoch [2/5], Step [48150/68337], Loss: 5.1766\n",
      "Epoch [2/5], Step [48225/68337], Loss: 5.1569\n",
      "Epoch [2/5], Step [48300/68337], Loss: 4.9493\n",
      "Epoch [2/5], Step [48375/68337], Loss: 5.1169\n",
      "Epoch [2/5], Step [48450/68337], Loss: 5.2754\n",
      "Epoch [2/5], Step [48525/68337], Loss: 5.1880\n",
      "Epoch [2/5], Step [48600/68337], Loss: 5.1253\n",
      "Epoch [2/5], Step [48675/68337], Loss: 5.0709\n",
      "Epoch [2/5], Step [48750/68337], Loss: 5.0669\n",
      "Epoch [2/5], Step [48825/68337], Loss: 5.0227\n",
      "Epoch [2/5], Step [48900/68337], Loss: 5.2150\n",
      "Epoch [2/5], Step [48975/68337], Loss: 5.0487\n",
      "Epoch [2/5], Step [49050/68337], Loss: 5.0572\n",
      "Epoch [2/5], Step [49125/68337], Loss: 5.0541\n",
      "Epoch [2/5], Step [49200/68337], Loss: 4.9459\n",
      "Epoch [2/5], Step [49275/68337], Loss: 5.1953\n",
      "Epoch [2/5], Step [49350/68337], Loss: 5.0108\n",
      "Epoch [2/5], Step [49425/68337], Loss: 5.2492\n",
      "Epoch [2/5], Step [49500/68337], Loss: 5.0264\n",
      "Epoch [2/5], Step [49575/68337], Loss: 5.0663\n",
      "Epoch [2/5], Step [49650/68337], Loss: 5.1294\n",
      "Epoch [2/5], Step [49725/68337], Loss: 4.9714\n",
      "Epoch [2/5], Step [49800/68337], Loss: 4.9739\n",
      "Epoch [2/5], Step [49875/68337], Loss: 5.0602\n",
      "Epoch [2/5], Step [49950/68337], Loss: 5.0688\n",
      "Validation perplexity: 153.20121919687898\n",
      "Epoch [2/5], Step [50025/68337], Loss: 5.0542\n",
      "Epoch [2/5], Step [50100/68337], Loss: 5.1714\n",
      "Epoch [2/5], Step [50175/68337], Loss: 5.0481\n",
      "Epoch [2/5], Step [50250/68337], Loss: 5.0193\n",
      "Epoch [2/5], Step [50325/68337], Loss: 5.1131\n",
      "Epoch [2/5], Step [50400/68337], Loss: 5.0365\n",
      "Epoch [2/5], Step [50475/68337], Loss: 5.2102\n",
      "Epoch [2/5], Step [50550/68337], Loss: 5.1726\n",
      "Epoch [2/5], Step [50625/68337], Loss: 5.1209\n",
      "Epoch [2/5], Step [50700/68337], Loss: 5.2176\n",
      "Epoch [2/5], Step [50775/68337], Loss: 5.1244\n",
      "Epoch [2/5], Step [50850/68337], Loss: 5.2283\n",
      "Epoch [2/5], Step [50925/68337], Loss: 5.0343\n",
      "Epoch [2/5], Step [51000/68337], Loss: 5.0678\n",
      "Epoch [2/5], Step [51075/68337], Loss: 4.9811\n",
      "Epoch [2/5], Step [51150/68337], Loss: 5.0852\n",
      "Epoch [2/5], Step [51225/68337], Loss: 5.0647\n",
      "Epoch [2/5], Step [51300/68337], Loss: 5.1798\n",
      "Epoch [2/5], Step [51375/68337], Loss: 5.1933\n",
      "Epoch [2/5], Step [51450/68337], Loss: 5.2004\n",
      "Epoch [2/5], Step [51525/68337], Loss: 5.1633\n",
      "Epoch [2/5], Step [51600/68337], Loss: 4.9840\n",
      "Epoch [2/5], Step [51675/68337], Loss: 5.2933\n",
      "Epoch [2/5], Step [51750/68337], Loss: 5.0835\n",
      "Epoch [2/5], Step [51825/68337], Loss: 5.1509\n",
      "Epoch [2/5], Step [51900/68337], Loss: 5.0981\n",
      "Epoch [2/5], Step [51975/68337], Loss: 5.1594\n",
      "Epoch [2/5], Step [52050/68337], Loss: 5.1350\n",
      "Epoch [2/5], Step [52125/68337], Loss: 5.1746\n",
      "Epoch [2/5], Step [52200/68337], Loss: 5.0748\n",
      "Epoch [2/5], Step [52275/68337], Loss: 5.3321\n",
      "Epoch [2/5], Step [52350/68337], Loss: 5.1862\n",
      "Epoch [2/5], Step [52425/68337], Loss: 4.9964\n",
      "Epoch [2/5], Step [52500/68337], Loss: 5.0434\n",
      "Epoch [2/5], Step [52575/68337], Loss: 5.0572\n",
      "Epoch [2/5], Step [52650/68337], Loss: 5.1322\n",
      "Epoch [2/5], Step [52725/68337], Loss: 5.0369\n",
      "Epoch [2/5], Step [52800/68337], Loss: 5.1709\n",
      "Epoch [2/5], Step [52875/68337], Loss: 5.2721\n",
      "Epoch [2/5], Step [52950/68337], Loss: 4.9562\n",
      "Epoch [2/5], Step [53025/68337], Loss: 5.1636\n",
      "Epoch [2/5], Step [53100/68337], Loss: 5.0109\n",
      "Epoch [2/5], Step [53175/68337], Loss: 5.1680\n",
      "Epoch [2/5], Step [53250/68337], Loss: 5.1023\n",
      "Epoch [2/5], Step [53325/68337], Loss: 5.1016\n",
      "Epoch [2/5], Step [53400/68337], Loss: 5.1002\n",
      "Epoch [2/5], Step [53475/68337], Loss: 5.1707\n",
      "Epoch [2/5], Step [53550/68337], Loss: 5.0949\n",
      "Epoch [2/5], Step [53625/68337], Loss: 5.0639\n",
      "Epoch [2/5], Step [53700/68337], Loss: 4.9029\n",
      "Epoch [2/5], Step [53775/68337], Loss: 4.9938\n",
      "Epoch [2/5], Step [53850/68337], Loss: 4.9860\n",
      "Epoch [2/5], Step [53925/68337], Loss: 5.2548\n",
      "Epoch [2/5], Step [54000/68337], Loss: 4.9762\n",
      "Epoch [2/5], Step [54075/68337], Loss: 5.0726\n",
      "Epoch [2/5], Step [54150/68337], Loss: 5.1222\n",
      "Epoch [2/5], Step [54225/68337], Loss: 5.1257\n",
      "Epoch [2/5], Step [54300/68337], Loss: 5.0844\n",
      "Epoch [2/5], Step [54375/68337], Loss: 5.0732\n",
      "Epoch [2/5], Step [54450/68337], Loss: 5.0899\n",
      "Epoch [2/5], Step [54525/68337], Loss: 5.0470\n",
      "Epoch [2/5], Step [54600/68337], Loss: 5.0437\n",
      "Epoch [2/5], Step [54675/68337], Loss: 5.0970\n",
      "Epoch [2/5], Step [54750/68337], Loss: 4.8911\n",
      "Epoch [2/5], Step [54825/68337], Loss: 5.1032\n",
      "Epoch [2/5], Step [54900/68337], Loss: 4.9355\n",
      "Epoch [2/5], Step [54975/68337], Loss: 5.1522\n",
      "Epoch [2/5], Step [55050/68337], Loss: 4.9246\n",
      "Epoch [2/5], Step [55125/68337], Loss: 5.0310\n",
      "Epoch [2/5], Step [55200/68337], Loss: 5.1103\n",
      "Epoch [2/5], Step [55275/68337], Loss: 5.1844\n",
      "Epoch [2/5], Step [55350/68337], Loss: 5.0711\n",
      "Epoch [2/5], Step [55425/68337], Loss: 5.2930\n",
      "Epoch [2/5], Step [55500/68337], Loss: 5.1196\n",
      "Epoch [2/5], Step [55575/68337], Loss: 5.1224\n",
      "Epoch [2/5], Step [55650/68337], Loss: 4.9114\n",
      "Epoch [2/5], Step [55725/68337], Loss: 5.1755\n",
      "Epoch [2/5], Step [55800/68337], Loss: 4.9859\n",
      "Epoch [2/5], Step [55875/68337], Loss: 5.2015\n",
      "Epoch [2/5], Step [55950/68337], Loss: 5.0475\n",
      "Epoch [2/5], Step [56025/68337], Loss: 5.1868\n",
      "Epoch [2/5], Step [56100/68337], Loss: 5.1768\n",
      "Epoch [2/5], Step [56175/68337], Loss: 5.2198\n",
      "Epoch [2/5], Step [56250/68337], Loss: 5.1528\n",
      "Epoch [2/5], Step [56325/68337], Loss: 4.9811\n",
      "Epoch [2/5], Step [56400/68337], Loss: 5.2181\n",
      "Epoch [2/5], Step [56475/68337], Loss: 5.0959\n",
      "Epoch [2/5], Step [56550/68337], Loss: 5.0088\n",
      "Epoch [2/5], Step [56625/68337], Loss: 5.0333\n",
      "Epoch [2/5], Step [56700/68337], Loss: 5.1423\n",
      "Epoch [2/5], Step [56775/68337], Loss: 5.2779\n",
      "Epoch [2/5], Step [56850/68337], Loss: 5.0059\n",
      "Epoch [2/5], Step [56925/68337], Loss: 5.0635\n",
      "Epoch [2/5], Step [57000/68337], Loss: 5.2199\n",
      "Epoch [2/5], Step [57075/68337], Loss: 5.0699\n",
      "Epoch [2/5], Step [57150/68337], Loss: 4.9369\n",
      "Epoch [2/5], Step [57225/68337], Loss: 5.1638\n",
      "Epoch [2/5], Step [57300/68337], Loss: 5.1995\n",
      "Epoch [2/5], Step [57375/68337], Loss: 5.1304\n",
      "Epoch [2/5], Step [57450/68337], Loss: 5.0531\n",
      "Epoch [2/5], Step [57525/68337], Loss: 4.9605\n",
      "Epoch [2/5], Step [57600/68337], Loss: 5.2367\n",
      "Epoch [2/5], Step [57675/68337], Loss: 4.8631\n",
      "Epoch [2/5], Step [57750/68337], Loss: 5.0570\n",
      "Epoch [2/5], Step [57825/68337], Loss: 5.0116\n",
      "Epoch [2/5], Step [57900/68337], Loss: 5.1560\n",
      "Epoch [2/5], Step [57975/68337], Loss: 5.1086\n",
      "Epoch [2/5], Step [58050/68337], Loss: 4.9125\n",
      "Epoch [2/5], Step [58125/68337], Loss: 5.1222\n",
      "Epoch [2/5], Step [58200/68337], Loss: 5.1215\n",
      "Epoch [2/5], Step [58275/68337], Loss: 5.0584\n",
      "Epoch [2/5], Step [58350/68337], Loss: 5.1386\n",
      "Epoch [2/5], Step [58425/68337], Loss: 5.1294\n",
      "Epoch [2/5], Step [58500/68337], Loss: 4.9872\n",
      "Epoch [2/5], Step [58575/68337], Loss: 5.1383\n",
      "Epoch [2/5], Step [58650/68337], Loss: 5.0020\n",
      "Epoch [2/5], Step [58725/68337], Loss: 5.1864\n",
      "Epoch [2/5], Step [58800/68337], Loss: 5.0781\n",
      "Epoch [2/5], Step [58875/68337], Loss: 5.1880\n",
      "Epoch [2/5], Step [58950/68337], Loss: 5.0906\n",
      "Epoch [2/5], Step [59025/68337], Loss: 5.1356\n",
      "Epoch [2/5], Step [59100/68337], Loss: 5.0350\n",
      "Epoch [2/5], Step [59175/68337], Loss: 5.1915\n",
      "Epoch [2/5], Step [59250/68337], Loss: 5.0165\n",
      "Epoch [2/5], Step [59325/68337], Loss: 4.9464\n",
      "Epoch [2/5], Step [59400/68337], Loss: 4.9627\n",
      "Epoch [2/5], Step [59475/68337], Loss: 4.9742\n",
      "Epoch [2/5], Step [59550/68337], Loss: 5.1384\n",
      "Epoch [2/5], Step [59625/68337], Loss: 5.2598\n",
      "Epoch [2/5], Step [59700/68337], Loss: 5.2497\n",
      "Epoch [2/5], Step [59775/68337], Loss: 5.0857\n",
      "Epoch [2/5], Step [59850/68337], Loss: 5.0354\n",
      "Epoch [2/5], Step [59925/68337], Loss: 5.0178\n",
      "Epoch [2/5], Step [60000/68337], Loss: 5.1662\n",
      "Validation perplexity: 152.3034176333173\n",
      "Epoch [2/5], Step [60075/68337], Loss: 4.9778\n",
      "Epoch [2/5], Step [60150/68337], Loss: 4.9991\n",
      "Epoch [2/5], Step [60225/68337], Loss: 5.1761\n",
      "Epoch [2/5], Step [60300/68337], Loss: 5.2408\n",
      "Epoch [2/5], Step [60375/68337], Loss: 4.9065\n",
      "Epoch [2/5], Step [60450/68337], Loss: 5.0980\n",
      "Epoch [2/5], Step [60525/68337], Loss: 5.0987\n",
      "Epoch [2/5], Step [60600/68337], Loss: 4.9173\n",
      "Epoch [2/5], Step [60675/68337], Loss: 5.0153\n",
      "Epoch [2/5], Step [60750/68337], Loss: 4.9738\n",
      "Epoch [2/5], Step [60825/68337], Loss: 5.1411\n",
      "Epoch [2/5], Step [60900/68337], Loss: 4.9566\n",
      "Epoch [2/5], Step [60975/68337], Loss: 4.9923\n",
      "Epoch [2/5], Step [61050/68337], Loss: 5.2624\n",
      "Epoch [2/5], Step [61125/68337], Loss: 5.0998\n",
      "Epoch [2/5], Step [61200/68337], Loss: 5.1997\n",
      "Epoch [2/5], Step [61275/68337], Loss: 5.0418\n",
      "Epoch [2/5], Step [61350/68337], Loss: 5.1342\n",
      "Epoch [2/5], Step [61425/68337], Loss: 4.8953\n",
      "Epoch [2/5], Step [61500/68337], Loss: 5.0236\n",
      "Epoch [2/5], Step [61575/68337], Loss: 5.1318\n",
      "Epoch [2/5], Step [61650/68337], Loss: 5.1169\n",
      "Epoch [2/5], Step [61725/68337], Loss: 4.9075\n",
      "Epoch [2/5], Step [61800/68337], Loss: 5.0445\n",
      "Epoch [2/5], Step [61875/68337], Loss: 5.0046\n",
      "Epoch [2/5], Step [61950/68337], Loss: 4.9036\n",
      "Epoch [2/5], Step [62025/68337], Loss: 5.0094\n",
      "Epoch [2/5], Step [62100/68337], Loss: 4.9890\n",
      "Epoch [2/5], Step [62175/68337], Loss: 5.1799\n",
      "Epoch [2/5], Step [62250/68337], Loss: 5.0921\n",
      "Epoch [2/5], Step [62325/68337], Loss: 5.1483\n",
      "Epoch [2/5], Step [62400/68337], Loss: 4.9913\n",
      "Epoch [2/5], Step [62475/68337], Loss: 5.0569\n",
      "Epoch [2/5], Step [62550/68337], Loss: 4.9996\n",
      "Epoch [2/5], Step [62625/68337], Loss: 5.1202\n",
      "Epoch [2/5], Step [62700/68337], Loss: 4.9762\n",
      "Epoch [2/5], Step [62775/68337], Loss: 5.1632\n",
      "Epoch [2/5], Step [62850/68337], Loss: 5.0736\n",
      "Epoch [2/5], Step [62925/68337], Loss: 5.0686\n",
      "Epoch [2/5], Step [63000/68337], Loss: 5.1847\n",
      "Epoch [2/5], Step [63075/68337], Loss: 5.0190\n",
      "Epoch [2/5], Step [63150/68337], Loss: 5.1519\n",
      "Epoch [2/5], Step [63225/68337], Loss: 4.9980\n",
      "Epoch [2/5], Step [63300/68337], Loss: 5.0897\n",
      "Epoch [2/5], Step [63375/68337], Loss: 5.0050\n",
      "Epoch [2/5], Step [63450/68337], Loss: 5.1237\n",
      "Epoch [2/5], Step [63525/68337], Loss: 5.0372\n",
      "Epoch [2/5], Step [63600/68337], Loss: 5.1304\n",
      "Epoch [2/5], Step [63675/68337], Loss: 5.0648\n",
      "Epoch [2/5], Step [63750/68337], Loss: 5.0203\n",
      "Epoch [2/5], Step [63825/68337], Loss: 4.8712\n",
      "Epoch [2/5], Step [63900/68337], Loss: 5.0668\n",
      "Epoch [2/5], Step [63975/68337], Loss: 5.2001\n",
      "Epoch [2/5], Step [64050/68337], Loss: 5.1320\n",
      "Epoch [2/5], Step [64125/68337], Loss: 5.1561\n",
      "Epoch [2/5], Step [64200/68337], Loss: 5.1908\n",
      "Epoch [2/5], Step [64275/68337], Loss: 5.0959\n",
      "Epoch [2/5], Step [64350/68337], Loss: 5.2369\n",
      "Epoch [2/5], Step [64425/68337], Loss: 4.9261\n",
      "Epoch [2/5], Step [64500/68337], Loss: 5.1842\n",
      "Epoch [2/5], Step [64575/68337], Loss: 5.1942\n",
      "Epoch [2/5], Step [64650/68337], Loss: 5.0247\n",
      "Epoch [2/5], Step [64725/68337], Loss: 5.1592\n",
      "Epoch [2/5], Step [64800/68337], Loss: 5.0666\n",
      "Epoch [2/5], Step [64875/68337], Loss: 5.0179\n",
      "Epoch [2/5], Step [64950/68337], Loss: 5.2385\n",
      "Epoch [2/5], Step [65025/68337], Loss: 5.0274\n",
      "Epoch [2/5], Step [65100/68337], Loss: 5.0115\n",
      "Epoch [2/5], Step [65175/68337], Loss: 5.0680\n",
      "Epoch [2/5], Step [65250/68337], Loss: 5.1004\n",
      "Epoch [2/5], Step [65325/68337], Loss: 5.1915\n",
      "Epoch [2/5], Step [65400/68337], Loss: 5.0876\n",
      "Epoch [2/5], Step [65475/68337], Loss: 4.9237\n",
      "Epoch [2/5], Step [65550/68337], Loss: 5.0549\n",
      "Epoch [2/5], Step [65625/68337], Loss: 5.0818\n",
      "Epoch [2/5], Step [65700/68337], Loss: 5.2078\n",
      "Epoch [2/5], Step [65775/68337], Loss: 4.9437\n",
      "Epoch [2/5], Step [65850/68337], Loss: 4.9813\n",
      "Epoch [2/5], Step [65925/68337], Loss: 4.8457\n",
      "Epoch [2/5], Step [66000/68337], Loss: 5.1403\n",
      "Epoch [2/5], Step [66075/68337], Loss: 5.1835\n",
      "Epoch [2/5], Step [66150/68337], Loss: 5.0075\n",
      "Epoch [2/5], Step [66225/68337], Loss: 5.0551\n",
      "Epoch [2/5], Step [66300/68337], Loss: 5.0392\n",
      "Epoch [2/5], Step [66375/68337], Loss: 5.0670\n",
      "Epoch [2/5], Step [66450/68337], Loss: 5.0995\n",
      "Epoch [2/5], Step [66525/68337], Loss: 5.1052\n",
      "Epoch [2/5], Step [66600/68337], Loss: 5.3364\n",
      "Epoch [2/5], Step [66675/68337], Loss: 5.1484\n",
      "Epoch [2/5], Step [66750/68337], Loss: 5.0976\n",
      "Epoch [2/5], Step [66825/68337], Loss: 5.2437\n",
      "Epoch [2/5], Step [66900/68337], Loss: 5.0908\n",
      "Epoch [2/5], Step [66975/68337], Loss: 5.0113\n",
      "Epoch [2/5], Step [67050/68337], Loss: 5.1455\n",
      "Epoch [2/5], Step [67125/68337], Loss: 5.2363\n",
      "Epoch [2/5], Step [67200/68337], Loss: 5.1085\n",
      "Epoch [2/5], Step [67275/68337], Loss: 5.1694\n",
      "Epoch [2/5], Step [67350/68337], Loss: 5.0504\n",
      "Epoch [2/5], Step [67425/68337], Loss: 5.1413\n",
      "Epoch [2/5], Step [67500/68337], Loss: 5.1543\n",
      "Epoch [2/5], Step [67575/68337], Loss: 4.9498\n",
      "Epoch [2/5], Step [67650/68337], Loss: 5.0686\n",
      "Epoch [2/5], Step [67725/68337], Loss: 5.0055\n",
      "Epoch [2/5], Step [67800/68337], Loss: 5.0883\n",
      "Epoch [2/5], Step [67875/68337], Loss: 5.0947\n",
      "Epoch [2/5], Step [67950/68337], Loss: 5.1291\n",
      "Epoch [2/5], Step [68025/68337], Loss: 4.9044\n",
      "Epoch [2/5], Step [68100/68337], Loss: 5.0518\n",
      "Epoch [2/5], Step [68175/68337], Loss: 5.1967\n",
      "Epoch [2/5], Step [68250/68337], Loss: 5.1449\n",
      "Epoch [2/5], Step [68325/68337], Loss: 5.1090\n",
      "Epoch [2/5] Average Loss: 5.1043, Perplexity: 164.74\n",
      "Epoch [3/5], Step [0/68337], Loss: 5.0352\n",
      "Validation perplexity: 151.62019147217276\n",
      "Epoch [3/5], Step [75/68337], Loss: 4.8721\n",
      "Epoch [3/5], Step [150/68337], Loss: 5.1424\n",
      "Epoch [3/5], Step [225/68337], Loss: 4.9815\n",
      "Epoch [3/5], Step [300/68337], Loss: 5.1428\n",
      "Epoch [3/5], Step [375/68337], Loss: 5.0988\n",
      "Epoch [3/5], Step [450/68337], Loss: 5.1153\n",
      "Epoch [3/5], Step [525/68337], Loss: 5.1562\n",
      "Epoch [3/5], Step [600/68337], Loss: 5.0817\n",
      "Epoch [3/5], Step [675/68337], Loss: 5.0655\n",
      "Epoch [3/5], Step [750/68337], Loss: 5.0873\n",
      "Epoch [3/5], Step [825/68337], Loss: 5.0803\n",
      "Epoch [3/5], Step [900/68337], Loss: 5.2460\n",
      "Epoch [3/5], Step [975/68337], Loss: 4.8654\n",
      "Epoch [3/5], Step [1050/68337], Loss: 5.1766\n",
      "Epoch [3/5], Step [1125/68337], Loss: 5.2255\n",
      "Epoch [3/5], Step [1200/68337], Loss: 5.1829\n",
      "Epoch [3/5], Step [1275/68337], Loss: 5.0343\n",
      "Epoch [3/5], Step [1350/68337], Loss: 4.9252\n",
      "Epoch [3/5], Step [1425/68337], Loss: 5.1584\n",
      "Epoch [3/5], Step [1500/68337], Loss: 4.9261\n",
      "Epoch [3/5], Step [1575/68337], Loss: 5.1433\n",
      "Epoch [3/5], Step [1650/68337], Loss: 5.0047\n",
      "Epoch [3/5], Step [1725/68337], Loss: 5.1927\n",
      "Epoch [3/5], Step [1800/68337], Loss: 4.9995\n",
      "Epoch [3/5], Step [1875/68337], Loss: 5.1608\n",
      "Epoch [3/5], Step [1950/68337], Loss: 4.9974\n",
      "Epoch [3/5], Step [2025/68337], Loss: 5.1824\n",
      "Epoch [3/5], Step [2100/68337], Loss: 5.1739\n",
      "Epoch [3/5], Step [2175/68337], Loss: 4.8819\n",
      "Epoch [3/5], Step [2250/68337], Loss: 5.1067\n",
      "Epoch [3/5], Step [2325/68337], Loss: 5.2506\n",
      "Epoch [3/5], Step [2400/68337], Loss: 4.9078\n",
      "Epoch [3/5], Step [2475/68337], Loss: 5.0705\n",
      "Epoch [3/5], Step [2550/68337], Loss: 4.9024\n",
      "Epoch [3/5], Step [2625/68337], Loss: 5.1860\n",
      "Epoch [3/5], Step [2700/68337], Loss: 5.1131\n",
      "Epoch [3/5], Step [2775/68337], Loss: 5.0156\n",
      "Epoch [3/5], Step [2850/68337], Loss: 4.9460\n",
      "Epoch [3/5], Step [2925/68337], Loss: 5.0303\n",
      "Epoch [3/5], Step [3000/68337], Loss: 5.0344\n",
      "Epoch [3/5], Step [3075/68337], Loss: 5.1457\n",
      "Epoch [3/5], Step [3150/68337], Loss: 4.9015\n",
      "Epoch [3/5], Step [3225/68337], Loss: 5.0387\n",
      "Epoch [3/5], Step [3300/68337], Loss: 5.2205\n",
      "Epoch [3/5], Step [3375/68337], Loss: 5.0131\n",
      "Epoch [3/5], Step [3450/68337], Loss: 5.0344\n",
      "Epoch [3/5], Step [3525/68337], Loss: 5.1015\n",
      "Epoch [3/5], Step [3600/68337], Loss: 5.3251\n",
      "Epoch [3/5], Step [3675/68337], Loss: 5.0428\n",
      "Epoch [3/5], Step [3750/68337], Loss: 5.2010\n",
      "Epoch [3/5], Step [3825/68337], Loss: 4.9874\n",
      "Epoch [3/5], Step [3900/68337], Loss: 4.9744\n",
      "Epoch [3/5], Step [3975/68337], Loss: 4.9908\n",
      "Epoch [3/5], Step [4050/68337], Loss: 5.0607\n",
      "Epoch [3/5], Step [4125/68337], Loss: 5.0587\n",
      "Epoch [3/5], Step [4200/68337], Loss: 5.0156\n",
      "Epoch [3/5], Step [4275/68337], Loss: 4.9433\n",
      "Epoch [3/5], Step [4350/68337], Loss: 5.1504\n",
      "Epoch [3/5], Step [4425/68337], Loss: 4.9643\n",
      "Epoch [3/5], Step [4500/68337], Loss: 5.1804\n",
      "Epoch [3/5], Step [4575/68337], Loss: 5.0630\n",
      "Epoch [3/5], Step [4650/68337], Loss: 5.2186\n",
      "Epoch [3/5], Step [4725/68337], Loss: 5.2610\n",
      "Epoch [3/5], Step [4800/68337], Loss: 4.8838\n",
      "Epoch [3/5], Step [4875/68337], Loss: 5.0758\n",
      "Epoch [3/5], Step [4950/68337], Loss: 5.0400\n",
      "Epoch [3/5], Step [5025/68337], Loss: 5.0933\n",
      "Epoch [3/5], Step [5100/68337], Loss: 5.1218\n",
      "Epoch [3/5], Step [5175/68337], Loss: 5.0544\n",
      "Epoch [3/5], Step [5250/68337], Loss: 5.1806\n",
      "Epoch [3/5], Step [5325/68337], Loss: 5.1002\n",
      "Epoch [3/5], Step [5400/68337], Loss: 5.1622\n",
      "Epoch [3/5], Step [5475/68337], Loss: 5.1399\n",
      "Epoch [3/5], Step [5550/68337], Loss: 5.0532\n",
      "Epoch [3/5], Step [5625/68337], Loss: 5.1103\n",
      "Epoch [3/5], Step [5700/68337], Loss: 5.1745\n",
      "Epoch [3/5], Step [5775/68337], Loss: 5.0128\n",
      "Epoch [3/5], Step [5850/68337], Loss: 4.9339\n",
      "Epoch [3/5], Step [5925/68337], Loss: 5.1315\n",
      "Epoch [3/5], Step [6000/68337], Loss: 5.1925\n",
      "Epoch [3/5], Step [6075/68337], Loss: 4.9574\n",
      "Epoch [3/5], Step [6150/68337], Loss: 4.8185\n",
      "Epoch [3/5], Step [6225/68337], Loss: 5.0324\n",
      "Epoch [3/5], Step [6300/68337], Loss: 4.9132\n",
      "Epoch [3/5], Step [6375/68337], Loss: 5.1830\n",
      "Epoch [3/5], Step [6450/68337], Loss: 5.1181\n",
      "Epoch [3/5], Step [6525/68337], Loss: 4.9685\n",
      "Epoch [3/5], Step [6600/68337], Loss: 5.0490\n",
      "Epoch [3/5], Step [6675/68337], Loss: 4.9271\n",
      "Epoch [3/5], Step [6750/68337], Loss: 5.0821\n",
      "Epoch [3/5], Step [6825/68337], Loss: 4.9860\n",
      "Epoch [3/5], Step [6900/68337], Loss: 4.9936\n",
      "Epoch [3/5], Step [6975/68337], Loss: 5.0996\n",
      "Epoch [3/5], Step [7050/68337], Loss: 4.9376\n",
      "Epoch [3/5], Step [7125/68337], Loss: 5.1531\n",
      "Epoch [3/5], Step [7200/68337], Loss: 5.0840\n",
      "Epoch [3/5], Step [7275/68337], Loss: 4.9615\n",
      "Epoch [3/5], Step [7350/68337], Loss: 5.0022\n",
      "Epoch [3/5], Step [7425/68337], Loss: 5.0929\n",
      "Epoch [3/5], Step [7500/68337], Loss: 4.9678\n",
      "Epoch [3/5], Step [7575/68337], Loss: 4.8796\n",
      "Epoch [3/5], Step [7650/68337], Loss: 5.1056\n",
      "Epoch [3/5], Step [7725/68337], Loss: 5.0288\n",
      "Epoch [3/5], Step [7800/68337], Loss: 5.0146\n",
      "Epoch [3/5], Step [7875/68337], Loss: 5.1029\n",
      "Epoch [3/5], Step [7950/68337], Loss: 5.0320\n",
      "Epoch [3/5], Step [8025/68337], Loss: 5.1883\n",
      "Epoch [3/5], Step [8100/68337], Loss: 5.1559\n",
      "Epoch [3/5], Step [8175/68337], Loss: 4.9918\n",
      "Epoch [3/5], Step [8250/68337], Loss: 5.0008\n",
      "Epoch [3/5], Step [8325/68337], Loss: 5.0202\n",
      "Epoch [3/5], Step [8400/68337], Loss: 5.2405\n",
      "Epoch [3/5], Step [8475/68337], Loss: 5.1279\n",
      "Epoch [3/5], Step [8550/68337], Loss: 4.9522\n",
      "Epoch [3/5], Step [8625/68337], Loss: 5.0960\n",
      "Epoch [3/5], Step [8700/68337], Loss: 5.1728\n",
      "Epoch [3/5], Step [8775/68337], Loss: 5.1197\n",
      "Epoch [3/5], Step [8850/68337], Loss: 5.0789\n",
      "Epoch [3/5], Step [8925/68337], Loss: 5.1249\n",
      "Epoch [3/5], Step [9000/68337], Loss: 4.9224\n",
      "Epoch [3/5], Step [9075/68337], Loss: 4.9693\n",
      "Epoch [3/5], Step [9150/68337], Loss: 5.0382\n",
      "Epoch [3/5], Step [9225/68337], Loss: 5.2640\n",
      "Epoch [3/5], Step [9300/68337], Loss: 5.1998\n",
      "Epoch [3/5], Step [9375/68337], Loss: 4.9998\n",
      "Epoch [3/5], Step [9450/68337], Loss: 5.1197\n",
      "Epoch [3/5], Step [9525/68337], Loss: 5.0508\n",
      "Epoch [3/5], Step [9600/68337], Loss: 5.0300\n",
      "Epoch [3/5], Step [9675/68337], Loss: 4.9562\n",
      "Epoch [3/5], Step [9750/68337], Loss: 5.0568\n",
      "Epoch [3/5], Step [9825/68337], Loss: 5.0033\n",
      "Epoch [3/5], Step [9900/68337], Loss: 4.9797\n",
      "Epoch [3/5], Step [9975/68337], Loss: 5.0826\n",
      "Validation perplexity: 151.54394907550318\n",
      "Epoch [3/5], Step [10050/68337], Loss: 4.9783\n",
      "Epoch [3/5], Step [10125/68337], Loss: 5.1590\n",
      "Epoch [3/5], Step [10200/68337], Loss: 5.0867\n",
      "Epoch [3/5], Step [10275/68337], Loss: 5.2098\n",
      "Epoch [3/5], Step [10350/68337], Loss: 4.9683\n",
      "Epoch [3/5], Step [10425/68337], Loss: 4.9919\n",
      "Epoch [3/5], Step [10500/68337], Loss: 4.9059\n",
      "Epoch [3/5], Step [10575/68337], Loss: 5.2541\n",
      "Epoch [3/5], Step [10650/68337], Loss: 5.0756\n",
      "Epoch [3/5], Step [10725/68337], Loss: 5.0660\n",
      "Epoch [3/5], Step [10800/68337], Loss: 5.1311\n",
      "Epoch [3/5], Step [10875/68337], Loss: 5.0377\n",
      "Epoch [3/5], Step [10950/68337], Loss: 4.9972\n",
      "Epoch [3/5], Step [11025/68337], Loss: 5.0915\n",
      "Epoch [3/5], Step [11100/68337], Loss: 5.0233\n",
      "Epoch [3/5], Step [11175/68337], Loss: 5.1161\n",
      "Epoch [3/5], Step [11250/68337], Loss: 5.1102\n",
      "Epoch [3/5], Step [11325/68337], Loss: 4.9744\n",
      "Epoch [3/5], Step [11400/68337], Loss: 5.0254\n",
      "Epoch [3/5], Step [11475/68337], Loss: 5.0750\n",
      "Epoch [3/5], Step [11550/68337], Loss: 5.1554\n",
      "Epoch [3/5], Step [11625/68337], Loss: 5.1003\n",
      "Epoch [3/5], Step [11700/68337], Loss: 4.9620\n",
      "Epoch [3/5], Step [11775/68337], Loss: 4.9707\n",
      "Epoch [3/5], Step [11850/68337], Loss: 5.0884\n",
      "Epoch [3/5], Step [11925/68337], Loss: 4.9888\n",
      "Epoch [3/5], Step [12000/68337], Loss: 5.0631\n",
      "Epoch [3/5], Step [12075/68337], Loss: 5.0509\n",
      "Epoch [3/5], Step [12150/68337], Loss: 5.0184\n",
      "Epoch [3/5], Step [12225/68337], Loss: 5.1598\n",
      "Epoch [3/5], Step [12300/68337], Loss: 5.0451\n",
      "Epoch [3/5], Step [12375/68337], Loss: 5.1335\n",
      "Epoch [3/5], Step [12450/68337], Loss: 5.2350\n",
      "Epoch [3/5], Step [12525/68337], Loss: 5.0435\n",
      "Epoch [3/5], Step [12600/68337], Loss: 5.1573\n",
      "Epoch [3/5], Step [12675/68337], Loss: 5.1185\n",
      "Epoch [3/5], Step [12750/68337], Loss: 4.9503\n",
      "Epoch [3/5], Step [12825/68337], Loss: 5.0640\n",
      "Epoch [3/5], Step [12900/68337], Loss: 5.1551\n",
      "Epoch [3/5], Step [12975/68337], Loss: 5.1443\n",
      "Epoch [3/5], Step [13050/68337], Loss: 5.0996\n",
      "Epoch [3/5], Step [13125/68337], Loss: 5.2647\n",
      "Epoch [3/5], Step [13200/68337], Loss: 5.1138\n",
      "Epoch [3/5], Step [13275/68337], Loss: 5.0716\n",
      "Epoch [3/5], Step [13350/68337], Loss: 5.0884\n",
      "Epoch [3/5], Step [13425/68337], Loss: 5.1225\n",
      "Epoch [3/5], Step [13500/68337], Loss: 5.1239\n",
      "Epoch [3/5], Step [13575/68337], Loss: 5.0963\n",
      "Epoch [3/5], Step [13650/68337], Loss: 5.1180\n",
      "Epoch [3/5], Step [13725/68337], Loss: 5.1584\n",
      "Epoch [3/5], Step [13800/68337], Loss: 5.1086\n",
      "Epoch [3/5], Step [13875/68337], Loss: 4.8072\n",
      "Epoch [3/5], Step [13950/68337], Loss: 5.0861\n",
      "Epoch [3/5], Step [14025/68337], Loss: 5.2054\n",
      "Epoch [3/5], Step [14100/68337], Loss: 5.2105\n",
      "Epoch [3/5], Step [14175/68337], Loss: 5.1291\n",
      "Epoch [3/5], Step [14250/68337], Loss: 5.0762\n",
      "Epoch [3/5], Step [14325/68337], Loss: 5.0325\n",
      "Epoch [3/5], Step [14400/68337], Loss: 5.0442\n",
      "Epoch [3/5], Step [14475/68337], Loss: 5.1462\n",
      "Epoch [3/5], Step [14550/68337], Loss: 5.0950\n",
      "Epoch [3/5], Step [14625/68337], Loss: 4.9584\n",
      "Epoch [3/5], Step [14700/68337], Loss: 5.0132\n",
      "Epoch [3/5], Step [14775/68337], Loss: 5.1888\n",
      "Epoch [3/5], Step [14850/68337], Loss: 5.0225\n",
      "Epoch [3/5], Step [14925/68337], Loss: 5.0507\n",
      "Epoch [3/5], Step [15000/68337], Loss: 5.0031\n",
      "Epoch [3/5], Step [15075/68337], Loss: 5.1207\n",
      "Epoch [3/5], Step [15150/68337], Loss: 4.9627\n",
      "Epoch [3/5], Step [15225/68337], Loss: 5.0037\n",
      "Epoch [3/5], Step [15300/68337], Loss: 5.1656\n",
      "Epoch [3/5], Step [15375/68337], Loss: 5.0573\n",
      "Epoch [3/5], Step [15450/68337], Loss: 5.0612\n",
      "Epoch [3/5], Step [15525/68337], Loss: 5.0373\n",
      "Epoch [3/5], Step [15600/68337], Loss: 5.1810\n",
      "Epoch [3/5], Step [15675/68337], Loss: 5.0734\n",
      "Epoch [3/5], Step [15750/68337], Loss: 5.1252\n",
      "Epoch [3/5], Step [15825/68337], Loss: 5.0061\n",
      "Epoch [3/5], Step [15900/68337], Loss: 5.1624\n",
      "Epoch [3/5], Step [15975/68337], Loss: 5.1990\n",
      "Epoch [3/5], Step [16050/68337], Loss: 4.8378\n",
      "Epoch [3/5], Step [16125/68337], Loss: 5.0060\n",
      "Epoch [3/5], Step [16200/68337], Loss: 5.0906\n",
      "Epoch [3/5], Step [16275/68337], Loss: 5.0674\n",
      "Epoch [3/5], Step [16350/68337], Loss: 4.9500\n",
      "Epoch [3/5], Step [16425/68337], Loss: 5.2256\n",
      "Epoch [3/5], Step [16500/68337], Loss: 5.1519\n",
      "Epoch [3/5], Step [16575/68337], Loss: 5.1600\n",
      "Epoch [3/5], Step [16650/68337], Loss: 4.9396\n",
      "Epoch [3/5], Step [16725/68337], Loss: 5.0303\n",
      "Epoch [3/5], Step [16800/68337], Loss: 5.2108\n",
      "Epoch [3/5], Step [16875/68337], Loss: 4.9922\n",
      "Epoch [3/5], Step [16950/68337], Loss: 5.0276\n",
      "Epoch [3/5], Step [17025/68337], Loss: 5.1424\n",
      "Epoch [3/5], Step [17100/68337], Loss: 4.9426\n",
      "Epoch [3/5], Step [17175/68337], Loss: 4.9382\n",
      "Epoch [3/5], Step [17250/68337], Loss: 4.9729\n",
      "Epoch [3/5], Step [17325/68337], Loss: 5.1479\n",
      "Epoch [3/5], Step [17400/68337], Loss: 5.1975\n",
      "Epoch [3/5], Step [17475/68337], Loss: 5.1279\n",
      "Epoch [3/5], Step [17550/68337], Loss: 5.0141\n",
      "Epoch [3/5], Step [17625/68337], Loss: 5.1919\n",
      "Epoch [3/5], Step [17700/68337], Loss: 5.0374\n",
      "Epoch [3/5], Step [17775/68337], Loss: 4.8694\n",
      "Epoch [3/5], Step [17850/68337], Loss: 5.0432\n",
      "Epoch [3/5], Step [17925/68337], Loss: 4.9928\n",
      "Epoch [3/5], Step [18000/68337], Loss: 5.2269\n",
      "Epoch [3/5], Step [18075/68337], Loss: 5.1075\n",
      "Epoch [3/5], Step [18150/68337], Loss: 5.0695\n",
      "Epoch [3/5], Step [18225/68337], Loss: 4.9778\n",
      "Epoch [3/5], Step [18300/68337], Loss: 5.0556\n",
      "Epoch [3/5], Step [18375/68337], Loss: 4.9872\n",
      "Epoch [3/5], Step [18450/68337], Loss: 5.2143\n",
      "Epoch [3/5], Step [18525/68337], Loss: 5.1530\n",
      "Epoch [3/5], Step [18600/68337], Loss: 5.0637\n",
      "Epoch [3/5], Step [18675/68337], Loss: 4.9963\n",
      "Epoch [3/5], Step [18750/68337], Loss: 4.9495\n",
      "Epoch [3/5], Step [18825/68337], Loss: 5.2068\n",
      "Epoch [3/5], Step [18900/68337], Loss: 4.9723\n",
      "Epoch [3/5], Step [18975/68337], Loss: 5.0251\n",
      "Epoch [3/5], Step [19050/68337], Loss: 5.1122\n",
      "Epoch [3/5], Step [19125/68337], Loss: 5.0776\n",
      "Epoch [3/5], Step [19200/68337], Loss: 4.8982\n",
      "Epoch [3/5], Step [19275/68337], Loss: 5.1501\n",
      "Epoch [3/5], Step [19350/68337], Loss: 5.1207\n",
      "Epoch [3/5], Step [19425/68337], Loss: 5.1222\n",
      "Epoch [3/5], Step [19500/68337], Loss: 5.0431\n",
      "Epoch [3/5], Step [19575/68337], Loss: 5.1409\n",
      "Epoch [3/5], Step [19650/68337], Loss: 5.0566\n",
      "Epoch [3/5], Step [19725/68337], Loss: 5.1615\n",
      "Epoch [3/5], Step [19800/68337], Loss: 5.1282\n",
      "Epoch [3/5], Step [19875/68337], Loss: 5.0991\n",
      "Epoch [3/5], Step [19950/68337], Loss: 5.1996\n",
      "Validation perplexity: 150.58319493081618\n",
      "Epoch [3/5], Step [20025/68337], Loss: 5.1438\n",
      "Epoch [3/5], Step [20100/68337], Loss: 4.9814\n",
      "Epoch [3/5], Step [20175/68337], Loss: 5.0299\n",
      "Epoch [3/5], Step [20250/68337], Loss: 5.1468\n",
      "Epoch [3/5], Step [20325/68337], Loss: 5.1491\n",
      "Epoch [3/5], Step [20400/68337], Loss: 5.0896\n",
      "Epoch [3/5], Step [20475/68337], Loss: 5.1097\n",
      "Epoch [3/5], Step [20550/68337], Loss: 5.2695\n",
      "Epoch [3/5], Step [20625/68337], Loss: 5.0591\n",
      "Epoch [3/5], Step [20700/68337], Loss: 5.0867\n",
      "Epoch [3/5], Step [20775/68337], Loss: 5.1127\n",
      "Epoch [3/5], Step [20850/68337], Loss: 4.9873\n",
      "Epoch [3/5], Step [20925/68337], Loss: 5.0117\n",
      "Epoch [3/5], Step [21000/68337], Loss: 5.0653\n",
      "Epoch [3/5], Step [21075/68337], Loss: 5.0123\n",
      "Epoch [3/5], Step [21150/68337], Loss: 5.0417\n",
      "Epoch [3/5], Step [21225/68337], Loss: 5.0478\n",
      "Epoch [3/5], Step [21300/68337], Loss: 4.9170\n",
      "Epoch [3/5], Step [21375/68337], Loss: 5.0382\n",
      "Epoch [3/5], Step [21450/68337], Loss: 5.0976\n",
      "Epoch [3/5], Step [21525/68337], Loss: 5.0845\n",
      "Epoch [3/5], Step [21600/68337], Loss: 5.1693\n",
      "Epoch [3/5], Step [21675/68337], Loss: 5.0035\n",
      "Epoch [3/5], Step [21750/68337], Loss: 5.2380\n",
      "Epoch [3/5], Step [21825/68337], Loss: 4.9432\n",
      "Epoch [3/5], Step [21900/68337], Loss: 4.8486\n",
      "Epoch [3/5], Step [21975/68337], Loss: 5.2075\n",
      "Epoch [3/5], Step [22050/68337], Loss: 5.0173\n",
      "Epoch [3/5], Step [22125/68337], Loss: 5.1852\n",
      "Epoch [3/5], Step [22200/68337], Loss: 5.1210\n",
      "Epoch [3/5], Step [22275/68337], Loss: 4.9908\n",
      "Epoch [3/5], Step [22350/68337], Loss: 4.9507\n",
      "Epoch [3/5], Step [22425/68337], Loss: 5.0680\n",
      "Epoch [3/5], Step [22500/68337], Loss: 5.1121\n",
      "Epoch [3/5], Step [22575/68337], Loss: 5.0641\n",
      "Epoch [3/5], Step [22650/68337], Loss: 5.0666\n",
      "Epoch [3/5], Step [22725/68337], Loss: 5.1964\n",
      "Epoch [3/5], Step [22800/68337], Loss: 5.0696\n",
      "Epoch [3/5], Step [22875/68337], Loss: 4.9973\n",
      "Epoch [3/5], Step [22950/68337], Loss: 5.0883\n",
      "Epoch [3/5], Step [23025/68337], Loss: 5.0580\n",
      "Epoch [3/5], Step [23100/68337], Loss: 5.0122\n",
      "Epoch [3/5], Step [23175/68337], Loss: 5.1279\n",
      "Epoch [3/5], Step [23250/68337], Loss: 5.0522\n",
      "Epoch [3/5], Step [23325/68337], Loss: 5.1098\n",
      "Epoch [3/5], Step [23400/68337], Loss: 5.1387\n",
      "Epoch [3/5], Step [23475/68337], Loss: 4.9112\n",
      "Epoch [3/5], Step [23550/68337], Loss: 4.9626\n",
      "Epoch [3/5], Step [23625/68337], Loss: 5.2532\n",
      "Epoch [3/5], Step [23700/68337], Loss: 5.1120\n",
      "Epoch [3/5], Step [23775/68337], Loss: 4.8259\n",
      "Epoch [3/5], Step [23850/68337], Loss: 5.0543\n",
      "Epoch [3/5], Step [23925/68337], Loss: 5.0732\n",
      "Epoch [3/5], Step [24000/68337], Loss: 4.9544\n",
      "Epoch [3/5], Step [24075/68337], Loss: 5.1050\n",
      "Epoch [3/5], Step [24150/68337], Loss: 5.0200\n",
      "Epoch [3/5], Step [24225/68337], Loss: 5.0737\n",
      "Epoch [3/5], Step [24300/68337], Loss: 5.2319\n",
      "Epoch [3/5], Step [24375/68337], Loss: 5.1478\n",
      "Epoch [3/5], Step [24450/68337], Loss: 5.0456\n",
      "Epoch [3/5], Step [24525/68337], Loss: 5.2921\n",
      "Epoch [3/5], Step [24600/68337], Loss: 5.1968\n",
      "Epoch [3/5], Step [24675/68337], Loss: 5.1191\n",
      "Epoch [3/5], Step [24750/68337], Loss: 5.0670\n",
      "Epoch [3/5], Step [24825/68337], Loss: 5.1781\n",
      "Epoch [3/5], Step [24900/68337], Loss: 5.1730\n",
      "Epoch [3/5], Step [24975/68337], Loss: 4.9952\n",
      "Epoch [3/5], Step [25050/68337], Loss: 5.0068\n",
      "Epoch [3/5], Step [25125/68337], Loss: 5.0315\n",
      "Epoch [3/5], Step [25200/68337], Loss: 5.0856\n",
      "Epoch [3/5], Step [25275/68337], Loss: 5.0606\n",
      "Epoch [3/5], Step [25350/68337], Loss: 5.0373\n",
      "Epoch [3/5], Step [25425/68337], Loss: 4.9726\n",
      "Epoch [3/5], Step [25500/68337], Loss: 5.0738\n",
      "Epoch [3/5], Step [25575/68337], Loss: 5.1149\n",
      "Epoch [3/5], Step [25650/68337], Loss: 5.1416\n",
      "Epoch [3/5], Step [25725/68337], Loss: 5.2190\n",
      "Epoch [3/5], Step [25800/68337], Loss: 5.1583\n",
      "Epoch [3/5], Step [25875/68337], Loss: 5.0026\n",
      "Epoch [3/5], Step [25950/68337], Loss: 5.1576\n",
      "Epoch [3/5], Step [26025/68337], Loss: 5.2692\n",
      "Epoch [3/5], Step [26100/68337], Loss: 5.1320\n",
      "Epoch [3/5], Step [26175/68337], Loss: 4.9523\n",
      "Epoch [3/5], Step [26250/68337], Loss: 5.0234\n",
      "Epoch [3/5], Step [26325/68337], Loss: 5.1892\n",
      "Epoch [3/5], Step [26400/68337], Loss: 5.0459\n",
      "Epoch [3/5], Step [26475/68337], Loss: 5.0152\n",
      "Epoch [3/5], Step [26550/68337], Loss: 5.1270\n",
      "Epoch [3/5], Step [26625/68337], Loss: 5.0138\n",
      "Epoch [3/5], Step [26700/68337], Loss: 5.0457\n",
      "Epoch [3/5], Step [26775/68337], Loss: 5.2436\n",
      "Epoch [3/5], Step [26850/68337], Loss: 5.0040\n",
      "Epoch [3/5], Step [26925/68337], Loss: 5.0564\n",
      "Epoch [3/5], Step [27000/68337], Loss: 5.0674\n",
      "Epoch [3/5], Step [27075/68337], Loss: 5.0990\n",
      "Epoch [3/5], Step [27150/68337], Loss: 4.8439\n",
      "Epoch [3/5], Step [27225/68337], Loss: 4.9750\n",
      "Epoch [3/5], Step [27300/68337], Loss: 5.0785\n",
      "Epoch [3/5], Step [27375/68337], Loss: 5.0849\n",
      "Epoch [3/5], Step [27450/68337], Loss: 4.9702\n",
      "Epoch [3/5], Step [27525/68337], Loss: 5.0028\n",
      "Epoch [3/5], Step [27600/68337], Loss: 5.0553\n",
      "Epoch [3/5], Step [27675/68337], Loss: 5.1368\n",
      "Epoch [3/5], Step [27750/68337], Loss: 4.8408\n",
      "Epoch [3/5], Step [27825/68337], Loss: 5.2074\n",
      "Epoch [3/5], Step [27900/68337], Loss: 5.1430\n",
      "Epoch [3/5], Step [27975/68337], Loss: 4.9284\n",
      "Epoch [3/5], Step [28050/68337], Loss: 5.1198\n",
      "Epoch [3/5], Step [28125/68337], Loss: 5.0838\n",
      "Epoch [3/5], Step [28200/68337], Loss: 4.9962\n",
      "Epoch [3/5], Step [28275/68337], Loss: 5.1042\n",
      "Epoch [3/5], Step [28350/68337], Loss: 5.0738\n",
      "Epoch [3/5], Step [28425/68337], Loss: 5.1575\n",
      "Epoch [3/5], Step [28500/68337], Loss: 5.0872\n",
      "Epoch [3/5], Step [28575/68337], Loss: 5.2155\n",
      "Epoch [3/5], Step [28650/68337], Loss: 5.0359\n",
      "Epoch [3/5], Step [28725/68337], Loss: 5.0706\n",
      "Epoch [3/5], Step [28800/68337], Loss: 5.0720\n",
      "Epoch [3/5], Step [28875/68337], Loss: 5.1768\n",
      "Epoch [3/5], Step [28950/68337], Loss: 4.9510\n",
      "Epoch [3/5], Step [29025/68337], Loss: 5.2064\n",
      "Epoch [3/5], Step [29100/68337], Loss: 5.1930\n",
      "Epoch [3/5], Step [29175/68337], Loss: 4.9216\n",
      "Epoch [3/5], Step [29250/68337], Loss: 5.0029\n",
      "Epoch [3/5], Step [29325/68337], Loss: 5.1050\n",
      "Epoch [3/5], Step [29400/68337], Loss: 5.1237\n",
      "Epoch [3/5], Step [29475/68337], Loss: 5.2268\n",
      "Epoch [3/5], Step [29550/68337], Loss: 5.0864\n",
      "Epoch [3/5], Step [29625/68337], Loss: 4.9767\n",
      "Epoch [3/5], Step [29700/68337], Loss: 4.9388\n",
      "Epoch [3/5], Step [29775/68337], Loss: 5.1898\n",
      "Epoch [3/5], Step [29850/68337], Loss: 5.1800\n",
      "Epoch [3/5], Step [29925/68337], Loss: 5.0855\n",
      "Epoch [3/5], Step [30000/68337], Loss: 5.1203\n",
      "Validation perplexity: 150.9847823929348\n",
      "Epoch [3/5], Step [30075/68337], Loss: 5.0532\n",
      "Epoch [3/5], Step [30150/68337], Loss: 4.8731\n",
      "Epoch [3/5], Step [30225/68337], Loss: 5.0099\n",
      "Epoch [3/5], Step [30300/68337], Loss: 4.9302\n",
      "Epoch [3/5], Step [30375/68337], Loss: 5.1680\n",
      "Epoch [3/5], Step [30450/68337], Loss: 5.0672\n",
      "Epoch [3/5], Step [30525/68337], Loss: 5.2096\n",
      "Epoch [3/5], Step [30600/68337], Loss: 5.0158\n",
      "Epoch [3/5], Step [30675/68337], Loss: 4.9609\n",
      "Epoch [3/5], Step [30750/68337], Loss: 5.1229\n",
      "Epoch [3/5], Step [30825/68337], Loss: 5.1529\n",
      "Epoch [3/5], Step [30900/68337], Loss: 4.9967\n",
      "Epoch [3/5], Step [30975/68337], Loss: 5.1394\n",
      "Epoch [3/5], Step [31050/68337], Loss: 5.1099\n",
      "Epoch [3/5], Step [31125/68337], Loss: 5.1088\n",
      "Epoch [3/5], Step [31200/68337], Loss: 5.1287\n",
      "Epoch [3/5], Step [31275/68337], Loss: 5.0868\n",
      "Epoch [3/5], Step [31350/68337], Loss: 4.9500\n",
      "Epoch [3/5], Step [31425/68337], Loss: 5.0272\n",
      "Epoch [3/5], Step [31500/68337], Loss: 4.9327\n",
      "Epoch [3/5], Step [31575/68337], Loss: 5.1253\n",
      "Epoch [3/5], Step [31650/68337], Loss: 5.1219\n",
      "Epoch [3/5], Step [31725/68337], Loss: 4.9134\n",
      "Epoch [3/5], Step [31800/68337], Loss: 5.0229\n",
      "Epoch [3/5], Step [31875/68337], Loss: 5.2176\n",
      "Epoch [3/5], Step [31950/68337], Loss: 4.9853\n",
      "Epoch [3/5], Step [32025/68337], Loss: 5.0649\n",
      "Epoch [3/5], Step [32100/68337], Loss: 4.9964\n",
      "Epoch [3/5], Step [32175/68337], Loss: 5.1861\n",
      "Epoch [3/5], Step [32250/68337], Loss: 5.1650\n",
      "Epoch [3/5], Step [32325/68337], Loss: 5.1243\n",
      "Epoch [3/5], Step [32400/68337], Loss: 5.1013\n",
      "Epoch [3/5], Step [32475/68337], Loss: 4.9026\n",
      "Epoch [3/5], Step [32550/68337], Loss: 5.1937\n",
      "Epoch [3/5], Step [32625/68337], Loss: 5.1426\n",
      "Epoch [3/5], Step [32700/68337], Loss: 5.0672\n",
      "Epoch [3/5], Step [32775/68337], Loss: 5.0568\n",
      "Epoch [3/5], Step [32850/68337], Loss: 5.0086\n",
      "Epoch [3/5], Step [32925/68337], Loss: 5.0622\n",
      "Epoch [3/5], Step [33000/68337], Loss: 4.9533\n",
      "Epoch [3/5], Step [33075/68337], Loss: 4.9716\n",
      "Epoch [3/5], Step [33150/68337], Loss: 4.9737\n",
      "Epoch [3/5], Step [33225/68337], Loss: 4.9535\n",
      "Epoch [3/5], Step [33300/68337], Loss: 5.0528\n",
      "Epoch [3/5], Step [33375/68337], Loss: 4.9990\n",
      "Epoch [3/5], Step [33450/68337], Loss: 5.1223\n",
      "Epoch [3/5], Step [33525/68337], Loss: 5.0673\n",
      "Epoch [3/5], Step [33600/68337], Loss: 5.0352\n",
      "Epoch [3/5], Step [33675/68337], Loss: 4.9689\n",
      "Epoch [3/5], Step [33750/68337], Loss: 4.9036\n",
      "Epoch [3/5], Step [33825/68337], Loss: 5.1015\n",
      "Epoch [3/5], Step [33900/68337], Loss: 4.8674\n",
      "Epoch [3/5], Step [33975/68337], Loss: 5.1268\n",
      "Epoch [3/5], Step [34050/68337], Loss: 5.0346\n",
      "Epoch [3/5], Step [34125/68337], Loss: 4.9726\n",
      "Epoch [3/5], Step [34200/68337], Loss: 5.0083\n",
      "Epoch [3/5], Step [34275/68337], Loss: 5.0570\n",
      "Epoch [3/5], Step [34350/68337], Loss: 5.0690\n",
      "Epoch [3/5], Step [34425/68337], Loss: 5.0158\n",
      "Epoch [3/5], Step [34500/68337], Loss: 5.2036\n",
      "Epoch [3/5], Step [34575/68337], Loss: 5.0505\n",
      "Epoch [3/5], Step [34650/68337], Loss: 5.1411\n",
      "Epoch [3/5], Step [34725/68337], Loss: 4.9770\n",
      "Epoch [3/5], Step [34800/68337], Loss: 5.1568\n",
      "Epoch [3/5], Step [34875/68337], Loss: 4.9732\n",
      "Epoch [3/5], Step [34950/68337], Loss: 4.8512\n",
      "Epoch [3/5], Step [35025/68337], Loss: 4.8804\n",
      "Epoch [3/5], Step [35100/68337], Loss: 5.0120\n",
      "Epoch [3/5], Step [35175/68337], Loss: 5.1186\n",
      "Epoch [3/5], Step [35250/68337], Loss: 4.8902\n",
      "Epoch [3/5], Step [35325/68337], Loss: 5.0381\n",
      "Epoch [3/5], Step [35400/68337], Loss: 5.0875\n",
      "Epoch [3/5], Step [35475/68337], Loss: 5.0460\n",
      "Epoch [3/5], Step [35550/68337], Loss: 5.0107\n",
      "Epoch [3/5], Step [35625/68337], Loss: 4.9772\n",
      "Epoch [3/5], Step [35700/68337], Loss: 5.1500\n",
      "Epoch [3/5], Step [35775/68337], Loss: 5.0218\n",
      "Epoch [3/5], Step [35850/68337], Loss: 5.0009\n",
      "Epoch [3/5], Step [35925/68337], Loss: 5.1349\n",
      "Epoch [3/5], Step [36000/68337], Loss: 5.1390\n",
      "Epoch [3/5], Step [36075/68337], Loss: 5.0890\n",
      "Epoch [3/5], Step [36150/68337], Loss: 4.9531\n",
      "Epoch [3/5], Step [36225/68337], Loss: 5.1782\n",
      "Epoch [3/5], Step [36300/68337], Loss: 5.1132\n",
      "Epoch [3/5], Step [36375/68337], Loss: 5.3298\n",
      "Epoch [3/5], Step [36450/68337], Loss: 5.0454\n",
      "Epoch [3/5], Step [36525/68337], Loss: 5.2538\n",
      "Epoch [3/5], Step [36600/68337], Loss: 5.0408\n",
      "Epoch [3/5], Step [36675/68337], Loss: 5.0941\n",
      "Epoch [3/5], Step [36750/68337], Loss: 5.0525\n",
      "Epoch [3/5], Step [36825/68337], Loss: 5.1048\n",
      "Epoch [3/5], Step [36900/68337], Loss: 5.0620\n",
      "Epoch [3/5], Step [36975/68337], Loss: 5.1619\n",
      "Epoch [3/5], Step [37050/68337], Loss: 5.0188\n",
      "Epoch [3/5], Step [37125/68337], Loss: 5.1410\n",
      "Epoch [3/5], Step [37200/68337], Loss: 5.0873\n",
      "Epoch [3/5], Step [37275/68337], Loss: 4.9155\n",
      "Epoch [3/5], Step [37350/68337], Loss: 5.1365\n",
      "Epoch [3/5], Step [37425/68337], Loss: 5.2052\n",
      "Epoch [3/5], Step [37500/68337], Loss: 5.0702\n",
      "Epoch [3/5], Step [37575/68337], Loss: 5.0292\n",
      "Epoch [3/5], Step [37650/68337], Loss: 5.0566\n",
      "Epoch [3/5], Step [37725/68337], Loss: 4.9314\n",
      "Epoch [3/5], Step [37800/68337], Loss: 5.1455\n",
      "Epoch [3/5], Step [37875/68337], Loss: 5.1161\n",
      "Epoch [3/5], Step [37950/68337], Loss: 4.9768\n",
      "Epoch [3/5], Step [38025/68337], Loss: 5.1835\n",
      "Epoch [3/5], Step [38100/68337], Loss: 5.1198\n",
      "Epoch [3/5], Step [38175/68337], Loss: 5.0723\n",
      "Epoch [3/5], Step [38250/68337], Loss: 5.1246\n",
      "Epoch [3/5], Step [38325/68337], Loss: 5.1842\n",
      "Epoch [3/5], Step [38400/68337], Loss: 5.1064\n",
      "Epoch [3/5], Step [38475/68337], Loss: 4.8027\n",
      "Epoch [3/5], Step [38550/68337], Loss: 5.0999\n",
      "Epoch [3/5], Step [38625/68337], Loss: 5.0798\n",
      "Epoch [3/5], Step [38700/68337], Loss: 5.0610\n",
      "Epoch [3/5], Step [38775/68337], Loss: 4.9841\n",
      "Epoch [3/5], Step [38850/68337], Loss: 5.2498\n",
      "Epoch [3/5], Step [38925/68337], Loss: 5.0341\n",
      "Epoch [3/5], Step [39000/68337], Loss: 5.1342\n",
      "Epoch [3/5], Step [39075/68337], Loss: 5.0607\n",
      "Epoch [3/5], Step [39150/68337], Loss: 5.0644\n",
      "Epoch [3/5], Step [39225/68337], Loss: 5.0841\n",
      "Epoch [3/5], Step [39300/68337], Loss: 5.0317\n",
      "Epoch [3/5], Step [39375/68337], Loss: 4.9614\n",
      "Epoch [3/5], Step [39450/68337], Loss: 5.1417\n",
      "Epoch [3/5], Step [39525/68337], Loss: 4.9784\n",
      "Epoch [3/5], Step [39600/68337], Loss: 5.0605\n",
      "Epoch [3/5], Step [39675/68337], Loss: 5.1134\n",
      "Epoch [3/5], Step [39750/68337], Loss: 5.1285\n",
      "Epoch [3/5], Step [39825/68337], Loss: 5.1431\n",
      "Epoch [3/5], Step [39900/68337], Loss: 4.9593\n",
      "Epoch [3/5], Step [39975/68337], Loss: 5.1362\n",
      "Validation perplexity: 149.44522468963464\n",
      "Epoch [3/5], Step [40050/68337], Loss: 5.0587\n",
      "Epoch [3/5], Step [40125/68337], Loss: 5.0825\n",
      "Epoch [3/5], Step [40200/68337], Loss: 5.0529\n",
      "Epoch [3/5], Step [40275/68337], Loss: 5.0974\n",
      "Epoch [3/5], Step [40350/68337], Loss: 5.1482\n",
      "Epoch [3/5], Step [40425/68337], Loss: 4.9690\n",
      "Epoch [3/5], Step [40500/68337], Loss: 5.1997\n",
      "Epoch [3/5], Step [40575/68337], Loss: 4.9953\n",
      "Epoch [3/5], Step [40650/68337], Loss: 5.2340\n",
      "Epoch [3/5], Step [40725/68337], Loss: 5.1323\n",
      "Epoch [3/5], Step [40800/68337], Loss: 4.9192\n",
      "Epoch [3/5], Step [40875/68337], Loss: 5.0114\n",
      "Epoch [3/5], Step [40950/68337], Loss: 5.2065\n",
      "Epoch [3/5], Step [41025/68337], Loss: 4.9903\n",
      "Epoch [3/5], Step [41100/68337], Loss: 5.0056\n",
      "Epoch [3/5], Step [41175/68337], Loss: 5.1649\n",
      "Epoch [3/5], Step [41250/68337], Loss: 5.1895\n",
      "Epoch [3/5], Step [41325/68337], Loss: 5.1228\n",
      "Epoch [3/5], Step [41400/68337], Loss: 4.9865\n",
      "Epoch [3/5], Step [41475/68337], Loss: 5.0687\n",
      "Epoch [3/5], Step [41550/68337], Loss: 5.0690\n",
      "Epoch [3/5], Step [41625/68337], Loss: 4.8079\n",
      "Epoch [3/5], Step [41700/68337], Loss: 5.0190\n",
      "Epoch [3/5], Step [41775/68337], Loss: 5.0180\n",
      "Epoch [3/5], Step [41850/68337], Loss: 5.2004\n",
      "Epoch [3/5], Step [41925/68337], Loss: 4.9455\n",
      "Epoch [3/5], Step [42000/68337], Loss: 5.1382\n",
      "Epoch [3/5], Step [42075/68337], Loss: 5.1057\n",
      "Epoch [3/5], Step [42150/68337], Loss: 5.0413\n",
      "Epoch [3/5], Step [42225/68337], Loss: 5.0493\n",
      "Epoch [3/5], Step [42300/68337], Loss: 5.0213\n",
      "Epoch [3/5], Step [42375/68337], Loss: 5.0267\n",
      "Epoch [3/5], Step [42450/68337], Loss: 4.7914\n",
      "Epoch [3/5], Step [42525/68337], Loss: 5.1023\n",
      "Epoch [3/5], Step [42600/68337], Loss: 5.0064\n",
      "Epoch [3/5], Step [42675/68337], Loss: 4.9711\n",
      "Epoch [3/5], Step [42750/68337], Loss: 5.0719\n",
      "Epoch [3/5], Step [42825/68337], Loss: 4.8736\n",
      "Epoch [3/5], Step [42900/68337], Loss: 4.9869\n",
      "Epoch [3/5], Step [42975/68337], Loss: 4.9367\n",
      "Epoch [3/5], Step [43050/68337], Loss: 5.0091\n",
      "Epoch [3/5], Step [43125/68337], Loss: 5.0062\n",
      "Epoch [3/5], Step [43200/68337], Loss: 5.0550\n",
      "Epoch [3/5], Step [43275/68337], Loss: 5.0011\n",
      "Epoch [3/5], Step [43350/68337], Loss: 5.1993\n",
      "Epoch [3/5], Step [43425/68337], Loss: 5.1131\n",
      "Epoch [3/5], Step [43500/68337], Loss: 5.0296\n",
      "Epoch [3/5], Step [43575/68337], Loss: 4.9777\n",
      "Epoch [3/5], Step [43650/68337], Loss: 5.1148\n",
      "Epoch [3/5], Step [43725/68337], Loss: 4.9758\n",
      "Epoch [3/5], Step [43800/68337], Loss: 5.0147\n",
      "Epoch [3/5], Step [43875/68337], Loss: 5.0989\n",
      "Epoch [3/5], Step [43950/68337], Loss: 5.1906\n",
      "Epoch [3/5], Step [44025/68337], Loss: 5.0920\n",
      "Epoch [3/5], Step [44100/68337], Loss: 4.9250\n",
      "Epoch [3/5], Step [44175/68337], Loss: 5.0975\n",
      "Epoch [3/5], Step [44250/68337], Loss: 4.9400\n",
      "Epoch [3/5], Step [44325/68337], Loss: 5.1559\n",
      "Epoch [3/5], Step [44400/68337], Loss: 5.1108\n",
      "Epoch [3/5], Step [44475/68337], Loss: 5.1187\n",
      "Epoch [3/5], Step [44550/68337], Loss: 5.0055\n",
      "Epoch [3/5], Step [44625/68337], Loss: 5.0575\n",
      "Epoch [3/5], Step [44700/68337], Loss: 4.8486\n",
      "Epoch [3/5], Step [44775/68337], Loss: 4.8910\n",
      "Epoch [3/5], Step [44850/68337], Loss: 4.9309\n",
      "Epoch [3/5], Step [44925/68337], Loss: 5.1885\n",
      "Epoch [3/5], Step [45000/68337], Loss: 5.0901\n",
      "Epoch [3/5], Step [45075/68337], Loss: 5.1328\n",
      "Epoch [3/5], Step [45150/68337], Loss: 5.0420\n",
      "Epoch [3/5], Step [45225/68337], Loss: 5.0908\n",
      "Epoch [3/5], Step [45300/68337], Loss: 4.9649\n",
      "Epoch [3/5], Step [45375/68337], Loss: 5.1233\n",
      "Epoch [3/5], Step [45450/68337], Loss: 5.0330\n",
      "Epoch [3/5], Step [45525/68337], Loss: 4.9510\n",
      "Epoch [3/5], Step [45600/68337], Loss: 5.0177\n",
      "Epoch [3/5], Step [45675/68337], Loss: 5.0227\n",
      "Epoch [3/5], Step [45750/68337], Loss: 5.0492\n",
      "Epoch [3/5], Step [45825/68337], Loss: 5.0196\n",
      "Epoch [3/5], Step [45900/68337], Loss: 5.0254\n",
      "Epoch [3/5], Step [45975/68337], Loss: 5.2565\n",
      "Epoch [3/5], Step [46050/68337], Loss: 5.1089\n",
      "Epoch [3/5], Step [46125/68337], Loss: 5.0823\n",
      "Epoch [3/5], Step [46200/68337], Loss: 5.0553\n",
      "Epoch [3/5], Step [46275/68337], Loss: 5.0643\n",
      "Epoch [3/5], Step [46350/68337], Loss: 5.0415\n",
      "Epoch [3/5], Step [46425/68337], Loss: 5.0397\n",
      "Epoch [3/5], Step [46500/68337], Loss: 4.9668\n",
      "Epoch [3/5], Step [46575/68337], Loss: 5.1414\n",
      "Epoch [3/5], Step [46650/68337], Loss: 4.9058\n",
      "Epoch [3/5], Step [46725/68337], Loss: 5.0168\n",
      "Epoch [3/5], Step [46800/68337], Loss: 5.1698\n",
      "Epoch [3/5], Step [46875/68337], Loss: 4.9518\n",
      "Epoch [3/5], Step [46950/68337], Loss: 5.1573\n",
      "Epoch [3/5], Step [47025/68337], Loss: 5.0749\n",
      "Epoch [3/5], Step [47100/68337], Loss: 4.9899\n",
      "Epoch [3/5], Step [47175/68337], Loss: 5.2207\n",
      "Epoch [3/5], Step [47250/68337], Loss: 5.1760\n",
      "Epoch [3/5], Step [47325/68337], Loss: 5.0884\n",
      "Epoch [3/5], Step [47400/68337], Loss: 5.1147\n",
      "Epoch [3/5], Step [47475/68337], Loss: 5.1298\n",
      "Epoch [3/5], Step [47550/68337], Loss: 5.0622\n",
      "Epoch [3/5], Step [47625/68337], Loss: 5.0497\n",
      "Epoch [3/5], Step [47700/68337], Loss: 4.9874\n",
      "Epoch [3/5], Step [47775/68337], Loss: 5.0508\n",
      "Epoch [3/5], Step [47850/68337], Loss: 4.8662\n",
      "Epoch [3/5], Step [47925/68337], Loss: 5.0934\n",
      "Epoch [3/5], Step [48000/68337], Loss: 5.0695\n",
      "Epoch [3/5], Step [48075/68337], Loss: 4.9173\n",
      "Epoch [3/5], Step [48150/68337], Loss: 5.1062\n",
      "Epoch [3/5], Step [48225/68337], Loss: 5.1514\n",
      "Epoch [3/5], Step [48300/68337], Loss: 5.1035\n",
      "Epoch [3/5], Step [48375/68337], Loss: 4.9690\n",
      "Epoch [3/5], Step [48450/68337], Loss: 5.0529\n",
      "Epoch [3/5], Step [48525/68337], Loss: 5.1239\n",
      "Epoch [3/5], Step [48600/68337], Loss: 5.0622\n",
      "Epoch [3/5], Step [48675/68337], Loss: 5.1116\n",
      "Epoch [3/5], Step [48750/68337], Loss: 4.8959\n",
      "Epoch [3/5], Step [48825/68337], Loss: 4.9582\n",
      "Epoch [3/5], Step [48900/68337], Loss: 5.0617\n",
      "Epoch [3/5], Step [48975/68337], Loss: 5.1026\n",
      "Epoch [3/5], Step [49050/68337], Loss: 5.0339\n",
      "Epoch [3/5], Step [49125/68337], Loss: 5.0849\n",
      "Epoch [3/5], Step [49200/68337], Loss: 5.1363\n",
      "Epoch [3/5], Step [49275/68337], Loss: 5.0751\n",
      "Epoch [3/5], Step [49350/68337], Loss: 5.0172\n",
      "Epoch [3/5], Step [49425/68337], Loss: 4.9996\n",
      "Epoch [3/5], Step [49500/68337], Loss: 5.0926\n",
      "Epoch [3/5], Step [49575/68337], Loss: 4.9427\n",
      "Epoch [3/5], Step [49650/68337], Loss: 4.9299\n",
      "Epoch [3/5], Step [49725/68337], Loss: 5.0111\n",
      "Epoch [3/5], Step [49800/68337], Loss: 4.9430\n",
      "Epoch [3/5], Step [49875/68337], Loss: 5.0488\n",
      "Epoch [3/5], Step [49950/68337], Loss: 5.2219\n",
      "Validation perplexity: 149.47453799083044\n",
      "Epoch [3/5], Step [50025/68337], Loss: 5.0671\n",
      "Epoch [3/5], Step [50100/68337], Loss: 4.9186\n",
      "Epoch [3/5], Step [50175/68337], Loss: 4.9688\n",
      "Epoch [3/5], Step [50250/68337], Loss: 5.0244\n",
      "Epoch [3/5], Step [50325/68337], Loss: 5.1000\n",
      "Epoch [3/5], Step [50400/68337], Loss: 4.9420\n",
      "Epoch [3/5], Step [50475/68337], Loss: 5.1556\n",
      "Epoch [3/5], Step [50550/68337], Loss: 5.0737\n",
      "Epoch [3/5], Step [50625/68337], Loss: 4.9469\n",
      "Epoch [3/5], Step [50700/68337], Loss: 5.2279\n",
      "Epoch [3/5], Step [50775/68337], Loss: 5.1776\n",
      "Epoch [3/5], Step [50850/68337], Loss: 5.0597\n",
      "Epoch [3/5], Step [50925/68337], Loss: 5.0384\n",
      "Epoch [3/5], Step [51000/68337], Loss: 5.0754\n",
      "Epoch [3/5], Step [51075/68337], Loss: 4.9129\n",
      "Epoch [3/5], Step [51150/68337], Loss: 5.0243\n",
      "Epoch [3/5], Step [51225/68337], Loss: 5.1733\n",
      "Epoch [3/5], Step [51300/68337], Loss: 5.0219\n",
      "Epoch [3/5], Step [51375/68337], Loss: 5.1622\n",
      "Epoch [3/5], Step [51450/68337], Loss: 4.8873\n",
      "Epoch [3/5], Step [51525/68337], Loss: 5.2211\n",
      "Epoch [3/5], Step [51600/68337], Loss: 5.0790\n",
      "Epoch [3/5], Step [51675/68337], Loss: 5.0432\n",
      "Epoch [3/5], Step [51750/68337], Loss: 5.0408\n",
      "Epoch [3/5], Step [51825/68337], Loss: 4.9873\n",
      "Epoch [3/5], Step [51900/68337], Loss: 5.0107\n",
      "Epoch [3/5], Step [51975/68337], Loss: 5.1322\n",
      "Epoch [3/5], Step [52050/68337], Loss: 5.1194\n",
      "Epoch [3/5], Step [52125/68337], Loss: 5.0921\n",
      "Epoch [3/5], Step [52200/68337], Loss: 5.1573\n",
      "Epoch [3/5], Step [52275/68337], Loss: 5.0430\n",
      "Epoch [3/5], Step [52350/68337], Loss: 5.0978\n",
      "Epoch [3/5], Step [52425/68337], Loss: 4.9262\n",
      "Epoch [3/5], Step [52500/68337], Loss: 5.0101\n",
      "Epoch [3/5], Step [52575/68337], Loss: 5.0625\n",
      "Epoch [3/5], Step [52650/68337], Loss: 4.9282\n",
      "Epoch [3/5], Step [52725/68337], Loss: 5.1075\n",
      "Epoch [3/5], Step [52800/68337], Loss: 4.9223\n",
      "Epoch [3/5], Step [52875/68337], Loss: 4.9397\n",
      "Epoch [3/5], Step [52950/68337], Loss: 5.0586\n",
      "Epoch [3/5], Step [53025/68337], Loss: 4.9840\n",
      "Epoch [3/5], Step [53100/68337], Loss: 5.1672\n",
      "Epoch [3/5], Step [53175/68337], Loss: 5.1774\n",
      "Epoch [3/5], Step [53250/68337], Loss: 5.0817\n",
      "Epoch [3/5], Step [53325/68337], Loss: 5.2629\n",
      "Epoch [3/5], Step [53400/68337], Loss: 5.0997\n",
      "Epoch [3/5], Step [53475/68337], Loss: 4.9593\n",
      "Epoch [3/5], Step [53550/68337], Loss: 5.2671\n",
      "Epoch [3/5], Step [53625/68337], Loss: 5.1177\n",
      "Epoch [3/5], Step [53700/68337], Loss: 4.9927\n",
      "Epoch [3/5], Step [53775/68337], Loss: 5.0153\n",
      "Epoch [3/5], Step [53850/68337], Loss: 4.9678\n",
      "Epoch [3/5], Step [53925/68337], Loss: 5.1140\n",
      "Epoch [3/5], Step [54000/68337], Loss: 5.0770\n",
      "Epoch [3/5], Step [54075/68337], Loss: 4.9410\n",
      "Epoch [3/5], Step [54150/68337], Loss: 5.1999\n",
      "Epoch [3/5], Step [54225/68337], Loss: 4.8410\n",
      "Epoch [3/5], Step [54300/68337], Loss: 5.0567\n",
      "Epoch [3/5], Step [54375/68337], Loss: 5.2937\n",
      "Epoch [3/5], Step [54450/68337], Loss: 4.9704\n",
      "Epoch [3/5], Step [54525/68337], Loss: 5.1222\n",
      "Epoch [3/5], Step [54600/68337], Loss: 4.9514\n",
      "Epoch [3/5], Step [54675/68337], Loss: 5.0771\n",
      "Epoch [3/5], Step [54750/68337], Loss: 5.1996\n",
      "Epoch [3/5], Step [54825/68337], Loss: 4.9725\n",
      "Epoch [3/5], Step [54900/68337], Loss: 5.2507\n",
      "Epoch [3/5], Step [54975/68337], Loss: 4.9624\n",
      "Epoch [3/5], Step [55050/68337], Loss: 4.8999\n",
      "Epoch [3/5], Step [55125/68337], Loss: 4.9915\n",
      "Epoch [3/5], Step [55200/68337], Loss: 5.0530\n",
      "Epoch [3/5], Step [55275/68337], Loss: 5.0838\n",
      "Epoch [3/5], Step [55350/68337], Loss: 5.1225\n",
      "Epoch [3/5], Step [55425/68337], Loss: 5.0624\n",
      "Epoch [3/5], Step [55500/68337], Loss: 5.0979\n",
      "Epoch [3/5], Step [55575/68337], Loss: 5.0810\n",
      "Epoch [3/5], Step [55650/68337], Loss: 5.0239\n",
      "Epoch [3/5], Step [55725/68337], Loss: 4.9401\n",
      "Epoch [3/5], Step [55800/68337], Loss: 5.1597\n",
      "Epoch [3/5], Step [55875/68337], Loss: 5.0360\n",
      "Epoch [3/5], Step [55950/68337], Loss: 5.0693\n",
      "Epoch [3/5], Step [56025/68337], Loss: 5.1420\n",
      "Epoch [3/5], Step [56100/68337], Loss: 5.0771\n",
      "Epoch [3/5], Step [56175/68337], Loss: 5.0605\n",
      "Epoch [3/5], Step [56250/68337], Loss: 4.8007\n",
      "Epoch [3/5], Step [56325/68337], Loss: 5.0471\n",
      "Epoch [3/5], Step [56400/68337], Loss: 5.2057\n",
      "Epoch [3/5], Step [56475/68337], Loss: 5.1900\n",
      "Epoch [3/5], Step [56550/68337], Loss: 5.0229\n",
      "Epoch [3/5], Step [56625/68337], Loss: 5.0507\n",
      "Epoch [3/5], Step [56700/68337], Loss: 5.1929\n",
      "Epoch [3/5], Step [56775/68337], Loss: 4.9212\n",
      "Epoch [3/5], Step [56850/68337], Loss: 5.0592\n",
      "Epoch [3/5], Step [56925/68337], Loss: 5.0223\n",
      "Epoch [3/5], Step [57000/68337], Loss: 4.9415\n",
      "Epoch [3/5], Step [57075/68337], Loss: 5.2155\n",
      "Epoch [3/5], Step [57150/68337], Loss: 5.0501\n",
      "Epoch [3/5], Step [57225/68337], Loss: 5.0128\n",
      "Epoch [3/5], Step [57300/68337], Loss: 4.9984\n",
      "Epoch [3/5], Step [57375/68337], Loss: 5.0890\n",
      "Epoch [3/5], Step [57450/68337], Loss: 5.0756\n",
      "Epoch [3/5], Step [57525/68337], Loss: 5.0330\n",
      "Epoch [3/5], Step [57600/68337], Loss: 5.0226\n",
      "Epoch [3/5], Step [57675/68337], Loss: 5.1591\n",
      "Epoch [3/5], Step [57750/68337], Loss: 5.0850\n",
      "Epoch [3/5], Step [57825/68337], Loss: 5.0304\n",
      "Epoch [3/5], Step [57900/68337], Loss: 5.0196\n",
      "Epoch [3/5], Step [57975/68337], Loss: 5.0814\n",
      "Epoch [3/5], Step [58050/68337], Loss: 5.1433\n",
      "Epoch [3/5], Step [58125/68337], Loss: 4.9303\n",
      "Epoch [3/5], Step [58200/68337], Loss: 4.8757\n",
      "Epoch [3/5], Step [58275/68337], Loss: 5.0212\n",
      "Epoch [3/5], Step [58350/68337], Loss: 5.0345\n",
      "Epoch [3/5], Step [58425/68337], Loss: 5.0670\n",
      "Epoch [3/5], Step [58500/68337], Loss: 4.9974\n",
      "Epoch [3/5], Step [58575/68337], Loss: 4.9992\n",
      "Epoch [3/5], Step [58650/68337], Loss: 5.0549\n",
      "Epoch [3/5], Step [58725/68337], Loss: 5.1087\n",
      "Epoch [3/5], Step [58800/68337], Loss: 4.9230\n",
      "Epoch [3/5], Step [58875/68337], Loss: 5.0370\n",
      "Epoch [3/5], Step [58950/68337], Loss: 5.0531\n",
      "Epoch [3/5], Step [59025/68337], Loss: 4.9370\n",
      "Epoch [3/5], Step [59100/68337], Loss: 4.9184\n",
      "Epoch [3/5], Step [59175/68337], Loss: 5.1372\n",
      "Epoch [3/5], Step [59250/68337], Loss: 4.9879\n",
      "Epoch [3/5], Step [59325/68337], Loss: 5.0644\n",
      "Epoch [3/5], Step [59400/68337], Loss: 5.0561\n",
      "Epoch [3/5], Step [59475/68337], Loss: 5.1750\n",
      "Epoch [3/5], Step [59550/68337], Loss: 5.0364\n",
      "Epoch [3/5], Step [59625/68337], Loss: 5.0069\n",
      "Epoch [3/5], Step [59700/68337], Loss: 5.0106\n",
      "Epoch [3/5], Step [59775/68337], Loss: 5.0830\n",
      "Epoch [3/5], Step [59850/68337], Loss: 5.0160\n",
      "Epoch [3/5], Step [59925/68337], Loss: 5.2106\n",
      "Epoch [3/5], Step [60000/68337], Loss: 4.9452\n",
      "Validation perplexity: 148.64578142635483\n",
      "Epoch [3/5], Step [60075/68337], Loss: 5.0672\n",
      "Epoch [3/5], Step [60150/68337], Loss: 4.9668\n",
      "Epoch [3/5], Step [60225/68337], Loss: 5.0082\n",
      "Epoch [3/5], Step [60300/68337], Loss: 4.8967\n",
      "Epoch [3/5], Step [60375/68337], Loss: 5.0678\n",
      "Epoch [3/5], Step [60450/68337], Loss: 4.8884\n",
      "Epoch [3/5], Step [60525/68337], Loss: 5.0411\n",
      "Epoch [3/5], Step [60600/68337], Loss: 5.0628\n",
      "Epoch [3/5], Step [60675/68337], Loss: 4.9564\n",
      "Epoch [3/5], Step [60750/68337], Loss: 5.0782\n",
      "Epoch [3/5], Step [60825/68337], Loss: 5.0681\n",
      "Epoch [3/5], Step [60900/68337], Loss: 5.1613\n",
      "Epoch [3/5], Step [60975/68337], Loss: 4.9700\n",
      "Epoch [3/5], Step [61050/68337], Loss: 5.0596\n",
      "Epoch [3/5], Step [61125/68337], Loss: 5.2287\n",
      "Epoch [3/5], Step [61200/68337], Loss: 4.9300\n",
      "Epoch [3/5], Step [61275/68337], Loss: 5.0334\n",
      "Epoch [3/5], Step [61350/68337], Loss: 5.1487\n",
      "Epoch [3/5], Step [61425/68337], Loss: 5.1065\n",
      "Epoch [3/5], Step [61500/68337], Loss: 5.2280\n",
      "Epoch [3/5], Step [61575/68337], Loss: 5.0914\n",
      "Epoch [3/5], Step [61650/68337], Loss: 5.0087\n",
      "Epoch [3/5], Step [61725/68337], Loss: 4.8636\n",
      "Epoch [3/5], Step [61800/68337], Loss: 5.1719\n",
      "Epoch [3/5], Step [61875/68337], Loss: 5.0061\n",
      "Epoch [3/5], Step [61950/68337], Loss: 5.0243\n",
      "Epoch [3/5], Step [62025/68337], Loss: 4.9900\n",
      "Epoch [3/5], Step [62100/68337], Loss: 4.9653\n",
      "Epoch [3/5], Step [62175/68337], Loss: 4.9561\n",
      "Epoch [3/5], Step [62250/68337], Loss: 5.1575\n",
      "Epoch [3/5], Step [62325/68337], Loss: 5.1207\n",
      "Epoch [3/5], Step [62400/68337], Loss: 4.9174\n",
      "Epoch [3/5], Step [62475/68337], Loss: 5.1331\n",
      "Epoch [3/5], Step [62550/68337], Loss: 5.0779\n",
      "Epoch [3/5], Step [62625/68337], Loss: 4.9893\n",
      "Epoch [3/5], Step [62700/68337], Loss: 5.1877\n",
      "Epoch [3/5], Step [62775/68337], Loss: 5.0399\n",
      "Epoch [3/5], Step [62850/68337], Loss: 5.0014\n",
      "Epoch [3/5], Step [62925/68337], Loss: 4.9630\n",
      "Epoch [3/5], Step [63000/68337], Loss: 5.0029\n",
      "Epoch [3/5], Step [63075/68337], Loss: 5.0665\n",
      "Epoch [3/5], Step [63150/68337], Loss: 4.9845\n",
      "Epoch [3/5], Step [63225/68337], Loss: 5.0560\n",
      "Epoch [3/5], Step [63300/68337], Loss: 5.0259\n",
      "Epoch [3/5], Step [63375/68337], Loss: 5.1083\n",
      "Epoch [3/5], Step [63450/68337], Loss: 5.1507\n",
      "Epoch [3/5], Step [63525/68337], Loss: 5.0222\n",
      "Epoch [3/5], Step [63600/68337], Loss: 4.9205\n",
      "Epoch [3/5], Step [63675/68337], Loss: 5.1024\n",
      "Epoch [3/5], Step [63750/68337], Loss: 5.1418\n",
      "Epoch [3/5], Step [63825/68337], Loss: 4.9244\n",
      "Epoch [3/5], Step [63900/68337], Loss: 5.0708\n",
      "Epoch [3/5], Step [63975/68337], Loss: 5.0159\n",
      "Epoch [3/5], Step [64050/68337], Loss: 5.1017\n",
      "Epoch [3/5], Step [64125/68337], Loss: 4.9760\n",
      "Epoch [3/5], Step [64200/68337], Loss: 5.0565\n",
      "Epoch [3/5], Step [64275/68337], Loss: 5.0405\n",
      "Epoch [3/5], Step [64350/68337], Loss: 5.1972\n",
      "Epoch [3/5], Step [64425/68337], Loss: 5.1043\n",
      "Epoch [3/5], Step [64500/68337], Loss: 5.1060\n",
      "Epoch [3/5], Step [64575/68337], Loss: 4.9454\n",
      "Epoch [3/5], Step [64650/68337], Loss: 5.2130\n",
      "Epoch [3/5], Step [64725/68337], Loss: 5.0127\n",
      "Epoch [3/5], Step [64800/68337], Loss: 5.0563\n",
      "Epoch [3/5], Step [64875/68337], Loss: 5.0162\n",
      "Epoch [3/5], Step [64950/68337], Loss: 5.1071\n",
      "Epoch [3/5], Step [65025/68337], Loss: 4.9710\n",
      "Epoch [3/5], Step [65100/68337], Loss: 4.8898\n",
      "Epoch [3/5], Step [65175/68337], Loss: 5.0911\n",
      "Epoch [3/5], Step [65250/68337], Loss: 5.1364\n",
      "Epoch [3/5], Step [65325/68337], Loss: 5.0510\n",
      "Epoch [3/5], Step [65400/68337], Loss: 5.0420\n",
      "Epoch [3/5], Step [65475/68337], Loss: 5.1269\n",
      "Epoch [3/5], Step [65550/68337], Loss: 5.0978\n",
      "Epoch [3/5], Step [65625/68337], Loss: 5.0752\n",
      "Epoch [3/5], Step [65700/68337], Loss: 4.9947\n",
      "Epoch [3/5], Step [65775/68337], Loss: 4.9627\n",
      "Epoch [3/5], Step [65850/68337], Loss: 5.0396\n",
      "Epoch [3/5], Step [65925/68337], Loss: 4.9357\n",
      "Epoch [3/5], Step [66000/68337], Loss: 5.3149\n",
      "Epoch [3/5], Step [66075/68337], Loss: 5.0788\n",
      "Epoch [3/5], Step [66150/68337], Loss: 4.9837\n",
      "Epoch [3/5], Step [66225/68337], Loss: 5.1051\n",
      "Epoch [3/5], Step [66300/68337], Loss: 5.1180\n",
      "Epoch [3/5], Step [66375/68337], Loss: 4.8767\n",
      "Epoch [3/5], Step [66450/68337], Loss: 4.9322\n",
      "Epoch [3/5], Step [66525/68337], Loss: 5.0041\n",
      "Epoch [3/5], Step [66600/68337], Loss: 5.2087\n",
      "Epoch [3/5], Step [66675/68337], Loss: 5.1589\n",
      "Epoch [3/5], Step [66750/68337], Loss: 5.0617\n",
      "Epoch [3/5], Step [66825/68337], Loss: 5.1433\n",
      "Epoch [3/5], Step [66900/68337], Loss: 5.1163\n",
      "Epoch [3/5], Step [66975/68337], Loss: 4.9563\n",
      "Epoch [3/5], Step [67050/68337], Loss: 5.0114\n",
      "Epoch [3/5], Step [67125/68337], Loss: 4.9922\n",
      "Epoch [3/5], Step [67200/68337], Loss: 5.0887\n",
      "Epoch [3/5], Step [67275/68337], Loss: 5.1794\n",
      "Epoch [3/5], Step [67350/68337], Loss: 5.0769\n",
      "Epoch [3/5], Step [67425/68337], Loss: 5.0877\n",
      "Epoch [3/5], Step [67500/68337], Loss: 4.9391\n",
      "Epoch [3/5], Step [67575/68337], Loss: 4.9903\n",
      "Epoch [3/5], Step [67650/68337], Loss: 5.1271\n",
      "Epoch [3/5], Step [67725/68337], Loss: 5.0125\n",
      "Epoch [3/5], Step [67800/68337], Loss: 5.0865\n",
      "Epoch [3/5], Step [67875/68337], Loss: 4.9603\n",
      "Epoch [3/5], Step [67950/68337], Loss: 4.8819\n",
      "Epoch [3/5], Step [68025/68337], Loss: 5.0421\n",
      "Epoch [3/5], Step [68100/68337], Loss: 5.2243\n",
      "Epoch [3/5], Step [68175/68337], Loss: 5.1360\n",
      "Epoch [3/5], Step [68250/68337], Loss: 4.8728\n",
      "Epoch [3/5], Step [68325/68337], Loss: 5.0571\n",
      "Epoch [3/5] Average Loss: 5.0586, Perplexity: 157.37\n",
      "Epoch [4/5], Step [0/68337], Loss: 5.1023\n",
      "Validation perplexity: 148.4682867945134\n",
      "Epoch [4/5], Step [75/68337], Loss: 4.9954\n",
      "Epoch [4/5], Step [150/68337], Loss: 5.0473\n",
      "Epoch [4/5], Step [225/68337], Loss: 4.9787\n",
      "Epoch [4/5], Step [300/68337], Loss: 5.0312\n",
      "Epoch [4/5], Step [375/68337], Loss: 5.0990\n",
      "Epoch [4/5], Step [450/68337], Loss: 4.9712\n",
      "Epoch [4/5], Step [525/68337], Loss: 5.0451\n",
      "Epoch [4/5], Step [600/68337], Loss: 5.1047\n",
      "Epoch [4/5], Step [675/68337], Loss: 4.9949\n",
      "Epoch [4/5], Step [750/68337], Loss: 5.0529\n",
      "Epoch [4/5], Step [825/68337], Loss: 5.0160\n",
      "Epoch [4/5], Step [900/68337], Loss: 5.0663\n",
      "Epoch [4/5], Step [975/68337], Loss: 5.1404\n",
      "Epoch [4/5], Step [1050/68337], Loss: 5.0421\n",
      "Epoch [4/5], Step [1125/68337], Loss: 5.0414\n",
      "Epoch [4/5], Step [1200/68337], Loss: 4.8740\n",
      "Epoch [4/5], Step [1275/68337], Loss: 5.0274\n",
      "Epoch [4/5], Step [1350/68337], Loss: 4.9966\n",
      "Epoch [4/5], Step [1425/68337], Loss: 5.0765\n",
      "Epoch [4/5], Step [1500/68337], Loss: 4.9176\n",
      "Epoch [4/5], Step [1575/68337], Loss: 5.2061\n",
      "Epoch [4/5], Step [1650/68337], Loss: 5.2017\n",
      "Epoch [4/5], Step [1725/68337], Loss: 4.8956\n",
      "Epoch [4/5], Step [1800/68337], Loss: 5.0912\n",
      "Epoch [4/5], Step [1875/68337], Loss: 4.9683\n",
      "Epoch [4/5], Step [1950/68337], Loss: 5.0434\n",
      "Epoch [4/5], Step [2025/68337], Loss: 5.0624\n",
      "Epoch [4/5], Step [2100/68337], Loss: 5.0917\n",
      "Epoch [4/5], Step [2175/68337], Loss: 5.1599\n",
      "Epoch [4/5], Step [2250/68337], Loss: 4.9319\n",
      "Epoch [4/5], Step [2325/68337], Loss: 5.0082\n",
      "Epoch [4/5], Step [2400/68337], Loss: 4.8726\n",
      "Epoch [4/5], Step [2475/68337], Loss: 5.0387\n",
      "Epoch [4/5], Step [2550/68337], Loss: 5.1090\n",
      "Epoch [4/5], Step [2625/68337], Loss: 5.0102\n",
      "Epoch [4/5], Step [2700/68337], Loss: 4.9627\n",
      "Epoch [4/5], Step [2775/68337], Loss: 5.0331\n",
      "Epoch [4/5], Step [2850/68337], Loss: 5.0193\n",
      "Epoch [4/5], Step [2925/68337], Loss: 5.0624\n",
      "Epoch [4/5], Step [3000/68337], Loss: 5.0810\n",
      "Epoch [4/5], Step [3075/68337], Loss: 5.0499\n",
      "Epoch [4/5], Step [3150/68337], Loss: 4.9324\n",
      "Epoch [4/5], Step [3225/68337], Loss: 4.9668\n",
      "Epoch [4/5], Step [3300/68337], Loss: 5.2144\n",
      "Epoch [4/5], Step [3375/68337], Loss: 5.0775\n",
      "Epoch [4/5], Step [3450/68337], Loss: 5.1174\n",
      "Epoch [4/5], Step [3525/68337], Loss: 5.0470\n",
      "Epoch [4/5], Step [3600/68337], Loss: 5.0350\n",
      "Epoch [4/5], Step [3675/68337], Loss: 5.0540\n",
      "Epoch [4/5], Step [3750/68337], Loss: 4.9819\n",
      "Epoch [4/5], Step [3825/68337], Loss: 4.9727\n",
      "Epoch [4/5], Step [3900/68337], Loss: 5.1651\n",
      "Epoch [4/5], Step [3975/68337], Loss: 5.1071\n",
      "Epoch [4/5], Step [4050/68337], Loss: 4.9165\n",
      "Epoch [4/5], Step [4125/68337], Loss: 5.0009\n",
      "Epoch [4/5], Step [4200/68337], Loss: 4.9692\n",
      "Epoch [4/5], Step [4275/68337], Loss: 5.2179\n",
      "Epoch [4/5], Step [4350/68337], Loss: 5.2233\n",
      "Epoch [4/5], Step [4425/68337], Loss: 5.1135\n",
      "Epoch [4/5], Step [4500/68337], Loss: 5.0120\n",
      "Epoch [4/5], Step [4575/68337], Loss: 5.0847\n",
      "Epoch [4/5], Step [4650/68337], Loss: 5.1511\n",
      "Epoch [4/5], Step [4725/68337], Loss: 5.0994\n",
      "Epoch [4/5], Step [4800/68337], Loss: 5.0938\n",
      "Epoch [4/5], Step [4875/68337], Loss: 4.9576\n",
      "Epoch [4/5], Step [4950/68337], Loss: 4.8767\n",
      "Epoch [4/5], Step [5025/68337], Loss: 4.9712\n",
      "Epoch [4/5], Step [5100/68337], Loss: 5.0792\n",
      "Epoch [4/5], Step [5175/68337], Loss: 4.8664\n",
      "Epoch [4/5], Step [5250/68337], Loss: 4.9688\n",
      "Epoch [4/5], Step [5325/68337], Loss: 5.1549\n",
      "Epoch [4/5], Step [5400/68337], Loss: 4.7684\n",
      "Epoch [4/5], Step [5475/68337], Loss: 4.8922\n",
      "Epoch [4/5], Step [5550/68337], Loss: 5.0193\n",
      "Epoch [4/5], Step [5625/68337], Loss: 5.0902\n",
      "Epoch [4/5], Step [5700/68337], Loss: 5.0701\n",
      "Epoch [4/5], Step [5775/68337], Loss: 4.9630\n",
      "Epoch [4/5], Step [5850/68337], Loss: 5.0881\n",
      "Epoch [4/5], Step [5925/68337], Loss: 4.9763\n",
      "Epoch [4/5], Step [6000/68337], Loss: 5.1956\n",
      "Epoch [4/5], Step [6075/68337], Loss: 4.8592\n",
      "Epoch [4/5], Step [6150/68337], Loss: 5.0535\n",
      "Epoch [4/5], Step [6225/68337], Loss: 4.9627\n",
      "Epoch [4/5], Step [6300/68337], Loss: 4.9631\n",
      "Epoch [4/5], Step [6375/68337], Loss: 5.0510\n",
      "Epoch [4/5], Step [6450/68337], Loss: 4.9672\n",
      "Epoch [4/5], Step [6525/68337], Loss: 4.8475\n",
      "Epoch [4/5], Step [6600/68337], Loss: 5.1274\n",
      "Epoch [4/5], Step [6675/68337], Loss: 5.0114\n",
      "Epoch [4/5], Step [6750/68337], Loss: 5.1536\n",
      "Epoch [4/5], Step [6825/68337], Loss: 5.2800\n",
      "Epoch [4/5], Step [6900/68337], Loss: 4.9259\n",
      "Epoch [4/5], Step [6975/68337], Loss: 5.0447\n",
      "Epoch [4/5], Step [7050/68337], Loss: 5.0431\n",
      "Epoch [4/5], Step [7125/68337], Loss: 5.1439\n",
      "Epoch [4/5], Step [7200/68337], Loss: 5.0941\n",
      "Epoch [4/5], Step [7275/68337], Loss: 5.0209\n",
      "Epoch [4/5], Step [7350/68337], Loss: 5.0131\n",
      "Epoch [4/5], Step [7425/68337], Loss: 4.9477\n",
      "Epoch [4/5], Step [7500/68337], Loss: 5.0652\n",
      "Epoch [4/5], Step [7575/68337], Loss: 5.0650\n",
      "Epoch [4/5], Step [7650/68337], Loss: 4.9443\n",
      "Epoch [4/5], Step [7725/68337], Loss: 4.9628\n",
      "Epoch [4/5], Step [7800/68337], Loss: 5.1551\n",
      "Epoch [4/5], Step [7875/68337], Loss: 4.9677\n",
      "Epoch [4/5], Step [7950/68337], Loss: 5.0791\n",
      "Epoch [4/5], Step [8025/68337], Loss: 4.8380\n",
      "Epoch [4/5], Step [8100/68337], Loss: 5.2082\n",
      "Epoch [4/5], Step [8175/68337], Loss: 4.9768\n",
      "Epoch [4/5], Step [8250/68337], Loss: 4.9812\n",
      "Epoch [4/5], Step [8325/68337], Loss: 4.9603\n",
      "Epoch [4/5], Step [8400/68337], Loss: 5.0410\n",
      "Epoch [4/5], Step [8475/68337], Loss: 5.0125\n",
      "Epoch [4/5], Step [8550/68337], Loss: 5.1001\n",
      "Epoch [4/5], Step [8625/68337], Loss: 4.9438\n",
      "Epoch [4/5], Step [8700/68337], Loss: 4.8479\n",
      "Epoch [4/5], Step [8775/68337], Loss: 5.0677\n",
      "Epoch [4/5], Step [8850/68337], Loss: 4.9930\n",
      "Epoch [4/5], Step [8925/68337], Loss: 5.1344\n",
      "Epoch [4/5], Step [9000/68337], Loss: 5.2157\n",
      "Epoch [4/5], Step [9075/68337], Loss: 5.0377\n",
      "Epoch [4/5], Step [9150/68337], Loss: 4.9960\n",
      "Epoch [4/5], Step [9225/68337], Loss: 4.9989\n",
      "Epoch [4/5], Step [9300/68337], Loss: 5.0932\n",
      "Epoch [4/5], Step [9375/68337], Loss: 5.0462\n",
      "Epoch [4/5], Step [9450/68337], Loss: 5.2314\n",
      "Epoch [4/5], Step [9525/68337], Loss: 5.1877\n",
      "Epoch [4/5], Step [9600/68337], Loss: 4.9023\n",
      "Epoch [4/5], Step [9675/68337], Loss: 4.9862\n",
      "Epoch [4/5], Step [9750/68337], Loss: 5.0368\n",
      "Epoch [4/5], Step [9825/68337], Loss: 5.1631\n",
      "Epoch [4/5], Step [9900/68337], Loss: 4.9985\n",
      "Epoch [4/5], Step [9975/68337], Loss: 5.0578\n",
      "Validation perplexity: 148.6510546491345\n",
      "Epoch [4/5], Step [10050/68337], Loss: 4.9951\n",
      "Epoch [4/5], Step [10125/68337], Loss: 5.1582\n",
      "Epoch [4/5], Step [10200/68337], Loss: 5.0803\n",
      "Epoch [4/5], Step [10275/68337], Loss: 5.0484\n",
      "Epoch [4/5], Step [10350/68337], Loss: 5.0058\n",
      "Epoch [4/5], Step [10425/68337], Loss: 5.0761\n",
      "Epoch [4/5], Step [10500/68337], Loss: 4.9970\n",
      "Epoch [4/5], Step [10575/68337], Loss: 5.1492\n",
      "Epoch [4/5], Step [10650/68337], Loss: 4.9458\n",
      "Epoch [4/5], Step [10725/68337], Loss: 5.1215\n",
      "Epoch [4/5], Step [10800/68337], Loss: 5.1578\n",
      "Epoch [4/5], Step [10875/68337], Loss: 5.2236\n",
      "Epoch [4/5], Step [10950/68337], Loss: 5.1126\n",
      "Epoch [4/5], Step [11025/68337], Loss: 5.0183\n",
      "Epoch [4/5], Step [11100/68337], Loss: 5.1032\n",
      "Epoch [4/5], Step [11175/68337], Loss: 5.0647\n",
      "Epoch [4/5], Step [11250/68337], Loss: 4.9348\n",
      "Epoch [4/5], Step [11325/68337], Loss: 4.9220\n",
      "Epoch [4/5], Step [11400/68337], Loss: 5.1470\n",
      "Epoch [4/5], Step [11475/68337], Loss: 5.0923\n",
      "Epoch [4/5], Step [11550/68337], Loss: 5.0716\n",
      "Epoch [4/5], Step [11625/68337], Loss: 5.1278\n",
      "Epoch [4/5], Step [11700/68337], Loss: 5.0431\n",
      "Epoch [4/5], Step [11775/68337], Loss: 4.9676\n",
      "Epoch [4/5], Step [11850/68337], Loss: 4.9334\n",
      "Epoch [4/5], Step [11925/68337], Loss: 5.0719\n",
      "Epoch [4/5], Step [12000/68337], Loss: 5.1030\n",
      "Epoch [4/5], Step [12075/68337], Loss: 5.0644\n",
      "Epoch [4/5], Step [12150/68337], Loss: 4.9433\n",
      "Epoch [4/5], Step [12225/68337], Loss: 5.0281\n",
      "Epoch [4/5], Step [12300/68337], Loss: 4.9369\n",
      "Epoch [4/5], Step [12375/68337], Loss: 5.1270\n",
      "Epoch [4/5], Step [12450/68337], Loss: 4.8611\n",
      "Epoch [4/5], Step [12525/68337], Loss: 5.0430\n",
      "Epoch [4/5], Step [12600/68337], Loss: 5.1604\n",
      "Epoch [4/5], Step [12675/68337], Loss: 5.0606\n",
      "Epoch [4/5], Step [12750/68337], Loss: 5.0172\n",
      "Epoch [4/5], Step [12825/68337], Loss: 4.9558\n",
      "Epoch [4/5], Step [12900/68337], Loss: 4.9448\n",
      "Epoch [4/5], Step [12975/68337], Loss: 4.9878\n",
      "Epoch [4/5], Step [13050/68337], Loss: 4.9380\n",
      "Epoch [4/5], Step [13125/68337], Loss: 5.0358\n",
      "Epoch [4/5], Step [13200/68337], Loss: 4.9547\n",
      "Epoch [4/5], Step [13275/68337], Loss: 5.1146\n",
      "Epoch [4/5], Step [13350/68337], Loss: 5.0841\n",
      "Epoch [4/5], Step [13425/68337], Loss: 4.8846\n",
      "Epoch [4/5], Step [13500/68337], Loss: 5.0639\n",
      "Epoch [4/5], Step [13575/68337], Loss: 4.9200\n",
      "Epoch [4/5], Step [13650/68337], Loss: 4.9821\n",
      "Epoch [4/5], Step [13725/68337], Loss: 5.0176\n",
      "Epoch [4/5], Step [13800/68337], Loss: 5.1200\n",
      "Epoch [4/5], Step [13875/68337], Loss: 4.9508\n",
      "Epoch [4/5], Step [13950/68337], Loss: 5.1619\n",
      "Epoch [4/5], Step [14025/68337], Loss: 5.0177\n",
      "Epoch [4/5], Step [14100/68337], Loss: 5.1675\n",
      "Epoch [4/5], Step [14175/68337], Loss: 4.8997\n",
      "Epoch [4/5], Step [14250/68337], Loss: 5.0350\n",
      "Epoch [4/5], Step [14325/68337], Loss: 5.1133\n",
      "Epoch [4/5], Step [14400/68337], Loss: 5.0673\n",
      "Epoch [4/5], Step [14475/68337], Loss: 5.0649\n",
      "Epoch [4/5], Step [14550/68337], Loss: 5.0887\n",
      "Epoch [4/5], Step [14625/68337], Loss: 5.1321\n",
      "Epoch [4/5], Step [14700/68337], Loss: 5.0423\n",
      "Epoch [4/5], Step [14775/68337], Loss: 4.9926\n",
      "Epoch [4/5], Step [14850/68337], Loss: 5.1251\n",
      "Epoch [4/5], Step [14925/68337], Loss: 4.9633\n",
      "Epoch [4/5], Step [15000/68337], Loss: 5.1025\n",
      "Epoch [4/5], Step [15075/68337], Loss: 5.0559\n",
      "Epoch [4/5], Step [15150/68337], Loss: 5.2159\n",
      "Epoch [4/5], Step [15225/68337], Loss: 4.9514\n",
      "Epoch [4/5], Step [15300/68337], Loss: 5.0028\n",
      "Epoch [4/5], Step [15375/68337], Loss: 5.1438\n",
      "Epoch [4/5], Step [15450/68337], Loss: 4.8968\n",
      "Epoch [4/5], Step [15525/68337], Loss: 4.9702\n",
      "Epoch [4/5], Step [15600/68337], Loss: 5.1377\n",
      "Epoch [4/5], Step [15675/68337], Loss: 5.0145\n",
      "Epoch [4/5], Step [15750/68337], Loss: 4.9469\n",
      "Epoch [4/5], Step [15825/68337], Loss: 4.9887\n",
      "Epoch [4/5], Step [15900/68337], Loss: 4.9892\n",
      "Epoch [4/5], Step [15975/68337], Loss: 4.8946\n",
      "Epoch [4/5], Step [16050/68337], Loss: 5.0426\n",
      "Epoch [4/5], Step [16125/68337], Loss: 5.0099\n",
      "Epoch [4/5], Step [16200/68337], Loss: 5.1574\n",
      "Epoch [4/5], Step [16275/68337], Loss: 4.9130\n",
      "Epoch [4/5], Step [16350/68337], Loss: 4.8956\n",
      "Epoch [4/5], Step [16425/68337], Loss: 5.0909\n",
      "Epoch [4/5], Step [16500/68337], Loss: 4.9902\n",
      "Epoch [4/5], Step [16575/68337], Loss: 4.8240\n",
      "Epoch [4/5], Step [16650/68337], Loss: 5.0174\n",
      "Epoch [4/5], Step [16725/68337], Loss: 5.0351\n",
      "Epoch [4/5], Step [16800/68337], Loss: 5.2008\n",
      "Epoch [4/5], Step [16875/68337], Loss: 5.1254\n",
      "Epoch [4/5], Step [16950/68337], Loss: 4.9386\n",
      "Epoch [4/5], Step [17025/68337], Loss: 5.1024\n",
      "Epoch [4/5], Step [17100/68337], Loss: 5.0121\n",
      "Epoch [4/5], Step [17175/68337], Loss: 5.0694\n",
      "Epoch [4/5], Step [17250/68337], Loss: 4.9488\n",
      "Epoch [4/5], Step [17325/68337], Loss: 5.0417\n",
      "Epoch [4/5], Step [17400/68337], Loss: 5.1603\n",
      "Epoch [4/5], Step [17475/68337], Loss: 5.1143\n",
      "Epoch [4/5], Step [17550/68337], Loss: 5.0879\n",
      "Epoch [4/5], Step [17625/68337], Loss: 5.0607\n",
      "Epoch [4/5], Step [17700/68337], Loss: 4.9737\n",
      "Epoch [4/5], Step [17775/68337], Loss: 4.9097\n",
      "Epoch [4/5], Step [17850/68337], Loss: 5.1797\n",
      "Epoch [4/5], Step [17925/68337], Loss: 5.0752\n",
      "Epoch [4/5], Step [18000/68337], Loss: 4.9666\n",
      "Epoch [4/5], Step [18075/68337], Loss: 4.8973\n",
      "Epoch [4/5], Step [18150/68337], Loss: 4.9556\n",
      "Epoch [4/5], Step [18225/68337], Loss: 5.1288\n",
      "Epoch [4/5], Step [18300/68337], Loss: 5.1203\n",
      "Epoch [4/5], Step [18375/68337], Loss: 4.9505\n",
      "Epoch [4/5], Step [18450/68337], Loss: 5.0698\n",
      "Epoch [4/5], Step [18525/68337], Loss: 4.9522\n",
      "Epoch [4/5], Step [18600/68337], Loss: 5.0595\n",
      "Epoch [4/5], Step [18675/68337], Loss: 4.8933\n",
      "Epoch [4/5], Step [18750/68337], Loss: 4.9866\n",
      "Epoch [4/5], Step [18825/68337], Loss: 5.1046\n",
      "Epoch [4/5], Step [18900/68337], Loss: 5.0678\n",
      "Epoch [4/5], Step [18975/68337], Loss: 4.9344\n",
      "Epoch [4/5], Step [19050/68337], Loss: 4.9844\n",
      "Epoch [4/5], Step [19125/68337], Loss: 5.0598\n",
      "Epoch [4/5], Step [19200/68337], Loss: 5.1024\n",
      "Epoch [4/5], Step [19275/68337], Loss: 5.1291\n",
      "Epoch [4/5], Step [19350/68337], Loss: 5.1010\n",
      "Epoch [4/5], Step [19425/68337], Loss: 5.0828\n",
      "Epoch [4/5], Step [19500/68337], Loss: 5.0693\n",
      "Epoch [4/5], Step [19575/68337], Loss: 5.0864\n",
      "Epoch [4/5], Step [19650/68337], Loss: 4.9918\n",
      "Epoch [4/5], Step [19725/68337], Loss: 4.9886\n",
      "Epoch [4/5], Step [19800/68337], Loss: 4.9922\n",
      "Epoch [4/5], Step [19875/68337], Loss: 5.1230\n",
      "Epoch [4/5], Step [19950/68337], Loss: 5.0214\n",
      "Validation perplexity: 148.48984981539357\n",
      "Epoch [4/5], Step [20025/68337], Loss: 5.0227\n",
      "Epoch [4/5], Step [20100/68337], Loss: 5.0421\n",
      "Epoch [4/5], Step [20175/68337], Loss: 5.1153\n",
      "Epoch [4/5], Step [20250/68337], Loss: 5.0254\n",
      "Epoch [4/5], Step [20325/68337], Loss: 5.0072\n",
      "Epoch [4/5], Step [20400/68337], Loss: 5.0777\n",
      "Epoch [4/5], Step [20475/68337], Loss: 5.0837\n",
      "Epoch [4/5], Step [20550/68337], Loss: 5.1466\n",
      "Epoch [4/5], Step [20625/68337], Loss: 5.1297\n",
      "Epoch [4/5], Step [20700/68337], Loss: 5.0376\n",
      "Epoch [4/5], Step [20775/68337], Loss: 4.9977\n",
      "Epoch [4/5], Step [20850/68337], Loss: 5.0760\n",
      "Epoch [4/5], Step [20925/68337], Loss: 5.0451\n",
      "Epoch [4/5], Step [21000/68337], Loss: 4.8668\n",
      "Epoch [4/5], Step [21075/68337], Loss: 5.1015\n",
      "Epoch [4/5], Step [21150/68337], Loss: 5.1627\n",
      "Epoch [4/5], Step [21225/68337], Loss: 5.0478\n",
      "Epoch [4/5], Step [21300/68337], Loss: 5.2401\n",
      "Epoch [4/5], Step [21375/68337], Loss: 4.9838\n",
      "Epoch [4/5], Step [21450/68337], Loss: 4.8678\n",
      "Epoch [4/5], Step [21525/68337], Loss: 5.0092\n",
      "Epoch [4/5], Step [21600/68337], Loss: 5.1463\n",
      "Epoch [4/5], Step [21675/68337], Loss: 5.1181\n",
      "Epoch [4/5], Step [21750/68337], Loss: 4.8849\n",
      "Epoch [4/5], Step [21825/68337], Loss: 4.9126\n",
      "Epoch [4/5], Step [21900/68337], Loss: 4.9769\n",
      "Epoch [4/5], Step [21975/68337], Loss: 4.9350\n",
      "Epoch [4/5], Step [22050/68337], Loss: 5.0852\n",
      "Epoch [4/5], Step [22125/68337], Loss: 5.0532\n",
      "Epoch [4/5], Step [22200/68337], Loss: 5.1061\n",
      "Epoch [4/5], Step [22275/68337], Loss: 5.0243\n",
      "Epoch [4/5], Step [22350/68337], Loss: 5.1388\n",
      "Epoch [4/5], Step [22425/68337], Loss: 5.0979\n",
      "Epoch [4/5], Step [22500/68337], Loss: 5.0036\n",
      "Epoch [4/5], Step [22575/68337], Loss: 4.8868\n",
      "Epoch [4/5], Step [22650/68337], Loss: 5.0719\n",
      "Epoch [4/5], Step [22725/68337], Loss: 4.9701\n",
      "Epoch [4/5], Step [22800/68337], Loss: 4.9408\n",
      "Epoch [4/5], Step [22875/68337], Loss: 5.0069\n",
      "Epoch [4/5], Step [22950/68337], Loss: 5.1082\n",
      "Epoch [4/5], Step [23025/68337], Loss: 5.1341\n",
      "Epoch [4/5], Step [23100/68337], Loss: 5.0295\n",
      "Epoch [4/5], Step [23175/68337], Loss: 5.0554\n",
      "Epoch [4/5], Step [23250/68337], Loss: 4.9970\n",
      "Epoch [4/5], Step [23325/68337], Loss: 4.9200\n",
      "Epoch [4/5], Step [23400/68337], Loss: 4.9734\n",
      "Epoch [4/5], Step [23475/68337], Loss: 5.0227\n",
      "Epoch [4/5], Step [23550/68337], Loss: 5.0624\n",
      "Epoch [4/5], Step [23625/68337], Loss: 4.9922\n",
      "Epoch [4/5], Step [23700/68337], Loss: 5.0269\n",
      "Epoch [4/5], Step [23775/68337], Loss: 4.9600\n",
      "Epoch [4/5], Step [23850/68337], Loss: 5.0207\n",
      "Epoch [4/5], Step [23925/68337], Loss: 5.1269\n",
      "Epoch [4/5], Step [24000/68337], Loss: 4.9405\n",
      "Epoch [4/5], Step [24075/68337], Loss: 4.9429\n",
      "Epoch [4/5], Step [24150/68337], Loss: 4.9093\n",
      "Epoch [4/5], Step [24225/68337], Loss: 4.8998\n",
      "Epoch [4/5], Step [24300/68337], Loss: 5.0337\n",
      "Epoch [4/5], Step [24375/68337], Loss: 5.0789\n",
      "Epoch [4/5], Step [24450/68337], Loss: 5.0312\n",
      "Epoch [4/5], Step [24525/68337], Loss: 5.1371\n",
      "Epoch [4/5], Step [24600/68337], Loss: 5.0306\n",
      "Epoch [4/5], Step [24675/68337], Loss: 5.0589\n",
      "Epoch [4/5], Step [24750/68337], Loss: 5.1211\n",
      "Epoch [4/5], Step [24825/68337], Loss: 5.1181\n",
      "Epoch [4/5], Step [24900/68337], Loss: 4.8997\n",
      "Epoch [4/5], Step [24975/68337], Loss: 5.0207\n",
      "Epoch [4/5], Step [25050/68337], Loss: 5.1089\n",
      "Epoch [4/5], Step [25125/68337], Loss: 5.0244\n",
      "Epoch [4/5], Step [25200/68337], Loss: 4.9992\n",
      "Epoch [4/5], Step [25275/68337], Loss: 5.0750\n",
      "Epoch [4/5], Step [25350/68337], Loss: 5.1484\n",
      "Epoch [4/5], Step [25425/68337], Loss: 4.9928\n",
      "Epoch [4/5], Step [25500/68337], Loss: 4.8776\n",
      "Epoch [4/5], Step [25575/68337], Loss: 5.1344\n",
      "Epoch [4/5], Step [25650/68337], Loss: 5.0014\n",
      "Epoch [4/5], Step [25725/68337], Loss: 5.1529\n",
      "Epoch [4/5], Step [25800/68337], Loss: 5.0490\n",
      "Epoch [4/5], Step [25875/68337], Loss: 4.9263\n",
      "Epoch [4/5], Step [25950/68337], Loss: 5.1719\n",
      "Epoch [4/5], Step [26025/68337], Loss: 5.1574\n",
      "Epoch [4/5], Step [26100/68337], Loss: 5.2126\n",
      "Epoch [4/5], Step [26175/68337], Loss: 4.9560\n",
      "Epoch [4/5], Step [26250/68337], Loss: 5.1228\n",
      "Epoch [4/5], Step [26325/68337], Loss: 4.9892\n",
      "Epoch [4/5], Step [26400/68337], Loss: 4.8822\n",
      "Epoch [4/5], Step [26475/68337], Loss: 4.9150\n",
      "Epoch [4/5], Step [26550/68337], Loss: 5.0163\n",
      "Epoch [4/5], Step [26625/68337], Loss: 5.0108\n",
      "Epoch [4/5], Step [26700/68337], Loss: 5.0272\n",
      "Epoch [4/5], Step [26775/68337], Loss: 5.0523\n",
      "Epoch [4/5], Step [26850/68337], Loss: 5.0897\n",
      "Epoch [4/5], Step [26925/68337], Loss: 4.9844\n",
      "Epoch [4/5], Step [27000/68337], Loss: 5.0605\n",
      "Epoch [4/5], Step [27075/68337], Loss: 5.0867\n",
      "Epoch [4/5], Step [27150/68337], Loss: 4.9354\n",
      "Epoch [4/5], Step [27225/68337], Loss: 5.0957\n",
      "Epoch [4/5], Step [27300/68337], Loss: 5.0676\n",
      "Epoch [4/5], Step [27375/68337], Loss: 4.9683\n",
      "Epoch [4/5], Step [27450/68337], Loss: 5.0803\n",
      "Epoch [4/5], Step [27525/68337], Loss: 5.0047\n",
      "Epoch [4/5], Step [27600/68337], Loss: 5.0245\n",
      "Epoch [4/5], Step [27675/68337], Loss: 5.2238\n",
      "Epoch [4/5], Step [27750/68337], Loss: 5.0235\n",
      "Epoch [4/5], Step [27825/68337], Loss: 5.0178\n",
      "Epoch [4/5], Step [27900/68337], Loss: 5.0525\n",
      "Epoch [4/5], Step [27975/68337], Loss: 5.0281\n",
      "Epoch [4/5], Step [28050/68337], Loss: 5.1066\n",
      "Epoch [4/5], Step [28125/68337], Loss: 5.0083\n",
      "Epoch [4/5], Step [28200/68337], Loss: 4.9456\n",
      "Epoch [4/5], Step [28275/68337], Loss: 4.9356\n",
      "Epoch [4/5], Step [28350/68337], Loss: 5.0247\n",
      "Epoch [4/5], Step [28425/68337], Loss: 5.0673\n",
      "Epoch [4/5], Step [28500/68337], Loss: 5.1127\n",
      "Epoch [4/5], Step [28575/68337], Loss: 4.9803\n",
      "Epoch [4/5], Step [28650/68337], Loss: 5.2898\n",
      "Epoch [4/5], Step [28725/68337], Loss: 4.9891\n",
      "Epoch [4/5], Step [28800/68337], Loss: 5.1001\n",
      "Epoch [4/5], Step [28875/68337], Loss: 5.1514\n",
      "Epoch [4/5], Step [28950/68337], Loss: 5.1722\n",
      "Epoch [4/5], Step [29025/68337], Loss: 4.9378\n",
      "Epoch [4/5], Step [29100/68337], Loss: 4.8906\n",
      "Epoch [4/5], Step [29175/68337], Loss: 4.9722\n",
      "Epoch [4/5], Step [29250/68337], Loss: 5.0519\n",
      "Epoch [4/5], Step [29325/68337], Loss: 4.9575\n",
      "Epoch [4/5], Step [29400/68337], Loss: 5.0944\n",
      "Epoch [4/5], Step [29475/68337], Loss: 5.1750\n",
      "Epoch [4/5], Step [29550/68337], Loss: 5.1109\n",
      "Epoch [4/5], Step [29625/68337], Loss: 5.2053\n",
      "Epoch [4/5], Step [29700/68337], Loss: 5.0004\n",
      "Epoch [4/5], Step [29775/68337], Loss: 4.9116\n",
      "Epoch [4/5], Step [29850/68337], Loss: 5.0992\n",
      "Epoch [4/5], Step [29925/68337], Loss: 4.8961\n",
      "Epoch [4/5], Step [30000/68337], Loss: 5.0639\n",
      "Validation perplexity: 147.96710882265413\n",
      "Epoch [4/5], Step [30075/68337], Loss: 4.9460\n",
      "Epoch [4/5], Step [30150/68337], Loss: 4.9574\n",
      "Epoch [4/5], Step [30225/68337], Loss: 4.9672\n",
      "Epoch [4/5], Step [30300/68337], Loss: 5.1385\n",
      "Epoch [4/5], Step [30375/68337], Loss: 5.1481\n",
      "Epoch [4/5], Step [30450/68337], Loss: 4.9722\n",
      "Epoch [4/5], Step [30525/68337], Loss: 5.0437\n",
      "Epoch [4/5], Step [30600/68337], Loss: 4.8797\n",
      "Epoch [4/5], Step [30675/68337], Loss: 5.0855\n",
      "Epoch [4/5], Step [30750/68337], Loss: 5.0111\n",
      "Epoch [4/5], Step [30825/68337], Loss: 4.8905\n",
      "Epoch [4/5], Step [30900/68337], Loss: 5.0902\n",
      "Epoch [4/5], Step [30975/68337], Loss: 4.9537\n",
      "Epoch [4/5], Step [31050/68337], Loss: 4.9247\n",
      "Epoch [4/5], Step [31125/68337], Loss: 4.9782\n",
      "Epoch [4/5], Step [31200/68337], Loss: 5.1679\n",
      "Epoch [4/5], Step [31275/68337], Loss: 5.1043\n",
      "Epoch [4/5], Step [31350/68337], Loss: 4.9934\n",
      "Epoch [4/5], Step [31425/68337], Loss: 5.0911\n",
      "Epoch [4/5], Step [31500/68337], Loss: 5.0873\n",
      "Epoch [4/5], Step [31575/68337], Loss: 5.0399\n",
      "Epoch [4/5], Step [31650/68337], Loss: 4.9337\n",
      "Epoch [4/5], Step [31725/68337], Loss: 5.0934\n",
      "Epoch [4/5], Step [31800/68337], Loss: 4.9314\n",
      "Epoch [4/5], Step [31875/68337], Loss: 5.0651\n",
      "Epoch [4/5], Step [31950/68337], Loss: 4.8592\n",
      "Epoch [4/5], Step [32025/68337], Loss: 5.0676\n",
      "Epoch [4/5], Step [32100/68337], Loss: 5.0502\n",
      "Epoch [4/5], Step [32175/68337], Loss: 5.1100\n",
      "Epoch [4/5], Step [32250/68337], Loss: 5.2030\n",
      "Epoch [4/5], Step [32325/68337], Loss: 5.1032\n",
      "Epoch [4/5], Step [32400/68337], Loss: 5.1728\n",
      "Epoch [4/5], Step [32475/68337], Loss: 4.9472\n",
      "Epoch [4/5], Step [32550/68337], Loss: 4.9057\n",
      "Epoch [4/5], Step [32625/68337], Loss: 5.1737\n",
      "Epoch [4/5], Step [32700/68337], Loss: 5.1330\n",
      "Epoch [4/5], Step [32775/68337], Loss: 5.1122\n",
      "Epoch [4/5], Step [32850/68337], Loss: 4.9923\n",
      "Epoch [4/5], Step [32925/68337], Loss: 5.1730\n",
      "Epoch [4/5], Step [33000/68337], Loss: 5.1891\n",
      "Epoch [4/5], Step [33075/68337], Loss: 4.9693\n",
      "Epoch [4/5], Step [33150/68337], Loss: 4.9072\n",
      "Epoch [4/5], Step [33225/68337], Loss: 5.1615\n",
      "Epoch [4/5], Step [33300/68337], Loss: 5.0566\n",
      "Epoch [4/5], Step [33375/68337], Loss: 5.0872\n",
      "Epoch [4/5], Step [33450/68337], Loss: 4.9957\n",
      "Epoch [4/5], Step [33525/68337], Loss: 4.9947\n",
      "Epoch [4/5], Step [33600/68337], Loss: 5.0656\n",
      "Epoch [4/5], Step [33675/68337], Loss: 5.0691\n",
      "Epoch [4/5], Step [33750/68337], Loss: 5.1191\n",
      "Epoch [4/5], Step [33825/68337], Loss: 5.0295\n",
      "Epoch [4/5], Step [33900/68337], Loss: 5.0838\n",
      "Epoch [4/5], Step [33975/68337], Loss: 4.9127\n",
      "Epoch [4/5], Step [34050/68337], Loss: 5.1424\n",
      "Epoch [4/5], Step [34125/68337], Loss: 4.7209\n",
      "Epoch [4/5], Step [34200/68337], Loss: 5.0273\n",
      "Epoch [4/5], Step [34275/68337], Loss: 5.0370\n",
      "Epoch [4/5], Step [34350/68337], Loss: 5.2032\n",
      "Epoch [4/5], Step [34425/68337], Loss: 5.1121\n",
      "Epoch [4/5], Step [34500/68337], Loss: 5.0121\n",
      "Epoch [4/5], Step [34575/68337], Loss: 5.0588\n",
      "Epoch [4/5], Step [34650/68337], Loss: 5.0224\n",
      "Epoch [4/5], Step [34725/68337], Loss: 5.0435\n",
      "Epoch [4/5], Step [34800/68337], Loss: 5.0836\n",
      "Epoch [4/5], Step [34875/68337], Loss: 4.9193\n",
      "Epoch [4/5], Step [34950/68337], Loss: 5.0864\n",
      "Epoch [4/5], Step [35025/68337], Loss: 5.1491\n",
      "Epoch [4/5], Step [35100/68337], Loss: 5.0606\n",
      "Epoch [4/5], Step [35175/68337], Loss: 4.9919\n",
      "Epoch [4/5], Step [35250/68337], Loss: 4.9764\n",
      "Epoch [4/5], Step [35325/68337], Loss: 4.9377\n",
      "Epoch [4/5], Step [35400/68337], Loss: 5.0784\n",
      "Epoch [4/5], Step [35475/68337], Loss: 4.9973\n",
      "Epoch [4/5], Step [35550/68337], Loss: 5.0448\n",
      "Epoch [4/5], Step [35625/68337], Loss: 5.0030\n",
      "Epoch [4/5], Step [35700/68337], Loss: 5.1101\n",
      "Epoch [4/5], Step [35775/68337], Loss: 5.0422\n",
      "Epoch [4/5], Step [35850/68337], Loss: 5.0304\n",
      "Epoch [4/5], Step [35925/68337], Loss: 5.0945\n",
      "Epoch [4/5], Step [36000/68337], Loss: 4.9548\n",
      "Epoch [4/5], Step [36075/68337], Loss: 5.0417\n",
      "Epoch [4/5], Step [36150/68337], Loss: 5.0382\n",
      "Epoch [4/5], Step [36225/68337], Loss: 5.1423\n",
      "Epoch [4/5], Step [36300/68337], Loss: 5.0104\n",
      "Epoch [4/5], Step [36375/68337], Loss: 5.0682\n",
      "Epoch [4/5], Step [36450/68337], Loss: 4.9144\n",
      "Epoch [4/5], Step [36525/68337], Loss: 5.0989\n",
      "Epoch [4/5], Step [36600/68337], Loss: 5.0271\n",
      "Epoch [4/5], Step [36675/68337], Loss: 4.8588\n",
      "Epoch [4/5], Step [36750/68337], Loss: 5.0640\n",
      "Epoch [4/5], Step [36825/68337], Loss: 5.0055\n",
      "Epoch [4/5], Step [36900/68337], Loss: 5.0147\n",
      "Epoch [4/5], Step [36975/68337], Loss: 5.0835\n",
      "Epoch [4/5], Step [37050/68337], Loss: 5.1071\n",
      "Epoch [4/5], Step [37125/68337], Loss: 4.8766\n",
      "Epoch [4/5], Step [37200/68337], Loss: 5.0845\n",
      "Epoch [4/5], Step [37275/68337], Loss: 4.8404\n",
      "Epoch [4/5], Step [37350/68337], Loss: 5.0319\n",
      "Epoch [4/5], Step [37425/68337], Loss: 5.0839\n",
      "Epoch [4/5], Step [37500/68337], Loss: 5.1156\n",
      "Epoch [4/5], Step [37575/68337], Loss: 4.9878\n",
      "Epoch [4/5], Step [37650/68337], Loss: 5.2462\n",
      "Epoch [4/5], Step [37725/68337], Loss: 5.0521\n",
      "Epoch [4/5], Step [37800/68337], Loss: 5.1567\n",
      "Epoch [4/5], Step [37875/68337], Loss: 4.9975\n",
      "Epoch [4/5], Step [37950/68337], Loss: 5.1456\n",
      "Epoch [4/5], Step [38025/68337], Loss: 5.0989\n",
      "Epoch [4/5], Step [38100/68337], Loss: 5.1344\n",
      "Epoch [4/5], Step [38175/68337], Loss: 4.8547\n",
      "Epoch [4/5], Step [38250/68337], Loss: 5.0899\n",
      "Epoch [4/5], Step [38325/68337], Loss: 5.0128\n",
      "Epoch [4/5], Step [38400/68337], Loss: 4.9849\n",
      "Epoch [4/5], Step [38475/68337], Loss: 4.9899\n",
      "Epoch [4/5], Step [38550/68337], Loss: 5.1530\n",
      "Epoch [4/5], Step [38625/68337], Loss: 5.0546\n",
      "Epoch [4/5], Step [38700/68337], Loss: 5.1080\n",
      "Epoch [4/5], Step [38775/68337], Loss: 4.9783\n",
      "Epoch [4/5], Step [38850/68337], Loss: 5.0104\n",
      "Epoch [4/5], Step [38925/68337], Loss: 4.9141\n",
      "Epoch [4/5], Step [39000/68337], Loss: 5.1160\n",
      "Epoch [4/5], Step [39075/68337], Loss: 5.0915\n",
      "Epoch [4/5], Step [39150/68337], Loss: 5.0632\n",
      "Epoch [4/5], Step [39225/68337], Loss: 4.9462\n",
      "Epoch [4/5], Step [39300/68337], Loss: 5.0659\n",
      "Epoch [4/5], Step [39375/68337], Loss: 4.9840\n",
      "Epoch [4/5], Step [39450/68337], Loss: 4.9491\n",
      "Epoch [4/5], Step [39525/68337], Loss: 5.0446\n",
      "Epoch [4/5], Step [39600/68337], Loss: 4.9926\n",
      "Epoch [4/5], Step [39675/68337], Loss: 5.0082\n",
      "Epoch [4/5], Step [39750/68337], Loss: 5.0275\n",
      "Epoch [4/5], Step [39825/68337], Loss: 5.1153\n",
      "Epoch [4/5], Step [39900/68337], Loss: 5.0607\n",
      "Epoch [4/5], Step [39975/68337], Loss: 4.9172\n",
      "Validation perplexity: 147.79490162939055\n",
      "Epoch [4/5], Step [40050/68337], Loss: 5.0163\n",
      "Epoch [4/5], Step [40125/68337], Loss: 4.9227\n",
      "Epoch [4/5], Step [40200/68337], Loss: 5.0559\n",
      "Epoch [4/5], Step [40275/68337], Loss: 5.0311\n",
      "Epoch [4/5], Step [40350/68337], Loss: 5.0672\n",
      "Epoch [4/5], Step [40425/68337], Loss: 4.8684\n",
      "Epoch [4/5], Step [40500/68337], Loss: 5.0018\n",
      "Epoch [4/5], Step [40575/68337], Loss: 4.8974\n",
      "Epoch [4/5], Step [40650/68337], Loss: 5.0567\n",
      "Epoch [4/5], Step [40725/68337], Loss: 5.0024\n",
      "Epoch [4/5], Step [40800/68337], Loss: 4.9981\n",
      "Epoch [4/5], Step [40875/68337], Loss: 5.1530\n",
      "Epoch [4/5], Step [40950/68337], Loss: 5.0471\n",
      "Epoch [4/5], Step [41025/68337], Loss: 5.0756\n",
      "Epoch [4/5], Step [41100/68337], Loss: 5.1186\n",
      "Epoch [4/5], Step [41175/68337], Loss: 5.0345\n",
      "Epoch [4/5], Step [41250/68337], Loss: 4.8619\n",
      "Epoch [4/5], Step [41325/68337], Loss: 4.9142\n",
      "Epoch [4/5], Step [41400/68337], Loss: 5.2800\n",
      "Epoch [4/5], Step [41475/68337], Loss: 4.8993\n",
      "Epoch [4/5], Step [41550/68337], Loss: 5.1915\n",
      "Epoch [4/5], Step [41625/68337], Loss: 5.0961\n",
      "Epoch [4/5], Step [41700/68337], Loss: 4.9451\n",
      "Epoch [4/5], Step [41775/68337], Loss: 5.0409\n",
      "Epoch [4/5], Step [41850/68337], Loss: 5.0965\n",
      "Epoch [4/5], Step [41925/68337], Loss: 4.9012\n",
      "Epoch [4/5], Step [42000/68337], Loss: 5.0150\n",
      "Epoch [4/5], Step [42075/68337], Loss: 4.9715\n",
      "Epoch [4/5], Step [42150/68337], Loss: 5.0013\n",
      "Epoch [4/5], Step [42225/68337], Loss: 4.8133\n",
      "Epoch [4/5], Step [42300/68337], Loss: 5.1125\n",
      "Epoch [4/5], Step [42375/68337], Loss: 5.1306\n",
      "Epoch [4/5], Step [42450/68337], Loss: 5.0121\n",
      "Epoch [4/5], Step [42525/68337], Loss: 5.0790\n",
      "Epoch [4/5], Step [42600/68337], Loss: 4.9693\n",
      "Epoch [4/5], Step [42675/68337], Loss: 5.0119\n",
      "Epoch [4/5], Step [42750/68337], Loss: 5.0617\n",
      "Epoch [4/5], Step [42825/68337], Loss: 4.9839\n",
      "Epoch [4/5], Step [42900/68337], Loss: 5.1495\n",
      "Epoch [4/5], Step [42975/68337], Loss: 5.1288\n",
      "Epoch [4/5], Step [43050/68337], Loss: 5.0515\n",
      "Epoch [4/5], Step [43125/68337], Loss: 5.0300\n",
      "Epoch [4/5], Step [43200/68337], Loss: 5.0380\n",
      "Epoch [4/5], Step [43275/68337], Loss: 4.9344\n",
      "Epoch [4/5], Step [43350/68337], Loss: 4.9224\n",
      "Epoch [4/5], Step [43425/68337], Loss: 4.9122\n",
      "Epoch [4/5], Step [43500/68337], Loss: 4.9692\n",
      "Epoch [4/5], Step [43575/68337], Loss: 5.0556\n",
      "Epoch [4/5], Step [43650/68337], Loss: 5.1120\n",
      "Epoch [4/5], Step [43725/68337], Loss: 5.0743\n",
      "Epoch [4/5], Step [43800/68337], Loss: 4.9812\n",
      "Epoch [4/5], Step [43875/68337], Loss: 5.0179\n",
      "Epoch [4/5], Step [43950/68337], Loss: 5.0143\n",
      "Epoch [4/5], Step [44025/68337], Loss: 4.7660\n",
      "Epoch [4/5], Step [44100/68337], Loss: 5.0600\n",
      "Epoch [4/5], Step [44175/68337], Loss: 5.0336\n",
      "Epoch [4/5], Step [44250/68337], Loss: 5.0127\n",
      "Epoch [4/5], Step [44325/68337], Loss: 4.9791\n",
      "Epoch [4/5], Step [44400/68337], Loss: 5.0041\n",
      "Epoch [4/5], Step [44475/68337], Loss: 5.0670\n",
      "Epoch [4/5], Step [44550/68337], Loss: 4.9524\n",
      "Epoch [4/5], Step [44625/68337], Loss: 5.1497\n",
      "Epoch [4/5], Step [44700/68337], Loss: 5.0425\n",
      "Epoch [4/5], Step [44775/68337], Loss: 5.1897\n",
      "Epoch [4/5], Step [44850/68337], Loss: 4.9579\n",
      "Epoch [4/5], Step [44925/68337], Loss: 5.0533\n",
      "Epoch [4/5], Step [45000/68337], Loss: 5.1144\n",
      "Epoch [4/5], Step [45075/68337], Loss: 5.1936\n",
      "Epoch [4/5], Step [45150/68337], Loss: 5.0733\n",
      "Epoch [4/5], Step [45225/68337], Loss: 5.1324\n",
      "Epoch [4/5], Step [45300/68337], Loss: 5.0736\n",
      "Epoch [4/5], Step [45375/68337], Loss: 5.0278\n",
      "Epoch [4/5], Step [45450/68337], Loss: 5.0657\n",
      "Epoch [4/5], Step [45525/68337], Loss: 4.9348\n",
      "Epoch [4/5], Step [45600/68337], Loss: 4.8488\n",
      "Epoch [4/5], Step [45675/68337], Loss: 5.0430\n",
      "Epoch [4/5], Step [45750/68337], Loss: 4.9809\n",
      "Epoch [4/5], Step [45825/68337], Loss: 5.0601\n",
      "Epoch [4/5], Step [45900/68337], Loss: 5.1353\n",
      "Epoch [4/5], Step [45975/68337], Loss: 4.9957\n",
      "Epoch [4/5], Step [46050/68337], Loss: 5.1457\n",
      "Epoch [4/5], Step [46125/68337], Loss: 4.9946\n",
      "Epoch [4/5], Step [46200/68337], Loss: 5.1469\n",
      "Epoch [4/5], Step [46275/68337], Loss: 4.9590\n",
      "Epoch [4/5], Step [46350/68337], Loss: 5.2097\n",
      "Epoch [4/5], Step [46425/68337], Loss: 5.2115\n",
      "Epoch [4/5], Step [46500/68337], Loss: 4.8606\n",
      "Epoch [4/5], Step [46575/68337], Loss: 5.0736\n",
      "Epoch [4/5], Step [46650/68337], Loss: 5.1135\n",
      "Epoch [4/5], Step [46725/68337], Loss: 5.1023\n",
      "Epoch [4/5], Step [46800/68337], Loss: 4.9178\n",
      "Epoch [4/5], Step [46875/68337], Loss: 5.0895\n",
      "Epoch [4/5], Step [46950/68337], Loss: 4.9903\n",
      "Epoch [4/5], Step [47025/68337], Loss: 4.8728\n",
      "Epoch [4/5], Step [47100/68337], Loss: 4.9285\n",
      "Epoch [4/5], Step [47175/68337], Loss: 4.9096\n",
      "Epoch [4/5], Step [47250/68337], Loss: 5.0653\n",
      "Epoch [4/5], Step [47325/68337], Loss: 4.8115\n",
      "Epoch [4/5], Step [47400/68337], Loss: 4.8393\n",
      "Epoch [4/5], Step [47475/68337], Loss: 5.1413\n",
      "Epoch [4/5], Step [47550/68337], Loss: 4.9347\n",
      "Epoch [4/5], Step [47625/68337], Loss: 5.2134\n",
      "Epoch [4/5], Step [47700/68337], Loss: 4.9650\n",
      "Epoch [4/5], Step [47775/68337], Loss: 4.9059\n",
      "Epoch [4/5], Step [47850/68337], Loss: 5.1648\n",
      "Epoch [4/5], Step [47925/68337], Loss: 5.0217\n",
      "Epoch [4/5], Step [48000/68337], Loss: 4.9452\n",
      "Epoch [4/5], Step [48075/68337], Loss: 5.0906\n",
      "Epoch [4/5], Step [48150/68337], Loss: 5.0926\n",
      "Epoch [4/5], Step [48225/68337], Loss: 4.9825\n",
      "Epoch [4/5], Step [48300/68337], Loss: 4.9701\n",
      "Epoch [4/5], Step [48375/68337], Loss: 5.0552\n",
      "Epoch [4/5], Step [48450/68337], Loss: 5.1286\n",
      "Epoch [4/5], Step [48525/68337], Loss: 5.0184\n",
      "Epoch [4/5], Step [48600/68337], Loss: 5.0820\n",
      "Epoch [4/5], Step [48675/68337], Loss: 5.0450\n",
      "Epoch [4/5], Step [48750/68337], Loss: 5.0248\n",
      "Epoch [4/5], Step [48825/68337], Loss: 5.0404\n",
      "Epoch [4/5], Step [48900/68337], Loss: 4.9001\n",
      "Epoch [4/5], Step [48975/68337], Loss: 4.9463\n",
      "Epoch [4/5], Step [49050/68337], Loss: 5.0670\n",
      "Epoch [4/5], Step [49125/68337], Loss: 4.8497\n",
      "Epoch [4/5], Step [49200/68337], Loss: 5.0234\n",
      "Epoch [4/5], Step [49275/68337], Loss: 5.0265\n",
      "Epoch [4/5], Step [49350/68337], Loss: 5.0951\n",
      "Epoch [4/5], Step [49425/68337], Loss: 5.0695\n",
      "Epoch [4/5], Step [49500/68337], Loss: 5.0375\n",
      "Epoch [4/5], Step [49575/68337], Loss: 4.9866\n",
      "Epoch [4/5], Step [49650/68337], Loss: 4.9425\n",
      "Epoch [4/5], Step [49725/68337], Loss: 5.0788\n",
      "Epoch [4/5], Step [49800/68337], Loss: 5.1860\n",
      "Epoch [4/5], Step [49875/68337], Loss: 5.1115\n",
      "Epoch [4/5], Step [49950/68337], Loss: 5.1435\n",
      "Validation perplexity: 147.4119506291335\n",
      "Epoch [4/5], Step [50025/68337], Loss: 5.0995\n",
      "Epoch [4/5], Step [50100/68337], Loss: 5.1584\n",
      "Epoch [4/5], Step [50175/68337], Loss: 5.0987\n",
      "Epoch [4/5], Step [50250/68337], Loss: 4.9175\n",
      "Epoch [4/5], Step [50325/68337], Loss: 4.8986\n",
      "Epoch [4/5], Step [50400/68337], Loss: 5.0046\n",
      "Epoch [4/5], Step [50475/68337], Loss: 5.0584\n",
      "Epoch [4/5], Step [50550/68337], Loss: 5.0149\n",
      "Epoch [4/5], Step [50625/68337], Loss: 4.9920\n",
      "Epoch [4/5], Step [50700/68337], Loss: 4.9888\n",
      "Epoch [4/5], Step [50775/68337], Loss: 5.0927\n",
      "Epoch [4/5], Step [50850/68337], Loss: 4.9525\n",
      "Epoch [4/5], Step [50925/68337], Loss: 4.9952\n",
      "Epoch [4/5], Step [51000/68337], Loss: 4.9975\n",
      "Epoch [4/5], Step [51075/68337], Loss: 4.9460\n",
      "Epoch [4/5], Step [51150/68337], Loss: 5.0188\n",
      "Epoch [4/5], Step [51225/68337], Loss: 4.9973\n",
      "Epoch [4/5], Step [51300/68337], Loss: 5.0208\n",
      "Epoch [4/5], Step [51375/68337], Loss: 4.9242\n",
      "Epoch [4/5], Step [51450/68337], Loss: 5.0872\n",
      "Epoch [4/5], Step [51525/68337], Loss: 5.2151\n",
      "Epoch [4/5], Step [51600/68337], Loss: 5.1270\n",
      "Epoch [4/5], Step [51675/68337], Loss: 4.9296\n",
      "Epoch [4/5], Step [51750/68337], Loss: 5.0080\n",
      "Epoch [4/5], Step [51825/68337], Loss: 5.0409\n",
      "Epoch [4/5], Step [51900/68337], Loss: 5.1263\n",
      "Epoch [4/5], Step [51975/68337], Loss: 5.1135\n",
      "Epoch [4/5], Step [52050/68337], Loss: 5.0897\n",
      "Epoch [4/5], Step [52125/68337], Loss: 5.1912\n",
      "Epoch [4/5], Step [52200/68337], Loss: 5.1272\n",
      "Epoch [4/5], Step [52275/68337], Loss: 4.9500\n",
      "Epoch [4/5], Step [52350/68337], Loss: 4.9293\n",
      "Epoch [4/5], Step [52425/68337], Loss: 5.1273\n",
      "Epoch [4/5], Step [52500/68337], Loss: 4.9987\n",
      "Epoch [4/5], Step [52575/68337], Loss: 5.0555\n",
      "Epoch [4/5], Step [52650/68337], Loss: 5.0353\n",
      "Epoch [4/5], Step [52725/68337], Loss: 5.0630\n",
      "Epoch [4/5], Step [52800/68337], Loss: 4.9704\n",
      "Epoch [4/5], Step [52875/68337], Loss: 5.0125\n",
      "Epoch [4/5], Step [52950/68337], Loss: 4.9966\n",
      "Epoch [4/5], Step [53025/68337], Loss: 4.9808\n",
      "Epoch [4/5], Step [53100/68337], Loss: 5.0066\n",
      "Epoch [4/5], Step [53175/68337], Loss: 4.9912\n",
      "Epoch [4/5], Step [53250/68337], Loss: 5.0594\n",
      "Epoch [4/5], Step [53325/68337], Loss: 5.1065\n",
      "Epoch [4/5], Step [53400/68337], Loss: 5.0661\n",
      "Epoch [4/5], Step [53475/68337], Loss: 4.9979\n",
      "Epoch [4/5], Step [53550/68337], Loss: 5.0292\n",
      "Epoch [4/5], Step [53625/68337], Loss: 4.9799\n",
      "Epoch [4/5], Step [53700/68337], Loss: 5.0241\n",
      "Epoch [4/5], Step [53775/68337], Loss: 5.0069\n",
      "Epoch [4/5], Step [53850/68337], Loss: 5.0186\n",
      "Epoch [4/5], Step [53925/68337], Loss: 5.1157\n",
      "Epoch [4/5], Step [54000/68337], Loss: 5.0215\n",
      "Epoch [4/5], Step [54075/68337], Loss: 5.0088\n",
      "Epoch [4/5], Step [54150/68337], Loss: 4.8706\n",
      "Epoch [4/5], Step [54225/68337], Loss: 4.9911\n",
      "Epoch [4/5], Step [54300/68337], Loss: 4.8792\n",
      "Epoch [4/5], Step [54375/68337], Loss: 5.2312\n",
      "Epoch [4/5], Step [54450/68337], Loss: 5.0652\n",
      "Epoch [4/5], Step [54525/68337], Loss: 4.9379\n",
      "Epoch [4/5], Step [54600/68337], Loss: 5.0345\n",
      "Epoch [4/5], Step [54675/68337], Loss: 5.1582\n",
      "Epoch [4/5], Step [54750/68337], Loss: 5.0252\n",
      "Epoch [4/5], Step [54825/68337], Loss: 4.9717\n",
      "Epoch [4/5], Step [54900/68337], Loss: 5.0291\n",
      "Epoch [4/5], Step [54975/68337], Loss: 5.0580\n",
      "Epoch [4/5], Step [55050/68337], Loss: 5.1475\n",
      "Epoch [4/5], Step [55125/68337], Loss: 4.9599\n",
      "Epoch [4/5], Step [55200/68337], Loss: 4.8707\n",
      "Epoch [4/5], Step [55275/68337], Loss: 5.0063\n",
      "Epoch [4/5], Step [55350/68337], Loss: 4.9854\n",
      "Epoch [4/5], Step [55425/68337], Loss: 5.0828\n",
      "Epoch [4/5], Step [55500/68337], Loss: 5.0005\n",
      "Epoch [4/5], Step [55575/68337], Loss: 5.0608\n",
      "Epoch [4/5], Step [55650/68337], Loss: 5.0118\n",
      "Epoch [4/5], Step [55725/68337], Loss: 5.0745\n",
      "Epoch [4/5], Step [55800/68337], Loss: 5.0051\n",
      "Epoch [4/5], Step [55875/68337], Loss: 5.2563\n",
      "Epoch [4/5], Step [55950/68337], Loss: 5.0169\n",
      "Epoch [4/5], Step [56025/68337], Loss: 4.8666\n",
      "Epoch [4/5], Step [56100/68337], Loss: 4.9310\n",
      "Epoch [4/5], Step [56175/68337], Loss: 4.9923\n",
      "Epoch [4/5], Step [56250/68337], Loss: 5.1109\n",
      "Epoch [4/5], Step [56325/68337], Loss: 4.9874\n",
      "Epoch [4/5], Step [56400/68337], Loss: 4.9705\n",
      "Epoch [4/5], Step [56475/68337], Loss: 5.0818\n",
      "Epoch [4/5], Step [56550/68337], Loss: 5.0325\n",
      "Epoch [4/5], Step [56625/68337], Loss: 5.2019\n",
      "Epoch [4/5], Step [56700/68337], Loss: 5.0263\n",
      "Epoch [4/5], Step [56775/68337], Loss: 5.0100\n",
      "Epoch [4/5], Step [56850/68337], Loss: 4.9835\n",
      "Epoch [4/5], Step [56925/68337], Loss: 5.1674\n",
      "Epoch [4/5], Step [57000/68337], Loss: 5.1710\n",
      "Epoch [4/5], Step [57075/68337], Loss: 5.0160\n",
      "Epoch [4/5], Step [57150/68337], Loss: 5.1321\n",
      "Epoch [4/5], Step [57225/68337], Loss: 5.0426\n",
      "Epoch [4/5], Step [57300/68337], Loss: 5.1839\n",
      "Epoch [4/5], Step [57375/68337], Loss: 5.0162\n",
      "Epoch [4/5], Step [57450/68337], Loss: 5.0326\n",
      "Epoch [4/5], Step [57525/68337], Loss: 4.8922\n",
      "Epoch [4/5], Step [57600/68337], Loss: 5.1099\n",
      "Epoch [4/5], Step [57675/68337], Loss: 4.8909\n",
      "Epoch [4/5], Step [57750/68337], Loss: 4.8004\n",
      "Epoch [4/5], Step [57825/68337], Loss: 5.0506\n",
      "Epoch [4/5], Step [57900/68337], Loss: 5.0272\n",
      "Epoch [4/5], Step [57975/68337], Loss: 5.1038\n",
      "Epoch [4/5], Step [58050/68337], Loss: 5.1171\n",
      "Epoch [4/5], Step [58125/68337], Loss: 5.1208\n",
      "Epoch [4/5], Step [58200/68337], Loss: 4.9118\n",
      "Epoch [4/5], Step [58275/68337], Loss: 5.0697\n",
      "Epoch [4/5], Step [58350/68337], Loss: 4.9490\n",
      "Epoch [4/5], Step [58425/68337], Loss: 5.0608\n",
      "Epoch [4/5], Step [58500/68337], Loss: 5.0305\n",
      "Epoch [4/5], Step [58575/68337], Loss: 5.1406\n",
      "Epoch [4/5], Step [58650/68337], Loss: 5.0745\n",
      "Epoch [4/5], Step [58725/68337], Loss: 5.1202\n",
      "Epoch [4/5], Step [58800/68337], Loss: 4.9560\n",
      "Epoch [4/5], Step [58875/68337], Loss: 5.1024\n",
      "Epoch [4/5], Step [58950/68337], Loss: 4.9967\n",
      "Epoch [4/5], Step [59025/68337], Loss: 5.1127\n",
      "Epoch [4/5], Step [59100/68337], Loss: 5.0346\n",
      "Epoch [4/5], Step [59175/68337], Loss: 5.0165\n",
      "Epoch [4/5], Step [59250/68337], Loss: 5.0768\n",
      "Epoch [4/5], Step [59325/68337], Loss: 5.1908\n",
      "Epoch [4/5], Step [59400/68337], Loss: 5.0064\n",
      "Epoch [4/5], Step [59475/68337], Loss: 4.8700\n",
      "Epoch [4/5], Step [59550/68337], Loss: 5.1033\n",
      "Epoch [4/5], Step [59625/68337], Loss: 5.0320\n",
      "Epoch [4/5], Step [59700/68337], Loss: 4.8939\n",
      "Epoch [4/5], Step [59775/68337], Loss: 4.9112\n",
      "Epoch [4/5], Step [59850/68337], Loss: 5.0999\n",
      "Epoch [4/5], Step [59925/68337], Loss: 5.0260\n",
      "Epoch [4/5], Step [60000/68337], Loss: 4.9842\n",
      "Validation perplexity: 147.15371318177174\n",
      "Epoch [4/5], Step [60075/68337], Loss: 4.9579\n",
      "Epoch [4/5], Step [60150/68337], Loss: 5.1236\n",
      "Epoch [4/5], Step [60225/68337], Loss: 4.8506\n",
      "Epoch [4/5], Step [60300/68337], Loss: 5.0409\n",
      "Epoch [4/5], Step [60375/68337], Loss: 5.1057\n",
      "Epoch [4/5], Step [60450/68337], Loss: 4.9984\n",
      "Epoch [4/5], Step [60525/68337], Loss: 4.9581\n",
      "Epoch [4/5], Step [60600/68337], Loss: 5.0684\n",
      "Epoch [4/5], Step [60675/68337], Loss: 5.0207\n",
      "Epoch [4/5], Step [60750/68337], Loss: 5.0283\n",
      "Epoch [4/5], Step [60825/68337], Loss: 4.9910\n",
      "Epoch [4/5], Step [60900/68337], Loss: 5.1502\n",
      "Epoch [4/5], Step [60975/68337], Loss: 5.0569\n",
      "Epoch [4/5], Step [61050/68337], Loss: 5.0789\n",
      "Epoch [4/5], Step [61125/68337], Loss: 4.9584\n",
      "Epoch [4/5], Step [61200/68337], Loss: 5.0853\n",
      "Epoch [4/5], Step [61275/68337], Loss: 5.0404\n",
      "Epoch [4/5], Step [61350/68337], Loss: 5.0340\n",
      "Epoch [4/5], Step [61425/68337], Loss: 5.1215\n",
      "Epoch [4/5], Step [61500/68337], Loss: 5.0935\n",
      "Epoch [4/5], Step [61575/68337], Loss: 4.9419\n",
      "Epoch [4/5], Step [61650/68337], Loss: 5.0056\n",
      "Epoch [4/5], Step [61725/68337], Loss: 5.1047\n",
      "Epoch [4/5], Step [61800/68337], Loss: 4.9062\n",
      "Epoch [4/5], Step [61875/68337], Loss: 5.0922\n",
      "Epoch [4/5], Step [61950/68337], Loss: 5.0835\n",
      "Epoch [4/5], Step [62025/68337], Loss: 5.0733\n",
      "Epoch [4/5], Step [62100/68337], Loss: 5.0336\n",
      "Epoch [4/5], Step [62175/68337], Loss: 4.9820\n",
      "Epoch [4/5], Step [62250/68337], Loss: 5.1048\n",
      "Epoch [4/5], Step [62325/68337], Loss: 5.0677\n",
      "Epoch [4/5], Step [62400/68337], Loss: 5.0161\n",
      "Epoch [4/5], Step [62475/68337], Loss: 4.9983\n",
      "Epoch [4/5], Step [62550/68337], Loss: 5.0434\n",
      "Epoch [4/5], Step [62625/68337], Loss: 5.0541\n",
      "Epoch [4/5], Step [62700/68337], Loss: 4.9636\n",
      "Epoch [4/5], Step [62775/68337], Loss: 5.0944\n",
      "Epoch [4/5], Step [62850/68337], Loss: 5.0844\n",
      "Epoch [4/5], Step [62925/68337], Loss: 5.0180\n",
      "Epoch [4/5], Step [63000/68337], Loss: 5.0783\n",
      "Epoch [4/5], Step [63075/68337], Loss: 4.9433\n",
      "Epoch [4/5], Step [63150/68337], Loss: 4.9301\n",
      "Epoch [4/5], Step [63225/68337], Loss: 4.9221\n",
      "Epoch [4/5], Step [63300/68337], Loss: 4.9264\n",
      "Epoch [4/5], Step [63375/68337], Loss: 5.0104\n",
      "Epoch [4/5], Step [63450/68337], Loss: 5.1343\n",
      "Epoch [4/5], Step [63525/68337], Loss: 5.0485\n",
      "Epoch [4/5], Step [63600/68337], Loss: 4.9715\n",
      "Epoch [4/5], Step [63675/68337], Loss: 4.9893\n",
      "Epoch [4/5], Step [63750/68337], Loss: 4.8946\n",
      "Epoch [4/5], Step [63825/68337], Loss: 5.0833\n",
      "Epoch [4/5], Step [63900/68337], Loss: 5.1821\n",
      "Epoch [4/5], Step [63975/68337], Loss: 4.9956\n",
      "Epoch [4/5], Step [64050/68337], Loss: 4.9143\n",
      "Epoch [4/5], Step [64125/68337], Loss: 5.1618\n",
      "Epoch [4/5], Step [64200/68337], Loss: 4.9709\n",
      "Epoch [4/5], Step [64275/68337], Loss: 4.9730\n",
      "Epoch [4/5], Step [64350/68337], Loss: 5.1377\n",
      "Epoch [4/5], Step [64425/68337], Loss: 5.1149\n",
      "Epoch [4/5], Step [64500/68337], Loss: 5.2315\n",
      "Epoch [4/5], Step [64575/68337], Loss: 5.0223\n",
      "Epoch [4/5], Step [64650/68337], Loss: 4.8352\n",
      "Epoch [4/5], Step [64725/68337], Loss: 4.9239\n",
      "Epoch [4/5], Step [64800/68337], Loss: 5.1327\n",
      "Epoch [4/5], Step [64875/68337], Loss: 5.1913\n",
      "Epoch [4/5], Step [64950/68337], Loss: 4.9997\n",
      "Epoch [4/5], Step [65025/68337], Loss: 4.9037\n",
      "Epoch [4/5], Step [65100/68337], Loss: 5.0004\n",
      "Epoch [4/5], Step [65175/68337], Loss: 4.9239\n",
      "Epoch [4/5], Step [65250/68337], Loss: 4.9184\n",
      "Epoch [4/5], Step [65325/68337], Loss: 5.1225\n",
      "Epoch [4/5], Step [65400/68337], Loss: 4.9922\n",
      "Epoch [4/5], Step [65475/68337], Loss: 4.9169\n",
      "Epoch [4/5], Step [65550/68337], Loss: 4.9523\n",
      "Epoch [4/5], Step [65625/68337], Loss: 5.0632\n",
      "Epoch [4/5], Step [65700/68337], Loss: 5.0349\n",
      "Epoch [4/5], Step [65775/68337], Loss: 5.2062\n",
      "Epoch [4/5], Step [65850/68337], Loss: 5.0098\n",
      "Epoch [4/5], Step [65925/68337], Loss: 4.9996\n",
      "Epoch [4/5], Step [66000/68337], Loss: 5.0688\n",
      "Epoch [4/5], Step [66075/68337], Loss: 4.8971\n",
      "Epoch [4/5], Step [66150/68337], Loss: 4.9627\n",
      "Epoch [4/5], Step [66225/68337], Loss: 4.9850\n",
      "Epoch [4/5], Step [66300/68337], Loss: 4.9839\n",
      "Epoch [4/5], Step [66375/68337], Loss: 5.0124\n",
      "Epoch [4/5], Step [66450/68337], Loss: 4.9199\n",
      "Epoch [4/5], Step [66525/68337], Loss: 5.0260\n",
      "Epoch [4/5], Step [66600/68337], Loss: 5.1008\n",
      "Epoch [4/5], Step [66675/68337], Loss: 5.0801\n",
      "Epoch [4/5], Step [66750/68337], Loss: 5.0270\n",
      "Epoch [4/5], Step [66825/68337], Loss: 5.0054\n",
      "Epoch [4/5], Step [66900/68337], Loss: 5.0996\n",
      "Epoch [4/5], Step [66975/68337], Loss: 4.8877\n",
      "Epoch [4/5], Step [67050/68337], Loss: 5.0653\n",
      "Epoch [4/5], Step [67125/68337], Loss: 4.9545\n",
      "Epoch [4/5], Step [67200/68337], Loss: 5.0034\n",
      "Epoch [4/5], Step [67275/68337], Loss: 4.9729\n",
      "Epoch [4/5], Step [67350/68337], Loss: 5.0472\n",
      "Epoch [4/5], Step [67425/68337], Loss: 5.0606\n",
      "Epoch [4/5], Step [67500/68337], Loss: 5.0759\n",
      "Epoch [4/5], Step [67575/68337], Loss: 5.1977\n",
      "Epoch [4/5], Step [67650/68337], Loss: 5.1698\n",
      "Epoch [4/5], Step [67725/68337], Loss: 4.9252\n",
      "Epoch [4/5], Step [67800/68337], Loss: 5.0235\n",
      "Epoch [4/5], Step [67875/68337], Loss: 4.9539\n",
      "Epoch [4/5], Step [67950/68337], Loss: 5.0752\n",
      "Epoch [4/5], Step [68025/68337], Loss: 4.9149\n",
      "Epoch [4/5], Step [68100/68337], Loss: 5.1551\n",
      "Epoch [4/5], Step [68175/68337], Loss: 5.0359\n",
      "Epoch [4/5], Step [68250/68337], Loss: 5.0751\n",
      "Epoch [4/5], Step [68325/68337], Loss: 5.1130\n",
      "Epoch [4/5] Average Loss: 5.0385, Perplexity: 154.24\n",
      "Epoch [5/5], Step [0/68337], Loss: 5.0735\n",
      "Validation perplexity: 147.1046549577225\n",
      "Epoch [5/5], Step [75/68337], Loss: 4.9129\n",
      "Epoch [5/5], Step [150/68337], Loss: 5.0829\n",
      "Epoch [5/5], Step [225/68337], Loss: 4.8463\n",
      "Epoch [5/5], Step [300/68337], Loss: 4.9594\n",
      "Epoch [5/5], Step [375/68337], Loss: 5.2056\n",
      "Epoch [5/5], Step [450/68337], Loss: 4.9818\n",
      "Epoch [5/5], Step [525/68337], Loss: 4.9559\n",
      "Epoch [5/5], Step [600/68337], Loss: 4.9767\n",
      "Epoch [5/5], Step [675/68337], Loss: 5.0652\n",
      "Epoch [5/5], Step [750/68337], Loss: 5.1048\n",
      "Epoch [5/5], Step [825/68337], Loss: 5.1424\n",
      "Epoch [5/5], Step [900/68337], Loss: 4.9245\n",
      "Epoch [5/5], Step [975/68337], Loss: 4.9694\n",
      "Epoch [5/5], Step [1050/68337], Loss: 5.1341\n",
      "Epoch [5/5], Step [1125/68337], Loss: 5.0293\n",
      "Epoch [5/5], Step [1200/68337], Loss: 5.0028\n",
      "Epoch [5/5], Step [1275/68337], Loss: 4.9937\n",
      "Epoch [5/5], Step [1350/68337], Loss: 5.0496\n",
      "Epoch [5/5], Step [1425/68337], Loss: 5.1697\n",
      "Epoch [5/5], Step [1500/68337], Loss: 4.8693\n",
      "Epoch [5/5], Step [1575/68337], Loss: 4.9857\n",
      "Epoch [5/5], Step [1650/68337], Loss: 4.8679\n",
      "Epoch [5/5], Step [1725/68337], Loss: 4.8624\n",
      "Epoch [5/5], Step [1800/68337], Loss: 5.1017\n",
      "Epoch [5/5], Step [1875/68337], Loss: 5.0990\n",
      "Epoch [5/5], Step [1950/68337], Loss: 5.0462\n",
      "Epoch [5/5], Step [2025/68337], Loss: 4.9914\n",
      "Epoch [5/5], Step [2100/68337], Loss: 4.9417\n",
      "Epoch [5/5], Step [2175/68337], Loss: 5.2015\n",
      "Epoch [5/5], Step [2250/68337], Loss: 4.9459\n",
      "Epoch [5/5], Step [2325/68337], Loss: 5.0173\n",
      "Epoch [5/5], Step [2400/68337], Loss: 5.0010\n",
      "Epoch [5/5], Step [2475/68337], Loss: 4.8615\n",
      "Epoch [5/5], Step [2550/68337], Loss: 5.1313\n",
      "Epoch [5/5], Step [2625/68337], Loss: 5.1279\n",
      "Epoch [5/5], Step [2700/68337], Loss: 4.9668\n",
      "Epoch [5/5], Step [2775/68337], Loss: 5.0076\n",
      "Epoch [5/5], Step [2850/68337], Loss: 4.9845\n",
      "Epoch [5/5], Step [2925/68337], Loss: 4.9508\n",
      "Epoch [5/5], Step [3000/68337], Loss: 4.9293\n",
      "Epoch [5/5], Step [3075/68337], Loss: 5.2335\n",
      "Epoch [5/5], Step [3150/68337], Loss: 5.1977\n",
      "Epoch [5/5], Step [3225/68337], Loss: 5.0602\n",
      "Epoch [5/5], Step [3300/68337], Loss: 4.8630\n",
      "Epoch [5/5], Step [3375/68337], Loss: 4.9139\n",
      "Epoch [5/5], Step [3450/68337], Loss: 4.9631\n",
      "Epoch [5/5], Step [3525/68337], Loss: 5.1270\n",
      "Epoch [5/5], Step [3600/68337], Loss: 4.9056\n",
      "Epoch [5/5], Step [3675/68337], Loss: 4.8636\n",
      "Epoch [5/5], Step [3750/68337], Loss: 4.9406\n",
      "Epoch [5/5], Step [3825/68337], Loss: 4.7767\n",
      "Epoch [5/5], Step [3900/68337], Loss: 4.9648\n",
      "Epoch [5/5], Step [3975/68337], Loss: 5.0794\n",
      "Epoch [5/5], Step [4050/68337], Loss: 5.0318\n",
      "Epoch [5/5], Step [4125/68337], Loss: 4.9018\n",
      "Epoch [5/5], Step [4200/68337], Loss: 4.9877\n",
      "Epoch [5/5], Step [4275/68337], Loss: 5.0299\n",
      "Epoch [5/5], Step [4350/68337], Loss: 5.1150\n",
      "Epoch [5/5], Step [4425/68337], Loss: 4.9521\n",
      "Epoch [5/5], Step [4500/68337], Loss: 4.9796\n",
      "Epoch [5/5], Step [4575/68337], Loss: 4.9007\n",
      "Epoch [5/5], Step [4650/68337], Loss: 5.1362\n",
      "Epoch [5/5], Step [4725/68337], Loss: 5.2104\n",
      "Epoch [5/5], Step [4800/68337], Loss: 4.9336\n",
      "Epoch [5/5], Step [4875/68337], Loss: 5.0810\n",
      "Epoch [5/5], Step [4950/68337], Loss: 4.9310\n",
      "Epoch [5/5], Step [5025/68337], Loss: 5.0213\n",
      "Epoch [5/5], Step [5100/68337], Loss: 5.0389\n",
      "Epoch [5/5], Step [5175/68337], Loss: 5.0325\n",
      "Epoch [5/5], Step [5250/68337], Loss: 4.8891\n",
      "Epoch [5/5], Step [5325/68337], Loss: 5.1435\n",
      "Epoch [5/5], Step [5400/68337], Loss: 4.9514\n",
      "Epoch [5/5], Step [5475/68337], Loss: 5.0921\n",
      "Epoch [5/5], Step [5550/68337], Loss: 5.1045\n",
      "Epoch [5/5], Step [5625/68337], Loss: 4.9774\n",
      "Epoch [5/5], Step [5700/68337], Loss: 4.9150\n",
      "Epoch [5/5], Step [5775/68337], Loss: 4.9972\n",
      "Epoch [5/5], Step [5850/68337], Loss: 4.9860\n",
      "Epoch [5/5], Step [5925/68337], Loss: 5.0323\n",
      "Epoch [5/5], Step [6000/68337], Loss: 4.8128\n",
      "Epoch [5/5], Step [6075/68337], Loss: 5.0843\n",
      "Epoch [5/5], Step [6150/68337], Loss: 5.1228\n",
      "Epoch [5/5], Step [6225/68337], Loss: 5.0976\n",
      "Epoch [5/5], Step [6300/68337], Loss: 5.1539\n",
      "Epoch [5/5], Step [6375/68337], Loss: 5.0739\n",
      "Epoch [5/5], Step [6450/68337], Loss: 5.1137\n",
      "Epoch [5/5], Step [6525/68337], Loss: 4.9609\n",
      "Epoch [5/5], Step [6600/68337], Loss: 5.1240\n",
      "Epoch [5/5], Step [6675/68337], Loss: 5.0090\n",
      "Epoch [5/5], Step [6750/68337], Loss: 5.1025\n",
      "Epoch [5/5], Step [6825/68337], Loss: 5.1650\n",
      "Epoch [5/5], Step [6900/68337], Loss: 4.9292\n",
      "Epoch [5/5], Step [6975/68337], Loss: 5.0289\n",
      "Epoch [5/5], Step [7050/68337], Loss: 4.9933\n",
      "Epoch [5/5], Step [7125/68337], Loss: 4.9410\n",
      "Epoch [5/5], Step [7200/68337], Loss: 5.1138\n",
      "Epoch [5/5], Step [7275/68337], Loss: 5.0241\n",
      "Epoch [5/5], Step [7350/68337], Loss: 5.1463\n",
      "Epoch [5/5], Step [7425/68337], Loss: 5.1015\n",
      "Epoch [5/5], Step [7500/68337], Loss: 4.9550\n",
      "Epoch [5/5], Step [7575/68337], Loss: 5.0244\n",
      "Epoch [5/5], Step [7650/68337], Loss: 5.0124\n",
      "Epoch [5/5], Step [7725/68337], Loss: 4.9110\n",
      "Epoch [5/5], Step [7800/68337], Loss: 5.2016\n",
      "Epoch [5/5], Step [7875/68337], Loss: 5.0766\n",
      "Epoch [5/5], Step [7950/68337], Loss: 5.0038\n",
      "Epoch [5/5], Step [8025/68337], Loss: 4.8373\n",
      "Epoch [5/5], Step [8100/68337], Loss: 5.0258\n",
      "Epoch [5/5], Step [8175/68337], Loss: 4.9420\n",
      "Epoch [5/5], Step [8250/68337], Loss: 4.9078\n",
      "Epoch [5/5], Step [8325/68337], Loss: 4.9510\n",
      "Epoch [5/5], Step [8400/68337], Loss: 4.7989\n",
      "Epoch [5/5], Step [8475/68337], Loss: 5.1035\n",
      "Epoch [5/5], Step [8550/68337], Loss: 5.0483\n",
      "Epoch [5/5], Step [8625/68337], Loss: 5.2339\n",
      "Epoch [5/5], Step [8700/68337], Loss: 4.9883\n",
      "Epoch [5/5], Step [8775/68337], Loss: 5.0158\n",
      "Epoch [5/5], Step [8850/68337], Loss: 4.9756\n",
      "Epoch [5/5], Step [8925/68337], Loss: 5.0246\n",
      "Epoch [5/5], Step [9000/68337], Loss: 5.0867\n",
      "Epoch [5/5], Step [9075/68337], Loss: 5.0652\n",
      "Epoch [5/5], Step [9150/68337], Loss: 4.9150\n",
      "Epoch [5/5], Step [9225/68337], Loss: 5.0023\n",
      "Epoch [5/5], Step [9300/68337], Loss: 5.0515\n",
      "Epoch [5/5], Step [9375/68337], Loss: 5.0003\n",
      "Epoch [5/5], Step [9450/68337], Loss: 4.9746\n",
      "Epoch [5/5], Step [9525/68337], Loss: 5.1537\n",
      "Epoch [5/5], Step [9600/68337], Loss: 4.9096\n",
      "Epoch [5/5], Step [9675/68337], Loss: 4.9295\n",
      "Epoch [5/5], Step [9750/68337], Loss: 4.9484\n",
      "Epoch [5/5], Step [9825/68337], Loss: 5.0162\n",
      "Epoch [5/5], Step [9900/68337], Loss: 5.0436\n",
      "Epoch [5/5], Step [9975/68337], Loss: 5.1254\n",
      "Validation perplexity: 147.29371474885713\n",
      "Epoch [5/5], Step [10050/68337], Loss: 5.2632\n",
      "Epoch [5/5], Step [10125/68337], Loss: 5.0547\n",
      "Epoch [5/5], Step [10200/68337], Loss: 5.0323\n",
      "Epoch [5/5], Step [10275/68337], Loss: 4.9705\n",
      "Epoch [5/5], Step [10350/68337], Loss: 5.2339\n",
      "Epoch [5/5], Step [10425/68337], Loss: 5.0361\n",
      "Epoch [5/5], Step [10500/68337], Loss: 4.9752\n",
      "Epoch [5/5], Step [10575/68337], Loss: 5.0306\n",
      "Epoch [5/5], Step [10650/68337], Loss: 5.0106\n",
      "Epoch [5/5], Step [10725/68337], Loss: 5.0142\n",
      "Epoch [5/5], Step [10800/68337], Loss: 5.0501\n",
      "Epoch [5/5], Step [10875/68337], Loss: 5.1062\n",
      "Epoch [5/5], Step [10950/68337], Loss: 5.1864\n",
      "Epoch [5/5], Step [11025/68337], Loss: 5.2573\n",
      "Epoch [5/5], Step [11100/68337], Loss: 4.9644\n",
      "Epoch [5/5], Step [11175/68337], Loss: 4.9621\n",
      "Epoch [5/5], Step [11250/68337], Loss: 5.0433\n",
      "Epoch [5/5], Step [11325/68337], Loss: 5.0643\n",
      "Epoch [5/5], Step [11400/68337], Loss: 5.1489\n",
      "Epoch [5/5], Step [11475/68337], Loss: 4.9898\n",
      "Epoch [5/5], Step [11550/68337], Loss: 4.8272\n",
      "Epoch [5/5], Step [11625/68337], Loss: 4.9887\n",
      "Epoch [5/5], Step [11700/68337], Loss: 5.0599\n",
      "Epoch [5/5], Step [11775/68337], Loss: 4.9243\n",
      "Epoch [5/5], Step [11850/68337], Loss: 5.1082\n",
      "Epoch [5/5], Step [11925/68337], Loss: 4.9732\n",
      "Epoch [5/5], Step [12000/68337], Loss: 4.9120\n",
      "Epoch [5/5], Step [12075/68337], Loss: 5.1127\n",
      "Epoch [5/5], Step [12150/68337], Loss: 4.9924\n",
      "Epoch [5/5], Step [12225/68337], Loss: 4.9965\n",
      "Epoch [5/5], Step [12300/68337], Loss: 4.9649\n",
      "Epoch [5/5], Step [12375/68337], Loss: 5.0315\n",
      "Epoch [5/5], Step [12450/68337], Loss: 4.8657\n",
      "Epoch [5/5], Step [12525/68337], Loss: 4.9507\n",
      "Epoch [5/5], Step [12600/68337], Loss: 5.0786\n",
      "Epoch [5/5], Step [12675/68337], Loss: 5.0493\n",
      "Epoch [5/5], Step [12750/68337], Loss: 5.1949\n",
      "Epoch [5/5], Step [12825/68337], Loss: 5.0388\n",
      "Epoch [5/5], Step [12900/68337], Loss: 4.9846\n",
      "Epoch [5/5], Step [12975/68337], Loss: 5.0407\n",
      "Epoch [5/5], Step [13050/68337], Loss: 4.8514\n",
      "Epoch [5/5], Step [13125/68337], Loss: 5.2346\n",
      "Epoch [5/5], Step [13200/68337], Loss: 5.1211\n",
      "Epoch [5/5], Step [13275/68337], Loss: 5.2196\n",
      "Epoch [5/5], Step [13350/68337], Loss: 5.0432\n",
      "Epoch [5/5], Step [13425/68337], Loss: 5.1260\n",
      "Epoch [5/5], Step [13500/68337], Loss: 4.9575\n",
      "Epoch [5/5], Step [13575/68337], Loss: 5.0132\n",
      "Epoch [5/5], Step [13650/68337], Loss: 4.9514\n",
      "Epoch [5/5], Step [13725/68337], Loss: 5.0805\n",
      "Epoch [5/5], Step [13800/68337], Loss: 4.7564\n",
      "Epoch [5/5], Step [13875/68337], Loss: 4.9779\n",
      "Epoch [5/5], Step [13950/68337], Loss: 4.9364\n",
      "Epoch [5/5], Step [14025/68337], Loss: 5.0566\n",
      "Epoch [5/5], Step [14100/68337], Loss: 5.1425\n",
      "Epoch [5/5], Step [14175/68337], Loss: 5.0808\n",
      "Epoch [5/5], Step [14250/68337], Loss: 5.0423\n",
      "Epoch [5/5], Step [14325/68337], Loss: 5.1779\n",
      "Epoch [5/5], Step [14400/68337], Loss: 5.1069\n",
      "Epoch [5/5], Step [14475/68337], Loss: 4.9996\n",
      "Epoch [5/5], Step [14550/68337], Loss: 5.0566\n",
      "Epoch [5/5], Step [14625/68337], Loss: 5.0385\n",
      "Epoch [5/5], Step [14700/68337], Loss: 5.0769\n",
      "Epoch [5/5], Step [14775/68337], Loss: 4.9623\n",
      "Epoch [5/5], Step [14850/68337], Loss: 4.9925\n",
      "Epoch [5/5], Step [14925/68337], Loss: 4.9912\n",
      "Epoch [5/5], Step [15000/68337], Loss: 5.0606\n",
      "Epoch [5/5], Step [15075/68337], Loss: 5.0671\n",
      "Epoch [5/5], Step [15150/68337], Loss: 4.9275\n",
      "Epoch [5/5], Step [15225/68337], Loss: 4.8718\n",
      "Epoch [5/5], Step [15300/68337], Loss: 5.0573\n",
      "Epoch [5/5], Step [15375/68337], Loss: 5.0733\n",
      "Epoch [5/5], Step [15450/68337], Loss: 5.0146\n",
      "Epoch [5/5], Step [15525/68337], Loss: 4.9405\n",
      "Epoch [5/5], Step [15600/68337], Loss: 4.9651\n",
      "Epoch [5/5], Step [15675/68337], Loss: 5.2862\n",
      "Epoch [5/5], Step [15750/68337], Loss: 4.9229\n",
      "Epoch [5/5], Step [15825/68337], Loss: 4.9472\n",
      "Epoch [5/5], Step [15900/68337], Loss: 5.1018\n",
      "Epoch [5/5], Step [15975/68337], Loss: 5.1044\n",
      "Epoch [5/5], Step [16050/68337], Loss: 5.0721\n",
      "Epoch [5/5], Step [16125/68337], Loss: 4.9218\n",
      "Epoch [5/5], Step [16200/68337], Loss: 5.0671\n",
      "Epoch [5/5], Step [16275/68337], Loss: 4.9251\n",
      "Epoch [5/5], Step [16350/68337], Loss: 4.9994\n",
      "Epoch [5/5], Step [16425/68337], Loss: 4.8870\n",
      "Epoch [5/5], Step [16500/68337], Loss: 5.0005\n",
      "Epoch [5/5], Step [16575/68337], Loss: 5.0150\n",
      "Epoch [5/5], Step [16650/68337], Loss: 4.9368\n",
      "Epoch [5/5], Step [16725/68337], Loss: 4.9916\n",
      "Epoch [5/5], Step [16800/68337], Loss: 4.9643\n",
      "Epoch [5/5], Step [16875/68337], Loss: 5.0086\n",
      "Epoch [5/5], Step [16950/68337], Loss: 4.8995\n",
      "Epoch [5/5], Step [17025/68337], Loss: 4.8971\n",
      "Epoch [5/5], Step [17100/68337], Loss: 5.0747\n",
      "Epoch [5/5], Step [17175/68337], Loss: 4.9830\n",
      "Epoch [5/5], Step [17250/68337], Loss: 5.1811\n",
      "Epoch [5/5], Step [17325/68337], Loss: 4.9387\n",
      "Epoch [5/5], Step [17400/68337], Loss: 5.2474\n",
      "Epoch [5/5], Step [17475/68337], Loss: 5.0298\n",
      "Epoch [5/5], Step [17550/68337], Loss: 4.8576\n",
      "Epoch [5/5], Step [17625/68337], Loss: 4.9973\n",
      "Epoch [5/5], Step [17700/68337], Loss: 5.0379\n",
      "Epoch [5/5], Step [17775/68337], Loss: 4.9673\n",
      "Epoch [5/5], Step [17850/68337], Loss: 5.2046\n",
      "Epoch [5/5], Step [17925/68337], Loss: 5.1383\n",
      "Epoch [5/5], Step [18000/68337], Loss: 4.9164\n",
      "Epoch [5/5], Step [18075/68337], Loss: 5.1147\n",
      "Epoch [5/5], Step [18150/68337], Loss: 5.1181\n",
      "Epoch [5/5], Step [18225/68337], Loss: 4.9611\n",
      "Epoch [5/5], Step [18300/68337], Loss: 4.9176\n",
      "Epoch [5/5], Step [18375/68337], Loss: 5.0505\n",
      "Epoch [5/5], Step [18450/68337], Loss: 4.9560\n",
      "Epoch [5/5], Step [18525/68337], Loss: 5.1844\n",
      "Epoch [5/5], Step [18600/68337], Loss: 5.0617\n",
      "Epoch [5/5], Step [18675/68337], Loss: 5.1079\n",
      "Epoch [5/5], Step [18750/68337], Loss: 4.9686\n",
      "Epoch [5/5], Step [18825/68337], Loss: 4.9709\n",
      "Epoch [5/5], Step [18900/68337], Loss: 5.1317\n",
      "Epoch [5/5], Step [18975/68337], Loss: 5.1040\n",
      "Epoch [5/5], Step [19050/68337], Loss: 5.0930\n",
      "Epoch [5/5], Step [19125/68337], Loss: 5.2574\n",
      "Epoch [5/5], Step [19200/68337], Loss: 4.9687\n",
      "Epoch [5/5], Step [19275/68337], Loss: 5.0725\n",
      "Epoch [5/5], Step [19350/68337], Loss: 5.0624\n",
      "Epoch [5/5], Step [19425/68337], Loss: 5.0517\n",
      "Epoch [5/5], Step [19500/68337], Loss: 5.0156\n",
      "Epoch [5/5], Step [19575/68337], Loss: 5.0409\n",
      "Epoch [5/5], Step [19650/68337], Loss: 5.0029\n",
      "Epoch [5/5], Step [19725/68337], Loss: 4.9540\n",
      "Epoch [5/5], Step [19800/68337], Loss: 4.9566\n",
      "Epoch [5/5], Step [19875/68337], Loss: 5.0121\n",
      "Epoch [5/5], Step [19950/68337], Loss: 5.0897\n",
      "Validation perplexity: 146.99462992754243\n",
      "Epoch [5/5], Step [20025/68337], Loss: 5.0661\n",
      "Epoch [5/5], Step [20100/68337], Loss: 5.0149\n",
      "Epoch [5/5], Step [20175/68337], Loss: 4.8540\n",
      "Epoch [5/5], Step [20250/68337], Loss: 4.9920\n",
      "Epoch [5/5], Step [20325/68337], Loss: 4.9220\n",
      "Epoch [5/5], Step [20400/68337], Loss: 5.0241\n",
      "Epoch [5/5], Step [20475/68337], Loss: 5.2392\n",
      "Epoch [5/5], Step [20550/68337], Loss: 5.2228\n",
      "Epoch [5/5], Step [20625/68337], Loss: 4.9999\n",
      "Epoch [5/5], Step [20700/68337], Loss: 5.0736\n",
      "Epoch [5/5], Step [20775/68337], Loss: 4.9886\n",
      "Epoch [5/5], Step [20850/68337], Loss: 5.0844\n",
      "Epoch [5/5], Step [20925/68337], Loss: 4.9935\n",
      "Epoch [5/5], Step [21000/68337], Loss: 5.1649\n",
      "Epoch [5/5], Step [21075/68337], Loss: 5.0035\n",
      "Epoch [5/5], Step [21150/68337], Loss: 5.2089\n",
      "Epoch [5/5], Step [21225/68337], Loss: 4.9970\n",
      "Epoch [5/5], Step [21300/68337], Loss: 5.0279\n",
      "Epoch [5/5], Step [21375/68337], Loss: 5.1841\n",
      "Epoch [5/5], Step [21450/68337], Loss: 4.9609\n",
      "Epoch [5/5], Step [21525/68337], Loss: 4.9662\n",
      "Epoch [5/5], Step [21600/68337], Loss: 4.9601\n",
      "Epoch [5/5], Step [21675/68337], Loss: 5.0853\n",
      "Epoch [5/5], Step [21750/68337], Loss: 5.1212\n",
      "Epoch [5/5], Step [21825/68337], Loss: 5.0976\n",
      "Epoch [5/5], Step [21900/68337], Loss: 4.8684\n",
      "Epoch [5/5], Step [21975/68337], Loss: 5.1542\n",
      "Epoch [5/5], Step [22050/68337], Loss: 5.0174\n",
      "Epoch [5/5], Step [22125/68337], Loss: 5.0313\n",
      "Epoch [5/5], Step [22200/68337], Loss: 4.9563\n",
      "Epoch [5/5], Step [22275/68337], Loss: 5.0612\n",
      "Epoch [5/5], Step [22350/68337], Loss: 4.8846\n",
      "Epoch [5/5], Step [22425/68337], Loss: 5.1196\n",
      "Epoch [5/5], Step [22500/68337], Loss: 4.9529\n",
      "Epoch [5/5], Step [22575/68337], Loss: 5.0677\n",
      "Epoch [5/5], Step [22650/68337], Loss: 4.9797\n",
      "Epoch [5/5], Step [22725/68337], Loss: 5.1502\n",
      "Epoch [5/5], Step [22800/68337], Loss: 5.1011\n",
      "Epoch [5/5], Step [22875/68337], Loss: 5.1493\n",
      "Epoch [5/5], Step [22950/68337], Loss: 4.9897\n",
      "Epoch [5/5], Step [23025/68337], Loss: 4.8629\n",
      "Epoch [5/5], Step [23100/68337], Loss: 4.9497\n",
      "Epoch [5/5], Step [23175/68337], Loss: 5.0692\n",
      "Epoch [5/5], Step [23250/68337], Loss: 5.0603\n",
      "Epoch [5/5], Step [23325/68337], Loss: 4.9822\n",
      "Epoch [5/5], Step [23400/68337], Loss: 4.8383\n",
      "Epoch [5/5], Step [23475/68337], Loss: 4.9868\n",
      "Epoch [5/5], Step [23550/68337], Loss: 5.1067\n",
      "Epoch [5/5], Step [23625/68337], Loss: 5.0072\n",
      "Epoch [5/5], Step [23700/68337], Loss: 5.0851\n",
      "Epoch [5/5], Step [23775/68337], Loss: 4.9471\n",
      "Epoch [5/5], Step [23850/68337], Loss: 4.9050\n",
      "Epoch [5/5], Step [23925/68337], Loss: 5.0184\n",
      "Epoch [5/5], Step [24000/68337], Loss: 4.9164\n",
      "Epoch [5/5], Step [24075/68337], Loss: 4.9642\n",
      "Epoch [5/5], Step [24150/68337], Loss: 5.1465\n",
      "Epoch [5/5], Step [24225/68337], Loss: 5.0217\n",
      "Epoch [5/5], Step [24300/68337], Loss: 5.0595\n",
      "Epoch [5/5], Step [24375/68337], Loss: 4.9632\n",
      "Epoch [5/5], Step [24450/68337], Loss: 4.9494\n",
      "Epoch [5/5], Step [24525/68337], Loss: 5.0638\n",
      "Epoch [5/5], Step [24600/68337], Loss: 5.1282\n",
      "Epoch [5/5], Step [24675/68337], Loss: 4.9491\n",
      "Epoch [5/5], Step [24750/68337], Loss: 5.0007\n",
      "Epoch [5/5], Step [24825/68337], Loss: 4.8863\n",
      "Epoch [5/5], Step [24900/68337], Loss: 5.0673\n",
      "Epoch [5/5], Step [24975/68337], Loss: 5.0518\n",
      "Epoch [5/5], Step [25050/68337], Loss: 4.9805\n",
      "Epoch [5/5], Step [25125/68337], Loss: 4.9849\n",
      "Epoch [5/5], Step [25200/68337], Loss: 4.9237\n",
      "Epoch [5/5], Step [25275/68337], Loss: 4.9813\n",
      "Epoch [5/5], Step [25350/68337], Loss: 5.1721\n",
      "Epoch [5/5], Step [25425/68337], Loss: 5.1009\n",
      "Epoch [5/5], Step [25500/68337], Loss: 4.9557\n",
      "Epoch [5/5], Step [25575/68337], Loss: 5.2381\n",
      "Epoch [5/5], Step [25650/68337], Loss: 4.9160\n",
      "Epoch [5/5], Step [25725/68337], Loss: 5.0934\n",
      "Epoch [5/5], Step [25800/68337], Loss: 5.1650\n",
      "Epoch [5/5], Step [25875/68337], Loss: 5.0851\n",
      "Epoch [5/5], Step [25950/68337], Loss: 4.9873\n",
      "Epoch [5/5], Step [26025/68337], Loss: 5.1139\n",
      "Epoch [5/5], Step [26100/68337], Loss: 5.1754\n",
      "Epoch [5/5], Step [26175/68337], Loss: 4.9927\n",
      "Epoch [5/5], Step [26250/68337], Loss: 5.1913\n",
      "Epoch [5/5], Step [26325/68337], Loss: 5.1841\n",
      "Epoch [5/5], Step [26400/68337], Loss: 5.1235\n",
      "Epoch [5/5], Step [26475/68337], Loss: 5.0253\n",
      "Epoch [5/5], Step [26550/68337], Loss: 4.9463\n",
      "Epoch [5/5], Step [26625/68337], Loss: 5.0271\n",
      "Epoch [5/5], Step [26700/68337], Loss: 5.0292\n",
      "Epoch [5/5], Step [26775/68337], Loss: 5.1480\n",
      "Epoch [5/5], Step [26850/68337], Loss: 5.0270\n",
      "Epoch [5/5], Step [26925/68337], Loss: 4.9595\n",
      "Epoch [5/5], Step [27000/68337], Loss: 5.0929\n",
      "Epoch [5/5], Step [27075/68337], Loss: 5.1084\n",
      "Epoch [5/5], Step [27150/68337], Loss: 5.0666\n",
      "Epoch [5/5], Step [27225/68337], Loss: 5.1103\n",
      "Epoch [5/5], Step [27300/68337], Loss: 5.1633\n",
      "Epoch [5/5], Step [27375/68337], Loss: 5.0563\n",
      "Epoch [5/5], Step [27450/68337], Loss: 5.0590\n",
      "Epoch [5/5], Step [27525/68337], Loss: 4.9892\n",
      "Epoch [5/5], Step [27600/68337], Loss: 5.2007\n",
      "Epoch [5/5], Step [27675/68337], Loss: 5.1386\n",
      "Epoch [5/5], Step [27750/68337], Loss: 5.0363\n",
      "Epoch [5/5], Step [27825/68337], Loss: 4.8022\n",
      "Epoch [5/5], Step [27900/68337], Loss: 5.0471\n",
      "Epoch [5/5], Step [27975/68337], Loss: 4.9791\n",
      "Epoch [5/5], Step [28050/68337], Loss: 5.0339\n",
      "Epoch [5/5], Step [28125/68337], Loss: 4.8191\n",
      "Epoch [5/5], Step [28200/68337], Loss: 4.8028\n",
      "Epoch [5/5], Step [28275/68337], Loss: 4.9455\n",
      "Epoch [5/5], Step [28350/68337], Loss: 4.9263\n",
      "Epoch [5/5], Step [28425/68337], Loss: 4.9548\n",
      "Epoch [5/5], Step [28500/68337], Loss: 5.0027\n",
      "Epoch [5/5], Step [28575/68337], Loss: 5.1388\n",
      "Epoch [5/5], Step [28650/68337], Loss: 5.0943\n",
      "Epoch [5/5], Step [28725/68337], Loss: 5.0572\n",
      "Epoch [5/5], Step [28800/68337], Loss: 5.2050\n",
      "Epoch [5/5], Step [28875/68337], Loss: 4.8796\n",
      "Epoch [5/5], Step [28950/68337], Loss: 5.1248\n",
      "Epoch [5/5], Step [29025/68337], Loss: 4.9341\n",
      "Epoch [5/5], Step [29100/68337], Loss: 4.8856\n",
      "Epoch [5/5], Step [29175/68337], Loss: 4.8649\n",
      "Epoch [5/5], Step [29250/68337], Loss: 4.8566\n",
      "Epoch [5/5], Step [29325/68337], Loss: 5.0319\n",
      "Epoch [5/5], Step [29400/68337], Loss: 5.0698\n",
      "Epoch [5/5], Step [29475/68337], Loss: 4.9268\n",
      "Epoch [5/5], Step [29550/68337], Loss: 4.9614\n",
      "Epoch [5/5], Step [29625/68337], Loss: 5.1026\n",
      "Epoch [5/5], Step [29700/68337], Loss: 5.1856\n",
      "Epoch [5/5], Step [29775/68337], Loss: 4.8803\n",
      "Epoch [5/5], Step [29850/68337], Loss: 4.9676\n",
      "Epoch [5/5], Step [29925/68337], Loss: 5.0857\n",
      "Epoch [5/5], Step [30000/68337], Loss: 4.9667\n",
      "Validation perplexity: 147.1779939296264\n",
      "Epoch [5/5], Step [30075/68337], Loss: 5.0037\n",
      "Epoch [5/5], Step [30150/68337], Loss: 5.0490\n",
      "Epoch [5/5], Step [30225/68337], Loss: 4.9749\n",
      "Epoch [5/5], Step [30300/68337], Loss: 4.9610\n",
      "Epoch [5/5], Step [30375/68337], Loss: 5.0068\n",
      "Epoch [5/5], Step [30450/68337], Loss: 4.9880\n",
      "Epoch [5/5], Step [30525/68337], Loss: 4.8969\n",
      "Epoch [5/5], Step [30600/68337], Loss: 5.0605\n",
      "Epoch [5/5], Step [30675/68337], Loss: 4.8743\n",
      "Epoch [5/5], Step [30750/68337], Loss: 4.8763\n",
      "Epoch [5/5], Step [30825/68337], Loss: 5.0171\n",
      "Epoch [5/5], Step [30900/68337], Loss: 5.2227\n",
      "Epoch [5/5], Step [30975/68337], Loss: 5.0208\n",
      "Epoch [5/5], Step [31050/68337], Loss: 4.9895\n",
      "Epoch [5/5], Step [31125/68337], Loss: 5.1682\n",
      "Epoch [5/5], Step [31200/68337], Loss: 5.1450\n",
      "Epoch [5/5], Step [31275/68337], Loss: 5.3276\n",
      "Epoch [5/5], Step [31350/68337], Loss: 5.1762\n",
      "Epoch [5/5], Step [31425/68337], Loss: 5.0748\n",
      "Epoch [5/5], Step [31500/68337], Loss: 4.8980\n",
      "Epoch [5/5], Step [31575/68337], Loss: 4.8992\n",
      "Epoch [5/5], Step [31650/68337], Loss: 4.8051\n",
      "Epoch [5/5], Step [31725/68337], Loss: 5.0945\n",
      "Epoch [5/5], Step [31800/68337], Loss: 5.2653\n",
      "Epoch [5/5], Step [31875/68337], Loss: 4.9412\n",
      "Epoch [5/5], Step [31950/68337], Loss: 5.1054\n",
      "Epoch [5/5], Step [32025/68337], Loss: 5.1912\n",
      "Epoch [5/5], Step [32100/68337], Loss: 5.0359\n",
      "Epoch [5/5], Step [32175/68337], Loss: 4.9742\n",
      "Epoch [5/5], Step [32250/68337], Loss: 5.0779\n",
      "Epoch [5/5], Step [32325/68337], Loss: 4.9711\n",
      "Epoch [5/5], Step [32400/68337], Loss: 5.0746\n",
      "Epoch [5/5], Step [32475/68337], Loss: 4.8198\n",
      "Epoch [5/5], Step [32550/68337], Loss: 4.9718\n",
      "Epoch [5/5], Step [32625/68337], Loss: 5.1412\n",
      "Epoch [5/5], Step [32700/68337], Loss: 4.9552\n",
      "Epoch [5/5], Step [32775/68337], Loss: 4.8705\n",
      "Epoch [5/5], Step [32850/68337], Loss: 5.0033\n",
      "Epoch [5/5], Step [32925/68337], Loss: 4.9415\n",
      "Epoch [5/5], Step [33000/68337], Loss: 5.0453\n",
      "Epoch [5/5], Step [33075/68337], Loss: 5.0490\n",
      "Epoch [5/5], Step [33150/68337], Loss: 5.0965\n",
      "Epoch [5/5], Step [33225/68337], Loss: 5.0444\n",
      "Epoch [5/5], Step [33300/68337], Loss: 4.9974\n",
      "Epoch [5/5], Step [33375/68337], Loss: 4.9554\n",
      "Epoch [5/5], Step [33450/68337], Loss: 4.9256\n",
      "Epoch [5/5], Step [33525/68337], Loss: 4.9031\n",
      "Epoch [5/5], Step [33600/68337], Loss: 4.9572\n",
      "Epoch [5/5], Step [33675/68337], Loss: 5.0320\n",
      "Epoch [5/5], Step [33750/68337], Loss: 5.0578\n",
      "Epoch [5/5], Step [33825/68337], Loss: 5.0174\n",
      "Epoch [5/5], Step [33900/68337], Loss: 4.9715\n",
      "Epoch [5/5], Step [33975/68337], Loss: 5.0395\n",
      "Epoch [5/5], Step [34050/68337], Loss: 5.0636\n",
      "Epoch [5/5], Step [34125/68337], Loss: 5.1220\n",
      "Epoch [5/5], Step [34200/68337], Loss: 4.9900\n",
      "Epoch [5/5], Step [34275/68337], Loss: 5.1252\n",
      "Epoch [5/5], Step [34350/68337], Loss: 5.0158\n",
      "Epoch [5/5], Step [34425/68337], Loss: 5.1898\n",
      "Epoch [5/5], Step [34500/68337], Loss: 5.1365\n",
      "Epoch [5/5], Step [34575/68337], Loss: 5.0584\n",
      "Epoch [5/5], Step [34650/68337], Loss: 5.1355\n",
      "Epoch [5/5], Step [34725/68337], Loss: 4.9628\n",
      "Epoch [5/5], Step [34800/68337], Loss: 4.9760\n",
      "Epoch [5/5], Step [34875/68337], Loss: 4.9776\n",
      "Epoch [5/5], Step [34950/68337], Loss: 5.1143\n",
      "Epoch [5/5], Step [35025/68337], Loss: 5.0542\n",
      "Epoch [5/5], Step [35100/68337], Loss: 5.0225\n",
      "Epoch [5/5], Step [35175/68337], Loss: 4.9722\n",
      "Epoch [5/5], Step [35250/68337], Loss: 5.0951\n",
      "Epoch [5/5], Step [35325/68337], Loss: 5.0107\n",
      "Epoch [5/5], Step [35400/68337], Loss: 5.0136\n",
      "Epoch [5/5], Step [35475/68337], Loss: 5.0329\n",
      "Epoch [5/5], Step [35550/68337], Loss: 4.9996\n",
      "Epoch [5/5], Step [35625/68337], Loss: 5.0051\n",
      "Epoch [5/5], Step [35700/68337], Loss: 5.0632\n",
      "Epoch [5/5], Step [35775/68337], Loss: 4.9900\n",
      "Epoch [5/5], Step [35850/68337], Loss: 4.7433\n",
      "Epoch [5/5], Step [35925/68337], Loss: 5.1911\n",
      "Epoch [5/5], Step [36000/68337], Loss: 5.0726\n",
      "Epoch [5/5], Step [36075/68337], Loss: 5.0558\n",
      "Epoch [5/5], Step [36150/68337], Loss: 4.9872\n",
      "Epoch [5/5], Step [36225/68337], Loss: 5.1256\n",
      "Epoch [5/5], Step [36300/68337], Loss: 5.1169\n",
      "Epoch [5/5], Step [36375/68337], Loss: 5.0358\n",
      "Epoch [5/5], Step [36450/68337], Loss: 5.0792\n",
      "Epoch [5/5], Step [36525/68337], Loss: 4.9763\n",
      "Epoch [5/5], Step [36600/68337], Loss: 5.1061\n",
      "Epoch [5/5], Step [36675/68337], Loss: 4.9742\n",
      "Epoch [5/5], Step [36750/68337], Loss: 4.9735\n",
      "Epoch [5/5], Step [36825/68337], Loss: 5.0432\n",
      "Epoch [5/5], Step [36900/68337], Loss: 5.0843\n",
      "Epoch [5/5], Step [36975/68337], Loss: 5.0659\n",
      "Epoch [5/5], Step [37050/68337], Loss: 5.0079\n",
      "Epoch [5/5], Step [37125/68337], Loss: 5.0951\n",
      "Epoch [5/5], Step [37200/68337], Loss: 4.8528\n",
      "Epoch [5/5], Step [37275/68337], Loss: 5.0146\n",
      "Epoch [5/5], Step [37350/68337], Loss: 5.1802\n",
      "Epoch [5/5], Step [37425/68337], Loss: 4.8977\n",
      "Epoch [5/5], Step [37500/68337], Loss: 5.0095\n",
      "Epoch [5/5], Step [37575/68337], Loss: 4.9686\n",
      "Epoch [5/5], Step [37650/68337], Loss: 5.1834\n",
      "Epoch [5/5], Step [37725/68337], Loss: 5.0883\n",
      "Epoch [5/5], Step [37800/68337], Loss: 5.0098\n",
      "Epoch [5/5], Step [37875/68337], Loss: 5.1623\n",
      "Epoch [5/5], Step [37950/68337], Loss: 5.0817\n",
      "Epoch [5/5], Step [38025/68337], Loss: 5.2714\n",
      "Epoch [5/5], Step [38100/68337], Loss: 5.0117\n",
      "Epoch [5/5], Step [38175/68337], Loss: 4.8609\n",
      "Epoch [5/5], Step [38250/68337], Loss: 5.1392\n",
      "Epoch [5/5], Step [38325/68337], Loss: 4.9382\n",
      "Epoch [5/5], Step [38400/68337], Loss: 5.0909\n",
      "Epoch [5/5], Step [38475/68337], Loss: 5.1513\n",
      "Epoch [5/5], Step [38550/68337], Loss: 5.0965\n",
      "Epoch [5/5], Step [38625/68337], Loss: 4.9409\n",
      "Epoch [5/5], Step [38700/68337], Loss: 5.0340\n",
      "Epoch [5/5], Step [38775/68337], Loss: 5.0358\n",
      "Epoch [5/5], Step [38850/68337], Loss: 5.0838\n",
      "Epoch [5/5], Step [38925/68337], Loss: 4.9465\n",
      "Epoch [5/5], Step [39000/68337], Loss: 5.0564\n",
      "Epoch [5/5], Step [39075/68337], Loss: 5.0930\n",
      "Epoch [5/5], Step [39150/68337], Loss: 4.9344\n",
      "Epoch [5/5], Step [39225/68337], Loss: 5.0284\n",
      "Epoch [5/5], Step [39300/68337], Loss: 4.8821\n",
      "Epoch [5/5], Step [39375/68337], Loss: 4.9055\n",
      "Epoch [5/5], Step [39450/68337], Loss: 5.1332\n",
      "Epoch [5/5], Step [39525/68337], Loss: 5.0495\n",
      "Epoch [5/5], Step [39600/68337], Loss: 5.1338\n",
      "Epoch [5/5], Step [39675/68337], Loss: 5.0583\n",
      "Epoch [5/5], Step [39750/68337], Loss: 5.1639\n",
      "Epoch [5/5], Step [39825/68337], Loss: 4.9743\n",
      "Epoch [5/5], Step [39900/68337], Loss: 4.9976\n",
      "Epoch [5/5], Step [39975/68337], Loss: 5.0054\n",
      "Validation perplexity: 146.75241954874576\n",
      "Epoch [5/5], Step [40050/68337], Loss: 4.9284\n",
      "Epoch [5/5], Step [40125/68337], Loss: 5.0424\n",
      "Epoch [5/5], Step [40200/68337], Loss: 5.0439\n",
      "Epoch [5/5], Step [40275/68337], Loss: 4.8192\n",
      "Epoch [5/5], Step [40350/68337], Loss: 4.9763\n",
      "Epoch [5/5], Step [40425/68337], Loss: 5.1251\n",
      "Epoch [5/5], Step [40500/68337], Loss: 5.0005\n",
      "Epoch [5/5], Step [40575/68337], Loss: 5.1260\n",
      "Epoch [5/5], Step [40650/68337], Loss: 5.0180\n",
      "Epoch [5/5], Step [40725/68337], Loss: 5.0518\n",
      "Epoch [5/5], Step [40800/68337], Loss: 5.1555\n",
      "Epoch [5/5], Step [40875/68337], Loss: 4.9540\n",
      "Epoch [5/5], Step [40950/68337], Loss: 5.1899\n",
      "Epoch [5/5], Step [41025/68337], Loss: 5.0884\n",
      "Epoch [5/5], Step [41100/68337], Loss: 5.0122\n",
      "Epoch [5/5], Step [41175/68337], Loss: 5.2061\n",
      "Epoch [5/5], Step [41250/68337], Loss: 5.0433\n",
      "Epoch [5/5], Step [41325/68337], Loss: 5.1303\n",
      "Epoch [5/5], Step [41400/68337], Loss: 5.1166\n",
      "Epoch [5/5], Step [41475/68337], Loss: 4.9209\n",
      "Epoch [5/5], Step [41550/68337], Loss: 5.0249\n",
      "Epoch [5/5], Step [41625/68337], Loss: 5.0549\n",
      "Epoch [5/5], Step [41700/68337], Loss: 5.1489\n",
      "Epoch [5/5], Step [41775/68337], Loss: 5.0728\n",
      "Epoch [5/5], Step [41850/68337], Loss: 5.1306\n",
      "Epoch [5/5], Step [41925/68337], Loss: 5.0483\n",
      "Epoch [5/5], Step [42000/68337], Loss: 5.0159\n",
      "Epoch [5/5], Step [42075/68337], Loss: 5.1542\n",
      "Epoch [5/5], Step [42150/68337], Loss: 5.0428\n",
      "Epoch [5/5], Step [42225/68337], Loss: 5.0989\n",
      "Epoch [5/5], Step [42300/68337], Loss: 5.0649\n",
      "Epoch [5/5], Step [42375/68337], Loss: 5.0953\n",
      "Epoch [5/5], Step [42450/68337], Loss: 5.1338\n",
      "Epoch [5/5], Step [42525/68337], Loss: 4.9989\n",
      "Epoch [5/5], Step [42600/68337], Loss: 5.0700\n",
      "Epoch [5/5], Step [42675/68337], Loss: 5.0718\n",
      "Epoch [5/5], Step [42750/68337], Loss: 5.1119\n",
      "Epoch [5/5], Step [42825/68337], Loss: 5.0206\n",
      "Epoch [5/5], Step [42900/68337], Loss: 4.9241\n",
      "Epoch [5/5], Step [42975/68337], Loss: 4.9577\n",
      "Epoch [5/5], Step [43050/68337], Loss: 4.8358\n",
      "Epoch [5/5], Step [43125/68337], Loss: 5.0039\n",
      "Epoch [5/5], Step [43200/68337], Loss: 4.9687\n",
      "Epoch [5/5], Step [43275/68337], Loss: 5.1099\n",
      "Epoch [5/5], Step [43350/68337], Loss: 4.9166\n",
      "Epoch [5/5], Step [43425/68337], Loss: 5.0978\n",
      "Epoch [5/5], Step [43500/68337], Loss: 5.0739\n",
      "Epoch [5/5], Step [43575/68337], Loss: 4.9530\n",
      "Epoch [5/5], Step [43650/68337], Loss: 4.9841\n",
      "Epoch [5/5], Step [43725/68337], Loss: 4.9842\n",
      "Epoch [5/5], Step [43800/68337], Loss: 4.9409\n",
      "Epoch [5/5], Step [43875/68337], Loss: 4.9970\n",
      "Epoch [5/5], Step [43950/68337], Loss: 4.9849\n",
      "Epoch [5/5], Step [44025/68337], Loss: 4.9149\n",
      "Epoch [5/5], Step [44100/68337], Loss: 5.0907\n",
      "Epoch [5/5], Step [44175/68337], Loss: 4.9873\n",
      "Epoch [5/5], Step [44250/68337], Loss: 4.8623\n",
      "Epoch [5/5], Step [44325/68337], Loss: 5.0825\n",
      "Epoch [5/5], Step [44400/68337], Loss: 4.9875\n",
      "Epoch [5/5], Step [44475/68337], Loss: 5.0310\n",
      "Epoch [5/5], Step [44550/68337], Loss: 5.1992\n",
      "Epoch [5/5], Step [44625/68337], Loss: 4.8287\n",
      "Epoch [5/5], Step [44700/68337], Loss: 5.1250\n",
      "Epoch [5/5], Step [44775/68337], Loss: 5.0292\n",
      "Epoch [5/5], Step [44850/68337], Loss: 4.9887\n",
      "Epoch [5/5], Step [44925/68337], Loss: 5.0130\n",
      "Epoch [5/5], Step [45000/68337], Loss: 5.0970\n",
      "Epoch [5/5], Step [45075/68337], Loss: 5.0731\n",
      "Epoch [5/5], Step [45150/68337], Loss: 4.9629\n",
      "Epoch [5/5], Step [45225/68337], Loss: 5.0051\n",
      "Epoch [5/5], Step [45300/68337], Loss: 4.9996\n",
      "Epoch [5/5], Step [45375/68337], Loss: 4.8905\n",
      "Epoch [5/5], Step [45450/68337], Loss: 4.9441\n",
      "Epoch [5/5], Step [45525/68337], Loss: 5.0141\n",
      "Epoch [5/5], Step [45600/68337], Loss: 5.1378\n",
      "Epoch [5/5], Step [45675/68337], Loss: 4.9261\n",
      "Epoch [5/5], Step [45750/68337], Loss: 4.9516\n",
      "Epoch [5/5], Step [45825/68337], Loss: 5.0648\n",
      "Epoch [5/5], Step [45900/68337], Loss: 4.9959\n",
      "Epoch [5/5], Step [45975/68337], Loss: 5.0285\n",
      "Epoch [5/5], Step [46050/68337], Loss: 5.0117\n",
      "Epoch [5/5], Step [46125/68337], Loss: 4.9636\n",
      "Epoch [5/5], Step [46200/68337], Loss: 5.0080\n",
      "Epoch [5/5], Step [46275/68337], Loss: 5.0269\n",
      "Epoch [5/5], Step [46350/68337], Loss: 4.9762\n",
      "Epoch [5/5], Step [46425/68337], Loss: 4.9637\n",
      "Epoch [5/5], Step [46500/68337], Loss: 4.8983\n",
      "Epoch [5/5], Step [46575/68337], Loss: 4.9673\n",
      "Epoch [5/5], Step [46650/68337], Loss: 5.1594\n",
      "Epoch [5/5], Step [46725/68337], Loss: 4.9280\n",
      "Epoch [5/5], Step [46800/68337], Loss: 5.1028\n",
      "Epoch [5/5], Step [46875/68337], Loss: 4.8391\n",
      "Epoch [5/5], Step [46950/68337], Loss: 5.1464\n",
      "Epoch [5/5], Step [47025/68337], Loss: 5.0987\n",
      "Epoch [5/5], Step [47100/68337], Loss: 5.0241\n",
      "Epoch [5/5], Step [47175/68337], Loss: 4.9945\n",
      "Epoch [5/5], Step [47250/68337], Loss: 5.0325\n",
      "Epoch [5/5], Step [47325/68337], Loss: 5.0743\n",
      "Epoch [5/5], Step [47400/68337], Loss: 5.0207\n",
      "Epoch [5/5], Step [47475/68337], Loss: 5.0542\n",
      "Epoch [5/5], Step [47550/68337], Loss: 5.1777\n",
      "Epoch [5/5], Step [47625/68337], Loss: 4.9937\n",
      "Epoch [5/5], Step [47700/68337], Loss: 5.2605\n",
      "Epoch [5/5], Step [47775/68337], Loss: 4.9541\n",
      "Epoch [5/5], Step [47850/68337], Loss: 4.9793\n",
      "Epoch [5/5], Step [47925/68337], Loss: 4.9826\n",
      "Epoch [5/5], Step [48000/68337], Loss: 4.9990\n",
      "Epoch [5/5], Step [48075/68337], Loss: 5.1118\n",
      "Epoch [5/5], Step [48150/68337], Loss: 5.1255\n",
      "Epoch [5/5], Step [48225/68337], Loss: 4.9969\n",
      "Epoch [5/5], Step [48300/68337], Loss: 4.8846\n",
      "Epoch [5/5], Step [48375/68337], Loss: 5.0379\n",
      "Epoch [5/5], Step [48450/68337], Loss: 5.0162\n",
      "Epoch [5/5], Step [48525/68337], Loss: 5.0271\n",
      "Epoch [5/5], Step [48600/68337], Loss: 5.1064\n",
      "Epoch [5/5], Step [48675/68337], Loss: 5.0817\n",
      "Epoch [5/5], Step [48750/68337], Loss: 5.0952\n",
      "Epoch [5/5], Step [48825/68337], Loss: 5.0981\n",
      "Epoch [5/5], Step [48900/68337], Loss: 5.0455\n",
      "Epoch [5/5], Step [48975/68337], Loss: 5.0764\n",
      "Epoch [5/5], Step [49050/68337], Loss: 5.0374\n",
      "Epoch [5/5], Step [49125/68337], Loss: 5.0263\n",
      "Epoch [5/5], Step [49200/68337], Loss: 4.9306\n",
      "Epoch [5/5], Step [49275/68337], Loss: 5.1073\n",
      "Epoch [5/5], Step [49350/68337], Loss: 5.0899\n",
      "Epoch [5/5], Step [49425/68337], Loss: 4.9928\n",
      "Epoch [5/5], Step [49500/68337], Loss: 4.7058\n",
      "Epoch [5/5], Step [49575/68337], Loss: 5.0996\n",
      "Epoch [5/5], Step [49650/68337], Loss: 5.1564\n",
      "Epoch [5/5], Step [49725/68337], Loss: 4.9890\n",
      "Epoch [5/5], Step [49800/68337], Loss: 5.0779\n",
      "Epoch [5/5], Step [49875/68337], Loss: 4.9763\n",
      "Epoch [5/5], Step [49950/68337], Loss: 5.0226\n",
      "Validation perplexity: 146.4874101760165\n",
      "Epoch [5/5], Step [50025/68337], Loss: 5.0192\n",
      "Epoch [5/5], Step [50100/68337], Loss: 5.1496\n",
      "Epoch [5/5], Step [50175/68337], Loss: 5.1165\n",
      "Epoch [5/5], Step [50250/68337], Loss: 5.0830\n",
      "Epoch [5/5], Step [50325/68337], Loss: 5.0165\n",
      "Epoch [5/5], Step [50400/68337], Loss: 5.0905\n",
      "Epoch [5/5], Step [50475/68337], Loss: 5.1185\n",
      "Epoch [5/5], Step [50550/68337], Loss: 4.9749\n",
      "Epoch [5/5], Step [50625/68337], Loss: 5.0705\n",
      "Epoch [5/5], Step [50700/68337], Loss: 4.9568\n",
      "Epoch [5/5], Step [50775/68337], Loss: 4.9420\n",
      "Epoch [5/5], Step [50850/68337], Loss: 5.0419\n",
      "Epoch [5/5], Step [50925/68337], Loss: 5.0194\n",
      "Epoch [5/5], Step [51000/68337], Loss: 5.1227\n",
      "Epoch [5/5], Step [51075/68337], Loss: 5.0687\n",
      "Epoch [5/5], Step [51150/68337], Loss: 5.0019\n",
      "Epoch [5/5], Step [51225/68337], Loss: 4.9840\n",
      "Epoch [5/5], Step [51300/68337], Loss: 4.9881\n",
      "Epoch [5/5], Step [51375/68337], Loss: 5.1637\n",
      "Epoch [5/5], Step [51450/68337], Loss: 5.1717\n",
      "Epoch [5/5], Step [51525/68337], Loss: 4.9623\n",
      "Epoch [5/5], Step [51600/68337], Loss: 4.9382\n",
      "Epoch [5/5], Step [51675/68337], Loss: 4.9976\n",
      "Epoch [5/5], Step [51750/68337], Loss: 4.9560\n",
      "Epoch [5/5], Step [51825/68337], Loss: 5.0228\n",
      "Epoch [5/5], Step [51900/68337], Loss: 5.2260\n",
      "Epoch [5/5], Step [51975/68337], Loss: 5.0790\n",
      "Epoch [5/5], Step [52050/68337], Loss: 4.9983\n",
      "Epoch [5/5], Step [52125/68337], Loss: 5.0322\n",
      "Epoch [5/5], Step [52200/68337], Loss: 5.0987\n",
      "Epoch [5/5], Step [52275/68337], Loss: 5.0179\n",
      "Epoch [5/5], Step [52350/68337], Loss: 4.8962\n",
      "Epoch [5/5], Step [52425/68337], Loss: 4.9265\n",
      "Epoch [5/5], Step [52500/68337], Loss: 5.0150\n",
      "Epoch [5/5], Step [52575/68337], Loss: 4.9584\n",
      "Epoch [5/5], Step [52650/68337], Loss: 4.8766\n",
      "Epoch [5/5], Step [52725/68337], Loss: 5.0671\n",
      "Epoch [5/5], Step [52800/68337], Loss: 5.0370\n",
      "Epoch [5/5], Step [52875/68337], Loss: 5.0560\n",
      "Epoch [5/5], Step [52950/68337], Loss: 4.9138\n",
      "Epoch [5/5], Step [53025/68337], Loss: 5.0541\n",
      "Epoch [5/5], Step [53100/68337], Loss: 5.0232\n",
      "Epoch [5/5], Step [53175/68337], Loss: 4.9462\n",
      "Epoch [5/5], Step [53250/68337], Loss: 5.1433\n",
      "Epoch [5/5], Step [53325/68337], Loss: 4.8913\n",
      "Epoch [5/5], Step [53400/68337], Loss: 5.0454\n",
      "Epoch [5/5], Step [53475/68337], Loss: 4.9474\n",
      "Epoch [5/5], Step [53550/68337], Loss: 4.9652\n",
      "Epoch [5/5], Step [53625/68337], Loss: 4.8856\n",
      "Epoch [5/5], Step [53700/68337], Loss: 5.1017\n",
      "Epoch [5/5], Step [53775/68337], Loss: 5.1237\n",
      "Epoch [5/5], Step [53850/68337], Loss: 4.8882\n",
      "Epoch [5/5], Step [53925/68337], Loss: 4.9747\n",
      "Epoch [5/5], Step [54000/68337], Loss: 5.0485\n",
      "Epoch [5/5], Step [54075/68337], Loss: 5.0413\n",
      "Epoch [5/5], Step [54150/68337], Loss: 5.1040\n",
      "Epoch [5/5], Step [54225/68337], Loss: 5.0307\n",
      "Epoch [5/5], Step [54300/68337], Loss: 4.9370\n",
      "Epoch [5/5], Step [54375/68337], Loss: 4.9486\n",
      "Epoch [5/5], Step [54450/68337], Loss: 4.9356\n",
      "Epoch [5/5], Step [54525/68337], Loss: 4.9883\n",
      "Epoch [5/5], Step [54600/68337], Loss: 5.0850\n",
      "Epoch [5/5], Step [54675/68337], Loss: 4.9346\n",
      "Epoch [5/5], Step [54750/68337], Loss: 4.9251\n",
      "Epoch [5/5], Step [54825/68337], Loss: 5.1224\n",
      "Epoch [5/5], Step [54900/68337], Loss: 4.9670\n",
      "Epoch [5/5], Step [54975/68337], Loss: 5.1808\n",
      "Epoch [5/5], Step [55050/68337], Loss: 5.0625\n",
      "Epoch [5/5], Step [55125/68337], Loss: 5.0147\n",
      "Epoch [5/5], Step [55200/68337], Loss: 5.0280\n",
      "Epoch [5/5], Step [55275/68337], Loss: 5.1632\n",
      "Epoch [5/5], Step [55350/68337], Loss: 4.9654\n",
      "Epoch [5/5], Step [55425/68337], Loss: 4.9754\n",
      "Epoch [5/5], Step [55500/68337], Loss: 4.9972\n",
      "Epoch [5/5], Step [55575/68337], Loss: 4.9306\n",
      "Epoch [5/5], Step [55650/68337], Loss: 5.1027\n",
      "Epoch [5/5], Step [55725/68337], Loss: 5.0204\n",
      "Epoch [5/5], Step [55800/68337], Loss: 4.9674\n",
      "Epoch [5/5], Step [55875/68337], Loss: 5.0698\n",
      "Epoch [5/5], Step [55950/68337], Loss: 5.0206\n",
      "Epoch [5/5], Step [56025/68337], Loss: 5.1437\n",
      "Epoch [5/5], Step [56100/68337], Loss: 4.9830\n",
      "Epoch [5/5], Step [56175/68337], Loss: 5.1247\n",
      "Epoch [5/5], Step [56250/68337], Loss: 5.0513\n",
      "Epoch [5/5], Step [56325/68337], Loss: 5.0370\n",
      "Epoch [5/5], Step [56400/68337], Loss: 5.1915\n",
      "Epoch [5/5], Step [56475/68337], Loss: 5.1413\n",
      "Epoch [5/5], Step [56550/68337], Loss: 5.0493\n",
      "Epoch [5/5], Step [56625/68337], Loss: 4.8641\n",
      "Epoch [5/5], Step [56700/68337], Loss: 4.9162\n",
      "Epoch [5/5], Step [56775/68337], Loss: 4.9740\n",
      "Epoch [5/5], Step [56850/68337], Loss: 5.0907\n",
      "Epoch [5/5], Step [56925/68337], Loss: 4.9899\n",
      "Epoch [5/5], Step [57000/68337], Loss: 4.9408\n",
      "Epoch [5/5], Step [57075/68337], Loss: 4.9359\n",
      "Epoch [5/5], Step [57150/68337], Loss: 4.9994\n",
      "Epoch [5/5], Step [57225/68337], Loss: 5.0419\n",
      "Epoch [5/5], Step [57300/68337], Loss: 5.0168\n",
      "Epoch [5/5], Step [57375/68337], Loss: 4.9461\n",
      "Epoch [5/5], Step [57450/68337], Loss: 4.9112\n",
      "Epoch [5/5], Step [57525/68337], Loss: 5.0426\n",
      "Epoch [5/5], Step [57600/68337], Loss: 4.9942\n",
      "Epoch [5/5], Step [57675/68337], Loss: 4.9947\n",
      "Epoch [5/5], Step [57750/68337], Loss: 5.1872\n",
      "Epoch [5/5], Step [57825/68337], Loss: 5.0846\n",
      "Epoch [5/5], Step [57900/68337], Loss: 5.0152\n",
      "Epoch [5/5], Step [57975/68337], Loss: 5.1920\n",
      "Epoch [5/5], Step [58050/68337], Loss: 5.1288\n",
      "Epoch [5/5], Step [58125/68337], Loss: 5.0955\n",
      "Epoch [5/5], Step [58200/68337], Loss: 5.0674\n",
      "Epoch [5/5], Step [58275/68337], Loss: 5.0301\n",
      "Epoch [5/5], Step [58350/68337], Loss: 5.0592\n",
      "Epoch [5/5], Step [58425/68337], Loss: 5.1987\n",
      "Epoch [5/5], Step [58500/68337], Loss: 5.0985\n",
      "Epoch [5/5], Step [58575/68337], Loss: 5.0982\n",
      "Epoch [5/5], Step [58650/68337], Loss: 5.1213\n",
      "Epoch [5/5], Step [58725/68337], Loss: 5.0587\n",
      "Epoch [5/5], Step [58800/68337], Loss: 4.9837\n",
      "Epoch [5/5], Step [58875/68337], Loss: 4.9163\n",
      "Epoch [5/5], Step [58950/68337], Loss: 5.0857\n",
      "Epoch [5/5], Step [59025/68337], Loss: 5.0256\n",
      "Epoch [5/5], Step [59100/68337], Loss: 4.9743\n",
      "Epoch [5/5], Step [59175/68337], Loss: 5.2426\n",
      "Epoch [5/5], Step [59250/68337], Loss: 5.1486\n",
      "Epoch [5/5], Step [59325/68337], Loss: 4.9775\n",
      "Epoch [5/5], Step [59400/68337], Loss: 5.0182\n",
      "Epoch [5/5], Step [59475/68337], Loss: 5.0072\n",
      "Epoch [5/5], Step [59550/68337], Loss: 4.9360\n",
      "Epoch [5/5], Step [59625/68337], Loss: 4.9896\n",
      "Epoch [5/5], Step [59700/68337], Loss: 4.9883\n",
      "Epoch [5/5], Step [59775/68337], Loss: 5.1548\n",
      "Epoch [5/5], Step [59850/68337], Loss: 5.0151\n",
      "Epoch [5/5], Step [59925/68337], Loss: 5.0112\n",
      "Epoch [5/5], Step [60000/68337], Loss: 5.2539\n",
      "Validation perplexity: 146.4871791261116\n",
      "Epoch [5/5], Step [60075/68337], Loss: 4.9156\n",
      "Epoch [5/5], Step [60150/68337], Loss: 5.0488\n",
      "Epoch [5/5], Step [60225/68337], Loss: 4.9279\n",
      "Epoch [5/5], Step [60300/68337], Loss: 5.1386\n",
      "Epoch [5/5], Step [60375/68337], Loss: 5.0853\n",
      "Epoch [5/5], Step [60450/68337], Loss: 5.0191\n",
      "Epoch [5/5], Step [60525/68337], Loss: 4.9722\n",
      "Epoch [5/5], Step [60600/68337], Loss: 4.9137\n",
      "Epoch [5/5], Step [60675/68337], Loss: 4.9853\n",
      "Epoch [5/5], Step [60750/68337], Loss: 5.1403\n",
      "Epoch [5/5], Step [60825/68337], Loss: 5.2069\n",
      "Epoch [5/5], Step [60900/68337], Loss: 5.1434\n",
      "Epoch [5/5], Step [60975/68337], Loss: 5.0775\n",
      "Epoch [5/5], Step [61050/68337], Loss: 5.0138\n",
      "Epoch [5/5], Step [61125/68337], Loss: 4.9930\n",
      "Epoch [5/5], Step [61200/68337], Loss: 5.0756\n",
      "Epoch [5/5], Step [61275/68337], Loss: 5.0106\n",
      "Epoch [5/5], Step [61350/68337], Loss: 4.8365\n",
      "Epoch [5/5], Step [61425/68337], Loss: 5.2318\n",
      "Epoch [5/5], Step [61500/68337], Loss: 4.9494\n",
      "Epoch [5/5], Step [61575/68337], Loss: 4.9550\n",
      "Epoch [5/5], Step [61650/68337], Loss: 5.0520\n",
      "Epoch [5/5], Step [61725/68337], Loss: 5.1141\n",
      "Epoch [5/5], Step [61800/68337], Loss: 5.0308\n",
      "Epoch [5/5], Step [61875/68337], Loss: 5.0381\n",
      "Epoch [5/5], Step [61950/68337], Loss: 5.0485\n",
      "Epoch [5/5], Step [62025/68337], Loss: 5.0275\n",
      "Epoch [5/5], Step [62100/68337], Loss: 5.1056\n",
      "Epoch [5/5], Step [62175/68337], Loss: 4.9235\n",
      "Epoch [5/5], Step [62250/68337], Loss: 5.0875\n",
      "Epoch [5/5], Step [62325/68337], Loss: 5.1614\n",
      "Epoch [5/5], Step [62400/68337], Loss: 4.9451\n",
      "Epoch [5/5], Step [62475/68337], Loss: 5.0351\n",
      "Epoch [5/5], Step [62550/68337], Loss: 5.1212\n",
      "Epoch [5/5], Step [62625/68337], Loss: 5.1023\n",
      "Epoch [5/5], Step [62700/68337], Loss: 5.0238\n",
      "Epoch [5/5], Step [62775/68337], Loss: 5.0055\n",
      "Epoch [5/5], Step [62850/68337], Loss: 4.9416\n",
      "Epoch [5/5], Step [62925/68337], Loss: 4.9028\n",
      "Epoch [5/5], Step [63000/68337], Loss: 4.9563\n",
      "Epoch [5/5], Step [63075/68337], Loss: 5.0417\n",
      "Epoch [5/5], Step [63150/68337], Loss: 4.9500\n",
      "Epoch [5/5], Step [63225/68337], Loss: 4.8077\n",
      "Epoch [5/5], Step [63300/68337], Loss: 4.9461\n",
      "Epoch [5/5], Step [63375/68337], Loss: 5.0494\n",
      "Epoch [5/5], Step [63450/68337], Loss: 4.9686\n",
      "Epoch [5/5], Step [63525/68337], Loss: 4.8906\n",
      "Epoch [5/5], Step [63600/68337], Loss: 5.0313\n",
      "Epoch [5/5], Step [63675/68337], Loss: 5.0470\n",
      "Epoch [5/5], Step [63750/68337], Loss: 4.9879\n",
      "Epoch [5/5], Step [63825/68337], Loss: 5.1741\n",
      "Epoch [5/5], Step [63900/68337], Loss: 5.0873\n",
      "Epoch [5/5], Step [63975/68337], Loss: 5.0552\n",
      "Epoch [5/5], Step [64050/68337], Loss: 5.0247\n",
      "Epoch [5/5], Step [64125/68337], Loss: 4.9311\n",
      "Epoch [5/5], Step [64200/68337], Loss: 4.9900\n",
      "Epoch [5/5], Step [64275/68337], Loss: 5.0913\n",
      "Epoch [5/5], Step [64350/68337], Loss: 5.1921\n",
      "Epoch [5/5], Step [64425/68337], Loss: 4.9895\n",
      "Epoch [5/5], Step [64500/68337], Loss: 5.0340\n",
      "Epoch [5/5], Step [64575/68337], Loss: 4.8367\n",
      "Epoch [5/5], Step [64650/68337], Loss: 5.0271\n",
      "Epoch [5/5], Step [64725/68337], Loss: 5.0479\n",
      "Epoch [5/5], Step [64800/68337], Loss: 4.9791\n",
      "Epoch [5/5], Step [64875/68337], Loss: 5.0592\n",
      "Epoch [5/5], Step [64950/68337], Loss: 5.0072\n",
      "Epoch [5/5], Step [65025/68337], Loss: 5.0794\n",
      "Epoch [5/5], Step [65100/68337], Loss: 5.1075\n",
      "Epoch [5/5], Step [65175/68337], Loss: 4.8289\n",
      "Epoch [5/5], Step [65250/68337], Loss: 5.0003\n",
      "Epoch [5/5], Step [65325/68337], Loss: 4.9365\n",
      "Epoch [5/5], Step [65400/68337], Loss: 5.0170\n",
      "Epoch [5/5], Step [65475/68337], Loss: 5.0119\n",
      "Epoch [5/5], Step [65550/68337], Loss: 5.0354\n",
      "Epoch [5/5], Step [65625/68337], Loss: 5.2134\n",
      "Epoch [5/5], Step [65700/68337], Loss: 5.1324\n",
      "Epoch [5/5], Step [65775/68337], Loss: 4.8961\n",
      "Epoch [5/5], Step [65850/68337], Loss: 5.0551\n",
      "Epoch [5/5], Step [65925/68337], Loss: 5.0457\n",
      "Epoch [5/5], Step [66000/68337], Loss: 4.9491\n",
      "Epoch [5/5], Step [66075/68337], Loss: 5.0379\n",
      "Epoch [5/5], Step [66150/68337], Loss: 4.9966\n",
      "Epoch [5/5], Step [66225/68337], Loss: 5.0121\n",
      "Epoch [5/5], Step [66300/68337], Loss: 5.0576\n",
      "Epoch [5/5], Step [66375/68337], Loss: 5.0320\n",
      "Epoch [5/5], Step [66450/68337], Loss: 5.1549\n",
      "Epoch [5/5], Step [66525/68337], Loss: 4.6930\n",
      "Epoch [5/5], Step [66600/68337], Loss: 4.7984\n",
      "Epoch [5/5], Step [66675/68337], Loss: 5.2318\n",
      "Epoch [5/5], Step [66750/68337], Loss: 5.2165\n",
      "Epoch [5/5], Step [66825/68337], Loss: 4.8440\n",
      "Epoch [5/5], Step [66900/68337], Loss: 4.9533\n",
      "Epoch [5/5], Step [66975/68337], Loss: 5.1798\n",
      "Epoch [5/5], Step [67050/68337], Loss: 4.9906\n",
      "Epoch [5/5], Step [67125/68337], Loss: 4.9234\n",
      "Epoch [5/5], Step [67200/68337], Loss: 4.9759\n",
      "Epoch [5/5], Step [67275/68337], Loss: 5.0335\n",
      "Epoch [5/5], Step [67350/68337], Loss: 5.0190\n",
      "Epoch [5/5], Step [67425/68337], Loss: 5.1000\n",
      "Epoch [5/5], Step [67500/68337], Loss: 5.1893\n",
      "Epoch [5/5], Step [67575/68337], Loss: 5.0423\n",
      "Epoch [5/5], Step [67650/68337], Loss: 4.9173\n",
      "Epoch [5/5], Step [67725/68337], Loss: 5.1450\n",
      "Epoch [5/5], Step [67800/68337], Loss: 4.9356\n",
      "Epoch [5/5], Step [67875/68337], Loss: 5.0240\n",
      "Epoch [5/5], Step [67950/68337], Loss: 5.1320\n",
      "Epoch [5/5], Step [68025/68337], Loss: 4.9091\n",
      "Epoch [5/5], Step [68100/68337], Loss: 4.9211\n",
      "Epoch [5/5], Step [68175/68337], Loss: 5.1451\n",
      "Epoch [5/5], Step [68250/68337], Loss: 5.0639\n",
      "Epoch [5/5], Step [68325/68337], Loss: 5.0250\n",
      "Epoch [5/5] Average Loss: 5.0278, Perplexity: 152.60\n"
     ]
    }
   ],
   "source": [
    "from src.model import RegularizedLanguageModel, SimpleLanguageModel,LanguageModelExtraRelu\n",
    "from src.trainComplete import TrainComplete\n",
    "trainclass = TrainComplete(text_path = text_path,path_to_save_folder= path_to_save_folder,tokenizer = tokenizer,allowed_special=False)\n",
    "\n",
    "model = LanguageModelExtraRelu(vocab_size, embedding_dim, context_length).to(device)\n",
    "\n",
    "\n",
    "trainclass.train(model,\n",
    "              vocab_size,device,raw_text,\"pers_LinearRelu_dopout_2Relu_ep5_batchsize32_evaluate_every10000\",\n",
    "                print_every=75,evaluate_every=10000,optimizer=None,criterion=None,\n",
    "              batch_size = 32,\n",
    "              embedding_dim = 128,\n",
    "              context_length = 32,\n",
    "              num_epochs = 5\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095bcd72-1159-45df-86ca-dc74a9a9bddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b114ef8-aba8-478e-9e48-19bfb82a4559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e56383a9-cfa9-4a08-ac9b-dd478cd634eb",
   "metadata": {},
   "source": [
    "## Generate Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9481004-79e1-46bc-9981-809f17576d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import generate_text\n",
    "start_text = \" من در راه\"\n",
    "for x in range(10):\n",
    "    generated_text = generate_text(model, tokenizer, start_text, device=device, context_length=20)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c60278-566c-454d-b078-e9d07de941c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Extra Trainining Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224e67e-4246-4e77-8463-5e80cb2de3d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0653dac0-5a2e-49e9-b968-c57d112eb530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create Dataset 2720000 / 2733504Started Training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Long and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     13\u001b[0m num_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;66;03m# 8 #2\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m MultiHeadCausalAttentionCorrect(\n\u001b[1;32m     16\u001b[0m     embedding_dim, attention_dim, num_heads, context_length, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m     17\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrainclass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mraw_text\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpers_test_attention_multiHeadTest_eval10000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mevaluate_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m              \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m              \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m              \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontext_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m              \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[43m             \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/vast-react/home/joris.hellwig/u13685/jupyterhub-gwdg/src/trainComplete.py:51\u001b[0m, in \u001b[0;36mTrainComplete.train\u001b[0;34m(self, model, vocab_size, device, raw_text, train_run_label, print_every, evaluate_every, optimizer, criterion, batch_size, embedding_dim, context_length, num_epochs)\u001b[0m\n\u001b[1;32m     49\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m train_dataloader\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_attention_training:\n\u001b[0;32m---> 51\u001b[0m     (all_losses,train_losses,perplexities,all_perplex) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mpath_to_save_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mtrain_run_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mevaluate_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdev_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:   \n\u001b[1;32m     60\u001b[0m     (all_losses,train_losses,perplexities,all_perplex) \u001b[38;5;241m=\u001b[39m train(model,\n\u001b[1;32m     61\u001b[0m                                                            num_epochs,\n\u001b[1;32m     62\u001b[0m                                                            optimizer,criterion,data_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m                                                            evaluate_every,dev_dataloader,\n\u001b[1;32m     67\u001b[0m                                                            print_every)\n",
      "File \u001b[0;32m/mnt/vast-react/home/joris.hellwig/u13685/jupyterhub-gwdg/src/train.py:90\u001b[0m, in \u001b[0;36mtrain_attention\u001b[0;34m(model, num_epochs, optimizer, criterion, data_loader, path_to_save_folder, train_run_label, vocab_size, device, evaluate_every, dev_dataloader, print_every)\u001b[0m\n\u001b[1;32m     87\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     89\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 90\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     93\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/vast-react/home/joris.hellwig/u13685/jupyterhub-gwdg/src/attentionModel.py:198\u001b[0m, in \u001b[0;36mMultiHeadCausalAttentionCorrect.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    196\u001b[0m     batch_size, seq_length \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;66;03m#, _\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_k(x)\n\u001b[1;32m    200\u001b[0m     V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_v(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Long and Float"
     ]
    }
   ],
   "source": [
    "from src.trainComplete import TrainComplete\n",
    "from src.attentionModel import LanguageModelWithAttention,MultiHeadCausalAttentionCorrect\n",
    "\n",
    "\n",
    "trainclass = TrainComplete(text_path = text_path,path_to_save_folder= path_to_save_folder,tokenizer = tokenizer,\n",
    "                           allowed_special=False, is_attention_training = True)\n",
    "\n",
    "\n",
    "context_length = 32  # Increased context size\n",
    "embedding_dim = 128\n",
    "attention_dim = 64\n",
    "hidden_dim = 64\n",
    "num_heads = 4 # 8 #2\n",
    "\n",
    "model = MultiHeadCausalAttentionCorrect(\n",
    "    embedding_dim, attention_dim, num_heads, context_length, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "trainclass.train(model,\n",
    "              vocab_size,device,raw_text,\"pers_test_attention_multiHeadTest_eval10000\",\n",
    "                print_every=75,evaluate_every=10000,optimizer=None,criterion=None,\n",
    "              batch_size = 32,\n",
    "              embedding_dim = embedding_dim,\n",
    "              context_length = context_length,\n",
    "              num_epochs = 1,\n",
    "\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8a220-0d91-4bfb-9096-5e7f847fb103",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Evening Training RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e72aec-83d6-4104-be20-e3970605fabe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:  100000  lines seperators replaced\n",
      "Total lines not removed :  97334\n",
      "Total lines replaced 39736\n",
      "Total lines replaced 1561\n",
      " Create Dataset 2720000 / 2721620Epoch [1/4], Step [0/68040], Loss: 10.7264\n",
      "Validation perplexity: 41300.6966252291\n",
      "Epoch [1/4], Step [75/68040], Loss: 8.8826\n",
      "Epoch [1/4], Step [150/68040], Loss: 7.6357\n",
      "Epoch [1/4], Step [225/68040], Loss: 7.2412\n",
      "Epoch [1/4], Step [300/68040], Loss: 6.9935\n",
      "Epoch [1/4], Step [375/68040], Loss: 6.9512\n",
      "Epoch [1/4], Step [450/68040], Loss: 6.8882\n",
      "Epoch [1/4], Step [525/68040], Loss: 6.6721\n",
      "Epoch [1/4], Step [600/68040], Loss: 6.8100\n",
      "Epoch [1/4], Step [675/68040], Loss: 6.7722\n",
      "Epoch [1/4], Step [750/68040], Loss: 6.5833\n",
      "Epoch [1/4], Step [825/68040], Loss: 6.6680\n",
      "Epoch [1/4], Step [900/68040], Loss: 6.4995\n",
      "Epoch [1/4], Step [975/68040], Loss: 6.6120\n",
      "Epoch [1/4], Step [1050/68040], Loss: 6.4162\n",
      "Epoch [1/4], Step [1125/68040], Loss: 6.4613\n",
      "Epoch [1/4], Step [1200/68040], Loss: 6.5663\n",
      "Epoch [1/4], Step [1275/68040], Loss: 6.4200\n",
      "Epoch [1/4], Step [1350/68040], Loss: 6.2067\n",
      "Epoch [1/4], Step [1425/68040], Loss: 6.1961\n",
      "Epoch [1/4], Step [1500/68040], Loss: 6.2883\n",
      "Epoch [1/4], Step [1575/68040], Loss: 6.1925\n",
      "Epoch [1/4], Step [1650/68040], Loss: 6.4509\n",
      "Epoch [1/4], Step [1725/68040], Loss: 6.5303\n",
      "Epoch [1/4], Step [1800/68040], Loss: 6.3294\n",
      "Epoch [1/4], Step [1875/68040], Loss: 6.2004\n",
      "Epoch [1/4], Step [1950/68040], Loss: 6.2781\n",
      "Epoch [1/4], Step [2025/68040], Loss: 6.2245\n",
      "Epoch [1/4], Step [2100/68040], Loss: 5.9872\n",
      "Epoch [1/4], Step [2175/68040], Loss: 6.1808\n",
      "Epoch [1/4], Step [2250/68040], Loss: 6.2613\n",
      "Epoch [1/4], Step [2325/68040], Loss: 6.1031\n",
      "Epoch [1/4], Step [2400/68040], Loss: 6.1136\n",
      "Epoch [1/4], Step [2475/68040], Loss: 6.3031\n",
      "Epoch [1/4], Step [2550/68040], Loss: 6.0929\n",
      "Epoch [1/4], Step [2625/68040], Loss: 5.9648\n",
      "Epoch [1/4], Step [2700/68040], Loss: 6.1801\n",
      "Epoch [1/4], Step [2775/68040], Loss: 6.1648\n",
      "Epoch [1/4], Step [2850/68040], Loss: 6.2574\n",
      "Epoch [1/4], Step [2925/68040], Loss: 5.8719\n",
      "Epoch [1/4], Step [3000/68040], Loss: 5.9131\n",
      "Epoch [1/4], Step [3075/68040], Loss: 6.0297\n",
      "Epoch [1/4], Step [3150/68040], Loss: 5.6615\n",
      "Epoch [1/4], Step [3225/68040], Loss: 5.9160\n",
      "Epoch [1/4], Step [3300/68040], Loss: 6.0810\n",
      "Epoch [1/4], Step [3375/68040], Loss: 6.0343\n",
      "Epoch [1/4], Step [3450/68040], Loss: 5.7826\n",
      "Epoch [1/4], Step [3525/68040], Loss: 6.1733\n",
      "Epoch [1/4], Step [3600/68040], Loss: 5.9352\n",
      "Epoch [1/4], Step [3675/68040], Loss: 6.0355\n",
      "Epoch [1/4], Step [3750/68040], Loss: 5.8709\n",
      "Epoch [1/4], Step [3825/68040], Loss: 5.8692\n",
      "Epoch [1/4], Step [3900/68040], Loss: 5.8592\n",
      "Epoch [1/4], Step [3975/68040], Loss: 5.9014\n",
      "Epoch [1/4], Step [4050/68040], Loss: 6.0298\n",
      "Epoch [1/4], Step [4125/68040], Loss: 5.6960\n",
      "Epoch [1/4], Step [4200/68040], Loss: 6.0432\n",
      "Epoch [1/4], Step [4275/68040], Loss: 5.9663\n",
      "Epoch [1/4], Step [4350/68040], Loss: 5.9602\n",
      "Epoch [1/4], Step [4425/68040], Loss: 5.8895\n",
      "Epoch [1/4], Step [4500/68040], Loss: 6.0694\n",
      "Epoch [1/4], Step [4575/68040], Loss: 5.8732\n",
      "Epoch [1/4], Step [4650/68040], Loss: 5.8358\n",
      "Epoch [1/4], Step [4725/68040], Loss: 5.9934\n",
      "Epoch [1/4], Step [4800/68040], Loss: 5.6818\n",
      "Epoch [1/4], Step [4875/68040], Loss: 6.0112\n",
      "Epoch [1/4], Step [4950/68040], Loss: 6.0362\n",
      "Epoch [1/4], Step [5025/68040], Loss: 5.9939\n",
      "Epoch [1/4], Step [5100/68040], Loss: 6.0169\n",
      "Epoch [1/4], Step [5175/68040], Loss: 5.9518\n",
      "Epoch [1/4], Step [5250/68040], Loss: 5.7086\n",
      "Epoch [1/4], Step [5325/68040], Loss: 5.7925\n",
      "Epoch [1/4], Step [5400/68040], Loss: 6.0122\n",
      "Epoch [1/4], Step [5475/68040], Loss: 5.5563\n",
      "Epoch [1/4], Step [5550/68040], Loss: 5.9383\n",
      "Epoch [1/4], Step [5625/68040], Loss: 5.7643\n",
      "Epoch [1/4], Step [5700/68040], Loss: 5.9279\n",
      "Epoch [1/4], Step [5775/68040], Loss: 5.9996\n",
      "Epoch [1/4], Step [5850/68040], Loss: 5.8191\n",
      "Epoch [1/4], Step [5925/68040], Loss: 5.9508\n",
      "Epoch [1/4], Step [6000/68040], Loss: 5.7017\n",
      "Epoch [1/4], Step [6075/68040], Loss: 5.5763\n",
      "Epoch [1/4], Step [6150/68040], Loss: 5.8291\n",
      "Epoch [1/4], Step [6225/68040], Loss: 5.5504\n",
      "Epoch [1/4], Step [6300/68040], Loss: 5.6100\n",
      "Epoch [1/4], Step [6375/68040], Loss: 5.7091\n",
      "Epoch [1/4], Step [6450/68040], Loss: 5.6007\n",
      "Epoch [1/4], Step [6525/68040], Loss: 5.4318\n",
      "Epoch [1/4], Step [6600/68040], Loss: 5.9545\n",
      "Epoch [1/4], Step [6675/68040], Loss: 5.7906\n",
      "Epoch [1/4], Step [6750/68040], Loss: 5.9165\n",
      "Epoch [1/4], Step [6825/68040], Loss: 5.6168\n",
      "Epoch [1/4], Step [6900/68040], Loss: 5.9591\n",
      "Epoch [1/4], Step [6975/68040], Loss: 5.8120\n",
      "Epoch [1/4], Step [7050/68040], Loss: 5.7425\n",
      "Epoch [1/4], Step [7125/68040], Loss: 5.7606\n",
      "Epoch [1/4], Step [7200/68040], Loss: 5.5239\n",
      "Epoch [1/4], Step [7275/68040], Loss: 5.8139\n",
      "Epoch [1/4], Step [7350/68040], Loss: 5.7849\n",
      "Epoch [1/4], Step [7425/68040], Loss: 5.7297\n",
      "Epoch [1/4], Step [7500/68040], Loss: 5.7147\n",
      "Epoch [1/4], Step [7575/68040], Loss: 5.6260\n",
      "Epoch [1/4], Step [7650/68040], Loss: 5.9667\n",
      "Epoch [1/4], Step [7725/68040], Loss: 5.6963\n",
      "Epoch [1/4], Step [7800/68040], Loss: 5.7570\n",
      "Epoch [1/4], Step [7875/68040], Loss: 5.7375\n",
      "Epoch [1/4], Step [7950/68040], Loss: 5.7794\n",
      "Epoch [1/4], Step [8025/68040], Loss: 5.5380\n",
      "Epoch [1/4], Step [8100/68040], Loss: 5.8956\n",
      "Epoch [1/4], Step [8175/68040], Loss: 5.5817\n",
      "Epoch [1/4], Step [8250/68040], Loss: 5.7391\n",
      "Epoch [1/4], Step [8325/68040], Loss: 5.7425\n",
      "Epoch [1/4], Step [8400/68040], Loss: 5.7495\n",
      "Epoch [1/4], Step [8475/68040], Loss: 5.5184\n",
      "Epoch [1/4], Step [8550/68040], Loss: 5.5946\n",
      "Epoch [1/4], Step [8625/68040], Loss: 5.7018\n",
      "Epoch [1/4], Step [8700/68040], Loss: 5.6914\n",
      "Epoch [1/4], Step [8775/68040], Loss: 5.7528\n",
      "Epoch [1/4], Step [8850/68040], Loss: 5.5906\n",
      "Epoch [1/4], Step [8925/68040], Loss: 5.8083\n",
      "Epoch [1/4], Step [9000/68040], Loss: 5.7822\n",
      "Epoch [1/4], Step [9075/68040], Loss: 5.6861\n",
      "Epoch [1/4], Step [9150/68040], Loss: 5.4673\n",
      "Epoch [1/4], Step [9225/68040], Loss: 5.4503\n",
      "Epoch [1/4], Step [9300/68040], Loss: 5.7170\n",
      "Epoch [1/4], Step [9375/68040], Loss: 5.4855\n",
      "Epoch [1/4], Step [9450/68040], Loss: 5.4914\n",
      "Epoch [1/4], Step [9525/68040], Loss: 5.5149\n",
      "Epoch [1/4], Step [9600/68040], Loss: 5.5287\n",
      "Epoch [1/4], Step [9675/68040], Loss: 5.8534\n",
      "Epoch [1/4], Step [9750/68040], Loss: 5.7050\n",
      "Epoch [1/4], Step [9825/68040], Loss: 5.5201\n",
      "Epoch [1/4], Step [9900/68040], Loss: 5.8108\n",
      "Epoch [1/4], Step [9975/68040], Loss: 5.7741\n",
      "Validation perplexity: 239.3849898148968\n",
      "Epoch [1/4], Step [10050/68040], Loss: 5.7118\n",
      "Epoch [1/4], Step [10125/68040], Loss: 5.7975\n",
      "Epoch [1/4], Step [10200/68040], Loss: 5.6442\n",
      "Epoch [1/4], Step [10275/68040], Loss: 5.6498\n",
      "Epoch [1/4], Step [10350/68040], Loss: 5.6627\n",
      "Epoch [1/4], Step [10425/68040], Loss: 5.5234\n",
      "Epoch [1/4], Step [10500/68040], Loss: 5.5756\n",
      "Epoch [1/4], Step [10575/68040], Loss: 5.4752\n",
      "Epoch [1/4], Step [10650/68040], Loss: 5.5282\n",
      "Epoch [1/4], Step [10725/68040], Loss: 5.5622\n",
      "Epoch [1/4], Step [10800/68040], Loss: 5.6538\n",
      "Epoch [1/4], Step [10875/68040], Loss: 5.4215\n",
      "Epoch [1/4], Step [10950/68040], Loss: 5.5201\n",
      "Epoch [1/4], Step [11025/68040], Loss: 5.5864\n",
      "Epoch [1/4], Step [11100/68040], Loss: 5.3923\n",
      "Epoch [1/4], Step [11175/68040], Loss: 5.6711\n",
      "Epoch [1/4], Step [11250/68040], Loss: 5.6243\n",
      "Epoch [1/4], Step [11325/68040], Loss: 5.3925\n",
      "Epoch [1/4], Step [11400/68040], Loss: 5.6546\n",
      "Epoch [1/4], Step [11475/68040], Loss: 5.5567\n",
      "Epoch [1/4], Step [11550/68040], Loss: 5.7953\n",
      "Epoch [1/4], Step [11625/68040], Loss: 5.6168\n",
      "Epoch [1/4], Step [11700/68040], Loss: 5.6147\n",
      "Epoch [1/4], Step [11775/68040], Loss: 5.4405\n",
      "Epoch [1/4], Step [11850/68040], Loss: 5.5202\n",
      "Epoch [1/4], Step [11925/68040], Loss: 5.6772\n",
      "Epoch [1/4], Step [12000/68040], Loss: 5.6230\n",
      "Epoch [1/4], Step [12075/68040], Loss: 5.4316\n",
      "Epoch [1/4], Step [12150/68040], Loss: 5.4233\n",
      "Epoch [1/4], Step [12225/68040], Loss: 5.5800\n",
      "Epoch [1/4], Step [12300/68040], Loss: 5.5829\n",
      "Epoch [1/4], Step [12375/68040], Loss: 5.6138\n",
      "Epoch [1/4], Step [12450/68040], Loss: 5.5656\n",
      "Epoch [1/4], Step [12525/68040], Loss: 5.6166\n",
      "Epoch [1/4], Step [12600/68040], Loss: 5.4541\n",
      "Epoch [1/4], Step [12675/68040], Loss: 5.4749\n",
      "Epoch [1/4], Step [12750/68040], Loss: 5.4051\n",
      "Epoch [1/4], Step [12825/68040], Loss: 5.5225\n",
      "Epoch [1/4], Step [12900/68040], Loss: 5.4709\n",
      "Epoch [1/4], Step [12975/68040], Loss: 5.4344\n",
      "Epoch [1/4], Step [13050/68040], Loss: 5.5336\n",
      "Epoch [1/4], Step [13125/68040], Loss: 5.3682\n",
      "Epoch [1/4], Step [13200/68040], Loss: 5.5442\n",
      "Epoch [1/4], Step [13275/68040], Loss: 5.5980\n",
      "Epoch [1/4], Step [13350/68040], Loss: 5.5754\n",
      "Epoch [1/4], Step [13425/68040], Loss: 5.5160\n",
      "Epoch [1/4], Step [13500/68040], Loss: 5.4298\n",
      "Epoch [1/4], Step [13575/68040], Loss: 5.5976\n",
      "Epoch [1/4], Step [13650/68040], Loss: 5.5130\n",
      "Epoch [1/4], Step [13725/68040], Loss: 5.6873\n",
      "Epoch [1/4], Step [13800/68040], Loss: 5.5086\n",
      "Epoch [1/4], Step [13875/68040], Loss: 5.5619\n",
      "Epoch [1/4], Step [13950/68040], Loss: 5.1579\n",
      "Epoch [1/4], Step [14025/68040], Loss: 5.3907\n",
      "Epoch [1/4], Step [14100/68040], Loss: 5.5376\n",
      "Epoch [1/4], Step [14175/68040], Loss: 5.6269\n",
      "Epoch [1/4], Step [14250/68040], Loss: 5.4930\n",
      "Epoch [1/4], Step [14325/68040], Loss: 5.3706\n",
      "Epoch [1/4], Step [14400/68040], Loss: 5.4164\n",
      "Epoch [1/4], Step [14475/68040], Loss: 5.3725\n",
      "Epoch [1/4], Step [14550/68040], Loss: 5.4820\n",
      "Epoch [1/4], Step [14625/68040], Loss: 5.5281\n",
      "Epoch [1/4], Step [14700/68040], Loss: 5.5071\n",
      "Epoch [1/4], Step [14775/68040], Loss: 5.5788\n",
      "Epoch [1/4], Step [14850/68040], Loss: 5.5546\n",
      "Epoch [1/4], Step [14925/68040], Loss: 5.3452\n",
      "Epoch [1/4], Step [15000/68040], Loss: 5.4456\n",
      "Epoch [1/4], Step [15075/68040], Loss: 5.3690\n",
      "Epoch [1/4], Step [15150/68040], Loss: 5.6275\n",
      "Epoch [1/4], Step [15225/68040], Loss: 5.5627\n",
      "Epoch [1/4], Step [15300/68040], Loss: 5.4887\n",
      "Epoch [1/4], Step [15375/68040], Loss: 5.3711\n",
      "Epoch [1/4], Step [15450/68040], Loss: 5.4916\n",
      "Epoch [1/4], Step [15525/68040], Loss: 5.4608\n",
      "Epoch [1/4], Step [15600/68040], Loss: 5.2101\n",
      "Epoch [1/4], Step [15675/68040], Loss: 5.4543\n",
      "Epoch [1/4], Step [15750/68040], Loss: 5.5777\n",
      "Epoch [1/4], Step [15825/68040], Loss: 5.4267\n",
      "Epoch [1/4], Step [15900/68040], Loss: 5.4724\n",
      "Epoch [1/4], Step [15975/68040], Loss: 5.6028\n",
      "Epoch [1/4], Step [16050/68040], Loss: 5.3078\n",
      "Epoch [1/4], Step [16125/68040], Loss: 5.6203\n",
      "Epoch [1/4], Step [16200/68040], Loss: 5.1918\n",
      "Epoch [1/4], Step [16275/68040], Loss: 5.4726\n",
      "Epoch [1/4], Step [16350/68040], Loss: 5.6257\n",
      "Epoch [1/4], Step [16425/68040], Loss: 5.4704\n",
      "Epoch [1/4], Step [16500/68040], Loss: 5.3050\n",
      "Epoch [1/4], Step [16575/68040], Loss: 5.3776\n",
      "Epoch [1/4], Step [16650/68040], Loss: 5.4009\n",
      "Epoch [1/4], Step [16725/68040], Loss: 5.4022\n",
      "Epoch [1/4], Step [16800/68040], Loss: 5.4007\n",
      "Epoch [1/4], Step [16875/68040], Loss: 5.3591\n",
      "Epoch [1/4], Step [16950/68040], Loss: 5.5545\n",
      "Epoch [1/4], Step [17025/68040], Loss: 5.4690\n",
      "Epoch [1/4], Step [17100/68040], Loss: 5.5833\n",
      "Epoch [1/4], Step [17175/68040], Loss: 5.5066\n",
      "Epoch [1/4], Step [17250/68040], Loss: 5.3528\n",
      "Epoch [1/4], Step [17325/68040], Loss: 5.4631\n",
      "Epoch [1/4], Step [17400/68040], Loss: 5.4832\n",
      "Epoch [1/4], Step [17475/68040], Loss: 5.4920\n",
      "Epoch [1/4], Step [17550/68040], Loss: 5.4699\n",
      "Epoch [1/4], Step [17625/68040], Loss: 5.5446\n",
      "Epoch [1/4], Step [17700/68040], Loss: 5.4064\n",
      "Epoch [1/4], Step [17775/68040], Loss: 5.3246\n",
      "Epoch [1/4], Step [17850/68040], Loss: 5.4150\n",
      "Epoch [1/4], Step [17925/68040], Loss: 5.3429\n",
      "Epoch [1/4], Step [18000/68040], Loss: 5.3637\n",
      "Epoch [1/4], Step [18075/68040], Loss: 5.4456\n",
      "Epoch [1/4], Step [18150/68040], Loss: 5.2827\n",
      "Epoch [1/4], Step [18225/68040], Loss: 5.3042\n",
      "Epoch [1/4], Step [18300/68040], Loss: 5.5111\n",
      "Epoch [1/4], Step [18375/68040], Loss: 5.5168\n",
      "Epoch [1/4], Step [18450/68040], Loss: 5.3311\n",
      "Epoch [1/4], Step [18525/68040], Loss: 5.3934\n",
      "Epoch [1/4], Step [18600/68040], Loss: 5.4199\n",
      "Epoch [1/4], Step [18675/68040], Loss: 5.3630\n",
      "Epoch [1/4], Step [18750/68040], Loss: 5.6579\n",
      "Epoch [1/4], Step [18825/68040], Loss: 5.4968\n",
      "Epoch [1/4], Step [18900/68040], Loss: 5.3172\n",
      "Epoch [1/4], Step [18975/68040], Loss: 5.5978\n",
      "Epoch [1/4], Step [19050/68040], Loss: 5.2709\n",
      "Epoch [1/4], Step [19125/68040], Loss: 5.3750\n",
      "Epoch [1/4], Step [19200/68040], Loss: 5.5640\n",
      "Epoch [1/4], Step [19275/68040], Loss: 5.4847\n",
      "Epoch [1/4], Step [19350/68040], Loss: 5.4625\n",
      "Epoch [1/4], Step [19425/68040], Loss: 5.3738\n",
      "Epoch [1/4], Step [19500/68040], Loss: 5.3572\n",
      "Epoch [1/4], Step [19575/68040], Loss: 5.4710\n",
      "Epoch [1/4], Step [19650/68040], Loss: 5.3874\n",
      "Epoch [1/4], Step [19725/68040], Loss: 5.2821\n",
      "Epoch [1/4], Step [19800/68040], Loss: 5.3403\n",
      "Epoch [1/4], Step [19875/68040], Loss: 5.5694\n",
      "Epoch [1/4], Step [19950/68040], Loss: 5.5249\n",
      "Validation perplexity: 196.2447972845473\n",
      "Epoch [1/4], Step [20025/68040], Loss: 5.6070\n",
      "Epoch [1/4], Step [20100/68040], Loss: 5.4999\n",
      "Epoch [1/4], Step [20175/68040], Loss: 5.5797\n",
      "Epoch [1/4], Step [20250/68040], Loss: 5.4009\n",
      "Epoch [1/4], Step [20325/68040], Loss: 5.4781\n",
      "Epoch [1/4], Step [20400/68040], Loss: 5.4050\n",
      "Epoch [1/4], Step [20475/68040], Loss: 5.2922\n",
      "Epoch [1/4], Step [20550/68040], Loss: 5.2874\n",
      "Epoch [1/4], Step [20625/68040], Loss: 5.2094\n",
      "Epoch [1/4], Step [20700/68040], Loss: 5.5140\n",
      "Epoch [1/4], Step [20775/68040], Loss: 5.5119\n",
      "Epoch [1/4], Step [20850/68040], Loss: 5.5266\n",
      "Epoch [1/4], Step [20925/68040], Loss: 5.5525\n",
      "Epoch [1/4], Step [21000/68040], Loss: 5.2724\n",
      "Epoch [1/4], Step [21075/68040], Loss: 5.5837\n",
      "Epoch [1/4], Step [21150/68040], Loss: 5.4439\n",
      "Epoch [1/4], Step [21225/68040], Loss: 5.4472\n",
      "Epoch [1/4], Step [21300/68040], Loss: 5.2858\n",
      "Epoch [1/4], Step [21375/68040], Loss: 5.4687\n",
      "Epoch [1/4], Step [21450/68040], Loss: 5.5154\n",
      "Epoch [1/4], Step [21525/68040], Loss: 5.3061\n",
      "Epoch [1/4], Step [21600/68040], Loss: 5.6147\n",
      "Epoch [1/4], Step [21675/68040], Loss: 5.4415\n",
      "Epoch [1/4], Step [21750/68040], Loss: 5.3966\n",
      "Epoch [1/4], Step [21825/68040], Loss: 5.4208\n",
      "Epoch [1/4], Step [21900/68040], Loss: 5.2683\n",
      "Epoch [1/4], Step [21975/68040], Loss: 5.5325\n",
      "Epoch [1/4], Step [22050/68040], Loss: 5.4733\n",
      "Epoch [1/4], Step [22125/68040], Loss: 5.6497\n",
      "Epoch [1/4], Step [22200/68040], Loss: 5.6284\n",
      "Epoch [1/4], Step [22275/68040], Loss: 5.6708\n",
      "Epoch [1/4], Step [22350/68040], Loss: 5.4580\n",
      "Epoch [1/4], Step [22425/68040], Loss: 5.3804\n",
      "Epoch [1/4], Step [22500/68040], Loss: 5.2069\n",
      "Epoch [1/4], Step [22575/68040], Loss: 5.4009\n",
      "Epoch [1/4], Step [22650/68040], Loss: 5.3746\n",
      "Epoch [1/4], Step [22725/68040], Loss: 5.3044\n",
      "Epoch [1/4], Step [22800/68040], Loss: 5.4583\n",
      "Epoch [1/4], Step [22875/68040], Loss: 5.5249\n",
      "Epoch [1/4], Step [22950/68040], Loss: 5.3393\n",
      "Epoch [1/4], Step [23025/68040], Loss: 5.4584\n",
      "Epoch [1/4], Step [23100/68040], Loss: 5.2648\n",
      "Epoch [1/4], Step [23175/68040], Loss: 5.3522\n",
      "Epoch [1/4], Step [23250/68040], Loss: 5.3778\n",
      "Epoch [1/4], Step [23325/68040], Loss: 5.3843\n",
      "Epoch [1/4], Step [23400/68040], Loss: 5.3370\n",
      "Epoch [1/4], Step [23475/68040], Loss: 5.4588\n",
      "Epoch [1/4], Step [23550/68040], Loss: 5.3468\n",
      "Epoch [1/4], Step [23625/68040], Loss: 5.4697\n",
      "Epoch [1/4], Step [23700/68040], Loss: 5.1832\n",
      "Epoch [1/4], Step [23775/68040], Loss: 5.4269\n",
      "Epoch [1/4], Step [23850/68040], Loss: 5.5009\n",
      "Epoch [1/4], Step [23925/68040], Loss: 5.4769\n",
      "Epoch [1/4], Step [24000/68040], Loss: 5.4216\n",
      "Epoch [1/4], Step [24075/68040], Loss: 5.3405\n",
      "Epoch [1/4], Step [24150/68040], Loss: 5.4513\n",
      "Epoch [1/4], Step [24225/68040], Loss: 5.5486\n",
      "Epoch [1/4], Step [24300/68040], Loss: 5.3704\n",
      "Epoch [1/4], Step [24375/68040], Loss: 5.3640\n",
      "Epoch [1/4], Step [24450/68040], Loss: 5.3986\n",
      "Epoch [1/4], Step [24525/68040], Loss: 5.5118\n",
      "Epoch [1/4], Step [24600/68040], Loss: 5.3039\n",
      "Epoch [1/4], Step [24675/68040], Loss: 5.2536\n",
      "Epoch [1/4], Step [24750/68040], Loss: 5.5752\n",
      "Epoch [1/4], Step [24825/68040], Loss: 5.4100\n",
      "Epoch [1/4], Step [24900/68040], Loss: 5.4957\n",
      "Epoch [1/4], Step [24975/68040], Loss: 5.1907\n",
      "Epoch [1/4], Step [25050/68040], Loss: 5.2296\n",
      "Epoch [1/4], Step [25125/68040], Loss: 5.4832\n",
      "Epoch [1/4], Step [25200/68040], Loss: 5.2320\n",
      "Epoch [1/4], Step [25275/68040], Loss: 5.4077\n",
      "Epoch [1/4], Step [25350/68040], Loss: 5.4033\n",
      "Epoch [1/4], Step [25425/68040], Loss: 5.4376\n",
      "Epoch [1/4], Step [25500/68040], Loss: 5.2608\n",
      "Epoch [1/4], Step [25575/68040], Loss: 5.3475\n",
      "Epoch [1/4], Step [25650/68040], Loss: 5.2491\n",
      "Epoch [1/4], Step [25725/68040], Loss: 5.2470\n",
      "Epoch [1/4], Step [25800/68040], Loss: 5.4394\n",
      "Epoch [1/4], Step [25875/68040], Loss: 5.2792\n",
      "Epoch [1/4], Step [25950/68040], Loss: 5.2502\n",
      "Epoch [1/4], Step [26025/68040], Loss: 5.4294\n",
      "Epoch [1/4], Step [26100/68040], Loss: 5.3591\n",
      "Epoch [1/4], Step [26175/68040], Loss: 5.3475\n",
      "Epoch [1/4], Step [26250/68040], Loss: 5.2215\n",
      "Epoch [1/4], Step [26325/68040], Loss: 5.2728\n",
      "Epoch [1/4], Step [26400/68040], Loss: 5.4455\n",
      "Epoch [1/4], Step [26475/68040], Loss: 5.3776\n",
      "Epoch [1/4], Step [26550/68040], Loss: 5.3843\n",
      "Epoch [1/4], Step [26625/68040], Loss: 5.1900\n",
      "Epoch [1/4], Step [26700/68040], Loss: 5.3670\n",
      "Epoch [1/4], Step [26775/68040], Loss: 5.3508\n",
      "Epoch [1/4], Step [26850/68040], Loss: 5.2703\n",
      "Epoch [1/4], Step [26925/68040], Loss: 5.3714\n",
      "Epoch [1/4], Step [27000/68040], Loss: 5.2531\n",
      "Epoch [1/4], Step [27075/68040], Loss: 5.2931\n",
      "Epoch [1/4], Step [27150/68040], Loss: 5.4633\n",
      "Epoch [1/4], Step [27225/68040], Loss: 5.4853\n",
      "Epoch [1/4], Step [27300/68040], Loss: 5.4210\n",
      "Epoch [1/4], Step [27375/68040], Loss: 5.2735\n",
      "Epoch [1/4], Step [27450/68040], Loss: 5.4297\n",
      "Epoch [1/4], Step [27525/68040], Loss: 5.1332\n",
      "Epoch [1/4], Step [27600/68040], Loss: 5.4124\n",
      "Epoch [1/4], Step [27675/68040], Loss: 5.3570\n",
      "Epoch [1/4], Step [27750/68040], Loss: 5.2093\n",
      "Epoch [1/4], Step [27825/68040], Loss: 5.2199\n",
      "Epoch [1/4], Step [27900/68040], Loss: 5.2784\n",
      "Epoch [1/4], Step [27975/68040], Loss: 5.3952\n",
      "Epoch [1/4], Step [28050/68040], Loss: 5.3265\n",
      "Epoch [1/4], Step [28125/68040], Loss: 5.3719\n",
      "Epoch [1/4], Step [28200/68040], Loss: 5.2664\n",
      "Epoch [1/4], Step [28275/68040], Loss: 5.3978\n",
      "Epoch [1/4], Step [28350/68040], Loss: 5.2728\n",
      "Epoch [1/4], Step [28425/68040], Loss: 5.3531\n",
      "Epoch [1/4], Step [28500/68040], Loss: 5.4012\n",
      "Epoch [1/4], Step [28575/68040], Loss: 5.4264\n",
      "Epoch [1/4], Step [28650/68040], Loss: 5.4065\n",
      "Epoch [1/4], Step [28725/68040], Loss: 5.3155\n",
      "Epoch [1/4], Step [28800/68040], Loss: 5.4378\n",
      "Epoch [1/4], Step [28875/68040], Loss: 5.4349\n",
      "Epoch [1/4], Step [28950/68040], Loss: 5.3802\n",
      "Epoch [1/4], Step [29025/68040], Loss: 5.4100\n",
      "Epoch [1/4], Step [29100/68040], Loss: 5.4594\n",
      "Epoch [1/4], Step [29175/68040], Loss: 5.2747\n",
      "Epoch [1/4], Step [29250/68040], Loss: 5.2294\n",
      "Epoch [1/4], Step [29325/68040], Loss: 5.1944\n",
      "Epoch [1/4], Step [29400/68040], Loss: 5.2625\n",
      "Epoch [1/4], Step [29475/68040], Loss: 5.1866\n",
      "Epoch [1/4], Step [29550/68040], Loss: 5.2573\n",
      "Epoch [1/4], Step [29625/68040], Loss: 5.3537\n",
      "Epoch [1/4], Step [29700/68040], Loss: 5.3867\n",
      "Epoch [1/4], Step [29775/68040], Loss: 5.2913\n",
      "Epoch [1/4], Step [29850/68040], Loss: 5.4050\n",
      "Epoch [1/4], Step [29925/68040], Loss: 5.3120\n",
      "Epoch [1/4], Step [30000/68040], Loss: 5.3679\n",
      "Validation perplexity: 178.8310060552943\n",
      "Epoch [1/4], Step [30075/68040], Loss: 5.2793\n",
      "Epoch [1/4], Step [30150/68040], Loss: 5.4042\n",
      "Epoch [1/4], Step [30225/68040], Loss: 5.3180\n",
      "Epoch [1/4], Step [30300/68040], Loss: 5.2924\n",
      "Epoch [1/4], Step [30375/68040], Loss: 5.1512\n",
      "Epoch [1/4], Step [30450/68040], Loss: 5.2070\n",
      "Epoch [1/4], Step [30525/68040], Loss: 5.3199\n",
      "Epoch [1/4], Step [30600/68040], Loss: 5.3631\n",
      "Epoch [1/4], Step [30675/68040], Loss: 4.9899\n",
      "Epoch [1/4], Step [30750/68040], Loss: 5.2953\n",
      "Epoch [1/4], Step [30825/68040], Loss: 5.3128\n",
      "Epoch [1/4], Step [30900/68040], Loss: 5.1270\n",
      "Epoch [1/4], Step [30975/68040], Loss: 5.5790\n",
      "Epoch [1/4], Step [31050/68040], Loss: 5.2819\n",
      "Epoch [1/4], Step [31125/68040], Loss: 5.1250\n",
      "Epoch [1/4], Step [31200/68040], Loss: 5.3451\n",
      "Epoch [1/4], Step [31275/68040], Loss: 5.3232\n",
      "Epoch [1/4], Step [31350/68040], Loss: 5.1768\n",
      "Epoch [1/4], Step [31425/68040], Loss: 5.2700\n",
      "Epoch [1/4], Step [31500/68040], Loss: 5.0238\n",
      "Epoch [1/4], Step [31575/68040], Loss: 5.4239\n",
      "Epoch [1/4], Step [31650/68040], Loss: 5.2475\n",
      "Epoch [1/4], Step [31725/68040], Loss: 5.5636\n",
      "Epoch [1/4], Step [31800/68040], Loss: 5.4118\n",
      "Epoch [1/4], Step [31875/68040], Loss: 5.4897\n",
      "Epoch [1/4], Step [31950/68040], Loss: 5.3905\n",
      "Epoch [1/4], Step [32025/68040], Loss: 5.3151\n",
      "Epoch [1/4], Step [32100/68040], Loss: 5.2876\n",
      "Epoch [1/4], Step [32175/68040], Loss: 5.2301\n",
      "Epoch [1/4], Step [32250/68040], Loss: 5.3323\n",
      "Epoch [1/4], Step [32325/68040], Loss: 5.4248\n",
      "Epoch [1/4], Step [32400/68040], Loss: 5.3809\n",
      "Epoch [1/4], Step [32475/68040], Loss: 5.2218\n",
      "Epoch [1/4], Step [32550/68040], Loss: 5.2805\n",
      "Epoch [1/4], Step [32625/68040], Loss: 5.3310\n",
      "Epoch [1/4], Step [32700/68040], Loss: 5.4718\n",
      "Epoch [1/4], Step [32775/68040], Loss: 5.3067\n",
      "Epoch [1/4], Step [32850/68040], Loss: 5.2944\n",
      "Epoch [1/4], Step [32925/68040], Loss: 5.3582\n",
      "Epoch [1/4], Step [33000/68040], Loss: 5.2710\n",
      "Epoch [1/4], Step [33075/68040], Loss: 5.2245\n",
      "Epoch [1/4], Step [33150/68040], Loss: 5.4203\n",
      "Epoch [1/4], Step [33225/68040], Loss: 5.4029\n",
      "Epoch [1/4], Step [33300/68040], Loss: 5.4322\n",
      "Epoch [1/4], Step [33375/68040], Loss: 5.1944\n",
      "Epoch [1/4], Step [33450/68040], Loss: 5.3046\n",
      "Epoch [1/4], Step [33525/68040], Loss: 5.0972\n",
      "Epoch [1/4], Step [33600/68040], Loss: 5.2312\n",
      "Epoch [1/4], Step [33675/68040], Loss: 5.2730\n",
      "Epoch [1/4], Step [33750/68040], Loss: 5.2581\n",
      "Epoch [1/4], Step [33825/68040], Loss: 5.3367\n",
      "Epoch [1/4], Step [33900/68040], Loss: 5.2164\n",
      "Epoch [1/4], Step [33975/68040], Loss: 5.3176\n",
      "Epoch [1/4], Step [34050/68040], Loss: 5.3306\n",
      "Epoch [1/4], Step [34125/68040], Loss: 5.2858\n",
      "Epoch [1/4], Step [34200/68040], Loss: 5.2536\n",
      "Epoch [1/4], Step [34275/68040], Loss: 5.3258\n",
      "Epoch [1/4], Step [34350/68040], Loss: 5.4201\n",
      "Epoch [1/4], Step [34425/68040], Loss: 5.3082\n",
      "Epoch [1/4], Step [34500/68040], Loss: 5.3574\n",
      "Epoch [1/4], Step [34575/68040], Loss: 5.2230\n",
      "Epoch [1/4], Step [34650/68040], Loss: 5.2769\n",
      "Epoch [1/4], Step [34725/68040], Loss: 5.1330\n",
      "Epoch [1/4], Step [34800/68040], Loss: 5.3145\n",
      "Epoch [1/4], Step [34875/68040], Loss: 5.3150\n",
      "Epoch [1/4], Step [34950/68040], Loss: 5.2572\n",
      "Epoch [1/4], Step [35025/68040], Loss: 5.2401\n",
      "Epoch [1/4], Step [35100/68040], Loss: 5.4432\n",
      "Epoch [1/4], Step [35175/68040], Loss: 5.3321\n",
      "Epoch [1/4], Step [35250/68040], Loss: 5.3234\n",
      "Epoch [1/4], Step [35325/68040], Loss: 4.9777\n",
      "Epoch [1/4], Step [35400/68040], Loss: 5.2216\n",
      "Epoch [1/4], Step [35475/68040], Loss: 5.2689\n",
      "Epoch [1/4], Step [35550/68040], Loss: 5.2923\n",
      "Epoch [1/4], Step [35625/68040], Loss: 5.4105\n",
      "Epoch [1/4], Step [35700/68040], Loss: 5.3939\n",
      "Epoch [1/4], Step [35775/68040], Loss: 5.3497\n",
      "Epoch [1/4], Step [35850/68040], Loss: 5.2357\n",
      "Epoch [1/4], Step [35925/68040], Loss: 5.0853\n",
      "Epoch [1/4], Step [36000/68040], Loss: 5.1329\n",
      "Epoch [1/4], Step [36075/68040], Loss: 5.2275\n",
      "Epoch [1/4], Step [36150/68040], Loss: 5.0677\n",
      "Epoch [1/4], Step [36225/68040], Loss: 5.2568\n",
      "Epoch [1/4], Step [36300/68040], Loss: 5.3745\n",
      "Epoch [1/4], Step [36375/68040], Loss: 5.2685\n",
      "Epoch [1/4], Step [36450/68040], Loss: 5.1599\n",
      "Epoch [1/4], Step [36525/68040], Loss: 5.2856\n",
      "Epoch [1/4], Step [36600/68040], Loss: 5.4455\n",
      "Epoch [1/4], Step [36675/68040], Loss: 5.1210\n",
      "Epoch [1/4], Step [36750/68040], Loss: 5.1669\n",
      "Epoch [1/4], Step [36825/68040], Loss: 5.2692\n",
      "Epoch [1/4], Step [36900/68040], Loss: 5.2511\n",
      "Epoch [1/4], Step [36975/68040], Loss: 5.2639\n",
      "Epoch [1/4], Step [37050/68040], Loss: 5.1051\n",
      "Epoch [1/4], Step [37125/68040], Loss: 5.3177\n",
      "Epoch [1/4], Step [37200/68040], Loss: 5.3673\n",
      "Epoch [1/4], Step [37275/68040], Loss: 5.2291\n",
      "Epoch [1/4], Step [37350/68040], Loss: 5.2379\n",
      "Epoch [1/4], Step [37425/68040], Loss: 5.4126\n",
      "Epoch [1/4], Step [37500/68040], Loss: 5.3091\n",
      "Epoch [1/4], Step [37575/68040], Loss: 5.2938\n",
      "Epoch [1/4], Step [37650/68040], Loss: 5.3843\n",
      "Epoch [1/4], Step [37725/68040], Loss: 5.3070\n",
      "Epoch [1/4], Step [37800/68040], Loss: 5.3070\n",
      "Epoch [1/4], Step [37875/68040], Loss: 5.3955\n",
      "Epoch [1/4], Step [37950/68040], Loss: 5.3029\n",
      "Epoch [1/4], Step [38025/68040], Loss: 5.2524\n",
      "Epoch [1/4], Step [38100/68040], Loss: 5.2700\n",
      "Epoch [1/4], Step [38175/68040], Loss: 5.2143\n",
      "Epoch [1/4], Step [38250/68040], Loss: 5.1399\n",
      "Epoch [1/4], Step [38325/68040], Loss: 5.2625\n",
      "Epoch [1/4], Step [38400/68040], Loss: 5.4105\n",
      "Epoch [1/4], Step [38475/68040], Loss: 5.2146\n",
      "Epoch [1/4], Step [38550/68040], Loss: 5.3132\n",
      "Epoch [1/4], Step [38625/68040], Loss: 5.3142\n",
      "Epoch [1/4], Step [38700/68040], Loss: 5.2644\n",
      "Epoch [1/4], Step [38775/68040], Loss: 5.3478\n",
      "Epoch [1/4], Step [38850/68040], Loss: 5.1630\n",
      "Epoch [1/4], Step [38925/68040], Loss: 5.2768\n",
      "Epoch [1/4], Step [39000/68040], Loss: 5.1397\n",
      "Epoch [1/4], Step [39075/68040], Loss: 5.2903\n",
      "Epoch [1/4], Step [39150/68040], Loss: 5.3973\n",
      "Epoch [1/4], Step [39225/68040], Loss: 5.1976\n",
      "Epoch [1/4], Step [39300/68040], Loss: 5.1686\n",
      "Epoch [1/4], Step [39375/68040], Loss: 5.5253\n",
      "Epoch [1/4], Step [39450/68040], Loss: 5.4298\n",
      "Epoch [1/4], Step [39525/68040], Loss: 5.2762\n",
      "Epoch [1/4], Step [39600/68040], Loss: 5.3164\n",
      "Epoch [1/4], Step [39675/68040], Loss: 5.3064\n",
      "Epoch [1/4], Step [39750/68040], Loss: 5.2981\n",
      "Epoch [1/4], Step [39825/68040], Loss: 5.2981\n",
      "Epoch [1/4], Step [39900/68040], Loss: 5.1708\n",
      "Epoch [1/4], Step [39975/68040], Loss: 5.3639\n",
      "Validation perplexity: 168.50520872426313\n",
      "Epoch [1/4], Step [40050/68040], Loss: 5.1415\n",
      "Epoch [1/4], Step [40125/68040], Loss: 5.4015\n",
      "Epoch [1/4], Step [40200/68040], Loss: 5.2621\n",
      "Epoch [1/4], Step [40275/68040], Loss: 5.3435\n",
      "Epoch [1/4], Step [40350/68040], Loss: 5.1678\n",
      "Epoch [1/4], Step [40425/68040], Loss: 5.1673\n",
      "Epoch [1/4], Step [40500/68040], Loss: 5.1380\n",
      "Epoch [1/4], Step [40575/68040], Loss: 5.3325\n",
      "Epoch [1/4], Step [40650/68040], Loss: 5.2375\n",
      "Epoch [1/4], Step [40725/68040], Loss: 5.1922\n",
      "Epoch [1/4], Step [40800/68040], Loss: 5.2999\n",
      "Epoch [1/4], Step [40875/68040], Loss: 5.1690\n",
      "Epoch [1/4], Step [40950/68040], Loss: 5.2831\n",
      "Epoch [1/4], Step [41025/68040], Loss: 5.3511\n",
      "Epoch [1/4], Step [41100/68040], Loss: 5.3291\n",
      "Epoch [1/4], Step [41175/68040], Loss: 5.2508\n",
      "Epoch [1/4], Step [41250/68040], Loss: 5.0374\n",
      "Epoch [1/4], Step [41325/68040], Loss: 5.1553\n",
      "Epoch [1/4], Step [41400/68040], Loss: 5.1918\n",
      "Epoch [1/4], Step [41475/68040], Loss: 5.3030\n",
      "Epoch [1/4], Step [41550/68040], Loss: 5.2800\n",
      "Epoch [1/4], Step [41625/68040], Loss: 5.3667\n",
      "Epoch [1/4], Step [41700/68040], Loss: 5.3792\n",
      "Epoch [1/4], Step [41775/68040], Loss: 5.0856\n",
      "Epoch [1/4], Step [41850/68040], Loss: 5.2240\n",
      "Epoch [1/4], Step [41925/68040], Loss: 5.1844\n",
      "Epoch [1/4], Step [42000/68040], Loss: 5.2966\n",
      "Epoch [1/4], Step [42075/68040], Loss: 5.2730\n",
      "Epoch [1/4], Step [42150/68040], Loss: 5.0898\n",
      "Epoch [1/4], Step [42225/68040], Loss: 5.1930\n",
      "Epoch [1/4], Step [42300/68040], Loss: 5.2017\n",
      "Epoch [1/4], Step [42375/68040], Loss: 5.2859\n",
      "Epoch [1/4], Step [42450/68040], Loss: 5.2659\n",
      "Epoch [1/4], Step [42525/68040], Loss: 5.1708\n",
      "Epoch [1/4], Step [42600/68040], Loss: 5.1604\n",
      "Epoch [1/4], Step [42675/68040], Loss: 5.3006\n",
      "Epoch [1/4], Step [42750/68040], Loss: 5.2666\n",
      "Epoch [1/4], Step [42825/68040], Loss: 5.1716\n",
      "Epoch [1/4], Step [42900/68040], Loss: 5.2547\n",
      "Epoch [1/4], Step [42975/68040], Loss: 5.1574\n",
      "Epoch [1/4], Step [43050/68040], Loss: 5.3422\n",
      "Epoch [1/4], Step [43125/68040], Loss: 5.3122\n",
      "Epoch [1/4], Step [43200/68040], Loss: 5.1776\n",
      "Epoch [1/4], Step [43275/68040], Loss: 5.2342\n",
      "Epoch [1/4], Step [43350/68040], Loss: 5.2362\n",
      "Epoch [1/4], Step [43425/68040], Loss: 5.1886\n",
      "Epoch [1/4], Step [43500/68040], Loss: 5.1060\n",
      "Epoch [1/4], Step [43575/68040], Loss: 5.3417\n",
      "Epoch [1/4], Step [43650/68040], Loss: 5.1901\n",
      "Epoch [1/4], Step [43725/68040], Loss: 5.2277\n",
      "Epoch [1/4], Step [43800/68040], Loss: 5.2728\n",
      "Epoch [1/4], Step [43875/68040], Loss: 5.3327\n",
      "Epoch [1/4], Step [43950/68040], Loss: 5.4535\n",
      "Epoch [1/4], Step [44025/68040], Loss: 5.3229\n",
      "Epoch [1/4], Step [44100/68040], Loss: 5.1608\n",
      "Epoch [1/4], Step [44175/68040], Loss: 5.1871\n",
      "Epoch [1/4], Step [44250/68040], Loss: 5.3138\n",
      "Epoch [1/4], Step [44325/68040], Loss: 5.2918\n",
      "Epoch [1/4], Step [44400/68040], Loss: 5.2413\n",
      "Epoch [1/4], Step [44475/68040], Loss: 5.2685\n",
      "Epoch [1/4], Step [44550/68040], Loss: 5.1934\n",
      "Epoch [1/4], Step [44625/68040], Loss: 5.4311\n",
      "Epoch [1/4], Step [44700/68040], Loss: 5.2197\n",
      "Epoch [1/4], Step [44775/68040], Loss: 5.4101\n",
      "Epoch [1/4], Step [44850/68040], Loss: 5.0737\n",
      "Epoch [1/4], Step [44925/68040], Loss: 5.2731\n",
      "Epoch [1/4], Step [45000/68040], Loss: 5.3983\n",
      "Epoch [1/4], Step [45075/68040], Loss: 5.2299\n",
      "Epoch [1/4], Step [45150/68040], Loss: 5.1219\n",
      "Epoch [1/4], Step [45225/68040], Loss: 5.3243\n",
      "Epoch [1/4], Step [45300/68040], Loss: 5.2729\n",
      "Epoch [1/4], Step [45375/68040], Loss: 5.1819\n",
      "Epoch [1/4], Step [45450/68040], Loss: 5.3625\n",
      "Epoch [1/4], Step [45525/68040], Loss: 5.3351\n",
      "Epoch [1/4], Step [45600/68040], Loss: 5.2677\n",
      "Epoch [1/4], Step [45675/68040], Loss: 5.2893\n",
      "Epoch [1/4], Step [45750/68040], Loss: 5.5136\n",
      "Epoch [1/4], Step [45825/68040], Loss: 5.3045\n",
      "Epoch [1/4], Step [45900/68040], Loss: 5.2952\n",
      "Epoch [1/4], Step [45975/68040], Loss: 5.1689\n",
      "Epoch [1/4], Step [46050/68040], Loss: 5.1361\n",
      "Epoch [1/4], Step [46125/68040], Loss: 5.2184\n",
      "Epoch [1/4], Step [46200/68040], Loss: 5.3451\n",
      "Epoch [1/4], Step [46275/68040], Loss: 5.0590\n",
      "Epoch [1/4], Step [46350/68040], Loss: 5.2567\n",
      "Epoch [1/4], Step [46425/68040], Loss: 5.1684\n",
      "Epoch [1/4], Step [46500/68040], Loss: 5.4507\n",
      "Epoch [1/4], Step [46575/68040], Loss: 5.1809\n",
      "Epoch [1/4], Step [46650/68040], Loss: 5.1798\n",
      "Epoch [1/4], Step [46725/68040], Loss: 5.3767\n",
      "Epoch [1/4], Step [46800/68040], Loss: 5.3448\n",
      "Epoch [1/4], Step [46875/68040], Loss: 5.3816\n",
      "Epoch [1/4], Step [46950/68040], Loss: 5.4281\n",
      "Epoch [1/4], Step [47025/68040], Loss: 5.3303\n",
      "Epoch [1/4], Step [47100/68040], Loss: 5.2071\n",
      "Epoch [1/4], Step [47175/68040], Loss: 5.2459\n",
      "Epoch [1/4], Step [47250/68040], Loss: 5.2930\n",
      "Epoch [1/4], Step [47325/68040], Loss: 5.2041\n",
      "Epoch [1/4], Step [47400/68040], Loss: 5.3760\n",
      "Epoch [1/4], Step [47475/68040], Loss: 5.1144\n",
      "Epoch [1/4], Step [47550/68040], Loss: 5.2167\n",
      "Epoch [1/4], Step [47625/68040], Loss: 5.1556\n",
      "Epoch [1/4], Step [47700/68040], Loss: 5.2274\n",
      "Epoch [1/4], Step [47775/68040], Loss: 5.1538\n",
      "Epoch [1/4], Step [47850/68040], Loss: 5.2172\n",
      "Epoch [1/4], Step [47925/68040], Loss: 5.0021\n",
      "Epoch [1/4], Step [48000/68040], Loss: 5.1239\n",
      "Epoch [1/4], Step [48075/68040], Loss: 5.3014\n",
      "Epoch [1/4], Step [48150/68040], Loss: 5.2502\n",
      "Epoch [1/4], Step [48225/68040], Loss: 5.1387\n",
      "Epoch [1/4], Step [48300/68040], Loss: 5.3583\n",
      "Epoch [1/4], Step [48375/68040], Loss: 5.1923\n",
      "Epoch [1/4], Step [48450/68040], Loss: 5.1429\n",
      "Epoch [1/4], Step [48525/68040], Loss: 5.0439\n",
      "Epoch [1/4], Step [48600/68040], Loss: 5.2404\n",
      "Epoch [1/4], Step [48675/68040], Loss: 5.2374\n",
      "Epoch [1/4], Step [48750/68040], Loss: 5.2347\n",
      "Epoch [1/4], Step [48825/68040], Loss: 5.2489\n",
      "Epoch [1/4], Step [48900/68040], Loss: 5.2055\n",
      "Epoch [1/4], Step [48975/68040], Loss: 5.2710\n",
      "Epoch [1/4], Step [49050/68040], Loss: 5.0206\n",
      "Epoch [1/4], Step [49125/68040], Loss: 5.3474\n",
      "Epoch [1/4], Step [49200/68040], Loss: 5.3610\n",
      "Epoch [1/4], Step [49275/68040], Loss: 5.2364\n",
      "Epoch [1/4], Step [49350/68040], Loss: 5.1625\n",
      "Epoch [1/4], Step [49425/68040], Loss: 5.2828\n",
      "Epoch [1/4], Step [49500/68040], Loss: 5.0222\n",
      "Epoch [1/4], Step [49575/68040], Loss: 5.1351\n",
      "Epoch [1/4], Step [49650/68040], Loss: 5.2164\n",
      "Epoch [1/4], Step [49725/68040], Loss: 5.3527\n",
      "Epoch [1/4], Step [49800/68040], Loss: 5.2519\n",
      "Epoch [1/4], Step [49875/68040], Loss: 5.2813\n",
      "Epoch [1/4], Step [49950/68040], Loss: 5.1291\n",
      "Validation perplexity: 161.476252292538\n",
      "Epoch [1/4], Step [50025/68040], Loss: 5.2070\n",
      "Epoch [1/4], Step [50100/68040], Loss: 5.1501\n",
      "Epoch [1/4], Step [50175/68040], Loss: 5.1937\n",
      "Epoch [1/4], Step [50250/68040], Loss: 5.2794\n",
      "Epoch [1/4], Step [50325/68040], Loss: 5.2731\n",
      "Epoch [1/4], Step [50400/68040], Loss: 5.3091\n",
      "Epoch [1/4], Step [50475/68040], Loss: 5.3158\n",
      "Epoch [1/4], Step [50550/68040], Loss: 5.0882\n",
      "Epoch [1/4], Step [50625/68040], Loss: 5.3309\n",
      "Epoch [1/4], Step [50700/68040], Loss: 5.1713\n",
      "Epoch [1/4], Step [50775/68040], Loss: 5.1777\n",
      "Epoch [1/4], Step [50850/68040], Loss: 5.2347\n",
      "Epoch [1/4], Step [50925/68040], Loss: 5.1335\n",
      "Epoch [1/4], Step [51000/68040], Loss: 5.2458\n",
      "Epoch [1/4], Step [51075/68040], Loss: 5.2900\n",
      "Epoch [1/4], Step [51150/68040], Loss: 5.1535\n",
      "Epoch [1/4], Step [51225/68040], Loss: 5.2175\n",
      "Epoch [1/4], Step [51300/68040], Loss: 5.3256\n",
      "Epoch [1/4], Step [51375/68040], Loss: 5.2326\n",
      "Epoch [1/4], Step [51450/68040], Loss: 5.1892\n",
      "Epoch [1/4], Step [51525/68040], Loss: 5.2160\n",
      "Epoch [1/4], Step [51600/68040], Loss: 5.1826\n",
      "Epoch [1/4], Step [51675/68040], Loss: 5.1065\n",
      "Epoch [1/4], Step [51750/68040], Loss: 5.1880\n",
      "Epoch [1/4], Step [51825/68040], Loss: 5.1854\n",
      "Epoch [1/4], Step [51900/68040], Loss: 5.2763\n",
      "Epoch [1/4], Step [51975/68040], Loss: 5.1598\n",
      "Epoch [1/4], Step [52050/68040], Loss: 5.2824\n",
      "Epoch [1/4], Step [52125/68040], Loss: 5.2040\n",
      "Epoch [1/4], Step [52200/68040], Loss: 5.0681\n",
      "Epoch [1/4], Step [52275/68040], Loss: 5.2399\n",
      "Epoch [1/4], Step [52350/68040], Loss: 5.1330\n",
      "Epoch [1/4], Step [52425/68040], Loss: 5.2294\n",
      "Epoch [1/4], Step [52500/68040], Loss: 5.3381\n",
      "Epoch [1/4], Step [52575/68040], Loss: 5.1865\n",
      "Epoch [1/4], Step [52650/68040], Loss: 5.3531\n",
      "Epoch [1/4], Step [52725/68040], Loss: 5.1755\n",
      "Epoch [1/4], Step [52800/68040], Loss: 5.1717\n",
      "Epoch [1/4], Step [52875/68040], Loss: 5.2706\n",
      "Epoch [1/4], Step [52950/68040], Loss: 5.2192\n",
      "Epoch [1/4], Step [53025/68040], Loss: 5.2355\n",
      "Epoch [1/4], Step [53100/68040], Loss: 5.2798\n",
      "Epoch [1/4], Step [53175/68040], Loss: 5.2386\n",
      "Epoch [1/4], Step [53250/68040], Loss: 5.4160\n",
      "Epoch [1/4], Step [53325/68040], Loss: 5.1826\n",
      "Epoch [1/4], Step [53400/68040], Loss: 4.9877\n",
      "Epoch [1/4], Step [53475/68040], Loss: 5.1848\n",
      "Epoch [1/4], Step [53550/68040], Loss: 5.1853\n",
      "Epoch [1/4], Step [53625/68040], Loss: 5.3593\n",
      "Epoch [1/4], Step [53700/68040], Loss: 5.2213\n",
      "Epoch [1/4], Step [53775/68040], Loss: 5.0639\n",
      "Epoch [1/4], Step [53850/68040], Loss: 5.2049\n",
      "Epoch [1/4], Step [53925/68040], Loss: 5.3597\n",
      "Epoch [1/4], Step [54000/68040], Loss: 5.2164\n",
      "Epoch [1/4], Step [54075/68040], Loss: 5.1071\n",
      "Epoch [1/4], Step [54150/68040], Loss: 4.9927\n",
      "Epoch [1/4], Step [54225/68040], Loss: 5.3682\n",
      "Epoch [1/4], Step [54300/68040], Loss: 5.3322\n",
      "Epoch [1/4], Step [54375/68040], Loss: 4.9496\n",
      "Epoch [1/4], Step [54450/68040], Loss: 5.0678\n",
      "Epoch [1/4], Step [54525/68040], Loss: 5.2415\n",
      "Epoch [1/4], Step [54600/68040], Loss: 5.2151\n",
      "Epoch [1/4], Step [54675/68040], Loss: 5.1217\n",
      "Epoch [1/4], Step [54750/68040], Loss: 5.1165\n",
      "Epoch [1/4], Step [54825/68040], Loss: 5.3089\n",
      "Epoch [1/4], Step [54900/68040], Loss: 5.2473\n",
      "Epoch [1/4], Step [54975/68040], Loss: 5.2548\n",
      "Epoch [1/4], Step [55050/68040], Loss: 5.2152\n",
      "Epoch [1/4], Step [55125/68040], Loss: 5.2538\n",
      "Epoch [1/4], Step [55200/68040], Loss: 5.3035\n",
      "Epoch [1/4], Step [55275/68040], Loss: 5.1981\n",
      "Epoch [1/4], Step [55350/68040], Loss: 5.0729\n",
      "Epoch [1/4], Step [55425/68040], Loss: 5.1883\n",
      "Epoch [1/4], Step [55500/68040], Loss: 5.2288\n",
      "Epoch [1/4], Step [55575/68040], Loss: 5.1482\n",
      "Epoch [1/4], Step [55650/68040], Loss: 5.2166\n",
      "Epoch [1/4], Step [55725/68040], Loss: 5.1967\n",
      "Epoch [1/4], Step [55800/68040], Loss: 5.0916\n",
      "Epoch [1/4], Step [55875/68040], Loss: 5.1455\n",
      "Epoch [1/4], Step [55950/68040], Loss: 5.2039\n",
      "Epoch [1/4], Step [56025/68040], Loss: 5.3115\n",
      "Epoch [1/4], Step [56100/68040], Loss: 5.2887\n",
      "Epoch [1/4], Step [56175/68040], Loss: 5.2143\n",
      "Epoch [1/4], Step [56250/68040], Loss: 5.0782\n",
      "Epoch [1/4], Step [56325/68040], Loss: 5.3346\n",
      "Epoch [1/4], Step [56400/68040], Loss: 5.2829\n",
      "Epoch [1/4], Step [56475/68040], Loss: 5.1671\n",
      "Epoch [1/4], Step [56550/68040], Loss: 5.3584\n",
      "Epoch [1/4], Step [56625/68040], Loss: 5.0076\n",
      "Epoch [1/4], Step [56700/68040], Loss: 5.3670\n",
      "Epoch [1/4], Step [56775/68040], Loss: 5.1706\n",
      "Epoch [1/4], Step [56850/68040], Loss: 5.2116\n",
      "Epoch [1/4], Step [56925/68040], Loss: 5.3293\n",
      "Epoch [1/4], Step [57000/68040], Loss: 5.2971\n",
      "Epoch [1/4], Step [57075/68040], Loss: 5.3137\n",
      "Epoch [1/4], Step [57150/68040], Loss: 5.1441\n",
      "Epoch [1/4], Step [57225/68040], Loss: 5.1005\n",
      "Epoch [1/4], Step [57300/68040], Loss: 5.2700\n",
      "Epoch [1/4], Step [57375/68040], Loss: 5.1289\n",
      "Epoch [1/4], Step [57450/68040], Loss: 5.0815\n",
      "Epoch [1/4], Step [57525/68040], Loss: 5.2704\n",
      "Epoch [1/4], Step [57600/68040], Loss: 5.0837\n",
      "Epoch [1/4], Step [57675/68040], Loss: 5.4170\n",
      "Epoch [1/4], Step [57750/68040], Loss: 5.1617\n",
      "Epoch [1/4], Step [57825/68040], Loss: 5.2199\n",
      "Epoch [1/4], Step [57900/68040], Loss: 5.1711\n",
      "Epoch [1/4], Step [57975/68040], Loss: 5.2109\n",
      "Epoch [1/4], Step [58050/68040], Loss: 5.2122\n",
      "Epoch [1/4], Step [58125/68040], Loss: 5.2055\n",
      "Epoch [1/4], Step [58200/68040], Loss: 5.1198\n",
      "Epoch [1/4], Step [58275/68040], Loss: 5.2319\n",
      "Epoch [1/4], Step [58350/68040], Loss: 5.1288\n",
      "Epoch [1/4], Step [58425/68040], Loss: 5.1036\n",
      "Epoch [1/4], Step [58500/68040], Loss: 5.1027\n",
      "Epoch [1/4], Step [58575/68040], Loss: 5.1670\n",
      "Epoch [1/4], Step [58650/68040], Loss: 5.2235\n",
      "Epoch [1/4], Step [58725/68040], Loss: 5.3813\n",
      "Epoch [1/4], Step [58800/68040], Loss: 5.3699\n",
      "Epoch [1/4], Step [58875/68040], Loss: 5.2043\n",
      "Epoch [1/4], Step [58950/68040], Loss: 5.1294\n",
      "Epoch [1/4], Step [59025/68040], Loss: 5.1614\n",
      "Epoch [1/4], Step [59100/68040], Loss: 5.2041\n",
      "Epoch [1/4], Step [59175/68040], Loss: 5.1541\n",
      "Epoch [1/4], Step [59250/68040], Loss: 5.1997\n",
      "Epoch [1/4], Step [59325/68040], Loss: 5.1563\n",
      "Epoch [1/4], Step [59400/68040], Loss: 5.3941\n",
      "Epoch [1/4], Step [59475/68040], Loss: 5.1816\n",
      "Epoch [1/4], Step [59550/68040], Loss: 5.2692\n",
      "Epoch [1/4], Step [59625/68040], Loss: 5.1253\n",
      "Epoch [1/4], Step [59700/68040], Loss: 5.1162\n",
      "Epoch [1/4], Step [59775/68040], Loss: 5.2633\n",
      "Epoch [1/4], Step [59850/68040], Loss: 5.3158\n",
      "Epoch [1/4], Step [59925/68040], Loss: 5.1031\n",
      "Epoch [1/4], Step [60000/68040], Loss: 5.1057\n",
      "Validation perplexity: 156.18918288518714\n",
      "Epoch [1/4], Step [60075/68040], Loss: 5.3256\n",
      "Epoch [1/4], Step [60150/68040], Loss: 5.0445\n",
      "Epoch [1/4], Step [60225/68040], Loss: 5.2765\n",
      "Epoch [1/4], Step [60300/68040], Loss: 5.1790\n",
      "Epoch [1/4], Step [60375/68040], Loss: 5.0940\n",
      "Epoch [1/4], Step [60450/68040], Loss: 5.3447\n",
      "Epoch [1/4], Step [60525/68040], Loss: 5.2948\n",
      "Epoch [1/4], Step [60600/68040], Loss: 5.3254\n",
      "Epoch [1/4], Step [60675/68040], Loss: 5.0695\n",
      "Epoch [1/4], Step [60750/68040], Loss: 5.2996\n",
      "Epoch [1/4], Step [60825/68040], Loss: 5.1276\n",
      "Epoch [1/4], Step [60900/68040], Loss: 5.2180\n",
      "Epoch [1/4], Step [60975/68040], Loss: 5.0145\n",
      "Epoch [1/4], Step [61050/68040], Loss: 5.2181\n",
      "Epoch [1/4], Step [61125/68040], Loss: 5.2300\n",
      "Epoch [1/4], Step [61200/68040], Loss: 5.2910\n",
      "Epoch [1/4], Step [61275/68040], Loss: 5.0376\n",
      "Epoch [1/4], Step [61350/68040], Loss: 5.2563\n",
      "Epoch [1/4], Step [61425/68040], Loss: 5.1351\n",
      "Epoch [1/4], Step [61500/68040], Loss: 5.2585\n",
      "Epoch [1/4], Step [61575/68040], Loss: 5.3423\n",
      "Epoch [1/4], Step [61650/68040], Loss: 5.3314\n",
      "Epoch [1/4], Step [61725/68040], Loss: 5.2175\n",
      "Epoch [1/4], Step [61800/68040], Loss: 5.0506\n",
      "Epoch [1/4], Step [61875/68040], Loss: 5.2186\n",
      "Epoch [1/4], Step [61950/68040], Loss: 5.2607\n",
      "Epoch [1/4], Step [62025/68040], Loss: 5.1005\n",
      "Epoch [1/4], Step [62100/68040], Loss: 5.3478\n",
      "Epoch [1/4], Step [62175/68040], Loss: 4.9768\n",
      "Epoch [1/4], Step [62250/68040], Loss: 5.2558\n",
      "Epoch [1/4], Step [62325/68040], Loss: 4.9807\n",
      "Epoch [1/4], Step [62400/68040], Loss: 4.9373\n",
      "Epoch [1/4], Step [62475/68040], Loss: 5.1365\n",
      "Epoch [1/4], Step [62550/68040], Loss: 5.1628\n",
      "Epoch [1/4], Step [62625/68040], Loss: 5.1913\n",
      "Epoch [1/4], Step [62700/68040], Loss: 5.1448\n",
      "Epoch [1/4], Step [62775/68040], Loss: 5.1568\n",
      "Epoch [1/4], Step [62850/68040], Loss: 5.1990\n",
      "Epoch [1/4], Step [62925/68040], Loss: 5.1754\n",
      "Epoch [1/4], Step [63000/68040], Loss: 5.2108\n",
      "Epoch [1/4], Step [63075/68040], Loss: 4.9531\n",
      "Epoch [1/4], Step [63150/68040], Loss: 5.1192\n",
      "Epoch [1/4], Step [63225/68040], Loss: 5.2477\n",
      "Epoch [1/4], Step [63300/68040], Loss: 5.1186\n",
      "Epoch [1/4], Step [63375/68040], Loss: 5.2464\n",
      "Epoch [1/4], Step [63450/68040], Loss: 5.0411\n",
      "Epoch [1/4], Step [63525/68040], Loss: 5.2072\n",
      "Epoch [1/4], Step [63600/68040], Loss: 5.1171\n",
      "Epoch [1/4], Step [63675/68040], Loss: 5.2770\n",
      "Epoch [1/4], Step [63750/68040], Loss: 4.9053\n",
      "Epoch [1/4], Step [63825/68040], Loss: 5.3124\n",
      "Epoch [1/4], Step [63900/68040], Loss: 5.2066\n",
      "Epoch [1/4], Step [63975/68040], Loss: 5.3658\n",
      "Epoch [1/4], Step [64050/68040], Loss: 5.1769\n",
      "Epoch [1/4], Step [64125/68040], Loss: 5.2300\n",
      "Epoch [1/4], Step [64200/68040], Loss: 5.1438\n",
      "Epoch [1/4], Step [64275/68040], Loss: 5.2598\n",
      "Epoch [1/4], Step [64350/68040], Loss: 5.1819\n",
      "Epoch [1/4], Step [64425/68040], Loss: 5.1110\n",
      "Epoch [1/4], Step [64500/68040], Loss: 5.3601\n",
      "Epoch [1/4], Step [64575/68040], Loss: 5.0233\n",
      "Epoch [1/4], Step [64650/68040], Loss: 5.1891\n",
      "Epoch [1/4], Step [64725/68040], Loss: 5.2371\n",
      "Epoch [1/4], Step [64800/68040], Loss: 5.2170\n",
      "Epoch [1/4], Step [64875/68040], Loss: 5.2818\n",
      "Epoch [1/4], Step [64950/68040], Loss: 5.1028\n",
      "Epoch [1/4], Step [65025/68040], Loss: 5.1561\n",
      "Epoch [1/4], Step [65100/68040], Loss: 5.2447\n",
      "Epoch [1/4], Step [65175/68040], Loss: 5.0908\n",
      "Epoch [1/4], Step [65250/68040], Loss: 5.2969\n",
      "Epoch [1/4], Step [65325/68040], Loss: 5.0301\n",
      "Epoch [1/4], Step [65400/68040], Loss: 5.0698\n",
      "Epoch [1/4], Step [65475/68040], Loss: 5.2618\n",
      "Epoch [1/4], Step [65550/68040], Loss: 5.2009\n",
      "Epoch [1/4], Step [65625/68040], Loss: 5.0654\n",
      "Epoch [1/4], Step [65700/68040], Loss: 5.1511\n",
      "Epoch [1/4], Step [65775/68040], Loss: 5.1767\n",
      "Epoch [1/4], Step [65850/68040], Loss: 5.1946\n",
      "Epoch [1/4], Step [65925/68040], Loss: 4.9804\n",
      "Epoch [1/4], Step [66000/68040], Loss: 5.1774\n",
      "Epoch [1/4], Step [66075/68040], Loss: 5.2671\n",
      "Epoch [1/4], Step [66150/68040], Loss: 5.2627\n",
      "Epoch [1/4], Step [66225/68040], Loss: 5.1580\n",
      "Epoch [1/4], Step [66300/68040], Loss: 5.2883\n",
      "Epoch [1/4], Step [66375/68040], Loss: 5.0543\n",
      "Epoch [1/4], Step [66450/68040], Loss: 5.1740\n",
      "Epoch [1/4], Step [66525/68040], Loss: 5.3112\n",
      "Epoch [1/4], Step [66600/68040], Loss: 5.1758\n",
      "Epoch [1/4], Step [66675/68040], Loss: 5.2775\n",
      "Epoch [1/4], Step [66750/68040], Loss: 5.3305\n",
      "Epoch [1/4], Step [66825/68040], Loss: 5.0296\n",
      "Epoch [1/4], Step [66900/68040], Loss: 5.1523\n",
      "Epoch [1/4], Step [66975/68040], Loss: 5.0943\n",
      "Epoch [1/4], Step [67050/68040], Loss: 5.2629\n",
      "Epoch [1/4], Step [67125/68040], Loss: 5.0861\n",
      "Epoch [1/4], Step [67200/68040], Loss: 5.2117\n",
      "Epoch [1/4], Step [67275/68040], Loss: 5.1572\n",
      "Epoch [1/4], Step [67350/68040], Loss: 5.3517\n",
      "Epoch [1/4], Step [67425/68040], Loss: 5.1050\n",
      "Epoch [1/4], Step [67500/68040], Loss: 5.1896\n",
      "Epoch [1/4], Step [67575/68040], Loss: 5.2183\n",
      "Epoch [1/4], Step [67650/68040], Loss: 5.0788\n",
      "Epoch [1/4], Step [67725/68040], Loss: 5.1388\n",
      "Epoch [1/4], Step [67800/68040], Loss: 5.2987\n",
      "Epoch [1/4], Step [67875/68040], Loss: 5.2978\n",
      "Epoch [1/4], Step [67950/68040], Loss: 5.1735\n",
      "Epoch [1/4], Step [68025/68040], Loss: 5.1272\n",
      "Epoch [1/4] Average Loss: 5.4070, Perplexity: 222.96\n",
      "Epoch [2/4], Step [0/68040], Loss: 5.2712\n",
      "Validation perplexity: 152.70467541735854\n",
      "Epoch [2/4], Step [75/68040], Loss: 5.2694\n",
      "Epoch [2/4], Step [150/68040], Loss: 5.2307\n",
      "Epoch [2/4], Step [225/68040], Loss: 5.2079\n",
      "Epoch [2/4], Step [300/68040], Loss: 5.2424\n",
      "Epoch [2/4], Step [375/68040], Loss: 5.1809\n",
      "Epoch [2/4], Step [450/68040], Loss: 5.1245\n",
      "Epoch [2/4], Step [525/68040], Loss: 5.2497\n",
      "Epoch [2/4], Step [600/68040], Loss: 5.1047\n",
      "Epoch [2/4], Step [675/68040], Loss: 5.2443\n",
      "Epoch [2/4], Step [750/68040], Loss: 5.1611\n",
      "Epoch [2/4], Step [825/68040], Loss: 5.1582\n",
      "Epoch [2/4], Step [900/68040], Loss: 5.2580\n",
      "Epoch [2/4], Step [975/68040], Loss: 5.1298\n",
      "Epoch [2/4], Step [1050/68040], Loss: 5.2476\n",
      "Epoch [2/4], Step [1125/68040], Loss: 5.3090\n",
      "Epoch [2/4], Step [1200/68040], Loss: 5.2277\n",
      "Epoch [2/4], Step [1275/68040], Loss: 5.1600\n",
      "Epoch [2/4], Step [1350/68040], Loss: 5.1359\n",
      "Epoch [2/4], Step [1425/68040], Loss: 5.1158\n",
      "Epoch [2/4], Step [1500/68040], Loss: 5.2936\n",
      "Epoch [2/4], Step [1575/68040], Loss: 5.1821\n",
      "Epoch [2/4], Step [1650/68040], Loss: 5.0858\n",
      "Epoch [2/4], Step [1725/68040], Loss: 5.1398\n",
      "Epoch [2/4], Step [1800/68040], Loss: 5.1713\n",
      "Epoch [2/4], Step [1875/68040], Loss: 5.2645\n",
      "Epoch [2/4], Step [1950/68040], Loss: 5.2618\n",
      "Epoch [2/4], Step [2025/68040], Loss: 5.1303\n",
      "Epoch [2/4], Step [2100/68040], Loss: 5.3758\n",
      "Epoch [2/4], Step [2175/68040], Loss: 5.1772\n",
      "Epoch [2/4], Step [2250/68040], Loss: 5.0315\n",
      "Epoch [2/4], Step [2325/68040], Loss: 5.0950\n",
      "Epoch [2/4], Step [2400/68040], Loss: 5.1324\n",
      "Epoch [2/4], Step [2475/68040], Loss: 5.2988\n",
      "Epoch [2/4], Step [2550/68040], Loss: 5.1525\n",
      "Epoch [2/4], Step [2625/68040], Loss: 5.1004\n",
      "Epoch [2/4], Step [2700/68040], Loss: 5.1252\n",
      "Epoch [2/4], Step [2775/68040], Loss: 5.1786\n",
      "Epoch [2/4], Step [2850/68040], Loss: 5.2066\n",
      "Epoch [2/4], Step [2925/68040], Loss: 5.3364\n",
      "Epoch [2/4], Step [3000/68040], Loss: 5.1036\n",
      "Epoch [2/4], Step [3075/68040], Loss: 5.2865\n",
      "Epoch [2/4], Step [3150/68040], Loss: 5.1184\n",
      "Epoch [2/4], Step [3225/68040], Loss: 5.0708\n",
      "Epoch [2/4], Step [3300/68040], Loss: 5.1461\n",
      "Epoch [2/4], Step [3375/68040], Loss: 5.0411\n",
      "Epoch [2/4], Step [3450/68040], Loss: 5.2226\n",
      "Epoch [2/4], Step [3525/68040], Loss: 5.2090\n",
      "Epoch [2/4], Step [3600/68040], Loss: 5.3019\n",
      "Epoch [2/4], Step [3675/68040], Loss: 5.1610\n",
      "Epoch [2/4], Step [3750/68040], Loss: 5.1113\n",
      "Epoch [2/4], Step [3825/68040], Loss: 5.1403\n",
      "Epoch [2/4], Step [3900/68040], Loss: 5.0802\n",
      "Epoch [2/4], Step [3975/68040], Loss: 5.2859\n",
      "Epoch [2/4], Step [4050/68040], Loss: 4.9818\n",
      "Epoch [2/4], Step [4125/68040], Loss: 5.2041\n",
      "Epoch [2/4], Step [4200/68040], Loss: 5.1561\n",
      "Epoch [2/4], Step [4275/68040], Loss: 5.1114\n",
      "Epoch [2/4], Step [4350/68040], Loss: 5.0530\n",
      "Epoch [2/4], Step [4425/68040], Loss: 5.1042\n",
      "Epoch [2/4], Step [4500/68040], Loss: 5.0509\n",
      "Epoch [2/4], Step [4575/68040], Loss: 5.1022\n",
      "Epoch [2/4], Step [4650/68040], Loss: 5.0709\n",
      "Epoch [2/4], Step [4725/68040], Loss: 5.2573\n",
      "Epoch [2/4], Step [4800/68040], Loss: 5.2082\n",
      "Epoch [2/4], Step [4875/68040], Loss: 5.2012\n",
      "Epoch [2/4], Step [4950/68040], Loss: 5.0598\n",
      "Epoch [2/4], Step [5025/68040], Loss: 5.1155\n",
      "Epoch [2/4], Step [5100/68040], Loss: 5.0203\n",
      "Epoch [2/4], Step [5175/68040], Loss: 5.1942\n",
      "Epoch [2/4], Step [5250/68040], Loss: 5.1448\n",
      "Epoch [2/4], Step [5325/68040], Loss: 5.1514\n",
      "Epoch [2/4], Step [5400/68040], Loss: 5.3935\n",
      "Epoch [2/4], Step [5475/68040], Loss: 5.0886\n",
      "Epoch [2/4], Step [5550/68040], Loss: 4.9057\n",
      "Epoch [2/4], Step [5625/68040], Loss: 5.3393\n",
      "Epoch [2/4], Step [5700/68040], Loss: 4.9409\n",
      "Epoch [2/4], Step [5775/68040], Loss: 5.1759\n",
      "Epoch [2/4], Step [5850/68040], Loss: 5.2307\n",
      "Epoch [2/4], Step [5925/68040], Loss: 5.1121\n",
      "Epoch [2/4], Step [6000/68040], Loss: 5.1800\n",
      "Epoch [2/4], Step [6075/68040], Loss: 5.0683\n",
      "Epoch [2/4], Step [6150/68040], Loss: 5.4061\n",
      "Epoch [2/4], Step [6225/68040], Loss: 5.1074\n",
      "Epoch [2/4], Step [6300/68040], Loss: 5.2862\n",
      "Epoch [2/4], Step [6375/68040], Loss: 5.1300\n",
      "Epoch [2/4], Step [6450/68040], Loss: 5.1265\n",
      "Epoch [2/4], Step [6525/68040], Loss: 5.0534\n",
      "Epoch [2/4], Step [6600/68040], Loss: 5.1547\n",
      "Epoch [2/4], Step [6675/68040], Loss: 5.1339\n",
      "Epoch [2/4], Step [6750/68040], Loss: 5.1097\n",
      "Epoch [2/4], Step [6825/68040], Loss: 5.2609\n",
      "Epoch [2/4], Step [6900/68040], Loss: 5.1052\n",
      "Epoch [2/4], Step [6975/68040], Loss: 5.0961\n",
      "Epoch [2/4], Step [7050/68040], Loss: 5.1061\n",
      "Epoch [2/4], Step [7125/68040], Loss: 5.1045\n",
      "Epoch [2/4], Step [7200/68040], Loss: 5.0800\n",
      "Epoch [2/4], Step [7275/68040], Loss: 5.3265\n",
      "Epoch [2/4], Step [7350/68040], Loss: 5.2131\n",
      "Epoch [2/4], Step [7425/68040], Loss: 5.2600\n",
      "Epoch [2/4], Step [7500/68040], Loss: 5.2407\n",
      "Epoch [2/4], Step [7575/68040], Loss: 5.1448\n",
      "Epoch [2/4], Step [7650/68040], Loss: 4.9702\n",
      "Epoch [2/4], Step [7725/68040], Loss: 5.3599\n",
      "Epoch [2/4], Step [7800/68040], Loss: 5.1651\n",
      "Epoch [2/4], Step [7875/68040], Loss: 5.1393\n",
      "Epoch [2/4], Step [7950/68040], Loss: 5.2325\n",
      "Epoch [2/4], Step [8025/68040], Loss: 5.1249\n",
      "Epoch [2/4], Step [8100/68040], Loss: 5.1931\n",
      "Epoch [2/4], Step [8175/68040], Loss: 5.1151\n",
      "Epoch [2/4], Step [8250/68040], Loss: 5.1939\n",
      "Epoch [2/4], Step [8325/68040], Loss: 5.1177\n",
      "Epoch [2/4], Step [8400/68040], Loss: 5.1255\n",
      "Epoch [2/4], Step [8475/68040], Loss: 5.0599\n",
      "Epoch [2/4], Step [8550/68040], Loss: 5.2802\n",
      "Epoch [2/4], Step [8625/68040], Loss: 5.1133\n",
      "Epoch [2/4], Step [8700/68040], Loss: 5.1689\n",
      "Epoch [2/4], Step [8775/68040], Loss: 5.0291\n",
      "Epoch [2/4], Step [8850/68040], Loss: 5.1766\n",
      "Epoch [2/4], Step [8925/68040], Loss: 5.1629\n",
      "Epoch [2/4], Step [9000/68040], Loss: 5.3851\n",
      "Epoch [2/4], Step [9075/68040], Loss: 5.2983\n",
      "Epoch [2/4], Step [9150/68040], Loss: 5.2308\n",
      "Epoch [2/4], Step [9225/68040], Loss: 5.1809\n",
      "Epoch [2/4], Step [9300/68040], Loss: 5.2094\n",
      "Epoch [2/4], Step [9375/68040], Loss: 5.2499\n",
      "Epoch [2/4], Step [9450/68040], Loss: 5.0841\n",
      "Epoch [2/4], Step [9525/68040], Loss: 5.2051\n",
      "Epoch [2/4], Step [9600/68040], Loss: 4.8159\n",
      "Epoch [2/4], Step [9675/68040], Loss: 5.0856\n",
      "Epoch [2/4], Step [9750/68040], Loss: 5.3203\n",
      "Epoch [2/4], Step [9825/68040], Loss: 5.2566\n",
      "Epoch [2/4], Step [9900/68040], Loss: 5.1747\n",
      "Epoch [2/4], Step [9975/68040], Loss: 5.0798\n",
      "Validation perplexity: 149.44864756901958\n",
      "Epoch [2/4], Step [10050/68040], Loss: 4.9957\n",
      "Epoch [2/4], Step [10125/68040], Loss: 5.1968\n",
      "Epoch [2/4], Step [10200/68040], Loss: 5.1773\n",
      "Epoch [2/4], Step [10275/68040], Loss: 5.1564\n",
      "Epoch [2/4], Step [10350/68040], Loss: 5.2465\n",
      "Epoch [2/4], Step [10425/68040], Loss: 5.0679\n",
      "Epoch [2/4], Step [10500/68040], Loss: 4.9446\n",
      "Epoch [2/4], Step [10575/68040], Loss: 5.3022\n",
      "Epoch [2/4], Step [10650/68040], Loss: 5.1336\n",
      "Epoch [2/4], Step [10725/68040], Loss: 5.0906\n",
      "Epoch [2/4], Step [10800/68040], Loss: 5.1365\n",
      "Epoch [2/4], Step [10875/68040], Loss: 5.2845\n",
      "Epoch [2/4], Step [10950/68040], Loss: 5.0959\n",
      "Epoch [2/4], Step [11025/68040], Loss: 5.1147\n",
      "Epoch [2/4], Step [11100/68040], Loss: 5.2278\n",
      "Epoch [2/4], Step [11175/68040], Loss: 5.2454\n",
      "Epoch [2/4], Step [11250/68040], Loss: 5.1880\n",
      "Epoch [2/4], Step [11325/68040], Loss: 5.2600\n",
      "Epoch [2/4], Step [11400/68040], Loss: 5.1615\n",
      "Epoch [2/4], Step [11475/68040], Loss: 5.0940\n",
      "Epoch [2/4], Step [11550/68040], Loss: 5.2296\n",
      "Epoch [2/4], Step [11625/68040], Loss: 5.1226\n",
      "Epoch [2/4], Step [11700/68040], Loss: 5.0655\n",
      "Epoch [2/4], Step [11775/68040], Loss: 5.1775\n",
      "Epoch [2/4], Step [11850/68040], Loss: 5.3112\n",
      "Epoch [2/4], Step [11925/68040], Loss: 5.0788\n",
      "Epoch [2/4], Step [12000/68040], Loss: 5.0898\n",
      "Epoch [2/4], Step [12075/68040], Loss: 5.0104\n",
      "Epoch [2/4], Step [12150/68040], Loss: 5.1953\n",
      "Epoch [2/4], Step [12225/68040], Loss: 5.0949\n",
      "Epoch [2/4], Step [12300/68040], Loss: 5.1171\n",
      "Epoch [2/4], Step [12375/68040], Loss: 5.1847\n",
      "Epoch [2/4], Step [12450/68040], Loss: 5.0865\n",
      "Epoch [2/4], Step [12525/68040], Loss: 5.2718\n",
      "Epoch [2/4], Step [12600/68040], Loss: 5.0170\n",
      "Epoch [2/4], Step [12675/68040], Loss: 5.2122\n",
      "Epoch [2/4], Step [12750/68040], Loss: 4.9361\n",
      "Epoch [2/4], Step [12825/68040], Loss: 4.9020\n",
      "Epoch [2/4], Step [12900/68040], Loss: 5.1434\n",
      "Epoch [2/4], Step [12975/68040], Loss: 5.0770\n",
      "Epoch [2/4], Step [13050/68040], Loss: 5.0812\n",
      "Epoch [2/4], Step [13125/68040], Loss: 5.3701\n",
      "Epoch [2/4], Step [13200/68040], Loss: 5.0703\n",
      "Epoch [2/4], Step [13275/68040], Loss: 5.1297\n",
      "Epoch [2/4], Step [13350/68040], Loss: 5.0588\n",
      "Epoch [2/4], Step [13425/68040], Loss: 5.1116\n",
      "Epoch [2/4], Step [13500/68040], Loss: 5.1800\n",
      "Epoch [2/4], Step [13575/68040], Loss: 5.1967\n",
      "Epoch [2/4], Step [13650/68040], Loss: 5.1043\n",
      "Epoch [2/4], Step [13725/68040], Loss: 5.1545\n",
      "Epoch [2/4], Step [13800/68040], Loss: 5.0416\n",
      "Epoch [2/4], Step [13875/68040], Loss: 4.9924\n",
      "Epoch [2/4], Step [13950/68040], Loss: 5.0996\n",
      "Epoch [2/4], Step [14025/68040], Loss: 5.1676\n",
      "Epoch [2/4], Step [14100/68040], Loss: 5.0363\n",
      "Epoch [2/4], Step [14175/68040], Loss: 4.9685\n",
      "Epoch [2/4], Step [14250/68040], Loss: 5.0282\n",
      "Epoch [2/4], Step [14325/68040], Loss: 5.1423\n",
      "Epoch [2/4], Step [14400/68040], Loss: 5.0335\n",
      "Epoch [2/4], Step [14475/68040], Loss: 5.2246\n",
      "Epoch [2/4], Step [14550/68040], Loss: 5.0521\n",
      "Epoch [2/4], Step [14625/68040], Loss: 5.2095\n",
      "Epoch [2/4], Step [14700/68040], Loss: 5.0885\n",
      "Epoch [2/4], Step [14775/68040], Loss: 5.1266\n",
      "Epoch [2/4], Step [14850/68040], Loss: 5.1271\n",
      "Epoch [2/4], Step [14925/68040], Loss: 4.9803\n",
      "Epoch [2/4], Step [15000/68040], Loss: 5.2048\n",
      "Epoch [2/4], Step [15075/68040], Loss: 5.1699\n",
      "Epoch [2/4], Step [15150/68040], Loss: 5.0956\n",
      "Epoch [2/4], Step [15225/68040], Loss: 5.1492\n",
      "Epoch [2/4], Step [15300/68040], Loss: 5.0685\n",
      "Epoch [2/4], Step [15375/68040], Loss: 5.0845\n",
      "Epoch [2/4], Step [15450/68040], Loss: 5.1147\n",
      "Epoch [2/4], Step [15525/68040], Loss: 4.9810\n",
      "Epoch [2/4], Step [15600/68040], Loss: 5.1231\n",
      "Epoch [2/4], Step [15675/68040], Loss: 4.9332\n",
      "Epoch [2/4], Step [15750/68040], Loss: 5.2043\n",
      "Epoch [2/4], Step [15825/68040], Loss: 5.1852\n",
      "Epoch [2/4], Step [15900/68040], Loss: 4.8634\n",
      "Epoch [2/4], Step [15975/68040], Loss: 5.1550\n",
      "Epoch [2/4], Step [16050/68040], Loss: 4.9310\n",
      "Epoch [2/4], Step [16125/68040], Loss: 4.8923\n",
      "Epoch [2/4], Step [16200/68040], Loss: 5.1514\n",
      "Epoch [2/4], Step [16275/68040], Loss: 4.9828\n",
      "Epoch [2/4], Step [16350/68040], Loss: 5.1895\n",
      "Epoch [2/4], Step [16425/68040], Loss: 5.1430\n",
      "Epoch [2/4], Step [16500/68040], Loss: 5.3449\n",
      "Epoch [2/4], Step [16575/68040], Loss: 4.9941\n",
      "Epoch [2/4], Step [16650/68040], Loss: 5.0189\n",
      "Epoch [2/4], Step [16725/68040], Loss: 4.9594\n",
      "Epoch [2/4], Step [16800/68040], Loss: 5.3578\n",
      "Epoch [2/4], Step [16875/68040], Loss: 5.0417\n",
      "Epoch [2/4], Step [16950/68040], Loss: 5.3491\n",
      "Epoch [2/4], Step [17025/68040], Loss: 5.1468\n",
      "Epoch [2/4], Step [17100/68040], Loss: 5.0733\n",
      "Epoch [2/4], Step [17175/68040], Loss: 5.1116\n",
      "Epoch [2/4], Step [17250/68040], Loss: 5.1319\n",
      "Epoch [2/4], Step [17325/68040], Loss: 5.1251\n",
      "Epoch [2/4], Step [17400/68040], Loss: 5.1964\n",
      "Epoch [2/4], Step [17475/68040], Loss: 5.2503\n",
      "Epoch [2/4], Step [17550/68040], Loss: 5.2488\n",
      "Epoch [2/4], Step [17625/68040], Loss: 4.9939\n",
      "Epoch [2/4], Step [17700/68040], Loss: 5.2695\n",
      "Epoch [2/4], Step [17775/68040], Loss: 5.3239\n",
      "Epoch [2/4], Step [17850/68040], Loss: 5.0865\n",
      "Epoch [2/4], Step [17925/68040], Loss: 5.2886\n",
      "Epoch [2/4], Step [18000/68040], Loss: 5.0028\n",
      "Epoch [2/4], Step [18075/68040], Loss: 5.1349\n",
      "Epoch [2/4], Step [18150/68040], Loss: 5.1846\n",
      "Epoch [2/4], Step [18225/68040], Loss: 5.2625\n",
      "Epoch [2/4], Step [18300/68040], Loss: 5.2651\n",
      "Epoch [2/4], Step [18375/68040], Loss: 5.1406\n",
      "Epoch [2/4], Step [18450/68040], Loss: 5.1814\n",
      "Epoch [2/4], Step [18525/68040], Loss: 5.1908\n",
      "Epoch [2/4], Step [18600/68040], Loss: 5.0698\n",
      "Epoch [2/4], Step [18675/68040], Loss: 5.3705\n",
      "Epoch [2/4], Step [18750/68040], Loss: 5.0403\n",
      "Epoch [2/4], Step [18825/68040], Loss: 5.0655\n",
      "Epoch [2/4], Step [18900/68040], Loss: 5.0458\n",
      "Epoch [2/4], Step [18975/68040], Loss: 5.3075\n",
      "Epoch [2/4], Step [19050/68040], Loss: 5.0231\n",
      "Epoch [2/4], Step [19125/68040], Loss: 5.0923\n",
      "Epoch [2/4], Step [19200/68040], Loss: 5.1687\n",
      "Epoch [2/4], Step [19275/68040], Loss: 5.1459\n",
      "Epoch [2/4], Step [19350/68040], Loss: 4.9412\n",
      "Epoch [2/4], Step [19425/68040], Loss: 5.1641\n",
      "Epoch [2/4], Step [19500/68040], Loss: 5.1330\n",
      "Epoch [2/4], Step [19575/68040], Loss: 5.2230\n",
      "Epoch [2/4], Step [19650/68040], Loss: 5.0783\n",
      "Epoch [2/4], Step [19725/68040], Loss: 4.9982\n",
      "Epoch [2/4], Step [19800/68040], Loss: 4.8708\n",
      "Epoch [2/4], Step [19875/68040], Loss: 5.0052\n",
      "Epoch [2/4], Step [19950/68040], Loss: 5.0557\n",
      "Validation perplexity: 146.77722503006075\n",
      "Epoch [2/4], Step [20025/68040], Loss: 5.1298\n",
      "Epoch [2/4], Step [20100/68040], Loss: 5.1062\n",
      "Epoch [2/4], Step [20175/68040], Loss: 5.0473\n",
      "Epoch [2/4], Step [20250/68040], Loss: 5.1186\n",
      "Epoch [2/4], Step [20325/68040], Loss: 5.4023\n",
      "Epoch [2/4], Step [20400/68040], Loss: 5.2581\n",
      "Epoch [2/4], Step [20475/68040], Loss: 5.0236\n",
      "Epoch [2/4], Step [20550/68040], Loss: 5.2908\n",
      "Epoch [2/4], Step [20625/68040], Loss: 5.1207\n",
      "Epoch [2/4], Step [20700/68040], Loss: 5.2900\n",
      "Epoch [2/4], Step [20775/68040], Loss: 5.2588\n",
      "Epoch [2/4], Step [20850/68040], Loss: 5.0612\n",
      "Epoch [2/4], Step [20925/68040], Loss: 5.0684\n",
      "Epoch [2/4], Step [21000/68040], Loss: 5.2547\n",
      "Epoch [2/4], Step [21075/68040], Loss: 5.0111\n",
      "Epoch [2/4], Step [21150/68040], Loss: 5.1286\n",
      "Epoch [2/4], Step [21225/68040], Loss: 5.0074\n",
      "Epoch [2/4], Step [21300/68040], Loss: 5.2129\n",
      "Epoch [2/4], Step [21375/68040], Loss: 5.0861\n",
      "Epoch [2/4], Step [21450/68040], Loss: 5.1503\n",
      "Epoch [2/4], Step [21525/68040], Loss: 5.2342\n",
      "Epoch [2/4], Step [21600/68040], Loss: 5.3399\n",
      "Epoch [2/4], Step [21675/68040], Loss: 5.0146\n",
      "Epoch [2/4], Step [21750/68040], Loss: 5.2361\n",
      "Epoch [2/4], Step [21825/68040], Loss: 5.0379\n",
      "Epoch [2/4], Step [21900/68040], Loss: 4.9718\n",
      "Epoch [2/4], Step [21975/68040], Loss: 4.8870\n",
      "Epoch [2/4], Step [22050/68040], Loss: 5.3614\n",
      "Epoch [2/4], Step [22125/68040], Loss: 5.0512\n",
      "Epoch [2/4], Step [22200/68040], Loss: 5.0690\n",
      "Epoch [2/4], Step [22275/68040], Loss: 5.0496\n",
      "Epoch [2/4], Step [22350/68040], Loss: 5.1263\n",
      "Epoch [2/4], Step [22425/68040], Loss: 5.1714\n",
      "Epoch [2/4], Step [22500/68040], Loss: 4.9397\n",
      "Epoch [2/4], Step [22575/68040], Loss: 5.1603\n",
      "Epoch [2/4], Step [22650/68040], Loss: 5.1396\n",
      "Epoch [2/4], Step [22725/68040], Loss: 5.2463\n",
      "Epoch [2/4], Step [22800/68040], Loss: 4.9065\n",
      "Epoch [2/4], Step [22875/68040], Loss: 5.0994\n",
      "Epoch [2/4], Step [22950/68040], Loss: 5.0336\n",
      "Epoch [2/4], Step [23025/68040], Loss: 5.1378\n",
      "Epoch [2/4], Step [23100/68040], Loss: 5.1123\n",
      "Epoch [2/4], Step [23175/68040], Loss: 5.2901\n",
      "Epoch [2/4], Step [23250/68040], Loss: 5.1943\n",
      "Epoch [2/4], Step [23325/68040], Loss: 5.2427\n",
      "Epoch [2/4], Step [23400/68040], Loss: 5.1394\n",
      "Epoch [2/4], Step [23475/68040], Loss: 5.1265\n",
      "Epoch [2/4], Step [23550/68040], Loss: 5.1145\n",
      "Epoch [2/4], Step [23625/68040], Loss: 5.2351\n",
      "Epoch [2/4], Step [23700/68040], Loss: 5.1476\n",
      "Epoch [2/4], Step [23775/68040], Loss: 5.1712\n",
      "Epoch [2/4], Step [23850/68040], Loss: 5.1275\n",
      "Epoch [2/4], Step [23925/68040], Loss: 4.9756\n",
      "Epoch [2/4], Step [24000/68040], Loss: 5.1707\n",
      "Epoch [2/4], Step [24075/68040], Loss: 5.0977\n",
      "Epoch [2/4], Step [24150/68040], Loss: 5.1949\n",
      "Epoch [2/4], Step [24225/68040], Loss: 5.2712\n",
      "Epoch [2/4], Step [24300/68040], Loss: 5.0047\n",
      "Epoch [2/4], Step [24375/68040], Loss: 5.0647\n",
      "Epoch [2/4], Step [24450/68040], Loss: 5.1711\n",
      "Epoch [2/4], Step [24525/68040], Loss: 5.1485\n",
      "Epoch [2/4], Step [24600/68040], Loss: 5.1544\n",
      "Epoch [2/4], Step [24675/68040], Loss: 5.0936\n",
      "Epoch [2/4], Step [24750/68040], Loss: 5.2806\n",
      "Epoch [2/4], Step [24825/68040], Loss: 5.0514\n",
      "Epoch [2/4], Step [24900/68040], Loss: 5.1915\n",
      "Epoch [2/4], Step [24975/68040], Loss: 5.2873\n",
      "Epoch [2/4], Step [25050/68040], Loss: 5.0969\n",
      "Epoch [2/4], Step [25125/68040], Loss: 5.2230\n",
      "Epoch [2/4], Step [25200/68040], Loss: 5.1518\n",
      "Epoch [2/4], Step [25275/68040], Loss: 5.2187\n",
      "Epoch [2/4], Step [25350/68040], Loss: 4.9869\n",
      "Epoch [2/4], Step [25425/68040], Loss: 5.1590\n",
      "Epoch [2/4], Step [25500/68040], Loss: 5.2053\n",
      "Epoch [2/4], Step [25575/68040], Loss: 5.1855\n",
      "Epoch [2/4], Step [25650/68040], Loss: 4.9337\n",
      "Epoch [2/4], Step [25725/68040], Loss: 5.1616\n",
      "Epoch [2/4], Step [25800/68040], Loss: 5.0926\n",
      "Epoch [2/4], Step [25875/68040], Loss: 5.1895\n",
      "Epoch [2/4], Step [25950/68040], Loss: 5.0956\n",
      "Epoch [2/4], Step [26025/68040], Loss: 5.3197\n",
      "Epoch [2/4], Step [26100/68040], Loss: 5.0749\n",
      "Epoch [2/4], Step [26175/68040], Loss: 5.1099\n",
      "Epoch [2/4], Step [26250/68040], Loss: 5.2862\n",
      "Epoch [2/4], Step [26325/68040], Loss: 5.2406\n",
      "Epoch [2/4], Step [26400/68040], Loss: 5.1322\n",
      "Epoch [2/4], Step [26475/68040], Loss: 5.2743\n",
      "Epoch [2/4], Step [26550/68040], Loss: 5.3076\n",
      "Epoch [2/4], Step [26625/68040], Loss: 5.0644\n",
      "Epoch [2/4], Step [26700/68040], Loss: 5.0888\n",
      "Epoch [2/4], Step [26775/68040], Loss: 5.1984\n",
      "Epoch [2/4], Step [26850/68040], Loss: 4.9151\n",
      "Epoch [2/4], Step [26925/68040], Loss: 5.0085\n",
      "Epoch [2/4], Step [27000/68040], Loss: 4.9266\n",
      "Epoch [2/4], Step [27075/68040], Loss: 5.1258\n",
      "Epoch [2/4], Step [27150/68040], Loss: 5.0763\n",
      "Epoch [2/4], Step [27225/68040], Loss: 5.1384\n",
      "Epoch [2/4], Step [27300/68040], Loss: 5.0987\n",
      "Epoch [2/4], Step [27375/68040], Loss: 5.2087\n",
      "Epoch [2/4], Step [27450/68040], Loss: 4.9666\n",
      "Epoch [2/4], Step [27525/68040], Loss: 5.0655\n",
      "Epoch [2/4], Step [27600/68040], Loss: 5.0167\n",
      "Epoch [2/4], Step [27675/68040], Loss: 5.0526\n",
      "Epoch [2/4], Step [27750/68040], Loss: 5.0066\n",
      "Epoch [2/4], Step [27825/68040], Loss: 4.9945\n",
      "Epoch [2/4], Step [27900/68040], Loss: 5.1591\n",
      "Epoch [2/4], Step [27975/68040], Loss: 5.1540\n",
      "Epoch [2/4], Step [28050/68040], Loss: 5.1567\n",
      "Epoch [2/4], Step [28125/68040], Loss: 5.1548\n",
      "Epoch [2/4], Step [28200/68040], Loss: 5.0484\n",
      "Epoch [2/4], Step [28275/68040], Loss: 5.0894\n",
      "Epoch [2/4], Step [28350/68040], Loss: 5.1884\n",
      "Epoch [2/4], Step [28425/68040], Loss: 5.1908\n",
      "Epoch [2/4], Step [28500/68040], Loss: 5.0182\n",
      "Epoch [2/4], Step [28575/68040], Loss: 5.1096\n",
      "Epoch [2/4], Step [28650/68040], Loss: 5.0210\n",
      "Epoch [2/4], Step [28725/68040], Loss: 5.0282\n",
      "Epoch [2/4], Step [28800/68040], Loss: 5.1865\n",
      "Epoch [2/4], Step [28875/68040], Loss: 4.9413\n",
      "Epoch [2/4], Step [28950/68040], Loss: 5.1009\n",
      "Epoch [2/4], Step [29025/68040], Loss: 5.1287\n",
      "Epoch [2/4], Step [29100/68040], Loss: 5.1459\n",
      "Epoch [2/4], Step [29175/68040], Loss: 5.0361\n",
      "Epoch [2/4], Step [29250/68040], Loss: 5.0911\n",
      "Epoch [2/4], Step [29325/68040], Loss: 5.0482\n",
      "Epoch [2/4], Step [29400/68040], Loss: 5.0816\n",
      "Epoch [2/4], Step [29475/68040], Loss: 5.2457\n",
      "Epoch [2/4], Step [29550/68040], Loss: 5.2112\n",
      "Epoch [2/4], Step [29625/68040], Loss: 5.0126\n",
      "Epoch [2/4], Step [29700/68040], Loss: 5.0777\n",
      "Epoch [2/4], Step [29775/68040], Loss: 5.2675\n",
      "Epoch [2/4], Step [29850/68040], Loss: 5.2189\n",
      "Epoch [2/4], Step [29925/68040], Loss: 5.1270\n",
      "Epoch [2/4], Step [30000/68040], Loss: 5.1444\n",
      "Validation perplexity: 144.59734635635664\n",
      "Epoch [2/4], Step [30075/68040], Loss: 5.0844\n",
      "Epoch [2/4], Step [30150/68040], Loss: 4.9920\n",
      "Epoch [2/4], Step [30225/68040], Loss: 5.2199\n",
      "Epoch [2/4], Step [30300/68040], Loss: 5.1230\n",
      "Epoch [2/4], Step [30375/68040], Loss: 5.2089\n",
      "Epoch [2/4], Step [30450/68040], Loss: 4.9236\n",
      "Epoch [2/4], Step [30525/68040], Loss: 5.1378\n",
      "Epoch [2/4], Step [30600/68040], Loss: 5.1497\n",
      "Epoch [2/4], Step [30675/68040], Loss: 4.9439\n",
      "Epoch [2/4], Step [30750/68040], Loss: 5.1970\n",
      "Epoch [2/4], Step [30825/68040], Loss: 5.0219\n",
      "Epoch [2/4], Step [30900/68040], Loss: 5.1203\n",
      "Epoch [2/4], Step [30975/68040], Loss: 5.2217\n",
      "Epoch [2/4], Step [31050/68040], Loss: 5.0372\n",
      "Epoch [2/4], Step [31125/68040], Loss: 5.0879\n",
      "Epoch [2/4], Step [31200/68040], Loss: 5.0332\n",
      "Epoch [2/4], Step [31275/68040], Loss: 5.0733\n",
      "Epoch [2/4], Step [31350/68040], Loss: 5.1818\n",
      "Epoch [2/4], Step [31425/68040], Loss: 5.1644\n",
      "Epoch [2/4], Step [31500/68040], Loss: 5.2533\n",
      "Epoch [2/4], Step [31575/68040], Loss: 5.0472\n",
      "Epoch [2/4], Step [31650/68040], Loss: 5.2592\n",
      "Epoch [2/4], Step [31725/68040], Loss: 5.0440\n",
      "Epoch [2/4], Step [31800/68040], Loss: 5.0388\n",
      "Epoch [2/4], Step [31875/68040], Loss: 5.1337\n",
      "Epoch [2/4], Step [31950/68040], Loss: 5.1230\n",
      "Epoch [2/4], Step [32025/68040], Loss: 4.9688\n",
      "Epoch [2/4], Step [32100/68040], Loss: 5.2753\n",
      "Epoch [2/4], Step [32175/68040], Loss: 5.2364\n",
      "Epoch [2/4], Step [32250/68040], Loss: 5.0336\n",
      "Epoch [2/4], Step [32325/68040], Loss: 5.0301\n",
      "Epoch [2/4], Step [32400/68040], Loss: 5.0595\n",
      "Epoch [2/4], Step [32475/68040], Loss: 5.0463\n",
      "Epoch [2/4], Step [32550/68040], Loss: 5.1800\n",
      "Epoch [2/4], Step [32625/68040], Loss: 4.9280\n",
      "Epoch [2/4], Step [32700/68040], Loss: 4.9395\n",
      "Epoch [2/4], Step [32775/68040], Loss: 4.9584\n",
      "Epoch [2/4], Step [32850/68040], Loss: 5.2584\n",
      "Epoch [2/4], Step [32925/68040], Loss: 5.2282\n",
      "Epoch [2/4], Step [33000/68040], Loss: 5.1514\n",
      "Epoch [2/4], Step [33075/68040], Loss: 5.2173\n",
      "Epoch [2/4], Step [33150/68040], Loss: 5.0841\n",
      "Epoch [2/4], Step [33225/68040], Loss: 4.9907\n",
      "Epoch [2/4], Step [33300/68040], Loss: 5.1700\n",
      "Epoch [2/4], Step [33375/68040], Loss: 5.1169\n",
      "Epoch [2/4], Step [33450/68040], Loss: 4.9992\n",
      "Epoch [2/4], Step [33525/68040], Loss: 5.1155\n",
      "Epoch [2/4], Step [33600/68040], Loss: 5.0879\n",
      "Epoch [2/4], Step [33675/68040], Loss: 4.8895\n",
      "Epoch [2/4], Step [33750/68040], Loss: 5.1639\n",
      "Epoch [2/4], Step [33825/68040], Loss: 5.0728\n",
      "Epoch [2/4], Step [33900/68040], Loss: 5.1121\n",
      "Epoch [2/4], Step [33975/68040], Loss: 5.3400\n",
      "Epoch [2/4], Step [34050/68040], Loss: 5.2102\n",
      "Epoch [2/4], Step [34125/68040], Loss: 5.1955\n",
      "Epoch [2/4], Step [34200/68040], Loss: 5.2419\n",
      "Epoch [2/4], Step [34275/68040], Loss: 5.2000\n",
      "Epoch [2/4], Step [34350/68040], Loss: 4.9722\n",
      "Epoch [2/4], Step [34425/68040], Loss: 5.3332\n",
      "Epoch [2/4], Step [34500/68040], Loss: 5.0768\n",
      "Epoch [2/4], Step [34575/68040], Loss: 5.3252\n",
      "Epoch [2/4], Step [34650/68040], Loss: 5.2384\n",
      "Epoch [2/4], Step [34725/68040], Loss: 5.1360\n",
      "Epoch [2/4], Step [34800/68040], Loss: 5.0899\n",
      "Epoch [2/4], Step [34875/68040], Loss: 5.0257\n",
      "Epoch [2/4], Step [34950/68040], Loss: 5.1421\n",
      "Epoch [2/4], Step [35025/68040], Loss: 5.0129\n",
      "Epoch [2/4], Step [35100/68040], Loss: 5.1243\n",
      "Epoch [2/4], Step [35175/68040], Loss: 5.3155\n",
      "Epoch [2/4], Step [35250/68040], Loss: 5.1250\n",
      "Epoch [2/4], Step [35325/68040], Loss: 5.0310\n",
      "Epoch [2/4], Step [35400/68040], Loss: 5.3081\n",
      "Epoch [2/4], Step [35475/68040], Loss: 5.2144\n",
      "Epoch [2/4], Step [35550/68040], Loss: 5.0034\n",
      "Epoch [2/4], Step [35625/68040], Loss: 5.1966\n",
      "Epoch [2/4], Step [35700/68040], Loss: 4.9724\n",
      "Epoch [2/4], Step [35775/68040], Loss: 5.1246\n",
      "Epoch [2/4], Step [35850/68040], Loss: 4.9639\n",
      "Epoch [2/4], Step [35925/68040], Loss: 5.0730\n",
      "Epoch [2/4], Step [36000/68040], Loss: 5.0223\n",
      "Epoch [2/4], Step [36075/68040], Loss: 5.1148\n",
      "Epoch [2/4], Step [36150/68040], Loss: 5.0641\n",
      "Epoch [2/4], Step [36225/68040], Loss: 5.1281\n",
      "Epoch [2/4], Step [36300/68040], Loss: 5.1976\n",
      "Epoch [2/4], Step [36375/68040], Loss: 5.0525\n",
      "Epoch [2/4], Step [36450/68040], Loss: 5.1926\n",
      "Epoch [2/4], Step [36525/68040], Loss: 5.0417\n",
      "Epoch [2/4], Step [36600/68040], Loss: 5.0330\n",
      "Epoch [2/4], Step [36675/68040], Loss: 5.2410\n",
      "Epoch [2/4], Step [36750/68040], Loss: 5.2316\n",
      "Epoch [2/4], Step [36825/68040], Loss: 5.2253\n",
      "Epoch [2/4], Step [36900/68040], Loss: 5.1793\n",
      "Epoch [2/4], Step [36975/68040], Loss: 5.1318\n",
      "Epoch [2/4], Step [37050/68040], Loss: 5.0653\n",
      "Epoch [2/4], Step [37125/68040], Loss: 5.0295\n",
      "Epoch [2/4], Step [37200/68040], Loss: 4.9704\n",
      "Epoch [2/4], Step [37275/68040], Loss: 5.0964\n",
      "Epoch [2/4], Step [37350/68040], Loss: 5.1201\n",
      "Epoch [2/4], Step [37425/68040], Loss: 5.0669\n",
      "Epoch [2/4], Step [37500/68040], Loss: 5.1237\n",
      "Epoch [2/4], Step [37575/68040], Loss: 4.9216\n",
      "Epoch [2/4], Step [37650/68040], Loss: 5.1893\n",
      "Epoch [2/4], Step [37725/68040], Loss: 5.0431\n",
      "Epoch [2/4], Step [37800/68040], Loss: 5.0048\n",
      "Epoch [2/4], Step [37875/68040], Loss: 5.1904\n",
      "Epoch [2/4], Step [37950/68040], Loss: 4.9842\n",
      "Epoch [2/4], Step [38025/68040], Loss: 5.1257\n",
      "Epoch [2/4], Step [38100/68040], Loss: 5.0411\n",
      "Epoch [2/4], Step [38175/68040], Loss: 5.3206\n",
      "Epoch [2/4], Step [38250/68040], Loss: 4.9683\n",
      "Epoch [2/4], Step [38325/68040], Loss: 4.9873\n",
      "Epoch [2/4], Step [38400/68040], Loss: 5.2192\n",
      "Epoch [2/4], Step [38475/68040], Loss: 4.9829\n",
      "Epoch [2/4], Step [38550/68040], Loss: 5.0493\n",
      "Epoch [2/4], Step [38625/68040], Loss: 5.2496\n",
      "Epoch [2/4], Step [38700/68040], Loss: 5.1393\n",
      "Epoch [2/4], Step [38775/68040], Loss: 5.0838\n",
      "Epoch [2/4], Step [38850/68040], Loss: 4.9045\n",
      "Epoch [2/4], Step [38925/68040], Loss: 5.1094\n",
      "Epoch [2/4], Step [39000/68040], Loss: 5.0673\n",
      "Epoch [2/4], Step [39075/68040], Loss: 5.1894\n",
      "Epoch [2/4], Step [39150/68040], Loss: 5.0868\n",
      "Epoch [2/4], Step [39225/68040], Loss: 5.2806\n",
      "Epoch [2/4], Step [39300/68040], Loss: 5.1364\n",
      "Epoch [2/4], Step [39375/68040], Loss: 5.0548\n",
      "Epoch [2/4], Step [39450/68040], Loss: 4.9617\n",
      "Epoch [2/4], Step [39525/68040], Loss: 5.1687\n",
      "Epoch [2/4], Step [39600/68040], Loss: 5.1959\n",
      "Epoch [2/4], Step [39675/68040], Loss: 5.2582\n",
      "Epoch [2/4], Step [39750/68040], Loss: 5.1495\n",
      "Epoch [2/4], Step [39825/68040], Loss: 4.9529\n",
      "Epoch [2/4], Step [39900/68040], Loss: 5.0583\n",
      "Epoch [2/4], Step [39975/68040], Loss: 5.1244\n",
      "Validation perplexity: 142.75019325549545\n",
      "Epoch [2/4], Step [40050/68040], Loss: 5.1237\n",
      "Epoch [2/4], Step [40125/68040], Loss: 5.0948\n",
      "Epoch [2/4], Step [40200/68040], Loss: 5.0435\n",
      "Epoch [2/4], Step [40275/68040], Loss: 5.1251\n",
      "Epoch [2/4], Step [40350/68040], Loss: 4.9530\n",
      "Epoch [2/4], Step [40425/68040], Loss: 5.1373\n",
      "Epoch [2/4], Step [40500/68040], Loss: 4.9500\n",
      "Epoch [2/4], Step [40575/68040], Loss: 5.1289\n",
      "Epoch [2/4], Step [40650/68040], Loss: 5.1393\n",
      "Epoch [2/4], Step [40725/68040], Loss: 5.1848\n",
      "Epoch [2/4], Step [40800/68040], Loss: 5.0532\n",
      "Epoch [2/4], Step [40875/68040], Loss: 5.0235\n",
      "Epoch [2/4], Step [40950/68040], Loss: 5.1051\n",
      "Epoch [2/4], Step [41025/68040], Loss: 5.1469\n",
      "Epoch [2/4], Step [41100/68040], Loss: 4.8526\n",
      "Epoch [2/4], Step [41175/68040], Loss: 4.9939\n",
      "Epoch [2/4], Step [41250/68040], Loss: 5.2052\n",
      "Epoch [2/4], Step [41325/68040], Loss: 5.0829\n",
      "Epoch [2/4], Step [41400/68040], Loss: 4.8591\n",
      "Epoch [2/4], Step [41475/68040], Loss: 5.3327\n",
      "Epoch [2/4], Step [41550/68040], Loss: 5.0691\n",
      "Epoch [2/4], Step [41625/68040], Loss: 5.0110\n",
      "Epoch [2/4], Step [41700/68040], Loss: 5.0140\n",
      "Epoch [2/4], Step [41775/68040], Loss: 5.1462\n",
      "Epoch [2/4], Step [41850/68040], Loss: 5.1274\n",
      "Epoch [2/4], Step [41925/68040], Loss: 5.1357\n",
      "Epoch [2/4], Step [42000/68040], Loss: 5.0581\n",
      "Epoch [2/4], Step [42075/68040], Loss: 4.9135\n",
      "Epoch [2/4], Step [42150/68040], Loss: 5.0926\n",
      "Epoch [2/4], Step [42225/68040], Loss: 5.1745\n",
      "Epoch [2/4], Step [42300/68040], Loss: 5.1237\n",
      "Epoch [2/4], Step [42375/68040], Loss: 5.0035\n",
      "Epoch [2/4], Step [42450/68040], Loss: 5.0801\n",
      "Epoch [2/4], Step [42525/68040], Loss: 5.1741\n",
      "Epoch [2/4], Step [42600/68040], Loss: 5.0876\n",
      "Epoch [2/4], Step [42675/68040], Loss: 5.1113\n",
      "Epoch [2/4], Step [42750/68040], Loss: 5.0964\n",
      "Epoch [2/4], Step [42825/68040], Loss: 5.1195\n",
      "Epoch [2/4], Step [42900/68040], Loss: 5.0360\n",
      "Epoch [2/4], Step [42975/68040], Loss: 5.1700\n",
      "Epoch [2/4], Step [43050/68040], Loss: 5.0670\n",
      "Epoch [2/4], Step [43125/68040], Loss: 5.0566\n",
      "Epoch [2/4], Step [43200/68040], Loss: 5.2263\n",
      "Epoch [2/4], Step [43275/68040], Loss: 4.9708\n",
      "Epoch [2/4], Step [43350/68040], Loss: 5.0021\n",
      "Epoch [2/4], Step [43425/68040], Loss: 5.1505\n",
      "Epoch [2/4], Step [43500/68040], Loss: 4.9847\n",
      "Epoch [2/4], Step [43575/68040], Loss: 5.0926\n",
      "Epoch [2/4], Step [43650/68040], Loss: 5.1617\n",
      "Epoch [2/4], Step [43725/68040], Loss: 5.3363\n",
      "Epoch [2/4], Step [43800/68040], Loss: 5.1467\n",
      "Epoch [2/4], Step [43875/68040], Loss: 5.0084\n",
      "Epoch [2/4], Step [43950/68040], Loss: 4.9683\n",
      "Epoch [2/4], Step [44025/68040], Loss: 5.3743\n",
      "Epoch [2/4], Step [44100/68040], Loss: 4.9314\n",
      "Epoch [2/4], Step [44175/68040], Loss: 4.9880\n",
      "Epoch [2/4], Step [44250/68040], Loss: 5.0682\n",
      "Epoch [2/4], Step [44325/68040], Loss: 5.1674\n",
      "Epoch [2/4], Step [44400/68040], Loss: 5.2180\n",
      "Epoch [2/4], Step [44475/68040], Loss: 5.2023\n",
      "Epoch [2/4], Step [44550/68040], Loss: 4.9981\n",
      "Epoch [2/4], Step [44625/68040], Loss: 5.0962\n",
      "Epoch [2/4], Step [44700/68040], Loss: 5.1546\n",
      "Epoch [2/4], Step [44775/68040], Loss: 5.2167\n",
      "Epoch [2/4], Step [44850/68040], Loss: 4.8719\n",
      "Epoch [2/4], Step [44925/68040], Loss: 5.0120\n",
      "Epoch [2/4], Step [45000/68040], Loss: 5.0882\n",
      "Epoch [2/4], Step [45075/68040], Loss: 5.0556\n",
      "Epoch [2/4], Step [45150/68040], Loss: 5.0499\n",
      "Epoch [2/4], Step [45225/68040], Loss: 4.9886\n",
      "Epoch [2/4], Step [45300/68040], Loss: 5.1267\n",
      "Epoch [2/4], Step [45375/68040], Loss: 5.2058\n",
      "Epoch [2/4], Step [45450/68040], Loss: 5.0861\n",
      "Epoch [2/4], Step [45525/68040], Loss: 4.9713\n",
      "Epoch [2/4], Step [45600/68040], Loss: 5.1128\n",
      "Epoch [2/4], Step [45675/68040], Loss: 4.8363\n",
      "Epoch [2/4], Step [45750/68040], Loss: 5.1258\n",
      "Epoch [2/4], Step [45825/68040], Loss: 5.0744\n",
      "Epoch [2/4], Step [45900/68040], Loss: 5.0251\n",
      "Epoch [2/4], Step [45975/68040], Loss: 5.0277\n",
      "Epoch [2/4], Step [46050/68040], Loss: 5.1553\n",
      "Epoch [2/4], Step [46125/68040], Loss: 5.1125\n",
      "Epoch [2/4], Step [46200/68040], Loss: 4.9880\n",
      "Epoch [2/4], Step [46275/68040], Loss: 5.0533\n",
      "Epoch [2/4], Step [46350/68040], Loss: 5.2000\n",
      "Epoch [2/4], Step [46425/68040], Loss: 5.1418\n",
      "Epoch [2/4], Step [46500/68040], Loss: 5.1239\n",
      "Epoch [2/4], Step [46575/68040], Loss: 5.1249\n",
      "Epoch [2/4], Step [46650/68040], Loss: 5.1497\n",
      "Epoch [2/4], Step [46725/68040], Loss: 5.0389\n",
      "Epoch [2/4], Step [46800/68040], Loss: 5.1393\n",
      "Epoch [2/4], Step [46875/68040], Loss: 5.1446\n",
      "Epoch [2/4], Step [46950/68040], Loss: 5.0087\n",
      "Epoch [2/4], Step [47025/68040], Loss: 5.2615\n",
      "Epoch [2/4], Step [47100/68040], Loss: 5.1469\n",
      "Epoch [2/4], Step [47175/68040], Loss: 5.3156\n",
      "Epoch [2/4], Step [47250/68040], Loss: 4.9439\n",
      "Epoch [2/4], Step [47325/68040], Loss: 4.9904\n",
      "Epoch [2/4], Step [47400/68040], Loss: 5.1418\n",
      "Epoch [2/4], Step [47475/68040], Loss: 5.2193\n",
      "Epoch [2/4], Step [47550/68040], Loss: 5.2016\n",
      "Epoch [2/4], Step [47625/68040], Loss: 5.0382\n",
      "Epoch [2/4], Step [47700/68040], Loss: 5.0612\n",
      "Epoch [2/4], Step [47775/68040], Loss: 4.9787\n",
      "Epoch [2/4], Step [47850/68040], Loss: 5.2142\n",
      "Epoch [2/4], Step [47925/68040], Loss: 5.1252\n",
      "Epoch [2/4], Step [48000/68040], Loss: 5.2308\n",
      "Epoch [2/4], Step [48075/68040], Loss: 5.0160\n",
      "Epoch [2/4], Step [48150/68040], Loss: 5.3085\n",
      "Epoch [2/4], Step [48225/68040], Loss: 5.1206\n",
      "Epoch [2/4], Step [48300/68040], Loss: 5.1508\n",
      "Epoch [2/4], Step [48375/68040], Loss: 4.9847\n",
      "Epoch [2/4], Step [48450/68040], Loss: 4.9352\n",
      "Epoch [2/4], Step [48525/68040], Loss: 5.1897\n",
      "Epoch [2/4], Step [48600/68040], Loss: 5.1208\n",
      "Epoch [2/4], Step [48675/68040], Loss: 5.2278\n",
      "Epoch [2/4], Step [48750/68040], Loss: 4.9093\n",
      "Epoch [2/4], Step [48825/68040], Loss: 5.1363\n",
      "Epoch [2/4], Step [48900/68040], Loss: 5.1266\n",
      "Epoch [2/4], Step [48975/68040], Loss: 5.1472\n",
      "Epoch [2/4], Step [49050/68040], Loss: 5.0805\n",
      "Epoch [2/4], Step [49125/68040], Loss: 5.0744\n",
      "Epoch [2/4], Step [49200/68040], Loss: 5.2252\n",
      "Epoch [2/4], Step [49275/68040], Loss: 4.8348\n",
      "Epoch [2/4], Step [49350/68040], Loss: 5.1293\n",
      "Epoch [2/4], Step [49425/68040], Loss: 5.0903\n",
      "Epoch [2/4], Step [49500/68040], Loss: 5.0449\n",
      "Epoch [2/4], Step [49575/68040], Loss: 4.9762\n",
      "Epoch [2/4], Step [49650/68040], Loss: 5.2011\n",
      "Epoch [2/4], Step [49725/68040], Loss: 5.0160\n",
      "Epoch [2/4], Step [49800/68040], Loss: 5.1047\n",
      "Epoch [2/4], Step [49875/68040], Loss: 5.0846\n",
      "Epoch [2/4], Step [49950/68040], Loss: 5.0997\n",
      "Validation perplexity: 141.21457396262153\n",
      "Epoch [2/4], Step [50025/68040], Loss: 4.9853\n",
      "Epoch [2/4], Step [50100/68040], Loss: 4.9289\n",
      "Epoch [2/4], Step [50175/68040], Loss: 5.0560\n",
      "Epoch [2/4], Step [50250/68040], Loss: 5.0836\n",
      "Epoch [2/4], Step [50325/68040], Loss: 5.1748\n",
      "Epoch [2/4], Step [50400/68040], Loss: 5.1473\n",
      "Epoch [2/4], Step [50475/68040], Loss: 4.9612\n",
      "Epoch [2/4], Step [50550/68040], Loss: 5.1904\n",
      "Epoch [2/4], Step [50625/68040], Loss: 4.8773\n",
      "Epoch [2/4], Step [50700/68040], Loss: 5.1342\n",
      "Epoch [2/4], Step [50775/68040], Loss: 5.2858\n",
      "Epoch [2/4], Step [50850/68040], Loss: 5.0547\n",
      "Epoch [2/4], Step [50925/68040], Loss: 5.1318\n",
      "Epoch [2/4], Step [51000/68040], Loss: 4.8928\n",
      "Epoch [2/4], Step [51075/68040], Loss: 4.9542\n",
      "Epoch [2/4], Step [51150/68040], Loss: 5.0924\n",
      "Epoch [2/4], Step [51225/68040], Loss: 4.8395\n",
      "Epoch [2/4], Step [51300/68040], Loss: 5.1841\n",
      "Epoch [2/4], Step [51375/68040], Loss: 5.0833\n",
      "Epoch [2/4], Step [51450/68040], Loss: 5.1684\n",
      "Epoch [2/4], Step [51525/68040], Loss: 5.0408\n",
      "Epoch [2/4], Step [51600/68040], Loss: 5.1722\n",
      "Epoch [2/4], Step [51675/68040], Loss: 5.2094\n",
      "Epoch [2/4], Step [51750/68040], Loss: 5.0680\n",
      "Epoch [2/4], Step [51825/68040], Loss: 5.0454\n",
      "Epoch [2/4], Step [51900/68040], Loss: 5.1933\n",
      "Epoch [2/4], Step [51975/68040], Loss: 5.0999\n",
      "Epoch [2/4], Step [52050/68040], Loss: 5.1092\n",
      "Epoch [2/4], Step [52125/68040], Loss: 5.0648\n",
      "Epoch [2/4], Step [52200/68040], Loss: 5.1854\n",
      "Epoch [2/4], Step [52275/68040], Loss: 5.1503\n",
      "Epoch [2/4], Step [52350/68040], Loss: 5.1316\n",
      "Epoch [2/4], Step [52425/68040], Loss: 5.2042\n",
      "Epoch [2/4], Step [52500/68040], Loss: 5.1061\n",
      "Epoch [2/4], Step [52575/68040], Loss: 5.1062\n",
      "Epoch [2/4], Step [52650/68040], Loss: 5.0734\n",
      "Epoch [2/4], Step [52725/68040], Loss: 5.0843\n",
      "Epoch [2/4], Step [52800/68040], Loss: 5.2130\n",
      "Epoch [2/4], Step [52875/68040], Loss: 5.0153\n",
      "Epoch [2/4], Step [52950/68040], Loss: 4.8926\n",
      "Epoch [2/4], Step [53025/68040], Loss: 4.9212\n",
      "Epoch [2/4], Step [53100/68040], Loss: 5.0009\n",
      "Epoch [2/4], Step [53175/68040], Loss: 5.1228\n",
      "Epoch [2/4], Step [53250/68040], Loss: 5.2108\n",
      "Epoch [2/4], Step [53325/68040], Loss: 5.1668\n",
      "Epoch [2/4], Step [53400/68040], Loss: 5.1610\n",
      "Epoch [2/4], Step [53475/68040], Loss: 5.2276\n",
      "Epoch [2/4], Step [53550/68040], Loss: 5.1789\n",
      "Epoch [2/4], Step [53625/68040], Loss: 5.1325\n",
      "Epoch [2/4], Step [53700/68040], Loss: 5.2104\n",
      "Epoch [2/4], Step [53775/68040], Loss: 5.0255\n",
      "Epoch [2/4], Step [53850/68040], Loss: 5.1594\n",
      "Epoch [2/4], Step [53925/68040], Loss: 5.1513\n",
      "Epoch [2/4], Step [54000/68040], Loss: 5.0490\n",
      "Epoch [2/4], Step [54075/68040], Loss: 5.2414\n",
      "Epoch [2/4], Step [54150/68040], Loss: 5.2710\n",
      "Epoch [2/4], Step [54225/68040], Loss: 5.0706\n",
      "Epoch [2/4], Step [54300/68040], Loss: 5.2131\n",
      "Epoch [2/4], Step [54375/68040], Loss: 5.0439\n",
      "Epoch [2/4], Step [54450/68040], Loss: 5.1727\n",
      "Epoch [2/4], Step [54525/68040], Loss: 5.0104\n",
      "Epoch [2/4], Step [54600/68040], Loss: 5.2182\n",
      "Epoch [2/4], Step [54675/68040], Loss: 5.0851\n",
      "Epoch [2/4], Step [54750/68040], Loss: 5.1129\n",
      "Epoch [2/4], Step [54825/68040], Loss: 5.1135\n",
      "Epoch [2/4], Step [54900/68040], Loss: 4.8988\n",
      "Epoch [2/4], Step [54975/68040], Loss: 5.1880\n",
      "Epoch [2/4], Step [55050/68040], Loss: 5.2130\n",
      "Epoch [2/4], Step [55125/68040], Loss: 4.9495\n",
      "Epoch [2/4], Step [55200/68040], Loss: 5.0948\n",
      "Epoch [2/4], Step [55275/68040], Loss: 5.0491\n",
      "Epoch [2/4], Step [55350/68040], Loss: 4.9686\n",
      "Epoch [2/4], Step [55425/68040], Loss: 5.1078\n",
      "Epoch [2/4], Step [55500/68040], Loss: 5.0424\n",
      "Epoch [2/4], Step [55575/68040], Loss: 5.0007\n",
      "Epoch [2/4], Step [55650/68040], Loss: 5.0912\n",
      "Epoch [2/4], Step [55725/68040], Loss: 5.3418\n",
      "Epoch [2/4], Step [55800/68040], Loss: 5.0064\n",
      "Epoch [2/4], Step [55875/68040], Loss: 5.0115\n",
      "Epoch [2/4], Step [55950/68040], Loss: 5.1197\n",
      "Epoch [2/4], Step [56025/68040], Loss: 5.0083\n",
      "Epoch [2/4], Step [56100/68040], Loss: 5.0513\n",
      "Epoch [2/4], Step [56175/68040], Loss: 5.0708\n",
      "Epoch [2/4], Step [56250/68040], Loss: 5.1408\n",
      "Epoch [2/4], Step [56325/68040], Loss: 5.2565\n",
      "Epoch [2/4], Step [56400/68040], Loss: 5.2002\n",
      "Epoch [2/4], Step [56475/68040], Loss: 5.1590\n",
      "Epoch [2/4], Step [56550/68040], Loss: 5.1361\n",
      "Epoch [2/4], Step [56625/68040], Loss: 5.1850\n",
      "Epoch [2/4], Step [56700/68040], Loss: 4.9963\n",
      "Epoch [2/4], Step [56775/68040], Loss: 5.1643\n",
      "Epoch [2/4], Step [56850/68040], Loss: 5.1937\n",
      "Epoch [2/4], Step [56925/68040], Loss: 5.0420\n",
      "Epoch [2/4], Step [57000/68040], Loss: 5.0642\n",
      "Epoch [2/4], Step [57075/68040], Loss: 5.2048\n",
      "Epoch [2/4], Step [57150/68040], Loss: 5.1350\n",
      "Epoch [2/4], Step [57225/68040], Loss: 4.8093\n",
      "Epoch [2/4], Step [57300/68040], Loss: 5.1825\n",
      "Epoch [2/4], Step [57375/68040], Loss: 5.1977\n",
      "Epoch [2/4], Step [57450/68040], Loss: 5.1950\n",
      "Epoch [2/4], Step [57525/68040], Loss: 5.2630\n",
      "Epoch [2/4], Step [57600/68040], Loss: 4.9300\n",
      "Epoch [2/4], Step [57675/68040], Loss: 4.9854\n",
      "Epoch [2/4], Step [57750/68040], Loss: 5.0065\n",
      "Epoch [2/4], Step [57825/68040], Loss: 5.0700\n",
      "Epoch [2/4], Step [57900/68040], Loss: 5.1622\n",
      "Epoch [2/4], Step [57975/68040], Loss: 5.0081\n",
      "Epoch [2/4], Step [58050/68040], Loss: 5.0507\n",
      "Epoch [2/4], Step [58125/68040], Loss: 5.1467\n",
      "Epoch [2/4], Step [58200/68040], Loss: 5.1780\n",
      "Epoch [2/4], Step [58275/68040], Loss: 5.0239\n",
      "Epoch [2/4], Step [58350/68040], Loss: 5.1020\n",
      "Epoch [2/4], Step [58425/68040], Loss: 5.1506\n",
      "Epoch [2/4], Step [58500/68040], Loss: 5.1554\n",
      "Epoch [2/4], Step [58575/68040], Loss: 5.1159\n",
      "Epoch [2/4], Step [58650/68040], Loss: 5.1828\n",
      "Epoch [2/4], Step [58725/68040], Loss: 5.1566\n",
      "Epoch [2/4], Step [58800/68040], Loss: 5.0636\n",
      "Epoch [2/4], Step [58875/68040], Loss: 4.9693\n",
      "Epoch [2/4], Step [58950/68040], Loss: 4.9093\n",
      "Epoch [2/4], Step [59025/68040], Loss: 4.9898\n",
      "Epoch [2/4], Step [59100/68040], Loss: 5.1069\n",
      "Epoch [2/4], Step [59175/68040], Loss: 5.0998\n",
      "Epoch [2/4], Step [59250/68040], Loss: 5.0992\n",
      "Epoch [2/4], Step [59325/68040], Loss: 5.2338\n",
      "Epoch [2/4], Step [59400/68040], Loss: 5.0413\n",
      "Epoch [2/4], Step [59475/68040], Loss: 5.0002\n",
      "Epoch [2/4], Step [59550/68040], Loss: 4.9895\n",
      "Epoch [2/4], Step [59625/68040], Loss: 5.0861\n",
      "Epoch [2/4], Step [59700/68040], Loss: 5.1623\n",
      "Epoch [2/4], Step [59775/68040], Loss: 5.2952\n",
      "Epoch [2/4], Step [59850/68040], Loss: 5.0002\n",
      "Epoch [2/4], Step [59925/68040], Loss: 5.2870\n",
      "Epoch [2/4], Step [60000/68040], Loss: 5.1582\n",
      "Validation perplexity: 140.0442128828404\n",
      "Epoch [2/4], Step [60075/68040], Loss: 5.0757\n",
      "Epoch [2/4], Step [60150/68040], Loss: 5.0729\n",
      "Epoch [2/4], Step [60225/68040], Loss: 4.9195\n",
      "Epoch [2/4], Step [60300/68040], Loss: 5.2409\n",
      "Epoch [2/4], Step [60375/68040], Loss: 5.1233\n",
      "Epoch [2/4], Step [60450/68040], Loss: 5.0803\n",
      "Epoch [2/4], Step [60525/68040], Loss: 4.9492\n",
      "Epoch [2/4], Step [60600/68040], Loss: 5.2367\n",
      "Epoch [2/4], Step [60675/68040], Loss: 5.0280\n",
      "Epoch [2/4], Step [60750/68040], Loss: 4.9728\n",
      "Epoch [2/4], Step [60825/68040], Loss: 5.1049\n",
      "Epoch [2/4], Step [60900/68040], Loss: 4.8487\n",
      "Epoch [2/4], Step [60975/68040], Loss: 4.9573\n",
      "Epoch [2/4], Step [61050/68040], Loss: 4.9910\n",
      "Epoch [2/4], Step [61125/68040], Loss: 5.1265\n",
      "Epoch [2/4], Step [61200/68040], Loss: 4.9893\n",
      "Epoch [2/4], Step [61275/68040], Loss: 5.1472\n",
      "Epoch [2/4], Step [61350/68040], Loss: 5.0649\n",
      "Epoch [2/4], Step [61425/68040], Loss: 5.0876\n",
      "Epoch [2/4], Step [61500/68040], Loss: 5.0210\n",
      "Epoch [2/4], Step [61575/68040], Loss: 5.1901\n",
      "Epoch [2/4], Step [61650/68040], Loss: 4.9845\n",
      "Epoch [2/4], Step [61725/68040], Loss: 5.1989\n",
      "Epoch [2/4], Step [61800/68040], Loss: 4.8896\n",
      "Epoch [2/4], Step [61875/68040], Loss: 5.0785\n",
      "Epoch [2/4], Step [61950/68040], Loss: 4.9237\n",
      "Epoch [2/4], Step [62025/68040], Loss: 4.8870\n",
      "Epoch [2/4], Step [62100/68040], Loss: 5.0265\n",
      "Epoch [2/4], Step [62175/68040], Loss: 4.9124\n",
      "Epoch [2/4], Step [62250/68040], Loss: 5.0847\n",
      "Epoch [2/4], Step [62325/68040], Loss: 5.0812\n",
      "Epoch [2/4], Step [62400/68040], Loss: 5.0599\n",
      "Epoch [2/4], Step [62475/68040], Loss: 4.9341\n",
      "Epoch [2/4], Step [62550/68040], Loss: 5.1497\n",
      "Epoch [2/4], Step [62625/68040], Loss: 4.9067\n",
      "Epoch [2/4], Step [62700/68040], Loss: 5.0369\n",
      "Epoch [2/4], Step [62775/68040], Loss: 5.1007\n",
      "Epoch [2/4], Step [62850/68040], Loss: 4.9726\n",
      "Epoch [2/4], Step [62925/68040], Loss: 5.1746\n",
      "Epoch [2/4], Step [63000/68040], Loss: 5.2198\n",
      "Epoch [2/4], Step [63075/68040], Loss: 5.0236\n",
      "Epoch [2/4], Step [63150/68040], Loss: 5.0372\n",
      "Epoch [2/4], Step [63225/68040], Loss: 4.8810\n",
      "Epoch [2/4], Step [63300/68040], Loss: 5.1126\n",
      "Epoch [2/4], Step [63375/68040], Loss: 4.9707\n",
      "Epoch [2/4], Step [63450/68040], Loss: 5.0908\n",
      "Epoch [2/4], Step [63525/68040], Loss: 5.1554\n",
      "Epoch [2/4], Step [63600/68040], Loss: 5.2526\n",
      "Epoch [2/4], Step [63675/68040], Loss: 4.9973\n",
      "Epoch [2/4], Step [63750/68040], Loss: 4.9730\n",
      "Epoch [2/4], Step [63825/68040], Loss: 5.0129\n",
      "Epoch [2/4], Step [63900/68040], Loss: 5.0747\n",
      "Epoch [2/4], Step [63975/68040], Loss: 5.1006\n",
      "Epoch [2/4], Step [64050/68040], Loss: 5.0461\n",
      "Epoch [2/4], Step [64125/68040], Loss: 5.0966\n",
      "Epoch [2/4], Step [64200/68040], Loss: 5.1068\n",
      "Epoch [2/4], Step [64275/68040], Loss: 5.0861\n",
      "Epoch [2/4], Step [64350/68040], Loss: 5.2719\n",
      "Epoch [2/4], Step [64425/68040], Loss: 5.1173\n",
      "Epoch [2/4], Step [64500/68040], Loss: 5.1310\n",
      "Epoch [2/4], Step [64575/68040], Loss: 4.9608\n",
      "Epoch [2/4], Step [64650/68040], Loss: 5.2664\n",
      "Epoch [2/4], Step [64725/68040], Loss: 5.0174\n",
      "Epoch [2/4], Step [64800/68040], Loss: 4.9141\n",
      "Epoch [2/4], Step [64875/68040], Loss: 5.1766\n",
      "Epoch [2/4], Step [64950/68040], Loss: 5.2274\n",
      "Epoch [2/4], Step [65025/68040], Loss: 5.1902\n",
      "Epoch [2/4], Step [65100/68040], Loss: 5.0493\n",
      "Epoch [2/4], Step [65175/68040], Loss: 5.1270\n",
      "Epoch [2/4], Step [65250/68040], Loss: 5.0266\n",
      "Epoch [2/4], Step [65325/68040], Loss: 5.1182\n",
      "Epoch [2/4], Step [65400/68040], Loss: 5.0529\n",
      "Epoch [2/4], Step [65475/68040], Loss: 5.1491\n",
      "Epoch [2/4], Step [65550/68040], Loss: 4.8324\n",
      "Epoch [2/4], Step [65625/68040], Loss: 5.1762\n",
      "Epoch [2/4], Step [65700/68040], Loss: 4.9065\n",
      "Epoch [2/4], Step [65775/68040], Loss: 5.1158\n",
      "Epoch [2/4], Step [65850/68040], Loss: 5.2199\n",
      "Epoch [2/4], Step [65925/68040], Loss: 5.0689\n",
      "Epoch [2/4], Step [66000/68040], Loss: 5.1855\n",
      "Epoch [2/4], Step [66075/68040], Loss: 5.0598\n",
      "Epoch [2/4], Step [66150/68040], Loss: 5.1054\n",
      "Epoch [2/4], Step [66225/68040], Loss: 5.0270\n",
      "Epoch [2/4], Step [66300/68040], Loss: 5.1699\n",
      "Epoch [2/4], Step [66375/68040], Loss: 5.1231\n",
      "Epoch [2/4], Step [66450/68040], Loss: 5.0171\n",
      "Epoch [2/4], Step [66525/68040], Loss: 4.9900\n",
      "Epoch [2/4], Step [66600/68040], Loss: 5.1335\n",
      "Epoch [2/4], Step [66675/68040], Loss: 5.0492\n",
      "Epoch [2/4], Step [66750/68040], Loss: 5.0060\n",
      "Epoch [2/4], Step [66825/68040], Loss: 5.1377\n",
      "Epoch [2/4], Step [66900/68040], Loss: 5.0182\n",
      "Epoch [2/4], Step [66975/68040], Loss: 5.1611\n",
      "Epoch [2/4], Step [67050/68040], Loss: 5.0756\n",
      "Epoch [2/4], Step [67125/68040], Loss: 5.1111\n",
      "Epoch [2/4], Step [67200/68040], Loss: 5.1439\n",
      "Epoch [2/4], Step [67275/68040], Loss: 5.0078\n",
      "Epoch [2/4], Step [67350/68040], Loss: 5.1354\n",
      "Epoch [2/4], Step [67425/68040], Loss: 4.9452\n",
      "Epoch [2/4], Step [67500/68040], Loss: 5.1152\n",
      "Epoch [2/4], Step [67575/68040], Loss: 5.0728\n",
      "Epoch [2/4], Step [67650/68040], Loss: 5.2261\n",
      "Epoch [2/4], Step [67725/68040], Loss: 5.0660\n",
      "Epoch [2/4], Step [67800/68040], Loss: 5.0133\n",
      "Epoch [2/4], Step [67875/68040], Loss: 5.1373\n",
      "Epoch [2/4], Step [67950/68040], Loss: 5.1489\n",
      "Epoch [2/4], Step [68025/68040], Loss: 5.1075\n",
      "Epoch [2/4] Average Loss: 5.1163, Perplexity: 166.71\n",
      "Epoch [3/4], Step [0/68040], Loss: 5.1805\n",
      "Validation perplexity: 138.8860227471629\n",
      "Epoch [3/4], Step [75/68040], Loss: 5.0475\n",
      "Epoch [3/4], Step [150/68040], Loss: 4.9419\n",
      "Epoch [3/4], Step [225/68040], Loss: 4.9234\n",
      "Epoch [3/4], Step [300/68040], Loss: 5.1421\n",
      "Epoch [3/4], Step [375/68040], Loss: 5.2762\n",
      "Epoch [3/4], Step [450/68040], Loss: 5.0567\n",
      "Epoch [3/4], Step [525/68040], Loss: 5.3134\n",
      "Epoch [3/4], Step [600/68040], Loss: 5.1790\n",
      "Epoch [3/4], Step [675/68040], Loss: 5.1622\n",
      "Epoch [3/4], Step [750/68040], Loss: 5.0048\n",
      "Epoch [3/4], Step [825/68040], Loss: 5.2867\n",
      "Epoch [3/4], Step [900/68040], Loss: 5.0568\n",
      "Epoch [3/4], Step [975/68040], Loss: 5.0326\n",
      "Epoch [3/4], Step [1050/68040], Loss: 5.0358\n",
      "Epoch [3/4], Step [1125/68040], Loss: 5.1982\n",
      "Epoch [3/4], Step [1200/68040], Loss: 4.9086\n",
      "Epoch [3/4], Step [1275/68040], Loss: 5.0320\n",
      "Epoch [3/4], Step [1350/68040], Loss: 4.9528\n",
      "Epoch [3/4], Step [1425/68040], Loss: 5.0106\n",
      "Epoch [3/4], Step [1500/68040], Loss: 5.0808\n",
      "Epoch [3/4], Step [1575/68040], Loss: 5.0816\n",
      "Epoch [3/4], Step [1650/68040], Loss: 5.0202\n",
      "Epoch [3/4], Step [1725/68040], Loss: 4.9735\n",
      "Epoch [3/4], Step [1800/68040], Loss: 5.1559\n",
      "Epoch [3/4], Step [1875/68040], Loss: 5.1272\n",
      "Epoch [3/4], Step [1950/68040], Loss: 5.0284\n",
      "Epoch [3/4], Step [2025/68040], Loss: 5.1714\n",
      "Epoch [3/4], Step [2100/68040], Loss: 5.2030\n",
      "Epoch [3/4], Step [2175/68040], Loss: 4.8746\n",
      "Epoch [3/4], Step [2250/68040], Loss: 5.1857\n",
      "Epoch [3/4], Step [2325/68040], Loss: 4.8842\n",
      "Epoch [3/4], Step [2400/68040], Loss: 4.8062\n",
      "Epoch [3/4], Step [2475/68040], Loss: 5.0099\n",
      "Epoch [3/4], Step [2550/68040], Loss: 5.1311\n",
      "Epoch [3/4], Step [2625/68040], Loss: 5.0470\n",
      "Epoch [3/4], Step [2700/68040], Loss: 5.0331\n",
      "Epoch [3/4], Step [2775/68040], Loss: 5.1349\n",
      "Epoch [3/4], Step [2850/68040], Loss: 5.0569\n",
      "Epoch [3/4], Step [2925/68040], Loss: 5.0807\n",
      "Epoch [3/4], Step [3000/68040], Loss: 4.9504\n",
      "Epoch [3/4], Step [3075/68040], Loss: 5.0736\n",
      "Epoch [3/4], Step [3150/68040], Loss: 4.9989\n",
      "Epoch [3/4], Step [3225/68040], Loss: 5.2297\n",
      "Epoch [3/4], Step [3300/68040], Loss: 5.0814\n",
      "Epoch [3/4], Step [3375/68040], Loss: 4.9370\n",
      "Epoch [3/4], Step [3450/68040], Loss: 5.0320\n",
      "Epoch [3/4], Step [3525/68040], Loss: 4.9719\n",
      "Epoch [3/4], Step [3600/68040], Loss: 4.8360\n",
      "Epoch [3/4], Step [3675/68040], Loss: 5.1093\n",
      "Epoch [3/4], Step [3750/68040], Loss: 5.0725\n",
      "Epoch [3/4], Step [3825/68040], Loss: 5.0580\n",
      "Epoch [3/4], Step [3900/68040], Loss: 5.1283\n",
      "Epoch [3/4], Step [3975/68040], Loss: 5.0052\n",
      "Epoch [3/4], Step [4050/68040], Loss: 4.9197\n",
      "Epoch [3/4], Step [4125/68040], Loss: 5.1896\n",
      "Epoch [3/4], Step [4200/68040], Loss: 4.9735\n",
      "Epoch [3/4], Step [4275/68040], Loss: 5.0307\n",
      "Epoch [3/4], Step [4350/68040], Loss: 4.9341\n",
      "Epoch [3/4], Step [4425/68040], Loss: 4.9357\n",
      "Epoch [3/4], Step [4500/68040], Loss: 5.0224\n",
      "Epoch [3/4], Step [4575/68040], Loss: 5.2192\n",
      "Epoch [3/4], Step [4650/68040], Loss: 5.1300\n",
      "Epoch [3/4], Step [4725/68040], Loss: 4.9272\n",
      "Epoch [3/4], Step [4800/68040], Loss: 5.1416\n",
      "Epoch [3/4], Step [4875/68040], Loss: 4.9838\n",
      "Epoch [3/4], Step [4950/68040], Loss: 5.0443\n",
      "Epoch [3/4], Step [5025/68040], Loss: 4.9719\n",
      "Epoch [3/4], Step [5100/68040], Loss: 5.0769\n",
      "Epoch [3/4], Step [5175/68040], Loss: 5.1497\n",
      "Epoch [3/4], Step [5250/68040], Loss: 5.2582\n",
      "Epoch [3/4], Step [5325/68040], Loss: 5.0173\n",
      "Epoch [3/4], Step [5400/68040], Loss: 5.1600\n",
      "Epoch [3/4], Step [5475/68040], Loss: 5.1632\n",
      "Epoch [3/4], Step [5550/68040], Loss: 5.0442\n",
      "Epoch [3/4], Step [5625/68040], Loss: 5.2672\n",
      "Epoch [3/4], Step [5700/68040], Loss: 5.1194\n",
      "Epoch [3/4], Step [5775/68040], Loss: 5.1058\n",
      "Epoch [3/4], Step [5850/68040], Loss: 4.9565\n",
      "Epoch [3/4], Step [5925/68040], Loss: 4.9025\n",
      "Epoch [3/4], Step [6000/68040], Loss: 5.0751\n",
      "Epoch [3/4], Step [6075/68040], Loss: 4.9763\n",
      "Epoch [3/4], Step [6150/68040], Loss: 4.9117\n",
      "Epoch [3/4], Step [6225/68040], Loss: 5.0946\n",
      "Epoch [3/4], Step [6300/68040], Loss: 5.0174\n",
      "Epoch [3/4], Step [6375/68040], Loss: 5.0295\n",
      "Epoch [3/4], Step [6450/68040], Loss: 5.1961\n",
      "Epoch [3/4], Step [6525/68040], Loss: 5.0986\n",
      "Epoch [3/4], Step [6600/68040], Loss: 5.1253\n",
      "Epoch [3/4], Step [6675/68040], Loss: 5.0046\n",
      "Epoch [3/4], Step [6750/68040], Loss: 4.9857\n",
      "Epoch [3/4], Step [6825/68040], Loss: 5.0731\n",
      "Epoch [3/4], Step [6900/68040], Loss: 4.7464\n",
      "Epoch [3/4], Step [6975/68040], Loss: 5.1434\n",
      "Epoch [3/4], Step [7050/68040], Loss: 5.1116\n",
      "Epoch [3/4], Step [7125/68040], Loss: 5.0864\n",
      "Epoch [3/4], Step [7200/68040], Loss: 4.9755\n",
      "Epoch [3/4], Step [7275/68040], Loss: 5.0119\n",
      "Epoch [3/4], Step [7350/68040], Loss: 5.0603\n",
      "Epoch [3/4], Step [7425/68040], Loss: 5.0991\n",
      "Epoch [3/4], Step [7500/68040], Loss: 5.2401\n",
      "Epoch [3/4], Step [7575/68040], Loss: 4.9729\n",
      "Epoch [3/4], Step [7650/68040], Loss: 5.1906\n",
      "Epoch [3/4], Step [7725/68040], Loss: 5.0579\n",
      "Epoch [3/4], Step [7800/68040], Loss: 4.9489\n",
      "Epoch [3/4], Step [7875/68040], Loss: 5.1047\n",
      "Epoch [3/4], Step [7950/68040], Loss: 5.1887\n",
      "Epoch [3/4], Step [8025/68040], Loss: 4.9770\n",
      "Epoch [3/4], Step [8100/68040], Loss: 4.9863\n",
      "Epoch [3/4], Step [8175/68040], Loss: 5.0440\n",
      "Epoch [3/4], Step [8250/68040], Loss: 5.1493\n",
      "Epoch [3/4], Step [8325/68040], Loss: 5.0284\n",
      "Epoch [3/4], Step [8400/68040], Loss: 5.1243\n",
      "Epoch [3/4], Step [8475/68040], Loss: 5.0776\n",
      "Epoch [3/4], Step [8550/68040], Loss: 5.0745\n",
      "Epoch [3/4], Step [8625/68040], Loss: 5.1006\n",
      "Epoch [3/4], Step [8700/68040], Loss: 4.9736\n",
      "Epoch [3/4], Step [8775/68040], Loss: 5.1226\n",
      "Epoch [3/4], Step [8850/68040], Loss: 5.0918\n",
      "Epoch [3/4], Step [8925/68040], Loss: 4.9374\n",
      "Epoch [3/4], Step [9000/68040], Loss: 5.0475\n",
      "Epoch [3/4], Step [9075/68040], Loss: 5.0906\n",
      "Epoch [3/4], Step [9150/68040], Loss: 5.0481\n",
      "Epoch [3/4], Step [9225/68040], Loss: 4.8708\n",
      "Epoch [3/4], Step [9300/68040], Loss: 5.2350\n",
      "Epoch [3/4], Step [9375/68040], Loss: 5.0109\n",
      "Epoch [3/4], Step [9450/68040], Loss: 5.0478\n",
      "Epoch [3/4], Step [9525/68040], Loss: 5.0304\n",
      "Epoch [3/4], Step [9600/68040], Loss: 4.9732\n",
      "Epoch [3/4], Step [9675/68040], Loss: 4.8278\n",
      "Epoch [3/4], Step [9750/68040], Loss: 5.0493\n",
      "Epoch [3/4], Step [9825/68040], Loss: 5.2316\n",
      "Epoch [3/4], Step [9900/68040], Loss: 5.0116\n",
      "Epoch [3/4], Step [9975/68040], Loss: 5.0924\n",
      "Validation perplexity: 138.23199243803592\n",
      "Epoch [3/4], Step [10050/68040], Loss: 5.0590\n",
      "Epoch [3/4], Step [10125/68040], Loss: 5.1317\n",
      "Epoch [3/4], Step [10200/68040], Loss: 4.9645\n",
      "Epoch [3/4], Step [10275/68040], Loss: 5.0807\n",
      "Epoch [3/4], Step [10350/68040], Loss: 5.1767\n",
      "Epoch [3/4], Step [10425/68040], Loss: 5.2571\n",
      "Epoch [3/4], Step [10500/68040], Loss: 5.1284\n",
      "Epoch [3/4], Step [10575/68040], Loss: 5.0345\n",
      "Epoch [3/4], Step [10650/68040], Loss: 4.8862\n",
      "Epoch [3/4], Step [10725/68040], Loss: 5.0001\n",
      "Epoch [3/4], Step [10800/68040], Loss: 5.2077\n",
      "Epoch [3/4], Step [10875/68040], Loss: 5.0046\n",
      "Epoch [3/4], Step [10950/68040], Loss: 5.0527\n",
      "Epoch [3/4], Step [11025/68040], Loss: 5.0460\n",
      "Epoch [3/4], Step [11100/68040], Loss: 4.9906\n",
      "Epoch [3/4], Step [11175/68040], Loss: 5.1289\n",
      "Epoch [3/4], Step [11250/68040], Loss: 4.9996\n",
      "Epoch [3/4], Step [11325/68040], Loss: 5.0303\n",
      "Epoch [3/4], Step [11400/68040], Loss: 5.1052\n",
      "Epoch [3/4], Step [11475/68040], Loss: 5.1154\n",
      "Epoch [3/4], Step [11550/68040], Loss: 5.2118\n",
      "Epoch [3/4], Step [11625/68040], Loss: 5.1890\n",
      "Epoch [3/4], Step [11700/68040], Loss: 5.0807\n",
      "Epoch [3/4], Step [11775/68040], Loss: 5.1952\n",
      "Epoch [3/4], Step [11850/68040], Loss: 4.9451\n",
      "Epoch [3/4], Step [11925/68040], Loss: 4.9708\n",
      "Epoch [3/4], Step [12000/68040], Loss: 5.1576\n",
      "Epoch [3/4], Step [12075/68040], Loss: 5.1372\n",
      "Epoch [3/4], Step [12150/68040], Loss: 4.9587\n",
      "Epoch [3/4], Step [12225/68040], Loss: 5.1600\n",
      "Epoch [3/4], Step [12300/68040], Loss: 5.0391\n",
      "Epoch [3/4], Step [12375/68040], Loss: 5.1866\n",
      "Epoch [3/4], Step [12450/68040], Loss: 5.1956\n",
      "Epoch [3/4], Step [12525/68040], Loss: 4.9125\n",
      "Epoch [3/4], Step [12600/68040], Loss: 4.9829\n",
      "Epoch [3/4], Step [12675/68040], Loss: 5.0342\n",
      "Epoch [3/4], Step [12750/68040], Loss: 5.0188\n",
      "Epoch [3/4], Step [12825/68040], Loss: 5.2282\n",
      "Epoch [3/4], Step [12900/68040], Loss: 5.0962\n",
      "Epoch [3/4], Step [12975/68040], Loss: 4.9127\n",
      "Epoch [3/4], Step [13050/68040], Loss: 4.9736\n",
      "Epoch [3/4], Step [13125/68040], Loss: 4.9719\n",
      "Epoch [3/4], Step [13200/68040], Loss: 4.9835\n",
      "Epoch [3/4], Step [13275/68040], Loss: 5.0470\n",
      "Epoch [3/4], Step [13350/68040], Loss: 5.0437\n",
      "Epoch [3/4], Step [13425/68040], Loss: 5.0294\n",
      "Epoch [3/4], Step [13500/68040], Loss: 5.1816\n",
      "Epoch [3/4], Step [13575/68040], Loss: 5.0780\n",
      "Epoch [3/4], Step [13650/68040], Loss: 5.0027\n",
      "Epoch [3/4], Step [13725/68040], Loss: 5.1831\n",
      "Epoch [3/4], Step [13800/68040], Loss: 5.0163\n",
      "Epoch [3/4], Step [13875/68040], Loss: 5.0474\n",
      "Epoch [3/4], Step [13950/68040], Loss: 5.0234\n",
      "Epoch [3/4], Step [14025/68040], Loss: 5.1082\n",
      "Epoch [3/4], Step [14100/68040], Loss: 5.1445\n",
      "Epoch [3/4], Step [14175/68040], Loss: 5.1319\n",
      "Epoch [3/4], Step [14250/68040], Loss: 5.0788\n",
      "Epoch [3/4], Step [14325/68040], Loss: 5.1305\n",
      "Epoch [3/4], Step [14400/68040], Loss: 5.0594\n",
      "Epoch [3/4], Step [14475/68040], Loss: 5.0174\n",
      "Epoch [3/4], Step [14550/68040], Loss: 5.0417\n",
      "Epoch [3/4], Step [14625/68040], Loss: 5.0802\n",
      "Epoch [3/4], Step [14700/68040], Loss: 5.0726\n",
      "Epoch [3/4], Step [14775/68040], Loss: 4.9955\n",
      "Epoch [3/4], Step [14850/68040], Loss: 5.1090\n",
      "Epoch [3/4], Step [14925/68040], Loss: 5.0336\n",
      "Epoch [3/4], Step [15000/68040], Loss: 5.0453\n",
      "Epoch [3/4], Step [15075/68040], Loss: 5.2018\n",
      "Epoch [3/4], Step [15150/68040], Loss: 5.2193\n",
      "Epoch [3/4], Step [15225/68040], Loss: 5.1710\n",
      "Epoch [3/4], Step [15300/68040], Loss: 4.9646\n",
      "Epoch [3/4], Step [15375/68040], Loss: 4.9750\n",
      "Epoch [3/4], Step [15450/68040], Loss: 4.8231\n",
      "Epoch [3/4], Step [15525/68040], Loss: 4.9583\n",
      "Epoch [3/4], Step [15600/68040], Loss: 5.0062\n",
      "Epoch [3/4], Step [15675/68040], Loss: 5.1910\n",
      "Epoch [3/4], Step [15750/68040], Loss: 4.9690\n",
      "Epoch [3/4], Step [15825/68040], Loss: 5.1246\n",
      "Epoch [3/4], Step [15900/68040], Loss: 4.9241\n",
      "Epoch [3/4], Step [15975/68040], Loss: 5.0279\n",
      "Epoch [3/4], Step [16050/68040], Loss: 5.1629\n",
      "Epoch [3/4], Step [16125/68040], Loss: 5.0502\n",
      "Epoch [3/4], Step [16200/68040], Loss: 4.9283\n",
      "Epoch [3/4], Step [16275/68040], Loss: 5.0595\n",
      "Epoch [3/4], Step [16350/68040], Loss: 4.9183\n",
      "Epoch [3/4], Step [16425/68040], Loss: 5.1438\n",
      "Epoch [3/4], Step [16500/68040], Loss: 5.1247\n",
      "Epoch [3/4], Step [16575/68040], Loss: 5.1842\n",
      "Epoch [3/4], Step [16650/68040], Loss: 4.9948\n",
      "Epoch [3/4], Step [16725/68040], Loss: 4.9699\n",
      "Epoch [3/4], Step [16800/68040], Loss: 5.1071\n",
      "Epoch [3/4], Step [16875/68040], Loss: 4.8581\n",
      "Epoch [3/4], Step [16950/68040], Loss: 4.9707\n",
      "Epoch [3/4], Step [17025/68040], Loss: 4.8032\n",
      "Epoch [3/4], Step [17100/68040], Loss: 5.0623\n",
      "Epoch [3/4], Step [17175/68040], Loss: 5.0029\n",
      "Epoch [3/4], Step [17250/68040], Loss: 5.1013\n",
      "Epoch [3/4], Step [17325/68040], Loss: 4.8235\n",
      "Epoch [3/4], Step [17400/68040], Loss: 5.1159\n",
      "Epoch [3/4], Step [17475/68040], Loss: 5.1603\n",
      "Epoch [3/4], Step [17550/68040], Loss: 4.9597\n",
      "Epoch [3/4], Step [17625/68040], Loss: 5.1580\n",
      "Epoch [3/4], Step [17700/68040], Loss: 5.0951\n",
      "Epoch [3/4], Step [17775/68040], Loss: 5.1843\n",
      "Epoch [3/4], Step [17850/68040], Loss: 4.8912\n",
      "Epoch [3/4], Step [17925/68040], Loss: 5.1356\n",
      "Epoch [3/4], Step [18000/68040], Loss: 5.1938\n",
      "Epoch [3/4], Step [18075/68040], Loss: 5.1827\n",
      "Epoch [3/4], Step [18150/68040], Loss: 5.1472\n",
      "Epoch [3/4], Step [18225/68040], Loss: 4.8890\n",
      "Epoch [3/4], Step [18300/68040], Loss: 5.0338\n",
      "Epoch [3/4], Step [18375/68040], Loss: 5.0574\n",
      "Epoch [3/4], Step [18450/68040], Loss: 5.0713\n",
      "Epoch [3/4], Step [18525/68040], Loss: 5.0333\n",
      "Epoch [3/4], Step [18600/68040], Loss: 5.2299\n",
      "Epoch [3/4], Step [18675/68040], Loss: 5.0120\n",
      "Epoch [3/4], Step [18750/68040], Loss: 5.0137\n",
      "Epoch [3/4], Step [18825/68040], Loss: 5.0657\n",
      "Epoch [3/4], Step [18900/68040], Loss: 5.1042\n",
      "Epoch [3/4], Step [18975/68040], Loss: 5.0540\n",
      "Epoch [3/4], Step [19050/68040], Loss: 5.0243\n",
      "Epoch [3/4], Step [19125/68040], Loss: 5.0455\n",
      "Epoch [3/4], Step [19200/68040], Loss: 4.9563\n",
      "Epoch [3/4], Step [19275/68040], Loss: 5.0426\n",
      "Epoch [3/4], Step [19350/68040], Loss: 5.1128\n",
      "Epoch [3/4], Step [19425/68040], Loss: 5.1335\n",
      "Epoch [3/4], Step [19500/68040], Loss: 5.1172\n",
      "Epoch [3/4], Step [19575/68040], Loss: 5.0514\n",
      "Epoch [3/4], Step [19650/68040], Loss: 4.9929\n",
      "Epoch [3/4], Step [19725/68040], Loss: 5.2349\n",
      "Epoch [3/4], Step [19800/68040], Loss: 5.0542\n",
      "Epoch [3/4], Step [19875/68040], Loss: 5.1113\n",
      "Epoch [3/4], Step [19950/68040], Loss: 4.9704\n",
      "Validation perplexity: 137.4137043050209\n",
      "Epoch [3/4], Step [20025/68040], Loss: 5.1194\n",
      "Epoch [3/4], Step [20100/68040], Loss: 5.1939\n",
      "Epoch [3/4], Step [20175/68040], Loss: 5.0150\n",
      "Epoch [3/4], Step [20250/68040], Loss: 5.0332\n",
      "Epoch [3/4], Step [20325/68040], Loss: 5.1639\n",
      "Epoch [3/4], Step [20400/68040], Loss: 5.1322\n",
      "Epoch [3/4], Step [20475/68040], Loss: 5.0656\n",
      "Epoch [3/4], Step [20550/68040], Loss: 5.1510\n",
      "Epoch [3/4], Step [20625/68040], Loss: 4.9772\n",
      "Epoch [3/4], Step [20700/68040], Loss: 4.9841\n",
      "Epoch [3/4], Step [20775/68040], Loss: 4.9986\n",
      "Epoch [3/4], Step [20850/68040], Loss: 4.9131\n",
      "Epoch [3/4], Step [20925/68040], Loss: 5.0757\n",
      "Epoch [3/4], Step [21000/68040], Loss: 4.8729\n",
      "Epoch [3/4], Step [21075/68040], Loss: 5.0487\n",
      "Epoch [3/4], Step [21150/68040], Loss: 5.1700\n",
      "Epoch [3/4], Step [21225/68040], Loss: 5.0067\n",
      "Epoch [3/4], Step [21300/68040], Loss: 5.0470\n",
      "Epoch [3/4], Step [21375/68040], Loss: 4.9276\n",
      "Epoch [3/4], Step [21450/68040], Loss: 4.9928\n",
      "Epoch [3/4], Step [21525/68040], Loss: 4.9469\n",
      "Epoch [3/4], Step [21600/68040], Loss: 5.0162\n",
      "Epoch [3/4], Step [21675/68040], Loss: 4.9624\n",
      "Epoch [3/4], Step [21750/68040], Loss: 5.0252\n",
      "Epoch [3/4], Step [21825/68040], Loss: 5.1200\n",
      "Epoch [3/4], Step [21900/68040], Loss: 4.8049\n",
      "Epoch [3/4], Step [21975/68040], Loss: 5.0470\n",
      "Epoch [3/4], Step [22050/68040], Loss: 4.9268\n",
      "Epoch [3/4], Step [22125/68040], Loss: 5.0443\n",
      "Epoch [3/4], Step [22200/68040], Loss: 5.0802\n",
      "Epoch [3/4], Step [22275/68040], Loss: 4.6827\n",
      "Epoch [3/4], Step [22350/68040], Loss: 4.9437\n",
      "Epoch [3/4], Step [22425/68040], Loss: 5.0599\n",
      "Epoch [3/4], Step [22500/68040], Loss: 5.0661\n",
      "Epoch [3/4], Step [22575/68040], Loss: 4.8734\n",
      "Epoch [3/4], Step [22650/68040], Loss: 4.9058\n",
      "Epoch [3/4], Step [22725/68040], Loss: 4.9271\n",
      "Epoch [3/4], Step [22800/68040], Loss: 5.1662\n",
      "Epoch [3/4], Step [22875/68040], Loss: 5.1490\n",
      "Epoch [3/4], Step [22950/68040], Loss: 4.9755\n",
      "Epoch [3/4], Step [23025/68040], Loss: 5.0689\n",
      "Epoch [3/4], Step [23100/68040], Loss: 5.0130\n",
      "Epoch [3/4], Step [23175/68040], Loss: 5.1002\n",
      "Epoch [3/4], Step [23250/68040], Loss: 5.1194\n",
      "Epoch [3/4], Step [23325/68040], Loss: 5.0814\n",
      "Epoch [3/4], Step [23400/68040], Loss: 5.0793\n",
      "Epoch [3/4], Step [23475/68040], Loss: 4.9956\n",
      "Epoch [3/4], Step [23550/68040], Loss: 5.0403\n",
      "Epoch [3/4], Step [23625/68040], Loss: 4.9838\n",
      "Epoch [3/4], Step [23700/68040], Loss: 5.0919\n",
      "Epoch [3/4], Step [23775/68040], Loss: 5.0517\n",
      "Epoch [3/4], Step [23850/68040], Loss: 5.1134\n",
      "Epoch [3/4], Step [23925/68040], Loss: 5.2363\n",
      "Epoch [3/4], Step [24000/68040], Loss: 5.1643\n",
      "Epoch [3/4], Step [24075/68040], Loss: 4.9841\n",
      "Epoch [3/4], Step [24150/68040], Loss: 5.0986\n",
      "Epoch [3/4], Step [24225/68040], Loss: 4.9151\n",
      "Epoch [3/4], Step [24300/68040], Loss: 5.0088\n",
      "Epoch [3/4], Step [24375/68040], Loss: 4.9887\n",
      "Epoch [3/4], Step [24450/68040], Loss: 4.8543\n",
      "Epoch [3/4], Step [24525/68040], Loss: 5.0107\n",
      "Epoch [3/4], Step [24600/68040], Loss: 5.2337\n",
      "Epoch [3/4], Step [24675/68040], Loss: 5.0870\n",
      "Epoch [3/4], Step [24750/68040], Loss: 5.1062\n",
      "Epoch [3/4], Step [24825/68040], Loss: 5.1045\n",
      "Epoch [3/4], Step [24900/68040], Loss: 4.7864\n",
      "Epoch [3/4], Step [24975/68040], Loss: 5.2451\n",
      "Epoch [3/4], Step [25050/68040], Loss: 5.0069\n",
      "Epoch [3/4], Step [25125/68040], Loss: 4.9525\n",
      "Epoch [3/4], Step [25200/68040], Loss: 4.8830\n",
      "Epoch [3/4], Step [25275/68040], Loss: 4.8689\n",
      "Epoch [3/4], Step [25350/68040], Loss: 5.1374\n",
      "Epoch [3/4], Step [25425/68040], Loss: 4.9163\n",
      "Epoch [3/4], Step [25500/68040], Loss: 5.0249\n",
      "Epoch [3/4], Step [25575/68040], Loss: 4.9881\n",
      "Epoch [3/4], Step [25650/68040], Loss: 5.1717\n",
      "Epoch [3/4], Step [25725/68040], Loss: 5.2378\n",
      "Epoch [3/4], Step [25800/68040], Loss: 5.1538\n",
      "Epoch [3/4], Step [25875/68040], Loss: 5.0926\n",
      "Epoch [3/4], Step [25950/68040], Loss: 5.0146\n",
      "Epoch [3/4], Step [26025/68040], Loss: 4.9255\n",
      "Epoch [3/4], Step [26100/68040], Loss: 4.9740\n",
      "Epoch [3/4], Step [26175/68040], Loss: 5.0985\n",
      "Epoch [3/4], Step [26250/68040], Loss: 5.0924\n",
      "Epoch [3/4], Step [26325/68040], Loss: 4.9123\n",
      "Epoch [3/4], Step [26400/68040], Loss: 5.1102\n",
      "Epoch [3/4], Step [26475/68040], Loss: 5.1069\n",
      "Epoch [3/4], Step [26550/68040], Loss: 5.1584\n",
      "Epoch [3/4], Step [26625/68040], Loss: 5.3474\n",
      "Epoch [3/4], Step [26700/68040], Loss: 5.0671\n",
      "Epoch [3/4], Step [26775/68040], Loss: 4.9592\n",
      "Epoch [3/4], Step [26850/68040], Loss: 5.1588\n",
      "Epoch [3/4], Step [26925/68040], Loss: 5.0633\n",
      "Epoch [3/4], Step [27000/68040], Loss: 5.1262\n",
      "Epoch [3/4], Step [27075/68040], Loss: 5.0643\n",
      "Epoch [3/4], Step [27150/68040], Loss: 5.2150\n",
      "Epoch [3/4], Step [27225/68040], Loss: 4.9656\n",
      "Epoch [3/4], Step [27300/68040], Loss: 4.9595\n",
      "Epoch [3/4], Step [27375/68040], Loss: 5.1249\n",
      "Epoch [3/4], Step [27450/68040], Loss: 5.2123\n",
      "Epoch [3/4], Step [27525/68040], Loss: 5.1579\n",
      "Epoch [3/4], Step [27600/68040], Loss: 4.9721\n",
      "Epoch [3/4], Step [27675/68040], Loss: 5.2294\n",
      "Epoch [3/4], Step [27750/68040], Loss: 5.0532\n",
      "Epoch [3/4], Step [27825/68040], Loss: 5.0943\n",
      "Epoch [3/4], Step [27900/68040], Loss: 5.0485\n",
      "Epoch [3/4], Step [27975/68040], Loss: 5.2203\n",
      "Epoch [3/4], Step [28050/68040], Loss: 4.9969\n",
      "Epoch [3/4], Step [28125/68040], Loss: 5.0576\n",
      "Epoch [3/4], Step [28200/68040], Loss: 4.9214\n",
      "Epoch [3/4], Step [28275/68040], Loss: 5.0197\n",
      "Epoch [3/4], Step [28350/68040], Loss: 5.0035\n",
      "Epoch [3/4], Step [28425/68040], Loss: 5.1484\n",
      "Epoch [3/4], Step [28500/68040], Loss: 5.2391\n",
      "Epoch [3/4], Step [28575/68040], Loss: 5.1155\n",
      "Epoch [3/4], Step [28650/68040], Loss: 4.9193\n",
      "Epoch [3/4], Step [28725/68040], Loss: 5.3098\n",
      "Epoch [3/4], Step [28800/68040], Loss: 5.1335\n",
      "Epoch [3/4], Step [28875/68040], Loss: 5.1677\n",
      "Epoch [3/4], Step [28950/68040], Loss: 5.1180\n",
      "Epoch [3/4], Step [29025/68040], Loss: 5.1784\n",
      "Epoch [3/4], Step [29100/68040], Loss: 5.1299\n",
      "Epoch [3/4], Step [29175/68040], Loss: 4.7476\n",
      "Epoch [3/4], Step [29250/68040], Loss: 4.9974\n",
      "Epoch [3/4], Step [29325/68040], Loss: 4.9526\n",
      "Epoch [3/4], Step [29400/68040], Loss: 5.2097\n",
      "Epoch [3/4], Step [29475/68040], Loss: 4.9617\n",
      "Epoch [3/4], Step [29550/68040], Loss: 5.1014\n",
      "Epoch [3/4], Step [29625/68040], Loss: 5.1656\n",
      "Epoch [3/4], Step [29700/68040], Loss: 5.1594\n",
      "Epoch [3/4], Step [29775/68040], Loss: 4.9811\n",
      "Epoch [3/4], Step [29850/68040], Loss: 5.1305\n",
      "Epoch [3/4], Step [29925/68040], Loss: 5.0745\n",
      "Epoch [3/4], Step [30000/68040], Loss: 5.0984\n",
      "Validation perplexity: 136.64992308393948\n",
      "Epoch [3/4], Step [30075/68040], Loss: 5.1440\n",
      "Epoch [3/4], Step [30150/68040], Loss: 5.0389\n",
      "Epoch [3/4], Step [30225/68040], Loss: 5.0751\n",
      "Epoch [3/4], Step [30300/68040], Loss: 5.1087\n",
      "Epoch [3/4], Step [30375/68040], Loss: 5.0003\n",
      "Epoch [3/4], Step [30450/68040], Loss: 5.0103\n",
      "Epoch [3/4], Step [30525/68040], Loss: 5.0562\n",
      "Epoch [3/4], Step [30600/68040], Loss: 5.0611\n",
      "Epoch [3/4], Step [30675/68040], Loss: 4.9693\n",
      "Epoch [3/4], Step [30750/68040], Loss: 5.0658\n",
      "Epoch [3/4], Step [30825/68040], Loss: 5.1444\n",
      "Epoch [3/4], Step [30900/68040], Loss: 4.9136\n",
      "Epoch [3/4], Step [30975/68040], Loss: 5.1358\n",
      "Epoch [3/4], Step [31050/68040], Loss: 5.0216\n",
      "Epoch [3/4], Step [31125/68040], Loss: 4.8277\n",
      "Epoch [3/4], Step [31200/68040], Loss: 5.0128\n",
      "Epoch [3/4], Step [31275/68040], Loss: 4.9702\n",
      "Epoch [3/4], Step [31350/68040], Loss: 5.0678\n",
      "Epoch [3/4], Step [31425/68040], Loss: 5.0660\n",
      "Epoch [3/4], Step [31500/68040], Loss: 4.8378\n",
      "Epoch [3/4], Step [31575/68040], Loss: 5.1911\n",
      "Epoch [3/4], Step [31650/68040], Loss: 5.0921\n",
      "Epoch [3/4], Step [31725/68040], Loss: 5.1181\n",
      "Epoch [3/4], Step [31800/68040], Loss: 5.0822\n",
      "Epoch [3/4], Step [31875/68040], Loss: 4.9299\n",
      "Epoch [3/4], Step [31950/68040], Loss: 4.9364\n",
      "Epoch [3/4], Step [32025/68040], Loss: 5.0930\n",
      "Epoch [3/4], Step [32100/68040], Loss: 5.0302\n",
      "Epoch [3/4], Step [32175/68040], Loss: 5.1133\n",
      "Epoch [3/4], Step [32250/68040], Loss: 5.1935\n",
      "Epoch [3/4], Step [32325/68040], Loss: 5.0219\n",
      "Epoch [3/4], Step [32400/68040], Loss: 5.1117\n",
      "Epoch [3/4], Step [32475/68040], Loss: 5.1840\n",
      "Epoch [3/4], Step [32550/68040], Loss: 5.0340\n",
      "Epoch [3/4], Step [32625/68040], Loss: 5.1456\n",
      "Epoch [3/4], Step [32700/68040], Loss: 4.9731\n",
      "Epoch [3/4], Step [32775/68040], Loss: 5.0332\n",
      "Epoch [3/4], Step [32850/68040], Loss: 5.0750\n",
      "Epoch [3/4], Step [32925/68040], Loss: 5.0073\n",
      "Epoch [3/4], Step [33000/68040], Loss: 5.1166\n",
      "Epoch [3/4], Step [33075/68040], Loss: 5.2602\n",
      "Epoch [3/4], Step [33150/68040], Loss: 5.0684\n",
      "Epoch [3/4], Step [33225/68040], Loss: 4.9183\n",
      "Epoch [3/4], Step [33300/68040], Loss: 4.9824\n",
      "Epoch [3/4], Step [33375/68040], Loss: 4.9907\n",
      "Epoch [3/4], Step [33450/68040], Loss: 5.1424\n",
      "Epoch [3/4], Step [33525/68040], Loss: 5.1523\n",
      "Epoch [3/4], Step [33600/68040], Loss: 5.1003\n",
      "Epoch [3/4], Step [33675/68040], Loss: 5.1477\n",
      "Epoch [3/4], Step [33750/68040], Loss: 5.1258\n",
      "Epoch [3/4], Step [33825/68040], Loss: 4.9782\n",
      "Epoch [3/4], Step [33900/68040], Loss: 5.1159\n",
      "Epoch [3/4], Step [33975/68040], Loss: 5.1456\n",
      "Epoch [3/4], Step [34050/68040], Loss: 4.9504\n",
      "Epoch [3/4], Step [34125/68040], Loss: 4.9596\n",
      "Epoch [3/4], Step [34200/68040], Loss: 4.9744\n",
      "Epoch [3/4], Step [34275/68040], Loss: 4.9645\n",
      "Epoch [3/4], Step [34350/68040], Loss: 5.0478\n",
      "Epoch [3/4], Step [34425/68040], Loss: 5.0454\n",
      "Epoch [3/4], Step [34500/68040], Loss: 5.1346\n",
      "Epoch [3/4], Step [34575/68040], Loss: 5.1021\n",
      "Epoch [3/4], Step [34650/68040], Loss: 5.2220\n",
      "Epoch [3/4], Step [34725/68040], Loss: 5.1743\n",
      "Epoch [3/4], Step [34800/68040], Loss: 5.2019\n",
      "Epoch [3/4], Step [34875/68040], Loss: 4.9852\n",
      "Epoch [3/4], Step [34950/68040], Loss: 4.9709\n",
      "Epoch [3/4], Step [35025/68040], Loss: 5.1341\n",
      "Epoch [3/4], Step [35100/68040], Loss: 5.1918\n",
      "Epoch [3/4], Step [35175/68040], Loss: 5.1401\n",
      "Epoch [3/4], Step [35250/68040], Loss: 5.0477\n",
      "Epoch [3/4], Step [35325/68040], Loss: 5.0034\n",
      "Epoch [3/4], Step [35400/68040], Loss: 5.2319\n",
      "Epoch [3/4], Step [35475/68040], Loss: 4.8431\n",
      "Epoch [3/4], Step [35550/68040], Loss: 4.9426\n",
      "Epoch [3/4], Step [35625/68040], Loss: 5.0540\n",
      "Epoch [3/4], Step [35700/68040], Loss: 4.9141\n",
      "Epoch [3/4], Step [35775/68040], Loss: 5.0256\n",
      "Epoch [3/4], Step [35850/68040], Loss: 5.0753\n",
      "Epoch [3/4], Step [35925/68040], Loss: 5.1434\n",
      "Epoch [3/4], Step [36000/68040], Loss: 5.1228\n",
      "Epoch [3/4], Step [36075/68040], Loss: 5.0494\n",
      "Epoch [3/4], Step [36150/68040], Loss: 5.1152\n",
      "Epoch [3/4], Step [36225/68040], Loss: 5.1218\n",
      "Epoch [3/4], Step [36300/68040], Loss: 5.0439\n",
      "Epoch [3/4], Step [36375/68040], Loss: 5.2619\n",
      "Epoch [3/4], Step [36450/68040], Loss: 4.9220\n",
      "Epoch [3/4], Step [36525/68040], Loss: 5.0883\n",
      "Epoch [3/4], Step [36600/68040], Loss: 5.0639\n",
      "Epoch [3/4], Step [36675/68040], Loss: 5.1024\n",
      "Epoch [3/4], Step [36750/68040], Loss: 5.2309\n",
      "Epoch [3/4], Step [36825/68040], Loss: 5.0405\n",
      "Epoch [3/4], Step [36900/68040], Loss: 5.1478\n",
      "Epoch [3/4], Step [36975/68040], Loss: 4.9942\n",
      "Epoch [3/4], Step [37050/68040], Loss: 5.1729\n",
      "Epoch [3/4], Step [37125/68040], Loss: 5.0563\n",
      "Epoch [3/4], Step [37200/68040], Loss: 5.1953\n",
      "Epoch [3/4], Step [37275/68040], Loss: 5.0965\n",
      "Epoch [3/4], Step [37350/68040], Loss: 5.0320\n",
      "Epoch [3/4], Step [37425/68040], Loss: 4.9785\n",
      "Epoch [3/4], Step [37500/68040], Loss: 4.9400\n",
      "Epoch [3/4], Step [37575/68040], Loss: 5.0910\n",
      "Epoch [3/4], Step [37650/68040], Loss: 5.0590\n",
      "Epoch [3/4], Step [37725/68040], Loss: 5.2031\n",
      "Epoch [3/4], Step [37800/68040], Loss: 5.1452\n",
      "Epoch [3/4], Step [37875/68040], Loss: 4.9764\n",
      "Epoch [3/4], Step [37950/68040], Loss: 4.9403\n",
      "Epoch [3/4], Step [38025/68040], Loss: 5.0713\n",
      "Epoch [3/4], Step [38100/68040], Loss: 4.8546\n",
      "Epoch [3/4], Step [38175/68040], Loss: 5.1374\n",
      "Epoch [3/4], Step [38250/68040], Loss: 5.1636\n",
      "Epoch [3/4], Step [38325/68040], Loss: 4.9827\n",
      "Epoch [3/4], Step [38400/68040], Loss: 5.0441\n",
      "Epoch [3/4], Step [38475/68040], Loss: 4.8480\n",
      "Epoch [3/4], Step [38550/68040], Loss: 5.1690\n",
      "Epoch [3/4], Step [38625/68040], Loss: 4.9984\n",
      "Epoch [3/4], Step [38700/68040], Loss: 4.9429\n",
      "Epoch [3/4], Step [38775/68040], Loss: 4.9713\n",
      "Epoch [3/4], Step [38850/68040], Loss: 5.1099\n",
      "Epoch [3/4], Step [38925/68040], Loss: 5.1278\n",
      "Epoch [3/4], Step [39000/68040], Loss: 5.2164\n",
      "Epoch [3/4], Step [39075/68040], Loss: 5.0639\n",
      "Epoch [3/4], Step [39150/68040], Loss: 5.0127\n",
      "Epoch [3/4], Step [39225/68040], Loss: 4.9391\n",
      "Epoch [3/4], Step [39300/68040], Loss: 5.0139\n",
      "Epoch [3/4], Step [39375/68040], Loss: 4.9710\n",
      "Epoch [3/4], Step [39450/68040], Loss: 5.0361\n",
      "Epoch [3/4], Step [39525/68040], Loss: 5.1726\n",
      "Epoch [3/4], Step [39600/68040], Loss: 5.0983\n",
      "Epoch [3/4], Step [39675/68040], Loss: 5.1606\n",
      "Epoch [3/4], Step [39750/68040], Loss: 5.0708\n",
      "Epoch [3/4], Step [39825/68040], Loss: 5.0508\n",
      "Epoch [3/4], Step [39900/68040], Loss: 5.0035\n",
      "Epoch [3/4], Step [39975/68040], Loss: 5.0885\n",
      "Validation perplexity: 135.9074596021565\n",
      "Epoch [3/4], Step [40050/68040], Loss: 5.2256\n",
      "Epoch [3/4], Step [40125/68040], Loss: 4.8879\n",
      "Epoch [3/4], Step [40200/68040], Loss: 5.0901\n",
      "Epoch [3/4], Step [40275/68040], Loss: 4.9651\n",
      "Epoch [3/4], Step [40350/68040], Loss: 5.1093\n",
      "Epoch [3/4], Step [40425/68040], Loss: 5.0724\n",
      "Epoch [3/4], Step [40500/68040], Loss: 4.9847\n",
      "Epoch [3/4], Step [40575/68040], Loss: 4.9580\n",
      "Epoch [3/4], Step [40650/68040], Loss: 5.1593\n",
      "Epoch [3/4], Step [40725/68040], Loss: 5.0298\n",
      "Epoch [3/4], Step [40800/68040], Loss: 4.9109\n",
      "Epoch [3/4], Step [40875/68040], Loss: 4.9929\n",
      "Epoch [3/4], Step [40950/68040], Loss: 5.0114\n",
      "Epoch [3/4], Step [41025/68040], Loss: 4.9888\n",
      "Epoch [3/4], Step [41100/68040], Loss: 5.0192\n",
      "Epoch [3/4], Step [41175/68040], Loss: 5.1265\n",
      "Epoch [3/4], Step [41250/68040], Loss: 5.1375\n",
      "Epoch [3/4], Step [41325/68040], Loss: 4.9405\n",
      "Epoch [3/4], Step [41400/68040], Loss: 4.9831\n",
      "Epoch [3/4], Step [41475/68040], Loss: 5.3259\n",
      "Epoch [3/4], Step [41550/68040], Loss: 4.7945\n",
      "Epoch [3/4], Step [41625/68040], Loss: 5.1876\n",
      "Epoch [3/4], Step [41700/68040], Loss: 4.9617\n",
      "Epoch [3/4], Step [41775/68040], Loss: 5.1963\n",
      "Epoch [3/4], Step [41850/68040], Loss: 5.0106\n",
      "Epoch [3/4], Step [41925/68040], Loss: 5.0923\n",
      "Epoch [3/4], Step [42000/68040], Loss: 5.1249\n",
      "Epoch [3/4], Step [42075/68040], Loss: 5.1199\n",
      "Epoch [3/4], Step [42150/68040], Loss: 5.1529\n",
      "Epoch [3/4], Step [42225/68040], Loss: 5.0237\n",
      "Epoch [3/4], Step [42300/68040], Loss: 5.1546\n",
      "Epoch [3/4], Step [42375/68040], Loss: 5.0362\n",
      "Epoch [3/4], Step [42450/68040], Loss: 5.1971\n",
      "Epoch [3/4], Step [42525/68040], Loss: 5.1691\n",
      "Epoch [3/4], Step [42600/68040], Loss: 5.0839\n",
      "Epoch [3/4], Step [42675/68040], Loss: 4.9467\n",
      "Epoch [3/4], Step [42750/68040], Loss: 5.1095\n",
      "Epoch [3/4], Step [42825/68040], Loss: 4.8602\n",
      "Epoch [3/4], Step [42900/68040], Loss: 4.8393\n",
      "Epoch [3/4], Step [42975/68040], Loss: 5.2487\n",
      "Epoch [3/4], Step [43050/68040], Loss: 5.1164\n",
      "Epoch [3/4], Step [43125/68040], Loss: 5.0943\n",
      "Epoch [3/4], Step [43200/68040], Loss: 4.7429\n",
      "Epoch [3/4], Step [43275/68040], Loss: 5.1164\n",
      "Epoch [3/4], Step [43350/68040], Loss: 5.0201\n",
      "Epoch [3/4], Step [43425/68040], Loss: 4.9659\n",
      "Epoch [3/4], Step [43500/68040], Loss: 5.0265\n",
      "Epoch [3/4], Step [43575/68040], Loss: 4.9332\n",
      "Epoch [3/4], Step [43650/68040], Loss: 5.2770\n",
      "Epoch [3/4], Step [43725/68040], Loss: 5.0481\n",
      "Epoch [3/4], Step [43800/68040], Loss: 4.9590\n",
      "Epoch [3/4], Step [43875/68040], Loss: 4.8531\n",
      "Epoch [3/4], Step [43950/68040], Loss: 4.9803\n",
      "Epoch [3/4], Step [44025/68040], Loss: 4.9571\n",
      "Epoch [3/4], Step [44100/68040], Loss: 5.0417\n",
      "Epoch [3/4], Step [44175/68040], Loss: 5.1853\n",
      "Epoch [3/4], Step [44250/68040], Loss: 5.1336\n",
      "Epoch [3/4], Step [44325/68040], Loss: 5.0577\n",
      "Epoch [3/4], Step [44400/68040], Loss: 5.0226\n",
      "Epoch [3/4], Step [44475/68040], Loss: 5.0969\n",
      "Epoch [3/4], Step [44550/68040], Loss: 4.9623\n",
      "Epoch [3/4], Step [44625/68040], Loss: 5.0702\n",
      "Epoch [3/4], Step [44700/68040], Loss: 5.0688\n",
      "Epoch [3/4], Step [44775/68040], Loss: 5.0293\n",
      "Epoch [3/4], Step [44850/68040], Loss: 4.9851\n",
      "Epoch [3/4], Step [44925/68040], Loss: 5.2772\n",
      "Epoch [3/4], Step [45000/68040], Loss: 5.1590\n",
      "Epoch [3/4], Step [45075/68040], Loss: 5.0691\n",
      "Epoch [3/4], Step [45150/68040], Loss: 4.9900\n",
      "Epoch [3/4], Step [45225/68040], Loss: 5.2093\n",
      "Epoch [3/4], Step [45300/68040], Loss: 5.1326\n",
      "Epoch [3/4], Step [45375/68040], Loss: 5.0163\n",
      "Epoch [3/4], Step [45450/68040], Loss: 5.0327\n",
      "Epoch [3/4], Step [45525/68040], Loss: 5.2467\n",
      "Epoch [3/4], Step [45600/68040], Loss: 4.9973\n",
      "Epoch [3/4], Step [45675/68040], Loss: 5.0573\n",
      "Epoch [3/4], Step [45750/68040], Loss: 5.0307\n",
      "Epoch [3/4], Step [45825/68040], Loss: 5.0770\n",
      "Epoch [3/4], Step [45900/68040], Loss: 5.0410\n",
      "Epoch [3/4], Step [45975/68040], Loss: 5.0073\n",
      "Epoch [3/4], Step [46050/68040], Loss: 5.0971\n",
      "Epoch [3/4], Step [46125/68040], Loss: 5.0788\n",
      "Epoch [3/4], Step [46200/68040], Loss: 5.1928\n",
      "Epoch [3/4], Step [46275/68040], Loss: 5.0623\n",
      "Epoch [3/4], Step [46350/68040], Loss: 5.0595\n",
      "Epoch [3/4], Step [46425/68040], Loss: 4.8657\n",
      "Epoch [3/4], Step [46500/68040], Loss: 5.0319\n",
      "Epoch [3/4], Step [46575/68040], Loss: 4.9422\n",
      "Epoch [3/4], Step [46650/68040], Loss: 4.9350\n",
      "Epoch [3/4], Step [46725/68040], Loss: 4.9246\n",
      "Epoch [3/4], Step [46800/68040], Loss: 4.9104\n",
      "Epoch [3/4], Step [46875/68040], Loss: 4.9904\n",
      "Epoch [3/4], Step [46950/68040], Loss: 5.0467\n",
      "Epoch [3/4], Step [47025/68040], Loss: 4.9672\n",
      "Epoch [3/4], Step [47100/68040], Loss: 5.1382\n",
      "Epoch [3/4], Step [47175/68040], Loss: 5.3321\n",
      "Epoch [3/4], Step [47250/68040], Loss: 4.9324\n",
      "Epoch [3/4], Step [47325/68040], Loss: 5.1765\n",
      "Epoch [3/4], Step [47400/68040], Loss: 5.1429\n",
      "Epoch [3/4], Step [47475/68040], Loss: 4.9249\n",
      "Epoch [3/4], Step [47550/68040], Loss: 4.9591\n",
      "Epoch [3/4], Step [47625/68040], Loss: 4.9866\n",
      "Epoch [3/4], Step [47700/68040], Loss: 5.0048\n",
      "Epoch [3/4], Step [47775/68040], Loss: 5.0800\n",
      "Epoch [3/4], Step [47850/68040], Loss: 5.0771\n",
      "Epoch [3/4], Step [47925/68040], Loss: 4.9979\n",
      "Epoch [3/4], Step [48000/68040], Loss: 5.2489\n",
      "Epoch [3/4], Step [48075/68040], Loss: 4.9649\n",
      "Epoch [3/4], Step [48150/68040], Loss: 5.1386\n",
      "Epoch [3/4], Step [48225/68040], Loss: 5.1210\n",
      "Epoch [3/4], Step [48300/68040], Loss: 5.1264\n",
      "Epoch [3/4], Step [48375/68040], Loss: 5.0051\n",
      "Epoch [3/4], Step [48450/68040], Loss: 4.9473\n",
      "Epoch [3/4], Step [48525/68040], Loss: 4.8927\n",
      "Epoch [3/4], Step [48600/68040], Loss: 5.1552\n",
      "Epoch [3/4], Step [48675/68040], Loss: 5.0799\n",
      "Epoch [3/4], Step [48750/68040], Loss: 5.0817\n",
      "Epoch [3/4], Step [48825/68040], Loss: 5.0423\n",
      "Epoch [3/4], Step [48900/68040], Loss: 5.2342\n",
      "Epoch [3/4], Step [48975/68040], Loss: 4.8202\n",
      "Epoch [3/4], Step [49050/68040], Loss: 5.1108\n",
      "Epoch [3/4], Step [49125/68040], Loss: 5.0944\n",
      "Epoch [3/4], Step [49200/68040], Loss: 5.0453\n",
      "Epoch [3/4], Step [49275/68040], Loss: 5.0856\n",
      "Epoch [3/4], Step [49350/68040], Loss: 4.9572\n",
      "Epoch [3/4], Step [49425/68040], Loss: 5.0912\n",
      "Epoch [3/4], Step [49500/68040], Loss: 5.2621\n",
      "Epoch [3/4], Step [49575/68040], Loss: 5.0930\n",
      "Epoch [3/4], Step [49650/68040], Loss: 5.0448\n",
      "Epoch [3/4], Step [49725/68040], Loss: 5.1307\n",
      "Epoch [3/4], Step [49800/68040], Loss: 5.2074\n",
      "Epoch [3/4], Step [49875/68040], Loss: 5.0613\n",
      "Epoch [3/4], Step [49950/68040], Loss: 5.1382\n",
      "Validation perplexity: 135.4813835323257\n",
      "Epoch [3/4], Step [50025/68040], Loss: 5.0306\n",
      "Epoch [3/4], Step [50100/68040], Loss: 4.7791\n",
      "Epoch [3/4], Step [50175/68040], Loss: 5.0333\n",
      "Epoch [3/4], Step [50250/68040], Loss: 5.0131\n",
      "Epoch [3/4], Step [50325/68040], Loss: 5.2155\n",
      "Epoch [3/4], Step [50400/68040], Loss: 5.0345\n",
      "Epoch [3/4], Step [50475/68040], Loss: 4.9913\n",
      "Epoch [3/4], Step [50550/68040], Loss: 5.1291\n",
      "Epoch [3/4], Step [50625/68040], Loss: 4.8947\n",
      "Epoch [3/4], Step [50700/68040], Loss: 5.0481\n",
      "Epoch [3/4], Step [50775/68040], Loss: 5.0870\n",
      "Epoch [3/4], Step [50850/68040], Loss: 5.1767\n",
      "Epoch [3/4], Step [50925/68040], Loss: 5.0537\n",
      "Epoch [3/4], Step [51000/68040], Loss: 5.0835\n",
      "Epoch [3/4], Step [51075/68040], Loss: 5.0122\n",
      "Epoch [3/4], Step [51150/68040], Loss: 5.0294\n",
      "Epoch [3/4], Step [51225/68040], Loss: 5.0563\n",
      "Epoch [3/4], Step [51300/68040], Loss: 5.1039\n",
      "Epoch [3/4], Step [51375/68040], Loss: 4.9869\n",
      "Epoch [3/4], Step [51450/68040], Loss: 5.0573\n",
      "Epoch [3/4], Step [51525/68040], Loss: 5.2174\n",
      "Epoch [3/4], Step [51600/68040], Loss: 5.1243\n",
      "Epoch [3/4], Step [51675/68040], Loss: 5.0127\n",
      "Epoch [3/4], Step [51750/68040], Loss: 5.0788\n",
      "Epoch [3/4], Step [51825/68040], Loss: 5.0795\n",
      "Epoch [3/4], Step [51900/68040], Loss: 5.0935\n",
      "Epoch [3/4], Step [51975/68040], Loss: 4.8920\n",
      "Epoch [3/4], Step [52050/68040], Loss: 4.9363\n",
      "Epoch [3/4], Step [52125/68040], Loss: 4.9683\n",
      "Epoch [3/4], Step [52200/68040], Loss: 4.9522\n",
      "Epoch [3/4], Step [52275/68040], Loss: 5.0089\n",
      "Epoch [3/4], Step [52350/68040], Loss: 4.8413\n",
      "Epoch [3/4], Step [52425/68040], Loss: 5.1639\n",
      "Epoch [3/4], Step [52500/68040], Loss: 5.0315\n",
      "Epoch [3/4], Step [52575/68040], Loss: 5.0840\n",
      "Epoch [3/4], Step [52650/68040], Loss: 5.0113\n",
      "Epoch [3/4], Step [52725/68040], Loss: 5.0415\n",
      "Epoch [3/4], Step [52800/68040], Loss: 5.1403\n",
      "Epoch [3/4], Step [52875/68040], Loss: 5.1534\n",
      "Epoch [3/4], Step [52950/68040], Loss: 5.0936\n",
      "Epoch [3/4], Step [53025/68040], Loss: 5.3349\n",
      "Epoch [3/4], Step [53100/68040], Loss: 5.1633\n",
      "Epoch [3/4], Step [53175/68040], Loss: 5.0170\n",
      "Epoch [3/4], Step [53250/68040], Loss: 4.8004\n",
      "Epoch [3/4], Step [53325/68040], Loss: 5.0299\n",
      "Epoch [3/4], Step [53400/68040], Loss: 4.9547\n",
      "Epoch [3/4], Step [53475/68040], Loss: 5.0396\n",
      "Epoch [3/4], Step [53550/68040], Loss: 5.0631\n",
      "Epoch [3/4], Step [53625/68040], Loss: 5.0563\n",
      "Epoch [3/4], Step [53700/68040], Loss: 5.1376\n",
      "Epoch [3/4], Step [53775/68040], Loss: 5.1581\n",
      "Epoch [3/4], Step [53850/68040], Loss: 5.0832\n",
      "Epoch [3/4], Step [53925/68040], Loss: 4.9264\n",
      "Epoch [3/4], Step [54000/68040], Loss: 5.1300\n",
      "Epoch [3/4], Step [54075/68040], Loss: 5.2571\n",
      "Epoch [3/4], Step [54150/68040], Loss: 5.0708\n",
      "Epoch [3/4], Step [54225/68040], Loss: 5.0954\n",
      "Epoch [3/4], Step [54300/68040], Loss: 5.1154\n",
      "Epoch [3/4], Step [54375/68040], Loss: 4.7893\n",
      "Epoch [3/4], Step [54450/68040], Loss: 4.8655\n",
      "Epoch [3/4], Step [54525/68040], Loss: 5.0890\n",
      "Epoch [3/4], Step [54600/68040], Loss: 5.0450\n",
      "Epoch [3/4], Step [54675/68040], Loss: 5.0832\n",
      "Epoch [3/4], Step [54750/68040], Loss: 5.0421\n",
      "Epoch [3/4], Step [54825/68040], Loss: 5.1321\n",
      "Epoch [3/4], Step [54900/68040], Loss: 5.1613\n",
      "Epoch [3/4], Step [54975/68040], Loss: 5.0672\n",
      "Epoch [3/4], Step [55050/68040], Loss: 5.1011\n",
      "Epoch [3/4], Step [55125/68040], Loss: 5.0983\n",
      "Epoch [3/4], Step [55200/68040], Loss: 4.8538\n",
      "Epoch [3/4], Step [55275/68040], Loss: 4.9402\n",
      "Epoch [3/4], Step [55350/68040], Loss: 4.9529\n",
      "Epoch [3/4], Step [55425/68040], Loss: 5.0295\n",
      "Epoch [3/4], Step [55500/68040], Loss: 5.1028\n",
      "Epoch [3/4], Step [55575/68040], Loss: 4.8538\n",
      "Epoch [3/4], Step [55650/68040], Loss: 5.0600\n",
      "Epoch [3/4], Step [55725/68040], Loss: 5.1179\n",
      "Epoch [3/4], Step [55800/68040], Loss: 5.0235\n",
      "Epoch [3/4], Step [55875/68040], Loss: 4.9112\n",
      "Epoch [3/4], Step [55950/68040], Loss: 5.0124\n",
      "Epoch [3/4], Step [56025/68040], Loss: 4.9061\n",
      "Epoch [3/4], Step [56100/68040], Loss: 4.9957\n",
      "Epoch [3/4], Step [56175/68040], Loss: 5.0944\n",
      "Epoch [3/4], Step [56250/68040], Loss: 5.0320\n",
      "Epoch [3/4], Step [56325/68040], Loss: 5.2069\n",
      "Epoch [3/4], Step [56400/68040], Loss: 5.0453\n",
      "Epoch [3/4], Step [56475/68040], Loss: 4.9766\n",
      "Epoch [3/4], Step [56550/68040], Loss: 5.0443\n",
      "Epoch [3/4], Step [56625/68040], Loss: 5.0737\n",
      "Epoch [3/4], Step [56700/68040], Loss: 5.1367\n",
      "Epoch [3/4], Step [56775/68040], Loss: 5.1960\n",
      "Epoch [3/4], Step [56850/68040], Loss: 5.1242\n",
      "Epoch [3/4], Step [56925/68040], Loss: 5.0221\n",
      "Epoch [3/4], Step [57000/68040], Loss: 5.1540\n",
      "Epoch [3/4], Step [57075/68040], Loss: 5.1742\n",
      "Epoch [3/4], Step [57150/68040], Loss: 4.9820\n",
      "Epoch [3/4], Step [57225/68040], Loss: 5.0812\n",
      "Epoch [3/4], Step [57300/68040], Loss: 4.9697\n",
      "Epoch [3/4], Step [57375/68040], Loss: 4.9569\n",
      "Epoch [3/4], Step [57450/68040], Loss: 5.0964\n",
      "Epoch [3/4], Step [57525/68040], Loss: 5.0233\n",
      "Epoch [3/4], Step [57600/68040], Loss: 4.9102\n",
      "Epoch [3/4], Step [57675/68040], Loss: 4.9668\n",
      "Epoch [3/4], Step [57750/68040], Loss: 5.0591\n",
      "Epoch [3/4], Step [57825/68040], Loss: 5.0369\n",
      "Epoch [3/4], Step [57900/68040], Loss: 5.0847\n",
      "Epoch [3/4], Step [57975/68040], Loss: 4.9327\n",
      "Epoch [3/4], Step [58050/68040], Loss: 5.1331\n",
      "Epoch [3/4], Step [58125/68040], Loss: 4.9018\n",
      "Epoch [3/4], Step [58200/68040], Loss: 5.1488\n",
      "Epoch [3/4], Step [58275/68040], Loss: 4.7799\n",
      "Epoch [3/4], Step [58350/68040], Loss: 5.0947\n",
      "Epoch [3/4], Step [58425/68040], Loss: 5.0718\n",
      "Epoch [3/4], Step [58500/68040], Loss: 4.9680\n",
      "Epoch [3/4], Step [58575/68040], Loss: 5.0980\n",
      "Epoch [3/4], Step [58650/68040], Loss: 4.9892\n",
      "Epoch [3/4], Step [58725/68040], Loss: 4.9957\n",
      "Epoch [3/4], Step [58800/68040], Loss: 4.9341\n",
      "Epoch [3/4], Step [58875/68040], Loss: 5.1898\n",
      "Epoch [3/4], Step [58950/68040], Loss: 5.1633\n",
      "Epoch [3/4], Step [59025/68040], Loss: 5.2037\n",
      "Epoch [3/4], Step [59100/68040], Loss: 5.1487\n",
      "Epoch [3/4], Step [59175/68040], Loss: 4.7972\n",
      "Epoch [3/4], Step [59250/68040], Loss: 5.0990\n",
      "Epoch [3/4], Step [59325/68040], Loss: 5.0374\n",
      "Epoch [3/4], Step [59400/68040], Loss: 5.0487\n",
      "Epoch [3/4], Step [59475/68040], Loss: 5.2217\n",
      "Epoch [3/4], Step [59550/68040], Loss: 5.0952\n",
      "Epoch [3/4], Step [59625/68040], Loss: 4.9683\n",
      "Epoch [3/4], Step [59700/68040], Loss: 4.9844\n",
      "Epoch [3/4], Step [59775/68040], Loss: 5.0278\n",
      "Epoch [3/4], Step [59850/68040], Loss: 5.0963\n",
      "Epoch [3/4], Step [59925/68040], Loss: 5.1489\n",
      "Epoch [3/4], Step [60000/68040], Loss: 5.1603\n",
      "Validation perplexity: 134.92742578500295\n",
      "Epoch [3/4], Step [60075/68040], Loss: 5.0828\n",
      "Epoch [3/4], Step [60150/68040], Loss: 5.1498\n",
      "Epoch [3/4], Step [60225/68040], Loss: 5.0415\n",
      "Epoch [3/4], Step [60300/68040], Loss: 4.9343\n",
      "Epoch [3/4], Step [60375/68040], Loss: 4.8619\n",
      "Epoch [3/4], Step [60450/68040], Loss: 5.1016\n",
      "Epoch [3/4], Step [60525/68040], Loss: 5.0634\n",
      "Epoch [3/4], Step [60600/68040], Loss: 4.9581\n",
      "Epoch [3/4], Step [60675/68040], Loss: 5.0625\n",
      "Epoch [3/4], Step [60750/68040], Loss: 5.0066\n",
      "Epoch [3/4], Step [60825/68040], Loss: 5.2093\n",
      "Epoch [3/4], Step [60900/68040], Loss: 5.1580\n",
      "Epoch [3/4], Step [60975/68040], Loss: 5.2001\n",
      "Epoch [3/4], Step [61050/68040], Loss: 4.8567\n",
      "Epoch [3/4], Step [61125/68040], Loss: 5.0093\n",
      "Epoch [3/4], Step [61200/68040], Loss: 5.0866\n",
      "Epoch [3/4], Step [61275/68040], Loss: 5.2888\n",
      "Epoch [3/4], Step [61350/68040], Loss: 4.9145\n",
      "Epoch [3/4], Step [61425/68040], Loss: 5.0546\n",
      "Epoch [3/4], Step [61500/68040], Loss: 5.0886\n",
      "Epoch [3/4], Step [61575/68040], Loss: 5.0075\n",
      "Epoch [3/4], Step [61650/68040], Loss: 5.0202\n",
      "Epoch [3/4], Step [61725/68040], Loss: 4.8945\n",
      "Epoch [3/4], Step [61800/68040], Loss: 5.0781\n",
      "Epoch [3/4], Step [61875/68040], Loss: 4.9972\n",
      "Epoch [3/4], Step [61950/68040], Loss: 5.1975\n",
      "Epoch [3/4], Step [62025/68040], Loss: 5.1433\n",
      "Epoch [3/4], Step [62100/68040], Loss: 5.0317\n",
      "Epoch [3/4], Step [62175/68040], Loss: 5.0027\n",
      "Epoch [3/4], Step [62250/68040], Loss: 5.1552\n",
      "Epoch [3/4], Step [62325/68040], Loss: 5.1372\n",
      "Epoch [3/4], Step [62400/68040], Loss: 4.9681\n",
      "Epoch [3/4], Step [62475/68040], Loss: 5.2117\n",
      "Epoch [3/4], Step [62550/68040], Loss: 4.8799\n",
      "Epoch [3/4], Step [62625/68040], Loss: 5.0016\n",
      "Epoch [3/4], Step [62700/68040], Loss: 5.2830\n",
      "Epoch [3/4], Step [62775/68040], Loss: 5.0035\n",
      "Epoch [3/4], Step [62850/68040], Loss: 5.0498\n",
      "Epoch [3/4], Step [62925/68040], Loss: 4.9509\n",
      "Epoch [3/4], Step [63000/68040], Loss: 5.2120\n",
      "Epoch [3/4], Step [63075/68040], Loss: 4.9991\n",
      "Epoch [3/4], Step [63150/68040], Loss: 5.0538\n",
      "Epoch [3/4], Step [63225/68040], Loss: 4.9605\n",
      "Epoch [3/4], Step [63300/68040], Loss: 4.8948\n",
      "Epoch [3/4], Step [63375/68040], Loss: 4.9353\n",
      "Epoch [3/4], Step [63450/68040], Loss: 5.0081\n",
      "Epoch [3/4], Step [63525/68040], Loss: 5.1043\n",
      "Epoch [3/4], Step [63600/68040], Loss: 4.8361\n",
      "Epoch [3/4], Step [63675/68040], Loss: 5.0463\n",
      "Epoch [3/4], Step [63750/68040], Loss: 4.9429\n",
      "Epoch [3/4], Step [63825/68040], Loss: 4.9378\n",
      "Epoch [3/4], Step [63900/68040], Loss: 5.1062\n",
      "Epoch [3/4], Step [63975/68040], Loss: 5.0941\n",
      "Epoch [3/4], Step [64050/68040], Loss: 5.0808\n",
      "Epoch [3/4], Step [64125/68040], Loss: 4.9058\n",
      "Epoch [3/4], Step [64200/68040], Loss: 4.9114\n",
      "Epoch [3/4], Step [64275/68040], Loss: 5.1562\n",
      "Epoch [3/4], Step [64350/68040], Loss: 5.1398\n",
      "Epoch [3/4], Step [64425/68040], Loss: 4.9876\n",
      "Epoch [3/4], Step [64500/68040], Loss: 5.0359\n",
      "Epoch [3/4], Step [64575/68040], Loss: 5.1100\n",
      "Epoch [3/4], Step [64650/68040], Loss: 5.1779\n",
      "Epoch [3/4], Step [64725/68040], Loss: 5.0408\n",
      "Epoch [3/4], Step [64800/68040], Loss: 4.9266\n",
      "Epoch [3/4], Step [64875/68040], Loss: 5.0540\n",
      "Epoch [3/4], Step [64950/68040], Loss: 5.1165\n",
      "Epoch [3/4], Step [65025/68040], Loss: 5.1606\n",
      "Epoch [3/4], Step [65100/68040], Loss: 4.9985\n",
      "Epoch [3/4], Step [65175/68040], Loss: 5.0130\n",
      "Epoch [3/4], Step [65250/68040], Loss: 5.0969\n",
      "Epoch [3/4], Step [65325/68040], Loss: 5.0975\n",
      "Epoch [3/4], Step [65400/68040], Loss: 4.9395\n",
      "Epoch [3/4], Step [65475/68040], Loss: 5.1392\n",
      "Epoch [3/4], Step [65550/68040], Loss: 5.0038\n",
      "Epoch [3/4], Step [65625/68040], Loss: 5.0901\n",
      "Epoch [3/4], Step [65700/68040], Loss: 4.8804\n",
      "Epoch [3/4], Step [65775/68040], Loss: 4.9689\n",
      "Epoch [3/4], Step [65850/68040], Loss: 5.0686\n",
      "Epoch [3/4], Step [65925/68040], Loss: 5.1184\n",
      "Epoch [3/4], Step [66000/68040], Loss: 5.0889\n",
      "Epoch [3/4], Step [66075/68040], Loss: 5.1932\n",
      "Epoch [3/4], Step [66150/68040], Loss: 4.9381\n",
      "Epoch [3/4], Step [66225/68040], Loss: 5.0520\n",
      "Epoch [3/4], Step [66300/68040], Loss: 5.0195\n",
      "Epoch [3/4], Step [66375/68040], Loss: 5.0198\n",
      "Epoch [3/4], Step [66450/68040], Loss: 4.9625\n",
      "Epoch [3/4], Step [66525/68040], Loss: 5.0201\n",
      "Epoch [3/4], Step [66600/68040], Loss: 4.9850\n",
      "Epoch [3/4], Step [66675/68040], Loss: 5.1160\n",
      "Epoch [3/4], Step [66750/68040], Loss: 5.0594\n",
      "Epoch [3/4], Step [66825/68040], Loss: 5.2262\n",
      "Epoch [3/4], Step [66900/68040], Loss: 5.0057\n",
      "Epoch [3/4], Step [66975/68040], Loss: 4.9667\n",
      "Epoch [3/4], Step [67050/68040], Loss: 5.1293\n",
      "Epoch [3/4], Step [67125/68040], Loss: 4.9651\n",
      "Epoch [3/4], Step [67200/68040], Loss: 5.1833\n",
      "Epoch [3/4], Step [67275/68040], Loss: 5.0936\n",
      "Epoch [3/4], Step [67350/68040], Loss: 5.0543\n",
      "Epoch [3/4], Step [67425/68040], Loss: 4.9648\n",
      "Epoch [3/4], Step [67500/68040], Loss: 4.9170\n",
      "Epoch [3/4], Step [67575/68040], Loss: 5.0449\n",
      "Epoch [3/4], Step [67650/68040], Loss: 5.0790\n",
      "Epoch [3/4], Step [67725/68040], Loss: 5.0973\n",
      "Epoch [3/4], Step [67800/68040], Loss: 5.0789\n",
      "Epoch [3/4], Step [67875/68040], Loss: 4.9127\n",
      "Epoch [3/4], Step [67950/68040], Loss: 5.0722\n",
      "Epoch [3/4], Step [68025/68040], Loss: 5.0530\n",
      "Epoch [3/4] Average Loss: 5.0617, Perplexity: 157.86\n",
      "Epoch [4/4], Step [0/68040], Loss: 4.9373\n",
      "Validation perplexity: 134.49583949287714\n",
      "Epoch [4/4], Step [75/68040], Loss: 4.8957\n",
      "Epoch [4/4], Step [150/68040], Loss: 5.0602\n",
      "Epoch [4/4], Step [225/68040], Loss: 4.9903\n",
      "Epoch [4/4], Step [300/68040], Loss: 5.1092\n",
      "Epoch [4/4], Step [375/68040], Loss: 4.9055\n",
      "Epoch [4/4], Step [450/68040], Loss: 5.1033\n",
      "Epoch [4/4], Step [525/68040], Loss: 5.0880\n",
      "Epoch [4/4], Step [600/68040], Loss: 4.8987\n",
      "Epoch [4/4], Step [675/68040], Loss: 5.0308\n",
      "Epoch [4/4], Step [750/68040], Loss: 4.8138\n",
      "Epoch [4/4], Step [825/68040], Loss: 5.1422\n",
      "Epoch [4/4], Step [900/68040], Loss: 5.0729\n",
      "Epoch [4/4], Step [975/68040], Loss: 4.9328\n",
      "Epoch [4/4], Step [1050/68040], Loss: 5.0262\n",
      "Epoch [4/4], Step [1125/68040], Loss: 4.9721\n",
      "Epoch [4/4], Step [1200/68040], Loss: 4.9374\n",
      "Epoch [4/4], Step [1275/68040], Loss: 5.0747\n",
      "Epoch [4/4], Step [1350/68040], Loss: 5.0856\n",
      "Epoch [4/4], Step [1425/68040], Loss: 5.0599\n",
      "Epoch [4/4], Step [1500/68040], Loss: 5.0224\n",
      "Epoch [4/4], Step [1575/68040], Loss: 5.1615\n",
      "Epoch [4/4], Step [1650/68040], Loss: 5.0554\n",
      "Epoch [4/4], Step [1725/68040], Loss: 4.7743\n",
      "Epoch [4/4], Step [1800/68040], Loss: 5.0834\n",
      "Epoch [4/4], Step [1875/68040], Loss: 5.0313\n",
      "Epoch [4/4], Step [1950/68040], Loss: 5.1037\n",
      "Epoch [4/4], Step [2025/68040], Loss: 4.8330\n",
      "Epoch [4/4], Step [2100/68040], Loss: 4.9493\n",
      "Epoch [4/4], Step [2175/68040], Loss: 5.1179\n",
      "Epoch [4/4], Step [2250/68040], Loss: 5.0445\n",
      "Epoch [4/4], Step [2325/68040], Loss: 4.9447\n",
      "Epoch [4/4], Step [2400/68040], Loss: 5.2802\n",
      "Epoch [4/4], Step [2475/68040], Loss: 5.1308\n",
      "Epoch [4/4], Step [2550/68040], Loss: 5.0491\n",
      "Epoch [4/4], Step [2625/68040], Loss: 4.9509\n",
      "Epoch [4/4], Step [2700/68040], Loss: 4.9420\n",
      "Epoch [4/4], Step [2775/68040], Loss: 4.9624\n",
      "Epoch [4/4], Step [2850/68040], Loss: 4.9330\n",
      "Epoch [4/4], Step [2925/68040], Loss: 4.9563\n",
      "Epoch [4/4], Step [3000/68040], Loss: 5.0217\n",
      "Epoch [4/4], Step [3075/68040], Loss: 5.0303\n",
      "Epoch [4/4], Step [3150/68040], Loss: 5.0593\n",
      "Epoch [4/4], Step [3225/68040], Loss: 4.9763\n",
      "Epoch [4/4], Step [3300/68040], Loss: 5.0566\n",
      "Epoch [4/4], Step [3375/68040], Loss: 4.8113\n",
      "Epoch [4/4], Step [3450/68040], Loss: 5.1338\n",
      "Epoch [4/4], Step [3525/68040], Loss: 5.0215\n",
      "Epoch [4/4], Step [3600/68040], Loss: 5.0144\n",
      "Epoch [4/4], Step [3675/68040], Loss: 5.1082\n",
      "Epoch [4/4], Step [3750/68040], Loss: 5.0245\n",
      "Epoch [4/4], Step [3825/68040], Loss: 4.9335\n",
      "Epoch [4/4], Step [3900/68040], Loss: 5.0672\n",
      "Epoch [4/4], Step [3975/68040], Loss: 5.0564\n",
      "Epoch [4/4], Step [4050/68040], Loss: 5.0231\n",
      "Epoch [4/4], Step [4125/68040], Loss: 5.0109\n",
      "Epoch [4/4], Step [4200/68040], Loss: 5.0464\n",
      "Epoch [4/4], Step [4275/68040], Loss: 5.2669\n",
      "Epoch [4/4], Step [4350/68040], Loss: 5.0606\n",
      "Epoch [4/4], Step [4425/68040], Loss: 5.1122\n",
      "Epoch [4/4], Step [4500/68040], Loss: 5.1716\n",
      "Epoch [4/4], Step [4575/68040], Loss: 4.9255\n",
      "Epoch [4/4], Step [4650/68040], Loss: 5.0986\n",
      "Epoch [4/4], Step [4725/68040], Loss: 5.0497\n",
      "Epoch [4/4], Step [4800/68040], Loss: 4.9618\n",
      "Epoch [4/4], Step [4875/68040], Loss: 5.2181\n",
      "Epoch [4/4], Step [4950/68040], Loss: 5.1754\n",
      "Epoch [4/4], Step [5025/68040], Loss: 5.0889\n",
      "Epoch [4/4], Step [5100/68040], Loss: 5.0804\n",
      "Epoch [4/4], Step [5175/68040], Loss: 5.0942\n",
      "Epoch [4/4], Step [5250/68040], Loss: 5.0632\n",
      "Epoch [4/4], Step [5325/68040], Loss: 5.0017\n",
      "Epoch [4/4], Step [5400/68040], Loss: 5.0715\n",
      "Epoch [4/4], Step [5475/68040], Loss: 5.1578\n",
      "Epoch [4/4], Step [5550/68040], Loss: 4.8827\n",
      "Epoch [4/4], Step [5625/68040], Loss: 5.0987\n",
      "Epoch [4/4], Step [5700/68040], Loss: 5.0206\n",
      "Epoch [4/4], Step [5775/68040], Loss: 4.9510\n",
      "Epoch [4/4], Step [5850/68040], Loss: 4.9437\n",
      "Epoch [4/4], Step [5925/68040], Loss: 4.9612\n",
      "Epoch [4/4], Step [6000/68040], Loss: 5.0191\n",
      "Epoch [4/4], Step [6075/68040], Loss: 5.2216\n",
      "Epoch [4/4], Step [6150/68040], Loss: 5.0189\n",
      "Epoch [4/4], Step [6225/68040], Loss: 4.9862\n",
      "Epoch [4/4], Step [6300/68040], Loss: 5.2505\n",
      "Epoch [4/4], Step [6375/68040], Loss: 5.1743\n",
      "Epoch [4/4], Step [6450/68040], Loss: 5.0594\n",
      "Epoch [4/4], Step [6525/68040], Loss: 4.9659\n",
      "Epoch [4/4], Step [6600/68040], Loss: 5.1202\n",
      "Epoch [4/4], Step [6675/68040], Loss: 4.9718\n",
      "Epoch [4/4], Step [6750/68040], Loss: 4.9490\n",
      "Epoch [4/4], Step [6825/68040], Loss: 5.1133\n",
      "Epoch [4/4], Step [6900/68040], Loss: 4.9802\n",
      "Epoch [4/4], Step [6975/68040], Loss: 5.0780\n",
      "Epoch [4/4], Step [7050/68040], Loss: 5.0055\n",
      "Epoch [4/4], Step [7125/68040], Loss: 4.8818\n",
      "Epoch [4/4], Step [7200/68040], Loss: 5.2660\n",
      "Epoch [4/4], Step [7275/68040], Loss: 5.0053\n",
      "Epoch [4/4], Step [7350/68040], Loss: 4.9964\n",
      "Epoch [4/4], Step [7425/68040], Loss: 5.1864\n",
      "Epoch [4/4], Step [7500/68040], Loss: 5.2055\n",
      "Epoch [4/4], Step [7575/68040], Loss: 5.0191\n",
      "Epoch [4/4], Step [7650/68040], Loss: 4.8096\n",
      "Epoch [4/4], Step [7725/68040], Loss: 5.1683\n",
      "Epoch [4/4], Step [7800/68040], Loss: 5.1942\n",
      "Epoch [4/4], Step [7875/68040], Loss: 4.9397\n",
      "Epoch [4/4], Step [7950/68040], Loss: 5.2636\n",
      "Epoch [4/4], Step [8025/68040], Loss: 5.0427\n",
      "Epoch [4/4], Step [8100/68040], Loss: 5.0805\n",
      "Epoch [4/4], Step [8175/68040], Loss: 5.2082\n",
      "Epoch [4/4], Step [8250/68040], Loss: 5.0417\n",
      "Epoch [4/4], Step [8325/68040], Loss: 4.9883\n",
      "Epoch [4/4], Step [8400/68040], Loss: 4.9233\n",
      "Epoch [4/4], Step [8475/68040], Loss: 4.9987\n",
      "Epoch [4/4], Step [8550/68040], Loss: 5.1437\n",
      "Epoch [4/4], Step [8625/68040], Loss: 4.9478\n",
      "Epoch [4/4], Step [8700/68040], Loss: 5.1164\n",
      "Epoch [4/4], Step [8775/68040], Loss: 5.0920\n",
      "Epoch [4/4], Step [8850/68040], Loss: 5.1060\n",
      "Epoch [4/4], Step [8925/68040], Loss: 5.0289\n",
      "Epoch [4/4], Step [9000/68040], Loss: 5.1208\n",
      "Epoch [4/4], Step [9075/68040], Loss: 5.1229\n",
      "Epoch [4/4], Step [9150/68040], Loss: 5.1000\n",
      "Epoch [4/4], Step [9225/68040], Loss: 5.1462\n",
      "Epoch [4/4], Step [9300/68040], Loss: 4.9044\n",
      "Epoch [4/4], Step [9375/68040], Loss: 5.1709\n",
      "Epoch [4/4], Step [9450/68040], Loss: 4.9080\n",
      "Epoch [4/4], Step [9525/68040], Loss: 5.1771\n",
      "Epoch [4/4], Step [9600/68040], Loss: 4.8923\n",
      "Epoch [4/4], Step [9675/68040], Loss: 4.8371\n",
      "Epoch [4/4], Step [9750/68040], Loss: 4.9125\n",
      "Epoch [4/4], Step [9825/68040], Loss: 4.9941\n",
      "Epoch [4/4], Step [9900/68040], Loss: 5.0666\n",
      "Epoch [4/4], Step [9975/68040], Loss: 5.1037\n",
      "Validation perplexity: 134.31415210019634\n",
      "Epoch [4/4], Step [10050/68040], Loss: 5.1595\n",
      "Epoch [4/4], Step [10125/68040], Loss: 5.0112\n",
      "Epoch [4/4], Step [10200/68040], Loss: 5.0976\n",
      "Epoch [4/4], Step [10275/68040], Loss: 5.1014\n",
      "Epoch [4/4], Step [10350/68040], Loss: 5.0101\n",
      "Epoch [4/4], Step [10425/68040], Loss: 5.0362\n",
      "Epoch [4/4], Step [10500/68040], Loss: 4.8565\n",
      "Epoch [4/4], Step [10575/68040], Loss: 5.0559\n",
      "Epoch [4/4], Step [10650/68040], Loss: 5.3816\n",
      "Epoch [4/4], Step [10725/68040], Loss: 5.0958\n",
      "Epoch [4/4], Step [10800/68040], Loss: 4.9010\n",
      "Epoch [4/4], Step [10875/68040], Loss: 5.0661\n",
      "Epoch [4/4], Step [10950/68040], Loss: 4.9614\n",
      "Epoch [4/4], Step [11025/68040], Loss: 4.9783\n",
      "Epoch [4/4], Step [11100/68040], Loss: 5.0511\n",
      "Epoch [4/4], Step [11175/68040], Loss: 5.0724\n",
      "Epoch [4/4], Step [11250/68040], Loss: 5.0513\n",
      "Epoch [4/4], Step [11325/68040], Loss: 4.9864\n",
      "Epoch [4/4], Step [11400/68040], Loss: 5.0661\n",
      "Epoch [4/4], Step [11475/68040], Loss: 5.0452\n",
      "Epoch [4/4], Step [11550/68040], Loss: 5.1302\n",
      "Epoch [4/4], Step [11625/68040], Loss: 4.9073\n",
      "Epoch [4/4], Step [11700/68040], Loss: 4.9767\n",
      "Epoch [4/4], Step [11775/68040], Loss: 4.8947\n",
      "Epoch [4/4], Step [11850/68040], Loss: 4.9446\n",
      "Epoch [4/4], Step [11925/68040], Loss: 5.0374\n",
      "Epoch [4/4], Step [12000/68040], Loss: 4.8766\n",
      "Epoch [4/4], Step [12075/68040], Loss: 5.1011\n",
      "Epoch [4/4], Step [12150/68040], Loss: 5.1066\n",
      "Epoch [4/4], Step [12225/68040], Loss: 5.1019\n",
      "Epoch [4/4], Step [12300/68040], Loss: 5.0166\n",
      "Epoch [4/4], Step [12375/68040], Loss: 5.1134\n",
      "Epoch [4/4], Step [12450/68040], Loss: 4.8983\n",
      "Epoch [4/4], Step [12525/68040], Loss: 5.1406\n",
      "Epoch [4/4], Step [12600/68040], Loss: 5.0536\n",
      "Epoch [4/4], Step [12675/68040], Loss: 5.0262\n",
      "Epoch [4/4], Step [12750/68040], Loss: 5.0112\n",
      "Epoch [4/4], Step [12825/68040], Loss: 5.1596\n",
      "Epoch [4/4], Step [12900/68040], Loss: 4.9174\n",
      "Epoch [4/4], Step [12975/68040], Loss: 5.0462\n",
      "Epoch [4/4], Step [13050/68040], Loss: 5.0358\n",
      "Epoch [4/4], Step [13125/68040], Loss: 5.0108\n",
      "Epoch [4/4], Step [13200/68040], Loss: 5.0880\n",
      "Epoch [4/4], Step [13275/68040], Loss: 4.9635\n",
      "Epoch [4/4], Step [13350/68040], Loss: 5.0961\n",
      "Epoch [4/4], Step [13425/68040], Loss: 5.0094\n",
      "Epoch [4/4], Step [13500/68040], Loss: 4.9673\n",
      "Epoch [4/4], Step [13575/68040], Loss: 5.0390\n",
      "Epoch [4/4], Step [13650/68040], Loss: 4.9724\n",
      "Epoch [4/4], Step [13725/68040], Loss: 5.1461\n",
      "Epoch [4/4], Step [13800/68040], Loss: 5.0479\n",
      "Epoch [4/4], Step [13875/68040], Loss: 5.0266\n",
      "Epoch [4/4], Step [13950/68040], Loss: 4.9346\n",
      "Epoch [4/4], Step [14025/68040], Loss: 5.0379\n",
      "Epoch [4/4], Step [14100/68040], Loss: 5.0243\n",
      "Epoch [4/4], Step [14175/68040], Loss: 5.1224\n",
      "Epoch [4/4], Step [14250/68040], Loss: 4.9959\n",
      "Epoch [4/4], Step [14325/68040], Loss: 5.0164\n",
      "Epoch [4/4], Step [14400/68040], Loss: 5.0614\n",
      "Epoch [4/4], Step [14475/68040], Loss: 5.1610\n",
      "Epoch [4/4], Step [14550/68040], Loss: 5.1595\n",
      "Epoch [4/4], Step [14625/68040], Loss: 5.0528\n",
      "Epoch [4/4], Step [14700/68040], Loss: 5.1349\n",
      "Epoch [4/4], Step [14775/68040], Loss: 4.9493\n",
      "Epoch [4/4], Step [14850/68040], Loss: 5.0128\n",
      "Epoch [4/4], Step [14925/68040], Loss: 4.9987\n",
      "Epoch [4/4], Step [15000/68040], Loss: 5.1210\n",
      "Epoch [4/4], Step [15075/68040], Loss: 5.1136\n",
      "Epoch [4/4], Step [15150/68040], Loss: 5.1046\n",
      "Epoch [4/4], Step [15225/68040], Loss: 5.1601\n",
      "Epoch [4/4], Step [15300/68040], Loss: 4.8221\n",
      "Epoch [4/4], Step [15375/68040], Loss: 5.0162\n",
      "Epoch [4/4], Step [15450/68040], Loss: 4.8901\n",
      "Epoch [4/4], Step [15525/68040], Loss: 5.1092\n",
      "Epoch [4/4], Step [15600/68040], Loss: 5.0271\n",
      "Epoch [4/4], Step [15675/68040], Loss: 4.9134\n",
      "Epoch [4/4], Step [15750/68040], Loss: 5.0418\n",
      "Epoch [4/4], Step [15825/68040], Loss: 5.0811\n",
      "Epoch [4/4], Step [15900/68040], Loss: 5.0805\n",
      "Epoch [4/4], Step [15975/68040], Loss: 4.9272\n",
      "Epoch [4/4], Step [16050/68040], Loss: 5.2520\n",
      "Epoch [4/4], Step [16125/68040], Loss: 4.9989\n",
      "Epoch [4/4], Step [16200/68040], Loss: 5.0889\n",
      "Epoch [4/4], Step [16275/68040], Loss: 5.0089\n",
      "Epoch [4/4], Step [16350/68040], Loss: 5.1121\n",
      "Epoch [4/4], Step [16425/68040], Loss: 4.9376\n",
      "Epoch [4/4], Step [16500/68040], Loss: 4.9705\n",
      "Epoch [4/4], Step [16575/68040], Loss: 5.3300\n",
      "Epoch [4/4], Step [16650/68040], Loss: 5.0148\n",
      "Epoch [4/4], Step [16725/68040], Loss: 4.9907\n",
      "Epoch [4/4], Step [16800/68040], Loss: 4.9398\n",
      "Epoch [4/4], Step [16875/68040], Loss: 4.9512\n",
      "Epoch [4/4], Step [16950/68040], Loss: 5.0228\n",
      "Epoch [4/4], Step [17025/68040], Loss: 5.0794\n",
      "Epoch [4/4], Step [17100/68040], Loss: 5.2303\n",
      "Epoch [4/4], Step [17175/68040], Loss: 5.1041\n",
      "Epoch [4/4], Step [17250/68040], Loss: 5.1069\n",
      "Epoch [4/4], Step [17325/68040], Loss: 5.1631\n",
      "Epoch [4/4], Step [17400/68040], Loss: 4.9798\n",
      "Epoch [4/4], Step [17475/68040], Loss: 5.0430\n",
      "Epoch [4/4], Step [17550/68040], Loss: 4.9915\n",
      "Epoch [4/4], Step [17625/68040], Loss: 4.8966\n",
      "Epoch [4/4], Step [17700/68040], Loss: 5.1411\n",
      "Epoch [4/4], Step [17775/68040], Loss: 5.0349\n",
      "Epoch [4/4], Step [17850/68040], Loss: 5.1273\n",
      "Epoch [4/4], Step [17925/68040], Loss: 4.9400\n",
      "Epoch [4/4], Step [18000/68040], Loss: 4.7710\n",
      "Epoch [4/4], Step [18075/68040], Loss: 5.1389\n",
      "Epoch [4/4], Step [18150/68040], Loss: 4.9759\n",
      "Epoch [4/4], Step [18225/68040], Loss: 5.0774\n",
      "Epoch [4/4], Step [18300/68040], Loss: 4.8455\n",
      "Epoch [4/4], Step [18375/68040], Loss: 4.9509\n",
      "Epoch [4/4], Step [18450/68040], Loss: 5.0623\n",
      "Epoch [4/4], Step [18525/68040], Loss: 4.9751\n",
      "Epoch [4/4], Step [18600/68040], Loss: 4.8241\n",
      "Epoch [4/4], Step [18675/68040], Loss: 5.0902\n",
      "Epoch [4/4], Step [18750/68040], Loss: 4.8946\n",
      "Epoch [4/4], Step [18825/68040], Loss: 5.1536\n",
      "Epoch [4/4], Step [18900/68040], Loss: 5.0166\n",
      "Epoch [4/4], Step [18975/68040], Loss: 4.9726\n",
      "Epoch [4/4], Step [19050/68040], Loss: 5.0091\n",
      "Epoch [4/4], Step [19125/68040], Loss: 4.9699\n",
      "Epoch [4/4], Step [19200/68040], Loss: 5.0409\n",
      "Epoch [4/4], Step [19275/68040], Loss: 5.1451\n",
      "Epoch [4/4], Step [19350/68040], Loss: 5.0010\n",
      "Epoch [4/4], Step [19425/68040], Loss: 5.0025\n",
      "Epoch [4/4], Step [19500/68040], Loss: 5.0090\n",
      "Epoch [4/4], Step [19575/68040], Loss: 4.9500\n",
      "Epoch [4/4], Step [19650/68040], Loss: 5.1466\n",
      "Epoch [4/4], Step [19725/68040], Loss: 5.0161\n",
      "Epoch [4/4], Step [19800/68040], Loss: 5.0995\n",
      "Epoch [4/4], Step [19875/68040], Loss: 5.0025\n",
      "Epoch [4/4], Step [19950/68040], Loss: 4.9591\n",
      "Validation perplexity: 133.75060765610664\n",
      "Epoch [4/4], Step [20025/68040], Loss: 5.1797\n",
      "Epoch [4/4], Step [20100/68040], Loss: 5.1302\n",
      "Epoch [4/4], Step [20175/68040], Loss: 5.0679\n",
      "Epoch [4/4], Step [20250/68040], Loss: 5.1267\n",
      "Epoch [4/4], Step [20325/68040], Loss: 4.9611\n",
      "Epoch [4/4], Step [20400/68040], Loss: 4.9842\n",
      "Epoch [4/4], Step [20475/68040], Loss: 4.9872\n",
      "Epoch [4/4], Step [20550/68040], Loss: 5.0306\n",
      "Epoch [4/4], Step [20625/68040], Loss: 5.1017\n",
      "Epoch [4/4], Step [20700/68040], Loss: 5.0417\n",
      "Epoch [4/4], Step [20775/68040], Loss: 5.0189\n",
      "Epoch [4/4], Step [20850/68040], Loss: 5.0173\n",
      "Epoch [4/4], Step [20925/68040], Loss: 5.0237\n",
      "Epoch [4/4], Step [21000/68040], Loss: 5.1373\n",
      "Epoch [4/4], Step [21075/68040], Loss: 4.7821\n",
      "Epoch [4/4], Step [21150/68040], Loss: 4.9620\n",
      "Epoch [4/4], Step [21225/68040], Loss: 4.9801\n",
      "Epoch [4/4], Step [21300/68040], Loss: 5.0159\n",
      "Epoch [4/4], Step [21375/68040], Loss: 4.9326\n",
      "Epoch [4/4], Step [21450/68040], Loss: 4.9253\n",
      "Epoch [4/4], Step [21525/68040], Loss: 5.0336\n",
      "Epoch [4/4], Step [21600/68040], Loss: 5.0990\n",
      "Epoch [4/4], Step [21675/68040], Loss: 4.9697\n",
      "Epoch [4/4], Step [21750/68040], Loss: 4.9159\n",
      "Epoch [4/4], Step [21825/68040], Loss: 5.1010\n",
      "Epoch [4/4], Step [21900/68040], Loss: 4.9722\n",
      "Epoch [4/4], Step [21975/68040], Loss: 4.9956\n",
      "Epoch [4/4], Step [22050/68040], Loss: 4.9038\n",
      "Epoch [4/4], Step [22125/68040], Loss: 4.9839\n",
      "Epoch [4/4], Step [22200/68040], Loss: 5.0531\n",
      "Epoch [4/4], Step [22275/68040], Loss: 5.0763\n",
      "Epoch [4/4], Step [22350/68040], Loss: 5.1029\n",
      "Epoch [4/4], Step [22425/68040], Loss: 5.1904\n",
      "Epoch [4/4], Step [22500/68040], Loss: 5.2916\n",
      "Epoch [4/4], Step [22575/68040], Loss: 5.0555\n",
      "Epoch [4/4], Step [22650/68040], Loss: 4.9670\n",
      "Epoch [4/4], Step [22725/68040], Loss: 5.2464\n",
      "Epoch [4/4], Step [22800/68040], Loss: 5.0273\n",
      "Epoch [4/4], Step [22875/68040], Loss: 4.9916\n",
      "Epoch [4/4], Step [22950/68040], Loss: 5.1010\n",
      "Epoch [4/4], Step [23025/68040], Loss: 4.9941\n",
      "Epoch [4/4], Step [23100/68040], Loss: 5.1236\n",
      "Epoch [4/4], Step [23175/68040], Loss: 5.1685\n",
      "Epoch [4/4], Step [23250/68040], Loss: 5.1355\n",
      "Epoch [4/4], Step [23325/68040], Loss: 5.1328\n",
      "Epoch [4/4], Step [23400/68040], Loss: 5.1498\n",
      "Epoch [4/4], Step [23475/68040], Loss: 4.8991\n",
      "Epoch [4/4], Step [23550/68040], Loss: 5.1075\n",
      "Epoch [4/4], Step [23625/68040], Loss: 5.0755\n",
      "Epoch [4/4], Step [23700/68040], Loss: 5.0251\n",
      "Epoch [4/4], Step [23775/68040], Loss: 5.0697\n",
      "Epoch [4/4], Step [23850/68040], Loss: 5.0778\n",
      "Epoch [4/4], Step [23925/68040], Loss: 5.2663\n",
      "Epoch [4/4], Step [24000/68040], Loss: 4.8698\n",
      "Epoch [4/4], Step [24075/68040], Loss: 5.0236\n",
      "Epoch [4/4], Step [24150/68040], Loss: 5.0889\n",
      "Epoch [4/4], Step [24225/68040], Loss: 5.2606\n",
      "Epoch [4/4], Step [24300/68040], Loss: 5.1179\n",
      "Epoch [4/4], Step [24375/68040], Loss: 5.0861\n",
      "Epoch [4/4], Step [24450/68040], Loss: 5.0295\n",
      "Epoch [4/4], Step [24525/68040], Loss: 5.0678\n",
      "Epoch [4/4], Step [24600/68040], Loss: 4.9158\n",
      "Epoch [4/4], Step [24675/68040], Loss: 5.0459\n",
      "Epoch [4/4], Step [24750/68040], Loss: 5.1778\n",
      "Epoch [4/4], Step [24825/68040], Loss: 4.9680\n",
      "Epoch [4/4], Step [24900/68040], Loss: 5.1129\n",
      "Epoch [4/4], Step [24975/68040], Loss: 5.1237\n",
      "Epoch [4/4], Step [25050/68040], Loss: 4.9433\n",
      "Epoch [4/4], Step [25125/68040], Loss: 4.8899\n",
      "Epoch [4/4], Step [25200/68040], Loss: 5.1904\n",
      "Epoch [4/4], Step [25275/68040], Loss: 5.1314\n",
      "Epoch [4/4], Step [25350/68040], Loss: 5.0847\n",
      "Epoch [4/4], Step [25425/68040], Loss: 4.9758\n",
      "Epoch [4/4], Step [25500/68040], Loss: 4.9928\n",
      "Epoch [4/4], Step [25575/68040], Loss: 5.0385\n",
      "Epoch [4/4], Step [25650/68040], Loss: 5.1490\n",
      "Epoch [4/4], Step [25725/68040], Loss: 5.1723\n",
      "Epoch [4/4], Step [25800/68040], Loss: 5.1189\n",
      "Epoch [4/4], Step [25875/68040], Loss: 5.0962\n",
      "Epoch [4/4], Step [25950/68040], Loss: 5.0944\n",
      "Epoch [4/4], Step [26025/68040], Loss: 5.0157\n",
      "Epoch [4/4], Step [26100/68040], Loss: 5.1695\n",
      "Epoch [4/4], Step [26175/68040], Loss: 5.1082\n",
      "Epoch [4/4], Step [26250/68040], Loss: 5.0681\n",
      "Epoch [4/4], Step [26325/68040], Loss: 4.9287\n",
      "Epoch [4/4], Step [26400/68040], Loss: 4.9116\n",
      "Epoch [4/4], Step [26475/68040], Loss: 5.0429\n",
      "Epoch [4/4], Step [26550/68040], Loss: 4.9612\n",
      "Epoch [4/4], Step [26625/68040], Loss: 5.1020\n",
      "Epoch [4/4], Step [26700/68040], Loss: 5.1628\n",
      "Epoch [4/4], Step [26775/68040], Loss: 5.0204\n",
      "Epoch [4/4], Step [26850/68040], Loss: 4.9601\n",
      "Epoch [4/4], Step [26925/68040], Loss: 5.0651\n",
      "Epoch [4/4], Step [27000/68040], Loss: 5.0959\n",
      "Epoch [4/4], Step [27075/68040], Loss: 5.0943\n",
      "Epoch [4/4], Step [27150/68040], Loss: 4.8361\n",
      "Epoch [4/4], Step [27225/68040], Loss: 5.0547\n",
      "Epoch [4/4], Step [27300/68040], Loss: 5.0765\n",
      "Epoch [4/4], Step [27375/68040], Loss: 5.0737\n",
      "Epoch [4/4], Step [27450/68040], Loss: 5.0317\n",
      "Epoch [4/4], Step [27525/68040], Loss: 4.8419\n",
      "Epoch [4/4], Step [27600/68040], Loss: 5.0119\n",
      "Epoch [4/4], Step [27675/68040], Loss: 5.0396\n",
      "Epoch [4/4], Step [27750/68040], Loss: 5.1123\n",
      "Epoch [4/4], Step [27825/68040], Loss: 5.0832\n",
      "Epoch [4/4], Step [27900/68040], Loss: 4.9816\n",
      "Epoch [4/4], Step [27975/68040], Loss: 5.0103\n",
      "Epoch [4/4], Step [28050/68040], Loss: 5.1203\n",
      "Epoch [4/4], Step [28125/68040], Loss: 4.8707\n",
      "Epoch [4/4], Step [28200/68040], Loss: 4.9378\n",
      "Epoch [4/4], Step [28275/68040], Loss: 4.8429\n",
      "Epoch [4/4], Step [28350/68040], Loss: 5.0997\n",
      "Epoch [4/4], Step [28425/68040], Loss: 5.0961\n",
      "Epoch [4/4], Step [28500/68040], Loss: 5.0230\n",
      "Epoch [4/4], Step [28575/68040], Loss: 5.1361\n",
      "Epoch [4/4], Step [28650/68040], Loss: 5.0786\n",
      "Epoch [4/4], Step [28725/68040], Loss: 4.8927\n",
      "Epoch [4/4], Step [28800/68040], Loss: 5.1196\n",
      "Epoch [4/4], Step [28875/68040], Loss: 4.9740\n",
      "Epoch [4/4], Step [28950/68040], Loss: 4.9430\n",
      "Epoch [4/4], Step [29025/68040], Loss: 5.0835\n",
      "Epoch [4/4], Step [29100/68040], Loss: 5.0577\n",
      "Epoch [4/4], Step [29175/68040], Loss: 5.1268\n",
      "Epoch [4/4], Step [29250/68040], Loss: 5.1261\n",
      "Epoch [4/4], Step [29325/68040], Loss: 5.0654\n",
      "Epoch [4/4], Step [29400/68040], Loss: 5.0994\n",
      "Epoch [4/4], Step [29475/68040], Loss: 5.1176\n",
      "Epoch [4/4], Step [29550/68040], Loss: 4.9207\n",
      "Epoch [4/4], Step [29625/68040], Loss: 5.0554\n",
      "Epoch [4/4], Step [29700/68040], Loss: 5.1233\n",
      "Epoch [4/4], Step [29775/68040], Loss: 5.1738\n",
      "Epoch [4/4], Step [29850/68040], Loss: 4.9929\n",
      "Epoch [4/4], Step [29925/68040], Loss: 5.1414\n",
      "Epoch [4/4], Step [30000/68040], Loss: 5.0602\n",
      "Validation perplexity: 133.6059566207922\n",
      "Epoch [4/4], Step [30075/68040], Loss: 4.9184\n",
      "Epoch [4/4], Step [30150/68040], Loss: 4.9956\n",
      "Epoch [4/4], Step [30225/68040], Loss: 5.1053\n",
      "Epoch [4/4], Step [30300/68040], Loss: 4.9780\n",
      "Epoch [4/4], Step [30375/68040], Loss: 4.9296\n",
      "Epoch [4/4], Step [30450/68040], Loss: 4.9892\n",
      "Epoch [4/4], Step [30525/68040], Loss: 5.0009\n",
      "Epoch [4/4], Step [30600/68040], Loss: 5.0952\n",
      "Epoch [4/4], Step [30675/68040], Loss: 5.1128\n",
      "Epoch [4/4], Step [30750/68040], Loss: 5.0579\n",
      "Epoch [4/4], Step [30825/68040], Loss: 5.1633\n",
      "Epoch [4/4], Step [30900/68040], Loss: 5.0113\n",
      "Epoch [4/4], Step [30975/68040], Loss: 5.1269\n",
      "Epoch [4/4], Step [31050/68040], Loss: 5.1046\n",
      "Epoch [4/4], Step [31125/68040], Loss: 5.0060\n",
      "Epoch [4/4], Step [31200/68040], Loss: 4.9334\n",
      "Epoch [4/4], Step [31275/68040], Loss: 5.0173\n",
      "Epoch [4/4], Step [31350/68040], Loss: 5.0621\n",
      "Epoch [4/4], Step [31425/68040], Loss: 5.1360\n",
      "Epoch [4/4], Step [31500/68040], Loss: 4.9767\n",
      "Epoch [4/4], Step [31575/68040], Loss: 5.0105\n",
      "Epoch [4/4], Step [31650/68040], Loss: 4.9515\n",
      "Epoch [4/4], Step [31725/68040], Loss: 5.0658\n",
      "Epoch [4/4], Step [31800/68040], Loss: 5.0039\n",
      "Epoch [4/4], Step [31875/68040], Loss: 5.0709\n",
      "Epoch [4/4], Step [31950/68040], Loss: 4.9318\n",
      "Epoch [4/4], Step [32025/68040], Loss: 5.2374\n",
      "Epoch [4/4], Step [32100/68040], Loss: 4.9876\n",
      "Epoch [4/4], Step [32175/68040], Loss: 4.8805\n",
      "Epoch [4/4], Step [32250/68040], Loss: 4.9764\n",
      "Epoch [4/4], Step [32325/68040], Loss: 5.0935\n",
      "Epoch [4/4], Step [32400/68040], Loss: 5.0081\n",
      "Epoch [4/4], Step [32475/68040], Loss: 4.9747\n",
      "Epoch [4/4], Step [32550/68040], Loss: 5.1478\n",
      "Epoch [4/4], Step [32625/68040], Loss: 5.0853\n",
      "Epoch [4/4], Step [32700/68040], Loss: 4.9796\n",
      "Epoch [4/4], Step [32775/68040], Loss: 5.1014\n",
      "Epoch [4/4], Step [32850/68040], Loss: 5.0357\n",
      "Epoch [4/4], Step [32925/68040], Loss: 4.9059\n",
      "Epoch [4/4], Step [33000/68040], Loss: 4.9476\n",
      "Epoch [4/4], Step [33075/68040], Loss: 4.9888\n",
      "Epoch [4/4], Step [33150/68040], Loss: 5.0229\n",
      "Epoch [4/4], Step [33225/68040], Loss: 4.9460\n",
      "Epoch [4/4], Step [33300/68040], Loss: 5.0210\n",
      "Epoch [4/4], Step [33375/68040], Loss: 5.0124\n",
      "Epoch [4/4], Step [33450/68040], Loss: 5.0866\n",
      "Epoch [4/4], Step [33525/68040], Loss: 5.1711\n",
      "Epoch [4/4], Step [33600/68040], Loss: 5.1055\n",
      "Epoch [4/4], Step [33675/68040], Loss: 5.0336\n",
      "Epoch [4/4], Step [33750/68040], Loss: 5.0223\n",
      "Epoch [4/4], Step [33825/68040], Loss: 4.8778\n",
      "Epoch [4/4], Step [33900/68040], Loss: 5.0763\n",
      "Epoch [4/4], Step [33975/68040], Loss: 4.9403\n",
      "Epoch [4/4], Step [34050/68040], Loss: 5.0045\n",
      "Epoch [4/4], Step [34125/68040], Loss: 4.9700\n",
      "Epoch [4/4], Step [34200/68040], Loss: 5.1148\n",
      "Epoch [4/4], Step [34275/68040], Loss: 5.0867\n",
      "Epoch [4/4], Step [34350/68040], Loss: 5.1655\n",
      "Epoch [4/4], Step [34425/68040], Loss: 5.1907\n",
      "Epoch [4/4], Step [34500/68040], Loss: 5.0110\n",
      "Epoch [4/4], Step [34575/68040], Loss: 5.0230\n",
      "Epoch [4/4], Step [34650/68040], Loss: 5.0029\n",
      "Epoch [4/4], Step [34725/68040], Loss: 5.0184\n",
      "Epoch [4/4], Step [34800/68040], Loss: 5.1414\n",
      "Epoch [4/4], Step [34875/68040], Loss: 5.0056\n",
      "Epoch [4/4], Step [34950/68040], Loss: 5.0257\n",
      "Epoch [4/4], Step [35025/68040], Loss: 4.9978\n",
      "Epoch [4/4], Step [35100/68040], Loss: 4.9326\n",
      "Epoch [4/4], Step [35175/68040], Loss: 5.1606\n",
      "Epoch [4/4], Step [35250/68040], Loss: 5.0209\n",
      "Epoch [4/4], Step [35325/68040], Loss: 5.0412\n",
      "Epoch [4/4], Step [35400/68040], Loss: 5.0021\n",
      "Epoch [4/4], Step [35475/68040], Loss: 5.0065\n",
      "Epoch [4/4], Step [35550/68040], Loss: 5.1602\n",
      "Epoch [4/4], Step [35625/68040], Loss: 5.0832\n",
      "Epoch [4/4], Step [35700/68040], Loss: 4.9343\n",
      "Epoch [4/4], Step [35775/68040], Loss: 5.1224\n",
      "Epoch [4/4], Step [35850/68040], Loss: 5.0435\n",
      "Epoch [4/4], Step [35925/68040], Loss: 5.0814\n",
      "Epoch [4/4], Step [36000/68040], Loss: 4.9336\n",
      "Epoch [4/4], Step [36075/68040], Loss: 5.0899\n",
      "Epoch [4/4], Step [36150/68040], Loss: 5.0361\n",
      "Epoch [4/4], Step [36225/68040], Loss: 4.9152\n",
      "Epoch [4/4], Step [36300/68040], Loss: 4.8976\n",
      "Epoch [4/4], Step [36375/68040], Loss: 5.1288\n",
      "Epoch [4/4], Step [36450/68040], Loss: 5.1661\n",
      "Epoch [4/4], Step [36525/68040], Loss: 4.9861\n",
      "Epoch [4/4], Step [36600/68040], Loss: 5.1469\n",
      "Epoch [4/4], Step [36675/68040], Loss: 4.9777\n",
      "Epoch [4/4], Step [36750/68040], Loss: 5.1437\n",
      "Epoch [4/4], Step [36825/68040], Loss: 5.1700\n",
      "Epoch [4/4], Step [36900/68040], Loss: 5.2335\n",
      "Epoch [4/4], Step [36975/68040], Loss: 5.0904\n",
      "Epoch [4/4], Step [37050/68040], Loss: 5.1322\n",
      "Epoch [4/4], Step [37125/68040], Loss: 5.1453\n",
      "Epoch [4/4], Step [37200/68040], Loss: 4.8257\n",
      "Epoch [4/4], Step [37275/68040], Loss: 5.1325\n",
      "Epoch [4/4], Step [37350/68040], Loss: 5.1721\n",
      "Epoch [4/4], Step [37425/68040], Loss: 5.1283\n",
      "Epoch [4/4], Step [37500/68040], Loss: 5.0190\n",
      "Epoch [4/4], Step [37575/68040], Loss: 5.2371\n",
      "Epoch [4/4], Step [37650/68040], Loss: 5.1105\n",
      "Epoch [4/4], Step [37725/68040], Loss: 5.0835\n",
      "Epoch [4/4], Step [37800/68040], Loss: 4.8688\n",
      "Epoch [4/4], Step [37875/68040], Loss: 5.0649\n",
      "Epoch [4/4], Step [37950/68040], Loss: 5.0505\n",
      "Epoch [4/4], Step [38025/68040], Loss: 5.0767\n",
      "Epoch [4/4], Step [38100/68040], Loss: 5.0472\n",
      "Epoch [4/4], Step [38175/68040], Loss: 4.9556\n",
      "Epoch [4/4], Step [38250/68040], Loss: 4.9853\n",
      "Epoch [4/4], Step [38325/68040], Loss: 5.1539\n",
      "Epoch [4/4], Step [38400/68040], Loss: 5.0108\n",
      "Epoch [4/4], Step [38475/68040], Loss: 5.1146\n",
      "Epoch [4/4], Step [38550/68040], Loss: 5.0863\n",
      "Epoch [4/4], Step [38625/68040], Loss: 5.0186\n",
      "Epoch [4/4], Step [38700/68040], Loss: 5.1566\n",
      "Epoch [4/4], Step [38775/68040], Loss: 5.0280\n",
      "Epoch [4/4], Step [38850/68040], Loss: 5.0794\n",
      "Epoch [4/4], Step [38925/68040], Loss: 4.9464\n",
      "Epoch [4/4], Step [39000/68040], Loss: 5.0846\n",
      "Epoch [4/4], Step [39075/68040], Loss: 5.1027\n",
      "Epoch [4/4], Step [39150/68040], Loss: 5.0638\n",
      "Epoch [4/4], Step [39225/68040], Loss: 5.1422\n",
      "Epoch [4/4], Step [39300/68040], Loss: 5.1069\n",
      "Epoch [4/4], Step [39375/68040], Loss: 5.2487\n",
      "Epoch [4/4], Step [39450/68040], Loss: 4.9835\n",
      "Epoch [4/4], Step [39525/68040], Loss: 5.0114\n",
      "Epoch [4/4], Step [39600/68040], Loss: 5.0993\n",
      "Epoch [4/4], Step [39675/68040], Loss: 4.8682\n",
      "Epoch [4/4], Step [39750/68040], Loss: 5.1765\n",
      "Epoch [4/4], Step [39825/68040], Loss: 5.0022\n",
      "Epoch [4/4], Step [39900/68040], Loss: 5.1094\n",
      "Epoch [4/4], Step [39975/68040], Loss: 5.1753\n",
      "Validation perplexity: 133.25091007629982\n",
      "Epoch [4/4], Step [40050/68040], Loss: 4.8747\n",
      "Epoch [4/4], Step [40125/68040], Loss: 5.0188\n",
      "Epoch [4/4], Step [40200/68040], Loss: 5.0286\n",
      "Epoch [4/4], Step [40275/68040], Loss: 4.9109\n",
      "Epoch [4/4], Step [40350/68040], Loss: 4.9887\n",
      "Epoch [4/4], Step [40425/68040], Loss: 5.0356\n",
      "Epoch [4/4], Step [40500/68040], Loss: 5.1929\n",
      "Epoch [4/4], Step [40575/68040], Loss: 4.9291\n",
      "Epoch [4/4], Step [40650/68040], Loss: 4.9805\n",
      "Epoch [4/4], Step [40725/68040], Loss: 4.7985\n",
      "Epoch [4/4], Step [40800/68040], Loss: 5.0100\n",
      "Epoch [4/4], Step [40875/68040], Loss: 5.2679\n",
      "Epoch [4/4], Step [40950/68040], Loss: 4.9183\n",
      "Epoch [4/4], Step [41025/68040], Loss: 4.9803\n",
      "Epoch [4/4], Step [41100/68040], Loss: 5.0951\n",
      "Epoch [4/4], Step [41175/68040], Loss: 5.1669\n",
      "Epoch [4/4], Step [41250/68040], Loss: 5.1850\n",
      "Epoch [4/4], Step [41325/68040], Loss: 5.0507\n",
      "Epoch [4/4], Step [41400/68040], Loss: 5.0385\n",
      "Epoch [4/4], Step [41475/68040], Loss: 4.9475\n",
      "Epoch [4/4], Step [41550/68040], Loss: 4.9944\n",
      "Epoch [4/4], Step [41625/68040], Loss: 5.0445\n",
      "Epoch [4/4], Step [41700/68040], Loss: 5.1606\n",
      "Epoch [4/4], Step [41775/68040], Loss: 5.0274\n",
      "Epoch [4/4], Step [41850/68040], Loss: 5.0007\n",
      "Epoch [4/4], Step [41925/68040], Loss: 5.0937\n",
      "Epoch [4/4], Step [42000/68040], Loss: 4.9614\n",
      "Epoch [4/4], Step [42075/68040], Loss: 5.0277\n",
      "Epoch [4/4], Step [42150/68040], Loss: 5.1520\n",
      "Epoch [4/4], Step [42225/68040], Loss: 5.1828\n",
      "Epoch [4/4], Step [42300/68040], Loss: 5.0132\n",
      "Epoch [4/4], Step [42375/68040], Loss: 4.8692\n",
      "Epoch [4/4], Step [42450/68040], Loss: 5.0566\n",
      "Epoch [4/4], Step [42525/68040], Loss: 5.1617\n",
      "Epoch [4/4], Step [42600/68040], Loss: 5.0527\n",
      "Epoch [4/4], Step [42675/68040], Loss: 5.0667\n",
      "Epoch [4/4], Step [42750/68040], Loss: 5.0257\n",
      "Epoch [4/4], Step [42825/68040], Loss: 5.1630\n",
      "Epoch [4/4], Step [42900/68040], Loss: 5.0954\n",
      "Epoch [4/4], Step [42975/68040], Loss: 5.1057\n",
      "Epoch [4/4], Step [43050/68040], Loss: 4.9677\n",
      "Epoch [4/4], Step [43125/68040], Loss: 5.0781\n",
      "Epoch [4/4], Step [43200/68040], Loss: 4.9651\n",
      "Epoch [4/4], Step [43275/68040], Loss: 5.1779\n",
      "Epoch [4/4], Step [43350/68040], Loss: 5.1511\n",
      "Epoch [4/4], Step [43425/68040], Loss: 5.0201\n",
      "Epoch [4/4], Step [43500/68040], Loss: 4.9928\n",
      "Epoch [4/4], Step [43575/68040], Loss: 5.1047\n",
      "Epoch [4/4], Step [43650/68040], Loss: 5.0402\n",
      "Epoch [4/4], Step [43725/68040], Loss: 4.9851\n",
      "Epoch [4/4], Step [43800/68040], Loss: 5.1936\n",
      "Epoch [4/4], Step [43875/68040], Loss: 4.8637\n",
      "Epoch [4/4], Step [43950/68040], Loss: 5.0461\n",
      "Epoch [4/4], Step [44025/68040], Loss: 4.9588\n",
      "Epoch [4/4], Step [44100/68040], Loss: 5.0820\n",
      "Epoch [4/4], Step [44175/68040], Loss: 4.9720\n",
      "Epoch [4/4], Step [44250/68040], Loss: 5.0268\n",
      "Epoch [4/4], Step [44325/68040], Loss: 4.9101\n",
      "Epoch [4/4], Step [44400/68040], Loss: 4.9296\n",
      "Epoch [4/4], Step [44475/68040], Loss: 5.0436\n",
      "Epoch [4/4], Step [44550/68040], Loss: 4.8618\n",
      "Epoch [4/4], Step [44625/68040], Loss: 4.9145\n",
      "Epoch [4/4], Step [44700/68040], Loss: 5.0627\n",
      "Epoch [4/4], Step [44775/68040], Loss: 4.9708\n",
      "Epoch [4/4], Step [44850/68040], Loss: 4.9736\n",
      "Epoch [4/4], Step [44925/68040], Loss: 5.0618\n",
      "Epoch [4/4], Step [45000/68040], Loss: 5.0656\n",
      "Epoch [4/4], Step [45075/68040], Loss: 5.0787\n",
      "Epoch [4/4], Step [45150/68040], Loss: 5.0726\n",
      "Epoch [4/4], Step [45225/68040], Loss: 5.1472\n",
      "Epoch [4/4], Step [45300/68040], Loss: 4.8867\n",
      "Epoch [4/4], Step [45375/68040], Loss: 5.0955\n",
      "Epoch [4/4], Step [45450/68040], Loss: 5.0980\n",
      "Epoch [4/4], Step [45525/68040], Loss: 5.0812\n",
      "Epoch [4/4], Step [45600/68040], Loss: 5.0355\n",
      "Epoch [4/4], Step [45675/68040], Loss: 5.0833\n",
      "Epoch [4/4], Step [45750/68040], Loss: 5.0419\n",
      "Epoch [4/4], Step [45825/68040], Loss: 5.0676\n",
      "Epoch [4/4], Step [45900/68040], Loss: 4.8493\n",
      "Epoch [4/4], Step [45975/68040], Loss: 5.1091\n",
      "Epoch [4/4], Step [46050/68040], Loss: 4.8641\n",
      "Epoch [4/4], Step [46125/68040], Loss: 5.1685\n",
      "Epoch [4/4], Step [46200/68040], Loss: 5.0139\n",
      "Epoch [4/4], Step [46275/68040], Loss: 4.8652\n",
      "Epoch [4/4], Step [46350/68040], Loss: 4.9923\n",
      "Epoch [4/4], Step [46425/68040], Loss: 4.9952\n",
      "Epoch [4/4], Step [46500/68040], Loss: 5.1127\n",
      "Epoch [4/4], Step [46575/68040], Loss: 4.9246\n",
      "Epoch [4/4], Step [46650/68040], Loss: 5.1580\n",
      "Epoch [4/4], Step [46725/68040], Loss: 5.1409\n",
      "Epoch [4/4], Step [46800/68040], Loss: 4.9544\n",
      "Epoch [4/4], Step [46875/68040], Loss: 5.1526\n",
      "Epoch [4/4], Step [46950/68040], Loss: 5.1149\n",
      "Epoch [4/4], Step [47025/68040], Loss: 4.9507\n",
      "Epoch [4/4], Step [47100/68040], Loss: 5.2223\n",
      "Epoch [4/4], Step [47175/68040], Loss: 5.0088\n",
      "Epoch [4/4], Step [47250/68040], Loss: 5.0886\n",
      "Epoch [4/4], Step [47325/68040], Loss: 4.9670\n",
      "Epoch [4/4], Step [47400/68040], Loss: 5.0998\n",
      "Epoch [4/4], Step [47475/68040], Loss: 5.1140\n",
      "Epoch [4/4], Step [47550/68040], Loss: 4.9241\n",
      "Epoch [4/4], Step [47625/68040], Loss: 5.0323\n",
      "Epoch [4/4], Step [47700/68040], Loss: 5.1555\n",
      "Epoch [4/4], Step [47775/68040], Loss: 4.8618\n",
      "Epoch [4/4], Step [47850/68040], Loss: 5.0911\n",
      "Epoch [4/4], Step [47925/68040], Loss: 5.0264\n",
      "Epoch [4/4], Step [48000/68040], Loss: 5.0646\n",
      "Epoch [4/4], Step [48075/68040], Loss: 5.0287\n",
      "Epoch [4/4], Step [48150/68040], Loss: 4.9939\n",
      "Epoch [4/4], Step [48225/68040], Loss: 4.9673\n",
      "Epoch [4/4], Step [48300/68040], Loss: 5.0628\n",
      "Epoch [4/4], Step [48375/68040], Loss: 5.1345\n",
      "Epoch [4/4], Step [48450/68040], Loss: 5.0245\n",
      "Epoch [4/4], Step [48525/68040], Loss: 5.1514\n",
      "Epoch [4/4], Step [48600/68040], Loss: 4.9673\n",
      "Epoch [4/4], Step [48675/68040], Loss: 4.9502\n",
      "Epoch [4/4], Step [48750/68040], Loss: 4.9243\n",
      "Epoch [4/4], Step [48825/68040], Loss: 5.0485\n",
      "Epoch [4/4], Step [48900/68040], Loss: 5.0149\n",
      "Epoch [4/4], Step [48975/68040], Loss: 4.8922\n",
      "Epoch [4/4], Step [49050/68040], Loss: 4.9666\n",
      "Epoch [4/4], Step [49125/68040], Loss: 5.1267\n",
      "Epoch [4/4], Step [49200/68040], Loss: 5.2399\n",
      "Epoch [4/4], Step [49275/68040], Loss: 5.0082\n",
      "Epoch [4/4], Step [49350/68040], Loss: 5.0466\n",
      "Epoch [4/4], Step [49425/68040], Loss: 5.0630\n",
      "Epoch [4/4], Step [49500/68040], Loss: 5.1463\n",
      "Epoch [4/4], Step [49575/68040], Loss: 4.9215\n",
      "Epoch [4/4], Step [49650/68040], Loss: 5.0637\n",
      "Epoch [4/4], Step [49725/68040], Loss: 5.0729\n",
      "Epoch [4/4], Step [49800/68040], Loss: 4.9947\n",
      "Epoch [4/4], Step [49875/68040], Loss: 4.7756\n",
      "Epoch [4/4], Step [49950/68040], Loss: 5.0982\n",
      "Validation perplexity: 132.9725983125476\n",
      "Epoch [4/4], Step [50025/68040], Loss: 5.1052\n",
      "Epoch [4/4], Step [50100/68040], Loss: 5.0266\n",
      "Epoch [4/4], Step [50175/68040], Loss: 4.9197\n",
      "Epoch [4/4], Step [50250/68040], Loss: 4.9688\n",
      "Epoch [4/4], Step [50325/68040], Loss: 4.9596\n",
      "Epoch [4/4], Step [50400/68040], Loss: 4.8771\n",
      "Epoch [4/4], Step [50475/68040], Loss: 4.9378\n",
      "Epoch [4/4], Step [50550/68040], Loss: 5.0107\n",
      "Epoch [4/4], Step [50625/68040], Loss: 4.8805\n",
      "Epoch [4/4], Step [50700/68040], Loss: 4.7564\n",
      "Epoch [4/4], Step [50775/68040], Loss: 4.9667\n",
      "Epoch [4/4], Step [50850/68040], Loss: 4.9020\n",
      "Epoch [4/4], Step [50925/68040], Loss: 4.9411\n",
      "Epoch [4/4], Step [51000/68040], Loss: 5.0576\n",
      "Epoch [4/4], Step [51075/68040], Loss: 5.0048\n",
      "Epoch [4/4], Step [51150/68040], Loss: 4.8598\n",
      "Epoch [4/4], Step [51225/68040], Loss: 5.1837\n",
      "Epoch [4/4], Step [51300/68040], Loss: 4.9742\n",
      "Epoch [4/4], Step [51375/68040], Loss: 5.1354\n",
      "Epoch [4/4], Step [51450/68040], Loss: 5.0114\n",
      "Epoch [4/4], Step [51525/68040], Loss: 5.1905\n",
      "Epoch [4/4], Step [51600/68040], Loss: 5.0677\n",
      "Epoch [4/4], Step [51675/68040], Loss: 5.2031\n",
      "Epoch [4/4], Step [51750/68040], Loss: 5.1336\n",
      "Epoch [4/4], Step [51825/68040], Loss: 5.0487\n",
      "Epoch [4/4], Step [51900/68040], Loss: 5.1079\n",
      "Epoch [4/4], Step [51975/68040], Loss: 5.1277\n",
      "Epoch [4/4], Step [52050/68040], Loss: 5.1786\n",
      "Epoch [4/4], Step [52125/68040], Loss: 5.0836\n",
      "Epoch [4/4], Step [52200/68040], Loss: 5.1665\n",
      "Epoch [4/4], Step [52275/68040], Loss: 4.9847\n",
      "Epoch [4/4], Step [52350/68040], Loss: 4.9528\n",
      "Epoch [4/4], Step [52425/68040], Loss: 5.1445\n",
      "Epoch [4/4], Step [52500/68040], Loss: 5.0030\n",
      "Epoch [4/4], Step [52575/68040], Loss: 5.0031\n",
      "Epoch [4/4], Step [52650/68040], Loss: 4.9408\n",
      "Epoch [4/4], Step [52725/68040], Loss: 4.9189\n",
      "Epoch [4/4], Step [52800/68040], Loss: 5.1414\n",
      "Epoch [4/4], Step [52875/68040], Loss: 5.1583\n",
      "Epoch [4/4], Step [52950/68040], Loss: 4.9123\n",
      "Epoch [4/4], Step [53025/68040], Loss: 5.0076\n",
      "Epoch [4/4], Step [53100/68040], Loss: 4.9666\n",
      "Epoch [4/4], Step [53175/68040], Loss: 4.9266\n",
      "Epoch [4/4], Step [53250/68040], Loss: 4.9332\n",
      "Epoch [4/4], Step [53325/68040], Loss: 5.1151\n",
      "Epoch [4/4], Step [53400/68040], Loss: 5.1090\n",
      "Epoch [4/4], Step [53475/68040], Loss: 4.8941\n",
      "Epoch [4/4], Step [53550/68040], Loss: 5.2133\n",
      "Epoch [4/4], Step [53625/68040], Loss: 4.9600\n",
      "Epoch [4/4], Step [53700/68040], Loss: 4.9368\n",
      "Epoch [4/4], Step [53775/68040], Loss: 4.9229\n",
      "Epoch [4/4], Step [53850/68040], Loss: 5.0358\n",
      "Epoch [4/4], Step [53925/68040], Loss: 4.9767\n",
      "Epoch [4/4], Step [54000/68040], Loss: 5.1851\n",
      "Epoch [4/4], Step [54075/68040], Loss: 5.0234\n",
      "Epoch [4/4], Step [54150/68040], Loss: 5.1100\n",
      "Epoch [4/4], Step [54225/68040], Loss: 4.9664\n",
      "Epoch [4/4], Step [54300/68040], Loss: 4.9797\n",
      "Epoch [4/4], Step [54375/68040], Loss: 5.1345\n",
      "Epoch [4/4], Step [54450/68040], Loss: 5.0194\n",
      "Epoch [4/4], Step [54525/68040], Loss: 5.1810\n",
      "Epoch [4/4], Step [54600/68040], Loss: 5.1303\n",
      "Epoch [4/4], Step [54675/68040], Loss: 5.0700\n",
      "Epoch [4/4], Step [54750/68040], Loss: 5.1391\n",
      "Epoch [4/4], Step [54825/68040], Loss: 4.8735\n",
      "Epoch [4/4], Step [54900/68040], Loss: 5.0662\n",
      "Epoch [4/4], Step [54975/68040], Loss: 4.9886\n",
      "Epoch [4/4], Step [55050/68040], Loss: 5.1765\n",
      "Epoch [4/4], Step [55125/68040], Loss: 5.0023\n",
      "Epoch [4/4], Step [55200/68040], Loss: 5.1390\n",
      "Epoch [4/4], Step [55275/68040], Loss: 5.0410\n",
      "Epoch [4/4], Step [55350/68040], Loss: 4.9671\n",
      "Epoch [4/4], Step [55425/68040], Loss: 5.0745\n",
      "Epoch [4/4], Step [55500/68040], Loss: 5.0144\n",
      "Epoch [4/4], Step [55575/68040], Loss: 5.0510\n",
      "Epoch [4/4], Step [55650/68040], Loss: 5.0097\n",
      "Epoch [4/4], Step [55725/68040], Loss: 5.0363\n",
      "Epoch [4/4], Step [55800/68040], Loss: 4.9717\n",
      "Epoch [4/4], Step [55875/68040], Loss: 5.2325\n",
      "Epoch [4/4], Step [55950/68040], Loss: 5.0819\n",
      "Epoch [4/4], Step [56025/68040], Loss: 5.1987\n",
      "Epoch [4/4], Step [56100/68040], Loss: 5.0567\n",
      "Epoch [4/4], Step [56175/68040], Loss: 4.8676\n",
      "Epoch [4/4], Step [56250/68040], Loss: 5.2047\n",
      "Epoch [4/4], Step [56325/68040], Loss: 5.1386\n",
      "Epoch [4/4], Step [56400/68040], Loss: 4.7194\n",
      "Epoch [4/4], Step [56475/68040], Loss: 5.1981\n",
      "Epoch [4/4], Step [56550/68040], Loss: 5.0646\n",
      "Epoch [4/4], Step [56625/68040], Loss: 5.0326\n",
      "Epoch [4/4], Step [56700/68040], Loss: 5.1642\n",
      "Epoch [4/4], Step [56775/68040], Loss: 5.0909\n",
      "Epoch [4/4], Step [56850/68040], Loss: 5.0731\n",
      "Epoch [4/4], Step [56925/68040], Loss: 5.0710\n",
      "Epoch [4/4], Step [57000/68040], Loss: 5.0023\n",
      "Epoch [4/4], Step [57075/68040], Loss: 5.1111\n",
      "Epoch [4/4], Step [57150/68040], Loss: 5.0880\n",
      "Epoch [4/4], Step [57225/68040], Loss: 4.9569\n",
      "Epoch [4/4], Step [57300/68040], Loss: 5.0169\n",
      "Epoch [4/4], Step [57375/68040], Loss: 4.9068\n",
      "Epoch [4/4], Step [57450/68040], Loss: 5.0471\n",
      "Epoch [4/4], Step [57525/68040], Loss: 5.1292\n",
      "Epoch [4/4], Step [57600/68040], Loss: 5.1209\n",
      "Epoch [4/4], Step [57675/68040], Loss: 5.0749\n",
      "Epoch [4/4], Step [57750/68040], Loss: 4.9275\n",
      "Epoch [4/4], Step [57825/68040], Loss: 5.0691\n",
      "Epoch [4/4], Step [57900/68040], Loss: 5.0128\n",
      "Epoch [4/4], Step [57975/68040], Loss: 5.0516\n",
      "Epoch [4/4], Step [58050/68040], Loss: 4.9652\n",
      "Epoch [4/4], Step [58125/68040], Loss: 5.0282\n",
      "Epoch [4/4], Step [58200/68040], Loss: 5.0912\n",
      "Epoch [4/4], Step [58275/68040], Loss: 5.0103\n",
      "Epoch [4/4], Step [58350/68040], Loss: 4.9593\n",
      "Epoch [4/4], Step [58425/68040], Loss: 5.0189\n",
      "Epoch [4/4], Step [58500/68040], Loss: 5.0805\n",
      "Epoch [4/4], Step [58575/68040], Loss: 5.0542\n",
      "Epoch [4/4], Step [58650/68040], Loss: 5.0843\n",
      "Epoch [4/4], Step [58725/68040], Loss: 5.0004\n",
      "Epoch [4/4], Step [58800/68040], Loss: 5.0889\n",
      "Epoch [4/4], Step [58875/68040], Loss: 4.9738\n",
      "Epoch [4/4], Step [58950/68040], Loss: 5.0914\n",
      "Epoch [4/4], Step [59025/68040], Loss: 4.9696\n",
      "Epoch [4/4], Step [59100/68040], Loss: 5.0758\n",
      "Epoch [4/4], Step [59175/68040], Loss: 4.9872\n",
      "Epoch [4/4], Step [59250/68040], Loss: 4.7639\n",
      "Epoch [4/4], Step [59325/68040], Loss: 4.9963\n",
      "Epoch [4/4], Step [59400/68040], Loss: 5.0271\n",
      "Epoch [4/4], Step [59475/68040], Loss: 5.2001\n",
      "Epoch [4/4], Step [59550/68040], Loss: 5.0032\n",
      "Epoch [4/4], Step [59625/68040], Loss: 5.1430\n",
      "Epoch [4/4], Step [59700/68040], Loss: 5.0288\n",
      "Epoch [4/4], Step [59775/68040], Loss: 4.9746\n",
      "Epoch [4/4], Step [59850/68040], Loss: 5.0709\n",
      "Epoch [4/4], Step [59925/68040], Loss: 5.0814\n",
      "Epoch [4/4], Step [60000/68040], Loss: 5.0418\n",
      "Validation perplexity: 132.69615278402063\n",
      "Epoch [4/4], Step [60075/68040], Loss: 4.9349\n",
      "Epoch [4/4], Step [60150/68040], Loss: 5.1733\n",
      "Epoch [4/4], Step [60225/68040], Loss: 4.7381\n",
      "Epoch [4/4], Step [60300/68040], Loss: 4.9217\n",
      "Epoch [4/4], Step [60375/68040], Loss: 5.0086\n",
      "Epoch [4/4], Step [60450/68040], Loss: 5.0006\n",
      "Epoch [4/4], Step [60525/68040], Loss: 5.0469\n",
      "Epoch [4/4], Step [60600/68040], Loss: 5.0623\n",
      "Epoch [4/4], Step [60675/68040], Loss: 5.0244\n",
      "Epoch [4/4], Step [60750/68040], Loss: 5.0130\n",
      "Epoch [4/4], Step [60825/68040], Loss: 5.2006\n",
      "Epoch [4/4], Step [60900/68040], Loss: 5.0443\n",
      "Epoch [4/4], Step [60975/68040], Loss: 5.1013\n",
      "Epoch [4/4], Step [61050/68040], Loss: 5.0247\n",
      "Epoch [4/4], Step [61125/68040], Loss: 4.9406\n",
      "Epoch [4/4], Step [61200/68040], Loss: 5.1854\n",
      "Epoch [4/4], Step [61275/68040], Loss: 5.1461\n",
      "Epoch [4/4], Step [61350/68040], Loss: 4.9452\n",
      "Epoch [4/4], Step [61425/68040], Loss: 4.9852\n",
      "Epoch [4/4], Step [61500/68040], Loss: 4.8948\n",
      "Epoch [4/4], Step [61575/68040], Loss: 4.9640\n",
      "Epoch [4/4], Step [61650/68040], Loss: 5.0321\n",
      "Epoch [4/4], Step [61725/68040], Loss: 5.0622\n",
      "Epoch [4/4], Step [61800/68040], Loss: 4.8569\n",
      "Epoch [4/4], Step [61875/68040], Loss: 4.9366\n",
      "Epoch [4/4], Step [61950/68040], Loss: 5.0287\n",
      "Epoch [4/4], Step [62025/68040], Loss: 5.1542\n",
      "Epoch [4/4], Step [62100/68040], Loss: 5.0045\n",
      "Epoch [4/4], Step [62175/68040], Loss: 5.0755\n",
      "Epoch [4/4], Step [62250/68040], Loss: 5.1723\n",
      "Epoch [4/4], Step [62325/68040], Loss: 5.1387\n",
      "Epoch [4/4], Step [62400/68040], Loss: 4.9099\n",
      "Epoch [4/4], Step [62475/68040], Loss: 4.9515\n",
      "Epoch [4/4], Step [62550/68040], Loss: 5.1715\n",
      "Epoch [4/4], Step [62625/68040], Loss: 5.1614\n",
      "Epoch [4/4], Step [62700/68040], Loss: 4.9264\n",
      "Epoch [4/4], Step [62775/68040], Loss: 5.0569\n",
      "Epoch [4/4], Step [62850/68040], Loss: 4.7507\n",
      "Epoch [4/4], Step [62925/68040], Loss: 5.0035\n",
      "Epoch [4/4], Step [63000/68040], Loss: 4.8339\n",
      "Epoch [4/4], Step [63075/68040], Loss: 4.8843\n",
      "Epoch [4/4], Step [63150/68040], Loss: 5.0064\n",
      "Epoch [4/4], Step [63225/68040], Loss: 5.2017\n",
      "Epoch [4/4], Step [63300/68040], Loss: 5.0959\n",
      "Epoch [4/4], Step [63375/68040], Loss: 5.0421\n",
      "Epoch [4/4], Step [63450/68040], Loss: 4.9492\n",
      "Epoch [4/4], Step [63525/68040], Loss: 5.0140\n",
      "Epoch [4/4], Step [63600/68040], Loss: 4.9172\n",
      "Epoch [4/4], Step [63675/68040], Loss: 5.1527\n",
      "Epoch [4/4], Step [63750/68040], Loss: 5.0630\n",
      "Epoch [4/4], Step [63825/68040], Loss: 4.9711\n",
      "Epoch [4/4], Step [63900/68040], Loss: 4.8614\n",
      "Epoch [4/4], Step [63975/68040], Loss: 5.1502\n",
      "Epoch [4/4], Step [64050/68040], Loss: 4.9908\n",
      "Epoch [4/4], Step [64125/68040], Loss: 4.9794\n",
      "Epoch [4/4], Step [64200/68040], Loss: 4.8953\n",
      "Epoch [4/4], Step [64275/68040], Loss: 5.0464\n",
      "Epoch [4/4], Step [64350/68040], Loss: 4.8747\n",
      "Epoch [4/4], Step [64425/68040], Loss: 5.0976\n",
      "Epoch [4/4], Step [64500/68040], Loss: 5.0739\n",
      "Epoch [4/4], Step [64575/68040], Loss: 5.0208\n",
      "Epoch [4/4], Step [64650/68040], Loss: 5.2138\n",
      "Epoch [4/4], Step [64725/68040], Loss: 5.1139\n",
      "Epoch [4/4], Step [64800/68040], Loss: 5.2755\n",
      "Epoch [4/4], Step [64875/68040], Loss: 4.9834\n",
      "Epoch [4/4], Step [64950/68040], Loss: 5.0755\n",
      "Epoch [4/4], Step [65025/68040], Loss: 4.8717\n",
      "Epoch [4/4], Step [65100/68040], Loss: 5.0700\n",
      "Epoch [4/4], Step [65175/68040], Loss: 5.0098\n",
      "Epoch [4/4], Step [65250/68040], Loss: 5.0894\n",
      "Epoch [4/4], Step [65325/68040], Loss: 5.1887\n",
      "Epoch [4/4], Step [65400/68040], Loss: 4.9842\n",
      "Epoch [4/4], Step [65475/68040], Loss: 4.9921\n",
      "Epoch [4/4], Step [65550/68040], Loss: 5.0418\n",
      "Epoch [4/4], Step [65625/68040], Loss: 4.9924\n",
      "Epoch [4/4], Step [65700/68040], Loss: 4.9616\n",
      "Epoch [4/4], Step [65775/68040], Loss: 4.9139\n",
      "Epoch [4/4], Step [65850/68040], Loss: 5.0251\n",
      "Epoch [4/4], Step [65925/68040], Loss: 5.0177\n",
      "Epoch [4/4], Step [66000/68040], Loss: 5.1776\n",
      "Epoch [4/4], Step [66075/68040], Loss: 5.0049\n",
      "Epoch [4/4], Step [66150/68040], Loss: 5.2040\n",
      "Epoch [4/4], Step [66225/68040], Loss: 5.1945\n",
      "Epoch [4/4], Step [66300/68040], Loss: 5.0820\n",
      "Epoch [4/4], Step [66375/68040], Loss: 5.1720\n",
      "Epoch [4/4], Step [66450/68040], Loss: 4.9560\n",
      "Epoch [4/4], Step [66525/68040], Loss: 5.0316\n",
      "Epoch [4/4], Step [66600/68040], Loss: 5.0766\n",
      "Epoch [4/4], Step [66675/68040], Loss: 5.0145\n",
      "Epoch [4/4], Step [66750/68040], Loss: 5.0117\n",
      "Epoch [4/4], Step [66825/68040], Loss: 5.0650\n",
      "Epoch [4/4], Step [66900/68040], Loss: 4.9842\n",
      "Epoch [4/4], Step [66975/68040], Loss: 5.0309\n",
      "Epoch [4/4], Step [67050/68040], Loss: 5.0185\n",
      "Epoch [4/4], Step [67125/68040], Loss: 5.0207\n",
      "Epoch [4/4], Step [67200/68040], Loss: 5.0546\n",
      "Epoch [4/4], Step [67275/68040], Loss: 5.1344\n",
      "Epoch [4/4], Step [67350/68040], Loss: 5.1266\n",
      "Epoch [4/4], Step [67425/68040], Loss: 4.9521\n",
      "Epoch [4/4], Step [67500/68040], Loss: 5.0323\n",
      "Epoch [4/4], Step [67575/68040], Loss: 4.7644\n",
      "Epoch [4/4], Step [67650/68040], Loss: 5.0785\n",
      "Epoch [4/4], Step [67725/68040], Loss: 4.9884\n",
      "Epoch [4/4], Step [67800/68040], Loss: 4.8650\n",
      "Epoch [4/4], Step [67875/68040], Loss: 5.0712\n",
      "Epoch [4/4], Step [67950/68040], Loss: 4.8738\n",
      "Epoch [4/4], Step [68025/68040], Loss: 5.0449\n",
      "Epoch [4/4] Average Loss: 5.0395, Perplexity: 154.39\n"
     ]
    }
   ],
   "source": [
    "from src.model import RegularizedLanguageModel\n",
    "from src.trainComplete import TrainComplete\n",
    "from src.helper  import clean_pers_text_replace, get_cleaned_text,clean_pers_remove,clean_text_pers_both\n",
    "\n",
    "raw_text = get_cleaned_text(text_path,clean_pers_remove)\n",
    "trainclass = TrainComplete(text_path = text_path,path_to_save_folder= path_to_save_folder,tokenizer = tokenizer,allowed_special=False)\n",
    "\n",
    "model = RegularizedLanguageModel(vocab_size, embedding_dim, context_length, dropout=0.2).to(device)\n",
    "\n",
    "\n",
    "trainclass.train(model,\n",
    "              vocab_size,device,raw_text,\"pers_standardLinearNotRelu_ep4_evaluate10000_preprocessingRemove\",\n",
    "                print_every=75,evaluate_every=10000,optimizer=None,criterion=None,\n",
    "              batch_size = 32,\n",
    "              embedding_dim = 128,\n",
    "              context_length = 32,\n",
    "              num_epochs = 4\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bbebfe0-d4ad-449c-a8d3-c9dd7c347c77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:  100000  lines seperators replaced\n",
      "Total lines not removed :  97334\n",
      "Total lines replaced 39736\n",
      "Total lines replaced 1561\n",
      "In:  0  lines seperators replaced\n",
      "Total lines replaced 92506\n",
      "Total lines replaced 0\n",
      "Total lines replaced 16\n",
      " Create Dataset 2560000 / 2562673Epoch [1/4], Step [0/64066], Loss: 10.6831\n",
      "Validation perplexity: 40421.322676752956\n",
      "Epoch [1/4], Step [75/64066], Loss: 8.9874\n",
      "Epoch [1/4], Step [150/64066], Loss: 7.5029\n",
      "Epoch [1/4], Step [225/64066], Loss: 7.3693\n",
      "Epoch [1/4], Step [300/64066], Loss: 7.1476\n",
      "Epoch [1/4], Step [375/64066], Loss: 7.2701\n",
      "Epoch [1/4], Step [450/64066], Loss: 6.8898\n",
      "Epoch [1/4], Step [525/64066], Loss: 6.9117\n",
      "Epoch [1/4], Step [600/64066], Loss: 6.8769\n",
      "Epoch [1/4], Step [675/64066], Loss: 6.8044\n",
      "Epoch [1/4], Step [750/64066], Loss: 6.6115\n",
      "Epoch [1/4], Step [825/64066], Loss: 6.7131\n",
      "Epoch [1/4], Step [900/64066], Loss: 6.7361\n",
      "Epoch [1/4], Step [975/64066], Loss: 6.8109\n",
      "Epoch [1/4], Step [1050/64066], Loss: 6.7611\n",
      "Epoch [1/4], Step [1125/64066], Loss: 6.7505\n",
      "Epoch [1/4], Step [1200/64066], Loss: 6.5187\n",
      "Epoch [1/4], Step [1275/64066], Loss: 6.6104\n",
      "Epoch [1/4], Step [1350/64066], Loss: 6.4591\n",
      "Epoch [1/4], Step [1425/64066], Loss: 6.5400\n",
      "Epoch [1/4], Step [1500/64066], Loss: 6.5175\n",
      "Epoch [1/4], Step [1575/64066], Loss: 6.4947\n",
      "Epoch [1/4], Step [1650/64066], Loss: 6.6540\n",
      "Epoch [1/4], Step [1725/64066], Loss: 6.2446\n",
      "Epoch [1/4], Step [1800/64066], Loss: 6.4978\n",
      "Epoch [1/4], Step [1875/64066], Loss: 6.6027\n",
      "Epoch [1/4], Step [1950/64066], Loss: 6.4096\n",
      "Epoch [1/4], Step [2025/64066], Loss: 6.4545\n",
      "Epoch [1/4], Step [2100/64066], Loss: 6.2468\n",
      "Epoch [1/4], Step [2175/64066], Loss: 6.5282\n",
      "Epoch [1/4], Step [2250/64066], Loss: 6.4512\n",
      "Epoch [1/4], Step [2325/64066], Loss: 6.3707\n",
      "Epoch [1/4], Step [2400/64066], Loss: 6.4520\n",
      "Epoch [1/4], Step [2475/64066], Loss: 6.3398\n",
      "Epoch [1/4], Step [2550/64066], Loss: 6.3938\n",
      "Epoch [1/4], Step [2625/64066], Loss: 6.2315\n",
      "Epoch [1/4], Step [2700/64066], Loss: 6.2941\n",
      "Epoch [1/4], Step [2775/64066], Loss: 6.2786\n",
      "Epoch [1/4], Step [2850/64066], Loss: 6.1327\n",
      "Epoch [1/4], Step [2925/64066], Loss: 6.0972\n",
      "Epoch [1/4], Step [3000/64066], Loss: 6.2365\n",
      "Epoch [1/4], Step [3075/64066], Loss: 6.2568\n",
      "Epoch [1/4], Step [3150/64066], Loss: 6.3711\n",
      "Epoch [1/4], Step [3225/64066], Loss: 6.3200\n",
      "Epoch [1/4], Step [3300/64066], Loss: 6.2864\n",
      "Epoch [1/4], Step [3375/64066], Loss: 6.2781\n",
      "Epoch [1/4], Step [3450/64066], Loss: 6.2554\n",
      "Epoch [1/4], Step [3525/64066], Loss: 6.3397\n",
      "Epoch [1/4], Step [3600/64066], Loss: 6.2091\n",
      "Epoch [1/4], Step [3675/64066], Loss: 6.0287\n",
      "Epoch [1/4], Step [3750/64066], Loss: 6.3588\n",
      "Epoch [1/4], Step [3825/64066], Loss: 6.1436\n",
      "Epoch [1/4], Step [3900/64066], Loss: 6.0496\n",
      "Epoch [1/4], Step [3975/64066], Loss: 6.2315\n",
      "Epoch [1/4], Step [4050/64066], Loss: 6.1410\n",
      "Epoch [1/4], Step [4125/64066], Loss: 6.1226\n",
      "Epoch [1/4], Step [4200/64066], Loss: 6.2225\n",
      "Epoch [1/4], Step [4275/64066], Loss: 6.2763\n",
      "Epoch [1/4], Step [4350/64066], Loss: 6.1008\n",
      "Epoch [1/4], Step [4425/64066], Loss: 6.2282\n",
      "Epoch [1/4], Step [4500/64066], Loss: 5.8888\n",
      "Epoch [1/4], Step [4575/64066], Loss: 6.0603\n",
      "Epoch [1/4], Step [4650/64066], Loss: 6.1259\n",
      "Epoch [1/4], Step [4725/64066], Loss: 6.2310\n",
      "Epoch [1/4], Step [4800/64066], Loss: 6.0013\n",
      "Epoch [1/4], Step [4875/64066], Loss: 6.1282\n",
      "Epoch [1/4], Step [4950/64066], Loss: 5.7759\n",
      "Epoch [1/4], Step [5025/64066], Loss: 6.1498\n",
      "Epoch [1/4], Step [5100/64066], Loss: 5.7722\n",
      "Epoch [1/4], Step [5175/64066], Loss: 6.0084\n",
      "Epoch [1/4], Step [5250/64066], Loss: 6.0410\n",
      "Epoch [1/4], Step [5325/64066], Loss: 5.9802\n",
      "Epoch [1/4], Step [5400/64066], Loss: 5.7068\n",
      "Epoch [1/4], Step [5475/64066], Loss: 6.0807\n",
      "Epoch [1/4], Step [5550/64066], Loss: 6.1458\n",
      "Epoch [1/4], Step [5625/64066], Loss: 6.0401\n",
      "Epoch [1/4], Step [5700/64066], Loss: 6.0716\n",
      "Epoch [1/4], Step [5775/64066], Loss: 6.0958\n",
      "Epoch [1/4], Step [5850/64066], Loss: 6.0111\n",
      "Epoch [1/4], Step [5925/64066], Loss: 6.0618\n",
      "Epoch [1/4], Step [6000/64066], Loss: 6.0244\n",
      "Epoch [1/4], Step [6075/64066], Loss: 6.0597\n",
      "Epoch [1/4], Step [6150/64066], Loss: 5.8062\n",
      "Epoch [1/4], Step [6225/64066], Loss: 5.8959\n",
      "Epoch [1/4], Step [6300/64066], Loss: 5.9618\n",
      "Epoch [1/4], Step [6375/64066], Loss: 6.0552\n",
      "Epoch [1/4], Step [6450/64066], Loss: 6.1070\n",
      "Epoch [1/4], Step [6525/64066], Loss: 5.9432\n",
      "Epoch [1/4], Step [6600/64066], Loss: 5.8720\n",
      "Epoch [1/4], Step [6675/64066], Loss: 5.8803\n",
      "Epoch [1/4], Step [6750/64066], Loss: 5.9436\n",
      "Epoch [1/4], Step [6825/64066], Loss: 5.7874\n",
      "Epoch [1/4], Step [6900/64066], Loss: 5.8736\n",
      "Epoch [1/4], Step [6975/64066], Loss: 5.9600\n",
      "Epoch [1/4], Step [7050/64066], Loss: 5.8419\n",
      "Epoch [1/4], Step [7125/64066], Loss: 5.8728\n",
      "Epoch [1/4], Step [7200/64066], Loss: 5.9648\n",
      "Epoch [1/4], Step [7275/64066], Loss: 5.9110\n",
      "Epoch [1/4], Step [7350/64066], Loss: 5.9270\n",
      "Epoch [1/4], Step [7425/64066], Loss: 5.8371\n",
      "Epoch [1/4], Step [7500/64066], Loss: 5.7843\n",
      "Epoch [1/4], Step [7575/64066], Loss: 5.8763\n",
      "Epoch [1/4], Step [7650/64066], Loss: 6.0556\n",
      "Epoch [1/4], Step [7725/64066], Loss: 5.8078\n",
      "Epoch [1/4], Step [7800/64066], Loss: 5.7698\n",
      "Epoch [1/4], Step [7875/64066], Loss: 5.9109\n",
      "Epoch [1/4], Step [7950/64066], Loss: 5.8150\n",
      "Epoch [1/4], Step [8025/64066], Loss: 5.9066\n",
      "Epoch [1/4], Step [8100/64066], Loss: 6.1204\n",
      "Epoch [1/4], Step [8175/64066], Loss: 5.9704\n",
      "Epoch [1/4], Step [8250/64066], Loss: 5.8851\n",
      "Epoch [1/4], Step [8325/64066], Loss: 5.8936\n",
      "Epoch [1/4], Step [8400/64066], Loss: 5.9222\n",
      "Epoch [1/4], Step [8475/64066], Loss: 5.9212\n",
      "Epoch [1/4], Step [8550/64066], Loss: 5.6588\n",
      "Epoch [1/4], Step [8625/64066], Loss: 5.7287\n",
      "Epoch [1/4], Step [8700/64066], Loss: 5.9041\n",
      "Epoch [1/4], Step [8775/64066], Loss: 5.6402\n",
      "Epoch [1/4], Step [8850/64066], Loss: 6.0075\n",
      "Epoch [1/4], Step [8925/64066], Loss: 5.7534\n",
      "Epoch [1/4], Step [9000/64066], Loss: 5.5856\n",
      "Epoch [1/4], Step [9075/64066], Loss: 5.8465\n",
      "Epoch [1/4], Step [9150/64066], Loss: 5.7102\n",
      "Epoch [1/4], Step [9225/64066], Loss: 5.5921\n",
      "Epoch [1/4], Step [9300/64066], Loss: 5.7046\n",
      "Epoch [1/4], Step [9375/64066], Loss: 5.6177\n",
      "Epoch [1/4], Step [9450/64066], Loss: 5.9455\n",
      "Epoch [1/4], Step [9525/64066], Loss: 6.0478\n",
      "Epoch [1/4], Step [9600/64066], Loss: 5.7565\n",
      "Epoch [1/4], Step [9675/64066], Loss: 5.6495\n",
      "Epoch [1/4], Step [9750/64066], Loss: 5.9474\n",
      "Epoch [1/4], Step [9825/64066], Loss: 5.8381\n",
      "Epoch [1/4], Step [9900/64066], Loss: 5.7597\n",
      "Epoch [1/4], Step [9975/64066], Loss: 5.7403\n",
      "Validation perplexity: 285.59295290864236\n",
      "Epoch [1/4], Step [10050/64066], Loss: 5.7933\n",
      "Epoch [1/4], Step [10125/64066], Loss: 5.5654\n",
      "Epoch [1/4], Step [10200/64066], Loss: 5.5129\n",
      "Epoch [1/4], Step [10275/64066], Loss: 5.7382\n",
      "Epoch [1/4], Step [10350/64066], Loss: 5.7053\n",
      "Epoch [1/4], Step [10425/64066], Loss: 5.8493\n",
      "Epoch [1/4], Step [10500/64066], Loss: 5.8441\n",
      "Epoch [1/4], Step [10575/64066], Loss: 5.7621\n",
      "Epoch [1/4], Step [10650/64066], Loss: 5.7924\n",
      "Epoch [1/4], Step [10725/64066], Loss: 5.5609\n",
      "Epoch [1/4], Step [10800/64066], Loss: 5.5798\n",
      "Epoch [1/4], Step [10875/64066], Loss: 5.7462\n",
      "Epoch [1/4], Step [10950/64066], Loss: 5.8676\n",
      "Epoch [1/4], Step [11025/64066], Loss: 5.6687\n",
      "Epoch [1/4], Step [11100/64066], Loss: 5.7392\n",
      "Epoch [1/4], Step [11175/64066], Loss: 5.9316\n",
      "Epoch [1/4], Step [11250/64066], Loss: 5.6243\n",
      "Epoch [1/4], Step [11325/64066], Loss: 5.8854\n",
      "Epoch [1/4], Step [11400/64066], Loss: 5.7163\n",
      "Epoch [1/4], Step [11475/64066], Loss: 5.7294\n",
      "Epoch [1/4], Step [11550/64066], Loss: 5.7398\n",
      "Epoch [1/4], Step [11625/64066], Loss: 6.0358\n",
      "Epoch [1/4], Step [11700/64066], Loss: 5.5082\n",
      "Epoch [1/4], Step [11775/64066], Loss: 5.7075\n",
      "Epoch [1/4], Step [11850/64066], Loss: 5.7110\n",
      "Epoch [1/4], Step [11925/64066], Loss: 5.6742\n",
      "Epoch [1/4], Step [12000/64066], Loss: 5.8080\n",
      "Epoch [1/4], Step [12075/64066], Loss: 5.5058\n",
      "Epoch [1/4], Step [12150/64066], Loss: 5.6693\n",
      "Epoch [1/4], Step [12225/64066], Loss: 5.5896\n",
      "Epoch [1/4], Step [12300/64066], Loss: 5.5958\n",
      "Epoch [1/4], Step [12375/64066], Loss: 5.7702\n",
      "Epoch [1/4], Step [12450/64066], Loss: 5.7706\n",
      "Epoch [1/4], Step [12525/64066], Loss: 5.8349\n",
      "Epoch [1/4], Step [12600/64066], Loss: 5.7059\n",
      "Epoch [1/4], Step [12675/64066], Loss: 5.5239\n",
      "Epoch [1/4], Step [12750/64066], Loss: 5.6078\n",
      "Epoch [1/4], Step [12825/64066], Loss: 5.5998\n",
      "Epoch [1/4], Step [12900/64066], Loss: 5.6253\n",
      "Epoch [1/4], Step [12975/64066], Loss: 5.6613\n",
      "Epoch [1/4], Step [13050/64066], Loss: 5.6631\n",
      "Epoch [1/4], Step [13125/64066], Loss: 5.8502\n",
      "Epoch [1/4], Step [13200/64066], Loss: 5.6918\n",
      "Epoch [1/4], Step [13275/64066], Loss: 5.8329\n",
      "Epoch [1/4], Step [13350/64066], Loss: 5.7228\n",
      "Epoch [1/4], Step [13425/64066], Loss: 5.7134\n",
      "Epoch [1/4], Step [13500/64066], Loss: 5.6719\n",
      "Epoch [1/4], Step [13575/64066], Loss: 5.8196\n",
      "Epoch [1/4], Step [13650/64066], Loss: 5.8580\n",
      "Epoch [1/4], Step [13725/64066], Loss: 5.6625\n",
      "Epoch [1/4], Step [13800/64066], Loss: 5.6270\n",
      "Epoch [1/4], Step [13875/64066], Loss: 5.4931\n",
      "Epoch [1/4], Step [13950/64066], Loss: 5.5794\n",
      "Epoch [1/4], Step [14025/64066], Loss: 5.5127\n",
      "Epoch [1/4], Step [14100/64066], Loss: 5.8496\n",
      "Epoch [1/4], Step [14175/64066], Loss: 5.5387\n",
      "Epoch [1/4], Step [14250/64066], Loss: 5.8570\n",
      "Epoch [1/4], Step [14325/64066], Loss: 5.5479\n",
      "Epoch [1/4], Step [14400/64066], Loss: 5.6889\n",
      "Epoch [1/4], Step [14475/64066], Loss: 5.6114\n",
      "Epoch [1/4], Step [14550/64066], Loss: 5.6128\n",
      "Epoch [1/4], Step [14625/64066], Loss: 5.5558\n",
      "Epoch [1/4], Step [14700/64066], Loss: 5.5908\n",
      "Epoch [1/4], Step [14775/64066], Loss: 5.6746\n",
      "Epoch [1/4], Step [14850/64066], Loss: 5.6911\n",
      "Epoch [1/4], Step [14925/64066], Loss: 5.3505\n",
      "Epoch [1/4], Step [15000/64066], Loss: 5.8362\n",
      "Epoch [1/4], Step [15075/64066], Loss: 5.5587\n",
      "Epoch [1/4], Step [15150/64066], Loss: 5.8298\n",
      "Epoch [1/4], Step [15225/64066], Loss: 5.6683\n",
      "Epoch [1/4], Step [15300/64066], Loss: 5.6894\n",
      "Epoch [1/4], Step [15375/64066], Loss: 5.5221\n",
      "Epoch [1/4], Step [15450/64066], Loss: 5.5692\n",
      "Epoch [1/4], Step [15525/64066], Loss: 5.9601\n",
      "Epoch [1/4], Step [15600/64066], Loss: 5.6410\n",
      "Epoch [1/4], Step [15675/64066], Loss: 5.4665\n",
      "Epoch [1/4], Step [15750/64066], Loss: 5.5497\n",
      "Epoch [1/4], Step [15825/64066], Loss: 5.4724\n",
      "Epoch [1/4], Step [15900/64066], Loss: 5.7265\n",
      "Epoch [1/4], Step [15975/64066], Loss: 5.6476\n",
      "Epoch [1/4], Step [16050/64066], Loss: 5.7298\n",
      "Epoch [1/4], Step [16125/64066], Loss: 5.4401\n",
      "Epoch [1/4], Step [16200/64066], Loss: 5.6058\n",
      "Epoch [1/4], Step [16275/64066], Loss: 5.6563\n",
      "Epoch [1/4], Step [16350/64066], Loss: 5.6066\n",
      "Epoch [1/4], Step [16425/64066], Loss: 5.7166\n",
      "Epoch [1/4], Step [16500/64066], Loss: 5.7636\n",
      "Epoch [1/4], Step [16575/64066], Loss: 5.6300\n",
      "Epoch [1/4], Step [16650/64066], Loss: 5.7110\n",
      "Epoch [1/4], Step [16725/64066], Loss: 5.7232\n",
      "Epoch [1/4], Step [16800/64066], Loss: 5.7074\n",
      "Epoch [1/4], Step [16875/64066], Loss: 5.7086\n",
      "Epoch [1/4], Step [16950/64066], Loss: 5.6298\n",
      "Epoch [1/4], Step [17025/64066], Loss: 5.4737\n",
      "Epoch [1/4], Step [17100/64066], Loss: 5.6690\n",
      "Epoch [1/4], Step [17175/64066], Loss: 5.6716\n",
      "Epoch [1/4], Step [17250/64066], Loss: 5.4930\n",
      "Epoch [1/4], Step [17325/64066], Loss: 5.6730\n",
      "Epoch [1/4], Step [17400/64066], Loss: 5.6655\n",
      "Epoch [1/4], Step [17475/64066], Loss: 5.3973\n",
      "Epoch [1/4], Step [17550/64066], Loss: 5.5451\n",
      "Epoch [1/4], Step [17625/64066], Loss: 5.6832\n",
      "Epoch [1/4], Step [17700/64066], Loss: 5.7890\n",
      "Epoch [1/4], Step [17775/64066], Loss: 5.7362\n",
      "Epoch [1/4], Step [17850/64066], Loss: 5.5139\n",
      "Epoch [1/4], Step [17925/64066], Loss: 5.5153\n",
      "Epoch [1/4], Step [18000/64066], Loss: 5.4601\n",
      "Epoch [1/4], Step [18075/64066], Loss: 5.6082\n",
      "Epoch [1/4], Step [18150/64066], Loss: 5.6041\n",
      "Epoch [1/4], Step [18225/64066], Loss: 5.5269\n",
      "Epoch [1/4], Step [18300/64066], Loss: 5.6583\n",
      "Epoch [1/4], Step [18375/64066], Loss: 5.6276\n",
      "Epoch [1/4], Step [18450/64066], Loss: 5.4823\n",
      "Epoch [1/4], Step [18525/64066], Loss: 5.5928\n",
      "Epoch [1/4], Step [18600/64066], Loss: 5.4599\n",
      "Epoch [1/4], Step [18675/64066], Loss: 5.5884\n",
      "Epoch [1/4], Step [18750/64066], Loss: 5.7311\n",
      "Epoch [1/4], Step [18825/64066], Loss: 5.7133\n",
      "Epoch [1/4], Step [18900/64066], Loss: 5.7607\n",
      "Epoch [1/4], Step [18975/64066], Loss: 5.6997\n",
      "Epoch [1/4], Step [19050/64066], Loss: 5.2754\n",
      "Epoch [1/4], Step [19125/64066], Loss: 5.4060\n",
      "Epoch [1/4], Step [19200/64066], Loss: 5.7296\n",
      "Epoch [1/4], Step [19275/64066], Loss: 5.4775\n",
      "Epoch [1/4], Step [19350/64066], Loss: 5.4418\n",
      "Epoch [1/4], Step [19425/64066], Loss: 5.6040\n",
      "Epoch [1/4], Step [19500/64066], Loss: 5.4463\n",
      "Epoch [1/4], Step [19575/64066], Loss: 5.6473\n",
      "Epoch [1/4], Step [19650/64066], Loss: 5.5367\n",
      "Epoch [1/4], Step [19725/64066], Loss: 5.7346\n",
      "Epoch [1/4], Step [19800/64066], Loss: 5.5347\n",
      "Epoch [1/4], Step [19875/64066], Loss: 5.4539\n",
      "Epoch [1/4], Step [19950/64066], Loss: 5.4554\n",
      "Validation perplexity: 232.4010020857418\n",
      "Epoch [1/4], Step [20025/64066], Loss: 5.5444\n",
      "Epoch [1/4], Step [20100/64066], Loss: 5.4952\n",
      "Epoch [1/4], Step [20175/64066], Loss: 5.5181\n",
      "Epoch [1/4], Step [20250/64066], Loss: 5.6618\n",
      "Epoch [1/4], Step [20325/64066], Loss: 5.7338\n",
      "Epoch [1/4], Step [20400/64066], Loss: 5.5472\n",
      "Epoch [1/4], Step [20475/64066], Loss: 5.5956\n",
      "Epoch [1/4], Step [20550/64066], Loss: 5.5330\n",
      "Epoch [1/4], Step [20625/64066], Loss: 5.5305\n",
      "Epoch [1/4], Step [20700/64066], Loss: 5.6304\n",
      "Epoch [1/4], Step [20775/64066], Loss: 5.8417\n",
      "Epoch [1/4], Step [20850/64066], Loss: 5.4310\n",
      "Epoch [1/4], Step [20925/64066], Loss: 5.5003\n",
      "Epoch [1/4], Step [21000/64066], Loss: 5.5852\n",
      "Epoch [1/4], Step [21075/64066], Loss: 5.7547\n",
      "Epoch [1/4], Step [21150/64066], Loss: 5.4689\n",
      "Epoch [1/4], Step [21225/64066], Loss: 5.4388\n",
      "Epoch [1/4], Step [21300/64066], Loss: 5.5427\n",
      "Epoch [1/4], Step [21375/64066], Loss: 5.6925\n",
      "Epoch [1/4], Step [21450/64066], Loss: 5.7784\n",
      "Epoch [1/4], Step [21525/64066], Loss: 5.6631\n",
      "Epoch [1/4], Step [21600/64066], Loss: 5.6830\n",
      "Epoch [1/4], Step [21675/64066], Loss: 5.5318\n",
      "Epoch [1/4], Step [21750/64066], Loss: 5.4457\n",
      "Epoch [1/4], Step [21825/64066], Loss: 5.6770\n",
      "Epoch [1/4], Step [21900/64066], Loss: 5.5661\n",
      "Epoch [1/4], Step [21975/64066], Loss: 5.3414\n",
      "Epoch [1/4], Step [22050/64066], Loss: 5.3133\n",
      "Epoch [1/4], Step [22125/64066], Loss: 5.4701\n",
      "Epoch [1/4], Step [22200/64066], Loss: 5.4915\n",
      "Epoch [1/4], Step [22275/64066], Loss: 5.7208\n",
      "Epoch [1/4], Step [22350/64066], Loss: 5.6488\n",
      "Epoch [1/4], Step [22425/64066], Loss: 5.5770\n",
      "Epoch [1/4], Step [22500/64066], Loss: 5.6444\n",
      "Epoch [1/4], Step [22575/64066], Loss: 5.2344\n",
      "Epoch [1/4], Step [22650/64066], Loss: 5.7054\n",
      "Epoch [1/4], Step [22725/64066], Loss: 5.3956\n",
      "Epoch [1/4], Step [22800/64066], Loss: 5.4405\n",
      "Epoch [1/4], Step [22875/64066], Loss: 5.6691\n",
      "Epoch [1/4], Step [22950/64066], Loss: 5.6525\n",
      "Epoch [1/4], Step [23025/64066], Loss: 5.6268\n",
      "Epoch [1/4], Step [23100/64066], Loss: 5.6523\n",
      "Epoch [1/4], Step [23175/64066], Loss: 5.6480\n",
      "Epoch [1/4], Step [23250/64066], Loss: 5.6715\n",
      "Epoch [1/4], Step [23325/64066], Loss: 5.4083\n",
      "Epoch [1/4], Step [23400/64066], Loss: 5.6729\n",
      "Epoch [1/4], Step [23475/64066], Loss: 5.6768\n",
      "Epoch [1/4], Step [23550/64066], Loss: 5.5575\n",
      "Epoch [1/4], Step [23625/64066], Loss: 5.6678\n",
      "Epoch [1/4], Step [23700/64066], Loss: 5.5288\n",
      "Epoch [1/4], Step [23775/64066], Loss: 5.6423\n",
      "Epoch [1/4], Step [23850/64066], Loss: 5.6161\n",
      "Epoch [1/4], Step [23925/64066], Loss: 5.6783\n",
      "Epoch [1/4], Step [24000/64066], Loss: 5.6113\n",
      "Epoch [1/4], Step [24075/64066], Loss: 5.4633\n",
      "Epoch [1/4], Step [24150/64066], Loss: 5.6021\n",
      "Epoch [1/4], Step [24225/64066], Loss: 5.7546\n",
      "Epoch [1/4], Step [24300/64066], Loss: 5.4275\n",
      "Epoch [1/4], Step [24375/64066], Loss: 5.6867\n",
      "Epoch [1/4], Step [24450/64066], Loss: 5.4840\n",
      "Epoch [1/4], Step [24525/64066], Loss: 5.4760\n",
      "Epoch [1/4], Step [24600/64066], Loss: 5.3397\n",
      "Epoch [1/4], Step [24675/64066], Loss: 5.6654\n",
      "Epoch [1/4], Step [24750/64066], Loss: 5.5919\n",
      "Epoch [1/4], Step [24825/64066], Loss: 5.6924\n",
      "Epoch [1/4], Step [24900/64066], Loss: 5.5682\n",
      "Epoch [1/4], Step [24975/64066], Loss: 5.4101\n",
      "Epoch [1/4], Step [25050/64066], Loss: 5.6867\n",
      "Epoch [1/4], Step [25125/64066], Loss: 5.6054\n",
      "Epoch [1/4], Step [25200/64066], Loss: 5.5272\n",
      "Epoch [1/4], Step [25275/64066], Loss: 5.5493\n",
      "Epoch [1/4], Step [25350/64066], Loss: 5.7549\n",
      "Epoch [1/4], Step [25425/64066], Loss: 5.6963\n",
      "Epoch [1/4], Step [25500/64066], Loss: 5.3173\n",
      "Epoch [1/4], Step [25575/64066], Loss: 5.3685\n",
      "Epoch [1/4], Step [25650/64066], Loss: 5.4798\n",
      "Epoch [1/4], Step [25725/64066], Loss: 5.5104\n",
      "Epoch [1/4], Step [25800/64066], Loss: 5.6744\n",
      "Epoch [1/4], Step [25875/64066], Loss: 5.4899\n",
      "Epoch [1/4], Step [25950/64066], Loss: 5.5524\n",
      "Epoch [1/4], Step [26025/64066], Loss: 5.4472\n",
      "Epoch [1/4], Step [26100/64066], Loss: 5.5913\n",
      "Epoch [1/4], Step [26175/64066], Loss: 5.4929\n",
      "Epoch [1/4], Step [26250/64066], Loss: 5.4546\n",
      "Epoch [1/4], Step [26325/64066], Loss: 5.5775\n",
      "Epoch [1/4], Step [26400/64066], Loss: 5.6054\n",
      "Epoch [1/4], Step [26475/64066], Loss: 5.4225\n",
      "Epoch [1/4], Step [26550/64066], Loss: 5.6290\n",
      "Epoch [1/4], Step [26625/64066], Loss: 5.5330\n",
      "Epoch [1/4], Step [26700/64066], Loss: 5.4128\n",
      "Epoch [1/4], Step [26775/64066], Loss: 5.3885\n",
      "Epoch [1/4], Step [26850/64066], Loss: 5.4152\n",
      "Epoch [1/4], Step [26925/64066], Loss: 5.5761\n",
      "Epoch [1/4], Step [27000/64066], Loss: 5.4809\n",
      "Epoch [1/4], Step [27075/64066], Loss: 5.3859\n",
      "Epoch [1/4], Step [27150/64066], Loss: 5.3150\n",
      "Epoch [1/4], Step [27225/64066], Loss: 5.3831\n",
      "Epoch [1/4], Step [27300/64066], Loss: 5.5149\n",
      "Epoch [1/4], Step [27375/64066], Loss: 5.3656\n",
      "Epoch [1/4], Step [27450/64066], Loss: 5.5179\n",
      "Epoch [1/4], Step [27525/64066], Loss: 5.6771\n",
      "Epoch [1/4], Step [27600/64066], Loss: 5.3151\n",
      "Epoch [1/4], Step [27675/64066], Loss: 5.4973\n",
      "Epoch [1/4], Step [27750/64066], Loss: 5.5469\n",
      "Epoch [1/4], Step [27825/64066], Loss: 5.5524\n",
      "Epoch [1/4], Step [27900/64066], Loss: 5.7003\n",
      "Epoch [1/4], Step [27975/64066], Loss: 5.5369\n",
      "Epoch [1/4], Step [28050/64066], Loss: 5.5707\n",
      "Epoch [1/4], Step [28125/64066], Loss: 5.5595\n",
      "Epoch [1/4], Step [28200/64066], Loss: 5.6736\n",
      "Epoch [1/4], Step [28275/64066], Loss: 5.4698\n",
      "Epoch [1/4], Step [28350/64066], Loss: 5.3722\n",
      "Epoch [1/4], Step [28425/64066], Loss: 5.5928\n",
      "Epoch [1/4], Step [28500/64066], Loss: 5.5328\n",
      "Epoch [1/4], Step [28575/64066], Loss: 5.4663\n",
      "Epoch [1/4], Step [28650/64066], Loss: 5.6252\n",
      "Epoch [1/4], Step [28725/64066], Loss: 5.3746\n",
      "Epoch [1/4], Step [28800/64066], Loss: 5.6490\n",
      "Epoch [1/4], Step [28875/64066], Loss: 5.4398\n",
      "Epoch [1/4], Step [28950/64066], Loss: 5.6400\n",
      "Epoch [1/4], Step [29025/64066], Loss: 5.4815\n",
      "Epoch [1/4], Step [29100/64066], Loss: 5.3408\n",
      "Epoch [1/4], Step [29175/64066], Loss: 5.5833\n",
      "Epoch [1/4], Step [29250/64066], Loss: 5.3282\n",
      "Epoch [1/4], Step [29325/64066], Loss: 5.4432\n",
      "Epoch [1/4], Step [29400/64066], Loss: 5.5944\n",
      "Epoch [1/4], Step [29475/64066], Loss: 5.4160\n",
      "Epoch [1/4], Step [29550/64066], Loss: 5.6694\n",
      "Epoch [1/4], Step [29625/64066], Loss: 5.5477\n",
      "Epoch [1/4], Step [29700/64066], Loss: 5.6829\n",
      "Epoch [1/4], Step [29775/64066], Loss: 5.4212\n",
      "Epoch [1/4], Step [29850/64066], Loss: 5.5886\n",
      "Epoch [1/4], Step [29925/64066], Loss: 5.5483\n",
      "Epoch [1/4], Step [30000/64066], Loss: 5.5611\n",
      "Validation perplexity: 211.0770002231256\n",
      "Epoch [1/4], Step [30075/64066], Loss: 5.4775\n",
      "Epoch [1/4], Step [30150/64066], Loss: 5.5544\n",
      "Epoch [1/4], Step [30225/64066], Loss: 5.5214\n",
      "Epoch [1/4], Step [30300/64066], Loss: 5.4869\n",
      "Epoch [1/4], Step [30375/64066], Loss: 5.4826\n",
      "Epoch [1/4], Step [30450/64066], Loss: 5.4402\n",
      "Epoch [1/4], Step [30525/64066], Loss: 5.4500\n",
      "Epoch [1/4], Step [30600/64066], Loss: 5.4354\n",
      "Epoch [1/4], Step [30675/64066], Loss: 5.4043\n",
      "Epoch [1/4], Step [30750/64066], Loss: 5.5786\n",
      "Epoch [1/4], Step [30825/64066], Loss: 5.4677\n",
      "Epoch [1/4], Step [30900/64066], Loss: 5.5421\n",
      "Epoch [1/4], Step [30975/64066], Loss: 5.4919\n",
      "Epoch [1/4], Step [31050/64066], Loss: 5.5012\n",
      "Epoch [1/4], Step [31125/64066], Loss: 5.4239\n",
      "Epoch [1/4], Step [31200/64066], Loss: 5.2464\n",
      "Epoch [1/4], Step [31275/64066], Loss: 5.6815\n",
      "Epoch [1/4], Step [31350/64066], Loss: 5.5347\n",
      "Epoch [1/4], Step [31425/64066], Loss: 5.5465\n",
      "Epoch [1/4], Step [31500/64066], Loss: 5.5257\n",
      "Epoch [1/4], Step [31575/64066], Loss: 5.6477\n",
      "Epoch [1/4], Step [31650/64066], Loss: 5.7633\n",
      "Epoch [1/4], Step [31725/64066], Loss: 5.3764\n",
      "Epoch [1/4], Step [31800/64066], Loss: 5.4027\n",
      "Epoch [1/4], Step [31875/64066], Loss: 5.5447\n",
      "Epoch [1/4], Step [31950/64066], Loss: 5.4474\n",
      "Epoch [1/4], Step [32025/64066], Loss: 5.5268\n",
      "Epoch [1/4], Step [32100/64066], Loss: 5.5175\n",
      "Epoch [1/4], Step [32175/64066], Loss: 5.5005\n",
      "Epoch [1/4], Step [32250/64066], Loss: 5.6156\n",
      "Epoch [1/4], Step [32325/64066], Loss: 5.6690\n",
      "Epoch [1/4], Step [32400/64066], Loss: 5.4618\n",
      "Epoch [1/4], Step [32475/64066], Loss: 5.5481\n",
      "Epoch [1/4], Step [32550/64066], Loss: 5.5176\n",
      "Epoch [1/4], Step [32625/64066], Loss: 5.6554\n",
      "Epoch [1/4], Step [32700/64066], Loss: 5.6997\n",
      "Epoch [1/4], Step [32775/64066], Loss: 5.5173\n",
      "Epoch [1/4], Step [32850/64066], Loss: 5.6226\n",
      "Epoch [1/4], Step [32925/64066], Loss: 5.3393\n",
      "Epoch [1/4], Step [33000/64066], Loss: 5.4221\n",
      "Epoch [1/4], Step [33075/64066], Loss: 5.3651\n",
      "Epoch [1/4], Step [33150/64066], Loss: 5.4762\n",
      "Epoch [1/4], Step [33225/64066], Loss: 5.5522\n",
      "Epoch [1/4], Step [33300/64066], Loss: 5.5465\n",
      "Epoch [1/4], Step [33375/64066], Loss: 5.4540\n",
      "Epoch [1/4], Step [33450/64066], Loss: 5.4128\n",
      "Epoch [1/4], Step [33525/64066], Loss: 5.3819\n",
      "Epoch [1/4], Step [33600/64066], Loss: 5.3540\n",
      "Epoch [1/4], Step [33675/64066], Loss: 5.5614\n",
      "Epoch [1/4], Step [33750/64066], Loss: 5.3043\n",
      "Epoch [1/4], Step [33825/64066], Loss: 5.5710\n",
      "Epoch [1/4], Step [33900/64066], Loss: 5.6016\n",
      "Epoch [1/4], Step [33975/64066], Loss: 5.3005\n",
      "Epoch [1/4], Step [34050/64066], Loss: 5.5087\n",
      "Epoch [1/4], Step [34125/64066], Loss: 5.3189\n",
      "Epoch [1/4], Step [34200/64066], Loss: 5.6404\n",
      "Epoch [1/4], Step [34275/64066], Loss: 5.3793\n",
      "Epoch [1/4], Step [34350/64066], Loss: 5.3407\n",
      "Epoch [1/4], Step [34425/64066], Loss: 5.3274\n",
      "Epoch [1/4], Step [34500/64066], Loss: 5.5395\n",
      "Epoch [1/4], Step [34575/64066], Loss: 5.3735\n",
      "Epoch [1/4], Step [34650/64066], Loss: 5.4869\n",
      "Epoch [1/4], Step [34725/64066], Loss: 5.4971\n",
      "Epoch [1/4], Step [34800/64066], Loss: 5.4423\n",
      "Epoch [1/4], Step [34875/64066], Loss: 5.5195\n",
      "Epoch [1/4], Step [34950/64066], Loss: 5.3724\n",
      "Epoch [1/4], Step [35025/64066], Loss: 5.4469\n",
      "Epoch [1/4], Step [35100/64066], Loss: 5.4448\n",
      "Epoch [1/4], Step [35175/64066], Loss: 5.5705\n",
      "Epoch [1/4], Step [35250/64066], Loss: 5.3935\n",
      "Epoch [1/4], Step [35325/64066], Loss: 5.3954\n",
      "Epoch [1/4], Step [35400/64066], Loss: 5.5555\n",
      "Epoch [1/4], Step [35475/64066], Loss: 5.5942\n",
      "Epoch [1/4], Step [35550/64066], Loss: 5.4558\n",
      "Epoch [1/4], Step [35625/64066], Loss: 5.4650\n",
      "Epoch [1/4], Step [35700/64066], Loss: 5.3827\n",
      "Epoch [1/4], Step [35775/64066], Loss: 5.4289\n",
      "Epoch [1/4], Step [35850/64066], Loss: 5.2655\n",
      "Epoch [1/4], Step [35925/64066], Loss: 5.4605\n",
      "Epoch [1/4], Step [36000/64066], Loss: 5.4354\n",
      "Epoch [1/4], Step [36075/64066], Loss: 5.3985\n",
      "Epoch [1/4], Step [36150/64066], Loss: 5.3799\n",
      "Epoch [1/4], Step [36225/64066], Loss: 5.5224\n",
      "Epoch [1/4], Step [36300/64066], Loss: 5.4510\n",
      "Epoch [1/4], Step [36375/64066], Loss: 5.2379\n",
      "Epoch [1/4], Step [36450/64066], Loss: 5.3105\n",
      "Epoch [1/4], Step [36525/64066], Loss: 5.4286\n",
      "Epoch [1/4], Step [36600/64066], Loss: 5.4922\n",
      "Epoch [1/4], Step [36675/64066], Loss: 5.4551\n",
      "Epoch [1/4], Step [36750/64066], Loss: 5.4034\n",
      "Epoch [1/4], Step [36825/64066], Loss: 5.4389\n",
      "Epoch [1/4], Step [36900/64066], Loss: 5.6491\n",
      "Epoch [1/4], Step [36975/64066], Loss: 5.4316\n",
      "Epoch [1/4], Step [37050/64066], Loss: 5.3468\n",
      "Epoch [1/4], Step [37125/64066], Loss: 5.4624\n",
      "Epoch [1/4], Step [37200/64066], Loss: 5.3848\n",
      "Epoch [1/4], Step [37275/64066], Loss: 5.4192\n",
      "Epoch [1/4], Step [37350/64066], Loss: 5.5633\n",
      "Epoch [1/4], Step [37425/64066], Loss: 5.4411\n",
      "Epoch [1/4], Step [37500/64066], Loss: 5.4683\n",
      "Epoch [1/4], Step [37575/64066], Loss: 5.5780\n",
      "Epoch [1/4], Step [37650/64066], Loss: 5.2940\n",
      "Epoch [1/4], Step [37725/64066], Loss: 5.5479\n",
      "Epoch [1/4], Step [37800/64066], Loss: 5.3923\n",
      "Epoch [1/4], Step [37875/64066], Loss: 5.2448\n",
      "Epoch [1/4], Step [37950/64066], Loss: 5.4407\n",
      "Epoch [1/4], Step [38025/64066], Loss: 5.3786\n",
      "Epoch [1/4], Step [38100/64066], Loss: 5.3296\n",
      "Epoch [1/4], Step [38175/64066], Loss: 5.5859\n",
      "Epoch [1/4], Step [38250/64066], Loss: 5.4437\n",
      "Epoch [1/4], Step [38325/64066], Loss: 5.4494\n",
      "Epoch [1/4], Step [38400/64066], Loss: 5.2345\n",
      "Epoch [1/4], Step [38475/64066], Loss: 5.5666\n",
      "Epoch [1/4], Step [38550/64066], Loss: 5.6197\n",
      "Epoch [1/4], Step [38625/64066], Loss: 5.5529\n",
      "Epoch [1/4], Step [38700/64066], Loss: 5.4920\n",
      "Epoch [1/4], Step [38775/64066], Loss: 5.4237\n",
      "Epoch [1/4], Step [38850/64066], Loss: 5.3103\n",
      "Epoch [1/4], Step [38925/64066], Loss: 5.5156\n",
      "Epoch [1/4], Step [39000/64066], Loss: 5.3404\n",
      "Epoch [1/4], Step [39075/64066], Loss: 5.3778\n",
      "Epoch [1/4], Step [39150/64066], Loss: 5.4306\n",
      "Epoch [1/4], Step [39225/64066], Loss: 5.3949\n",
      "Epoch [1/4], Step [39300/64066], Loss: 5.3886\n",
      "Epoch [1/4], Step [39375/64066], Loss: 5.4809\n",
      "Epoch [1/4], Step [39450/64066], Loss: 5.3550\n",
      "Epoch [1/4], Step [39525/64066], Loss: 5.4188\n",
      "Epoch [1/4], Step [39600/64066], Loss: 5.3267\n",
      "Epoch [1/4], Step [39675/64066], Loss: 5.4670\n",
      "Epoch [1/4], Step [39750/64066], Loss: 5.4528\n",
      "Epoch [1/4], Step [39825/64066], Loss: 5.3864\n",
      "Epoch [1/4], Step [39900/64066], Loss: 5.6920\n",
      "Epoch [1/4], Step [39975/64066], Loss: 5.0952\n",
      "Validation perplexity: 198.279867119915\n",
      "Epoch [1/4], Step [40050/64066], Loss: 5.5772\n",
      "Epoch [1/4], Step [40125/64066], Loss: 5.6228\n",
      "Epoch [1/4], Step [40200/64066], Loss: 5.6475\n",
      "Epoch [1/4], Step [40275/64066], Loss: 5.5427\n",
      "Epoch [1/4], Step [40350/64066], Loss: 5.4989\n",
      "Epoch [1/4], Step [40425/64066], Loss: 5.6327\n",
      "Epoch [1/4], Step [40500/64066], Loss: 5.4606\n",
      "Epoch [1/4], Step [40575/64066], Loss: 5.5531\n",
      "Epoch [1/4], Step [40650/64066], Loss: 5.3021\n",
      "Epoch [1/4], Step [40725/64066], Loss: 5.4222\n",
      "Epoch [1/4], Step [40800/64066], Loss: 5.4861\n",
      "Epoch [1/4], Step [40875/64066], Loss: 5.4422\n",
      "Epoch [1/4], Step [40950/64066], Loss: 5.2834\n",
      "Epoch [1/4], Step [41025/64066], Loss: 5.5292\n",
      "Epoch [1/4], Step [41100/64066], Loss: 5.3754\n",
      "Epoch [1/4], Step [41175/64066], Loss: 5.3883\n",
      "Epoch [1/4], Step [41250/64066], Loss: 5.4069\n",
      "Epoch [1/4], Step [41325/64066], Loss: 5.7087\n",
      "Epoch [1/4], Step [41400/64066], Loss: 5.7404\n",
      "Epoch [1/4], Step [41475/64066], Loss: 5.4730\n",
      "Epoch [1/4], Step [41550/64066], Loss: 5.3404\n",
      "Epoch [1/4], Step [41625/64066], Loss: 5.4375\n",
      "Epoch [1/4], Step [41700/64066], Loss: 5.3647\n",
      "Epoch [1/4], Step [41775/64066], Loss: 5.5032\n",
      "Epoch [1/4], Step [41850/64066], Loss: 5.5381\n",
      "Epoch [1/4], Step [41925/64066], Loss: 5.3369\n",
      "Epoch [1/4], Step [42000/64066], Loss: 5.3512\n",
      "Epoch [1/4], Step [42075/64066], Loss: 5.4177\n",
      "Epoch [1/4], Step [42150/64066], Loss: 5.4017\n",
      "Epoch [1/4], Step [42225/64066], Loss: 5.3594\n",
      "Epoch [1/4], Step [42300/64066], Loss: 5.2593\n",
      "Epoch [1/4], Step [42375/64066], Loss: 5.4256\n",
      "Epoch [1/4], Step [42450/64066], Loss: 5.5901\n",
      "Epoch [1/4], Step [42525/64066], Loss: 5.3922\n",
      "Epoch [1/4], Step [42600/64066], Loss: 5.3952\n",
      "Epoch [1/4], Step [42675/64066], Loss: 5.5133\n",
      "Epoch [1/4], Step [42750/64066], Loss: 5.3041\n",
      "Epoch [1/4], Step [42825/64066], Loss: 5.5196\n",
      "Epoch [1/4], Step [42900/64066], Loss: 5.3729\n",
      "Epoch [1/4], Step [42975/64066], Loss: 5.5630\n",
      "Epoch [1/4], Step [43050/64066], Loss: 5.3987\n",
      "Epoch [1/4], Step [43125/64066], Loss: 5.5457\n",
      "Epoch [1/4], Step [43200/64066], Loss: 5.4206\n",
      "Epoch [1/4], Step [43275/64066], Loss: 5.4392\n",
      "Epoch [1/4], Step [43350/64066], Loss: 5.5563\n",
      "Epoch [1/4], Step [43425/64066], Loss: 5.4825\n",
      "Epoch [1/4], Step [43500/64066], Loss: 5.4780\n",
      "Epoch [1/4], Step [43575/64066], Loss: 5.4305\n",
      "Epoch [1/4], Step [43650/64066], Loss: 5.4225\n",
      "Epoch [1/4], Step [43725/64066], Loss: 5.4634\n",
      "Epoch [1/4], Step [43800/64066], Loss: 5.4871\n",
      "Epoch [1/4], Step [43875/64066], Loss: 5.4533\n",
      "Epoch [1/4], Step [43950/64066], Loss: 5.2737\n",
      "Epoch [1/4], Step [44025/64066], Loss: 5.4428\n",
      "Epoch [1/4], Step [44100/64066], Loss: 5.3292\n",
      "Epoch [1/4], Step [44175/64066], Loss: 5.5135\n",
      "Epoch [1/4], Step [44250/64066], Loss: 5.6095\n",
      "Epoch [1/4], Step [44325/64066], Loss: 5.3589\n",
      "Epoch [1/4], Step [44400/64066], Loss: 5.4928\n",
      "Epoch [1/4], Step [44475/64066], Loss: 5.4045\n",
      "Epoch [1/4], Step [44550/64066], Loss: 5.6074\n",
      "Epoch [1/4], Step [44625/64066], Loss: 5.5246\n",
      "Epoch [1/4], Step [44700/64066], Loss: 5.1990\n",
      "Epoch [1/4], Step [44775/64066], Loss: 5.5836\n",
      "Epoch [1/4], Step [44850/64066], Loss: 5.4431\n",
      "Epoch [1/4], Step [44925/64066], Loss: 5.3874\n",
      "Epoch [1/4], Step [45000/64066], Loss: 5.4743\n",
      "Epoch [1/4], Step [45075/64066], Loss: 5.2498\n",
      "Epoch [1/4], Step [45150/64066], Loss: 5.3438\n",
      "Epoch [1/4], Step [45225/64066], Loss: 5.4314\n",
      "Epoch [1/4], Step [45300/64066], Loss: 5.2656\n",
      "Epoch [1/4], Step [45375/64066], Loss: 5.2416\n",
      "Epoch [1/4], Step [45450/64066], Loss: 5.1034\n",
      "Epoch [1/4], Step [45525/64066], Loss: 5.4039\n",
      "Epoch [1/4], Step [45600/64066], Loss: 5.3908\n",
      "Epoch [1/4], Step [45675/64066], Loss: 5.4679\n",
      "Epoch [1/4], Step [45750/64066], Loss: 5.3959\n",
      "Epoch [1/4], Step [45825/64066], Loss: 5.4831\n",
      "Epoch [1/4], Step [45900/64066], Loss: 5.3593\n",
      "Epoch [1/4], Step [45975/64066], Loss: 5.5716\n",
      "Epoch [1/4], Step [46050/64066], Loss: 5.3128\n",
      "Epoch [1/4], Step [46125/64066], Loss: 5.3595\n",
      "Epoch [1/4], Step [46200/64066], Loss: 5.2765\n",
      "Epoch [1/4], Step [46275/64066], Loss: 5.4773\n",
      "Epoch [1/4], Step [46350/64066], Loss: 5.3135\n",
      "Epoch [1/4], Step [46425/64066], Loss: 5.2209\n",
      "Epoch [1/4], Step [46500/64066], Loss: 5.3197\n",
      "Epoch [1/4], Step [46575/64066], Loss: 5.4206\n",
      "Epoch [1/4], Step [46650/64066], Loss: 5.4648\n",
      "Epoch [1/4], Step [46725/64066], Loss: 5.3192\n",
      "Epoch [1/4], Step [46800/64066], Loss: 5.3664\n",
      "Epoch [1/4], Step [46875/64066], Loss: 5.5463\n",
      "Epoch [1/4], Step [46950/64066], Loss: 5.3156\n",
      "Epoch [1/4], Step [47025/64066], Loss: 5.3563\n",
      "Epoch [1/4], Step [47100/64066], Loss: 5.5330\n",
      "Epoch [1/4], Step [47175/64066], Loss: 5.5002\n",
      "Epoch [1/4], Step [47250/64066], Loss: 5.3835\n",
      "Epoch [1/4], Step [47325/64066], Loss: 5.4489\n",
      "Epoch [1/4], Step [47400/64066], Loss: 5.5076\n",
      "Epoch [1/4], Step [47475/64066], Loss: 5.3153\n",
      "Epoch [1/4], Step [47550/64066], Loss: 5.5185\n",
      "Epoch [1/4], Step [47625/64066], Loss: 5.3744\n",
      "Epoch [1/4], Step [47700/64066], Loss: 5.2954\n",
      "Epoch [1/4], Step [47775/64066], Loss: 5.4172\n",
      "Epoch [1/4], Step [47850/64066], Loss: 5.5755\n",
      "Epoch [1/4], Step [47925/64066], Loss: 5.4628\n",
      "Epoch [1/4], Step [48000/64066], Loss: 5.2098\n",
      "Epoch [1/4], Step [48075/64066], Loss: 5.3628\n",
      "Epoch [1/4], Step [48150/64066], Loss: 5.2233\n",
      "Epoch [1/4], Step [48225/64066], Loss: 5.6154\n",
      "Epoch [1/4], Step [48300/64066], Loss: 5.4027\n",
      "Epoch [1/4], Step [48375/64066], Loss: 5.3480\n",
      "Epoch [1/4], Step [48450/64066], Loss: 5.6806\n",
      "Epoch [1/4], Step [48525/64066], Loss: 5.3577\n",
      "Epoch [1/4], Step [48600/64066], Loss: 5.2561\n",
      "Epoch [1/4], Step [48675/64066], Loss: 5.2431\n",
      "Epoch [1/4], Step [48750/64066], Loss: 5.3636\n",
      "Epoch [1/4], Step [48825/64066], Loss: 5.4861\n",
      "Epoch [1/4], Step [48900/64066], Loss: 5.3937\n",
      "Epoch [1/4], Step [48975/64066], Loss: 5.3040\n",
      "Epoch [1/4], Step [49050/64066], Loss: 5.5203\n",
      "Epoch [1/4], Step [49125/64066], Loss: 5.5264\n",
      "Epoch [1/4], Step [49200/64066], Loss: 5.4240\n",
      "Epoch [1/4], Step [49275/64066], Loss: 5.3971\n",
      "Epoch [1/4], Step [49350/64066], Loss: 5.5512\n",
      "Epoch [1/4], Step [49425/64066], Loss: 5.2024\n",
      "Epoch [1/4], Step [49500/64066], Loss: 5.2945\n",
      "Epoch [1/4], Step [49575/64066], Loss: 5.3109\n",
      "Epoch [1/4], Step [49650/64066], Loss: 5.3953\n",
      "Epoch [1/4], Step [49725/64066], Loss: 5.4346\n",
      "Epoch [1/4], Step [49800/64066], Loss: 5.3199\n",
      "Epoch [1/4], Step [49875/64066], Loss: 5.5797\n",
      "Epoch [1/4], Step [49950/64066], Loss: 5.4138\n",
      "Validation perplexity: 189.13021187992936\n",
      "Epoch [1/4], Step [50025/64066], Loss: 5.3486\n",
      "Epoch [1/4], Step [50100/64066], Loss: 5.3352\n",
      "Epoch [1/4], Step [50175/64066], Loss: 5.3608\n",
      "Epoch [1/4], Step [50250/64066], Loss: 5.4690\n",
      "Epoch [1/4], Step [50325/64066], Loss: 5.6018\n",
      "Epoch [1/4], Step [50400/64066], Loss: 5.6037\n",
      "Epoch [1/4], Step [50475/64066], Loss: 5.1513\n",
      "Epoch [1/4], Step [50550/64066], Loss: 5.3735\n",
      "Epoch [1/4], Step [50625/64066], Loss: 5.4135\n",
      "Epoch [1/4], Step [50700/64066], Loss: 5.4551\n",
      "Epoch [1/4], Step [50775/64066], Loss: 5.5663\n",
      "Epoch [1/4], Step [50850/64066], Loss: 5.3109\n",
      "Epoch [1/4], Step [50925/64066], Loss: 5.6172\n",
      "Epoch [1/4], Step [51000/64066], Loss: 5.4575\n",
      "Epoch [1/4], Step [51075/64066], Loss: 5.4060\n",
      "Epoch [1/4], Step [51150/64066], Loss: 5.4927\n",
      "Epoch [1/4], Step [51225/64066], Loss: 5.1827\n",
      "Epoch [1/4], Step [51300/64066], Loss: 5.4036\n",
      "Epoch [1/4], Step [51375/64066], Loss: 5.2533\n",
      "Epoch [1/4], Step [51450/64066], Loss: 5.3898\n",
      "Epoch [1/4], Step [51525/64066], Loss: 5.4861\n",
      "Epoch [1/4], Step [51600/64066], Loss: 5.5240\n",
      "Epoch [1/4], Step [51675/64066], Loss: 5.3693\n",
      "Epoch [1/4], Step [51750/64066], Loss: 5.1778\n",
      "Epoch [1/4], Step [51825/64066], Loss: 5.3331\n",
      "Epoch [1/4], Step [51900/64066], Loss: 5.2718\n",
      "Epoch [1/4], Step [51975/64066], Loss: 5.3382\n",
      "Epoch [1/4], Step [52050/64066], Loss: 5.5982\n",
      "Epoch [1/4], Step [52125/64066], Loss: 5.2954\n",
      "Epoch [1/4], Step [52200/64066], Loss: 5.4531\n",
      "Epoch [1/4], Step [52275/64066], Loss: 5.3397\n",
      "Epoch [1/4], Step [52350/64066], Loss: 5.3333\n",
      "Epoch [1/4], Step [52425/64066], Loss: 5.4606\n",
      "Epoch [1/4], Step [52500/64066], Loss: 5.3148\n",
      "Epoch [1/4], Step [52575/64066], Loss: 5.5751\n",
      "Epoch [1/4], Step [52650/64066], Loss: 5.3878\n",
      "Epoch [1/4], Step [52725/64066], Loss: 5.3151\n",
      "Epoch [1/4], Step [52800/64066], Loss: 5.2723\n",
      "Epoch [1/4], Step [52875/64066], Loss: 5.3031\n",
      "Epoch [1/4], Step [52950/64066], Loss: 5.2141\n",
      "Epoch [1/4], Step [53025/64066], Loss: 5.3640\n",
      "Epoch [1/4], Step [53100/64066], Loss: 5.2338\n",
      "Epoch [1/4], Step [53175/64066], Loss: 5.2082\n",
      "Epoch [1/4], Step [53250/64066], Loss: 5.4732\n",
      "Epoch [1/4], Step [53325/64066], Loss: 5.3988\n",
      "Epoch [1/4], Step [53400/64066], Loss: 5.2889\n",
      "Epoch [1/4], Step [53475/64066], Loss: 5.5066\n",
      "Epoch [1/4], Step [53550/64066], Loss: 5.2527\n",
      "Epoch [1/4], Step [53625/64066], Loss: 5.2013\n",
      "Epoch [1/4], Step [53700/64066], Loss: 5.2795\n",
      "Epoch [1/4], Step [53775/64066], Loss: 5.5358\n",
      "Epoch [1/4], Step [53850/64066], Loss: 5.4902\n",
      "Epoch [1/4], Step [53925/64066], Loss: 5.4956\n",
      "Epoch [1/4], Step [54000/64066], Loss: 5.3901\n",
      "Epoch [1/4], Step [54075/64066], Loss: 5.1529\n",
      "Epoch [1/4], Step [54150/64066], Loss: 5.2388\n",
      "Epoch [1/4], Step [54225/64066], Loss: 5.4178\n",
      "Epoch [1/4], Step [54300/64066], Loss: 5.3777\n",
      "Epoch [1/4], Step [54375/64066], Loss: 5.4826\n",
      "Epoch [1/4], Step [54450/64066], Loss: 5.2858\n",
      "Epoch [1/4], Step [54525/64066], Loss: 5.5491\n",
      "Epoch [1/4], Step [54600/64066], Loss: 5.4746\n",
      "Epoch [1/4], Step [54675/64066], Loss: 5.3120\n",
      "Epoch [1/4], Step [54750/64066], Loss: 5.1993\n",
      "Epoch [1/4], Step [54825/64066], Loss: 5.2875\n",
      "Epoch [1/4], Step [54900/64066], Loss: 5.4320\n",
      "Epoch [1/4], Step [54975/64066], Loss: 5.4053\n",
      "Epoch [1/4], Step [55050/64066], Loss: 5.4505\n",
      "Epoch [1/4], Step [55125/64066], Loss: 5.2919\n",
      "Epoch [1/4], Step [55200/64066], Loss: 5.3642\n",
      "Epoch [1/4], Step [55275/64066], Loss: 5.3659\n",
      "Epoch [1/4], Step [55350/64066], Loss: 5.4124\n",
      "Epoch [1/4], Step [55425/64066], Loss: 5.4984\n",
      "Epoch [1/4], Step [55500/64066], Loss: 5.4852\n",
      "Epoch [1/4], Step [55575/64066], Loss: 5.4506\n",
      "Epoch [1/4], Step [55650/64066], Loss: 5.4693\n",
      "Epoch [1/4], Step [55725/64066], Loss: 5.4087\n",
      "Epoch [1/4], Step [55800/64066], Loss: 5.4700\n",
      "Epoch [1/4], Step [55875/64066], Loss: 5.2718\n",
      "Epoch [1/4], Step [55950/64066], Loss: 5.4946\n",
      "Epoch [1/4], Step [56025/64066], Loss: 5.5058\n",
      "Epoch [1/4], Step [56100/64066], Loss: 5.4772\n",
      "Epoch [1/4], Step [56175/64066], Loss: 5.3646\n",
      "Epoch [1/4], Step [56250/64066], Loss: 5.4507\n",
      "Epoch [1/4], Step [56325/64066], Loss: 5.3226\n",
      "Epoch [1/4], Step [56400/64066], Loss: 5.2781\n",
      "Epoch [1/4], Step [56475/64066], Loss: 5.2572\n",
      "Epoch [1/4], Step [56550/64066], Loss: 5.1849\n",
      "Epoch [1/4], Step [56625/64066], Loss: 5.5005\n",
      "Epoch [1/4], Step [56700/64066], Loss: 5.5229\n",
      "Epoch [1/4], Step [56775/64066], Loss: 5.3998\n",
      "Epoch [1/4], Step [56850/64066], Loss: 5.3969\n",
      "Epoch [1/4], Step [56925/64066], Loss: 5.3512\n",
      "Epoch [1/4], Step [57000/64066], Loss: 5.3570\n",
      "Epoch [1/4], Step [57075/64066], Loss: 5.4404\n",
      "Epoch [1/4], Step [57150/64066], Loss: 5.1823\n",
      "Epoch [1/4], Step [57225/64066], Loss: 5.3998\n",
      "Epoch [1/4], Step [57300/64066], Loss: 5.4231\n",
      "Epoch [1/4], Step [57375/64066], Loss: 5.3415\n",
      "Epoch [1/4], Step [57450/64066], Loss: 5.3266\n",
      "Epoch [1/4], Step [57525/64066], Loss: 5.2727\n",
      "Epoch [1/4], Step [57600/64066], Loss: 5.4078\n",
      "Epoch [1/4], Step [57675/64066], Loss: 5.3628\n",
      "Epoch [1/4], Step [57750/64066], Loss: 5.3674\n",
      "Epoch [1/4], Step [57825/64066], Loss: 5.3986\n",
      "Epoch [1/4], Step [57900/64066], Loss: 5.4908\n",
      "Epoch [1/4], Step [57975/64066], Loss: 5.4391\n",
      "Epoch [1/4], Step [58050/64066], Loss: 5.3339\n",
      "Epoch [1/4], Step [58125/64066], Loss: 5.3697\n",
      "Epoch [1/4], Step [58200/64066], Loss: 5.4769\n",
      "Epoch [1/4], Step [58275/64066], Loss: 5.4544\n",
      "Epoch [1/4], Step [58350/64066], Loss: 5.4881\n",
      "Epoch [1/4], Step [58425/64066], Loss: 5.4084\n",
      "Epoch [1/4], Step [58500/64066], Loss: 5.4996\n",
      "Epoch [1/4], Step [58575/64066], Loss: 5.3812\n",
      "Epoch [1/4], Step [58650/64066], Loss: 5.3621\n",
      "Epoch [1/4], Step [58725/64066], Loss: 5.2615\n",
      "Epoch [1/4], Step [58800/64066], Loss: 5.3533\n",
      "Epoch [1/4], Step [58875/64066], Loss: 5.2642\n",
      "Epoch [1/4], Step [58950/64066], Loss: 5.5353\n",
      "Epoch [1/4], Step [59025/64066], Loss: 5.4239\n",
      "Epoch [1/4], Step [59100/64066], Loss: 5.2717\n",
      "Epoch [1/4], Step [59175/64066], Loss: 5.3253\n",
      "Epoch [1/4], Step [59250/64066], Loss: 5.4173\n",
      "Epoch [1/4], Step [59325/64066], Loss: 5.4591\n",
      "Epoch [1/4], Step [59400/64066], Loss: 5.3528\n",
      "Epoch [1/4], Step [59475/64066], Loss: 5.3202\n",
      "Epoch [1/4], Step [59550/64066], Loss: 5.4271\n",
      "Epoch [1/4], Step [59625/64066], Loss: 5.4031\n",
      "Epoch [1/4], Step [59700/64066], Loss: 5.4038\n",
      "Epoch [1/4], Step [59775/64066], Loss: 5.3674\n",
      "Epoch [1/4], Step [59850/64066], Loss: 5.4509\n",
      "Epoch [1/4], Step [59925/64066], Loss: 5.4252\n",
      "Epoch [1/4], Step [60000/64066], Loss: 5.3741\n",
      "Validation perplexity: 182.6968025477244\n",
      "Epoch [1/4], Step [60075/64066], Loss: 5.5127\n",
      "Epoch [1/4], Step [60150/64066], Loss: 5.4215\n",
      "Epoch [1/4], Step [60225/64066], Loss: 5.1873\n",
      "Epoch [1/4], Step [60300/64066], Loss: 5.5568\n",
      "Epoch [1/4], Step [60375/64066], Loss: 5.4089\n",
      "Epoch [1/4], Step [60450/64066], Loss: 5.3120\n",
      "Epoch [1/4], Step [60525/64066], Loss: 5.3797\n",
      "Epoch [1/4], Step [60600/64066], Loss: 5.3899\n",
      "Epoch [1/4], Step [60675/64066], Loss: 5.4226\n",
      "Epoch [1/4], Step [60750/64066], Loss: 5.2709\n",
      "Epoch [1/4], Step [60825/64066], Loss: 5.3383\n",
      "Epoch [1/4], Step [60900/64066], Loss: 5.3335\n",
      "Epoch [1/4], Step [60975/64066], Loss: 5.3280\n",
      "Epoch [1/4], Step [61050/64066], Loss: 5.4240\n",
      "Epoch [1/4], Step [61125/64066], Loss: 5.4323\n",
      "Epoch [1/4], Step [61200/64066], Loss: 5.5539\n",
      "Epoch [1/4], Step [61275/64066], Loss: 5.4759\n",
      "Epoch [1/4], Step [61350/64066], Loss: 5.3780\n",
      "Epoch [1/4], Step [61425/64066], Loss: 5.3053\n",
      "Epoch [1/4], Step [61500/64066], Loss: 5.3859\n",
      "Epoch [1/4], Step [61575/64066], Loss: 5.3351\n",
      "Epoch [1/4], Step [61650/64066], Loss: 5.3071\n",
      "Epoch [1/4], Step [61725/64066], Loss: 5.4413\n",
      "Epoch [1/4], Step [61800/64066], Loss: 5.4201\n",
      "Epoch [1/4], Step [61875/64066], Loss: 5.3800\n",
      "Epoch [1/4], Step [61950/64066], Loss: 5.3249\n",
      "Epoch [1/4], Step [62025/64066], Loss: 5.4516\n",
      "Epoch [1/4], Step [62100/64066], Loss: 5.6351\n",
      "Epoch [1/4], Step [62175/64066], Loss: 5.4508\n",
      "Epoch [1/4], Step [62250/64066], Loss: 5.2308\n",
      "Epoch [1/4], Step [62325/64066], Loss: 5.2295\n",
      "Epoch [1/4], Step [62400/64066], Loss: 5.3468\n",
      "Epoch [1/4], Step [62475/64066], Loss: 5.1876\n",
      "Epoch [1/4], Step [62550/64066], Loss: 5.3051\n",
      "Epoch [1/4], Step [62625/64066], Loss: 5.4408\n",
      "Epoch [1/4], Step [62700/64066], Loss: 5.1726\n",
      "Epoch [1/4], Step [62775/64066], Loss: 5.3653\n",
      "Epoch [1/4], Step [62850/64066], Loss: 5.3516\n",
      "Epoch [1/4], Step [62925/64066], Loss: 5.1409\n",
      "Epoch [1/4], Step [63000/64066], Loss: 5.3550\n",
      "Epoch [1/4], Step [63075/64066], Loss: 5.4206\n",
      "Epoch [1/4], Step [63150/64066], Loss: 5.3208\n",
      "Epoch [1/4], Step [63225/64066], Loss: 5.3094\n",
      "Epoch [1/4], Step [63300/64066], Loss: 5.1963\n",
      "Epoch [1/4], Step [63375/64066], Loss: 5.3667\n",
      "Epoch [1/4], Step [63450/64066], Loss: 5.2938\n",
      "Epoch [1/4], Step [63525/64066], Loss: 5.2282\n",
      "Epoch [1/4], Step [63600/64066], Loss: 5.4342\n",
      "Epoch [1/4], Step [63675/64066], Loss: 5.3392\n",
      "Epoch [1/4], Step [63750/64066], Loss: 5.3416\n",
      "Epoch [1/4], Step [63825/64066], Loss: 5.4222\n",
      "Epoch [1/4], Step [63900/64066], Loss: 5.2137\n",
      "Epoch [1/4], Step [63975/64066], Loss: 5.3131\n",
      "Epoch [1/4], Step [64050/64066], Loss: 5.5310\n",
      "Epoch [1/4] Average Loss: 5.5936, Perplexity: 268.71\n",
      "Epoch [2/4], Step [0/64066], Loss: 5.1840\n",
      "Validation perplexity: 180.49272504913566\n",
      "Epoch [2/4], Step [75/64066], Loss: 5.2417\n",
      "Epoch [2/4], Step [150/64066], Loss: 5.4303\n",
      "Epoch [2/4], Step [225/64066], Loss: 5.4694\n",
      "Epoch [2/4], Step [300/64066], Loss: 5.4142\n",
      "Epoch [2/4], Step [375/64066], Loss: 5.2427\n",
      "Epoch [2/4], Step [450/64066], Loss: 5.2829\n",
      "Epoch [2/4], Step [525/64066], Loss: 5.2893\n",
      "Epoch [2/4], Step [600/64066], Loss: 5.4799\n",
      "Epoch [2/4], Step [675/64066], Loss: 5.5750\n",
      "Epoch [2/4], Step [750/64066], Loss: 5.4671\n",
      "Epoch [2/4], Step [825/64066], Loss: 5.2941\n",
      "Epoch [2/4], Step [900/64066], Loss: 5.2338\n",
      "Epoch [2/4], Step [975/64066], Loss: 5.2614\n",
      "Epoch [2/4], Step [1050/64066], Loss: 5.2588\n",
      "Epoch [2/4], Step [1125/64066], Loss: 5.4375\n",
      "Epoch [2/4], Step [1200/64066], Loss: 5.4035\n",
      "Epoch [2/4], Step [1275/64066], Loss: 5.3005\n",
      "Epoch [2/4], Step [1350/64066], Loss: 5.2211\n",
      "Epoch [2/4], Step [1425/64066], Loss: 5.4380\n",
      "Epoch [2/4], Step [1500/64066], Loss: 5.4121\n",
      "Epoch [2/4], Step [1575/64066], Loss: 5.3450\n",
      "Epoch [2/4], Step [1650/64066], Loss: 5.3769\n",
      "Epoch [2/4], Step [1725/64066], Loss: 5.4977\n",
      "Epoch [2/4], Step [1800/64066], Loss: 5.4694\n",
      "Epoch [2/4], Step [1875/64066], Loss: 5.4014\n",
      "Epoch [2/4], Step [1950/64066], Loss: 5.4106\n",
      "Epoch [2/4], Step [2025/64066], Loss: 5.1854\n",
      "Epoch [2/4], Step [2100/64066], Loss: 5.4250\n",
      "Epoch [2/4], Step [2175/64066], Loss: 5.2067\n",
      "Epoch [2/4], Step [2250/64066], Loss: 5.2564\n",
      "Epoch [2/4], Step [2325/64066], Loss: 5.3664\n",
      "Epoch [2/4], Step [2400/64066], Loss: 5.3878\n",
      "Epoch [2/4], Step [2475/64066], Loss: 5.3966\n",
      "Epoch [2/4], Step [2550/64066], Loss: 5.4453\n",
      "Epoch [2/4], Step [2625/64066], Loss: 5.3213\n",
      "Epoch [2/4], Step [2700/64066], Loss: 5.4015\n",
      "Epoch [2/4], Step [2775/64066], Loss: 5.3233\n",
      "Epoch [2/4], Step [2850/64066], Loss: 5.3043\n",
      "Epoch [2/4], Step [2925/64066], Loss: 5.4095\n",
      "Epoch [2/4], Step [3000/64066], Loss: 5.2227\n",
      "Epoch [2/4], Step [3075/64066], Loss: 5.3200\n",
      "Epoch [2/4], Step [3150/64066], Loss: 5.3603\n",
      "Epoch [2/4], Step [3225/64066], Loss: 5.3129\n",
      "Epoch [2/4], Step [3300/64066], Loss: 5.5044\n",
      "Epoch [2/4], Step [3375/64066], Loss: 5.4542\n",
      "Epoch [2/4], Step [3450/64066], Loss: 5.4579\n",
      "Epoch [2/4], Step [3525/64066], Loss: 5.4110\n",
      "Epoch [2/4], Step [3600/64066], Loss: 5.5384\n",
      "Epoch [2/4], Step [3675/64066], Loss: 5.4212\n",
      "Epoch [2/4], Step [3750/64066], Loss: 5.2794\n",
      "Epoch [2/4], Step [3825/64066], Loss: 5.2844\n",
      "Epoch [2/4], Step [3900/64066], Loss: 5.3381\n",
      "Epoch [2/4], Step [3975/64066], Loss: 5.2185\n",
      "Epoch [2/4], Step [4050/64066], Loss: 5.4420\n",
      "Epoch [2/4], Step [4125/64066], Loss: 5.2442\n",
      "Epoch [2/4], Step [4200/64066], Loss: 5.4492\n",
      "Epoch [2/4], Step [4275/64066], Loss: 5.3442\n",
      "Epoch [2/4], Step [4350/64066], Loss: 5.3154\n",
      "Epoch [2/4], Step [4425/64066], Loss: 5.2102\n",
      "Epoch [2/4], Step [4500/64066], Loss: 5.2068\n",
      "Epoch [2/4], Step [4575/64066], Loss: 5.1763\n",
      "Epoch [2/4], Step [4650/64066], Loss: 5.4188\n",
      "Epoch [2/4], Step [4725/64066], Loss: 5.3558\n",
      "Epoch [2/4], Step [4800/64066], Loss: 5.2919\n",
      "Epoch [2/4], Step [4875/64066], Loss: 5.0980\n",
      "Epoch [2/4], Step [4950/64066], Loss: 5.3373\n",
      "Epoch [2/4], Step [5025/64066], Loss: 5.4872\n",
      "Epoch [2/4], Step [5100/64066], Loss: 5.2431\n",
      "Epoch [2/4], Step [5175/64066], Loss: 5.2661\n",
      "Epoch [2/4], Step [5250/64066], Loss: 5.3544\n",
      "Epoch [2/4], Step [5325/64066], Loss: 5.3733\n",
      "Epoch [2/4], Step [5400/64066], Loss: 5.1769\n",
      "Epoch [2/4], Step [5475/64066], Loss: 5.2639\n",
      "Epoch [2/4], Step [5550/64066], Loss: 5.2600\n",
      "Epoch [2/4], Step [5625/64066], Loss: 5.3114\n",
      "Epoch [2/4], Step [5700/64066], Loss: 5.2414\n",
      "Epoch [2/4], Step [5775/64066], Loss: 5.3832\n",
      "Epoch [2/4], Step [5850/64066], Loss: 5.2703\n",
      "Epoch [2/4], Step [5925/64066], Loss: 5.3985\n",
      "Epoch [2/4], Step [6000/64066], Loss: 5.4250\n",
      "Epoch [2/4], Step [6075/64066], Loss: 5.3833\n",
      "Epoch [2/4], Step [6150/64066], Loss: 5.2648\n",
      "Epoch [2/4], Step [6225/64066], Loss: 5.1934\n",
      "Epoch [2/4], Step [6300/64066], Loss: 5.4256\n",
      "Epoch [2/4], Step [6375/64066], Loss: 5.1855\n",
      "Epoch [2/4], Step [6450/64066], Loss: 5.1721\n",
      "Epoch [2/4], Step [6525/64066], Loss: 5.2440\n",
      "Epoch [2/4], Step [6600/64066], Loss: 5.3847\n",
      "Epoch [2/4], Step [6675/64066], Loss: 5.2989\n",
      "Epoch [2/4], Step [6750/64066], Loss: 5.3145\n",
      "Epoch [2/4], Step [6825/64066], Loss: 5.2754\n",
      "Epoch [2/4], Step [6900/64066], Loss: 5.3881\n",
      "Epoch [2/4], Step [6975/64066], Loss: 5.4258\n",
      "Epoch [2/4], Step [7050/64066], Loss: 5.3357\n",
      "Epoch [2/4], Step [7125/64066], Loss: 5.3975\n",
      "Epoch [2/4], Step [7200/64066], Loss: 5.5368\n",
      "Epoch [2/4], Step [7275/64066], Loss: 5.3737\n",
      "Epoch [2/4], Step [7350/64066], Loss: 5.1736\n",
      "Epoch [2/4], Step [7425/64066], Loss: 5.2522\n",
      "Epoch [2/4], Step [7500/64066], Loss: 5.5112\n",
      "Epoch [2/4], Step [7575/64066], Loss: 5.3612\n",
      "Epoch [2/4], Step [7650/64066], Loss: 5.4154\n",
      "Epoch [2/4], Step [7725/64066], Loss: 5.3195\n",
      "Epoch [2/4], Step [7800/64066], Loss: 5.3006\n",
      "Epoch [2/4], Step [7875/64066], Loss: 5.3646\n",
      "Epoch [2/4], Step [7950/64066], Loss: 5.5110\n",
      "Epoch [2/4], Step [8025/64066], Loss: 5.3523\n",
      "Epoch [2/4], Step [8100/64066], Loss: 5.3559\n",
      "Epoch [2/4], Step [8175/64066], Loss: 5.2708\n",
      "Epoch [2/4], Step [8250/64066], Loss: 5.3172\n",
      "Epoch [2/4], Step [8325/64066], Loss: 5.3584\n",
      "Epoch [2/4], Step [8400/64066], Loss: 5.2669\n",
      "Epoch [2/4], Step [8475/64066], Loss: 5.4103\n",
      "Epoch [2/4], Step [8550/64066], Loss: 5.1944\n",
      "Epoch [2/4], Step [8625/64066], Loss: 5.0877\n",
      "Epoch [2/4], Step [8700/64066], Loss: 5.5138\n",
      "Epoch [2/4], Step [8775/64066], Loss: 5.4966\n",
      "Epoch [2/4], Step [8850/64066], Loss: 5.3850\n",
      "Epoch [2/4], Step [8925/64066], Loss: 5.3466\n",
      "Epoch [2/4], Step [9000/64066], Loss: 5.4541\n",
      "Epoch [2/4], Step [9075/64066], Loss: 5.3597\n",
      "Epoch [2/4], Step [9150/64066], Loss: 5.3999\n",
      "Epoch [2/4], Step [9225/64066], Loss: 5.1766\n",
      "Epoch [2/4], Step [9300/64066], Loss: 5.2898\n",
      "Epoch [2/4], Step [9375/64066], Loss: 5.2950\n",
      "Epoch [2/4], Step [9450/64066], Loss: 5.2915\n",
      "Epoch [2/4], Step [9525/64066], Loss: 5.3200\n",
      "Epoch [2/4], Step [9600/64066], Loss: 5.4508\n",
      "Epoch [2/4], Step [9675/64066], Loss: 5.3784\n",
      "Epoch [2/4], Step [9750/64066], Loss: 5.3139\n",
      "Epoch [2/4], Step [9825/64066], Loss: 5.2614\n",
      "Epoch [2/4], Step [9900/64066], Loss: 5.3670\n",
      "Epoch [2/4], Step [9975/64066], Loss: 5.1683\n",
      "Validation perplexity: 176.06024689314623\n",
      "Epoch [2/4], Step [10050/64066], Loss: 5.3435\n",
      "Epoch [2/4], Step [10125/64066], Loss: 5.4538\n",
      "Epoch [2/4], Step [10200/64066], Loss: 5.4724\n",
      "Epoch [2/4], Step [10275/64066], Loss: 5.3298\n",
      "Epoch [2/4], Step [10350/64066], Loss: 5.2419\n",
      "Epoch [2/4], Step [10425/64066], Loss: 5.3119\n",
      "Epoch [2/4], Step [10500/64066], Loss: 5.3117\n",
      "Epoch [2/4], Step [10575/64066], Loss: 5.4012\n",
      "Epoch [2/4], Step [10650/64066], Loss: 5.1847\n",
      "Epoch [2/4], Step [10725/64066], Loss: 5.3802\n",
      "Epoch [2/4], Step [10800/64066], Loss: 5.2419\n",
      "Epoch [2/4], Step [10875/64066], Loss: 5.2498\n",
      "Epoch [2/4], Step [10950/64066], Loss: 5.4317\n",
      "Epoch [2/4], Step [11025/64066], Loss: 5.3126\n",
      "Epoch [2/4], Step [11100/64066], Loss: 5.3090\n",
      "Epoch [2/4], Step [11175/64066], Loss: 5.5368\n",
      "Epoch [2/4], Step [11250/64066], Loss: 5.2119\n",
      "Epoch [2/4], Step [11325/64066], Loss: 5.3484\n",
      "Epoch [2/4], Step [11400/64066], Loss: 5.4248\n",
      "Epoch [2/4], Step [11475/64066], Loss: 5.4226\n",
      "Epoch [2/4], Step [11550/64066], Loss: 5.3194\n",
      "Epoch [2/4], Step [11625/64066], Loss: 5.1550\n",
      "Epoch [2/4], Step [11700/64066], Loss: 5.3410\n",
      "Epoch [2/4], Step [11775/64066], Loss: 5.3362\n",
      "Epoch [2/4], Step [11850/64066], Loss: 5.5499\n",
      "Epoch [2/4], Step [11925/64066], Loss: 5.4098\n",
      "Epoch [2/4], Step [12000/64066], Loss: 5.1045\n",
      "Epoch [2/4], Step [12075/64066], Loss: 5.2898\n",
      "Epoch [2/4], Step [12150/64066], Loss: 5.3279\n",
      "Epoch [2/4], Step [12225/64066], Loss: 5.3815\n",
      "Epoch [2/4], Step [12300/64066], Loss: 5.3105\n",
      "Epoch [2/4], Step [12375/64066], Loss: 5.4713\n",
      "Epoch [2/4], Step [12450/64066], Loss: 5.2668\n",
      "Epoch [2/4], Step [12525/64066], Loss: 5.4298\n",
      "Epoch [2/4], Step [12600/64066], Loss: 5.3165\n",
      "Epoch [2/4], Step [12675/64066], Loss: 5.4104\n",
      "Epoch [2/4], Step [12750/64066], Loss: 5.3228\n",
      "Epoch [2/4], Step [12825/64066], Loss: 5.1501\n",
      "Epoch [2/4], Step [12900/64066], Loss: 5.4042\n",
      "Epoch [2/4], Step [12975/64066], Loss: 5.2579\n",
      "Epoch [2/4], Step [13050/64066], Loss: 5.1370\n",
      "Epoch [2/4], Step [13125/64066], Loss: 5.2309\n",
      "Epoch [2/4], Step [13200/64066], Loss: 5.1933\n",
      "Epoch [2/4], Step [13275/64066], Loss: 5.2653\n",
      "Epoch [2/4], Step [13350/64066], Loss: 5.4732\n",
      "Epoch [2/4], Step [13425/64066], Loss: 5.1670\n",
      "Epoch [2/4], Step [13500/64066], Loss: 5.1296\n",
      "Epoch [2/4], Step [13575/64066], Loss: 5.4339\n",
      "Epoch [2/4], Step [13650/64066], Loss: 5.2450\n",
      "Epoch [2/4], Step [13725/64066], Loss: 5.1785\n",
      "Epoch [2/4], Step [13800/64066], Loss: 5.2177\n",
      "Epoch [2/4], Step [13875/64066], Loss: 5.2185\n",
      "Epoch [2/4], Step [13950/64066], Loss: 5.3598\n",
      "Epoch [2/4], Step [14025/64066], Loss: 5.4386\n",
      "Epoch [2/4], Step [14100/64066], Loss: 5.2299\n",
      "Epoch [2/4], Step [14175/64066], Loss: 5.3225\n",
      "Epoch [2/4], Step [14250/64066], Loss: 5.2575\n",
      "Epoch [2/4], Step [14325/64066], Loss: 5.3794\n",
      "Epoch [2/4], Step [14400/64066], Loss: 5.3701\n",
      "Epoch [2/4], Step [14475/64066], Loss: 5.2637\n",
      "Epoch [2/4], Step [14550/64066], Loss: 5.3956\n",
      "Epoch [2/4], Step [14625/64066], Loss: 5.1962\n",
      "Epoch [2/4], Step [14700/64066], Loss: 5.2414\n",
      "Epoch [2/4], Step [14775/64066], Loss: 5.3259\n",
      "Epoch [2/4], Step [14850/64066], Loss: 5.1671\n",
      "Epoch [2/4], Step [14925/64066], Loss: 5.1828\n",
      "Epoch [2/4], Step [15000/64066], Loss: 5.3314\n",
      "Epoch [2/4], Step [15075/64066], Loss: 5.2723\n",
      "Epoch [2/4], Step [15150/64066], Loss: 5.3487\n",
      "Epoch [2/4], Step [15225/64066], Loss: 5.4503\n",
      "Epoch [2/4], Step [15300/64066], Loss: 5.3761\n",
      "Epoch [2/4], Step [15375/64066], Loss: 5.1714\n",
      "Epoch [2/4], Step [15450/64066], Loss: 5.1791\n",
      "Epoch [2/4], Step [15525/64066], Loss: 5.1788\n",
      "Epoch [2/4], Step [15600/64066], Loss: 5.3258\n",
      "Epoch [2/4], Step [15675/64066], Loss: 5.3628\n",
      "Epoch [2/4], Step [15750/64066], Loss: 5.2548\n",
      "Epoch [2/4], Step [15825/64066], Loss: 5.2260\n",
      "Epoch [2/4], Step [15900/64066], Loss: 5.4550\n",
      "Epoch [2/4], Step [15975/64066], Loss: 5.3187\n",
      "Epoch [2/4], Step [16050/64066], Loss: 5.3753\n",
      "Epoch [2/4], Step [16125/64066], Loss: 5.2842\n",
      "Epoch [2/4], Step [16200/64066], Loss: 5.1555\n",
      "Epoch [2/4], Step [16275/64066], Loss: 5.4297\n",
      "Epoch [2/4], Step [16350/64066], Loss: 5.3007\n",
      "Epoch [2/4], Step [16425/64066], Loss: 5.2848\n",
      "Epoch [2/4], Step [16500/64066], Loss: 5.4396\n",
      "Epoch [2/4], Step [16575/64066], Loss: 5.1649\n",
      "Epoch [2/4], Step [16650/64066], Loss: 5.2633\n",
      "Epoch [2/4], Step [16725/64066], Loss: 5.2853\n",
      "Epoch [2/4], Step [16800/64066], Loss: 5.4169\n",
      "Epoch [2/4], Step [16875/64066], Loss: 5.2815\n",
      "Epoch [2/4], Step [16950/64066], Loss: 5.1576\n",
      "Epoch [2/4], Step [17025/64066], Loss: 5.3598\n",
      "Epoch [2/4], Step [17100/64066], Loss: 5.3030\n",
      "Epoch [2/4], Step [17175/64066], Loss: 5.2726\n",
      "Epoch [2/4], Step [17250/64066], Loss: 5.3971\n",
      "Epoch [2/4], Step [17325/64066], Loss: 5.1568\n",
      "Epoch [2/4], Step [17400/64066], Loss: 5.1635\n",
      "Epoch [2/4], Step [17475/64066], Loss: 5.3157\n",
      "Epoch [2/4], Step [17550/64066], Loss: 5.4169\n",
      "Epoch [2/4], Step [17625/64066], Loss: 5.4209\n",
      "Epoch [2/4], Step [17700/64066], Loss: 5.2043\n",
      "Epoch [2/4], Step [17775/64066], Loss: 5.2813\n",
      "Epoch [2/4], Step [17850/64066], Loss: 5.3091\n",
      "Epoch [2/4], Step [17925/64066], Loss: 5.2133\n",
      "Epoch [2/4], Step [18000/64066], Loss: 5.4564\n",
      "Epoch [2/4], Step [18075/64066], Loss: 5.3758\n",
      "Epoch [2/4], Step [18150/64066], Loss: 5.3890\n",
      "Epoch [2/4], Step [18225/64066], Loss: 5.1989\n",
      "Epoch [2/4], Step [18300/64066], Loss: 5.2080\n",
      "Epoch [2/4], Step [18375/64066], Loss: 5.1802\n",
      "Epoch [2/4], Step [18450/64066], Loss: 5.3592\n",
      "Epoch [2/4], Step [18525/64066], Loss: 5.3236\n",
      "Epoch [2/4], Step [18600/64066], Loss: 5.2024\n",
      "Epoch [2/4], Step [18675/64066], Loss: 5.2566\n",
      "Epoch [2/4], Step [18750/64066], Loss: 5.4695\n",
      "Epoch [2/4], Step [18825/64066], Loss: 5.3666\n",
      "Epoch [2/4], Step [18900/64066], Loss: 5.3771\n",
      "Epoch [2/4], Step [18975/64066], Loss: 5.2215\n",
      "Epoch [2/4], Step [19050/64066], Loss: 5.4036\n",
      "Epoch [2/4], Step [19125/64066], Loss: 5.1993\n",
      "Epoch [2/4], Step [19200/64066], Loss: 5.3970\n",
      "Epoch [2/4], Step [19275/64066], Loss: 5.3874\n",
      "Epoch [2/4], Step [19350/64066], Loss: 5.4371\n",
      "Epoch [2/4], Step [19425/64066], Loss: 5.3542\n",
      "Epoch [2/4], Step [19500/64066], Loss: 5.3478\n",
      "Epoch [2/4], Step [19575/64066], Loss: 5.2975\n",
      "Epoch [2/4], Step [19650/64066], Loss: 5.3427\n",
      "Epoch [2/4], Step [19725/64066], Loss: 5.1956\n",
      "Epoch [2/4], Step [19800/64066], Loss: 5.2519\n",
      "Epoch [2/4], Step [19875/64066], Loss: 5.3656\n",
      "Epoch [2/4], Step [19950/64066], Loss: 5.2608\n",
      "Validation perplexity: 172.30875969100728\n",
      "Epoch [2/4], Step [20025/64066], Loss: 5.3130\n",
      "Epoch [2/4], Step [20100/64066], Loss: 5.2945\n",
      "Epoch [2/4], Step [20175/64066], Loss: 5.2583\n",
      "Epoch [2/4], Step [20250/64066], Loss: 5.1193\n",
      "Epoch [2/4], Step [20325/64066], Loss: 5.2111\n",
      "Epoch [2/4], Step [20400/64066], Loss: 5.3668\n",
      "Epoch [2/4], Step [20475/64066], Loss: 5.2022\n",
      "Epoch [2/4], Step [20550/64066], Loss: 5.2920\n",
      "Epoch [2/4], Step [20625/64066], Loss: 5.3594\n",
      "Epoch [2/4], Step [20700/64066], Loss: 5.1986\n",
      "Epoch [2/4], Step [20775/64066], Loss: 5.2683\n",
      "Epoch [2/4], Step [20850/64066], Loss: 5.4449\n",
      "Epoch [2/4], Step [20925/64066], Loss: 5.4840\n",
      "Epoch [2/4], Step [21000/64066], Loss: 5.2507\n",
      "Epoch [2/4], Step [21075/64066], Loss: 5.1968\n",
      "Epoch [2/4], Step [21150/64066], Loss: 5.3926\n",
      "Epoch [2/4], Step [21225/64066], Loss: 5.5554\n",
      "Epoch [2/4], Step [21300/64066], Loss: 5.4068\n",
      "Epoch [2/4], Step [21375/64066], Loss: 5.1742\n",
      "Epoch [2/4], Step [21450/64066], Loss: 5.2690\n",
      "Epoch [2/4], Step [21525/64066], Loss: 5.1589\n",
      "Epoch [2/4], Step [21600/64066], Loss: 5.2689\n",
      "Epoch [2/4], Step [21675/64066], Loss: 5.4762\n",
      "Epoch [2/4], Step [21750/64066], Loss: 5.3046\n",
      "Epoch [2/4], Step [21825/64066], Loss: 5.2825\n",
      "Epoch [2/4], Step [21900/64066], Loss: 5.2001\n",
      "Epoch [2/4], Step [21975/64066], Loss: 5.4474\n",
      "Epoch [2/4], Step [22050/64066], Loss: 5.2267\n",
      "Epoch [2/4], Step [22125/64066], Loss: 5.3111\n",
      "Epoch [2/4], Step [22200/64066], Loss: 5.3770\n",
      "Epoch [2/4], Step [22275/64066], Loss: 5.2276\n",
      "Epoch [2/4], Step [22350/64066], Loss: 5.2715\n",
      "Epoch [2/4], Step [22425/64066], Loss: 5.1825\n",
      "Epoch [2/4], Step [22500/64066], Loss: 5.2088\n",
      "Epoch [2/4], Step [22575/64066], Loss: 5.3495\n",
      "Epoch [2/4], Step [22650/64066], Loss: 5.1680\n",
      "Epoch [2/4], Step [22725/64066], Loss: 5.2362\n",
      "Epoch [2/4], Step [22800/64066], Loss: 5.1269\n",
      "Epoch [2/4], Step [22875/64066], Loss: 5.2708\n",
      "Epoch [2/4], Step [22950/64066], Loss: 5.1416\n",
      "Epoch [2/4], Step [23025/64066], Loss: 5.4504\n",
      "Epoch [2/4], Step [23100/64066], Loss: 5.2505\n",
      "Epoch [2/4], Step [23175/64066], Loss: 5.1624\n",
      "Epoch [2/4], Step [23250/64066], Loss: 5.2442\n",
      "Epoch [2/4], Step [23325/64066], Loss: 5.2638\n",
      "Epoch [2/4], Step [23400/64066], Loss: 5.2480\n",
      "Epoch [2/4], Step [23475/64066], Loss: 5.2260\n",
      "Epoch [2/4], Step [23550/64066], Loss: 5.2924\n",
      "Epoch [2/4], Step [23625/64066], Loss: 5.2731\n",
      "Epoch [2/4], Step [23700/64066], Loss: 5.2827\n",
      "Epoch [2/4], Step [23775/64066], Loss: 5.3430\n",
      "Epoch [2/4], Step [23850/64066], Loss: 5.3597\n",
      "Epoch [2/4], Step [23925/64066], Loss: 5.3012\n",
      "Epoch [2/4], Step [24000/64066], Loss: 5.3774\n",
      "Epoch [2/4], Step [24075/64066], Loss: 5.3319\n",
      "Epoch [2/4], Step [24150/64066], Loss: 5.3277\n",
      "Epoch [2/4], Step [24225/64066], Loss: 5.1976\n",
      "Epoch [2/4], Step [24300/64066], Loss: 5.2250\n",
      "Epoch [2/4], Step [24375/64066], Loss: 5.3186\n",
      "Epoch [2/4], Step [24450/64066], Loss: 5.1464\n",
      "Epoch [2/4], Step [24525/64066], Loss: 5.2743\n",
      "Epoch [2/4], Step [24600/64066], Loss: 5.0389\n",
      "Epoch [2/4], Step [24675/64066], Loss: 5.2087\n",
      "Epoch [2/4], Step [24750/64066], Loss: 5.4013\n",
      "Epoch [2/4], Step [24825/64066], Loss: 5.2967\n",
      "Epoch [2/4], Step [24900/64066], Loss: 5.2591\n",
      "Epoch [2/4], Step [24975/64066], Loss: 5.3840\n",
      "Epoch [2/4], Step [25050/64066], Loss: 5.2557\n",
      "Epoch [2/4], Step [25125/64066], Loss: 5.3065\n",
      "Epoch [2/4], Step [25200/64066], Loss: 5.3848\n",
      "Epoch [2/4], Step [25275/64066], Loss: 5.3292\n",
      "Epoch [2/4], Step [25350/64066], Loss: 5.1708\n",
      "Epoch [2/4], Step [25425/64066], Loss: 5.4126\n",
      "Epoch [2/4], Step [25500/64066], Loss: 5.4247\n",
      "Epoch [2/4], Step [25575/64066], Loss: 5.2839\n",
      "Epoch [2/4], Step [25650/64066], Loss: 5.1206\n",
      "Epoch [2/4], Step [25725/64066], Loss: 5.3278\n",
      "Epoch [2/4], Step [25800/64066], Loss: 5.3072\n",
      "Epoch [2/4], Step [25875/64066], Loss: 5.3358\n",
      "Epoch [2/4], Step [25950/64066], Loss: 5.4894\n",
      "Epoch [2/4], Step [26025/64066], Loss: 5.1792\n",
      "Epoch [2/4], Step [26100/64066], Loss: 5.3156\n",
      "Epoch [2/4], Step [26175/64066], Loss: 5.2455\n",
      "Epoch [2/4], Step [26250/64066], Loss: 5.4412\n",
      "Epoch [2/4], Step [26325/64066], Loss: 5.3163\n",
      "Epoch [2/4], Step [26400/64066], Loss: 5.2860\n",
      "Epoch [2/4], Step [26475/64066], Loss: 5.5413\n",
      "Epoch [2/4], Step [26550/64066], Loss: 5.2687\n",
      "Epoch [2/4], Step [26625/64066], Loss: 5.2927\n",
      "Epoch [2/4], Step [26700/64066], Loss: 5.3383\n",
      "Epoch [2/4], Step [26775/64066], Loss: 5.3639\n",
      "Epoch [2/4], Step [26850/64066], Loss: 5.4156\n",
      "Epoch [2/4], Step [26925/64066], Loss: 5.3686\n",
      "Epoch [2/4], Step [27000/64066], Loss: 5.2105\n",
      "Epoch [2/4], Step [27075/64066], Loss: 5.2790\n",
      "Epoch [2/4], Step [27150/64066], Loss: 5.2779\n",
      "Epoch [2/4], Step [27225/64066], Loss: 5.3285\n",
      "Epoch [2/4], Step [27300/64066], Loss: 5.2758\n",
      "Epoch [2/4], Step [27375/64066], Loss: 5.2679\n",
      "Epoch [2/4], Step [27450/64066], Loss: 5.0460\n",
      "Epoch [2/4], Step [27525/64066], Loss: 5.1898\n",
      "Epoch [2/4], Step [27600/64066], Loss: 5.3021\n",
      "Epoch [2/4], Step [27675/64066], Loss: 5.2890\n",
      "Epoch [2/4], Step [27750/64066], Loss: 5.2758\n",
      "Epoch [2/4], Step [27825/64066], Loss: 5.3938\n",
      "Epoch [2/4], Step [27900/64066], Loss: 5.3450\n",
      "Epoch [2/4], Step [27975/64066], Loss: 5.3654\n",
      "Epoch [2/4], Step [28050/64066], Loss: 5.0783\n",
      "Epoch [2/4], Step [28125/64066], Loss: 5.3277\n",
      "Epoch [2/4], Step [28200/64066], Loss: 5.4269\n",
      "Epoch [2/4], Step [28275/64066], Loss: 5.2917\n",
      "Epoch [2/4], Step [28350/64066], Loss: 5.3207\n",
      "Epoch [2/4], Step [28425/64066], Loss: 5.5646\n",
      "Epoch [2/4], Step [28500/64066], Loss: 5.0496\n",
      "Epoch [2/4], Step [28575/64066], Loss: 5.3785\n",
      "Epoch [2/4], Step [28650/64066], Loss: 5.3135\n",
      "Epoch [2/4], Step [28725/64066], Loss: 5.2373\n",
      "Epoch [2/4], Step [28800/64066], Loss: 5.3220\n",
      "Epoch [2/4], Step [28875/64066], Loss: 5.2392\n",
      "Epoch [2/4], Step [28950/64066], Loss: 5.4268\n",
      "Epoch [2/4], Step [29025/64066], Loss: 5.3976\n",
      "Epoch [2/4], Step [29100/64066], Loss: 5.2938\n",
      "Epoch [2/4], Step [29175/64066], Loss: 5.3138\n",
      "Epoch [2/4], Step [29250/64066], Loss: 5.5167\n",
      "Epoch [2/4], Step [29325/64066], Loss: 5.1610\n",
      "Epoch [2/4], Step [29400/64066], Loss: 5.1250\n",
      "Epoch [2/4], Step [29475/64066], Loss: 5.3538\n",
      "Epoch [2/4], Step [29550/64066], Loss: 5.2542\n",
      "Epoch [2/4], Step [29625/64066], Loss: 5.1953\n",
      "Epoch [2/4], Step [29700/64066], Loss: 5.2129\n",
      "Epoch [2/4], Step [29775/64066], Loss: 5.1568\n",
      "Epoch [2/4], Step [29850/64066], Loss: 5.2957\n",
      "Epoch [2/4], Step [29925/64066], Loss: 5.2637\n",
      "Epoch [2/4], Step [30000/64066], Loss: 5.4129\n",
      "Validation perplexity: 169.3035989143734\n",
      "Epoch [2/4], Step [30075/64066], Loss: 5.2683\n",
      "Epoch [2/4], Step [30150/64066], Loss: 5.3975\n",
      "Epoch [2/4], Step [30225/64066], Loss: 5.3064\n",
      "Epoch [2/4], Step [30300/64066], Loss: 5.1567\n",
      "Epoch [2/4], Step [30375/64066], Loss: 5.1869\n",
      "Epoch [2/4], Step [30450/64066], Loss: 5.3037\n",
      "Epoch [2/4], Step [30525/64066], Loss: 5.3313\n",
      "Epoch [2/4], Step [30600/64066], Loss: 5.3106\n",
      "Epoch [2/4], Step [30675/64066], Loss: 5.2636\n",
      "Epoch [2/4], Step [30750/64066], Loss: 5.2061\n",
      "Epoch [2/4], Step [30825/64066], Loss: 5.4604\n",
      "Epoch [2/4], Step [30900/64066], Loss: 5.2702\n",
      "Epoch [2/4], Step [30975/64066], Loss: 5.4409\n",
      "Epoch [2/4], Step [31050/64066], Loss: 5.3702\n",
      "Epoch [2/4], Step [31125/64066], Loss: 5.2594\n",
      "Epoch [2/4], Step [31200/64066], Loss: 5.1777\n",
      "Epoch [2/4], Step [31275/64066], Loss: 5.1810\n",
      "Epoch [2/4], Step [31350/64066], Loss: 5.2697\n",
      "Epoch [2/4], Step [31425/64066], Loss: 5.3134\n",
      "Epoch [2/4], Step [31500/64066], Loss: 5.3206\n",
      "Epoch [2/4], Step [31575/64066], Loss: 5.1319\n",
      "Epoch [2/4], Step [31650/64066], Loss: 5.1038\n",
      "Epoch [2/4], Step [31725/64066], Loss: 5.1224\n",
      "Epoch [2/4], Step [31800/64066], Loss: 5.3537\n",
      "Epoch [2/4], Step [31875/64066], Loss: 5.2217\n",
      "Epoch [2/4], Step [31950/64066], Loss: 5.2887\n",
      "Epoch [2/4], Step [32025/64066], Loss: 5.2785\n",
      "Epoch [2/4], Step [32100/64066], Loss: 5.3721\n",
      "Epoch [2/4], Step [32175/64066], Loss: 5.2653\n",
      "Epoch [2/4], Step [32250/64066], Loss: 5.2190\n",
      "Epoch [2/4], Step [32325/64066], Loss: 5.3805\n",
      "Epoch [2/4], Step [32400/64066], Loss: 5.2623\n",
      "Epoch [2/4], Step [32475/64066], Loss: 5.3754\n",
      "Epoch [2/4], Step [32550/64066], Loss: 5.1492\n",
      "Epoch [2/4], Step [32625/64066], Loss: 5.3128\n",
      "Epoch [2/4], Step [32700/64066], Loss: 5.3148\n",
      "Epoch [2/4], Step [32775/64066], Loss: 5.3559\n",
      "Epoch [2/4], Step [32850/64066], Loss: 5.1563\n",
      "Epoch [2/4], Step [32925/64066], Loss: 5.4021\n",
      "Epoch [2/4], Step [33000/64066], Loss: 5.2300\n",
      "Epoch [2/4], Step [33075/64066], Loss: 5.3499\n",
      "Epoch [2/4], Step [33150/64066], Loss: 5.3505\n",
      "Epoch [2/4], Step [33225/64066], Loss: 5.2972\n",
      "Epoch [2/4], Step [33300/64066], Loss: 5.1836\n",
      "Epoch [2/4], Step [33375/64066], Loss: 5.2343\n",
      "Epoch [2/4], Step [33450/64066], Loss: 5.3188\n",
      "Epoch [2/4], Step [33525/64066], Loss: 5.3215\n",
      "Epoch [2/4], Step [33600/64066], Loss: 5.5064\n",
      "Epoch [2/4], Step [33675/64066], Loss: 5.3324\n",
      "Epoch [2/4], Step [33750/64066], Loss: 5.3666\n",
      "Epoch [2/4], Step [33825/64066], Loss: 5.3515\n",
      "Epoch [2/4], Step [33900/64066], Loss: 5.2255\n",
      "Epoch [2/4], Step [33975/64066], Loss: 5.2060\n",
      "Epoch [2/4], Step [34050/64066], Loss: 5.2850\n",
      "Epoch [2/4], Step [34125/64066], Loss: 5.1382\n",
      "Epoch [2/4], Step [34200/64066], Loss: 5.2234\n",
      "Epoch [2/4], Step [34275/64066], Loss: 5.0566\n",
      "Epoch [2/4], Step [34350/64066], Loss: 5.3152\n",
      "Epoch [2/4], Step [34425/64066], Loss: 5.3184\n",
      "Epoch [2/4], Step [34500/64066], Loss: 5.2190\n",
      "Epoch [2/4], Step [34575/64066], Loss: 5.4031\n",
      "Epoch [2/4], Step [34650/64066], Loss: 5.1396\n",
      "Epoch [2/4], Step [34725/64066], Loss: 5.2390\n",
      "Epoch [2/4], Step [34800/64066], Loss: 5.2067\n",
      "Epoch [2/4], Step [34875/64066], Loss: 5.2225\n",
      "Epoch [2/4], Step [34950/64066], Loss: 5.2779\n",
      "Epoch [2/4], Step [35025/64066], Loss: 5.3139\n",
      "Epoch [2/4], Step [35100/64066], Loss: 5.2645\n",
      "Epoch [2/4], Step [35175/64066], Loss: 5.2911\n",
      "Epoch [2/4], Step [35250/64066], Loss: 5.4645\n",
      "Epoch [2/4], Step [35325/64066], Loss: 5.0846\n",
      "Epoch [2/4], Step [35400/64066], Loss: 5.3639\n",
      "Epoch [2/4], Step [35475/64066], Loss: 5.3519\n",
      "Epoch [2/4], Step [35550/64066], Loss: 5.4686\n",
      "Epoch [2/4], Step [35625/64066], Loss: 5.2542\n",
      "Epoch [2/4], Step [35700/64066], Loss: 5.2643\n",
      "Epoch [2/4], Step [35775/64066], Loss: 5.4885\n",
      "Epoch [2/4], Step [35850/64066], Loss: 5.3269\n",
      "Epoch [2/4], Step [35925/64066], Loss: 5.1525\n",
      "Epoch [2/4], Step [36000/64066], Loss: 5.3228\n",
      "Epoch [2/4], Step [36075/64066], Loss: 5.3130\n",
      "Epoch [2/4], Step [36150/64066], Loss: 5.0698\n",
      "Epoch [2/4], Step [36225/64066], Loss: 5.1932\n",
      "Epoch [2/4], Step [36300/64066], Loss: 5.3321\n",
      "Epoch [2/4], Step [36375/64066], Loss: 5.1989\n",
      "Epoch [2/4], Step [36450/64066], Loss: 5.1601\n",
      "Epoch [2/4], Step [36525/64066], Loss: 5.2152\n",
      "Epoch [2/4], Step [36600/64066], Loss: 5.4122\n",
      "Epoch [2/4], Step [36675/64066], Loss: 5.3036\n",
      "Epoch [2/4], Step [36750/64066], Loss: 5.2599\n",
      "Epoch [2/4], Step [36825/64066], Loss: 5.4350\n",
      "Epoch [2/4], Step [36900/64066], Loss: 5.1226\n",
      "Epoch [2/4], Step [36975/64066], Loss: 5.3666\n",
      "Epoch [2/4], Step [37050/64066], Loss: 5.2198\n",
      "Epoch [2/4], Step [37125/64066], Loss: 5.3038\n",
      "Epoch [2/4], Step [37200/64066], Loss: 5.1439\n",
      "Epoch [2/4], Step [37275/64066], Loss: 5.1138\n",
      "Epoch [2/4], Step [37350/64066], Loss: 5.3953\n",
      "Epoch [2/4], Step [37425/64066], Loss: 5.2729\n",
      "Epoch [2/4], Step [37500/64066], Loss: 5.2189\n",
      "Epoch [2/4], Step [37575/64066], Loss: 5.2545\n",
      "Epoch [2/4], Step [37650/64066], Loss: 5.2455\n",
      "Epoch [2/4], Step [37725/64066], Loss: 5.2673\n",
      "Epoch [2/4], Step [37800/64066], Loss: 5.1967\n",
      "Epoch [2/4], Step [37875/64066], Loss: 5.3178\n",
      "Epoch [2/4], Step [37950/64066], Loss: 5.1979\n",
      "Epoch [2/4], Step [38025/64066], Loss: 5.1663\n",
      "Epoch [2/4], Step [38100/64066], Loss: 5.1890\n",
      "Epoch [2/4], Step [38175/64066], Loss: 5.2078\n",
      "Epoch [2/4], Step [38250/64066], Loss: 5.2131\n",
      "Epoch [2/4], Step [38325/64066], Loss: 5.1759\n",
      "Epoch [2/4], Step [38400/64066], Loss: 5.4367\n",
      "Epoch [2/4], Step [38475/64066], Loss: 5.3114\n",
      "Epoch [2/4], Step [38550/64066], Loss: 5.2766\n",
      "Epoch [2/4], Step [38625/64066], Loss: 5.3725\n",
      "Epoch [2/4], Step [38700/64066], Loss: 5.1972\n",
      "Epoch [2/4], Step [38775/64066], Loss: 5.2284\n",
      "Epoch [2/4], Step [38850/64066], Loss: 5.5715\n",
      "Epoch [2/4], Step [38925/64066], Loss: 5.2710\n",
      "Epoch [2/4], Step [39000/64066], Loss: 5.3397\n",
      "Epoch [2/4], Step [39075/64066], Loss: 5.2692\n",
      "Epoch [2/4], Step [39150/64066], Loss: 5.3285\n",
      "Epoch [2/4], Step [39225/64066], Loss: 5.2434\n",
      "Epoch [2/4], Step [39300/64066], Loss: 5.2681\n",
      "Epoch [2/4], Step [39375/64066], Loss: 5.2812\n",
      "Epoch [2/4], Step [39450/64066], Loss: 5.2125\n",
      "Epoch [2/4], Step [39525/64066], Loss: 5.2238\n",
      "Epoch [2/4], Step [39600/64066], Loss: 5.3281\n",
      "Epoch [2/4], Step [39675/64066], Loss: 5.2542\n",
      "Epoch [2/4], Step [39750/64066], Loss: 5.2566\n",
      "Epoch [2/4], Step [39825/64066], Loss: 5.2918\n",
      "Epoch [2/4], Step [39900/64066], Loss: 5.3145\n",
      "Epoch [2/4], Step [39975/64066], Loss: 5.3033\n",
      "Validation perplexity: 166.9747590802414\n",
      "Epoch [2/4], Step [40050/64066], Loss: 5.3082\n",
      "Epoch [2/4], Step [40125/64066], Loss: 5.4273\n",
      "Epoch [2/4], Step [40200/64066], Loss: 5.1988\n",
      "Epoch [2/4], Step [40275/64066], Loss: 5.1810\n",
      "Epoch [2/4], Step [40350/64066], Loss: 5.2498\n",
      "Epoch [2/4], Step [40425/64066], Loss: 5.2680\n",
      "Epoch [2/4], Step [40500/64066], Loss: 5.2877\n",
      "Epoch [2/4], Step [40575/64066], Loss: 5.3007\n",
      "Epoch [2/4], Step [40650/64066], Loss: 5.1943\n",
      "Epoch [2/4], Step [40725/64066], Loss: 5.2798\n",
      "Epoch [2/4], Step [40800/64066], Loss: 5.3426\n",
      "Epoch [2/4], Step [40875/64066], Loss: 5.1762\n",
      "Epoch [2/4], Step [40950/64066], Loss: 5.2083\n",
      "Epoch [2/4], Step [41025/64066], Loss: 5.1453\n",
      "Epoch [2/4], Step [41100/64066], Loss: 5.1887\n",
      "Epoch [2/4], Step [41175/64066], Loss: 5.1132\n",
      "Epoch [2/4], Step [41250/64066], Loss: 5.2208\n",
      "Epoch [2/4], Step [41325/64066], Loss: 5.2903\n",
      "Epoch [2/4], Step [41400/64066], Loss: 5.2436\n",
      "Epoch [2/4], Step [41475/64066], Loss: 5.4380\n",
      "Epoch [2/4], Step [41550/64066], Loss: 5.2578\n",
      "Epoch [2/4], Step [41625/64066], Loss: 5.3543\n",
      "Epoch [2/4], Step [41700/64066], Loss: 5.1511\n",
      "Epoch [2/4], Step [41775/64066], Loss: 5.2022\n",
      "Epoch [2/4], Step [41850/64066], Loss: 5.0622\n",
      "Epoch [2/4], Step [41925/64066], Loss: 5.2948\n",
      "Epoch [2/4], Step [42000/64066], Loss: 5.1607\n",
      "Epoch [2/4], Step [42075/64066], Loss: 5.2458\n",
      "Epoch [2/4], Step [42150/64066], Loss: 5.2750\n",
      "Epoch [2/4], Step [42225/64066], Loss: 5.3680\n",
      "Epoch [2/4], Step [42300/64066], Loss: 5.1733\n",
      "Epoch [2/4], Step [42375/64066], Loss: 5.3245\n",
      "Epoch [2/4], Step [42450/64066], Loss: 5.2192\n",
      "Epoch [2/4], Step [42525/64066], Loss: 5.2221\n",
      "Epoch [2/4], Step [42600/64066], Loss: 5.3719\n",
      "Epoch [2/4], Step [42675/64066], Loss: 5.2264\n",
      "Epoch [2/4], Step [42750/64066], Loss: 5.3207\n",
      "Epoch [2/4], Step [42825/64066], Loss: 5.1945\n",
      "Epoch [2/4], Step [42900/64066], Loss: 5.2320\n",
      "Epoch [2/4], Step [42975/64066], Loss: 5.2541\n",
      "Epoch [2/4], Step [43050/64066], Loss: 5.3074\n",
      "Epoch [2/4], Step [43125/64066], Loss: 5.3352\n",
      "Epoch [2/4], Step [43200/64066], Loss: 5.2043\n",
      "Epoch [2/4], Step [43275/64066], Loss: 5.2129\n",
      "Epoch [2/4], Step [43350/64066], Loss: 5.1454\n",
      "Epoch [2/4], Step [43425/64066], Loss: 5.3319\n",
      "Epoch [2/4], Step [43500/64066], Loss: 5.2615\n",
      "Epoch [2/4], Step [43575/64066], Loss: 5.2733\n",
      "Epoch [2/4], Step [43650/64066], Loss: 5.3653\n",
      "Epoch [2/4], Step [43725/64066], Loss: 5.3701\n",
      "Epoch [2/4], Step [43800/64066], Loss: 5.1957\n",
      "Epoch [2/4], Step [43875/64066], Loss: 5.1448\n",
      "Epoch [2/4], Step [43950/64066], Loss: 5.2310\n",
      "Epoch [2/4], Step [44025/64066], Loss: 5.3236\n",
      "Epoch [2/4], Step [44100/64066], Loss: 5.2989\n",
      "Epoch [2/4], Step [44175/64066], Loss: 5.4344\n",
      "Epoch [2/4], Step [44250/64066], Loss: 5.3513\n",
      "Epoch [2/4], Step [44325/64066], Loss: 5.3820\n",
      "Epoch [2/4], Step [44400/64066], Loss: 5.3234\n",
      "Epoch [2/4], Step [44475/64066], Loss: 5.2966\n",
      "Epoch [2/4], Step [44550/64066], Loss: 5.1214\n",
      "Epoch [2/4], Step [44625/64066], Loss: 5.3332\n",
      "Epoch [2/4], Step [44700/64066], Loss: 5.2494\n",
      "Epoch [2/4], Step [44775/64066], Loss: 5.1830\n",
      "Epoch [2/4], Step [44850/64066], Loss: 5.4558\n",
      "Epoch [2/4], Step [44925/64066], Loss: 5.3245\n",
      "Epoch [2/4], Step [45000/64066], Loss: 5.0815\n",
      "Epoch [2/4], Step [45075/64066], Loss: 5.1270\n",
      "Epoch [2/4], Step [45150/64066], Loss: 5.2593\n",
      "Epoch [2/4], Step [45225/64066], Loss: 5.2889\n",
      "Epoch [2/4], Step [45300/64066], Loss: 5.2668\n",
      "Epoch [2/4], Step [45375/64066], Loss: 5.3535\n",
      "Epoch [2/4], Step [45450/64066], Loss: 5.1581\n",
      "Epoch [2/4], Step [45525/64066], Loss: 5.4337\n",
      "Epoch [2/4], Step [45600/64066], Loss: 5.3889\n",
      "Epoch [2/4], Step [45675/64066], Loss: 5.2254\n",
      "Epoch [2/4], Step [45750/64066], Loss: 5.4392\n",
      "Epoch [2/4], Step [45825/64066], Loss: 5.2318\n",
      "Epoch [2/4], Step [45900/64066], Loss: 5.1672\n",
      "Epoch [2/4], Step [45975/64066], Loss: 5.1837\n",
      "Epoch [2/4], Step [46050/64066], Loss: 5.2657\n",
      "Epoch [2/4], Step [46125/64066], Loss: 5.2887\n",
      "Epoch [2/4], Step [46200/64066], Loss: 5.1642\n",
      "Epoch [2/4], Step [46275/64066], Loss: 5.3450\n",
      "Epoch [2/4], Step [46350/64066], Loss: 5.3794\n",
      "Epoch [2/4], Step [46425/64066], Loss: 5.3129\n",
      "Epoch [2/4], Step [46500/64066], Loss: 5.1703\n",
      "Epoch [2/4], Step [46575/64066], Loss: 4.9816\n",
      "Epoch [2/4], Step [46650/64066], Loss: 5.2543\n",
      "Epoch [2/4], Step [46725/64066], Loss: 5.3010\n",
      "Epoch [2/4], Step [46800/64066], Loss: 5.2454\n",
      "Epoch [2/4], Step [46875/64066], Loss: 5.2653\n",
      "Epoch [2/4], Step [46950/64066], Loss: 5.2536\n",
      "Epoch [2/4], Step [47025/64066], Loss: 5.1425\n",
      "Epoch [2/4], Step [47100/64066], Loss: 5.2941\n",
      "Epoch [2/4], Step [47175/64066], Loss: 5.2417\n",
      "Epoch [2/4], Step [47250/64066], Loss: 5.2200\n",
      "Epoch [2/4], Step [47325/64066], Loss: 5.1974\n",
      "Epoch [2/4], Step [47400/64066], Loss: 5.2903\n",
      "Epoch [2/4], Step [47475/64066], Loss: 5.1751\n",
      "Epoch [2/4], Step [47550/64066], Loss: 5.2009\n",
      "Epoch [2/4], Step [47625/64066], Loss: 5.1993\n",
      "Epoch [2/4], Step [47700/64066], Loss: 5.2220\n",
      "Epoch [2/4], Step [47775/64066], Loss: 5.0091\n",
      "Epoch [2/4], Step [47850/64066], Loss: 5.2951\n",
      "Epoch [2/4], Step [47925/64066], Loss: 5.3136\n",
      "Epoch [2/4], Step [48000/64066], Loss: 5.4876\n",
      "Epoch [2/4], Step [48075/64066], Loss: 5.3141\n",
      "Epoch [2/4], Step [48150/64066], Loss: 5.2228\n",
      "Epoch [2/4], Step [48225/64066], Loss: 5.3606\n",
      "Epoch [2/4], Step [48300/64066], Loss: 5.2140\n",
      "Epoch [2/4], Step [48375/64066], Loss: 5.3639\n",
      "Epoch [2/4], Step [48450/64066], Loss: 5.2025\n",
      "Epoch [2/4], Step [48525/64066], Loss: 5.2358\n",
      "Epoch [2/4], Step [48600/64066], Loss: 5.1802\n",
      "Epoch [2/4], Step [48675/64066], Loss: 5.2886\n",
      "Epoch [2/4], Step [48750/64066], Loss: 5.0745\n",
      "Epoch [2/4], Step [48825/64066], Loss: 5.1537\n",
      "Epoch [2/4], Step [48900/64066], Loss: 5.2755\n",
      "Epoch [2/4], Step [48975/64066], Loss: 5.1008\n",
      "Epoch [2/4], Step [49050/64066], Loss: 5.1284\n",
      "Epoch [2/4], Step [49125/64066], Loss: 5.2992\n",
      "Epoch [2/4], Step [49200/64066], Loss: 5.1834\n",
      "Epoch [2/4], Step [49275/64066], Loss: 5.3587\n",
      "Epoch [2/4], Step [49350/64066], Loss: 5.4092\n",
      "Epoch [2/4], Step [49425/64066], Loss: 5.4660\n",
      "Epoch [2/4], Step [49500/64066], Loss: 5.2653\n",
      "Epoch [2/4], Step [49575/64066], Loss: 5.3532\n",
      "Epoch [2/4], Step [49650/64066], Loss: 5.2253\n",
      "Epoch [2/4], Step [49725/64066], Loss: 5.3837\n",
      "Epoch [2/4], Step [49800/64066], Loss: 5.1875\n",
      "Epoch [2/4], Step [49875/64066], Loss: 5.3008\n",
      "Epoch [2/4], Step [49950/64066], Loss: 5.1593\n",
      "Validation perplexity: 164.84233781547425\n",
      "Epoch [2/4], Step [50025/64066], Loss: 5.3091\n",
      "Epoch [2/4], Step [50100/64066], Loss: 5.3450\n",
      "Epoch [2/4], Step [50175/64066], Loss: 5.2113\n",
      "Epoch [2/4], Step [50250/64066], Loss: 5.1226\n",
      "Epoch [2/4], Step [50325/64066], Loss: 5.3158\n",
      "Epoch [2/4], Step [50400/64066], Loss: 5.2611\n",
      "Epoch [2/4], Step [50475/64066], Loss: 5.1559\n",
      "Epoch [2/4], Step [50550/64066], Loss: 5.3706\n",
      "Epoch [2/4], Step [50625/64066], Loss: 5.1899\n",
      "Epoch [2/4], Step [50700/64066], Loss: 5.2010\n",
      "Epoch [2/4], Step [50775/64066], Loss: 5.1963\n",
      "Epoch [2/4], Step [50850/64066], Loss: 5.2515\n",
      "Epoch [2/4], Step [50925/64066], Loss: 5.2360\n",
      "Epoch [2/4], Step [51000/64066], Loss: 5.2003\n",
      "Epoch [2/4], Step [51075/64066], Loss: 5.1636\n",
      "Epoch [2/4], Step [51150/64066], Loss: 5.2472\n",
      "Epoch [2/4], Step [51225/64066], Loss: 5.1993\n",
      "Epoch [2/4], Step [51300/64066], Loss: 5.2886\n",
      "Epoch [2/4], Step [51375/64066], Loss: 5.2882\n",
      "Epoch [2/4], Step [51450/64066], Loss: 5.4846\n",
      "Epoch [2/4], Step [51525/64066], Loss: 5.1143\n",
      "Epoch [2/4], Step [51600/64066], Loss: 5.2832\n",
      "Epoch [2/4], Step [51675/64066], Loss: 5.1574\n",
      "Epoch [2/4], Step [51750/64066], Loss: 5.2122\n",
      "Epoch [2/4], Step [51825/64066], Loss: 5.2927\n",
      "Epoch [2/4], Step [51900/64066], Loss: 5.3087\n",
      "Epoch [2/4], Step [51975/64066], Loss: 5.0951\n",
      "Epoch [2/4], Step [52050/64066], Loss: 5.3566\n",
      "Epoch [2/4], Step [52125/64066], Loss: 5.3247\n",
      "Epoch [2/4], Step [52200/64066], Loss: 5.3379\n",
      "Epoch [2/4], Step [52275/64066], Loss: 5.1988\n",
      "Epoch [2/4], Step [52350/64066], Loss: 5.3418\n",
      "Epoch [2/4], Step [52425/64066], Loss: 5.1518\n",
      "Epoch [2/4], Step [52500/64066], Loss: 5.3422\n",
      "Epoch [2/4], Step [52575/64066], Loss: 5.2185\n",
      "Epoch [2/4], Step [52650/64066], Loss: 5.3127\n",
      "Epoch [2/4], Step [52725/64066], Loss: 5.3922\n",
      "Epoch [2/4], Step [52800/64066], Loss: 5.2096\n",
      "Epoch [2/4], Step [52875/64066], Loss: 5.3041\n",
      "Epoch [2/4], Step [52950/64066], Loss: 5.1774\n",
      "Epoch [2/4], Step [53025/64066], Loss: 5.3066\n",
      "Epoch [2/4], Step [53100/64066], Loss: 5.1054\n",
      "Epoch [2/4], Step [53175/64066], Loss: 5.3032\n",
      "Epoch [2/4], Step [53250/64066], Loss: 5.4946\n",
      "Epoch [2/4], Step [53325/64066], Loss: 5.2417\n",
      "Epoch [2/4], Step [53400/64066], Loss: 5.2394\n",
      "Epoch [2/4], Step [53475/64066], Loss: 5.3550\n",
      "Epoch [2/4], Step [53550/64066], Loss: 5.3641\n",
      "Epoch [2/4], Step [53625/64066], Loss: 5.2303\n",
      "Epoch [2/4], Step [53700/64066], Loss: 5.1989\n",
      "Epoch [2/4], Step [53775/64066], Loss: 5.3058\n",
      "Epoch [2/4], Step [53850/64066], Loss: 5.2992\n",
      "Epoch [2/4], Step [53925/64066], Loss: 5.2656\n",
      "Epoch [2/4], Step [54000/64066], Loss: 5.2098\n",
      "Epoch [2/4], Step [54075/64066], Loss: 5.3273\n",
      "Epoch [2/4], Step [54150/64066], Loss: 5.4113\n",
      "Epoch [2/4], Step [54225/64066], Loss: 5.0344\n",
      "Epoch [2/4], Step [54300/64066], Loss: 5.1521\n",
      "Epoch [2/4], Step [54375/64066], Loss: 5.3116\n",
      "Epoch [2/4], Step [54450/64066], Loss: 5.3453\n",
      "Epoch [2/4], Step [54525/64066], Loss: 5.3080\n",
      "Epoch [2/4], Step [54600/64066], Loss: 5.2677\n",
      "Epoch [2/4], Step [54675/64066], Loss: 5.1124\n",
      "Epoch [2/4], Step [54750/64066], Loss: 5.2990\n",
      "Epoch [2/4], Step [54825/64066], Loss: 5.2518\n",
      "Epoch [2/4], Step [54900/64066], Loss: 5.2037\n",
      "Epoch [2/4], Step [54975/64066], Loss: 5.1763\n",
      "Epoch [2/4], Step [55050/64066], Loss: 5.2332\n",
      "Epoch [2/4], Step [55125/64066], Loss: 5.2352\n",
      "Epoch [2/4], Step [55200/64066], Loss: 5.2603\n",
      "Epoch [2/4], Step [55275/64066], Loss: 5.1943\n",
      "Epoch [2/4], Step [55350/64066], Loss: 5.1884\n",
      "Epoch [2/4], Step [55425/64066], Loss: 5.1926\n",
      "Epoch [2/4], Step [55500/64066], Loss: 5.1840\n",
      "Epoch [2/4], Step [55575/64066], Loss: 5.2691\n",
      "Epoch [2/4], Step [55650/64066], Loss: 5.3048\n",
      "Epoch [2/4], Step [55725/64066], Loss: 5.2812\n",
      "Epoch [2/4], Step [55800/64066], Loss: 5.2273\n",
      "Epoch [2/4], Step [55875/64066], Loss: 5.1140\n",
      "Epoch [2/4], Step [55950/64066], Loss: 5.0873\n",
      "Epoch [2/4], Step [56025/64066], Loss: 5.3065\n",
      "Epoch [2/4], Step [56100/64066], Loss: 5.2755\n",
      "Epoch [2/4], Step [56175/64066], Loss: 5.1773\n",
      "Epoch [2/4], Step [56250/64066], Loss: 5.4200\n",
      "Epoch [2/4], Step [56325/64066], Loss: 5.3135\n",
      "Epoch [2/4], Step [56400/64066], Loss: 5.2090\n",
      "Epoch [2/4], Step [56475/64066], Loss: 5.3064\n",
      "Epoch [2/4], Step [56550/64066], Loss: 5.3301\n",
      "Epoch [2/4], Step [56625/64066], Loss: 5.2299\n",
      "Epoch [2/4], Step [56700/64066], Loss: 5.2712\n",
      "Epoch [2/4], Step [56775/64066], Loss: 5.1986\n",
      "Epoch [2/4], Step [56850/64066], Loss: 5.3862\n",
      "Epoch [2/4], Step [56925/64066], Loss: 5.1737\n",
      "Epoch [2/4], Step [57000/64066], Loss: 5.1382\n",
      "Epoch [2/4], Step [57075/64066], Loss: 5.2293\n",
      "Epoch [2/4], Step [57150/64066], Loss: 5.4372\n",
      "Epoch [2/4], Step [57225/64066], Loss: 5.2936\n",
      "Epoch [2/4], Step [57300/64066], Loss: 5.1845\n",
      "Epoch [2/4], Step [57375/64066], Loss: 5.1767\n",
      "Epoch [2/4], Step [57450/64066], Loss: 5.2144\n",
      "Epoch [2/4], Step [57525/64066], Loss: 5.1102\n",
      "Epoch [2/4], Step [57600/64066], Loss: 5.2835\n",
      "Epoch [2/4], Step [57675/64066], Loss: 5.2231\n",
      "Epoch [2/4], Step [57750/64066], Loss: 5.1308\n",
      "Epoch [2/4], Step [57825/64066], Loss: 5.3474\n",
      "Epoch [2/4], Step [57900/64066], Loss: 5.1569\n",
      "Epoch [2/4], Step [57975/64066], Loss: 5.2049\n",
      "Epoch [2/4], Step [58050/64066], Loss: 5.3316\n",
      "Epoch [2/4], Step [58125/64066], Loss: 5.3667\n",
      "Epoch [2/4], Step [58200/64066], Loss: 5.1994\n",
      "Epoch [2/4], Step [58275/64066], Loss: 5.1923\n",
      "Epoch [2/4], Step [58350/64066], Loss: 5.1256\n",
      "Epoch [2/4], Step [58425/64066], Loss: 5.2344\n",
      "Epoch [2/4], Step [58500/64066], Loss: 5.1273\n",
      "Epoch [2/4], Step [58575/64066], Loss: 5.2183\n",
      "Epoch [2/4], Step [58650/64066], Loss: 5.4968\n",
      "Epoch [2/4], Step [58725/64066], Loss: 5.2546\n",
      "Epoch [2/4], Step [58800/64066], Loss: 5.0308\n",
      "Epoch [2/4], Step [58875/64066], Loss: 5.3409\n",
      "Epoch [2/4], Step [58950/64066], Loss: 5.3018\n",
      "Epoch [2/4], Step [59025/64066], Loss: 5.2500\n",
      "Epoch [2/4], Step [59100/64066], Loss: 5.2453\n",
      "Epoch [2/4], Step [59175/64066], Loss: 5.1300\n",
      "Epoch [2/4], Step [59250/64066], Loss: 5.2425\n",
      "Epoch [2/4], Step [59325/64066], Loss: 5.3069\n",
      "Epoch [2/4], Step [59400/64066], Loss: 5.3308\n",
      "Epoch [2/4], Step [59475/64066], Loss: 5.3618\n",
      "Epoch [2/4], Step [59550/64066], Loss: 5.3432\n",
      "Epoch [2/4], Step [59625/64066], Loss: 5.1531\n",
      "Epoch [2/4], Step [59700/64066], Loss: 5.2205\n",
      "Epoch [2/4], Step [59775/64066], Loss: 5.2826\n",
      "Epoch [2/4], Step [59850/64066], Loss: 5.0841\n",
      "Epoch [2/4], Step [59925/64066], Loss: 5.2817\n",
      "Epoch [2/4], Step [60000/64066], Loss: 5.3635\n",
      "Validation perplexity: 163.42067326435145\n",
      "Epoch [2/4], Step [60075/64066], Loss: 5.1559\n",
      "Epoch [2/4], Step [60150/64066], Loss: 5.2941\n",
      "Epoch [2/4], Step [60225/64066], Loss: 5.3480\n",
      "Epoch [2/4], Step [60300/64066], Loss: 5.1922\n",
      "Epoch [2/4], Step [60375/64066], Loss: 5.3030\n",
      "Epoch [2/4], Step [60450/64066], Loss: 5.2360\n",
      "Epoch [2/4], Step [60525/64066], Loss: 5.1326\n",
      "Epoch [2/4], Step [60600/64066], Loss: 5.1038\n",
      "Epoch [2/4], Step [60675/64066], Loss: 5.2878\n",
      "Epoch [2/4], Step [60750/64066], Loss: 5.3144\n",
      "Epoch [2/4], Step [60825/64066], Loss: 5.2622\n",
      "Epoch [2/4], Step [60900/64066], Loss: 5.2151\n",
      "Epoch [2/4], Step [60975/64066], Loss: 5.2355\n",
      "Epoch [2/4], Step [61050/64066], Loss: 5.2875\n",
      "Epoch [2/4], Step [61125/64066], Loss: 5.2312\n",
      "Epoch [2/4], Step [61200/64066], Loss: 5.2490\n",
      "Epoch [2/4], Step [61275/64066], Loss: 5.2302\n",
      "Epoch [2/4], Step [61350/64066], Loss: 5.2183\n",
      "Epoch [2/4], Step [61425/64066], Loss: 5.2709\n",
      "Epoch [2/4], Step [61500/64066], Loss: 5.1867\n",
      "Epoch [2/4], Step [61575/64066], Loss: 4.9949\n",
      "Epoch [2/4], Step [61650/64066], Loss: 5.2203\n",
      "Epoch [2/4], Step [61725/64066], Loss: 5.1030\n",
      "Epoch [2/4], Step [61800/64066], Loss: 5.0537\n",
      "Epoch [2/4], Step [61875/64066], Loss: 5.3187\n",
      "Epoch [2/4], Step [61950/64066], Loss: 5.3668\n",
      "Epoch [2/4], Step [62025/64066], Loss: 5.3224\n",
      "Epoch [2/4], Step [62100/64066], Loss: 5.1556\n",
      "Epoch [2/4], Step [62175/64066], Loss: 5.2346\n",
      "Epoch [2/4], Step [62250/64066], Loss: 5.1772\n",
      "Epoch [2/4], Step [62325/64066], Loss: 5.3621\n",
      "Epoch [2/4], Step [62400/64066], Loss: 5.2198\n",
      "Epoch [2/4], Step [62475/64066], Loss: 5.2447\n",
      "Epoch [2/4], Step [62550/64066], Loss: 5.2126\n",
      "Epoch [2/4], Step [62625/64066], Loss: 5.1526\n",
      "Epoch [2/4], Step [62700/64066], Loss: 5.3213\n",
      "Epoch [2/4], Step [62775/64066], Loss: 5.2156\n",
      "Epoch [2/4], Step [62850/64066], Loss: 5.1823\n",
      "Epoch [2/4], Step [62925/64066], Loss: 5.3733\n",
      "Epoch [2/4], Step [63000/64066], Loss: 5.3202\n",
      "Epoch [2/4], Step [63075/64066], Loss: 5.3233\n",
      "Epoch [2/4], Step [63150/64066], Loss: 5.2697\n",
      "Epoch [2/4], Step [63225/64066], Loss: 5.2514\n",
      "Epoch [2/4], Step [63300/64066], Loss: 5.2051\n",
      "Epoch [2/4], Step [63375/64066], Loss: 5.2938\n",
      "Epoch [2/4], Step [63450/64066], Loss: 5.2500\n",
      "Epoch [2/4], Step [63525/64066], Loss: 5.1682\n",
      "Epoch [2/4], Step [63600/64066], Loss: 5.2150\n",
      "Epoch [2/4], Step [63675/64066], Loss: 5.3255\n",
      "Epoch [2/4], Step [63750/64066], Loss: 5.3559\n",
      "Epoch [2/4], Step [63825/64066], Loss: 5.1022\n",
      "Epoch [2/4], Step [63900/64066], Loss: 5.2513\n",
      "Epoch [2/4], Step [63975/64066], Loss: 5.2653\n",
      "Epoch [2/4], Step [64050/64066], Loss: 5.2147\n",
      "Epoch [2/4] Average Loss: 5.2845, Perplexity: 197.26\n",
      "Epoch [3/4], Step [0/64066], Loss: 5.1812\n",
      "Validation perplexity: 162.5561000658299\n",
      "Epoch [3/4], Step [75/64066], Loss: 5.0860\n",
      "Epoch [3/4], Step [150/64066], Loss: 5.3227\n",
      "Epoch [3/4], Step [225/64066], Loss: 4.9852\n",
      "Epoch [3/4], Step [300/64066], Loss: 5.3919\n",
      "Epoch [3/4], Step [375/64066], Loss: 5.2914\n",
      "Epoch [3/4], Step [450/64066], Loss: 5.0714\n",
      "Epoch [3/4], Step [525/64066], Loss: 5.2810\n",
      "Epoch [3/4], Step [600/64066], Loss: 5.2374\n",
      "Epoch [3/4], Step [675/64066], Loss: 5.2571\n",
      "Epoch [3/4], Step [750/64066], Loss: 5.3097\n",
      "Epoch [3/4], Step [825/64066], Loss: 5.2525\n",
      "Epoch [3/4], Step [900/64066], Loss: 5.2823\n",
      "Epoch [3/4], Step [975/64066], Loss: 5.1959\n",
      "Epoch [3/4], Step [1050/64066], Loss: 5.1961\n",
      "Epoch [3/4], Step [1125/64066], Loss: 5.0953\n",
      "Epoch [3/4], Step [1200/64066], Loss: 5.3202\n",
      "Epoch [3/4], Step [1275/64066], Loss: 5.3019\n",
      "Epoch [3/4], Step [1350/64066], Loss: 5.4315\n",
      "Epoch [3/4], Step [1425/64066], Loss: 5.3040\n",
      "Epoch [3/4], Step [1500/64066], Loss: 5.1654\n",
      "Epoch [3/4], Step [1575/64066], Loss: 5.2884\n",
      "Epoch [3/4], Step [1650/64066], Loss: 5.2111\n",
      "Epoch [3/4], Step [1725/64066], Loss: 5.3460\n",
      "Epoch [3/4], Step [1800/64066], Loss: 5.1301\n",
      "Epoch [3/4], Step [1875/64066], Loss: 5.3621\n",
      "Epoch [3/4], Step [1950/64066], Loss: 5.2771\n",
      "Epoch [3/4], Step [2025/64066], Loss: 5.2757\n",
      "Epoch [3/4], Step [2100/64066], Loss: 5.2261\n",
      "Epoch [3/4], Step [2175/64066], Loss: 5.3828\n",
      "Epoch [3/4], Step [2250/64066], Loss: 5.3447\n",
      "Epoch [3/4], Step [2325/64066], Loss: 5.4498\n",
      "Epoch [3/4], Step [2400/64066], Loss: 5.2711\n",
      "Epoch [3/4], Step [2475/64066], Loss: 5.2518\n",
      "Epoch [3/4], Step [2550/64066], Loss: 5.2242\n",
      "Epoch [3/4], Step [2625/64066], Loss: 5.1493\n",
      "Epoch [3/4], Step [2700/64066], Loss: 5.1652\n",
      "Epoch [3/4], Step [2775/64066], Loss: 5.4381\n",
      "Epoch [3/4], Step [2850/64066], Loss: 5.2597\n",
      "Epoch [3/4], Step [2925/64066], Loss: 5.1691\n",
      "Epoch [3/4], Step [3000/64066], Loss: 5.1790\n",
      "Epoch [3/4], Step [3075/64066], Loss: 5.1305\n",
      "Epoch [3/4], Step [3150/64066], Loss: 5.2737\n",
      "Epoch [3/4], Step [3225/64066], Loss: 5.3636\n",
      "Epoch [3/4], Step [3300/64066], Loss: 5.2737\n",
      "Epoch [3/4], Step [3375/64066], Loss: 5.3784\n",
      "Epoch [3/4], Step [3450/64066], Loss: 5.2174\n",
      "Epoch [3/4], Step [3525/64066], Loss: 5.2369\n",
      "Epoch [3/4], Step [3600/64066], Loss: 5.1168\n",
      "Epoch [3/4], Step [3675/64066], Loss: 5.1882\n",
      "Epoch [3/4], Step [3750/64066], Loss: 5.2939\n",
      "Epoch [3/4], Step [3825/64066], Loss: 5.2496\n",
      "Epoch [3/4], Step [3900/64066], Loss: 5.2835\n",
      "Epoch [3/4], Step [3975/64066], Loss: 5.3067\n",
      "Epoch [3/4], Step [4050/64066], Loss: 5.1259\n",
      "Epoch [3/4], Step [4125/64066], Loss: 5.1431\n",
      "Epoch [3/4], Step [4200/64066], Loss: 5.3798\n",
      "Epoch [3/4], Step [4275/64066], Loss: 5.1839\n",
      "Epoch [3/4], Step [4350/64066], Loss: 5.1934\n",
      "Epoch [3/4], Step [4425/64066], Loss: 5.3683\n",
      "Epoch [3/4], Step [4500/64066], Loss: 5.1838\n",
      "Epoch [3/4], Step [4575/64066], Loss: 5.1460\n",
      "Epoch [3/4], Step [4650/64066], Loss: 5.2249\n",
      "Epoch [3/4], Step [4725/64066], Loss: 5.0898\n",
      "Epoch [3/4], Step [4800/64066], Loss: 5.3228\n",
      "Epoch [3/4], Step [4875/64066], Loss: 5.1477\n",
      "Epoch [3/4], Step [4950/64066], Loss: 5.2594\n",
      "Epoch [3/4], Step [5025/64066], Loss: 5.1324\n",
      "Epoch [3/4], Step [5100/64066], Loss: 5.1835\n",
      "Epoch [3/4], Step [5175/64066], Loss: 5.3919\n",
      "Epoch [3/4], Step [5250/64066], Loss: 5.1389\n",
      "Epoch [3/4], Step [5325/64066], Loss: 5.3040\n",
      "Epoch [3/4], Step [5400/64066], Loss: 5.4483\n",
      "Epoch [3/4], Step [5475/64066], Loss: 5.1856\n",
      "Epoch [3/4], Step [5550/64066], Loss: 5.1874\n",
      "Epoch [3/4], Step [5625/64066], Loss: 5.1428\n",
      "Epoch [3/4], Step [5700/64066], Loss: 5.1266\n",
      "Epoch [3/4], Step [5775/64066], Loss: 5.1244\n",
      "Epoch [3/4], Step [5850/64066], Loss: 5.1972\n",
      "Epoch [3/4], Step [5925/64066], Loss: 5.1956\n",
      "Epoch [3/4], Step [6000/64066], Loss: 5.2777\n",
      "Epoch [3/4], Step [6075/64066], Loss: 5.3913\n",
      "Epoch [3/4], Step [6150/64066], Loss: 5.1785\n",
      "Epoch [3/4], Step [6225/64066], Loss: 5.2066\n",
      "Epoch [3/4], Step [6300/64066], Loss: 5.2817\n",
      "Epoch [3/4], Step [6375/64066], Loss: 5.2676\n",
      "Epoch [3/4], Step [6450/64066], Loss: 5.2053\n",
      "Epoch [3/4], Step [6525/64066], Loss: 5.4228\n",
      "Epoch [3/4], Step [6600/64066], Loss: 5.1128\n",
      "Epoch [3/4], Step [6675/64066], Loss: 5.1081\n",
      "Epoch [3/4], Step [6750/64066], Loss: 5.4007\n",
      "Epoch [3/4], Step [6825/64066], Loss: 5.2917\n",
      "Epoch [3/4], Step [6900/64066], Loss: 5.2843\n",
      "Epoch [3/4], Step [6975/64066], Loss: 5.4687\n",
      "Epoch [3/4], Step [7050/64066], Loss: 5.2986\n",
      "Epoch [3/4], Step [7125/64066], Loss: 5.2491\n",
      "Epoch [3/4], Step [7200/64066], Loss: 5.2989\n",
      "Epoch [3/4], Step [7275/64066], Loss: 5.2096\n",
      "Epoch [3/4], Step [7350/64066], Loss: 5.3753\n",
      "Epoch [3/4], Step [7425/64066], Loss: 5.1388\n",
      "Epoch [3/4], Step [7500/64066], Loss: 5.1304\n",
      "Epoch [3/4], Step [7575/64066], Loss: 5.3475\n",
      "Epoch [3/4], Step [7650/64066], Loss: 5.1829\n",
      "Epoch [3/4], Step [7725/64066], Loss: 5.2350\n",
      "Epoch [3/4], Step [7800/64066], Loss: 5.1905\n",
      "Epoch [3/4], Step [7875/64066], Loss: 5.1156\n",
      "Epoch [3/4], Step [7950/64066], Loss: 5.2993\n",
      "Epoch [3/4], Step [8025/64066], Loss: 5.2831\n",
      "Epoch [3/4], Step [8100/64066], Loss: 5.1734\n",
      "Epoch [3/4], Step [8175/64066], Loss: 5.2365\n",
      "Epoch [3/4], Step [8250/64066], Loss: 5.2513\n",
      "Epoch [3/4], Step [8325/64066], Loss: 5.2821\n",
      "Epoch [3/4], Step [8400/64066], Loss: 5.1925\n",
      "Epoch [3/4], Step [8475/64066], Loss: 5.2377\n",
      "Epoch [3/4], Step [8550/64066], Loss: 5.1715\n",
      "Epoch [3/4], Step [8625/64066], Loss: 5.1898\n",
      "Epoch [3/4], Step [8700/64066], Loss: 5.2663\n",
      "Epoch [3/4], Step [8775/64066], Loss: 5.3773\n",
      "Epoch [3/4], Step [8850/64066], Loss: 5.1656\n",
      "Epoch [3/4], Step [8925/64066], Loss: 5.2618\n",
      "Epoch [3/4], Step [9000/64066], Loss: 5.2866\n",
      "Epoch [3/4], Step [9075/64066], Loss: 5.3222\n",
      "Epoch [3/4], Step [9150/64066], Loss: 5.3685\n",
      "Epoch [3/4], Step [9225/64066], Loss: 5.1796\n",
      "Epoch [3/4], Step [9300/64066], Loss: 5.0849\n",
      "Epoch [3/4], Step [9375/64066], Loss: 5.2560\n",
      "Epoch [3/4], Step [9450/64066], Loss: 5.1493\n",
      "Epoch [3/4], Step [9525/64066], Loss: 5.3655\n",
      "Epoch [3/4], Step [9600/64066], Loss: 5.2911\n",
      "Epoch [3/4], Step [9675/64066], Loss: 5.2284\n",
      "Epoch [3/4], Step [9750/64066], Loss: 5.1969\n",
      "Epoch [3/4], Step [9825/64066], Loss: 5.2692\n",
      "Epoch [3/4], Step [9900/64066], Loss: 5.2194\n",
      "Epoch [3/4], Step [9975/64066], Loss: 5.0885\n",
      "Validation perplexity: 161.42901128058102\n",
      "Epoch [3/4], Step [10050/64066], Loss: 5.2881\n",
      "Epoch [3/4], Step [10125/64066], Loss: 5.2720\n",
      "Epoch [3/4], Step [10200/64066], Loss: 5.0544\n",
      "Epoch [3/4], Step [10275/64066], Loss: 5.2913\n",
      "Epoch [3/4], Step [10350/64066], Loss: 5.2592\n",
      "Epoch [3/4], Step [10425/64066], Loss: 5.3247\n",
      "Epoch [3/4], Step [10500/64066], Loss: 5.1890\n",
      "Epoch [3/4], Step [10575/64066], Loss: 5.2410\n",
      "Epoch [3/4], Step [10650/64066], Loss: 5.2645\n",
      "Epoch [3/4], Step [10725/64066], Loss: 5.0627\n",
      "Epoch [3/4], Step [10800/64066], Loss: 5.4257\n",
      "Epoch [3/4], Step [10875/64066], Loss: 5.4096\n",
      "Epoch [3/4], Step [10950/64066], Loss: 5.3449\n",
      "Epoch [3/4], Step [11025/64066], Loss: 5.2412\n",
      "Epoch [3/4], Step [11100/64066], Loss: 5.3775\n",
      "Epoch [3/4], Step [11175/64066], Loss: 5.4287\n",
      "Epoch [3/4], Step [11250/64066], Loss: 5.1953\n",
      "Epoch [3/4], Step [11325/64066], Loss: 5.1994\n",
      "Epoch [3/4], Step [11400/64066], Loss: 5.2375\n",
      "Epoch [3/4], Step [11475/64066], Loss: 5.2944\n",
      "Epoch [3/4], Step [11550/64066], Loss: 5.3559\n",
      "Epoch [3/4], Step [11625/64066], Loss: 5.2563\n",
      "Epoch [3/4], Step [11700/64066], Loss: 5.3547\n",
      "Epoch [3/4], Step [11775/64066], Loss: 5.2913\n",
      "Epoch [3/4], Step [11850/64066], Loss: 5.2953\n",
      "Epoch [3/4], Step [11925/64066], Loss: 5.2276\n",
      "Epoch [3/4], Step [12000/64066], Loss: 5.1049\n",
      "Epoch [3/4], Step [12075/64066], Loss: 5.2177\n",
      "Epoch [3/4], Step [12150/64066], Loss: 5.3071\n",
      "Epoch [3/4], Step [12225/64066], Loss: 5.2282\n",
      "Epoch [3/4], Step [12300/64066], Loss: 5.3435\n",
      "Epoch [3/4], Step [12375/64066], Loss: 5.2970\n",
      "Epoch [3/4], Step [12450/64066], Loss: 5.1338\n",
      "Epoch [3/4], Step [12525/64066], Loss: 5.1757\n",
      "Epoch [3/4], Step [12600/64066], Loss: 5.2334\n",
      "Epoch [3/4], Step [12675/64066], Loss: 5.1797\n",
      "Epoch [3/4], Step [12750/64066], Loss: 5.2466\n",
      "Epoch [3/4], Step [12825/64066], Loss: 5.2945\n",
      "Epoch [3/4], Step [12900/64066], Loss: 5.2388\n",
      "Epoch [3/4], Step [12975/64066], Loss: 5.1703\n",
      "Epoch [3/4], Step [13050/64066], Loss: 5.3663\n",
      "Epoch [3/4], Step [13125/64066], Loss: 5.0919\n",
      "Epoch [3/4], Step [13200/64066], Loss: 5.3059\n",
      "Epoch [3/4], Step [13275/64066], Loss: 5.2237\n",
      "Epoch [3/4], Step [13350/64066], Loss: 5.2757\n",
      "Epoch [3/4], Step [13425/64066], Loss: 4.9896\n",
      "Epoch [3/4], Step [13500/64066], Loss: 5.2761\n",
      "Epoch [3/4], Step [13575/64066], Loss: 5.4122\n",
      "Epoch [3/4], Step [13650/64066], Loss: 5.2254\n",
      "Epoch [3/4], Step [13725/64066], Loss: 5.1407\n",
      "Epoch [3/4], Step [13800/64066], Loss: 5.2050\n",
      "Epoch [3/4], Step [13875/64066], Loss: 5.2872\n",
      "Epoch [3/4], Step [13950/64066], Loss: 4.9875\n",
      "Epoch [3/4], Step [14025/64066], Loss: 5.2039\n",
      "Epoch [3/4], Step [14100/64066], Loss: 5.3984\n",
      "Epoch [3/4], Step [14175/64066], Loss: 5.2736\n",
      "Epoch [3/4], Step [14250/64066], Loss: 5.1560\n",
      "Epoch [3/4], Step [14325/64066], Loss: 5.1329\n",
      "Epoch [3/4], Step [14400/64066], Loss: 5.3258\n",
      "Epoch [3/4], Step [14475/64066], Loss: 5.2889\n",
      "Epoch [3/4], Step [14550/64066], Loss: 5.1969\n",
      "Epoch [3/4], Step [14625/64066], Loss: 5.2523\n",
      "Epoch [3/4], Step [14700/64066], Loss: 5.3306\n",
      "Epoch [3/4], Step [14775/64066], Loss: 5.1496\n",
      "Epoch [3/4], Step [14850/64066], Loss: 5.2200\n",
      "Epoch [3/4], Step [14925/64066], Loss: 5.2203\n",
      "Epoch [3/4], Step [15000/64066], Loss: 5.1008\n",
      "Epoch [3/4], Step [15075/64066], Loss: 5.2224\n",
      "Epoch [3/4], Step [15150/64066], Loss: 5.2358\n",
      "Epoch [3/4], Step [15225/64066], Loss: 5.2161\n",
      "Epoch [3/4], Step [15300/64066], Loss: 5.2615\n",
      "Epoch [3/4], Step [15375/64066], Loss: 5.2882\n",
      "Epoch [3/4], Step [15450/64066], Loss: 5.0991\n",
      "Epoch [3/4], Step [15525/64066], Loss: 5.1283\n",
      "Epoch [3/4], Step [15600/64066], Loss: 5.1121\n",
      "Epoch [3/4], Step [15675/64066], Loss: 5.2701\n",
      "Epoch [3/4], Step [15750/64066], Loss: 5.2387\n",
      "Epoch [3/4], Step [15825/64066], Loss: 5.2163\n",
      "Epoch [3/4], Step [15900/64066], Loss: 5.2678\n",
      "Epoch [3/4], Step [15975/64066], Loss: 5.3080\n",
      "Epoch [3/4], Step [16050/64066], Loss: 5.2681\n",
      "Epoch [3/4], Step [16125/64066], Loss: 5.2517\n",
      "Epoch [3/4], Step [16200/64066], Loss: 5.2422\n",
      "Epoch [3/4], Step [16275/64066], Loss: 5.3506\n",
      "Epoch [3/4], Step [16350/64066], Loss: 5.3337\n",
      "Epoch [3/4], Step [16425/64066], Loss: 5.2916\n",
      "Epoch [3/4], Step [16500/64066], Loss: 5.0939\n",
      "Epoch [3/4], Step [16575/64066], Loss: 5.1578\n",
      "Epoch [3/4], Step [16650/64066], Loss: 5.3230\n",
      "Epoch [3/4], Step [16725/64066], Loss: 5.3325\n",
      "Epoch [3/4], Step [16800/64066], Loss: 5.1148\n",
      "Epoch [3/4], Step [16875/64066], Loss: 5.1997\n",
      "Epoch [3/4], Step [16950/64066], Loss: 5.1488\n",
      "Epoch [3/4], Step [17025/64066], Loss: 5.3819\n",
      "Epoch [3/4], Step [17100/64066], Loss: 5.2088\n",
      "Epoch [3/4], Step [17175/64066], Loss: 5.1570\n",
      "Epoch [3/4], Step [17250/64066], Loss: 5.1995\n",
      "Epoch [3/4], Step [17325/64066], Loss: 5.1909\n",
      "Epoch [3/4], Step [17400/64066], Loss: 5.1966\n",
      "Epoch [3/4], Step [17475/64066], Loss: 5.0986\n",
      "Epoch [3/4], Step [17550/64066], Loss: 5.1260\n",
      "Epoch [3/4], Step [17625/64066], Loss: 5.2817\n",
      "Epoch [3/4], Step [17700/64066], Loss: 5.2792\n",
      "Epoch [3/4], Step [17775/64066], Loss: 5.2204\n",
      "Epoch [3/4], Step [17850/64066], Loss: 5.2108\n",
      "Epoch [3/4], Step [17925/64066], Loss: 5.2414\n",
      "Epoch [3/4], Step [18000/64066], Loss: 5.3119\n",
      "Epoch [3/4], Step [18075/64066], Loss: 5.2887\n",
      "Epoch [3/4], Step [18150/64066], Loss: 5.2415\n",
      "Epoch [3/4], Step [18225/64066], Loss: 5.2054\n",
      "Epoch [3/4], Step [18300/64066], Loss: 5.1178\n",
      "Epoch [3/4], Step [18375/64066], Loss: 5.2482\n",
      "Epoch [3/4], Step [18450/64066], Loss: 5.2883\n",
      "Epoch [3/4], Step [18525/64066], Loss: 5.2254\n",
      "Epoch [3/4], Step [18600/64066], Loss: 5.1171\n",
      "Epoch [3/4], Step [18675/64066], Loss: 5.2928\n",
      "Epoch [3/4], Step [18750/64066], Loss: 5.1654\n",
      "Epoch [3/4], Step [18825/64066], Loss: 5.2450\n",
      "Epoch [3/4], Step [18900/64066], Loss: 5.2085\n",
      "Epoch [3/4], Step [18975/64066], Loss: 5.2244\n",
      "Epoch [3/4], Step [19050/64066], Loss: 5.3232\n",
      "Epoch [3/4], Step [19125/64066], Loss: 5.2726\n",
      "Epoch [3/4], Step [19200/64066], Loss: 5.2960\n",
      "Epoch [3/4], Step [19275/64066], Loss: 5.2771\n",
      "Epoch [3/4], Step [19350/64066], Loss: 5.4275\n",
      "Epoch [3/4], Step [19425/64066], Loss: 5.3384\n",
      "Epoch [3/4], Step [19500/64066], Loss: 5.4125\n",
      "Epoch [3/4], Step [19575/64066], Loss: 5.2018\n",
      "Epoch [3/4], Step [19650/64066], Loss: 5.1659\n",
      "Epoch [3/4], Step [19725/64066], Loss: 5.2798\n",
      "Epoch [3/4], Step [19800/64066], Loss: 5.0294\n",
      "Epoch [3/4], Step [19875/64066], Loss: 5.2253\n",
      "Epoch [3/4], Step [19950/64066], Loss: 5.2280\n",
      "Validation perplexity: 160.26656797540218\n",
      "Epoch [3/4], Step [20025/64066], Loss: 5.3197\n",
      "Epoch [3/4], Step [20100/64066], Loss: 5.3679\n",
      "Epoch [3/4], Step [20175/64066], Loss: 5.1257\n",
      "Epoch [3/4], Step [20250/64066], Loss: 5.3012\n",
      "Epoch [3/4], Step [20325/64066], Loss: 5.1710\n",
      "Epoch [3/4], Step [20400/64066], Loss: 5.0402\n",
      "Epoch [3/4], Step [20475/64066], Loss: 5.0458\n",
      "Epoch [3/4], Step [20550/64066], Loss: 5.1936\n",
      "Epoch [3/4], Step [20625/64066], Loss: 5.2699\n",
      "Epoch [3/4], Step [20700/64066], Loss: 5.2006\n",
      "Epoch [3/4], Step [20775/64066], Loss: 5.2344\n",
      "Epoch [3/4], Step [20850/64066], Loss: 5.3144\n",
      "Epoch [3/4], Step [20925/64066], Loss: 5.0345\n",
      "Epoch [3/4], Step [21000/64066], Loss: 5.2021\n",
      "Epoch [3/4], Step [21075/64066], Loss: 5.1590\n",
      "Epoch [3/4], Step [21150/64066], Loss: 5.3789\n",
      "Epoch [3/4], Step [21225/64066], Loss: 5.0680\n",
      "Epoch [3/4], Step [21300/64066], Loss: 5.2846\n",
      "Epoch [3/4], Step [21375/64066], Loss: 5.3162\n",
      "Epoch [3/4], Step [21450/64066], Loss: 5.1595\n",
      "Epoch [3/4], Step [21525/64066], Loss: 5.2174\n",
      "Epoch [3/4], Step [21600/64066], Loss: 5.3136\n",
      "Epoch [3/4], Step [21675/64066], Loss: 5.2987\n",
      "Epoch [3/4], Step [21750/64066], Loss: 5.2568\n",
      "Epoch [3/4], Step [21825/64066], Loss: 5.2915\n",
      "Epoch [3/4], Step [21900/64066], Loss: 5.3787\n",
      "Epoch [3/4], Step [21975/64066], Loss: 5.2913\n",
      "Epoch [3/4], Step [22050/64066], Loss: 5.1464\n",
      "Epoch [3/4], Step [22125/64066], Loss: 5.2582\n",
      "Epoch [3/4], Step [22200/64066], Loss: 5.1334\n",
      "Epoch [3/4], Step [22275/64066], Loss: 5.2655\n",
      "Epoch [3/4], Step [22350/64066], Loss: 5.1544\n",
      "Epoch [3/4], Step [22425/64066], Loss: 5.0497\n",
      "Epoch [3/4], Step [22500/64066], Loss: 5.2012\n",
      "Epoch [3/4], Step [22575/64066], Loss: 5.4363\n",
      "Epoch [3/4], Step [22650/64066], Loss: 5.1724\n",
      "Epoch [3/4], Step [22725/64066], Loss: 4.9945\n",
      "Epoch [3/4], Step [22800/64066], Loss: 5.2192\n",
      "Epoch [3/4], Step [22875/64066], Loss: 5.0613\n",
      "Epoch [3/4], Step [22950/64066], Loss: 5.2040\n",
      "Epoch [3/4], Step [23025/64066], Loss: 5.1461\n",
      "Epoch [3/4], Step [23100/64066], Loss: 5.3250\n",
      "Epoch [3/4], Step [23175/64066], Loss: 5.3558\n",
      "Epoch [3/4], Step [23250/64066], Loss: 5.2000\n",
      "Epoch [3/4], Step [23325/64066], Loss: 5.1320\n",
      "Epoch [3/4], Step [23400/64066], Loss: 5.2031\n",
      "Epoch [3/4], Step [23475/64066], Loss: 5.2253\n",
      "Epoch [3/4], Step [23550/64066], Loss: 4.8810\n",
      "Epoch [3/4], Step [23625/64066], Loss: 5.0057\n",
      "Epoch [3/4], Step [23700/64066], Loss: 5.1895\n",
      "Epoch [3/4], Step [23775/64066], Loss: 5.2418\n",
      "Epoch [3/4], Step [23850/64066], Loss: 5.1584\n",
      "Epoch [3/4], Step [23925/64066], Loss: 5.3034\n",
      "Epoch [3/4], Step [24000/64066], Loss: 5.3076\n",
      "Epoch [3/4], Step [24075/64066], Loss: 5.1993\n",
      "Epoch [3/4], Step [24150/64066], Loss: 5.3312\n",
      "Epoch [3/4], Step [24225/64066], Loss: 5.3913\n",
      "Epoch [3/4], Step [24300/64066], Loss: 5.1484\n",
      "Epoch [3/4], Step [24375/64066], Loss: 5.0883\n",
      "Epoch [3/4], Step [24450/64066], Loss: 5.1630\n",
      "Epoch [3/4], Step [24525/64066], Loss: 5.1053\n",
      "Epoch [3/4], Step [24600/64066], Loss: 5.1769\n",
      "Epoch [3/4], Step [24675/64066], Loss: 5.1849\n",
      "Epoch [3/4], Step [24750/64066], Loss: 5.2230\n",
      "Epoch [3/4], Step [24825/64066], Loss: 5.1850\n",
      "Epoch [3/4], Step [24900/64066], Loss: 5.2692\n",
      "Epoch [3/4], Step [24975/64066], Loss: 5.2083\n",
      "Epoch [3/4], Step [25050/64066], Loss: 5.2178\n",
      "Epoch [3/4], Step [25125/64066], Loss: 5.3008\n",
      "Epoch [3/4], Step [25200/64066], Loss: 5.2979\n",
      "Epoch [3/4], Step [25275/64066], Loss: 5.1631\n",
      "Epoch [3/4], Step [25350/64066], Loss: 5.2868\n",
      "Epoch [3/4], Step [25425/64066], Loss: 5.2577\n",
      "Epoch [3/4], Step [25500/64066], Loss: 5.2080\n",
      "Epoch [3/4], Step [25575/64066], Loss: 5.1189\n",
      "Epoch [3/4], Step [25650/64066], Loss: 5.3036\n",
      "Epoch [3/4], Step [25725/64066], Loss: 5.2072\n",
      "Epoch [3/4], Step [25800/64066], Loss: 5.1763\n",
      "Epoch [3/4], Step [25875/64066], Loss: 5.2354\n",
      "Epoch [3/4], Step [25950/64066], Loss: 5.3419\n",
      "Epoch [3/4], Step [26025/64066], Loss: 5.2648\n",
      "Epoch [3/4], Step [26100/64066], Loss: 5.0761\n",
      "Epoch [3/4], Step [26175/64066], Loss: 5.2926\n",
      "Epoch [3/4], Step [26250/64066], Loss: 5.3265\n",
      "Epoch [3/4], Step [26325/64066], Loss: 5.0667\n",
      "Epoch [3/4], Step [26400/64066], Loss: 5.3628\n",
      "Epoch [3/4], Step [26475/64066], Loss: 5.1930\n",
      "Epoch [3/4], Step [26550/64066], Loss: 5.3208\n",
      "Epoch [3/4], Step [26625/64066], Loss: 5.2200\n",
      "Epoch [3/4], Step [26700/64066], Loss: 5.4420\n",
      "Epoch [3/4], Step [26775/64066], Loss: 5.3252\n",
      "Epoch [3/4], Step [26850/64066], Loss: 5.2284\n",
      "Epoch [3/4], Step [26925/64066], Loss: 5.0733\n",
      "Epoch [3/4], Step [27000/64066], Loss: 5.1162\n",
      "Epoch [3/4], Step [27075/64066], Loss: 5.3557\n",
      "Epoch [3/4], Step [27150/64066], Loss: 5.2837\n",
      "Epoch [3/4], Step [27225/64066], Loss: 5.1209\n",
      "Epoch [3/4], Step [27300/64066], Loss: 5.3032\n",
      "Epoch [3/4], Step [27375/64066], Loss: 5.3413\n",
      "Epoch [3/4], Step [27450/64066], Loss: 5.1091\n",
      "Epoch [3/4], Step [27525/64066], Loss: 5.2259\n",
      "Epoch [3/4], Step [27600/64066], Loss: 5.2046\n",
      "Epoch [3/4], Step [27675/64066], Loss: 5.2345\n",
      "Epoch [3/4], Step [27750/64066], Loss: 5.1540\n",
      "Epoch [3/4], Step [27825/64066], Loss: 5.1537\n",
      "Epoch [3/4], Step [27900/64066], Loss: 5.2213\n",
      "Epoch [3/4], Step [27975/64066], Loss: 5.3699\n",
      "Epoch [3/4], Step [28050/64066], Loss: 5.2599\n",
      "Epoch [3/4], Step [28125/64066], Loss: 5.0278\n",
      "Epoch [3/4], Step [28200/64066], Loss: 5.3075\n",
      "Epoch [3/4], Step [28275/64066], Loss: 5.1995\n",
      "Epoch [3/4], Step [28350/64066], Loss: 5.2313\n",
      "Epoch [3/4], Step [28425/64066], Loss: 5.2694\n",
      "Epoch [3/4], Step [28500/64066], Loss: 5.1424\n",
      "Epoch [3/4], Step [28575/64066], Loss: 5.1506\n",
      "Epoch [3/4], Step [28650/64066], Loss: 5.2816\n",
      "Epoch [3/4], Step [28725/64066], Loss: 5.3454\n",
      "Epoch [3/4], Step [28800/64066], Loss: 5.2959\n",
      "Epoch [3/4], Step [28875/64066], Loss: 5.2810\n",
      "Epoch [3/4], Step [28950/64066], Loss: 5.1914\n",
      "Epoch [3/4], Step [29025/64066], Loss: 5.1099\n",
      "Epoch [3/4], Step [29100/64066], Loss: 5.1185\n",
      "Epoch [3/4], Step [29175/64066], Loss: 5.3878\n",
      "Epoch [3/4], Step [29250/64066], Loss: 5.3216\n",
      "Epoch [3/4], Step [29325/64066], Loss: 5.2473\n",
      "Epoch [3/4], Step [29400/64066], Loss: 5.2514\n",
      "Epoch [3/4], Step [29475/64066], Loss: 5.2065\n",
      "Epoch [3/4], Step [29550/64066], Loss: 5.1326\n",
      "Epoch [3/4], Step [29625/64066], Loss: 5.3217\n",
      "Epoch [3/4], Step [29700/64066], Loss: 5.2683\n",
      "Epoch [3/4], Step [29775/64066], Loss: 5.2811\n",
      "Epoch [3/4], Step [29850/64066], Loss: 5.2441\n",
      "Epoch [3/4], Step [29925/64066], Loss: 5.3718\n",
      "Epoch [3/4], Step [30000/64066], Loss: 5.2333\n",
      "Validation perplexity: 159.38914560207493\n",
      "Epoch [3/4], Step [30075/64066], Loss: 5.3014\n",
      "Epoch [3/4], Step [30150/64066], Loss: 5.2463\n",
      "Epoch [3/4], Step [30225/64066], Loss: 5.2328\n",
      "Epoch [3/4], Step [30300/64066], Loss: 5.1969\n",
      "Epoch [3/4], Step [30375/64066], Loss: 5.0900\n",
      "Epoch [3/4], Step [30450/64066], Loss: 5.3004\n",
      "Epoch [3/4], Step [30525/64066], Loss: 5.1624\n",
      "Epoch [3/4], Step [30600/64066], Loss: 5.1084\n",
      "Epoch [3/4], Step [30675/64066], Loss: 5.3204\n",
      "Epoch [3/4], Step [30750/64066], Loss: 5.2456\n",
      "Epoch [3/4], Step [30825/64066], Loss: 5.0729\n",
      "Epoch [3/4], Step [30900/64066], Loss: 5.2720\n",
      "Epoch [3/4], Step [30975/64066], Loss: 5.2996\n",
      "Epoch [3/4], Step [31050/64066], Loss: 5.1018\n",
      "Epoch [3/4], Step [31125/64066], Loss: 5.0137\n",
      "Epoch [3/4], Step [31200/64066], Loss: 5.3067\n",
      "Epoch [3/4], Step [31275/64066], Loss: 5.2254\n",
      "Epoch [3/4], Step [31350/64066], Loss: 5.2290\n",
      "Epoch [3/4], Step [31425/64066], Loss: 5.2203\n",
      "Epoch [3/4], Step [31500/64066], Loss: 5.1726\n",
      "Epoch [3/4], Step [31575/64066], Loss: 5.2302\n",
      "Epoch [3/4], Step [31650/64066], Loss: 5.0957\n",
      "Epoch [3/4], Step [31725/64066], Loss: 5.2503\n",
      "Epoch [3/4], Step [31800/64066], Loss: 5.1178\n",
      "Epoch [3/4], Step [31875/64066], Loss: 5.2884\n",
      "Epoch [3/4], Step [31950/64066], Loss: 5.1323\n",
      "Epoch [3/4], Step [32025/64066], Loss: 5.1062\n",
      "Epoch [3/4], Step [32100/64066], Loss: 5.1491\n",
      "Epoch [3/4], Step [32175/64066], Loss: 5.2011\n",
      "Epoch [3/4], Step [32250/64066], Loss: 5.1569\n",
      "Epoch [3/4], Step [32325/64066], Loss: 5.1930\n",
      "Epoch [3/4], Step [32400/64066], Loss: 5.3344\n",
      "Epoch [3/4], Step [32475/64066], Loss: 5.0786\n",
      "Epoch [3/4], Step [32550/64066], Loss: 5.2211\n",
      "Epoch [3/4], Step [32625/64066], Loss: 5.2069\n",
      "Epoch [3/4], Step [32700/64066], Loss: 5.2787\n",
      "Epoch [3/4], Step [32775/64066], Loss: 5.2741\n",
      "Epoch [3/4], Step [32850/64066], Loss: 5.2652\n",
      "Epoch [3/4], Step [32925/64066], Loss: 5.1540\n",
      "Epoch [3/4], Step [33000/64066], Loss: 5.3046\n",
      "Epoch [3/4], Step [33075/64066], Loss: 4.9868\n",
      "Epoch [3/4], Step [33150/64066], Loss: 5.2238\n",
      "Epoch [3/4], Step [33225/64066], Loss: 5.2388\n",
      "Epoch [3/4], Step [33300/64066], Loss: 5.3316\n",
      "Epoch [3/4], Step [33375/64066], Loss: 5.2334\n",
      "Epoch [3/4], Step [33450/64066], Loss: 5.2177\n",
      "Epoch [3/4], Step [33525/64066], Loss: 5.2522\n",
      "Epoch [3/4], Step [33600/64066], Loss: 5.2214\n",
      "Epoch [3/4], Step [33675/64066], Loss: 5.3652\n",
      "Epoch [3/4], Step [33750/64066], Loss: 5.1205\n",
      "Epoch [3/4], Step [33825/64066], Loss: 5.2064\n",
      "Epoch [3/4], Step [33900/64066], Loss: 5.3015\n",
      "Epoch [3/4], Step [33975/64066], Loss: 5.0001\n",
      "Epoch [3/4], Step [34050/64066], Loss: 5.2431\n",
      "Epoch [3/4], Step [34125/64066], Loss: 5.2816\n",
      "Epoch [3/4], Step [34200/64066], Loss: 5.3024\n",
      "Epoch [3/4], Step [34275/64066], Loss: 5.1462\n",
      "Epoch [3/4], Step [34350/64066], Loss: 5.2259\n",
      "Epoch [3/4], Step [34425/64066], Loss: 5.2953\n",
      "Epoch [3/4], Step [34500/64066], Loss: 5.2274\n",
      "Epoch [3/4], Step [34575/64066], Loss: 5.0114\n",
      "Epoch [3/4], Step [34650/64066], Loss: 5.3701\n",
      "Epoch [3/4], Step [34725/64066], Loss: 5.3220\n",
      "Epoch [3/4], Step [34800/64066], Loss: 5.3641\n",
      "Epoch [3/4], Step [34875/64066], Loss: 5.3017\n",
      "Epoch [3/4], Step [34950/64066], Loss: 5.2233\n",
      "Epoch [3/4], Step [35025/64066], Loss: 5.4671\n",
      "Epoch [3/4], Step [35100/64066], Loss: 5.2596\n",
      "Epoch [3/4], Step [35175/64066], Loss: 5.2558\n",
      "Epoch [3/4], Step [35250/64066], Loss: 5.0251\n",
      "Epoch [3/4], Step [35325/64066], Loss: 5.2445\n",
      "Epoch [3/4], Step [35400/64066], Loss: 5.3962\n",
      "Epoch [3/4], Step [35475/64066], Loss: 5.2671\n",
      "Epoch [3/4], Step [35550/64066], Loss: 5.1358\n",
      "Epoch [3/4], Step [35625/64066], Loss: 5.1787\n",
      "Epoch [3/4], Step [35700/64066], Loss: 5.3142\n",
      "Epoch [3/4], Step [35775/64066], Loss: 5.3320\n",
      "Epoch [3/4], Step [35850/64066], Loss: 5.1887\n",
      "Epoch [3/4], Step [35925/64066], Loss: 5.0919\n",
      "Epoch [3/4], Step [36000/64066], Loss: 5.2540\n",
      "Epoch [3/4], Step [36075/64066], Loss: 5.2468\n",
      "Epoch [3/4], Step [36150/64066], Loss: 5.1802\n",
      "Epoch [3/4], Step [36225/64066], Loss: 5.1320\n",
      "Epoch [3/4], Step [36300/64066], Loss: 5.3130\n",
      "Epoch [3/4], Step [36375/64066], Loss: 5.4059\n",
      "Epoch [3/4], Step [36450/64066], Loss: 5.2778\n",
      "Epoch [3/4], Step [36525/64066], Loss: 5.3263\n",
      "Epoch [3/4], Step [36600/64066], Loss: 5.2699\n",
      "Epoch [3/4], Step [36675/64066], Loss: 5.2996\n",
      "Epoch [3/4], Step [36750/64066], Loss: 5.2300\n",
      "Epoch [3/4], Step [36825/64066], Loss: 5.1399\n",
      "Epoch [3/4], Step [36900/64066], Loss: 5.4128\n",
      "Epoch [3/4], Step [36975/64066], Loss: 5.2471\n",
      "Epoch [3/4], Step [37050/64066], Loss: 5.4159\n",
      "Epoch [3/4], Step [37125/64066], Loss: 5.1048\n",
      "Epoch [3/4], Step [37200/64066], Loss: 5.4364\n",
      "Epoch [3/4], Step [37275/64066], Loss: 5.3216\n",
      "Epoch [3/4], Step [37350/64066], Loss: 5.0420\n",
      "Epoch [3/4], Step [37425/64066], Loss: 5.1986\n",
      "Epoch [3/4], Step [37500/64066], Loss: 5.2119\n",
      "Epoch [3/4], Step [37575/64066], Loss: 5.2923\n",
      "Epoch [3/4], Step [37650/64066], Loss: 5.3326\n",
      "Epoch [3/4], Step [37725/64066], Loss: 5.2570\n",
      "Epoch [3/4], Step [37800/64066], Loss: 5.2751\n",
      "Epoch [3/4], Step [37875/64066], Loss: 5.2432\n",
      "Epoch [3/4], Step [37950/64066], Loss: 5.0864\n",
      "Epoch [3/4], Step [38025/64066], Loss: 5.1952\n",
      "Epoch [3/4], Step [38100/64066], Loss: 5.2453\n",
      "Epoch [3/4], Step [38175/64066], Loss: 5.3607\n",
      "Epoch [3/4], Step [38250/64066], Loss: 5.3287\n",
      "Epoch [3/4], Step [38325/64066], Loss: 5.1965\n",
      "Epoch [3/4], Step [38400/64066], Loss: 5.1866\n",
      "Epoch [3/4], Step [38475/64066], Loss: 5.2414\n",
      "Epoch [3/4], Step [38550/64066], Loss: 5.0599\n",
      "Epoch [3/4], Step [38625/64066], Loss: 5.2119\n",
      "Epoch [3/4], Step [38700/64066], Loss: 5.1393\n",
      "Epoch [3/4], Step [38775/64066], Loss: 5.2913\n",
      "Epoch [3/4], Step [38850/64066], Loss: 5.1656\n",
      "Epoch [3/4], Step [38925/64066], Loss: 5.1555\n",
      "Epoch [3/4], Step [39000/64066], Loss: 5.1824\n",
      "Epoch [3/4], Step [39075/64066], Loss: 5.2167\n",
      "Epoch [3/4], Step [39150/64066], Loss: 5.1458\n",
      "Epoch [3/4], Step [39225/64066], Loss: 5.2907\n",
      "Epoch [3/4], Step [39300/64066], Loss: 5.2361\n",
      "Epoch [3/4], Step [39375/64066], Loss: 5.0990\n",
      "Epoch [3/4], Step [39450/64066], Loss: 5.3135\n",
      "Epoch [3/4], Step [39525/64066], Loss: 5.1973\n",
      "Epoch [3/4], Step [39600/64066], Loss: 5.2947\n",
      "Epoch [3/4], Step [39675/64066], Loss: 5.2940\n",
      "Epoch [3/4], Step [39750/64066], Loss: 5.2167\n",
      "Epoch [3/4], Step [39825/64066], Loss: 5.2642\n",
      "Epoch [3/4], Step [39900/64066], Loss: 5.3243\n",
      "Epoch [3/4], Step [39975/64066], Loss: 5.1636\n",
      "Validation perplexity: 158.53851786297707\n",
      "Epoch [3/4], Step [40050/64066], Loss: 5.2603\n",
      "Epoch [3/4], Step [40125/64066], Loss: 5.2466\n",
      "Epoch [3/4], Step [40200/64066], Loss: 5.1818\n",
      "Epoch [3/4], Step [40275/64066], Loss: 5.0567\n",
      "Epoch [3/4], Step [40350/64066], Loss: 5.2306\n",
      "Epoch [3/4], Step [40425/64066], Loss: 5.1994\n",
      "Epoch [3/4], Step [40500/64066], Loss: 5.3583\n",
      "Epoch [3/4], Step [40575/64066], Loss: 5.1389\n",
      "Epoch [3/4], Step [40650/64066], Loss: 5.2648\n",
      "Epoch [3/4], Step [40725/64066], Loss: 5.0715\n",
      "Epoch [3/4], Step [40800/64066], Loss: 5.2840\n",
      "Epoch [3/4], Step [40875/64066], Loss: 5.1565\n",
      "Epoch [3/4], Step [40950/64066], Loss: 5.2875\n",
      "Epoch [3/4], Step [41025/64066], Loss: 5.2745\n",
      "Epoch [3/4], Step [41100/64066], Loss: 5.3090\n",
      "Epoch [3/4], Step [41175/64066], Loss: 5.2286\n",
      "Epoch [3/4], Step [41250/64066], Loss: 5.3318\n",
      "Epoch [3/4], Step [41325/64066], Loss: 5.1903\n",
      "Epoch [3/4], Step [41400/64066], Loss: 5.2590\n",
      "Epoch [3/4], Step [41475/64066], Loss: 5.3092\n",
      "Epoch [3/4], Step [41550/64066], Loss: 5.1848\n",
      "Epoch [3/4], Step [41625/64066], Loss: 5.2805\n",
      "Epoch [3/4], Step [41700/64066], Loss: 5.1265\n",
      "Epoch [3/4], Step [41775/64066], Loss: 5.1402\n",
      "Epoch [3/4], Step [41850/64066], Loss: 5.1849\n",
      "Epoch [3/4], Step [41925/64066], Loss: 5.3245\n",
      "Epoch [3/4], Step [42000/64066], Loss: 5.3237\n",
      "Epoch [3/4], Step [42075/64066], Loss: 5.2973\n",
      "Epoch [3/4], Step [42150/64066], Loss: 5.3152\n",
      "Epoch [3/4], Step [42225/64066], Loss: 5.1465\n",
      "Epoch [3/4], Step [42300/64066], Loss: 5.1775\n",
      "Epoch [3/4], Step [42375/64066], Loss: 5.3639\n",
      "Epoch [3/4], Step [42450/64066], Loss: 5.1499\n",
      "Epoch [3/4], Step [42525/64066], Loss: 5.3467\n",
      "Epoch [3/4], Step [42600/64066], Loss: 5.0777\n",
      "Epoch [3/4], Step [42675/64066], Loss: 5.1055\n",
      "Epoch [3/4], Step [42750/64066], Loss: 5.2566\n",
      "Epoch [3/4], Step [42825/64066], Loss: 5.2439\n",
      "Epoch [3/4], Step [42900/64066], Loss: 5.1064\n",
      "Epoch [3/4], Step [42975/64066], Loss: 5.1861\n",
      "Epoch [3/4], Step [43050/64066], Loss: 5.2060\n",
      "Epoch [3/4], Step [43125/64066], Loss: 5.3342\n",
      "Epoch [3/4], Step [43200/64066], Loss: 5.1566\n",
      "Epoch [3/4], Step [43275/64066], Loss: 5.1026\n",
      "Epoch [3/4], Step [43350/64066], Loss: 5.2432\n",
      "Epoch [3/4], Step [43425/64066], Loss: 5.3246\n",
      "Epoch [3/4], Step [43500/64066], Loss: 5.2337\n",
      "Epoch [3/4], Step [43575/64066], Loss: 5.2287\n",
      "Epoch [3/4], Step [43650/64066], Loss: 5.2997\n",
      "Epoch [3/4], Step [43725/64066], Loss: 5.2102\n",
      "Epoch [3/4], Step [43800/64066], Loss: 5.1759\n",
      "Epoch [3/4], Step [43875/64066], Loss: 5.0572\n",
      "Epoch [3/4], Step [43950/64066], Loss: 5.2624\n",
      "Epoch [3/4], Step [44025/64066], Loss: 5.1281\n",
      "Epoch [3/4], Step [44100/64066], Loss: 5.2435\n",
      "Epoch [3/4], Step [44175/64066], Loss: 5.1074\n",
      "Epoch [3/4], Step [44250/64066], Loss: 5.2957\n",
      "Epoch [3/4], Step [44325/64066], Loss: 5.2051\n",
      "Epoch [3/4], Step [44400/64066], Loss: 5.3500\n",
      "Epoch [3/4], Step [44475/64066], Loss: 5.0912\n",
      "Epoch [3/4], Step [44550/64066], Loss: 5.1935\n",
      "Epoch [3/4], Step [44625/64066], Loss: 5.0860\n",
      "Epoch [3/4], Step [44700/64066], Loss: 5.3486\n",
      "Epoch [3/4], Step [44775/64066], Loss: 5.1677\n",
      "Epoch [3/4], Step [44850/64066], Loss: 5.3414\n",
      "Epoch [3/4], Step [44925/64066], Loss: 5.1801\n",
      "Epoch [3/4], Step [45000/64066], Loss: 5.2012\n",
      "Epoch [3/4], Step [45075/64066], Loss: 5.2436\n",
      "Epoch [3/4], Step [45150/64066], Loss: 5.2347\n",
      "Epoch [3/4], Step [45225/64066], Loss: 5.2239\n",
      "Epoch [3/4], Step [45300/64066], Loss: 5.1008\n",
      "Epoch [3/4], Step [45375/64066], Loss: 5.1441\n",
      "Epoch [3/4], Step [45450/64066], Loss: 5.2441\n",
      "Epoch [3/4], Step [45525/64066], Loss: 5.0761\n",
      "Epoch [3/4], Step [45600/64066], Loss: 5.2156\n",
      "Epoch [3/4], Step [45675/64066], Loss: 5.2106\n",
      "Epoch [3/4], Step [45750/64066], Loss: 5.3657\n",
      "Epoch [3/4], Step [45825/64066], Loss: 5.2690\n",
      "Epoch [3/4], Step [45900/64066], Loss: 5.2889\n",
      "Epoch [3/4], Step [45975/64066], Loss: 5.3144\n",
      "Epoch [3/4], Step [46050/64066], Loss: 5.2064\n",
      "Epoch [3/4], Step [46125/64066], Loss: 5.3321\n",
      "Epoch [3/4], Step [46200/64066], Loss: 5.2880\n",
      "Epoch [3/4], Step [46275/64066], Loss: 5.1687\n",
      "Epoch [3/4], Step [46350/64066], Loss: 5.2675\n",
      "Epoch [3/4], Step [46425/64066], Loss: 5.2612\n",
      "Epoch [3/4], Step [46500/64066], Loss: 5.1879\n",
      "Epoch [3/4], Step [46575/64066], Loss: 5.1886\n",
      "Epoch [3/4], Step [46650/64066], Loss: 5.2152\n",
      "Epoch [3/4], Step [46725/64066], Loss: 5.1795\n",
      "Epoch [3/4], Step [46800/64066], Loss: 5.1283\n",
      "Epoch [3/4], Step [46875/64066], Loss: 5.3597\n",
      "Epoch [3/4], Step [46950/64066], Loss: 5.2127\n",
      "Epoch [3/4], Step [47025/64066], Loss: 5.2901\n",
      "Epoch [3/4], Step [47100/64066], Loss: 5.1862\n",
      "Epoch [3/4], Step [47175/64066], Loss: 5.0951\n",
      "Epoch [3/4], Step [47250/64066], Loss: 5.0987\n",
      "Epoch [3/4], Step [47325/64066], Loss: 5.0264\n",
      "Epoch [3/4], Step [47400/64066], Loss: 5.1207\n",
      "Epoch [3/4], Step [47475/64066], Loss: 5.2984\n",
      "Epoch [3/4], Step [47550/64066], Loss: 5.2113\n",
      "Epoch [3/4], Step [47625/64066], Loss: 5.1741\n",
      "Epoch [3/4], Step [47700/64066], Loss: 5.2726\n",
      "Epoch [3/4], Step [47775/64066], Loss: 5.2159\n",
      "Epoch [3/4], Step [47850/64066], Loss: 5.1944\n",
      "Epoch [3/4], Step [47925/64066], Loss: 5.1240\n",
      "Epoch [3/4], Step [48000/64066], Loss: 5.1250\n",
      "Epoch [3/4], Step [48075/64066], Loss: 5.0437\n",
      "Epoch [3/4], Step [48150/64066], Loss: 5.2473\n",
      "Epoch [3/4], Step [48225/64066], Loss: 5.1548\n",
      "Epoch [3/4], Step [48300/64066], Loss: 5.0635\n",
      "Epoch [3/4], Step [48375/64066], Loss: 5.2208\n",
      "Epoch [3/4], Step [48450/64066], Loss: 5.1275\n",
      "Epoch [3/4], Step [48525/64066], Loss: 5.2725\n",
      "Epoch [3/4], Step [48600/64066], Loss: 5.2532\n",
      "Epoch [3/4], Step [48675/64066], Loss: 5.0361\n",
      "Epoch [3/4], Step [48750/64066], Loss: 5.2435\n",
      "Epoch [3/4], Step [48825/64066], Loss: 5.2663\n",
      "Epoch [3/4], Step [48900/64066], Loss: 5.4280\n",
      "Epoch [3/4], Step [48975/64066], Loss: 5.3045\n",
      "Epoch [3/4], Step [49050/64066], Loss: 5.3143\n",
      "Epoch [3/4], Step [49125/64066], Loss: 5.2658\n",
      "Epoch [3/4], Step [49200/64066], Loss: 5.2507\n",
      "Epoch [3/4], Step [49275/64066], Loss: 5.0899\n",
      "Epoch [3/4], Step [49350/64066], Loss: 5.1492\n",
      "Epoch [3/4], Step [49425/64066], Loss: 5.1435\n",
      "Epoch [3/4], Step [49500/64066], Loss: 5.2040\n",
      "Epoch [3/4], Step [49575/64066], Loss: 5.2290\n",
      "Epoch [3/4], Step [49650/64066], Loss: 5.1061\n",
      "Epoch [3/4], Step [49725/64066], Loss: 5.2140\n",
      "Epoch [3/4], Step [49800/64066], Loss: 5.1926\n",
      "Epoch [3/4], Step [49875/64066], Loss: 5.2911\n",
      "Epoch [3/4], Step [49950/64066], Loss: 5.2328\n",
      "Validation perplexity: 157.51825632286716\n",
      "Epoch [3/4], Step [50025/64066], Loss: 5.2743\n",
      "Epoch [3/4], Step [50100/64066], Loss: 5.1992\n",
      "Epoch [3/4], Step [50175/64066], Loss: 5.3320\n",
      "Epoch [3/4], Step [50250/64066], Loss: 5.4631\n",
      "Epoch [3/4], Step [50325/64066], Loss: 5.1809\n",
      "Epoch [3/4], Step [50400/64066], Loss: 5.1168\n",
      "Epoch [3/4], Step [50475/64066], Loss: 5.1103\n",
      "Epoch [3/4], Step [50550/64066], Loss: 5.1799\n",
      "Epoch [3/4], Step [50625/64066], Loss: 4.9844\n",
      "Epoch [3/4], Step [50700/64066], Loss: 5.2344\n",
      "Epoch [3/4], Step [50775/64066], Loss: 5.3831\n",
      "Epoch [3/4], Step [50850/64066], Loss: 5.4003\n",
      "Epoch [3/4], Step [50925/64066], Loss: 5.3703\n",
      "Epoch [3/4], Step [51000/64066], Loss: 5.2210\n",
      "Epoch [3/4], Step [51075/64066], Loss: 5.2484\n",
      "Epoch [3/4], Step [51150/64066], Loss: 4.9020\n",
      "Epoch [3/4], Step [51225/64066], Loss: 5.2071\n",
      "Epoch [3/4], Step [51300/64066], Loss: 5.2526\n",
      "Epoch [3/4], Step [51375/64066], Loss: 5.2377\n",
      "Epoch [3/4], Step [51450/64066], Loss: 5.0655\n",
      "Epoch [3/4], Step [51525/64066], Loss: 5.2422\n",
      "Epoch [3/4], Step [51600/64066], Loss: 5.2513\n",
      "Epoch [3/4], Step [51675/64066], Loss: 5.2092\n",
      "Epoch [3/4], Step [51750/64066], Loss: 5.2780\n",
      "Epoch [3/4], Step [51825/64066], Loss: 5.1747\n",
      "Epoch [3/4], Step [51900/64066], Loss: 5.1767\n",
      "Epoch [3/4], Step [51975/64066], Loss: 5.3376\n",
      "Epoch [3/4], Step [52050/64066], Loss: 5.1948\n",
      "Epoch [3/4], Step [52125/64066], Loss: 5.1174\n",
      "Epoch [3/4], Step [52200/64066], Loss: 5.2969\n",
      "Epoch [3/4], Step [52275/64066], Loss: 5.0926\n",
      "Epoch [3/4], Step [52350/64066], Loss: 5.1444\n",
      "Epoch [3/4], Step [52425/64066], Loss: 5.2567\n",
      "Epoch [3/4], Step [52500/64066], Loss: 5.2235\n",
      "Epoch [3/4], Step [52575/64066], Loss: 5.2422\n",
      "Epoch [3/4], Step [52650/64066], Loss: 5.2366\n",
      "Epoch [3/4], Step [52725/64066], Loss: 5.2625\n",
      "Epoch [3/4], Step [52800/64066], Loss: 5.2651\n",
      "Epoch [3/4], Step [52875/64066], Loss: 5.2429\n",
      "Epoch [3/4], Step [52950/64066], Loss: 5.1900\n",
      "Epoch [3/4], Step [53025/64066], Loss: 5.2602\n",
      "Epoch [3/4], Step [53100/64066], Loss: 5.2614\n",
      "Epoch [3/4], Step [53175/64066], Loss: 5.1418\n",
      "Epoch [3/4], Step [53250/64066], Loss: 5.3129\n",
      "Epoch [3/4], Step [53325/64066], Loss: 5.1790\n",
      "Epoch [3/4], Step [53400/64066], Loss: 5.3027\n",
      "Epoch [3/4], Step [53475/64066], Loss: 5.2253\n",
      "Epoch [3/4], Step [53550/64066], Loss: 5.2020\n",
      "Epoch [3/4], Step [53625/64066], Loss: 5.3719\n",
      "Epoch [3/4], Step [53700/64066], Loss: 5.1360\n",
      "Epoch [3/4], Step [53775/64066], Loss: 5.2006\n",
      "Epoch [3/4], Step [53850/64066], Loss: 5.1337\n",
      "Epoch [3/4], Step [53925/64066], Loss: 5.3826\n",
      "Epoch [3/4], Step [54000/64066], Loss: 5.3302\n",
      "Epoch [3/4], Step [54075/64066], Loss: 5.2536\n",
      "Epoch [3/4], Step [54150/64066], Loss: 5.1238\n",
      "Epoch [3/4], Step [54225/64066], Loss: 5.3518\n",
      "Epoch [3/4], Step [54300/64066], Loss: 5.0495\n",
      "Epoch [3/4], Step [54375/64066], Loss: 5.3544\n",
      "Epoch [3/4], Step [54450/64066], Loss: 5.3485\n",
      "Epoch [3/4], Step [54525/64066], Loss: 5.4655\n",
      "Epoch [3/4], Step [54600/64066], Loss: 5.2325\n",
      "Epoch [3/4], Step [54675/64066], Loss: 5.2898\n",
      "Epoch [3/4], Step [54750/64066], Loss: 5.2695\n",
      "Epoch [3/4], Step [54825/64066], Loss: 5.4063\n",
      "Epoch [3/4], Step [54900/64066], Loss: 5.1379\n",
      "Epoch [3/4], Step [54975/64066], Loss: 5.1586\n",
      "Epoch [3/4], Step [55050/64066], Loss: 5.1740\n",
      "Epoch [3/4], Step [55125/64066], Loss: 5.1925\n",
      "Epoch [3/4], Step [55200/64066], Loss: 5.1521\n",
      "Epoch [3/4], Step [55275/64066], Loss: 5.3439\n",
      "Epoch [3/4], Step [55350/64066], Loss: 5.2562\n",
      "Epoch [3/4], Step [55425/64066], Loss: 5.2360\n",
      "Epoch [3/4], Step [55500/64066], Loss: 5.1500\n",
      "Epoch [3/4], Step [55575/64066], Loss: 5.1306\n",
      "Epoch [3/4], Step [55650/64066], Loss: 5.3097\n",
      "Epoch [3/4], Step [55725/64066], Loss: 4.9202\n",
      "Epoch [3/4], Step [55800/64066], Loss: 5.1586\n",
      "Epoch [3/4], Step [55875/64066], Loss: 5.0967\n",
      "Epoch [3/4], Step [55950/64066], Loss: 5.2625\n",
      "Epoch [3/4], Step [56025/64066], Loss: 5.2636\n",
      "Epoch [3/4], Step [56100/64066], Loss: 5.0737\n",
      "Epoch [3/4], Step [56175/64066], Loss: 5.1809\n",
      "Epoch [3/4], Step [56250/64066], Loss: 5.2205\n",
      "Epoch [3/4], Step [56325/64066], Loss: 5.3321\n",
      "Epoch [3/4], Step [56400/64066], Loss: 5.1419\n",
      "Epoch [3/4], Step [56475/64066], Loss: 5.2329\n",
      "Epoch [3/4], Step [56550/64066], Loss: 5.1989\n",
      "Epoch [3/4], Step [56625/64066], Loss: 5.2330\n",
      "Epoch [3/4], Step [56700/64066], Loss: 5.2517\n",
      "Epoch [3/4], Step [56775/64066], Loss: 5.3611\n",
      "Epoch [3/4], Step [56850/64066], Loss: 5.1911\n",
      "Epoch [3/4], Step [56925/64066], Loss: 5.1475\n",
      "Epoch [3/4], Step [57000/64066], Loss: 5.2125\n",
      "Epoch [3/4], Step [57075/64066], Loss: 5.1967\n",
      "Epoch [3/4], Step [57150/64066], Loss: 5.1911\n",
      "Epoch [3/4], Step [57225/64066], Loss: 5.1870\n",
      "Epoch [3/4], Step [57300/64066], Loss: 5.2724\n",
      "Epoch [3/4], Step [57375/64066], Loss: 5.1906\n",
      "Epoch [3/4], Step [57450/64066], Loss: 5.2472\n",
      "Epoch [3/4], Step [57525/64066], Loss: 5.1110\n",
      "Epoch [3/4], Step [57600/64066], Loss: 5.3789\n",
      "Epoch [3/4], Step [57675/64066], Loss: 5.2469\n",
      "Epoch [3/4], Step [57750/64066], Loss: 5.1692\n",
      "Epoch [3/4], Step [57825/64066], Loss: 5.1228\n",
      "Epoch [3/4], Step [57900/64066], Loss: 5.4158\n",
      "Epoch [3/4], Step [57975/64066], Loss: 5.2628\n",
      "Epoch [3/4], Step [58050/64066], Loss: 5.4201\n",
      "Epoch [3/4], Step [58125/64066], Loss: 5.2154\n",
      "Epoch [3/4], Step [58200/64066], Loss: 5.1956\n",
      "Epoch [3/4], Step [58275/64066], Loss: 5.1769\n",
      "Epoch [3/4], Step [58350/64066], Loss: 5.2089\n",
      "Epoch [3/4], Step [58425/64066], Loss: 5.2817\n",
      "Epoch [3/4], Step [58500/64066], Loss: 5.1205\n",
      "Epoch [3/4], Step [58575/64066], Loss: 5.2529\n",
      "Epoch [3/4], Step [58650/64066], Loss: 5.0744\n",
      "Epoch [3/4], Step [58725/64066], Loss: 5.2311\n",
      "Epoch [3/4], Step [58800/64066], Loss: 5.2813\n",
      "Epoch [3/4], Step [58875/64066], Loss: 5.2711\n",
      "Epoch [3/4], Step [58950/64066], Loss: 5.3560\n",
      "Epoch [3/4], Step [59025/64066], Loss: 5.2621\n",
      "Epoch [3/4], Step [59100/64066], Loss: 5.2060\n",
      "Epoch [3/4], Step [59175/64066], Loss: 5.3759\n",
      "Epoch [3/4], Step [59250/64066], Loss: 5.1895\n",
      "Epoch [3/4], Step [59325/64066], Loss: 5.1319\n",
      "Epoch [3/4], Step [59400/64066], Loss: 5.2791\n",
      "Epoch [3/4], Step [59475/64066], Loss: 5.2087\n",
      "Epoch [3/4], Step [59550/64066], Loss: 5.2995\n",
      "Epoch [3/4], Step [59625/64066], Loss: 5.1960\n",
      "Epoch [3/4], Step [59700/64066], Loss: 5.1049\n",
      "Epoch [3/4], Step [59775/64066], Loss: 5.3020\n",
      "Epoch [3/4], Step [59850/64066], Loss: 5.3215\n",
      "Epoch [3/4], Step [59925/64066], Loss: 5.0381\n",
      "Epoch [3/4], Step [60000/64066], Loss: 5.0795\n",
      "Validation perplexity: 157.04590094346074\n",
      "Epoch [3/4], Step [60075/64066], Loss: 5.1570\n",
      "Epoch [3/4], Step [60150/64066], Loss: 5.1590\n",
      "Epoch [3/4], Step [60225/64066], Loss: 5.3210\n",
      "Epoch [3/4], Step [60300/64066], Loss: 5.1341\n",
      "Epoch [3/4], Step [60375/64066], Loss: 5.2221\n",
      "Epoch [3/4], Step [60450/64066], Loss: 5.1436\n",
      "Epoch [3/4], Step [60525/64066], Loss: 5.1559\n",
      "Epoch [3/4], Step [60600/64066], Loss: 5.3172\n",
      "Epoch [3/4], Step [60675/64066], Loss: 5.3122\n",
      "Epoch [3/4], Step [60750/64066], Loss: 5.3432\n",
      "Epoch [3/4], Step [60825/64066], Loss: 5.3133\n",
      "Epoch [3/4], Step [60900/64066], Loss: 5.0021\n",
      "Epoch [3/4], Step [60975/64066], Loss: 5.0509\n",
      "Epoch [3/4], Step [61050/64066], Loss: 5.1744\n",
      "Epoch [3/4], Step [61125/64066], Loss: 5.2049\n",
      "Epoch [3/4], Step [61200/64066], Loss: 5.1795\n",
      "Epoch [3/4], Step [61275/64066], Loss: 5.2416\n",
      "Epoch [3/4], Step [61350/64066], Loss: 5.1143\n",
      "Epoch [3/4], Step [61425/64066], Loss: 5.2055\n",
      "Epoch [3/4], Step [61500/64066], Loss: 5.3658\n",
      "Epoch [3/4], Step [61575/64066], Loss: 5.3093\n",
      "Epoch [3/4], Step [61650/64066], Loss: 5.1567\n",
      "Epoch [3/4], Step [61725/64066], Loss: 5.1629\n",
      "Epoch [3/4], Step [61800/64066], Loss: 5.2151\n",
      "Epoch [3/4], Step [61875/64066], Loss: 5.1622\n",
      "Epoch [3/4], Step [61950/64066], Loss: 5.0865\n",
      "Epoch [3/4], Step [62025/64066], Loss: 5.1369\n",
      "Epoch [3/4], Step [62100/64066], Loss: 5.1768\n",
      "Epoch [3/4], Step [62175/64066], Loss: 5.3348\n",
      "Epoch [3/4], Step [62250/64066], Loss: 5.1988\n",
      "Epoch [3/4], Step [62325/64066], Loss: 5.2858\n",
      "Epoch [3/4], Step [62400/64066], Loss: 5.1145\n",
      "Epoch [3/4], Step [62475/64066], Loss: 5.1310\n",
      "Epoch [3/4], Step [62550/64066], Loss: 5.2127\n",
      "Epoch [3/4], Step [62625/64066], Loss: 5.2796\n",
      "Epoch [3/4], Step [62700/64066], Loss: 5.3061\n",
      "Epoch [3/4], Step [62775/64066], Loss: 5.2921\n",
      "Epoch [3/4], Step [62850/64066], Loss: 5.2624\n",
      "Epoch [3/4], Step [62925/64066], Loss: 5.1949\n",
      "Epoch [3/4], Step [63000/64066], Loss: 5.2499\n",
      "Epoch [3/4], Step [63075/64066], Loss: 5.0880\n",
      "Epoch [3/4], Step [63150/64066], Loss: 5.2369\n",
      "Epoch [3/4], Step [63225/64066], Loss: 5.1434\n",
      "Epoch [3/4], Step [63300/64066], Loss: 5.3966\n",
      "Epoch [3/4], Step [63375/64066], Loss: 5.2666\n",
      "Epoch [3/4], Step [63450/64066], Loss: 5.3442\n",
      "Epoch [3/4], Step [63525/64066], Loss: 5.1999\n",
      "Epoch [3/4], Step [63600/64066], Loss: 5.2690\n",
      "Epoch [3/4], Step [63675/64066], Loss: 5.2384\n",
      "Epoch [3/4], Step [63750/64066], Loss: 5.1720\n",
      "Epoch [3/4], Step [63825/64066], Loss: 5.1740\n",
      "Epoch [3/4], Step [63900/64066], Loss: 5.2051\n",
      "Epoch [3/4], Step [63975/64066], Loss: 5.1224\n",
      "Epoch [3/4], Step [64050/64066], Loss: 5.2995\n",
      "Epoch [3/4] Average Loss: 5.2245, Perplexity: 185.77\n",
      "Epoch [4/4], Step [0/64066], Loss: 5.2735\n",
      "Validation perplexity: 156.83438436888213\n",
      "Epoch [4/4], Step [75/64066], Loss: 5.3653\n",
      "Epoch [4/4], Step [150/64066], Loss: 5.2589\n",
      "Epoch [4/4], Step [225/64066], Loss: 5.2561\n",
      "Epoch [4/4], Step [300/64066], Loss: 5.3385\n",
      "Epoch [4/4], Step [375/64066], Loss: 5.2391\n",
      "Epoch [4/4], Step [450/64066], Loss: 5.1549\n",
      "Epoch [4/4], Step [525/64066], Loss: 5.2510\n",
      "Epoch [4/4], Step [600/64066], Loss: 5.3819\n",
      "Epoch [4/4], Step [675/64066], Loss: 5.0680\n",
      "Epoch [4/4], Step [750/64066], Loss: 5.3261\n",
      "Epoch [4/4], Step [825/64066], Loss: 5.2251\n",
      "Epoch [4/4], Step [900/64066], Loss: 5.1300\n",
      "Epoch [4/4], Step [975/64066], Loss: 5.1613\n",
      "Epoch [4/4], Step [1050/64066], Loss: 5.2605\n",
      "Epoch [4/4], Step [1125/64066], Loss: 5.0177\n",
      "Epoch [4/4], Step [1200/64066], Loss: 5.2256\n",
      "Epoch [4/4], Step [1275/64066], Loss: 5.1711\n",
      "Epoch [4/4], Step [1350/64066], Loss: 5.1933\n",
      "Epoch [4/4], Step [1425/64066], Loss: 5.3748\n",
      "Epoch [4/4], Step [1500/64066], Loss: 5.1214\n",
      "Epoch [4/4], Step [1575/64066], Loss: 5.2112\n",
      "Epoch [4/4], Step [1650/64066], Loss: 5.0854\n",
      "Epoch [4/4], Step [1725/64066], Loss: 5.1881\n",
      "Epoch [4/4], Step [1800/64066], Loss: 5.2612\n",
      "Epoch [4/4], Step [1875/64066], Loss: 5.2978\n",
      "Epoch [4/4], Step [1950/64066], Loss: 5.3396\n",
      "Epoch [4/4], Step [2025/64066], Loss: 5.3334\n",
      "Epoch [4/4], Step [2100/64066], Loss: 5.1648\n",
      "Epoch [4/4], Step [2175/64066], Loss: 5.3000\n",
      "Epoch [4/4], Step [2250/64066], Loss: 5.0542\n",
      "Epoch [4/4], Step [2325/64066], Loss: 5.1887\n",
      "Epoch [4/4], Step [2400/64066], Loss: 5.3039\n",
      "Epoch [4/4], Step [2475/64066], Loss: 5.1681\n",
      "Epoch [4/4], Step [2550/64066], Loss: 5.2564\n",
      "Epoch [4/4], Step [2625/64066], Loss: 5.4202\n",
      "Epoch [4/4], Step [2700/64066], Loss: 5.1828\n",
      "Epoch [4/4], Step [2775/64066], Loss: 5.1935\n",
      "Epoch [4/4], Step [2850/64066], Loss: 5.2521\n",
      "Epoch [4/4], Step [2925/64066], Loss: 5.1541\n",
      "Epoch [4/4], Step [3000/64066], Loss: 5.1389\n",
      "Epoch [4/4], Step [3075/64066], Loss: 5.0895\n",
      "Epoch [4/4], Step [3150/64066], Loss: 5.1040\n",
      "Epoch [4/4], Step [3225/64066], Loss: 5.2491\n",
      "Epoch [4/4], Step [3300/64066], Loss: 5.2415\n",
      "Epoch [4/4], Step [3375/64066], Loss: 5.1112\n",
      "Epoch [4/4], Step [3450/64066], Loss: 5.4128\n",
      "Epoch [4/4], Step [3525/64066], Loss: 5.2038\n",
      "Epoch [4/4], Step [3600/64066], Loss: 5.2713\n",
      "Epoch [4/4], Step [3675/64066], Loss: 5.2892\n",
      "Epoch [4/4], Step [3750/64066], Loss: 5.1137\n",
      "Epoch [4/4], Step [3825/64066], Loss: 5.2491\n",
      "Epoch [4/4], Step [3900/64066], Loss: 5.1178\n",
      "Epoch [4/4], Step [3975/64066], Loss: 5.3465\n",
      "Epoch [4/4], Step [4050/64066], Loss: 5.2382\n",
      "Epoch [4/4], Step [4125/64066], Loss: 5.1813\n",
      "Epoch [4/4], Step [4200/64066], Loss: 5.1764\n",
      "Epoch [4/4], Step [4275/64066], Loss: 5.1997\n",
      "Epoch [4/4], Step [4350/64066], Loss: 5.2660\n",
      "Epoch [4/4], Step [4425/64066], Loss: 5.2537\n",
      "Epoch [4/4], Step [4500/64066], Loss: 5.1268\n",
      "Epoch [4/4], Step [4575/64066], Loss: 5.2400\n",
      "Epoch [4/4], Step [4650/64066], Loss: 5.3076\n",
      "Epoch [4/4], Step [4725/64066], Loss: 5.1028\n",
      "Epoch [4/4], Step [4800/64066], Loss: 5.1714\n",
      "Epoch [4/4], Step [4875/64066], Loss: 5.3720\n",
      "Epoch [4/4], Step [4950/64066], Loss: 5.1365\n",
      "Epoch [4/4], Step [5025/64066], Loss: 5.2570\n",
      "Epoch [4/4], Step [5100/64066], Loss: 5.1101\n",
      "Epoch [4/4], Step [5175/64066], Loss: 5.2394\n",
      "Epoch [4/4], Step [5250/64066], Loss: 5.2179\n",
      "Epoch [4/4], Step [5325/64066], Loss: 5.0700\n",
      "Epoch [4/4], Step [5400/64066], Loss: 5.1627\n",
      "Epoch [4/4], Step [5475/64066], Loss: 5.2078\n",
      "Epoch [4/4], Step [5550/64066], Loss: 5.2140\n",
      "Epoch [4/4], Step [5625/64066], Loss: 5.1801\n",
      "Epoch [4/4], Step [5700/64066], Loss: 5.3206\n",
      "Epoch [4/4], Step [5775/64066], Loss: 5.2610\n",
      "Epoch [4/4], Step [5850/64066], Loss: 5.3601\n",
      "Epoch [4/4], Step [5925/64066], Loss: 5.2443\n",
      "Epoch [4/4], Step [6000/64066], Loss: 5.1493\n",
      "Epoch [4/4], Step [6075/64066], Loss: 5.2354\n",
      "Epoch [4/4], Step [6150/64066], Loss: 5.1917\n",
      "Epoch [4/4], Step [6225/64066], Loss: 5.1358\n",
      "Epoch [4/4], Step [6300/64066], Loss: 5.2434\n",
      "Epoch [4/4], Step [6375/64066], Loss: 5.0200\n",
      "Epoch [4/4], Step [6450/64066], Loss: 5.2122\n",
      "Epoch [4/4], Step [6525/64066], Loss: 5.3481\n",
      "Epoch [4/4], Step [6600/64066], Loss: 5.1831\n",
      "Epoch [4/4], Step [6675/64066], Loss: 5.1359\n",
      "Epoch [4/4], Step [6750/64066], Loss: 5.2949\n",
      "Epoch [4/4], Step [6825/64066], Loss: 5.1295\n",
      "Epoch [4/4], Step [6900/64066], Loss: 5.2548\n",
      "Epoch [4/4], Step [6975/64066], Loss: 5.2412\n",
      "Epoch [4/4], Step [7050/64066], Loss: 5.3189\n",
      "Epoch [4/4], Step [7125/64066], Loss: 5.3171\n",
      "Epoch [4/4], Step [7200/64066], Loss: 5.1942\n",
      "Epoch [4/4], Step [7275/64066], Loss: 5.1673\n",
      "Epoch [4/4], Step [7350/64066], Loss: 5.2178\n",
      "Epoch [4/4], Step [7425/64066], Loss: 5.2543\n",
      "Epoch [4/4], Step [7500/64066], Loss: 5.2625\n",
      "Epoch [4/4], Step [7575/64066], Loss: 5.3646\n",
      "Epoch [4/4], Step [7650/64066], Loss: 5.0985\n",
      "Epoch [4/4], Step [7725/64066], Loss: 5.2564\n",
      "Epoch [4/4], Step [7800/64066], Loss: 5.1654\n",
      "Epoch [4/4], Step [7875/64066], Loss: 5.2068\n",
      "Epoch [4/4], Step [7950/64066], Loss: 5.4336\n",
      "Epoch [4/4], Step [8025/64066], Loss: 5.2438\n",
      "Epoch [4/4], Step [8100/64066], Loss: 5.1647\n",
      "Epoch [4/4], Step [8175/64066], Loss: 5.3180\n",
      "Epoch [4/4], Step [8250/64066], Loss: 5.1513\n",
      "Epoch [4/4], Step [8325/64066], Loss: 5.1177\n",
      "Epoch [4/4], Step [8400/64066], Loss: 5.3222\n",
      "Epoch [4/4], Step [8475/64066], Loss: 5.1742\n",
      "Epoch [4/4], Step [8550/64066], Loss: 5.2480\n",
      "Epoch [4/4], Step [8625/64066], Loss: 5.1939\n",
      "Epoch [4/4], Step [8700/64066], Loss: 5.0411\n",
      "Epoch [4/4], Step [8775/64066], Loss: 5.2744\n",
      "Epoch [4/4], Step [8850/64066], Loss: 5.2071\n",
      "Epoch [4/4], Step [8925/64066], Loss: 5.1456\n",
      "Epoch [4/4], Step [9000/64066], Loss: 5.1777\n",
      "Epoch [4/4], Step [9075/64066], Loss: 5.0708\n",
      "Epoch [4/4], Step [9150/64066], Loss: 5.2461\n",
      "Epoch [4/4], Step [9225/64066], Loss: 5.2489\n",
      "Epoch [4/4], Step [9300/64066], Loss: 5.2166\n",
      "Epoch [4/4], Step [9375/64066], Loss: 5.1343\n",
      "Epoch [4/4], Step [9450/64066], Loss: 5.2418\n",
      "Epoch [4/4], Step [9525/64066], Loss: 5.0895\n",
      "Epoch [4/4], Step [9600/64066], Loss: 5.2604\n",
      "Epoch [4/4], Step [9675/64066], Loss: 5.1996\n",
      "Epoch [4/4], Step [9750/64066], Loss: 5.2276\n",
      "Epoch [4/4], Step [9825/64066], Loss: 5.1943\n",
      "Epoch [4/4], Step [9900/64066], Loss: 5.1527\n",
      "Epoch [4/4], Step [9975/64066], Loss: 5.3397\n",
      "Validation perplexity: 156.3437178621157\n",
      "Epoch [4/4], Step [10050/64066], Loss: 5.1185\n",
      "Epoch [4/4], Step [10125/64066], Loss: 5.1434\n",
      "Epoch [4/4], Step [10200/64066], Loss: 5.2567\n",
      "Epoch [4/4], Step [10275/64066], Loss: 5.1554\n",
      "Epoch [4/4], Step [10350/64066], Loss: 5.1478\n",
      "Epoch [4/4], Step [10425/64066], Loss: 5.1424\n",
      "Epoch [4/4], Step [10500/64066], Loss: 5.3053\n",
      "Epoch [4/4], Step [10575/64066], Loss: 5.1274\n",
      "Epoch [4/4], Step [10650/64066], Loss: 5.1594\n",
      "Epoch [4/4], Step [10725/64066], Loss: 5.2420\n",
      "Epoch [4/4], Step [10800/64066], Loss: 5.2297\n",
      "Epoch [4/4], Step [10875/64066], Loss: 5.2151\n",
      "Epoch [4/4], Step [10950/64066], Loss: 5.1049\n",
      "Epoch [4/4], Step [11025/64066], Loss: 5.0059\n",
      "Epoch [4/4], Step [11100/64066], Loss: 5.2539\n",
      "Epoch [4/4], Step [11175/64066], Loss: 5.0550\n",
      "Epoch [4/4], Step [11250/64066], Loss: 5.0364\n",
      "Epoch [4/4], Step [11325/64066], Loss: 5.1462\n",
      "Epoch [4/4], Step [11400/64066], Loss: 5.2247\n",
      "Epoch [4/4], Step [11475/64066], Loss: 5.1583\n",
      "Epoch [4/4], Step [11550/64066], Loss: 5.3569\n",
      "Epoch [4/4], Step [11625/64066], Loss: 5.0805\n",
      "Epoch [4/4], Step [11700/64066], Loss: 5.1436\n",
      "Epoch [4/4], Step [11775/64066], Loss: 5.2813\n",
      "Epoch [4/4], Step [11850/64066], Loss: 5.2405\n",
      "Epoch [4/4], Step [11925/64066], Loss: 5.1629\n",
      "Epoch [4/4], Step [12000/64066], Loss: 5.2268\n",
      "Epoch [4/4], Step [12075/64066], Loss: 5.3769\n",
      "Epoch [4/4], Step [12150/64066], Loss: 5.1580\n",
      "Epoch [4/4], Step [12225/64066], Loss: 5.3027\n",
      "Epoch [4/4], Step [12300/64066], Loss: 5.1285\n",
      "Epoch [4/4], Step [12375/64066], Loss: 5.2875\n",
      "Epoch [4/4], Step [12450/64066], Loss: 5.3320\n",
      "Epoch [4/4], Step [12525/64066], Loss: 5.0966\n",
      "Epoch [4/4], Step [12600/64066], Loss: 5.1087\n",
      "Epoch [4/4], Step [12675/64066], Loss: 5.1725\n",
      "Epoch [4/4], Step [12750/64066], Loss: 5.2177\n",
      "Epoch [4/4], Step [12825/64066], Loss: 5.1990\n",
      "Epoch [4/4], Step [12900/64066], Loss: 5.3563\n",
      "Epoch [4/4], Step [12975/64066], Loss: 5.2255\n",
      "Epoch [4/4], Step [13050/64066], Loss: 5.2125\n",
      "Epoch [4/4], Step [13125/64066], Loss: 5.1338\n",
      "Epoch [4/4], Step [13200/64066], Loss: 4.8967\n",
      "Epoch [4/4], Step [13275/64066], Loss: 5.2069\n",
      "Epoch [4/4], Step [13350/64066], Loss: 5.2767\n",
      "Epoch [4/4], Step [13425/64066], Loss: 5.4186\n",
      "Epoch [4/4], Step [13500/64066], Loss: 5.1527\n",
      "Epoch [4/4], Step [13575/64066], Loss: 5.2481\n",
      "Epoch [4/4], Step [13650/64066], Loss: 5.3181\n",
      "Epoch [4/4], Step [13725/64066], Loss: 4.9930\n",
      "Epoch [4/4], Step [13800/64066], Loss: 5.3145\n",
      "Epoch [4/4], Step [13875/64066], Loss: 5.2165\n",
      "Epoch [4/4], Step [13950/64066], Loss: 5.1823\n",
      "Epoch [4/4], Step [14025/64066], Loss: 5.2745\n",
      "Epoch [4/4], Step [14100/64066], Loss: 5.2200\n",
      "Epoch [4/4], Step [14175/64066], Loss: 5.1827\n",
      "Epoch [4/4], Step [14250/64066], Loss: 5.1406\n",
      "Epoch [4/4], Step [14325/64066], Loss: 5.2203\n",
      "Epoch [4/4], Step [14400/64066], Loss: 5.2404\n",
      "Epoch [4/4], Step [14475/64066], Loss: 5.0691\n",
      "Epoch [4/4], Step [14550/64066], Loss: 5.3722\n",
      "Epoch [4/4], Step [14625/64066], Loss: 5.1459\n",
      "Epoch [4/4], Step [14700/64066], Loss: 5.1434\n",
      "Epoch [4/4], Step [14775/64066], Loss: 5.1939\n",
      "Epoch [4/4], Step [14850/64066], Loss: 5.2466\n",
      "Epoch [4/4], Step [14925/64066], Loss: 5.2564\n",
      "Epoch [4/4], Step [15000/64066], Loss: 5.2867\n",
      "Epoch [4/4], Step [15075/64066], Loss: 5.2326\n",
      "Epoch [4/4], Step [15150/64066], Loss: 5.2630\n",
      "Epoch [4/4], Step [15225/64066], Loss: 5.3037\n",
      "Epoch [4/4], Step [15300/64066], Loss: 5.2760\n",
      "Epoch [4/4], Step [15375/64066], Loss: 5.0795\n",
      "Epoch [4/4], Step [15450/64066], Loss: 5.0693\n",
      "Epoch [4/4], Step [15525/64066], Loss: 5.2529\n",
      "Epoch [4/4], Step [15600/64066], Loss: 5.0789\n",
      "Epoch [4/4], Step [15675/64066], Loss: 5.2712\n",
      "Epoch [4/4], Step [15750/64066], Loss: 5.1534\n",
      "Epoch [4/4], Step [15825/64066], Loss: 5.3030\n",
      "Epoch [4/4], Step [15900/64066], Loss: 5.1707\n",
      "Epoch [4/4], Step [15975/64066], Loss: 4.9551\n",
      "Epoch [4/4], Step [16050/64066], Loss: 5.2819\n",
      "Epoch [4/4], Step [16125/64066], Loss: 5.2181\n",
      "Epoch [4/4], Step [16200/64066], Loss: 5.0179\n",
      "Epoch [4/4], Step [16275/64066], Loss: 5.1116\n",
      "Epoch [4/4], Step [16350/64066], Loss: 5.0970\n",
      "Epoch [4/4], Step [16425/64066], Loss: 5.0776\n",
      "Epoch [4/4], Step [16500/64066], Loss: 5.3548\n",
      "Epoch [4/4], Step [16575/64066], Loss: 5.2593\n",
      "Epoch [4/4], Step [16650/64066], Loss: 5.1707\n",
      "Epoch [4/4], Step [16725/64066], Loss: 5.1932\n",
      "Epoch [4/4], Step [16800/64066], Loss: 5.2309\n",
      "Epoch [4/4], Step [16875/64066], Loss: 5.2903\n",
      "Epoch [4/4], Step [16950/64066], Loss: 5.3088\n",
      "Epoch [4/4], Step [17025/64066], Loss: 5.2432\n",
      "Epoch [4/4], Step [17100/64066], Loss: 5.2016\n",
      "Epoch [4/4], Step [17175/64066], Loss: 5.3394\n",
      "Epoch [4/4], Step [17250/64066], Loss: 5.2216\n",
      "Epoch [4/4], Step [17325/64066], Loss: 5.2008\n",
      "Epoch [4/4], Step [17400/64066], Loss: 5.1468\n",
      "Epoch [4/4], Step [17475/64066], Loss: 5.1859\n",
      "Epoch [4/4], Step [17550/64066], Loss: 5.2659\n",
      "Epoch [4/4], Step [17625/64066], Loss: 5.2067\n",
      "Epoch [4/4], Step [17700/64066], Loss: 5.2834\n",
      "Epoch [4/4], Step [17775/64066], Loss: 5.1801\n",
      "Epoch [4/4], Step [17850/64066], Loss: 5.2787\n",
      "Epoch [4/4], Step [17925/64066], Loss: 5.1148\n",
      "Epoch [4/4], Step [18000/64066], Loss: 5.3582\n",
      "Epoch [4/4], Step [18075/64066], Loss: 5.0494\n",
      "Epoch [4/4], Step [18150/64066], Loss: 5.1930\n",
      "Epoch [4/4], Step [18225/64066], Loss: 5.0659\n",
      "Epoch [4/4], Step [18300/64066], Loss: 5.1056\n",
      "Epoch [4/4], Step [18375/64066], Loss: 5.2193\n",
      "Epoch [4/4], Step [18450/64066], Loss: 5.1413\n",
      "Epoch [4/4], Step [18525/64066], Loss: 5.1679\n",
      "Epoch [4/4], Step [18600/64066], Loss: 5.3129\n",
      "Epoch [4/4], Step [18675/64066], Loss: 5.2155\n",
      "Epoch [4/4], Step [18750/64066], Loss: 5.3069\n",
      "Epoch [4/4], Step [18825/64066], Loss: 5.0774\n",
      "Epoch [4/4], Step [18900/64066], Loss: 5.2810\n",
      "Epoch [4/4], Step [18975/64066], Loss: 5.1772\n",
      "Epoch [4/4], Step [19050/64066], Loss: 5.0795\n",
      "Epoch [4/4], Step [19125/64066], Loss: 5.0542\n",
      "Epoch [4/4], Step [19200/64066], Loss: 5.1504\n",
      "Epoch [4/4], Step [19275/64066], Loss: 5.4356\n",
      "Epoch [4/4], Step [19350/64066], Loss: 5.2211\n",
      "Epoch [4/4], Step [19425/64066], Loss: 5.2999\n",
      "Epoch [4/4], Step [19500/64066], Loss: 5.3337\n",
      "Epoch [4/4], Step [19575/64066], Loss: 5.2017\n",
      "Epoch [4/4], Step [19650/64066], Loss: 5.2126\n",
      "Epoch [4/4], Step [19725/64066], Loss: 5.1950\n",
      "Epoch [4/4], Step [19800/64066], Loss: 5.2987\n",
      "Epoch [4/4], Step [19875/64066], Loss: 5.2862\n",
      "Epoch [4/4], Step [19950/64066], Loss: 5.2718\n",
      "Validation perplexity: 155.88034285465824\n",
      "Epoch [4/4], Step [20025/64066], Loss: 5.2291\n",
      "Epoch [4/4], Step [20100/64066], Loss: 4.9707\n",
      "Epoch [4/4], Step [20175/64066], Loss: 5.0767\n",
      "Epoch [4/4], Step [20250/64066], Loss: 5.2267\n",
      "Epoch [4/4], Step [20325/64066], Loss: 5.2258\n",
      "Epoch [4/4], Step [20400/64066], Loss: 5.1830\n",
      "Epoch [4/4], Step [20475/64066], Loss: 5.2299\n",
      "Epoch [4/4], Step [20550/64066], Loss: 5.2483\n",
      "Epoch [4/4], Step [20625/64066], Loss: 5.1893\n",
      "Epoch [4/4], Step [20700/64066], Loss: 5.3961\n",
      "Epoch [4/4], Step [20775/64066], Loss: 5.1992\n",
      "Epoch [4/4], Step [20850/64066], Loss: 5.2258\n",
      "Epoch [4/4], Step [20925/64066], Loss: 5.3259\n",
      "Epoch [4/4], Step [21000/64066], Loss: 5.1484\n",
      "Epoch [4/4], Step [21075/64066], Loss: 5.2593\n",
      "Epoch [4/4], Step [21150/64066], Loss: 5.2463\n",
      "Epoch [4/4], Step [21225/64066], Loss: 5.2240\n",
      "Epoch [4/4], Step [21300/64066], Loss: 5.1372\n",
      "Epoch [4/4], Step [21375/64066], Loss: 5.3273\n",
      "Epoch [4/4], Step [21450/64066], Loss: 5.2826\n",
      "Epoch [4/4], Step [21525/64066], Loss: 5.1355\n",
      "Epoch [4/4], Step [21600/64066], Loss: 5.0999\n",
      "Epoch [4/4], Step [21675/64066], Loss: 5.0188\n",
      "Epoch [4/4], Step [21750/64066], Loss: 5.1311\n",
      "Epoch [4/4], Step [21825/64066], Loss: 5.2048\n",
      "Epoch [4/4], Step [21900/64066], Loss: 5.2119\n",
      "Epoch [4/4], Step [21975/64066], Loss: 5.0419\n",
      "Epoch [4/4], Step [22050/64066], Loss: 5.2619\n",
      "Epoch [4/4], Step [22125/64066], Loss: 5.1173\n",
      "Epoch [4/4], Step [22200/64066], Loss: 5.2204\n",
      "Epoch [4/4], Step [22275/64066], Loss: 5.1677\n",
      "Epoch [4/4], Step [22350/64066], Loss: 5.3599\n",
      "Epoch [4/4], Step [22425/64066], Loss: 5.3080\n",
      "Epoch [4/4], Step [22500/64066], Loss: 5.2537\n",
      "Epoch [4/4], Step [22575/64066], Loss: 5.3248\n",
      "Epoch [4/4], Step [22650/64066], Loss: 5.2207\n",
      "Epoch [4/4], Step [22725/64066], Loss: 5.2656\n",
      "Epoch [4/4], Step [22800/64066], Loss: 5.1540\n",
      "Epoch [4/4], Step [22875/64066], Loss: 5.2380\n",
      "Epoch [4/4], Step [22950/64066], Loss: 5.4089\n",
      "Epoch [4/4], Step [23025/64066], Loss: 5.2699\n",
      "Epoch [4/4], Step [23100/64066], Loss: 5.3185\n",
      "Epoch [4/4], Step [23175/64066], Loss: 5.3871\n",
      "Epoch [4/4], Step [23250/64066], Loss: 5.2775\n",
      "Epoch [4/4], Step [23325/64066], Loss: 5.4299\n",
      "Epoch [4/4], Step [23400/64066], Loss: 5.2381\n",
      "Epoch [4/4], Step [23475/64066], Loss: 5.2651\n",
      "Epoch [4/4], Step [23550/64066], Loss: 5.2447\n",
      "Epoch [4/4], Step [23625/64066], Loss: 5.0854\n",
      "Epoch [4/4], Step [23700/64066], Loss: 5.1202\n",
      "Epoch [4/4], Step [23775/64066], Loss: 5.1315\n",
      "Epoch [4/4], Step [23850/64066], Loss: 5.2818\n",
      "Epoch [4/4], Step [23925/64066], Loss: 5.1386\n",
      "Epoch [4/4], Step [24000/64066], Loss: 5.3904\n",
      "Epoch [4/4], Step [24075/64066], Loss: 5.1939\n",
      "Epoch [4/4], Step [24150/64066], Loss: 5.1398\n",
      "Epoch [4/4], Step [24225/64066], Loss: 5.3463\n",
      "Epoch [4/4], Step [24300/64066], Loss: 5.1735\n",
      "Epoch [4/4], Step [24375/64066], Loss: 5.2850\n",
      "Epoch [4/4], Step [24450/64066], Loss: 5.1062\n",
      "Epoch [4/4], Step [24525/64066], Loss: 5.2162\n",
      "Epoch [4/4], Step [24600/64066], Loss: 5.2265\n",
      "Epoch [4/4], Step [24675/64066], Loss: 5.0864\n",
      "Epoch [4/4], Step [24750/64066], Loss: 5.2301\n",
      "Epoch [4/4], Step [24825/64066], Loss: 5.2514\n",
      "Epoch [4/4], Step [24900/64066], Loss: 5.2034\n",
      "Epoch [4/4], Step [24975/64066], Loss: 5.3126\n",
      "Epoch [4/4], Step [25050/64066], Loss: 5.2298\n",
      "Epoch [4/4], Step [25125/64066], Loss: 5.1708\n",
      "Epoch [4/4], Step [25200/64066], Loss: 5.4389\n",
      "Epoch [4/4], Step [25275/64066], Loss: 5.2460\n",
      "Epoch [4/4], Step [25350/64066], Loss: 5.1839\n",
      "Epoch [4/4], Step [25425/64066], Loss: 5.2174\n",
      "Epoch [4/4], Step [25500/64066], Loss: 5.1293\n",
      "Epoch [4/4], Step [25575/64066], Loss: 5.0613\n",
      "Epoch [4/4], Step [25650/64066], Loss: 5.2242\n",
      "Epoch [4/4], Step [25725/64066], Loss: 5.0730\n",
      "Epoch [4/4], Step [25800/64066], Loss: 5.2255\n",
      "Epoch [4/4], Step [25875/64066], Loss: 5.1907\n",
      "Epoch [4/4], Step [25950/64066], Loss: 5.1716\n",
      "Epoch [4/4], Step [26025/64066], Loss: 5.2821\n",
      "Epoch [4/4], Step [26100/64066], Loss: 5.2751\n",
      "Epoch [4/4], Step [26175/64066], Loss: 5.1427\n",
      "Epoch [4/4], Step [26250/64066], Loss: 5.2968\n",
      "Epoch [4/4], Step [26325/64066], Loss: 5.0068\n",
      "Epoch [4/4], Step [26400/64066], Loss: 5.3104\n",
      "Epoch [4/4], Step [26475/64066], Loss: 5.3008\n",
      "Epoch [4/4], Step [26550/64066], Loss: 5.1472\n",
      "Epoch [4/4], Step [26625/64066], Loss: 5.3295\n",
      "Epoch [4/4], Step [26700/64066], Loss: 5.3061\n",
      "Epoch [4/4], Step [26775/64066], Loss: 5.1037\n",
      "Epoch [4/4], Step [26850/64066], Loss: 5.2029\n",
      "Epoch [4/4], Step [26925/64066], Loss: 5.1645\n",
      "Epoch [4/4], Step [27000/64066], Loss: 5.2318\n",
      "Epoch [4/4], Step [27075/64066], Loss: 5.0411\n",
      "Epoch [4/4], Step [27150/64066], Loss: 5.0492\n",
      "Epoch [4/4], Step [27225/64066], Loss: 5.3141\n",
      "Epoch [4/4], Step [27300/64066], Loss: 5.1184\n",
      "Epoch [4/4], Step [27375/64066], Loss: 5.1308\n",
      "Epoch [4/4], Step [27450/64066], Loss: 5.1644\n",
      "Epoch [4/4], Step [27525/64066], Loss: 5.2515\n",
      "Epoch [4/4], Step [27600/64066], Loss: 5.3685\n",
      "Epoch [4/4], Step [27675/64066], Loss: 5.1400\n",
      "Epoch [4/4], Step [27750/64066], Loss: 5.3145\n",
      "Epoch [4/4], Step [27825/64066], Loss: 5.1309\n",
      "Epoch [4/4], Step [27900/64066], Loss: 5.2090\n",
      "Epoch [4/4], Step [27975/64066], Loss: 5.1841\n",
      "Epoch [4/4], Step [28050/64066], Loss: 5.0808\n",
      "Epoch [4/4], Step [28125/64066], Loss: 5.2622\n",
      "Epoch [4/4], Step [28200/64066], Loss: 5.1686\n",
      "Epoch [4/4], Step [28275/64066], Loss: 5.2021\n",
      "Epoch [4/4], Step [28350/64066], Loss: 5.0764\n",
      "Epoch [4/4], Step [28425/64066], Loss: 5.2655\n",
      "Epoch [4/4], Step [28500/64066], Loss: 5.2533\n",
      "Epoch [4/4], Step [28575/64066], Loss: 5.1437\n",
      "Epoch [4/4], Step [28650/64066], Loss: 5.1379\n",
      "Epoch [4/4], Step [28725/64066], Loss: 5.0395\n",
      "Epoch [4/4], Step [28800/64066], Loss: 5.1911\n",
      "Epoch [4/4], Step [28875/64066], Loss: 5.2629\n",
      "Epoch [4/4], Step [28950/64066], Loss: 5.1511\n",
      "Epoch [4/4], Step [29025/64066], Loss: 5.1514\n",
      "Epoch [4/4], Step [29100/64066], Loss: 5.0513\n",
      "Epoch [4/4], Step [29175/64066], Loss: 5.3569\n",
      "Epoch [4/4], Step [29250/64066], Loss: 5.2629\n",
      "Epoch [4/4], Step [29325/64066], Loss: 5.2153\n",
      "Epoch [4/4], Step [29400/64066], Loss: 5.3439\n",
      "Epoch [4/4], Step [29475/64066], Loss: 5.1187\n",
      "Epoch [4/4], Step [29550/64066], Loss: 5.1196\n",
      "Epoch [4/4], Step [29625/64066], Loss: 5.1364\n",
      "Epoch [4/4], Step [29700/64066], Loss: 5.2258\n",
      "Epoch [4/4], Step [29775/64066], Loss: 5.0985\n",
      "Epoch [4/4], Step [29850/64066], Loss: 5.1761\n",
      "Epoch [4/4], Step [29925/64066], Loss: 5.3568\n",
      "Epoch [4/4], Step [30000/64066], Loss: 5.1599\n",
      "Validation perplexity: 155.36184597823953\n",
      "Epoch [4/4], Step [30075/64066], Loss: 5.3113\n",
      "Epoch [4/4], Step [30150/64066], Loss: 5.1309\n",
      "Epoch [4/4], Step [30225/64066], Loss: 5.1950\n",
      "Epoch [4/4], Step [30300/64066], Loss: 5.0015\n",
      "Epoch [4/4], Step [30375/64066], Loss: 5.0973\n",
      "Epoch [4/4], Step [30450/64066], Loss: 5.2042\n",
      "Epoch [4/4], Step [30525/64066], Loss: 5.1797\n",
      "Epoch [4/4], Step [30600/64066], Loss: 5.0930\n",
      "Epoch [4/4], Step [30675/64066], Loss: 5.2763\n",
      "Epoch [4/4], Step [30750/64066], Loss: 5.2567\n",
      "Epoch [4/4], Step [30825/64066], Loss: 5.1089\n",
      "Epoch [4/4], Step [30900/64066], Loss: 5.2315\n",
      "Epoch [4/4], Step [30975/64066], Loss: 5.2437\n",
      "Epoch [4/4], Step [31050/64066], Loss: 5.1930\n",
      "Epoch [4/4], Step [31125/64066], Loss: 5.2057\n",
      "Epoch [4/4], Step [31200/64066], Loss: 5.1356\n",
      "Epoch [4/4], Step [31275/64066], Loss: 5.3659\n",
      "Epoch [4/4], Step [31350/64066], Loss: 5.3527\n",
      "Epoch [4/4], Step [31425/64066], Loss: 5.0521\n",
      "Epoch [4/4], Step [31500/64066], Loss: 5.2780\n",
      "Epoch [4/4], Step [31575/64066], Loss: 5.1021\n",
      "Epoch [4/4], Step [31650/64066], Loss: 5.2224\n",
      "Epoch [4/4], Step [31725/64066], Loss: 5.2811\n",
      "Epoch [4/4], Step [31800/64066], Loss: 5.2923\n",
      "Epoch [4/4], Step [31875/64066], Loss: 5.2838\n",
      "Epoch [4/4], Step [31950/64066], Loss: 5.0870\n",
      "Epoch [4/4], Step [32025/64066], Loss: 5.0648\n",
      "Epoch [4/4], Step [32100/64066], Loss: 5.1035\n",
      "Epoch [4/4], Step [32175/64066], Loss: 5.2634\n",
      "Epoch [4/4], Step [32250/64066], Loss: 5.1913\n",
      "Epoch [4/4], Step [32325/64066], Loss: 5.2199\n",
      "Epoch [4/4], Step [32400/64066], Loss: 5.3279\n",
      "Epoch [4/4], Step [32475/64066], Loss: 5.3660\n",
      "Epoch [4/4], Step [32550/64066], Loss: 5.3345\n",
      "Epoch [4/4], Step [32625/64066], Loss: 5.2632\n",
      "Epoch [4/4], Step [32700/64066], Loss: 5.2269\n",
      "Epoch [4/4], Step [32775/64066], Loss: 5.2060\n",
      "Epoch [4/4], Step [32850/64066], Loss: 5.1606\n",
      "Epoch [4/4], Step [32925/64066], Loss: 5.2123\n",
      "Epoch [4/4], Step [33000/64066], Loss: 5.2070\n",
      "Epoch [4/4], Step [33075/64066], Loss: 5.2977\n",
      "Epoch [4/4], Step [33150/64066], Loss: 5.1141\n",
      "Epoch [4/4], Step [33225/64066], Loss: 5.1891\n",
      "Epoch [4/4], Step [33300/64066], Loss: 5.2189\n",
      "Epoch [4/4], Step [33375/64066], Loss: 5.2417\n",
      "Epoch [4/4], Step [33450/64066], Loss: 5.0294\n",
      "Epoch [4/4], Step [33525/64066], Loss: 5.2624\n",
      "Epoch [4/4], Step [33600/64066], Loss: 5.1111\n",
      "Epoch [4/4], Step [33675/64066], Loss: 5.0813\n",
      "Epoch [4/4], Step [33750/64066], Loss: 5.0472\n",
      "Epoch [4/4], Step [33825/64066], Loss: 5.2244\n",
      "Epoch [4/4], Step [33900/64066], Loss: 5.0021\n",
      "Epoch [4/4], Step [33975/64066], Loss: 5.2860\n",
      "Epoch [4/4], Step [34050/64066], Loss: 5.3261\n",
      "Epoch [4/4], Step [34125/64066], Loss: 5.0220\n",
      "Epoch [4/4], Step [34200/64066], Loss: 5.3191\n",
      "Epoch [4/4], Step [34275/64066], Loss: 5.1812\n",
      "Epoch [4/4], Step [34350/64066], Loss: 5.0750\n",
      "Epoch [4/4], Step [34425/64066], Loss: 5.2464\n",
      "Epoch [4/4], Step [34500/64066], Loss: 5.1560\n",
      "Epoch [4/4], Step [34575/64066], Loss: 5.1875\n",
      "Epoch [4/4], Step [34650/64066], Loss: 5.1337\n",
      "Epoch [4/4], Step [34725/64066], Loss: 5.2277\n",
      "Epoch [4/4], Step [34800/64066], Loss: 5.2587\n",
      "Epoch [4/4], Step [34875/64066], Loss: 5.2368\n",
      "Epoch [4/4], Step [34950/64066], Loss: 5.1671\n",
      "Epoch [4/4], Step [35025/64066], Loss: 5.2015\n",
      "Epoch [4/4], Step [35100/64066], Loss: 5.3079\n",
      "Epoch [4/4], Step [35175/64066], Loss: 5.0830\n",
      "Epoch [4/4], Step [35250/64066], Loss: 5.2202\n",
      "Epoch [4/4], Step [35325/64066], Loss: 5.2649\n",
      "Epoch [4/4], Step [35400/64066], Loss: 5.3193\n",
      "Epoch [4/4], Step [35475/64066], Loss: 5.2520\n",
      "Epoch [4/4], Step [35550/64066], Loss: 5.1806\n",
      "Epoch [4/4], Step [35625/64066], Loss: 5.1053\n",
      "Epoch [4/4], Step [35700/64066], Loss: 5.2847\n",
      "Epoch [4/4], Step [35775/64066], Loss: 5.1308\n",
      "Epoch [4/4], Step [35850/64066], Loss: 5.1280\n",
      "Epoch [4/4], Step [35925/64066], Loss: 5.2601\n",
      "Epoch [4/4], Step [36000/64066], Loss: 5.1323\n",
      "Epoch [4/4], Step [36075/64066], Loss: 5.1153\n",
      "Epoch [4/4], Step [36150/64066], Loss: 5.3299\n",
      "Epoch [4/4], Step [36225/64066], Loss: 5.1184\n",
      "Epoch [4/4], Step [36300/64066], Loss: 5.0280\n",
      "Epoch [4/4], Step [36375/64066], Loss: 5.2272\n",
      "Epoch [4/4], Step [36450/64066], Loss: 5.1256\n",
      "Epoch [4/4], Step [36525/64066], Loss: 5.0848\n",
      "Epoch [4/4], Step [36600/64066], Loss: 5.2819\n",
      "Epoch [4/4], Step [36675/64066], Loss: 5.3362\n",
      "Epoch [4/4], Step [36750/64066], Loss: 5.1993\n",
      "Epoch [4/4], Step [36825/64066], Loss: 5.1675\n",
      "Epoch [4/4], Step [36900/64066], Loss: 5.2427\n",
      "Epoch [4/4], Step [36975/64066], Loss: 5.2385\n",
      "Epoch [4/4], Step [37050/64066], Loss: 5.0526\n",
      "Epoch [4/4], Step [37125/64066], Loss: 5.1235\n",
      "Epoch [4/4], Step [37200/64066], Loss: 5.2296\n",
      "Epoch [4/4], Step [37275/64066], Loss: 5.1719\n",
      "Epoch [4/4], Step [37350/64066], Loss: 5.2991\n",
      "Epoch [4/4], Step [37425/64066], Loss: 5.0995\n",
      "Epoch [4/4], Step [37500/64066], Loss: 5.3309\n",
      "Epoch [4/4], Step [37575/64066], Loss: 5.2668\n",
      "Epoch [4/4], Step [37650/64066], Loss: 5.4071\n",
      "Epoch [4/4], Step [37725/64066], Loss: 5.0813\n",
      "Epoch [4/4], Step [37800/64066], Loss: 5.1827\n",
      "Epoch [4/4], Step [37875/64066], Loss: 5.1889\n",
      "Epoch [4/4], Step [37950/64066], Loss: 5.2401\n",
      "Epoch [4/4], Step [38025/64066], Loss: 5.0925\n",
      "Epoch [4/4], Step [38100/64066], Loss: 5.2977\n",
      "Epoch [4/4], Step [38175/64066], Loss: 5.2574\n",
      "Epoch [4/4], Step [38250/64066], Loss: 5.3122\n",
      "Epoch [4/4], Step [38325/64066], Loss: 5.2402\n",
      "Epoch [4/4], Step [38400/64066], Loss: 5.0941\n",
      "Epoch [4/4], Step [38475/64066], Loss: 5.3825\n",
      "Epoch [4/4], Step [38550/64066], Loss: 5.4008\n",
      "Epoch [4/4], Step [38625/64066], Loss: 5.3015\n",
      "Epoch [4/4], Step [38700/64066], Loss: 5.2647\n",
      "Epoch [4/4], Step [38775/64066], Loss: 5.3036\n",
      "Epoch [4/4], Step [38850/64066], Loss: 5.3438\n",
      "Epoch [4/4], Step [38925/64066], Loss: 5.1323\n",
      "Epoch [4/4], Step [39000/64066], Loss: 5.2401\n",
      "Epoch [4/4], Step [39075/64066], Loss: 5.3622\n",
      "Epoch [4/4], Step [39150/64066], Loss: 5.3942\n",
      "Epoch [4/4], Step [39225/64066], Loss: 5.2537\n",
      "Epoch [4/4], Step [39300/64066], Loss: 5.2732\n",
      "Epoch [4/4], Step [39375/64066], Loss: 5.1506\n",
      "Epoch [4/4], Step [39450/64066], Loss: 5.3102\n",
      "Epoch [4/4], Step [39525/64066], Loss: 5.1596\n",
      "Epoch [4/4], Step [39600/64066], Loss: 5.2733\n",
      "Epoch [4/4], Step [39675/64066], Loss: 5.2681\n",
      "Epoch [4/4], Step [39750/64066], Loss: 5.2599\n",
      "Epoch [4/4], Step [39825/64066], Loss: 5.0392\n",
      "Epoch [4/4], Step [39900/64066], Loss: 5.1654\n",
      "Epoch [4/4], Step [39975/64066], Loss: 5.0819\n",
      "Validation perplexity: 155.04387132726066\n",
      "Epoch [4/4], Step [40050/64066], Loss: 5.4918\n",
      "Epoch [4/4], Step [40125/64066], Loss: 4.9963\n",
      "Epoch [4/4], Step [40200/64066], Loss: 5.1735\n",
      "Epoch [4/4], Step [40275/64066], Loss: 5.0557\n",
      "Epoch [4/4], Step [40350/64066], Loss: 5.2817\n",
      "Epoch [4/4], Step [40425/64066], Loss: 5.1167\n",
      "Epoch [4/4], Step [40500/64066], Loss: 5.2218\n",
      "Epoch [4/4], Step [40575/64066], Loss: 5.0027\n",
      "Epoch [4/4], Step [40650/64066], Loss: 4.8802\n",
      "Epoch [4/4], Step [40725/64066], Loss: 5.2096\n",
      "Epoch [4/4], Step [40800/64066], Loss: 5.2742\n",
      "Epoch [4/4], Step [40875/64066], Loss: 5.0757\n",
      "Epoch [4/4], Step [40950/64066], Loss: 5.0053\n",
      "Epoch [4/4], Step [41025/64066], Loss: 5.2441\n",
      "Epoch [4/4], Step [41100/64066], Loss: 5.3279\n",
      "Epoch [4/4], Step [41175/64066], Loss: 5.1017\n",
      "Epoch [4/4], Step [41250/64066], Loss: 5.2364\n",
      "Epoch [4/4], Step [41325/64066], Loss: 5.1352\n",
      "Epoch [4/4], Step [41400/64066], Loss: 5.1497\n",
      "Epoch [4/4], Step [41475/64066], Loss: 5.0402\n",
      "Epoch [4/4], Step [41550/64066], Loss: 5.2066\n",
      "Epoch [4/4], Step [41625/64066], Loss: 5.1276\n",
      "Epoch [4/4], Step [41700/64066], Loss: 5.1358\n",
      "Epoch [4/4], Step [41775/64066], Loss: 4.9857\n",
      "Epoch [4/4], Step [41850/64066], Loss: 5.3972\n",
      "Epoch [4/4], Step [41925/64066], Loss: 5.2581\n",
      "Epoch [4/4], Step [42000/64066], Loss: 5.3003\n",
      "Epoch [4/4], Step [42075/64066], Loss: 5.3218\n",
      "Epoch [4/4], Step [42150/64066], Loss: 5.1830\n",
      "Epoch [4/4], Step [42225/64066], Loss: 5.1069\n",
      "Epoch [4/4], Step [42300/64066], Loss: 5.3183\n",
      "Epoch [4/4], Step [42375/64066], Loss: 5.1392\n",
      "Epoch [4/4], Step [42450/64066], Loss: 5.1322\n",
      "Epoch [4/4], Step [42525/64066], Loss: 5.1491\n",
      "Epoch [4/4], Step [42600/64066], Loss: 5.2191\n",
      "Epoch [4/4], Step [42675/64066], Loss: 5.2067\n",
      "Epoch [4/4], Step [42750/64066], Loss: 5.3340\n",
      "Epoch [4/4], Step [42825/64066], Loss: 5.2951\n",
      "Epoch [4/4], Step [42900/64066], Loss: 5.0883\n",
      "Epoch [4/4], Step [42975/64066], Loss: 5.0601\n",
      "Epoch [4/4], Step [43050/64066], Loss: 5.2819\n",
      "Epoch [4/4], Step [43125/64066], Loss: 5.0924\n",
      "Epoch [4/4], Step [43200/64066], Loss: 5.2721\n",
      "Epoch [4/4], Step [43275/64066], Loss: 5.2142\n",
      "Epoch [4/4], Step [43350/64066], Loss: 5.2623\n",
      "Epoch [4/4], Step [43425/64066], Loss: 5.4956\n",
      "Epoch [4/4], Step [43500/64066], Loss: 5.1053\n",
      "Epoch [4/4], Step [43575/64066], Loss: 5.1717\n",
      "Epoch [4/4], Step [43650/64066], Loss: 4.9838\n",
      "Epoch [4/4], Step [43725/64066], Loss: 5.0565\n",
      "Epoch [4/4], Step [43800/64066], Loss: 5.1356\n",
      "Epoch [4/4], Step [43875/64066], Loss: 5.0939\n",
      "Epoch [4/4], Step [43950/64066], Loss: 5.2194\n",
      "Epoch [4/4], Step [44025/64066], Loss: 5.1662\n",
      "Epoch [4/4], Step [44100/64066], Loss: 5.2297\n",
      "Epoch [4/4], Step [44175/64066], Loss: 5.2036\n",
      "Epoch [4/4], Step [44250/64066], Loss: 5.0778\n",
      "Epoch [4/4], Step [44325/64066], Loss: 5.2666\n",
      "Epoch [4/4], Step [44400/64066], Loss: 5.2335\n",
      "Epoch [4/4], Step [44475/64066], Loss: 5.1642\n",
      "Epoch [4/4], Step [44550/64066], Loss: 5.2159\n",
      "Epoch [4/4], Step [44625/64066], Loss: 5.2801\n",
      "Epoch [4/4], Step [44700/64066], Loss: 5.2236\n",
      "Epoch [4/4], Step [44775/64066], Loss: 5.4852\n",
      "Epoch [4/4], Step [44850/64066], Loss: 4.9983\n",
      "Epoch [4/4], Step [44925/64066], Loss: 5.1785\n",
      "Epoch [4/4], Step [45000/64066], Loss: 5.1960\n",
      "Epoch [4/4], Step [45075/64066], Loss: 5.0543\n",
      "Epoch [4/4], Step [45150/64066], Loss: 5.1564\n",
      "Epoch [4/4], Step [45225/64066], Loss: 5.1896\n",
      "Epoch [4/4], Step [45300/64066], Loss: 4.9899\n",
      "Epoch [4/4], Step [45375/64066], Loss: 5.1798\n",
      "Epoch [4/4], Step [45450/64066], Loss: 5.0630\n",
      "Epoch [4/4], Step [45525/64066], Loss: 5.2150\n",
      "Epoch [4/4], Step [45600/64066], Loss: 5.0869\n",
      "Epoch [4/4], Step [45675/64066], Loss: 5.1677\n",
      "Epoch [4/4], Step [45750/64066], Loss: 5.2847\n",
      "Epoch [4/4], Step [45825/64066], Loss: 5.2141\n",
      "Epoch [4/4], Step [45900/64066], Loss: 5.1906\n",
      "Epoch [4/4], Step [45975/64066], Loss: 5.1171\n",
      "Epoch [4/4], Step [46050/64066], Loss: 5.2472\n",
      "Epoch [4/4], Step [46125/64066], Loss: 5.1365\n",
      "Epoch [4/4], Step [46200/64066], Loss: 5.3356\n",
      "Epoch [4/4], Step [46275/64066], Loss: 5.1827\n",
      "Epoch [4/4], Step [46350/64066], Loss: 5.1309\n",
      "Epoch [4/4], Step [46425/64066], Loss: 5.1194\n",
      "Epoch [4/4], Step [46500/64066], Loss: 5.2127\n",
      "Epoch [4/4], Step [46575/64066], Loss: 5.1533\n",
      "Epoch [4/4], Step [46650/64066], Loss: 5.2931\n",
      "Epoch [4/4], Step [46725/64066], Loss: 5.3594\n",
      "Epoch [4/4], Step [46800/64066], Loss: 5.2187\n",
      "Epoch [4/4], Step [46875/64066], Loss: 5.1202\n",
      "Epoch [4/4], Step [46950/64066], Loss: 5.1380\n",
      "Epoch [4/4], Step [47025/64066], Loss: 5.2008\n",
      "Epoch [4/4], Step [47100/64066], Loss: 5.2414\n",
      "Epoch [4/4], Step [47175/64066], Loss: 5.1673\n",
      "Epoch [4/4], Step [47250/64066], Loss: 5.3236\n",
      "Epoch [4/4], Step [47325/64066], Loss: 5.0749\n",
      "Epoch [4/4], Step [47400/64066], Loss: 5.0460\n",
      "Epoch [4/4], Step [47475/64066], Loss: 5.3531\n",
      "Epoch [4/4], Step [47550/64066], Loss: 5.1955\n",
      "Epoch [4/4], Step [47625/64066], Loss: 5.1868\n",
      "Epoch [4/4], Step [47700/64066], Loss: 5.1840\n",
      "Epoch [4/4], Step [47775/64066], Loss: 5.2191\n",
      "Epoch [4/4], Step [47850/64066], Loss: 5.2049\n",
      "Epoch [4/4], Step [47925/64066], Loss: 5.0416\n",
      "Epoch [4/4], Step [48000/64066], Loss: 5.1920\n",
      "Epoch [4/4], Step [48075/64066], Loss: 5.3983\n",
      "Epoch [4/4], Step [48150/64066], Loss: 5.2041\n",
      "Epoch [4/4], Step [48225/64066], Loss: 5.0935\n",
      "Epoch [4/4], Step [48300/64066], Loss: 5.2807\n",
      "Epoch [4/4], Step [48375/64066], Loss: 5.2028\n",
      "Epoch [4/4], Step [48450/64066], Loss: 5.2361\n",
      "Epoch [4/4], Step [48525/64066], Loss: 5.2990\n",
      "Epoch [4/4], Step [48600/64066], Loss: 5.1439\n",
      "Epoch [4/4], Step [48675/64066], Loss: 5.1394\n",
      "Epoch [4/4], Step [48750/64066], Loss: 5.3064\n",
      "Epoch [4/4], Step [48825/64066], Loss: 5.1706\n",
      "Epoch [4/4], Step [48900/64066], Loss: 5.2582\n",
      "Epoch [4/4], Step [48975/64066], Loss: 5.3337\n",
      "Epoch [4/4], Step [49050/64066], Loss: 5.1849\n",
      "Epoch [4/4], Step [49125/64066], Loss: 5.0046\n",
      "Epoch [4/4], Step [49200/64066], Loss: 5.0146\n",
      "Epoch [4/4], Step [49275/64066], Loss: 5.2398\n",
      "Epoch [4/4], Step [49350/64066], Loss: 5.0090\n",
      "Epoch [4/4], Step [49425/64066], Loss: 5.0976\n",
      "Epoch [4/4], Step [49500/64066], Loss: 5.2784\n",
      "Epoch [4/4], Step [49575/64066], Loss: 5.1993\n",
      "Epoch [4/4], Step [49650/64066], Loss: 5.1544\n",
      "Epoch [4/4], Step [49725/64066], Loss: 5.2958\n",
      "Epoch [4/4], Step [49800/64066], Loss: 5.0872\n",
      "Epoch [4/4], Step [49875/64066], Loss: 5.2107\n",
      "Epoch [4/4], Step [49950/64066], Loss: 5.1888\n",
      "Validation perplexity: 154.7272337224244\n",
      "Epoch [4/4], Step [50025/64066], Loss: 5.1830\n",
      "Epoch [4/4], Step [50100/64066], Loss: 5.1835\n",
      "Epoch [4/4], Step [50175/64066], Loss: 5.0999\n",
      "Epoch [4/4], Step [50250/64066], Loss: 5.0179\n",
      "Epoch [4/4], Step [50325/64066], Loss: 5.2668\n",
      "Epoch [4/4], Step [50400/64066], Loss: 5.3814\n",
      "Epoch [4/4], Step [50475/64066], Loss: 5.1279\n",
      "Epoch [4/4], Step [50550/64066], Loss: 5.2477\n",
      "Epoch [4/4], Step [50625/64066], Loss: 5.1902\n",
      "Epoch [4/4], Step [50700/64066], Loss: 5.0916\n",
      "Epoch [4/4], Step [50775/64066], Loss: 5.1704\n",
      "Epoch [4/4], Step [50850/64066], Loss: 5.0910\n",
      "Epoch [4/4], Step [50925/64066], Loss: 5.3671\n",
      "Epoch [4/4], Step [51000/64066], Loss: 5.1060\n",
      "Epoch [4/4], Step [51075/64066], Loss: 5.2118\n",
      "Epoch [4/4], Step [51150/64066], Loss: 5.1838\n",
      "Epoch [4/4], Step [51225/64066], Loss: 5.2429\n",
      "Epoch [4/4], Step [51300/64066], Loss: 5.3242\n",
      "Epoch [4/4], Step [51375/64066], Loss: 5.4080\n",
      "Epoch [4/4], Step [51450/64066], Loss: 5.1310\n",
      "Epoch [4/4], Step [51525/64066], Loss: 5.2801\n",
      "Epoch [4/4], Step [51600/64066], Loss: 5.1243\n",
      "Epoch [4/4], Step [51675/64066], Loss: 4.9528\n",
      "Epoch [4/4], Step [51750/64066], Loss: 5.2867\n",
      "Epoch [4/4], Step [51825/64066], Loss: 5.1669\n",
      "Epoch [4/4], Step [51900/64066], Loss: 5.4206\n",
      "Epoch [4/4], Step [51975/64066], Loss: 5.2615\n",
      "Epoch [4/4], Step [52050/64066], Loss: 5.0820\n",
      "Epoch [4/4], Step [52125/64066], Loss: 5.3209\n",
      "Epoch [4/4], Step [52200/64066], Loss: 5.0943\n",
      "Epoch [4/4], Step [52275/64066], Loss: 5.1548\n",
      "Epoch [4/4], Step [52350/64066], Loss: 5.2827\n",
      "Epoch [4/4], Step [52425/64066], Loss: 5.1516\n",
      "Epoch [4/4], Step [52500/64066], Loss: 5.1978\n",
      "Epoch [4/4], Step [52575/64066], Loss: 5.1651\n",
      "Epoch [4/4], Step [52650/64066], Loss: 5.1269\n",
      "Epoch [4/4], Step [52725/64066], Loss: 5.2410\n",
      "Epoch [4/4], Step [52800/64066], Loss: 5.2423\n",
      "Epoch [4/4], Step [52875/64066], Loss: 5.2468\n",
      "Epoch [4/4], Step [52950/64066], Loss: 5.1835\n",
      "Epoch [4/4], Step [53025/64066], Loss: 5.2244\n",
      "Epoch [4/4], Step [53100/64066], Loss: 5.3598\n",
      "Epoch [4/4], Step [53175/64066], Loss: 5.2214\n",
      "Epoch [4/4], Step [53250/64066], Loss: 5.0727\n",
      "Epoch [4/4], Step [53325/64066], Loss: 5.1552\n",
      "Epoch [4/4], Step [53400/64066], Loss: 5.2051\n",
      "Epoch [4/4], Step [53475/64066], Loss: 5.1190\n",
      "Epoch [4/4], Step [53550/64066], Loss: 5.2687\n",
      "Epoch [4/4], Step [53625/64066], Loss: 5.0049\n",
      "Epoch [4/4], Step [53700/64066], Loss: 5.1597\n",
      "Epoch [4/4], Step [53775/64066], Loss: 5.1980\n",
      "Epoch [4/4], Step [53850/64066], Loss: 5.3424\n",
      "Epoch [4/4], Step [53925/64066], Loss: 5.1379\n",
      "Epoch [4/4], Step [54000/64066], Loss: 5.2993\n",
      "Epoch [4/4], Step [54075/64066], Loss: 4.9828\n",
      "Epoch [4/4], Step [54150/64066], Loss: 5.1110\n",
      "Epoch [4/4], Step [54225/64066], Loss: 5.1684\n",
      "Epoch [4/4], Step [54300/64066], Loss: 5.2319\n",
      "Epoch [4/4], Step [54375/64066], Loss: 5.2459\n",
      "Epoch [4/4], Step [54450/64066], Loss: 5.2876\n",
      "Epoch [4/4], Step [54525/64066], Loss: 5.1123\n",
      "Epoch [4/4], Step [54600/64066], Loss: 5.0620\n",
      "Epoch [4/4], Step [54675/64066], Loss: 5.1935\n",
      "Epoch [4/4], Step [54750/64066], Loss: 5.3289\n",
      "Epoch [4/4], Step [54825/64066], Loss: 5.2135\n",
      "Epoch [4/4], Step [54900/64066], Loss: 5.0118\n",
      "Epoch [4/4], Step [54975/64066], Loss: 5.1784\n",
      "Epoch [4/4], Step [55050/64066], Loss: 5.2546\n",
      "Epoch [4/4], Step [55125/64066], Loss: 5.1251\n",
      "Epoch [4/4], Step [55200/64066], Loss: 5.1217\n",
      "Epoch [4/4], Step [55275/64066], Loss: 5.2647\n",
      "Epoch [4/4], Step [55350/64066], Loss: 5.1789\n",
      "Epoch [4/4], Step [55425/64066], Loss: 5.1417\n",
      "Epoch [4/4], Step [55500/64066], Loss: 5.2041\n",
      "Epoch [4/4], Step [55575/64066], Loss: 5.2418\n",
      "Epoch [4/4], Step [55650/64066], Loss: 5.3536\n",
      "Epoch [4/4], Step [55725/64066], Loss: 5.1402\n",
      "Epoch [4/4], Step [55800/64066], Loss: 5.2191\n",
      "Epoch [4/4], Step [55875/64066], Loss: 5.0634\n",
      "Epoch [4/4], Step [55950/64066], Loss: 5.2695\n",
      "Epoch [4/4], Step [56025/64066], Loss: 5.1851\n",
      "Epoch [4/4], Step [56100/64066], Loss: 5.2000\n",
      "Epoch [4/4], Step [56175/64066], Loss: 5.3119\n",
      "Epoch [4/4], Step [56250/64066], Loss: 5.2276\n",
      "Epoch [4/4], Step [56325/64066], Loss: 5.1334\n",
      "Epoch [4/4], Step [56400/64066], Loss: 5.3426\n",
      "Epoch [4/4], Step [56475/64066], Loss: 5.2212\n",
      "Epoch [4/4], Step [56550/64066], Loss: 5.2611\n",
      "Epoch [4/4], Step [56625/64066], Loss: 5.1078\n",
      "Epoch [4/4], Step [56700/64066], Loss: 5.1687\n",
      "Epoch [4/4], Step [56775/64066], Loss: 5.1459\n",
      "Epoch [4/4], Step [56850/64066], Loss: 5.2720\n",
      "Epoch [4/4], Step [56925/64066], Loss: 5.3041\n",
      "Epoch [4/4], Step [57000/64066], Loss: 5.2009\n",
      "Epoch [4/4], Step [57075/64066], Loss: 5.2523\n",
      "Epoch [4/4], Step [57150/64066], Loss: 5.1238\n",
      "Epoch [4/4], Step [57225/64066], Loss: 5.2255\n",
      "Epoch [4/4], Step [57300/64066], Loss: 5.1124\n",
      "Epoch [4/4], Step [57375/64066], Loss: 5.3035\n",
      "Epoch [4/4], Step [57450/64066], Loss: 5.3014\n",
      "Epoch [4/4], Step [57525/64066], Loss: 5.1938\n",
      "Epoch [4/4], Step [57600/64066], Loss: 5.1533\n",
      "Epoch [4/4], Step [57675/64066], Loss: 5.3317\n",
      "Epoch [4/4], Step [57750/64066], Loss: 5.2334\n",
      "Epoch [4/4], Step [57825/64066], Loss: 5.1306\n",
      "Epoch [4/4], Step [57900/64066], Loss: 5.0915\n",
      "Epoch [4/4], Step [57975/64066], Loss: 5.2397\n",
      "Epoch [4/4], Step [58050/64066], Loss: 5.2191\n",
      "Epoch [4/4], Step [58125/64066], Loss: 5.3739\n",
      "Epoch [4/4], Step [58200/64066], Loss: 5.1316\n",
      "Epoch [4/4], Step [58275/64066], Loss: 5.2039\n",
      "Epoch [4/4], Step [58350/64066], Loss: 5.2043\n",
      "Epoch [4/4], Step [58425/64066], Loss: 5.2518\n",
      "Epoch [4/4], Step [58500/64066], Loss: 5.1462\n",
      "Epoch [4/4], Step [58575/64066], Loss: 5.2604\n",
      "Epoch [4/4], Step [58650/64066], Loss: 5.3052\n",
      "Epoch [4/4], Step [58725/64066], Loss: 5.1724\n",
      "Epoch [4/4], Step [58800/64066], Loss: 5.1353\n",
      "Epoch [4/4], Step [58875/64066], Loss: 5.1202\n",
      "Epoch [4/4], Step [58950/64066], Loss: 5.3010\n",
      "Epoch [4/4], Step [59025/64066], Loss: 5.2540\n",
      "Epoch [4/4], Step [59100/64066], Loss: 5.3081\n",
      "Epoch [4/4], Step [59175/64066], Loss: 5.1766\n",
      "Epoch [4/4], Step [59250/64066], Loss: 5.1164\n",
      "Epoch [4/4], Step [59325/64066], Loss: 5.3660\n",
      "Epoch [4/4], Step [59400/64066], Loss: 5.2350\n",
      "Epoch [4/4], Step [59475/64066], Loss: 5.2273\n",
      "Epoch [4/4], Step [59550/64066], Loss: 5.1971\n",
      "Epoch [4/4], Step [59625/64066], Loss: 5.2600\n",
      "Epoch [4/4], Step [59700/64066], Loss: 5.2219\n",
      "Epoch [4/4], Step [59775/64066], Loss: 5.1000\n",
      "Epoch [4/4], Step [59850/64066], Loss: 5.2476\n",
      "Epoch [4/4], Step [59925/64066], Loss: 5.2399\n",
      "Epoch [4/4], Step [60000/64066], Loss: 5.2872\n",
      "Validation perplexity: 154.2245493320835\n",
      "Epoch [4/4], Step [60075/64066], Loss: 5.1273\n",
      "Epoch [4/4], Step [60150/64066], Loss: 5.1863\n",
      "Epoch [4/4], Step [60225/64066], Loss: 5.1214\n",
      "Epoch [4/4], Step [60300/64066], Loss: 5.0475\n",
      "Epoch [4/4], Step [60375/64066], Loss: 5.1298\n",
      "Epoch [4/4], Step [60450/64066], Loss: 5.1837\n",
      "Epoch [4/4], Step [60525/64066], Loss: 5.1381\n",
      "Epoch [4/4], Step [60600/64066], Loss: 4.9012\n",
      "Epoch [4/4], Step [60675/64066], Loss: 5.2086\n",
      "Epoch [4/4], Step [60750/64066], Loss: 5.2226\n",
      "Epoch [4/4], Step [60825/64066], Loss: 5.1949\n",
      "Epoch [4/4], Step [60900/64066], Loss: 5.2568\n",
      "Epoch [4/4], Step [60975/64066], Loss: 5.2098\n",
      "Epoch [4/4], Step [61050/64066], Loss: 5.1576\n",
      "Epoch [4/4], Step [61125/64066], Loss: 5.1665\n",
      "Epoch [4/4], Step [61200/64066], Loss: 5.2656\n",
      "Epoch [4/4], Step [61275/64066], Loss: 5.2490\n",
      "Epoch [4/4], Step [61350/64066], Loss: 5.2029\n",
      "Epoch [4/4], Step [61425/64066], Loss: 5.0990\n",
      "Epoch [4/4], Step [61500/64066], Loss: 5.2154\n",
      "Epoch [4/4], Step [61575/64066], Loss: 5.3142\n",
      "Epoch [4/4], Step [61650/64066], Loss: 5.0394\n",
      "Epoch [4/4], Step [61725/64066], Loss: 5.2778\n",
      "Epoch [4/4], Step [61800/64066], Loss: 5.3371\n",
      "Epoch [4/4], Step [61875/64066], Loss: 5.1862\n",
      "Epoch [4/4], Step [61950/64066], Loss: 5.0780\n",
      "Epoch [4/4], Step [62025/64066], Loss: 5.3622\n",
      "Epoch [4/4], Step [62100/64066], Loss: 5.3461\n",
      "Epoch [4/4], Step [62175/64066], Loss: 5.2052\n",
      "Epoch [4/4], Step [62250/64066], Loss: 5.2001\n",
      "Epoch [4/4], Step [62325/64066], Loss: 5.1431\n",
      "Epoch [4/4], Step [62400/64066], Loss: 5.0565\n",
      "Epoch [4/4], Step [62475/64066], Loss: 5.2173\n",
      "Epoch [4/4], Step [62550/64066], Loss: 4.9761\n",
      "Epoch [4/4], Step [62625/64066], Loss: 5.1915\n",
      "Epoch [4/4], Step [62700/64066], Loss: 5.1783\n",
      "Epoch [4/4], Step [62775/64066], Loss: 5.0725\n",
      "Epoch [4/4], Step [62850/64066], Loss: 5.1174\n",
      "Epoch [4/4], Step [62925/64066], Loss: 5.2831\n",
      "Epoch [4/4], Step [63000/64066], Loss: 5.0951\n",
      "Epoch [4/4], Step [63075/64066], Loss: 5.2124\n",
      "Epoch [4/4], Step [63150/64066], Loss: 5.1581\n",
      "Epoch [4/4], Step [63225/64066], Loss: 5.2130\n",
      "Epoch [4/4], Step [63300/64066], Loss: 5.2614\n",
      "Epoch [4/4], Step [63375/64066], Loss: 5.1197\n",
      "Epoch [4/4], Step [63450/64066], Loss: 4.8651\n",
      "Epoch [4/4], Step [63525/64066], Loss: 5.1770\n",
      "Epoch [4/4], Step [63600/64066], Loss: 5.1615\n",
      "Epoch [4/4], Step [63675/64066], Loss: 5.3032\n",
      "Epoch [4/4], Step [63750/64066], Loss: 5.0997\n",
      "Epoch [4/4], Step [63825/64066], Loss: 5.1188\n",
      "Epoch [4/4], Step [63900/64066], Loss: 5.1912\n",
      "Epoch [4/4], Step [63975/64066], Loss: 5.1014\n",
      "Epoch [4/4], Step [64050/64066], Loss: 5.2185\n",
      "Epoch [4/4] Average Loss: 5.2001, Perplexity: 181.28\n",
      "In:  100000  lines seperators replaced\n",
      "Total lines replaced 95195\n",
      "Total lines replaced 40842\n",
      "Total lines replaced 1591\n"
     ]
    }
   ],
   "source": [
    "from src.model import RegularizedLanguageModel\n",
    "from src.trainComplete import TrainComplete\n",
    "from src.helper  import clean_pers_text_replace, get_cleaned_text,clean_pers_remove,clean_text_pers_both\n",
    "\n",
    "raw_text = get_cleaned_text(text_path,clean_text_pers_both)\n",
    "trainclass = TrainComplete(text_path = text_path,path_to_save_folder= path_to_save_folder,tokenizer = tokenizer,allowed_special=False)\n",
    "\n",
    "model = RegularizedLanguageModel(vocab_size, embedding_dim, context_length, dropout=0.2).to(device)\n",
    "\n",
    "\n",
    "trainclass.train(model,\n",
    "              vocab_size,device,raw_text,\"pers_standardLinearNotRelu_ep4_evaluate10000_preprocessingBoth\",\n",
    "                print_every=75,evaluate_every=10000,optimizer=None,criterion=None,\n",
    "              batch_size = 32,\n",
    "              embedding_dim = 128,\n",
    "              context_length = 32,\n",
    "              num_epochs = 4\n",
    "             )\n",
    "\n",
    "\n",
    "raw_text = get_cleaned_text(text_path,clean_pers_text_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d456d47-5f5f-4c89-922a-a3ef7d6c10b0",
   "metadata": {},
   "source": [
    "### Current Training Runn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098f7291-5dae-4d59-a41f-258b3c138751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In:  100000  lines seperators replaced\n",
      "Total lines replaced 95195\n",
      "Total lines replaced 40842\n",
      "Total lines replaced 1591\n",
      " Create Dataset 2720000 / 2733504Started Training\n",
      "Epoch [1/10], Step [0/68337], Loss: 10.3260\n",
      "Validation perplexity: 30267.173516670315\n",
      "Epoch [1/10], Step [75/68337], Loss: 7.3800\n",
      "Epoch [1/10], Step [150/68337], Loss: 7.3111\n",
      "Epoch [1/10], Step [225/68337], Loss: 7.2131\n",
      "Epoch [1/10], Step [300/68337], Loss: 7.0706\n",
      "Epoch [1/10], Step [375/68337], Loss: 7.0923\n",
      "Epoch [1/10], Step [450/68337], Loss: 7.0575\n",
      "Epoch [1/10], Step [525/68337], Loss: 7.3701\n",
      "Epoch [1/10], Step [600/68337], Loss: 6.7626\n",
      "Epoch [1/10], Step [675/68337], Loss: 6.9516\n",
      "Epoch [1/10], Step [750/68337], Loss: 7.0001\n",
      "Epoch [1/10], Step [825/68337], Loss: 6.7930\n",
      "Epoch [1/10], Step [900/68337], Loss: 6.7681\n",
      "Epoch [1/10], Step [975/68337], Loss: 6.8923\n",
      "Epoch [1/10], Step [1050/68337], Loss: 6.5430\n",
      "Epoch [1/10], Step [1125/68337], Loss: 6.9252\n",
      "Epoch [1/10], Step [1200/68337], Loss: 6.8056\n",
      "Epoch [1/10], Step [1275/68337], Loss: 6.5720\n",
      "Epoch [1/10], Step [1350/68337], Loss: 6.7258\n",
      "Epoch [1/10], Step [1425/68337], Loss: 6.7250\n",
      "Epoch [1/10], Step [1500/68337], Loss: 6.7928\n",
      "Epoch [1/10], Step [1575/68337], Loss: 6.6178\n",
      "Epoch [1/10], Step [1650/68337], Loss: 6.6133\n",
      "Epoch [1/10], Step [1725/68337], Loss: 6.5853\n",
      "Epoch [1/10], Step [1800/68337], Loss: 6.6990\n",
      "Epoch [1/10], Step [1875/68337], Loss: 6.7170\n",
      "Epoch [1/10], Step [1950/68337], Loss: 6.5463\n",
      "Epoch [1/10], Step [2025/68337], Loss: 6.3645\n",
      "Epoch [1/10], Step [2100/68337], Loss: 6.1376\n",
      "Epoch [1/10], Step [2175/68337], Loss: 6.6459\n",
      "Epoch [1/10], Step [2250/68337], Loss: 6.4534\n",
      "Epoch [1/10], Step [2325/68337], Loss: 6.5254\n",
      "Epoch [1/10], Step [2400/68337], Loss: 6.6254\n",
      "Epoch [1/10], Step [2475/68337], Loss: 6.4578\n",
      "Epoch [1/10], Step [2550/68337], Loss: 6.1903\n",
      "Epoch [1/10], Step [2625/68337], Loss: 6.5962\n",
      "Epoch [1/10], Step [2700/68337], Loss: 6.2301\n",
      "Epoch [1/10], Step [2775/68337], Loss: 6.3907\n",
      "Epoch [1/10], Step [2850/68337], Loss: 6.2274\n",
      "Epoch [1/10], Step [2925/68337], Loss: 6.5425\n",
      "Epoch [1/10], Step [3000/68337], Loss: 6.2663\n",
      "Epoch [1/10], Step [3075/68337], Loss: 6.3911\n",
      "Epoch [1/10], Step [3150/68337], Loss: 6.2380\n",
      "Epoch [1/10], Step [3225/68337], Loss: 6.2778\n",
      "Epoch [1/10], Step [3300/68337], Loss: 6.1740\n",
      "Epoch [1/10], Step [3375/68337], Loss: 6.4387\n",
      "Epoch [1/10], Step [3450/68337], Loss: 6.3006\n",
      "Epoch [1/10], Step [3525/68337], Loss: 6.1335\n",
      "Epoch [1/10], Step [3600/68337], Loss: 6.2192\n",
      "Epoch [1/10], Step [3675/68337], Loss: 6.4773\n",
      "Epoch [1/10], Step [3750/68337], Loss: 6.2543\n",
      "Epoch [1/10], Step [3825/68337], Loss: 6.0808\n",
      "Epoch [1/10], Step [3900/68337], Loss: 6.3687\n",
      "Epoch [1/10], Step [3975/68337], Loss: 6.3764\n",
      "Epoch [1/10], Step [4050/68337], Loss: 6.2294\n",
      "Epoch [1/10], Step [4125/68337], Loss: 6.2690\n",
      "Epoch [1/10], Step [4200/68337], Loss: 6.2328\n",
      "Epoch [1/10], Step [4275/68337], Loss: 6.1771\n",
      "Epoch [1/10], Step [4350/68337], Loss: 6.0836\n",
      "Epoch [1/10], Step [4425/68337], Loss: 6.1736\n",
      "Epoch [1/10], Step [4500/68337], Loss: 6.4580\n",
      "Epoch [1/10], Step [4575/68337], Loss: 6.3009\n",
      "Epoch [1/10], Step [4650/68337], Loss: 6.2437\n",
      "Epoch [1/10], Step [4725/68337], Loss: 5.9850\n",
      "Epoch [1/10], Step [4800/68337], Loss: 6.1798\n",
      "Epoch [1/10], Step [4875/68337], Loss: 6.0519\n",
      "Epoch [1/10], Step [4950/68337], Loss: 6.0624\n",
      "Epoch [1/10], Step [5025/68337], Loss: 5.8412\n",
      "Epoch [1/10], Step [5100/68337], Loss: 6.0684\n",
      "Epoch [1/10], Step [5175/68337], Loss: 6.1595\n",
      "Epoch [1/10], Step [5250/68337], Loss: 6.0517\n",
      "Epoch [1/10], Step [5325/68337], Loss: 6.0502\n",
      "Epoch [1/10], Step [5400/68337], Loss: 6.0193\n",
      "Epoch [1/10], Step [5475/68337], Loss: 6.3395\n",
      "Epoch [1/10], Step [5550/68337], Loss: 6.1331\n",
      "Epoch [1/10], Step [5625/68337], Loss: 5.9739\n",
      "Epoch [1/10], Step [5700/68337], Loss: 5.9599\n",
      "Epoch [1/10], Step [5775/68337], Loss: 5.9854\n",
      "Epoch [1/10], Step [5850/68337], Loss: 6.0208\n",
      "Epoch [1/10], Step [5925/68337], Loss: 6.1676\n",
      "Epoch [1/10], Step [6000/68337], Loss: 6.0718\n",
      "Epoch [1/10], Step [6075/68337], Loss: 6.2036\n",
      "Epoch [1/10], Step [6150/68337], Loss: 6.0754\n",
      "Epoch [1/10], Step [6225/68337], Loss: 5.8009\n",
      "Epoch [1/10], Step [6300/68337], Loss: 5.9080\n",
      "Epoch [1/10], Step [6375/68337], Loss: 6.0607\n",
      "Epoch [1/10], Step [6450/68337], Loss: 5.9785\n",
      "Epoch [1/10], Step [6525/68337], Loss: 5.8131\n",
      "Epoch [1/10], Step [6600/68337], Loss: 6.0151\n",
      "Epoch [1/10], Step [6675/68337], Loss: 6.0128\n",
      "Epoch [1/10], Step [6750/68337], Loss: 6.0581\n",
      "Epoch [1/10], Step [6825/68337], Loss: 6.2096\n",
      "Epoch [1/10], Step [6900/68337], Loss: 5.8058\n",
      "Epoch [1/10], Step [6975/68337], Loss: 5.8169\n",
      "Epoch [1/10], Step [7050/68337], Loss: 5.8245\n",
      "Epoch [1/10], Step [7125/68337], Loss: 5.9568\n",
      "Epoch [1/10], Step [7200/68337], Loss: 6.1036\n",
      "Epoch [1/10], Step [7275/68337], Loss: 5.7590\n",
      "Epoch [1/10], Step [7350/68337], Loss: 5.8318\n",
      "Epoch [1/10], Step [7425/68337], Loss: 6.0037\n",
      "Epoch [1/10], Step [7500/68337], Loss: 6.1815\n",
      "Epoch [1/10], Step [7575/68337], Loss: 5.9811\n",
      "Epoch [1/10], Step [7650/68337], Loss: 5.7969\n",
      "Epoch [1/10], Step [7725/68337], Loss: 6.1951\n",
      "Epoch [1/10], Step [7800/68337], Loss: 5.9450\n",
      "Epoch [1/10], Step [7875/68337], Loss: 5.8031\n",
      "Epoch [1/10], Step [7950/68337], Loss: 6.0884\n",
      "Epoch [1/10], Step [8025/68337], Loss: 6.1437\n",
      "Epoch [1/10], Step [8100/68337], Loss: 5.8017\n",
      "Epoch [1/10], Step [8175/68337], Loss: 6.0134\n",
      "Epoch [1/10], Step [8250/68337], Loss: 5.8302\n",
      "Epoch [1/10], Step [8325/68337], Loss: 5.7693\n",
      "Epoch [1/10], Step [8400/68337], Loss: 5.8260\n",
      "Epoch [1/10], Step [8475/68337], Loss: 5.7791\n",
      "Epoch [1/10], Step [8550/68337], Loss: 5.9397\n",
      "Epoch [1/10], Step [8625/68337], Loss: 6.0894\n",
      "Epoch [1/10], Step [8700/68337], Loss: 5.9851\n",
      "Epoch [1/10], Step [8775/68337], Loss: 5.8802\n",
      "Epoch [1/10], Step [8850/68337], Loss: 6.0470\n",
      "Epoch [1/10], Step [8925/68337], Loss: 5.7263\n",
      "Epoch [1/10], Step [9000/68337], Loss: 5.9657\n",
      "Epoch [1/10], Step [9075/68337], Loss: 5.8853\n",
      "Epoch [1/10], Step [9150/68337], Loss: 5.9797\n",
      "Epoch [1/10], Step [9225/68337], Loss: 5.7620\n",
      "Epoch [1/10], Step [9300/68337], Loss: 5.8416\n",
      "Epoch [1/10], Step [9375/68337], Loss: 5.8022\n",
      "Epoch [1/10], Step [9450/68337], Loss: 6.0381\n",
      "Epoch [1/10], Step [9525/68337], Loss: 5.9447\n",
      "Epoch [1/10], Step [9600/68337], Loss: 5.7568\n",
      "Epoch [1/10], Step [9675/68337], Loss: 6.0056\n",
      "Epoch [1/10], Step [9750/68337], Loss: 5.7881\n",
      "Epoch [1/10], Step [9825/68337], Loss: 5.9796\n",
      "Epoch [1/10], Step [9900/68337], Loss: 5.7952\n",
      "Epoch [1/10], Step [9975/68337], Loss: 5.9132\n",
      "Validation perplexity: 298.81251697126066\n",
      "Epoch [1/10], Step [10050/68337], Loss: 5.8156\n",
      "Epoch [1/10], Step [10125/68337], Loss: 5.9200\n",
      "Epoch [1/10], Step [10200/68337], Loss: 5.9429\n",
      "Epoch [1/10], Step [10275/68337], Loss: 5.7866\n",
      "Epoch [1/10], Step [10350/68337], Loss: 5.8719\n",
      "Epoch [1/10], Step [10425/68337], Loss: 5.8282\n",
      "Epoch [1/10], Step [10500/68337], Loss: 5.7519\n",
      "Epoch [1/10], Step [10575/68337], Loss: 5.9853\n",
      "Epoch [1/10], Step [10650/68337], Loss: 5.8461\n",
      "Epoch [1/10], Step [10725/68337], Loss: 5.8837\n",
      "Epoch [1/10], Step [10800/68337], Loss: 5.8976\n",
      "Epoch [1/10], Step [10875/68337], Loss: 5.7396\n",
      "Epoch [1/10], Step [10950/68337], Loss: 5.8786\n",
      "Epoch [1/10], Step [11025/68337], Loss: 5.8848\n",
      "Epoch [1/10], Step [11100/68337], Loss: 5.9476\n",
      "Epoch [1/10], Step [11175/68337], Loss: 5.7783\n",
      "Epoch [1/10], Step [11250/68337], Loss: 5.8571\n",
      "Epoch [1/10], Step [11325/68337], Loss: 5.7923\n",
      "Epoch [1/10], Step [11400/68337], Loss: 5.6730\n",
      "Epoch [1/10], Step [11475/68337], Loss: 5.8753\n",
      "Epoch [1/10], Step [11550/68337], Loss: 5.9114\n",
      "Epoch [1/10], Step [11625/68337], Loss: 5.6554\n",
      "Epoch [1/10], Step [11700/68337], Loss: 5.5436\n",
      "Epoch [1/10], Step [11775/68337], Loss: 6.1938\n",
      "Epoch [1/10], Step [11850/68337], Loss: 5.6451\n",
      "Epoch [1/10], Step [11925/68337], Loss: 5.8836\n",
      "Epoch [1/10], Step [12000/68337], Loss: 5.5779\n",
      "Epoch [1/10], Step [12075/68337], Loss: 5.7674\n",
      "Epoch [1/10], Step [12150/68337], Loss: 5.6771\n",
      "Epoch [1/10], Step [12225/68337], Loss: 5.7155\n",
      "Epoch [1/10], Step [12300/68337], Loss: 5.8343\n",
      "Epoch [1/10], Step [12375/68337], Loss: 5.9612\n",
      "Epoch [1/10], Step [12450/68337], Loss: 5.8129\n",
      "Epoch [1/10], Step [12525/68337], Loss: 5.8435\n",
      "Epoch [1/10], Step [12600/68337], Loss: 5.7432\n",
      "Epoch [1/10], Step [12675/68337], Loss: 5.9318\n",
      "Epoch [1/10], Step [12750/68337], Loss: 5.7780\n",
      "Epoch [1/10], Step [12825/68337], Loss: 5.4774\n",
      "Epoch [1/10], Step [12900/68337], Loss: 5.7622\n",
      "Epoch [1/10], Step [12975/68337], Loss: 6.0758\n",
      "Epoch [1/10], Step [13050/68337], Loss: 5.7500\n",
      "Epoch [1/10], Step [13125/68337], Loss: 5.6811\n",
      "Epoch [1/10], Step [13200/68337], Loss: 5.7671\n",
      "Epoch [1/10], Step [13275/68337], Loss: 5.7232\n",
      "Epoch [1/10], Step [13350/68337], Loss: 5.8783\n",
      "Epoch [1/10], Step [13425/68337], Loss: 5.8266\n",
      "Epoch [1/10], Step [13500/68337], Loss: 5.5253\n",
      "Epoch [1/10], Step [13575/68337], Loss: 5.9378\n",
      "Epoch [1/10], Step [13650/68337], Loss: 5.6071\n",
      "Epoch [1/10], Step [13725/68337], Loss: 5.7163\n",
      "Epoch [1/10], Step [13800/68337], Loss: 5.6671\n",
      "Epoch [1/10], Step [13875/68337], Loss: 5.7789\n",
      "Epoch [1/10], Step [13950/68337], Loss: 5.5249\n",
      "Epoch [1/10], Step [14025/68337], Loss: 5.7993\n",
      "Epoch [1/10], Step [14100/68337], Loss: 6.0793\n",
      "Epoch [1/10], Step [14175/68337], Loss: 5.9618\n",
      "Epoch [1/10], Step [14250/68337], Loss: 5.6063\n",
      "Epoch [1/10], Step [14325/68337], Loss: 5.3341\n",
      "Epoch [1/10], Step [14400/68337], Loss: 5.7303\n",
      "Epoch [1/10], Step [14475/68337], Loss: 5.8062\n",
      "Epoch [1/10], Step [14550/68337], Loss: 5.6910\n",
      "Epoch [1/10], Step [14625/68337], Loss: 5.7999\n",
      "Epoch [1/10], Step [14700/68337], Loss: 5.6058\n",
      "Epoch [1/10], Step [14775/68337], Loss: 5.7458\n",
      "Epoch [1/10], Step [14850/68337], Loss: 6.0071\n",
      "Epoch [1/10], Step [14925/68337], Loss: 5.9031\n",
      "Epoch [1/10], Step [15000/68337], Loss: 5.7427\n",
      "Epoch [1/10], Step [15075/68337], Loss: 5.7377\n",
      "Epoch [1/10], Step [15150/68337], Loss: 5.7974\n",
      "Epoch [1/10], Step [15225/68337], Loss: 5.9654\n",
      "Epoch [1/10], Step [15300/68337], Loss: 5.5809\n",
      "Epoch [1/10], Step [15375/68337], Loss: 5.8039\n",
      "Epoch [1/10], Step [15450/68337], Loss: 5.7223\n",
      "Epoch [1/10], Step [15525/68337], Loss: 5.8677\n",
      "Epoch [1/10], Step [15600/68337], Loss: 5.9086\n",
      "Epoch [1/10], Step [15675/68337], Loss: 5.5581\n",
      "Epoch [1/10], Step [15750/68337], Loss: 5.7192\n",
      "Epoch [1/10], Step [15825/68337], Loss: 5.6517\n",
      "Epoch [1/10], Step [15900/68337], Loss: 5.8048\n",
      "Epoch [1/10], Step [15975/68337], Loss: 5.4515\n",
      "Epoch [1/10], Step [16050/68337], Loss: 5.6633\n",
      "Epoch [1/10], Step [16125/68337], Loss: 5.6075\n",
      "Epoch [1/10], Step [16200/68337], Loss: 5.7286\n",
      "Epoch [1/10], Step [16275/68337], Loss: 5.7031\n",
      "Epoch [1/10], Step [16350/68337], Loss: 5.5102\n",
      "Epoch [1/10], Step [16425/68337], Loss: 5.5800\n",
      "Epoch [1/10], Step [16500/68337], Loss: 5.8149\n",
      "Epoch [1/10], Step [16575/68337], Loss: 5.6440\n",
      "Epoch [1/10], Step [16650/68337], Loss: 5.6468\n",
      "Epoch [1/10], Step [16725/68337], Loss: 5.7056\n",
      "Epoch [1/10], Step [16800/68337], Loss: 5.7555\n",
      "Epoch [1/10], Step [16875/68337], Loss: 5.7691\n",
      "Epoch [1/10], Step [16950/68337], Loss: 5.5114\n",
      "Epoch [1/10], Step [17025/68337], Loss: 5.7252\n",
      "Epoch [1/10], Step [17100/68337], Loss: 5.5994\n",
      "Epoch [1/10], Step [17175/68337], Loss: 5.5931\n",
      "Epoch [1/10], Step [17250/68337], Loss: 5.6489\n",
      "Epoch [1/10], Step [17325/68337], Loss: 5.7383\n",
      "Epoch [1/10], Step [17400/68337], Loss: 5.7004\n",
      "Epoch [1/10], Step [17475/68337], Loss: 5.5758\n",
      "Epoch [1/10], Step [17550/68337], Loss: 5.4061\n",
      "Epoch [1/10], Step [17625/68337], Loss: 5.6605\n",
      "Epoch [1/10], Step [17700/68337], Loss: 5.9119\n",
      "Epoch [1/10], Step [17775/68337], Loss: 5.6037\n",
      "Epoch [1/10], Step [17850/68337], Loss: 5.6974\n",
      "Epoch [1/10], Step [17925/68337], Loss: 5.7910\n",
      "Epoch [1/10], Step [18000/68337], Loss: 5.5983\n",
      "Epoch [1/10], Step [18075/68337], Loss: 5.3750\n",
      "Epoch [1/10], Step [18150/68337], Loss: 5.5673\n",
      "Epoch [1/10], Step [18225/68337], Loss: 5.5548\n",
      "Epoch [1/10], Step [18300/68337], Loss: 5.4685\n",
      "Epoch [1/10], Step [18375/68337], Loss: 5.6322\n",
      "Epoch [1/10], Step [18450/68337], Loss: 5.6692\n",
      "Epoch [1/10], Step [18525/68337], Loss: 5.7897\n",
      "Epoch [1/10], Step [18600/68337], Loss: 5.7187\n",
      "Epoch [1/10], Step [18675/68337], Loss: 5.8747\n",
      "Epoch [1/10], Step [18750/68337], Loss: 5.4839\n",
      "Epoch [1/10], Step [18825/68337], Loss: 5.6568\n",
      "Epoch [1/10], Step [18900/68337], Loss: 5.5764\n",
      "Epoch [1/10], Step [18975/68337], Loss: 5.6707\n",
      "Epoch [1/10], Step [19050/68337], Loss: 5.7117\n",
      "Epoch [1/10], Step [19125/68337], Loss: 5.5444\n",
      "Epoch [1/10], Step [19200/68337], Loss: 5.6560\n",
      "Epoch [1/10], Step [19275/68337], Loss: 5.6542\n",
      "Epoch [1/10], Step [19350/68337], Loss: 5.6421\n",
      "Epoch [1/10], Step [19425/68337], Loss: 5.4440\n",
      "Epoch [1/10], Step [19500/68337], Loss: 5.5988\n",
      "Epoch [1/10], Step [19575/68337], Loss: 5.5263\n",
      "Epoch [1/10], Step [19650/68337], Loss: 5.4835\n",
      "Epoch [1/10], Step [19725/68337], Loss: 5.8483\n",
      "Epoch [1/10], Step [19800/68337], Loss: 5.7658\n",
      "Epoch [1/10], Step [19875/68337], Loss: 5.7103\n",
      "Epoch [1/10], Step [19950/68337], Loss: 5.8378\n",
      "Validation perplexity: 228.68216809288927\n",
      "Epoch [1/10], Step [20025/68337], Loss: 5.8317\n",
      "Epoch [1/10], Step [20100/68337], Loss: 5.4082\n",
      "Epoch [1/10], Step [20175/68337], Loss: 5.3943\n",
      "Epoch [1/10], Step [20250/68337], Loss: 5.5873\n",
      "Epoch [1/10], Step [20325/68337], Loss: 5.4389\n",
      "Epoch [1/10], Step [20400/68337], Loss: 5.5550\n",
      "Epoch [1/10], Step [20475/68337], Loss: 5.8733\n",
      "Epoch [1/10], Step [20550/68337], Loss: 5.7164\n",
      "Epoch [1/10], Step [20625/68337], Loss: 5.6856\n",
      "Epoch [1/10], Step [20700/68337], Loss: 5.6611\n",
      "Epoch [1/10], Step [20775/68337], Loss: 5.7083\n",
      "Epoch [1/10], Step [20850/68337], Loss: 5.5726\n",
      "Epoch [1/10], Step [20925/68337], Loss: 5.7223\n",
      "Epoch [1/10], Step [21000/68337], Loss: 5.6781\n",
      "Epoch [1/10], Step [21075/68337], Loss: 5.6148\n",
      "Epoch [1/10], Step [21150/68337], Loss: 5.6434\n",
      "Epoch [1/10], Step [21225/68337], Loss: 5.4054\n",
      "Epoch [1/10], Step [21300/68337], Loss: 5.6577\n",
      "Epoch [1/10], Step [21375/68337], Loss: 5.5273\n",
      "Epoch [1/10], Step [21450/68337], Loss: 5.3768\n",
      "Epoch [1/10], Step [21525/68337], Loss: 5.6744\n",
      "Epoch [1/10], Step [21600/68337], Loss: 5.5545\n",
      "Epoch [1/10], Step [21675/68337], Loss: 5.5784\n",
      "Epoch [1/10], Step [21750/68337], Loss: 5.7387\n",
      "Epoch [1/10], Step [21825/68337], Loss: 5.7012\n",
      "Epoch [1/10], Step [21900/68337], Loss: 5.6635\n",
      "Epoch [1/10], Step [21975/68337], Loss: 5.6088\n",
      "Epoch [1/10], Step [22050/68337], Loss: 5.5736\n",
      "Epoch [1/10], Step [22125/68337], Loss: 5.4178\n",
      "Epoch [1/10], Step [22200/68337], Loss: 5.5192\n",
      "Epoch [1/10], Step [22275/68337], Loss: 5.6017\n",
      "Epoch [1/10], Step [22350/68337], Loss: 5.4024\n",
      "Epoch [1/10], Step [22425/68337], Loss: 5.4163\n",
      "Epoch [1/10], Step [22500/68337], Loss: 5.7745\n",
      "Epoch [1/10], Step [22575/68337], Loss: 5.5837\n",
      "Epoch [1/10], Step [22650/68337], Loss: 5.5457\n",
      "Epoch [1/10], Step [22725/68337], Loss: 5.4230\n",
      "Epoch [1/10], Step [22800/68337], Loss: 5.6617\n",
      "Epoch [1/10], Step [22875/68337], Loss: 5.4784\n",
      "Epoch [1/10], Step [22950/68337], Loss: 5.4002\n",
      "Epoch [1/10], Step [23025/68337], Loss: 5.5383\n",
      "Epoch [1/10], Step [23100/68337], Loss: 5.5179\n",
      "Epoch [1/10], Step [23175/68337], Loss: 5.4494\n",
      "Epoch [1/10], Step [23250/68337], Loss: 5.4925\n",
      "Epoch [1/10], Step [23325/68337], Loss: 5.6085\n",
      "Epoch [1/10], Step [23400/68337], Loss: 5.7252\n",
      "Epoch [1/10], Step [23475/68337], Loss: 5.6068\n",
      "Epoch [1/10], Step [23550/68337], Loss: 5.7079\n",
      "Epoch [1/10], Step [23625/68337], Loss: 5.4701\n",
      "Epoch [1/10], Step [23700/68337], Loss: 5.6871\n",
      "Epoch [1/10], Step [23775/68337], Loss: 5.4656\n",
      "Epoch [1/10], Step [23850/68337], Loss: 5.5982\n",
      "Epoch [1/10], Step [23925/68337], Loss: 5.6526\n",
      "Epoch [1/10], Step [24000/68337], Loss: 5.4616\n",
      "Epoch [1/10], Step [24075/68337], Loss: 5.5308\n",
      "Epoch [1/10], Step [24150/68337], Loss: 5.6072\n",
      "Epoch [1/10], Step [24225/68337], Loss: 5.6227\n",
      "Epoch [1/10], Step [24300/68337], Loss: 5.7722\n",
      "Epoch [1/10], Step [24375/68337], Loss: 5.4691\n",
      "Epoch [1/10], Step [24450/68337], Loss: 5.6782\n",
      "Epoch [1/10], Step [24525/68337], Loss: 5.7571\n",
      "Epoch [1/10], Step [24600/68337], Loss: 5.5258\n",
      "Epoch [1/10], Step [24675/68337], Loss: 5.3556\n",
      "Epoch [1/10], Step [24750/68337], Loss: 5.5925\n",
      "Epoch [1/10], Step [24825/68337], Loss: 5.5893\n",
      "Epoch [1/10], Step [24900/68337], Loss: 5.4912\n",
      "Epoch [1/10], Step [24975/68337], Loss: 5.7878\n",
      "Epoch [1/10], Step [25050/68337], Loss: 5.5404\n",
      "Epoch [1/10], Step [25125/68337], Loss: 5.2271\n",
      "Epoch [1/10], Step [25200/68337], Loss: 5.5586\n",
      "Epoch [1/10], Step [25275/68337], Loss: 5.6062\n",
      "Epoch [1/10], Step [25350/68337], Loss: 5.5639\n",
      "Epoch [1/10], Step [25425/68337], Loss: 5.4032\n",
      "Epoch [1/10], Step [25500/68337], Loss: 5.7350\n",
      "Epoch [1/10], Step [25575/68337], Loss: 5.6307\n",
      "Epoch [1/10], Step [25650/68337], Loss: 5.6193\n",
      "Epoch [1/10], Step [25725/68337], Loss: 5.6259\n",
      "Epoch [1/10], Step [25800/68337], Loss: 5.4396\n",
      "Epoch [1/10], Step [25875/68337], Loss: 5.6852\n",
      "Epoch [1/10], Step [25950/68337], Loss: 5.5940\n",
      "Epoch [1/10], Step [26025/68337], Loss: 5.5658\n",
      "Epoch [1/10], Step [26100/68337], Loss: 5.5437\n",
      "Epoch [1/10], Step [26175/68337], Loss: 5.4974\n",
      "Epoch [1/10], Step [26250/68337], Loss: 5.4841\n",
      "Epoch [1/10], Step [26325/68337], Loss: 5.8252\n",
      "Epoch [1/10], Step [26400/68337], Loss: 5.5636\n",
      "Epoch [1/10], Step [26475/68337], Loss: 5.4906\n",
      "Epoch [1/10], Step [26550/68337], Loss: 5.5187\n",
      "Epoch [1/10], Step [26625/68337], Loss: 5.6229\n",
      "Epoch [1/10], Step [26700/68337], Loss: 5.7063\n",
      "Epoch [1/10], Step [26775/68337], Loss: 5.6492\n",
      "Epoch [1/10], Step [26850/68337], Loss: 5.6985\n",
      "Epoch [1/10], Step [26925/68337], Loss: 5.3539\n",
      "Epoch [1/10], Step [27000/68337], Loss: 5.4592\n",
      "Epoch [1/10], Step [27075/68337], Loss: 5.2185\n",
      "Epoch [1/10], Step [27150/68337], Loss: 5.5828\n",
      "Epoch [1/10], Step [27225/68337], Loss: 5.5008\n",
      "Epoch [1/10], Step [27300/68337], Loss: 5.2667\n",
      "Epoch [1/10], Step [27375/68337], Loss: 5.3124\n",
      "Epoch [1/10], Step [27450/68337], Loss: 5.5883\n",
      "Epoch [1/10], Step [27525/68337], Loss: 5.5102\n",
      "Epoch [1/10], Step [27600/68337], Loss: 5.2256\n",
      "Epoch [1/10], Step [27675/68337], Loss: 5.5593\n",
      "Epoch [1/10], Step [27750/68337], Loss: 5.5181\n",
      "Epoch [1/10], Step [27825/68337], Loss: 5.4950\n",
      "Epoch [1/10], Step [27900/68337], Loss: 5.5252\n",
      "Epoch [1/10], Step [27975/68337], Loss: 5.7262\n",
      "Epoch [1/10], Step [28050/68337], Loss: 5.6316\n",
      "Epoch [1/10], Step [28125/68337], Loss: 5.3616\n",
      "Epoch [1/10], Step [28200/68337], Loss: 5.3610\n",
      "Epoch [1/10], Step [28275/68337], Loss: 5.5180\n",
      "Epoch [1/10], Step [28350/68337], Loss: 5.3166\n",
      "Epoch [1/10], Step [28425/68337], Loss: 5.4662\n",
      "Epoch [1/10], Step [28500/68337], Loss: 5.4830\n",
      "Epoch [1/10], Step [28575/68337], Loss: 5.5225\n",
      "Epoch [1/10], Step [28650/68337], Loss: 5.5334\n",
      "Epoch [1/10], Step [28725/68337], Loss: 5.5153\n",
      "Epoch [1/10], Step [28800/68337], Loss: 5.5204\n",
      "Epoch [1/10], Step [28875/68337], Loss: 5.3967\n",
      "Epoch [1/10], Step [28950/68337], Loss: 5.4074\n",
      "Epoch [1/10], Step [29025/68337], Loss: 5.6118\n",
      "Epoch [1/10], Step [29100/68337], Loss: 5.5604\n",
      "Epoch [1/10], Step [29175/68337], Loss: 5.8805\n",
      "Epoch [1/10], Step [29250/68337], Loss: 5.1545\n",
      "Epoch [1/10], Step [29325/68337], Loss: 5.4796\n",
      "Epoch [1/10], Step [29400/68337], Loss: 5.6048\n",
      "Epoch [1/10], Step [29475/68337], Loss: 5.3476\n",
      "Epoch [1/10], Step [29550/68337], Loss: 5.5113\n",
      "Epoch [1/10], Step [29625/68337], Loss: 5.5078\n",
      "Epoch [1/10], Step [29700/68337], Loss: 5.5974\n",
      "Epoch [1/10], Step [29775/68337], Loss: 5.4712\n",
      "Epoch [1/10], Step [29850/68337], Loss: 5.3754\n",
      "Epoch [1/10], Step [29925/68337], Loss: 5.5103\n",
      "Epoch [1/10], Step [30000/68337], Loss: 5.3568\n",
      "Validation perplexity: 201.82804835211078\n",
      "Epoch [1/10], Step [30075/68337], Loss: 5.5744\n",
      "Epoch [1/10], Step [30150/68337], Loss: 5.4936\n",
      "Epoch [1/10], Step [30225/68337], Loss: 5.4452\n",
      "Epoch [1/10], Step [30300/68337], Loss: 5.5046\n",
      "Epoch [1/10], Step [30375/68337], Loss: 5.6056\n",
      "Epoch [1/10], Step [30450/68337], Loss: 5.5437\n",
      "Epoch [1/10], Step [30525/68337], Loss: 5.4723\n",
      "Epoch [1/10], Step [30600/68337], Loss: 5.6537\n",
      "Epoch [1/10], Step [30675/68337], Loss: 5.5452\n",
      "Epoch [1/10], Step [30750/68337], Loss: 5.3302\n",
      "Epoch [1/10], Step [30825/68337], Loss: 5.4514\n",
      "Epoch [1/10], Step [30900/68337], Loss: 5.4646\n",
      "Epoch [1/10], Step [30975/68337], Loss: 5.5767\n",
      "Epoch [1/10], Step [31050/68337], Loss: 5.4937\n",
      "Epoch [1/10], Step [31125/68337], Loss: 5.4946\n",
      "Epoch [1/10], Step [31200/68337], Loss: 5.5601\n",
      "Epoch [1/10], Step [31275/68337], Loss: 5.4205\n",
      "Epoch [1/10], Step [31350/68337], Loss: 5.5980\n",
      "Epoch [1/10], Step [31425/68337], Loss: 5.5370\n",
      "Epoch [1/10], Step [31500/68337], Loss: 5.5434\n",
      "Epoch [1/10], Step [31575/68337], Loss: 5.5474\n",
      "Epoch [1/10], Step [31650/68337], Loss: 5.4508\n",
      "Epoch [1/10], Step [31725/68337], Loss: 5.3953\n",
      "Epoch [1/10], Step [31800/68337], Loss: 5.5934\n",
      "Epoch [1/10], Step [31875/68337], Loss: 5.3640\n",
      "Epoch [1/10], Step [31950/68337], Loss: 5.5736\n",
      "Epoch [1/10], Step [32025/68337], Loss: 5.4895\n",
      "Epoch [1/10], Step [32100/68337], Loss: 5.3843\n",
      "Epoch [1/10], Step [32175/68337], Loss: 5.2832\n",
      "Epoch [1/10], Step [32250/68337], Loss: 5.4033\n",
      "Epoch [1/10], Step [32325/68337], Loss: 5.6898\n",
      "Epoch [1/10], Step [32400/68337], Loss: 5.3061\n",
      "Epoch [1/10], Step [32475/68337], Loss: 5.5493\n",
      "Epoch [1/10], Step [32550/68337], Loss: 5.4465\n",
      "Epoch [1/10], Step [32625/68337], Loss: 5.3230\n",
      "Epoch [1/10], Step [32700/68337], Loss: 5.6879\n",
      "Epoch [1/10], Step [32775/68337], Loss: 5.4121\n",
      "Epoch [1/10], Step [32850/68337], Loss: 5.3279\n",
      "Epoch [1/10], Step [32925/68337], Loss: 5.6204\n",
      "Epoch [1/10], Step [33000/68337], Loss: 5.5714\n",
      "Epoch [1/10], Step [33075/68337], Loss: 5.5590\n",
      "Epoch [1/10], Step [33150/68337], Loss: 5.4285\n",
      "Epoch [1/10], Step [33225/68337], Loss: 5.4915\n",
      "Epoch [1/10], Step [33300/68337], Loss: 5.6494\n",
      "Epoch [1/10], Step [33375/68337], Loss: 5.4480\n",
      "Epoch [1/10], Step [33450/68337], Loss: 5.6738\n",
      "Epoch [1/10], Step [33525/68337], Loss: 5.3928\n",
      "Epoch [1/10], Step [33600/68337], Loss: 5.5295\n",
      "Epoch [1/10], Step [33675/68337], Loss: 5.4322\n",
      "Epoch [1/10], Step [33750/68337], Loss: 5.4109\n",
      "Epoch [1/10], Step [33825/68337], Loss: 5.6119\n",
      "Epoch [1/10], Step [33900/68337], Loss: 5.5159\n",
      "Epoch [1/10], Step [33975/68337], Loss: 5.2993\n",
      "Epoch [1/10], Step [34050/68337], Loss: 5.4325\n",
      "Epoch [1/10], Step [34125/68337], Loss: 5.4492\n",
      "Epoch [1/10], Step [34200/68337], Loss: 5.4774\n",
      "Epoch [1/10], Step [34275/68337], Loss: 5.1451\n",
      "Epoch [1/10], Step [34350/68337], Loss: 5.7141\n",
      "Epoch [1/10], Step [34425/68337], Loss: 5.5347\n",
      "Epoch [1/10], Step [34500/68337], Loss: 5.4439\n",
      "Epoch [1/10], Step [34575/68337], Loss: 5.2067\n",
      "Epoch [1/10], Step [34650/68337], Loss: 5.4851\n",
      "Epoch [1/10], Step [34725/68337], Loss: 5.4778\n",
      "Epoch [1/10], Step [34800/68337], Loss: 5.6114\n",
      "Epoch [1/10], Step [34875/68337], Loss: 5.5552\n",
      "Epoch [1/10], Step [34950/68337], Loss: 5.5745\n",
      "Epoch [1/10], Step [35025/68337], Loss: 5.4057\n",
      "Epoch [1/10], Step [35100/68337], Loss: 5.3785\n",
      "Epoch [1/10], Step [35175/68337], Loss: 5.6041\n",
      "Epoch [1/10], Step [35250/68337], Loss: 5.2003\n",
      "Epoch [1/10], Step [35325/68337], Loss: 5.5298\n",
      "Epoch [1/10], Step [35400/68337], Loss: 5.5460\n",
      "Epoch [1/10], Step [35475/68337], Loss: 5.2860\n",
      "Epoch [1/10], Step [35550/68337], Loss: 5.4311\n",
      "Epoch [1/10], Step [35625/68337], Loss: 5.2806\n",
      "Epoch [1/10], Step [35700/68337], Loss: 5.6485\n",
      "Epoch [1/10], Step [35775/68337], Loss: 5.3910\n",
      "Epoch [1/10], Step [35850/68337], Loss: 5.3315\n",
      "Epoch [1/10], Step [35925/68337], Loss: 5.2724\n",
      "Epoch [1/10], Step [36000/68337], Loss: 5.4381\n",
      "Epoch [1/10], Step [36075/68337], Loss: 5.4703\n",
      "Epoch [1/10], Step [36150/68337], Loss: 5.3299\n",
      "Epoch [1/10], Step [36225/68337], Loss: 5.5748\n",
      "Epoch [1/10], Step [36300/68337], Loss: 5.5165\n",
      "Epoch [1/10], Step [36375/68337], Loss: 5.5713\n",
      "Epoch [1/10], Step [36450/68337], Loss: 5.4923\n",
      "Epoch [1/10], Step [36525/68337], Loss: 5.5112\n",
      "Epoch [1/10], Step [36600/68337], Loss: 5.4684\n",
      "Epoch [1/10], Step [36675/68337], Loss: 5.5919\n",
      "Epoch [1/10], Step [36750/68337], Loss: 5.5099\n",
      "Epoch [1/10], Step [36825/68337], Loss: 5.3963\n",
      "Epoch [1/10], Step [36900/68337], Loss: 5.4215\n",
      "Epoch [1/10], Step [36975/68337], Loss: 5.4898\n",
      "Epoch [1/10], Step [37050/68337], Loss: 5.5229\n",
      "Epoch [1/10], Step [37125/68337], Loss: 5.4821\n",
      "Epoch [1/10], Step [37200/68337], Loss: 5.5358\n",
      "Epoch [1/10], Step [37275/68337], Loss: 5.4310\n",
      "Epoch [1/10], Step [37350/68337], Loss: 5.5541\n",
      "Epoch [1/10], Step [37425/68337], Loss: 5.3472\n",
      "Epoch [1/10], Step [37500/68337], Loss: 5.3579\n",
      "Epoch [1/10], Step [37575/68337], Loss: 5.3777\n",
      "Epoch [1/10], Step [37650/68337], Loss: 5.3106\n",
      "Epoch [1/10], Step [37725/68337], Loss: 5.5400\n",
      "Epoch [1/10], Step [37800/68337], Loss: 5.3987\n",
      "Epoch [1/10], Step [37875/68337], Loss: 5.3598\n",
      "Epoch [1/10], Step [37950/68337], Loss: 5.3494\n",
      "Epoch [1/10], Step [38025/68337], Loss: 5.5225\n",
      "Epoch [1/10], Step [38100/68337], Loss: 5.2695\n",
      "Epoch [1/10], Step [38175/68337], Loss: 5.4531\n",
      "Epoch [1/10], Step [38250/68337], Loss: 5.3783\n",
      "Epoch [1/10], Step [38325/68337], Loss: 5.5053\n",
      "Epoch [1/10], Step [38400/68337], Loss: 5.2124\n",
      "Epoch [1/10], Step [38475/68337], Loss: 5.4719\n",
      "Epoch [1/10], Step [38550/68337], Loss: 5.6534\n",
      "Epoch [1/10], Step [38625/68337], Loss: 5.3940\n",
      "Epoch [1/10], Step [38700/68337], Loss: 5.3533\n",
      "Epoch [1/10], Step [38775/68337], Loss: 5.3656\n",
      "Epoch [1/10], Step [38850/68337], Loss: 5.3574\n",
      "Epoch [1/10], Step [38925/68337], Loss: 5.3736\n",
      "Epoch [1/10], Step [39000/68337], Loss: 5.5449\n",
      "Epoch [1/10], Step [39075/68337], Loss: 5.4629\n",
      "Epoch [1/10], Step [39150/68337], Loss: 5.2043\n",
      "Epoch [1/10], Step [39225/68337], Loss: 5.4468\n",
      "Epoch [1/10], Step [39300/68337], Loss: 5.3420\n",
      "Epoch [1/10], Step [39375/68337], Loss: 5.4460\n",
      "Epoch [1/10], Step [39450/68337], Loss: 5.5022\n",
      "Epoch [1/10], Step [39525/68337], Loss: 5.4182\n",
      "Epoch [1/10], Step [39600/68337], Loss: 5.3960\n",
      "Epoch [1/10], Step [39675/68337], Loss: 5.4290\n",
      "Epoch [1/10], Step [39750/68337], Loss: 5.4731\n",
      "Epoch [1/10], Step [39825/68337], Loss: 5.5067\n",
      "Epoch [1/10], Step [39900/68337], Loss: 5.4327\n",
      "Epoch [1/10], Step [39975/68337], Loss: 5.3717\n",
      "Validation perplexity: 184.8097846897233\n",
      "Epoch [1/10], Step [40050/68337], Loss: 5.4471\n",
      "Epoch [1/10], Step [40125/68337], Loss: 5.4595\n",
      "Epoch [1/10], Step [40200/68337], Loss: 5.4869\n",
      "Epoch [1/10], Step [40275/68337], Loss: 5.3716\n",
      "Epoch [1/10], Step [40350/68337], Loss: 5.2440\n",
      "Epoch [1/10], Step [40425/68337], Loss: 5.6551\n",
      "Epoch [1/10], Step [40500/68337], Loss: 5.3330\n",
      "Epoch [1/10], Step [40575/68337], Loss: 5.3827\n",
      "Epoch [1/10], Step [40650/68337], Loss: 5.3191\n",
      "Epoch [1/10], Step [40725/68337], Loss: 5.4303\n",
      "Epoch [1/10], Step [40800/68337], Loss: 5.4697\n",
      "Epoch [1/10], Step [40875/68337], Loss: 5.3333\n",
      "Epoch [1/10], Step [40950/68337], Loss: 5.5731\n",
      "Epoch [1/10], Step [41025/68337], Loss: 5.3872\n",
      "Epoch [1/10], Step [41100/68337], Loss: 5.4434\n",
      "Epoch [1/10], Step [41175/68337], Loss: 5.4869\n",
      "Epoch [1/10], Step [41250/68337], Loss: 5.3976\n",
      "Epoch [1/10], Step [41325/68337], Loss: 5.2163\n",
      "Epoch [1/10], Step [41400/68337], Loss: 5.5134\n",
      "Epoch [1/10], Step [41475/68337], Loss: 5.2468\n",
      "Epoch [1/10], Step [41550/68337], Loss: 5.3748\n",
      "Epoch [1/10], Step [41625/68337], Loss: 5.4406\n",
      "Epoch [1/10], Step [41700/68337], Loss: 5.3733\n",
      "Epoch [1/10], Step [41775/68337], Loss: 5.3867\n",
      "Epoch [1/10], Step [41850/68337], Loss: 5.5015\n",
      "Epoch [1/10], Step [41925/68337], Loss: 5.0986\n",
      "Epoch [1/10], Step [42000/68337], Loss: 5.5210\n",
      "Epoch [1/10], Step [42075/68337], Loss: 5.4783\n",
      "Epoch [1/10], Step [42150/68337], Loss: 5.5872\n",
      "Epoch [1/10], Step [42225/68337], Loss: 5.4077\n",
      "Epoch [1/10], Step [42300/68337], Loss: 5.2768\n",
      "Epoch [1/10], Step [42375/68337], Loss: 5.2289\n",
      "Epoch [1/10], Step [42450/68337], Loss: 5.3814\n",
      "Epoch [1/10], Step [42525/68337], Loss: 5.6069\n",
      "Epoch [1/10], Step [42600/68337], Loss: 5.3844\n",
      "Epoch [1/10], Step [42675/68337], Loss: 5.5528\n",
      "Epoch [1/10], Step [42750/68337], Loss: 5.2048\n",
      "Epoch [1/10], Step [42825/68337], Loss: 5.4546\n",
      "Epoch [1/10], Step [42900/68337], Loss: 5.6120\n",
      "Epoch [1/10], Step [42975/68337], Loss: 5.5087\n",
      "Epoch [1/10], Step [43050/68337], Loss: 5.5158\n",
      "Epoch [1/10], Step [43125/68337], Loss: 5.3358\n",
      "Epoch [1/10], Step [43200/68337], Loss: 5.3067\n",
      "Epoch [1/10], Step [43275/68337], Loss: 5.4641\n",
      "Epoch [1/10], Step [43350/68337], Loss: 5.3657\n",
      "Epoch [1/10], Step [43425/68337], Loss: 5.2057\n",
      "Epoch [1/10], Step [43500/68337], Loss: 5.3222\n",
      "Epoch [1/10], Step [43575/68337], Loss: 5.4168\n",
      "Epoch [1/10], Step [43650/68337], Loss: 5.4156\n",
      "Epoch [1/10], Step [43725/68337], Loss: 5.4121\n",
      "Epoch [1/10], Step [43800/68337], Loss: 5.4780\n",
      "Epoch [1/10], Step [43875/68337], Loss: 5.2448\n",
      "Epoch [1/10], Step [43950/68337], Loss: 5.5028\n",
      "Epoch [1/10], Step [44025/68337], Loss: 5.2650\n",
      "Epoch [1/10], Step [44100/68337], Loss: 5.3003\n",
      "Epoch [1/10], Step [44175/68337], Loss: 5.2691\n",
      "Epoch [1/10], Step [44250/68337], Loss: 5.4132\n",
      "Epoch [1/10], Step [44325/68337], Loss: 5.5186\n",
      "Epoch [1/10], Step [44400/68337], Loss: 5.5219\n",
      "Epoch [1/10], Step [44475/68337], Loss: 5.4305\n",
      "Epoch [1/10], Step [44550/68337], Loss: 5.3163\n",
      "Epoch [1/10], Step [44625/68337], Loss: 5.3898\n",
      "Epoch [1/10], Step [44700/68337], Loss: 5.2284\n",
      "Epoch [1/10], Step [44775/68337], Loss: 5.5936\n",
      "Epoch [1/10], Step [44850/68337], Loss: 5.3611\n",
      "Epoch [1/10], Step [44925/68337], Loss: 5.2865\n",
      "Epoch [1/10], Step [45000/68337], Loss: 5.3650\n",
      "Epoch [1/10], Step [45075/68337], Loss: 5.3641\n",
      "Epoch [1/10], Step [45150/68337], Loss: 5.3520\n",
      "Epoch [1/10], Step [45225/68337], Loss: 5.3243\n",
      "Epoch [1/10], Step [45300/68337], Loss: 5.4558\n",
      "Epoch [1/10], Step [45375/68337], Loss: 5.4373\n",
      "Epoch [1/10], Step [45450/68337], Loss: 5.3431\n",
      "Epoch [1/10], Step [45525/68337], Loss: 5.1896\n",
      "Epoch [1/10], Step [45600/68337], Loss: 5.1865\n",
      "Epoch [1/10], Step [45675/68337], Loss: 5.6884\n",
      "Epoch [1/10], Step [45750/68337], Loss: 5.4349\n",
      "Epoch [1/10], Step [45825/68337], Loss: 5.2820\n",
      "Epoch [1/10], Step [45900/68337], Loss: 5.5285\n",
      "Epoch [1/10], Step [45975/68337], Loss: 5.3802\n",
      "Epoch [1/10], Step [46050/68337], Loss: 5.2680\n",
      "Epoch [1/10], Step [46125/68337], Loss: 5.5495\n",
      "Epoch [1/10], Step [46200/68337], Loss: 5.1454\n",
      "Epoch [1/10], Step [46275/68337], Loss: 5.2687\n",
      "Epoch [1/10], Step [46350/68337], Loss: 5.2032\n",
      "Epoch [1/10], Step [46425/68337], Loss: 5.7302\n",
      "Epoch [1/10], Step [46500/68337], Loss: 5.4915\n",
      "Epoch [1/10], Step [46575/68337], Loss: 5.2477\n",
      "Epoch [1/10], Step [46650/68337], Loss: 5.3406\n",
      "Epoch [1/10], Step [46725/68337], Loss: 5.3452\n",
      "Epoch [1/10], Step [46800/68337], Loss: 5.3333\n",
      "Epoch [1/10], Step [46875/68337], Loss: 5.4605\n",
      "Epoch [1/10], Step [46950/68337], Loss: 5.4768\n",
      "Epoch [1/10], Step [47025/68337], Loss: 5.3507\n",
      "Epoch [1/10], Step [47100/68337], Loss: 5.2717\n",
      "Epoch [1/10], Step [47175/68337], Loss: 5.5013\n",
      "Epoch [1/10], Step [47250/68337], Loss: 5.1352\n",
      "Epoch [1/10], Step [47325/68337], Loss: 5.3334\n",
      "Epoch [1/10], Step [47400/68337], Loss: 5.0440\n",
      "Epoch [1/10], Step [47475/68337], Loss: 5.4806\n",
      "Epoch [1/10], Step [47550/68337], Loss: 5.3738\n",
      "Epoch [1/10], Step [47625/68337], Loss: 5.3490\n",
      "Epoch [1/10], Step [47700/68337], Loss: 5.3133\n",
      "Epoch [1/10], Step [47775/68337], Loss: 5.5422\n",
      "Epoch [1/10], Step [47850/68337], Loss: 5.2156\n",
      "Epoch [1/10], Step [47925/68337], Loss: 5.4105\n",
      "Epoch [1/10], Step [48000/68337], Loss: 5.2102\n",
      "Epoch [1/10], Step [48075/68337], Loss: 5.3408\n",
      "Epoch [1/10], Step [48150/68337], Loss: 5.5260\n",
      "Epoch [1/10], Step [48225/68337], Loss: 5.3167\n",
      "Epoch [1/10], Step [48300/68337], Loss: 5.3957\n",
      "Epoch [1/10], Step [48375/68337], Loss: 5.3732\n",
      "Epoch [1/10], Step [48450/68337], Loss: 5.3979\n",
      "Epoch [1/10], Step [48525/68337], Loss: 5.1870\n",
      "Epoch [1/10], Step [48600/68337], Loss: 5.2763\n",
      "Epoch [1/10], Step [48675/68337], Loss: 5.3893\n",
      "Epoch [1/10], Step [48750/68337], Loss: 5.4352\n",
      "Epoch [1/10], Step [48825/68337], Loss: 5.2551\n",
      "Epoch [1/10], Step [48900/68337], Loss: 5.1835\n",
      "Epoch [1/10], Step [48975/68337], Loss: 5.2153\n",
      "Epoch [1/10], Step [49050/68337], Loss: 5.4073\n",
      "Epoch [1/10], Step [49125/68337], Loss: 5.3624\n",
      "Epoch [1/10], Step [49200/68337], Loss: 5.5617\n",
      "Epoch [1/10], Step [49275/68337], Loss: 5.0345\n",
      "Epoch [1/10], Step [49350/68337], Loss: 5.2695\n",
      "Epoch [1/10], Step [49425/68337], Loss: 5.2145\n",
      "Epoch [1/10], Step [49500/68337], Loss: 5.4681\n",
      "Epoch [1/10], Step [49575/68337], Loss: 5.3671\n",
      "Epoch [1/10], Step [49650/68337], Loss: 5.3389\n",
      "Epoch [1/10], Step [49725/68337], Loss: 5.3847\n",
      "Epoch [1/10], Step [49800/68337], Loss: 5.3118\n",
      "Epoch [1/10], Step [49875/68337], Loss: 5.3419\n",
      "Epoch [1/10], Step [49950/68337], Loss: 5.5402\n",
      "Validation perplexity: 175.01902543450186\n",
      "Epoch [1/10], Step [50025/68337], Loss: 5.3029\n",
      "Epoch [1/10], Step [50100/68337], Loss: 5.3308\n",
      "Epoch [1/10], Step [50175/68337], Loss: 5.2109\n",
      "Epoch [1/10], Step [50250/68337], Loss: 5.4336\n",
      "Epoch [1/10], Step [50325/68337], Loss: 5.0620\n",
      "Epoch [1/10], Step [50400/68337], Loss: 5.3613\n",
      "Epoch [1/10], Step [50475/68337], Loss: 5.4203\n",
      "Epoch [1/10], Step [50550/68337], Loss: 5.1828\n",
      "Epoch [1/10], Step [50625/68337], Loss: 5.4221\n",
      "Epoch [1/10], Step [50700/68337], Loss: 5.4583\n",
      "Epoch [1/10], Step [50775/68337], Loss: 5.3140\n",
      "Epoch [1/10], Step [50850/68337], Loss: 5.3157\n",
      "Epoch [1/10], Step [50925/68337], Loss: 5.4354\n",
      "Epoch [1/10], Step [51000/68337], Loss: 5.4029\n",
      "Epoch [1/10], Step [51075/68337], Loss: 5.2819\n",
      "Epoch [1/10], Step [51150/68337], Loss: 5.2555\n",
      "Epoch [1/10], Step [51225/68337], Loss: 5.3626\n",
      "Epoch [1/10], Step [51300/68337], Loss: 5.3661\n",
      "Epoch [1/10], Step [51375/68337], Loss: 5.1918\n",
      "Epoch [1/10], Step [51450/68337], Loss: 5.2318\n",
      "Epoch [1/10], Step [51525/68337], Loss: 5.3099\n",
      "Epoch [1/10], Step [51600/68337], Loss: 5.2463\n",
      "Epoch [1/10], Step [51675/68337], Loss: 5.1726\n",
      "Epoch [1/10], Step [51750/68337], Loss: 5.3423\n",
      "Epoch [1/10], Step [51825/68337], Loss: 5.4235\n",
      "Epoch [1/10], Step [51900/68337], Loss: 5.3139\n",
      "Epoch [1/10], Step [51975/68337], Loss: 5.3358\n",
      "Epoch [1/10], Step [52050/68337], Loss: 5.3940\n",
      "Epoch [1/10], Step [52125/68337], Loss: 5.3240\n",
      "Epoch [1/10], Step [52200/68337], Loss: 5.6048\n",
      "Epoch [1/10], Step [52275/68337], Loss: 5.6282\n",
      "Epoch [1/10], Step [52350/68337], Loss: 5.5006\n",
      "Epoch [1/10], Step [52425/68337], Loss: 5.3949\n",
      "Epoch [1/10], Step [52500/68337], Loss: 5.3083\n",
      "Epoch [1/10], Step [52575/68337], Loss: 5.3426\n",
      "Epoch [1/10], Step [52650/68337], Loss: 5.3735\n",
      "Epoch [1/10], Step [52725/68337], Loss: 5.3329\n",
      "Epoch [1/10], Step [52800/68337], Loss: 5.3733\n",
      "Epoch [1/10], Step [52875/68337], Loss: 5.3997\n",
      "Epoch [1/10], Step [52950/68337], Loss: 5.3468\n",
      "Epoch [1/10], Step [53025/68337], Loss: 5.6823\n",
      "Epoch [1/10], Step [53100/68337], Loss: 5.4584\n",
      "Epoch [1/10], Step [53175/68337], Loss: 5.3856\n",
      "Epoch [1/10], Step [53250/68337], Loss: 5.4814\n",
      "Epoch [1/10], Step [53325/68337], Loss: 5.3661\n",
      "Epoch [1/10], Step [53400/68337], Loss: 5.2073\n",
      "Epoch [1/10], Step [53475/68337], Loss: 5.5075\n",
      "Epoch [1/10], Step [53550/68337], Loss: 5.1778\n",
      "Epoch [1/10], Step [53625/68337], Loss: 5.2954\n",
      "Epoch [1/10], Step [53700/68337], Loss: 5.5042\n",
      "Epoch [1/10], Step [53775/68337], Loss: 5.2460\n",
      "Epoch [1/10], Step [53850/68337], Loss: 5.4847\n",
      "Epoch [1/10], Step [53925/68337], Loss: 5.3623\n",
      "Epoch [1/10], Step [54000/68337], Loss: 5.3536\n",
      "Epoch [1/10], Step [54075/68337], Loss: 5.5056\n",
      "Epoch [1/10], Step [54150/68337], Loss: 5.5485\n",
      "Epoch [1/10], Step [54225/68337], Loss: 5.2412\n",
      "Epoch [1/10], Step [54300/68337], Loss: 5.3821\n",
      "Epoch [1/10], Step [54375/68337], Loss: 5.2603\n",
      "Epoch [1/10], Step [54450/68337], Loss: 5.2670\n",
      "Epoch [1/10], Step [54525/68337], Loss: 5.1054\n",
      "Epoch [1/10], Step [54600/68337], Loss: 5.3086\n",
      "Epoch [1/10], Step [54675/68337], Loss: 5.4988\n",
      "Epoch [1/10], Step [54750/68337], Loss: 5.2877\n",
      "Epoch [1/10], Step [54825/68337], Loss: 5.3795\n",
      "Epoch [1/10], Step [54900/68337], Loss: 5.5742\n",
      "Epoch [1/10], Step [54975/68337], Loss: 5.3760\n",
      "Epoch [1/10], Step [55050/68337], Loss: 5.3117\n",
      "Epoch [1/10], Step [55125/68337], Loss: 5.3744\n",
      "Epoch [1/10], Step [55200/68337], Loss: 5.4950\n",
      "Epoch [1/10], Step [55275/68337], Loss: 5.1957\n",
      "Epoch [1/10], Step [55350/68337], Loss: 5.4300\n",
      "Epoch [1/10], Step [55425/68337], Loss: 5.3623\n",
      "Epoch [1/10], Step [55500/68337], Loss: 5.5402\n",
      "Epoch [1/10], Step [55575/68337], Loss: 5.2991\n",
      "Epoch [1/10], Step [55650/68337], Loss: 5.0878\n",
      "Epoch [1/10], Step [55725/68337], Loss: 5.4000\n",
      "Epoch [1/10], Step [55800/68337], Loss: 5.4119\n",
      "Epoch [1/10], Step [55875/68337], Loss: 5.2814\n",
      "Epoch [1/10], Step [55950/68337], Loss: 5.3850\n",
      "Epoch [1/10], Step [56025/68337], Loss: 5.3432\n",
      "Epoch [1/10], Step [56100/68337], Loss: 5.3448\n",
      "Epoch [1/10], Step [56175/68337], Loss: 5.3743\n",
      "Epoch [1/10], Step [56250/68337], Loss: 5.2986\n",
      "Epoch [1/10], Step [56325/68337], Loss: 5.2990\n",
      "Epoch [1/10], Step [56400/68337], Loss: 5.3849\n",
      "Epoch [1/10], Step [56475/68337], Loss: 5.3432\n",
      "Epoch [1/10], Step [56550/68337], Loss: 5.5202\n",
      "Epoch [1/10], Step [56625/68337], Loss: 5.1737\n",
      "Epoch [1/10], Step [56700/68337], Loss: 5.2514\n",
      "Epoch [1/10], Step [56775/68337], Loss: 5.6636\n",
      "Epoch [1/10], Step [56850/68337], Loss: 5.4861\n",
      "Epoch [1/10], Step [56925/68337], Loss: 5.3435\n",
      "Epoch [1/10], Step [57000/68337], Loss: 5.3069\n",
      "Epoch [1/10], Step [57075/68337], Loss: 5.4261\n",
      "Epoch [1/10], Step [57150/68337], Loss: 5.5716\n",
      "Epoch [1/10], Step [57225/68337], Loss: 5.2777\n",
      "Epoch [1/10], Step [57300/68337], Loss: 5.4613\n",
      "Epoch [1/10], Step [57375/68337], Loss: 5.5215\n",
      "Epoch [1/10], Step [57450/68337], Loss: 5.3536\n",
      "Epoch [1/10], Step [57525/68337], Loss: 5.2628\n",
      "Epoch [1/10], Step [57600/68337], Loss: 5.0631\n",
      "Epoch [1/10], Step [57675/68337], Loss: 5.2188\n",
      "Epoch [1/10], Step [57750/68337], Loss: 5.4669\n",
      "Epoch [1/10], Step [57825/68337], Loss: 5.3161\n",
      "Epoch [1/10], Step [57900/68337], Loss: 5.4441\n",
      "Epoch [1/10], Step [57975/68337], Loss: 5.3678\n",
      "Epoch [1/10], Step [58050/68337], Loss: 5.2668\n",
      "Epoch [1/10], Step [58125/68337], Loss: 5.4920\n",
      "Epoch [1/10], Step [58200/68337], Loss: 5.2617\n",
      "Epoch [1/10], Step [58275/68337], Loss: 5.2758\n",
      "Epoch [1/10], Step [58350/68337], Loss: 5.2860\n",
      "Epoch [1/10], Step [58425/68337], Loss: 5.2151\n",
      "Epoch [1/10], Step [58500/68337], Loss: 5.5120\n",
      "Epoch [1/10], Step [58575/68337], Loss: 5.1738\n",
      "Epoch [1/10], Step [58650/68337], Loss: 5.4389\n",
      "Epoch [1/10], Step [58725/68337], Loss: 5.3408\n",
      "Epoch [1/10], Step [58800/68337], Loss: 5.3756\n",
      "Epoch [1/10], Step [58875/68337], Loss: 5.4058\n",
      "Epoch [1/10], Step [58950/68337], Loss: 5.5703\n",
      "Epoch [1/10], Step [59025/68337], Loss: 5.2050\n",
      "Epoch [1/10], Step [59100/68337], Loss: 5.5046\n",
      "Epoch [1/10], Step [59175/68337], Loss: 5.2340\n",
      "Epoch [1/10], Step [59250/68337], Loss: 5.5720\n",
      "Epoch [1/10], Step [59325/68337], Loss: 5.3456\n",
      "Epoch [1/10], Step [59400/68337], Loss: 5.4162\n",
      "Epoch [1/10], Step [59475/68337], Loss: 5.2088\n",
      "Epoch [1/10], Step [59550/68337], Loss: 5.3460\n",
      "Epoch [1/10], Step [59625/68337], Loss: 5.2346\n",
      "Epoch [1/10], Step [59700/68337], Loss: 5.4229\n",
      "Epoch [1/10], Step [59775/68337], Loss: 5.1832\n",
      "Epoch [1/10], Step [59850/68337], Loss: 5.3996\n",
      "Epoch [1/10], Step [59925/68337], Loss: 5.3077\n",
      "Epoch [1/10], Step [60000/68337], Loss: 5.2554\n",
      "Validation perplexity: 167.6673019727876\n",
      "Epoch [1/10], Step [60075/68337], Loss: 5.4703\n",
      "Epoch [1/10], Step [60150/68337], Loss: 5.5454\n",
      "Epoch [1/10], Step [60225/68337], Loss: 5.1469\n",
      "Epoch [1/10], Step [60300/68337], Loss: 5.2892\n",
      "Epoch [1/10], Step [60375/68337], Loss: 5.3132\n",
      "Epoch [1/10], Step [60450/68337], Loss: 5.3924\n",
      "Epoch [1/10], Step [60525/68337], Loss: 5.4589\n",
      "Epoch [1/10], Step [60600/68337], Loss: 5.3767\n",
      "Epoch [1/10], Step [60675/68337], Loss: 5.1096\n",
      "Epoch [1/10], Step [60750/68337], Loss: 5.2851\n",
      "Epoch [1/10], Step [60825/68337], Loss: 5.3060\n",
      "Epoch [1/10], Step [60900/68337], Loss: 5.2280\n",
      "Epoch [1/10], Step [60975/68337], Loss: 5.3849\n",
      "Epoch [1/10], Step [61050/68337], Loss: 5.3115\n",
      "Epoch [1/10], Step [61125/68337], Loss: 5.2577\n",
      "Epoch [1/10], Step [61200/68337], Loss: 5.3330\n",
      "Epoch [1/10], Step [61275/68337], Loss: 5.1280\n",
      "Epoch [1/10], Step [61350/68337], Loss: 5.3241\n",
      "Epoch [1/10], Step [61425/68337], Loss: 5.4158\n",
      "Epoch [1/10], Step [61500/68337], Loss: 5.5257\n",
      "Epoch [1/10], Step [61575/68337], Loss: 5.5075\n",
      "Epoch [1/10], Step [61650/68337], Loss: 5.2238\n",
      "Epoch [1/10], Step [61725/68337], Loss: 5.3154\n",
      "Epoch [1/10], Step [61800/68337], Loss: 5.3817\n",
      "Epoch [1/10], Step [61875/68337], Loss: 5.5017\n",
      "Epoch [1/10], Step [61950/68337], Loss: 5.5286\n",
      "Epoch [1/10], Step [62025/68337], Loss: 5.4818\n",
      "Epoch [1/10], Step [62100/68337], Loss: 5.3933\n",
      "Epoch [1/10], Step [62175/68337], Loss: 5.3335\n",
      "Epoch [1/10], Step [62250/68337], Loss: 5.2585\n",
      "Epoch [1/10], Step [62325/68337], Loss: 5.3241\n",
      "Epoch [1/10], Step [62400/68337], Loss: 5.5554\n",
      "Epoch [1/10], Step [62475/68337], Loss: 5.2396\n",
      "Epoch [1/10], Step [62550/68337], Loss: 5.2031\n",
      "Epoch [1/10], Step [62625/68337], Loss: 5.5511\n",
      "Epoch [1/10], Step [62700/68337], Loss: 5.4728\n",
      "Epoch [1/10], Step [62775/68337], Loss: 5.4593\n",
      "Epoch [1/10], Step [62850/68337], Loss: 5.2194\n",
      "Epoch [1/10], Step [62925/68337], Loss: 5.3734\n",
      "Epoch [1/10], Step [63000/68337], Loss: 5.4514\n",
      "Epoch [1/10], Step [63075/68337], Loss: 5.2758\n",
      "Epoch [1/10], Step [63150/68337], Loss: 5.3179\n",
      "Epoch [1/10], Step [63225/68337], Loss: 5.2928\n",
      "Epoch [1/10], Step [63300/68337], Loss: 5.4358\n",
      "Epoch [1/10], Step [63375/68337], Loss: 5.4772\n",
      "Epoch [1/10], Step [63450/68337], Loss: 5.3526\n",
      "Epoch [1/10], Step [63525/68337], Loss: 5.5400\n",
      "Epoch [1/10], Step [63600/68337], Loss: 5.3367\n",
      "Epoch [1/10], Step [63675/68337], Loss: 5.3528\n",
      "Epoch [1/10], Step [63750/68337], Loss: 5.4137\n",
      "Epoch [1/10], Step [63825/68337], Loss: 5.2971\n",
      "Epoch [1/10], Step [63900/68337], Loss: 5.4513\n",
      "Epoch [1/10], Step [63975/68337], Loss: 5.4170\n",
      "Epoch [1/10], Step [64050/68337], Loss: 5.3912\n",
      "Epoch [1/10], Step [64125/68337], Loss: 5.4908\n",
      "Epoch [1/10], Step [64200/68337], Loss: 5.3122\n",
      "Epoch [1/10], Step [64275/68337], Loss: 5.5530\n",
      "Epoch [1/10], Step [64350/68337], Loss: 5.2773\n",
      "Epoch [1/10], Step [64425/68337], Loss: 5.2625\n",
      "Epoch [1/10], Step [64500/68337], Loss: 5.3795\n",
      "Epoch [1/10], Step [64575/68337], Loss: 5.3105\n",
      "Epoch [1/10], Step [64650/68337], Loss: 5.2676\n",
      "Epoch [1/10], Step [64725/68337], Loss: 5.2989\n",
      "Epoch [1/10], Step [64800/68337], Loss: 5.1949\n",
      "Epoch [1/10], Step [64875/68337], Loss: 5.3531\n",
      "Epoch [1/10], Step [64950/68337], Loss: 5.2360\n",
      "Epoch [1/10], Step [65025/68337], Loss: 5.3705\n",
      "Epoch [1/10], Step [65100/68337], Loss: 5.0215\n",
      "Epoch [1/10], Step [65175/68337], Loss: 5.4338\n",
      "Epoch [1/10], Step [65250/68337], Loss: 5.0662\n",
      "Epoch [1/10], Step [65325/68337], Loss: 5.1451\n",
      "Epoch [1/10], Step [65400/68337], Loss: 5.4866\n",
      "Epoch [1/10], Step [65475/68337], Loss: 5.4733\n",
      "Epoch [1/10], Step [65550/68337], Loss: 5.3464\n",
      "Epoch [1/10], Step [65625/68337], Loss: 5.2465\n",
      "Epoch [1/10], Step [65700/68337], Loss: 5.3135\n",
      "Epoch [1/10], Step [65775/68337], Loss: 5.3978\n",
      "Epoch [1/10], Step [65850/68337], Loss: 5.4378\n",
      "Epoch [1/10], Step [65925/68337], Loss: 5.4366\n",
      "Epoch [1/10], Step [66000/68337], Loss: 5.3748\n",
      "Epoch [1/10], Step [66075/68337], Loss: 5.4102\n",
      "Epoch [1/10], Step [66150/68337], Loss: 5.1658\n",
      "Epoch [1/10], Step [66225/68337], Loss: 5.3074\n",
      "Epoch [1/10], Step [66300/68337], Loss: 5.2906\n",
      "Epoch [1/10], Step [66375/68337], Loss: 5.2115\n",
      "Epoch [1/10], Step [66450/68337], Loss: 5.3769\n",
      "Epoch [1/10], Step [66525/68337], Loss: 5.2191\n",
      "Epoch [1/10], Step [66600/68337], Loss: 5.2519\n",
      "Epoch [1/10], Step [66675/68337], Loss: 5.1709\n",
      "Epoch [1/10], Step [66750/68337], Loss: 5.3776\n",
      "Epoch [1/10], Step [66825/68337], Loss: 5.2591\n",
      "Epoch [1/10], Step [66900/68337], Loss: 5.3135\n",
      "Epoch [1/10], Step [66975/68337], Loss: 5.3905\n",
      "Epoch [1/10], Step [67050/68337], Loss: 5.6373\n",
      "Epoch [1/10], Step [67125/68337], Loss: 5.2371\n",
      "Epoch [1/10], Step [67200/68337], Loss: 5.4864\n",
      "Epoch [1/10], Step [67275/68337], Loss: 5.2508\n",
      "Epoch [1/10], Step [67350/68337], Loss: 5.2337\n",
      "Epoch [1/10], Step [67425/68337], Loss: 5.4324\n",
      "Epoch [1/10], Step [67500/68337], Loss: 5.0872\n",
      "Epoch [1/10], Step [67575/68337], Loss: 5.2133\n",
      "Epoch [1/10], Step [67650/68337], Loss: 5.2068\n",
      "Epoch [1/10], Step [67725/68337], Loss: 5.3218\n",
      "Epoch [1/10], Step [67800/68337], Loss: 5.2523\n",
      "Epoch [1/10], Step [67875/68337], Loss: 5.3521\n",
      "Epoch [1/10], Step [67950/68337], Loss: 5.3732\n",
      "Epoch [1/10], Step [68025/68337], Loss: 5.2668\n",
      "Epoch [1/10], Step [68100/68337], Loss: 5.5162\n",
      "Epoch [1/10], Step [68175/68337], Loss: 5.3927\n",
      "Epoch [1/10], Step [68250/68337], Loss: 5.3589\n",
      "Epoch [1/10], Step [68325/68337], Loss: 5.7488\n",
      "Epoch [1/10] Average Loss: 5.5831, Perplexity: 265.90\n",
      "Epoch [2/10], Step [0/68337], Loss: 5.3036\n",
      "Validation perplexity: 162.65332120197468\n",
      "Epoch [2/10], Step [75/68337], Loss: 5.2124\n",
      "Epoch [2/10], Step [150/68337], Loss: 5.3512\n",
      "Epoch [2/10], Step [225/68337], Loss: 5.3429\n",
      "Epoch [2/10], Step [300/68337], Loss: 4.9874\n",
      "Epoch [2/10], Step [375/68337], Loss: 5.2459\n",
      "Epoch [2/10], Step [450/68337], Loss: 5.3285\n",
      "Epoch [2/10], Step [525/68337], Loss: 5.2391\n",
      "Epoch [2/10], Step [600/68337], Loss: 5.2100\n",
      "Epoch [2/10], Step [675/68337], Loss: 5.2039\n",
      "Epoch [2/10], Step [750/68337], Loss: 5.3485\n",
      "Epoch [2/10], Step [825/68337], Loss: 5.2396\n",
      "Epoch [2/10], Step [900/68337], Loss: 5.2120\n",
      "Epoch [2/10], Step [975/68337], Loss: 5.3527\n",
      "Epoch [2/10], Step [1050/68337], Loss: 5.3329\n",
      "Epoch [2/10], Step [1125/68337], Loss: 5.0004\n",
      "Epoch [2/10], Step [1200/68337], Loss: 5.3173\n",
      "Epoch [2/10], Step [1275/68337], Loss: 5.5400\n",
      "Epoch [2/10], Step [1350/68337], Loss: 5.2014\n",
      "Epoch [2/10], Step [1425/68337], Loss: 5.1966\n",
      "Epoch [2/10], Step [1500/68337], Loss: 5.0780\n",
      "Epoch [2/10], Step [1575/68337], Loss: 5.5509\n",
      "Epoch [2/10], Step [1650/68337], Loss: 5.1615\n",
      "Epoch [2/10], Step [1725/68337], Loss: 5.1446\n",
      "Epoch [2/10], Step [1800/68337], Loss: 5.2901\n",
      "Epoch [2/10], Step [1875/68337], Loss: 5.2571\n",
      "Epoch [2/10], Step [1950/68337], Loss: 5.3726\n",
      "Epoch [2/10], Step [2025/68337], Loss: 5.3050\n",
      "Epoch [2/10], Step [2100/68337], Loss: 5.0406\n",
      "Epoch [2/10], Step [2175/68337], Loss: 5.2903\n",
      "Epoch [2/10], Step [2250/68337], Loss: 5.1155\n",
      "Epoch [2/10], Step [2325/68337], Loss: 5.3154\n",
      "Epoch [2/10], Step [2400/68337], Loss: 5.3018\n",
      "Epoch [2/10], Step [2475/68337], Loss: 5.4751\n",
      "Epoch [2/10], Step [2550/68337], Loss: 5.4533\n",
      "Epoch [2/10], Step [2625/68337], Loss: 5.3079\n",
      "Epoch [2/10], Step [2700/68337], Loss: 5.2911\n",
      "Epoch [2/10], Step [2775/68337], Loss: 5.2562\n",
      "Epoch [2/10], Step [2850/68337], Loss: 5.3461\n",
      "Epoch [2/10], Step [2925/68337], Loss: 5.4621\n",
      "Epoch [2/10], Step [3000/68337], Loss: 5.3433\n",
      "Epoch [2/10], Step [3075/68337], Loss: 5.2621\n",
      "Epoch [2/10], Step [3150/68337], Loss: 5.2690\n",
      "Epoch [2/10], Step [3225/68337], Loss: 5.2533\n",
      "Epoch [2/10], Step [3300/68337], Loss: 5.1913\n",
      "Epoch [2/10], Step [3375/68337], Loss: 5.2277\n",
      "Epoch [2/10], Step [3450/68337], Loss: 5.2260\n",
      "Epoch [2/10], Step [3525/68337], Loss: 5.3535\n",
      "Epoch [2/10], Step [3600/68337], Loss: 5.1938\n",
      "Epoch [2/10], Step [3675/68337], Loss: 5.2961\n",
      "Epoch [2/10], Step [3750/68337], Loss: 5.1411\n",
      "Epoch [2/10], Step [3825/68337], Loss: 5.1500\n",
      "Epoch [2/10], Step [3900/68337], Loss: 5.3839\n",
      "Epoch [2/10], Step [3975/68337], Loss: 5.3323\n",
      "Epoch [2/10], Step [4050/68337], Loss: 5.3662\n",
      "Epoch [2/10], Step [4125/68337], Loss: 5.1484\n",
      "Epoch [2/10], Step [4200/68337], Loss: 5.3510\n",
      "Epoch [2/10], Step [4275/68337], Loss: 5.3516\n",
      "Epoch [2/10], Step [4350/68337], Loss: 5.2270\n",
      "Epoch [2/10], Step [4425/68337], Loss: 5.1390\n",
      "Epoch [2/10], Step [4500/68337], Loss: 5.2487\n",
      "Epoch [2/10], Step [4575/68337], Loss: 5.3745\n",
      "Epoch [2/10], Step [4650/68337], Loss: 5.1343\n",
      "Epoch [2/10], Step [4725/68337], Loss: 5.3912\n",
      "Epoch [2/10], Step [4800/68337], Loss: 5.3201\n",
      "Epoch [2/10], Step [4875/68337], Loss: 5.5784\n",
      "Epoch [2/10], Step [4950/68337], Loss: 5.3927\n",
      "Epoch [2/10], Step [5025/68337], Loss: 5.3966\n",
      "Epoch [2/10], Step [5100/68337], Loss: 5.0490\n",
      "Epoch [2/10], Step [5175/68337], Loss: 5.3210\n",
      "Epoch [2/10], Step [5250/68337], Loss: 5.1841\n",
      "Epoch [2/10], Step [5325/68337], Loss: 5.2873\n",
      "Epoch [2/10], Step [5400/68337], Loss: 5.1490\n",
      "Epoch [2/10], Step [5475/68337], Loss: 5.3812\n",
      "Epoch [2/10], Step [5550/68337], Loss: 5.5185\n",
      "Epoch [2/10], Step [5625/68337], Loss: 5.1076\n",
      "Epoch [2/10], Step [5700/68337], Loss: 5.2411\n",
      "Epoch [2/10], Step [5775/68337], Loss: 5.4168\n",
      "Epoch [2/10], Step [5850/68337], Loss: 5.2212\n",
      "Epoch [2/10], Step [5925/68337], Loss: 5.1533\n",
      "Epoch [2/10], Step [6000/68337], Loss: 5.5674\n",
      "Epoch [2/10], Step [6075/68337], Loss: 5.1710\n",
      "Epoch [2/10], Step [6150/68337], Loss: 5.3171\n",
      "Epoch [2/10], Step [6225/68337], Loss: 5.1724\n",
      "Epoch [2/10], Step [6300/68337], Loss: 5.1115\n",
      "Epoch [2/10], Step [6375/68337], Loss: 5.3353\n",
      "Epoch [2/10], Step [6450/68337], Loss: 5.4676\n",
      "Epoch [2/10], Step [6525/68337], Loss: 5.4969\n",
      "Epoch [2/10], Step [6600/68337], Loss: 5.2610\n",
      "Epoch [2/10], Step [6675/68337], Loss: 5.1540\n",
      "Epoch [2/10], Step [6750/68337], Loss: 5.3256\n",
      "Epoch [2/10], Step [6825/68337], Loss: 5.2606\n",
      "Epoch [2/10], Step [6900/68337], Loss: 5.2859\n",
      "Epoch [2/10], Step [6975/68337], Loss: 5.3198\n",
      "Epoch [2/10], Step [7050/68337], Loss: 5.2627\n",
      "Epoch [2/10], Step [7125/68337], Loss: 5.3092\n",
      "Epoch [2/10], Step [7200/68337], Loss: 5.1637\n",
      "Epoch [2/10], Step [7275/68337], Loss: 5.3835\n",
      "Epoch [2/10], Step [7350/68337], Loss: 5.4518\n",
      "Epoch [2/10], Step [7425/68337], Loss: 5.2003\n",
      "Epoch [2/10], Step [7500/68337], Loss: 5.0693\n",
      "Epoch [2/10], Step [7575/68337], Loss: 5.4169\n",
      "Epoch [2/10], Step [7650/68337], Loss: 5.5031\n",
      "Epoch [2/10], Step [7725/68337], Loss: 5.3205\n",
      "Epoch [2/10], Step [7800/68337], Loss: 5.0834\n",
      "Epoch [2/10], Step [7875/68337], Loss: 5.4517\n",
      "Epoch [2/10], Step [7950/68337], Loss: 5.4206\n",
      "Epoch [2/10], Step [8025/68337], Loss: 5.2118\n",
      "Epoch [2/10], Step [8100/68337], Loss: 5.3525\n",
      "Epoch [2/10], Step [8175/68337], Loss: 5.4722\n",
      "Epoch [2/10], Step [8250/68337], Loss: 5.4646\n",
      "Epoch [2/10], Step [8325/68337], Loss: 5.4016\n",
      "Epoch [2/10], Step [8400/68337], Loss: 5.5492\n",
      "Epoch [2/10], Step [8475/68337], Loss: 5.2416\n",
      "Epoch [2/10], Step [8550/68337], Loss: 5.4765\n",
      "Epoch [2/10], Step [8625/68337], Loss: 5.3796\n",
      "Epoch [2/10], Step [8700/68337], Loss: 5.2390\n",
      "Epoch [2/10], Step [8775/68337], Loss: 5.4546\n",
      "Epoch [2/10], Step [8850/68337], Loss: 5.3299\n",
      "Epoch [2/10], Step [8925/68337], Loss: 5.3055\n",
      "Epoch [2/10], Step [9000/68337], Loss: 5.4555\n",
      "Epoch [2/10], Step [9075/68337], Loss: 5.2171\n",
      "Epoch [2/10], Step [9150/68337], Loss: 5.1605\n",
      "Epoch [2/10], Step [9225/68337], Loss: 4.9873\n",
      "Epoch [2/10], Step [9300/68337], Loss: 5.4448\n",
      "Epoch [2/10], Step [9375/68337], Loss: 5.2043\n",
      "Epoch [2/10], Step [9450/68337], Loss: 5.5077\n",
      "Epoch [2/10], Step [9525/68337], Loss: 5.3325\n",
      "Epoch [2/10], Step [9600/68337], Loss: 5.3176\n",
      "Epoch [2/10], Step [9675/68337], Loss: 5.2376\n",
      "Epoch [2/10], Step [9750/68337], Loss: 5.2030\n",
      "Epoch [2/10], Step [9825/68337], Loss: 5.4782\n",
      "Epoch [2/10], Step [9900/68337], Loss: 5.4480\n",
      "Epoch [2/10], Step [9975/68337], Loss: 5.0922\n",
      "Validation perplexity: 157.2057957340604\n",
      "Epoch [2/10], Step [10050/68337], Loss: 5.2691\n",
      "Epoch [2/10], Step [10125/68337], Loss: 5.2683\n",
      "Epoch [2/10], Step [10200/68337], Loss: 5.2032\n",
      "Epoch [2/10], Step [10275/68337], Loss: 5.2478\n",
      "Epoch [2/10], Step [10350/68337], Loss: 5.1580\n",
      "Epoch [2/10], Step [10425/68337], Loss: 5.2118\n",
      "Epoch [2/10], Step [10500/68337], Loss: 5.1978\n",
      "Epoch [2/10], Step [10575/68337], Loss: 5.1861\n",
      "Epoch [2/10], Step [10650/68337], Loss: 5.2786\n",
      "Epoch [2/10], Step [10725/68337], Loss: 5.0885\n",
      "Epoch [2/10], Step [10800/68337], Loss: 5.2573\n",
      "Epoch [2/10], Step [10875/68337], Loss: 5.3085\n",
      "Epoch [2/10], Step [10950/68337], Loss: 5.3270\n",
      "Epoch [2/10], Step [11025/68337], Loss: 5.2875\n",
      "Epoch [2/10], Step [11100/68337], Loss: 5.1252\n",
      "Epoch [2/10], Step [11175/68337], Loss: 5.3269\n",
      "Epoch [2/10], Step [11250/68337], Loss: 5.3022\n",
      "Epoch [2/10], Step [11325/68337], Loss: 5.4599\n",
      "Epoch [2/10], Step [11400/68337], Loss: 5.3678\n",
      "Epoch [2/10], Step [11475/68337], Loss: 5.3274\n",
      "Epoch [2/10], Step [11550/68337], Loss: 5.3258\n",
      "Epoch [2/10], Step [11625/68337], Loss: 5.4569\n",
      "Epoch [2/10], Step [11700/68337], Loss: 5.3552\n",
      "Epoch [2/10], Step [11775/68337], Loss: 5.2463\n",
      "Epoch [2/10], Step [11850/68337], Loss: 5.1352\n",
      "Epoch [2/10], Step [11925/68337], Loss: 5.3862\n",
      "Epoch [2/10], Step [12000/68337], Loss: 5.4466\n",
      "Epoch [2/10], Step [12075/68337], Loss: 5.1115\n",
      "Epoch [2/10], Step [12150/68337], Loss: 5.2244\n",
      "Epoch [2/10], Step [12225/68337], Loss: 5.5637\n",
      "Epoch [2/10], Step [12300/68337], Loss: 5.0804\n",
      "Epoch [2/10], Step [12375/68337], Loss: 5.2210\n",
      "Epoch [2/10], Step [12450/68337], Loss: 5.2084\n",
      "Epoch [2/10], Step [12525/68337], Loss: 5.0825\n",
      "Epoch [2/10], Step [12600/68337], Loss: 5.3327\n",
      "Epoch [2/10], Step [12675/68337], Loss: 5.4058\n",
      "Epoch [2/10], Step [12750/68337], Loss: 5.4176\n",
      "Epoch [2/10], Step [12825/68337], Loss: 5.3576\n",
      "Epoch [2/10], Step [12900/68337], Loss: 5.1426\n",
      "Epoch [2/10], Step [12975/68337], Loss: 5.2978\n",
      "Epoch [2/10], Step [13050/68337], Loss: 5.1413\n",
      "Epoch [2/10], Step [13125/68337], Loss: 5.3964\n",
      "Epoch [2/10], Step [13200/68337], Loss: 5.0326\n",
      "Epoch [2/10], Step [13275/68337], Loss: 5.2261\n",
      "Epoch [2/10], Step [13350/68337], Loss: 5.2138\n",
      "Epoch [2/10], Step [13425/68337], Loss: 5.3657\n",
      "Epoch [2/10], Step [13500/68337], Loss: 5.3545\n",
      "Epoch [2/10], Step [13575/68337], Loss: 5.2467\n",
      "Epoch [2/10], Step [13650/68337], Loss: 5.3292\n",
      "Epoch [2/10], Step [13725/68337], Loss: 5.2349\n",
      "Epoch [2/10], Step [13800/68337], Loss: 5.3513\n",
      "Epoch [2/10], Step [13875/68337], Loss: 5.1604\n",
      "Epoch [2/10], Step [13950/68337], Loss: 5.2253\n",
      "Epoch [2/10], Step [14025/68337], Loss: 5.2974\n",
      "Epoch [2/10], Step [14100/68337], Loss: 5.3362\n",
      "Epoch [2/10], Step [14175/68337], Loss: 5.2862\n",
      "Epoch [2/10], Step [14250/68337], Loss: 5.2791\n",
      "Epoch [2/10], Step [14325/68337], Loss: 5.4558\n",
      "Epoch [2/10], Step [14400/68337], Loss: 5.3437\n",
      "Epoch [2/10], Step [14475/68337], Loss: 5.3175\n",
      "Epoch [2/10], Step [14550/68337], Loss: 5.4045\n",
      "Epoch [2/10], Step [14625/68337], Loss: 5.4174\n",
      "Epoch [2/10], Step [14700/68337], Loss: 5.1902\n",
      "Epoch [2/10], Step [14775/68337], Loss: 5.1430\n",
      "Epoch [2/10], Step [14850/68337], Loss: 5.1248\n",
      "Epoch [2/10], Step [14925/68337], Loss: 5.4374\n",
      "Epoch [2/10], Step [15000/68337], Loss: 5.3603\n",
      "Epoch [2/10], Step [15075/68337], Loss: 5.1695\n",
      "Epoch [2/10], Step [15150/68337], Loss: 5.2855\n",
      "Epoch [2/10], Step [15225/68337], Loss: 5.3094\n",
      "Epoch [2/10], Step [15300/68337], Loss: 5.3001\n",
      "Epoch [2/10], Step [15375/68337], Loss: 5.3297\n",
      "Epoch [2/10], Step [15450/68337], Loss: 5.1874\n",
      "Epoch [2/10], Step [15525/68337], Loss: 5.1677\n",
      "Epoch [2/10], Step [15600/68337], Loss: 5.0818\n",
      "Epoch [2/10], Step [15675/68337], Loss: 5.2501\n",
      "Epoch [2/10], Step [15750/68337], Loss: 5.1993\n",
      "Epoch [2/10], Step [15825/68337], Loss: 5.4316\n",
      "Epoch [2/10], Step [15900/68337], Loss: 5.2375\n",
      "Epoch [2/10], Step [15975/68337], Loss: 5.1003\n",
      "Epoch [2/10], Step [16050/68337], Loss: 5.2904\n",
      "Epoch [2/10], Step [16125/68337], Loss: 5.2242\n",
      "Epoch [2/10], Step [16200/68337], Loss: 5.2675\n",
      "Epoch [2/10], Step [16275/68337], Loss: 5.2165\n",
      "Epoch [2/10], Step [16350/68337], Loss: 5.3635\n",
      "Epoch [2/10], Step [16425/68337], Loss: 5.3557\n",
      "Epoch [2/10], Step [16500/68337], Loss: 5.2805\n",
      "Epoch [2/10], Step [16575/68337], Loss: 5.2272\n",
      "Epoch [2/10], Step [16650/68337], Loss: 5.0504\n",
      "Epoch [2/10], Step [16725/68337], Loss: 5.2513\n",
      "Epoch [2/10], Step [16800/68337], Loss: 5.6081\n",
      "Epoch [2/10], Step [16875/68337], Loss: 5.1598\n",
      "Epoch [2/10], Step [16950/68337], Loss: 5.1917\n",
      "Epoch [2/10], Step [17025/68337], Loss: 5.1422\n",
      "Epoch [2/10], Step [17100/68337], Loss: 5.1368\n",
      "Epoch [2/10], Step [17175/68337], Loss: 5.2376\n",
      "Epoch [2/10], Step [17250/68337], Loss: 5.4420\n",
      "Epoch [2/10], Step [17325/68337], Loss: 5.1811\n",
      "Epoch [2/10], Step [17400/68337], Loss: 5.1398\n",
      "Epoch [2/10], Step [17475/68337], Loss: 5.2851\n",
      "Epoch [2/10], Step [17550/68337], Loss: 5.3155\n",
      "Epoch [2/10], Step [17625/68337], Loss: 5.0939\n",
      "Epoch [2/10], Step [17700/68337], Loss: 5.0714\n",
      "Epoch [2/10], Step [17775/68337], Loss: 5.3554\n",
      "Epoch [2/10], Step [17850/68337], Loss: 5.3251\n",
      "Epoch [2/10], Step [17925/68337], Loss: 5.3672\n",
      "Epoch [2/10], Step [18000/68337], Loss: 5.2035\n",
      "Epoch [2/10], Step [18075/68337], Loss: 5.3484\n",
      "Epoch [2/10], Step [18150/68337], Loss: 5.3618\n",
      "Epoch [2/10], Step [18225/68337], Loss: 5.2489\n",
      "Epoch [2/10], Step [18300/68337], Loss: 5.3703\n",
      "Epoch [2/10], Step [18375/68337], Loss: 4.9933\n",
      "Epoch [2/10], Step [18450/68337], Loss: 5.4142\n",
      "Epoch [2/10], Step [18525/68337], Loss: 5.2919\n",
      "Epoch [2/10], Step [18600/68337], Loss: 5.1967\n",
      "Epoch [2/10], Step [18675/68337], Loss: 5.1464\n",
      "Epoch [2/10], Step [18750/68337], Loss: 5.3073\n",
      "Epoch [2/10], Step [18825/68337], Loss: 5.3220\n",
      "Epoch [2/10], Step [18900/68337], Loss: 5.1615\n",
      "Epoch [2/10], Step [18975/68337], Loss: 5.2484\n",
      "Epoch [2/10], Step [19050/68337], Loss: 5.1547\n",
      "Epoch [2/10], Step [19125/68337], Loss: 5.2370\n",
      "Epoch [2/10], Step [19200/68337], Loss: 5.2174\n",
      "Epoch [2/10], Step [19275/68337], Loss: 5.2869\n",
      "Epoch [2/10], Step [19350/68337], Loss: 5.3162\n",
      "Epoch [2/10], Step [19425/68337], Loss: 5.1795\n",
      "Epoch [2/10], Step [19500/68337], Loss: 5.2168\n",
      "Epoch [2/10], Step [19575/68337], Loss: 5.2501\n",
      "Epoch [2/10], Step [19650/68337], Loss: 5.0765\n",
      "Epoch [2/10], Step [19725/68337], Loss: 5.2785\n",
      "Epoch [2/10], Step [19800/68337], Loss: 5.2010\n",
      "Epoch [2/10], Step [19875/68337], Loss: 5.2254\n",
      "Epoch [2/10], Step [19950/68337], Loss: 5.2626\n",
      "Validation perplexity: 153.87188305109646\n",
      "Epoch [2/10], Step [20025/68337], Loss: 5.2172\n",
      "Epoch [2/10], Step [20100/68337], Loss: 5.1920\n",
      "Epoch [2/10], Step [20175/68337], Loss: 5.2591\n",
      "Epoch [2/10], Step [20250/68337], Loss: 5.2690\n",
      "Epoch [2/10], Step [20325/68337], Loss: 5.3102\n",
      "Epoch [2/10], Step [20400/68337], Loss: 5.1646\n",
      "Epoch [2/10], Step [20475/68337], Loss: 5.2641\n",
      "Epoch [2/10], Step [20550/68337], Loss: 5.2507\n",
      "Epoch [2/10], Step [20625/68337], Loss: 5.3629\n",
      "Epoch [2/10], Step [20700/68337], Loss: 5.3272\n",
      "Epoch [2/10], Step [20775/68337], Loss: 5.0133\n",
      "Epoch [2/10], Step [20850/68337], Loss: 5.1883\n",
      "Epoch [2/10], Step [20925/68337], Loss: 5.2395\n",
      "Epoch [2/10], Step [21000/68337], Loss: 5.3928\n",
      "Epoch [2/10], Step [21075/68337], Loss: 5.2251\n",
      "Epoch [2/10], Step [21150/68337], Loss: 5.0603\n",
      "Epoch [2/10], Step [21225/68337], Loss: 5.2796\n",
      "Epoch [2/10], Step [21300/68337], Loss: 5.2376\n",
      "Epoch [2/10], Step [21375/68337], Loss: 5.2898\n",
      "Epoch [2/10], Step [21450/68337], Loss: 5.3313\n",
      "Epoch [2/10], Step [21525/68337], Loss: 5.2437\n",
      "Epoch [2/10], Step [21600/68337], Loss: 5.4286\n",
      "Epoch [2/10], Step [21675/68337], Loss: 4.9632\n",
      "Epoch [2/10], Step [21750/68337], Loss: 5.0632\n",
      "Epoch [2/10], Step [21825/68337], Loss: 5.2783\n",
      "Epoch [2/10], Step [21900/68337], Loss: 5.2291\n",
      "Epoch [2/10], Step [21975/68337], Loss: 5.0460\n",
      "Epoch [2/10], Step [22050/68337], Loss: 5.2278\n",
      "Epoch [2/10], Step [22125/68337], Loss: 5.2541\n",
      "Epoch [2/10], Step [22200/68337], Loss: 4.9981\n",
      "Epoch [2/10], Step [22275/68337], Loss: 5.1586\n",
      "Epoch [2/10], Step [22350/68337], Loss: 5.2196\n",
      "Epoch [2/10], Step [22425/68337], Loss: 5.1653\n",
      "Epoch [2/10], Step [22500/68337], Loss: 5.2625\n",
      "Epoch [2/10], Step [22575/68337], Loss: 5.4179\n",
      "Epoch [2/10], Step [22650/68337], Loss: 5.1548\n",
      "Epoch [2/10], Step [22725/68337], Loss: 5.3045\n",
      "Epoch [2/10], Step [22800/68337], Loss: 5.4621\n",
      "Epoch [2/10], Step [22875/68337], Loss: 5.0956\n",
      "Epoch [2/10], Step [22950/68337], Loss: 5.3003\n",
      "Epoch [2/10], Step [23025/68337], Loss: 5.1526\n",
      "Epoch [2/10], Step [23100/68337], Loss: 5.4097\n",
      "Epoch [2/10], Step [23175/68337], Loss: 5.3038\n",
      "Epoch [2/10], Step [23250/68337], Loss: 5.2098\n",
      "Epoch [2/10], Step [23325/68337], Loss: 5.3656\n",
      "Epoch [2/10], Step [23400/68337], Loss: 5.3768\n",
      "Epoch [2/10], Step [23475/68337], Loss: 5.1649\n",
      "Epoch [2/10], Step [23550/68337], Loss: 5.2685\n",
      "Epoch [2/10], Step [23625/68337], Loss: 5.1386\n",
      "Epoch [2/10], Step [23700/68337], Loss: 5.1756\n",
      "Epoch [2/10], Step [23775/68337], Loss: 5.2751\n",
      "Epoch [2/10], Step [23850/68337], Loss: 5.1930\n",
      "Epoch [2/10], Step [23925/68337], Loss: 5.4989\n",
      "Epoch [2/10], Step [24000/68337], Loss: 5.1408\n",
      "Epoch [2/10], Step [24075/68337], Loss: 5.2635\n",
      "Epoch [2/10], Step [24150/68337], Loss: 5.3943\n",
      "Epoch [2/10], Step [24225/68337], Loss: 5.0517\n",
      "Epoch [2/10], Step [24300/68337], Loss: 5.4352\n",
      "Epoch [2/10], Step [24375/68337], Loss: 4.8885\n",
      "Epoch [2/10], Step [24450/68337], Loss: 5.3043\n",
      "Epoch [2/10], Step [24525/68337], Loss: 5.2827\n",
      "Epoch [2/10], Step [24600/68337], Loss: 5.1660\n",
      "Epoch [2/10], Step [24675/68337], Loss: 5.2776\n",
      "Epoch [2/10], Step [24750/68337], Loss: 5.2426\n",
      "Epoch [2/10], Step [24825/68337], Loss: 5.2345\n",
      "Epoch [2/10], Step [24900/68337], Loss: 5.2085\n",
      "Epoch [2/10], Step [24975/68337], Loss: 5.3614\n",
      "Epoch [2/10], Step [25050/68337], Loss: 5.2504\n",
      "Epoch [2/10], Step [25125/68337], Loss: 5.2552\n",
      "Epoch [2/10], Step [25200/68337], Loss: 4.9352\n",
      "Epoch [2/10], Step [25275/68337], Loss: 5.3044\n",
      "Epoch [2/10], Step [25350/68337], Loss: 4.8798\n",
      "Epoch [2/10], Step [25425/68337], Loss: 5.2938\n",
      "Epoch [2/10], Step [25500/68337], Loss: 5.4151\n",
      "Epoch [2/10], Step [25575/68337], Loss: 5.3953\n",
      "Epoch [2/10], Step [25650/68337], Loss: 5.4727\n",
      "Epoch [2/10], Step [25725/68337], Loss: 5.2679\n",
      "Epoch [2/10], Step [25800/68337], Loss: 5.2306\n",
      "Epoch [2/10], Step [25875/68337], Loss: 5.2463\n",
      "Epoch [2/10], Step [25950/68337], Loss: 5.2761\n",
      "Epoch [2/10], Step [26025/68337], Loss: 5.1460\n",
      "Epoch [2/10], Step [26100/68337], Loss: 5.1410\n",
      "Epoch [2/10], Step [26175/68337], Loss: 5.2898\n",
      "Epoch [2/10], Step [26250/68337], Loss: 5.3162\n",
      "Epoch [2/10], Step [26325/68337], Loss: 5.2593\n",
      "Epoch [2/10], Step [26400/68337], Loss: 5.4036\n",
      "Epoch [2/10], Step [26475/68337], Loss: 5.2697\n",
      "Epoch [2/10], Step [26550/68337], Loss: 5.1350\n",
      "Epoch [2/10], Step [26625/68337], Loss: 5.3947\n",
      "Epoch [2/10], Step [26700/68337], Loss: 5.4132\n",
      "Epoch [2/10], Step [26775/68337], Loss: 5.2246\n",
      "Epoch [2/10], Step [26850/68337], Loss: 5.2260\n",
      "Epoch [2/10], Step [26925/68337], Loss: 5.1407\n",
      "Epoch [2/10], Step [27000/68337], Loss: 5.2075\n",
      "Epoch [2/10], Step [27075/68337], Loss: 5.2709\n",
      "Epoch [2/10], Step [27150/68337], Loss: 5.0997\n",
      "Epoch [2/10], Step [27225/68337], Loss: 5.5405\n",
      "Epoch [2/10], Step [27300/68337], Loss: 5.2871\n",
      "Epoch [2/10], Step [27375/68337], Loss: 5.1434\n",
      "Epoch [2/10], Step [27450/68337], Loss: 5.3820\n",
      "Epoch [2/10], Step [27525/68337], Loss: 5.3872\n",
      "Epoch [2/10], Step [27600/68337], Loss: 5.1918\n",
      "Epoch [2/10], Step [27675/68337], Loss: 5.1392\n",
      "Epoch [2/10], Step [27750/68337], Loss: 5.1503\n",
      "Epoch [2/10], Step [27825/68337], Loss: 5.2052\n",
      "Epoch [2/10], Step [27900/68337], Loss: 5.2125\n",
      "Epoch [2/10], Step [27975/68337], Loss: 5.2071\n",
      "Epoch [2/10], Step [28050/68337], Loss: 5.4166\n",
      "Epoch [2/10], Step [28125/68337], Loss: 5.5722\n",
      "Epoch [2/10], Step [28200/68337], Loss: 5.1976\n",
      "Epoch [2/10], Step [28275/68337], Loss: 5.2358\n",
      "Epoch [2/10], Step [28350/68337], Loss: 5.1739\n",
      "Epoch [2/10], Step [28425/68337], Loss: 5.4266\n",
      "Epoch [2/10], Step [28500/68337], Loss: 5.3389\n",
      "Epoch [2/10], Step [28575/68337], Loss: 5.3749\n",
      "Epoch [2/10], Step [28650/68337], Loss: 5.1375\n",
      "Epoch [2/10], Step [28725/68337], Loss: 5.2982\n",
      "Epoch [2/10], Step [28800/68337], Loss: 5.1296\n",
      "Epoch [2/10], Step [28875/68337], Loss: 5.3133\n",
      "Epoch [2/10], Step [28950/68337], Loss: 5.4996\n",
      "Epoch [2/10], Step [29025/68337], Loss: 5.2004\n",
      "Epoch [2/10], Step [29100/68337], Loss: 5.3291\n",
      "Epoch [2/10], Step [29175/68337], Loss: 5.0804\n",
      "Epoch [2/10], Step [29250/68337], Loss: 5.2871\n",
      "Epoch [2/10], Step [29325/68337], Loss: 5.0326\n",
      "Epoch [2/10], Step [29400/68337], Loss: 5.3055\n",
      "Epoch [2/10], Step [29475/68337], Loss: 5.3515\n",
      "Epoch [2/10], Step [29550/68337], Loss: 5.4215\n",
      "Epoch [2/10], Step [29625/68337], Loss: 5.2825\n",
      "Epoch [2/10], Step [29700/68337], Loss: 5.3117\n",
      "Epoch [2/10], Step [29775/68337], Loss: 5.1804\n",
      "Epoch [2/10], Step [29850/68337], Loss: 5.2170\n",
      "Epoch [2/10], Step [29925/68337], Loss: 5.1769\n",
      "Epoch [2/10], Step [30000/68337], Loss: 5.1163\n",
      "Validation perplexity: 150.93030651406798\n",
      "Epoch [2/10], Step [30075/68337], Loss: 5.1213\n",
      "Epoch [2/10], Step [30150/68337], Loss: 5.3656\n",
      "Epoch [2/10], Step [30225/68337], Loss: 5.2627\n",
      "Epoch [2/10], Step [30300/68337], Loss: 5.3384\n",
      "Epoch [2/10], Step [30375/68337], Loss: 5.2838\n",
      "Epoch [2/10], Step [30450/68337], Loss: 5.4562\n",
      "Epoch [2/10], Step [30525/68337], Loss: 5.1887\n",
      "Epoch [2/10], Step [30600/68337], Loss: 5.2511\n",
      "Epoch [2/10], Step [30675/68337], Loss: 5.2340\n",
      "Epoch [2/10], Step [30750/68337], Loss: 5.0465\n",
      "Epoch [2/10], Step [30825/68337], Loss: 5.2970\n",
      "Epoch [2/10], Step [30900/68337], Loss: 5.3521\n",
      "Epoch [2/10], Step [30975/68337], Loss: 5.2337\n",
      "Epoch [2/10], Step [31050/68337], Loss: 5.0957\n",
      "Epoch [2/10], Step [31125/68337], Loss: 5.2599\n",
      "Epoch [2/10], Step [31200/68337], Loss: 5.0672\n",
      "Epoch [2/10], Step [31275/68337], Loss: 4.9984\n",
      "Epoch [2/10], Step [31350/68337], Loss: 5.3290\n",
      "Epoch [2/10], Step [31425/68337], Loss: 5.2401\n",
      "Epoch [2/10], Step [31500/68337], Loss: 5.2565\n",
      "Epoch [2/10], Step [31575/68337], Loss: 5.1962\n",
      "Epoch [2/10], Step [31650/68337], Loss: 5.2398\n",
      "Epoch [2/10], Step [31725/68337], Loss: 5.2894\n",
      "Epoch [2/10], Step [31800/68337], Loss: 5.1439\n",
      "Epoch [2/10], Step [31875/68337], Loss: 4.9598\n",
      "Epoch [2/10], Step [31950/68337], Loss: 5.2064\n",
      "Epoch [2/10], Step [32025/68337], Loss: 5.1887\n",
      "Epoch [2/10], Step [32100/68337], Loss: 5.1826\n",
      "Epoch [2/10], Step [32175/68337], Loss: 5.0594\n",
      "Epoch [2/10], Step [32250/68337], Loss: 5.1568\n",
      "Epoch [2/10], Step [32325/68337], Loss: 5.3374\n",
      "Epoch [2/10], Step [32400/68337], Loss: 5.2242\n",
      "Epoch [2/10], Step [32475/68337], Loss: 5.2456\n",
      "Epoch [2/10], Step [32550/68337], Loss: 5.2804\n",
      "Epoch [2/10], Step [32625/68337], Loss: 5.0928\n",
      "Epoch [2/10], Step [32700/68337], Loss: 5.2105\n",
      "Epoch [2/10], Step [32775/68337], Loss: 5.1884\n",
      "Epoch [2/10], Step [32850/68337], Loss: 5.2007\n",
      "Epoch [2/10], Step [32925/68337], Loss: 5.1152\n",
      "Epoch [2/10], Step [33000/68337], Loss: 5.3138\n",
      "Epoch [2/10], Step [33075/68337], Loss: 5.3061\n",
      "Epoch [2/10], Step [33150/68337], Loss: 5.1163\n",
      "Epoch [2/10], Step [33225/68337], Loss: 5.1788\n",
      "Epoch [2/10], Step [33300/68337], Loss: 5.1783\n",
      "Epoch [2/10], Step [33375/68337], Loss: 5.1006\n",
      "Epoch [2/10], Step [33450/68337], Loss: 5.2888\n",
      "Epoch [2/10], Step [33525/68337], Loss: 5.1882\n",
      "Epoch [2/10], Step [33600/68337], Loss: 5.3054\n",
      "Epoch [2/10], Step [33675/68337], Loss: 5.3285\n",
      "Epoch [2/10], Step [33750/68337], Loss: 5.2049\n",
      "Epoch [2/10], Step [33825/68337], Loss: 5.2963\n",
      "Epoch [2/10], Step [33900/68337], Loss: 5.2889\n",
      "Epoch [2/10], Step [33975/68337], Loss: 5.2011\n",
      "Epoch [2/10], Step [34050/68337], Loss: 5.3185\n",
      "Epoch [2/10], Step [34125/68337], Loss: 5.3753\n",
      "Epoch [2/10], Step [34200/68337], Loss: 5.1457\n",
      "Epoch [2/10], Step [34275/68337], Loss: 5.0534\n",
      "Epoch [2/10], Step [34350/68337], Loss: 5.3993\n",
      "Epoch [2/10], Step [34425/68337], Loss: 5.4356\n",
      "Epoch [2/10], Step [34500/68337], Loss: 5.2570\n",
      "Epoch [2/10], Step [34575/68337], Loss: 5.4323\n",
      "Epoch [2/10], Step [34650/68337], Loss: 5.0043\n",
      "Epoch [2/10], Step [34725/68337], Loss: 5.0701\n",
      "Epoch [2/10], Step [34800/68337], Loss: 5.2329\n",
      "Epoch [2/10], Step [34875/68337], Loss: 5.3032\n",
      "Epoch [2/10], Step [34950/68337], Loss: 5.3707\n",
      "Epoch [2/10], Step [35025/68337], Loss: 5.2534\n",
      "Epoch [2/10], Step [35100/68337], Loss: 5.2999\n",
      "Epoch [2/10], Step [35175/68337], Loss: 5.3692\n",
      "Epoch [2/10], Step [35250/68337], Loss: 5.2335\n",
      "Epoch [2/10], Step [35325/68337], Loss: 5.3135\n",
      "Epoch [2/10], Step [35400/68337], Loss: 5.0170\n",
      "Epoch [2/10], Step [35475/68337], Loss: 5.3075\n",
      "Epoch [2/10], Step [35550/68337], Loss: 5.2503\n",
      "Epoch [2/10], Step [35625/68337], Loss: 4.9419\n",
      "Epoch [2/10], Step [35700/68337], Loss: 5.2553\n",
      "Epoch [2/10], Step [35775/68337], Loss: 5.3173\n",
      "Epoch [2/10], Step [35850/68337], Loss: 5.2111\n",
      "Epoch [2/10], Step [35925/68337], Loss: 5.2822\n",
      "Epoch [2/10], Step [36000/68337], Loss: 5.2062\n",
      "Epoch [2/10], Step [36075/68337], Loss: 5.3840\n",
      "Epoch [2/10], Step [36150/68337], Loss: 5.3857\n",
      "Epoch [2/10], Step [36225/68337], Loss: 5.3543\n",
      "Epoch [2/10], Step [36300/68337], Loss: 5.1246\n",
      "Epoch [2/10], Step [36375/68337], Loss: 5.4048\n",
      "Epoch [2/10], Step [36450/68337], Loss: 5.1603\n",
      "Epoch [2/10], Step [36525/68337], Loss: 5.3324\n",
      "Epoch [2/10], Step [36600/68337], Loss: 4.9954\n",
      "Epoch [2/10], Step [36675/68337], Loss: 4.9966\n",
      "Epoch [2/10], Step [36750/68337], Loss: 5.2280\n",
      "Epoch [2/10], Step [36825/68337], Loss: 5.4664\n",
      "Epoch [2/10], Step [36900/68337], Loss: 5.2052\n",
      "Epoch [2/10], Step [36975/68337], Loss: 5.3811\n",
      "Epoch [2/10], Step [37050/68337], Loss: 5.1828\n",
      "Epoch [2/10], Step [37125/68337], Loss: 5.2239\n",
      "Epoch [2/10], Step [37200/68337], Loss: 5.3333\n",
      "Epoch [2/10], Step [37275/68337], Loss: 5.1522\n",
      "Epoch [2/10], Step [37350/68337], Loss: 5.2069\n",
      "Epoch [2/10], Step [37425/68337], Loss: 5.0460\n",
      "Epoch [2/10], Step [37500/68337], Loss: 5.0671\n",
      "Epoch [2/10], Step [37575/68337], Loss: 5.4654\n",
      "Epoch [2/10], Step [37650/68337], Loss: 5.3742\n",
      "Epoch [2/10], Step [37725/68337], Loss: 5.2506\n",
      "Epoch [2/10], Step [37800/68337], Loss: 5.1670\n",
      "Epoch [2/10], Step [37875/68337], Loss: 5.2461\n",
      "Epoch [2/10], Step [37950/68337], Loss: 5.2303\n",
      "Epoch [2/10], Step [38025/68337], Loss: 5.3610\n",
      "Epoch [2/10], Step [38100/68337], Loss: 5.2870\n",
      "Epoch [2/10], Step [38175/68337], Loss: 5.0867\n",
      "Epoch [2/10], Step [38250/68337], Loss: 5.1960\n",
      "Epoch [2/10], Step [38325/68337], Loss: 5.4160\n",
      "Epoch [2/10], Step [38400/68337], Loss: 5.2104\n",
      "Epoch [2/10], Step [38475/68337], Loss: 5.2419\n",
      "Epoch [2/10], Step [38550/68337], Loss: 5.1264\n",
      "Epoch [2/10], Step [38625/68337], Loss: 5.4044\n",
      "Epoch [2/10], Step [38700/68337], Loss: 5.3081\n",
      "Epoch [2/10], Step [38775/68337], Loss: 5.2652\n",
      "Epoch [2/10], Step [38850/68337], Loss: 5.0799\n",
      "Epoch [2/10], Step [38925/68337], Loss: 5.3414\n",
      "Epoch [2/10], Step [39000/68337], Loss: 5.4313\n",
      "Epoch [2/10], Step [39075/68337], Loss: 5.4412\n",
      "Epoch [2/10], Step [39150/68337], Loss: 5.1894\n",
      "Epoch [2/10], Step [39225/68337], Loss: 5.1093\n",
      "Epoch [2/10], Step [39300/68337], Loss: 5.2441\n",
      "Epoch [2/10], Step [39375/68337], Loss: 5.4007\n",
      "Epoch [2/10], Step [39450/68337], Loss: 5.0973\n",
      "Epoch [2/10], Step [39525/68337], Loss: 5.3549\n",
      "Epoch [2/10], Step [39600/68337], Loss: 5.1100\n",
      "Epoch [2/10], Step [39675/68337], Loss: 5.3277\n",
      "Epoch [2/10], Step [39750/68337], Loss: 5.3625\n",
      "Epoch [2/10], Step [39825/68337], Loss: 5.1427\n",
      "Epoch [2/10], Step [39900/68337], Loss: 5.1018\n",
      "Epoch [2/10], Step [39975/68337], Loss: 5.2978\n",
      "Validation perplexity: 148.3523627312198\n",
      "Epoch [2/10], Step [40050/68337], Loss: 5.2892\n",
      "Epoch [2/10], Step [40125/68337], Loss: 5.1506\n",
      "Epoch [2/10], Step [40200/68337], Loss: 5.3185\n",
      "Epoch [2/10], Step [40275/68337], Loss: 5.1172\n",
      "Epoch [2/10], Step [40350/68337], Loss: 5.1785\n",
      "Epoch [2/10], Step [40425/68337], Loss: 5.2140\n",
      "Epoch [2/10], Step [40500/68337], Loss: 5.3570\n",
      "Epoch [2/10], Step [40575/68337], Loss: 5.2776\n",
      "Epoch [2/10], Step [40650/68337], Loss: 5.3779\n",
      "Epoch [2/10], Step [40725/68337], Loss: 5.2752\n",
      "Epoch [2/10], Step [40800/68337], Loss: 5.1869\n",
      "Epoch [2/10], Step [40875/68337], Loss: 5.2832\n",
      "Epoch [2/10], Step [40950/68337], Loss: 5.3813\n",
      "Epoch [2/10], Step [41025/68337], Loss: 5.3024\n",
      "Epoch [2/10], Step [41100/68337], Loss: 5.3074\n",
      "Epoch [2/10], Step [41175/68337], Loss: 5.3047\n",
      "Epoch [2/10], Step [41250/68337], Loss: 5.1234\n",
      "Epoch [2/10], Step [41325/68337], Loss: 5.3881\n",
      "Epoch [2/10], Step [41400/68337], Loss: 5.0659\n",
      "Epoch [2/10], Step [41475/68337], Loss: 5.1413\n",
      "Epoch [2/10], Step [41550/68337], Loss: 5.1710\n",
      "Epoch [2/10], Step [41625/68337], Loss: 5.0663\n",
      "Epoch [2/10], Step [41700/68337], Loss: 5.0545\n",
      "Epoch [2/10], Step [41775/68337], Loss: 5.4640\n",
      "Epoch [2/10], Step [41850/68337], Loss: 5.3884\n",
      "Epoch [2/10], Step [41925/68337], Loss: 5.0597\n",
      "Epoch [2/10], Step [42000/68337], Loss: 5.3608\n",
      "Epoch [2/10], Step [42075/68337], Loss: 5.1409\n",
      "Epoch [2/10], Step [42150/68337], Loss: 5.2393\n",
      "Epoch [2/10], Step [42225/68337], Loss: 5.2932\n",
      "Epoch [2/10], Step [42300/68337], Loss: 5.3152\n",
      "Epoch [2/10], Step [42375/68337], Loss: 5.1186\n",
      "Epoch [2/10], Step [42450/68337], Loss: 5.1126\n",
      "Epoch [2/10], Step [42525/68337], Loss: 5.1344\n",
      "Epoch [2/10], Step [42600/68337], Loss: 5.1136\n",
      "Epoch [2/10], Step [42675/68337], Loss: 5.2431\n",
      "Epoch [2/10], Step [42750/68337], Loss: 5.1412\n",
      "Epoch [2/10], Step [42825/68337], Loss: 5.1756\n",
      "Epoch [2/10], Step [42900/68337], Loss: 5.2635\n",
      "Epoch [2/10], Step [42975/68337], Loss: 5.2142\n",
      "Epoch [2/10], Step [43050/68337], Loss: 5.2274\n",
      "Epoch [2/10], Step [43125/68337], Loss: 5.2921\n",
      "Epoch [2/10], Step [43200/68337], Loss: 5.2266\n",
      "Epoch [2/10], Step [43275/68337], Loss: 5.2965\n",
      "Epoch [2/10], Step [43350/68337], Loss: 4.9767\n",
      "Epoch [2/10], Step [43425/68337], Loss: 5.1919\n",
      "Epoch [2/10], Step [43500/68337], Loss: 5.1310\n",
      "Epoch [2/10], Step [43575/68337], Loss: 5.2538\n",
      "Epoch [2/10], Step [43650/68337], Loss: 5.2302\n",
      "Epoch [2/10], Step [43725/68337], Loss: 5.2910\n",
      "Epoch [2/10], Step [43800/68337], Loss: 5.0392\n",
      "Epoch [2/10], Step [43875/68337], Loss: 5.3107\n",
      "Epoch [2/10], Step [43950/68337], Loss: 5.1749\n",
      "Epoch [2/10], Step [44025/68337], Loss: 5.2667\n",
      "Epoch [2/10], Step [44100/68337], Loss: 5.0788\n",
      "Epoch [2/10], Step [44175/68337], Loss: 5.4151\n",
      "Epoch [2/10], Step [44250/68337], Loss: 5.2893\n",
      "Epoch [2/10], Step [44325/68337], Loss: 5.1325\n",
      "Epoch [2/10], Step [44400/68337], Loss: 5.3152\n",
      "Epoch [2/10], Step [44475/68337], Loss: 5.2873\n",
      "Epoch [2/10], Step [44550/68337], Loss: 5.1426\n",
      "Epoch [2/10], Step [44625/68337], Loss: 5.4558\n",
      "Epoch [2/10], Step [44700/68337], Loss: 5.1021\n",
      "Epoch [2/10], Step [44775/68337], Loss: 5.1218\n",
      "Epoch [2/10], Step [44850/68337], Loss: 5.2826\n",
      "Epoch [2/10], Step [44925/68337], Loss: 5.2246\n",
      "Epoch [2/10], Step [45000/68337], Loss: 5.3447\n",
      "Epoch [2/10], Step [45075/68337], Loss: 5.4153\n",
      "Epoch [2/10], Step [45150/68337], Loss: 5.2951\n",
      "Epoch [2/10], Step [45225/68337], Loss: 4.9959\n",
      "Epoch [2/10], Step [45300/68337], Loss: 5.2783\n",
      "Epoch [2/10], Step [45375/68337], Loss: 5.1900\n",
      "Epoch [2/10], Step [45450/68337], Loss: 5.1616\n",
      "Epoch [2/10], Step [45525/68337], Loss: 5.1030\n",
      "Epoch [2/10], Step [45600/68337], Loss: 5.1813\n",
      "Epoch [2/10], Step [45675/68337], Loss: 5.3586\n",
      "Epoch [2/10], Step [45750/68337], Loss: 5.3619\n",
      "Epoch [2/10], Step [45825/68337], Loss: 5.0003\n",
      "Epoch [2/10], Step [45900/68337], Loss: 5.3446\n",
      "Epoch [2/10], Step [45975/68337], Loss: 5.0307\n",
      "Epoch [2/10], Step [46050/68337], Loss: 5.3540\n",
      "Epoch [2/10], Step [46125/68337], Loss: 5.3623\n",
      "Epoch [2/10], Step [46200/68337], Loss: 5.2863\n",
      "Epoch [2/10], Step [46275/68337], Loss: 5.1961\n",
      "Epoch [2/10], Step [46350/68337], Loss: 5.3590\n",
      "Epoch [2/10], Step [46425/68337], Loss: 5.3772\n",
      "Epoch [2/10], Step [46500/68337], Loss: 5.4838\n",
      "Epoch [2/10], Step [46575/68337], Loss: 5.3874\n",
      "Epoch [2/10], Step [46650/68337], Loss: 5.1871\n",
      "Epoch [2/10], Step [46725/68337], Loss: 5.5026\n",
      "Epoch [2/10], Step [46800/68337], Loss: 5.1545\n",
      "Epoch [2/10], Step [46875/68337], Loss: 5.1469\n",
      "Epoch [2/10], Step [46950/68337], Loss: 5.0811\n",
      "Epoch [2/10], Step [47025/68337], Loss: 5.2186\n",
      "Epoch [2/10], Step [47100/68337], Loss: 5.2654\n",
      "Epoch [2/10], Step [47175/68337], Loss: 5.1619\n",
      "Epoch [2/10], Step [47250/68337], Loss: 5.2859\n",
      "Epoch [2/10], Step [47325/68337], Loss: 5.1976\n",
      "Epoch [2/10], Step [47400/68337], Loss: 5.2241\n",
      "Epoch [2/10], Step [47475/68337], Loss: 5.1756\n",
      "Epoch [2/10], Step [47550/68337], Loss: 5.2042\n",
      "Epoch [2/10], Step [47625/68337], Loss: 5.1829\n",
      "Epoch [2/10], Step [47700/68337], Loss: 5.3628\n",
      "Epoch [2/10], Step [47775/68337], Loss: 5.2570\n",
      "Epoch [2/10], Step [47850/68337], Loss: 5.1323\n",
      "Epoch [2/10], Step [47925/68337], Loss: 5.3817\n",
      "Epoch [2/10], Step [48000/68337], Loss: 5.2896\n",
      "Epoch [2/10], Step [48075/68337], Loss: 5.0190\n",
      "Epoch [2/10], Step [48150/68337], Loss: 5.3610\n",
      "Epoch [2/10], Step [48225/68337], Loss: 4.9035\n",
      "Epoch [2/10], Step [48300/68337], Loss: 5.3453\n",
      "Epoch [2/10], Step [48375/68337], Loss: 5.2780\n",
      "Epoch [2/10], Step [48450/68337], Loss: 5.1587\n",
      "Epoch [2/10], Step [48525/68337], Loss: 4.9882\n",
      "Epoch [2/10], Step [48600/68337], Loss: 5.1680\n",
      "Epoch [2/10], Step [48675/68337], Loss: 5.2449\n",
      "Epoch [2/10], Step [48750/68337], Loss: 4.9868\n",
      "Epoch [2/10], Step [48825/68337], Loss: 5.0688\n",
      "Epoch [2/10], Step [48900/68337], Loss: 5.2662\n",
      "Epoch [2/10], Step [48975/68337], Loss: 5.2003\n",
      "Epoch [2/10], Step [49050/68337], Loss: 5.1963\n",
      "Epoch [2/10], Step [49125/68337], Loss: 5.3022\n",
      "Epoch [2/10], Step [49200/68337], Loss: 5.2679\n",
      "Epoch [2/10], Step [49275/68337], Loss: 5.1633\n",
      "Epoch [2/10], Step [49350/68337], Loss: 5.0251\n",
      "Epoch [2/10], Step [49425/68337], Loss: 5.0659\n",
      "Epoch [2/10], Step [49500/68337], Loss: 5.1647\n",
      "Epoch [2/10], Step [49575/68337], Loss: 5.1484\n",
      "Epoch [2/10], Step [49650/68337], Loss: 5.1884\n",
      "Epoch [2/10], Step [49725/68337], Loss: 5.1360\n",
      "Epoch [2/10], Step [49800/68337], Loss: 5.1379\n",
      "Epoch [2/10], Step [49875/68337], Loss: 5.3786\n",
      "Epoch [2/10], Step [49950/68337], Loss: 5.2299\n",
      "Validation perplexity: 145.73222538893927\n",
      "Epoch [2/10], Step [50025/68337], Loss: 5.2396\n",
      "Epoch [2/10], Step [50100/68337], Loss: 5.2415\n",
      "Epoch [2/10], Step [50175/68337], Loss: 5.1606\n",
      "Epoch [2/10], Step [50250/68337], Loss: 5.1875\n",
      "Epoch [2/10], Step [50325/68337], Loss: 5.2153\n",
      "Epoch [2/10], Step [50400/68337], Loss: 5.1704\n",
      "Epoch [2/10], Step [50475/68337], Loss: 5.3296\n",
      "Epoch [2/10], Step [50550/68337], Loss: 5.1252\n",
      "Epoch [2/10], Step [50625/68337], Loss: 5.1203\n",
      "Epoch [2/10], Step [50700/68337], Loss: 5.1846\n",
      "Epoch [2/10], Step [50775/68337], Loss: 5.1941\n",
      "Epoch [2/10], Step [50850/68337], Loss: 5.2701\n",
      "Epoch [2/10], Step [50925/68337], Loss: 5.0903\n",
      "Epoch [2/10], Step [51000/68337], Loss: 5.0339\n",
      "Epoch [2/10], Step [51075/68337], Loss: 5.1173\n",
      "Epoch [2/10], Step [51150/68337], Loss: 5.3916\n",
      "Epoch [2/10], Step [51225/68337], Loss: 5.2783\n",
      "Epoch [2/10], Step [51300/68337], Loss: 5.1964\n",
      "Epoch [2/10], Step [51375/68337], Loss: 4.9809\n",
      "Epoch [2/10], Step [51450/68337], Loss: 5.3097\n",
      "Epoch [2/10], Step [51525/68337], Loss: 5.4124\n",
      "Epoch [2/10], Step [51600/68337], Loss: 5.3864\n",
      "Epoch [2/10], Step [51675/68337], Loss: 5.2771\n",
      "Epoch [2/10], Step [51750/68337], Loss: 5.3615\n",
      "Epoch [2/10], Step [51825/68337], Loss: 5.2689\n",
      "Epoch [2/10], Step [51900/68337], Loss: 5.2572\n",
      "Epoch [2/10], Step [51975/68337], Loss: 5.0989\n",
      "Epoch [2/10], Step [52050/68337], Loss: 5.2880\n",
      "Epoch [2/10], Step [52125/68337], Loss: 5.4181\n",
      "Epoch [2/10], Step [52200/68337], Loss: 5.4125\n",
      "Epoch [2/10], Step [52275/68337], Loss: 5.1557\n",
      "Epoch [2/10], Step [52350/68337], Loss: 5.2676\n",
      "Epoch [2/10], Step [52425/68337], Loss: 5.3102\n",
      "Epoch [2/10], Step [52500/68337], Loss: 5.3628\n",
      "Epoch [2/10], Step [52575/68337], Loss: 5.2162\n",
      "Epoch [2/10], Step [52650/68337], Loss: 5.2290\n",
      "Epoch [2/10], Step [52725/68337], Loss: 5.4317\n",
      "Epoch [2/10], Step [52800/68337], Loss: 5.1580\n",
      "Epoch [2/10], Step [52875/68337], Loss: 5.1534\n",
      "Epoch [2/10], Step [52950/68337], Loss: 5.1817\n",
      "Epoch [2/10], Step [53025/68337], Loss: 5.2476\n",
      "Epoch [2/10], Step [53100/68337], Loss: 5.3438\n",
      "Epoch [2/10], Step [53175/68337], Loss: 5.0138\n",
      "Epoch [2/10], Step [53250/68337], Loss: 5.1928\n",
      "Epoch [2/10], Step [53325/68337], Loss: 5.2101\n",
      "Epoch [2/10], Step [53400/68337], Loss: 5.3148\n",
      "Epoch [2/10], Step [53475/68337], Loss: 5.1473\n",
      "Epoch [2/10], Step [53550/68337], Loss: 5.3536\n",
      "Epoch [2/10], Step [53625/68337], Loss: 5.3130\n",
      "Epoch [2/10], Step [53700/68337], Loss: 5.1633\n",
      "Epoch [2/10], Step [53775/68337], Loss: 5.3580\n",
      "Epoch [2/10], Step [53850/68337], Loss: 5.1443\n",
      "Epoch [2/10], Step [53925/68337], Loss: 5.1132\n",
      "Epoch [2/10], Step [54000/68337], Loss: 5.0460\n",
      "Epoch [2/10], Step [54075/68337], Loss: 5.2725\n",
      "Epoch [2/10], Step [54150/68337], Loss: 5.2356\n",
      "Epoch [2/10], Step [54225/68337], Loss: 5.1630\n",
      "Epoch [2/10], Step [54300/68337], Loss: 5.0688\n",
      "Epoch [2/10], Step [54375/68337], Loss: 5.0216\n",
      "Epoch [2/10], Step [54450/68337], Loss: 5.2905\n",
      "Epoch [2/10], Step [54525/68337], Loss: 5.2919\n",
      "Epoch [2/10], Step [54600/68337], Loss: 5.1793\n",
      "Epoch [2/10], Step [54675/68337], Loss: 5.1908\n",
      "Epoch [2/10], Step [54750/68337], Loss: 5.3086\n",
      "Epoch [2/10], Step [54825/68337], Loss: 5.1525\n",
      "Epoch [2/10], Step [54900/68337], Loss: 5.0521\n",
      "Epoch [2/10], Step [54975/68337], Loss: 5.1806\n",
      "Epoch [2/10], Step [55050/68337], Loss: 5.1441\n",
      "Epoch [2/10], Step [55125/68337], Loss: 5.3978\n",
      "Epoch [2/10], Step [55200/68337], Loss: 5.2627\n",
      "Epoch [2/10], Step [55275/68337], Loss: 5.2673\n",
      "Epoch [2/10], Step [55350/68337], Loss: 5.1749\n",
      "Epoch [2/10], Step [55425/68337], Loss: 5.0126\n",
      "Epoch [2/10], Step [55500/68337], Loss: 5.1981\n",
      "Epoch [2/10], Step [55575/68337], Loss: 5.3356\n",
      "Epoch [2/10], Step [55650/68337], Loss: 5.1753\n",
      "Epoch [2/10], Step [55725/68337], Loss: 5.1252\n",
      "Epoch [2/10], Step [55800/68337], Loss: 5.1497\n",
      "Epoch [2/10], Step [55875/68337], Loss: 5.1837\n",
      "Epoch [2/10], Step [55950/68337], Loss: 5.1579\n",
      "Epoch [2/10], Step [56025/68337], Loss: 5.3699\n",
      "Epoch [2/10], Step [56100/68337], Loss: 5.0992\n",
      "Epoch [2/10], Step [56175/68337], Loss: 5.2032\n",
      "Epoch [2/10], Step [56250/68337], Loss: 5.2378\n",
      "Epoch [2/10], Step [56325/68337], Loss: 5.0967\n",
      "Epoch [2/10], Step [56400/68337], Loss: 5.3380\n",
      "Epoch [2/10], Step [56475/68337], Loss: 5.3470\n",
      "Epoch [2/10], Step [56550/68337], Loss: 5.4259\n",
      "Epoch [2/10], Step [56625/68337], Loss: 5.4312\n",
      "Epoch [2/10], Step [56700/68337], Loss: 5.3862\n",
      "Epoch [2/10], Step [56775/68337], Loss: 5.1915\n",
      "Epoch [2/10], Step [56850/68337], Loss: 5.1869\n",
      "Epoch [2/10], Step [56925/68337], Loss: 5.3156\n",
      "Epoch [2/10], Step [57000/68337], Loss: 5.2132\n",
      "Epoch [2/10], Step [57075/68337], Loss: 5.2720\n",
      "Epoch [2/10], Step [57150/68337], Loss: 5.2018\n",
      "Epoch [2/10], Step [57225/68337], Loss: 5.2396\n",
      "Epoch [2/10], Step [57300/68337], Loss: 5.2634\n",
      "Epoch [2/10], Step [57375/68337], Loss: 5.0758\n",
      "Epoch [2/10], Step [57450/68337], Loss: 5.2145\n",
      "Epoch [2/10], Step [57525/68337], Loss: 5.4463\n",
      "Epoch [2/10], Step [57600/68337], Loss: 5.1799\n",
      "Epoch [2/10], Step [57675/68337], Loss: 5.3246\n",
      "Epoch [2/10], Step [57750/68337], Loss: 5.1793\n",
      "Epoch [2/10], Step [57825/68337], Loss: 5.0947\n",
      "Epoch [2/10], Step [57900/68337], Loss: 5.1876\n",
      "Epoch [2/10], Step [57975/68337], Loss: 4.8556\n",
      "Epoch [2/10], Step [58050/68337], Loss: 5.3746\n",
      "Epoch [2/10], Step [58125/68337], Loss: 5.3390\n",
      "Epoch [2/10], Step [58200/68337], Loss: 5.2267\n",
      "Epoch [2/10], Step [58275/68337], Loss: 5.1580\n",
      "Epoch [2/10], Step [58350/68337], Loss: 5.2819\n",
      "Epoch [2/10], Step [58425/68337], Loss: 5.1432\n",
      "Epoch [2/10], Step [58500/68337], Loss: 5.0233\n",
      "Epoch [2/10], Step [58575/68337], Loss: 5.2935\n",
      "Epoch [2/10], Step [58650/68337], Loss: 5.2123\n",
      "Epoch [2/10], Step [58725/68337], Loss: 5.2932\n",
      "Epoch [2/10], Step [58800/68337], Loss: 5.3314\n",
      "Epoch [2/10], Step [58875/68337], Loss: 5.1725\n",
      "Epoch [2/10], Step [58950/68337], Loss: 5.1368\n",
      "Epoch [2/10], Step [59025/68337], Loss: 5.2300\n",
      "Epoch [2/10], Step [59100/68337], Loss: 5.3843\n",
      "Epoch [2/10], Step [59175/68337], Loss: 5.0220\n",
      "Epoch [2/10], Step [59250/68337], Loss: 5.2254\n",
      "Epoch [2/10], Step [59325/68337], Loss: 5.1788\n",
      "Epoch [2/10], Step [59400/68337], Loss: 5.2063\n",
      "Epoch [2/10], Step [59475/68337], Loss: 5.3063\n",
      "Epoch [2/10], Step [59550/68337], Loss: 5.0458\n",
      "Epoch [2/10], Step [59625/68337], Loss: 5.2240\n",
      "Epoch [2/10], Step [59700/68337], Loss: 5.2428\n",
      "Epoch [2/10], Step [59775/68337], Loss: 5.1617\n",
      "Epoch [2/10], Step [59850/68337], Loss: 5.1645\n",
      "Epoch [2/10], Step [59925/68337], Loss: 5.0018\n",
      "Epoch [2/10], Step [60000/68337], Loss: 5.0797\n",
      "Validation perplexity: 143.53698183339034\n",
      "Epoch [2/10], Step [60075/68337], Loss: 5.0838\n",
      "Epoch [2/10], Step [60150/68337], Loss: 5.2428\n",
      "Epoch [2/10], Step [60225/68337], Loss: 5.2112\n",
      "Epoch [2/10], Step [60300/68337], Loss: 5.1548\n",
      "Epoch [2/10], Step [60375/68337], Loss: 5.1280\n",
      "Epoch [2/10], Step [60450/68337], Loss: 5.2530\n",
      "Epoch [2/10], Step [60525/68337], Loss: 5.0714\n",
      "Epoch [2/10], Step [60600/68337], Loss: 5.2966\n",
      "Epoch [2/10], Step [60675/68337], Loss: 5.2341\n",
      "Epoch [2/10], Step [60750/68337], Loss: 5.0652\n",
      "Epoch [2/10], Step [60825/68337], Loss: 5.2412\n",
      "Epoch [2/10], Step [60900/68337], Loss: 5.4346\n",
      "Epoch [2/10], Step [60975/68337], Loss: 5.1006\n",
      "Epoch [2/10], Step [61050/68337], Loss: 5.2760\n",
      "Epoch [2/10], Step [61125/68337], Loss: 5.0022\n",
      "Epoch [2/10], Step [61200/68337], Loss: 5.3972\n",
      "Epoch [2/10], Step [61275/68337], Loss: 5.3638\n",
      "Epoch [2/10], Step [61350/68337], Loss: 5.1526\n",
      "Epoch [2/10], Step [61425/68337], Loss: 5.3295\n",
      "Epoch [2/10], Step [61500/68337], Loss: 5.2640\n",
      "Epoch [2/10], Step [61575/68337], Loss: 5.1449\n",
      "Epoch [2/10], Step [61650/68337], Loss: 5.0405\n",
      "Epoch [2/10], Step [61725/68337], Loss: 5.1927\n",
      "Epoch [2/10], Step [61800/68337], Loss: 5.1033\n",
      "Epoch [2/10], Step [61875/68337], Loss: 5.4055\n",
      "Epoch [2/10], Step [61950/68337], Loss: 5.3182\n",
      "Epoch [2/10], Step [62025/68337], Loss: 5.3653\n",
      "Epoch [2/10], Step [62100/68337], Loss: 5.2597\n",
      "Epoch [2/10], Step [62175/68337], Loss: 5.1687\n",
      "Epoch [2/10], Step [62250/68337], Loss: 5.1893\n",
      "Epoch [2/10], Step [62325/68337], Loss: 5.3106\n",
      "Epoch [2/10], Step [62400/68337], Loss: 5.0656\n",
      "Epoch [2/10], Step [62475/68337], Loss: 5.0441\n",
      "Epoch [2/10], Step [62550/68337], Loss: 5.1361\n",
      "Epoch [2/10], Step [62625/68337], Loss: 5.2912\n",
      "Epoch [2/10], Step [62700/68337], Loss: 5.1944\n",
      "Epoch [2/10], Step [62775/68337], Loss: 5.2395\n",
      "Epoch [2/10], Step [62850/68337], Loss: 5.1201\n",
      "Epoch [2/10], Step [62925/68337], Loss: 5.4154\n",
      "Epoch [2/10], Step [63000/68337], Loss: 5.2426\n",
      "Epoch [2/10], Step [63075/68337], Loss: 5.2256\n",
      "Epoch [2/10], Step [63150/68337], Loss: 5.0340\n",
      "Epoch [2/10], Step [63225/68337], Loss: 5.0014\n",
      "Epoch [2/10], Step [63300/68337], Loss: 5.2261\n",
      "Epoch [2/10], Step [63375/68337], Loss: 5.2610\n",
      "Epoch [2/10], Step [63450/68337], Loss: 5.0785\n",
      "Epoch [2/10], Step [63525/68337], Loss: 5.0124\n",
      "Epoch [2/10], Step [63600/68337], Loss: 5.1818\n",
      "Epoch [2/10], Step [63675/68337], Loss: 4.9401\n",
      "Epoch [2/10], Step [63750/68337], Loss: 5.1347\n",
      "Epoch [2/10], Step [63825/68337], Loss: 5.2741\n",
      "Epoch [2/10], Step [63900/68337], Loss: 5.0649\n",
      "Epoch [2/10], Step [63975/68337], Loss: 5.1772\n",
      "Epoch [2/10], Step [64050/68337], Loss: 5.3532\n",
      "Epoch [2/10], Step [64125/68337], Loss: 5.0899\n",
      "Epoch [2/10], Step [64200/68337], Loss: 5.0076\n",
      "Epoch [2/10], Step [64275/68337], Loss: 5.3158\n",
      "Epoch [2/10], Step [64350/68337], Loss: 5.0762\n",
      "Epoch [2/10], Step [64425/68337], Loss: 5.1958\n",
      "Epoch [2/10], Step [64500/68337], Loss: 5.2663\n",
      "Epoch [2/10], Step [64575/68337], Loss: 5.3010\n",
      "Epoch [2/10], Step [64650/68337], Loss: 5.1398\n",
      "Epoch [2/10], Step [64725/68337], Loss: 5.3198\n",
      "Epoch [2/10], Step [64800/68337], Loss: 5.2206\n",
      "Epoch [2/10], Step [64875/68337], Loss: 5.2477\n",
      "Epoch [2/10], Step [64950/68337], Loss: 5.2169\n",
      "Epoch [2/10], Step [65025/68337], Loss: 5.3829\n",
      "Epoch [2/10], Step [65100/68337], Loss: 5.1758\n",
      "Epoch [2/10], Step [65175/68337], Loss: 5.2756\n",
      "Epoch [2/10], Step [65250/68337], Loss: 5.3188\n",
      "Epoch [2/10], Step [65325/68337], Loss: 5.3121\n",
      "Epoch [2/10], Step [65400/68337], Loss: 5.2467\n",
      "Epoch [2/10], Step [65475/68337], Loss: 5.1584\n",
      "Epoch [2/10], Step [65550/68337], Loss: 5.1234\n",
      "Epoch [2/10], Step [65625/68337], Loss: 5.1152\n",
      "Epoch [2/10], Step [65700/68337], Loss: 5.0417\n",
      "Epoch [2/10], Step [65775/68337], Loss: 4.9680\n",
      "Epoch [2/10], Step [65850/68337], Loss: 5.2190\n",
      "Epoch [2/10], Step [65925/68337], Loss: 5.1767\n",
      "Epoch [2/10], Step [66000/68337], Loss: 5.0624\n",
      "Epoch [2/10], Step [66075/68337], Loss: 5.0462\n",
      "Epoch [2/10], Step [66150/68337], Loss: 5.0551\n",
      "Epoch [2/10], Step [66225/68337], Loss: 5.0917\n",
      "Epoch [2/10], Step [66300/68337], Loss: 4.9837\n",
      "Epoch [2/10], Step [66375/68337], Loss: 5.4511\n",
      "Epoch [2/10], Step [66450/68337], Loss: 4.9682\n",
      "Epoch [2/10], Step [66525/68337], Loss: 5.0487\n",
      "Epoch [2/10], Step [66600/68337], Loss: 5.1050\n",
      "Epoch [2/10], Step [66675/68337], Loss: 5.3739\n",
      "Epoch [2/10], Step [66750/68337], Loss: 5.0335\n",
      "Epoch [2/10], Step [66825/68337], Loss: 5.1483\n",
      "Epoch [2/10], Step [66900/68337], Loss: 5.4545\n",
      "Epoch [2/10], Step [66975/68337], Loss: 5.0750\n",
      "Epoch [2/10], Step [67050/68337], Loss: 5.0875\n",
      "Epoch [2/10], Step [67125/68337], Loss: 5.2130\n",
      "Epoch [2/10], Step [67200/68337], Loss: 5.4184\n",
      "Epoch [2/10], Step [67275/68337], Loss: 5.3540\n",
      "Epoch [2/10], Step [67350/68337], Loss: 5.0156\n",
      "Epoch [2/10], Step [67425/68337], Loss: 5.3019\n",
      "Epoch [2/10], Step [67500/68337], Loss: 4.9895\n",
      "Epoch [2/10], Step [67575/68337], Loss: 5.3321\n",
      "Epoch [2/10], Step [67650/68337], Loss: 5.1899\n",
      "Epoch [2/10], Step [67725/68337], Loss: 5.1774\n",
      "Epoch [2/10], Step [67800/68337], Loss: 5.2503\n",
      "Epoch [2/10], Step [67875/68337], Loss: 5.0728\n",
      "Epoch [2/10], Step [67950/68337], Loss: 5.2307\n",
      "Epoch [2/10], Step [68025/68337], Loss: 5.2151\n",
      "Epoch [2/10], Step [68100/68337], Loss: 5.1591\n",
      "Epoch [2/10], Step [68175/68337], Loss: 5.1803\n",
      "Epoch [2/10], Step [68250/68337], Loss: 5.0950\n",
      "Epoch [2/10], Step [68325/68337], Loss: 5.2220\n",
      "Epoch [2/10] Average Loss: 5.2428, Perplexity: 189.20\n",
      "Epoch [3/10], Step [0/68337], Loss: 4.9638\n",
      "Validation perplexity: 142.6374226149599\n",
      "Epoch [3/10], Step [75/68337], Loss: 5.2798\n",
      "Epoch [3/10], Step [150/68337], Loss: 5.3586\n",
      "Epoch [3/10], Step [225/68337], Loss: 5.2183\n",
      "Epoch [3/10], Step [300/68337], Loss: 5.3554\n",
      "Epoch [3/10], Step [375/68337], Loss: 5.2796\n",
      "Epoch [3/10], Step [450/68337], Loss: 5.1206\n",
      "Epoch [3/10], Step [525/68337], Loss: 5.2576\n",
      "Epoch [3/10], Step [600/68337], Loss: 5.1252\n",
      "Epoch [3/10], Step [675/68337], Loss: 5.1554\n",
      "Epoch [3/10], Step [750/68337], Loss: 5.1529\n",
      "Epoch [3/10], Step [825/68337], Loss: 5.2155\n",
      "Epoch [3/10], Step [900/68337], Loss: 5.1757\n",
      "Epoch [3/10], Step [975/68337], Loss: 5.2632\n",
      "Epoch [3/10], Step [1050/68337], Loss: 5.1166\n",
      "Epoch [3/10], Step [1125/68337], Loss: 5.0899\n",
      "Epoch [3/10], Step [1200/68337], Loss: 5.2937\n",
      "Epoch [3/10], Step [1275/68337], Loss: 5.1168\n",
      "Epoch [3/10], Step [1350/68337], Loss: 5.3040\n",
      "Epoch [3/10], Step [1425/68337], Loss: 5.2880\n",
      "Epoch [3/10], Step [1500/68337], Loss: 5.2484\n",
      "Epoch [3/10], Step [1575/68337], Loss: 4.9470\n",
      "Epoch [3/10], Step [1650/68337], Loss: 5.1375\n",
      "Epoch [3/10], Step [1725/68337], Loss: 5.2193\n",
      "Epoch [3/10], Step [1800/68337], Loss: 5.3393\n",
      "Epoch [3/10], Step [1875/68337], Loss: 4.9488\n",
      "Epoch [3/10], Step [1950/68337], Loss: 5.2438\n",
      "Epoch [3/10], Step [2025/68337], Loss: 5.1636\n",
      "Epoch [3/10], Step [2100/68337], Loss: 4.9833\n",
      "Epoch [3/10], Step [2175/68337], Loss: 5.0388\n",
      "Epoch [3/10], Step [2250/68337], Loss: 4.9356\n",
      "Epoch [3/10], Step [2325/68337], Loss: 5.0584\n",
      "Epoch [3/10], Step [2400/68337], Loss: 5.1659\n",
      "Epoch [3/10], Step [2475/68337], Loss: 5.1385\n",
      "Epoch [3/10], Step [2550/68337], Loss: 5.4613\n",
      "Epoch [3/10], Step [2625/68337], Loss: 5.1592\n",
      "Epoch [3/10], Step [2700/68337], Loss: 5.2051\n",
      "Epoch [3/10], Step [2775/68337], Loss: 5.1657\n",
      "Epoch [3/10], Step [2850/68337], Loss: 5.2427\n",
      "Epoch [3/10], Step [2925/68337], Loss: 5.2111\n",
      "Epoch [3/10], Step [3000/68337], Loss: 5.1963\n",
      "Epoch [3/10], Step [3075/68337], Loss: 5.2388\n",
      "Epoch [3/10], Step [3150/68337], Loss: 5.0891\n",
      "Epoch [3/10], Step [3225/68337], Loss: 5.2516\n",
      "Epoch [3/10], Step [3300/68337], Loss: 5.1911\n",
      "Epoch [3/10], Step [3375/68337], Loss: 4.9816\n",
      "Epoch [3/10], Step [3450/68337], Loss: 5.2335\n",
      "Epoch [3/10], Step [3525/68337], Loss: 5.1417\n",
      "Epoch [3/10], Step [3600/68337], Loss: 5.2467\n",
      "Epoch [3/10], Step [3675/68337], Loss: 5.1619\n",
      "Epoch [3/10], Step [3750/68337], Loss: 5.1379\n",
      "Epoch [3/10], Step [3825/68337], Loss: 5.4189\n",
      "Epoch [3/10], Step [3900/68337], Loss: 5.1919\n",
      "Epoch [3/10], Step [3975/68337], Loss: 5.1321\n",
      "Epoch [3/10], Step [4050/68337], Loss: 5.4234\n",
      "Epoch [3/10], Step [4125/68337], Loss: 5.3536\n",
      "Epoch [3/10], Step [4200/68337], Loss: 5.1698\n",
      "Epoch [3/10], Step [4275/68337], Loss: 5.1824\n",
      "Epoch [3/10], Step [4350/68337], Loss: 5.2304\n",
      "Epoch [3/10], Step [4425/68337], Loss: 5.1914\n",
      "Epoch [3/10], Step [4500/68337], Loss: 4.9993\n",
      "Epoch [3/10], Step [4575/68337], Loss: 5.1677\n",
      "Epoch [3/10], Step [4650/68337], Loss: 5.2528\n",
      "Epoch [3/10], Step [4725/68337], Loss: 5.1631\n",
      "Epoch [3/10], Step [4800/68337], Loss: 5.3853\n",
      "Epoch [3/10], Step [4875/68337], Loss: 5.1460\n",
      "Epoch [3/10], Step [4950/68337], Loss: 5.1718\n",
      "Epoch [3/10], Step [5025/68337], Loss: 4.8992\n",
      "Epoch [3/10], Step [5100/68337], Loss: 5.2791\n",
      "Epoch [3/10], Step [5175/68337], Loss: 5.1573\n",
      "Epoch [3/10], Step [5250/68337], Loss: 5.2488\n",
      "Epoch [3/10], Step [5325/68337], Loss: 5.1048\n",
      "Epoch [3/10], Step [5400/68337], Loss: 5.1634\n",
      "Epoch [3/10], Step [5475/68337], Loss: 5.0254\n",
      "Epoch [3/10], Step [5550/68337], Loss: 5.0483\n",
      "Epoch [3/10], Step [5625/68337], Loss: 5.3109\n",
      "Epoch [3/10], Step [5700/68337], Loss: 5.1029\n",
      "Epoch [3/10], Step [5775/68337], Loss: 5.1515\n",
      "Epoch [3/10], Step [5850/68337], Loss: 5.0795\n",
      "Epoch [3/10], Step [5925/68337], Loss: 5.0717\n",
      "Epoch [3/10], Step [6000/68337], Loss: 5.1870\n",
      "Epoch [3/10], Step [6075/68337], Loss: 5.2326\n",
      "Epoch [3/10], Step [6150/68337], Loss: 5.1224\n",
      "Epoch [3/10], Step [6225/68337], Loss: 5.0144\n",
      "Epoch [3/10], Step [6300/68337], Loss: 5.1296\n",
      "Epoch [3/10], Step [6375/68337], Loss: 5.2570\n",
      "Epoch [3/10], Step [6450/68337], Loss: 5.1640\n",
      "Epoch [3/10], Step [6525/68337], Loss: 5.1840\n",
      "Epoch [3/10], Step [6600/68337], Loss: 5.1772\n",
      "Epoch [3/10], Step [6675/68337], Loss: 5.2152\n",
      "Epoch [3/10], Step [6750/68337], Loss: 5.2786\n",
      "Epoch [3/10], Step [6825/68337], Loss: 5.1680\n",
      "Epoch [3/10], Step [6900/68337], Loss: 5.2545\n",
      "Epoch [3/10], Step [6975/68337], Loss: 5.0644\n",
      "Epoch [3/10], Step [7050/68337], Loss: 5.1515\n",
      "Epoch [3/10], Step [7125/68337], Loss: 5.2915\n",
      "Epoch [3/10], Step [7200/68337], Loss: 5.3094\n",
      "Epoch [3/10], Step [7275/68337], Loss: 5.1292\n",
      "Epoch [3/10], Step [7350/68337], Loss: 5.2975\n",
      "Epoch [3/10], Step [7425/68337], Loss: 4.9668\n",
      "Epoch [3/10], Step [7500/68337], Loss: 5.1920\n",
      "Epoch [3/10], Step [7575/68337], Loss: 5.1271\n",
      "Epoch [3/10], Step [7650/68337], Loss: 5.3497\n",
      "Epoch [3/10], Step [7725/68337], Loss: 5.3503\n",
      "Epoch [3/10], Step [7800/68337], Loss: 5.1626\n",
      "Epoch [3/10], Step [7875/68337], Loss: 5.2628\n",
      "Epoch [3/10], Step [7950/68337], Loss: 5.0430\n",
      "Epoch [3/10], Step [8025/68337], Loss: 5.2381\n",
      "Epoch [3/10], Step [8100/68337], Loss: 5.1181\n",
      "Epoch [3/10], Step [8175/68337], Loss: 5.1562\n",
      "Epoch [3/10], Step [8250/68337], Loss: 5.5350\n",
      "Epoch [3/10], Step [8325/68337], Loss: 5.2885\n",
      "Epoch [3/10], Step [8400/68337], Loss: 5.1078\n",
      "Epoch [3/10], Step [8475/68337], Loss: 5.1407\n",
      "Epoch [3/10], Step [8550/68337], Loss: 5.1217\n",
      "Epoch [3/10], Step [8625/68337], Loss: 5.2271\n",
      "Epoch [3/10], Step [8700/68337], Loss: 5.2389\n",
      "Epoch [3/10], Step [8775/68337], Loss: 5.1300\n",
      "Epoch [3/10], Step [8850/68337], Loss: 5.1875\n",
      "Epoch [3/10], Step [8925/68337], Loss: 5.3893\n",
      "Epoch [3/10], Step [9000/68337], Loss: 5.3338\n",
      "Epoch [3/10], Step [9075/68337], Loss: 5.2871\n",
      "Epoch [3/10], Step [9150/68337], Loss: 5.1540\n",
      "Epoch [3/10], Step [9225/68337], Loss: 5.1757\n",
      "Epoch [3/10], Step [9300/68337], Loss: 5.2190\n",
      "Epoch [3/10], Step [9375/68337], Loss: 5.2837\n",
      "Epoch [3/10], Step [9450/68337], Loss: 5.2383\n",
      "Epoch [3/10], Step [9525/68337], Loss: 5.1790\n",
      "Epoch [3/10], Step [9600/68337], Loss: 5.1874\n",
      "Epoch [3/10], Step [9675/68337], Loss: 5.2570\n",
      "Epoch [3/10], Step [9750/68337], Loss: 5.2425\n",
      "Epoch [3/10], Step [9825/68337], Loss: 5.0838\n",
      "Epoch [3/10], Step [9900/68337], Loss: 5.2609\n",
      "Epoch [3/10], Step [9975/68337], Loss: 5.2197\n",
      "Validation perplexity: 140.34708638595626\n",
      "Epoch [3/10], Step [10050/68337], Loss: 5.1110\n",
      "Epoch [3/10], Step [10125/68337], Loss: 5.0788\n",
      "Epoch [3/10], Step [10200/68337], Loss: 5.1257\n",
      "Epoch [3/10], Step [10275/68337], Loss: 5.1018\n",
      "Epoch [3/10], Step [10350/68337], Loss: 5.3337\n",
      "Epoch [3/10], Step [10425/68337], Loss: 5.2081\n",
      "Epoch [3/10], Step [10500/68337], Loss: 5.1442\n",
      "Epoch [3/10], Step [10575/68337], Loss: 5.2956\n",
      "Epoch [3/10], Step [10650/68337], Loss: 5.4061\n",
      "Epoch [3/10], Step [10725/68337], Loss: 5.3150\n",
      "Epoch [3/10], Step [10800/68337], Loss: 5.4231\n",
      "Epoch [3/10], Step [10875/68337], Loss: 5.1934\n",
      "Epoch [3/10], Step [10950/68337], Loss: 5.3119\n",
      "Epoch [3/10], Step [11025/68337], Loss: 5.2316\n",
      "Epoch [3/10], Step [11100/68337], Loss: 5.0955\n",
      "Epoch [3/10], Step [11175/68337], Loss: 4.9880\n",
      "Epoch [3/10], Step [11250/68337], Loss: 5.1782\n",
      "Epoch [3/10], Step [11325/68337], Loss: 5.2588\n",
      "Epoch [3/10], Step [11400/68337], Loss: 5.2224\n",
      "Epoch [3/10], Step [11475/68337], Loss: 5.3104\n",
      "Epoch [3/10], Step [11550/68337], Loss: 5.1310\n",
      "Epoch [3/10], Step [11625/68337], Loss: 5.0450\n",
      "Epoch [3/10], Step [11700/68337], Loss: 5.2462\n",
      "Epoch [3/10], Step [11775/68337], Loss: 5.3377\n",
      "Epoch [3/10], Step [11850/68337], Loss: 5.2108\n",
      "Epoch [3/10], Step [11925/68337], Loss: 5.1237\n",
      "Epoch [3/10], Step [12000/68337], Loss: 5.1891\n",
      "Epoch [3/10], Step [12075/68337], Loss: 5.2536\n",
      "Epoch [3/10], Step [12150/68337], Loss: 5.4437\n",
      "Epoch [3/10], Step [12225/68337], Loss: 5.2208\n",
      "Epoch [3/10], Step [12300/68337], Loss: 5.2883\n",
      "Epoch [3/10], Step [12375/68337], Loss: 5.1415\n",
      "Epoch [3/10], Step [12450/68337], Loss: 5.1071\n",
      "Epoch [3/10], Step [12525/68337], Loss: 5.3243\n",
      "Epoch [3/10], Step [12600/68337], Loss: 5.1107\n",
      "Epoch [3/10], Step [12675/68337], Loss: 5.3392\n",
      "Epoch [3/10], Step [12750/68337], Loss: 5.0265\n",
      "Epoch [3/10], Step [12825/68337], Loss: 5.1376\n",
      "Epoch [3/10], Step [12900/68337], Loss: 5.1641\n",
      "Epoch [3/10], Step [12975/68337], Loss: 5.2480\n",
      "Epoch [3/10], Step [13050/68337], Loss: 5.4082\n",
      "Epoch [3/10], Step [13125/68337], Loss: 4.9085\n",
      "Epoch [3/10], Step [13200/68337], Loss: 5.4739\n",
      "Epoch [3/10], Step [13275/68337], Loss: 5.2745\n",
      "Epoch [3/10], Step [13350/68337], Loss: 5.3100\n",
      "Epoch [3/10], Step [13425/68337], Loss: 5.2961\n",
      "Epoch [3/10], Step [13500/68337], Loss: 5.0170\n",
      "Epoch [3/10], Step [13575/68337], Loss: 4.8367\n",
      "Epoch [3/10], Step [13650/68337], Loss: 5.0989\n",
      "Epoch [3/10], Step [13725/68337], Loss: 5.2801\n",
      "Epoch [3/10], Step [13800/68337], Loss: 5.3371\n",
      "Epoch [3/10], Step [13875/68337], Loss: 5.3291\n",
      "Epoch [3/10], Step [13950/68337], Loss: 5.3302\n",
      "Epoch [3/10], Step [14025/68337], Loss: 5.2481\n",
      "Epoch [3/10], Step [14100/68337], Loss: 5.1499\n",
      "Epoch [3/10], Step [14175/68337], Loss: 5.0695\n",
      "Epoch [3/10], Step [14250/68337], Loss: 5.1618\n",
      "Epoch [3/10], Step [14325/68337], Loss: 5.3903\n",
      "Epoch [3/10], Step [14400/68337], Loss: 5.1797\n",
      "Epoch [3/10], Step [14475/68337], Loss: 5.2104\n",
      "Epoch [3/10], Step [14550/68337], Loss: 5.5064\n",
      "Epoch [3/10], Step [14625/68337], Loss: 5.1965\n",
      "Epoch [3/10], Step [14700/68337], Loss: 5.0811\n",
      "Epoch [3/10], Step [14775/68337], Loss: 5.2433\n",
      "Epoch [3/10], Step [14850/68337], Loss: 5.0637\n",
      "Epoch [3/10], Step [14925/68337], Loss: 4.9683\n",
      "Epoch [3/10], Step [15000/68337], Loss: 5.0789\n",
      "Epoch [3/10], Step [15075/68337], Loss: 5.1686\n",
      "Epoch [3/10], Step [15150/68337], Loss: 5.1797\n",
      "Epoch [3/10], Step [15225/68337], Loss: 5.4127\n",
      "Epoch [3/10], Step [15300/68337], Loss: 5.3204\n",
      "Epoch [3/10], Step [15375/68337], Loss: 5.1863\n",
      "Epoch [3/10], Step [15450/68337], Loss: 5.2666\n",
      "Epoch [3/10], Step [15525/68337], Loss: 5.0940\n",
      "Epoch [3/10], Step [15600/68337], Loss: 5.1012\n",
      "Epoch [3/10], Step [15675/68337], Loss: 5.1375\n",
      "Epoch [3/10], Step [15750/68337], Loss: 5.2655\n",
      "Epoch [3/10], Step [15825/68337], Loss: 5.2702\n",
      "Epoch [3/10], Step [15900/68337], Loss: 5.2234\n",
      "Epoch [3/10], Step [15975/68337], Loss: 5.0638\n",
      "Epoch [3/10], Step [16050/68337], Loss: 5.0857\n",
      "Epoch [3/10], Step [16125/68337], Loss: 5.1581\n",
      "Epoch [3/10], Step [16200/68337], Loss: 5.1384\n",
      "Epoch [3/10], Step [16275/68337], Loss: 4.9613\n",
      "Epoch [3/10], Step [16350/68337], Loss: 5.3205\n",
      "Epoch [3/10], Step [16425/68337], Loss: 5.0925\n",
      "Epoch [3/10], Step [16500/68337], Loss: 5.1067\n",
      "Epoch [3/10], Step [16575/68337], Loss: 5.1383\n",
      "Epoch [3/10], Step [16650/68337], Loss: 5.2938\n",
      "Epoch [3/10], Step [16725/68337], Loss: 5.2719\n",
      "Epoch [3/10], Step [16800/68337], Loss: 5.3334\n",
      "Epoch [3/10], Step [16875/68337], Loss: 5.4898\n",
      "Epoch [3/10], Step [16950/68337], Loss: 5.1597\n",
      "Epoch [3/10], Step [17025/68337], Loss: 5.2078\n",
      "Epoch [3/10], Step [17100/68337], Loss: 5.0290\n",
      "Epoch [3/10], Step [17175/68337], Loss: 5.2468\n",
      "Epoch [3/10], Step [17250/68337], Loss: 5.1137\n",
      "Epoch [3/10], Step [17325/68337], Loss: 5.3516\n",
      "Epoch [3/10], Step [17400/68337], Loss: 5.1892\n",
      "Epoch [3/10], Step [17475/68337], Loss: 5.3109\n",
      "Epoch [3/10], Step [17550/68337], Loss: 4.9992\n",
      "Epoch [3/10], Step [17625/68337], Loss: 5.2035\n",
      "Epoch [3/10], Step [17700/68337], Loss: 5.1564\n",
      "Epoch [3/10], Step [17775/68337], Loss: 5.1333\n",
      "Epoch [3/10], Step [17850/68337], Loss: 4.9278\n",
      "Epoch [3/10], Step [17925/68337], Loss: 5.1537\n",
      "Epoch [3/10], Step [18000/68337], Loss: 5.2486\n",
      "Epoch [3/10], Step [18075/68337], Loss: 5.0841\n",
      "Epoch [3/10], Step [18150/68337], Loss: 5.2267\n",
      "Epoch [3/10], Step [18225/68337], Loss: 5.0974\n",
      "Epoch [3/10], Step [18300/68337], Loss: 4.9942\n",
      "Epoch [3/10], Step [18375/68337], Loss: 5.1968\n",
      "Epoch [3/10], Step [18450/68337], Loss: 5.0629\n",
      "Epoch [3/10], Step [18525/68337], Loss: 5.1957\n",
      "Epoch [3/10], Step [18600/68337], Loss: 5.1476\n",
      "Epoch [3/10], Step [18675/68337], Loss: 5.3110\n",
      "Epoch [3/10], Step [18750/68337], Loss: 5.0572\n",
      "Epoch [3/10], Step [18825/68337], Loss: 5.1525\n",
      "Epoch [3/10], Step [18900/68337], Loss: 5.1574\n",
      "Epoch [3/10], Step [18975/68337], Loss: 4.9991\n",
      "Epoch [3/10], Step [19050/68337], Loss: 5.0571\n",
      "Epoch [3/10], Step [19125/68337], Loss: 5.0825\n",
      "Epoch [3/10], Step [19200/68337], Loss: 5.1435\n",
      "Epoch [3/10], Step [19275/68337], Loss: 5.0594\n",
      "Epoch [3/10], Step [19350/68337], Loss: 4.9897\n",
      "Epoch [3/10], Step [19425/68337], Loss: 5.1762\n",
      "Epoch [3/10], Step [19500/68337], Loss: 4.8829\n",
      "Epoch [3/10], Step [19575/68337], Loss: 5.3399\n",
      "Epoch [3/10], Step [19650/68337], Loss: 5.0820\n",
      "Epoch [3/10], Step [19725/68337], Loss: 5.2563\n",
      "Epoch [3/10], Step [19800/68337], Loss: 5.0656\n",
      "Epoch [3/10], Step [19875/68337], Loss: 5.2435\n",
      "Epoch [3/10], Step [19950/68337], Loss: 5.2529\n",
      "Validation perplexity: 139.08920445653317\n",
      "Epoch [3/10], Step [20025/68337], Loss: 5.1996\n",
      "Epoch [3/10], Step [20100/68337], Loss: 5.0599\n",
      "Epoch [3/10], Step [20175/68337], Loss: 5.3075\n",
      "Epoch [3/10], Step [20250/68337], Loss: 5.1838\n",
      "Epoch [3/10], Step [20325/68337], Loss: 5.4073\n",
      "Epoch [3/10], Step [20400/68337], Loss: 5.2556\n",
      "Epoch [3/10], Step [20475/68337], Loss: 5.1428\n",
      "Epoch [3/10], Step [20550/68337], Loss: 5.1298\n",
      "Epoch [3/10], Step [20625/68337], Loss: 5.0283\n",
      "Epoch [3/10], Step [20700/68337], Loss: 5.2394\n",
      "Epoch [3/10], Step [20775/68337], Loss: 5.3506\n",
      "Epoch [3/10], Step [20850/68337], Loss: 5.3197\n",
      "Epoch [3/10], Step [20925/68337], Loss: 5.1860\n",
      "Epoch [3/10], Step [21000/68337], Loss: 5.2228\n",
      "Epoch [3/10], Step [21075/68337], Loss: 5.2007\n",
      "Epoch [3/10], Step [21150/68337], Loss: 5.1683\n",
      "Epoch [3/10], Step [21225/68337], Loss: 5.2017\n",
      "Epoch [3/10], Step [21300/68337], Loss: 5.0904\n",
      "Epoch [3/10], Step [21375/68337], Loss: 5.1758\n",
      "Epoch [3/10], Step [21450/68337], Loss: 5.1645\n",
      "Epoch [3/10], Step [21525/68337], Loss: 5.3149\n",
      "Epoch [3/10], Step [21600/68337], Loss: 5.2497\n",
      "Epoch [3/10], Step [21675/68337], Loss: 5.0289\n",
      "Epoch [3/10], Step [21750/68337], Loss: 5.3246\n",
      "Epoch [3/10], Step [21825/68337], Loss: 5.0299\n",
      "Epoch [3/10], Step [21900/68337], Loss: 5.0706\n",
      "Epoch [3/10], Step [21975/68337], Loss: 5.1553\n",
      "Epoch [3/10], Step [22050/68337], Loss: 5.2682\n",
      "Epoch [3/10], Step [22125/68337], Loss: 5.1106\n",
      "Epoch [3/10], Step [22200/68337], Loss: 5.2564\n",
      "Epoch [3/10], Step [22275/68337], Loss: 5.1974\n",
      "Epoch [3/10], Step [22350/68337], Loss: 5.1157\n",
      "Epoch [3/10], Step [22425/68337], Loss: 5.3055\n",
      "Epoch [3/10], Step [22500/68337], Loss: 5.2783\n",
      "Epoch [3/10], Step [22575/68337], Loss: 5.1822\n",
      "Epoch [3/10], Step [22650/68337], Loss: 5.1622\n",
      "Epoch [3/10], Step [22725/68337], Loss: 5.1094\n",
      "Epoch [3/10], Step [22800/68337], Loss: 5.2170\n",
      "Epoch [3/10], Step [22875/68337], Loss: 5.1086\n",
      "Epoch [3/10], Step [22950/68337], Loss: 5.2617\n",
      "Epoch [3/10], Step [23025/68337], Loss: 5.1453\n",
      "Epoch [3/10], Step [23100/68337], Loss: 5.0901\n",
      "Epoch [3/10], Step [23175/68337], Loss: 5.2799\n",
      "Epoch [3/10], Step [23250/68337], Loss: 5.2092\n",
      "Epoch [3/10], Step [23325/68337], Loss: 5.1064\n",
      "Epoch [3/10], Step [23400/68337], Loss: 5.3343\n",
      "Epoch [3/10], Step [23475/68337], Loss: 5.3387\n",
      "Epoch [3/10], Step [23550/68337], Loss: 5.0663\n",
      "Epoch [3/10], Step [23625/68337], Loss: 5.2013\n",
      "Epoch [3/10], Step [23700/68337], Loss: 5.1507\n",
      "Epoch [3/10], Step [23775/68337], Loss: 5.2937\n",
      "Epoch [3/10], Step [23850/68337], Loss: 5.1193\n",
      "Epoch [3/10], Step [23925/68337], Loss: 5.0675\n",
      "Epoch [3/10], Step [24000/68337], Loss: 5.2596\n",
      "Epoch [3/10], Step [24075/68337], Loss: 5.2683\n",
      "Epoch [3/10], Step [24150/68337], Loss: 4.9521\n",
      "Epoch [3/10], Step [24225/68337], Loss: 5.2071\n",
      "Epoch [3/10], Step [24300/68337], Loss: 5.1311\n",
      "Epoch [3/10], Step [24375/68337], Loss: 5.0729\n",
      "Epoch [3/10], Step [24450/68337], Loss: 5.2264\n",
      "Epoch [3/10], Step [24525/68337], Loss: 5.3682\n",
      "Epoch [3/10], Step [24600/68337], Loss: 5.2076\n",
      "Epoch [3/10], Step [24675/68337], Loss: 5.2076\n",
      "Epoch [3/10], Step [24750/68337], Loss: 5.2610\n",
      "Epoch [3/10], Step [24825/68337], Loss: 5.1964\n",
      "Epoch [3/10], Step [24900/68337], Loss: 5.1766\n",
      "Epoch [3/10], Step [24975/68337], Loss: 5.2184\n",
      "Epoch [3/10], Step [25050/68337], Loss: 5.2973\n",
      "Epoch [3/10], Step [25125/68337], Loss: 5.1708\n",
      "Epoch [3/10], Step [25200/68337], Loss: 5.1094\n",
      "Epoch [3/10], Step [25275/68337], Loss: 5.1957\n",
      "Epoch [3/10], Step [25350/68337], Loss: 5.0666\n",
      "Epoch [3/10], Step [25425/68337], Loss: 4.8945\n",
      "Epoch [3/10], Step [25500/68337], Loss: 5.1970\n",
      "Epoch [3/10], Step [25575/68337], Loss: 5.2087\n",
      "Epoch [3/10], Step [25650/68337], Loss: 5.2260\n",
      "Epoch [3/10], Step [25725/68337], Loss: 5.1981\n",
      "Epoch [3/10], Step [25800/68337], Loss: 5.1918\n",
      "Epoch [3/10], Step [25875/68337], Loss: 5.1902\n",
      "Epoch [3/10], Step [25950/68337], Loss: 5.2519\n",
      "Epoch [3/10], Step [26025/68337], Loss: 5.1933\n",
      "Epoch [3/10], Step [26100/68337], Loss: 5.1547\n",
      "Epoch [3/10], Step [26175/68337], Loss: 5.1374\n",
      "Epoch [3/10], Step [26250/68337], Loss: 5.1375\n",
      "Epoch [3/10], Step [26325/68337], Loss: 5.1530\n",
      "Epoch [3/10], Step [26400/68337], Loss: 5.1787\n",
      "Epoch [3/10], Step [26475/68337], Loss: 5.2540\n",
      "Epoch [3/10], Step [26550/68337], Loss: 5.0762\n",
      "Epoch [3/10], Step [26625/68337], Loss: 5.2650\n",
      "Epoch [3/10], Step [26700/68337], Loss: 4.9290\n",
      "Epoch [3/10], Step [26775/68337], Loss: 5.3490\n",
      "Epoch [3/10], Step [26850/68337], Loss: 5.2805\n",
      "Epoch [3/10], Step [26925/68337], Loss: 5.0976\n",
      "Epoch [3/10], Step [27000/68337], Loss: 5.2542\n",
      "Epoch [3/10], Step [27075/68337], Loss: 5.0007\n",
      "Epoch [3/10], Step [27150/68337], Loss: 5.1610\n",
      "Epoch [3/10], Step [27225/68337], Loss: 5.2307\n",
      "Epoch [3/10], Step [27300/68337], Loss: 5.1319\n",
      "Epoch [3/10], Step [27375/68337], Loss: 5.2768\n",
      "Epoch [3/10], Step [27450/68337], Loss: 5.2755\n",
      "Epoch [3/10], Step [27525/68337], Loss: 5.4045\n",
      "Epoch [3/10], Step [27600/68337], Loss: 5.1931\n",
      "Epoch [3/10], Step [27675/68337], Loss: 5.2085\n",
      "Epoch [3/10], Step [27750/68337], Loss: 5.3827\n",
      "Epoch [3/10], Step [27825/68337], Loss: 5.2993\n",
      "Epoch [3/10], Step [27900/68337], Loss: 5.0884\n",
      "Epoch [3/10], Step [27975/68337], Loss: 4.9984\n",
      "Epoch [3/10], Step [28050/68337], Loss: 5.0920\n",
      "Epoch [3/10], Step [28125/68337], Loss: 5.3654\n",
      "Epoch [3/10], Step [28200/68337], Loss: 4.9745\n",
      "Epoch [3/10], Step [28275/68337], Loss: 4.9419\n",
      "Epoch [3/10], Step [28350/68337], Loss: 5.1419\n",
      "Epoch [3/10], Step [28425/68337], Loss: 5.1672\n",
      "Epoch [3/10], Step [28500/68337], Loss: 5.2241\n",
      "Epoch [3/10], Step [28575/68337], Loss: 5.1149\n",
      "Epoch [3/10], Step [28650/68337], Loss: 5.1060\n",
      "Epoch [3/10], Step [28725/68337], Loss: 5.1011\n",
      "Epoch [3/10], Step [28800/68337], Loss: 5.1395\n",
      "Epoch [3/10], Step [28875/68337], Loss: 5.2439\n",
      "Epoch [3/10], Step [28950/68337], Loss: 4.9232\n",
      "Epoch [3/10], Step [29025/68337], Loss: 5.2295\n",
      "Epoch [3/10], Step [29100/68337], Loss: 4.9844\n",
      "Epoch [3/10], Step [29175/68337], Loss: 5.1531\n",
      "Epoch [3/10], Step [29250/68337], Loss: 5.2370\n",
      "Epoch [3/10], Step [29325/68337], Loss: 5.2926\n",
      "Epoch [3/10], Step [29400/68337], Loss: 5.1022\n",
      "Epoch [3/10], Step [29475/68337], Loss: 5.0932\n",
      "Epoch [3/10], Step [29550/68337], Loss: 5.1491\n",
      "Epoch [3/10], Step [29625/68337], Loss: 5.1353\n",
      "Epoch [3/10], Step [29700/68337], Loss: 5.1118\n",
      "Epoch [3/10], Step [29775/68337], Loss: 5.3075\n",
      "Epoch [3/10], Step [29850/68337], Loss: 5.1598\n",
      "Epoch [3/10], Step [29925/68337], Loss: 5.3662\n",
      "Epoch [3/10], Step [30000/68337], Loss: 5.0571\n",
      "Validation perplexity: 137.35876395004482\n",
      "Epoch [3/10], Step [30075/68337], Loss: 5.1045\n",
      "Epoch [3/10], Step [30150/68337], Loss: 5.3106\n",
      "Epoch [3/10], Step [30225/68337], Loss: 5.1490\n",
      "Epoch [3/10], Step [30300/68337], Loss: 5.2957\n",
      "Epoch [3/10], Step [30375/68337], Loss: 5.2490\n",
      "Epoch [3/10], Step [30450/68337], Loss: 4.9879\n",
      "Epoch [3/10], Step [30525/68337], Loss: 5.0765\n",
      "Epoch [3/10], Step [30600/68337], Loss: 5.0921\n",
      "Epoch [3/10], Step [30675/68337], Loss: 5.0180\n",
      "Epoch [3/10], Step [30750/68337], Loss: 5.2059\n",
      "Epoch [3/10], Step [30825/68337], Loss: 5.1794\n",
      "Epoch [3/10], Step [30900/68337], Loss: 5.3076\n",
      "Epoch [3/10], Step [30975/68337], Loss: 5.1803\n",
      "Epoch [3/10], Step [31050/68337], Loss: 5.0661\n",
      "Epoch [3/10], Step [31125/68337], Loss: 5.2147\n",
      "Epoch [3/10], Step [31200/68337], Loss: 5.1689\n",
      "Epoch [3/10], Step [31275/68337], Loss: 5.2037\n",
      "Epoch [3/10], Step [31350/68337], Loss: 4.9926\n",
      "Epoch [3/10], Step [31425/68337], Loss: 5.3381\n",
      "Epoch [3/10], Step [31500/68337], Loss: 5.1718\n",
      "Epoch [3/10], Step [31575/68337], Loss: 5.3565\n",
      "Epoch [3/10], Step [31650/68337], Loss: 5.0572\n",
      "Epoch [3/10], Step [31725/68337], Loss: 5.3696\n",
      "Epoch [3/10], Step [31800/68337], Loss: 5.2391\n",
      "Epoch [3/10], Step [31875/68337], Loss: 5.3945\n",
      "Epoch [3/10], Step [31950/68337], Loss: 5.2433\n",
      "Epoch [3/10], Step [32025/68337], Loss: 5.1531\n",
      "Epoch [3/10], Step [32100/68337], Loss: 5.2373\n",
      "Epoch [3/10], Step [32175/68337], Loss: 5.4975\n",
      "Epoch [3/10], Step [32250/68337], Loss: 5.2084\n",
      "Epoch [3/10], Step [32325/68337], Loss: 5.2858\n",
      "Epoch [3/10], Step [32400/68337], Loss: 5.0920\n",
      "Epoch [3/10], Step [32475/68337], Loss: 5.2175\n",
      "Epoch [3/10], Step [32550/68337], Loss: 5.1768\n",
      "Epoch [3/10], Step [32625/68337], Loss: 5.1160\n",
      "Epoch [3/10], Step [32700/68337], Loss: 5.2441\n",
      "Epoch [3/10], Step [32775/68337], Loss: 5.1556\n",
      "Epoch [3/10], Step [32850/68337], Loss: 5.1611\n",
      "Epoch [3/10], Step [32925/68337], Loss: 5.1547\n",
      "Epoch [3/10], Step [33000/68337], Loss: 5.0334\n",
      "Epoch [3/10], Step [33075/68337], Loss: 5.0975\n",
      "Epoch [3/10], Step [33150/68337], Loss: 5.1844\n",
      "Epoch [3/10], Step [33225/68337], Loss: 4.9343\n",
      "Epoch [3/10], Step [33300/68337], Loss: 5.0339\n",
      "Epoch [3/10], Step [33375/68337], Loss: 5.1667\n",
      "Epoch [3/10], Step [33450/68337], Loss: 5.2879\n",
      "Epoch [3/10], Step [33525/68337], Loss: 5.1027\n",
      "Epoch [3/10], Step [33600/68337], Loss: 5.2619\n",
      "Epoch [3/10], Step [33675/68337], Loss: 5.1738\n",
      "Epoch [3/10], Step [33750/68337], Loss: 5.1812\n",
      "Epoch [3/10], Step [33825/68337], Loss: 5.5060\n",
      "Epoch [3/10], Step [33900/68337], Loss: 5.1367\n",
      "Epoch [3/10], Step [33975/68337], Loss: 5.2476\n",
      "Epoch [3/10], Step [34050/68337], Loss: 5.3141\n",
      "Epoch [3/10], Step [34125/68337], Loss: 5.3158\n",
      "Epoch [3/10], Step [34200/68337], Loss: 5.2518\n",
      "Epoch [3/10], Step [34275/68337], Loss: 5.4560\n",
      "Epoch [3/10], Step [34350/68337], Loss: 5.2012\n",
      "Epoch [3/10], Step [34425/68337], Loss: 5.2422\n",
      "Epoch [3/10], Step [34500/68337], Loss: 5.1189\n",
      "Epoch [3/10], Step [34575/68337], Loss: 5.0941\n",
      "Epoch [3/10], Step [34650/68337], Loss: 5.3153\n",
      "Epoch [3/10], Step [34725/68337], Loss: 5.0759\n",
      "Epoch [3/10], Step [34800/68337], Loss: 5.2698\n",
      "Epoch [3/10], Step [34875/68337], Loss: 5.2333\n",
      "Epoch [3/10], Step [34950/68337], Loss: 5.2887\n",
      "Epoch [3/10], Step [35025/68337], Loss: 5.2295\n",
      "Epoch [3/10], Step [35100/68337], Loss: 5.1401\n",
      "Epoch [3/10], Step [35175/68337], Loss: 5.1355\n",
      "Epoch [3/10], Step [35250/68337], Loss: 5.2525\n",
      "Epoch [3/10], Step [35325/68337], Loss: 5.0367\n",
      "Epoch [3/10], Step [35400/68337], Loss: 5.2693\n",
      "Epoch [3/10], Step [35475/68337], Loss: 5.3035\n",
      "Epoch [3/10], Step [35550/68337], Loss: 5.2211\n",
      "Epoch [3/10], Step [35625/68337], Loss: 5.0310\n",
      "Epoch [3/10], Step [35700/68337], Loss: 5.1354\n",
      "Epoch [3/10], Step [35775/68337], Loss: 5.1430\n",
      "Epoch [3/10], Step [35850/68337], Loss: 5.1224\n",
      "Epoch [3/10], Step [35925/68337], Loss: 5.2305\n",
      "Epoch [3/10], Step [36000/68337], Loss: 5.3076\n",
      "Epoch [3/10], Step [36075/68337], Loss: 5.2537\n",
      "Epoch [3/10], Step [36150/68337], Loss: 4.9483\n",
      "Epoch [3/10], Step [36225/68337], Loss: 5.2803\n",
      "Epoch [3/10], Step [36300/68337], Loss: 5.0379\n",
      "Epoch [3/10], Step [36375/68337], Loss: 5.1557\n",
      "Epoch [3/10], Step [36450/68337], Loss: 5.1740\n",
      "Epoch [3/10], Step [36525/68337], Loss: 5.2588\n",
      "Epoch [3/10], Step [36600/68337], Loss: 5.2725\n",
      "Epoch [3/10], Step [36675/68337], Loss: 5.2138\n",
      "Epoch [3/10], Step [36750/68337], Loss: 5.2221\n",
      "Epoch [3/10], Step [36825/68337], Loss: 5.2924\n",
      "Epoch [3/10], Step [36900/68337], Loss: 5.1571\n",
      "Epoch [3/10], Step [36975/68337], Loss: 5.3174\n",
      "Epoch [3/10], Step [37050/68337], Loss: 5.0416\n",
      "Epoch [3/10], Step [37125/68337], Loss: 5.0824\n",
      "Epoch [3/10], Step [37200/68337], Loss: 4.9460\n",
      "Epoch [3/10], Step [37275/68337], Loss: 5.1868\n",
      "Epoch [3/10], Step [37350/68337], Loss: 5.1914\n",
      "Epoch [3/10], Step [37425/68337], Loss: 5.1700\n",
      "Epoch [3/10], Step [37500/68337], Loss: 5.2300\n",
      "Epoch [3/10], Step [37575/68337], Loss: 5.0454\n",
      "Epoch [3/10], Step [37650/68337], Loss: 5.1865\n",
      "Epoch [3/10], Step [37725/68337], Loss: 5.4065\n",
      "Epoch [3/10], Step [37800/68337], Loss: 5.2232\n",
      "Epoch [3/10], Step [37875/68337], Loss: 5.2721\n",
      "Epoch [3/10], Step [37950/68337], Loss: 5.2304\n",
      "Epoch [3/10], Step [38025/68337], Loss: 5.1517\n",
      "Epoch [3/10], Step [38100/68337], Loss: 5.0799\n",
      "Epoch [3/10], Step [38175/68337], Loss: 5.0430\n",
      "Epoch [3/10], Step [38250/68337], Loss: 5.1486\n",
      "Epoch [3/10], Step [38325/68337], Loss: 5.1530\n",
      "Epoch [3/10], Step [38400/68337], Loss: 5.2404\n",
      "Epoch [3/10], Step [38475/68337], Loss: 5.1752\n",
      "Epoch [3/10], Step [38550/68337], Loss: 5.2341\n",
      "Epoch [3/10], Step [38625/68337], Loss: 5.1674\n",
      "Epoch [3/10], Step [38700/68337], Loss: 5.3635\n",
      "Epoch [3/10], Step [38775/68337], Loss: 5.3836\n",
      "Epoch [3/10], Step [38850/68337], Loss: 5.3097\n",
      "Epoch [3/10], Step [38925/68337], Loss: 5.1592\n",
      "Epoch [3/10], Step [39000/68337], Loss: 5.0291\n",
      "Epoch [3/10], Step [39075/68337], Loss: 5.2258\n",
      "Epoch [3/10], Step [39150/68337], Loss: 4.9781\n",
      "Epoch [3/10], Step [39225/68337], Loss: 5.2819\n",
      "Epoch [3/10], Step [39300/68337], Loss: 5.2895\n",
      "Epoch [3/10], Step [39375/68337], Loss: 5.1906\n",
      "Epoch [3/10], Step [39450/68337], Loss: 5.0848\n",
      "Epoch [3/10], Step [39525/68337], Loss: 5.2520\n",
      "Epoch [3/10], Step [39600/68337], Loss: 5.0573\n",
      "Epoch [3/10], Step [39675/68337], Loss: 5.1958\n",
      "Epoch [3/10], Step [39750/68337], Loss: 5.1453\n",
      "Epoch [3/10], Step [39825/68337], Loss: 5.0960\n",
      "Epoch [3/10], Step [39900/68337], Loss: 5.2592\n",
      "Epoch [3/10], Step [39975/68337], Loss: 5.1183\n",
      "Validation perplexity: 136.79912283092438\n",
      "Epoch [3/10], Step [40050/68337], Loss: 5.1532\n",
      "Epoch [3/10], Step [40125/68337], Loss: 5.2129\n",
      "Epoch [3/10], Step [40200/68337], Loss: 5.2087\n",
      "Epoch [3/10], Step [40275/68337], Loss: 5.1803\n",
      "Epoch [3/10], Step [40350/68337], Loss: 5.0645\n",
      "Epoch [3/10], Step [40425/68337], Loss: 5.4689\n",
      "Epoch [3/10], Step [40500/68337], Loss: 5.2596\n",
      "Epoch [3/10], Step [40575/68337], Loss: 5.2446\n",
      "Epoch [3/10], Step [40650/68337], Loss: 5.2152\n",
      "Epoch [3/10], Step [40725/68337], Loss: 4.8548\n",
      "Epoch [3/10], Step [40800/68337], Loss: 5.2948\n",
      "Epoch [3/10], Step [40875/68337], Loss: 5.1838\n",
      "Epoch [3/10], Step [40950/68337], Loss: 5.1260\n",
      "Epoch [3/10], Step [41025/68337], Loss: 5.0907\n",
      "Epoch [3/10], Step [41100/68337], Loss: 5.3227\n",
      "Epoch [3/10], Step [41175/68337], Loss: 5.2083\n",
      "Epoch [3/10], Step [41250/68337], Loss: 5.0652\n",
      "Epoch [3/10], Step [41325/68337], Loss: 5.0636\n",
      "Epoch [3/10], Step [41400/68337], Loss: 5.3140\n",
      "Epoch [3/10], Step [41475/68337], Loss: 5.0015\n",
      "Epoch [3/10], Step [41550/68337], Loss: 5.1401\n",
      "Epoch [3/10], Step [41625/68337], Loss: 5.1522\n",
      "Epoch [3/10], Step [41700/68337], Loss: 4.8738\n",
      "Epoch [3/10], Step [41775/68337], Loss: 5.2028\n",
      "Epoch [3/10], Step [41850/68337], Loss: 5.3490\n",
      "Epoch [3/10], Step [41925/68337], Loss: 4.8052\n",
      "Epoch [3/10], Step [42000/68337], Loss: 5.2089\n",
      "Epoch [3/10], Step [42075/68337], Loss: 5.1490\n",
      "Epoch [3/10], Step [42150/68337], Loss: 5.0405\n",
      "Epoch [3/10], Step [42225/68337], Loss: 4.9823\n",
      "Epoch [3/10], Step [42300/68337], Loss: 5.2653\n",
      "Epoch [3/10], Step [42375/68337], Loss: 4.9451\n",
      "Epoch [3/10], Step [42450/68337], Loss: 5.2393\n",
      "Epoch [3/10], Step [42525/68337], Loss: 5.2279\n",
      "Epoch [3/10], Step [42600/68337], Loss: 5.2077\n",
      "Epoch [3/10], Step [42675/68337], Loss: 5.1951\n",
      "Epoch [3/10], Step [42750/68337], Loss: 5.1419\n",
      "Epoch [3/10], Step [42825/68337], Loss: 5.1947\n",
      "Epoch [3/10], Step [42900/68337], Loss: 5.3082\n",
      "Epoch [3/10], Step [42975/68337], Loss: 5.0812\n",
      "Epoch [3/10], Step [43050/68337], Loss: 5.0803\n",
      "Epoch [3/10], Step [43125/68337], Loss: 5.2237\n",
      "Epoch [3/10], Step [43200/68337], Loss: 5.1469\n",
      "Epoch [3/10], Step [43275/68337], Loss: 5.1917\n",
      "Epoch [3/10], Step [43350/68337], Loss: 5.0979\n",
      "Epoch [3/10], Step [43425/68337], Loss: 5.1416\n",
      "Epoch [3/10], Step [43500/68337], Loss: 5.2082\n",
      "Epoch [3/10], Step [43575/68337], Loss: 5.3213\n",
      "Epoch [3/10], Step [43650/68337], Loss: 5.3451\n",
      "Epoch [3/10], Step [43725/68337], Loss: 5.1693\n",
      "Epoch [3/10], Step [43800/68337], Loss: 5.3206\n",
      "Epoch [3/10], Step [43875/68337], Loss: 5.1571\n",
      "Epoch [3/10], Step [43950/68337], Loss: 5.2907\n",
      "Epoch [3/10], Step [44025/68337], Loss: 5.0137\n",
      "Epoch [3/10], Step [44100/68337], Loss: 5.2766\n",
      "Epoch [3/10], Step [44175/68337], Loss: 5.2633\n",
      "Epoch [3/10], Step [44250/68337], Loss: 5.0858\n",
      "Epoch [3/10], Step [44325/68337], Loss: 5.0422\n",
      "Epoch [3/10], Step [44400/68337], Loss: 5.0379\n",
      "Epoch [3/10], Step [44475/68337], Loss: 5.0648\n",
      "Epoch [3/10], Step [44550/68337], Loss: 5.2139\n",
      "Epoch [3/10], Step [44625/68337], Loss: 5.1286\n",
      "Epoch [3/10], Step [44700/68337], Loss: 5.1262\n",
      "Epoch [3/10], Step [44775/68337], Loss: 5.0241\n",
      "Epoch [3/10], Step [44850/68337], Loss: 5.2166\n",
      "Epoch [3/10], Step [44925/68337], Loss: 4.9015\n",
      "Epoch [3/10], Step [45000/68337], Loss: 5.0615\n",
      "Epoch [3/10], Step [45075/68337], Loss: 5.2517\n",
      "Epoch [3/10], Step [45150/68337], Loss: 5.2332\n",
      "Epoch [3/10], Step [45225/68337], Loss: 5.2190\n",
      "Epoch [3/10], Step [45300/68337], Loss: 5.2191\n",
      "Epoch [3/10], Step [45375/68337], Loss: 5.2638\n",
      "Epoch [3/10], Step [45450/68337], Loss: 5.2864\n",
      "Epoch [3/10], Step [45525/68337], Loss: 5.1291\n",
      "Epoch [3/10], Step [45600/68337], Loss: 5.2033\n",
      "Epoch [3/10], Step [45675/68337], Loss: 5.2226\n",
      "Epoch [3/10], Step [45750/68337], Loss: 5.1779\n",
      "Epoch [3/10], Step [45825/68337], Loss: 5.2569\n",
      "Epoch [3/10], Step [45900/68337], Loss: 5.1962\n",
      "Epoch [3/10], Step [45975/68337], Loss: 5.1858\n",
      "Epoch [3/10], Step [46050/68337], Loss: 5.2243\n",
      "Epoch [3/10], Step [46125/68337], Loss: 5.1120\n",
      "Epoch [3/10], Step [46200/68337], Loss: 5.1780\n",
      "Epoch [3/10], Step [46275/68337], Loss: 5.0225\n",
      "Epoch [3/10], Step [46350/68337], Loss: 5.2174\n",
      "Epoch [3/10], Step [46425/68337], Loss: 4.9787\n",
      "Epoch [3/10], Step [46500/68337], Loss: 5.1139\n",
      "Epoch [3/10], Step [46575/68337], Loss: 5.0212\n",
      "Epoch [3/10], Step [46650/68337], Loss: 5.2032\n",
      "Epoch [3/10], Step [46725/68337], Loss: 5.1136\n",
      "Epoch [3/10], Step [46800/68337], Loss: 5.1875\n",
      "Epoch [3/10], Step [46875/68337], Loss: 4.9613\n",
      "Epoch [3/10], Step [46950/68337], Loss: 5.1784\n",
      "Epoch [3/10], Step [47025/68337], Loss: 5.0419\n",
      "Epoch [3/10], Step [47100/68337], Loss: 5.0060\n",
      "Epoch [3/10], Step [47175/68337], Loss: 4.9224\n",
      "Epoch [3/10], Step [47250/68337], Loss: 5.0289\n",
      "Epoch [3/10], Step [47325/68337], Loss: 5.3047\n",
      "Epoch [3/10], Step [47400/68337], Loss: 5.3276\n",
      "Epoch [3/10], Step [47475/68337], Loss: 5.1253\n",
      "Epoch [3/10], Step [47550/68337], Loss: 5.1744\n",
      "Epoch [3/10], Step [47625/68337], Loss: 5.1349\n",
      "Epoch [3/10], Step [47700/68337], Loss: 5.2206\n",
      "Epoch [3/10], Step [47775/68337], Loss: 5.1709\n",
      "Epoch [3/10], Step [47850/68337], Loss: 5.1697\n",
      "Epoch [3/10], Step [47925/68337], Loss: 5.2185\n",
      "Epoch [3/10], Step [48000/68337], Loss: 5.1440\n",
      "Epoch [3/10], Step [48075/68337], Loss: 5.2485\n",
      "Epoch [3/10], Step [48150/68337], Loss: 5.2834\n",
      "Epoch [3/10], Step [48225/68337], Loss: 5.1287\n",
      "Epoch [3/10], Step [48300/68337], Loss: 5.3053\n",
      "Epoch [3/10], Step [48375/68337], Loss: 5.2883\n",
      "Epoch [3/10], Step [48450/68337], Loss: 5.1912\n",
      "Epoch [3/10], Step [48525/68337], Loss: 5.4008\n",
      "Epoch [3/10], Step [48600/68337], Loss: 5.2331\n",
      "Epoch [3/10], Step [48675/68337], Loss: 5.2942\n",
      "Epoch [3/10], Step [48750/68337], Loss: 5.2030\n",
      "Epoch [3/10], Step [48825/68337], Loss: 5.3678\n",
      "Epoch [3/10], Step [48900/68337], Loss: 5.0796\n",
      "Epoch [3/10], Step [48975/68337], Loss: 5.0862\n",
      "Epoch [3/10], Step [49050/68337], Loss: 5.1548\n",
      "Epoch [3/10], Step [49125/68337], Loss: 5.1248\n",
      "Epoch [3/10], Step [49200/68337], Loss: 5.2330\n",
      "Epoch [3/10], Step [49275/68337], Loss: 5.0270\n",
      "Epoch [3/10], Step [49350/68337], Loss: 5.0068\n",
      "Epoch [3/10], Step [49425/68337], Loss: 5.3677\n",
      "Epoch [3/10], Step [49500/68337], Loss: 5.2240\n",
      "Epoch [3/10], Step [49575/68337], Loss: 5.2136\n",
      "Epoch [3/10], Step [49650/68337], Loss: 5.2689\n",
      "Epoch [3/10], Step [49725/68337], Loss: 5.2029\n",
      "Epoch [3/10], Step [49800/68337], Loss: 5.1744\n",
      "Epoch [3/10], Step [49875/68337], Loss: 5.0508\n",
      "Epoch [3/10], Step [49950/68337], Loss: 5.1225\n",
      "Validation perplexity: 135.6054524708897\n",
      "Epoch [3/10], Step [50025/68337], Loss: 5.1051\n",
      "Epoch [3/10], Step [50100/68337], Loss: 5.1559\n",
      "Epoch [3/10], Step [50175/68337], Loss: 5.2733\n",
      "Epoch [3/10], Step [50250/68337], Loss: 5.0265\n",
      "Epoch [3/10], Step [50325/68337], Loss: 5.1731\n",
      "Epoch [3/10], Step [50400/68337], Loss: 5.3313\n",
      "Epoch [3/10], Step [50475/68337], Loss: 5.2882\n",
      "Epoch [3/10], Step [50550/68337], Loss: 5.3621\n",
      "Epoch [3/10], Step [50625/68337], Loss: 5.2459\n",
      "Epoch [3/10], Step [50700/68337], Loss: 5.0472\n",
      "Epoch [3/10], Step [50775/68337], Loss: 5.2981\n",
      "Epoch [3/10], Step [50850/68337], Loss: 5.0725\n",
      "Epoch [3/10], Step [50925/68337], Loss: 5.2067\n",
      "Epoch [3/10], Step [51000/68337], Loss: 5.0448\n",
      "Epoch [3/10], Step [51075/68337], Loss: 5.2847\n",
      "Epoch [3/10], Step [51150/68337], Loss: 5.1046\n",
      "Epoch [3/10], Step [51225/68337], Loss: 5.2077\n",
      "Epoch [3/10], Step [51300/68337], Loss: 5.2407\n",
      "Epoch [3/10], Step [51375/68337], Loss: 5.0883\n",
      "Epoch [3/10], Step [51450/68337], Loss: 5.0464\n",
      "Epoch [3/10], Step [51525/68337], Loss: 5.1578\n",
      "Epoch [3/10], Step [51600/68337], Loss: 5.0408\n",
      "Epoch [3/10], Step [51675/68337], Loss: 5.1493\n",
      "Epoch [3/10], Step [51750/68337], Loss: 5.0471\n",
      "Epoch [3/10], Step [51825/68337], Loss: 5.2439\n",
      "Epoch [3/10], Step [51900/68337], Loss: 5.2521\n",
      "Epoch [3/10], Step [51975/68337], Loss: 4.9620\n",
      "Epoch [3/10], Step [52050/68337], Loss: 5.3105\n",
      "Epoch [3/10], Step [52125/68337], Loss: 5.2360\n",
      "Epoch [3/10], Step [52200/68337], Loss: 4.9918\n",
      "Epoch [3/10], Step [52275/68337], Loss: 5.2058\n",
      "Epoch [3/10], Step [52350/68337], Loss: 5.1551\n",
      "Epoch [3/10], Step [52425/68337], Loss: 5.3594\n",
      "Epoch [3/10], Step [52500/68337], Loss: 5.1091\n",
      "Epoch [3/10], Step [52575/68337], Loss: 4.9580\n",
      "Epoch [3/10], Step [52650/68337], Loss: 5.1717\n",
      "Epoch [3/10], Step [52725/68337], Loss: 5.0071\n",
      "Epoch [3/10], Step [52800/68337], Loss: 5.0222\n",
      "Epoch [3/10], Step [52875/68337], Loss: 5.0568\n",
      "Epoch [3/10], Step [52950/68337], Loss: 5.4043\n",
      "Epoch [3/10], Step [53025/68337], Loss: 5.3116\n",
      "Epoch [3/10], Step [53100/68337], Loss: 5.1818\n",
      "Epoch [3/10], Step [53175/68337], Loss: 5.3700\n",
      "Epoch [3/10], Step [53250/68337], Loss: 5.2141\n",
      "Epoch [3/10], Step [53325/68337], Loss: 5.2138\n",
      "Epoch [3/10], Step [53400/68337], Loss: 5.1797\n",
      "Epoch [3/10], Step [53475/68337], Loss: 5.1423\n",
      "Epoch [3/10], Step [53550/68337], Loss: 4.9771\n",
      "Epoch [3/10], Step [53625/68337], Loss: 5.0669\n",
      "Epoch [3/10], Step [53700/68337], Loss: 5.0887\n",
      "Epoch [3/10], Step [53775/68337], Loss: 5.2627\n",
      "Epoch [3/10], Step [53850/68337], Loss: 5.1815\n",
      "Epoch [3/10], Step [53925/68337], Loss: 5.2374\n",
      "Epoch [3/10], Step [54000/68337], Loss: 5.2984\n",
      "Epoch [3/10], Step [54075/68337], Loss: 5.1040\n",
      "Epoch [3/10], Step [54150/68337], Loss: 5.4034\n",
      "Epoch [3/10], Step [54225/68337], Loss: 5.0494\n",
      "Epoch [3/10], Step [54300/68337], Loss: 5.1747\n",
      "Epoch [3/10], Step [54375/68337], Loss: 5.2062\n",
      "Epoch [3/10], Step [54450/68337], Loss: 4.9109\n",
      "Epoch [3/10], Step [54525/68337], Loss: 5.2750\n",
      "Epoch [3/10], Step [54600/68337], Loss: 5.1029\n",
      "Epoch [3/10], Step [54675/68337], Loss: 5.2382\n",
      "Epoch [3/10], Step [54750/68337], Loss: 4.9479\n",
      "Epoch [3/10], Step [54825/68337], Loss: 5.2282\n",
      "Epoch [3/10], Step [54900/68337], Loss: 5.2946\n",
      "Epoch [3/10], Step [54975/68337], Loss: 5.1472\n",
      "Epoch [3/10], Step [55050/68337], Loss: 5.1323\n",
      "Epoch [3/10], Step [55125/68337], Loss: 5.1747\n",
      "Epoch [3/10], Step [55200/68337], Loss: 5.2741\n",
      "Epoch [3/10], Step [55275/68337], Loss: 5.0209\n",
      "Epoch [3/10], Step [55350/68337], Loss: 5.1473\n",
      "Epoch [3/10], Step [55425/68337], Loss: 5.1552\n",
      "Epoch [3/10], Step [55500/68337], Loss: 5.2143\n",
      "Epoch [3/10], Step [55575/68337], Loss: 5.0065\n",
      "Epoch [3/10], Step [55650/68337], Loss: 5.0279\n",
      "Epoch [3/10], Step [55725/68337], Loss: 5.0360\n",
      "Epoch [3/10], Step [55800/68337], Loss: 5.2173\n",
      "Epoch [3/10], Step [55875/68337], Loss: 5.1755\n",
      "Epoch [3/10], Step [55950/68337], Loss: 5.3674\n",
      "Epoch [3/10], Step [56025/68337], Loss: 4.9411\n",
      "Epoch [3/10], Step [56100/68337], Loss: 5.0616\n",
      "Epoch [3/10], Step [56175/68337], Loss: 5.1315\n",
      "Epoch [3/10], Step [56250/68337], Loss: 5.1330\n",
      "Epoch [3/10], Step [56325/68337], Loss: 5.0727\n",
      "Epoch [3/10], Step [56400/68337], Loss: 5.0976\n",
      "Epoch [3/10], Step [56475/68337], Loss: 5.1558\n",
      "Epoch [3/10], Step [56550/68337], Loss: 4.9619\n",
      "Epoch [3/10], Step [56625/68337], Loss: 5.2004\n",
      "Epoch [3/10], Step [56700/68337], Loss: 5.1069\n",
      "Epoch [3/10], Step [56775/68337], Loss: 5.0926\n",
      "Epoch [3/10], Step [56850/68337], Loss: 5.0166\n",
      "Epoch [3/10], Step [56925/68337], Loss: 5.3528\n",
      "Epoch [3/10], Step [57000/68337], Loss: 5.2626\n",
      "Epoch [3/10], Step [57075/68337], Loss: 5.3597\n",
      "Epoch [3/10], Step [57150/68337], Loss: 5.0809\n",
      "Epoch [3/10], Step [57225/68337], Loss: 5.2526\n",
      "Epoch [3/10], Step [57300/68337], Loss: 5.1456\n",
      "Epoch [3/10], Step [57375/68337], Loss: 5.2533\n",
      "Epoch [3/10], Step [57450/68337], Loss: 5.0440\n",
      "Epoch [3/10], Step [57525/68337], Loss: 5.2008\n",
      "Epoch [3/10], Step [57600/68337], Loss: 5.3089\n",
      "Epoch [3/10], Step [57675/68337], Loss: 5.1135\n",
      "Epoch [3/10], Step [57750/68337], Loss: 5.0699\n",
      "Epoch [3/10], Step [57825/68337], Loss: 5.2448\n",
      "Epoch [3/10], Step [57900/68337], Loss: 5.1850\n",
      "Epoch [3/10], Step [57975/68337], Loss: 5.0446\n",
      "Epoch [3/10], Step [58050/68337], Loss: 5.2616\n",
      "Epoch [3/10], Step [58125/68337], Loss: 4.9987\n",
      "Epoch [3/10], Step [58200/68337], Loss: 5.1747\n",
      "Epoch [3/10], Step [58275/68337], Loss: 4.9232\n",
      "Epoch [3/10], Step [58350/68337], Loss: 5.1489\n",
      "Epoch [3/10], Step [58425/68337], Loss: 5.1520\n",
      "Epoch [3/10], Step [58500/68337], Loss: 5.1740\n",
      "Epoch [3/10], Step [58575/68337], Loss: 5.2724\n",
      "Epoch [3/10], Step [58650/68337], Loss: 5.1672\n",
      "Epoch [3/10], Step [58725/68337], Loss: 5.0518\n",
      "Epoch [3/10], Step [58800/68337], Loss: 5.1303\n",
      "Epoch [3/10], Step [58875/68337], Loss: 5.1948\n",
      "Epoch [3/10], Step [58950/68337], Loss: 5.1693\n",
      "Epoch [3/10], Step [59025/68337], Loss: 5.2336\n",
      "Epoch [3/10], Step [59100/68337], Loss: 5.2531\n",
      "Epoch [3/10], Step [59175/68337], Loss: 5.3073\n",
      "Epoch [3/10], Step [59250/68337], Loss: 5.3626\n",
      "Epoch [3/10], Step [59325/68337], Loss: 5.0278\n",
      "Epoch [3/10], Step [59400/68337], Loss: 4.9808\n",
      "Epoch [3/10], Step [59475/68337], Loss: 5.2842\n",
      "Epoch [3/10], Step [59550/68337], Loss: 5.1671\n",
      "Epoch [3/10], Step [59625/68337], Loss: 4.8813\n",
      "Epoch [3/10], Step [59700/68337], Loss: 5.3288\n",
      "Epoch [3/10], Step [59775/68337], Loss: 5.0815\n",
      "Epoch [3/10], Step [59850/68337], Loss: 5.1128\n",
      "Epoch [3/10], Step [59925/68337], Loss: 5.0006\n",
      "Epoch [3/10], Step [60000/68337], Loss: 5.3171\n",
      "Validation perplexity: 134.95108553406314\n",
      "Epoch [3/10], Step [60075/68337], Loss: 5.2490\n",
      "Epoch [3/10], Step [60150/68337], Loss: 5.2164\n",
      "Epoch [3/10], Step [60225/68337], Loss: 5.3335\n",
      "Epoch [3/10], Step [60300/68337], Loss: 4.9910\n",
      "Epoch [3/10], Step [60375/68337], Loss: 4.9326\n",
      "Epoch [3/10], Step [60450/68337], Loss: 5.0292\n",
      "Epoch [3/10], Step [60525/68337], Loss: 5.0904\n",
      "Epoch [3/10], Step [60600/68337], Loss: 5.1802\n",
      "Epoch [3/10], Step [60675/68337], Loss: 5.0449\n",
      "Epoch [3/10], Step [60750/68337], Loss: 5.2163\n",
      "Epoch [3/10], Step [60825/68337], Loss: 5.0444\n",
      "Epoch [3/10], Step [60900/68337], Loss: 5.2208\n",
      "Epoch [3/10], Step [60975/68337], Loss: 5.1344\n",
      "Epoch [3/10], Step [61050/68337], Loss: 5.2541\n",
      "Epoch [3/10], Step [61125/68337], Loss: 5.0006\n",
      "Epoch [3/10], Step [61200/68337], Loss: 5.2059\n",
      "Epoch [3/10], Step [61275/68337], Loss: 5.1095\n",
      "Epoch [3/10], Step [61350/68337], Loss: 5.3381\n",
      "Epoch [3/10], Step [61425/68337], Loss: 5.2418\n",
      "Epoch [3/10], Step [61500/68337], Loss: 5.1881\n",
      "Epoch [3/10], Step [61575/68337], Loss: 5.2314\n",
      "Epoch [3/10], Step [61650/68337], Loss: 4.9482\n",
      "Epoch [3/10], Step [61725/68337], Loss: 5.3718\n",
      "Epoch [3/10], Step [61800/68337], Loss: 4.9657\n",
      "Epoch [3/10], Step [61875/68337], Loss: 5.3138\n",
      "Epoch [3/10], Step [61950/68337], Loss: 5.1605\n",
      "Epoch [3/10], Step [62025/68337], Loss: 5.1989\n",
      "Epoch [3/10], Step [62100/68337], Loss: 5.1999\n",
      "Epoch [3/10], Step [62175/68337], Loss: 5.4928\n",
      "Epoch [3/10], Step [62250/68337], Loss: 5.1828\n",
      "Epoch [3/10], Step [62325/68337], Loss: 5.0199\n",
      "Epoch [3/10], Step [62400/68337], Loss: 5.2176\n",
      "Epoch [3/10], Step [62475/68337], Loss: 5.1398\n",
      "Epoch [3/10], Step [62550/68337], Loss: 5.0972\n",
      "Epoch [3/10], Step [62625/68337], Loss: 5.1538\n",
      "Epoch [3/10], Step [62700/68337], Loss: 5.0759\n",
      "Epoch [3/10], Step [62775/68337], Loss: 5.0223\n",
      "Epoch [3/10], Step [62850/68337], Loss: 5.4367\n",
      "Epoch [3/10], Step [62925/68337], Loss: 5.0101\n",
      "Epoch [3/10], Step [63000/68337], Loss: 5.2477\n",
      "Epoch [3/10], Step [63075/68337], Loss: 5.1503\n",
      "Epoch [3/10], Step [63150/68337], Loss: 5.2568\n",
      "Epoch [3/10], Step [63225/68337], Loss: 4.9536\n",
      "Epoch [3/10], Step [63300/68337], Loss: 5.1542\n",
      "Epoch [3/10], Step [63375/68337], Loss: 5.3262\n",
      "Epoch [3/10], Step [63450/68337], Loss: 5.0308\n",
      "Epoch [3/10], Step [63525/68337], Loss: 4.9287\n",
      "Epoch [3/10], Step [63600/68337], Loss: 5.2673\n",
      "Epoch [3/10], Step [63675/68337], Loss: 5.0655\n",
      "Epoch [3/10], Step [63750/68337], Loss: 5.1705\n",
      "Epoch [3/10], Step [63825/68337], Loss: 5.0893\n",
      "Epoch [3/10], Step [63900/68337], Loss: 4.8637\n",
      "Epoch [3/10], Step [63975/68337], Loss: 5.1665\n",
      "Epoch [3/10], Step [64050/68337], Loss: 5.1213\n",
      "Epoch [3/10], Step [64125/68337], Loss: 5.1028\n",
      "Epoch [3/10], Step [64200/68337], Loss: 5.2860\n",
      "Epoch [3/10], Step [64275/68337], Loss: 5.0062\n",
      "Epoch [3/10], Step [64350/68337], Loss: 5.2280\n",
      "Epoch [3/10], Step [64425/68337], Loss: 5.1291\n",
      "Epoch [3/10], Step [64500/68337], Loss: 5.1321\n",
      "Epoch [3/10], Step [64575/68337], Loss: 5.0866\n",
      "Epoch [3/10], Step [64650/68337], Loss: 5.0777\n",
      "Epoch [3/10], Step [64725/68337], Loss: 5.1491\n",
      "Epoch [3/10], Step [64800/68337], Loss: 5.2429\n",
      "Epoch [3/10], Step [64875/68337], Loss: 5.2068\n",
      "Epoch [3/10], Step [64950/68337], Loss: 5.1789\n",
      "Epoch [3/10], Step [65025/68337], Loss: 5.2130\n",
      "Epoch [3/10], Step [65100/68337], Loss: 5.1879\n",
      "Epoch [3/10], Step [65175/68337], Loss: 5.1231\n",
      "Epoch [3/10], Step [65250/68337], Loss: 5.1813\n",
      "Epoch [3/10], Step [65325/68337], Loss: 4.9236\n",
      "Epoch [3/10], Step [65400/68337], Loss: 5.2081\n",
      "Epoch [3/10], Step [65475/68337], Loss: 5.1048\n",
      "Epoch [3/10], Step [65550/68337], Loss: 5.3067\n",
      "Epoch [3/10], Step [65625/68337], Loss: 5.1694\n",
      "Epoch [3/10], Step [65700/68337], Loss: 5.1562\n",
      "Epoch [3/10], Step [65775/68337], Loss: 5.2405\n",
      "Epoch [3/10], Step [65850/68337], Loss: 5.0545\n",
      "Epoch [3/10], Step [65925/68337], Loss: 5.1035\n",
      "Epoch [3/10], Step [66000/68337], Loss: 5.0983\n",
      "Epoch [3/10], Step [66075/68337], Loss: 5.1288\n",
      "Epoch [3/10], Step [66150/68337], Loss: 4.8768\n",
      "Epoch [3/10], Step [66225/68337], Loss: 5.1259\n",
      "Epoch [3/10], Step [66300/68337], Loss: 5.2489\n",
      "Epoch [3/10], Step [66375/68337], Loss: 5.2935\n",
      "Epoch [3/10], Step [66450/68337], Loss: 5.0729\n",
      "Epoch [3/10], Step [66525/68337], Loss: 5.0885\n",
      "Epoch [3/10], Step [66600/68337], Loss: 5.0126\n",
      "Epoch [3/10], Step [66675/68337], Loss: 5.1561\n",
      "Epoch [3/10], Step [66750/68337], Loss: 5.3183\n",
      "Epoch [3/10], Step [66825/68337], Loss: 5.1129\n",
      "Epoch [3/10], Step [66900/68337], Loss: 5.0739\n",
      "Epoch [3/10], Step [66975/68337], Loss: 5.3408\n",
      "Epoch [3/10], Step [67050/68337], Loss: 5.2178\n",
      "Epoch [3/10], Step [67125/68337], Loss: 4.9661\n",
      "Epoch [3/10], Step [67200/68337], Loss: 5.1983\n",
      "Epoch [3/10], Step [67275/68337], Loss: 5.3051\n",
      "Epoch [3/10], Step [67350/68337], Loss: 5.2826\n",
      "Epoch [3/10], Step [67425/68337], Loss: 5.1373\n",
      "Epoch [3/10], Step [67500/68337], Loss: 5.3029\n",
      "Epoch [3/10], Step [67575/68337], Loss: 5.1064\n",
      "Epoch [3/10], Step [67650/68337], Loss: 5.0582\n",
      "Epoch [3/10], Step [67725/68337], Loss: 5.2748\n",
      "Epoch [3/10], Step [67800/68337], Loss: 5.2968\n",
      "Epoch [3/10], Step [67875/68337], Loss: 5.3107\n",
      "Epoch [3/10], Step [67950/68337], Loss: 5.3722\n",
      "Epoch [3/10], Step [68025/68337], Loss: 5.0852\n",
      "Epoch [3/10], Step [68100/68337], Loss: 5.0221\n",
      "Epoch [3/10], Step [68175/68337], Loss: 5.3826\n",
      "Epoch [3/10], Step [68250/68337], Loss: 4.9520\n",
      "Epoch [3/10], Step [68325/68337], Loss: 5.2005\n",
      "Epoch [3/10] Average Loss: 5.1712, Perplexity: 176.13\n",
      "Epoch [4/10], Step [0/68337], Loss: 4.9593\n",
      "Validation perplexity: 134.39552964305918\n",
      "Epoch [4/10], Step [75/68337], Loss: 5.0227\n",
      "Epoch [4/10], Step [150/68337], Loss: 5.0774\n",
      "Epoch [4/10], Step [225/68337], Loss: 5.0292\n",
      "Epoch [4/10], Step [300/68337], Loss: 5.0795\n",
      "Epoch [4/10], Step [375/68337], Loss: 5.0568\n",
      "Epoch [4/10], Step [450/68337], Loss: 5.1017\n",
      "Epoch [4/10], Step [525/68337], Loss: 5.1301\n",
      "Epoch [4/10], Step [600/68337], Loss: 4.8150\n",
      "Epoch [4/10], Step [675/68337], Loss: 5.2315\n",
      "Epoch [4/10], Step [750/68337], Loss: 5.3802\n",
      "Epoch [4/10], Step [825/68337], Loss: 5.2175\n",
      "Epoch [4/10], Step [900/68337], Loss: 5.1737\n",
      "Epoch [4/10], Step [975/68337], Loss: 4.9021\n",
      "Epoch [4/10], Step [1050/68337], Loss: 5.1607\n",
      "Epoch [4/10], Step [1125/68337], Loss: 5.0392\n",
      "Epoch [4/10], Step [1200/68337], Loss: 5.3402\n",
      "Epoch [4/10], Step [1275/68337], Loss: 5.2785\n",
      "Epoch [4/10], Step [1350/68337], Loss: 5.1575\n",
      "Epoch [4/10], Step [1425/68337], Loss: 5.1292\n",
      "Epoch [4/10], Step [1500/68337], Loss: 5.1954\n",
      "Epoch [4/10], Step [1575/68337], Loss: 5.3651\n",
      "Epoch [4/10], Step [1650/68337], Loss: 4.9924\n",
      "Epoch [4/10], Step [1725/68337], Loss: 5.1174\n",
      "Epoch [4/10], Step [1800/68337], Loss: 5.0889\n",
      "Epoch [4/10], Step [1875/68337], Loss: 5.1255\n",
      "Epoch [4/10], Step [1950/68337], Loss: 5.0116\n",
      "Epoch [4/10], Step [2025/68337], Loss: 5.2896\n",
      "Epoch [4/10], Step [2100/68337], Loss: 5.1009\n",
      "Epoch [4/10], Step [2175/68337], Loss: 5.2900\n",
      "Epoch [4/10], Step [2250/68337], Loss: 5.0550\n",
      "Epoch [4/10], Step [2325/68337], Loss: 5.1517\n",
      "Epoch [4/10], Step [2400/68337], Loss: 5.2195\n",
      "Epoch [4/10], Step [2475/68337], Loss: 5.1793\n",
      "Epoch [4/10], Step [2550/68337], Loss: 5.0714\n",
      "Epoch [4/10], Step [2625/68337], Loss: 5.2322\n",
      "Epoch [4/10], Step [2700/68337], Loss: 5.0798\n",
      "Epoch [4/10], Step [2775/68337], Loss: 4.9499\n",
      "Epoch [4/10], Step [2850/68337], Loss: 5.2785\n",
      "Epoch [4/10], Step [2925/68337], Loss: 5.0233\n",
      "Epoch [4/10], Step [3000/68337], Loss: 5.0750\n",
      "Epoch [4/10], Step [3075/68337], Loss: 4.9280\n",
      "Epoch [4/10], Step [3150/68337], Loss: 5.1688\n",
      "Epoch [4/10], Step [3225/68337], Loss: 4.9945\n",
      "Epoch [4/10], Step [3300/68337], Loss: 5.2111\n",
      "Epoch [4/10], Step [3375/68337], Loss: 5.2172\n",
      "Epoch [4/10], Step [3450/68337], Loss: 5.1171\n",
      "Epoch [4/10], Step [3525/68337], Loss: 4.9854\n",
      "Epoch [4/10], Step [3600/68337], Loss: 4.9972\n",
      "Epoch [4/10], Step [3675/68337], Loss: 5.2132\n",
      "Epoch [4/10], Step [3750/68337], Loss: 5.0987\n",
      "Epoch [4/10], Step [3825/68337], Loss: 5.0989\n",
      "Epoch [4/10], Step [3900/68337], Loss: 5.0755\n",
      "Epoch [4/10], Step [3975/68337], Loss: 5.1693\n",
      "Epoch [4/10], Step [4050/68337], Loss: 5.2738\n",
      "Epoch [4/10], Step [4125/68337], Loss: 5.1549\n",
      "Epoch [4/10], Step [4200/68337], Loss: 5.1100\n",
      "Epoch [4/10], Step [4275/68337], Loss: 5.1669\n",
      "Epoch [4/10], Step [4350/68337], Loss: 4.9258\n",
      "Epoch [4/10], Step [4425/68337], Loss: 4.9116\n",
      "Epoch [4/10], Step [4500/68337], Loss: 5.1401\n",
      "Epoch [4/10], Step [4575/68337], Loss: 5.1534\n",
      "Epoch [4/10], Step [4650/68337], Loss: 5.1812\n",
      "Epoch [4/10], Step [4725/68337], Loss: 5.0794\n",
      "Epoch [4/10], Step [4800/68337], Loss: 5.1462\n",
      "Epoch [4/10], Step [4875/68337], Loss: 5.3853\n",
      "Epoch [4/10], Step [4950/68337], Loss: 5.3111\n",
      "Epoch [4/10], Step [5025/68337], Loss: 5.0963\n",
      "Epoch [4/10], Step [5100/68337], Loss: 5.1273\n",
      "Epoch [4/10], Step [5175/68337], Loss: 5.1154\n",
      "Epoch [4/10], Step [5250/68337], Loss: 5.1036\n",
      "Epoch [4/10], Step [5325/68337], Loss: 5.1808\n",
      "Epoch [4/10], Step [5400/68337], Loss: 5.1060\n",
      "Epoch [4/10], Step [5475/68337], Loss: 5.1420\n",
      "Epoch [4/10], Step [5550/68337], Loss: 5.0006\n",
      "Epoch [4/10], Step [5625/68337], Loss: 5.0749\n",
      "Epoch [4/10], Step [5700/68337], Loss: 5.2143\n",
      "Epoch [4/10], Step [5775/68337], Loss: 5.1103\n",
      "Epoch [4/10], Step [5850/68337], Loss: 5.2930\n",
      "Epoch [4/10], Step [5925/68337], Loss: 5.2713\n",
      "Epoch [4/10], Step [6000/68337], Loss: 5.1541\n",
      "Epoch [4/10], Step [6075/68337], Loss: 5.1061\n",
      "Epoch [4/10], Step [6150/68337], Loss: 5.2346\n",
      "Epoch [4/10], Step [6225/68337], Loss: 5.2118\n",
      "Epoch [4/10], Step [6300/68337], Loss: 4.7755\n",
      "Epoch [4/10], Step [6375/68337], Loss: 5.0984\n",
      "Epoch [4/10], Step [6450/68337], Loss: 4.9743\n",
      "Epoch [4/10], Step [6525/68337], Loss: 5.0394\n",
      "Epoch [4/10], Step [6600/68337], Loss: 5.0263\n",
      "Epoch [4/10], Step [6675/68337], Loss: 5.0979\n",
      "Epoch [4/10], Step [6750/68337], Loss: 5.0247\n",
      "Epoch [4/10], Step [6825/68337], Loss: 4.9820\n",
      "Epoch [4/10], Step [6900/68337], Loss: 5.1510\n",
      "Epoch [4/10], Step [6975/68337], Loss: 5.3661\n",
      "Epoch [4/10], Step [7050/68337], Loss: 5.2668\n",
      "Epoch [4/10], Step [7125/68337], Loss: 5.0170\n",
      "Epoch [4/10], Step [7200/68337], Loss: 5.0081\n",
      "Epoch [4/10], Step [7275/68337], Loss: 5.2296\n",
      "Epoch [4/10], Step [7350/68337], Loss: 4.9786\n",
      "Epoch [4/10], Step [7425/68337], Loss: 5.2600\n",
      "Epoch [4/10], Step [7500/68337], Loss: 4.9864\n",
      "Epoch [4/10], Step [7575/68337], Loss: 5.0894\n",
      "Epoch [4/10], Step [7650/68337], Loss: 5.2203\n",
      "Epoch [4/10], Step [7725/68337], Loss: 5.1712\n",
      "Epoch [4/10], Step [7800/68337], Loss: 5.2953\n",
      "Epoch [4/10], Step [7875/68337], Loss: 5.1176\n",
      "Epoch [4/10], Step [7950/68337], Loss: 5.2585\n",
      "Epoch [4/10], Step [8025/68337], Loss: 5.2590\n",
      "Epoch [4/10], Step [8100/68337], Loss: 5.0950\n",
      "Epoch [4/10], Step [8175/68337], Loss: 5.4227\n",
      "Epoch [4/10], Step [8250/68337], Loss: 5.1501\n",
      "Epoch [4/10], Step [8325/68337], Loss: 5.1395\n",
      "Epoch [4/10], Step [8400/68337], Loss: 5.1991\n",
      "Epoch [4/10], Step [8475/68337], Loss: 4.9575\n",
      "Epoch [4/10], Step [8550/68337], Loss: 5.1774\n",
      "Epoch [4/10], Step [8625/68337], Loss: 5.1868\n",
      "Epoch [4/10], Step [8700/68337], Loss: 5.2411\n",
      "Epoch [4/10], Step [8775/68337], Loss: 5.0497\n",
      "Epoch [4/10], Step [8850/68337], Loss: 4.9534\n",
      "Epoch [4/10], Step [8925/68337], Loss: 5.1695\n",
      "Epoch [4/10], Step [9000/68337], Loss: 5.3401\n",
      "Epoch [4/10], Step [9075/68337], Loss: 5.0252\n",
      "Epoch [4/10], Step [9150/68337], Loss: 5.2639\n",
      "Epoch [4/10], Step [9225/68337], Loss: 5.0911\n",
      "Epoch [4/10], Step [9300/68337], Loss: 4.9682\n",
      "Epoch [4/10], Step [9375/68337], Loss: 5.0946\n",
      "Epoch [4/10], Step [9450/68337], Loss: 5.2210\n",
      "Epoch [4/10], Step [9525/68337], Loss: 5.3647\n",
      "Epoch [4/10], Step [9600/68337], Loss: 5.2140\n",
      "Epoch [4/10], Step [9675/68337], Loss: 5.0091\n",
      "Epoch [4/10], Step [9750/68337], Loss: 5.2379\n",
      "Epoch [4/10], Step [9825/68337], Loss: 5.0831\n",
      "Epoch [4/10], Step [9900/68337], Loss: 5.0557\n",
      "Epoch [4/10], Step [9975/68337], Loss: 5.1032\n",
      "Validation perplexity: 133.04104534951668\n",
      "Epoch [4/10], Step [10050/68337], Loss: 5.0713\n",
      "Epoch [4/10], Step [10125/68337], Loss: 5.0856\n",
      "Epoch [4/10], Step [10200/68337], Loss: 5.2470\n",
      "Epoch [4/10], Step [10275/68337], Loss: 5.3331\n",
      "Epoch [4/10], Step [10350/68337], Loss: 4.9925\n",
      "Epoch [4/10], Step [10425/68337], Loss: 5.0194\n",
      "Epoch [4/10], Step [10500/68337], Loss: 5.0900\n",
      "Epoch [4/10], Step [10575/68337], Loss: 5.1478\n",
      "Epoch [4/10], Step [10650/68337], Loss: 5.0260\n",
      "Epoch [4/10], Step [10725/68337], Loss: 5.2912\n",
      "Epoch [4/10], Step [10800/68337], Loss: 5.0239\n",
      "Epoch [4/10], Step [10875/68337], Loss: 5.0919\n",
      "Epoch [4/10], Step [10950/68337], Loss: 4.9710\n",
      "Epoch [4/10], Step [11025/68337], Loss: 5.1778\n",
      "Epoch [4/10], Step [11100/68337], Loss: 5.1589\n",
      "Epoch [4/10], Step [11175/68337], Loss: 5.1987\n",
      "Epoch [4/10], Step [11250/68337], Loss: 5.2086\n",
      "Epoch [4/10], Step [11325/68337], Loss: 5.0608\n",
      "Epoch [4/10], Step [11400/68337], Loss: 5.0891\n",
      "Epoch [4/10], Step [11475/68337], Loss: 5.1458\n",
      "Epoch [4/10], Step [11550/68337], Loss: 5.0741\n",
      "Epoch [4/10], Step [11625/68337], Loss: 5.2823\n",
      "Epoch [4/10], Step [11700/68337], Loss: 5.2000\n",
      "Epoch [4/10], Step [11775/68337], Loss: 4.9299\n",
      "Epoch [4/10], Step [11850/68337], Loss: 5.1371\n",
      "Epoch [4/10], Step [11925/68337], Loss: 5.0663\n",
      "Epoch [4/10], Step [12000/68337], Loss: 5.2222\n",
      "Epoch [4/10], Step [12075/68337], Loss: 5.2761\n",
      "Epoch [4/10], Step [12150/68337], Loss: 5.1581\n",
      "Epoch [4/10], Step [12225/68337], Loss: 5.2009\n",
      "Epoch [4/10], Step [12300/68337], Loss: 5.1488\n",
      "Epoch [4/10], Step [12375/68337], Loss: 5.1863\n",
      "Epoch [4/10], Step [12450/68337], Loss: 5.0910\n",
      "Epoch [4/10], Step [12525/68337], Loss: 5.0504\n",
      "Epoch [4/10], Step [12600/68337], Loss: 4.8934\n",
      "Epoch [4/10], Step [12675/68337], Loss: 5.0008\n",
      "Epoch [4/10], Step [12750/68337], Loss: 5.0751\n",
      "Epoch [4/10], Step [12825/68337], Loss: 5.2573\n",
      "Epoch [4/10], Step [12900/68337], Loss: 5.0711\n",
      "Epoch [4/10], Step [12975/68337], Loss: 5.4645\n",
      "Epoch [4/10], Step [13050/68337], Loss: 5.2171\n",
      "Epoch [4/10], Step [13125/68337], Loss: 5.1794\n",
      "Epoch [4/10], Step [13200/68337], Loss: 5.1216\n",
      "Epoch [4/10], Step [13275/68337], Loss: 5.0679\n",
      "Epoch [4/10], Step [13350/68337], Loss: 5.3418\n",
      "Epoch [4/10], Step [13425/68337], Loss: 5.1165\n",
      "Epoch [4/10], Step [13500/68337], Loss: 4.9220\n",
      "Epoch [4/10], Step [13575/68337], Loss: 4.9305\n",
      "Epoch [4/10], Step [13650/68337], Loss: 5.3631\n",
      "Epoch [4/10], Step [13725/68337], Loss: 5.0923\n",
      "Epoch [4/10], Step [13800/68337], Loss: 5.1940\n",
      "Epoch [4/10], Step [13875/68337], Loss: 5.1677\n",
      "Epoch [4/10], Step [13950/68337], Loss: 5.1019\n",
      "Epoch [4/10], Step [14025/68337], Loss: 5.0253\n",
      "Epoch [4/10], Step [14100/68337], Loss: 5.0920\n",
      "Epoch [4/10], Step [14175/68337], Loss: 5.3360\n",
      "Epoch [4/10], Step [14250/68337], Loss: 5.2631\n",
      "Epoch [4/10], Step [14325/68337], Loss: 5.0376\n",
      "Epoch [4/10], Step [14400/68337], Loss: 5.3329\n",
      "Epoch [4/10], Step [14475/68337], Loss: 5.1374\n",
      "Epoch [4/10], Step [14550/68337], Loss: 5.1844\n",
      "Epoch [4/10], Step [14625/68337], Loss: 5.2698\n",
      "Epoch [4/10], Step [14700/68337], Loss: 4.9887\n",
      "Epoch [4/10], Step [14775/68337], Loss: 4.9393\n",
      "Epoch [4/10], Step [14850/68337], Loss: 5.2997\n",
      "Epoch [4/10], Step [14925/68337], Loss: 5.2725\n",
      "Epoch [4/10], Step [15000/68337], Loss: 5.2004\n",
      "Epoch [4/10], Step [15075/68337], Loss: 5.0415\n",
      "Epoch [4/10], Step [15150/68337], Loss: 5.1338\n",
      "Epoch [4/10], Step [15225/68337], Loss: 5.0701\n",
      "Epoch [4/10], Step [15300/68337], Loss: 5.2718\n",
      "Epoch [4/10], Step [15375/68337], Loss: 5.0765\n",
      "Epoch [4/10], Step [15450/68337], Loss: 5.2907\n",
      "Epoch [4/10], Step [15525/68337], Loss: 5.2094\n",
      "Epoch [4/10], Step [15600/68337], Loss: 5.2155\n",
      "Epoch [4/10], Step [15675/68337], Loss: 5.0642\n",
      "Epoch [4/10], Step [15750/68337], Loss: 5.1450\n",
      "Epoch [4/10], Step [15825/68337], Loss: 5.1922\n",
      "Epoch [4/10], Step [15900/68337], Loss: 5.0815\n",
      "Epoch [4/10], Step [15975/68337], Loss: 5.2236\n",
      "Epoch [4/10], Step [16050/68337], Loss: 5.0034\n",
      "Epoch [4/10], Step [16125/68337], Loss: 5.1270\n",
      "Epoch [4/10], Step [16200/68337], Loss: 5.1281\n",
      "Epoch [4/10], Step [16275/68337], Loss: 5.0640\n",
      "Epoch [4/10], Step [16350/68337], Loss: 5.2392\n",
      "Epoch [4/10], Step [16425/68337], Loss: 5.1761\n",
      "Epoch [4/10], Step [16500/68337], Loss: 5.0801\n",
      "Epoch [4/10], Step [16575/68337], Loss: 5.0065\n",
      "Epoch [4/10], Step [16650/68337], Loss: 5.3975\n",
      "Epoch [4/10], Step [16725/68337], Loss: 5.1627\n",
      "Epoch [4/10], Step [16800/68337], Loss: 5.0822\n",
      "Epoch [4/10], Step [16875/68337], Loss: 5.0289\n",
      "Epoch [4/10], Step [16950/68337], Loss: 5.2127\n",
      "Epoch [4/10], Step [17025/68337], Loss: 4.9890\n",
      "Epoch [4/10], Step [17100/68337], Loss: 5.3237\n",
      "Epoch [4/10], Step [17175/68337], Loss: 5.0112\n",
      "Epoch [4/10], Step [17250/68337], Loss: 5.1681\n",
      "Epoch [4/10], Step [17325/68337], Loss: 5.0404\n",
      "Epoch [4/10], Step [17400/68337], Loss: 4.8510\n",
      "Epoch [4/10], Step [17475/68337], Loss: 5.1880\n",
      "Epoch [4/10], Step [17550/68337], Loss: 5.3124\n",
      "Epoch [4/10], Step [17625/68337], Loss: 5.3739\n",
      "Epoch [4/10], Step [17700/68337], Loss: 5.1101\n",
      "Epoch [4/10], Step [17775/68337], Loss: 5.3499\n",
      "Epoch [4/10], Step [17850/68337], Loss: 4.9407\n",
      "Epoch [4/10], Step [17925/68337], Loss: 5.2363\n",
      "Epoch [4/10], Step [18000/68337], Loss: 5.0928\n",
      "Epoch [4/10], Step [18075/68337], Loss: 4.9972\n",
      "Epoch [4/10], Step [18150/68337], Loss: 5.1023\n",
      "Epoch [4/10], Step [18225/68337], Loss: 5.2433\n",
      "Epoch [4/10], Step [18300/68337], Loss: 5.0500\n",
      "Epoch [4/10], Step [18375/68337], Loss: 5.0495\n",
      "Epoch [4/10], Step [18450/68337], Loss: 5.1143\n",
      "Epoch [4/10], Step [18525/68337], Loss: 5.1467\n",
      "Epoch [4/10], Step [18600/68337], Loss: 5.2699\n",
      "Epoch [4/10], Step [18675/68337], Loss: 5.2335\n",
      "Epoch [4/10], Step [18750/68337], Loss: 5.0444\n",
      "Epoch [4/10], Step [18825/68337], Loss: 5.1706\n",
      "Epoch [4/10], Step [18900/68337], Loss: 5.0183\n",
      "Epoch [4/10], Step [18975/68337], Loss: 5.2226\n",
      "Epoch [4/10], Step [19050/68337], Loss: 5.2442\n",
      "Epoch [4/10], Step [19125/68337], Loss: 4.9835\n",
      "Epoch [4/10], Step [19200/68337], Loss: 5.1968\n",
      "Epoch [4/10], Step [19275/68337], Loss: 5.1989\n",
      "Epoch [4/10], Step [19350/68337], Loss: 5.1330\n",
      "Epoch [4/10], Step [19425/68337], Loss: 5.1414\n",
      "Epoch [4/10], Step [19500/68337], Loss: 5.1604\n",
      "Epoch [4/10], Step [19575/68337], Loss: 5.2360\n",
      "Epoch [4/10], Step [19650/68337], Loss: 4.8476\n",
      "Epoch [4/10], Step [19725/68337], Loss: 5.1083\n",
      "Epoch [4/10], Step [19800/68337], Loss: 5.1386\n",
      "Epoch [4/10], Step [19875/68337], Loss: 5.2493\n",
      "Epoch [4/10], Step [19950/68337], Loss: 5.1962\n",
      "Validation perplexity: 132.69029353703\n",
      "Epoch [4/10], Step [20025/68337], Loss: 5.2102\n",
      "Epoch [4/10], Step [20100/68337], Loss: 5.2461\n",
      "Epoch [4/10], Step [20175/68337], Loss: 5.3176\n",
      "Epoch [4/10], Step [20250/68337], Loss: 5.3637\n",
      "Epoch [4/10], Step [20325/68337], Loss: 5.1138\n",
      "Epoch [4/10], Step [20400/68337], Loss: 5.1898\n",
      "Epoch [4/10], Step [20475/68337], Loss: 5.2710\n",
      "Epoch [4/10], Step [20550/68337], Loss: 5.2593\n",
      "Epoch [4/10], Step [20625/68337], Loss: 5.3488\n",
      "Epoch [4/10], Step [20700/68337], Loss: 5.1010\n",
      "Epoch [4/10], Step [20775/68337], Loss: 5.1886\n",
      "Epoch [4/10], Step [20850/68337], Loss: 5.2727\n",
      "Epoch [4/10], Step [20925/68337], Loss: 5.0177\n",
      "Epoch [4/10], Step [21000/68337], Loss: 5.0962\n",
      "Epoch [4/10], Step [21075/68337], Loss: 5.2738\n",
      "Epoch [4/10], Step [21150/68337], Loss: 5.1307\n",
      "Epoch [4/10], Step [21225/68337], Loss: 5.0786\n",
      "Epoch [4/10], Step [21300/68337], Loss: 5.2473\n",
      "Epoch [4/10], Step [21375/68337], Loss: 5.1236\n",
      "Epoch [4/10], Step [21450/68337], Loss: 5.0957\n",
      "Epoch [4/10], Step [21525/68337], Loss: 5.1000\n",
      "Epoch [4/10], Step [21600/68337], Loss: 5.2829\n",
      "Epoch [4/10], Step [21675/68337], Loss: 5.0198\n",
      "Epoch [4/10], Step [21750/68337], Loss: 5.1824\n",
      "Epoch [4/10], Step [21825/68337], Loss: 5.2539\n",
      "Epoch [4/10], Step [21900/68337], Loss: 5.0968\n",
      "Epoch [4/10], Step [21975/68337], Loss: 5.3294\n",
      "Epoch [4/10], Step [22050/68337], Loss: 4.9535\n",
      "Epoch [4/10], Step [22125/68337], Loss: 5.0262\n",
      "Epoch [4/10], Step [22200/68337], Loss: 5.2583\n",
      "Epoch [4/10], Step [22275/68337], Loss: 4.9531\n",
      "Epoch [4/10], Step [22350/68337], Loss: 5.1262\n",
      "Epoch [4/10], Step [22425/68337], Loss: 5.3689\n",
      "Epoch [4/10], Step [22500/68337], Loss: 5.0871\n",
      "Epoch [4/10], Step [22575/68337], Loss: 5.0633\n",
      "Epoch [4/10], Step [22650/68337], Loss: 5.0609\n",
      "Epoch [4/10], Step [22725/68337], Loss: 5.2375\n",
      "Epoch [4/10], Step [22800/68337], Loss: 5.2577\n",
      "Epoch [4/10], Step [22875/68337], Loss: 5.2597\n",
      "Epoch [4/10], Step [22950/68337], Loss: 5.1047\n",
      "Epoch [4/10], Step [23025/68337], Loss: 5.2639\n",
      "Epoch [4/10], Step [23100/68337], Loss: 5.1676\n",
      "Epoch [4/10], Step [23175/68337], Loss: 5.1662\n",
      "Epoch [4/10], Step [23250/68337], Loss: 5.1052\n",
      "Epoch [4/10], Step [23325/68337], Loss: 5.3227\n",
      "Epoch [4/10], Step [23400/68337], Loss: 5.1566\n",
      "Epoch [4/10], Step [23475/68337], Loss: 5.1155\n",
      "Epoch [4/10], Step [23550/68337], Loss: 5.1470\n",
      "Epoch [4/10], Step [23625/68337], Loss: 5.1569\n",
      "Epoch [4/10], Step [23700/68337], Loss: 5.1896\n",
      "Epoch [4/10], Step [23775/68337], Loss: 5.1496\n",
      "Epoch [4/10], Step [23850/68337], Loss: 5.3686\n",
      "Epoch [4/10], Step [23925/68337], Loss: 5.1496\n",
      "Epoch [4/10], Step [24000/68337], Loss: 5.2265\n",
      "Epoch [4/10], Step [24075/68337], Loss: 5.2532\n",
      "Epoch [4/10], Step [24150/68337], Loss: 5.4532\n",
      "Epoch [4/10], Step [24225/68337], Loss: 5.1813\n",
      "Epoch [4/10], Step [24300/68337], Loss: 5.1072\n",
      "Epoch [4/10], Step [24375/68337], Loss: 5.1805\n",
      "Epoch [4/10], Step [24450/68337], Loss: 5.1402\n",
      "Epoch [4/10], Step [24525/68337], Loss: 5.0202\n",
      "Epoch [4/10], Step [24600/68337], Loss: 5.2913\n",
      "Epoch [4/10], Step [24675/68337], Loss: 5.0989\n",
      "Epoch [4/10], Step [24750/68337], Loss: 5.3303\n",
      "Epoch [4/10], Step [24825/68337], Loss: 5.0387\n",
      "Epoch [4/10], Step [24900/68337], Loss: 5.3473\n",
      "Epoch [4/10], Step [24975/68337], Loss: 5.0871\n",
      "Epoch [4/10], Step [25050/68337], Loss: 5.0582\n",
      "Epoch [4/10], Step [25125/68337], Loss: 5.0041\n",
      "Epoch [4/10], Step [25200/68337], Loss: 5.3662\n",
      "Epoch [4/10], Step [25275/68337], Loss: 5.3373\n",
      "Epoch [4/10], Step [25350/68337], Loss: 5.1791\n",
      "Epoch [4/10], Step [25425/68337], Loss: 5.2307\n",
      "Epoch [4/10], Step [25500/68337], Loss: 5.1161\n",
      "Epoch [4/10], Step [25575/68337], Loss: 5.2064\n",
      "Epoch [4/10], Step [25650/68337], Loss: 5.3041\n",
      "Epoch [4/10], Step [25725/68337], Loss: 5.1277\n",
      "Epoch [4/10], Step [25800/68337], Loss: 5.0074\n",
      "Epoch [4/10], Step [25875/68337], Loss: 5.2744\n",
      "Epoch [4/10], Step [25950/68337], Loss: 5.0979\n",
      "Epoch [4/10], Step [26025/68337], Loss: 4.8347\n",
      "Epoch [4/10], Step [26100/68337], Loss: 5.3176\n",
      "Epoch [4/10], Step [26175/68337], Loss: 5.2508\n",
      "Epoch [4/10], Step [26250/68337], Loss: 5.2671\n",
      "Epoch [4/10], Step [26325/68337], Loss: 5.0736\n",
      "Epoch [4/10], Step [26400/68337], Loss: 5.2142\n",
      "Epoch [4/10], Step [26475/68337], Loss: 5.3228\n",
      "Epoch [4/10], Step [26550/68337], Loss: 5.1473\n",
      "Epoch [4/10], Step [26625/68337], Loss: 5.1951\n",
      "Epoch [4/10], Step [26700/68337], Loss: 5.0506\n",
      "Epoch [4/10], Step [26775/68337], Loss: 5.2432\n",
      "Epoch [4/10], Step [26850/68337], Loss: 5.3450\n",
      "Epoch [4/10], Step [26925/68337], Loss: 5.1401\n",
      "Epoch [4/10], Step [27000/68337], Loss: 5.1882\n",
      "Epoch [4/10], Step [27075/68337], Loss: 5.1399\n",
      "Epoch [4/10], Step [27150/68337], Loss: 5.0879\n",
      "Epoch [4/10], Step [27225/68337], Loss: 5.0984\n",
      "Epoch [4/10], Step [27300/68337], Loss: 5.0692\n",
      "Epoch [4/10], Step [27375/68337], Loss: 5.2039\n",
      "Epoch [4/10], Step [27450/68337], Loss: 5.0519\n",
      "Epoch [4/10], Step [27525/68337], Loss: 4.9110\n",
      "Epoch [4/10], Step [27600/68337], Loss: 5.1633\n",
      "Epoch [4/10], Step [27675/68337], Loss: 5.1729\n",
      "Epoch [4/10], Step [27750/68337], Loss: 5.0274\n",
      "Epoch [4/10], Step [27825/68337], Loss: 5.0939\n",
      "Epoch [4/10], Step [27900/68337], Loss: 5.3390\n",
      "Epoch [4/10], Step [27975/68337], Loss: 4.9658\n",
      "Epoch [4/10], Step [28050/68337], Loss: 5.0323\n",
      "Epoch [4/10], Step [28125/68337], Loss: 5.2832\n",
      "Epoch [4/10], Step [28200/68337], Loss: 5.0907\n",
      "Epoch [4/10], Step [28275/68337], Loss: 5.1869\n",
      "Epoch [4/10], Step [28350/68337], Loss: 5.1595\n",
      "Epoch [4/10], Step [28425/68337], Loss: 5.0993\n",
      "Epoch [4/10], Step [28500/68337], Loss: 5.0141\n",
      "Epoch [4/10], Step [28575/68337], Loss: 5.1870\n",
      "Epoch [4/10], Step [28650/68337], Loss: 5.2913\n",
      "Epoch [4/10], Step [28725/68337], Loss: 5.1076\n",
      "Epoch [4/10], Step [28800/68337], Loss: 4.9727\n",
      "Epoch [4/10], Step [28875/68337], Loss: 5.0396\n",
      "Epoch [4/10], Step [28950/68337], Loss: 5.1968\n",
      "Epoch [4/10], Step [29025/68337], Loss: 5.3040\n",
      "Epoch [4/10], Step [29100/68337], Loss: 5.2638\n",
      "Epoch [4/10], Step [29175/68337], Loss: 5.1481\n",
      "Epoch [4/10], Step [29250/68337], Loss: 5.0496\n",
      "Epoch [4/10], Step [29325/68337], Loss: 5.1130\n",
      "Epoch [4/10], Step [29400/68337], Loss: 5.3658\n",
      "Epoch [4/10], Step [29475/68337], Loss: 5.0059\n",
      "Epoch [4/10], Step [29550/68337], Loss: 5.2389\n",
      "Epoch [4/10], Step [29625/68337], Loss: 4.9851\n",
      "Epoch [4/10], Step [29700/68337], Loss: 5.2991\n",
      "Epoch [4/10], Step [29775/68337], Loss: 5.3435\n",
      "Epoch [4/10], Step [29850/68337], Loss: 5.1111\n",
      "Epoch [4/10], Step [29925/68337], Loss: 5.3843\n",
      "Epoch [4/10], Step [30000/68337], Loss: 5.1578\n",
      "Validation perplexity: 131.73254193084455\n",
      "Epoch [4/10], Step [30075/68337], Loss: 5.0242\n",
      "Epoch [4/10], Step [30150/68337], Loss: 5.0647\n",
      "Epoch [4/10], Step [30225/68337], Loss: 5.2112\n",
      "Epoch [4/10], Step [30300/68337], Loss: 5.4432\n",
      "Epoch [4/10], Step [30375/68337], Loss: 5.0668\n",
      "Epoch [4/10], Step [30450/68337], Loss: 5.1905\n",
      "Epoch [4/10], Step [30525/68337], Loss: 5.2323\n",
      "Epoch [4/10], Step [30600/68337], Loss: 5.1378\n",
      "Epoch [4/10], Step [30675/68337], Loss: 5.2182\n",
      "Epoch [4/10], Step [30750/68337], Loss: 5.0415\n",
      "Epoch [4/10], Step [30825/68337], Loss: 5.1669\n",
      "Epoch [4/10], Step [30900/68337], Loss: 5.2172\n",
      "Epoch [4/10], Step [30975/68337], Loss: 4.8707\n",
      "Epoch [4/10], Step [31050/68337], Loss: 5.2371\n",
      "Epoch [4/10], Step [31125/68337], Loss: 5.0740\n",
      "Epoch [4/10], Step [31200/68337], Loss: 5.2117\n",
      "Epoch [4/10], Step [31275/68337], Loss: 5.0407\n",
      "Epoch [4/10], Step [31350/68337], Loss: 5.1147\n",
      "Epoch [4/10], Step [31425/68337], Loss: 5.2073\n",
      "Epoch [4/10], Step [31500/68337], Loss: 5.1978\n",
      "Epoch [4/10], Step [31575/68337], Loss: 5.2953\n",
      "Epoch [4/10], Step [31650/68337], Loss: 5.1577\n",
      "Epoch [4/10], Step [31725/68337], Loss: 5.3138\n",
      "Epoch [4/10], Step [31800/68337], Loss: 5.1777\n",
      "Epoch [4/10], Step [31875/68337], Loss: 5.0558\n",
      "Epoch [4/10], Step [31950/68337], Loss: 5.0360\n",
      "Epoch [4/10], Step [32025/68337], Loss: 5.2897\n",
      "Epoch [4/10], Step [32100/68337], Loss: 5.1891\n",
      "Epoch [4/10], Step [32175/68337], Loss: 5.1138\n",
      "Epoch [4/10], Step [32250/68337], Loss: 4.9574\n",
      "Epoch [4/10], Step [32325/68337], Loss: 5.1927\n",
      "Epoch [4/10], Step [32400/68337], Loss: 5.1492\n",
      "Epoch [4/10], Step [32475/68337], Loss: 5.0890\n",
      "Epoch [4/10], Step [32550/68337], Loss: 4.9619\n",
      "Epoch [4/10], Step [32625/68337], Loss: 5.1471\n",
      "Epoch [4/10], Step [32700/68337], Loss: 5.0441\n",
      "Epoch [4/10], Step [32775/68337], Loss: 5.3214\n",
      "Epoch [4/10], Step [32850/68337], Loss: 5.1672\n",
      "Epoch [4/10], Step [32925/68337], Loss: 5.0703\n",
      "Epoch [4/10], Step [33000/68337], Loss: 5.0550\n",
      "Epoch [4/10], Step [33075/68337], Loss: 5.0665\n",
      "Epoch [4/10], Step [33150/68337], Loss: 5.0670\n",
      "Epoch [4/10], Step [33225/68337], Loss: 5.1957\n",
      "Epoch [4/10], Step [33300/68337], Loss: 4.8811\n",
      "Epoch [4/10], Step [33375/68337], Loss: 5.0634\n",
      "Epoch [4/10], Step [33450/68337], Loss: 5.0897\n",
      "Epoch [4/10], Step [33525/68337], Loss: 5.2908\n",
      "Epoch [4/10], Step [33600/68337], Loss: 5.0316\n",
      "Epoch [4/10], Step [33675/68337], Loss: 5.2662\n",
      "Epoch [4/10], Step [33750/68337], Loss: 5.1597\n",
      "Epoch [4/10], Step [33825/68337], Loss: 5.1170\n",
      "Epoch [4/10], Step [33900/68337], Loss: 5.2786\n",
      "Epoch [4/10], Step [33975/68337], Loss: 5.0672\n",
      "Epoch [4/10], Step [34050/68337], Loss: 5.0767\n",
      "Epoch [4/10], Step [34125/68337], Loss: 5.1122\n",
      "Epoch [4/10], Step [34200/68337], Loss: 5.0132\n",
      "Epoch [4/10], Step [34275/68337], Loss: 5.1427\n",
      "Epoch [4/10], Step [34350/68337], Loss: 5.1376\n",
      "Epoch [4/10], Step [34425/68337], Loss: 5.1434\n",
      "Epoch [4/10], Step [34500/68337], Loss: 5.2134\n",
      "Epoch [4/10], Step [34575/68337], Loss: 5.1307\n",
      "Epoch [4/10], Step [34650/68337], Loss: 5.2953\n",
      "Epoch [4/10], Step [34725/68337], Loss: 4.9351\n",
      "Epoch [4/10], Step [34800/68337], Loss: 5.0628\n",
      "Epoch [4/10], Step [34875/68337], Loss: 4.9304\n",
      "Epoch [4/10], Step [34950/68337], Loss: 5.1259\n",
      "Epoch [4/10], Step [35025/68337], Loss: 5.2077\n",
      "Epoch [4/10], Step [35100/68337], Loss: 5.0399\n",
      "Epoch [4/10], Step [35175/68337], Loss: 5.1977\n",
      "Epoch [4/10], Step [35250/68337], Loss: 5.1528\n",
      "Epoch [4/10], Step [35325/68337], Loss: 5.1983\n",
      "Epoch [4/10], Step [35400/68337], Loss: 5.0928\n",
      "Epoch [4/10], Step [35475/68337], Loss: 5.0609\n",
      "Epoch [4/10], Step [35550/68337], Loss: 5.1155\n",
      "Epoch [4/10], Step [35625/68337], Loss: 4.9899\n",
      "Epoch [4/10], Step [35700/68337], Loss: 5.0538\n",
      "Epoch [4/10], Step [35775/68337], Loss: 5.2874\n",
      "Epoch [4/10], Step [35850/68337], Loss: 5.1238\n",
      "Epoch [4/10], Step [35925/68337], Loss: 5.2250\n",
      "Epoch [4/10], Step [36000/68337], Loss: 5.0891\n",
      "Epoch [4/10], Step [36075/68337], Loss: 5.0689\n",
      "Epoch [4/10], Step [36150/68337], Loss: 5.0609\n",
      "Epoch [4/10], Step [36225/68337], Loss: 5.1810\n",
      "Epoch [4/10], Step [36300/68337], Loss: 5.0359\n",
      "Epoch [4/10], Step [36375/68337], Loss: 5.1078\n",
      "Epoch [4/10], Step [36450/68337], Loss: 5.2385\n",
      "Epoch [4/10], Step [36525/68337], Loss: 4.9514\n",
      "Epoch [4/10], Step [36600/68337], Loss: 5.1590\n",
      "Epoch [4/10], Step [36675/68337], Loss: 4.9093\n",
      "Epoch [4/10], Step [36750/68337], Loss: 5.1368\n",
      "Epoch [4/10], Step [36825/68337], Loss: 5.3118\n",
      "Epoch [4/10], Step [36900/68337], Loss: 5.1651\n",
      "Epoch [4/10], Step [36975/68337], Loss: 5.2202\n",
      "Epoch [4/10], Step [37050/68337], Loss: 5.1088\n",
      "Epoch [4/10], Step [37125/68337], Loss: 4.9944\n",
      "Epoch [4/10], Step [37200/68337], Loss: 5.2357\n",
      "Epoch [4/10], Step [37275/68337], Loss: 4.9921\n",
      "Epoch [4/10], Step [37350/68337], Loss: 5.1593\n",
      "Epoch [4/10], Step [37425/68337], Loss: 5.0655\n",
      "Epoch [4/10], Step [37500/68337], Loss: 5.2423\n",
      "Epoch [4/10], Step [37575/68337], Loss: 5.1255\n",
      "Epoch [4/10], Step [37650/68337], Loss: 5.1771\n",
      "Epoch [4/10], Step [37725/68337], Loss: 5.1017\n",
      "Epoch [4/10], Step [37800/68337], Loss: 5.2177\n",
      "Epoch [4/10], Step [37875/68337], Loss: 5.0983\n",
      "Epoch [4/10], Step [37950/68337], Loss: 5.1134\n",
      "Epoch [4/10], Step [38025/68337], Loss: 5.3288\n",
      "Epoch [4/10], Step [38100/68337], Loss: 5.2120\n",
      "Epoch [4/10], Step [38175/68337], Loss: 5.1213\n",
      "Epoch [4/10], Step [38250/68337], Loss: 5.1685\n",
      "Epoch [4/10], Step [38325/68337], Loss: 5.0513\n",
      "Epoch [4/10], Step [38400/68337], Loss: 5.1601\n",
      "Epoch [4/10], Step [38475/68337], Loss: 4.9649\n",
      "Epoch [4/10], Step [38550/68337], Loss: 5.0915\n",
      "Epoch [4/10], Step [38625/68337], Loss: 5.0291\n",
      "Epoch [4/10], Step [38700/68337], Loss: 5.3344\n",
      "Epoch [4/10], Step [38775/68337], Loss: 5.0178\n",
      "Epoch [4/10], Step [38850/68337], Loss: 4.9179\n",
      "Epoch [4/10], Step [38925/68337], Loss: 5.0105\n",
      "Epoch [4/10], Step [39000/68337], Loss: 5.0083\n",
      "Epoch [4/10], Step [39075/68337], Loss: 5.2358\n",
      "Epoch [4/10], Step [39150/68337], Loss: 5.1624\n",
      "Epoch [4/10], Step [39225/68337], Loss: 5.0370\n",
      "Epoch [4/10], Step [39300/68337], Loss: 5.2418\n",
      "Epoch [4/10], Step [39375/68337], Loss: 5.2901\n",
      "Epoch [4/10], Step [39450/68337], Loss: 5.0644\n",
      "Epoch [4/10], Step [39525/68337], Loss: 5.3056\n",
      "Epoch [4/10], Step [39600/68337], Loss: 5.0627\n",
      "Epoch [4/10], Step [39675/68337], Loss: 5.2381\n",
      "Epoch [4/10], Step [39750/68337], Loss: 5.1626\n",
      "Epoch [4/10], Step [39825/68337], Loss: 5.2676\n",
      "Epoch [4/10], Step [39900/68337], Loss: 5.1829\n",
      "Epoch [4/10], Step [39975/68337], Loss: 5.2256\n",
      "Validation perplexity: 131.1528706828502\n",
      "Epoch [4/10], Step [40050/68337], Loss: 5.1052\n",
      "Epoch [4/10], Step [40125/68337], Loss: 5.2184\n",
      "Epoch [4/10], Step [40200/68337], Loss: 5.1946\n",
      "Epoch [4/10], Step [40275/68337], Loss: 5.2753\n",
      "Epoch [4/10], Step [40350/68337], Loss: 5.0468\n",
      "Epoch [4/10], Step [40425/68337], Loss: 5.2564\n",
      "Epoch [4/10], Step [40500/68337], Loss: 5.0222\n",
      "Epoch [4/10], Step [40575/68337], Loss: 5.0558\n",
      "Epoch [4/10], Step [40650/68337], Loss: 5.0904\n",
      "Epoch [4/10], Step [40725/68337], Loss: 4.9613\n",
      "Epoch [4/10], Step [40800/68337], Loss: 5.0403\n",
      "Epoch [4/10], Step [40875/68337], Loss: 5.1205\n",
      "Epoch [4/10], Step [40950/68337], Loss: 5.0118\n",
      "Epoch [4/10], Step [41025/68337], Loss: 5.1347\n",
      "Epoch [4/10], Step [41100/68337], Loss: 5.1102\n",
      "Epoch [4/10], Step [41175/68337], Loss: 4.9464\n",
      "Epoch [4/10], Step [41250/68337], Loss: 5.2459\n",
      "Epoch [4/10], Step [41325/68337], Loss: 4.8700\n",
      "Epoch [4/10], Step [41400/68337], Loss: 5.0772\n",
      "Epoch [4/10], Step [41475/68337], Loss: 5.3523\n",
      "Epoch [4/10], Step [41550/68337], Loss: 5.0189\n",
      "Epoch [4/10], Step [41625/68337], Loss: 5.2460\n",
      "Epoch [4/10], Step [41700/68337], Loss: 4.9076\n",
      "Epoch [4/10], Step [41775/68337], Loss: 5.1030\n",
      "Epoch [4/10], Step [41850/68337], Loss: 5.2514\n",
      "Epoch [4/10], Step [41925/68337], Loss: 5.0137\n",
      "Epoch [4/10], Step [42000/68337], Loss: 4.9403\n",
      "Epoch [4/10], Step [42075/68337], Loss: 5.0851\n",
      "Epoch [4/10], Step [42150/68337], Loss: 5.0086\n",
      "Epoch [4/10], Step [42225/68337], Loss: 5.3562\n",
      "Epoch [4/10], Step [42300/68337], Loss: 5.0877\n",
      "Epoch [4/10], Step [42375/68337], Loss: 5.0067\n",
      "Epoch [4/10], Step [42450/68337], Loss: 5.3578\n",
      "Epoch [4/10], Step [42525/68337], Loss: 5.1634\n",
      "Epoch [4/10], Step [42600/68337], Loss: 5.2658\n",
      "Epoch [4/10], Step [42675/68337], Loss: 5.2382\n",
      "Epoch [4/10], Step [42750/68337], Loss: 5.0807\n",
      "Epoch [4/10], Step [42825/68337], Loss: 4.8846\n",
      "Epoch [4/10], Step [42900/68337], Loss: 5.3443\n",
      "Epoch [4/10], Step [42975/68337], Loss: 4.9352\n",
      "Epoch [4/10], Step [43050/68337], Loss: 5.1839\n",
      "Epoch [4/10], Step [43125/68337], Loss: 5.1383\n",
      "Epoch [4/10], Step [43200/68337], Loss: 5.2509\n",
      "Epoch [4/10], Step [43275/68337], Loss: 5.0170\n",
      "Epoch [4/10], Step [43350/68337], Loss: 5.1596\n",
      "Epoch [4/10], Step [43425/68337], Loss: 5.0202\n",
      "Epoch [4/10], Step [43500/68337], Loss: 5.1909\n",
      "Epoch [4/10], Step [43575/68337], Loss: 5.0203\n",
      "Epoch [4/10], Step [43650/68337], Loss: 5.2022\n",
      "Epoch [4/10], Step [43725/68337], Loss: 5.1444\n",
      "Epoch [4/10], Step [43800/68337], Loss: 5.2949\n",
      "Epoch [4/10], Step [43875/68337], Loss: 5.0488\n",
      "Epoch [4/10], Step [43950/68337], Loss: 5.2189\n",
      "Epoch [4/10], Step [44025/68337], Loss: 5.0278\n",
      "Epoch [4/10], Step [44100/68337], Loss: 5.1685\n",
      "Epoch [4/10], Step [44175/68337], Loss: 5.1702\n",
      "Epoch [4/10], Step [44250/68337], Loss: 4.9913\n",
      "Epoch [4/10], Step [44325/68337], Loss: 5.1018\n",
      "Epoch [4/10], Step [44400/68337], Loss: 5.2713\n",
      "Epoch [4/10], Step [44475/68337], Loss: 4.9437\n",
      "Epoch [4/10], Step [44550/68337], Loss: 5.0745\n",
      "Epoch [4/10], Step [44625/68337], Loss: 5.2383\n",
      "Epoch [4/10], Step [44700/68337], Loss: 4.8095\n",
      "Epoch [4/10], Step [44775/68337], Loss: 4.9437\n",
      "Epoch [4/10], Step [44850/68337], Loss: 5.3482\n",
      "Epoch [4/10], Step [44925/68337], Loss: 5.2864\n",
      "Epoch [4/10], Step [45000/68337], Loss: 5.1692\n",
      "Epoch [4/10], Step [45075/68337], Loss: 5.1767\n",
      "Epoch [4/10], Step [45150/68337], Loss: 5.0002\n",
      "Epoch [4/10], Step [45225/68337], Loss: 5.1535\n",
      "Epoch [4/10], Step [45300/68337], Loss: 5.0336\n",
      "Epoch [4/10], Step [45375/68337], Loss: 5.0291\n",
      "Epoch [4/10], Step [45450/68337], Loss: 5.2562\n",
      "Epoch [4/10], Step [45525/68337], Loss: 5.0640\n",
      "Epoch [4/10], Step [45600/68337], Loss: 5.1106\n",
      "Epoch [4/10], Step [45675/68337], Loss: 5.3398\n",
      "Epoch [4/10], Step [45750/68337], Loss: 5.0406\n",
      "Epoch [4/10], Step [45825/68337], Loss: 5.1314\n",
      "Epoch [4/10], Step [45900/68337], Loss: 5.2507\n",
      "Epoch [4/10], Step [45975/68337], Loss: 5.0654\n",
      "Epoch [4/10], Step [46050/68337], Loss: 5.2638\n",
      "Epoch [4/10], Step [46125/68337], Loss: 5.1067\n",
      "Epoch [4/10], Step [46200/68337], Loss: 5.0983\n",
      "Epoch [4/10], Step [46275/68337], Loss: 5.1251\n",
      "Epoch [4/10], Step [46350/68337], Loss: 5.1462\n",
      "Epoch [4/10], Step [46425/68337], Loss: 5.1761\n",
      "Epoch [4/10], Step [46500/68337], Loss: 5.0883\n",
      "Epoch [4/10], Step [46575/68337], Loss: 5.0625\n",
      "Epoch [4/10], Step [46650/68337], Loss: 4.9384\n",
      "Epoch [4/10], Step [46725/68337], Loss: 5.0880\n",
      "Epoch [4/10], Step [46800/68337], Loss: 5.0465\n",
      "Epoch [4/10], Step [46875/68337], Loss: 5.1238\n",
      "Epoch [4/10], Step [46950/68337], Loss: 5.0924\n",
      "Epoch [4/10], Step [47025/68337], Loss: 5.3944\n",
      "Epoch [4/10], Step [47100/68337], Loss: 5.0335\n",
      "Epoch [4/10], Step [47175/68337], Loss: 5.1577\n",
      "Epoch [4/10], Step [47250/68337], Loss: 5.0224\n",
      "Epoch [4/10], Step [47325/68337], Loss: 5.0499\n",
      "Epoch [4/10], Step [47400/68337], Loss: 5.0773\n",
      "Epoch [4/10], Step [47475/68337], Loss: 5.0304\n",
      "Epoch [4/10], Step [47550/68337], Loss: 5.0730\n",
      "Epoch [4/10], Step [47625/68337], Loss: 5.1464\n",
      "Epoch [4/10], Step [47700/68337], Loss: 4.9893\n",
      "Epoch [4/10], Step [47775/68337], Loss: 5.1400\n",
      "Epoch [4/10], Step [47850/68337], Loss: 5.0556\n",
      "Epoch [4/10], Step [47925/68337], Loss: 5.3663\n",
      "Epoch [4/10], Step [48000/68337], Loss: 5.0932\n",
      "Epoch [4/10], Step [48075/68337], Loss: 5.1386\n",
      "Epoch [4/10], Step [48150/68337], Loss: 5.2091\n",
      "Epoch [4/10], Step [48225/68337], Loss: 5.2845\n",
      "Epoch [4/10], Step [48300/68337], Loss: 5.0947\n",
      "Epoch [4/10], Step [48375/68337], Loss: 5.0025\n",
      "Epoch [4/10], Step [48450/68337], Loss: 5.3193\n",
      "Epoch [4/10], Step [48525/68337], Loss: 5.1703\n",
      "Epoch [4/10], Step [48600/68337], Loss: 4.9993\n",
      "Epoch [4/10], Step [48675/68337], Loss: 5.1403\n",
      "Epoch [4/10], Step [48750/68337], Loss: 4.9484\n",
      "Epoch [4/10], Step [48825/68337], Loss: 5.0233\n",
      "Epoch [4/10], Step [48900/68337], Loss: 5.3330\n",
      "Epoch [4/10], Step [48975/68337], Loss: 5.1331\n",
      "Epoch [4/10], Step [49050/68337], Loss: 5.0058\n",
      "Epoch [4/10], Step [49125/68337], Loss: 5.2742\n",
      "Epoch [4/10], Step [49200/68337], Loss: 5.0103\n",
      "Epoch [4/10], Step [49275/68337], Loss: 5.0746\n",
      "Epoch [4/10], Step [49350/68337], Loss: 5.1193\n",
      "Epoch [4/10], Step [49425/68337], Loss: 5.1611\n",
      "Epoch [4/10], Step [49500/68337], Loss: 5.0457\n",
      "Epoch [4/10], Step [49575/68337], Loss: 5.0501\n",
      "Epoch [4/10], Step [49650/68337], Loss: 5.0062\n",
      "Epoch [4/10], Step [49725/68337], Loss: 5.1890\n",
      "Epoch [4/10], Step [49800/68337], Loss: 4.9883\n",
      "Epoch [4/10], Step [49875/68337], Loss: 5.1586\n",
      "Epoch [4/10], Step [49950/68337], Loss: 5.0890\n",
      "Validation perplexity: 130.52481276742262\n",
      "Epoch [4/10], Step [50025/68337], Loss: 5.3121\n",
      "Epoch [4/10], Step [50100/68337], Loss: 5.0295\n",
      "Epoch [4/10], Step [50175/68337], Loss: 5.0770\n",
      "Epoch [4/10], Step [50250/68337], Loss: 5.0649\n",
      "Epoch [4/10], Step [50325/68337], Loss: 5.1987\n",
      "Epoch [4/10], Step [50400/68337], Loss: 5.0725\n",
      "Epoch [4/10], Step [50475/68337], Loss: 5.2317\n",
      "Epoch [4/10], Step [50550/68337], Loss: 5.2015\n",
      "Epoch [4/10], Step [50625/68337], Loss: 5.1656\n",
      "Epoch [4/10], Step [50700/68337], Loss: 5.1264\n",
      "Epoch [4/10], Step [50775/68337], Loss: 5.2157\n",
      "Epoch [4/10], Step [50850/68337], Loss: 5.1179\n",
      "Epoch [4/10], Step [50925/68337], Loss: 5.1202\n",
      "Epoch [4/10], Step [51000/68337], Loss: 5.1555\n",
      "Epoch [4/10], Step [51075/68337], Loss: 5.0781\n",
      "Epoch [4/10], Step [51150/68337], Loss: 5.2310\n",
      "Epoch [4/10], Step [51225/68337], Loss: 5.0790\n",
      "Epoch [4/10], Step [51300/68337], Loss: 5.1521\n",
      "Epoch [4/10], Step [51375/68337], Loss: 4.9748\n",
      "Epoch [4/10], Step [51450/68337], Loss: 5.0718\n",
      "Epoch [4/10], Step [51525/68337], Loss: 5.1772\n",
      "Epoch [4/10], Step [51600/68337], Loss: 5.1411\n",
      "Epoch [4/10], Step [51675/68337], Loss: 5.1487\n",
      "Epoch [4/10], Step [51750/68337], Loss: 5.0827\n",
      "Epoch [4/10], Step [51825/68337], Loss: 5.0429\n",
      "Epoch [4/10], Step [51900/68337], Loss: 5.1606\n",
      "Epoch [4/10], Step [51975/68337], Loss: 5.1925\n",
      "Epoch [4/10], Step [52050/68337], Loss: 5.2017\n",
      "Epoch [4/10], Step [52125/68337], Loss: 5.1337\n",
      "Epoch [4/10], Step [52200/68337], Loss: 5.2524\n",
      "Epoch [4/10], Step [52275/68337], Loss: 5.3139\n",
      "Epoch [4/10], Step [52350/68337], Loss: 5.1149\n",
      "Epoch [4/10], Step [52425/68337], Loss: 5.3489\n",
      "Epoch [4/10], Step [52500/68337], Loss: 5.1815\n",
      "Epoch [4/10], Step [52575/68337], Loss: 5.3185\n",
      "Epoch [4/10], Step [52650/68337], Loss: 4.9828\n",
      "Epoch [4/10], Step [52725/68337], Loss: 4.9658\n",
      "Epoch [4/10], Step [52800/68337], Loss: 5.1919\n",
      "Epoch [4/10], Step [52875/68337], Loss: 5.0750\n",
      "Epoch [4/10], Step [52950/68337], Loss: 5.0370\n",
      "Epoch [4/10], Step [53025/68337], Loss: 5.1313\n",
      "Epoch [4/10], Step [53100/68337], Loss: 5.0562\n",
      "Epoch [4/10], Step [53175/68337], Loss: 4.8960\n",
      "Epoch [4/10], Step [53250/68337], Loss: 4.8554\n",
      "Epoch [4/10], Step [53325/68337], Loss: 5.1209\n",
      "Epoch [4/10], Step [53400/68337], Loss: 5.1710\n",
      "Epoch [4/10], Step [53475/68337], Loss: 5.1516\n",
      "Epoch [4/10], Step [53550/68337], Loss: 4.9316\n",
      "Epoch [4/10], Step [53625/68337], Loss: 5.1793\n",
      "Epoch [4/10], Step [53700/68337], Loss: 5.0207\n",
      "Epoch [4/10], Step [53775/68337], Loss: 5.0333\n",
      "Epoch [4/10], Step [53850/68337], Loss: 5.2411\n",
      "Epoch [4/10], Step [53925/68337], Loss: 5.1931\n",
      "Epoch [4/10], Step [54000/68337], Loss: 5.1864\n",
      "Epoch [4/10], Step [54075/68337], Loss: 5.0256\n",
      "Epoch [4/10], Step [54150/68337], Loss: 5.2606\n",
      "Epoch [4/10], Step [54225/68337], Loss: 5.2828\n",
      "Epoch [4/10], Step [54300/68337], Loss: 5.1563\n",
      "Epoch [4/10], Step [54375/68337], Loss: 5.1794\n",
      "Epoch [4/10], Step [54450/68337], Loss: 5.1290\n",
      "Epoch [4/10], Step [54525/68337], Loss: 5.2642\n",
      "Epoch [4/10], Step [54600/68337], Loss: 4.9898\n",
      "Epoch [4/10], Step [54675/68337], Loss: 5.1606\n",
      "Epoch [4/10], Step [54750/68337], Loss: 5.1383\n",
      "Epoch [4/10], Step [54825/68337], Loss: 5.1475\n",
      "Epoch [4/10], Step [54900/68337], Loss: 5.1246\n",
      "Epoch [4/10], Step [54975/68337], Loss: 5.2281\n",
      "Epoch [4/10], Step [55050/68337], Loss: 4.9349\n",
      "Epoch [4/10], Step [55125/68337], Loss: 5.2596\n",
      "Epoch [4/10], Step [55200/68337], Loss: 5.3004\n",
      "Epoch [4/10], Step [55275/68337], Loss: 5.0268\n",
      "Epoch [4/10], Step [55350/68337], Loss: 5.0741\n",
      "Epoch [4/10], Step [55425/68337], Loss: 4.9426\n",
      "Epoch [4/10], Step [55500/68337], Loss: 5.1296\n",
      "Epoch [4/10], Step [55575/68337], Loss: 5.1791\n",
      "Epoch [4/10], Step [55650/68337], Loss: 5.0903\n",
      "Epoch [4/10], Step [55725/68337], Loss: 5.2716\n",
      "Epoch [4/10], Step [55800/68337], Loss: 5.1930\n",
      "Epoch [4/10], Step [55875/68337], Loss: 5.2015\n",
      "Epoch [4/10], Step [55950/68337], Loss: 5.2619\n",
      "Epoch [4/10], Step [56025/68337], Loss: 5.0288\n",
      "Epoch [4/10], Step [56100/68337], Loss: 5.2998\n",
      "Epoch [4/10], Step [56175/68337], Loss: 4.8220\n",
      "Epoch [4/10], Step [56250/68337], Loss: 5.1014\n",
      "Epoch [4/10], Step [56325/68337], Loss: 4.9779\n",
      "Epoch [4/10], Step [56400/68337], Loss: 5.0006\n",
      "Epoch [4/10], Step [56475/68337], Loss: 5.0781\n",
      "Epoch [4/10], Step [56550/68337], Loss: 5.1310\n",
      "Epoch [4/10], Step [56625/68337], Loss: 5.1915\n",
      "Epoch [4/10], Step [56700/68337], Loss: 4.9829\n",
      "Epoch [4/10], Step [56775/68337], Loss: 5.0849\n",
      "Epoch [4/10], Step [56850/68337], Loss: 5.0250\n",
      "Epoch [4/10], Step [56925/68337], Loss: 5.0687\n",
      "Epoch [4/10], Step [57000/68337], Loss: 5.0436\n",
      "Epoch [4/10], Step [57075/68337], Loss: 5.0337\n",
      "Epoch [4/10], Step [57150/68337], Loss: 5.1154\n",
      "Epoch [4/10], Step [57225/68337], Loss: 5.0922\n",
      "Epoch [4/10], Step [57300/68337], Loss: 5.2162\n",
      "Epoch [4/10], Step [57375/68337], Loss: 5.0929\n",
      "Epoch [4/10], Step [57450/68337], Loss: 5.1199\n",
      "Epoch [4/10], Step [57525/68337], Loss: 4.9591\n",
      "Epoch [4/10], Step [57600/68337], Loss: 5.2333\n",
      "Epoch [4/10], Step [57675/68337], Loss: 4.8782\n",
      "Epoch [4/10], Step [57750/68337], Loss: 5.2408\n",
      "Epoch [4/10], Step [57825/68337], Loss: 5.3008\n",
      "Epoch [4/10], Step [57900/68337], Loss: 5.1924\n",
      "Epoch [4/10], Step [57975/68337], Loss: 5.3074\n",
      "Epoch [4/10], Step [58050/68337], Loss: 5.1071\n",
      "Epoch [4/10], Step [58125/68337], Loss: 5.1172\n",
      "Epoch [4/10], Step [58200/68337], Loss: 5.0088\n",
      "Epoch [4/10], Step [58275/68337], Loss: 5.2843\n",
      "Epoch [4/10], Step [58350/68337], Loss: 5.3408\n",
      "Epoch [4/10], Step [58425/68337], Loss: 4.7226\n",
      "Epoch [4/10], Step [58500/68337], Loss: 5.1520\n",
      "Epoch [4/10], Step [58575/68337], Loss: 5.0324\n",
      "Epoch [4/10], Step [58650/68337], Loss: 5.0102\n",
      "Epoch [4/10], Step [58725/68337], Loss: 5.1057\n",
      "Epoch [4/10], Step [58800/68337], Loss: 5.0794\n",
      "Epoch [4/10], Step [58875/68337], Loss: 5.0908\n",
      "Epoch [4/10], Step [58950/68337], Loss: 4.9566\n",
      "Epoch [4/10], Step [59025/68337], Loss: 5.1537\n",
      "Epoch [4/10], Step [59100/68337], Loss: 5.2354\n",
      "Epoch [4/10], Step [59175/68337], Loss: 5.2988\n",
      "Epoch [4/10], Step [59250/68337], Loss: 5.0626\n",
      "Epoch [4/10], Step [59325/68337], Loss: 5.2448\n",
      "Epoch [4/10], Step [59400/68337], Loss: 5.3511\n",
      "Epoch [4/10], Step [59475/68337], Loss: 5.0958\n",
      "Epoch [4/10], Step [59550/68337], Loss: 5.0122\n",
      "Epoch [4/10], Step [59625/68337], Loss: 4.9698\n",
      "Epoch [4/10], Step [59700/68337], Loss: 5.0071\n",
      "Epoch [4/10], Step [59775/68337], Loss: 5.2911\n",
      "Epoch [4/10], Step [59850/68337], Loss: 5.1149\n",
      "Epoch [4/10], Step [59925/68337], Loss: 5.1560\n",
      "Epoch [4/10], Step [60000/68337], Loss: 4.8891\n",
      "Validation perplexity: 130.026582513573\n",
      "Epoch [4/10], Step [60075/68337], Loss: 5.0573\n",
      "Epoch [4/10], Step [60150/68337], Loss: 5.2276\n",
      "Epoch [4/10], Step [60225/68337], Loss: 4.9550\n",
      "Epoch [4/10], Step [60300/68337], Loss: 5.3056\n",
      "Epoch [4/10], Step [60375/68337], Loss: 5.4308\n",
      "Epoch [4/10], Step [60450/68337], Loss: 5.0570\n",
      "Epoch [4/10], Step [60525/68337], Loss: 5.1632\n",
      "Epoch [4/10], Step [60600/68337], Loss: 5.1340\n",
      "Epoch [4/10], Step [60675/68337], Loss: 5.0130\n",
      "Epoch [4/10], Step [60750/68337], Loss: 5.2845\n",
      "Epoch [4/10], Step [60825/68337], Loss: 5.4226\n",
      "Epoch [4/10], Step [60900/68337], Loss: 5.1701\n",
      "Epoch [4/10], Step [60975/68337], Loss: 4.9363\n",
      "Epoch [4/10], Step [61050/68337], Loss: 5.2416\n",
      "Epoch [4/10], Step [61125/68337], Loss: 5.1676\n",
      "Epoch [4/10], Step [61200/68337], Loss: 4.9615\n",
      "Epoch [4/10], Step [61275/68337], Loss: 5.3145\n",
      "Epoch [4/10], Step [61350/68337], Loss: 5.1008\n",
      "Epoch [4/10], Step [61425/68337], Loss: 5.1750\n",
      "Epoch [4/10], Step [61500/68337], Loss: 5.3900\n",
      "Epoch [4/10], Step [61575/68337], Loss: 5.1268\n",
      "Epoch [4/10], Step [61650/68337], Loss: 5.1959\n",
      "Epoch [4/10], Step [61725/68337], Loss: 5.0850\n",
      "Epoch [4/10], Step [61800/68337], Loss: 5.1949\n",
      "Epoch [4/10], Step [61875/68337], Loss: 5.0387\n",
      "Epoch [4/10], Step [61950/68337], Loss: 4.8001\n",
      "Epoch [4/10], Step [62025/68337], Loss: 5.1699\n",
      "Epoch [4/10], Step [62100/68337], Loss: 5.0909\n",
      "Epoch [4/10], Step [62175/68337], Loss: 5.0523\n",
      "Epoch [4/10], Step [62250/68337], Loss: 5.2410\n",
      "Epoch [4/10], Step [62325/68337], Loss: 5.1350\n",
      "Epoch [4/10], Step [62400/68337], Loss: 5.3759\n",
      "Epoch [4/10], Step [62475/68337], Loss: 5.0681\n",
      "Epoch [4/10], Step [62550/68337], Loss: 5.0453\n",
      "Epoch [4/10], Step [62625/68337], Loss: 5.0192\n",
      "Epoch [4/10], Step [62700/68337], Loss: 5.0605\n",
      "Epoch [4/10], Step [62775/68337], Loss: 4.8997\n",
      "Epoch [4/10], Step [62850/68337], Loss: 5.3078\n",
      "Epoch [4/10], Step [62925/68337], Loss: 5.1961\n",
      "Epoch [4/10], Step [63000/68337], Loss: 5.3566\n",
      "Epoch [4/10], Step [63075/68337], Loss: 4.8935\n",
      "Epoch [4/10], Step [63150/68337], Loss: 4.8216\n",
      "Epoch [4/10], Step [63225/68337], Loss: 5.3947\n",
      "Epoch [4/10], Step [63300/68337], Loss: 5.0363\n",
      "Epoch [4/10], Step [63375/68337], Loss: 5.1747\n",
      "Epoch [4/10], Step [63450/68337], Loss: 5.2827\n",
      "Epoch [4/10], Step [63525/68337], Loss: 5.0680\n",
      "Epoch [4/10], Step [63600/68337], Loss: 5.1282\n",
      "Epoch [4/10], Step [63675/68337], Loss: 5.0623\n",
      "Epoch [4/10], Step [63750/68337], Loss: 5.1422\n",
      "Epoch [4/10], Step [63825/68337], Loss: 5.2371\n",
      "Epoch [4/10], Step [63900/68337], Loss: 4.9244\n",
      "Epoch [4/10], Step [63975/68337], Loss: 5.1497\n",
      "Epoch [4/10], Step [64050/68337], Loss: 5.2045\n",
      "Epoch [4/10], Step [64125/68337], Loss: 5.1225\n",
      "Epoch [4/10], Step [64200/68337], Loss: 5.2051\n",
      "Epoch [4/10], Step [64275/68337], Loss: 5.2154\n",
      "Epoch [4/10], Step [64350/68337], Loss: 5.2496\n",
      "Epoch [4/10], Step [64425/68337], Loss: 5.1159\n",
      "Epoch [4/10], Step [64500/68337], Loss: 5.0572\n",
      "Epoch [4/10], Step [64575/68337], Loss: 5.2108\n",
      "Epoch [4/10], Step [64650/68337], Loss: 5.3088\n",
      "Epoch [4/10], Step [64725/68337], Loss: 5.1541\n",
      "Epoch [4/10], Step [64800/68337], Loss: 5.1363\n",
      "Epoch [4/10], Step [64875/68337], Loss: 5.0678\n",
      "Epoch [4/10], Step [64950/68337], Loss: 5.3111\n",
      "Epoch [4/10], Step [65025/68337], Loss: 5.0461\n",
      "Epoch [4/10], Step [65100/68337], Loss: 5.2711\n",
      "Epoch [4/10], Step [65175/68337], Loss: 5.1208\n",
      "Epoch [4/10], Step [65250/68337], Loss: 5.1588\n",
      "Epoch [4/10], Step [65325/68337], Loss: 5.1077\n",
      "Epoch [4/10], Step [65400/68337], Loss: 5.0300\n",
      "Epoch [4/10], Step [65475/68337], Loss: 4.9741\n",
      "Epoch [4/10], Step [65550/68337], Loss: 5.1544\n",
      "Epoch [4/10], Step [65625/68337], Loss: 5.1068\n",
      "Epoch [4/10], Step [65700/68337], Loss: 5.0655\n",
      "Epoch [4/10], Step [65775/68337], Loss: 5.2468\n",
      "Epoch [4/10], Step [65850/68337], Loss: 5.1377\n",
      "Epoch [4/10], Step [65925/68337], Loss: 4.9592\n",
      "Epoch [4/10], Step [66000/68337], Loss: 5.0322\n",
      "Epoch [4/10], Step [66075/68337], Loss: 5.2163\n",
      "Epoch [4/10], Step [66150/68337], Loss: 5.1711\n",
      "Epoch [4/10], Step [66225/68337], Loss: 5.1008\n",
      "Epoch [4/10], Step [66300/68337], Loss: 5.2376\n",
      "Epoch [4/10], Step [66375/68337], Loss: 5.0367\n",
      "Epoch [4/10], Step [66450/68337], Loss: 5.1279\n",
      "Epoch [4/10], Step [66525/68337], Loss: 5.1154\n",
      "Epoch [4/10], Step [66600/68337], Loss: 5.1747\n",
      "Epoch [4/10], Step [66675/68337], Loss: 5.0963\n",
      "Epoch [4/10], Step [66750/68337], Loss: 5.1464\n",
      "Epoch [4/10], Step [66825/68337], Loss: 5.0082\n",
      "Epoch [4/10], Step [66900/68337], Loss: 5.2420\n",
      "Epoch [4/10], Step [66975/68337], Loss: 5.1068\n",
      "Epoch [4/10], Step [67050/68337], Loss: 5.1838\n",
      "Epoch [4/10], Step [67125/68337], Loss: 5.0009\n",
      "Epoch [4/10], Step [67200/68337], Loss: 5.1056\n",
      "Epoch [4/10], Step [67275/68337], Loss: 5.1643\n",
      "Epoch [4/10], Step [67350/68337], Loss: 5.0077\n",
      "Epoch [4/10], Step [67425/68337], Loss: 5.1209\n",
      "Epoch [4/10], Step [67500/68337], Loss: 5.0847\n",
      "Epoch [4/10], Step [67575/68337], Loss: 5.0672\n",
      "Epoch [4/10], Step [67650/68337], Loss: 5.2679\n",
      "Epoch [4/10], Step [67725/68337], Loss: 5.2802\n",
      "Epoch [4/10], Step [67800/68337], Loss: 5.2605\n",
      "Epoch [4/10], Step [67875/68337], Loss: 4.7597\n",
      "Epoch [4/10], Step [67950/68337], Loss: 5.2172\n",
      "Epoch [4/10], Step [68025/68337], Loss: 5.4098\n",
      "Epoch [4/10], Step [68100/68337], Loss: 5.1706\n",
      "Epoch [4/10], Step [68175/68337], Loss: 4.9902\n",
      "Epoch [4/10], Step [68250/68337], Loss: 5.0401\n",
      "Epoch [4/10], Step [68325/68337], Loss: 5.1369\n",
      "Epoch [4/10] Average Loss: 5.1366, Perplexity: 170.13\n",
      "Epoch [5/10], Step [0/68337], Loss: 5.1521\n",
      "Validation perplexity: 129.46883488512634\n",
      "Epoch [5/10], Step [75/68337], Loss: 5.2367\n",
      "Epoch [5/10], Step [150/68337], Loss: 5.0861\n",
      "Epoch [5/10], Step [225/68337], Loss: 5.0837\n",
      "Epoch [5/10], Step [300/68337], Loss: 4.9336\n",
      "Epoch [5/10], Step [375/68337], Loss: 5.1213\n",
      "Epoch [5/10], Step [450/68337], Loss: 5.0672\n",
      "Epoch [5/10], Step [525/68337], Loss: 5.3581\n",
      "Epoch [5/10], Step [600/68337], Loss: 5.2015\n",
      "Epoch [5/10], Step [675/68337], Loss: 5.1153\n",
      "Epoch [5/10], Step [750/68337], Loss: 5.0816\n",
      "Epoch [5/10], Step [825/68337], Loss: 5.2167\n",
      "Epoch [5/10], Step [900/68337], Loss: 5.0340\n",
      "Epoch [5/10], Step [975/68337], Loss: 4.8708\n",
      "Epoch [5/10], Step [1050/68337], Loss: 5.1637\n",
      "Epoch [5/10], Step [1125/68337], Loss: 5.2985\n",
      "Epoch [5/10], Step [1200/68337], Loss: 5.1687\n",
      "Epoch [5/10], Step [1275/68337], Loss: 5.0534\n",
      "Epoch [5/10], Step [1350/68337], Loss: 5.2202\n",
      "Epoch [5/10], Step [1425/68337], Loss: 5.0579\n",
      "Epoch [5/10], Step [1500/68337], Loss: 5.2693\n",
      "Epoch [5/10], Step [1575/68337], Loss: 5.0900\n",
      "Epoch [5/10], Step [1650/68337], Loss: 5.0384\n",
      "Epoch [5/10], Step [1725/68337], Loss: 5.1848\n",
      "Epoch [5/10], Step [1800/68337], Loss: 5.1918\n",
      "Epoch [5/10], Step [1875/68337], Loss: 5.1649\n",
      "Epoch [5/10], Step [1950/68337], Loss: 4.9026\n",
      "Epoch [5/10], Step [2025/68337], Loss: 5.0818\n",
      "Epoch [5/10], Step [2100/68337], Loss: 5.2462\n",
      "Epoch [5/10], Step [2175/68337], Loss: 4.9949\n",
      "Epoch [5/10], Step [2250/68337], Loss: 5.0475\n",
      "Epoch [5/10], Step [2325/68337], Loss: 5.3298\n",
      "Epoch [5/10], Step [2400/68337], Loss: 5.1601\n",
      "Epoch [5/10], Step [2475/68337], Loss: 5.0490\n",
      "Epoch [5/10], Step [2550/68337], Loss: 5.0479\n",
      "Epoch [5/10], Step [2625/68337], Loss: 5.1983\n",
      "Epoch [5/10], Step [2700/68337], Loss: 5.3035\n",
      "Epoch [5/10], Step [2775/68337], Loss: 5.4160\n",
      "Epoch [5/10], Step [2850/68337], Loss: 5.1805\n",
      "Epoch [5/10], Step [2925/68337], Loss: 5.2972\n",
      "Epoch [5/10], Step [3000/68337], Loss: 5.2344\n",
      "Epoch [5/10], Step [3075/68337], Loss: 4.9831\n",
      "Epoch [5/10], Step [3150/68337], Loss: 4.9424\n",
      "Epoch [5/10], Step [3225/68337], Loss: 5.3535\n",
      "Epoch [5/10], Step [3300/68337], Loss: 5.0960\n",
      "Epoch [5/10], Step [3375/68337], Loss: 5.1192\n",
      "Epoch [5/10], Step [3450/68337], Loss: 5.1947\n",
      "Epoch [5/10], Step [3525/68337], Loss: 5.2010\n",
      "Epoch [5/10], Step [3600/68337], Loss: 4.7800\n",
      "Epoch [5/10], Step [3675/68337], Loss: 4.9656\n",
      "Epoch [5/10], Step [3750/68337], Loss: 5.1739\n",
      "Epoch [5/10], Step [3825/68337], Loss: 5.0546\n",
      "Epoch [5/10], Step [3900/68337], Loss: 4.8525\n",
      "Epoch [5/10], Step [3975/68337], Loss: 5.1991\n",
      "Epoch [5/10], Step [4050/68337], Loss: 5.1265\n",
      "Epoch [5/10], Step [4125/68337], Loss: 4.9300\n",
      "Epoch [5/10], Step [4200/68337], Loss: 5.0354\n",
      "Epoch [5/10], Step [4275/68337], Loss: 5.0877\n",
      "Epoch [5/10], Step [4350/68337], Loss: 5.2665\n",
      "Epoch [5/10], Step [4425/68337], Loss: 5.0572\n",
      "Epoch [5/10], Step [4500/68337], Loss: 5.2234\n",
      "Epoch [5/10], Step [4575/68337], Loss: 5.0823\n",
      "Epoch [5/10], Step [4650/68337], Loss: 5.0361\n",
      "Epoch [5/10], Step [4725/68337], Loss: 5.1537\n",
      "Epoch [5/10], Step [4800/68337], Loss: 5.0572\n",
      "Epoch [5/10], Step [4875/68337], Loss: 5.1148\n",
      "Epoch [5/10], Step [4950/68337], Loss: 4.9996\n",
      "Epoch [5/10], Step [5025/68337], Loss: 5.2967\n",
      "Epoch [5/10], Step [5100/68337], Loss: 4.9141\n",
      "Epoch [5/10], Step [5175/68337], Loss: 4.9590\n",
      "Epoch [5/10], Step [5250/68337], Loss: 5.2556\n",
      "Epoch [5/10], Step [5325/68337], Loss: 5.1979\n",
      "Epoch [5/10], Step [5400/68337], Loss: 5.3572\n",
      "Epoch [5/10], Step [5475/68337], Loss: 5.1957\n",
      "Epoch [5/10], Step [5550/68337], Loss: 4.9729\n",
      "Epoch [5/10], Step [5625/68337], Loss: 5.0225\n",
      "Epoch [5/10], Step [5700/68337], Loss: 5.0767\n",
      "Epoch [5/10], Step [5775/68337], Loss: 5.1492\n",
      "Epoch [5/10], Step [5850/68337], Loss: 5.0708\n",
      "Epoch [5/10], Step [5925/68337], Loss: 5.0446\n",
      "Epoch [5/10], Step [6000/68337], Loss: 5.0086\n",
      "Epoch [5/10], Step [6075/68337], Loss: 5.1801\n",
      "Epoch [5/10], Step [6150/68337], Loss: 5.3088\n",
      "Epoch [5/10], Step [6225/68337], Loss: 5.2540\n",
      "Epoch [5/10], Step [6300/68337], Loss: 4.9736\n",
      "Epoch [5/10], Step [6375/68337], Loss: 5.0937\n",
      "Epoch [5/10], Step [6450/68337], Loss: 4.9614\n",
      "Epoch [5/10], Step [6525/68337], Loss: 5.0953\n",
      "Epoch [5/10], Step [6600/68337], Loss: 5.0770\n",
      "Epoch [5/10], Step [6675/68337], Loss: 5.0070\n",
      "Epoch [5/10], Step [6750/68337], Loss: 5.2281\n",
      "Epoch [5/10], Step [6825/68337], Loss: 5.1417\n",
      "Epoch [5/10], Step [6900/68337], Loss: 5.0456\n",
      "Epoch [5/10], Step [6975/68337], Loss: 5.1445\n",
      "Epoch [5/10], Step [7050/68337], Loss: 5.1511\n",
      "Epoch [5/10], Step [7125/68337], Loss: 5.1924\n",
      "Epoch [5/10], Step [7200/68337], Loss: 4.9316\n",
      "Epoch [5/10], Step [7275/68337], Loss: 5.0847\n",
      "Epoch [5/10], Step [7350/68337], Loss: 5.1814\n",
      "Epoch [5/10], Step [7425/68337], Loss: 5.1786\n",
      "Epoch [5/10], Step [7500/68337], Loss: 5.1967\n",
      "Epoch [5/10], Step [7575/68337], Loss: 5.0575\n",
      "Epoch [5/10], Step [7650/68337], Loss: 5.2482\n",
      "Epoch [5/10], Step [7725/68337], Loss: 5.0052\n",
      "Epoch [5/10], Step [7800/68337], Loss: 4.9361\n",
      "Epoch [5/10], Step [7875/68337], Loss: 5.2462\n",
      "Epoch [5/10], Step [7950/68337], Loss: 5.1413\n",
      "Epoch [5/10], Step [8025/68337], Loss: 4.9340\n",
      "Epoch [5/10], Step [8100/68337], Loss: 5.0481\n",
      "Epoch [5/10], Step [8175/68337], Loss: 5.1220\n",
      "Epoch [5/10], Step [8250/68337], Loss: 4.9786\n",
      "Epoch [5/10], Step [8325/68337], Loss: 5.1545\n",
      "Epoch [5/10], Step [8400/68337], Loss: 5.2873\n",
      "Epoch [5/10], Step [8475/68337], Loss: 5.1663\n",
      "Epoch [5/10], Step [8550/68337], Loss: 5.1702\n",
      "Epoch [5/10], Step [8625/68337], Loss: 4.9307\n",
      "Epoch [5/10], Step [8700/68337], Loss: 5.0649\n",
      "Epoch [5/10], Step [8775/68337], Loss: 5.1632\n",
      "Epoch [5/10], Step [8850/68337], Loss: 5.1688\n",
      "Epoch [5/10], Step [8925/68337], Loss: 5.0758\n",
      "Epoch [5/10], Step [9000/68337], Loss: 5.2185\n",
      "Epoch [5/10], Step [9075/68337], Loss: 5.3258\n",
      "Epoch [5/10], Step [9150/68337], Loss: 5.2069\n",
      "Epoch [5/10], Step [9225/68337], Loss: 4.9055\n",
      "Epoch [5/10], Step [9300/68337], Loss: 5.1197\n",
      "Epoch [5/10], Step [9375/68337], Loss: 5.0300\n",
      "Epoch [5/10], Step [9450/68337], Loss: 4.7613\n",
      "Epoch [5/10], Step [9525/68337], Loss: 5.2867\n",
      "Epoch [5/10], Step [9600/68337], Loss: 4.9251\n",
      "Epoch [5/10], Step [9675/68337], Loss: 5.1469\n",
      "Epoch [5/10], Step [9750/68337], Loss: 5.0140\n",
      "Epoch [5/10], Step [9825/68337], Loss: 4.9820\n",
      "Epoch [5/10], Step [9900/68337], Loss: 4.9809\n",
      "Epoch [5/10], Step [9975/68337], Loss: 4.9886\n",
      "Validation perplexity: 128.38844086075292\n",
      "Epoch [5/10], Step [10050/68337], Loss: 5.2165\n",
      "Epoch [5/10], Step [10125/68337], Loss: 4.9560\n",
      "Epoch [5/10], Step [10200/68337], Loss: 5.2737\n",
      "Epoch [5/10], Step [10275/68337], Loss: 5.1023\n",
      "Epoch [5/10], Step [10350/68337], Loss: 4.8905\n",
      "Epoch [5/10], Step [10425/68337], Loss: 5.0640\n",
      "Epoch [5/10], Step [10500/68337], Loss: 5.2486\n",
      "Epoch [5/10], Step [10575/68337], Loss: 5.1001\n",
      "Epoch [5/10], Step [10650/68337], Loss: 5.1146\n",
      "Epoch [5/10], Step [10725/68337], Loss: 4.9592\n",
      "Epoch [5/10], Step [10800/68337], Loss: 4.8420\n",
      "Epoch [5/10], Step [10875/68337], Loss: 5.1200\n",
      "Epoch [5/10], Step [10950/68337], Loss: 5.1011\n",
      "Epoch [5/10], Step [11025/68337], Loss: 5.0243\n",
      "Epoch [5/10], Step [11100/68337], Loss: 5.1126\n",
      "Epoch [5/10], Step [11175/68337], Loss: 5.2528\n",
      "Epoch [5/10], Step [11250/68337], Loss: 5.1276\n",
      "Epoch [5/10], Step [11325/68337], Loss: 4.9801\n",
      "Epoch [5/10], Step [11400/68337], Loss: 5.1811\n",
      "Epoch [5/10], Step [11475/68337], Loss: 5.1201\n",
      "Epoch [5/10], Step [11550/68337], Loss: 4.9938\n",
      "Epoch [5/10], Step [11625/68337], Loss: 5.3018\n",
      "Epoch [5/10], Step [11700/68337], Loss: 5.0884\n",
      "Epoch [5/10], Step [11775/68337], Loss: 4.8791\n",
      "Epoch [5/10], Step [11850/68337], Loss: 5.0262\n",
      "Epoch [5/10], Step [11925/68337], Loss: 5.0433\n",
      "Epoch [5/10], Step [12000/68337], Loss: 5.2712\n",
      "Epoch [5/10], Step [12075/68337], Loss: 5.0667\n",
      "Epoch [5/10], Step [12150/68337], Loss: 5.1175\n",
      "Epoch [5/10], Step [12225/68337], Loss: 5.0855\n",
      "Epoch [5/10], Step [12300/68337], Loss: 5.1245\n",
      "Epoch [5/10], Step [12375/68337], Loss: 5.2535\n",
      "Epoch [5/10], Step [12450/68337], Loss: 5.1270\n",
      "Epoch [5/10], Step [12525/68337], Loss: 5.4692\n",
      "Epoch [5/10], Step [12600/68337], Loss: 5.1215\n",
      "Epoch [5/10], Step [12675/68337], Loss: 4.9123\n",
      "Epoch [5/10], Step [12750/68337], Loss: 5.0921\n",
      "Epoch [5/10], Step [12825/68337], Loss: 5.2943\n",
      "Epoch [5/10], Step [12900/68337], Loss: 5.0975\n",
      "Epoch [5/10], Step [12975/68337], Loss: 5.1416\n",
      "Epoch [5/10], Step [13050/68337], Loss: 4.7862\n",
      "Epoch [5/10], Step [13125/68337], Loss: 5.0887\n",
      "Epoch [5/10], Step [13200/68337], Loss: 5.1108\n",
      "Epoch [5/10], Step [13275/68337], Loss: 5.3390\n",
      "Epoch [5/10], Step [13350/68337], Loss: 5.2073\n",
      "Epoch [5/10], Step [13425/68337], Loss: 4.8758\n",
      "Epoch [5/10], Step [13500/68337], Loss: 5.1522\n",
      "Epoch [5/10], Step [13575/68337], Loss: 5.0035\n",
      "Epoch [5/10], Step [13650/68337], Loss: 4.9982\n",
      "Epoch [5/10], Step [13725/68337], Loss: 5.2183\n",
      "Epoch [5/10], Step [13800/68337], Loss: 5.2740\n",
      "Epoch [5/10], Step [13875/68337], Loss: 5.2067\n",
      "Epoch [5/10], Step [13950/68337], Loss: 5.2326\n",
      "Epoch [5/10], Step [14025/68337], Loss: 5.0309\n",
      "Epoch [5/10], Step [14100/68337], Loss: 5.2466\n",
      "Epoch [5/10], Step [14175/68337], Loss: 5.1399\n",
      "Epoch [5/10], Step [14250/68337], Loss: 5.0152\n",
      "Epoch [5/10], Step [14325/68337], Loss: 4.9345\n",
      "Epoch [5/10], Step [14400/68337], Loss: 5.1458\n",
      "Epoch [5/10], Step [14475/68337], Loss: 5.1090\n",
      "Epoch [5/10], Step [14550/68337], Loss: 5.2055\n",
      "Epoch [5/10], Step [14625/68337], Loss: 5.0714\n",
      "Epoch [5/10], Step [14700/68337], Loss: 5.2245\n",
      "Epoch [5/10], Step [14775/68337], Loss: 5.2064\n",
      "Epoch [5/10], Step [14850/68337], Loss: 5.1418\n",
      "Epoch [5/10], Step [14925/68337], Loss: 5.0174\n",
      "Epoch [5/10], Step [15000/68337], Loss: 4.9795\n",
      "Epoch [5/10], Step [15075/68337], Loss: 5.0675\n",
      "Epoch [5/10], Step [15150/68337], Loss: 5.1501\n",
      "Epoch [5/10], Step [15225/68337], Loss: 5.1363\n",
      "Epoch [5/10], Step [15300/68337], Loss: 5.0177\n",
      "Epoch [5/10], Step [15375/68337], Loss: 5.2654\n",
      "Epoch [5/10], Step [15450/68337], Loss: 5.0599\n",
      "Epoch [5/10], Step [15525/68337], Loss: 5.1973\n",
      "Epoch [5/10], Step [15600/68337], Loss: 5.3538\n",
      "Epoch [5/10], Step [15675/68337], Loss: 5.3398\n",
      "Epoch [5/10], Step [15750/68337], Loss: 5.1574\n",
      "Epoch [5/10], Step [15825/68337], Loss: 5.1233\n",
      "Epoch [5/10], Step [15900/68337], Loss: 5.2047\n",
      "Epoch [5/10], Step [15975/68337], Loss: 5.1682\n",
      "Epoch [5/10], Step [16050/68337], Loss: 5.2316\n",
      "Epoch [5/10], Step [16125/68337], Loss: 5.3136\n",
      "Epoch [5/10], Step [16200/68337], Loss: 5.1459\n",
      "Epoch [5/10], Step [16275/68337], Loss: 5.1455\n",
      "Epoch [5/10], Step [16350/68337], Loss: 5.1965\n",
      "Epoch [5/10], Step [16425/68337], Loss: 5.0135\n",
      "Epoch [5/10], Step [16500/68337], Loss: 5.3856\n",
      "Epoch [5/10], Step [16575/68337], Loss: 5.2438\n",
      "Epoch [5/10], Step [16650/68337], Loss: 5.1682\n",
      "Epoch [5/10], Step [16725/68337], Loss: 5.2079\n",
      "Epoch [5/10], Step [16800/68337], Loss: 5.1316\n",
      "Epoch [5/10], Step [16875/68337], Loss: 5.1391\n",
      "Epoch [5/10], Step [16950/68337], Loss: 5.0355\n",
      "Epoch [5/10], Step [17025/68337], Loss: 5.1997\n",
      "Epoch [5/10], Step [17100/68337], Loss: 5.2160\n",
      "Epoch [5/10], Step [17175/68337], Loss: 4.8398\n",
      "Epoch [5/10], Step [17250/68337], Loss: 5.0829\n",
      "Epoch [5/10], Step [17325/68337], Loss: 5.1235\n",
      "Epoch [5/10], Step [17400/68337], Loss: 5.2188\n",
      "Epoch [5/10], Step [17475/68337], Loss: 5.3291\n",
      "Epoch [5/10], Step [17550/68337], Loss: 5.2272\n",
      "Epoch [5/10], Step [17625/68337], Loss: 5.2627\n",
      "Epoch [5/10], Step [17700/68337], Loss: 5.0922\n",
      "Epoch [5/10], Step [17775/68337], Loss: 5.3114\n",
      "Epoch [5/10], Step [17850/68337], Loss: 5.0756\n",
      "Epoch [5/10], Step [17925/68337], Loss: 5.2044\n",
      "Epoch [5/10], Step [18000/68337], Loss: 4.8265\n",
      "Epoch [5/10], Step [18075/68337], Loss: 5.0828\n",
      "Epoch [5/10], Step [18150/68337], Loss: 5.1270\n",
      "Epoch [5/10], Step [18225/68337], Loss: 4.7745\n",
      "Epoch [5/10], Step [18300/68337], Loss: 5.1818\n",
      "Epoch [5/10], Step [18375/68337], Loss: 5.1007\n",
      "Epoch [5/10], Step [18450/68337], Loss: 5.2207\n",
      "Epoch [5/10], Step [18525/68337], Loss: 5.3831\n",
      "Epoch [5/10], Step [18600/68337], Loss: 5.1211\n",
      "Epoch [5/10], Step [18675/68337], Loss: 5.1697\n",
      "Epoch [5/10], Step [18750/68337], Loss: 5.0233\n",
      "Epoch [5/10], Step [18825/68337], Loss: 5.0162\n",
      "Epoch [5/10], Step [18900/68337], Loss: 5.0894\n",
      "Epoch [5/10], Step [18975/68337], Loss: 5.2044\n",
      "Epoch [5/10], Step [19050/68337], Loss: 4.8575\n",
      "Epoch [5/10], Step [19125/68337], Loss: 5.1384\n",
      "Epoch [5/10], Step [19200/68337], Loss: 5.0398\n",
      "Epoch [5/10], Step [19275/68337], Loss: 4.9616\n",
      "Epoch [5/10], Step [19350/68337], Loss: 4.9678\n",
      "Epoch [5/10], Step [19425/68337], Loss: 5.2289\n",
      "Epoch [5/10], Step [19500/68337], Loss: 5.0156\n",
      "Epoch [5/10], Step [19575/68337], Loss: 5.3193\n",
      "Epoch [5/10], Step [19650/68337], Loss: 4.8650\n",
      "Epoch [5/10], Step [19725/68337], Loss: 5.0241\n",
      "Epoch [5/10], Step [19800/68337], Loss: 4.9866\n",
      "Epoch [5/10], Step [19875/68337], Loss: 5.1261\n",
      "Epoch [5/10], Step [19950/68337], Loss: 5.1213\n",
      "Validation perplexity: 128.79245014447298\n",
      "Epoch [5/10], Step [20025/68337], Loss: 5.2987\n",
      "Epoch [5/10], Step [20100/68337], Loss: 4.9451\n",
      "Epoch [5/10], Step [20175/68337], Loss: 5.2402\n",
      "Epoch [5/10], Step [20250/68337], Loss: 4.9760\n",
      "Epoch [5/10], Step [20325/68337], Loss: 4.8395\n",
      "Epoch [5/10], Step [20400/68337], Loss: 5.1890\n",
      "Epoch [5/10], Step [20475/68337], Loss: 5.1815\n",
      "Epoch [5/10], Step [20550/68337], Loss: 5.1110\n",
      "Epoch [5/10], Step [20625/68337], Loss: 4.9537\n",
      "Epoch [5/10], Step [20700/68337], Loss: 5.2317\n",
      "Epoch [5/10], Step [20775/68337], Loss: 5.2131\n",
      "Epoch [5/10], Step [20850/68337], Loss: 5.1008\n",
      "Epoch [5/10], Step [20925/68337], Loss: 4.9250\n",
      "Epoch [5/10], Step [21000/68337], Loss: 4.8729\n",
      "Epoch [5/10], Step [21075/68337], Loss: 5.2420\n",
      "Epoch [5/10], Step [21150/68337], Loss: 5.1721\n",
      "Epoch [5/10], Step [21225/68337], Loss: 5.1996\n",
      "Epoch [5/10], Step [21300/68337], Loss: 5.1749\n",
      "Epoch [5/10], Step [21375/68337], Loss: 5.0038\n",
      "Epoch [5/10], Step [21450/68337], Loss: 5.2511\n",
      "Epoch [5/10], Step [21525/68337], Loss: 5.1856\n",
      "Epoch [5/10], Step [21600/68337], Loss: 5.0089\n",
      "Epoch [5/10], Step [21675/68337], Loss: 4.9568\n",
      "Epoch [5/10], Step [21750/68337], Loss: 5.1033\n",
      "Epoch [5/10], Step [21825/68337], Loss: 5.0314\n",
      "Epoch [5/10], Step [21900/68337], Loss: 5.3475\n",
      "Epoch [5/10], Step [21975/68337], Loss: 5.2377\n",
      "Epoch [5/10], Step [22050/68337], Loss: 5.0093\n",
      "Epoch [5/10], Step [22125/68337], Loss: 5.1611\n",
      "Epoch [5/10], Step [22200/68337], Loss: 5.2075\n",
      "Epoch [5/10], Step [22275/68337], Loss: 5.0896\n",
      "Epoch [5/10], Step [22350/68337], Loss: 5.1116\n",
      "Epoch [5/10], Step [22425/68337], Loss: 5.2577\n",
      "Epoch [5/10], Step [22500/68337], Loss: 5.0407\n",
      "Epoch [5/10], Step [22575/68337], Loss: 5.0814\n",
      "Epoch [5/10], Step [22650/68337], Loss: 5.0757\n",
      "Epoch [5/10], Step [22725/68337], Loss: 5.3017\n",
      "Epoch [5/10], Step [22800/68337], Loss: 5.1525\n",
      "Epoch [5/10], Step [22875/68337], Loss: 5.0970\n",
      "Epoch [5/10], Step [22950/68337], Loss: 5.1112\n",
      "Epoch [5/10], Step [23025/68337], Loss: 5.2212\n",
      "Epoch [5/10], Step [23100/68337], Loss: 4.9980\n",
      "Epoch [5/10], Step [23175/68337], Loss: 5.1342\n",
      "Epoch [5/10], Step [23250/68337], Loss: 5.1383\n",
      "Epoch [5/10], Step [23325/68337], Loss: 4.9447\n",
      "Epoch [5/10], Step [23400/68337], Loss: 5.2814\n",
      "Epoch [5/10], Step [23475/68337], Loss: 5.0945\n",
      "Epoch [5/10], Step [23550/68337], Loss: 5.0912\n",
      "Epoch [5/10], Step [23625/68337], Loss: 5.2556\n",
      "Epoch [5/10], Step [23700/68337], Loss: 4.9795\n",
      "Epoch [5/10], Step [23775/68337], Loss: 5.2322\n",
      "Epoch [5/10], Step [23850/68337], Loss: 5.1666\n",
      "Epoch [5/10], Step [23925/68337], Loss: 5.0675\n",
      "Epoch [5/10], Step [24000/68337], Loss: 5.2075\n",
      "Epoch [5/10], Step [24075/68337], Loss: 5.0574\n",
      "Epoch [5/10], Step [24150/68337], Loss: 5.0334\n",
      "Epoch [5/10], Step [24225/68337], Loss: 5.0244\n",
      "Epoch [5/10], Step [24300/68337], Loss: 5.0962\n",
      "Epoch [5/10], Step [24375/68337], Loss: 5.0924\n",
      "Epoch [5/10], Step [24450/68337], Loss: 5.2423\n",
      "Epoch [5/10], Step [24525/68337], Loss: 4.9939\n",
      "Epoch [5/10], Step [24600/68337], Loss: 5.0927\n",
      "Epoch [5/10], Step [24675/68337], Loss: 5.2114\n",
      "Epoch [5/10], Step [24750/68337], Loss: 5.1058\n",
      "Epoch [5/10], Step [24825/68337], Loss: 5.0097\n",
      "Epoch [5/10], Step [24900/68337], Loss: 5.0540\n",
      "Epoch [5/10], Step [24975/68337], Loss: 5.1208\n",
      "Epoch [5/10], Step [25050/68337], Loss: 5.1061\n",
      "Epoch [5/10], Step [25125/68337], Loss: 5.2981\n",
      "Epoch [5/10], Step [25200/68337], Loss: 5.1078\n",
      "Epoch [5/10], Step [25275/68337], Loss: 5.1995\n",
      "Epoch [5/10], Step [25350/68337], Loss: 5.1398\n",
      "Epoch [5/10], Step [25425/68337], Loss: 5.0310\n",
      "Epoch [5/10], Step [25500/68337], Loss: 5.1165\n",
      "Epoch [5/10], Step [25575/68337], Loss: 4.9813\n",
      "Epoch [5/10], Step [25650/68337], Loss: 5.1271\n",
      "Epoch [5/10], Step [25725/68337], Loss: 5.1178\n",
      "Epoch [5/10], Step [25800/68337], Loss: 5.1319\n",
      "Epoch [5/10], Step [25875/68337], Loss: 5.0776\n",
      "Epoch [5/10], Step [25950/68337], Loss: 4.9819\n",
      "Epoch [5/10], Step [26025/68337], Loss: 4.9773\n",
      "Epoch [5/10], Step [26100/68337], Loss: 5.1514\n",
      "Epoch [5/10], Step [26175/68337], Loss: 5.0858\n",
      "Epoch [5/10], Step [26250/68337], Loss: 5.2039\n",
      "Epoch [5/10], Step [26325/68337], Loss: 5.0582\n",
      "Epoch [5/10], Step [26400/68337], Loss: 5.1045\n",
      "Epoch [5/10], Step [26475/68337], Loss: 5.0080\n",
      "Epoch [5/10], Step [26550/68337], Loss: 5.1235\n",
      "Epoch [5/10], Step [26625/68337], Loss: 5.3150\n",
      "Epoch [5/10], Step [26700/68337], Loss: 5.1793\n",
      "Epoch [5/10], Step [26775/68337], Loss: 5.0977\n",
      "Epoch [5/10], Step [26850/68337], Loss: 4.9775\n",
      "Epoch [5/10], Step [26925/68337], Loss: 5.2997\n",
      "Epoch [5/10], Step [27000/68337], Loss: 5.2069\n",
      "Epoch [5/10], Step [27075/68337], Loss: 5.2058\n",
      "Epoch [5/10], Step [27150/68337], Loss: 4.9067\n",
      "Epoch [5/10], Step [27225/68337], Loss: 5.1652\n",
      "Epoch [5/10], Step [27300/68337], Loss: 5.2024\n",
      "Epoch [5/10], Step [27375/68337], Loss: 5.2320\n",
      "Epoch [5/10], Step [27450/68337], Loss: 5.0272\n",
      "Epoch [5/10], Step [27525/68337], Loss: 5.1383\n",
      "Epoch [5/10], Step [27600/68337], Loss: 5.1795\n",
      "Epoch [5/10], Step [27675/68337], Loss: 5.2038\n",
      "Epoch [5/10], Step [27750/68337], Loss: 5.3236\n",
      "Epoch [5/10], Step [27825/68337], Loss: 5.1372\n",
      "Epoch [5/10], Step [27900/68337], Loss: 4.9529\n",
      "Epoch [5/10], Step [27975/68337], Loss: 5.1337\n",
      "Epoch [5/10], Step [28050/68337], Loss: 5.0815\n",
      "Epoch [5/10], Step [28125/68337], Loss: 5.1000\n",
      "Epoch [5/10], Step [28200/68337], Loss: 5.1672\n",
      "Epoch [5/10], Step [28275/68337], Loss: 5.0870\n",
      "Epoch [5/10], Step [28350/68337], Loss: 5.1016\n",
      "Epoch [5/10], Step [28425/68337], Loss: 5.0414\n",
      "Epoch [5/10], Step [28500/68337], Loss: 5.2804\n",
      "Epoch [5/10], Step [28575/68337], Loss: 5.2293\n",
      "Epoch [5/10], Step [28650/68337], Loss: 5.0738\n",
      "Epoch [5/10], Step [28725/68337], Loss: 5.1733\n",
      "Epoch [5/10], Step [28800/68337], Loss: 5.0742\n",
      "Epoch [5/10], Step [28875/68337], Loss: 5.1595\n",
      "Epoch [5/10], Step [28950/68337], Loss: 5.2513\n",
      "Epoch [5/10], Step [29025/68337], Loss: 5.0585\n",
      "Epoch [5/10], Step [29100/68337], Loss: 5.1756\n",
      "Epoch [5/10], Step [29175/68337], Loss: 5.1814\n",
      "Epoch [5/10], Step [29250/68337], Loss: 5.1153\n",
      "Epoch [5/10], Step [29325/68337], Loss: 5.0843\n",
      "Epoch [5/10], Step [29400/68337], Loss: 5.0897\n",
      "Epoch [5/10], Step [29475/68337], Loss: 5.2530\n",
      "Epoch [5/10], Step [29550/68337], Loss: 5.1372\n",
      "Epoch [5/10], Step [29625/68337], Loss: 5.0929\n",
      "Epoch [5/10], Step [29700/68337], Loss: 5.0515\n",
      "Epoch [5/10], Step [29775/68337], Loss: 5.1129\n",
      "Epoch [5/10], Step [29850/68337], Loss: 5.1135\n",
      "Epoch [5/10], Step [29925/68337], Loss: 5.1580\n",
      "Epoch [5/10], Step [30000/68337], Loss: 5.2189\n",
      "Validation perplexity: 128.17044219103033\n",
      "Epoch [5/10], Step [30075/68337], Loss: 5.0640\n",
      "Epoch [5/10], Step [30150/68337], Loss: 5.0713\n",
      "Epoch [5/10], Step [30225/68337], Loss: 5.1031\n",
      "Epoch [5/10], Step [30300/68337], Loss: 4.9715\n",
      "Epoch [5/10], Step [30375/68337], Loss: 5.2081\n",
      "Epoch [5/10], Step [30450/68337], Loss: 5.3122\n",
      "Epoch [5/10], Step [30525/68337], Loss: 5.0740\n",
      "Epoch [5/10], Step [30600/68337], Loss: 5.3330\n",
      "Epoch [5/10], Step [30675/68337], Loss: 5.1593\n",
      "Epoch [5/10], Step [30750/68337], Loss: 4.9423\n",
      "Epoch [5/10], Step [30825/68337], Loss: 5.0795\n",
      "Epoch [5/10], Step [30900/68337], Loss: 4.9943\n",
      "Epoch [5/10], Step [30975/68337], Loss: 5.2842\n",
      "Epoch [5/10], Step [31050/68337], Loss: 4.9704\n",
      "Epoch [5/10], Step [31125/68337], Loss: 5.3798\n",
      "Epoch [5/10], Step [31200/68337], Loss: 5.1664\n",
      "Epoch [5/10], Step [31275/68337], Loss: 5.0633\n",
      "Epoch [5/10], Step [31350/68337], Loss: 5.2439\n",
      "Epoch [5/10], Step [31425/68337], Loss: 5.1368\n",
      "Epoch [5/10], Step [31500/68337], Loss: 5.2530\n",
      "Epoch [5/10], Step [31575/68337], Loss: 5.2477\n",
      "Epoch [5/10], Step [31650/68337], Loss: 5.2298\n",
      "Epoch [5/10], Step [31725/68337], Loss: 5.1833\n",
      "Epoch [5/10], Step [31800/68337], Loss: 5.0992\n",
      "Epoch [5/10], Step [31875/68337], Loss: 5.1977\n",
      "Epoch [5/10], Step [31950/68337], Loss: 5.1519\n",
      "Epoch [5/10], Step [32025/68337], Loss: 5.2906\n",
      "Epoch [5/10], Step [32100/68337], Loss: 5.0297\n",
      "Epoch [5/10], Step [32175/68337], Loss: 5.3135\n",
      "Epoch [5/10], Step [32250/68337], Loss: 5.2476\n",
      "Epoch [5/10], Step [32325/68337], Loss: 5.3159\n",
      "Epoch [5/10], Step [32400/68337], Loss: 5.1131\n",
      "Epoch [5/10], Step [32475/68337], Loss: 5.0585\n",
      "Epoch [5/10], Step [32550/68337], Loss: 5.0941\n",
      "Epoch [5/10], Step [32625/68337], Loss: 5.0738\n",
      "Epoch [5/10], Step [32700/68337], Loss: 5.0991\n",
      "Epoch [5/10], Step [32775/68337], Loss: 5.0230\n",
      "Epoch [5/10], Step [32850/68337], Loss: 5.0793\n",
      "Epoch [5/10], Step [32925/68337], Loss: 5.2276\n",
      "Epoch [5/10], Step [33000/68337], Loss: 5.1633\n",
      "Epoch [5/10], Step [33075/68337], Loss: 5.0838\n",
      "Epoch [5/10], Step [33150/68337], Loss: 4.9598\n",
      "Epoch [5/10], Step [33225/68337], Loss: 5.1848\n",
      "Epoch [5/10], Step [33300/68337], Loss: 5.1231\n",
      "Epoch [5/10], Step [33375/68337], Loss: 5.0696\n",
      "Epoch [5/10], Step [33450/68337], Loss: 5.3287\n",
      "Epoch [5/10], Step [33525/68337], Loss: 5.0967\n",
      "Epoch [5/10], Step [33600/68337], Loss: 5.1988\n",
      "Epoch [5/10], Step [33675/68337], Loss: 5.2857\n",
      "Epoch [5/10], Step [33750/68337], Loss: 5.0534\n",
      "Epoch [5/10], Step [33825/68337], Loss: 5.1345\n",
      "Epoch [5/10], Step [33900/68337], Loss: 5.2035\n",
      "Epoch [5/10], Step [33975/68337], Loss: 5.0578\n",
      "Epoch [5/10], Step [34050/68337], Loss: 5.1523\n",
      "Epoch [5/10], Step [34125/68337], Loss: 5.1507\n",
      "Epoch [5/10], Step [34200/68337], Loss: 5.2346\n",
      "Epoch [5/10], Step [34275/68337], Loss: 5.1103\n",
      "Epoch [5/10], Step [34350/68337], Loss: 5.0784\n",
      "Epoch [5/10], Step [34425/68337], Loss: 5.0468\n",
      "Epoch [5/10], Step [34500/68337], Loss: 5.0956\n",
      "Epoch [5/10], Step [34575/68337], Loss: 5.0498\n",
      "Epoch [5/10], Step [34650/68337], Loss: 5.1867\n",
      "Epoch [5/10], Step [34725/68337], Loss: 5.1249\n",
      "Epoch [5/10], Step [34800/68337], Loss: 5.3716\n",
      "Epoch [5/10], Step [34875/68337], Loss: 5.3593\n",
      "Epoch [5/10], Step [34950/68337], Loss: 5.1466\n",
      "Epoch [5/10], Step [35025/68337], Loss: 5.3934\n",
      "Epoch [5/10], Step [35100/68337], Loss: 5.1826\n",
      "Epoch [5/10], Step [35175/68337], Loss: 5.0872\n",
      "Epoch [5/10], Step [35250/68337], Loss: 5.1276\n",
      "Epoch [5/10], Step [35325/68337], Loss: 5.1337\n",
      "Epoch [5/10], Step [35400/68337], Loss: 5.1157\n",
      "Epoch [5/10], Step [35475/68337], Loss: 5.1062\n",
      "Epoch [5/10], Step [35550/68337], Loss: 4.9160\n",
      "Epoch [5/10], Step [35625/68337], Loss: 5.0242\n",
      "Epoch [5/10], Step [35700/68337], Loss: 5.0272\n",
      "Epoch [5/10], Step [35775/68337], Loss: 5.1774\n",
      "Epoch [5/10], Step [35850/68337], Loss: 4.9000\n",
      "Epoch [5/10], Step [35925/68337], Loss: 5.2305\n",
      "Epoch [5/10], Step [36000/68337], Loss: 5.2920\n",
      "Epoch [5/10], Step [36075/68337], Loss: 5.0090\n",
      "Epoch [5/10], Step [36150/68337], Loss: 4.9948\n",
      "Epoch [5/10], Step [36225/68337], Loss: 5.0304\n",
      "Epoch [5/10], Step [36300/68337], Loss: 5.0318\n",
      "Epoch [5/10], Step [36375/68337], Loss: 5.1868\n",
      "Epoch [5/10], Step [36450/68337], Loss: 5.5331\n",
      "Epoch [5/10], Step [36525/68337], Loss: 5.2030\n",
      "Epoch [5/10], Step [36600/68337], Loss: 5.2017\n",
      "Epoch [5/10], Step [36675/68337], Loss: 5.1662\n",
      "Epoch [5/10], Step [36750/68337], Loss: 4.9951\n",
      "Epoch [5/10], Step [36825/68337], Loss: 5.1447\n",
      "Epoch [5/10], Step [36900/68337], Loss: 5.2148\n",
      "Epoch [5/10], Step [36975/68337], Loss: 5.2493\n",
      "Epoch [5/10], Step [37050/68337], Loss: 4.8637\n",
      "Epoch [5/10], Step [37125/68337], Loss: 5.1551\n",
      "Epoch [5/10], Step [37200/68337], Loss: 5.1778\n",
      "Epoch [5/10], Step [37275/68337], Loss: 5.2342\n",
      "Epoch [5/10], Step [37350/68337], Loss: 5.0182\n",
      "Epoch [5/10], Step [37425/68337], Loss: 5.4094\n",
      "Epoch [5/10], Step [37500/68337], Loss: 5.2668\n",
      "Epoch [5/10], Step [37575/68337], Loss: 5.3324\n",
      "Epoch [5/10], Step [37650/68337], Loss: 5.3646\n",
      "Epoch [5/10], Step [37725/68337], Loss: 5.0636\n",
      "Epoch [5/10], Step [37800/68337], Loss: 5.1659\n",
      "Epoch [5/10], Step [37875/68337], Loss: 5.1004\n",
      "Epoch [5/10], Step [37950/68337], Loss: 5.1372\n",
      "Epoch [5/10], Step [38025/68337], Loss: 5.1727\n",
      "Epoch [5/10], Step [38100/68337], Loss: 4.8722\n",
      "Epoch [5/10], Step [38175/68337], Loss: 5.3402\n",
      "Epoch [5/10], Step [38250/68337], Loss: 5.0466\n",
      "Epoch [5/10], Step [38325/68337], Loss: 5.0846\n",
      "Epoch [5/10], Step [38400/68337], Loss: 4.7571\n",
      "Epoch [5/10], Step [38475/68337], Loss: 5.1428\n",
      "Epoch [5/10], Step [38550/68337], Loss: 5.2039\n",
      "Epoch [5/10], Step [38625/68337], Loss: 5.0022\n",
      "Epoch [5/10], Step [38700/68337], Loss: 5.1596\n",
      "Epoch [5/10], Step [38775/68337], Loss: 4.9338\n",
      "Epoch [5/10], Step [38850/68337], Loss: 5.0962\n",
      "Epoch [5/10], Step [38925/68337], Loss: 5.0385\n",
      "Epoch [5/10], Step [39000/68337], Loss: 5.2081\n",
      "Epoch [5/10], Step [39075/68337], Loss: 5.0994\n",
      "Epoch [5/10], Step [39150/68337], Loss: 5.0104\n",
      "Epoch [5/10], Step [39225/68337], Loss: 4.9543\n",
      "Epoch [5/10], Step [39300/68337], Loss: 5.1416\n",
      "Epoch [5/10], Step [39375/68337], Loss: 4.7798\n",
      "Epoch [5/10], Step [39450/68337], Loss: 5.2035\n",
      "Epoch [5/10], Step [39525/68337], Loss: 5.2779\n",
      "Epoch [5/10], Step [39600/68337], Loss: 5.0173\n",
      "Epoch [5/10], Step [39675/68337], Loss: 5.0473\n",
      "Epoch [5/10], Step [39750/68337], Loss: 5.2216\n",
      "Epoch [5/10], Step [39825/68337], Loss: 4.9480\n",
      "Epoch [5/10], Step [39900/68337], Loss: 5.0752\n",
      "Epoch [5/10], Step [39975/68337], Loss: 5.1109\n",
      "Validation perplexity: 127.35246816509667\n",
      "Epoch [5/10], Step [40050/68337], Loss: 5.1011\n",
      "Epoch [5/10], Step [40125/68337], Loss: 5.2854\n",
      "Epoch [5/10], Step [40200/68337], Loss: 5.2064\n",
      "Epoch [5/10], Step [40275/68337], Loss: 5.0036\n",
      "Epoch [5/10], Step [40350/68337], Loss: 5.1614\n",
      "Epoch [5/10], Step [40425/68337], Loss: 5.0425\n",
      "Epoch [5/10], Step [40500/68337], Loss: 5.1463\n",
      "Epoch [5/10], Step [40575/68337], Loss: 5.3573\n",
      "Epoch [5/10], Step [40650/68337], Loss: 5.0038\n",
      "Epoch [5/10], Step [40725/68337], Loss: 5.2138\n",
      "Epoch [5/10], Step [40800/68337], Loss: 5.0162\n",
      "Epoch [5/10], Step [40875/68337], Loss: 4.9253\n",
      "Epoch [5/10], Step [40950/68337], Loss: 5.1277\n",
      "Epoch [5/10], Step [41025/68337], Loss: 5.1776\n",
      "Epoch [5/10], Step [41100/68337], Loss: 5.0930\n",
      "Epoch [5/10], Step [41175/68337], Loss: 5.2218\n",
      "Epoch [5/10], Step [41250/68337], Loss: 5.1648\n",
      "Epoch [5/10], Step [41325/68337], Loss: 5.0279\n",
      "Epoch [5/10], Step [41400/68337], Loss: 5.0886\n",
      "Epoch [5/10], Step [41475/68337], Loss: 5.0680\n",
      "Epoch [5/10], Step [41550/68337], Loss: 4.9993\n",
      "Epoch [5/10], Step [41625/68337], Loss: 5.2204\n",
      "Epoch [5/10], Step [41700/68337], Loss: 4.7992\n",
      "Epoch [5/10], Step [41775/68337], Loss: 5.1611\n",
      "Epoch [5/10], Step [41850/68337], Loss: 5.1033\n",
      "Epoch [5/10], Step [41925/68337], Loss: 5.2242\n",
      "Epoch [5/10], Step [42000/68337], Loss: 5.0649\n",
      "Epoch [5/10], Step [42075/68337], Loss: 5.2045\n",
      "Epoch [5/10], Step [42150/68337], Loss: 5.1098\n",
      "Epoch [5/10], Step [42225/68337], Loss: 5.0127\n",
      "Epoch [5/10], Step [42300/68337], Loss: 5.0527\n",
      "Epoch [5/10], Step [42375/68337], Loss: 5.1815\n",
      "Epoch [5/10], Step [42450/68337], Loss: 5.1222\n",
      "Epoch [5/10], Step [42525/68337], Loss: 5.2690\n",
      "Epoch [5/10], Step [42600/68337], Loss: 5.2439\n",
      "Epoch [5/10], Step [42675/68337], Loss: 4.9656\n",
      "Epoch [5/10], Step [42750/68337], Loss: 4.9943\n",
      "Epoch [5/10], Step [42825/68337], Loss: 5.1752\n",
      "Epoch [5/10], Step [42900/68337], Loss: 4.9016\n",
      "Epoch [5/10], Step [42975/68337], Loss: 5.1774\n",
      "Epoch [5/10], Step [43050/68337], Loss: 5.1210\n",
      "Epoch [5/10], Step [43125/68337], Loss: 5.1472\n",
      "Epoch [5/10], Step [43200/68337], Loss: 4.8469\n",
      "Epoch [5/10], Step [43275/68337], Loss: 5.1015\n",
      "Epoch [5/10], Step [43350/68337], Loss: 5.1497\n",
      "Epoch [5/10], Step [43425/68337], Loss: 5.0474\n",
      "Epoch [5/10], Step [43500/68337], Loss: 5.0237\n",
      "Epoch [5/10], Step [43575/68337], Loss: 5.2169\n",
      "Epoch [5/10], Step [43650/68337], Loss: 5.2131\n",
      "Epoch [5/10], Step [43725/68337], Loss: 5.0761\n",
      "Epoch [5/10], Step [43800/68337], Loss: 5.2027\n",
      "Epoch [5/10], Step [43875/68337], Loss: 5.0860\n",
      "Epoch [5/10], Step [43950/68337], Loss: 5.1760\n",
      "Epoch [5/10], Step [44025/68337], Loss: 5.1065\n",
      "Epoch [5/10], Step [44100/68337], Loss: 5.1514\n",
      "Epoch [5/10], Step [44175/68337], Loss: 5.2231\n",
      "Epoch [5/10], Step [44250/68337], Loss: 5.1997\n",
      "Epoch [5/10], Step [44325/68337], Loss: 5.2097\n",
      "Epoch [5/10], Step [44400/68337], Loss: 5.1277\n",
      "Epoch [5/10], Step [44475/68337], Loss: 5.4558\n",
      "Epoch [5/10], Step [44550/68337], Loss: 5.0506\n",
      "Epoch [5/10], Step [44625/68337], Loss: 5.3256\n",
      "Epoch [5/10], Step [44700/68337], Loss: 4.9214\n",
      "Epoch [5/10], Step [44775/68337], Loss: 5.1378\n",
      "Epoch [5/10], Step [44850/68337], Loss: 5.1948\n",
      "Epoch [5/10], Step [44925/68337], Loss: 5.0045\n",
      "Epoch [5/10], Step [45000/68337], Loss: 5.0000\n",
      "Epoch [5/10], Step [45075/68337], Loss: 4.9713\n",
      "Epoch [5/10], Step [45150/68337], Loss: 5.1407\n",
      "Epoch [5/10], Step [45225/68337], Loss: 5.1418\n",
      "Epoch [5/10], Step [45300/68337], Loss: 5.1568\n",
      "Epoch [5/10], Step [45375/68337], Loss: 4.9589\n",
      "Epoch [5/10], Step [45450/68337], Loss: 5.1798\n",
      "Epoch [5/10], Step [45525/68337], Loss: 5.1631\n",
      "Epoch [5/10], Step [45600/68337], Loss: 5.1489\n",
      "Epoch [5/10], Step [45675/68337], Loss: 5.2270\n",
      "Epoch [5/10], Step [45750/68337], Loss: 5.1921\n",
      "Epoch [5/10], Step [45825/68337], Loss: 5.1555\n",
      "Epoch [5/10], Step [45900/68337], Loss: 5.1655\n",
      "Epoch [5/10], Step [45975/68337], Loss: 5.0103\n",
      "Epoch [5/10], Step [46050/68337], Loss: 5.1186\n",
      "Epoch [5/10], Step [46125/68337], Loss: 5.1979\n",
      "Epoch [5/10], Step [46200/68337], Loss: 4.9970\n",
      "Epoch [5/10], Step [46275/68337], Loss: 4.9153\n",
      "Epoch [5/10], Step [46350/68337], Loss: 4.9801\n",
      "Epoch [5/10], Step [46425/68337], Loss: 5.1400\n",
      "Epoch [5/10], Step [46500/68337], Loss: 5.2774\n",
      "Epoch [5/10], Step [46575/68337], Loss: 5.0891\n",
      "Epoch [5/10], Step [46650/68337], Loss: 5.2514\n",
      "Epoch [5/10], Step [46725/68337], Loss: 5.2663\n",
      "Epoch [5/10], Step [46800/68337], Loss: 4.8352\n",
      "Epoch [5/10], Step [46875/68337], Loss: 5.1206\n",
      "Epoch [5/10], Step [46950/68337], Loss: 5.1055\n",
      "Epoch [5/10], Step [47025/68337], Loss: 5.0210\n",
      "Epoch [5/10], Step [47100/68337], Loss: 5.2930\n",
      "Epoch [5/10], Step [47175/68337], Loss: 5.1624\n",
      "Epoch [5/10], Step [47250/68337], Loss: 5.0014\n",
      "Epoch [5/10], Step [47325/68337], Loss: 5.0233\n",
      "Epoch [5/10], Step [47400/68337], Loss: 5.2041\n",
      "Epoch [5/10], Step [47475/68337], Loss: 5.1928\n",
      "Epoch [5/10], Step [47550/68337], Loss: 5.0903\n",
      "Epoch [5/10], Step [47625/68337], Loss: 5.0946\n",
      "Epoch [5/10], Step [47700/68337], Loss: 5.0466\n",
      "Epoch [5/10], Step [47775/68337], Loss: 5.0116\n",
      "Epoch [5/10], Step [47850/68337], Loss: 5.2734\n",
      "Epoch [5/10], Step [47925/68337], Loss: 5.3345\n",
      "Epoch [5/10], Step [48000/68337], Loss: 5.1391\n",
      "Epoch [5/10], Step [48075/68337], Loss: 5.0348\n",
      "Epoch [5/10], Step [48150/68337], Loss: 5.3085\n",
      "Epoch [5/10], Step [48225/68337], Loss: 5.2046\n",
      "Epoch [5/10], Step [48300/68337], Loss: 4.9178\n",
      "Epoch [5/10], Step [48375/68337], Loss: 5.0807\n",
      "Epoch [5/10], Step [48450/68337], Loss: 5.0942\n",
      "Epoch [5/10], Step [48525/68337], Loss: 5.1712\n",
      "Epoch [5/10], Step [48600/68337], Loss: 5.0118\n",
      "Epoch [5/10], Step [48675/68337], Loss: 5.0724\n",
      "Epoch [5/10], Step [48750/68337], Loss: 5.1100\n",
      "Epoch [5/10], Step [48825/68337], Loss: 5.1003\n",
      "Epoch [5/10], Step [48900/68337], Loss: 5.2640\n",
      "Epoch [5/10], Step [48975/68337], Loss: 5.1744\n",
      "Epoch [5/10], Step [49050/68337], Loss: 5.2647\n",
      "Epoch [5/10], Step [49125/68337], Loss: 5.1780\n",
      "Epoch [5/10], Step [49200/68337], Loss: 4.9842\n",
      "Epoch [5/10], Step [49275/68337], Loss: 5.1747\n",
      "Epoch [5/10], Step [49350/68337], Loss: 5.0947\n",
      "Epoch [5/10], Step [49425/68337], Loss: 4.9660\n",
      "Epoch [5/10], Step [49500/68337], Loss: 5.0489\n",
      "Epoch [5/10], Step [49575/68337], Loss: 5.0231\n",
      "Epoch [5/10], Step [49650/68337], Loss: 5.1192\n",
      "Epoch [5/10], Step [49725/68337], Loss: 5.0629\n",
      "Epoch [5/10], Step [49800/68337], Loss: 5.1138\n",
      "Epoch [5/10], Step [49875/68337], Loss: 5.0439\n",
      "Epoch [5/10], Step [49950/68337], Loss: 4.8476\n",
      "Validation perplexity: 126.78796131774779\n",
      "Epoch [5/10], Step [50025/68337], Loss: 5.1905\n",
      "Epoch [5/10], Step [50100/68337], Loss: 5.0633\n",
      "Epoch [5/10], Step [50175/68337], Loss: 5.2395\n",
      "Epoch [5/10], Step [50250/68337], Loss: 5.0392\n",
      "Epoch [5/10], Step [50325/68337], Loss: 5.0820\n",
      "Epoch [5/10], Step [50400/68337], Loss: 5.0043\n",
      "Epoch [5/10], Step [50475/68337], Loss: 5.1963\n",
      "Epoch [5/10], Step [50550/68337], Loss: 5.2226\n",
      "Epoch [5/10], Step [50625/68337], Loss: 5.3111\n",
      "Epoch [5/10], Step [50700/68337], Loss: 5.1811\n",
      "Epoch [5/10], Step [50775/68337], Loss: 5.3766\n",
      "Epoch [5/10], Step [50850/68337], Loss: 5.2452\n",
      "Epoch [5/10], Step [50925/68337], Loss: 4.9384\n",
      "Epoch [5/10], Step [51000/68337], Loss: 5.1572\n",
      "Epoch [5/10], Step [51075/68337], Loss: 5.1295\n",
      "Epoch [5/10], Step [51150/68337], Loss: 5.1606\n",
      "Epoch [5/10], Step [51225/68337], Loss: 5.2188\n",
      "Epoch [5/10], Step [51300/68337], Loss: 5.1704\n",
      "Epoch [5/10], Step [51375/68337], Loss: 5.1990\n",
      "Epoch [5/10], Step [51450/68337], Loss: 5.0997\n",
      "Epoch [5/10], Step [51525/68337], Loss: 4.9331\n",
      "Epoch [5/10], Step [51600/68337], Loss: 5.1674\n",
      "Epoch [5/10], Step [51675/68337], Loss: 5.2791\n",
      "Epoch [5/10], Step [51750/68337], Loss: 5.4091\n",
      "Epoch [5/10], Step [51825/68337], Loss: 5.0225\n",
      "Epoch [5/10], Step [51900/68337], Loss: 5.0821\n",
      "Epoch [5/10], Step [51975/68337], Loss: 5.1336\n",
      "Epoch [5/10], Step [52050/68337], Loss: 5.1837\n",
      "Epoch [5/10], Step [52125/68337], Loss: 4.8923\n",
      "Epoch [5/10], Step [52200/68337], Loss: 5.0587\n",
      "Epoch [5/10], Step [52275/68337], Loss: 5.0640\n",
      "Epoch [5/10], Step [52350/68337], Loss: 5.2075\n",
      "Epoch [5/10], Step [52425/68337], Loss: 5.1137\n",
      "Epoch [5/10], Step [52500/68337], Loss: 5.0989\n",
      "Epoch [5/10], Step [52575/68337], Loss: 5.0851\n",
      "Epoch [5/10], Step [52650/68337], Loss: 5.1613\n",
      "Epoch [5/10], Step [52725/68337], Loss: 5.2031\n",
      "Epoch [5/10], Step [52800/68337], Loss: 5.4324\n",
      "Epoch [5/10], Step [52875/68337], Loss: 5.0678\n",
      "Epoch [5/10], Step [52950/68337], Loss: 4.8844\n",
      "Epoch [5/10], Step [53025/68337], Loss: 5.2052\n",
      "Epoch [5/10], Step [53100/68337], Loss: 5.0758\n",
      "Epoch [5/10], Step [53175/68337], Loss: 5.0052\n",
      "Epoch [5/10], Step [53250/68337], Loss: 5.2919\n",
      "Epoch [5/10], Step [53325/68337], Loss: 5.2519\n",
      "Epoch [5/10], Step [53400/68337], Loss: 5.1248\n",
      "Epoch [5/10], Step [53475/68337], Loss: 5.1545\n",
      "Epoch [5/10], Step [53550/68337], Loss: 5.3380\n",
      "Epoch [5/10], Step [53625/68337], Loss: 5.0946\n",
      "Epoch [5/10], Step [53700/68337], Loss: 5.2501\n",
      "Epoch [5/10], Step [53775/68337], Loss: 5.3129\n",
      "Epoch [5/10], Step [53850/68337], Loss: 5.2638\n",
      "Epoch [5/10], Step [53925/68337], Loss: 5.1925\n",
      "Epoch [5/10], Step [54000/68337], Loss: 5.3226\n",
      "Epoch [5/10], Step [54075/68337], Loss: 5.2810\n",
      "Epoch [5/10], Step [54150/68337], Loss: 5.1641\n",
      "Epoch [5/10], Step [54225/68337], Loss: 5.0048\n",
      "Epoch [5/10], Step [54300/68337], Loss: 5.3788\n",
      "Epoch [5/10], Step [54375/68337], Loss: 4.9503\n",
      "Epoch [5/10], Step [54450/68337], Loss: 4.9571\n",
      "Epoch [5/10], Step [54525/68337], Loss: 5.0729\n",
      "Epoch [5/10], Step [54600/68337], Loss: 5.1638\n",
      "Epoch [5/10], Step [54675/68337], Loss: 4.9969\n",
      "Epoch [5/10], Step [54750/68337], Loss: 5.1471\n",
      "Epoch [5/10], Step [54825/68337], Loss: 5.2285\n",
      "Epoch [5/10], Step [54900/68337], Loss: 5.1910\n",
      "Epoch [5/10], Step [54975/68337], Loss: 5.1820\n",
      "Epoch [5/10], Step [55050/68337], Loss: 5.2244\n",
      "Epoch [5/10], Step [55125/68337], Loss: 5.0618\n",
      "Epoch [5/10], Step [55200/68337], Loss: 5.0556\n",
      "Epoch [5/10], Step [55275/68337], Loss: 5.0957\n",
      "Epoch [5/10], Step [55350/68337], Loss: 5.2550\n",
      "Epoch [5/10], Step [55425/68337], Loss: 5.3287\n",
      "Epoch [5/10], Step [55500/68337], Loss: 4.8703\n",
      "Epoch [5/10], Step [55575/68337], Loss: 5.1815\n",
      "Epoch [5/10], Step [55650/68337], Loss: 4.9614\n",
      "Epoch [5/10], Step [55725/68337], Loss: 5.2412\n",
      "Epoch [5/10], Step [55800/68337], Loss: 5.0917\n",
      "Epoch [5/10], Step [55875/68337], Loss: 4.9127\n",
      "Epoch [5/10], Step [55950/68337], Loss: 5.0710\n",
      "Epoch [5/10], Step [56025/68337], Loss: 5.3479\n",
      "Epoch [5/10], Step [56100/68337], Loss: 5.2468\n",
      "Epoch [5/10], Step [56175/68337], Loss: 4.9710\n",
      "Epoch [5/10], Step [56250/68337], Loss: 5.2338\n",
      "Epoch [5/10], Step [56325/68337], Loss: 5.3054\n",
      "Epoch [5/10], Step [56400/68337], Loss: 5.1579\n",
      "Epoch [5/10], Step [56475/68337], Loss: 5.2918\n",
      "Epoch [5/10], Step [56550/68337], Loss: 5.1204\n",
      "Epoch [5/10], Step [56625/68337], Loss: 5.3399\n",
      "Epoch [5/10], Step [56700/68337], Loss: 4.9413\n",
      "Epoch [5/10], Step [56775/68337], Loss: 5.1628\n",
      "Epoch [5/10], Step [56850/68337], Loss: 5.2365\n",
      "Epoch [5/10], Step [56925/68337], Loss: 5.0578\n",
      "Epoch [5/10], Step [57000/68337], Loss: 5.0830\n",
      "Epoch [5/10], Step [57075/68337], Loss: 5.2565\n",
      "Epoch [5/10], Step [57150/68337], Loss: 5.1708\n",
      "Epoch [5/10], Step [57225/68337], Loss: 4.9997\n",
      "Epoch [5/10], Step [57300/68337], Loss: 5.2074\n",
      "Epoch [5/10], Step [57375/68337], Loss: 5.2063\n",
      "Epoch [5/10], Step [57450/68337], Loss: 4.8715\n",
      "Epoch [5/10], Step [57525/68337], Loss: 5.2746\n",
      "Epoch [5/10], Step [57600/68337], Loss: 5.0264\n",
      "Epoch [5/10], Step [57675/68337], Loss: 5.0046\n",
      "Epoch [5/10], Step [57750/68337], Loss: 4.9873\n",
      "Epoch [5/10], Step [57825/68337], Loss: 5.2640\n",
      "Epoch [5/10], Step [57900/68337], Loss: 5.1852\n",
      "Epoch [5/10], Step [57975/68337], Loss: 5.1195\n",
      "Epoch [5/10], Step [58050/68337], Loss: 5.1617\n",
      "Epoch [5/10], Step [58125/68337], Loss: 5.2967\n",
      "Epoch [5/10], Step [58200/68337], Loss: 5.0492\n",
      "Epoch [5/10], Step [58275/68337], Loss: 5.1732\n",
      "Epoch [5/10], Step [58350/68337], Loss: 4.9438\n",
      "Epoch [5/10], Step [58425/68337], Loss: 5.1524\n",
      "Epoch [5/10], Step [58500/68337], Loss: 5.0178\n",
      "Epoch [5/10], Step [58575/68337], Loss: 4.9374\n",
      "Epoch [5/10], Step [58650/68337], Loss: 5.0673\n",
      "Epoch [5/10], Step [58725/68337], Loss: 5.1319\n",
      "Epoch [5/10], Step [58800/68337], Loss: 5.1545\n",
      "Epoch [5/10], Step [58875/68337], Loss: 5.0866\n",
      "Epoch [5/10], Step [58950/68337], Loss: 5.2058\n",
      "Epoch [5/10], Step [59025/68337], Loss: 5.1170\n",
      "Epoch [5/10], Step [59100/68337], Loss: 5.0256\n",
      "Epoch [5/10], Step [59175/68337], Loss: 4.9603\n",
      "Epoch [5/10], Step [59250/68337], Loss: 5.0069\n",
      "Epoch [5/10], Step [59325/68337], Loss: 5.3700\n",
      "Epoch [5/10], Step [59400/68337], Loss: 5.1084\n",
      "Epoch [5/10], Step [59475/68337], Loss: 4.9703\n",
      "Epoch [5/10], Step [59550/68337], Loss: 4.9635\n",
      "Epoch [5/10], Step [59625/68337], Loss: 5.1574\n",
      "Epoch [5/10], Step [59700/68337], Loss: 5.2894\n",
      "Epoch [5/10], Step [59775/68337], Loss: 5.0812\n",
      "Epoch [5/10], Step [59850/68337], Loss: 5.2166\n",
      "Epoch [5/10], Step [59925/68337], Loss: 5.0344\n",
      "Epoch [5/10], Step [60000/68337], Loss: 5.2455\n",
      "Validation perplexity: 126.98577424931646\n",
      "Epoch [5/10], Step [60075/68337], Loss: 5.1818\n",
      "Epoch [5/10], Step [60150/68337], Loss: 5.1561\n",
      "Epoch [5/10], Step [60225/68337], Loss: 4.9305\n",
      "Epoch [5/10], Step [60300/68337], Loss: 4.9632\n",
      "Epoch [5/10], Step [60375/68337], Loss: 5.1157\n",
      "Epoch [5/10], Step [60450/68337], Loss: 5.1403\n",
      "Epoch [5/10], Step [60525/68337], Loss: 5.0497\n",
      "Epoch [5/10], Step [60600/68337], Loss: 5.1202\n",
      "Epoch [5/10], Step [60675/68337], Loss: 5.1235\n",
      "Epoch [5/10], Step [60750/68337], Loss: 5.0613\n",
      "Epoch [5/10], Step [60825/68337], Loss: 5.0685\n",
      "Epoch [5/10], Step [60900/68337], Loss: 5.1646\n",
      "Epoch [5/10], Step [60975/68337], Loss: 5.2869\n",
      "Epoch [5/10], Step [61050/68337], Loss: 5.2101\n",
      "Epoch [5/10], Step [61125/68337], Loss: 4.9650\n",
      "Epoch [5/10], Step [61200/68337], Loss: 5.1549\n",
      "Epoch [5/10], Step [61275/68337], Loss: 5.1527\n",
      "Epoch [5/10], Step [61350/68337], Loss: 5.1773\n",
      "Epoch [5/10], Step [61425/68337], Loss: 5.0522\n",
      "Epoch [5/10], Step [61500/68337], Loss: 5.1607\n",
      "Epoch [5/10], Step [61575/68337], Loss: 5.0741\n",
      "Epoch [5/10], Step [61650/68337], Loss: 5.1167\n",
      "Epoch [5/10], Step [61725/68337], Loss: 4.9831\n",
      "Epoch [5/10], Step [61800/68337], Loss: 5.0593\n",
      "Epoch [5/10], Step [61875/68337], Loss: 5.2602\n",
      "Epoch [5/10], Step [61950/68337], Loss: 5.0946\n",
      "Epoch [5/10], Step [62025/68337], Loss: 5.2919\n",
      "Epoch [5/10], Step [62100/68337], Loss: 5.3065\n",
      "Epoch [5/10], Step [62175/68337], Loss: 5.2021\n",
      "Epoch [5/10], Step [62250/68337], Loss: 5.0334\n",
      "Epoch [5/10], Step [62325/68337], Loss: 5.1986\n",
      "Epoch [5/10], Step [62400/68337], Loss: 4.9784\n",
      "Epoch [5/10], Step [62475/68337], Loss: 5.2800\n",
      "Epoch [5/10], Step [62550/68337], Loss: 5.0204\n",
      "Epoch [5/10], Step [62625/68337], Loss: 5.0489\n",
      "Epoch [5/10], Step [62700/68337], Loss: 5.2114\n",
      "Epoch [5/10], Step [62775/68337], Loss: 5.3696\n",
      "Epoch [5/10], Step [62850/68337], Loss: 5.1913\n",
      "Epoch [5/10], Step [62925/68337], Loss: 5.2123\n",
      "Epoch [5/10], Step [63000/68337], Loss: 4.9805\n",
      "Epoch [5/10], Step [63075/68337], Loss: 5.0811\n",
      "Epoch [5/10], Step [63150/68337], Loss: 5.1138\n",
      "Epoch [5/10], Step [63225/68337], Loss: 5.1390\n",
      "Epoch [5/10], Step [63300/68337], Loss: 5.2291\n",
      "Epoch [5/10], Step [63375/68337], Loss: 5.2422\n",
      "Epoch [5/10], Step [63450/68337], Loss: 5.0174\n",
      "Epoch [5/10], Step [63525/68337], Loss: 4.9963\n",
      "Epoch [5/10], Step [63600/68337], Loss: 4.9971\n",
      "Epoch [5/10], Step [63675/68337], Loss: 5.2022\n",
      "Epoch [5/10], Step [63750/68337], Loss: 5.1096\n",
      "Epoch [5/10], Step [63825/68337], Loss: 5.0031\n",
      "Epoch [5/10], Step [63900/68337], Loss: 5.0663\n",
      "Epoch [5/10], Step [63975/68337], Loss: 5.0890\n",
      "Epoch [5/10], Step [64050/68337], Loss: 5.0992\n",
      "Epoch [5/10], Step [64125/68337], Loss: 5.2708\n",
      "Epoch [5/10], Step [64200/68337], Loss: 5.2671\n",
      "Epoch [5/10], Step [64275/68337], Loss: 5.0488\n",
      "Epoch [5/10], Step [64350/68337], Loss: 4.9631\n",
      "Epoch [5/10], Step [64425/68337], Loss: 5.0419\n",
      "Epoch [5/10], Step [64500/68337], Loss: 4.8489\n",
      "Epoch [5/10], Step [64575/68337], Loss: 5.0696\n",
      "Epoch [5/10], Step [64650/68337], Loss: 5.0871\n",
      "Epoch [5/10], Step [64725/68337], Loss: 4.9552\n",
      "Epoch [5/10], Step [64800/68337], Loss: 5.2842\n",
      "Epoch [5/10], Step [64875/68337], Loss: 5.0292\n",
      "Epoch [5/10], Step [64950/68337], Loss: 5.0127\n",
      "Epoch [5/10], Step [65025/68337], Loss: 5.2342\n",
      "Epoch [5/10], Step [65100/68337], Loss: 4.9031\n",
      "Epoch [5/10], Step [65175/68337], Loss: 5.0668\n",
      "Epoch [5/10], Step [65250/68337], Loss: 5.3017\n",
      "Epoch [5/10], Step [65325/68337], Loss: 4.9897\n",
      "Epoch [5/10], Step [65400/68337], Loss: 5.0683\n",
      "Epoch [5/10], Step [65475/68337], Loss: 5.1304\n",
      "Epoch [5/10], Step [65550/68337], Loss: 4.9677\n",
      "Epoch [5/10], Step [65625/68337], Loss: 4.9555\n",
      "Epoch [5/10], Step [65700/68337], Loss: 5.0849\n",
      "Epoch [5/10], Step [65775/68337], Loss: 5.1030\n",
      "Epoch [5/10], Step [65850/68337], Loss: 5.2547\n",
      "Epoch [5/10], Step [65925/68337], Loss: 5.2434\n",
      "Epoch [5/10], Step [66000/68337], Loss: 5.1462\n",
      "Epoch [5/10], Step [66075/68337], Loss: 5.1832\n",
      "Epoch [5/10], Step [66150/68337], Loss: 5.0717\n",
      "Epoch [5/10], Step [66225/68337], Loss: 5.0418\n",
      "Epoch [5/10], Step [66300/68337], Loss: 5.1989\n",
      "Epoch [5/10], Step [66375/68337], Loss: 5.1251\n",
      "Epoch [5/10], Step [66450/68337], Loss: 5.0198\n",
      "Epoch [5/10], Step [66525/68337], Loss: 5.1382\n",
      "Epoch [5/10], Step [66600/68337], Loss: 5.0722\n",
      "Epoch [5/10], Step [66675/68337], Loss: 5.1963\n",
      "Epoch [5/10], Step [66750/68337], Loss: 5.2683\n",
      "Epoch [5/10], Step [66825/68337], Loss: 5.1464\n",
      "Epoch [5/10], Step [66900/68337], Loss: 5.0938\n",
      "Epoch [5/10], Step [66975/68337], Loss: 5.0462\n",
      "Epoch [5/10], Step [67050/68337], Loss: 4.8809\n",
      "Epoch [5/10], Step [67125/68337], Loss: 5.0292\n",
      "Epoch [5/10], Step [67200/68337], Loss: 5.1724\n",
      "Epoch [5/10], Step [67275/68337], Loss: 5.0750\n",
      "Epoch [5/10], Step [67350/68337], Loss: 5.1500\n",
      "Epoch [5/10], Step [67425/68337], Loss: 5.0243\n",
      "Epoch [5/10], Step [67500/68337], Loss: 5.3272\n",
      "Epoch [5/10], Step [67575/68337], Loss: 5.0780\n",
      "Epoch [5/10], Step [67650/68337], Loss: 5.2369\n",
      "Epoch [5/10], Step [67725/68337], Loss: 5.1576\n",
      "Epoch [5/10], Step [67800/68337], Loss: 5.1805\n",
      "Epoch [5/10], Step [67875/68337], Loss: 5.1756\n",
      "Epoch [5/10], Step [67950/68337], Loss: 4.8163\n",
      "Epoch [5/10], Step [68025/68337], Loss: 5.1377\n",
      "Epoch [5/10], Step [68100/68337], Loss: 5.2132\n",
      "Epoch [5/10], Step [68175/68337], Loss: 5.1793\n",
      "Epoch [5/10], Step [68250/68337], Loss: 5.0009\n",
      "Epoch [5/10], Step [68325/68337], Loss: 5.2407\n",
      "Epoch [5/10] Average Loss: 5.1155, Perplexity: 166.58\n",
      "Epoch [6/10], Step [0/68337], Loss: 5.1707\n",
      "Validation perplexity: 125.93788932966328\n",
      "Epoch [6/10], Step [75/68337], Loss: 5.1371\n",
      "Epoch [6/10], Step [150/68337], Loss: 5.2869\n",
      "Epoch [6/10], Step [225/68337], Loss: 5.0435\n",
      "Epoch [6/10], Step [300/68337], Loss: 5.0306\n",
      "Epoch [6/10], Step [375/68337], Loss: 5.0185\n",
      "Epoch [6/10], Step [450/68337], Loss: 5.0230\n",
      "Epoch [6/10], Step [525/68337], Loss: 5.0349\n",
      "Epoch [6/10], Step [600/68337], Loss: 5.2373\n",
      "Epoch [6/10], Step [675/68337], Loss: 4.9117\n",
      "Epoch [6/10], Step [750/68337], Loss: 5.2155\n",
      "Epoch [6/10], Step [825/68337], Loss: 5.0591\n",
      "Epoch [6/10], Step [900/68337], Loss: 4.8866\n",
      "Epoch [6/10], Step [975/68337], Loss: 5.2384\n",
      "Epoch [6/10], Step [1050/68337], Loss: 5.1958\n",
      "Epoch [6/10], Step [1125/68337], Loss: 5.3267\n",
      "Epoch [6/10], Step [1200/68337], Loss: 5.1569\n",
      "Epoch [6/10], Step [1275/68337], Loss: 5.0216\n",
      "Epoch [6/10], Step [1350/68337], Loss: 5.2013\n",
      "Epoch [6/10], Step [1425/68337], Loss: 5.0262\n",
      "Epoch [6/10], Step [1500/68337], Loss: 5.1019\n",
      "Epoch [6/10], Step [1575/68337], Loss: 5.2096\n",
      "Epoch [6/10], Step [1650/68337], Loss: 5.0307\n",
      "Epoch [6/10], Step [1725/68337], Loss: 5.0135\n",
      "Epoch [6/10], Step [1800/68337], Loss: 5.0907\n",
      "Epoch [6/10], Step [1875/68337], Loss: 5.2692\n",
      "Epoch [6/10], Step [1950/68337], Loss: 5.2945\n",
      "Epoch [6/10], Step [2025/68337], Loss: 5.0911\n",
      "Epoch [6/10], Step [2100/68337], Loss: 5.2858\n",
      "Epoch [6/10], Step [2175/68337], Loss: 5.2606\n",
      "Epoch [6/10], Step [2250/68337], Loss: 5.3319\n",
      "Epoch [6/10], Step [2325/68337], Loss: 5.0397\n",
      "Epoch [6/10], Step [2400/68337], Loss: 5.1021\n",
      "Epoch [6/10], Step [2475/68337], Loss: 5.1129\n",
      "Epoch [6/10], Step [2550/68337], Loss: 5.1737\n",
      "Epoch [6/10], Step [2625/68337], Loss: 5.0891\n",
      "Epoch [6/10], Step [2700/68337], Loss: 5.0524\n",
      "Epoch [6/10], Step [2775/68337], Loss: 5.0053\n",
      "Epoch [6/10], Step [2850/68337], Loss: 5.0570\n",
      "Epoch [6/10], Step [2925/68337], Loss: 5.0527\n",
      "Epoch [6/10], Step [3000/68337], Loss: 5.0386\n",
      "Epoch [6/10], Step [3075/68337], Loss: 4.8963\n",
      "Epoch [6/10], Step [3150/68337], Loss: 4.9943\n",
      "Epoch [6/10], Step [3225/68337], Loss: 4.9050\n",
      "Epoch [6/10], Step [3300/68337], Loss: 4.9863\n",
      "Epoch [6/10], Step [3375/68337], Loss: 5.0046\n",
      "Epoch [6/10], Step [3450/68337], Loss: 4.9765\n",
      "Epoch [6/10], Step [3525/68337], Loss: 5.2005\n",
      "Epoch [6/10], Step [3600/68337], Loss: 5.0329\n",
      "Epoch [6/10], Step [3675/68337], Loss: 5.1141\n",
      "Epoch [6/10], Step [3750/68337], Loss: 5.0118\n",
      "Epoch [6/10], Step [3825/68337], Loss: 5.2305\n",
      "Epoch [6/10], Step [3900/68337], Loss: 5.0112\n",
      "Epoch [6/10], Step [3975/68337], Loss: 5.0534\n",
      "Epoch [6/10], Step [4050/68337], Loss: 4.9252\n",
      "Epoch [6/10], Step [4125/68337], Loss: 5.2280\n",
      "Epoch [6/10], Step [4200/68337], Loss: 5.0122\n",
      "Epoch [6/10], Step [4275/68337], Loss: 5.1254\n",
      "Epoch [6/10], Step [4350/68337], Loss: 4.7476\n",
      "Epoch [6/10], Step [4425/68337], Loss: 4.9304\n",
      "Epoch [6/10], Step [4500/68337], Loss: 5.0425\n",
      "Epoch [6/10], Step [4575/68337], Loss: 5.2001\n",
      "Epoch [6/10], Step [4650/68337], Loss: 5.1708\n",
      "Epoch [6/10], Step [4725/68337], Loss: 5.1441\n",
      "Epoch [6/10], Step [4800/68337], Loss: 5.1425\n",
      "Epoch [6/10], Step [4875/68337], Loss: 5.1388\n",
      "Epoch [6/10], Step [4950/68337], Loss: 4.9613\n",
      "Epoch [6/10], Step [5025/68337], Loss: 5.2487\n",
      "Epoch [6/10], Step [5100/68337], Loss: 4.9561\n",
      "Epoch [6/10], Step [5175/68337], Loss: 5.0692\n",
      "Epoch [6/10], Step [5250/68337], Loss: 5.2456\n",
      "Epoch [6/10], Step [5325/68337], Loss: 4.9922\n",
      "Epoch [6/10], Step [5400/68337], Loss: 5.1832\n",
      "Epoch [6/10], Step [5475/68337], Loss: 5.2928\n",
      "Epoch [6/10], Step [5550/68337], Loss: 5.0640\n",
      "Epoch [6/10], Step [5625/68337], Loss: 5.2382\n",
      "Epoch [6/10], Step [5700/68337], Loss: 5.2008\n",
      "Epoch [6/10], Step [5775/68337], Loss: 5.0068\n",
      "Epoch [6/10], Step [5850/68337], Loss: 5.1194\n",
      "Epoch [6/10], Step [5925/68337], Loss: 5.0042\n",
      "Epoch [6/10], Step [6000/68337], Loss: 5.1551\n",
      "Epoch [6/10], Step [6075/68337], Loss: 4.9594\n",
      "Epoch [6/10], Step [6150/68337], Loss: 5.1958\n",
      "Epoch [6/10], Step [6225/68337], Loss: 5.0531\n",
      "Epoch [6/10], Step [6300/68337], Loss: 5.1853\n",
      "Epoch [6/10], Step [6375/68337], Loss: 5.1802\n",
      "Epoch [6/10], Step [6450/68337], Loss: 5.1750\n",
      "Epoch [6/10], Step [6525/68337], Loss: 5.0882\n",
      "Epoch [6/10], Step [6600/68337], Loss: 4.9034\n",
      "Epoch [6/10], Step [6675/68337], Loss: 5.0385\n",
      "Epoch [6/10], Step [6750/68337], Loss: 5.0544\n",
      "Epoch [6/10], Step [6825/68337], Loss: 5.1404\n",
      "Epoch [6/10], Step [6900/68337], Loss: 5.2582\n",
      "Epoch [6/10], Step [6975/68337], Loss: 5.1875\n",
      "Epoch [6/10], Step [7050/68337], Loss: 5.0460\n",
      "Epoch [6/10], Step [7125/68337], Loss: 5.1671\n",
      "Epoch [6/10], Step [7200/68337], Loss: 5.3821\n",
      "Epoch [6/10], Step [7275/68337], Loss: 5.0065\n",
      "Epoch [6/10], Step [7350/68337], Loss: 5.0942\n",
      "Epoch [6/10], Step [7425/68337], Loss: 5.0536\n",
      "Epoch [6/10], Step [7500/68337], Loss: 5.1757\n",
      "Epoch [6/10], Step [7575/68337], Loss: 5.2454\n",
      "Epoch [6/10], Step [7650/68337], Loss: 5.2396\n",
      "Epoch [6/10], Step [7725/68337], Loss: 5.0904\n",
      "Epoch [6/10], Step [7800/68337], Loss: 4.8441\n",
      "Epoch [6/10], Step [7875/68337], Loss: 5.2782\n",
      "Epoch [6/10], Step [7950/68337], Loss: 5.2737\n",
      "Epoch [6/10], Step [8025/68337], Loss: 4.8957\n",
      "Epoch [6/10], Step [8100/68337], Loss: 5.1221\n",
      "Epoch [6/10], Step [8175/68337], Loss: 5.1619\n",
      "Epoch [6/10], Step [8250/68337], Loss: 4.9233\n",
      "Epoch [6/10], Step [8325/68337], Loss: 5.0408\n",
      "Epoch [6/10], Step [8400/68337], Loss: 5.1896\n",
      "Epoch [6/10], Step [8475/68337], Loss: 4.9660\n",
      "Epoch [6/10], Step [8550/68337], Loss: 5.0385\n",
      "Epoch [6/10], Step [8625/68337], Loss: 5.3828\n",
      "Epoch [6/10], Step [8700/68337], Loss: 5.0765\n",
      "Epoch [6/10], Step [8775/68337], Loss: 5.1475\n",
      "Epoch [6/10], Step [8850/68337], Loss: 5.0727\n",
      "Epoch [6/10], Step [8925/68337], Loss: 5.1032\n",
      "Epoch [6/10], Step [9000/68337], Loss: 5.2317\n",
      "Epoch [6/10], Step [9075/68337], Loss: 5.0596\n",
      "Epoch [6/10], Step [9150/68337], Loss: 4.9647\n",
      "Epoch [6/10], Step [9225/68337], Loss: 5.0550\n",
      "Epoch [6/10], Step [9300/68337], Loss: 5.0974\n",
      "Epoch [6/10], Step [9375/68337], Loss: 5.0841\n",
      "Epoch [6/10], Step [9450/68337], Loss: 5.1358\n",
      "Epoch [6/10], Step [9525/68337], Loss: 5.1342\n",
      "Epoch [6/10], Step [9600/68337], Loss: 5.0716\n",
      "Epoch [6/10], Step [9675/68337], Loss: 5.1696\n",
      "Epoch [6/10], Step [9750/68337], Loss: 5.2753\n",
      "Epoch [6/10], Step [9825/68337], Loss: 5.2993\n",
      "Epoch [6/10], Step [9900/68337], Loss: 5.0375\n",
      "Epoch [6/10], Step [9975/68337], Loss: 5.0894\n",
      "Validation perplexity: 125.3576606959182\n",
      "Epoch [6/10], Step [10050/68337], Loss: 5.1848\n",
      "Epoch [6/10], Step [10125/68337], Loss: 5.1781\n",
      "Epoch [6/10], Step [10200/68337], Loss: 5.2184\n",
      "Epoch [6/10], Step [10275/68337], Loss: 5.0429\n",
      "Epoch [6/10], Step [10350/68337], Loss: 4.8591\n",
      "Epoch [6/10], Step [10425/68337], Loss: 5.0942\n",
      "Epoch [6/10], Step [10500/68337], Loss: 5.0990\n",
      "Epoch [6/10], Step [10575/68337], Loss: 5.0007\n",
      "Epoch [6/10], Step [10650/68337], Loss: 5.1511\n",
      "Epoch [6/10], Step [10725/68337], Loss: 5.0281\n",
      "Epoch [6/10], Step [10800/68337], Loss: 5.1882\n",
      "Epoch [6/10], Step [10875/68337], Loss: 4.8822\n",
      "Epoch [6/10], Step [10950/68337], Loss: 4.9526\n",
      "Epoch [6/10], Step [11025/68337], Loss: 4.9832\n",
      "Epoch [6/10], Step [11100/68337], Loss: 5.0157\n",
      "Epoch [6/10], Step [11175/68337], Loss: 5.0952\n",
      "Epoch [6/10], Step [11250/68337], Loss: 4.9458\n",
      "Epoch [6/10], Step [11325/68337], Loss: 5.1631\n",
      "Epoch [6/10], Step [11400/68337], Loss: 5.1569\n",
      "Epoch [6/10], Step [11475/68337], Loss: 5.0687\n",
      "Epoch [6/10], Step [11550/68337], Loss: 5.0572\n",
      "Epoch [6/10], Step [11625/68337], Loss: 4.9528\n",
      "Epoch [6/10], Step [11700/68337], Loss: 5.1930\n",
      "Epoch [6/10], Step [11775/68337], Loss: 5.2350\n",
      "Epoch [6/10], Step [11850/68337], Loss: 4.9598\n",
      "Epoch [6/10], Step [11925/68337], Loss: 5.0273\n",
      "Epoch [6/10], Step [12000/68337], Loss: 5.0951\n",
      "Epoch [6/10], Step [12075/68337], Loss: 5.1782\n",
      "Epoch [6/10], Step [12150/68337], Loss: 5.1467\n",
      "Epoch [6/10], Step [12225/68337], Loss: 5.1254\n",
      "Epoch [6/10], Step [12300/68337], Loss: 5.0691\n",
      "Epoch [6/10], Step [12375/68337], Loss: 5.2195\n",
      "Epoch [6/10], Step [12450/68337], Loss: 5.1122\n",
      "Epoch [6/10], Step [12525/68337], Loss: 5.1796\n",
      "Epoch [6/10], Step [12600/68337], Loss: 5.2497\n",
      "Epoch [6/10], Step [12675/68337], Loss: 5.0472\n",
      "Epoch [6/10], Step [12750/68337], Loss: 5.1738\n",
      "Epoch [6/10], Step [12825/68337], Loss: 5.1370\n",
      "Epoch [6/10], Step [12900/68337], Loss: 4.9359\n",
      "Epoch [6/10], Step [12975/68337], Loss: 5.1516\n",
      "Epoch [6/10], Step [13050/68337], Loss: 5.1432\n",
      "Epoch [6/10], Step [13125/68337], Loss: 5.2332\n",
      "Epoch [6/10], Step [13200/68337], Loss: 5.2402\n",
      "Epoch [6/10], Step [13275/68337], Loss: 5.0940\n",
      "Epoch [6/10], Step [13350/68337], Loss: 5.0783\n",
      "Epoch [6/10], Step [13425/68337], Loss: 4.8489\n",
      "Epoch [6/10], Step [13500/68337], Loss: 5.0438\n",
      "Epoch [6/10], Step [13575/68337], Loss: 5.0984\n",
      "Epoch [6/10], Step [13650/68337], Loss: 5.1534\n",
      "Epoch [6/10], Step [13725/68337], Loss: 4.7488\n",
      "Epoch [6/10], Step [13800/68337], Loss: 5.1756\n",
      "Epoch [6/10], Step [13875/68337], Loss: 5.2375\n",
      "Epoch [6/10], Step [13950/68337], Loss: 5.1116\n",
      "Epoch [6/10], Step [14025/68337], Loss: 5.1022\n",
      "Epoch [6/10], Step [14100/68337], Loss: 4.9307\n",
      "Epoch [6/10], Step [14175/68337], Loss: 4.9875\n",
      "Epoch [6/10], Step [14250/68337], Loss: 5.1535\n",
      "Epoch [6/10], Step [14325/68337], Loss: 5.2450\n",
      "Epoch [6/10], Step [14400/68337], Loss: 5.1534\n",
      "Epoch [6/10], Step [14475/68337], Loss: 5.3100\n",
      "Epoch [6/10], Step [14550/68337], Loss: 4.9876\n",
      "Epoch [6/10], Step [14625/68337], Loss: 5.0958\n",
      "Epoch [6/10], Step [14700/68337], Loss: 4.9933\n",
      "Epoch [6/10], Step [14775/68337], Loss: 5.1715\n",
      "Epoch [6/10], Step [14850/68337], Loss: 4.9923\n",
      "Epoch [6/10], Step [14925/68337], Loss: 5.3107\n",
      "Epoch [6/10], Step [15000/68337], Loss: 5.2184\n",
      "Epoch [6/10], Step [15075/68337], Loss: 5.0475\n",
      "Epoch [6/10], Step [15150/68337], Loss: 5.3949\n",
      "Epoch [6/10], Step [15225/68337], Loss: 5.0904\n",
      "Epoch [6/10], Step [15300/68337], Loss: 5.0015\n",
      "Epoch [6/10], Step [15375/68337], Loss: 5.1489\n",
      "Epoch [6/10], Step [15450/68337], Loss: 5.0943\n",
      "Epoch [6/10], Step [15525/68337], Loss: 4.9215\n",
      "Epoch [6/10], Step [15600/68337], Loss: 5.0984\n",
      "Epoch [6/10], Step [15675/68337], Loss: 5.2222\n",
      "Epoch [6/10], Step [15750/68337], Loss: 5.0903\n",
      "Epoch [6/10], Step [15825/68337], Loss: 5.2544\n",
      "Epoch [6/10], Step [15900/68337], Loss: 5.1715\n",
      "Epoch [6/10], Step [15975/68337], Loss: 5.0677\n",
      "Epoch [6/10], Step [16050/68337], Loss: 5.1067\n",
      "Epoch [6/10], Step [16125/68337], Loss: 5.0436\n",
      "Epoch [6/10], Step [16200/68337], Loss: 5.1870\n",
      "Epoch [6/10], Step [16275/68337], Loss: 5.0886\n",
      "Epoch [6/10], Step [16350/68337], Loss: 5.1303\n",
      "Epoch [6/10], Step [16425/68337], Loss: 5.1273\n",
      "Epoch [6/10], Step [16500/68337], Loss: 5.0602\n",
      "Epoch [6/10], Step [16575/68337], Loss: 4.8941\n",
      "Epoch [6/10], Step [16650/68337], Loss: 5.2255\n",
      "Epoch [6/10], Step [16725/68337], Loss: 5.1674\n",
      "Epoch [6/10], Step [16800/68337], Loss: 5.0941\n",
      "Epoch [6/10], Step [16875/68337], Loss: 5.0002\n",
      "Epoch [6/10], Step [16950/68337], Loss: 5.1103\n",
      "Epoch [6/10], Step [17025/68337], Loss: 5.1884\n",
      "Epoch [6/10], Step [17100/68337], Loss: 5.0879\n",
      "Epoch [6/10], Step [17175/68337], Loss: 4.8630\n",
      "Epoch [6/10], Step [17250/68337], Loss: 5.1749\n",
      "Epoch [6/10], Step [17325/68337], Loss: 5.1401\n",
      "Epoch [6/10], Step [17400/68337], Loss: 5.2060\n",
      "Epoch [6/10], Step [17475/68337], Loss: 5.1228\n",
      "Epoch [6/10], Step [17550/68337], Loss: 5.0808\n",
      "Epoch [6/10], Step [17625/68337], Loss: 5.2044\n",
      "Epoch [6/10], Step [17700/68337], Loss: 5.0158\n",
      "Epoch [6/10], Step [17775/68337], Loss: 4.9228\n",
      "Epoch [6/10], Step [17850/68337], Loss: 5.0498\n",
      "Epoch [6/10], Step [17925/68337], Loss: 5.1476\n",
      "Epoch [6/10], Step [18000/68337], Loss: 4.9214\n",
      "Epoch [6/10], Step [18075/68337], Loss: 5.0837\n",
      "Epoch [6/10], Step [18150/68337], Loss: 5.0504\n",
      "Epoch [6/10], Step [18225/68337], Loss: 5.0941\n",
      "Epoch [6/10], Step [18300/68337], Loss: 5.1274\n",
      "Epoch [6/10], Step [18375/68337], Loss: 5.0504\n",
      "Epoch [6/10], Step [18450/68337], Loss: 5.1186\n",
      "Epoch [6/10], Step [18525/68337], Loss: 5.2306\n",
      "Epoch [6/10], Step [18600/68337], Loss: 5.1479\n",
      "Epoch [6/10], Step [18675/68337], Loss: 5.1267\n",
      "Epoch [6/10], Step [18750/68337], Loss: 5.2856\n",
      "Epoch [6/10], Step [18825/68337], Loss: 5.2947\n",
      "Epoch [6/10], Step [18900/68337], Loss: 4.8057\n",
      "Epoch [6/10], Step [18975/68337], Loss: 5.0886\n",
      "Epoch [6/10], Step [19050/68337], Loss: 5.1433\n",
      "Epoch [6/10], Step [19125/68337], Loss: 5.1231\n",
      "Epoch [6/10], Step [19200/68337], Loss: 4.8025\n",
      "Epoch [6/10], Step [19275/68337], Loss: 5.1290\n",
      "Epoch [6/10], Step [19350/68337], Loss: 4.9771\n",
      "Epoch [6/10], Step [19425/68337], Loss: 5.2949\n",
      "Epoch [6/10], Step [19500/68337], Loss: 5.3093\n",
      "Epoch [6/10], Step [19575/68337], Loss: 5.3123\n",
      "Epoch [6/10], Step [19650/68337], Loss: 5.0111\n",
      "Epoch [6/10], Step [19725/68337], Loss: 5.2559\n",
      "Epoch [6/10], Step [19800/68337], Loss: 5.0532\n",
      "Epoch [6/10], Step [19875/68337], Loss: 5.2687\n",
      "Epoch [6/10], Step [19950/68337], Loss: 5.0586\n",
      "Validation perplexity: 125.63986970754489\n",
      "Epoch [6/10], Step [20025/68337], Loss: 5.3171\n",
      "Epoch [6/10], Step [20100/68337], Loss: 5.1348\n",
      "Epoch [6/10], Step [20175/68337], Loss: 5.3165\n",
      "Epoch [6/10], Step [20250/68337], Loss: 4.9024\n",
      "Epoch [6/10], Step [20325/68337], Loss: 5.0367\n",
      "Epoch [6/10], Step [20400/68337], Loss: 5.1050\n",
      "Epoch [6/10], Step [20475/68337], Loss: 5.2679\n",
      "Epoch [6/10], Step [20550/68337], Loss: 5.0277\n",
      "Epoch [6/10], Step [20625/68337], Loss: 5.1980\n",
      "Epoch [6/10], Step [20700/68337], Loss: 5.1404\n",
      "Epoch [6/10], Step [20775/68337], Loss: 5.1444\n",
      "Epoch [6/10], Step [20850/68337], Loss: 5.1313\n",
      "Epoch [6/10], Step [20925/68337], Loss: 5.0399\n",
      "Epoch [6/10], Step [21000/68337], Loss: 4.8954\n",
      "Epoch [6/10], Step [21075/68337], Loss: 5.2158\n",
      "Epoch [6/10], Step [21150/68337], Loss: 5.1295\n",
      "Epoch [6/10], Step [21225/68337], Loss: 5.2236\n",
      "Epoch [6/10], Step [21300/68337], Loss: 5.2424\n",
      "Epoch [6/10], Step [21375/68337], Loss: 5.2026\n",
      "Epoch [6/10], Step [21450/68337], Loss: 5.1403\n",
      "Epoch [6/10], Step [21525/68337], Loss: 5.0469\n",
      "Epoch [6/10], Step [21600/68337], Loss: 5.0523\n",
      "Epoch [6/10], Step [21675/68337], Loss: 4.9266\n",
      "Epoch [6/10], Step [21750/68337], Loss: 5.2800\n",
      "Epoch [6/10], Step [21825/68337], Loss: 5.0676\n",
      "Epoch [6/10], Step [21900/68337], Loss: 5.0994\n",
      "Epoch [6/10], Step [21975/68337], Loss: 5.1376\n",
      "Epoch [6/10], Step [22050/68337], Loss: 5.0264\n",
      "Epoch [6/10], Step [22125/68337], Loss: 4.9270\n",
      "Epoch [6/10], Step [22200/68337], Loss: 5.1032\n",
      "Epoch [6/10], Step [22275/68337], Loss: 5.0430\n",
      "Epoch [6/10], Step [22350/68337], Loss: 5.2530\n",
      "Epoch [6/10], Step [22425/68337], Loss: 5.2278\n",
      "Epoch [6/10], Step [22500/68337], Loss: 5.0471\n",
      "Epoch [6/10], Step [22575/68337], Loss: 5.0894\n",
      "Epoch [6/10], Step [22650/68337], Loss: 5.1915\n",
      "Epoch [6/10], Step [22725/68337], Loss: 5.0922\n",
      "Epoch [6/10], Step [22800/68337], Loss: 5.1941\n",
      "Epoch [6/10], Step [22875/68337], Loss: 5.1096\n",
      "Epoch [6/10], Step [22950/68337], Loss: 5.2765\n",
      "Epoch [6/10], Step [23025/68337], Loss: 4.9901\n",
      "Epoch [6/10], Step [23100/68337], Loss: 4.9437\n",
      "Epoch [6/10], Step [23175/68337], Loss: 5.2782\n",
      "Epoch [6/10], Step [23250/68337], Loss: 5.0954\n",
      "Epoch [6/10], Step [23325/68337], Loss: 4.9641\n",
      "Epoch [6/10], Step [23400/68337], Loss: 4.9610\n",
      "Epoch [6/10], Step [23475/68337], Loss: 5.1458\n",
      "Epoch [6/10], Step [23550/68337], Loss: 5.2359\n",
      "Epoch [6/10], Step [23625/68337], Loss: 5.2079\n",
      "Epoch [6/10], Step [23700/68337], Loss: 5.3274\n",
      "Epoch [6/10], Step [23775/68337], Loss: 5.2714\n",
      "Epoch [6/10], Step [23850/68337], Loss: 4.9543\n",
      "Epoch [6/10], Step [23925/68337], Loss: 5.0493\n",
      "Epoch [6/10], Step [24000/68337], Loss: 5.3609\n",
      "Epoch [6/10], Step [24075/68337], Loss: 5.0724\n",
      "Epoch [6/10], Step [24150/68337], Loss: 5.1825\n",
      "Epoch [6/10], Step [24225/68337], Loss: 5.0375\n",
      "Epoch [6/10], Step [24300/68337], Loss: 5.0464\n",
      "Epoch [6/10], Step [24375/68337], Loss: 5.2133\n",
      "Epoch [6/10], Step [24450/68337], Loss: 4.9827\n",
      "Epoch [6/10], Step [24525/68337], Loss: 5.1451\n",
      "Epoch [6/10], Step [24600/68337], Loss: 5.1212\n",
      "Epoch [6/10], Step [24675/68337], Loss: 5.1002\n",
      "Epoch [6/10], Step [24750/68337], Loss: 4.8892\n",
      "Epoch [6/10], Step [24825/68337], Loss: 5.1551\n",
      "Epoch [6/10], Step [24900/68337], Loss: 5.3471\n",
      "Epoch [6/10], Step [24975/68337], Loss: 5.0888\n",
      "Epoch [6/10], Step [25050/68337], Loss: 4.9806\n",
      "Epoch [6/10], Step [25125/68337], Loss: 5.2352\n",
      "Epoch [6/10], Step [25200/68337], Loss: 5.1042\n",
      "Epoch [6/10], Step [25275/68337], Loss: 4.9381\n",
      "Epoch [6/10], Step [25350/68337], Loss: 5.0914\n",
      "Epoch [6/10], Step [25425/68337], Loss: 5.1212\n",
      "Epoch [6/10], Step [25500/68337], Loss: 5.0891\n",
      "Epoch [6/10], Step [25575/68337], Loss: 4.9929\n",
      "Epoch [6/10], Step [25650/68337], Loss: 5.0900\n",
      "Epoch [6/10], Step [25725/68337], Loss: 5.0816\n",
      "Epoch [6/10], Step [25800/68337], Loss: 5.2371\n",
      "Epoch [6/10], Step [25875/68337], Loss: 5.2140\n",
      "Epoch [6/10], Step [25950/68337], Loss: 4.9597\n",
      "Epoch [6/10], Step [26025/68337], Loss: 4.9887\n",
      "Epoch [6/10], Step [26100/68337], Loss: 5.0501\n",
      "Epoch [6/10], Step [26175/68337], Loss: 5.2427\n",
      "Epoch [6/10], Step [26250/68337], Loss: 5.0102\n",
      "Epoch [6/10], Step [26325/68337], Loss: 5.1416\n",
      "Epoch [6/10], Step [26400/68337], Loss: 5.1562\n",
      "Epoch [6/10], Step [26475/68337], Loss: 4.9193\n",
      "Epoch [6/10], Step [26550/68337], Loss: 5.3818\n",
      "Epoch [6/10], Step [26625/68337], Loss: 4.9854\n",
      "Epoch [6/10], Step [26700/68337], Loss: 5.0482\n",
      "Epoch [6/10], Step [26775/68337], Loss: 5.1188\n",
      "Epoch [6/10], Step [26850/68337], Loss: 5.1632\n",
      "Epoch [6/10], Step [26925/68337], Loss: 5.2157\n",
      "Epoch [6/10], Step [27000/68337], Loss: 5.0409\n",
      "Epoch [6/10], Step [27075/68337], Loss: 4.9669\n",
      "Epoch [6/10], Step [27150/68337], Loss: 5.1855\n",
      "Epoch [6/10], Step [27225/68337], Loss: 4.9232\n",
      "Epoch [6/10], Step [27300/68337], Loss: 5.2658\n",
      "Epoch [6/10], Step [27375/68337], Loss: 5.0723\n",
      "Epoch [6/10], Step [27450/68337], Loss: 5.1588\n",
      "Epoch [6/10], Step [27525/68337], Loss: 4.9558\n",
      "Epoch [6/10], Step [27600/68337], Loss: 5.1318\n",
      "Epoch [6/10], Step [27675/68337], Loss: 5.4529\n",
      "Epoch [6/10], Step [27750/68337], Loss: 5.2012\n",
      "Epoch [6/10], Step [27825/68337], Loss: 5.0489\n",
      "Epoch [6/10], Step [27900/68337], Loss: 5.1948\n",
      "Epoch [6/10], Step [27975/68337], Loss: 5.1084\n",
      "Epoch [6/10], Step [28050/68337], Loss: 4.9889\n",
      "Epoch [6/10], Step [28125/68337], Loss: 5.3006\n",
      "Epoch [6/10], Step [28200/68337], Loss: 5.0985\n",
      "Epoch [6/10], Step [28275/68337], Loss: 5.0445\n",
      "Epoch [6/10], Step [28350/68337], Loss: 5.1695\n",
      "Epoch [6/10], Step [28425/68337], Loss: 5.2892\n",
      "Epoch [6/10], Step [28500/68337], Loss: 5.1122\n",
      "Epoch [6/10], Step [28575/68337], Loss: 5.1672\n",
      "Epoch [6/10], Step [28650/68337], Loss: 5.0126\n",
      "Epoch [6/10], Step [28725/68337], Loss: 5.0659\n",
      "Epoch [6/10], Step [28800/68337], Loss: 5.2127\n",
      "Epoch [6/10], Step [28875/68337], Loss: 5.0587\n",
      "Epoch [6/10], Step [28950/68337], Loss: 5.1732\n",
      "Epoch [6/10], Step [29025/68337], Loss: 4.8681\n",
      "Epoch [6/10], Step [29100/68337], Loss: 5.1673\n",
      "Epoch [6/10], Step [29175/68337], Loss: 5.0825\n",
      "Epoch [6/10], Step [29250/68337], Loss: 5.2052\n",
      "Epoch [6/10], Step [29325/68337], Loss: 5.1089\n",
      "Epoch [6/10], Step [29400/68337], Loss: 4.8999\n",
      "Epoch [6/10], Step [29475/68337], Loss: 5.1176\n",
      "Epoch [6/10], Step [29550/68337], Loss: 5.0404\n",
      "Epoch [6/10], Step [29625/68337], Loss: 4.9743\n",
      "Epoch [6/10], Step [29700/68337], Loss: 5.2155\n",
      "Epoch [6/10], Step [29775/68337], Loss: 5.0568\n",
      "Epoch [6/10], Step [29850/68337], Loss: 4.9551\n",
      "Epoch [6/10], Step [29925/68337], Loss: 5.1404\n",
      "Epoch [6/10], Step [30000/68337], Loss: 4.9161\n",
      "Validation perplexity: 125.37901922098875\n",
      "Epoch [6/10], Step [30075/68337], Loss: 5.0224\n",
      "Epoch [6/10], Step [30150/68337], Loss: 5.0039\n",
      "Epoch [6/10], Step [30225/68337], Loss: 5.1655\n",
      "Epoch [6/10], Step [30300/68337], Loss: 5.1002\n",
      "Epoch [6/10], Step [30375/68337], Loss: 5.1722\n",
      "Epoch [6/10], Step [30450/68337], Loss: 4.9065\n",
      "Epoch [6/10], Step [30525/68337], Loss: 5.1790\n",
      "Epoch [6/10], Step [30600/68337], Loss: 5.1298\n",
      "Epoch [6/10], Step [30675/68337], Loss: 4.9798\n",
      "Epoch [6/10], Step [30750/68337], Loss: 5.0084\n",
      "Epoch [6/10], Step [30825/68337], Loss: 5.1238\n",
      "Epoch [6/10], Step [30900/68337], Loss: 5.2000\n",
      "Epoch [6/10], Step [30975/68337], Loss: 5.0578\n",
      "Epoch [6/10], Step [31050/68337], Loss: 5.2301\n",
      "Epoch [6/10], Step [31125/68337], Loss: 5.1976\n",
      "Epoch [6/10], Step [31200/68337], Loss: 4.9608\n",
      "Epoch [6/10], Step [31275/68337], Loss: 5.0933\n",
      "Epoch [6/10], Step [31350/68337], Loss: 5.0241\n",
      "Epoch [6/10], Step [31425/68337], Loss: 5.3444\n",
      "Epoch [6/10], Step [31500/68337], Loss: 5.0138\n",
      "Epoch [6/10], Step [31575/68337], Loss: 5.0310\n",
      "Epoch [6/10], Step [31650/68337], Loss: 5.2365\n",
      "Epoch [6/10], Step [31725/68337], Loss: 5.3020\n",
      "Epoch [6/10], Step [31800/68337], Loss: 5.0282\n",
      "Epoch [6/10], Step [31875/68337], Loss: 5.0160\n",
      "Epoch [6/10], Step [31950/68337], Loss: 5.1324\n",
      "Epoch [6/10], Step [32025/68337], Loss: 5.1503\n",
      "Epoch [6/10], Step [32100/68337], Loss: 5.0365\n",
      "Epoch [6/10], Step [32175/68337], Loss: 5.0616\n",
      "Epoch [6/10], Step [32250/68337], Loss: 5.1556\n",
      "Epoch [6/10], Step [32325/68337], Loss: 5.1507\n",
      "Epoch [6/10], Step [32400/68337], Loss: 5.2548\n",
      "Epoch [6/10], Step [32475/68337], Loss: 5.2625\n",
      "Epoch [6/10], Step [32550/68337], Loss: 4.9759\n",
      "Epoch [6/10], Step [32625/68337], Loss: 5.1231\n",
      "Epoch [6/10], Step [32700/68337], Loss: 5.0352\n",
      "Epoch [6/10], Step [32775/68337], Loss: 5.1352\n",
      "Epoch [6/10], Step [32850/68337], Loss: 5.0250\n",
      "Epoch [6/10], Step [32925/68337], Loss: 5.0547\n",
      "Epoch [6/10], Step [33000/68337], Loss: 5.0053\n",
      "Epoch [6/10], Step [33075/68337], Loss: 5.1933\n",
      "Epoch [6/10], Step [33150/68337], Loss: 5.1780\n",
      "Epoch [6/10], Step [33225/68337], Loss: 5.1459\n",
      "Epoch [6/10], Step [33300/68337], Loss: 5.0707\n",
      "Epoch [6/10], Step [33375/68337], Loss: 5.0465\n",
      "Epoch [6/10], Step [33450/68337], Loss: 5.1613\n",
      "Epoch [6/10], Step [33525/68337], Loss: 5.3005\n",
      "Epoch [6/10], Step [33600/68337], Loss: 5.2729\n",
      "Epoch [6/10], Step [33675/68337], Loss: 5.1985\n",
      "Epoch [6/10], Step [33750/68337], Loss: 5.2540\n",
      "Epoch [6/10], Step [33825/68337], Loss: 5.1515\n",
      "Epoch [6/10], Step [33900/68337], Loss: 5.2512\n",
      "Epoch [6/10], Step [33975/68337], Loss: 5.1955\n",
      "Epoch [6/10], Step [34050/68337], Loss: 5.1358\n",
      "Epoch [6/10], Step [34125/68337], Loss: 5.0922\n",
      "Epoch [6/10], Step [34200/68337], Loss: 5.1895\n",
      "Epoch [6/10], Step [34275/68337], Loss: 5.2234\n",
      "Epoch [6/10], Step [34350/68337], Loss: 5.1520\n",
      "Epoch [6/10], Step [34425/68337], Loss: 5.2552\n",
      "Epoch [6/10], Step [34500/68337], Loss: 5.0646\n",
      "Epoch [6/10], Step [34575/68337], Loss: 5.1282\n",
      "Epoch [6/10], Step [34650/68337], Loss: 5.0615\n",
      "Epoch [6/10], Step [34725/68337], Loss: 5.0815\n",
      "Epoch [6/10], Step [34800/68337], Loss: 5.0518\n",
      "Epoch [6/10], Step [34875/68337], Loss: 5.2500\n",
      "Epoch [6/10], Step [34950/68337], Loss: 5.0961\n",
      "Epoch [6/10], Step [35025/68337], Loss: 5.0604\n",
      "Epoch [6/10], Step [35100/68337], Loss: 5.0747\n",
      "Epoch [6/10], Step [35175/68337], Loss: 5.0834\n",
      "Epoch [6/10], Step [35250/68337], Loss: 5.1731\n",
      "Epoch [6/10], Step [35325/68337], Loss: 5.0589\n",
      "Epoch [6/10], Step [35400/68337], Loss: 5.2907\n",
      "Epoch [6/10], Step [35475/68337], Loss: 5.1726\n",
      "Epoch [6/10], Step [35550/68337], Loss: 4.8491\n",
      "Epoch [6/10], Step [35625/68337], Loss: 5.0224\n",
      "Epoch [6/10], Step [35700/68337], Loss: 5.1047\n",
      "Epoch [6/10], Step [35775/68337], Loss: 5.0487\n",
      "Epoch [6/10], Step [35850/68337], Loss: 5.0401\n",
      "Epoch [6/10], Step [35925/68337], Loss: 4.9782\n",
      "Epoch [6/10], Step [36000/68337], Loss: 5.2034\n",
      "Epoch [6/10], Step [36075/68337], Loss: 5.2032\n",
      "Epoch [6/10], Step [36150/68337], Loss: 5.2404\n",
      "Epoch [6/10], Step [36225/68337], Loss: 5.2131\n",
      "Epoch [6/10], Step [36300/68337], Loss: 5.0970\n",
      "Epoch [6/10], Step [36375/68337], Loss: 5.1602\n",
      "Epoch [6/10], Step [36450/68337], Loss: 5.0863\n",
      "Epoch [6/10], Step [36525/68337], Loss: 5.1119\n",
      "Epoch [6/10], Step [36600/68337], Loss: 5.2631\n",
      "Epoch [6/10], Step [36675/68337], Loss: 4.9923\n",
      "Epoch [6/10], Step [36750/68337], Loss: 5.2398\n",
      "Epoch [6/10], Step [36825/68337], Loss: 4.9598\n",
      "Epoch [6/10], Step [36900/68337], Loss: 5.1524\n",
      "Epoch [6/10], Step [36975/68337], Loss: 5.1600\n",
      "Epoch [6/10], Step [37050/68337], Loss: 5.0681\n",
      "Epoch [6/10], Step [37125/68337], Loss: 5.2003\n",
      "Epoch [6/10], Step [37200/68337], Loss: 5.0939\n",
      "Epoch [6/10], Step [37275/68337], Loss: 5.0764\n",
      "Epoch [6/10], Step [37350/68337], Loss: 4.9884\n",
      "Epoch [6/10], Step [37425/68337], Loss: 5.1198\n",
      "Epoch [6/10], Step [37500/68337], Loss: 5.0848\n",
      "Epoch [6/10], Step [37575/68337], Loss: 5.1149\n",
      "Epoch [6/10], Step [37650/68337], Loss: 5.0255\n",
      "Epoch [6/10], Step [37725/68337], Loss: 5.2024\n",
      "Epoch [6/10], Step [37800/68337], Loss: 5.1541\n",
      "Epoch [6/10], Step [37875/68337], Loss: 4.9556\n",
      "Epoch [6/10], Step [37950/68337], Loss: 5.1298\n",
      "Epoch [6/10], Step [38025/68337], Loss: 4.9205\n",
      "Epoch [6/10], Step [38100/68337], Loss: 5.0392\n",
      "Epoch [6/10], Step [38175/68337], Loss: 4.9784\n",
      "Epoch [6/10], Step [38250/68337], Loss: 5.1127\n",
      "Epoch [6/10], Step [38325/68337], Loss: 5.0882\n",
      "Epoch [6/10], Step [38400/68337], Loss: 5.0868\n",
      "Epoch [6/10], Step [38475/68337], Loss: 5.3248\n",
      "Epoch [6/10], Step [38550/68337], Loss: 4.7008\n",
      "Epoch [6/10], Step [38625/68337], Loss: 5.3531\n",
      "Epoch [6/10], Step [38700/68337], Loss: 5.1395\n",
      "Epoch [6/10], Step [38775/68337], Loss: 5.1363\n",
      "Epoch [6/10], Step [38850/68337], Loss: 5.0888\n",
      "Epoch [6/10], Step [38925/68337], Loss: 5.1890\n",
      "Epoch [6/10], Step [39000/68337], Loss: 5.2667\n",
      "Epoch [6/10], Step [39075/68337], Loss: 5.0072\n",
      "Epoch [6/10], Step [39150/68337], Loss: 5.2490\n",
      "Epoch [6/10], Step [39225/68337], Loss: 5.0064\n",
      "Epoch [6/10], Step [39300/68337], Loss: 4.7805\n",
      "Epoch [6/10], Step [39375/68337], Loss: 5.2290\n",
      "Epoch [6/10], Step [39450/68337], Loss: 5.1195\n",
      "Epoch [6/10], Step [39525/68337], Loss: 5.0862\n",
      "Epoch [6/10], Step [39600/68337], Loss: 5.1688\n",
      "Epoch [6/10], Step [39675/68337], Loss: 5.0407\n",
      "Epoch [6/10], Step [39750/68337], Loss: 5.2829\n",
      "Epoch [6/10], Step [39825/68337], Loss: 5.1176\n",
      "Epoch [6/10], Step [39900/68337], Loss: 5.0931\n",
      "Epoch [6/10], Step [39975/68337], Loss: 5.2258\n",
      "Validation perplexity: 125.22730885327148\n",
      "Epoch [6/10], Step [40050/68337], Loss: 5.1732\n",
      "Epoch [6/10], Step [40125/68337], Loss: 5.1846\n",
      "Epoch [6/10], Step [40200/68337], Loss: 5.2790\n",
      "Epoch [6/10], Step [40275/68337], Loss: 5.0493\n",
      "Epoch [6/10], Step [40350/68337], Loss: 4.9524\n",
      "Epoch [6/10], Step [40425/68337], Loss: 5.1650\n",
      "Epoch [6/10], Step [40500/68337], Loss: 5.2375\n",
      "Epoch [6/10], Step [40575/68337], Loss: 5.2000\n",
      "Epoch [6/10], Step [40650/68337], Loss: 4.9379\n",
      "Epoch [6/10], Step [40725/68337], Loss: 5.0852\n",
      "Epoch [6/10], Step [40800/68337], Loss: 5.2175\n",
      "Epoch [6/10], Step [40875/68337], Loss: 5.1411\n",
      "Epoch [6/10], Step [40950/68337], Loss: 5.0661\n",
      "Epoch [6/10], Step [41025/68337], Loss: 5.3114\n",
      "Epoch [6/10], Step [41100/68337], Loss: 5.1692\n",
      "Epoch [6/10], Step [41175/68337], Loss: 5.0833\n",
      "Epoch [6/10], Step [41250/68337], Loss: 5.1074\n",
      "Epoch [6/10], Step [41325/68337], Loss: 5.2063\n",
      "Epoch [6/10], Step [41400/68337], Loss: 4.9445\n",
      "Epoch [6/10], Step [41475/68337], Loss: 5.1904\n",
      "Epoch [6/10], Step [41550/68337], Loss: 4.9858\n",
      "Epoch [6/10], Step [41625/68337], Loss: 5.0352\n",
      "Epoch [6/10], Step [41700/68337], Loss: 5.1614\n",
      "Epoch [6/10], Step [41775/68337], Loss: 4.8499\n",
      "Epoch [6/10], Step [41850/68337], Loss: 5.0066\n",
      "Epoch [6/10], Step [41925/68337], Loss: 4.9349\n",
      "Epoch [6/10], Step [42000/68337], Loss: 4.9502\n",
      "Epoch [6/10], Step [42075/68337], Loss: 5.0790\n",
      "Epoch [6/10], Step [42150/68337], Loss: 5.1286\n",
      "Epoch [6/10], Step [42225/68337], Loss: 4.9464\n",
      "Epoch [6/10], Step [42300/68337], Loss: 5.0074\n",
      "Epoch [6/10], Step [42375/68337], Loss: 5.1099\n",
      "Epoch [6/10], Step [42450/68337], Loss: 5.3028\n",
      "Epoch [6/10], Step [42525/68337], Loss: 5.3313\n",
      "Epoch [6/10], Step [42600/68337], Loss: 4.7725\n",
      "Epoch [6/10], Step [42675/68337], Loss: 5.1679\n",
      "Epoch [6/10], Step [42750/68337], Loss: 5.0775\n",
      "Epoch [6/10], Step [42825/68337], Loss: 5.3517\n",
      "Epoch [6/10], Step [42900/68337], Loss: 5.1091\n",
      "Epoch [6/10], Step [42975/68337], Loss: 5.2502\n",
      "Epoch [6/10], Step [43050/68337], Loss: 5.2658\n",
      "Epoch [6/10], Step [43125/68337], Loss: 5.1631\n",
      "Epoch [6/10], Step [43200/68337], Loss: 5.1521\n",
      "Epoch [6/10], Step [43275/68337], Loss: 5.2064\n",
      "Epoch [6/10], Step [43350/68337], Loss: 5.0527\n",
      "Epoch [6/10], Step [43425/68337], Loss: 5.0595\n",
      "Epoch [6/10], Step [43500/68337], Loss: 5.3311\n",
      "Epoch [6/10], Step [43575/68337], Loss: 5.1829\n",
      "Epoch [6/10], Step [43650/68337], Loss: 5.1302\n",
      "Epoch [6/10], Step [43725/68337], Loss: 4.9621\n",
      "Epoch [6/10], Step [43800/68337], Loss: 4.9803\n",
      "Epoch [6/10], Step [43875/68337], Loss: 5.0869\n",
      "Epoch [6/10], Step [43950/68337], Loss: 5.1786\n",
      "Epoch [6/10], Step [44025/68337], Loss: 5.0141\n",
      "Epoch [6/10], Step [44100/68337], Loss: 4.9976\n",
      "Epoch [6/10], Step [44175/68337], Loss: 5.1889\n",
      "Epoch [6/10], Step [44250/68337], Loss: 5.1565\n",
      "Epoch [6/10], Step [44325/68337], Loss: 4.8604\n",
      "Epoch [6/10], Step [44400/68337], Loss: 4.9463\n",
      "Epoch [6/10], Step [44475/68337], Loss: 5.1945\n",
      "Epoch [6/10], Step [44550/68337], Loss: 5.2105\n",
      "Epoch [6/10], Step [44625/68337], Loss: 5.2426\n",
      "Epoch [6/10], Step [44700/68337], Loss: 5.1360\n",
      "Epoch [6/10], Step [44775/68337], Loss: 5.1665\n",
      "Epoch [6/10], Step [44850/68337], Loss: 5.1875\n",
      "Epoch [6/10], Step [44925/68337], Loss: 4.9747\n",
      "Epoch [6/10], Step [45000/68337], Loss: 5.1017\n",
      "Epoch [6/10], Step [45075/68337], Loss: 5.1257\n",
      "Epoch [6/10], Step [45150/68337], Loss: 5.1115\n",
      "Epoch [6/10], Step [45225/68337], Loss: 4.8940\n",
      "Epoch [6/10], Step [45300/68337], Loss: 5.1694\n",
      "Epoch [6/10], Step [45375/68337], Loss: 5.1516\n",
      "Epoch [6/10], Step [45450/68337], Loss: 5.0347\n",
      "Epoch [6/10], Step [45525/68337], Loss: 5.1469\n",
      "Epoch [6/10], Step [45600/68337], Loss: 5.1401\n",
      "Epoch [6/10], Step [45675/68337], Loss: 5.1740\n",
      "Epoch [6/10], Step [45750/68337], Loss: 4.9340\n",
      "Epoch [6/10], Step [45825/68337], Loss: 4.9900\n",
      "Epoch [6/10], Step [45900/68337], Loss: 4.9526\n",
      "Epoch [6/10], Step [45975/68337], Loss: 5.0094\n",
      "Epoch [6/10], Step [46050/68337], Loss: 5.0754\n",
      "Epoch [6/10], Step [46125/68337], Loss: 5.1638\n",
      "Epoch [6/10], Step [46200/68337], Loss: 4.9752\n",
      "Epoch [6/10], Step [46275/68337], Loss: 5.1542\n",
      "Epoch [6/10], Step [46350/68337], Loss: 5.0578\n",
      "Epoch [6/10], Step [46425/68337], Loss: 5.2902\n",
      "Epoch [6/10], Step [46500/68337], Loss: 5.1516\n",
      "Epoch [6/10], Step [46575/68337], Loss: 5.2197\n",
      "Epoch [6/10], Step [46650/68337], Loss: 5.0184\n",
      "Epoch [6/10], Step [46725/68337], Loss: 5.0229\n",
      "Epoch [6/10], Step [46800/68337], Loss: 5.0614\n",
      "Epoch [6/10], Step [46875/68337], Loss: 5.0859\n",
      "Epoch [6/10], Step [46950/68337], Loss: 5.1557\n",
      "Epoch [6/10], Step [47025/68337], Loss: 5.0613\n",
      "Epoch [6/10], Step [47100/68337], Loss: 4.9341\n",
      "Epoch [6/10], Step [47175/68337], Loss: 5.0916\n",
      "Epoch [6/10], Step [47250/68337], Loss: 5.1557\n",
      "Epoch [6/10], Step [47325/68337], Loss: 4.9907\n",
      "Epoch [6/10], Step [47400/68337], Loss: 4.7909\n",
      "Epoch [6/10], Step [47475/68337], Loss: 5.0235\n",
      "Epoch [6/10], Step [47550/68337], Loss: 5.0614\n",
      "Epoch [6/10], Step [47625/68337], Loss: 5.2678\n",
      "Epoch [6/10], Step [47700/68337], Loss: 5.0905\n",
      "Epoch [6/10], Step [47775/68337], Loss: 5.0259\n",
      "Epoch [6/10], Step [47850/68337], Loss: 5.0238\n",
      "Epoch [6/10], Step [47925/68337], Loss: 5.0906\n",
      "Epoch [6/10], Step [48000/68337], Loss: 5.1719\n",
      "Epoch [6/10], Step [48075/68337], Loss: 5.4094\n",
      "Epoch [6/10], Step [48150/68337], Loss: 5.0627\n",
      "Epoch [6/10], Step [48225/68337], Loss: 5.2520\n",
      "Epoch [6/10], Step [48300/68337], Loss: 5.2150\n",
      "Epoch [6/10], Step [48375/68337], Loss: 5.1942\n",
      "Epoch [6/10], Step [48450/68337], Loss: 5.0480\n",
      "Epoch [6/10], Step [48525/68337], Loss: 5.0195\n",
      "Epoch [6/10], Step [48600/68337], Loss: 5.0799\n",
      "Epoch [6/10], Step [48675/68337], Loss: 5.1761\n",
      "Epoch [6/10], Step [48750/68337], Loss: 5.1295\n",
      "Epoch [6/10], Step [48825/68337], Loss: 5.0983\n",
      "Epoch [6/10], Step [48900/68337], Loss: 5.1991\n",
      "Epoch [6/10], Step [48975/68337], Loss: 5.1602\n",
      "Epoch [6/10], Step [49050/68337], Loss: 4.9873\n",
      "Epoch [6/10], Step [49125/68337], Loss: 5.1172\n",
      "Epoch [6/10], Step [49200/68337], Loss: 5.1017\n",
      "Epoch [6/10], Step [49275/68337], Loss: 5.0403\n",
      "Epoch [6/10], Step [49350/68337], Loss: 5.1959\n",
      "Epoch [6/10], Step [49425/68337], Loss: 5.2651\n",
      "Epoch [6/10], Step [49500/68337], Loss: 5.0736\n",
      "Epoch [6/10], Step [49575/68337], Loss: 4.9058\n",
      "Epoch [6/10], Step [49650/68337], Loss: 5.1509\n",
      "Epoch [6/10], Step [49725/68337], Loss: 5.2123\n",
      "Epoch [6/10], Step [49800/68337], Loss: 5.0371\n",
      "Epoch [6/10], Step [49875/68337], Loss: 5.1411\n",
      "Epoch [6/10], Step [49950/68337], Loss: 5.2793\n",
      "Validation perplexity: 124.34992596080298\n",
      "Epoch [6/10], Step [50025/68337], Loss: 4.9773\n",
      "Epoch [6/10], Step [50100/68337], Loss: 5.1444\n",
      "Epoch [6/10], Step [50175/68337], Loss: 5.2564\n",
      "Epoch [6/10], Step [50250/68337], Loss: 5.0051\n",
      "Epoch [6/10], Step [50325/68337], Loss: 5.0712\n",
      "Epoch [6/10], Step [50400/68337], Loss: 5.0771\n",
      "Epoch [6/10], Step [50475/68337], Loss: 5.1340\n",
      "Epoch [6/10], Step [50550/68337], Loss: 5.2048\n",
      "Epoch [6/10], Step [50625/68337], Loss: 5.0169\n",
      "Epoch [6/10], Step [50700/68337], Loss: 5.1543\n",
      "Epoch [6/10], Step [50775/68337], Loss: 5.3839\n",
      "Epoch [6/10], Step [50850/68337], Loss: 5.0101\n",
      "Epoch [6/10], Step [50925/68337], Loss: 5.1437\n",
      "Epoch [6/10], Step [51000/68337], Loss: 4.9329\n",
      "Epoch [6/10], Step [51075/68337], Loss: 5.1901\n",
      "Epoch [6/10], Step [51150/68337], Loss: 4.9896\n",
      "Epoch [6/10], Step [51225/68337], Loss: 5.3134\n",
      "Epoch [6/10], Step [51300/68337], Loss: 4.9984\n",
      "Epoch [6/10], Step [51375/68337], Loss: 5.0742\n",
      "Epoch [6/10], Step [51450/68337], Loss: 5.1137\n",
      "Epoch [6/10], Step [51525/68337], Loss: 5.1454\n",
      "Epoch [6/10], Step [51600/68337], Loss: 5.2763\n",
      "Epoch [6/10], Step [51675/68337], Loss: 5.0703\n",
      "Epoch [6/10], Step [51750/68337], Loss: 5.2786\n",
      "Epoch [6/10], Step [51825/68337], Loss: 4.9917\n",
      "Epoch [6/10], Step [51900/68337], Loss: 4.9758\n",
      "Epoch [6/10], Step [51975/68337], Loss: 5.0964\n",
      "Epoch [6/10], Step [52050/68337], Loss: 5.0762\n",
      "Epoch [6/10], Step [52125/68337], Loss: 5.0806\n",
      "Epoch [6/10], Step [52200/68337], Loss: 5.2356\n",
      "Epoch [6/10], Step [52275/68337], Loss: 5.2323\n",
      "Epoch [6/10], Step [52350/68337], Loss: 5.0081\n",
      "Epoch [6/10], Step [52425/68337], Loss: 5.0571\n",
      "Epoch [6/10], Step [52500/68337], Loss: 5.0447\n",
      "Epoch [6/10], Step [52575/68337], Loss: 5.0899\n",
      "Epoch [6/10], Step [52650/68337], Loss: 5.1060\n",
      "Epoch [6/10], Step [52725/68337], Loss: 4.9459\n",
      "Epoch [6/10], Step [52800/68337], Loss: 4.9820\n",
      "Epoch [6/10], Step [52875/68337], Loss: 5.1152\n",
      "Epoch [6/10], Step [52950/68337], Loss: 5.0602\n",
      "Epoch [6/10], Step [53025/68337], Loss: 5.2194\n",
      "Epoch [6/10], Step [53100/68337], Loss: 5.0278\n",
      "Epoch [6/10], Step [53175/68337], Loss: 5.1057\n",
      "Epoch [6/10], Step [53250/68337], Loss: 5.0389\n",
      "Epoch [6/10], Step [53325/68337], Loss: 5.2451\n",
      "Epoch [6/10], Step [53400/68337], Loss: 5.1611\n",
      "Epoch [6/10], Step [53475/68337], Loss: 5.1268\n",
      "Epoch [6/10], Step [53550/68337], Loss: 5.2175\n",
      "Epoch [6/10], Step [53625/68337], Loss: 5.0459\n",
      "Epoch [6/10], Step [53700/68337], Loss: 5.1310\n",
      "Epoch [6/10], Step [53775/68337], Loss: 5.0782\n",
      "Epoch [6/10], Step [53850/68337], Loss: 5.2125\n",
      "Epoch [6/10], Step [53925/68337], Loss: 5.0231\n",
      "Epoch [6/10], Step [54000/68337], Loss: 5.2627\n",
      "Epoch [6/10], Step [54075/68337], Loss: 5.0834\n",
      "Epoch [6/10], Step [54150/68337], Loss: 5.0036\n",
      "Epoch [6/10], Step [54225/68337], Loss: 5.0518\n",
      "Epoch [6/10], Step [54300/68337], Loss: 5.1605\n",
      "Epoch [6/10], Step [54375/68337], Loss: 5.1447\n",
      "Epoch [6/10], Step [54450/68337], Loss: 5.1374\n",
      "Epoch [6/10], Step [54525/68337], Loss: 5.1314\n",
      "Epoch [6/10], Step [54600/68337], Loss: 5.1877\n",
      "Epoch [6/10], Step [54675/68337], Loss: 4.9119\n",
      "Epoch [6/10], Step [54750/68337], Loss: 5.2096\n",
      "Epoch [6/10], Step [54825/68337], Loss: 4.9906\n",
      "Epoch [6/10], Step [54900/68337], Loss: 5.0459\n",
      "Epoch [6/10], Step [54975/68337], Loss: 5.1605\n",
      "Epoch [6/10], Step [55050/68337], Loss: 5.2235\n",
      "Epoch [6/10], Step [55125/68337], Loss: 5.2551\n",
      "Epoch [6/10], Step [55200/68337], Loss: 5.0446\n",
      "Epoch [6/10], Step [55275/68337], Loss: 4.9060\n",
      "Epoch [6/10], Step [55350/68337], Loss: 5.0237\n",
      "Epoch [6/10], Step [55425/68337], Loss: 4.7897\n",
      "Epoch [6/10], Step [55500/68337], Loss: 5.1740\n",
      "Epoch [6/10], Step [55575/68337], Loss: 5.2610\n",
      "Epoch [6/10], Step [55650/68337], Loss: 5.0852\n",
      "Epoch [6/10], Step [55725/68337], Loss: 4.8680\n",
      "Epoch [6/10], Step [55800/68337], Loss: 5.0294\n",
      "Epoch [6/10], Step [55875/68337], Loss: 5.0465\n",
      "Epoch [6/10], Step [55950/68337], Loss: 4.9301\n",
      "Epoch [6/10], Step [56025/68337], Loss: 4.9269\n",
      "Epoch [6/10], Step [56100/68337], Loss: 5.2457\n",
      "Epoch [6/10], Step [56175/68337], Loss: 5.0449\n",
      "Epoch [6/10], Step [56250/68337], Loss: 4.9995\n",
      "Epoch [6/10], Step [56325/68337], Loss: 5.0184\n",
      "Epoch [6/10], Step [56400/68337], Loss: 4.9990\n",
      "Epoch [6/10], Step [56475/68337], Loss: 5.0180\n",
      "Epoch [6/10], Step [56550/68337], Loss: 5.2965\n",
      "Epoch [6/10], Step [56625/68337], Loss: 5.0043\n",
      "Epoch [6/10], Step [56700/68337], Loss: 5.1443\n",
      "Epoch [6/10], Step [56775/68337], Loss: 4.9817\n",
      "Epoch [6/10], Step [56850/68337], Loss: 5.2132\n",
      "Epoch [6/10], Step [56925/68337], Loss: 5.2535\n",
      "Epoch [6/10], Step [57000/68337], Loss: 5.0685\n",
      "Epoch [6/10], Step [57075/68337], Loss: 5.0904\n",
      "Epoch [6/10], Step [57150/68337], Loss: 5.0009\n",
      "Epoch [6/10], Step [57225/68337], Loss: 5.1420\n",
      "Epoch [6/10], Step [57300/68337], Loss: 5.2467\n",
      "Epoch [6/10], Step [57375/68337], Loss: 4.9934\n",
      "Epoch [6/10], Step [57450/68337], Loss: 5.0488\n",
      "Epoch [6/10], Step [57525/68337], Loss: 5.0761\n",
      "Epoch [6/10], Step [57600/68337], Loss: 5.2926\n",
      "Epoch [6/10], Step [57675/68337], Loss: 5.0515\n",
      "Epoch [6/10], Step [57750/68337], Loss: 4.9558\n",
      "Epoch [6/10], Step [57825/68337], Loss: 5.1431\n",
      "Epoch [6/10], Step [57900/68337], Loss: 4.9957\n",
      "Epoch [6/10], Step [57975/68337], Loss: 5.1977\n",
      "Epoch [6/10], Step [58050/68337], Loss: 5.1103\n",
      "Epoch [6/10], Step [58125/68337], Loss: 5.2257\n",
      "Epoch [6/10], Step [58200/68337], Loss: 4.8598\n",
      "Epoch [6/10], Step [58275/68337], Loss: 5.1370\n",
      "Epoch [6/10], Step [58350/68337], Loss: 4.9868\n",
      "Epoch [6/10], Step [58425/68337], Loss: 5.0409\n",
      "Epoch [6/10], Step [58500/68337], Loss: 5.2693\n",
      "Epoch [6/10], Step [58575/68337], Loss: 4.9610\n",
      "Epoch [6/10], Step [58650/68337], Loss: 5.1757\n",
      "Epoch [6/10], Step [58725/68337], Loss: 5.2867\n",
      "Epoch [6/10], Step [58800/68337], Loss: 5.0396\n",
      "Epoch [6/10], Step [58875/68337], Loss: 5.3464\n",
      "Epoch [6/10], Step [58950/68337], Loss: 5.0952\n",
      "Epoch [6/10], Step [59025/68337], Loss: 5.0732\n",
      "Epoch [6/10], Step [59100/68337], Loss: 5.0317\n",
      "Epoch [6/10], Step [59175/68337], Loss: 4.9567\n",
      "Epoch [6/10], Step [59250/68337], Loss: 5.1731\n",
      "Epoch [6/10], Step [59325/68337], Loss: 5.1126\n",
      "Epoch [6/10], Step [59400/68337], Loss: 5.1299\n",
      "Epoch [6/10], Step [59475/68337], Loss: 5.0270\n",
      "Epoch [6/10], Step [59550/68337], Loss: 5.1329\n",
      "Epoch [6/10], Step [59625/68337], Loss: 4.9960\n",
      "Epoch [6/10], Step [59700/68337], Loss: 5.1606\n",
      "Epoch [6/10], Step [59775/68337], Loss: 5.0896\n",
      "Epoch [6/10], Step [59850/68337], Loss: 5.1635\n",
      "Epoch [6/10], Step [59925/68337], Loss: 5.1104\n",
      "Epoch [6/10], Step [60000/68337], Loss: 4.8374\n",
      "Validation perplexity: 124.5878902074738\n",
      "Epoch [6/10], Step [60075/68337], Loss: 5.0280\n",
      "Epoch [6/10], Step [60150/68337], Loss: 5.2635\n",
      "Epoch [6/10], Step [60225/68337], Loss: 5.2136\n",
      "Epoch [6/10], Step [60300/68337], Loss: 4.8521\n",
      "Epoch [6/10], Step [60375/68337], Loss: 5.2503\n",
      "Epoch [6/10], Step [60450/68337], Loss: 5.2251\n",
      "Epoch [6/10], Step [60525/68337], Loss: 5.0413\n",
      "Epoch [6/10], Step [60600/68337], Loss: 5.0794\n",
      "Epoch [6/10], Step [60675/68337], Loss: 5.1254\n",
      "Epoch [6/10], Step [60750/68337], Loss: 5.2651\n",
      "Epoch [6/10], Step [60825/68337], Loss: 5.2618\n",
      "Epoch [6/10], Step [60900/68337], Loss: 5.0146\n",
      "Epoch [6/10], Step [60975/68337], Loss: 5.1341\n",
      "Epoch [6/10], Step [61050/68337], Loss: 5.2528\n",
      "Epoch [6/10], Step [61125/68337], Loss: 5.0459\n",
      "Epoch [6/10], Step [61200/68337], Loss: 5.0471\n",
      "Epoch [6/10], Step [61275/68337], Loss: 5.0930\n",
      "Epoch [6/10], Step [61350/68337], Loss: 5.0974\n",
      "Epoch [6/10], Step [61425/68337], Loss: 5.1700\n",
      "Epoch [6/10], Step [61500/68337], Loss: 5.2423\n",
      "Epoch [6/10], Step [61575/68337], Loss: 4.9627\n",
      "Epoch [6/10], Step [61650/68337], Loss: 5.1138\n",
      "Epoch [6/10], Step [61725/68337], Loss: 5.2965\n",
      "Epoch [6/10], Step [61800/68337], Loss: 5.0711\n",
      "Epoch [6/10], Step [61875/68337], Loss: 5.0158\n",
      "Epoch [6/10], Step [61950/68337], Loss: 4.9113\n",
      "Epoch [6/10], Step [62025/68337], Loss: 5.0328\n",
      "Epoch [6/10], Step [62100/68337], Loss: 5.2339\n",
      "Epoch [6/10], Step [62175/68337], Loss: 5.1460\n",
      "Epoch [6/10], Step [62250/68337], Loss: 5.2237\n",
      "Epoch [6/10], Step [62325/68337], Loss: 5.1228\n",
      "Epoch [6/10], Step [62400/68337], Loss: 4.9168\n",
      "Epoch [6/10], Step [62475/68337], Loss: 5.1232\n",
      "Epoch [6/10], Step [62550/68337], Loss: 5.2094\n",
      "Epoch [6/10], Step [62625/68337], Loss: 5.1385\n",
      "Epoch [6/10], Step [62700/68337], Loss: 5.1800\n",
      "Epoch [6/10], Step [62775/68337], Loss: 5.2865\n",
      "Epoch [6/10], Step [62850/68337], Loss: 5.2206\n",
      "Epoch [6/10], Step [62925/68337], Loss: 4.9472\n",
      "Epoch [6/10], Step [63000/68337], Loss: 5.1409\n",
      "Epoch [6/10], Step [63075/68337], Loss: 5.0199\n",
      "Epoch [6/10], Step [63150/68337], Loss: 5.1226\n",
      "Epoch [6/10], Step [63225/68337], Loss: 4.9132\n",
      "Epoch [6/10], Step [63300/68337], Loss: 5.1134\n",
      "Epoch [6/10], Step [63375/68337], Loss: 5.0687\n",
      "Epoch [6/10], Step [63450/68337], Loss: 5.1137\n",
      "Epoch [6/10], Step [63525/68337], Loss: 5.2698\n",
      "Epoch [6/10], Step [63600/68337], Loss: 5.2249\n",
      "Epoch [6/10], Step [63675/68337], Loss: 5.1145\n",
      "Epoch [6/10], Step [63750/68337], Loss: 5.0394\n",
      "Epoch [6/10], Step [63825/68337], Loss: 5.2819\n",
      "Epoch [6/10], Step [63900/68337], Loss: 5.1616\n",
      "Epoch [6/10], Step [63975/68337], Loss: 5.0213\n",
      "Epoch [6/10], Step [64050/68337], Loss: 4.8455\n",
      "Epoch [6/10], Step [64125/68337], Loss: 5.0984\n",
      "Epoch [6/10], Step [64200/68337], Loss: 5.1378\n",
      "Epoch [6/10], Step [64275/68337], Loss: 5.2255\n",
      "Epoch [6/10], Step [64350/68337], Loss: 5.0042\n",
      "Epoch [6/10], Step [64425/68337], Loss: 5.2408\n",
      "Epoch [6/10], Step [64500/68337], Loss: 4.9052\n",
      "Epoch [6/10], Step [64575/68337], Loss: 5.0292\n",
      "Epoch [6/10], Step [64650/68337], Loss: 5.0023\n",
      "Epoch [6/10], Step [64725/68337], Loss: 5.3005\n",
      "Epoch [6/10], Step [64800/68337], Loss: 5.2938\n",
      "Epoch [6/10], Step [64875/68337], Loss: 4.9845\n",
      "Epoch [6/10], Step [64950/68337], Loss: 5.1039\n",
      "Epoch [6/10], Step [65025/68337], Loss: 5.2142\n",
      "Epoch [6/10], Step [65100/68337], Loss: 5.1816\n",
      "Epoch [6/10], Step [65175/68337], Loss: 5.1820\n",
      "Epoch [6/10], Step [65250/68337], Loss: 4.9321\n",
      "Epoch [6/10], Step [65325/68337], Loss: 5.1587\n",
      "Epoch [6/10], Step [65400/68337], Loss: 5.0793\n",
      "Epoch [6/10], Step [65475/68337], Loss: 5.1284\n",
      "Epoch [6/10], Step [65550/68337], Loss: 5.0297\n",
      "Epoch [6/10], Step [65625/68337], Loss: 5.2150\n",
      "Epoch [6/10], Step [65700/68337], Loss: 4.9248\n",
      "Epoch [6/10], Step [65775/68337], Loss: 5.1732\n",
      "Epoch [6/10], Step [65850/68337], Loss: 5.0472\n",
      "Epoch [6/10], Step [65925/68337], Loss: 5.3705\n",
      "Epoch [6/10], Step [66000/68337], Loss: 5.0291\n",
      "Epoch [6/10], Step [66075/68337], Loss: 5.0367\n",
      "Epoch [6/10], Step [66150/68337], Loss: 4.9296\n",
      "Epoch [6/10], Step [66225/68337], Loss: 5.1611\n",
      "Epoch [6/10], Step [66300/68337], Loss: 4.9336\n",
      "Epoch [6/10], Step [66375/68337], Loss: 4.8431\n",
      "Epoch [6/10], Step [66450/68337], Loss: 5.0027\n",
      "Epoch [6/10], Step [66525/68337], Loss: 4.9577\n",
      "Epoch [6/10], Step [66600/68337], Loss: 5.1015\n",
      "Epoch [6/10], Step [66675/68337], Loss: 5.0723\n",
      "Epoch [6/10], Step [66750/68337], Loss: 5.0054\n",
      "Epoch [6/10], Step [66825/68337], Loss: 5.0375\n",
      "Epoch [6/10], Step [66900/68337], Loss: 5.0333\n",
      "Epoch [6/10], Step [66975/68337], Loss: 5.2108\n",
      "Epoch [6/10], Step [67050/68337], Loss: 4.8934\n",
      "Epoch [6/10], Step [67125/68337], Loss: 5.0356\n",
      "Epoch [6/10], Step [67200/68337], Loss: 5.0833\n",
      "Epoch [6/10], Step [67275/68337], Loss: 5.2005\n",
      "Epoch [6/10], Step [67350/68337], Loss: 4.9611\n",
      "Epoch [6/10], Step [67425/68337], Loss: 5.1443\n",
      "Epoch [6/10], Step [67500/68337], Loss: 5.0794\n",
      "Epoch [6/10], Step [67575/68337], Loss: 5.1802\n",
      "Epoch [6/10], Step [67650/68337], Loss: 5.1454\n",
      "Epoch [6/10], Step [67725/68337], Loss: 5.0377\n",
      "Epoch [6/10], Step [67800/68337], Loss: 5.0333\n",
      "Epoch [6/10], Step [67875/68337], Loss: 5.1810\n",
      "Epoch [6/10], Step [67950/68337], Loss: 5.0629\n",
      "Epoch [6/10], Step [68025/68337], Loss: 5.2365\n",
      "Epoch [6/10], Step [68100/68337], Loss: 5.1398\n",
      "Epoch [6/10], Step [68175/68337], Loss: 5.1173\n",
      "Epoch [6/10], Step [68250/68337], Loss: 4.9869\n",
      "Epoch [6/10], Step [68325/68337], Loss: 5.1268\n",
      "Epoch [6/10] Average Loss: 5.1014, Perplexity: 164.26\n",
      "Epoch [7/10], Step [0/68337], Loss: 5.1200\n",
      "Validation perplexity: 124.06050928820632\n",
      "Epoch [7/10], Step [75/68337], Loss: 4.9450\n",
      "Epoch [7/10], Step [150/68337], Loss: 4.9739\n",
      "Epoch [7/10], Step [225/68337], Loss: 5.2478\n",
      "Epoch [7/10], Step [300/68337], Loss: 4.9887\n",
      "Epoch [7/10], Step [375/68337], Loss: 5.0782\n",
      "Epoch [7/10], Step [450/68337], Loss: 4.9914\n",
      "Epoch [7/10], Step [525/68337], Loss: 5.1416\n",
      "Epoch [7/10], Step [600/68337], Loss: 5.2958\n",
      "Epoch [7/10], Step [675/68337], Loss: 5.1716\n",
      "Epoch [7/10], Step [750/68337], Loss: 5.1030\n",
      "Epoch [7/10], Step [825/68337], Loss: 5.2266\n",
      "Epoch [7/10], Step [900/68337], Loss: 5.2748\n",
      "Epoch [7/10], Step [975/68337], Loss: 5.0733\n",
      "Epoch [7/10], Step [1050/68337], Loss: 4.8371\n",
      "Epoch [7/10], Step [1125/68337], Loss: 4.8998\n",
      "Epoch [7/10], Step [1200/68337], Loss: 5.1140\n",
      "Epoch [7/10], Step [1275/68337], Loss: 5.0096\n",
      "Epoch [7/10], Step [1350/68337], Loss: 4.9733\n",
      "Epoch [7/10], Step [1425/68337], Loss: 5.2176\n",
      "Epoch [7/10], Step [1500/68337], Loss: 5.2861\n",
      "Epoch [7/10], Step [1575/68337], Loss: 4.8720\n",
      "Epoch [7/10], Step [1650/68337], Loss: 4.7841\n",
      "Epoch [7/10], Step [1725/68337], Loss: 5.0329\n",
      "Epoch [7/10], Step [1800/68337], Loss: 5.0038\n",
      "Epoch [7/10], Step [1875/68337], Loss: 5.1582\n",
      "Epoch [7/10], Step [1950/68337], Loss: 5.1474\n",
      "Epoch [7/10], Step [2025/68337], Loss: 4.9936\n",
      "Epoch [7/10], Step [2100/68337], Loss: 5.2662\n",
      "Epoch [7/10], Step [2175/68337], Loss: 5.2149\n",
      "Epoch [7/10], Step [2250/68337], Loss: 5.0288\n",
      "Epoch [7/10], Step [2325/68337], Loss: 4.9995\n",
      "Epoch [7/10], Step [2400/68337], Loss: 5.1970\n",
      "Epoch [7/10], Step [2475/68337], Loss: 4.9361\n",
      "Epoch [7/10], Step [2550/68337], Loss: 4.9940\n",
      "Epoch [7/10], Step [2625/68337], Loss: 5.1678\n",
      "Epoch [7/10], Step [2700/68337], Loss: 5.1263\n",
      "Epoch [7/10], Step [2775/68337], Loss: 5.0775\n",
      "Epoch [7/10], Step [2850/68337], Loss: 4.9583\n",
      "Epoch [7/10], Step [2925/68337], Loss: 4.9596\n",
      "Epoch [7/10], Step [3000/68337], Loss: 5.1231\n",
      "Epoch [7/10], Step [3075/68337], Loss: 5.1278\n",
      "Epoch [7/10], Step [3150/68337], Loss: 5.1235\n",
      "Epoch [7/10], Step [3225/68337], Loss: 5.1059\n",
      "Epoch [7/10], Step [3300/68337], Loss: 5.0593\n",
      "Epoch [7/10], Step [3375/68337], Loss: 5.1134\n",
      "Epoch [7/10], Step [3450/68337], Loss: 5.2646\n",
      "Epoch [7/10], Step [3525/68337], Loss: 4.9409\n",
      "Epoch [7/10], Step [3600/68337], Loss: 5.1333\n",
      "Epoch [7/10], Step [3675/68337], Loss: 5.0399\n",
      "Epoch [7/10], Step [3750/68337], Loss: 4.9037\n",
      "Epoch [7/10], Step [3825/68337], Loss: 5.0223\n",
      "Epoch [7/10], Step [3900/68337], Loss: 5.2192\n",
      "Epoch [7/10], Step [3975/68337], Loss: 5.0198\n",
      "Epoch [7/10], Step [4050/68337], Loss: 5.0271\n",
      "Epoch [7/10], Step [4125/68337], Loss: 5.0932\n",
      "Epoch [7/10], Step [4200/68337], Loss: 5.1859\n",
      "Epoch [7/10], Step [4275/68337], Loss: 4.9719\n",
      "Epoch [7/10], Step [4350/68337], Loss: 5.0653\n",
      "Epoch [7/10], Step [4425/68337], Loss: 5.0118\n",
      "Epoch [7/10], Step [4500/68337], Loss: 4.9494\n",
      "Epoch [7/10], Step [4575/68337], Loss: 4.8645\n",
      "Epoch [7/10], Step [4650/68337], Loss: 5.1632\n",
      "Epoch [7/10], Step [4725/68337], Loss: 5.1346\n",
      "Epoch [7/10], Step [4800/68337], Loss: 5.0630\n",
      "Epoch [7/10], Step [4875/68337], Loss: 5.0074\n",
      "Epoch [7/10], Step [4950/68337], Loss: 5.0513\n",
      "Epoch [7/10], Step [5025/68337], Loss: 5.1330\n",
      "Epoch [7/10], Step [5100/68337], Loss: 5.1725\n",
      "Epoch [7/10], Step [5175/68337], Loss: 5.1487\n",
      "Epoch [7/10], Step [5250/68337], Loss: 5.1584\n",
      "Epoch [7/10], Step [5325/68337], Loss: 5.0212\n",
      "Epoch [7/10], Step [5400/68337], Loss: 5.0677\n",
      "Epoch [7/10], Step [5475/68337], Loss: 5.0303\n",
      "Epoch [7/10], Step [5550/68337], Loss: 5.0082\n",
      "Epoch [7/10], Step [5625/68337], Loss: 4.7307\n",
      "Epoch [7/10], Step [5700/68337], Loss: 4.9696\n",
      "Epoch [7/10], Step [5775/68337], Loss: 5.2194\n",
      "Epoch [7/10], Step [5850/68337], Loss: 5.1355\n",
      "Epoch [7/10], Step [5925/68337], Loss: 5.1508\n",
      "Epoch [7/10], Step [6000/68337], Loss: 4.9762\n",
      "Epoch [7/10], Step [6075/68337], Loss: 5.2007\n",
      "Epoch [7/10], Step [6150/68337], Loss: 4.9034\n",
      "Epoch [7/10], Step [6225/68337], Loss: 4.9548\n",
      "Epoch [7/10], Step [6300/68337], Loss: 5.0258\n",
      "Epoch [7/10], Step [6375/68337], Loss: 5.3593\n",
      "Epoch [7/10], Step [6450/68337], Loss: 5.0392\n",
      "Epoch [7/10], Step [6525/68337], Loss: 5.3487\n",
      "Epoch [7/10], Step [6600/68337], Loss: 5.3237\n",
      "Epoch [7/10], Step [6675/68337], Loss: 5.3023\n",
      "Epoch [7/10], Step [6750/68337], Loss: 5.0868\n",
      "Epoch [7/10], Step [6825/68337], Loss: 5.1369\n",
      "Epoch [7/10], Step [6900/68337], Loss: 5.0315\n",
      "Epoch [7/10], Step [6975/68337], Loss: 4.8402\n",
      "Epoch [7/10], Step [7050/68337], Loss: 4.9929\n",
      "Epoch [7/10], Step [7125/68337], Loss: 5.2239\n",
      "Epoch [7/10], Step [7200/68337], Loss: 5.3237\n",
      "Epoch [7/10], Step [7275/68337], Loss: 5.0378\n",
      "Epoch [7/10], Step [7350/68337], Loss: 5.1192\n",
      "Epoch [7/10], Step [7425/68337], Loss: 4.9706\n",
      "Epoch [7/10], Step [7500/68337], Loss: 5.0052\n",
      "Epoch [7/10], Step [7575/68337], Loss: 5.0246\n",
      "Epoch [7/10], Step [7650/68337], Loss: 5.0094\n",
      "Epoch [7/10], Step [7725/68337], Loss: 4.8701\n",
      "Epoch [7/10], Step [7800/68337], Loss: 4.7212\n",
      "Epoch [7/10], Step [7875/68337], Loss: 5.2185\n",
      "Epoch [7/10], Step [7950/68337], Loss: 4.9017\n",
      "Epoch [7/10], Step [8025/68337], Loss: 4.8372\n",
      "Epoch [7/10], Step [8100/68337], Loss: 5.3671\n",
      "Epoch [7/10], Step [8175/68337], Loss: 5.2543\n",
      "Epoch [7/10], Step [8250/68337], Loss: 5.2209\n",
      "Epoch [7/10], Step [8325/68337], Loss: 5.0093\n",
      "Epoch [7/10], Step [8400/68337], Loss: 5.0510\n",
      "Epoch [7/10], Step [8475/68337], Loss: 5.0008\n",
      "Epoch [7/10], Step [8550/68337], Loss: 5.0004\n",
      "Epoch [7/10], Step [8625/68337], Loss: 5.1621\n",
      "Epoch [7/10], Step [8700/68337], Loss: 5.3396\n",
      "Epoch [7/10], Step [8775/68337], Loss: 5.2332\n",
      "Epoch [7/10], Step [8850/68337], Loss: 5.2327\n",
      "Epoch [7/10], Step [8925/68337], Loss: 5.1251\n",
      "Epoch [7/10], Step [9000/68337], Loss: 4.9947\n",
      "Epoch [7/10], Step [9075/68337], Loss: 5.0786\n",
      "Epoch [7/10], Step [9150/68337], Loss: 5.1468\n",
      "Epoch [7/10], Step [9225/68337], Loss: 4.9221\n",
      "Epoch [7/10], Step [9300/68337], Loss: 5.1293\n",
      "Epoch [7/10], Step [9375/68337], Loss: 5.0374\n",
      "Epoch [7/10], Step [9450/68337], Loss: 5.4053\n",
      "Epoch [7/10], Step [9525/68337], Loss: 5.0881\n",
      "Epoch [7/10], Step [9600/68337], Loss: 5.1037\n",
      "Epoch [7/10], Step [9675/68337], Loss: 5.1949\n",
      "Epoch [7/10], Step [9750/68337], Loss: 5.0306\n",
      "Epoch [7/10], Step [9825/68337], Loss: 5.2023\n",
      "Epoch [7/10], Step [9900/68337], Loss: 4.9878\n",
      "Epoch [7/10], Step [9975/68337], Loss: 5.0069\n",
      "Validation perplexity: 123.97047158398982\n",
      "Epoch [7/10], Step [10050/68337], Loss: 5.0319\n",
      "Epoch [7/10], Step [10125/68337], Loss: 4.9411\n",
      "Epoch [7/10], Step [10200/68337], Loss: 5.0572\n",
      "Epoch [7/10], Step [10275/68337], Loss: 4.9733\n",
      "Epoch [7/10], Step [10350/68337], Loss: 5.2591\n",
      "Epoch [7/10], Step [10425/68337], Loss: 5.1277\n",
      "Epoch [7/10], Step [10500/68337], Loss: 5.0559\n",
      "Epoch [7/10], Step [10575/68337], Loss: 5.2300\n",
      "Epoch [7/10], Step [10650/68337], Loss: 5.1411\n",
      "Epoch [7/10], Step [10725/68337], Loss: 5.1062\n",
      "Epoch [7/10], Step [10800/68337], Loss: 4.9189\n",
      "Epoch [7/10], Step [10875/68337], Loss: 4.9099\n",
      "Epoch [7/10], Step [10950/68337], Loss: 5.2864\n",
      "Epoch [7/10], Step [11025/68337], Loss: 4.8329\n",
      "Epoch [7/10], Step [11100/68337], Loss: 5.1863\n",
      "Epoch [7/10], Step [11175/68337], Loss: 4.9806\n",
      "Epoch [7/10], Step [11250/68337], Loss: 5.2772\n",
      "Epoch [7/10], Step [11325/68337], Loss: 5.1198\n",
      "Epoch [7/10], Step [11400/68337], Loss: 5.0339\n",
      "Epoch [7/10], Step [11475/68337], Loss: 5.3172\n",
      "Epoch [7/10], Step [11550/68337], Loss: 5.2836\n",
      "Epoch [7/10], Step [11625/68337], Loss: 5.2157\n",
      "Epoch [7/10], Step [11700/68337], Loss: 4.9629\n",
      "Epoch [7/10], Step [11775/68337], Loss: 5.1302\n",
      "Epoch [7/10], Step [11850/68337], Loss: 5.1141\n",
      "Epoch [7/10], Step [11925/68337], Loss: 5.1479\n",
      "Epoch [7/10], Step [12000/68337], Loss: 5.0371\n",
      "Epoch [7/10], Step [12075/68337], Loss: 4.9028\n",
      "Epoch [7/10], Step [12150/68337], Loss: 5.2122\n",
      "Epoch [7/10], Step [12225/68337], Loss: 5.1477\n",
      "Epoch [7/10], Step [12300/68337], Loss: 5.3885\n",
      "Epoch [7/10], Step [12375/68337], Loss: 5.0449\n",
      "Epoch [7/10], Step [12450/68337], Loss: 4.9690\n",
      "Epoch [7/10], Step [12525/68337], Loss: 4.9346\n",
      "Epoch [7/10], Step [12600/68337], Loss: 5.1508\n",
      "Epoch [7/10], Step [12675/68337], Loss: 5.0397\n",
      "Epoch [7/10], Step [12750/68337], Loss: 5.1402\n",
      "Epoch [7/10], Step [12825/68337], Loss: 5.0454\n",
      "Epoch [7/10], Step [12900/68337], Loss: 4.9356\n",
      "Epoch [7/10], Step [12975/68337], Loss: 4.9941\n",
      "Epoch [7/10], Step [13050/68337], Loss: 5.3881\n",
      "Epoch [7/10], Step [13125/68337], Loss: 5.2483\n",
      "Epoch [7/10], Step [13200/68337], Loss: 5.0905\n",
      "Epoch [7/10], Step [13275/68337], Loss: 5.1955\n",
      "Epoch [7/10], Step [13350/68337], Loss: 5.0331\n",
      "Epoch [7/10], Step [13425/68337], Loss: 5.0161\n",
      "Epoch [7/10], Step [13500/68337], Loss: 5.1283\n",
      "Epoch [7/10], Step [13575/68337], Loss: 5.0775\n",
      "Epoch [7/10], Step [13650/68337], Loss: 5.0553\n",
      "Epoch [7/10], Step [13725/68337], Loss: 5.0373\n",
      "Epoch [7/10], Step [13800/68337], Loss: 5.1862\n",
      "Epoch [7/10], Step [13875/68337], Loss: 5.0181\n",
      "Epoch [7/10], Step [13950/68337], Loss: 5.0653\n",
      "Epoch [7/10], Step [14025/68337], Loss: 5.0613\n",
      "Epoch [7/10], Step [14100/68337], Loss: 5.1118\n",
      "Epoch [7/10], Step [14175/68337], Loss: 5.2357\n",
      "Epoch [7/10], Step [14250/68337], Loss: 5.0893\n",
      "Epoch [7/10], Step [14325/68337], Loss: 5.1441\n",
      "Epoch [7/10], Step [14400/68337], Loss: 5.2319\n",
      "Epoch [7/10], Step [14475/68337], Loss: 4.9513\n",
      "Epoch [7/10], Step [14550/68337], Loss: 4.8035\n",
      "Epoch [7/10], Step [14625/68337], Loss: 5.0819\n",
      "Epoch [7/10], Step [14700/68337], Loss: 5.0882\n",
      "Epoch [7/10], Step [14775/68337], Loss: 5.2046\n",
      "Epoch [7/10], Step [14850/68337], Loss: 5.0339\n",
      "Epoch [7/10], Step [14925/68337], Loss: 5.0607\n",
      "Epoch [7/10], Step [15000/68337], Loss: 5.1601\n",
      "Epoch [7/10], Step [15075/68337], Loss: 4.9690\n",
      "Epoch [7/10], Step [15150/68337], Loss: 5.0872\n",
      "Epoch [7/10], Step [15225/68337], Loss: 5.3308\n",
      "Epoch [7/10], Step [15300/68337], Loss: 5.2117\n",
      "Epoch [7/10], Step [15375/68337], Loss: 5.1810\n",
      "Epoch [7/10], Step [15450/68337], Loss: 4.8659\n",
      "Epoch [7/10], Step [15525/68337], Loss: 5.0373\n",
      "Epoch [7/10], Step [15600/68337], Loss: 5.2637\n",
      "Epoch [7/10], Step [15675/68337], Loss: 5.1220\n",
      "Epoch [7/10], Step [15750/68337], Loss: 5.1415\n",
      "Epoch [7/10], Step [15825/68337], Loss: 5.1946\n",
      "Epoch [7/10], Step [15900/68337], Loss: 5.1569\n",
      "Epoch [7/10], Step [15975/68337], Loss: 5.2618\n",
      "Epoch [7/10], Step [16050/68337], Loss: 4.9264\n",
      "Epoch [7/10], Step [16125/68337], Loss: 5.2061\n",
      "Epoch [7/10], Step [16200/68337], Loss: 5.0488\n",
      "Epoch [7/10], Step [16275/68337], Loss: 5.1095\n",
      "Epoch [7/10], Step [16350/68337], Loss: 5.2258\n",
      "Epoch [7/10], Step [16425/68337], Loss: 5.0375\n",
      "Epoch [7/10], Step [16500/68337], Loss: 4.9976\n",
      "Epoch [7/10], Step [16575/68337], Loss: 5.1988\n",
      "Epoch [7/10], Step [16650/68337], Loss: 5.3236\n",
      "Epoch [7/10], Step [16725/68337], Loss: 5.1593\n",
      "Epoch [7/10], Step [16800/68337], Loss: 5.0588\n",
      "Epoch [7/10], Step [16875/68337], Loss: 5.0040\n",
      "Epoch [7/10], Step [16950/68337], Loss: 5.1688\n",
      "Epoch [7/10], Step [17025/68337], Loss: 5.1014\n",
      "Epoch [7/10], Step [17100/68337], Loss: 5.0165\n",
      "Epoch [7/10], Step [17175/68337], Loss: 5.1456\n",
      "Epoch [7/10], Step [17250/68337], Loss: 5.1088\n",
      "Epoch [7/10], Step [17325/68337], Loss: 5.1077\n",
      "Epoch [7/10], Step [17400/68337], Loss: 5.3103\n",
      "Epoch [7/10], Step [17475/68337], Loss: 5.0968\n",
      "Epoch [7/10], Step [17550/68337], Loss: 5.0851\n",
      "Epoch [7/10], Step [17625/68337], Loss: 5.0896\n",
      "Epoch [7/10], Step [17700/68337], Loss: 5.1966\n",
      "Epoch [7/10], Step [17775/68337], Loss: 5.2730\n",
      "Epoch [7/10], Step [17850/68337], Loss: 5.1178\n",
      "Epoch [7/10], Step [17925/68337], Loss: 5.3330\n",
      "Epoch [7/10], Step [18000/68337], Loss: 4.9447\n",
      "Epoch [7/10], Step [18075/68337], Loss: 5.2182\n",
      "Epoch [7/10], Step [18150/68337], Loss: 5.0689\n",
      "Epoch [7/10], Step [18225/68337], Loss: 5.0135\n",
      "Epoch [7/10], Step [18300/68337], Loss: 5.1892\n",
      "Epoch [7/10], Step [18375/68337], Loss: 5.1031\n",
      "Epoch [7/10], Step [18450/68337], Loss: 4.9045\n",
      "Epoch [7/10], Step [18525/68337], Loss: 5.0206\n",
      "Epoch [7/10], Step [18600/68337], Loss: 5.2709\n",
      "Epoch [7/10], Step [18675/68337], Loss: 5.1878\n",
      "Epoch [7/10], Step [18750/68337], Loss: 5.2749\n",
      "Epoch [7/10], Step [18825/68337], Loss: 5.1733\n",
      "Epoch [7/10], Step [18900/68337], Loss: 4.9484\n",
      "Epoch [7/10], Step [18975/68337], Loss: 5.0534\n",
      "Epoch [7/10], Step [19050/68337], Loss: 5.2429\n",
      "Epoch [7/10], Step [19125/68337], Loss: 5.2148\n",
      "Epoch [7/10], Step [19200/68337], Loss: 4.9722\n",
      "Epoch [7/10], Step [19275/68337], Loss: 5.1747\n",
      "Epoch [7/10], Step [19350/68337], Loss: 4.9908\n",
      "Epoch [7/10], Step [19425/68337], Loss: 5.1382\n",
      "Epoch [7/10], Step [19500/68337], Loss: 5.0091\n",
      "Epoch [7/10], Step [19575/68337], Loss: 5.1639\n",
      "Epoch [7/10], Step [19650/68337], Loss: 5.1888\n",
      "Epoch [7/10], Step [19725/68337], Loss: 5.0143\n",
      "Epoch [7/10], Step [19800/68337], Loss: 5.0895\n",
      "Epoch [7/10], Step [19875/68337], Loss: 5.2815\n",
      "Epoch [7/10], Step [19950/68337], Loss: 4.9422\n",
      "Validation perplexity: 123.74556743629665\n",
      "Epoch [7/10], Step [20025/68337], Loss: 5.0241\n",
      "Epoch [7/10], Step [20100/68337], Loss: 4.9259\n",
      "Epoch [7/10], Step [20175/68337], Loss: 5.1620\n",
      "Epoch [7/10], Step [20250/68337], Loss: 5.0729\n",
      "Epoch [7/10], Step [20325/68337], Loss: 5.0243\n",
      "Epoch [7/10], Step [20400/68337], Loss: 5.0141\n",
      "Epoch [7/10], Step [20475/68337], Loss: 5.0129\n",
      "Epoch [7/10], Step [20550/68337], Loss: 5.0474\n",
      "Epoch [7/10], Step [20625/68337], Loss: 5.1417\n",
      "Epoch [7/10], Step [20700/68337], Loss: 5.0897\n",
      "Epoch [7/10], Step [20775/68337], Loss: 5.0382\n",
      "Epoch [7/10], Step [20850/68337], Loss: 5.1329\n",
      "Epoch [7/10], Step [20925/68337], Loss: 5.1177\n",
      "Epoch [7/10], Step [21000/68337], Loss: 5.1595\n",
      "Epoch [7/10], Step [21075/68337], Loss: 5.2770\n",
      "Epoch [7/10], Step [21150/68337], Loss: 5.0260\n",
      "Epoch [7/10], Step [21225/68337], Loss: 5.1015\n",
      "Epoch [7/10], Step [21300/68337], Loss: 4.9057\n",
      "Epoch [7/10], Step [21375/68337], Loss: 5.0990\n",
      "Epoch [7/10], Step [21450/68337], Loss: 5.3384\n",
      "Epoch [7/10], Step [21525/68337], Loss: 5.1574\n",
      "Epoch [7/10], Step [21600/68337], Loss: 5.0715\n",
      "Epoch [7/10], Step [21675/68337], Loss: 5.1817\n",
      "Epoch [7/10], Step [21750/68337], Loss: 5.0742\n",
      "Epoch [7/10], Step [21825/68337], Loss: 5.1623\n",
      "Epoch [7/10], Step [21900/68337], Loss: 5.1291\n",
      "Epoch [7/10], Step [21975/68337], Loss: 4.9652\n",
      "Epoch [7/10], Step [22050/68337], Loss: 5.2816\n",
      "Epoch [7/10], Step [22125/68337], Loss: 5.1866\n",
      "Epoch [7/10], Step [22200/68337], Loss: 4.9322\n",
      "Epoch [7/10], Step [22275/68337], Loss: 4.8496\n",
      "Epoch [7/10], Step [22350/68337], Loss: 4.9946\n",
      "Epoch [7/10], Step [22425/68337], Loss: 5.2748\n",
      "Epoch [7/10], Step [22500/68337], Loss: 4.9990\n",
      "Epoch [7/10], Step [22575/68337], Loss: 5.0203\n",
      "Epoch [7/10], Step [22650/68337], Loss: 5.2342\n",
      "Epoch [7/10], Step [22725/68337], Loss: 5.1290\n",
      "Epoch [7/10], Step [22800/68337], Loss: 5.1534\n",
      "Epoch [7/10], Step [22875/68337], Loss: 5.1921\n",
      "Epoch [7/10], Step [22950/68337], Loss: 4.9923\n",
      "Epoch [7/10], Step [23025/68337], Loss: 5.1402\n",
      "Epoch [7/10], Step [23100/68337], Loss: 5.0141\n",
      "Epoch [7/10], Step [23175/68337], Loss: 5.1350\n",
      "Epoch [7/10], Step [23250/68337], Loss: 4.8383\n",
      "Epoch [7/10], Step [23325/68337], Loss: 5.0258\n",
      "Epoch [7/10], Step [23400/68337], Loss: 5.1347\n",
      "Epoch [7/10], Step [23475/68337], Loss: 5.1564\n",
      "Epoch [7/10], Step [23550/68337], Loss: 5.1591\n",
      "Epoch [7/10], Step [23625/68337], Loss: 4.9740\n",
      "Epoch [7/10], Step [23700/68337], Loss: 5.0662\n",
      "Epoch [7/10], Step [23775/68337], Loss: 5.0098\n",
      "Epoch [7/10], Step [23850/68337], Loss: 5.2098\n",
      "Epoch [7/10], Step [23925/68337], Loss: 5.2719\n",
      "Epoch [7/10], Step [24000/68337], Loss: 5.1274\n",
      "Epoch [7/10], Step [24075/68337], Loss: 5.0448\n",
      "Epoch [7/10], Step [24150/68337], Loss: 5.0779\n",
      "Epoch [7/10], Step [24225/68337], Loss: 5.2646\n",
      "Epoch [7/10], Step [24300/68337], Loss: 4.9657\n",
      "Epoch [7/10], Step [24375/68337], Loss: 5.1823\n",
      "Epoch [7/10], Step [24450/68337], Loss: 5.0250\n",
      "Epoch [7/10], Step [24525/68337], Loss: 4.9586\n",
      "Epoch [7/10], Step [24600/68337], Loss: 5.1166\n",
      "Epoch [7/10], Step [24675/68337], Loss: 5.1971\n",
      "Epoch [7/10], Step [24750/68337], Loss: 5.1003\n",
      "Epoch [7/10], Step [24825/68337], Loss: 5.0353\n",
      "Epoch [7/10], Step [24900/68337], Loss: 4.9494\n",
      "Epoch [7/10], Step [24975/68337], Loss: 4.9777\n",
      "Epoch [7/10], Step [25050/68337], Loss: 5.1516\n",
      "Epoch [7/10], Step [25125/68337], Loss: 5.0173\n",
      "Epoch [7/10], Step [25200/68337], Loss: 5.1748\n",
      "Epoch [7/10], Step [25275/68337], Loss: 5.1338\n",
      "Epoch [7/10], Step [25350/68337], Loss: 5.0290\n",
      "Epoch [7/10], Step [25425/68337], Loss: 4.9385\n",
      "Epoch [7/10], Step [25500/68337], Loss: 5.1537\n",
      "Epoch [7/10], Step [25575/68337], Loss: 5.1063\n",
      "Epoch [7/10], Step [25650/68337], Loss: 5.0708\n",
      "Epoch [7/10], Step [25725/68337], Loss: 5.3349\n",
      "Epoch [7/10], Step [25800/68337], Loss: 5.2649\n",
      "Epoch [7/10], Step [25875/68337], Loss: 5.3623\n",
      "Epoch [7/10], Step [25950/68337], Loss: 5.0114\n",
      "Epoch [7/10], Step [26025/68337], Loss: 5.0368\n",
      "Epoch [7/10], Step [26100/68337], Loss: 5.1023\n",
      "Epoch [7/10], Step [26175/68337], Loss: 5.0269\n",
      "Epoch [7/10], Step [26250/68337], Loss: 5.1691\n",
      "Epoch [7/10], Step [26325/68337], Loss: 5.0759\n",
      "Epoch [7/10], Step [26400/68337], Loss: 4.8053\n",
      "Epoch [7/10], Step [26475/68337], Loss: 5.2575\n",
      "Epoch [7/10], Step [26550/68337], Loss: 5.0384\n",
      "Epoch [7/10], Step [26625/68337], Loss: 4.9935\n",
      "Epoch [7/10], Step [26700/68337], Loss: 5.1644\n",
      "Epoch [7/10], Step [26775/68337], Loss: 5.1162\n",
      "Epoch [7/10], Step [26850/68337], Loss: 5.1392\n",
      "Epoch [7/10], Step [26925/68337], Loss: 5.1231\n",
      "Epoch [7/10], Step [27000/68337], Loss: 5.1499\n",
      "Epoch [7/10], Step [27075/68337], Loss: 5.1632\n",
      "Epoch [7/10], Step [27150/68337], Loss: 5.0425\n",
      "Epoch [7/10], Step [27225/68337], Loss: 4.8485\n",
      "Epoch [7/10], Step [27300/68337], Loss: 5.1679\n",
      "Epoch [7/10], Step [27375/68337], Loss: 5.2981\n",
      "Epoch [7/10], Step [27450/68337], Loss: 4.9907\n",
      "Epoch [7/10], Step [27525/68337], Loss: 5.2893\n",
      "Epoch [7/10], Step [27600/68337], Loss: 5.0473\n",
      "Epoch [7/10], Step [27675/68337], Loss: 5.0576\n",
      "Epoch [7/10], Step [27750/68337], Loss: 5.1955\n",
      "Epoch [7/10], Step [27825/68337], Loss: 5.2930\n",
      "Epoch [7/10], Step [27900/68337], Loss: 5.3072\n",
      "Epoch [7/10], Step [27975/68337], Loss: 5.1932\n",
      "Epoch [7/10], Step [28050/68337], Loss: 5.1768\n",
      "Epoch [7/10], Step [28125/68337], Loss: 5.1489\n",
      "Epoch [7/10], Step [28200/68337], Loss: 5.0135\n",
      "Epoch [7/10], Step [28275/68337], Loss: 4.9726\n",
      "Epoch [7/10], Step [28350/68337], Loss: 5.2330\n",
      "Epoch [7/10], Step [28425/68337], Loss: 5.2207\n",
      "Epoch [7/10], Step [28500/68337], Loss: 5.1277\n",
      "Epoch [7/10], Step [28575/68337], Loss: 5.0871\n",
      "Epoch [7/10], Step [28650/68337], Loss: 5.3364\n",
      "Epoch [7/10], Step [28725/68337], Loss: 5.0518\n",
      "Epoch [7/10], Step [28800/68337], Loss: 4.9648\n",
      "Epoch [7/10], Step [28875/68337], Loss: 5.0113\n",
      "Epoch [7/10], Step [28950/68337], Loss: 4.9651\n",
      "Epoch [7/10], Step [29025/68337], Loss: 5.1702\n",
      "Epoch [7/10], Step [29100/68337], Loss: 5.0772\n",
      "Epoch [7/10], Step [29175/68337], Loss: 5.0362\n",
      "Epoch [7/10], Step [29250/68337], Loss: 5.2250\n",
      "Epoch [7/10], Step [29325/68337], Loss: 5.0170\n",
      "Epoch [7/10], Step [29400/68337], Loss: 5.0276\n",
      "Epoch [7/10], Step [29475/68337], Loss: 5.0756\n",
      "Epoch [7/10], Step [29550/68337], Loss: 5.1360\n",
      "Epoch [7/10], Step [29625/68337], Loss: 5.3208\n",
      "Epoch [7/10], Step [29700/68337], Loss: 5.1358\n",
      "Epoch [7/10], Step [29775/68337], Loss: 5.1466\n",
      "Epoch [7/10], Step [29850/68337], Loss: 5.1337\n",
      "Epoch [7/10], Step [29925/68337], Loss: 5.3285\n",
      "Epoch [7/10], Step [30000/68337], Loss: 5.0455\n",
      "Validation perplexity: 123.64654959498124\n",
      "Epoch [7/10], Step [30075/68337], Loss: 4.9629\n",
      "Epoch [7/10], Step [30150/68337], Loss: 5.0697\n",
      "Epoch [7/10], Step [30225/68337], Loss: 5.0711\n",
      "Epoch [7/10], Step [30300/68337], Loss: 5.1503\n",
      "Epoch [7/10], Step [30375/68337], Loss: 5.1130\n",
      "Epoch [7/10], Step [30450/68337], Loss: 5.1236\n",
      "Epoch [7/10], Step [30525/68337], Loss: 5.0044\n",
      "Epoch [7/10], Step [30600/68337], Loss: 4.9850\n",
      "Epoch [7/10], Step [30675/68337], Loss: 5.2134\n",
      "Epoch [7/10], Step [30750/68337], Loss: 4.9250\n",
      "Epoch [7/10], Step [30825/68337], Loss: 4.8980\n",
      "Epoch [7/10], Step [30900/68337], Loss: 5.0858\n",
      "Epoch [7/10], Step [30975/68337], Loss: 5.0966\n",
      "Epoch [7/10], Step [31050/68337], Loss: 5.0927\n",
      "Epoch [7/10], Step [31125/68337], Loss: 4.9890\n",
      "Epoch [7/10], Step [31200/68337], Loss: 5.0241\n",
      "Epoch [7/10], Step [31275/68337], Loss: 5.0879\n",
      "Epoch [7/10], Step [31350/68337], Loss: 5.0394\n",
      "Epoch [7/10], Step [31425/68337], Loss: 5.0618\n",
      "Epoch [7/10], Step [31500/68337], Loss: 5.2460\n",
      "Epoch [7/10], Step [31575/68337], Loss: 5.1382\n",
      "Epoch [7/10], Step [31650/68337], Loss: 4.9233\n",
      "Epoch [7/10], Step [31725/68337], Loss: 5.1093\n",
      "Epoch [7/10], Step [31800/68337], Loss: 5.1005\n",
      "Epoch [7/10], Step [31875/68337], Loss: 5.2283\n",
      "Epoch [7/10], Step [31950/68337], Loss: 5.1741\n",
      "Epoch [7/10], Step [32025/68337], Loss: 5.2006\n",
      "Epoch [7/10], Step [32100/68337], Loss: 5.0331\n",
      "Epoch [7/10], Step [32175/68337], Loss: 5.0602\n",
      "Epoch [7/10], Step [32250/68337], Loss: 5.0889\n",
      "Epoch [7/10], Step [32325/68337], Loss: 5.1515\n",
      "Epoch [7/10], Step [32400/68337], Loss: 5.1681\n",
      "Epoch [7/10], Step [32475/68337], Loss: 5.0970\n",
      "Epoch [7/10], Step [32550/68337], Loss: 5.0729\n",
      "Epoch [7/10], Step [32625/68337], Loss: 5.0592\n",
      "Epoch [7/10], Step [32700/68337], Loss: 5.0685\n",
      "Epoch [7/10], Step [32775/68337], Loss: 5.0451\n",
      "Epoch [7/10], Step [32850/68337], Loss: 4.9627\n",
      "Epoch [7/10], Step [32925/68337], Loss: 4.9520\n",
      "Epoch [7/10], Step [33000/68337], Loss: 4.9353\n",
      "Epoch [7/10], Step [33075/68337], Loss: 5.1431\n",
      "Epoch [7/10], Step [33150/68337], Loss: 5.2311\n",
      "Epoch [7/10], Step [33225/68337], Loss: 5.1086\n",
      "Epoch [7/10], Step [33300/68337], Loss: 4.9574\n",
      "Epoch [7/10], Step [33375/68337], Loss: 5.1436\n",
      "Epoch [7/10], Step [33450/68337], Loss: 4.9317\n",
      "Epoch [7/10], Step [33525/68337], Loss: 5.0685\n",
      "Epoch [7/10], Step [33600/68337], Loss: 5.0760\n",
      "Epoch [7/10], Step [33675/68337], Loss: 4.9960\n",
      "Epoch [7/10], Step [33750/68337], Loss: 5.0519\n",
      "Epoch [7/10], Step [33825/68337], Loss: 4.9301\n",
      "Epoch [7/10], Step [33900/68337], Loss: 5.1934\n",
      "Epoch [7/10], Step [33975/68337], Loss: 5.1427\n",
      "Epoch [7/10], Step [34050/68337], Loss: 5.1087\n",
      "Epoch [7/10], Step [34125/68337], Loss: 5.2004\n",
      "Epoch [7/10], Step [34200/68337], Loss: 5.1189\n",
      "Epoch [7/10], Step [34275/68337], Loss: 5.1130\n",
      "Epoch [7/10], Step [34350/68337], Loss: 5.0361\n",
      "Epoch [7/10], Step [34425/68337], Loss: 5.0028\n",
      "Epoch [7/10], Step [34500/68337], Loss: 5.0666\n",
      "Epoch [7/10], Step [34575/68337], Loss: 5.1838\n",
      "Epoch [7/10], Step [34650/68337], Loss: 5.0172\n",
      "Epoch [7/10], Step [34725/68337], Loss: 4.8734\n",
      "Epoch [7/10], Step [34800/68337], Loss: 5.1130\n",
      "Epoch [7/10], Step [34875/68337], Loss: 5.1079\n",
      "Epoch [7/10], Step [34950/68337], Loss: 5.1530\n",
      "Epoch [7/10], Step [35025/68337], Loss: 5.1832\n",
      "Epoch [7/10], Step [35100/68337], Loss: 4.9278\n",
      "Epoch [7/10], Step [35175/68337], Loss: 5.2239\n",
      "Epoch [7/10], Step [35250/68337], Loss: 5.2058\n",
      "Epoch [7/10], Step [35325/68337], Loss: 5.0887\n",
      "Epoch [7/10], Step [35400/68337], Loss: 5.2507\n",
      "Epoch [7/10], Step [35475/68337], Loss: 5.0487\n",
      "Epoch [7/10], Step [35550/68337], Loss: 5.0630\n",
      "Epoch [7/10], Step [35625/68337], Loss: 5.1364\n",
      "Epoch [7/10], Step [35700/68337], Loss: 5.2006\n",
      "Epoch [7/10], Step [35775/68337], Loss: 5.1262\n",
      "Epoch [7/10], Step [35850/68337], Loss: 4.8155\n",
      "Epoch [7/10], Step [35925/68337], Loss: 5.1490\n",
      "Epoch [7/10], Step [36000/68337], Loss: 5.1127\n",
      "Epoch [7/10], Step [36075/68337], Loss: 5.0046\n",
      "Epoch [7/10], Step [36150/68337], Loss: 5.1672\n",
      "Epoch [7/10], Step [36225/68337], Loss: 5.0921\n",
      "Epoch [7/10], Step [36300/68337], Loss: 4.9521\n",
      "Epoch [7/10], Step [36375/68337], Loss: 4.9795\n",
      "Epoch [7/10], Step [36450/68337], Loss: 5.1356\n",
      "Epoch [7/10], Step [36525/68337], Loss: 4.9937\n",
      "Epoch [7/10], Step [36600/68337], Loss: 5.1700\n",
      "Epoch [7/10], Step [36675/68337], Loss: 5.1909\n",
      "Epoch [7/10], Step [36750/68337], Loss: 5.0689\n",
      "Epoch [7/10], Step [36825/68337], Loss: 5.2042\n",
      "Epoch [7/10], Step [36900/68337], Loss: 5.2387\n",
      "Epoch [7/10], Step [36975/68337], Loss: 5.2159\n",
      "Epoch [7/10], Step [37050/68337], Loss: 5.2784\n",
      "Epoch [7/10], Step [37125/68337], Loss: 5.0579\n",
      "Epoch [7/10], Step [37200/68337], Loss: 5.1103\n",
      "Epoch [7/10], Step [37275/68337], Loss: 5.4021\n",
      "Epoch [7/10], Step [37350/68337], Loss: 4.9293\n",
      "Epoch [7/10], Step [37425/68337], Loss: 5.0395\n",
      "Epoch [7/10], Step [37500/68337], Loss: 4.8449\n",
      "Epoch [7/10], Step [37575/68337], Loss: 4.9314\n",
      "Epoch [7/10], Step [37650/68337], Loss: 5.1056\n",
      "Epoch [7/10], Step [37725/68337], Loss: 5.1707\n",
      "Epoch [7/10], Step [37800/68337], Loss: 5.0291\n",
      "Epoch [7/10], Step [37875/68337], Loss: 5.2203\n",
      "Epoch [7/10], Step [37950/68337], Loss: 4.8825\n",
      "Epoch [7/10], Step [38025/68337], Loss: 5.0933\n",
      "Epoch [7/10], Step [38100/68337], Loss: 5.0257\n",
      "Epoch [7/10], Step [38175/68337], Loss: 5.1501\n",
      "Epoch [7/10], Step [38250/68337], Loss: 5.2358\n",
      "Epoch [7/10], Step [38325/68337], Loss: 4.9487\n",
      "Epoch [7/10], Step [38400/68337], Loss: 5.1529\n",
      "Epoch [7/10], Step [38475/68337], Loss: 5.1675\n",
      "Epoch [7/10], Step [38550/68337], Loss: 5.0614\n",
      "Epoch [7/10], Step [38625/68337], Loss: 5.1412\n",
      "Epoch [7/10], Step [38700/68337], Loss: 4.8997\n",
      "Epoch [7/10], Step [38775/68337], Loss: 4.9471\n",
      "Epoch [7/10], Step [38850/68337], Loss: 5.2092\n",
      "Epoch [7/10], Step [38925/68337], Loss: 5.2470\n",
      "Epoch [7/10], Step [39000/68337], Loss: 4.9870\n",
      "Epoch [7/10], Step [39075/68337], Loss: 5.0185\n",
      "Epoch [7/10], Step [39150/68337], Loss: 5.2541\n",
      "Epoch [7/10], Step [39225/68337], Loss: 5.2881\n",
      "Epoch [7/10], Step [39300/68337], Loss: 4.8918\n",
      "Epoch [7/10], Step [39375/68337], Loss: 5.2099\n",
      "Epoch [7/10], Step [39450/68337], Loss: 5.2562\n",
      "Epoch [7/10], Step [39525/68337], Loss: 5.1236\n",
      "Epoch [7/10], Step [39600/68337], Loss: 5.0856\n",
      "Epoch [7/10], Step [39675/68337], Loss: 4.9586\n",
      "Epoch [7/10], Step [39750/68337], Loss: 5.1506\n",
      "Epoch [7/10], Step [39825/68337], Loss: 5.0519\n",
      "Epoch [7/10], Step [39900/68337], Loss: 5.0766\n",
      "Epoch [7/10], Step [39975/68337], Loss: 5.0873\n",
      "Validation perplexity: 123.43639888035419\n",
      "Epoch [7/10], Step [40050/68337], Loss: 5.2206\n",
      "Epoch [7/10], Step [40125/68337], Loss: 5.0843\n",
      "Epoch [7/10], Step [40200/68337], Loss: 5.0316\n",
      "Epoch [7/10], Step [40275/68337], Loss: 5.1459\n",
      "Epoch [7/10], Step [40350/68337], Loss: 4.9826\n",
      "Epoch [7/10], Step [40425/68337], Loss: 5.0870\n",
      "Epoch [7/10], Step [40500/68337], Loss: 5.0396\n",
      "Epoch [7/10], Step [40575/68337], Loss: 4.8988\n",
      "Epoch [7/10], Step [40650/68337], Loss: 5.3488\n",
      "Epoch [7/10], Step [40725/68337], Loss: 4.9589\n",
      "Epoch [7/10], Step [40800/68337], Loss: 5.0066\n",
      "Epoch [7/10], Step [40875/68337], Loss: 5.2505\n",
      "Epoch [7/10], Step [40950/68337], Loss: 4.9720\n",
      "Epoch [7/10], Step [41025/68337], Loss: 4.9301\n",
      "Epoch [7/10], Step [41100/68337], Loss: 4.9939\n",
      "Epoch [7/10], Step [41175/68337], Loss: 5.1168\n",
      "Epoch [7/10], Step [41250/68337], Loss: 5.2033\n",
      "Epoch [7/10], Step [41325/68337], Loss: 5.1414\n",
      "Epoch [7/10], Step [41400/68337], Loss: 5.1491\n",
      "Epoch [7/10], Step [41475/68337], Loss: 5.0362\n",
      "Epoch [7/10], Step [41550/68337], Loss: 5.0959\n",
      "Epoch [7/10], Step [41625/68337], Loss: 4.9316\n",
      "Epoch [7/10], Step [41700/68337], Loss: 4.8952\n",
      "Epoch [7/10], Step [41775/68337], Loss: 5.0832\n",
      "Epoch [7/10], Step [41850/68337], Loss: 5.1698\n",
      "Epoch [7/10], Step [41925/68337], Loss: 5.1998\n",
      "Epoch [7/10], Step [42000/68337], Loss: 5.0810\n",
      "Epoch [7/10], Step [42075/68337], Loss: 5.0785\n",
      "Epoch [7/10], Step [42150/68337], Loss: 5.0969\n",
      "Epoch [7/10], Step [42225/68337], Loss: 5.2171\n",
      "Epoch [7/10], Step [42300/68337], Loss: 5.1019\n",
      "Epoch [7/10], Step [42375/68337], Loss: 5.0865\n",
      "Epoch [7/10], Step [42450/68337], Loss: 5.0242\n",
      "Epoch [7/10], Step [42525/68337], Loss: 5.1261\n",
      "Epoch [7/10], Step [42600/68337], Loss: 5.1454\n",
      "Epoch [7/10], Step [42675/68337], Loss: 5.0800\n",
      "Epoch [7/10], Step [42750/68337], Loss: 5.0049\n",
      "Epoch [7/10], Step [42825/68337], Loss: 4.9618\n",
      "Epoch [7/10], Step [42900/68337], Loss: 5.1402\n",
      "Epoch [7/10], Step [42975/68337], Loss: 5.0012\n",
      "Epoch [7/10], Step [43050/68337], Loss: 5.0271\n",
      "Epoch [7/10], Step [43125/68337], Loss: 4.8783\n",
      "Epoch [7/10], Step [43200/68337], Loss: 5.1254\n",
      "Epoch [7/10], Step [43275/68337], Loss: 5.0987\n",
      "Epoch [7/10], Step [43350/68337], Loss: 5.0899\n",
      "Epoch [7/10], Step [43425/68337], Loss: 5.2458\n",
      "Epoch [7/10], Step [43500/68337], Loss: 5.1982\n",
      "Epoch [7/10], Step [43575/68337], Loss: 5.0255\n",
      "Epoch [7/10], Step [43650/68337], Loss: 5.1025\n",
      "Epoch [7/10], Step [43725/68337], Loss: 5.2895\n",
      "Epoch [7/10], Step [43800/68337], Loss: 4.9971\n",
      "Epoch [7/10], Step [43875/68337], Loss: 5.1037\n",
      "Epoch [7/10], Step [43950/68337], Loss: 4.9963\n",
      "Epoch [7/10], Step [44025/68337], Loss: 5.0246\n",
      "Epoch [7/10], Step [44100/68337], Loss: 4.9530\n",
      "Epoch [7/10], Step [44175/68337], Loss: 5.2008\n",
      "Epoch [7/10], Step [44250/68337], Loss: 5.1975\n",
      "Epoch [7/10], Step [44325/68337], Loss: 4.9671\n",
      "Epoch [7/10], Step [44400/68337], Loss: 5.1812\n",
      "Epoch [7/10], Step [44475/68337], Loss: 5.1278\n",
      "Epoch [7/10], Step [44550/68337], Loss: 5.1882\n",
      "Epoch [7/10], Step [44625/68337], Loss: 5.2216\n",
      "Epoch [7/10], Step [44700/68337], Loss: 5.0891\n",
      "Epoch [7/10], Step [44775/68337], Loss: 5.2327\n",
      "Epoch [7/10], Step [44850/68337], Loss: 5.0020\n",
      "Epoch [7/10], Step [44925/68337], Loss: 5.1115\n",
      "Epoch [7/10], Step [45000/68337], Loss: 4.9632\n",
      "Epoch [7/10], Step [45075/68337], Loss: 5.1938\n",
      "Epoch [7/10], Step [45150/68337], Loss: 5.0328\n",
      "Epoch [7/10], Step [45225/68337], Loss: 4.9995\n",
      "Epoch [7/10], Step [45300/68337], Loss: 5.2127\n",
      "Epoch [7/10], Step [45375/68337], Loss: 4.9843\n",
      "Epoch [7/10], Step [45450/68337], Loss: 5.0524\n",
      "Epoch [7/10], Step [45525/68337], Loss: 5.0639\n",
      "Epoch [7/10], Step [45600/68337], Loss: 5.1518\n",
      "Epoch [7/10], Step [45675/68337], Loss: 5.2613\n",
      "Epoch [7/10], Step [45750/68337], Loss: 5.1134\n",
      "Epoch [7/10], Step [45825/68337], Loss: 5.2807\n",
      "Epoch [7/10], Step [45900/68337], Loss: 5.2347\n",
      "Epoch [7/10], Step [45975/68337], Loss: 5.1147\n",
      "Epoch [7/10], Step [46050/68337], Loss: 5.2271\n",
      "Epoch [7/10], Step [46125/68337], Loss: 5.0591\n",
      "Epoch [7/10], Step [46200/68337], Loss: 5.1559\n",
      "Epoch [7/10], Step [46275/68337], Loss: 5.2375\n",
      "Epoch [7/10], Step [46350/68337], Loss: 5.1769\n",
      "Epoch [7/10], Step [46425/68337], Loss: 4.8603\n",
      "Epoch [7/10], Step [46500/68337], Loss: 5.0525\n",
      "Epoch [7/10], Step [46575/68337], Loss: 5.1630\n",
      "Epoch [7/10], Step [46650/68337], Loss: 5.0463\n",
      "Epoch [7/10], Step [46725/68337], Loss: 5.1931\n",
      "Epoch [7/10], Step [46800/68337], Loss: 5.1772\n",
      "Epoch [7/10], Step [46875/68337], Loss: 5.1036\n",
      "Epoch [7/10], Step [46950/68337], Loss: 5.0810\n",
      "Epoch [7/10], Step [47025/68337], Loss: 5.2373\n",
      "Epoch [7/10], Step [47100/68337], Loss: 4.8156\n",
      "Epoch [7/10], Step [47175/68337], Loss: 5.0421\n",
      "Epoch [7/10], Step [47250/68337], Loss: 5.2002\n",
      "Epoch [7/10], Step [47325/68337], Loss: 5.1592\n",
      "Epoch [7/10], Step [47400/68337], Loss: 4.9481\n",
      "Epoch [7/10], Step [47475/68337], Loss: 5.0144\n",
      "Epoch [7/10], Step [47550/68337], Loss: 5.1626\n",
      "Epoch [7/10], Step [47625/68337], Loss: 5.0055\n",
      "Epoch [7/10], Step [47700/68337], Loss: 5.1854\n",
      "Epoch [7/10], Step [47775/68337], Loss: 5.2345\n",
      "Epoch [7/10], Step [47850/68337], Loss: 5.0075\n",
      "Epoch [7/10], Step [47925/68337], Loss: 5.0373\n",
      "Epoch [7/10], Step [48000/68337], Loss: 5.0836\n",
      "Epoch [7/10], Step [48075/68337], Loss: 5.1888\n",
      "Epoch [7/10], Step [48150/68337], Loss: 5.2020\n",
      "Epoch [7/10], Step [48225/68337], Loss: 5.0672\n",
      "Epoch [7/10], Step [48300/68337], Loss: 5.1326\n",
      "Epoch [7/10], Step [48375/68337], Loss: 5.2795\n",
      "Epoch [7/10], Step [48450/68337], Loss: 5.0793\n",
      "Epoch [7/10], Step [48525/68337], Loss: 4.9084\n",
      "Epoch [7/10], Step [48600/68337], Loss: 5.0742\n",
      "Epoch [7/10], Step [48675/68337], Loss: 5.0700\n",
      "Epoch [7/10], Step [48750/68337], Loss: 5.2013\n",
      "Epoch [7/10], Step [48825/68337], Loss: 5.0749\n",
      "Epoch [7/10], Step [48900/68337], Loss: 5.1600\n",
      "Epoch [7/10], Step [48975/68337], Loss: 5.1204\n",
      "Epoch [7/10], Step [49050/68337], Loss: 5.0365\n",
      "Epoch [7/10], Step [49125/68337], Loss: 5.0084\n",
      "Epoch [7/10], Step [49200/68337], Loss: 5.1139\n",
      "Epoch [7/10], Step [49275/68337], Loss: 5.0185\n",
      "Epoch [7/10], Step [49350/68337], Loss: 5.1048\n",
      "Epoch [7/10], Step [49425/68337], Loss: 5.0164\n",
      "Epoch [7/10], Step [49500/68337], Loss: 4.9255\n",
      "Epoch [7/10], Step [49575/68337], Loss: 5.0388\n",
      "Epoch [7/10], Step [49650/68337], Loss: 5.0886\n",
      "Epoch [7/10], Step [49725/68337], Loss: 5.0185\n",
      "Epoch [7/10], Step [49800/68337], Loss: 5.0123\n",
      "Epoch [7/10], Step [49875/68337], Loss: 5.2727\n",
      "Epoch [7/10], Step [49950/68337], Loss: 5.2391\n",
      "Validation perplexity: 123.060419859133\n",
      "Epoch [7/10], Step [50025/68337], Loss: 5.0944\n",
      "Epoch [7/10], Step [50100/68337], Loss: 5.2441\n",
      "Epoch [7/10], Step [50175/68337], Loss: 5.1394\n",
      "Epoch [7/10], Step [50250/68337], Loss: 5.0007\n",
      "Epoch [7/10], Step [50325/68337], Loss: 4.9203\n",
      "Epoch [7/10], Step [50400/68337], Loss: 4.9641\n",
      "Epoch [7/10], Step [50475/68337], Loss: 5.0142\n",
      "Epoch [7/10], Step [50550/68337], Loss: 5.0762\n",
      "Epoch [7/10], Step [50625/68337], Loss: 5.1636\n",
      "Epoch [7/10], Step [50700/68337], Loss: 5.0317\n",
      "Epoch [7/10], Step [50775/68337], Loss: 5.1567\n",
      "Epoch [7/10], Step [50850/68337], Loss: 4.8912\n",
      "Epoch [7/10], Step [50925/68337], Loss: 5.0620\n",
      "Epoch [7/10], Step [51000/68337], Loss: 5.0843\n",
      "Epoch [7/10], Step [51075/68337], Loss: 5.0878\n",
      "Epoch [7/10], Step [51150/68337], Loss: 5.0233\n",
      "Epoch [7/10], Step [51225/68337], Loss: 5.2222\n",
      "Epoch [7/10], Step [51300/68337], Loss: 5.0729\n",
      "Epoch [7/10], Step [51375/68337], Loss: 5.1561\n",
      "Epoch [7/10], Step [51450/68337], Loss: 5.0238\n",
      "Epoch [7/10], Step [51525/68337], Loss: 4.8693\n",
      "Epoch [7/10], Step [51600/68337], Loss: 5.1372\n",
      "Epoch [7/10], Step [51675/68337], Loss: 5.0216\n",
      "Epoch [7/10], Step [51750/68337], Loss: 4.9767\n",
      "Epoch [7/10], Step [51825/68337], Loss: 5.0852\n",
      "Epoch [7/10], Step [51900/68337], Loss: 5.1064\n",
      "Epoch [7/10], Step [51975/68337], Loss: 5.2664\n",
      "Epoch [7/10], Step [52050/68337], Loss: 5.0730\n",
      "Epoch [7/10], Step [52125/68337], Loss: 5.0302\n",
      "Epoch [7/10], Step [52200/68337], Loss: 4.9103\n",
      "Epoch [7/10], Step [52275/68337], Loss: 5.0118\n",
      "Epoch [7/10], Step [52350/68337], Loss: 4.9344\n",
      "Epoch [7/10], Step [52425/68337], Loss: 5.0030\n",
      "Epoch [7/10], Step [52500/68337], Loss: 5.0000\n",
      "Epoch [7/10], Step [52575/68337], Loss: 5.0047\n",
      "Epoch [7/10], Step [52650/68337], Loss: 5.2692\n",
      "Epoch [7/10], Step [52725/68337], Loss: 4.8433\n",
      "Epoch [7/10], Step [52800/68337], Loss: 5.0608\n",
      "Epoch [7/10], Step [52875/68337], Loss: 5.2133\n",
      "Epoch [7/10], Step [52950/68337], Loss: 5.1872\n",
      "Epoch [7/10], Step [53025/68337], Loss: 5.0737\n",
      "Epoch [7/10], Step [53100/68337], Loss: 5.1281\n",
      "Epoch [7/10], Step [53175/68337], Loss: 5.1054\n",
      "Epoch [7/10], Step [53250/68337], Loss: 5.2117\n",
      "Epoch [7/10], Step [53325/68337], Loss: 4.9416\n",
      "Epoch [7/10], Step [53400/68337], Loss: 5.0359\n",
      "Epoch [7/10], Step [53475/68337], Loss: 5.0259\n",
      "Epoch [7/10], Step [53550/68337], Loss: 5.1019\n",
      "Epoch [7/10], Step [53625/68337], Loss: 5.2244\n",
      "Epoch [7/10], Step [53700/68337], Loss: 5.0089\n",
      "Epoch [7/10], Step [53775/68337], Loss: 4.9524\n",
      "Epoch [7/10], Step [53850/68337], Loss: 5.0988\n",
      "Epoch [7/10], Step [53925/68337], Loss: 5.2776\n",
      "Epoch [7/10], Step [54000/68337], Loss: 5.0659\n",
      "Epoch [7/10], Step [54075/68337], Loss: 4.9207\n",
      "Epoch [7/10], Step [54150/68337], Loss: 5.1480\n",
      "Epoch [7/10], Step [54225/68337], Loss: 5.2293\n",
      "Epoch [7/10], Step [54300/68337], Loss: 5.1477\n",
      "Epoch [7/10], Step [54375/68337], Loss: 5.1242\n",
      "Epoch [7/10], Step [54450/68337], Loss: 5.1345\n",
      "Epoch [7/10], Step [54525/68337], Loss: 4.9967\n",
      "Epoch [7/10], Step [54600/68337], Loss: 4.9821\n",
      "Epoch [7/10], Step [54675/68337], Loss: 4.9060\n",
      "Epoch [7/10], Step [54750/68337], Loss: 5.2204\n",
      "Epoch [7/10], Step [54825/68337], Loss: 4.9644\n",
      "Epoch [7/10], Step [54900/68337], Loss: 5.0997\n",
      "Epoch [7/10], Step [54975/68337], Loss: 5.1724\n",
      "Epoch [7/10], Step [55050/68337], Loss: 5.1385\n",
      "Epoch [7/10], Step [55125/68337], Loss: 5.0995\n",
      "Epoch [7/10], Step [55200/68337], Loss: 5.1957\n",
      "Epoch [7/10], Step [55275/68337], Loss: 4.9854\n",
      "Epoch [7/10], Step [55350/68337], Loss: 5.2332\n",
      "Epoch [7/10], Step [55425/68337], Loss: 5.1102\n",
      "Epoch [7/10], Step [55500/68337], Loss: 5.2871\n",
      "Epoch [7/10], Step [55575/68337], Loss: 5.0806\n",
      "Epoch [7/10], Step [55650/68337], Loss: 4.9976\n",
      "Epoch [7/10], Step [55725/68337], Loss: 5.1756\n",
      "Epoch [7/10], Step [55800/68337], Loss: 5.0631\n",
      "Epoch [7/10], Step [55875/68337], Loss: 4.9953\n",
      "Epoch [7/10], Step [55950/68337], Loss: 5.1987\n",
      "Epoch [7/10], Step [56025/68337], Loss: 5.1026\n",
      "Epoch [7/10], Step [56100/68337], Loss: 5.0042\n",
      "Epoch [7/10], Step [56175/68337], Loss: 5.1957\n",
      "Epoch [7/10], Step [56250/68337], Loss: 5.0853\n",
      "Epoch [7/10], Step [56325/68337], Loss: 5.1993\n",
      "Epoch [7/10], Step [56400/68337], Loss: 5.1282\n",
      "Epoch [7/10], Step [56475/68337], Loss: 5.1521\n",
      "Epoch [7/10], Step [56550/68337], Loss: 5.0979\n",
      "Epoch [7/10], Step [56625/68337], Loss: 4.9611\n",
      "Epoch [7/10], Step [56700/68337], Loss: 5.1776\n",
      "Epoch [7/10], Step [56775/68337], Loss: 5.0367\n",
      "Epoch [7/10], Step [56850/68337], Loss: 5.2069\n",
      "Epoch [7/10], Step [56925/68337], Loss: 5.1759\n",
      "Epoch [7/10], Step [57000/68337], Loss: 5.1145\n",
      "Epoch [7/10], Step [57075/68337], Loss: 5.1062\n",
      "Epoch [7/10], Step [57150/68337], Loss: 5.2986\n",
      "Epoch [7/10], Step [57225/68337], Loss: 5.1284\n",
      "Epoch [7/10], Step [57300/68337], Loss: 5.2713\n",
      "Epoch [7/10], Step [57375/68337], Loss: 5.0196\n",
      "Epoch [7/10], Step [57450/68337], Loss: 4.9589\n",
      "Epoch [7/10], Step [57525/68337], Loss: 4.9124\n",
      "Epoch [7/10], Step [57600/68337], Loss: 5.0843\n",
      "Epoch [7/10], Step [57675/68337], Loss: 5.1578\n",
      "Epoch [7/10], Step [57750/68337], Loss: 4.8953\n",
      "Epoch [7/10], Step [57825/68337], Loss: 5.2595\n",
      "Epoch [7/10], Step [57900/68337], Loss: 5.1948\n",
      "Epoch [7/10], Step [57975/68337], Loss: 5.0912\n",
      "Epoch [7/10], Step [58050/68337], Loss: 5.1567\n",
      "Epoch [7/10], Step [58125/68337], Loss: 5.0133\n",
      "Epoch [7/10], Step [58200/68337], Loss: 5.1166\n",
      "Epoch [7/10], Step [58275/68337], Loss: 5.0422\n",
      "Epoch [7/10], Step [58350/68337], Loss: 4.9564\n",
      "Epoch [7/10], Step [58425/68337], Loss: 4.9628\n",
      "Epoch [7/10], Step [58500/68337], Loss: 5.1800\n",
      "Epoch [7/10], Step [58575/68337], Loss: 5.1591\n",
      "Epoch [7/10], Step [58650/68337], Loss: 4.9249\n",
      "Epoch [7/10], Step [58725/68337], Loss: 4.9595\n",
      "Epoch [7/10], Step [58800/68337], Loss: 5.2037\n",
      "Epoch [7/10], Step [58875/68337], Loss: 5.0651\n",
      "Epoch [7/10], Step [58950/68337], Loss: 5.0243\n",
      "Epoch [7/10], Step [59025/68337], Loss: 5.1827\n",
      "Epoch [7/10], Step [59100/68337], Loss: 5.1268\n",
      "Epoch [7/10], Step [59175/68337], Loss: 4.9616\n",
      "Epoch [7/10], Step [59250/68337], Loss: 5.1552\n",
      "Epoch [7/10], Step [59325/68337], Loss: 5.0496\n",
      "Epoch [7/10], Step [59400/68337], Loss: 5.1368\n",
      "Epoch [7/10], Step [59475/68337], Loss: 4.8545\n",
      "Epoch [7/10], Step [59550/68337], Loss: 5.2967\n",
      "Epoch [7/10], Step [59625/68337], Loss: 4.9264\n",
      "Epoch [7/10], Step [59700/68337], Loss: 5.1306\n",
      "Epoch [7/10], Step [59775/68337], Loss: 5.2518\n",
      "Epoch [7/10], Step [59850/68337], Loss: 5.1268\n",
      "Epoch [7/10], Step [59925/68337], Loss: 5.0800\n",
      "Epoch [7/10], Step [60000/68337], Loss: 4.9629\n",
      "Validation perplexity: 122.61543882015566\n",
      "Epoch [7/10], Step [60075/68337], Loss: 5.0795\n",
      "Epoch [7/10], Step [60150/68337], Loss: 5.0992\n",
      "Epoch [7/10], Step [60225/68337], Loss: 4.9498\n",
      "Epoch [7/10], Step [60300/68337], Loss: 5.0900\n",
      "Epoch [7/10], Step [60375/68337], Loss: 5.1629\n",
      "Epoch [7/10], Step [60450/68337], Loss: 4.8466\n",
      "Epoch [7/10], Step [60525/68337], Loss: 5.2054\n",
      "Epoch [7/10], Step [60600/68337], Loss: 5.0068\n",
      "Epoch [7/10], Step [60675/68337], Loss: 5.2163\n",
      "Epoch [7/10], Step [60750/68337], Loss: 4.9833\n",
      "Epoch [7/10], Step [60825/68337], Loss: 5.0894\n",
      "Epoch [7/10], Step [60900/68337], Loss: 5.2947\n",
      "Epoch [7/10], Step [60975/68337], Loss: 5.1955\n",
      "Epoch [7/10], Step [61050/68337], Loss: 5.1160\n",
      "Epoch [7/10], Step [61125/68337], Loss: 5.1683\n",
      "Epoch [7/10], Step [61200/68337], Loss: 5.1793\n",
      "Epoch [7/10], Step [61275/68337], Loss: 4.9736\n",
      "Epoch [7/10], Step [61350/68337], Loss: 5.0155\n",
      "Epoch [7/10], Step [61425/68337], Loss: 5.1127\n",
      "Epoch [7/10], Step [61500/68337], Loss: 5.0787\n",
      "Epoch [7/10], Step [61575/68337], Loss: 5.0860\n",
      "Epoch [7/10], Step [61650/68337], Loss: 5.3563\n",
      "Epoch [7/10], Step [61725/68337], Loss: 5.0068\n",
      "Epoch [7/10], Step [61800/68337], Loss: 4.9564\n",
      "Epoch [7/10], Step [61875/68337], Loss: 5.0498\n",
      "Epoch [7/10], Step [61950/68337], Loss: 5.0094\n",
      "Epoch [7/10], Step [62025/68337], Loss: 4.9686\n",
      "Epoch [7/10], Step [62100/68337], Loss: 5.3195\n",
      "Epoch [7/10], Step [62175/68337], Loss: 5.1591\n",
      "Epoch [7/10], Step [62250/68337], Loss: 5.1267\n",
      "Epoch [7/10], Step [62325/68337], Loss: 4.9612\n",
      "Epoch [7/10], Step [62400/68337], Loss: 5.0928\n",
      "Epoch [7/10], Step [62475/68337], Loss: 5.1642\n",
      "Epoch [7/10], Step [62550/68337], Loss: 4.9641\n",
      "Epoch [7/10], Step [62625/68337], Loss: 4.9132\n",
      "Epoch [7/10], Step [62700/68337], Loss: 4.8427\n",
      "Epoch [7/10], Step [62775/68337], Loss: 4.9706\n",
      "Epoch [7/10], Step [62850/68337], Loss: 5.0501\n",
      "Epoch [7/10], Step [62925/68337], Loss: 4.8799\n",
      "Epoch [7/10], Step [63000/68337], Loss: 5.1007\n",
      "Epoch [7/10], Step [63075/68337], Loss: 5.3474\n",
      "Epoch [7/10], Step [63150/68337], Loss: 5.1071\n",
      "Epoch [7/10], Step [63225/68337], Loss: 5.0580\n",
      "Epoch [7/10], Step [63300/68337], Loss: 4.9591\n",
      "Epoch [7/10], Step [63375/68337], Loss: 5.2649\n",
      "Epoch [7/10], Step [63450/68337], Loss: 4.9307\n",
      "Epoch [7/10], Step [63525/68337], Loss: 5.1564\n",
      "Epoch [7/10], Step [63600/68337], Loss: 5.1408\n",
      "Epoch [7/10], Step [63675/68337], Loss: 5.4214\n",
      "Epoch [7/10], Step [63750/68337], Loss: 5.2114\n",
      "Epoch [7/10], Step [63825/68337], Loss: 5.2349\n",
      "Epoch [7/10], Step [63900/68337], Loss: 5.0320\n",
      "Epoch [7/10], Step [63975/68337], Loss: 4.9158\n",
      "Epoch [7/10], Step [64050/68337], Loss: 5.0966\n",
      "Epoch [7/10], Step [64125/68337], Loss: 5.2232\n",
      "Epoch [7/10], Step [64200/68337], Loss: 5.1249\n",
      "Epoch [7/10], Step [64275/68337], Loss: 5.0482\n",
      "Epoch [7/10], Step [64350/68337], Loss: 5.0532\n",
      "Epoch [7/10], Step [64425/68337], Loss: 5.0362\n",
      "Epoch [7/10], Step [64500/68337], Loss: 5.1840\n",
      "Epoch [7/10], Step [64575/68337], Loss: 4.9293\n",
      "Epoch [7/10], Step [64650/68337], Loss: 5.1615\n",
      "Epoch [7/10], Step [64725/68337], Loss: 5.2472\n",
      "Epoch [7/10], Step [64800/68337], Loss: 5.1922\n",
      "Epoch [7/10], Step [64875/68337], Loss: 5.0563\n",
      "Epoch [7/10], Step [64950/68337], Loss: 5.0364\n",
      "Epoch [7/10], Step [65025/68337], Loss: 5.1743\n",
      "Epoch [7/10], Step [65100/68337], Loss: 4.9899\n",
      "Epoch [7/10], Step [65175/68337], Loss: 5.0490\n",
      "Epoch [7/10], Step [65250/68337], Loss: 5.0845\n",
      "Epoch [7/10], Step [65325/68337], Loss: 5.1327\n",
      "Epoch [7/10], Step [65400/68337], Loss: 5.0436\n",
      "Epoch [7/10], Step [65475/68337], Loss: 5.0557\n",
      "Epoch [7/10], Step [65550/68337], Loss: 5.0319\n",
      "Epoch [7/10], Step [65625/68337], Loss: 5.0438\n",
      "Epoch [7/10], Step [65700/68337], Loss: 5.0393\n",
      "Epoch [7/10], Step [65775/68337], Loss: 4.9723\n",
      "Epoch [7/10], Step [65850/68337], Loss: 4.9870\n",
      "Epoch [7/10], Step [65925/68337], Loss: 5.0448\n",
      "Epoch [7/10], Step [66000/68337], Loss: 5.0722\n",
      "Epoch [7/10], Step [66075/68337], Loss: 4.9382\n",
      "Epoch [7/10], Step [66150/68337], Loss: 5.2176\n",
      "Epoch [7/10], Step [66225/68337], Loss: 4.9099\n",
      "Epoch [7/10], Step [66300/68337], Loss: 4.9775\n",
      "Epoch [7/10], Step [66375/68337], Loss: 5.0807\n",
      "Epoch [7/10], Step [66450/68337], Loss: 5.1049\n",
      "Epoch [7/10], Step [66525/68337], Loss: 5.0790\n",
      "Epoch [7/10], Step [66600/68337], Loss: 5.0571\n",
      "Epoch [7/10], Step [66675/68337], Loss: 4.8209\n",
      "Epoch [7/10], Step [66750/68337], Loss: 5.0168\n",
      "Epoch [7/10], Step [66825/68337], Loss: 4.8399\n",
      "Epoch [7/10], Step [66900/68337], Loss: 5.0854\n",
      "Epoch [7/10], Step [66975/68337], Loss: 4.9464\n",
      "Epoch [7/10], Step [67050/68337], Loss: 4.9684\n",
      "Epoch [7/10], Step [67125/68337], Loss: 4.9646\n",
      "Epoch [7/10], Step [67200/68337], Loss: 5.0569\n",
      "Epoch [7/10], Step [67275/68337], Loss: 5.0592\n",
      "Epoch [7/10], Step [67350/68337], Loss: 5.1630\n",
      "Epoch [7/10], Step [67425/68337], Loss: 5.1812\n",
      "Epoch [7/10], Step [67500/68337], Loss: 5.1016\n",
      "Epoch [7/10], Step [67575/68337], Loss: 5.2190\n",
      "Epoch [7/10], Step [67650/68337], Loss: 5.0802\n",
      "Epoch [7/10], Step [67725/68337], Loss: 4.9418\n",
      "Epoch [7/10], Step [67800/68337], Loss: 4.8842\n",
      "Epoch [7/10], Step [67875/68337], Loss: 5.0724\n",
      "Epoch [7/10], Step [67950/68337], Loss: 5.1384\n",
      "Epoch [7/10], Step [68025/68337], Loss: 5.0945\n",
      "Epoch [7/10], Step [68100/68337], Loss: 4.8336\n",
      "Epoch [7/10], Step [68175/68337], Loss: 5.2116\n",
      "Epoch [7/10], Step [68250/68337], Loss: 5.1136\n",
      "Epoch [7/10], Step [68325/68337], Loss: 5.1070\n",
      "Epoch [7/10] Average Loss: 5.0915, Perplexity: 162.64\n",
      "Epoch [8/10], Step [0/68337], Loss: 5.2078\n",
      "Validation perplexity: 122.76191579248191\n",
      "Epoch [8/10], Step [75/68337], Loss: 5.1425\n",
      "Epoch [8/10], Step [150/68337], Loss: 4.9444\n",
      "Epoch [8/10], Step [225/68337], Loss: 4.9996\n",
      "Epoch [8/10], Step [300/68337], Loss: 5.1645\n",
      "Epoch [8/10], Step [375/68337], Loss: 5.1036\n",
      "Epoch [8/10], Step [450/68337], Loss: 4.8904\n",
      "Epoch [8/10], Step [525/68337], Loss: 5.0042\n",
      "Epoch [8/10], Step [600/68337], Loss: 5.1224\n",
      "Epoch [8/10], Step [675/68337], Loss: 4.9769\n",
      "Epoch [8/10], Step [750/68337], Loss: 5.1709\n",
      "Epoch [8/10], Step [825/68337], Loss: 5.0243\n",
      "Epoch [8/10], Step [900/68337], Loss: 4.9291\n",
      "Epoch [8/10], Step [975/68337], Loss: 5.2671\n",
      "Epoch [8/10], Step [1050/68337], Loss: 5.0790\n",
      "Epoch [8/10], Step [1125/68337], Loss: 4.9463\n",
      "Epoch [8/10], Step [1200/68337], Loss: 4.9229\n",
      "Epoch [8/10], Step [1275/68337], Loss: 4.9101\n",
      "Epoch [8/10], Step [1350/68337], Loss: 5.3173\n",
      "Epoch [8/10], Step [1425/68337], Loss: 4.9155\n",
      "Epoch [8/10], Step [1500/68337], Loss: 4.8064\n",
      "Epoch [8/10], Step [1575/68337], Loss: 5.1420\n",
      "Epoch [8/10], Step [1650/68337], Loss: 5.0939\n",
      "Epoch [8/10], Step [1725/68337], Loss: 5.0311\n",
      "Epoch [8/10], Step [1800/68337], Loss: 4.7501\n",
      "Epoch [8/10], Step [1875/68337], Loss: 5.1573\n",
      "Epoch [8/10], Step [1950/68337], Loss: 5.1682\n",
      "Epoch [8/10], Step [2025/68337], Loss: 5.0798\n",
      "Epoch [8/10], Step [2100/68337], Loss: 5.0004\n",
      "Epoch [8/10], Step [2175/68337], Loss: 4.9651\n",
      "Epoch [8/10], Step [2250/68337], Loss: 5.2394\n",
      "Epoch [8/10], Step [2325/68337], Loss: 5.1176\n",
      "Epoch [8/10], Step [2400/68337], Loss: 5.3768\n",
      "Epoch [8/10], Step [2475/68337], Loss: 5.2284\n",
      "Epoch [8/10], Step [2550/68337], Loss: 5.1433\n",
      "Epoch [8/10], Step [2625/68337], Loss: 5.0222\n",
      "Epoch [8/10], Step [2700/68337], Loss: 5.0750\n",
      "Epoch [8/10], Step [2775/68337], Loss: 5.1609\n",
      "Epoch [8/10], Step [2850/68337], Loss: 5.0603\n",
      "Epoch [8/10], Step [2925/68337], Loss: 4.8657\n",
      "Epoch [8/10], Step [3000/68337], Loss: 5.1709\n",
      "Epoch [8/10], Step [3075/68337], Loss: 5.0299\n",
      "Epoch [8/10], Step [3150/68337], Loss: 4.9531\n",
      "Epoch [8/10], Step [3225/68337], Loss: 5.2128\n",
      "Epoch [8/10], Step [3300/68337], Loss: 5.0017\n",
      "Epoch [8/10], Step [3375/68337], Loss: 5.1203\n",
      "Epoch [8/10], Step [3450/68337], Loss: 5.1754\n",
      "Epoch [8/10], Step [3525/68337], Loss: 4.9229\n",
      "Epoch [8/10], Step [3600/68337], Loss: 5.1764\n",
      "Epoch [8/10], Step [3675/68337], Loss: 5.0698\n",
      "Epoch [8/10], Step [3750/68337], Loss: 5.2040\n",
      "Epoch [8/10], Step [3825/68337], Loss: 5.0775\n",
      "Epoch [8/10], Step [3900/68337], Loss: 5.1243\n",
      "Epoch [8/10], Step [3975/68337], Loss: 4.9810\n",
      "Epoch [8/10], Step [4050/68337], Loss: 4.9726\n",
      "Epoch [8/10], Step [4125/68337], Loss: 5.0595\n",
      "Epoch [8/10], Step [4200/68337], Loss: 5.1660\n",
      "Epoch [8/10], Step [4275/68337], Loss: 4.9762\n",
      "Epoch [8/10], Step [4350/68337], Loss: 5.2050\n",
      "Epoch [8/10], Step [4425/68337], Loss: 5.0703\n",
      "Epoch [8/10], Step [4500/68337], Loss: 5.2508\n",
      "Epoch [8/10], Step [4575/68337], Loss: 5.2303\n",
      "Epoch [8/10], Step [4650/68337], Loss: 4.7385\n",
      "Epoch [8/10], Step [4725/68337], Loss: 5.1327\n",
      "Epoch [8/10], Step [4800/68337], Loss: 5.1106\n",
      "Epoch [8/10], Step [4875/68337], Loss: 5.0211\n",
      "Epoch [8/10], Step [4950/68337], Loss: 5.0611\n",
      "Epoch [8/10], Step [5025/68337], Loss: 5.2254\n",
      "Epoch [8/10], Step [5100/68337], Loss: 5.4917\n",
      "Epoch [8/10], Step [5175/68337], Loss: 5.1473\n",
      "Epoch [8/10], Step [5250/68337], Loss: 5.1716\n",
      "Epoch [8/10], Step [5325/68337], Loss: 5.1173\n",
      "Epoch [8/10], Step [5400/68337], Loss: 5.1718\n",
      "Epoch [8/10], Step [5475/68337], Loss: 5.1506\n",
      "Epoch [8/10], Step [5550/68337], Loss: 4.9174\n",
      "Epoch [8/10], Step [5625/68337], Loss: 5.1196\n",
      "Epoch [8/10], Step [5700/68337], Loss: 5.1796\n",
      "Epoch [8/10], Step [5775/68337], Loss: 4.9727\n",
      "Epoch [8/10], Step [5850/68337], Loss: 5.0765\n",
      "Epoch [8/10], Step [5925/68337], Loss: 5.2044\n",
      "Epoch [8/10], Step [6000/68337], Loss: 5.2226\n",
      "Epoch [8/10], Step [6075/68337], Loss: 5.0805\n",
      "Epoch [8/10], Step [6150/68337], Loss: 5.0550\n",
      "Epoch [8/10], Step [6225/68337], Loss: 5.1122\n",
      "Epoch [8/10], Step [6300/68337], Loss: 5.1902\n",
      "Epoch [8/10], Step [6375/68337], Loss: 4.9214\n",
      "Epoch [8/10], Step [6450/68337], Loss: 5.1329\n",
      "Epoch [8/10], Step [6525/68337], Loss: 5.1728\n",
      "Epoch [8/10], Step [6600/68337], Loss: 5.0517\n",
      "Epoch [8/10], Step [6675/68337], Loss: 5.1145\n",
      "Epoch [8/10], Step [6750/68337], Loss: 5.1237\n",
      "Epoch [8/10], Step [6825/68337], Loss: 5.2000\n",
      "Epoch [8/10], Step [6900/68337], Loss: 5.0579\n",
      "Epoch [8/10], Step [6975/68337], Loss: 5.2586\n",
      "Epoch [8/10], Step [7050/68337], Loss: 5.2146\n",
      "Epoch [8/10], Step [7125/68337], Loss: 5.1259\n",
      "Epoch [8/10], Step [7200/68337], Loss: 4.9600\n",
      "Epoch [8/10], Step [7275/68337], Loss: 4.9521\n",
      "Epoch [8/10], Step [7350/68337], Loss: 5.1494\n",
      "Epoch [8/10], Step [7425/68337], Loss: 4.9449\n",
      "Epoch [8/10], Step [7500/68337], Loss: 5.1730\n",
      "Epoch [8/10], Step [7575/68337], Loss: 5.1065\n",
      "Epoch [8/10], Step [7650/68337], Loss: 4.7766\n",
      "Epoch [8/10], Step [7725/68337], Loss: 5.1306\n",
      "Epoch [8/10], Step [7800/68337], Loss: 5.2343\n",
      "Epoch [8/10], Step [7875/68337], Loss: 5.1142\n",
      "Epoch [8/10], Step [7950/68337], Loss: 5.0498\n",
      "Epoch [8/10], Step [8025/68337], Loss: 5.2024\n",
      "Epoch [8/10], Step [8100/68337], Loss: 5.2018\n",
      "Epoch [8/10], Step [8175/68337], Loss: 5.1207\n",
      "Epoch [8/10], Step [8250/68337], Loss: 5.1117\n",
      "Epoch [8/10], Step [8325/68337], Loss: 4.9378\n",
      "Epoch [8/10], Step [8400/68337], Loss: 4.9077\n",
      "Epoch [8/10], Step [8475/68337], Loss: 5.0515\n",
      "Epoch [8/10], Step [8550/68337], Loss: 4.8280\n",
      "Epoch [8/10], Step [8625/68337], Loss: 5.0539\n",
      "Epoch [8/10], Step [8700/68337], Loss: 4.8866\n",
      "Epoch [8/10], Step [8775/68337], Loss: 5.0795\n",
      "Epoch [8/10], Step [8850/68337], Loss: 4.9747\n",
      "Epoch [8/10], Step [8925/68337], Loss: 5.0110\n",
      "Epoch [8/10], Step [9000/68337], Loss: 5.0925\n",
      "Epoch [8/10], Step [9075/68337], Loss: 5.0718\n",
      "Epoch [8/10], Step [9150/68337], Loss: 5.1164\n",
      "Epoch [8/10], Step [9225/68337], Loss: 4.9549\n",
      "Epoch [8/10], Step [9300/68337], Loss: 5.2692\n",
      "Epoch [8/10], Step [9375/68337], Loss: 4.9996\n",
      "Epoch [8/10], Step [9450/68337], Loss: 4.9491\n",
      "Epoch [8/10], Step [9525/68337], Loss: 5.0465\n",
      "Epoch [8/10], Step [9600/68337], Loss: 5.1089\n",
      "Epoch [8/10], Step [9675/68337], Loss: 5.1640\n",
      "Epoch [8/10], Step [9750/68337], Loss: 5.2681\n",
      "Epoch [8/10], Step [9825/68337], Loss: 4.9625\n",
      "Epoch [8/10], Step [9900/68337], Loss: 5.2070\n",
      "Epoch [8/10], Step [9975/68337], Loss: 5.1220\n",
      "Validation perplexity: 122.33044048047687\n",
      "Epoch [8/10], Step [10050/68337], Loss: 5.1180\n",
      "Epoch [8/10], Step [10125/68337], Loss: 5.1304\n",
      "Epoch [8/10], Step [10200/68337], Loss: 4.9377\n",
      "Epoch [8/10], Step [10275/68337], Loss: 4.9478\n",
      "Epoch [8/10], Step [10350/68337], Loss: 4.9767\n",
      "Epoch [8/10], Step [10425/68337], Loss: 5.1113\n",
      "Epoch [8/10], Step [10500/68337], Loss: 5.0184\n",
      "Epoch [8/10], Step [10575/68337], Loss: 5.1246\n",
      "Epoch [8/10], Step [10650/68337], Loss: 5.2499\n",
      "Epoch [8/10], Step [10725/68337], Loss: 5.0821\n",
      "Epoch [8/10], Step [10800/68337], Loss: 5.0849\n",
      "Epoch [8/10], Step [10875/68337], Loss: 5.0286\n",
      "Epoch [8/10], Step [10950/68337], Loss: 5.0183\n",
      "Epoch [8/10], Step [11025/68337], Loss: 5.0438\n",
      "Epoch [8/10], Step [11100/68337], Loss: 4.8634\n",
      "Epoch [8/10], Step [11175/68337], Loss: 5.0812\n",
      "Epoch [8/10], Step [11250/68337], Loss: 4.8969\n",
      "Epoch [8/10], Step [11325/68337], Loss: 5.1791\n",
      "Epoch [8/10], Step [11400/68337], Loss: 4.9292\n",
      "Epoch [8/10], Step [11475/68337], Loss: 5.0492\n",
      "Epoch [8/10], Step [11550/68337], Loss: 5.0494\n",
      "Epoch [8/10], Step [11625/68337], Loss: 5.1024\n",
      "Epoch [8/10], Step [11700/68337], Loss: 5.1564\n",
      "Epoch [8/10], Step [11775/68337], Loss: 5.1112\n",
      "Epoch [8/10], Step [11850/68337], Loss: 5.2163\n",
      "Epoch [8/10], Step [11925/68337], Loss: 5.0532\n",
      "Epoch [8/10], Step [12000/68337], Loss: 5.0143\n",
      "Epoch [8/10], Step [12075/68337], Loss: 5.1225\n",
      "Epoch [8/10], Step [12150/68337], Loss: 4.9756\n",
      "Epoch [8/10], Step [12225/68337], Loss: 5.0421\n",
      "Epoch [8/10], Step [12300/68337], Loss: 5.0449\n",
      "Epoch [8/10], Step [12375/68337], Loss: 5.2402\n",
      "Epoch [8/10], Step [12450/68337], Loss: 5.0706\n",
      "Epoch [8/10], Step [12525/68337], Loss: 4.9997\n",
      "Epoch [8/10], Step [12600/68337], Loss: 4.9602\n",
      "Epoch [8/10], Step [12675/68337], Loss: 5.1853\n",
      "Epoch [8/10], Step [12750/68337], Loss: 5.0448\n",
      "Epoch [8/10], Step [12825/68337], Loss: 5.0656\n",
      "Epoch [8/10], Step [12900/68337], Loss: 5.0783\n",
      "Epoch [8/10], Step [12975/68337], Loss: 5.1855\n",
      "Epoch [8/10], Step [13050/68337], Loss: 4.9496\n",
      "Epoch [8/10], Step [13125/68337], Loss: 5.0711\n",
      "Epoch [8/10], Step [13200/68337], Loss: 5.1070\n",
      "Epoch [8/10], Step [13275/68337], Loss: 5.1319\n",
      "Epoch [8/10], Step [13350/68337], Loss: 5.0897\n",
      "Epoch [8/10], Step [13425/68337], Loss: 4.9620\n",
      "Epoch [8/10], Step [13500/68337], Loss: 5.2950\n",
      "Epoch [8/10], Step [13575/68337], Loss: 5.1944\n",
      "Epoch [8/10], Step [13650/68337], Loss: 5.1462\n",
      "Epoch [8/10], Step [13725/68337], Loss: 5.1331\n",
      "Epoch [8/10], Step [13800/68337], Loss: 5.1704\n",
      "Epoch [8/10], Step [13875/68337], Loss: 5.2286\n",
      "Epoch [8/10], Step [13950/68337], Loss: 5.0604\n",
      "Epoch [8/10], Step [14025/68337], Loss: 5.0757\n",
      "Epoch [8/10], Step [14100/68337], Loss: 5.2422\n",
      "Epoch [8/10], Step [14175/68337], Loss: 5.1427\n",
      "Epoch [8/10], Step [14250/68337], Loss: 5.0210\n",
      "Epoch [8/10], Step [14325/68337], Loss: 4.9212\n",
      "Epoch [8/10], Step [14400/68337], Loss: 4.9966\n",
      "Epoch [8/10], Step [14475/68337], Loss: 5.1603\n",
      "Epoch [8/10], Step [14550/68337], Loss: 5.2883\n",
      "Epoch [8/10], Step [14625/68337], Loss: 4.9969\n",
      "Epoch [8/10], Step [14700/68337], Loss: 5.0534\n",
      "Epoch [8/10], Step [14775/68337], Loss: 5.1289\n",
      "Epoch [8/10], Step [14850/68337], Loss: 5.1448\n",
      "Epoch [8/10], Step [14925/68337], Loss: 4.9899\n",
      "Epoch [8/10], Step [15000/68337], Loss: 5.0403\n",
      "Epoch [8/10], Step [15075/68337], Loss: 5.1135\n",
      "Epoch [8/10], Step [15150/68337], Loss: 5.2062\n",
      "Epoch [8/10], Step [15225/68337], Loss: 4.9210\n",
      "Epoch [8/10], Step [15300/68337], Loss: 4.9779\n",
      "Epoch [8/10], Step [15375/68337], Loss: 5.1532\n",
      "Epoch [8/10], Step [15450/68337], Loss: 5.1116\n",
      "Epoch [8/10], Step [15525/68337], Loss: 4.8602\n",
      "Epoch [8/10], Step [15600/68337], Loss: 5.0196\n",
      "Epoch [8/10], Step [15675/68337], Loss: 5.2841\n",
      "Epoch [8/10], Step [15750/68337], Loss: 5.2292\n",
      "Epoch [8/10], Step [15825/68337], Loss: 5.0704\n",
      "Epoch [8/10], Step [15900/68337], Loss: 5.0631\n",
      "Epoch [8/10], Step [15975/68337], Loss: 4.9840\n",
      "Epoch [8/10], Step [16050/68337], Loss: 5.1806\n",
      "Epoch [8/10], Step [16125/68337], Loss: 5.1509\n",
      "Epoch [8/10], Step [16200/68337], Loss: 5.1053\n",
      "Epoch [8/10], Step [16275/68337], Loss: 4.7792\n",
      "Epoch [8/10], Step [16350/68337], Loss: 5.0658\n",
      "Epoch [8/10], Step [16425/68337], Loss: 5.1977\n",
      "Epoch [8/10], Step [16500/68337], Loss: 5.0140\n",
      "Epoch [8/10], Step [16575/68337], Loss: 5.2957\n",
      "Epoch [8/10], Step [16650/68337], Loss: 5.0998\n",
      "Epoch [8/10], Step [16725/68337], Loss: 4.8712\n",
      "Epoch [8/10], Step [16800/68337], Loss: 5.1402\n",
      "Epoch [8/10], Step [16875/68337], Loss: 5.1511\n",
      "Epoch [8/10], Step [16950/68337], Loss: 5.0253\n",
      "Epoch [8/10], Step [17025/68337], Loss: 5.1748\n",
      "Epoch [8/10], Step [17100/68337], Loss: 4.9706\n",
      "Epoch [8/10], Step [17175/68337], Loss: 5.1275\n",
      "Epoch [8/10], Step [17250/68337], Loss: 5.2347\n",
      "Epoch [8/10], Step [17325/68337], Loss: 5.0751\n",
      "Epoch [8/10], Step [17400/68337], Loss: 5.0435\n",
      "Epoch [8/10], Step [17475/68337], Loss: 5.1132\n",
      "Epoch [8/10], Step [17550/68337], Loss: 5.1215\n",
      "Epoch [8/10], Step [17625/68337], Loss: 5.0115\n",
      "Epoch [8/10], Step [17700/68337], Loss: 5.2076\n",
      "Epoch [8/10], Step [17775/68337], Loss: 5.1838\n",
      "Epoch [8/10], Step [17850/68337], Loss: 5.2133\n",
      "Epoch [8/10], Step [17925/68337], Loss: 5.1948\n",
      "Epoch [8/10], Step [18000/68337], Loss: 5.0408\n",
      "Epoch [8/10], Step [18075/68337], Loss: 5.2032\n",
      "Epoch [8/10], Step [18150/68337], Loss: 5.0651\n",
      "Epoch [8/10], Step [18225/68337], Loss: 5.0516\n",
      "Epoch [8/10], Step [18300/68337], Loss: 5.2072\n",
      "Epoch [8/10], Step [18375/68337], Loss: 5.0155\n",
      "Epoch [8/10], Step [18450/68337], Loss: 5.2045\n",
      "Epoch [8/10], Step [18525/68337], Loss: 5.1309\n",
      "Epoch [8/10], Step [18600/68337], Loss: 4.9170\n",
      "Epoch [8/10], Step [18675/68337], Loss: 5.0626\n",
      "Epoch [8/10], Step [18750/68337], Loss: 4.7539\n",
      "Epoch [8/10], Step [18825/68337], Loss: 4.9518\n",
      "Epoch [8/10], Step [18900/68337], Loss: 4.9422\n",
      "Epoch [8/10], Step [18975/68337], Loss: 5.1490\n",
      "Epoch [8/10], Step [19050/68337], Loss: 5.1481\n",
      "Epoch [8/10], Step [19125/68337], Loss: 5.1563\n",
      "Epoch [8/10], Step [19200/68337], Loss: 5.1128\n",
      "Epoch [8/10], Step [19275/68337], Loss: 5.0204\n",
      "Epoch [8/10], Step [19350/68337], Loss: 5.0368\n",
      "Epoch [8/10], Step [19425/68337], Loss: 5.1231\n",
      "Epoch [8/10], Step [19500/68337], Loss: 5.1598\n",
      "Epoch [8/10], Step [19575/68337], Loss: 4.9201\n",
      "Epoch [8/10], Step [19650/68337], Loss: 4.9636\n",
      "Epoch [8/10], Step [19725/68337], Loss: 5.1812\n",
      "Epoch [8/10], Step [19800/68337], Loss: 5.1653\n",
      "Epoch [8/10], Step [19875/68337], Loss: 5.0733\n",
      "Epoch [8/10], Step [19950/68337], Loss: 5.1729\n",
      "Validation perplexity: 121.97346650219028\n",
      "Epoch [8/10], Step [20025/68337], Loss: 5.1971\n",
      "Epoch [8/10], Step [20100/68337], Loss: 5.2583\n",
      "Epoch [8/10], Step [20175/68337], Loss: 5.1788\n",
      "Epoch [8/10], Step [20250/68337], Loss: 4.8629\n",
      "Epoch [8/10], Step [20325/68337], Loss: 5.1381\n",
      "Epoch [8/10], Step [20400/68337], Loss: 4.9307\n",
      "Epoch [8/10], Step [20475/68337], Loss: 5.2485\n",
      "Epoch [8/10], Step [20550/68337], Loss: 5.0730\n",
      "Epoch [8/10], Step [20625/68337], Loss: 5.1935\n",
      "Epoch [8/10], Step [20700/68337], Loss: 4.9791\n",
      "Epoch [8/10], Step [20775/68337], Loss: 5.0849\n",
      "Epoch [8/10], Step [20850/68337], Loss: 5.1017\n",
      "Epoch [8/10], Step [20925/68337], Loss: 5.0445\n",
      "Epoch [8/10], Step [21000/68337], Loss: 4.9909\n",
      "Epoch [8/10], Step [21075/68337], Loss: 5.1045\n",
      "Epoch [8/10], Step [21150/68337], Loss: 5.1746\n",
      "Epoch [8/10], Step [21225/68337], Loss: 5.2627\n",
      "Epoch [8/10], Step [21300/68337], Loss: 5.0701\n",
      "Epoch [8/10], Step [21375/68337], Loss: 5.1208\n",
      "Epoch [8/10], Step [21450/68337], Loss: 5.1509\n",
      "Epoch [8/10], Step [21525/68337], Loss: 5.1978\n",
      "Epoch [8/10], Step [21600/68337], Loss: 5.0859\n",
      "Epoch [8/10], Step [21675/68337], Loss: 4.8314\n",
      "Epoch [8/10], Step [21750/68337], Loss: 5.0124\n",
      "Epoch [8/10], Step [21825/68337], Loss: 5.2095\n",
      "Epoch [8/10], Step [21900/68337], Loss: 4.9697\n",
      "Epoch [8/10], Step [21975/68337], Loss: 5.2783\n",
      "Epoch [8/10], Step [22050/68337], Loss: 5.0100\n",
      "Epoch [8/10], Step [22125/68337], Loss: 5.2113\n",
      "Epoch [8/10], Step [22200/68337], Loss: 4.9739\n",
      "Epoch [8/10], Step [22275/68337], Loss: 5.1950\n",
      "Epoch [8/10], Step [22350/68337], Loss: 5.0594\n",
      "Epoch [8/10], Step [22425/68337], Loss: 5.2508\n",
      "Epoch [8/10], Step [22500/68337], Loss: 4.8694\n",
      "Epoch [8/10], Step [22575/68337], Loss: 5.1484\n",
      "Epoch [8/10], Step [22650/68337], Loss: 5.2607\n",
      "Epoch [8/10], Step [22725/68337], Loss: 5.0091\n",
      "Epoch [8/10], Step [22800/68337], Loss: 4.8486\n",
      "Epoch [8/10], Step [22875/68337], Loss: 4.9498\n",
      "Epoch [8/10], Step [22950/68337], Loss: 4.9633\n",
      "Epoch [8/10], Step [23025/68337], Loss: 5.2513\n",
      "Epoch [8/10], Step [23100/68337], Loss: 4.9614\n",
      "Epoch [8/10], Step [23175/68337], Loss: 5.1802\n",
      "Epoch [8/10], Step [23250/68337], Loss: 5.1760\n",
      "Epoch [8/10], Step [23325/68337], Loss: 5.0913\n",
      "Epoch [8/10], Step [23400/68337], Loss: 5.0667\n",
      "Epoch [8/10], Step [23475/68337], Loss: 4.9720\n",
      "Epoch [8/10], Step [23550/68337], Loss: 4.9181\n",
      "Epoch [8/10], Step [23625/68337], Loss: 5.1435\n",
      "Epoch [8/10], Step [23700/68337], Loss: 5.0439\n",
      "Epoch [8/10], Step [23775/68337], Loss: 5.0705\n",
      "Epoch [8/10], Step [23850/68337], Loss: 5.0881\n",
      "Epoch [8/10], Step [23925/68337], Loss: 4.7945\n",
      "Epoch [8/10], Step [24000/68337], Loss: 5.2620\n",
      "Epoch [8/10], Step [24075/68337], Loss: 5.2367\n",
      "Epoch [8/10], Step [24150/68337], Loss: 5.1727\n",
      "Epoch [8/10], Step [24225/68337], Loss: 5.0513\n",
      "Epoch [8/10], Step [24300/68337], Loss: 4.9333\n",
      "Epoch [8/10], Step [24375/68337], Loss: 5.0780\n",
      "Epoch [8/10], Step [24450/68337], Loss: 4.9793\n",
      "Epoch [8/10], Step [24525/68337], Loss: 4.9697\n",
      "Epoch [8/10], Step [24600/68337], Loss: 5.0677\n",
      "Epoch [8/10], Step [24675/68337], Loss: 5.0374\n",
      "Epoch [8/10], Step [24750/68337], Loss: 5.0211\n",
      "Epoch [8/10], Step [24825/68337], Loss: 5.0999\n",
      "Epoch [8/10], Step [24900/68337], Loss: 5.1441\n",
      "Epoch [8/10], Step [24975/68337], Loss: 5.1092\n",
      "Epoch [8/10], Step [25050/68337], Loss: 4.9038\n",
      "Epoch [8/10], Step [25125/68337], Loss: 5.4038\n",
      "Epoch [8/10], Step [25200/68337], Loss: 5.0105\n",
      "Epoch [8/10], Step [25275/68337], Loss: 4.9735\n",
      "Epoch [8/10], Step [25350/68337], Loss: 5.0984\n",
      "Epoch [8/10], Step [25425/68337], Loss: 5.2831\n",
      "Epoch [8/10], Step [25500/68337], Loss: 4.9786\n",
      "Epoch [8/10], Step [25575/68337], Loss: 4.9215\n",
      "Epoch [8/10], Step [25650/68337], Loss: 5.1714\n",
      "Epoch [8/10], Step [25725/68337], Loss: 4.9359\n",
      "Epoch [8/10], Step [25800/68337], Loss: 5.1871\n",
      "Epoch [8/10], Step [25875/68337], Loss: 4.9575\n",
      "Epoch [8/10], Step [25950/68337], Loss: 5.0464\n",
      "Epoch [8/10], Step [26025/68337], Loss: 5.1317\n",
      "Epoch [8/10], Step [26100/68337], Loss: 5.0137\n",
      "Epoch [8/10], Step [26175/68337], Loss: 5.0184\n",
      "Epoch [8/10], Step [26250/68337], Loss: 5.1273\n",
      "Epoch [8/10], Step [26325/68337], Loss: 5.1592\n",
      "Epoch [8/10], Step [26400/68337], Loss: 5.2433\n",
      "Epoch [8/10], Step [26475/68337], Loss: 4.9150\n",
      "Epoch [8/10], Step [26550/68337], Loss: 5.1126\n",
      "Epoch [8/10], Step [26625/68337], Loss: 5.0456\n",
      "Epoch [8/10], Step [26700/68337], Loss: 4.8688\n",
      "Epoch [8/10], Step [26775/68337], Loss: 4.9853\n",
      "Epoch [8/10], Step [26850/68337], Loss: 5.0484\n",
      "Epoch [8/10], Step [26925/68337], Loss: 5.1626\n",
      "Epoch [8/10], Step [27000/68337], Loss: 5.1092\n",
      "Epoch [8/10], Step [27075/68337], Loss: 5.0533\n",
      "Epoch [8/10], Step [27150/68337], Loss: 5.2053\n",
      "Epoch [8/10], Step [27225/68337], Loss: 5.1067\n",
      "Epoch [8/10], Step [27300/68337], Loss: 5.2328\n",
      "Epoch [8/10], Step [27375/68337], Loss: 5.1893\n",
      "Epoch [8/10], Step [27450/68337], Loss: 5.0053\n",
      "Epoch [8/10], Step [27525/68337], Loss: 5.0227\n",
      "Epoch [8/10], Step [27600/68337], Loss: 5.2330\n",
      "Epoch [8/10], Step [27675/68337], Loss: 5.0760\n",
      "Epoch [8/10], Step [27750/68337], Loss: 5.1379\n",
      "Epoch [8/10], Step [27825/68337], Loss: 5.1720\n",
      "Epoch [8/10], Step [27900/68337], Loss: 5.1381\n",
      "Epoch [8/10], Step [27975/68337], Loss: 5.1800\n",
      "Epoch [8/10], Step [28050/68337], Loss: 5.1364\n",
      "Epoch [8/10], Step [28125/68337], Loss: 5.0419\n",
      "Epoch [8/10], Step [28200/68337], Loss: 5.2244\n",
      "Epoch [8/10], Step [28275/68337], Loss: 5.2931\n",
      "Epoch [8/10], Step [28350/68337], Loss: 5.3450\n",
      "Epoch [8/10], Step [28425/68337], Loss: 4.9475\n",
      "Epoch [8/10], Step [28500/68337], Loss: 5.1791\n",
      "Epoch [8/10], Step [28575/68337], Loss: 5.1592\n",
      "Epoch [8/10], Step [28650/68337], Loss: 5.1270\n",
      "Epoch [8/10], Step [28725/68337], Loss: 4.9370\n",
      "Epoch [8/10], Step [28800/68337], Loss: 4.9023\n",
      "Epoch [8/10], Step [28875/68337], Loss: 5.2767\n",
      "Epoch [8/10], Step [28950/68337], Loss: 5.1174\n",
      "Epoch [8/10], Step [29025/68337], Loss: 5.0560\n",
      "Epoch [8/10], Step [29100/68337], Loss: 4.9046\n",
      "Epoch [8/10], Step [29175/68337], Loss: 5.0709\n",
      "Epoch [8/10], Step [29250/68337], Loss: 5.1097\n",
      "Epoch [8/10], Step [29325/68337], Loss: 5.1392\n",
      "Epoch [8/10], Step [29400/68337], Loss: 5.2548\n",
      "Epoch [8/10], Step [29475/68337], Loss: 5.1568\n",
      "Epoch [8/10], Step [29550/68337], Loss: 5.2362\n",
      "Epoch [8/10], Step [29625/68337], Loss: 4.8561\n",
      "Epoch [8/10], Step [29700/68337], Loss: 5.2141\n",
      "Epoch [8/10], Step [29775/68337], Loss: 5.2664\n",
      "Epoch [8/10], Step [29850/68337], Loss: 5.2826\n",
      "Epoch [8/10], Step [29925/68337], Loss: 5.2149\n",
      "Epoch [8/10], Step [30000/68337], Loss: 4.9258\n",
      "Validation perplexity: 121.8835961734683\n",
      "Epoch [8/10], Step [30075/68337], Loss: 5.0881\n",
      "Epoch [8/10], Step [30150/68337], Loss: 5.0847\n",
      "Epoch [8/10], Step [30225/68337], Loss: 5.0839\n",
      "Epoch [8/10], Step [30300/68337], Loss: 5.2235\n",
      "Epoch [8/10], Step [30375/68337], Loss: 5.0567\n",
      "Epoch [8/10], Step [30450/68337], Loss: 4.8479\n",
      "Epoch [8/10], Step [30525/68337], Loss: 5.0668\n",
      "Epoch [8/10], Step [30600/68337], Loss: 5.1263\n",
      "Epoch [8/10], Step [30675/68337], Loss: 4.9521\n",
      "Epoch [8/10], Step [30750/68337], Loss: 5.1542\n",
      "Epoch [8/10], Step [30825/68337], Loss: 5.0502\n",
      "Epoch [8/10], Step [30900/68337], Loss: 5.1793\n",
      "Epoch [8/10], Step [30975/68337], Loss: 5.0561\n",
      "Epoch [8/10], Step [31050/68337], Loss: 5.0765\n",
      "Epoch [8/10], Step [31125/68337], Loss: 5.1542\n",
      "Epoch [8/10], Step [31200/68337], Loss: 5.1838\n",
      "Epoch [8/10], Step [31275/68337], Loss: 5.0130\n",
      "Epoch [8/10], Step [31350/68337], Loss: 5.1384\n",
      "Epoch [8/10], Step [31425/68337], Loss: 5.1002\n",
      "Epoch [8/10], Step [31500/68337], Loss: 5.0034\n",
      "Epoch [8/10], Step [31575/68337], Loss: 5.2328\n",
      "Epoch [8/10], Step [31650/68337], Loss: 4.9236\n",
      "Epoch [8/10], Step [31725/68337], Loss: 5.3211\n",
      "Epoch [8/10], Step [31800/68337], Loss: 4.9316\n",
      "Epoch [8/10], Step [31875/68337], Loss: 5.0501\n",
      "Epoch [8/10], Step [31950/68337], Loss: 4.9631\n",
      "Epoch [8/10], Step [32025/68337], Loss: 4.9384\n",
      "Epoch [8/10], Step [32100/68337], Loss: 4.9665\n",
      "Epoch [8/10], Step [32175/68337], Loss: 5.1579\n",
      "Epoch [8/10], Step [32250/68337], Loss: 5.0795\n",
      "Epoch [8/10], Step [32325/68337], Loss: 5.1876\n",
      "Epoch [8/10], Step [32400/68337], Loss: 5.1995\n",
      "Epoch [8/10], Step [32475/68337], Loss: 5.2194\n",
      "Epoch [8/10], Step [32550/68337], Loss: 5.0796\n",
      "Epoch [8/10], Step [32625/68337], Loss: 5.3999\n",
      "Epoch [8/10], Step [32700/68337], Loss: 5.1314\n",
      "Epoch [8/10], Step [32775/68337], Loss: 5.1350\n",
      "Epoch [8/10], Step [32850/68337], Loss: 5.1748\n",
      "Epoch [8/10], Step [32925/68337], Loss: 5.2407\n",
      "Epoch [8/10], Step [33000/68337], Loss: 5.0768\n",
      "Epoch [8/10], Step [33075/68337], Loss: 4.9590\n",
      "Epoch [8/10], Step [33150/68337], Loss: 5.0373\n",
      "Epoch [8/10], Step [33225/68337], Loss: 5.2621\n",
      "Epoch [8/10], Step [33300/68337], Loss: 5.0930\n",
      "Epoch [8/10], Step [33375/68337], Loss: 4.9947\n",
      "Epoch [8/10], Step [33450/68337], Loss: 5.1736\n",
      "Epoch [8/10], Step [33525/68337], Loss: 5.0015\n",
      "Epoch [8/10], Step [33600/68337], Loss: 5.0297\n",
      "Epoch [8/10], Step [33675/68337], Loss: 5.1058\n",
      "Epoch [8/10], Step [33750/68337], Loss: 5.1226\n",
      "Epoch [8/10], Step [33825/68337], Loss: 5.2821\n",
      "Epoch [8/10], Step [33900/68337], Loss: 5.1765\n",
      "Epoch [8/10], Step [33975/68337], Loss: 4.9874\n",
      "Epoch [8/10], Step [34050/68337], Loss: 5.1241\n",
      "Epoch [8/10], Step [34125/68337], Loss: 5.1548\n",
      "Epoch [8/10], Step [34200/68337], Loss: 5.3161\n",
      "Epoch [8/10], Step [34275/68337], Loss: 4.9972\n",
      "Epoch [8/10], Step [34350/68337], Loss: 5.1778\n",
      "Epoch [8/10], Step [34425/68337], Loss: 4.9216\n",
      "Epoch [8/10], Step [34500/68337], Loss: 4.8564\n",
      "Epoch [8/10], Step [34575/68337], Loss: 5.0320\n",
      "Epoch [8/10], Step [34650/68337], Loss: 5.1250\n",
      "Epoch [8/10], Step [34725/68337], Loss: 5.0008\n",
      "Epoch [8/10], Step [34800/68337], Loss: 4.9745\n",
      "Epoch [8/10], Step [34875/68337], Loss: 5.0537\n",
      "Epoch [8/10], Step [34950/68337], Loss: 5.0114\n",
      "Epoch [8/10], Step [35025/68337], Loss: 5.0033\n",
      "Epoch [8/10], Step [35100/68337], Loss: 5.1984\n",
      "Epoch [8/10], Step [35175/68337], Loss: 5.2869\n",
      "Epoch [8/10], Step [35250/68337], Loss: 5.0128\n",
      "Epoch [8/10], Step [35325/68337], Loss: 5.2680\n",
      "Epoch [8/10], Step [35400/68337], Loss: 5.1675\n",
      "Epoch [8/10], Step [35475/68337], Loss: 5.1694\n",
      "Epoch [8/10], Step [35550/68337], Loss: 5.1025\n",
      "Epoch [8/10], Step [35625/68337], Loss: 5.1740\n",
      "Epoch [8/10], Step [35700/68337], Loss: 5.0882\n",
      "Epoch [8/10], Step [35775/68337], Loss: 5.1555\n",
      "Epoch [8/10], Step [35850/68337], Loss: 5.1805\n",
      "Epoch [8/10], Step [35925/68337], Loss: 4.9699\n",
      "Epoch [8/10], Step [36000/68337], Loss: 5.0197\n",
      "Epoch [8/10], Step [36075/68337], Loss: 5.2217\n",
      "Epoch [8/10], Step [36150/68337], Loss: 5.3049\n",
      "Epoch [8/10], Step [36225/68337], Loss: 5.0656\n",
      "Epoch [8/10], Step [36300/68337], Loss: 5.1441\n",
      "Epoch [8/10], Step [36375/68337], Loss: 5.1468\n",
      "Epoch [8/10], Step [36450/68337], Loss: 4.9088\n",
      "Epoch [8/10], Step [36525/68337], Loss: 5.0498\n",
      "Epoch [8/10], Step [36600/68337], Loss: 5.0723\n",
      "Epoch [8/10], Step [36675/68337], Loss: 5.1499\n",
      "Epoch [8/10], Step [36750/68337], Loss: 5.2042\n",
      "Epoch [8/10], Step [36825/68337], Loss: 5.1104\n",
      "Epoch [8/10], Step [36900/68337], Loss: 5.0971\n",
      "Epoch [8/10], Step [36975/68337], Loss: 5.0553\n",
      "Epoch [8/10], Step [37050/68337], Loss: 5.1253\n",
      "Epoch [8/10], Step [37125/68337], Loss: 5.0189\n",
      "Epoch [8/10], Step [37200/68337], Loss: 5.0219\n",
      "Epoch [8/10], Step [37275/68337], Loss: 5.0276\n",
      "Epoch [8/10], Step [37350/68337], Loss: 5.1705\n",
      "Epoch [8/10], Step [37425/68337], Loss: 5.0958\n",
      "Epoch [8/10], Step [37500/68337], Loss: 5.0081\n",
      "Epoch [8/10], Step [37575/68337], Loss: 5.0212\n",
      "Epoch [8/10], Step [37650/68337], Loss: 5.0391\n",
      "Epoch [8/10], Step [37725/68337], Loss: 5.0760\n",
      "Epoch [8/10], Step [37800/68337], Loss: 4.9133\n",
      "Epoch [8/10], Step [37875/68337], Loss: 5.1636\n",
      "Epoch [8/10], Step [37950/68337], Loss: 5.0106\n",
      "Epoch [8/10], Step [38025/68337], Loss: 5.2497\n",
      "Epoch [8/10], Step [38100/68337], Loss: 5.0289\n",
      "Epoch [8/10], Step [38175/68337], Loss: 5.1192\n",
      "Epoch [8/10], Step [38250/68337], Loss: 4.8041\n",
      "Epoch [8/10], Step [38325/68337], Loss: 5.1462\n",
      "Epoch [8/10], Step [38400/68337], Loss: 5.0783\n",
      "Epoch [8/10], Step [38475/68337], Loss: 5.2345\n",
      "Epoch [8/10], Step [38550/68337], Loss: 5.3019\n",
      "Epoch [8/10], Step [38625/68337], Loss: 5.1105\n",
      "Epoch [8/10], Step [38700/68337], Loss: 5.0294\n",
      "Epoch [8/10], Step [38775/68337], Loss: 5.1427\n",
      "Epoch [8/10], Step [38850/68337], Loss: 5.1586\n",
      "Epoch [8/10], Step [38925/68337], Loss: 4.9400\n",
      "Epoch [8/10], Step [39000/68337], Loss: 5.1719\n",
      "Epoch [8/10], Step [39075/68337], Loss: 5.3513\n",
      "Epoch [8/10], Step [39150/68337], Loss: 4.9355\n",
      "Epoch [8/10], Step [39225/68337], Loss: 5.2212\n",
      "Epoch [8/10], Step [39300/68337], Loss: 5.0674\n",
      "Epoch [8/10], Step [39375/68337], Loss: 5.2136\n",
      "Epoch [8/10], Step [39450/68337], Loss: 5.1896\n",
      "Epoch [8/10], Step [39525/68337], Loss: 5.0536\n",
      "Epoch [8/10], Step [39600/68337], Loss: 5.0544\n",
      "Epoch [8/10], Step [39675/68337], Loss: 5.1087\n",
      "Epoch [8/10], Step [39750/68337], Loss: 4.9567\n",
      "Epoch [8/10], Step [39825/68337], Loss: 4.9534\n",
      "Epoch [8/10], Step [39900/68337], Loss: 4.9581\n",
      "Epoch [8/10], Step [39975/68337], Loss: 4.9758\n",
      "Validation perplexity: 122.46760125717454\n",
      "Epoch [8/10], Step [40050/68337], Loss: 4.9332\n",
      "Epoch [8/10], Step [40125/68337], Loss: 4.9656\n",
      "Epoch [8/10], Step [40200/68337], Loss: 5.0296\n",
      "Epoch [8/10], Step [40275/68337], Loss: 5.1378\n",
      "Epoch [8/10], Step [40350/68337], Loss: 5.0567\n",
      "Epoch [8/10], Step [40425/68337], Loss: 4.8405\n",
      "Epoch [8/10], Step [40500/68337], Loss: 5.2742\n",
      "Epoch [8/10], Step [40575/68337], Loss: 4.9942\n",
      "Epoch [8/10], Step [40650/68337], Loss: 5.1605\n",
      "Epoch [8/10], Step [40725/68337], Loss: 5.1846\n",
      "Epoch [8/10], Step [40800/68337], Loss: 5.2168\n",
      "Epoch [8/10], Step [40875/68337], Loss: 5.1672\n",
      "Epoch [8/10], Step [40950/68337], Loss: 5.0982\n",
      "Epoch [8/10], Step [41025/68337], Loss: 4.9934\n",
      "Epoch [8/10], Step [41100/68337], Loss: 5.2543\n",
      "Epoch [8/10], Step [41175/68337], Loss: 5.0515\n",
      "Epoch [8/10], Step [41250/68337], Loss: 4.9631\n",
      "Epoch [8/10], Step [41325/68337], Loss: 5.1796\n",
      "Epoch [8/10], Step [41400/68337], Loss: 5.0368\n",
      "Epoch [8/10], Step [41475/68337], Loss: 5.1392\n",
      "Epoch [8/10], Step [41550/68337], Loss: 5.1218\n",
      "Epoch [8/10], Step [41625/68337], Loss: 5.0182\n",
      "Epoch [8/10], Step [41700/68337], Loss: 5.1631\n",
      "Epoch [8/10], Step [41775/68337], Loss: 5.2938\n",
      "Epoch [8/10], Step [41850/68337], Loss: 5.2680\n",
      "Epoch [8/10], Step [41925/68337], Loss: 5.0629\n",
      "Epoch [8/10], Step [42000/68337], Loss: 4.9181\n",
      "Epoch [8/10], Step [42075/68337], Loss: 5.2002\n",
      "Epoch [8/10], Step [42150/68337], Loss: 5.0345\n",
      "Epoch [8/10], Step [42225/68337], Loss: 5.2519\n",
      "Epoch [8/10], Step [42300/68337], Loss: 5.1391\n",
      "Epoch [8/10], Step [42375/68337], Loss: 4.9836\n",
      "Epoch [8/10], Step [42450/68337], Loss: 5.2335\n",
      "Epoch [8/10], Step [42525/68337], Loss: 4.9949\n",
      "Epoch [8/10], Step [42600/68337], Loss: 4.9152\n",
      "Epoch [8/10], Step [42675/68337], Loss: 4.9877\n",
      "Epoch [8/10], Step [42750/68337], Loss: 5.0216\n",
      "Epoch [8/10], Step [42825/68337], Loss: 5.1096\n",
      "Epoch [8/10], Step [42900/68337], Loss: 5.1368\n",
      "Epoch [8/10], Step [42975/68337], Loss: 4.9125\n",
      "Epoch [8/10], Step [43050/68337], Loss: 5.1748\n",
      "Epoch [8/10], Step [43125/68337], Loss: 5.1564\n",
      "Epoch [8/10], Step [43200/68337], Loss: 5.0027\n",
      "Epoch [8/10], Step [43275/68337], Loss: 5.0789\n",
      "Epoch [8/10], Step [43350/68337], Loss: 5.0328\n",
      "Epoch [8/10], Step [43425/68337], Loss: 5.1785\n",
      "Epoch [8/10], Step [43500/68337], Loss: 5.1809\n",
      "Epoch [8/10], Step [43575/68337], Loss: 5.0345\n",
      "Epoch [8/10], Step [43650/68337], Loss: 5.1810\n",
      "Epoch [8/10], Step [43725/68337], Loss: 5.0683\n",
      "Epoch [8/10], Step [43800/68337], Loss: 5.1818\n",
      "Epoch [8/10], Step [43875/68337], Loss: 4.9403\n",
      "Epoch [8/10], Step [43950/68337], Loss: 5.1344\n",
      "Epoch [8/10], Step [44025/68337], Loss: 5.1433\n",
      "Epoch [8/10], Step [44100/68337], Loss: 4.9803\n",
      "Epoch [8/10], Step [44175/68337], Loss: 5.2303\n",
      "Epoch [8/10], Step [44250/68337], Loss: 5.0317\n",
      "Epoch [8/10], Step [44325/68337], Loss: 5.2215\n",
      "Epoch [8/10], Step [44400/68337], Loss: 5.2253\n",
      "Epoch [8/10], Step [44475/68337], Loss: 5.1897\n",
      "Epoch [8/10], Step [44550/68337], Loss: 5.2218\n",
      "Epoch [8/10], Step [44625/68337], Loss: 4.9390\n",
      "Epoch [8/10], Step [44700/68337], Loss: 5.1500\n",
      "Epoch [8/10], Step [44775/68337], Loss: 5.1993\n",
      "Epoch [8/10], Step [44850/68337], Loss: 5.0338\n",
      "Epoch [8/10], Step [44925/68337], Loss: 5.1656\n",
      "Epoch [8/10], Step [45000/68337], Loss: 5.0985\n",
      "Epoch [8/10], Step [45075/68337], Loss: 5.0850\n",
      "Epoch [8/10], Step [45150/68337], Loss: 5.2861\n",
      "Epoch [8/10], Step [45225/68337], Loss: 5.1299\n",
      "Epoch [8/10], Step [45300/68337], Loss: 5.1859\n",
      "Epoch [8/10], Step [45375/68337], Loss: 5.0435\n",
      "Epoch [8/10], Step [45450/68337], Loss: 5.1122\n",
      "Epoch [8/10], Step [45525/68337], Loss: 4.9629\n",
      "Epoch [8/10], Step [45600/68337], Loss: 5.1752\n",
      "Epoch [8/10], Step [45675/68337], Loss: 4.9889\n",
      "Epoch [8/10], Step [45750/68337], Loss: 5.2042\n",
      "Epoch [8/10], Step [45825/68337], Loss: 4.8682\n",
      "Epoch [8/10], Step [45900/68337], Loss: 5.1350\n",
      "Epoch [8/10], Step [45975/68337], Loss: 4.9665\n",
      "Epoch [8/10], Step [46050/68337], Loss: 5.1827\n",
      "Epoch [8/10], Step [46125/68337], Loss: 5.1235\n",
      "Epoch [8/10], Step [46200/68337], Loss: 5.0304\n",
      "Epoch [8/10], Step [46275/68337], Loss: 5.1317\n",
      "Epoch [8/10], Step [46350/68337], Loss: 5.0816\n",
      "Epoch [8/10], Step [46425/68337], Loss: 5.2106\n",
      "Epoch [8/10], Step [46500/68337], Loss: 5.1361\n",
      "Epoch [8/10], Step [46575/68337], Loss: 5.0859\n",
      "Epoch [8/10], Step [46650/68337], Loss: 5.0264\n",
      "Epoch [8/10], Step [46725/68337], Loss: 5.0896\n",
      "Epoch [8/10], Step [46800/68337], Loss: 5.0231\n",
      "Epoch [8/10], Step [46875/68337], Loss: 5.0324\n",
      "Epoch [8/10], Step [46950/68337], Loss: 5.0380\n",
      "Epoch [8/10], Step [47025/68337], Loss: 4.7536\n",
      "Epoch [8/10], Step [47100/68337], Loss: 5.0195\n",
      "Epoch [8/10], Step [47175/68337], Loss: 4.8439\n",
      "Epoch [8/10], Step [47250/68337], Loss: 4.9080\n",
      "Epoch [8/10], Step [47325/68337], Loss: 5.0389\n",
      "Epoch [8/10], Step [47400/68337], Loss: 5.0898\n",
      "Epoch [8/10], Step [47475/68337], Loss: 5.2209\n",
      "Epoch [8/10], Step [47550/68337], Loss: 5.1469\n",
      "Epoch [8/10], Step [47625/68337], Loss: 5.3012\n",
      "Epoch [8/10], Step [47700/68337], Loss: 4.9778\n",
      "Epoch [8/10], Step [47775/68337], Loss: 5.3635\n",
      "Epoch [8/10], Step [47850/68337], Loss: 5.1567\n",
      "Epoch [8/10], Step [47925/68337], Loss: 5.0921\n",
      "Epoch [8/10], Step [48000/68337], Loss: 4.9547\n",
      "Epoch [8/10], Step [48075/68337], Loss: 5.0689\n",
      "Epoch [8/10], Step [48150/68337], Loss: 5.1179\n",
      "Epoch [8/10], Step [48225/68337], Loss: 5.0313\n",
      "Epoch [8/10], Step [48300/68337], Loss: 5.2012\n",
      "Epoch [8/10], Step [48375/68337], Loss: 5.1641\n",
      "Epoch [8/10], Step [48450/68337], Loss: 5.2505\n",
      "Epoch [8/10], Step [48525/68337], Loss: 5.1028\n",
      "Epoch [8/10], Step [48600/68337], Loss: 5.0346\n",
      "Epoch [8/10], Step [48675/68337], Loss: 5.0422\n",
      "Epoch [8/10], Step [48750/68337], Loss: 5.1310\n",
      "Epoch [8/10], Step [48825/68337], Loss: 4.9616\n",
      "Epoch [8/10], Step [48900/68337], Loss: 5.1392\n",
      "Epoch [8/10], Step [48975/68337], Loss: 5.0057\n",
      "Epoch [8/10], Step [49050/68337], Loss: 5.1646\n",
      "Epoch [8/10], Step [49125/68337], Loss: 5.0078\n",
      "Epoch [8/10], Step [49200/68337], Loss: 5.1864\n",
      "Epoch [8/10], Step [49275/68337], Loss: 5.2184\n",
      "Epoch [8/10], Step [49350/68337], Loss: 5.1588\n",
      "Epoch [8/10], Step [49425/68337], Loss: 5.1134\n",
      "Epoch [8/10], Step [49500/68337], Loss: 5.0314\n",
      "Epoch [8/10], Step [49575/68337], Loss: 5.0915\n",
      "Epoch [8/10], Step [49650/68337], Loss: 5.2884\n",
      "Epoch [8/10], Step [49725/68337], Loss: 5.0681\n",
      "Epoch [8/10], Step [49800/68337], Loss: 5.2029\n",
      "Epoch [8/10], Step [49875/68337], Loss: 5.1133\n",
      "Epoch [8/10], Step [49950/68337], Loss: 4.9469\n",
      "Validation perplexity: 121.21092271875773\n",
      "Epoch [8/10], Step [50025/68337], Loss: 4.8376\n",
      "Epoch [8/10], Step [50100/68337], Loss: 5.1186\n",
      "Epoch [8/10], Step [50175/68337], Loss: 5.1413\n",
      "Epoch [8/10], Step [50250/68337], Loss: 4.8955\n",
      "Epoch [8/10], Step [50325/68337], Loss: 4.9908\n",
      "Epoch [8/10], Step [50400/68337], Loss: 5.1466\n",
      "Epoch [8/10], Step [50475/68337], Loss: 5.0972\n",
      "Epoch [8/10], Step [50550/68337], Loss: 5.1306\n",
      "Epoch [8/10], Step [50625/68337], Loss: 4.8518\n",
      "Epoch [8/10], Step [50700/68337], Loss: 5.0829\n",
      "Epoch [8/10], Step [50775/68337], Loss: 4.9479\n",
      "Epoch [8/10], Step [50850/68337], Loss: 5.2364\n",
      "Epoch [8/10], Step [50925/68337], Loss: 5.1426\n",
      "Epoch [8/10], Step [51000/68337], Loss: 5.1231\n",
      "Epoch [8/10], Step [51075/68337], Loss: 5.0962\n",
      "Epoch [8/10], Step [51150/68337], Loss: 4.9902\n",
      "Epoch [8/10], Step [51225/68337], Loss: 4.8605\n",
      "Epoch [8/10], Step [51300/68337], Loss: 5.1016\n",
      "Epoch [8/10], Step [51375/68337], Loss: 5.1544\n",
      "Epoch [8/10], Step [51450/68337], Loss: 5.0757\n",
      "Epoch [8/10], Step [51525/68337], Loss: 5.0989\n",
      "Epoch [8/10], Step [51600/68337], Loss: 5.0759\n",
      "Epoch [8/10], Step [51675/68337], Loss: 5.0062\n",
      "Epoch [8/10], Step [51750/68337], Loss: 5.2088\n",
      "Epoch [8/10], Step [51825/68337], Loss: 4.8997\n",
      "Epoch [8/10], Step [51900/68337], Loss: 5.2510\n",
      "Epoch [8/10], Step [51975/68337], Loss: 5.1154\n",
      "Epoch [8/10], Step [52050/68337], Loss: 5.2828\n",
      "Epoch [8/10], Step [52125/68337], Loss: 5.1216\n",
      "Epoch [8/10], Step [52200/68337], Loss: 5.0251\n",
      "Epoch [8/10], Step [52275/68337], Loss: 5.0307\n",
      "Epoch [8/10], Step [52350/68337], Loss: 4.8888\n",
      "Epoch [8/10], Step [52425/68337], Loss: 5.1854\n",
      "Epoch [8/10], Step [52500/68337], Loss: 5.1437\n",
      "Epoch [8/10], Step [52575/68337], Loss: 4.9155\n",
      "Epoch [8/10], Step [52650/68337], Loss: 5.0490\n",
      "Epoch [8/10], Step [52725/68337], Loss: 5.1034\n",
      "Epoch [8/10], Step [52800/68337], Loss: 4.8946\n",
      "Epoch [8/10], Step [52875/68337], Loss: 5.2198\n",
      "Epoch [8/10], Step [52950/68337], Loss: 4.9612\n",
      "Epoch [8/10], Step [53025/68337], Loss: 5.0857\n",
      "Epoch [8/10], Step [53100/68337], Loss: 5.1047\n",
      "Epoch [8/10], Step [53175/68337], Loss: 5.2990\n",
      "Epoch [8/10], Step [53250/68337], Loss: 5.0061\n",
      "Epoch [8/10], Step [53325/68337], Loss: 4.9790\n",
      "Epoch [8/10], Step [53400/68337], Loss: 4.9807\n",
      "Epoch [8/10], Step [53475/68337], Loss: 5.0323\n",
      "Epoch [8/10], Step [53550/68337], Loss: 5.2123\n",
      "Epoch [8/10], Step [53625/68337], Loss: 5.0539\n",
      "Epoch [8/10], Step [53700/68337], Loss: 4.9520\n",
      "Epoch [8/10], Step [53775/68337], Loss: 5.0719\n",
      "Epoch [8/10], Step [53850/68337], Loss: 5.0509\n",
      "Epoch [8/10], Step [53925/68337], Loss: 5.0407\n",
      "Epoch [8/10], Step [54000/68337], Loss: 4.9350\n",
      "Epoch [8/10], Step [54075/68337], Loss: 5.0809\n",
      "Epoch [8/10], Step [54150/68337], Loss: 5.1003\n",
      "Epoch [8/10], Step [54225/68337], Loss: 5.1009\n",
      "Epoch [8/10], Step [54300/68337], Loss: 5.0494\n",
      "Epoch [8/10], Step [54375/68337], Loss: 5.2793\n",
      "Epoch [8/10], Step [54450/68337], Loss: 5.1542\n",
      "Epoch [8/10], Step [54525/68337], Loss: 4.9760\n",
      "Epoch [8/10], Step [54600/68337], Loss: 4.8814\n",
      "Epoch [8/10], Step [54675/68337], Loss: 5.2777\n",
      "Epoch [8/10], Step [54750/68337], Loss: 4.9472\n",
      "Epoch [8/10], Step [54825/68337], Loss: 5.2090\n",
      "Epoch [8/10], Step [54900/68337], Loss: 5.3688\n",
      "Epoch [8/10], Step [54975/68337], Loss: 4.9900\n",
      "Epoch [8/10], Step [55050/68337], Loss: 5.0784\n",
      "Epoch [8/10], Step [55125/68337], Loss: 5.2163\n",
      "Epoch [8/10], Step [55200/68337], Loss: 4.8145\n",
      "Epoch [8/10], Step [55275/68337], Loss: 4.8852\n",
      "Epoch [8/10], Step [55350/68337], Loss: 5.0698\n",
      "Epoch [8/10], Step [55425/68337], Loss: 5.1518\n",
      "Epoch [8/10], Step [55500/68337], Loss: 5.2046\n",
      "Epoch [8/10], Step [55575/68337], Loss: 4.9831\n",
      "Epoch [8/10], Step [55650/68337], Loss: 5.1674\n",
      "Epoch [8/10], Step [55725/68337], Loss: 5.1833\n",
      "Epoch [8/10], Step [55800/68337], Loss: 5.0931\n",
      "Epoch [8/10], Step [55875/68337], Loss: 4.8916\n",
      "Epoch [8/10], Step [55950/68337], Loss: 4.9511\n",
      "Epoch [8/10], Step [56025/68337], Loss: 5.1753\n",
      "Epoch [8/10], Step [56100/68337], Loss: 5.2218\n",
      "Epoch [8/10], Step [56175/68337], Loss: 5.2359\n",
      "Epoch [8/10], Step [56250/68337], Loss: 4.9376\n",
      "Epoch [8/10], Step [56325/68337], Loss: 5.0409\n",
      "Epoch [8/10], Step [56400/68337], Loss: 5.2646\n",
      "Epoch [8/10], Step [56475/68337], Loss: 5.0901\n",
      "Epoch [8/10], Step [56550/68337], Loss: 5.2848\n",
      "Epoch [8/10], Step [56625/68337], Loss: 5.0791\n",
      "Epoch [8/10], Step [56700/68337], Loss: 4.9828\n",
      "Epoch [8/10], Step [56775/68337], Loss: 5.1856\n",
      "Epoch [8/10], Step [56850/68337], Loss: 5.2573\n",
      "Epoch [8/10], Step [56925/68337], Loss: 5.1664\n",
      "Epoch [8/10], Step [57000/68337], Loss: 4.9810\n",
      "Epoch [8/10], Step [57075/68337], Loss: 4.9268\n",
      "Epoch [8/10], Step [57150/68337], Loss: 5.1429\n",
      "Epoch [8/10], Step [57225/68337], Loss: 5.0054\n",
      "Epoch [8/10], Step [57300/68337], Loss: 5.1162\n",
      "Epoch [8/10], Step [57375/68337], Loss: 5.2163\n",
      "Epoch [8/10], Step [57450/68337], Loss: 5.4150\n",
      "Epoch [8/10], Step [57525/68337], Loss: 5.1185\n",
      "Epoch [8/10], Step [57600/68337], Loss: 5.4078\n",
      "Epoch [8/10], Step [57675/68337], Loss: 5.0324\n",
      "Epoch [8/10], Step [57750/68337], Loss: 5.0903\n",
      "Epoch [8/10], Step [57825/68337], Loss: 5.0163\n",
      "Epoch [8/10], Step [57900/68337], Loss: 4.7542\n",
      "Epoch [8/10], Step [57975/68337], Loss: 4.9272\n",
      "Epoch [8/10], Step [58050/68337], Loss: 5.1808\n",
      "Epoch [8/10], Step [58125/68337], Loss: 5.1946\n",
      "Epoch [8/10], Step [58200/68337], Loss: 5.1807\n",
      "Epoch [8/10], Step [58275/68337], Loss: 5.0742\n",
      "Epoch [8/10], Step [58350/68337], Loss: 4.9236\n",
      "Epoch [8/10], Step [58425/68337], Loss: 4.9328\n",
      "Epoch [8/10], Step [58500/68337], Loss: 5.1050\n",
      "Epoch [8/10], Step [58575/68337], Loss: 5.0044\n",
      "Epoch [8/10], Step [58650/68337], Loss: 5.2841\n",
      "Epoch [8/10], Step [58725/68337], Loss: 5.1289\n",
      "Epoch [8/10], Step [58800/68337], Loss: 5.0710\n",
      "Epoch [8/10], Step [58875/68337], Loss: 5.2169\n",
      "Epoch [8/10], Step [58950/68337], Loss: 4.9332\n",
      "Epoch [8/10], Step [59025/68337], Loss: 5.0150\n",
      "Epoch [8/10], Step [59100/68337], Loss: 5.1735\n",
      "Epoch [8/10], Step [59175/68337], Loss: 5.0409\n",
      "Epoch [8/10], Step [59250/68337], Loss: 5.0065\n",
      "Epoch [8/10], Step [59325/68337], Loss: 5.1119\n",
      "Epoch [8/10], Step [59400/68337], Loss: 5.0724\n",
      "Epoch [8/10], Step [59475/68337], Loss: 5.1518\n",
      "Epoch [8/10], Step [59550/68337], Loss: 5.2069\n",
      "Epoch [8/10], Step [59625/68337], Loss: 5.2986\n",
      "Epoch [8/10], Step [59700/68337], Loss: 5.0653\n",
      "Epoch [8/10], Step [59775/68337], Loss: 5.1172\n",
      "Epoch [8/10], Step [59850/68337], Loss: 5.2610\n",
      "Epoch [8/10], Step [59925/68337], Loss: 5.3245\n",
      "Epoch [8/10], Step [60000/68337], Loss: 5.1480\n",
      "Validation perplexity: 121.1338364155584\n",
      "Epoch [8/10], Step [60075/68337], Loss: 5.0345\n",
      "Epoch [8/10], Step [60150/68337], Loss: 4.9965\n",
      "Epoch [8/10], Step [60225/68337], Loss: 4.9878\n",
      "Epoch [8/10], Step [60300/68337], Loss: 5.0778\n",
      "Epoch [8/10], Step [60375/68337], Loss: 4.9710\n",
      "Epoch [8/10], Step [60450/68337], Loss: 4.8609\n",
      "Epoch [8/10], Step [60525/68337], Loss: 5.0560\n",
      "Epoch [8/10], Step [60600/68337], Loss: 5.0968\n",
      "Epoch [8/10], Step [60675/68337], Loss: 5.2092\n",
      "Epoch [8/10], Step [60750/68337], Loss: 5.1719\n",
      "Epoch [8/10], Step [60825/68337], Loss: 5.1263\n",
      "Epoch [8/10], Step [60900/68337], Loss: 5.1374\n",
      "Epoch [8/10], Step [60975/68337], Loss: 4.9602\n",
      "Epoch [8/10], Step [61050/68337], Loss: 5.2521\n",
      "Epoch [8/10], Step [61125/68337], Loss: 5.0701\n",
      "Epoch [8/10], Step [61200/68337], Loss: 5.0554\n",
      "Epoch [8/10], Step [61275/68337], Loss: 5.0690\n",
      "Epoch [8/10], Step [61350/68337], Loss: 5.0908\n",
      "Epoch [8/10], Step [61425/68337], Loss: 5.0749\n",
      "Epoch [8/10], Step [61500/68337], Loss: 4.8208\n",
      "Epoch [8/10], Step [61575/68337], Loss: 5.1417\n",
      "Epoch [8/10], Step [61650/68337], Loss: 5.1021\n",
      "Epoch [8/10], Step [61725/68337], Loss: 5.0478\n",
      "Epoch [8/10], Step [61800/68337], Loss: 4.9194\n",
      "Epoch [8/10], Step [61875/68337], Loss: 5.1015\n",
      "Epoch [8/10], Step [61950/68337], Loss: 5.2473\n",
      "Epoch [8/10], Step [62025/68337], Loss: 5.0007\n",
      "Epoch [8/10], Step [62100/68337], Loss: 5.0580\n",
      "Epoch [8/10], Step [62175/68337], Loss: 5.1276\n",
      "Epoch [8/10], Step [62250/68337], Loss: 5.0111\n",
      "Epoch [8/10], Step [62325/68337], Loss: 5.0145\n",
      "Epoch [8/10], Step [62400/68337], Loss: 5.0637\n",
      "Epoch [8/10], Step [62475/68337], Loss: 5.0700\n",
      "Epoch [8/10], Step [62550/68337], Loss: 5.0831\n",
      "Epoch [8/10], Step [62625/68337], Loss: 5.0398\n",
      "Epoch [8/10], Step [62700/68337], Loss: 5.0424\n",
      "Epoch [8/10], Step [62775/68337], Loss: 5.2558\n",
      "Epoch [8/10], Step [62850/68337], Loss: 5.1901\n",
      "Epoch [8/10], Step [62925/68337], Loss: 5.2955\n",
      "Epoch [8/10], Step [63000/68337], Loss: 5.1138\n",
      "Epoch [8/10], Step [63075/68337], Loss: 5.1202\n",
      "Epoch [8/10], Step [63150/68337], Loss: 4.9246\n",
      "Epoch [8/10], Step [63225/68337], Loss: 5.2321\n",
      "Epoch [8/10], Step [63300/68337], Loss: 5.0222\n",
      "Epoch [8/10], Step [63375/68337], Loss: 5.0064\n",
      "Epoch [8/10], Step [63450/68337], Loss: 5.1458\n",
      "Epoch [8/10], Step [63525/68337], Loss: 4.9510\n",
      "Epoch [8/10], Step [63600/68337], Loss: 5.1950\n",
      "Epoch [8/10], Step [63675/68337], Loss: 4.9627\n",
      "Epoch [8/10], Step [63750/68337], Loss: 5.1314\n",
      "Epoch [8/10], Step [63825/68337], Loss: 5.2053\n",
      "Epoch [8/10], Step [63900/68337], Loss: 5.1027\n",
      "Epoch [8/10], Step [63975/68337], Loss: 5.1411\n",
      "Epoch [8/10], Step [64050/68337], Loss: 4.9844\n",
      "Epoch [8/10], Step [64125/68337], Loss: 5.0787\n",
      "Epoch [8/10], Step [64200/68337], Loss: 5.1251\n",
      "Epoch [8/10], Step [64275/68337], Loss: 5.1085\n",
      "Epoch [8/10], Step [64350/68337], Loss: 5.2463\n",
      "Epoch [8/10], Step [64425/68337], Loss: 5.2184\n",
      "Epoch [8/10], Step [64500/68337], Loss: 5.1174\n",
      "Epoch [8/10], Step [64575/68337], Loss: 5.1518\n",
      "Epoch [8/10], Step [64650/68337], Loss: 5.1884\n",
      "Epoch [8/10], Step [64725/68337], Loss: 5.0779\n",
      "Epoch [8/10], Step [64800/68337], Loss: 5.1827\n",
      "Epoch [8/10], Step [64875/68337], Loss: 5.1685\n",
      "Epoch [8/10], Step [64950/68337], Loss: 4.9130\n",
      "Epoch [8/10], Step [65025/68337], Loss: 4.9860\n",
      "Epoch [8/10], Step [65100/68337], Loss: 5.0872\n",
      "Epoch [8/10], Step [65175/68337], Loss: 5.1650\n",
      "Epoch [8/10], Step [65250/68337], Loss: 4.8590\n",
      "Epoch [8/10], Step [65325/68337], Loss: 5.0045\n",
      "Epoch [8/10], Step [65400/68337], Loss: 5.1408\n",
      "Epoch [8/10], Step [65475/68337], Loss: 5.1714\n",
      "Epoch [8/10], Step [65550/68337], Loss: 5.1030\n",
      "Epoch [8/10], Step [65625/68337], Loss: 5.0896\n",
      "Epoch [8/10], Step [65700/68337], Loss: 5.0837\n",
      "Epoch [8/10], Step [65775/68337], Loss: 5.1332\n",
      "Epoch [8/10], Step [65850/68337], Loss: 5.0622\n",
      "Epoch [8/10], Step [65925/68337], Loss: 5.0046\n",
      "Epoch [8/10], Step [66000/68337], Loss: 5.1324\n",
      "Epoch [8/10], Step [66075/68337], Loss: 5.1219\n",
      "Epoch [8/10], Step [66150/68337], Loss: 5.0739\n",
      "Epoch [8/10], Step [66225/68337], Loss: 5.0468\n",
      "Epoch [8/10], Step [66300/68337], Loss: 4.9074\n",
      "Epoch [8/10], Step [66375/68337], Loss: 5.1348\n",
      "Epoch [8/10], Step [66450/68337], Loss: 5.2986\n",
      "Epoch [8/10], Step [66525/68337], Loss: 5.0498\n",
      "Epoch [8/10], Step [66600/68337], Loss: 5.1959\n",
      "Epoch [8/10], Step [66675/68337], Loss: 5.0270\n",
      "Epoch [8/10], Step [66750/68337], Loss: 5.2290\n",
      "Epoch [8/10], Step [66825/68337], Loss: 5.0025\n",
      "Epoch [8/10], Step [66900/68337], Loss: 5.1255\n",
      "Epoch [8/10], Step [66975/68337], Loss: 5.0083\n",
      "Epoch [8/10], Step [67050/68337], Loss: 5.0569\n",
      "Epoch [8/10], Step [67125/68337], Loss: 5.0366\n",
      "Epoch [8/10], Step [67200/68337], Loss: 4.9289\n",
      "Epoch [8/10], Step [67275/68337], Loss: 5.1568\n",
      "Epoch [8/10], Step [67350/68337], Loss: 5.2213\n",
      "Epoch [8/10], Step [67425/68337], Loss: 4.9901\n",
      "Epoch [8/10], Step [67500/68337], Loss: 5.0228\n",
      "Epoch [8/10], Step [67575/68337], Loss: 5.0551\n",
      "Epoch [8/10], Step [67650/68337], Loss: 5.2294\n",
      "Epoch [8/10], Step [67725/68337], Loss: 5.1377\n",
      "Epoch [8/10], Step [67800/68337], Loss: 5.0704\n",
      "Epoch [8/10], Step [67875/68337], Loss: 5.1772\n",
      "Epoch [8/10], Step [67950/68337], Loss: 5.1559\n",
      "Epoch [8/10], Step [68025/68337], Loss: 4.9918\n",
      "Epoch [8/10], Step [68100/68337], Loss: 5.2583\n",
      "Epoch [8/10], Step [68175/68337], Loss: 5.0648\n",
      "Epoch [8/10], Step [68250/68337], Loss: 5.0571\n",
      "Epoch [8/10], Step [68325/68337], Loss: 5.0564\n",
      "Epoch [8/10] Average Loss: 5.0838, Perplexity: 161.39\n",
      "Epoch [9/10], Step [0/68337], Loss: 5.0929\n",
      "Validation perplexity: 121.67858967696446\n",
      "Epoch [9/10], Step [75/68337], Loss: 5.0062\n",
      "Epoch [9/10], Step [150/68337], Loss: 5.1488\n",
      "Epoch [9/10], Step [225/68337], Loss: 5.0480\n",
      "Epoch [9/10], Step [300/68337], Loss: 4.9747\n",
      "Epoch [9/10], Step [375/68337], Loss: 5.0909\n",
      "Epoch [9/10], Step [450/68337], Loss: 5.1708\n",
      "Epoch [9/10], Step [525/68337], Loss: 4.9202\n",
      "Epoch [9/10], Step [600/68337], Loss: 5.2302\n",
      "Epoch [9/10], Step [675/68337], Loss: 5.0942\n",
      "Epoch [9/10], Step [750/68337], Loss: 4.9826\n",
      "Epoch [9/10], Step [825/68337], Loss: 5.1193\n",
      "Epoch [9/10], Step [900/68337], Loss: 5.1315\n",
      "Epoch [9/10], Step [975/68337], Loss: 5.0403\n",
      "Epoch [9/10], Step [1050/68337], Loss: 5.2442\n",
      "Epoch [9/10], Step [1125/68337], Loss: 4.9003\n",
      "Epoch [9/10], Step [1200/68337], Loss: 5.0602\n",
      "Epoch [9/10], Step [1275/68337], Loss: 4.8972\n",
      "Epoch [9/10], Step [1350/68337], Loss: 5.0297\n",
      "Epoch [9/10], Step [1425/68337], Loss: 5.1881\n",
      "Epoch [9/10], Step [1500/68337], Loss: 4.8022\n",
      "Epoch [9/10], Step [1575/68337], Loss: 5.1368\n",
      "Epoch [9/10], Step [1650/68337], Loss: 5.1134\n",
      "Epoch [9/10], Step [1725/68337], Loss: 5.1457\n",
      "Epoch [9/10], Step [1800/68337], Loss: 5.1742\n",
      "Epoch [9/10], Step [1875/68337], Loss: 5.1219\n",
      "Epoch [9/10], Step [1950/68337], Loss: 5.2263\n",
      "Epoch [9/10], Step [2025/68337], Loss: 5.3565\n",
      "Epoch [9/10], Step [2100/68337], Loss: 5.1426\n",
      "Epoch [9/10], Step [2175/68337], Loss: 4.9740\n",
      "Epoch [9/10], Step [2250/68337], Loss: 5.0581\n",
      "Epoch [9/10], Step [2325/68337], Loss: 5.0575\n",
      "Epoch [9/10], Step [2400/68337], Loss: 5.0935\n",
      "Epoch [9/10], Step [2475/68337], Loss: 5.1320\n",
      "Epoch [9/10], Step [2550/68337], Loss: 5.1440\n",
      "Epoch [9/10], Step [2625/68337], Loss: 5.1805\n",
      "Epoch [9/10], Step [2700/68337], Loss: 4.9307\n",
      "Epoch [9/10], Step [2775/68337], Loss: 5.0670\n",
      "Epoch [9/10], Step [2850/68337], Loss: 4.9187\n",
      "Epoch [9/10], Step [2925/68337], Loss: 5.2905\n",
      "Epoch [9/10], Step [3000/68337], Loss: 5.1370\n",
      "Epoch [9/10], Step [3075/68337], Loss: 5.0269\n",
      "Epoch [9/10], Step [3150/68337], Loss: 5.0672\n",
      "Epoch [9/10], Step [3225/68337], Loss: 5.0263\n",
      "Epoch [9/10], Step [3300/68337], Loss: 5.0614\n",
      "Epoch [9/10], Step [3375/68337], Loss: 5.1882\n",
      "Epoch [9/10], Step [3450/68337], Loss: 4.9639\n",
      "Epoch [9/10], Step [3525/68337], Loss: 5.2764\n",
      "Epoch [9/10], Step [3600/68337], Loss: 4.9750\n",
      "Epoch [9/10], Step [3675/68337], Loss: 5.1763\n",
      "Epoch [9/10], Step [3750/68337], Loss: 5.1441\n",
      "Epoch [9/10], Step [3825/68337], Loss: 4.8163\n",
      "Epoch [9/10], Step [3900/68337], Loss: 5.1765\n",
      "Epoch [9/10], Step [3975/68337], Loss: 5.0154\n",
      "Epoch [9/10], Step [4050/68337], Loss: 5.0988\n",
      "Epoch [9/10], Step [4125/68337], Loss: 4.8530\n",
      "Epoch [9/10], Step [4200/68337], Loss: 4.9657\n",
      "Epoch [9/10], Step [4275/68337], Loss: 4.9686\n",
      "Epoch [9/10], Step [4350/68337], Loss: 5.0815\n",
      "Epoch [9/10], Step [4425/68337], Loss: 5.0549\n",
      "Epoch [9/10], Step [4500/68337], Loss: 5.2301\n",
      "Epoch [9/10], Step [4575/68337], Loss: 5.2491\n",
      "Epoch [9/10], Step [4650/68337], Loss: 4.8711\n",
      "Epoch [9/10], Step [4725/68337], Loss: 5.1132\n",
      "Epoch [9/10], Step [4800/68337], Loss: 5.1090\n",
      "Epoch [9/10], Step [4875/68337], Loss: 5.2954\n",
      "Epoch [9/10], Step [4950/68337], Loss: 5.2758\n",
      "Epoch [9/10], Step [5025/68337], Loss: 4.9876\n",
      "Epoch [9/10], Step [5100/68337], Loss: 5.2832\n",
      "Epoch [9/10], Step [5175/68337], Loss: 5.0913\n",
      "Epoch [9/10], Step [5250/68337], Loss: 5.0787\n",
      "Epoch [9/10], Step [5325/68337], Loss: 5.0801\n",
      "Epoch [9/10], Step [5400/68337], Loss: 5.0363\n",
      "Epoch [9/10], Step [5475/68337], Loss: 5.2024\n",
      "Epoch [9/10], Step [5550/68337], Loss: 5.1072\n",
      "Epoch [9/10], Step [5625/68337], Loss: 5.0406\n",
      "Epoch [9/10], Step [5700/68337], Loss: 5.1810\n",
      "Epoch [9/10], Step [5775/68337], Loss: 4.8815\n",
      "Epoch [9/10], Step [5850/68337], Loss: 4.8858\n",
      "Epoch [9/10], Step [5925/68337], Loss: 5.0511\n",
      "Epoch [9/10], Step [6000/68337], Loss: 5.1359\n",
      "Epoch [9/10], Step [6075/68337], Loss: 4.8235\n",
      "Epoch [9/10], Step [6150/68337], Loss: 5.1858\n",
      "Epoch [9/10], Step [6225/68337], Loss: 5.1379\n",
      "Epoch [9/10], Step [6300/68337], Loss: 5.2483\n",
      "Epoch [9/10], Step [6375/68337], Loss: 5.0172\n",
      "Epoch [9/10], Step [6450/68337], Loss: 5.1131\n",
      "Epoch [9/10], Step [6525/68337], Loss: 5.2145\n",
      "Epoch [9/10], Step [6600/68337], Loss: 5.0047\n",
      "Epoch [9/10], Step [6675/68337], Loss: 5.2147\n",
      "Epoch [9/10], Step [6750/68337], Loss: 5.0783\n",
      "Epoch [9/10], Step [6825/68337], Loss: 5.1502\n",
      "Epoch [9/10], Step [6900/68337], Loss: 5.2100\n",
      "Epoch [9/10], Step [6975/68337], Loss: 5.0705\n",
      "Epoch [9/10], Step [7050/68337], Loss: 5.2734\n",
      "Epoch [9/10], Step [7125/68337], Loss: 4.9030\n",
      "Epoch [9/10], Step [7200/68337], Loss: 5.1392\n",
      "Epoch [9/10], Step [7275/68337], Loss: 5.1609\n",
      "Epoch [9/10], Step [7350/68337], Loss: 4.9782\n",
      "Epoch [9/10], Step [7425/68337], Loss: 4.8900\n",
      "Epoch [9/10], Step [7500/68337], Loss: 4.9942\n",
      "Epoch [9/10], Step [7575/68337], Loss: 5.0179\n",
      "Epoch [9/10], Step [7650/68337], Loss: 4.9942\n",
      "Epoch [9/10], Step [7725/68337], Loss: 5.1599\n",
      "Epoch [9/10], Step [7800/68337], Loss: 5.0307\n",
      "Epoch [9/10], Step [7875/68337], Loss: 5.0490\n",
      "Epoch [9/10], Step [7950/68337], Loss: 4.9227\n",
      "Epoch [9/10], Step [8025/68337], Loss: 5.1034\n",
      "Epoch [9/10], Step [8100/68337], Loss: 5.0024\n",
      "Epoch [9/10], Step [8175/68337], Loss: 5.1642\n",
      "Epoch [9/10], Step [8250/68337], Loss: 5.1308\n",
      "Epoch [9/10], Step [8325/68337], Loss: 5.3366\n",
      "Epoch [9/10], Step [8400/68337], Loss: 4.8519\n",
      "Epoch [9/10], Step [8475/68337], Loss: 5.1510\n",
      "Epoch [9/10], Step [8550/68337], Loss: 5.1451\n",
      "Epoch [9/10], Step [8625/68337], Loss: 5.0758\n",
      "Epoch [9/10], Step [8700/68337], Loss: 5.1553\n",
      "Epoch [9/10], Step [8775/68337], Loss: 5.1423\n",
      "Epoch [9/10], Step [8850/68337], Loss: 5.0750\n",
      "Epoch [9/10], Step [8925/68337], Loss: 5.0581\n",
      "Epoch [9/10], Step [9000/68337], Loss: 5.0904\n",
      "Epoch [9/10], Step [9075/68337], Loss: 5.2255\n",
      "Epoch [9/10], Step [9150/68337], Loss: 4.9500\n",
      "Epoch [9/10], Step [9225/68337], Loss: 5.1664\n",
      "Epoch [9/10], Step [9300/68337], Loss: 5.0532\n",
      "Epoch [9/10], Step [9375/68337], Loss: 5.0731\n",
      "Epoch [9/10], Step [9450/68337], Loss: 4.9068\n",
      "Epoch [9/10], Step [9525/68337], Loss: 4.9017\n",
      "Epoch [9/10], Step [9600/68337], Loss: 4.9949\n",
      "Epoch [9/10], Step [9675/68337], Loss: 5.1626\n",
      "Epoch [9/10], Step [9750/68337], Loss: 5.0360\n",
      "Epoch [9/10], Step [9825/68337], Loss: 4.9054\n",
      "Epoch [9/10], Step [9900/68337], Loss: 5.1705\n",
      "Epoch [9/10], Step [9975/68337], Loss: 5.1921\n",
      "Validation perplexity: 120.89122616531743\n",
      "Epoch [9/10], Step [10050/68337], Loss: 5.0134\n",
      "Epoch [9/10], Step [10125/68337], Loss: 5.1722\n",
      "Epoch [9/10], Step [10200/68337], Loss: 5.0443\n",
      "Epoch [9/10], Step [10275/68337], Loss: 4.9222\n",
      "Epoch [9/10], Step [10350/68337], Loss: 4.7827\n",
      "Epoch [9/10], Step [10425/68337], Loss: 5.0478\n",
      "Epoch [9/10], Step [10500/68337], Loss: 5.0295\n",
      "Epoch [9/10], Step [10575/68337], Loss: 5.0892\n",
      "Epoch [9/10], Step [10650/68337], Loss: 5.1463\n",
      "Epoch [9/10], Step [10725/68337], Loss: 5.0388\n",
      "Epoch [9/10], Step [10800/68337], Loss: 4.9804\n",
      "Epoch [9/10], Step [10875/68337], Loss: 5.1537\n",
      "Epoch [9/10], Step [10950/68337], Loss: 4.9671\n",
      "Epoch [9/10], Step [11025/68337], Loss: 5.0584\n",
      "Epoch [9/10], Step [11100/68337], Loss: 5.1495\n",
      "Epoch [9/10], Step [11175/68337], Loss: 4.9807\n",
      "Epoch [9/10], Step [11250/68337], Loss: 5.1635\n",
      "Epoch [9/10], Step [11325/68337], Loss: 5.2423\n",
      "Epoch [9/10], Step [11400/68337], Loss: 5.0272\n",
      "Epoch [9/10], Step [11475/68337], Loss: 4.8417\n",
      "Epoch [9/10], Step [11550/68337], Loss: 4.9404\n",
      "Epoch [9/10], Step [11625/68337], Loss: 4.9546\n",
      "Epoch [9/10], Step [11700/68337], Loss: 4.9714\n",
      "Epoch [9/10], Step [11775/68337], Loss: 5.2408\n",
      "Epoch [9/10], Step [11850/68337], Loss: 4.9421\n",
      "Epoch [9/10], Step [11925/68337], Loss: 5.1007\n",
      "Epoch [9/10], Step [12000/68337], Loss: 4.9774\n",
      "Epoch [9/10], Step [12075/68337], Loss: 5.1546\n",
      "Epoch [9/10], Step [12150/68337], Loss: 5.0447\n",
      "Epoch [9/10], Step [12225/68337], Loss: 4.9014\n",
      "Epoch [9/10], Step [12300/68337], Loss: 5.2984\n",
      "Epoch [9/10], Step [12375/68337], Loss: 4.9109\n",
      "Epoch [9/10], Step [12450/68337], Loss: 5.1382\n",
      "Epoch [9/10], Step [12525/68337], Loss: 5.0099\n",
      "Epoch [9/10], Step [12600/68337], Loss: 5.2085\n",
      "Epoch [9/10], Step [12675/68337], Loss: 5.0178\n",
      "Epoch [9/10], Step [12750/68337], Loss: 5.1940\n",
      "Epoch [9/10], Step [12825/68337], Loss: 5.0848\n",
      "Epoch [9/10], Step [12900/68337], Loss: 5.0019\n",
      "Epoch [9/10], Step [12975/68337], Loss: 5.1548\n",
      "Epoch [9/10], Step [13050/68337], Loss: 5.1039\n",
      "Epoch [9/10], Step [13125/68337], Loss: 5.2158\n",
      "Epoch [9/10], Step [13200/68337], Loss: 5.0478\n",
      "Epoch [9/10], Step [13275/68337], Loss: 5.0855\n",
      "Epoch [9/10], Step [13350/68337], Loss: 4.9533\n",
      "Epoch [9/10], Step [13425/68337], Loss: 5.0642\n",
      "Epoch [9/10], Step [13500/68337], Loss: 5.1411\n",
      "Epoch [9/10], Step [13575/68337], Loss: 5.0427\n",
      "Epoch [9/10], Step [13650/68337], Loss: 5.1306\n",
      "Epoch [9/10], Step [13725/68337], Loss: 4.8289\n",
      "Epoch [9/10], Step [13800/68337], Loss: 5.1950\n",
      "Epoch [9/10], Step [13875/68337], Loss: 5.1622\n",
      "Epoch [9/10], Step [13950/68337], Loss: 5.0609\n",
      "Epoch [9/10], Step [14025/68337], Loss: 4.9966\n",
      "Epoch [9/10], Step [14100/68337], Loss: 4.7976\n",
      "Epoch [9/10], Step [14175/68337], Loss: 5.0238\n",
      "Epoch [9/10], Step [14250/68337], Loss: 5.2019\n",
      "Epoch [9/10], Step [14325/68337], Loss: 5.0536\n",
      "Epoch [9/10], Step [14400/68337], Loss: 5.1018\n",
      "Epoch [9/10], Step [14475/68337], Loss: 5.1317\n",
      "Epoch [9/10], Step [14550/68337], Loss: 5.0362\n",
      "Epoch [9/10], Step [14625/68337], Loss: 5.1015\n",
      "Epoch [9/10], Step [14700/68337], Loss: 5.2271\n",
      "Epoch [9/10], Step [14775/68337], Loss: 4.8809\n",
      "Epoch [9/10], Step [14850/68337], Loss: 5.1106\n",
      "Epoch [9/10], Step [14925/68337], Loss: 5.2168\n",
      "Epoch [9/10], Step [15000/68337], Loss: 4.8448\n",
      "Epoch [9/10], Step [15075/68337], Loss: 5.0834\n",
      "Epoch [9/10], Step [15150/68337], Loss: 5.0227\n",
      "Epoch [9/10], Step [15225/68337], Loss: 5.0526\n",
      "Epoch [9/10], Step [15300/68337], Loss: 5.1264\n",
      "Epoch [9/10], Step [15375/68337], Loss: 5.0219\n",
      "Epoch [9/10], Step [15450/68337], Loss: 4.9777\n",
      "Epoch [9/10], Step [15525/68337], Loss: 5.2127\n",
      "Epoch [9/10], Step [15600/68337], Loss: 4.9102\n",
      "Epoch [9/10], Step [15675/68337], Loss: 4.9852\n",
      "Epoch [9/10], Step [15750/68337], Loss: 5.0965\n",
      "Epoch [9/10], Step [15825/68337], Loss: 5.1167\n",
      "Epoch [9/10], Step [15900/68337], Loss: 5.1498\n",
      "Epoch [9/10], Step [15975/68337], Loss: 5.0500\n",
      "Epoch [9/10], Step [16050/68337], Loss: 5.0340\n",
      "Epoch [9/10], Step [16125/68337], Loss: 5.0789\n",
      "Epoch [9/10], Step [16200/68337], Loss: 5.1071\n",
      "Epoch [9/10], Step [16275/68337], Loss: 5.0158\n",
      "Epoch [9/10], Step [16350/68337], Loss: 5.0546\n",
      "Epoch [9/10], Step [16425/68337], Loss: 5.1572\n",
      "Epoch [9/10], Step [16500/68337], Loss: 5.1085\n",
      "Epoch [9/10], Step [16575/68337], Loss: 5.0344\n",
      "Epoch [9/10], Step [16650/68337], Loss: 4.9720\n",
      "Epoch [9/10], Step [16725/68337], Loss: 5.0867\n",
      "Epoch [9/10], Step [16800/68337], Loss: 4.9505\n",
      "Epoch [9/10], Step [16875/68337], Loss: 5.1351\n",
      "Epoch [9/10], Step [16950/68337], Loss: 5.0638\n",
      "Epoch [9/10], Step [17025/68337], Loss: 5.0493\n",
      "Epoch [9/10], Step [17100/68337], Loss: 4.8923\n",
      "Epoch [9/10], Step [17175/68337], Loss: 5.0907\n",
      "Epoch [9/10], Step [17250/68337], Loss: 5.2198\n",
      "Epoch [9/10], Step [17325/68337], Loss: 5.0123\n",
      "Epoch [9/10], Step [17400/68337], Loss: 4.9781\n",
      "Epoch [9/10], Step [17475/68337], Loss: 5.2137\n",
      "Epoch [9/10], Step [17550/68337], Loss: 5.1423\n",
      "Epoch [9/10], Step [17625/68337], Loss: 5.1448\n",
      "Epoch [9/10], Step [17700/68337], Loss: 5.1513\n",
      "Epoch [9/10], Step [17775/68337], Loss: 5.0548\n",
      "Epoch [9/10], Step [17850/68337], Loss: 4.8447\n",
      "Epoch [9/10], Step [17925/68337], Loss: 5.0787\n",
      "Epoch [9/10], Step [18000/68337], Loss: 5.0371\n",
      "Epoch [9/10], Step [18075/68337], Loss: 5.2230\n",
      "Epoch [9/10], Step [18150/68337], Loss: 5.1531\n",
      "Epoch [9/10], Step [18225/68337], Loss: 5.1250\n",
      "Epoch [9/10], Step [18300/68337], Loss: 5.0766\n",
      "Epoch [9/10], Step [18375/68337], Loss: 5.3062\n",
      "Epoch [9/10], Step [18450/68337], Loss: 5.0918\n",
      "Epoch [9/10], Step [18525/68337], Loss: 5.0913\n",
      "Epoch [9/10], Step [18600/68337], Loss: 5.2336\n",
      "Epoch [9/10], Step [18675/68337], Loss: 5.0597\n",
      "Epoch [9/10], Step [18750/68337], Loss: 5.1298\n",
      "Epoch [9/10], Step [18825/68337], Loss: 4.9916\n",
      "Epoch [9/10], Step [18900/68337], Loss: 5.1845\n",
      "Epoch [9/10], Step [18975/68337], Loss: 4.9787\n",
      "Epoch [9/10], Step [19050/68337], Loss: 5.1236\n",
      "Epoch [9/10], Step [19125/68337], Loss: 5.0856\n",
      "Epoch [9/10], Step [19200/68337], Loss: 5.0575\n",
      "Epoch [9/10], Step [19275/68337], Loss: 4.9348\n",
      "Epoch [9/10], Step [19350/68337], Loss: 5.0294\n",
      "Epoch [9/10], Step [19425/68337], Loss: 4.9324\n",
      "Epoch [9/10], Step [19500/68337], Loss: 4.7815\n",
      "Epoch [9/10], Step [19575/68337], Loss: 4.9651\n",
      "Epoch [9/10], Step [19650/68337], Loss: 5.2662\n",
      "Epoch [9/10], Step [19725/68337], Loss: 5.1429\n",
      "Epoch [9/10], Step [19800/68337], Loss: 5.1370\n",
      "Epoch [9/10], Step [19875/68337], Loss: 4.9313\n",
      "Epoch [9/10], Step [19950/68337], Loss: 5.2416\n",
      "Validation perplexity: 120.85712705695356\n",
      "Epoch [9/10], Step [20025/68337], Loss: 5.3176\n",
      "Epoch [9/10], Step [20100/68337], Loss: 4.8760\n",
      "Epoch [9/10], Step [20175/68337], Loss: 5.0641\n",
      "Epoch [9/10], Step [20250/68337], Loss: 4.9656\n",
      "Epoch [9/10], Step [20325/68337], Loss: 5.1122\n",
      "Epoch [9/10], Step [20400/68337], Loss: 5.0887\n",
      "Epoch [9/10], Step [20475/68337], Loss: 5.1053\n",
      "Epoch [9/10], Step [20550/68337], Loss: 5.0361\n",
      "Epoch [9/10], Step [20625/68337], Loss: 4.8975\n",
      "Epoch [9/10], Step [20700/68337], Loss: 5.1369\n",
      "Epoch [9/10], Step [20775/68337], Loss: 5.0733\n",
      "Epoch [9/10], Step [20850/68337], Loss: 5.0171\n",
      "Epoch [9/10], Step [20925/68337], Loss: 5.3151\n",
      "Epoch [9/10], Step [21000/68337], Loss: 4.9923\n",
      "Epoch [9/10], Step [21075/68337], Loss: 5.0415\n",
      "Epoch [9/10], Step [21150/68337], Loss: 5.1299\n",
      "Epoch [9/10], Step [21225/68337], Loss: 5.1419\n",
      "Epoch [9/10], Step [21300/68337], Loss: 5.2432\n",
      "Epoch [9/10], Step [21375/68337], Loss: 5.1940\n",
      "Epoch [9/10], Step [21450/68337], Loss: 5.0053\n",
      "Epoch [9/10], Step [21525/68337], Loss: 5.1229\n",
      "Epoch [9/10], Step [21600/68337], Loss: 5.0616\n",
      "Epoch [9/10], Step [21675/68337], Loss: 5.1140\n",
      "Epoch [9/10], Step [21750/68337], Loss: 4.9699\n",
      "Epoch [9/10], Step [21825/68337], Loss: 4.9834\n",
      "Epoch [9/10], Step [21900/68337], Loss: 4.7607\n",
      "Epoch [9/10], Step [21975/68337], Loss: 4.9734\n",
      "Epoch [9/10], Step [22050/68337], Loss: 4.9989\n",
      "Epoch [9/10], Step [22125/68337], Loss: 5.0611\n",
      "Epoch [9/10], Step [22200/68337], Loss: 5.0532\n",
      "Epoch [9/10], Step [22275/68337], Loss: 5.1449\n",
      "Epoch [9/10], Step [22350/68337], Loss: 5.0939\n",
      "Epoch [9/10], Step [22425/68337], Loss: 5.2926\n",
      "Epoch [9/10], Step [22500/68337], Loss: 4.9540\n",
      "Epoch [9/10], Step [22575/68337], Loss: 5.2452\n",
      "Epoch [9/10], Step [22650/68337], Loss: 5.2399\n",
      "Epoch [9/10], Step [22725/68337], Loss: 5.0950\n",
      "Epoch [9/10], Step [22800/68337], Loss: 5.2116\n",
      "Epoch [9/10], Step [22875/68337], Loss: 4.9861\n",
      "Epoch [9/10], Step [22950/68337], Loss: 4.9722\n",
      "Epoch [9/10], Step [23025/68337], Loss: 5.1151\n",
      "Epoch [9/10], Step [23100/68337], Loss: 4.9566\n",
      "Epoch [9/10], Step [23175/68337], Loss: 4.8832\n",
      "Epoch [9/10], Step [23250/68337], Loss: 5.0495\n",
      "Epoch [9/10], Step [23325/68337], Loss: 4.9748\n",
      "Epoch [9/10], Step [23400/68337], Loss: 4.9798\n",
      "Epoch [9/10], Step [23475/68337], Loss: 5.0180\n",
      "Epoch [9/10], Step [23550/68337], Loss: 5.0446\n",
      "Epoch [9/10], Step [23625/68337], Loss: 5.1976\n",
      "Epoch [9/10], Step [23700/68337], Loss: 4.9200\n",
      "Epoch [9/10], Step [23775/68337], Loss: 5.0124\n",
      "Epoch [9/10], Step [23850/68337], Loss: 5.0489\n",
      "Epoch [9/10], Step [23925/68337], Loss: 5.0980\n",
      "Epoch [9/10], Step [24000/68337], Loss: 5.1840\n",
      "Epoch [9/10], Step [24075/68337], Loss: 5.1199\n",
      "Epoch [9/10], Step [24150/68337], Loss: 5.2542\n",
      "Epoch [9/10], Step [24225/68337], Loss: 5.0028\n",
      "Epoch [9/10], Step [24300/68337], Loss: 5.0772\n",
      "Epoch [9/10], Step [24375/68337], Loss: 4.8133\n",
      "Epoch [9/10], Step [24450/68337], Loss: 5.1535\n",
      "Epoch [9/10], Step [24525/68337], Loss: 5.2217\n",
      "Epoch [9/10], Step [24600/68337], Loss: 4.9594\n",
      "Epoch [9/10], Step [24675/68337], Loss: 5.1685\n",
      "Epoch [9/10], Step [24750/68337], Loss: 5.1407\n",
      "Epoch [9/10], Step [24825/68337], Loss: 5.0250\n",
      "Epoch [9/10], Step [24900/68337], Loss: 5.2628\n",
      "Epoch [9/10], Step [24975/68337], Loss: 4.9827\n",
      "Epoch [9/10], Step [25050/68337], Loss: 5.2329\n",
      "Epoch [9/10], Step [25125/68337], Loss: 5.1358\n",
      "Epoch [9/10], Step [25200/68337], Loss: 4.9858\n",
      "Epoch [9/10], Step [25275/68337], Loss: 5.0560\n",
      "Epoch [9/10], Step [25350/68337], Loss: 4.9797\n",
      "Epoch [9/10], Step [25425/68337], Loss: 4.9538\n",
      "Epoch [9/10], Step [25500/68337], Loss: 5.0998\n",
      "Epoch [9/10], Step [25575/68337], Loss: 4.9655\n",
      "Epoch [9/10], Step [25650/68337], Loss: 5.1791\n",
      "Epoch [9/10], Step [25725/68337], Loss: 5.1971\n",
      "Epoch [9/10], Step [25800/68337], Loss: 4.9844\n",
      "Epoch [9/10], Step [25875/68337], Loss: 5.1600\n",
      "Epoch [9/10], Step [25950/68337], Loss: 5.1691\n",
      "Epoch [9/10], Step [26025/68337], Loss: 5.0117\n",
      "Epoch [9/10], Step [26100/68337], Loss: 5.1379\n",
      "Epoch [9/10], Step [26175/68337], Loss: 5.0892\n",
      "Epoch [9/10], Step [26250/68337], Loss: 4.8995\n",
      "Epoch [9/10], Step [26325/68337], Loss: 4.8425\n",
      "Epoch [9/10], Step [26400/68337], Loss: 4.8451\n",
      "Epoch [9/10], Step [26475/68337], Loss: 5.1062\n",
      "Epoch [9/10], Step [26550/68337], Loss: 5.0390\n",
      "Epoch [9/10], Step [26625/68337], Loss: 5.1275\n",
      "Epoch [9/10], Step [26700/68337], Loss: 5.1105\n",
      "Epoch [9/10], Step [26775/68337], Loss: 5.0319\n",
      "Epoch [9/10], Step [26850/68337], Loss: 4.9947\n",
      "Epoch [9/10], Step [26925/68337], Loss: 5.1701\n",
      "Epoch [9/10], Step [27000/68337], Loss: 5.1430\n",
      "Epoch [9/10], Step [27075/68337], Loss: 5.0450\n",
      "Epoch [9/10], Step [27150/68337], Loss: 5.1150\n",
      "Epoch [9/10], Step [27225/68337], Loss: 5.0138\n",
      "Epoch [9/10], Step [27300/68337], Loss: 4.9351\n",
      "Epoch [9/10], Step [27375/68337], Loss: 5.0755\n",
      "Epoch [9/10], Step [27450/68337], Loss: 5.0003\n",
      "Epoch [9/10], Step [27525/68337], Loss: 4.9434\n",
      "Epoch [9/10], Step [27600/68337], Loss: 5.1549\n",
      "Epoch [9/10], Step [27675/68337], Loss: 5.1686\n",
      "Epoch [9/10], Step [27750/68337], Loss: 5.2277\n",
      "Epoch [9/10], Step [27825/68337], Loss: 4.8767\n",
      "Epoch [9/10], Step [27900/68337], Loss: 5.3037\n",
      "Epoch [9/10], Step [27975/68337], Loss: 5.1217\n",
      "Epoch [9/10], Step [28050/68337], Loss: 5.2183\n",
      "Epoch [9/10], Step [28125/68337], Loss: 5.1170\n",
      "Epoch [9/10], Step [28200/68337], Loss: 4.9438\n",
      "Epoch [9/10], Step [28275/68337], Loss: 5.1700\n",
      "Epoch [9/10], Step [28350/68337], Loss: 5.0554\n",
      "Epoch [9/10], Step [28425/68337], Loss: 5.1759\n",
      "Epoch [9/10], Step [28500/68337], Loss: 5.1915\n",
      "Epoch [9/10], Step [28575/68337], Loss: 5.0521\n",
      "Epoch [9/10], Step [28650/68337], Loss: 5.3029\n",
      "Epoch [9/10], Step [28725/68337], Loss: 5.1094\n",
      "Epoch [9/10], Step [28800/68337], Loss: 4.9168\n",
      "Epoch [9/10], Step [28875/68337], Loss: 5.1301\n",
      "Epoch [9/10], Step [28950/68337], Loss: 5.0343\n",
      "Epoch [9/10], Step [29025/68337], Loss: 5.1274\n",
      "Epoch [9/10], Step [29100/68337], Loss: 5.1075\n",
      "Epoch [9/10], Step [29175/68337], Loss: 5.3640\n",
      "Epoch [9/10], Step [29250/68337], Loss: 5.1286\n",
      "Epoch [9/10], Step [29325/68337], Loss: 5.1638\n",
      "Epoch [9/10], Step [29400/68337], Loss: 4.9006\n",
      "Epoch [9/10], Step [29475/68337], Loss: 4.9614\n",
      "Epoch [9/10], Step [29550/68337], Loss: 5.1301\n",
      "Epoch [9/10], Step [29625/68337], Loss: 5.1265\n",
      "Epoch [9/10], Step [29700/68337], Loss: 5.2039\n",
      "Epoch [9/10], Step [29775/68337], Loss: 5.1693\n",
      "Epoch [9/10], Step [29850/68337], Loss: 5.0101\n",
      "Epoch [9/10], Step [29925/68337], Loss: 5.3004\n",
      "Epoch [9/10], Step [30000/68337], Loss: 4.9831\n",
      "Validation perplexity: 121.17647359447652\n",
      "Epoch [9/10], Step [30075/68337], Loss: 5.0089\n",
      "Epoch [9/10], Step [30150/68337], Loss: 4.9185\n",
      "Epoch [9/10], Step [30225/68337], Loss: 5.0372\n",
      "Epoch [9/10], Step [30300/68337], Loss: 5.1427\n",
      "Epoch [9/10], Step [30375/68337], Loss: 5.0436\n",
      "Epoch [9/10], Step [30450/68337], Loss: 5.0247\n",
      "Epoch [9/10], Step [30525/68337], Loss: 5.1980\n",
      "Epoch [9/10], Step [30600/68337], Loss: 4.9323\n",
      "Epoch [9/10], Step [30675/68337], Loss: 5.0240\n",
      "Epoch [9/10], Step [30750/68337], Loss: 4.8705\n",
      "Epoch [9/10], Step [30825/68337], Loss: 5.0675\n",
      "Epoch [9/10], Step [30900/68337], Loss: 5.1391\n",
      "Epoch [9/10], Step [30975/68337], Loss: 4.9678\n",
      "Epoch [9/10], Step [31050/68337], Loss: 4.9379\n",
      "Epoch [9/10], Step [31125/68337], Loss: 5.1267\n",
      "Epoch [9/10], Step [31200/68337], Loss: 4.9637\n",
      "Epoch [9/10], Step [31275/68337], Loss: 5.0823\n",
      "Epoch [9/10], Step [31350/68337], Loss: 5.0210\n",
      "Epoch [9/10], Step [31425/68337], Loss: 4.9960\n",
      "Epoch [9/10], Step [31500/68337], Loss: 5.0173\n",
      "Epoch [9/10], Step [31575/68337], Loss: 4.8487\n",
      "Epoch [9/10], Step [31650/68337], Loss: 5.1054\n",
      "Epoch [9/10], Step [31725/68337], Loss: 5.1833\n",
      "Epoch [9/10], Step [31800/68337], Loss: 5.0421\n",
      "Epoch [9/10], Step [31875/68337], Loss: 5.0519\n",
      "Epoch [9/10], Step [31950/68337], Loss: 5.0195\n",
      "Epoch [9/10], Step [32025/68337], Loss: 5.0463\n",
      "Epoch [9/10], Step [32100/68337], Loss: 5.0868\n",
      "Epoch [9/10], Step [32175/68337], Loss: 5.2656\n",
      "Epoch [9/10], Step [32250/68337], Loss: 5.1085\n",
      "Epoch [9/10], Step [32325/68337], Loss: 5.0810\n",
      "Epoch [9/10], Step [32400/68337], Loss: 5.0466\n",
      "Epoch [9/10], Step [32475/68337], Loss: 5.1470\n",
      "Epoch [9/10], Step [32550/68337], Loss: 5.0350\n",
      "Epoch [9/10], Step [32625/68337], Loss: 4.9600\n",
      "Epoch [9/10], Step [32700/68337], Loss: 5.1059\n",
      "Epoch [9/10], Step [32775/68337], Loss: 5.0842\n",
      "Epoch [9/10], Step [32850/68337], Loss: 5.1026\n",
      "Epoch [9/10], Step [32925/68337], Loss: 5.0466\n",
      "Epoch [9/10], Step [33000/68337], Loss: 5.0966\n",
      "Epoch [9/10], Step [33075/68337], Loss: 5.0981\n",
      "Epoch [9/10], Step [33150/68337], Loss: 5.1528\n",
      "Epoch [9/10], Step [33225/68337], Loss: 4.9830\n",
      "Epoch [9/10], Step [33300/68337], Loss: 5.0714\n",
      "Epoch [9/10], Step [33375/68337], Loss: 5.0653\n",
      "Epoch [9/10], Step [33450/68337], Loss: 5.3099\n",
      "Epoch [9/10], Step [33525/68337], Loss: 5.0944\n",
      "Epoch [9/10], Step [33600/68337], Loss: 4.8436\n",
      "Epoch [9/10], Step [33675/68337], Loss: 4.8674\n",
      "Epoch [9/10], Step [33750/68337], Loss: 4.8831\n",
      "Epoch [9/10], Step [33825/68337], Loss: 5.0399\n",
      "Epoch [9/10], Step [33900/68337], Loss: 5.0313\n",
      "Epoch [9/10], Step [33975/68337], Loss: 5.1295\n",
      "Epoch [9/10], Step [34050/68337], Loss: 5.0687\n",
      "Epoch [9/10], Step [34125/68337], Loss: 4.9177\n",
      "Epoch [9/10], Step [34200/68337], Loss: 4.9345\n",
      "Epoch [9/10], Step [34275/68337], Loss: 5.1478\n",
      "Epoch [9/10], Step [34350/68337], Loss: 5.0834\n",
      "Epoch [9/10], Step [34425/68337], Loss: 4.9326\n",
      "Epoch [9/10], Step [34500/68337], Loss: 4.9462\n",
      "Epoch [9/10], Step [34575/68337], Loss: 5.1458\n",
      "Epoch [9/10], Step [34650/68337], Loss: 5.1859\n",
      "Epoch [9/10], Step [34725/68337], Loss: 4.8165\n",
      "Epoch [9/10], Step [34800/68337], Loss: 5.0853\n",
      "Epoch [9/10], Step [34875/68337], Loss: 4.9558\n",
      "Epoch [9/10], Step [34950/68337], Loss: 5.1324\n",
      "Epoch [9/10], Step [35025/68337], Loss: 5.1095\n",
      "Epoch [9/10], Step [35100/68337], Loss: 5.0372\n",
      "Epoch [9/10], Step [35175/68337], Loss: 5.0220\n",
      "Epoch [9/10], Step [35250/68337], Loss: 4.9156\n",
      "Epoch [9/10], Step [35325/68337], Loss: 4.9792\n",
      "Epoch [9/10], Step [35400/68337], Loss: 5.0129\n",
      "Epoch [9/10], Step [35475/68337], Loss: 5.0055\n",
      "Epoch [9/10], Step [35550/68337], Loss: 5.2002\n",
      "Epoch [9/10], Step [35625/68337], Loss: 5.1035\n",
      "Epoch [9/10], Step [35700/68337], Loss: 5.1844\n",
      "Epoch [9/10], Step [35775/68337], Loss: 4.9815\n",
      "Epoch [9/10], Step [35850/68337], Loss: 5.2021\n",
      "Epoch [9/10], Step [35925/68337], Loss: 5.1739\n",
      "Epoch [9/10], Step [36000/68337], Loss: 5.1479\n",
      "Epoch [9/10], Step [36075/68337], Loss: 4.9423\n",
      "Epoch [9/10], Step [36150/68337], Loss: 5.2886\n",
      "Epoch [9/10], Step [36225/68337], Loss: 5.0392\n",
      "Epoch [9/10], Step [36300/68337], Loss: 5.0528\n",
      "Epoch [9/10], Step [36375/68337], Loss: 5.0479\n",
      "Epoch [9/10], Step [36450/68337], Loss: 5.2436\n",
      "Epoch [9/10], Step [36525/68337], Loss: 5.0620\n",
      "Epoch [9/10], Step [36600/68337], Loss: 5.0536\n",
      "Epoch [9/10], Step [36675/68337], Loss: 4.9788\n",
      "Epoch [9/10], Step [36750/68337], Loss: 4.9691\n",
      "Epoch [9/10], Step [36825/68337], Loss: 5.1812\n",
      "Epoch [9/10], Step [36900/68337], Loss: 5.2095\n",
      "Epoch [9/10], Step [36975/68337], Loss: 4.9665\n",
      "Epoch [9/10], Step [37050/68337], Loss: 5.1482\n",
      "Epoch [9/10], Step [37125/68337], Loss: 5.0114\n",
      "Epoch [9/10], Step [37200/68337], Loss: 5.0112\n",
      "Epoch [9/10], Step [37275/68337], Loss: 5.0487\n",
      "Epoch [9/10], Step [37350/68337], Loss: 5.1474\n",
      "Epoch [9/10], Step [37425/68337], Loss: 5.3811\n",
      "Epoch [9/10], Step [37500/68337], Loss: 5.1002\n",
      "Epoch [9/10], Step [37575/68337], Loss: 5.2063\n",
      "Epoch [9/10], Step [37650/68337], Loss: 5.1945\n",
      "Epoch [9/10], Step [37725/68337], Loss: 5.1673\n",
      "Epoch [9/10], Step [37800/68337], Loss: 5.0043\n",
      "Epoch [9/10], Step [37875/68337], Loss: 5.1081\n",
      "Epoch [9/10], Step [37950/68337], Loss: 5.1078\n",
      "Epoch [9/10], Step [38025/68337], Loss: 5.1009\n",
      "Epoch [9/10], Step [38100/68337], Loss: 5.0047\n",
      "Epoch [9/10], Step [38175/68337], Loss: 5.0690\n",
      "Epoch [9/10], Step [38250/68337], Loss: 5.0928\n",
      "Epoch [9/10], Step [38325/68337], Loss: 5.0598\n",
      "Epoch [9/10], Step [38400/68337], Loss: 5.1814\n",
      "Epoch [9/10], Step [38475/68337], Loss: 5.0582\n",
      "Epoch [9/10], Step [38550/68337], Loss: 5.0349\n",
      "Epoch [9/10], Step [38625/68337], Loss: 4.8941\n",
      "Epoch [9/10], Step [38700/68337], Loss: 5.0552\n",
      "Epoch [9/10], Step [38775/68337], Loss: 5.0336\n",
      "Epoch [9/10], Step [38850/68337], Loss: 5.1321\n",
      "Epoch [9/10], Step [38925/68337], Loss: 5.1491\n",
      "Epoch [9/10], Step [39000/68337], Loss: 4.9066\n",
      "Epoch [9/10], Step [39075/68337], Loss: 4.9155\n",
      "Epoch [9/10], Step [39150/68337], Loss: 5.1102\n",
      "Epoch [9/10], Step [39225/68337], Loss: 5.0057\n",
      "Epoch [9/10], Step [39300/68337], Loss: 5.0555\n",
      "Epoch [9/10], Step [39375/68337], Loss: 5.2368\n",
      "Epoch [9/10], Step [39450/68337], Loss: 5.0052\n",
      "Epoch [9/10], Step [39525/68337], Loss: 5.1951\n",
      "Epoch [9/10], Step [39600/68337], Loss: 4.9662\n",
      "Epoch [9/10], Step [39675/68337], Loss: 5.1556\n",
      "Epoch [9/10], Step [39750/68337], Loss: 5.0494\n",
      "Epoch [9/10], Step [39825/68337], Loss: 5.1743\n",
      "Epoch [9/10], Step [39900/68337], Loss: 4.9453\n",
      "Epoch [9/10], Step [39975/68337], Loss: 4.9215\n",
      "Validation perplexity: 120.83524590263873\n",
      "Epoch [9/10], Step [40050/68337], Loss: 5.2098\n",
      "Epoch [9/10], Step [40125/68337], Loss: 4.9089\n",
      "Epoch [9/10], Step [40200/68337], Loss: 5.1308\n",
      "Epoch [9/10], Step [40275/68337], Loss: 5.0976\n",
      "Epoch [9/10], Step [40350/68337], Loss: 5.2148\n",
      "Epoch [9/10], Step [40425/68337], Loss: 5.0914\n",
      "Epoch [9/10], Step [40500/68337], Loss: 5.2429\n",
      "Epoch [9/10], Step [40575/68337], Loss: 5.0381\n",
      "Epoch [9/10], Step [40650/68337], Loss: 5.1101\n",
      "Epoch [9/10], Step [40725/68337], Loss: 4.9345\n",
      "Epoch [9/10], Step [40800/68337], Loss: 5.2558\n",
      "Epoch [9/10], Step [40875/68337], Loss: 5.1788\n",
      "Epoch [9/10], Step [40950/68337], Loss: 4.9190\n",
      "Epoch [9/10], Step [41025/68337], Loss: 5.0809\n",
      "Epoch [9/10], Step [41100/68337], Loss: 4.9194\n",
      "Epoch [9/10], Step [41175/68337], Loss: 5.0597\n",
      "Epoch [9/10], Step [41250/68337], Loss: 5.2709\n",
      "Epoch [9/10], Step [41325/68337], Loss: 5.1515\n",
      "Epoch [9/10], Step [41400/68337], Loss: 5.0988\n",
      "Epoch [9/10], Step [41475/68337], Loss: 5.2235\n",
      "Epoch [9/10], Step [41550/68337], Loss: 5.1116\n",
      "Epoch [9/10], Step [41625/68337], Loss: 5.0894\n",
      "Epoch [9/10], Step [41700/68337], Loss: 4.9214\n",
      "Epoch [9/10], Step [41775/68337], Loss: 5.1071\n",
      "Epoch [9/10], Step [41850/68337], Loss: 5.1028\n",
      "Epoch [9/10], Step [41925/68337], Loss: 5.1565\n",
      "Epoch [9/10], Step [42000/68337], Loss: 5.0333\n",
      "Epoch [9/10], Step [42075/68337], Loss: 5.0686\n",
      "Epoch [9/10], Step [42150/68337], Loss: 5.1807\n",
      "Epoch [9/10], Step [42225/68337], Loss: 5.1078\n",
      "Epoch [9/10], Step [42300/68337], Loss: 5.1128\n",
      "Epoch [9/10], Step [42375/68337], Loss: 5.1637\n",
      "Epoch [9/10], Step [42450/68337], Loss: 5.0929\n",
      "Epoch [9/10], Step [42525/68337], Loss: 5.2351\n",
      "Epoch [9/10], Step [42600/68337], Loss: 5.0857\n",
      "Epoch [9/10], Step [42675/68337], Loss: 5.2393\n",
      "Epoch [9/10], Step [42750/68337], Loss: 4.8647\n",
      "Epoch [9/10], Step [42825/68337], Loss: 4.9447\n",
      "Epoch [9/10], Step [42900/68337], Loss: 5.0685\n",
      "Epoch [9/10], Step [42975/68337], Loss: 5.1985\n",
      "Epoch [9/10], Step [43050/68337], Loss: 4.8433\n",
      "Epoch [9/10], Step [43125/68337], Loss: 5.1151\n",
      "Epoch [9/10], Step [43200/68337], Loss: 5.1411\n",
      "Epoch [9/10], Step [43275/68337], Loss: 5.2807\n",
      "Epoch [9/10], Step [43350/68337], Loss: 5.2573\n",
      "Epoch [9/10], Step [43425/68337], Loss: 5.3675\n",
      "Epoch [9/10], Step [43500/68337], Loss: 4.9704\n",
      "Epoch [9/10], Step [43575/68337], Loss: 4.9806\n",
      "Epoch [9/10], Step [43650/68337], Loss: 5.2598\n",
      "Epoch [9/10], Step [43725/68337], Loss: 5.1964\n",
      "Epoch [9/10], Step [43800/68337], Loss: 5.1197\n",
      "Epoch [9/10], Step [43875/68337], Loss: 5.1906\n",
      "Epoch [9/10], Step [43950/68337], Loss: 5.0847\n",
      "Epoch [9/10], Step [44025/68337], Loss: 4.9503\n",
      "Epoch [9/10], Step [44100/68337], Loss: 5.2445\n",
      "Epoch [9/10], Step [44175/68337], Loss: 4.8716\n",
      "Epoch [9/10], Step [44250/68337], Loss: 5.2146\n",
      "Epoch [9/10], Step [44325/68337], Loss: 5.2043\n",
      "Epoch [9/10], Step [44400/68337], Loss: 5.0393\n",
      "Epoch [9/10], Step [44475/68337], Loss: 5.0449\n",
      "Epoch [9/10], Step [44550/68337], Loss: 4.8998\n",
      "Epoch [9/10], Step [44625/68337], Loss: 5.3471\n",
      "Epoch [9/10], Step [44700/68337], Loss: 4.9544\n",
      "Epoch [9/10], Step [44775/68337], Loss: 5.0035\n",
      "Epoch [9/10], Step [44850/68337], Loss: 4.9549\n",
      "Epoch [9/10], Step [44925/68337], Loss: 5.1660\n",
      "Epoch [9/10], Step [45000/68337], Loss: 5.1235\n",
      "Epoch [9/10], Step [45075/68337], Loss: 5.0113\n",
      "Epoch [9/10], Step [45150/68337], Loss: 5.0064\n",
      "Epoch [9/10], Step [45225/68337], Loss: 4.8675\n",
      "Epoch [9/10], Step [45300/68337], Loss: 5.1288\n",
      "Epoch [9/10], Step [45375/68337], Loss: 4.7888\n",
      "Epoch [9/10], Step [45450/68337], Loss: 5.1937\n",
      "Epoch [9/10], Step [45525/68337], Loss: 5.1446\n",
      "Epoch [9/10], Step [45600/68337], Loss: 4.9529\n",
      "Epoch [9/10], Step [45675/68337], Loss: 4.8962\n",
      "Epoch [9/10], Step [45750/68337], Loss: 5.1909\n",
      "Epoch [9/10], Step [45825/68337], Loss: 5.3063\n",
      "Epoch [9/10], Step [45900/68337], Loss: 4.9064\n",
      "Epoch [9/10], Step [45975/68337], Loss: 5.1294\n",
      "Epoch [9/10], Step [46050/68337], Loss: 4.9231\n",
      "Epoch [9/10], Step [46125/68337], Loss: 5.0815\n",
      "Epoch [9/10], Step [46200/68337], Loss: 4.9935\n",
      "Epoch [9/10], Step [46275/68337], Loss: 5.1381\n",
      "Epoch [9/10], Step [46350/68337], Loss: 5.1069\n",
      "Epoch [9/10], Step [46425/68337], Loss: 5.3053\n",
      "Epoch [9/10], Step [46500/68337], Loss: 5.0331\n",
      "Epoch [9/10], Step [46575/68337], Loss: 5.2443\n",
      "Epoch [9/10], Step [46650/68337], Loss: 4.8565\n",
      "Epoch [9/10], Step [46725/68337], Loss: 4.8560\n",
      "Epoch [9/10], Step [46800/68337], Loss: 5.0219\n",
      "Epoch [9/10], Step [46875/68337], Loss: 5.2404\n",
      "Epoch [9/10], Step [46950/68337], Loss: 5.0863\n",
      "Epoch [9/10], Step [47025/68337], Loss: 5.2317\n",
      "Epoch [9/10], Step [47100/68337], Loss: 5.1645\n",
      "Epoch [9/10], Step [47175/68337], Loss: 5.1373\n",
      "Epoch [9/10], Step [47250/68337], Loss: 5.0926\n",
      "Epoch [9/10], Step [47325/68337], Loss: 4.9019\n",
      "Epoch [9/10], Step [47400/68337], Loss: 5.0608\n",
      "Epoch [9/10], Step [47475/68337], Loss: 5.2386\n",
      "Epoch [9/10], Step [47550/68337], Loss: 5.0644\n",
      "Epoch [9/10], Step [47625/68337], Loss: 5.0931\n",
      "Epoch [9/10], Step [47700/68337], Loss: 5.0268\n",
      "Epoch [9/10], Step [47775/68337], Loss: 4.9740\n",
      "Epoch [9/10], Step [47850/68337], Loss: 5.0793\n",
      "Epoch [9/10], Step [47925/68337], Loss: 4.9897\n",
      "Epoch [9/10], Step [48000/68337], Loss: 5.1902\n",
      "Epoch [9/10], Step [48075/68337], Loss: 5.1750\n",
      "Epoch [9/10], Step [48150/68337], Loss: 5.1076\n",
      "Epoch [9/10], Step [48225/68337], Loss: 4.9502\n",
      "Epoch [9/10], Step [48300/68337], Loss: 5.1282\n",
      "Epoch [9/10], Step [48375/68337], Loss: 4.9280\n",
      "Epoch [9/10], Step [48450/68337], Loss: 4.9359\n",
      "Epoch [9/10], Step [48525/68337], Loss: 5.1309\n",
      "Epoch [9/10], Step [48600/68337], Loss: 4.9684\n",
      "Epoch [9/10], Step [48675/68337], Loss: 4.9926\n",
      "Epoch [9/10], Step [48750/68337], Loss: 5.1507\n",
      "Epoch [9/10], Step [48825/68337], Loss: 5.2507\n",
      "Epoch [9/10], Step [48900/68337], Loss: 5.0795\n",
      "Epoch [9/10], Step [48975/68337], Loss: 5.0668\n",
      "Epoch [9/10], Step [49050/68337], Loss: 5.0713\n",
      "Epoch [9/10], Step [49125/68337], Loss: 5.0739\n",
      "Epoch [9/10], Step [49200/68337], Loss: 5.0828\n",
      "Epoch [9/10], Step [49275/68337], Loss: 5.1647\n",
      "Epoch [9/10], Step [49350/68337], Loss: 5.0266\n",
      "Epoch [9/10], Step [49425/68337], Loss: 5.1341\n",
      "Epoch [9/10], Step [49500/68337], Loss: 4.9186\n",
      "Epoch [9/10], Step [49575/68337], Loss: 5.1090\n",
      "Epoch [9/10], Step [49650/68337], Loss: 5.1508\n",
      "Epoch [9/10], Step [49725/68337], Loss: 5.1135\n",
      "Epoch [9/10], Step [49800/68337], Loss: 5.3503\n",
      "Epoch [9/10], Step [49875/68337], Loss: 4.9913\n",
      "Epoch [9/10], Step [49950/68337], Loss: 4.8128\n",
      "Validation perplexity: 120.37322252002409\n",
      "Epoch [9/10], Step [50025/68337], Loss: 5.0706\n",
      "Epoch [9/10], Step [50100/68337], Loss: 5.3227\n",
      "Epoch [9/10], Step [50175/68337], Loss: 5.1275\n",
      "Epoch [9/10], Step [50250/68337], Loss: 4.9912\n",
      "Epoch [9/10], Step [50325/68337], Loss: 4.9915\n",
      "Epoch [9/10], Step [50400/68337], Loss: 5.2019\n",
      "Epoch [9/10], Step [50475/68337], Loss: 5.2291\n",
      "Epoch [9/10], Step [50550/68337], Loss: 5.1213\n",
      "Epoch [9/10], Step [50625/68337], Loss: 5.1269\n",
      "Epoch [9/10], Step [50700/68337], Loss: 5.1835\n",
      "Epoch [9/10], Step [50775/68337], Loss: 4.8913\n",
      "Epoch [9/10], Step [50850/68337], Loss: 5.1013\n",
      "Epoch [9/10], Step [50925/68337], Loss: 5.3189\n",
      "Epoch [9/10], Step [51000/68337], Loss: 5.0403\n",
      "Epoch [9/10], Step [51075/68337], Loss: 5.0907\n",
      "Epoch [9/10], Step [51150/68337], Loss: 5.0384\n",
      "Epoch [9/10], Step [51225/68337], Loss: 5.0715\n",
      "Epoch [9/10], Step [51300/68337], Loss: 5.1182\n",
      "Epoch [9/10], Step [51375/68337], Loss: 5.1457\n",
      "Epoch [9/10], Step [51450/68337], Loss: 5.1475\n",
      "Epoch [9/10], Step [51525/68337], Loss: 5.1741\n",
      "Epoch [9/10], Step [51600/68337], Loss: 5.1505\n",
      "Epoch [9/10], Step [51675/68337], Loss: 4.9498\n",
      "Epoch [9/10], Step [51750/68337], Loss: 5.1708\n",
      "Epoch [9/10], Step [51825/68337], Loss: 5.0498\n",
      "Epoch [9/10], Step [51900/68337], Loss: 5.1845\n",
      "Epoch [9/10], Step [51975/68337], Loss: 5.2677\n",
      "Epoch [9/10], Step [52050/68337], Loss: 5.0926\n",
      "Epoch [9/10], Step [52125/68337], Loss: 4.9227\n",
      "Epoch [9/10], Step [52200/68337], Loss: 5.2200\n",
      "Epoch [9/10], Step [52275/68337], Loss: 5.0473\n",
      "Epoch [9/10], Step [52350/68337], Loss: 5.3483\n",
      "Epoch [9/10], Step [52425/68337], Loss: 5.0786\n",
      "Epoch [9/10], Step [52500/68337], Loss: 5.0899\n",
      "Epoch [9/10], Step [52575/68337], Loss: 5.0421\n",
      "Epoch [9/10], Step [52650/68337], Loss: 5.0816\n",
      "Epoch [9/10], Step [52725/68337], Loss: 5.2643\n",
      "Epoch [9/10], Step [52800/68337], Loss: 4.8435\n",
      "Epoch [9/10], Step [52875/68337], Loss: 5.0606\n",
      "Epoch [9/10], Step [52950/68337], Loss: 5.0406\n",
      "Epoch [9/10], Step [53025/68337], Loss: 5.1655\n",
      "Epoch [9/10], Step [53100/68337], Loss: 5.0204\n",
      "Epoch [9/10], Step [53175/68337], Loss: 4.8889\n",
      "Epoch [9/10], Step [53250/68337], Loss: 5.2384\n",
      "Epoch [9/10], Step [53325/68337], Loss: 4.9767\n",
      "Epoch [9/10], Step [53400/68337], Loss: 5.0795\n",
      "Epoch [9/10], Step [53475/68337], Loss: 5.0809\n",
      "Epoch [9/10], Step [53550/68337], Loss: 5.2433\n",
      "Epoch [9/10], Step [53625/68337], Loss: 4.9142\n",
      "Epoch [9/10], Step [53700/68337], Loss: 5.1712\n",
      "Epoch [9/10], Step [53775/68337], Loss: 5.1528\n",
      "Epoch [9/10], Step [53850/68337], Loss: 5.0624\n",
      "Epoch [9/10], Step [53925/68337], Loss: 4.9903\n",
      "Epoch [9/10], Step [54000/68337], Loss: 5.2497\n",
      "Epoch [9/10], Step [54075/68337], Loss: 5.1592\n",
      "Epoch [9/10], Step [54150/68337], Loss: 5.1667\n",
      "Epoch [9/10], Step [54225/68337], Loss: 5.1196\n",
      "Epoch [9/10], Step [54300/68337], Loss: 5.1832\n",
      "Epoch [9/10], Step [54375/68337], Loss: 4.7987\n",
      "Epoch [9/10], Step [54450/68337], Loss: 5.0596\n",
      "Epoch [9/10], Step [54525/68337], Loss: 5.0847\n",
      "Epoch [9/10], Step [54600/68337], Loss: 5.0708\n",
      "Epoch [9/10], Step [54675/68337], Loss: 5.0164\n",
      "Epoch [9/10], Step [54750/68337], Loss: 5.0431\n",
      "Epoch [9/10], Step [54825/68337], Loss: 5.1941\n",
      "Epoch [9/10], Step [54900/68337], Loss: 4.9830\n",
      "Epoch [9/10], Step [54975/68337], Loss: 5.0501\n",
      "Epoch [9/10], Step [55050/68337], Loss: 5.2280\n",
      "Epoch [9/10], Step [55125/68337], Loss: 5.0029\n",
      "Epoch [9/10], Step [55200/68337], Loss: 5.1156\n",
      "Epoch [9/10], Step [55275/68337], Loss: 4.9232\n",
      "Epoch [9/10], Step [55350/68337], Loss: 5.2377\n",
      "Epoch [9/10], Step [55425/68337], Loss: 4.9828\n",
      "Epoch [9/10], Step [55500/68337], Loss: 5.0348\n",
      "Epoch [9/10], Step [55575/68337], Loss: 4.8670\n",
      "Epoch [9/10], Step [55650/68337], Loss: 4.9354\n",
      "Epoch [9/10], Step [55725/68337], Loss: 5.3666\n",
      "Epoch [9/10], Step [55800/68337], Loss: 5.2071\n",
      "Epoch [9/10], Step [55875/68337], Loss: 5.1291\n",
      "Epoch [9/10], Step [55950/68337], Loss: 4.9606\n",
      "Epoch [9/10], Step [56025/68337], Loss: 5.1711\n",
      "Epoch [9/10], Step [56100/68337], Loss: 4.8660\n",
      "Epoch [9/10], Step [56175/68337], Loss: 5.0616\n",
      "Epoch [9/10], Step [56250/68337], Loss: 5.1348\n",
      "Epoch [9/10], Step [56325/68337], Loss: 5.1760\n",
      "Epoch [9/10], Step [56400/68337], Loss: 4.9710\n",
      "Epoch [9/10], Step [56475/68337], Loss: 4.9900\n",
      "Epoch [9/10], Step [56550/68337], Loss: 5.1696\n",
      "Epoch [9/10], Step [56625/68337], Loss: 5.1947\n",
      "Epoch [9/10], Step [56700/68337], Loss: 5.0752\n",
      "Epoch [9/10], Step [56775/68337], Loss: 5.0279\n",
      "Epoch [9/10], Step [56850/68337], Loss: 4.9775\n",
      "Epoch [9/10], Step [56925/68337], Loss: 4.9364\n",
      "Epoch [9/10], Step [57000/68337], Loss: 5.0433\n",
      "Epoch [9/10], Step [57075/68337], Loss: 5.2644\n",
      "Epoch [9/10], Step [57150/68337], Loss: 4.9788\n",
      "Epoch [9/10], Step [57225/68337], Loss: 5.1386\n",
      "Epoch [9/10], Step [57300/68337], Loss: 5.2665\n",
      "Epoch [9/10], Step [57375/68337], Loss: 4.9809\n",
      "Epoch [9/10], Step [57450/68337], Loss: 5.3677\n",
      "Epoch [9/10], Step [57525/68337], Loss: 5.1033\n",
      "Epoch [9/10], Step [57600/68337], Loss: 4.9320\n",
      "Epoch [9/10], Step [57675/68337], Loss: 5.1326\n",
      "Epoch [9/10], Step [57750/68337], Loss: 4.8151\n",
      "Epoch [9/10], Step [57825/68337], Loss: 5.2195\n",
      "Epoch [9/10], Step [57900/68337], Loss: 5.2970\n",
      "Epoch [9/10], Step [57975/68337], Loss: 5.1430\n",
      "Epoch [9/10], Step [58050/68337], Loss: 5.1582\n",
      "Epoch [9/10], Step [58125/68337], Loss: 5.0238\n",
      "Epoch [9/10], Step [58200/68337], Loss: 5.1577\n",
      "Epoch [9/10], Step [58275/68337], Loss: 5.0833\n",
      "Epoch [9/10], Step [58350/68337], Loss: 5.1880\n",
      "Epoch [9/10], Step [58425/68337], Loss: 5.1980\n",
      "Epoch [9/10], Step [58500/68337], Loss: 4.9886\n",
      "Epoch [9/10], Step [58575/68337], Loss: 5.1666\n",
      "Epoch [9/10], Step [58650/68337], Loss: 4.9627\n",
      "Epoch [9/10], Step [58725/68337], Loss: 5.1481\n",
      "Epoch [9/10], Step [58800/68337], Loss: 4.9807\n",
      "Epoch [9/10], Step [58875/68337], Loss: 5.1472\n",
      "Epoch [9/10], Step [58950/68337], Loss: 5.0956\n",
      "Epoch [9/10], Step [59025/68337], Loss: 5.1845\n",
      "Epoch [9/10], Step [59100/68337], Loss: 5.0581\n",
      "Epoch [9/10], Step [59175/68337], Loss: 4.9962\n",
      "Epoch [9/10], Step [59250/68337], Loss: 5.0171\n",
      "Epoch [9/10], Step [59325/68337], Loss: 4.9459\n",
      "Epoch [9/10], Step [59400/68337], Loss: 5.1791\n",
      "Epoch [9/10], Step [59475/68337], Loss: 5.0293\n",
      "Epoch [9/10], Step [59550/68337], Loss: 5.1075\n",
      "Epoch [9/10], Step [59625/68337], Loss: 4.9100\n",
      "Epoch [9/10], Step [59700/68337], Loss: 5.1259\n",
      "Epoch [9/10], Step [59775/68337], Loss: 4.9934\n",
      "Epoch [9/10], Step [59850/68337], Loss: 5.0202\n",
      "Epoch [9/10], Step [59925/68337], Loss: 4.9207\n",
      "Epoch [9/10], Step [60000/68337], Loss: 5.0204\n",
      "Validation perplexity: 120.92759639891635\n",
      "Epoch [9/10], Step [60075/68337], Loss: 5.1725\n",
      "Epoch [9/10], Step [60150/68337], Loss: 5.2162\n",
      "Epoch [9/10], Step [60225/68337], Loss: 4.9736\n",
      "Epoch [9/10], Step [60300/68337], Loss: 5.0879\n",
      "Epoch [9/10], Step [60375/68337], Loss: 5.1653\n",
      "Epoch [9/10], Step [60450/68337], Loss: 5.0332\n",
      "Epoch [9/10], Step [60525/68337], Loss: 5.0079\n",
      "Epoch [9/10], Step [60600/68337], Loss: 5.0581\n",
      "Epoch [9/10], Step [60675/68337], Loss: 5.1277\n",
      "Epoch [9/10], Step [60750/68337], Loss: 4.8604\n",
      "Epoch [9/10], Step [60825/68337], Loss: 5.0883\n",
      "Epoch [9/10], Step [60900/68337], Loss: 5.1383\n",
      "Epoch [9/10], Step [60975/68337], Loss: 5.1264\n",
      "Epoch [9/10], Step [61050/68337], Loss: 5.0135\n",
      "Epoch [9/10], Step [61125/68337], Loss: 5.1186\n",
      "Epoch [9/10], Step [61200/68337], Loss: 5.1589\n",
      "Epoch [9/10], Step [61275/68337], Loss: 5.2684\n",
      "Epoch [9/10], Step [61350/68337], Loss: 5.1108\n",
      "Epoch [9/10], Step [61425/68337], Loss: 4.8995\n",
      "Epoch [9/10], Step [61500/68337], Loss: 5.0969\n",
      "Epoch [9/10], Step [61575/68337], Loss: 5.0689\n",
      "Epoch [9/10], Step [61650/68337], Loss: 5.0454\n",
      "Epoch [9/10], Step [61725/68337], Loss: 4.9360\n",
      "Epoch [9/10], Step [61800/68337], Loss: 4.8965\n",
      "Epoch [9/10], Step [61875/68337], Loss: 5.0442\n",
      "Epoch [9/10], Step [61950/68337], Loss: 4.9491\n",
      "Epoch [9/10], Step [62025/68337], Loss: 5.1061\n",
      "Epoch [9/10], Step [62100/68337], Loss: 4.9712\n",
      "Epoch [9/10], Step [62175/68337], Loss: 5.0490\n",
      "Epoch [9/10], Step [62250/68337], Loss: 5.3760\n",
      "Epoch [9/10], Step [62325/68337], Loss: 5.0119\n",
      "Epoch [9/10], Step [62400/68337], Loss: 5.1238\n",
      "Epoch [9/10], Step [62475/68337], Loss: 5.1959\n",
      "Epoch [9/10], Step [62550/68337], Loss: 4.9593\n",
      "Epoch [9/10], Step [62625/68337], Loss: 5.1415\n",
      "Epoch [9/10], Step [62700/68337], Loss: 5.0955\n",
      "Epoch [9/10], Step [62775/68337], Loss: 5.1630\n",
      "Epoch [9/10], Step [62850/68337], Loss: 5.1531\n",
      "Epoch [9/10], Step [62925/68337], Loss: 5.1836\n",
      "Epoch [9/10], Step [63000/68337], Loss: 5.1184\n",
      "Epoch [9/10], Step [63075/68337], Loss: 5.0268\n",
      "Epoch [9/10], Step [63150/68337], Loss: 5.0142\n",
      "Epoch [9/10], Step [63225/68337], Loss: 5.2139\n",
      "Epoch [9/10], Step [63300/68337], Loss: 5.2650\n",
      "Epoch [9/10], Step [63375/68337], Loss: 5.0477\n",
      "Epoch [9/10], Step [63450/68337], Loss: 5.0382\n",
      "Epoch [9/10], Step [63525/68337], Loss: 5.3868\n",
      "Epoch [9/10], Step [63600/68337], Loss: 4.9838\n",
      "Epoch [9/10], Step [63675/68337], Loss: 5.3751\n",
      "Epoch [9/10], Step [63750/68337], Loss: 5.0183\n",
      "Epoch [9/10], Step [63825/68337], Loss: 5.2731\n",
      "Epoch [9/10], Step [63900/68337], Loss: 5.1673\n",
      "Epoch [9/10], Step [63975/68337], Loss: 4.9560\n",
      "Epoch [9/10], Step [64050/68337], Loss: 5.0439\n",
      "Epoch [9/10], Step [64125/68337], Loss: 5.2769\n",
      "Epoch [9/10], Step [64200/68337], Loss: 5.2502\n",
      "Epoch [9/10], Step [64275/68337], Loss: 4.9754\n",
      "Epoch [9/10], Step [64350/68337], Loss: 5.0624\n",
      "Epoch [9/10], Step [64425/68337], Loss: 5.0216\n",
      "Epoch [9/10], Step [64500/68337], Loss: 5.1925\n",
      "Epoch [9/10], Step [64575/68337], Loss: 5.2165\n",
      "Epoch [9/10], Step [64650/68337], Loss: 5.1811\n",
      "Epoch [9/10], Step [64725/68337], Loss: 5.0568\n",
      "Epoch [9/10], Step [64800/68337], Loss: 5.1594\n",
      "Epoch [9/10], Step [64875/68337], Loss: 5.0175\n",
      "Epoch [9/10], Step [64950/68337], Loss: 4.9628\n",
      "Epoch [9/10], Step [65025/68337], Loss: 5.1195\n",
      "Epoch [9/10], Step [65100/68337], Loss: 5.3418\n",
      "Epoch [9/10], Step [65175/68337], Loss: 5.1319\n",
      "Epoch [9/10], Step [65250/68337], Loss: 5.1739\n",
      "Epoch [9/10], Step [65325/68337], Loss: 5.1795\n",
      "Epoch [9/10], Step [65400/68337], Loss: 4.9133\n",
      "Epoch [9/10], Step [65475/68337], Loss: 5.0781\n",
      "Epoch [9/10], Step [65550/68337], Loss: 4.9372\n",
      "Epoch [9/10], Step [65625/68337], Loss: 5.1566\n",
      "Epoch [9/10], Step [65700/68337], Loss: 4.9506\n",
      "Epoch [9/10], Step [65775/68337], Loss: 5.1638\n",
      "Epoch [9/10], Step [65850/68337], Loss: 5.1727\n",
      "Epoch [9/10], Step [65925/68337], Loss: 5.1186\n",
      "Epoch [9/10], Step [66000/68337], Loss: 4.9854\n",
      "Epoch [9/10], Step [66075/68337], Loss: 5.1790\n",
      "Epoch [9/10], Step [66150/68337], Loss: 4.9017\n",
      "Epoch [9/10], Step [66225/68337], Loss: 5.3204\n",
      "Epoch [9/10], Step [66300/68337], Loss: 5.2731\n",
      "Epoch [9/10], Step [66375/68337], Loss: 5.0428\n",
      "Epoch [9/10], Step [66450/68337], Loss: 5.1333\n",
      "Epoch [9/10], Step [66525/68337], Loss: 4.9857\n",
      "Epoch [9/10], Step [66600/68337], Loss: 5.1309\n",
      "Epoch [9/10], Step [66675/68337], Loss: 5.0127\n",
      "Epoch [9/10], Step [66750/68337], Loss: 5.2987\n",
      "Epoch [9/10], Step [66825/68337], Loss: 5.0428\n",
      "Epoch [9/10], Step [66900/68337], Loss: 5.0620\n",
      "Epoch [9/10], Step [66975/68337], Loss: 5.2214\n",
      "Epoch [9/10], Step [67050/68337], Loss: 4.8503\n",
      "Epoch [9/10], Step [67125/68337], Loss: 4.9540\n",
      "Epoch [9/10], Step [67200/68337], Loss: 4.9642\n",
      "Epoch [9/10], Step [67275/68337], Loss: 4.9209\n",
      "Epoch [9/10], Step [67350/68337], Loss: 4.9367\n",
      "Epoch [9/10], Step [67425/68337], Loss: 5.2543\n",
      "Epoch [9/10], Step [67500/68337], Loss: 5.0664\n",
      "Epoch [9/10], Step [67575/68337], Loss: 5.0642\n",
      "Epoch [9/10], Step [67650/68337], Loss: 5.1097\n",
      "Epoch [9/10], Step [67725/68337], Loss: 5.0298\n",
      "Epoch [9/10], Step [67800/68337], Loss: 5.1832\n",
      "Epoch [9/10], Step [67875/68337], Loss: 5.2364\n",
      "Epoch [9/10], Step [67950/68337], Loss: 4.9000\n",
      "Epoch [9/10], Step [68025/68337], Loss: 4.9719\n",
      "Epoch [9/10], Step [68100/68337], Loss: 4.8258\n",
      "Epoch [9/10], Step [68175/68337], Loss: 4.9406\n",
      "Epoch [9/10], Step [68250/68337], Loss: 5.0338\n",
      "Epoch [9/10], Step [68325/68337], Loss: 5.0931\n",
      "Epoch [9/10] Average Loss: 5.0781, Perplexity: 160.46\n",
      "Epoch [10/10], Step [0/68337], Loss: 4.8875\n",
      "Validation perplexity: 120.06616962466425\n",
      "Epoch [10/10], Step [75/68337], Loss: 5.1303\n",
      "Epoch [10/10], Step [150/68337], Loss: 4.9756\n",
      "Epoch [10/10], Step [225/68337], Loss: 5.1611\n",
      "Epoch [10/10], Step [300/68337], Loss: 5.2536\n",
      "Epoch [10/10], Step [375/68337], Loss: 5.0321\n",
      "Epoch [10/10], Step [450/68337], Loss: 5.0938\n",
      "Epoch [10/10], Step [525/68337], Loss: 5.0679\n",
      "Epoch [10/10], Step [600/68337], Loss: 5.0911\n",
      "Epoch [10/10], Step [675/68337], Loss: 5.2701\n",
      "Epoch [10/10], Step [750/68337], Loss: 5.2455\n",
      "Epoch [10/10], Step [825/68337], Loss: 5.3832\n",
      "Epoch [10/10], Step [900/68337], Loss: 5.1997\n",
      "Epoch [10/10], Step [975/68337], Loss: 5.1036\n",
      "Epoch [10/10], Step [1050/68337], Loss: 5.0513\n",
      "Epoch [10/10], Step [1125/68337], Loss: 4.9144\n",
      "Epoch [10/10], Step [1200/68337], Loss: 5.0719\n",
      "Epoch [10/10], Step [1275/68337], Loss: 5.1225\n",
      "Epoch [10/10], Step [1350/68337], Loss: 5.1113\n",
      "Epoch [10/10], Step [1425/68337], Loss: 5.0494\n",
      "Epoch [10/10], Step [1500/68337], Loss: 5.2375\n",
      "Epoch [10/10], Step [1575/68337], Loss: 5.2604\n",
      "Epoch [10/10], Step [1650/68337], Loss: 5.1448\n",
      "Epoch [10/10], Step [1725/68337], Loss: 4.8495\n",
      "Epoch [10/10], Step [1800/68337], Loss: 5.1990\n",
      "Epoch [10/10], Step [1875/68337], Loss: 5.0047\n",
      "Epoch [10/10], Step [1950/68337], Loss: 5.1769\n",
      "Epoch [10/10], Step [2025/68337], Loss: 4.9293\n",
      "Epoch [10/10], Step [2100/68337], Loss: 5.0548\n",
      "Epoch [10/10], Step [2175/68337], Loss: 4.9639\n",
      "Epoch [10/10], Step [2250/68337], Loss: 5.0444\n",
      "Epoch [10/10], Step [2325/68337], Loss: 4.9459\n",
      "Epoch [10/10], Step [2400/68337], Loss: 5.2268\n",
      "Epoch [10/10], Step [2475/68337], Loss: 4.9746\n",
      "Epoch [10/10], Step [2550/68337], Loss: 5.0804\n",
      "Epoch [10/10], Step [2625/68337], Loss: 5.1809\n",
      "Epoch [10/10], Step [2700/68337], Loss: 5.3234\n",
      "Epoch [10/10], Step [2775/68337], Loss: 5.1437\n",
      "Epoch [10/10], Step [2850/68337], Loss: 5.0216\n",
      "Epoch [10/10], Step [2925/68337], Loss: 5.2226\n",
      "Epoch [10/10], Step [3000/68337], Loss: 4.9968\n",
      "Epoch [10/10], Step [3075/68337], Loss: 5.0269\n",
      "Epoch [10/10], Step [3150/68337], Loss: 5.0793\n",
      "Epoch [10/10], Step [3225/68337], Loss: 5.2314\n",
      "Epoch [10/10], Step [3300/68337], Loss: 5.1785\n",
      "Epoch [10/10], Step [3375/68337], Loss: 5.2171\n",
      "Epoch [10/10], Step [3450/68337], Loss: 5.0370\n",
      "Epoch [10/10], Step [3525/68337], Loss: 5.1804\n",
      "Epoch [10/10], Step [3600/68337], Loss: 4.9760\n",
      "Epoch [10/10], Step [3675/68337], Loss: 5.1659\n",
      "Epoch [10/10], Step [3750/68337], Loss: 4.9518\n",
      "Epoch [10/10], Step [3825/68337], Loss: 5.1854\n",
      "Epoch [10/10], Step [3900/68337], Loss: 4.7669\n",
      "Epoch [10/10], Step [3975/68337], Loss: 5.1451\n",
      "Epoch [10/10], Step [4050/68337], Loss: 5.1745\n",
      "Epoch [10/10], Step [4125/68337], Loss: 5.0936\n",
      "Epoch [10/10], Step [4200/68337], Loss: 5.1807\n",
      "Epoch [10/10], Step [4275/68337], Loss: 5.0199\n",
      "Epoch [10/10], Step [4350/68337], Loss: 5.2147\n",
      "Epoch [10/10], Step [4425/68337], Loss: 5.2182\n",
      "Epoch [10/10], Step [4500/68337], Loss: 5.1012\n",
      "Epoch [10/10], Step [4575/68337], Loss: 5.2927\n",
      "Epoch [10/10], Step [4650/68337], Loss: 5.0024\n",
      "Epoch [10/10], Step [4725/68337], Loss: 5.2649\n",
      "Epoch [10/10], Step [4800/68337], Loss: 5.0747\n",
      "Epoch [10/10], Step [4875/68337], Loss: 5.0750\n",
      "Epoch [10/10], Step [4950/68337], Loss: 5.3637\n",
      "Epoch [10/10], Step [5025/68337], Loss: 5.2015\n",
      "Epoch [10/10], Step [5100/68337], Loss: 5.0708\n",
      "Epoch [10/10], Step [5175/68337], Loss: 5.1348\n",
      "Epoch [10/10], Step [5250/68337], Loss: 4.9935\n",
      "Epoch [10/10], Step [5325/68337], Loss: 5.1104\n",
      "Epoch [10/10], Step [5400/68337], Loss: 4.9985\n",
      "Epoch [10/10], Step [5475/68337], Loss: 5.0806\n",
      "Epoch [10/10], Step [5550/68337], Loss: 5.1459\n",
      "Epoch [10/10], Step [5625/68337], Loss: 4.9500\n",
      "Epoch [10/10], Step [5700/68337], Loss: 4.8735\n",
      "Epoch [10/10], Step [5775/68337], Loss: 4.8286\n",
      "Epoch [10/10], Step [5850/68337], Loss: 5.1462\n",
      "Epoch [10/10], Step [5925/68337], Loss: 5.3072\n",
      "Epoch [10/10], Step [6000/68337], Loss: 5.0561\n",
      "Epoch [10/10], Step [6075/68337], Loss: 5.1492\n",
      "Epoch [10/10], Step [6150/68337], Loss: 5.0358\n",
      "Epoch [10/10], Step [6225/68337], Loss: 5.0338\n",
      "Epoch [10/10], Step [6300/68337], Loss: 5.1922\n",
      "Epoch [10/10], Step [6375/68337], Loss: 5.1417\n",
      "Epoch [10/10], Step [6450/68337], Loss: 4.9675\n",
      "Epoch [10/10], Step [6525/68337], Loss: 5.0324\n",
      "Epoch [10/10], Step [6600/68337], Loss: 5.1823\n",
      "Epoch [10/10], Step [6675/68337], Loss: 4.9937\n",
      "Epoch [10/10], Step [6750/68337], Loss: 5.0548\n",
      "Epoch [10/10], Step [6825/68337], Loss: 4.9174\n",
      "Epoch [10/10], Step [6900/68337], Loss: 4.9610\n",
      "Epoch [10/10], Step [6975/68337], Loss: 5.1217\n",
      "Epoch [10/10], Step [7050/68337], Loss: 5.0454\n",
      "Epoch [10/10], Step [7125/68337], Loss: 4.9226\n",
      "Epoch [10/10], Step [7200/68337], Loss: 5.1558\n",
      "Epoch [10/10], Step [7275/68337], Loss: 5.4114\n",
      "Epoch [10/10], Step [7350/68337], Loss: 5.0095\n",
      "Epoch [10/10], Step [7425/68337], Loss: 4.9593\n",
      "Epoch [10/10], Step [7500/68337], Loss: 5.2618\n",
      "Epoch [10/10], Step [7575/68337], Loss: 4.9955\n",
      "Epoch [10/10], Step [7650/68337], Loss: 5.0521\n",
      "Epoch [10/10], Step [7725/68337], Loss: 5.1815\n",
      "Epoch [10/10], Step [7800/68337], Loss: 4.9983\n",
      "Epoch [10/10], Step [7875/68337], Loss: 4.8643\n",
      "Epoch [10/10], Step [7950/68337], Loss: 5.2258\n",
      "Epoch [10/10], Step [8025/68337], Loss: 4.9888\n",
      "Epoch [10/10], Step [8100/68337], Loss: 5.1449\n",
      "Epoch [10/10], Step [8175/68337], Loss: 4.9551\n",
      "Epoch [10/10], Step [8250/68337], Loss: 4.9360\n",
      "Epoch [10/10], Step [8325/68337], Loss: 5.1261\n",
      "Epoch [10/10], Step [8400/68337], Loss: 5.0205\n",
      "Epoch [10/10], Step [8475/68337], Loss: 5.0623\n",
      "Epoch [10/10], Step [8550/68337], Loss: 5.1680\n",
      "Epoch [10/10], Step [8625/68337], Loss: 5.1210\n",
      "Epoch [10/10], Step [8700/68337], Loss: 5.0909\n",
      "Epoch [10/10], Step [8775/68337], Loss: 4.9835\n",
      "Epoch [10/10], Step [8850/68337], Loss: 5.0654\n",
      "Epoch [10/10], Step [8925/68337], Loss: 5.4831\n",
      "Epoch [10/10], Step [9000/68337], Loss: 5.2334\n",
      "Epoch [10/10], Step [9075/68337], Loss: 5.2501\n",
      "Epoch [10/10], Step [9150/68337], Loss: 5.1647\n",
      "Epoch [10/10], Step [9225/68337], Loss: 5.1262\n",
      "Epoch [10/10], Step [9300/68337], Loss: 5.0085\n",
      "Epoch [10/10], Step [9375/68337], Loss: 4.9792\n",
      "Epoch [10/10], Step [9450/68337], Loss: 5.0387\n",
      "Epoch [10/10], Step [9525/68337], Loss: 5.1141\n",
      "Epoch [10/10], Step [9600/68337], Loss: 5.1407\n",
      "Epoch [10/10], Step [9675/68337], Loss: 5.0851\n",
      "Epoch [10/10], Step [9750/68337], Loss: 5.1656\n",
      "Epoch [10/10], Step [9825/68337], Loss: 5.0258\n",
      "Epoch [10/10], Step [9900/68337], Loss: 5.2531\n",
      "Epoch [10/10], Step [9975/68337], Loss: 5.0958\n",
      "Validation perplexity: 120.57484551748394\n",
      "Epoch [10/10], Step [10050/68337], Loss: 5.1548\n",
      "Epoch [10/10], Step [10125/68337], Loss: 4.8832\n",
      "Epoch [10/10], Step [10200/68337], Loss: 5.0658\n",
      "Epoch [10/10], Step [10275/68337], Loss: 4.9295\n",
      "Epoch [10/10], Step [10350/68337], Loss: 5.1851\n",
      "Epoch [10/10], Step [10425/68337], Loss: 5.1804\n",
      "Epoch [10/10], Step [10500/68337], Loss: 5.1551\n",
      "Epoch [10/10], Step [10575/68337], Loss: 4.9359\n",
      "Epoch [10/10], Step [10650/68337], Loss: 4.8955\n",
      "Epoch [10/10], Step [10725/68337], Loss: 5.0836\n",
      "Epoch [10/10], Step [10800/68337], Loss: 5.1410\n",
      "Epoch [10/10], Step [10875/68337], Loss: 5.2067\n",
      "Epoch [10/10], Step [10950/68337], Loss: 5.0675\n",
      "Epoch [10/10], Step [11025/68337], Loss: 5.0990\n",
      "Epoch [10/10], Step [11100/68337], Loss: 5.3094\n",
      "Epoch [10/10], Step [11175/68337], Loss: 4.8397\n",
      "Epoch [10/10], Step [11250/68337], Loss: 4.8177\n",
      "Epoch [10/10], Step [11325/68337], Loss: 5.0560\n",
      "Epoch [10/10], Step [11400/68337], Loss: 5.0914\n",
      "Epoch [10/10], Step [11475/68337], Loss: 5.0088\n",
      "Epoch [10/10], Step [11550/68337], Loss: 5.0312\n",
      "Epoch [10/10], Step [11625/68337], Loss: 4.7183\n",
      "Epoch [10/10], Step [11700/68337], Loss: 5.1324\n",
      "Epoch [10/10], Step [11775/68337], Loss: 4.8609\n",
      "Epoch [10/10], Step [11850/68337], Loss: 5.0757\n",
      "Epoch [10/10], Step [11925/68337], Loss: 4.9851\n",
      "Epoch [10/10], Step [12000/68337], Loss: 5.0763\n",
      "Epoch [10/10], Step [12075/68337], Loss: 5.0027\n",
      "Epoch [10/10], Step [12150/68337], Loss: 4.9889\n",
      "Epoch [10/10], Step [12225/68337], Loss: 5.2245\n",
      "Epoch [10/10], Step [12300/68337], Loss: 5.0823\n",
      "Epoch [10/10], Step [12375/68337], Loss: 5.1417\n",
      "Epoch [10/10], Step [12450/68337], Loss: 5.1300\n",
      "Epoch [10/10], Step [12525/68337], Loss: 4.9948\n",
      "Epoch [10/10], Step [12600/68337], Loss: 4.7337\n",
      "Epoch [10/10], Step [12675/68337], Loss: 4.8276\n",
      "Epoch [10/10], Step [12750/68337], Loss: 5.1049\n",
      "Epoch [10/10], Step [12825/68337], Loss: 5.1554\n",
      "Epoch [10/10], Step [12900/68337], Loss: 5.2887\n",
      "Epoch [10/10], Step [12975/68337], Loss: 5.1320\n",
      "Epoch [10/10], Step [13050/68337], Loss: 5.1174\n",
      "Epoch [10/10], Step [13125/68337], Loss: 5.1053\n",
      "Epoch [10/10], Step [13200/68337], Loss: 5.2550\n",
      "Epoch [10/10], Step [13275/68337], Loss: 5.0151\n",
      "Epoch [10/10], Step [13350/68337], Loss: 4.9558\n",
      "Epoch [10/10], Step [13425/68337], Loss: 5.1045\n",
      "Epoch [10/10], Step [13500/68337], Loss: 4.9404\n",
      "Epoch [10/10], Step [13575/68337], Loss: 5.0970\n",
      "Epoch [10/10], Step [13650/68337], Loss: 5.2446\n",
      "Epoch [10/10], Step [13725/68337], Loss: 5.0553\n",
      "Epoch [10/10], Step [13800/68337], Loss: 4.9001\n",
      "Epoch [10/10], Step [13875/68337], Loss: 4.9897\n",
      "Epoch [10/10], Step [13950/68337], Loss: 5.0607\n",
      "Epoch [10/10], Step [14025/68337], Loss: 5.2103\n",
      "Epoch [10/10], Step [14100/68337], Loss: 4.9137\n",
      "Epoch [10/10], Step [14175/68337], Loss: 5.1117\n",
      "Epoch [10/10], Step [14250/68337], Loss: 5.3806\n",
      "Epoch [10/10], Step [14325/68337], Loss: 5.0537\n",
      "Epoch [10/10], Step [14400/68337], Loss: 5.3020\n",
      "Epoch [10/10], Step [14475/68337], Loss: 4.9609\n",
      "Epoch [10/10], Step [14550/68337], Loss: 4.8029\n",
      "Epoch [10/10], Step [14625/68337], Loss: 5.1234\n",
      "Epoch [10/10], Step [14700/68337], Loss: 5.0846\n",
      "Epoch [10/10], Step [14775/68337], Loss: 5.3206\n",
      "Epoch [10/10], Step [14850/68337], Loss: 5.1065\n",
      "Epoch [10/10], Step [14925/68337], Loss: 4.9917\n",
      "Epoch [10/10], Step [15000/68337], Loss: 5.0508\n",
      "Epoch [10/10], Step [15075/68337], Loss: 5.1646\n",
      "Epoch [10/10], Step [15150/68337], Loss: 5.1227\n",
      "Epoch [10/10], Step [15225/68337], Loss: 4.9124\n",
      "Epoch [10/10], Step [15300/68337], Loss: 5.1199\n",
      "Epoch [10/10], Step [15375/68337], Loss: 5.0300\n",
      "Epoch [10/10], Step [15450/68337], Loss: 5.2573\n",
      "Epoch [10/10], Step [15525/68337], Loss: 5.1478\n",
      "Epoch [10/10], Step [15600/68337], Loss: 4.9312\n",
      "Epoch [10/10], Step [15675/68337], Loss: 5.0751\n",
      "Epoch [10/10], Step [15750/68337], Loss: 4.9990\n",
      "Epoch [10/10], Step [15825/68337], Loss: 4.9581\n",
      "Epoch [10/10], Step [15900/68337], Loss: 4.8624\n",
      "Epoch [10/10], Step [15975/68337], Loss: 5.2295\n",
      "Epoch [10/10], Step [16050/68337], Loss: 5.0478\n",
      "Epoch [10/10], Step [16125/68337], Loss: 5.0620\n",
      "Epoch [10/10], Step [16200/68337], Loss: 5.1000\n",
      "Epoch [10/10], Step [16275/68337], Loss: 4.9401\n",
      "Epoch [10/10], Step [16350/68337], Loss: 5.1388\n",
      "Epoch [10/10], Step [16425/68337], Loss: 5.0710\n",
      "Epoch [10/10], Step [16500/68337], Loss: 5.0445\n",
      "Epoch [10/10], Step [16575/68337], Loss: 5.2118\n",
      "Epoch [10/10], Step [16650/68337], Loss: 5.0912\n",
      "Epoch [10/10], Step [16725/68337], Loss: 5.0100\n",
      "Epoch [10/10], Step [16800/68337], Loss: 5.2484\n",
      "Epoch [10/10], Step [16875/68337], Loss: 5.2961\n",
      "Epoch [10/10], Step [16950/68337], Loss: 5.2534\n",
      "Epoch [10/10], Step [17025/68337], Loss: 5.1033\n",
      "Epoch [10/10], Step [17100/68337], Loss: 4.9556\n",
      "Epoch [10/10], Step [17175/68337], Loss: 4.9966\n",
      "Epoch [10/10], Step [17250/68337], Loss: 4.9947\n",
      "Epoch [10/10], Step [17325/68337], Loss: 5.1545\n",
      "Epoch [10/10], Step [17400/68337], Loss: 5.1138\n",
      "Epoch [10/10], Step [17475/68337], Loss: 4.9192\n",
      "Epoch [10/10], Step [17550/68337], Loss: 4.9768\n",
      "Epoch [10/10], Step [17625/68337], Loss: 5.1656\n",
      "Epoch [10/10], Step [17700/68337], Loss: 5.2881\n",
      "Epoch [10/10], Step [17775/68337], Loss: 5.2489\n",
      "Epoch [10/10], Step [17850/68337], Loss: 4.9704\n",
      "Epoch [10/10], Step [17925/68337], Loss: 5.1909\n",
      "Epoch [10/10], Step [18000/68337], Loss: 5.0675\n",
      "Epoch [10/10], Step [18075/68337], Loss: 5.1394\n",
      "Epoch [10/10], Step [18150/68337], Loss: 4.9789\n",
      "Epoch [10/10], Step [18225/68337], Loss: 5.1972\n",
      "Epoch [10/10], Step [18300/68337], Loss: 5.0878\n",
      "Epoch [10/10], Step [18375/68337], Loss: 5.1484\n",
      "Epoch [10/10], Step [18450/68337], Loss: 5.2082\n",
      "Epoch [10/10], Step [18525/68337], Loss: 5.2736\n",
      "Epoch [10/10], Step [18600/68337], Loss: 5.1431\n",
      "Epoch [10/10], Step [18675/68337], Loss: 4.8415\n",
      "Epoch [10/10], Step [18750/68337], Loss: 5.0039\n",
      "Epoch [10/10], Step [18825/68337], Loss: 5.2119\n",
      "Epoch [10/10], Step [18900/68337], Loss: 4.8987\n",
      "Epoch [10/10], Step [18975/68337], Loss: 4.8863\n",
      "Epoch [10/10], Step [19050/68337], Loss: 5.1066\n",
      "Epoch [10/10], Step [19125/68337], Loss: 5.0542\n",
      "Epoch [10/10], Step [19200/68337], Loss: 5.1060\n",
      "Epoch [10/10], Step [19275/68337], Loss: 5.2272\n",
      "Epoch [10/10], Step [19350/68337], Loss: 5.2390\n",
      "Epoch [10/10], Step [19425/68337], Loss: 4.9253\n",
      "Epoch [10/10], Step [19500/68337], Loss: 5.2156\n",
      "Epoch [10/10], Step [19575/68337], Loss: 5.1564\n",
      "Epoch [10/10], Step [19650/68337], Loss: 5.0260\n",
      "Epoch [10/10], Step [19725/68337], Loss: 5.1460\n",
      "Epoch [10/10], Step [19800/68337], Loss: 4.9284\n",
      "Epoch [10/10], Step [19875/68337], Loss: 5.2321\n",
      "Epoch [10/10], Step [19950/68337], Loss: 5.2635\n",
      "Validation perplexity: 119.89402440176578\n",
      "Epoch [10/10], Step [20025/68337], Loss: 4.9900\n",
      "Epoch [10/10], Step [20100/68337], Loss: 5.2839\n",
      "Epoch [10/10], Step [20175/68337], Loss: 5.0660\n",
      "Epoch [10/10], Step [20250/68337], Loss: 5.1801\n",
      "Epoch [10/10], Step [20325/68337], Loss: 5.0633\n",
      "Epoch [10/10], Step [20400/68337], Loss: 5.0811\n",
      "Epoch [10/10], Step [20475/68337], Loss: 4.9661\n",
      "Epoch [10/10], Step [20550/68337], Loss: 5.0601\n",
      "Epoch [10/10], Step [20625/68337], Loss: 5.0152\n",
      "Epoch [10/10], Step [20700/68337], Loss: 5.0581\n",
      "Epoch [10/10], Step [20775/68337], Loss: 4.8506\n",
      "Epoch [10/10], Step [20850/68337], Loss: 5.0755\n",
      "Epoch [10/10], Step [20925/68337], Loss: 5.0359\n",
      "Epoch [10/10], Step [21000/68337], Loss: 5.1816\n",
      "Epoch [10/10], Step [21075/68337], Loss: 4.8781\n",
      "Epoch [10/10], Step [21150/68337], Loss: 5.0758\n",
      "Epoch [10/10], Step [21225/68337], Loss: 5.0508\n",
      "Epoch [10/10], Step [21300/68337], Loss: 5.0938\n",
      "Epoch [10/10], Step [21375/68337], Loss: 5.3092\n",
      "Epoch [10/10], Step [21450/68337], Loss: 5.1104\n",
      "Epoch [10/10], Step [21525/68337], Loss: 5.1594\n",
      "Epoch [10/10], Step [21600/68337], Loss: 5.1740\n",
      "Epoch [10/10], Step [21675/68337], Loss: 5.1727\n",
      "Epoch [10/10], Step [21750/68337], Loss: 5.1145\n",
      "Epoch [10/10], Step [21825/68337], Loss: 5.1146\n",
      "Epoch [10/10], Step [21900/68337], Loss: 4.9585\n",
      "Epoch [10/10], Step [21975/68337], Loss: 5.1651\n",
      "Epoch [10/10], Step [22050/68337], Loss: 5.0811\n",
      "Epoch [10/10], Step [22125/68337], Loss: 5.1831\n",
      "Epoch [10/10], Step [22200/68337], Loss: 4.8343\n",
      "Epoch [10/10], Step [22275/68337], Loss: 4.9186\n",
      "Epoch [10/10], Step [22350/68337], Loss: 4.9372\n",
      "Epoch [10/10], Step [22425/68337], Loss: 5.1833\n",
      "Epoch [10/10], Step [22500/68337], Loss: 5.0730\n",
      "Epoch [10/10], Step [22575/68337], Loss: 4.8788\n",
      "Epoch [10/10], Step [22650/68337], Loss: 4.9420\n",
      "Epoch [10/10], Step [22725/68337], Loss: 5.1254\n",
      "Epoch [10/10], Step [22800/68337], Loss: 5.0783\n",
      "Epoch [10/10], Step [22875/68337], Loss: 5.0221\n",
      "Epoch [10/10], Step [22950/68337], Loss: 5.2277\n",
      "Epoch [10/10], Step [23025/68337], Loss: 5.3556\n",
      "Epoch [10/10], Step [23100/68337], Loss: 5.0852\n",
      "Epoch [10/10], Step [23175/68337], Loss: 5.1686\n",
      "Epoch [10/10], Step [23250/68337], Loss: 5.1574\n",
      "Epoch [10/10], Step [23325/68337], Loss: 4.9604\n",
      "Epoch [10/10], Step [23400/68337], Loss: 4.9897\n",
      "Epoch [10/10], Step [23475/68337], Loss: 5.1758\n",
      "Epoch [10/10], Step [23550/68337], Loss: 5.0613\n",
      "Epoch [10/10], Step [23625/68337], Loss: 5.1762\n",
      "Epoch [10/10], Step [23700/68337], Loss: 5.1499\n",
      "Epoch [10/10], Step [23775/68337], Loss: 5.2032\n",
      "Epoch [10/10], Step [23850/68337], Loss: 4.8695\n",
      "Epoch [10/10], Step [23925/68337], Loss: 5.1370\n",
      "Epoch [10/10], Step [24000/68337], Loss: 5.2091\n",
      "Epoch [10/10], Step [24075/68337], Loss: 5.2667\n",
      "Epoch [10/10], Step [24150/68337], Loss: 5.0160\n",
      "Epoch [10/10], Step [24225/68337], Loss: 4.9592\n",
      "Epoch [10/10], Step [24300/68337], Loss: 5.0146\n",
      "Epoch [10/10], Step [24375/68337], Loss: 5.1741\n",
      "Epoch [10/10], Step [24450/68337], Loss: 5.0858\n",
      "Epoch [10/10], Step [24525/68337], Loss: 5.1472\n",
      "Epoch [10/10], Step [24600/68337], Loss: 4.9960\n",
      "Epoch [10/10], Step [24675/68337], Loss: 4.9462\n",
      "Epoch [10/10], Step [24750/68337], Loss: 5.1767\n",
      "Epoch [10/10], Step [24825/68337], Loss: 5.0232\n",
      "Epoch [10/10], Step [24900/68337], Loss: 4.9930\n",
      "Epoch [10/10], Step [24975/68337], Loss: 5.0252\n",
      "Epoch [10/10], Step [25050/68337], Loss: 5.2640\n",
      "Epoch [10/10], Step [25125/68337], Loss: 4.8390\n",
      "Epoch [10/10], Step [25200/68337], Loss: 5.1301\n",
      "Epoch [10/10], Step [25275/68337], Loss: 4.9336\n",
      "Epoch [10/10], Step [25350/68337], Loss: 5.0699\n",
      "Epoch [10/10], Step [25425/68337], Loss: 5.0907\n",
      "Epoch [10/10], Step [25500/68337], Loss: 5.0699\n",
      "Epoch [10/10], Step [25575/68337], Loss: 5.0019\n",
      "Epoch [10/10], Step [25650/68337], Loss: 5.1736\n",
      "Epoch [10/10], Step [25725/68337], Loss: 5.0520\n",
      "Epoch [10/10], Step [25800/68337], Loss: 5.1251\n",
      "Epoch [10/10], Step [25875/68337], Loss: 5.0750\n",
      "Epoch [10/10], Step [25950/68337], Loss: 5.0621\n",
      "Epoch [10/10], Step [26025/68337], Loss: 5.0898\n",
      "Epoch [10/10], Step [26100/68337], Loss: 4.9859\n",
      "Epoch [10/10], Step [26175/68337], Loss: 5.3010\n",
      "Epoch [10/10], Step [26250/68337], Loss: 4.9968\n",
      "Epoch [10/10], Step [26325/68337], Loss: 5.0430\n",
      "Epoch [10/10], Step [26400/68337], Loss: 4.9704\n",
      "Epoch [10/10], Step [26475/68337], Loss: 5.0287\n",
      "Epoch [10/10], Step [26550/68337], Loss: 4.8742\n",
      "Epoch [10/10], Step [26625/68337], Loss: 4.9476\n",
      "Epoch [10/10], Step [26700/68337], Loss: 5.0807\n",
      "Epoch [10/10], Step [26775/68337], Loss: 4.8926\n",
      "Epoch [10/10], Step [26850/68337], Loss: 4.9716\n",
      "Epoch [10/10], Step [26925/68337], Loss: 5.0132\n",
      "Epoch [10/10], Step [27000/68337], Loss: 5.0394\n",
      "Epoch [10/10], Step [27075/68337], Loss: 5.0498\n",
      "Epoch [10/10], Step [27150/68337], Loss: 5.0948\n",
      "Epoch [10/10], Step [27225/68337], Loss: 5.0611\n",
      "Epoch [10/10], Step [27300/68337], Loss: 5.0047\n",
      "Epoch [10/10], Step [27375/68337], Loss: 5.1719\n",
      "Epoch [10/10], Step [27450/68337], Loss: 5.1027\n",
      "Epoch [10/10], Step [27525/68337], Loss: 5.2863\n",
      "Epoch [10/10], Step [27600/68337], Loss: 5.0765\n",
      "Epoch [10/10], Step [27675/68337], Loss: 5.0323\n",
      "Epoch [10/10], Step [27750/68337], Loss: 4.9920\n",
      "Epoch [10/10], Step [27825/68337], Loss: 5.0188\n",
      "Epoch [10/10], Step [27900/68337], Loss: 5.3398\n",
      "Epoch [10/10], Step [27975/68337], Loss: 5.0387\n",
      "Epoch [10/10], Step [28050/68337], Loss: 4.9881\n",
      "Epoch [10/10], Step [28125/68337], Loss: 5.0145\n",
      "Epoch [10/10], Step [28200/68337], Loss: 5.0121\n",
      "Epoch [10/10], Step [28275/68337], Loss: 4.9353\n",
      "Epoch [10/10], Step [28350/68337], Loss: 5.0924\n",
      "Epoch [10/10], Step [28425/68337], Loss: 5.0452\n",
      "Epoch [10/10], Step [28500/68337], Loss: 5.2190\n",
      "Epoch [10/10], Step [28575/68337], Loss: 4.9786\n",
      "Epoch [10/10], Step [28650/68337], Loss: 5.0638\n",
      "Epoch [10/10], Step [28725/68337], Loss: 5.1452\n",
      "Epoch [10/10], Step [28800/68337], Loss: 4.9957\n",
      "Epoch [10/10], Step [28875/68337], Loss: 4.9562\n",
      "Epoch [10/10], Step [28950/68337], Loss: 5.2076\n",
      "Epoch [10/10], Step [29025/68337], Loss: 5.3005\n",
      "Epoch [10/10], Step [29100/68337], Loss: 5.1805\n",
      "Epoch [10/10], Step [29175/68337], Loss: 5.2599\n",
      "Epoch [10/10], Step [29250/68337], Loss: 5.2745\n",
      "Epoch [10/10], Step [29325/68337], Loss: 5.1485\n",
      "Epoch [10/10], Step [29400/68337], Loss: 5.0574\n",
      "Epoch [10/10], Step [29475/68337], Loss: 5.1768\n",
      "Epoch [10/10], Step [29550/68337], Loss: 4.9773\n",
      "Epoch [10/10], Step [29625/68337], Loss: 5.1150\n",
      "Epoch [10/10], Step [29700/68337], Loss: 5.1823\n",
      "Epoch [10/10], Step [29775/68337], Loss: 5.2565\n",
      "Epoch [10/10], Step [29850/68337], Loss: 5.1232\n",
      "Epoch [10/10], Step [29925/68337], Loss: 5.1988\n",
      "Epoch [10/10], Step [30000/68337], Loss: 4.9541\n",
      "Validation perplexity: 119.9652332947973\n",
      "Epoch [10/10], Step [30075/68337], Loss: 5.0165\n",
      "Epoch [10/10], Step [30150/68337], Loss: 5.0291\n",
      "Epoch [10/10], Step [30225/68337], Loss: 5.1388\n",
      "Epoch [10/10], Step [30300/68337], Loss: 5.2670\n",
      "Epoch [10/10], Step [30375/68337], Loss: 5.0541\n",
      "Epoch [10/10], Step [30450/68337], Loss: 5.0543\n",
      "Epoch [10/10], Step [30525/68337], Loss: 5.0495\n",
      "Epoch [10/10], Step [30600/68337], Loss: 5.1024\n",
      "Epoch [10/10], Step [30675/68337], Loss: 4.7737\n",
      "Epoch [10/10], Step [30750/68337], Loss: 5.0613\n",
      "Epoch [10/10], Step [30825/68337], Loss: 5.0812\n",
      "Epoch [10/10], Step [30900/68337], Loss: 4.9326\n",
      "Epoch [10/10], Step [30975/68337], Loss: 5.1073\n",
      "Epoch [10/10], Step [31050/68337], Loss: 5.2129\n",
      "Epoch [10/10], Step [31125/68337], Loss: 5.2878\n",
      "Epoch [10/10], Step [31200/68337], Loss: 5.0781\n",
      "Epoch [10/10], Step [31275/68337], Loss: 4.9747\n",
      "Epoch [10/10], Step [31350/68337], Loss: 5.0681\n",
      "Epoch [10/10], Step [31425/68337], Loss: 5.1568\n",
      "Epoch [10/10], Step [31500/68337], Loss: 5.0514\n",
      "Epoch [10/10], Step [31575/68337], Loss: 5.0591\n",
      "Epoch [10/10], Step [31650/68337], Loss: 5.0248\n",
      "Epoch [10/10], Step [31725/68337], Loss: 5.1072\n",
      "Epoch [10/10], Step [31800/68337], Loss: 5.1496\n",
      "Epoch [10/10], Step [31875/68337], Loss: 4.9437\n",
      "Epoch [10/10], Step [31950/68337], Loss: 5.0167\n",
      "Epoch [10/10], Step [32025/68337], Loss: 4.9663\n",
      "Epoch [10/10], Step [32100/68337], Loss: 4.9417\n",
      "Epoch [10/10], Step [32175/68337], Loss: 5.0313\n",
      "Epoch [10/10], Step [32250/68337], Loss: 5.1966\n",
      "Epoch [10/10], Step [32325/68337], Loss: 5.1535\n",
      "Epoch [10/10], Step [32400/68337], Loss: 5.2080\n",
      "Epoch [10/10], Step [32475/68337], Loss: 4.9166\n",
      "Epoch [10/10], Step [32550/68337], Loss: 4.9810\n",
      "Epoch [10/10], Step [32625/68337], Loss: 5.2656\n",
      "Epoch [10/10], Step [32700/68337], Loss: 5.1192\n",
      "Epoch [10/10], Step [32775/68337], Loss: 5.1316\n",
      "Epoch [10/10], Step [32850/68337], Loss: 4.9678\n",
      "Epoch [10/10], Step [32925/68337], Loss: 5.1244\n",
      "Epoch [10/10], Step [33000/68337], Loss: 4.9452\n",
      "Epoch [10/10], Step [33075/68337], Loss: 5.0884\n",
      "Epoch [10/10], Step [33150/68337], Loss: 4.9312\n",
      "Epoch [10/10], Step [33225/68337], Loss: 5.0105\n",
      "Epoch [10/10], Step [33300/68337], Loss: 4.9478\n",
      "Epoch [10/10], Step [33375/68337], Loss: 5.0550\n",
      "Epoch [10/10], Step [33450/68337], Loss: 5.1392\n",
      "Epoch [10/10], Step [33525/68337], Loss: 5.0480\n",
      "Epoch [10/10], Step [33600/68337], Loss: 4.9889\n",
      "Epoch [10/10], Step [33675/68337], Loss: 4.9379\n",
      "Epoch [10/10], Step [33750/68337], Loss: 5.1516\n",
      "Epoch [10/10], Step [33825/68337], Loss: 5.1723\n",
      "Epoch [10/10], Step [33900/68337], Loss: 4.9799\n",
      "Epoch [10/10], Step [33975/68337], Loss: 5.1875\n",
      "Epoch [10/10], Step [34050/68337], Loss: 5.0272\n",
      "Epoch [10/10], Step [34125/68337], Loss: 4.9328\n",
      "Epoch [10/10], Step [34200/68337], Loss: 4.8941\n",
      "Epoch [10/10], Step [34275/68337], Loss: 5.0360\n",
      "Epoch [10/10], Step [34350/68337], Loss: 5.0868\n",
      "Epoch [10/10], Step [34425/68337], Loss: 4.9876\n",
      "Epoch [10/10], Step [34500/68337], Loss: 5.0397\n",
      "Epoch [10/10], Step [34575/68337], Loss: 5.1524\n",
      "Epoch [10/10], Step [34650/68337], Loss: 5.0359\n",
      "Epoch [10/10], Step [34725/68337], Loss: 4.9492\n",
      "Epoch [10/10], Step [34800/68337], Loss: 5.1902\n",
      "Epoch [10/10], Step [34875/68337], Loss: 5.1210\n",
      "Epoch [10/10], Step [34950/68337], Loss: 5.0995\n",
      "Epoch [10/10], Step [35025/68337], Loss: 5.1258\n",
      "Epoch [10/10], Step [35100/68337], Loss: 4.9615\n",
      "Epoch [10/10], Step [35175/68337], Loss: 5.0779\n",
      "Epoch [10/10], Step [35250/68337], Loss: 4.9638\n",
      "Epoch [10/10], Step [35325/68337], Loss: 5.0054\n",
      "Epoch [10/10], Step [35400/68337], Loss: 4.9843\n",
      "Epoch [10/10], Step [35475/68337], Loss: 5.0949\n",
      "Epoch [10/10], Step [35550/68337], Loss: 5.0019\n",
      "Epoch [10/10], Step [35625/68337], Loss: 4.9540\n",
      "Epoch [10/10], Step [35700/68337], Loss: 4.9943\n",
      "Epoch [10/10], Step [35775/68337], Loss: 5.1610\n",
      "Epoch [10/10], Step [35850/68337], Loss: 5.2085\n",
      "Epoch [10/10], Step [35925/68337], Loss: 4.8068\n",
      "Epoch [10/10], Step [36000/68337], Loss: 5.0920\n",
      "Epoch [10/10], Step [36075/68337], Loss: 5.1995\n",
      "Epoch [10/10], Step [36150/68337], Loss: 4.9892\n",
      "Epoch [10/10], Step [36225/68337], Loss: 4.9096\n",
      "Epoch [10/10], Step [36300/68337], Loss: 5.1002\n",
      "Epoch [10/10], Step [36375/68337], Loss: 4.9017\n",
      "Epoch [10/10], Step [36450/68337], Loss: 5.2232\n",
      "Epoch [10/10], Step [36525/68337], Loss: 5.0776\n",
      "Epoch [10/10], Step [36600/68337], Loss: 4.9780\n",
      "Epoch [10/10], Step [36675/68337], Loss: 5.0604\n",
      "Epoch [10/10], Step [36750/68337], Loss: 5.1936\n",
      "Epoch [10/10], Step [36825/68337], Loss: 5.1141\n",
      "Epoch [10/10], Step [36900/68337], Loss: 5.2273\n",
      "Epoch [10/10], Step [36975/68337], Loss: 5.0490\n",
      "Epoch [10/10], Step [37050/68337], Loss: 4.9537\n",
      "Epoch [10/10], Step [37125/68337], Loss: 5.2081\n",
      "Epoch [10/10], Step [37200/68337], Loss: 4.9733\n",
      "Epoch [10/10], Step [37275/68337], Loss: 5.0158\n",
      "Epoch [10/10], Step [37350/68337], Loss: 5.0719\n",
      "Epoch [10/10], Step [37425/68337], Loss: 4.8920\n",
      "Epoch [10/10], Step [37500/68337], Loss: 5.1976\n",
      "Epoch [10/10], Step [37575/68337], Loss: 5.1171\n",
      "Epoch [10/10], Step [37650/68337], Loss: 4.9372\n",
      "Epoch [10/10], Step [37725/68337], Loss: 5.2955\n",
      "Epoch [10/10], Step [37800/68337], Loss: 5.1327\n",
      "Epoch [10/10], Step [37875/68337], Loss: 5.2758\n",
      "Epoch [10/10], Step [37950/68337], Loss: 5.2408\n",
      "Epoch [10/10], Step [38025/68337], Loss: 5.0767\n",
      "Epoch [10/10], Step [38100/68337], Loss: 5.1000\n",
      "Epoch [10/10], Step [38175/68337], Loss: 5.2191\n",
      "Epoch [10/10], Step [38250/68337], Loss: 5.0243\n",
      "Epoch [10/10], Step [38325/68337], Loss: 4.9533\n",
      "Epoch [10/10], Step [38400/68337], Loss: 5.1190\n",
      "Epoch [10/10], Step [38475/68337], Loss: 4.9885\n",
      "Epoch [10/10], Step [38550/68337], Loss: 4.8259\n",
      "Epoch [10/10], Step [38625/68337], Loss: 4.8965\n",
      "Epoch [10/10], Step [38700/68337], Loss: 4.9795\n",
      "Epoch [10/10], Step [38775/68337], Loss: 5.0287\n",
      "Epoch [10/10], Step [38850/68337], Loss: 5.0833\n",
      "Epoch [10/10], Step [38925/68337], Loss: 5.0788\n",
      "Epoch [10/10], Step [39000/68337], Loss: 5.0368\n",
      "Epoch [10/10], Step [39075/68337], Loss: 5.1422\n",
      "Epoch [10/10], Step [39150/68337], Loss: 4.9511\n",
      "Epoch [10/10], Step [39225/68337], Loss: 4.9714\n",
      "Epoch [10/10], Step [39300/68337], Loss: 5.2197\n",
      "Epoch [10/10], Step [39375/68337], Loss: 5.1691\n",
      "Epoch [10/10], Step [39450/68337], Loss: 5.1002\n",
      "Epoch [10/10], Step [39525/68337], Loss: 5.0747\n",
      "Epoch [10/10], Step [39600/68337], Loss: 5.0593\n",
      "Epoch [10/10], Step [39675/68337], Loss: 4.8502\n",
      "Epoch [10/10], Step [39750/68337], Loss: 5.0509\n",
      "Epoch [10/10], Step [39825/68337], Loss: 5.2717\n",
      "Epoch [10/10], Step [39900/68337], Loss: 5.2566\n",
      "Epoch [10/10], Step [39975/68337], Loss: 5.1031\n",
      "Validation perplexity: 119.8080047110785\n",
      "Epoch [10/10], Step [40050/68337], Loss: 4.9115\n",
      "Epoch [10/10], Step [40125/68337], Loss: 5.0996\n",
      "Epoch [10/10], Step [40200/68337], Loss: 5.2326\n",
      "Epoch [10/10], Step [40275/68337], Loss: 4.9903\n",
      "Epoch [10/10], Step [40350/68337], Loss: 5.0064\n",
      "Epoch [10/10], Step [40425/68337], Loss: 5.2136\n",
      "Epoch [10/10], Step [40500/68337], Loss: 5.0019\n",
      "Epoch [10/10], Step [40575/68337], Loss: 4.9571\n",
      "Epoch [10/10], Step [40650/68337], Loss: 5.0967\n",
      "Epoch [10/10], Step [40725/68337], Loss: 5.0281\n",
      "Epoch [10/10], Step [40800/68337], Loss: 5.2605\n",
      "Epoch [10/10], Step [40875/68337], Loss: 5.0794\n",
      "Epoch [10/10], Step [40950/68337], Loss: 4.9641\n",
      "Epoch [10/10], Step [41025/68337], Loss: 5.1854\n",
      "Epoch [10/10], Step [41100/68337], Loss: 5.0300\n",
      "Epoch [10/10], Step [41175/68337], Loss: 5.0893\n",
      "Epoch [10/10], Step [41250/68337], Loss: 4.9821\n",
      "Epoch [10/10], Step [41325/68337], Loss: 4.8261\n",
      "Epoch [10/10], Step [41400/68337], Loss: 5.0409\n",
      "Epoch [10/10], Step [41475/68337], Loss: 5.0919\n",
      "Epoch [10/10], Step [41550/68337], Loss: 4.9818\n",
      "Epoch [10/10], Step [41625/68337], Loss: 5.0393\n",
      "Epoch [10/10], Step [41700/68337], Loss: 4.9009\n",
      "Epoch [10/10], Step [41775/68337], Loss: 5.0857\n",
      "Epoch [10/10], Step [41850/68337], Loss: 5.1333\n",
      "Epoch [10/10], Step [41925/68337], Loss: 5.1161\n",
      "Epoch [10/10], Step [42000/68337], Loss: 5.0751\n",
      "Epoch [10/10], Step [42075/68337], Loss: 5.2636\n",
      "Epoch [10/10], Step [42150/68337], Loss: 5.0743\n",
      "Epoch [10/10], Step [42225/68337], Loss: 5.0403\n",
      "Epoch [10/10], Step [42300/68337], Loss: 5.0038\n",
      "Epoch [10/10], Step [42375/68337], Loss: 5.2155\n",
      "Epoch [10/10], Step [42450/68337], Loss: 5.1219\n",
      "Epoch [10/10], Step [42525/68337], Loss: 5.0520\n",
      "Epoch [10/10], Step [42600/68337], Loss: 4.9711\n",
      "Epoch [10/10], Step [42675/68337], Loss: 5.1281\n",
      "Epoch [10/10], Step [42750/68337], Loss: 5.1147\n",
      "Epoch [10/10], Step [42825/68337], Loss: 5.1092\n",
      "Epoch [10/10], Step [42900/68337], Loss: 5.1223\n",
      "Epoch [10/10], Step [42975/68337], Loss: 5.1356\n",
      "Epoch [10/10], Step [43050/68337], Loss: 5.2230\n",
      "Epoch [10/10], Step [43125/68337], Loss: 5.2177\n",
      "Epoch [10/10], Step [43200/68337], Loss: 5.1835\n",
      "Epoch [10/10], Step [43275/68337], Loss: 5.2245\n",
      "Epoch [10/10], Step [43350/68337], Loss: 4.9768\n",
      "Epoch [10/10], Step [43425/68337], Loss: 4.8544\n",
      "Epoch [10/10], Step [43500/68337], Loss: 4.9681\n",
      "Epoch [10/10], Step [43575/68337], Loss: 5.1428\n",
      "Epoch [10/10], Step [43650/68337], Loss: 5.1867\n",
      "Epoch [10/10], Step [43725/68337], Loss: 5.1744\n",
      "Epoch [10/10], Step [43800/68337], Loss: 5.1877\n",
      "Epoch [10/10], Step [43875/68337], Loss: 5.2166\n",
      "Epoch [10/10], Step [43950/68337], Loss: 5.1439\n",
      "Epoch [10/10], Step [44025/68337], Loss: 4.9790\n",
      "Epoch [10/10], Step [44100/68337], Loss: 5.2669\n",
      "Epoch [10/10], Step [44175/68337], Loss: 5.2056\n",
      "Epoch [10/10], Step [44250/68337], Loss: 5.0804\n",
      "Epoch [10/10], Step [44325/68337], Loss: 4.9364\n",
      "Epoch [10/10], Step [44400/68337], Loss: 5.1636\n",
      "Epoch [10/10], Step [44475/68337], Loss: 4.9882\n",
      "Epoch [10/10], Step [44550/68337], Loss: 5.1672\n",
      "Epoch [10/10], Step [44625/68337], Loss: 4.8542\n",
      "Epoch [10/10], Step [44700/68337], Loss: 5.0073\n",
      "Epoch [10/10], Step [44775/68337], Loss: 5.4013\n",
      "Epoch [10/10], Step [44850/68337], Loss: 5.0514\n",
      "Epoch [10/10], Step [44925/68337], Loss: 5.2073\n",
      "Epoch [10/10], Step [45000/68337], Loss: 4.9925\n",
      "Epoch [10/10], Step [45075/68337], Loss: 5.1280\n",
      "Epoch [10/10], Step [45150/68337], Loss: 5.2193\n",
      "Epoch [10/10], Step [45225/68337], Loss: 5.3070\n",
      "Epoch [10/10], Step [45300/68337], Loss: 5.0656\n",
      "Epoch [10/10], Step [45375/68337], Loss: 4.9783\n",
      "Epoch [10/10], Step [45450/68337], Loss: 4.9701\n",
      "Epoch [10/10], Step [45525/68337], Loss: 5.1465\n",
      "Epoch [10/10], Step [45600/68337], Loss: 5.1980\n",
      "Epoch [10/10], Step [45675/68337], Loss: 4.9077\n",
      "Epoch [10/10], Step [45750/68337], Loss: 5.0202\n",
      "Epoch [10/10], Step [45825/68337], Loss: 4.9124\n",
      "Epoch [10/10], Step [45900/68337], Loss: 5.0605\n",
      "Epoch [10/10], Step [45975/68337], Loss: 5.1565\n",
      "Epoch [10/10], Step [46050/68337], Loss: 5.0612\n",
      "Epoch [10/10], Step [46125/68337], Loss: 5.2086\n",
      "Epoch [10/10], Step [46200/68337], Loss: 5.1959\n",
      "Epoch [10/10], Step [46275/68337], Loss: 5.1466\n",
      "Epoch [10/10], Step [46350/68337], Loss: 5.1761\n",
      "Epoch [10/10], Step [46425/68337], Loss: 4.9780\n",
      "Epoch [10/10], Step [46500/68337], Loss: 5.0073\n",
      "Epoch [10/10], Step [46575/68337], Loss: 5.1217\n",
      "Epoch [10/10], Step [46650/68337], Loss: 5.0990\n",
      "Epoch [10/10], Step [46725/68337], Loss: 5.1067\n",
      "Epoch [10/10], Step [46800/68337], Loss: 4.9153\n",
      "Epoch [10/10], Step [46875/68337], Loss: 5.1341\n",
      "Epoch [10/10], Step [46950/68337], Loss: 4.8246\n",
      "Epoch [10/10], Step [47025/68337], Loss: 5.0289\n",
      "Epoch [10/10], Step [47100/68337], Loss: 4.9503\n",
      "Epoch [10/10], Step [47175/68337], Loss: 5.2300\n",
      "Epoch [10/10], Step [47250/68337], Loss: 5.0602\n",
      "Epoch [10/10], Step [47325/68337], Loss: 5.0544\n",
      "Epoch [10/10], Step [47400/68337], Loss: 5.0432\n",
      "Epoch [10/10], Step [47475/68337], Loss: 4.9455\n",
      "Epoch [10/10], Step [47550/68337], Loss: 5.0359\n",
      "Epoch [10/10], Step [47625/68337], Loss: 4.9934\n",
      "Epoch [10/10], Step [47700/68337], Loss: 5.1514\n",
      "Epoch [10/10], Step [47775/68337], Loss: 5.0846\n",
      "Epoch [10/10], Step [47850/68337], Loss: 5.1532\n",
      "Epoch [10/10], Step [47925/68337], Loss: 4.8097\n",
      "Epoch [10/10], Step [48000/68337], Loss: 4.9759\n",
      "Epoch [10/10], Step [48075/68337], Loss: 5.1426\n",
      "Epoch [10/10], Step [48150/68337], Loss: 5.0086\n",
      "Epoch [10/10], Step [48225/68337], Loss: 5.0556\n",
      "Epoch [10/10], Step [48300/68337], Loss: 5.0931\n",
      "Epoch [10/10], Step [48375/68337], Loss: 5.1329\n",
      "Epoch [10/10], Step [48450/68337], Loss: 4.8706\n",
      "Epoch [10/10], Step [48525/68337], Loss: 4.8961\n",
      "Epoch [10/10], Step [48600/68337], Loss: 4.9711\n",
      "Epoch [10/10], Step [48675/68337], Loss: 4.9814\n",
      "Epoch [10/10], Step [48750/68337], Loss: 5.0316\n",
      "Epoch [10/10], Step [48825/68337], Loss: 4.9714\n",
      "Epoch [10/10], Step [48900/68337], Loss: 4.8614\n",
      "Epoch [10/10], Step [48975/68337], Loss: 5.0064\n",
      "Epoch [10/10], Step [49050/68337], Loss: 5.1459\n",
      "Epoch [10/10], Step [49125/68337], Loss: 5.0079\n",
      "Epoch [10/10], Step [49200/68337], Loss: 5.2184\n",
      "Epoch [10/10], Step [49275/68337], Loss: 4.9878\n",
      "Epoch [10/10], Step [49350/68337], Loss: 5.0502\n",
      "Epoch [10/10], Step [49425/68337], Loss: 5.1009\n",
      "Epoch [10/10], Step [49500/68337], Loss: 5.2449\n",
      "Epoch [10/10], Step [49575/68337], Loss: 5.2300\n",
      "Epoch [10/10], Step [49650/68337], Loss: 4.9566\n",
      "Epoch [10/10], Step [49725/68337], Loss: 4.9556\n",
      "Epoch [10/10], Step [49800/68337], Loss: 5.0333\n",
      "Epoch [10/10], Step [49875/68337], Loss: 5.1215\n",
      "Epoch [10/10], Step [49950/68337], Loss: 5.1605\n",
      "Validation perplexity: 119.2740686384629\n",
      "Epoch [10/10], Step [50025/68337], Loss: 4.9604\n",
      "Epoch [10/10], Step [50100/68337], Loss: 5.4491\n",
      "Epoch [10/10], Step [50175/68337], Loss: 5.2278\n",
      "Epoch [10/10], Step [50250/68337], Loss: 4.9377\n",
      "Epoch [10/10], Step [50325/68337], Loss: 5.1393\n",
      "Epoch [10/10], Step [50400/68337], Loss: 4.8862\n",
      "Epoch [10/10], Step [50475/68337], Loss: 5.1057\n",
      "Epoch [10/10], Step [50550/68337], Loss: 5.1003\n",
      "Epoch [10/10], Step [50625/68337], Loss: 5.0766\n",
      "Epoch [10/10], Step [50700/68337], Loss: 4.8624\n",
      "Epoch [10/10], Step [50775/68337], Loss: 5.2087\n",
      "Epoch [10/10], Step [50850/68337], Loss: 5.0788\n",
      "Epoch [10/10], Step [50925/68337], Loss: 5.1026\n",
      "Epoch [10/10], Step [51000/68337], Loss: 5.1311\n",
      "Epoch [10/10], Step [51075/68337], Loss: 5.2566\n",
      "Epoch [10/10], Step [51150/68337], Loss: 5.3135\n",
      "Epoch [10/10], Step [51225/68337], Loss: 4.9224\n",
      "Epoch [10/10], Step [51300/68337], Loss: 4.9868\n",
      "Epoch [10/10], Step [51375/68337], Loss: 5.1278\n",
      "Epoch [10/10], Step [51450/68337], Loss: 5.0730\n",
      "Epoch [10/10], Step [51525/68337], Loss: 5.1786\n",
      "Epoch [10/10], Step [51600/68337], Loss: 4.8851\n",
      "Epoch [10/10], Step [51675/68337], Loss: 5.0505\n",
      "Epoch [10/10], Step [51750/68337], Loss: 4.9886\n",
      "Epoch [10/10], Step [51825/68337], Loss: 5.1167\n",
      "Epoch [10/10], Step [51900/68337], Loss: 4.9851\n",
      "Epoch [10/10], Step [51975/68337], Loss: 5.0400\n",
      "Epoch [10/10], Step [52050/68337], Loss: 4.9228\n",
      "Epoch [10/10], Step [52125/68337], Loss: 4.9931\n",
      "Epoch [10/10], Step [52200/68337], Loss: 5.2901\n",
      "Epoch [10/10], Step [52275/68337], Loss: 5.2633\n",
      "Epoch [10/10], Step [52350/68337], Loss: 5.0592\n",
      "Epoch [10/10], Step [52425/68337], Loss: 5.0974\n",
      "Epoch [10/10], Step [52500/68337], Loss: 5.3512\n",
      "Epoch [10/10], Step [52575/68337], Loss: 4.9699\n",
      "Epoch [10/10], Step [52650/68337], Loss: 4.9448\n",
      "Epoch [10/10], Step [52725/68337], Loss: 4.9972\n",
      "Epoch [10/10], Step [52800/68337], Loss: 5.0586\n",
      "Epoch [10/10], Step [52875/68337], Loss: 5.0971\n",
      "Epoch [10/10], Step [52950/68337], Loss: 5.1991\n",
      "Epoch [10/10], Step [53025/68337], Loss: 5.2312\n",
      "Epoch [10/10], Step [53100/68337], Loss: 5.3121\n",
      "Epoch [10/10], Step [53175/68337], Loss: 5.1725\n",
      "Epoch [10/10], Step [53250/68337], Loss: 5.1685\n",
      "Epoch [10/10], Step [53325/68337], Loss: 4.8471\n",
      "Epoch [10/10], Step [53400/68337], Loss: 5.1717\n",
      "Epoch [10/10], Step [53475/68337], Loss: 4.7880\n",
      "Epoch [10/10], Step [53550/68337], Loss: 5.1047\n",
      "Epoch [10/10], Step [53625/68337], Loss: 5.0734\n",
      "Epoch [10/10], Step [53700/68337], Loss: 5.2129\n",
      "Epoch [10/10], Step [53775/68337], Loss: 5.0332\n",
      "Epoch [10/10], Step [53850/68337], Loss: 5.0486\n",
      "Epoch [10/10], Step [53925/68337], Loss: 5.1973\n",
      "Epoch [10/10], Step [54000/68337], Loss: 4.9489\n",
      "Epoch [10/10], Step [54075/68337], Loss: 5.2188\n",
      "Epoch [10/10], Step [54150/68337], Loss: 5.0128\n",
      "Epoch [10/10], Step [54225/68337], Loss: 5.0015\n",
      "Epoch [10/10], Step [54300/68337], Loss: 5.1150\n",
      "Epoch [10/10], Step [54375/68337], Loss: 5.2627\n",
      "Epoch [10/10], Step [54450/68337], Loss: 5.0403\n",
      "Epoch [10/10], Step [54525/68337], Loss: 5.2317\n",
      "Epoch [10/10], Step [54600/68337], Loss: 5.1208\n",
      "Epoch [10/10], Step [54675/68337], Loss: 5.0258\n",
      "Epoch [10/10], Step [54750/68337], Loss: 5.1990\n",
      "Epoch [10/10], Step [54825/68337], Loss: 5.0660\n",
      "Epoch [10/10], Step [54900/68337], Loss: 4.9647\n",
      "Epoch [10/10], Step [54975/68337], Loss: 5.2611\n",
      "Epoch [10/10], Step [55050/68337], Loss: 5.0910\n",
      "Epoch [10/10], Step [55125/68337], Loss: 5.1202\n",
      "Epoch [10/10], Step [55200/68337], Loss: 5.0225\n",
      "Epoch [10/10], Step [55275/68337], Loss: 4.8982\n",
      "Epoch [10/10], Step [55350/68337], Loss: 5.1331\n",
      "Epoch [10/10], Step [55425/68337], Loss: 5.0046\n",
      "Epoch [10/10], Step [55500/68337], Loss: 5.2425\n",
      "Epoch [10/10], Step [55575/68337], Loss: 5.0818\n",
      "Epoch [10/10], Step [55650/68337], Loss: 4.9480\n",
      "Epoch [10/10], Step [55725/68337], Loss: 4.9747\n",
      "Epoch [10/10], Step [55800/68337], Loss: 5.0494\n",
      "Epoch [10/10], Step [55875/68337], Loss: 5.0667\n",
      "Epoch [10/10], Step [55950/68337], Loss: 4.9386\n",
      "Epoch [10/10], Step [56025/68337], Loss: 5.0993\n",
      "Epoch [10/10], Step [56100/68337], Loss: 4.9404\n",
      "Epoch [10/10], Step [56175/68337], Loss: 5.0622\n",
      "Epoch [10/10], Step [56250/68337], Loss: 5.0309\n",
      "Epoch [10/10], Step [56325/68337], Loss: 5.1453\n",
      "Epoch [10/10], Step [56400/68337], Loss: 4.9688\n",
      "Epoch [10/10], Step [56475/68337], Loss: 5.0541\n",
      "Epoch [10/10], Step [56550/68337], Loss: 4.9805\n",
      "Epoch [10/10], Step [56625/68337], Loss: 5.2319\n",
      "Epoch [10/10], Step [56700/68337], Loss: 5.1493\n",
      "Epoch [10/10], Step [56775/68337], Loss: 5.0263\n",
      "Epoch [10/10], Step [56850/68337], Loss: 5.1802\n",
      "Epoch [10/10], Step [56925/68337], Loss: 4.9947\n",
      "Epoch [10/10], Step [57000/68337], Loss: 5.1429\n",
      "Epoch [10/10], Step [57075/68337], Loss: 5.1110\n",
      "Epoch [10/10], Step [57150/68337], Loss: 5.1805\n",
      "Epoch [10/10], Step [57225/68337], Loss: 4.9880\n",
      "Epoch [10/10], Step [57300/68337], Loss: 5.0686\n",
      "Epoch [10/10], Step [57375/68337], Loss: 4.8191\n",
      "Epoch [10/10], Step [57450/68337], Loss: 5.0288\n",
      "Epoch [10/10], Step [57525/68337], Loss: 5.3282\n",
      "Epoch [10/10], Step [57600/68337], Loss: 5.0465\n",
      "Epoch [10/10], Step [57675/68337], Loss: 5.0530\n",
      "Epoch [10/10], Step [57750/68337], Loss: 5.2391\n",
      "Epoch [10/10], Step [57825/68337], Loss: 5.1252\n",
      "Epoch [10/10], Step [57900/68337], Loss: 5.2244\n",
      "Epoch [10/10], Step [57975/68337], Loss: 5.1755\n",
      "Epoch [10/10], Step [58050/68337], Loss: 5.1580\n",
      "Epoch [10/10], Step [58125/68337], Loss: 5.1885\n",
      "Epoch [10/10], Step [58200/68337], Loss: 5.1354\n",
      "Epoch [10/10], Step [58275/68337], Loss: 5.1474\n",
      "Epoch [10/10], Step [58350/68337], Loss: 5.2412\n",
      "Epoch [10/10], Step [58425/68337], Loss: 5.1352\n",
      "Epoch [10/10], Step [58500/68337], Loss: 4.9385\n",
      "Epoch [10/10], Step [58575/68337], Loss: 5.1333\n",
      "Epoch [10/10], Step [58650/68337], Loss: 5.1736\n",
      "Epoch [10/10], Step [58725/68337], Loss: 4.9513\n",
      "Epoch [10/10], Step [58800/68337], Loss: 5.1601\n",
      "Epoch [10/10], Step [58875/68337], Loss: 5.1590\n",
      "Epoch [10/10], Step [58950/68337], Loss: 5.1710\n",
      "Epoch [10/10], Step [59025/68337], Loss: 4.9497\n",
      "Epoch [10/10], Step [59100/68337], Loss: 4.9895\n",
      "Epoch [10/10], Step [59175/68337], Loss: 4.9652\n",
      "Epoch [10/10], Step [59250/68337], Loss: 4.9896\n",
      "Epoch [10/10], Step [59325/68337], Loss: 4.9515\n",
      "Epoch [10/10], Step [59400/68337], Loss: 4.8853\n",
      "Epoch [10/10], Step [59475/68337], Loss: 5.0945\n",
      "Epoch [10/10], Step [59550/68337], Loss: 4.9408\n",
      "Epoch [10/10], Step [59625/68337], Loss: 5.0346\n",
      "Epoch [10/10], Step [59700/68337], Loss: 5.0865\n",
      "Epoch [10/10], Step [59775/68337], Loss: 4.9821\n",
      "Epoch [10/10], Step [59850/68337], Loss: 5.1061\n",
      "Epoch [10/10], Step [59925/68337], Loss: 4.9563\n",
      "Epoch [10/10], Step [60000/68337], Loss: 5.1690\n",
      "Validation perplexity: 119.5786640505998\n",
      "Epoch [10/10], Step [60075/68337], Loss: 5.1501\n",
      "Epoch [10/10], Step [60150/68337], Loss: 5.0073\n",
      "Epoch [10/10], Step [60225/68337], Loss: 5.1655\n",
      "Epoch [10/10], Step [60300/68337], Loss: 5.1497\n",
      "Epoch [10/10], Step [60375/68337], Loss: 4.9848\n",
      "Epoch [10/10], Step [60450/68337], Loss: 5.1789\n",
      "Epoch [10/10], Step [60525/68337], Loss: 5.1514\n",
      "Epoch [10/10], Step [60600/68337], Loss: 4.8827\n",
      "Epoch [10/10], Step [60675/68337], Loss: 5.0996\n",
      "Epoch [10/10], Step [60750/68337], Loss: 4.9880\n",
      "Epoch [10/10], Step [60825/68337], Loss: 5.0556\n",
      "Epoch [10/10], Step [60900/68337], Loss: 5.0321\n",
      "Epoch [10/10], Step [60975/68337], Loss: 4.9977\n",
      "Epoch [10/10], Step [61050/68337], Loss: 4.8401\n",
      "Epoch [10/10], Step [61125/68337], Loss: 4.9204\n",
      "Epoch [10/10], Step [61200/68337], Loss: 5.1554\n",
      "Epoch [10/10], Step [61275/68337], Loss: 5.1028\n",
      "Epoch [10/10], Step [61350/68337], Loss: 5.2428\n",
      "Epoch [10/10], Step [61425/68337], Loss: 5.2715\n",
      "Epoch [10/10], Step [61500/68337], Loss: 5.0508\n",
      "Epoch [10/10], Step [61575/68337], Loss: 4.9084\n",
      "Epoch [10/10], Step [61650/68337], Loss: 5.0360\n",
      "Epoch [10/10], Step [61725/68337], Loss: 5.0814\n",
      "Epoch [10/10], Step [61800/68337], Loss: 4.9717\n",
      "Epoch [10/10], Step [61875/68337], Loss: 4.9616\n",
      "Epoch [10/10], Step [61950/68337], Loss: 5.1373\n",
      "Epoch [10/10], Step [62025/68337], Loss: 4.9895\n",
      "Epoch [10/10], Step [62100/68337], Loss: 4.9674\n",
      "Epoch [10/10], Step [62175/68337], Loss: 5.1305\n",
      "Epoch [10/10], Step [62250/68337], Loss: 5.1848\n",
      "Epoch [10/10], Step [62325/68337], Loss: 5.0302\n",
      "Epoch [10/10], Step [62400/68337], Loss: 4.8326\n",
      "Epoch [10/10], Step [62475/68337], Loss: 5.0323\n",
      "Epoch [10/10], Step [62550/68337], Loss: 5.0923\n",
      "Epoch [10/10], Step [62625/68337], Loss: 5.0232\n",
      "Epoch [10/10], Step [62700/68337], Loss: 5.0407\n",
      "Epoch [10/10], Step [62775/68337], Loss: 5.2648\n",
      "Epoch [10/10], Step [62850/68337], Loss: 5.2278\n",
      "Epoch [10/10], Step [62925/68337], Loss: 4.9819\n",
      "Epoch [10/10], Step [63000/68337], Loss: 5.1676\n",
      "Epoch [10/10], Step [63075/68337], Loss: 5.3211\n",
      "Epoch [10/10], Step [63150/68337], Loss: 5.0950\n",
      "Epoch [10/10], Step [63225/68337], Loss: 4.9392\n",
      "Epoch [10/10], Step [63300/68337], Loss: 4.9399\n",
      "Epoch [10/10], Step [63375/68337], Loss: 5.0201\n",
      "Epoch [10/10], Step [63450/68337], Loss: 5.1206\n",
      "Epoch [10/10], Step [63525/68337], Loss: 4.7779\n",
      "Epoch [10/10], Step [63600/68337], Loss: 5.1300\n",
      "Epoch [10/10], Step [63675/68337], Loss: 4.7959\n",
      "Epoch [10/10], Step [63750/68337], Loss: 5.0352\n",
      "Epoch [10/10], Step [63825/68337], Loss: 5.0930\n",
      "Epoch [10/10], Step [63900/68337], Loss: 5.0296\n",
      "Epoch [10/10], Step [63975/68337], Loss: 5.0423\n",
      "Epoch [10/10], Step [64050/68337], Loss: 4.8518\n",
      "Epoch [10/10], Step [64125/68337], Loss: 4.9743\n",
      "Epoch [10/10], Step [64200/68337], Loss: 5.0320\n",
      "Epoch [10/10], Step [64275/68337], Loss: 5.1160\n",
      "Epoch [10/10], Step [64350/68337], Loss: 5.0632\n",
      "Epoch [10/10], Step [64425/68337], Loss: 5.0992\n",
      "Epoch [10/10], Step [64500/68337], Loss: 5.0050\n",
      "Epoch [10/10], Step [64575/68337], Loss: 5.2296\n",
      "Epoch [10/10], Step [64650/68337], Loss: 5.0563\n",
      "Epoch [10/10], Step [64725/68337], Loss: 4.9411\n",
      "Epoch [10/10], Step [64800/68337], Loss: 4.6716\n",
      "Epoch [10/10], Step [64875/68337], Loss: 4.8666\n",
      "Epoch [10/10], Step [64950/68337], Loss: 5.1793\n",
      "Epoch [10/10], Step [65025/68337], Loss: 5.1374\n",
      "Epoch [10/10], Step [65100/68337], Loss: 5.0073\n",
      "Epoch [10/10], Step [65175/68337], Loss: 5.1234\n",
      "Epoch [10/10], Step [65250/68337], Loss: 5.0874\n",
      "Epoch [10/10], Step [65325/68337], Loss: 4.8513\n",
      "Epoch [10/10], Step [65400/68337], Loss: 4.9374\n",
      "Epoch [10/10], Step [65475/68337], Loss: 5.0848\n",
      "Epoch [10/10], Step [65550/68337], Loss: 5.1288\n",
      "Epoch [10/10], Step [65625/68337], Loss: 5.1124\n",
      "Epoch [10/10], Step [65700/68337], Loss: 5.0911\n",
      "Epoch [10/10], Step [65775/68337], Loss: 5.1842\n",
      "Epoch [10/10], Step [65850/68337], Loss: 5.2085\n",
      "Epoch [10/10], Step [65925/68337], Loss: 5.0890\n",
      "Epoch [10/10], Step [66000/68337], Loss: 5.1088\n",
      "Epoch [10/10], Step [66075/68337], Loss: 4.8236\n",
      "Epoch [10/10], Step [66150/68337], Loss: 5.0059\n",
      "Epoch [10/10], Step [66225/68337], Loss: 5.0925\n",
      "Epoch [10/10], Step [66300/68337], Loss: 5.0548\n",
      "Epoch [10/10], Step [66375/68337], Loss: 5.0412\n",
      "Epoch [10/10], Step [66450/68337], Loss: 5.0650\n",
      "Epoch [10/10], Step [66525/68337], Loss: 5.1612\n",
      "Epoch [10/10], Step [66600/68337], Loss: 4.9881\n",
      "Epoch [10/10], Step [66675/68337], Loss: 5.0379\n",
      "Epoch [10/10], Step [66750/68337], Loss: 5.1569\n",
      "Epoch [10/10], Step [66825/68337], Loss: 5.2308\n",
      "Epoch [10/10], Step [66900/68337], Loss: 5.0846\n",
      "Epoch [10/10], Step [66975/68337], Loss: 5.2287\n",
      "Epoch [10/10], Step [67050/68337], Loss: 4.8170\n",
      "Epoch [10/10], Step [67125/68337], Loss: 5.1045\n",
      "Epoch [10/10], Step [67200/68337], Loss: 5.0135\n",
      "Epoch [10/10], Step [67275/68337], Loss: 5.1310\n",
      "Epoch [10/10], Step [67350/68337], Loss: 5.1230\n",
      "Epoch [10/10], Step [67425/68337], Loss: 5.1202\n",
      "Epoch [10/10], Step [67500/68337], Loss: 5.0066\n",
      "Epoch [10/10], Step [67575/68337], Loss: 5.1770\n",
      "Epoch [10/10], Step [67650/68337], Loss: 5.0735\n",
      "Epoch [10/10], Step [67725/68337], Loss: 4.9462\n",
      "Epoch [10/10], Step [67800/68337], Loss: 5.0918\n",
      "Epoch [10/10], Step [67875/68337], Loss: 4.9606\n",
      "Epoch [10/10], Step [67950/68337], Loss: 5.2070\n",
      "Epoch [10/10], Step [68025/68337], Loss: 5.0439\n",
      "Epoch [10/10], Step [68100/68337], Loss: 5.1088\n",
      "Epoch [10/10], Step [68175/68337], Loss: 4.9444\n",
      "Epoch [10/10], Step [68250/68337], Loss: 5.1206\n",
      "Epoch [10/10], Step [68325/68337], Loss: 5.0621\n",
      "Epoch [10/10] Average Loss: 5.0736, Perplexity: 159.75\n"
     ]
    }
   ],
   "source": [
    "from src.trainComplete import TrainComplete\n",
    "from src.attentionModel import LanguageModelWithAttention\n",
    "\n",
    "raw_text = get_cleaned_text(text_path,clean_pers_text_replace)\n",
    "trainclass = TrainComplete(text_path = text_path,path_to_save_folder= path_to_save_folder,tokenizer = tokenizer,\n",
    "                           allowed_special=False, is_attention_training = True)\n",
    "\n",
    "\n",
    "context_length = 32  # Increased context size\n",
    "embedding_dim = 128\n",
    "attention_dim = 64\n",
    "hidden_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "model = LanguageModelWithAttention(\n",
    "    vocab_size, embedding_dim, attention_dim, context_length, hidden_dim, num_heads, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "trainclass.train(model,\n",
    "              vocab_size,device,raw_text,\"pers_attention_standard_dropout_ep10_eval10000\",\n",
    "                print_every=75,evaluate_every=10000,optimizer=None,criterion=None,\n",
    "              batch_size = 32,\n",
    "              embedding_dim = embedding_dim,\n",
    "              context_length = context_length,\n",
    "              num_epochs = 10\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fdd6df4-4a5f-4f08-a9ed-7f1f28f14a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create Dataset 2720000 / 2733504Started Training\n",
      "Epoch [1/5], Step [0/136675], Loss: 10.3109\n",
      "Validation perplexity: 29908.167241123712\n",
      "Epoch [1/5], Step [75/136675], Loss: 7.3794\n",
      "Epoch [1/5], Step [150/136675], Loss: 7.4273\n",
      "Epoch [1/5], Step [225/136675], Loss: 7.3885\n",
      "Epoch [1/5], Step [300/136675], Loss: 7.1044\n",
      "Epoch [1/5], Step [375/136675], Loss: 7.2786\n",
      "Epoch [1/5], Step [450/136675], Loss: 7.0428\n",
      "Epoch [1/5], Step [525/136675], Loss: 7.2214\n",
      "Epoch [1/5], Step [600/136675], Loss: 7.2302\n",
      "Epoch [1/5], Step [675/136675], Loss: 6.9159\n",
      "Epoch [1/5], Step [750/136675], Loss: 7.0736\n",
      "Epoch [1/5], Step [825/136675], Loss: 6.9514\n",
      "Epoch [1/5], Step [900/136675], Loss: 7.0399\n",
      "Epoch [1/5], Step [975/136675], Loss: 6.9347\n",
      "Epoch [1/5], Step [1050/136675], Loss: 7.0446\n",
      "Epoch [1/5], Step [1125/136675], Loss: 6.8281\n",
      "Epoch [1/5], Step [1200/136675], Loss: 6.9100\n",
      "Epoch [1/5], Step [1275/136675], Loss: 7.0114\n",
      "Epoch [1/5], Step [1350/136675], Loss: 6.9062\n",
      "Epoch [1/5], Step [1425/136675], Loss: 6.4081\n",
      "Epoch [1/5], Step [1500/136675], Loss: 6.9299\n",
      "Epoch [1/5], Step [1575/136675], Loss: 6.7620\n",
      "Epoch [1/5], Step [1650/136675], Loss: 6.5315\n",
      "Epoch [1/5], Step [1725/136675], Loss: 6.6924\n",
      "Epoch [1/5], Step [1800/136675], Loss: 6.7247\n",
      "Epoch [1/5], Step [1875/136675], Loss: 6.7185\n",
      "Epoch [1/5], Step [1950/136675], Loss: 7.1554\n",
      "Epoch [1/5], Step [2025/136675], Loss: 6.5040\n",
      "Epoch [1/5], Step [2100/136675], Loss: 6.6676\n",
      "Epoch [1/5], Step [2175/136675], Loss: 6.3991\n",
      "Epoch [1/5], Step [2250/136675], Loss: 6.5815\n",
      "Epoch [1/5], Step [2325/136675], Loss: 6.7553\n",
      "Epoch [1/5], Step [2400/136675], Loss: 6.6071\n",
      "Epoch [1/5], Step [2475/136675], Loss: 6.5626\n",
      "Epoch [1/5], Step [2550/136675], Loss: 6.6273\n",
      "Epoch [1/5], Step [2625/136675], Loss: 6.5344\n",
      "Epoch [1/5], Step [2700/136675], Loss: 6.2615\n",
      "Epoch [1/5], Step [2775/136675], Loss: 6.3564\n",
      "Epoch [1/5], Step [2850/136675], Loss: 6.5764\n",
      "Epoch [1/5], Step [2925/136675], Loss: 6.5354\n",
      "Epoch [1/5], Step [3000/136675], Loss: 6.4435\n",
      "Epoch [1/5], Step [3075/136675], Loss: 6.6922\n",
      "Epoch [1/5], Step [3150/136675], Loss: 6.7098\n",
      "Epoch [1/5], Step [3225/136675], Loss: 6.7106\n",
      "Epoch [1/5], Step [3300/136675], Loss: 6.3429\n",
      "Epoch [1/5], Step [3375/136675], Loss: 6.4292\n",
      "Epoch [1/5], Step [3450/136675], Loss: 6.6201\n",
      "Epoch [1/5], Step [3525/136675], Loss: 6.3203\n",
      "Epoch [1/5], Step [3600/136675], Loss: 6.2588\n",
      "Epoch [1/5], Step [3675/136675], Loss: 6.1150\n",
      "Epoch [1/5], Step [3750/136675], Loss: 6.5012\n",
      "Epoch [1/5], Step [3825/136675], Loss: 6.5887\n",
      "Epoch [1/5], Step [3900/136675], Loss: 6.4593\n",
      "Epoch [1/5], Step [3975/136675], Loss: 6.3713\n",
      "Epoch [1/5], Step [4050/136675], Loss: 6.5051\n",
      "Epoch [1/5], Step [4125/136675], Loss: 6.2367\n",
      "Epoch [1/5], Step [4200/136675], Loss: 6.4301\n",
      "Epoch [1/5], Step [4275/136675], Loss: 6.3556\n",
      "Epoch [1/5], Step [4350/136675], Loss: 6.2970\n",
      "Epoch [1/5], Step [4425/136675], Loss: 6.5283\n",
      "Epoch [1/5], Step [4500/136675], Loss: 6.2868\n",
      "Epoch [1/5], Step [4575/136675], Loss: 6.5235\n",
      "Epoch [1/5], Step [4650/136675], Loss: 6.3290\n",
      "Epoch [1/5], Step [4725/136675], Loss: 6.3954\n",
      "Epoch [1/5], Step [4800/136675], Loss: 6.4015\n",
      "Epoch [1/5], Step [4875/136675], Loss: 6.2005\n",
      "Epoch [1/5], Step [4950/136675], Loss: 6.3736\n",
      "Epoch [1/5], Step [5025/136675], Loss: 6.1028\n",
      "Epoch [1/5], Step [5100/136675], Loss: 6.2424\n",
      "Epoch [1/5], Step [5175/136675], Loss: 6.1012\n",
      "Epoch [1/5], Step [5250/136675], Loss: 6.1984\n",
      "Epoch [1/5], Step [5325/136675], Loss: 6.4794\n",
      "Epoch [1/5], Step [5400/136675], Loss: 6.0449\n",
      "Epoch [1/5], Step [5475/136675], Loss: 6.1833\n",
      "Epoch [1/5], Step [5550/136675], Loss: 6.1967\n",
      "Epoch [1/5], Step [5625/136675], Loss: 6.4107\n",
      "Epoch [1/5], Step [5700/136675], Loss: 6.2722\n",
      "Epoch [1/5], Step [5775/136675], Loss: 5.9474\n",
      "Epoch [1/5], Step [5850/136675], Loss: 6.0807\n",
      "Epoch [1/5], Step [5925/136675], Loss: 6.4396\n",
      "Epoch [1/5], Step [6000/136675], Loss: 6.1892\n",
      "Epoch [1/5], Step [6075/136675], Loss: 6.2150\n",
      "Epoch [1/5], Step [6150/136675], Loss: 6.7433\n",
      "Epoch [1/5], Step [6225/136675], Loss: 6.0987\n",
      "Epoch [1/5], Step [6300/136675], Loss: 6.4270\n",
      "Epoch [1/5], Step [6375/136675], Loss: 6.1957\n",
      "Epoch [1/5], Step [6450/136675], Loss: 6.2436\n",
      "Epoch [1/5], Step [6525/136675], Loss: 6.2470\n",
      "Epoch [1/5], Step [6600/136675], Loss: 6.4628\n",
      "Epoch [1/5], Step [6675/136675], Loss: 6.1501\n",
      "Epoch [1/5], Step [6750/136675], Loss: 6.2028\n",
      "Epoch [1/5], Step [6825/136675], Loss: 6.0692\n",
      "Epoch [1/5], Step [6900/136675], Loss: 6.0513\n",
      "Epoch [1/5], Step [6975/136675], Loss: 6.2329\n",
      "Epoch [1/5], Step [7050/136675], Loss: 6.2262\n",
      "Epoch [1/5], Step [7125/136675], Loss: 6.1688\n",
      "Epoch [1/5], Step [7200/136675], Loss: 5.9048\n",
      "Epoch [1/5], Step [7275/136675], Loss: 6.1300\n",
      "Epoch [1/5], Step [7350/136675], Loss: 6.2391\n",
      "Epoch [1/5], Step [7425/136675], Loss: 5.7740\n",
      "Epoch [1/5], Step [7500/136675], Loss: 6.2480\n",
      "Epoch [1/5], Step [7575/136675], Loss: 6.2671\n",
      "Epoch [1/5], Step [7650/136675], Loss: 6.0878\n",
      "Epoch [1/5], Step [7725/136675], Loss: 6.1335\n",
      "Epoch [1/5], Step [7800/136675], Loss: 6.1352\n",
      "Epoch [1/5], Step [7875/136675], Loss: 6.3102\n",
      "Epoch [1/5], Step [7950/136675], Loss: 6.1259\n",
      "Epoch [1/5], Step [8025/136675], Loss: 6.1999\n",
      "Epoch [1/5], Step [8100/136675], Loss: 6.0229\n",
      "Epoch [1/5], Step [8175/136675], Loss: 5.8143\n",
      "Epoch [1/5], Step [8250/136675], Loss: 5.7145\n",
      "Epoch [1/5], Step [8325/136675], Loss: 6.2924\n",
      "Epoch [1/5], Step [8400/136675], Loss: 6.1886\n",
      "Epoch [1/5], Step [8475/136675], Loss: 5.9527\n",
      "Epoch [1/5], Step [8550/136675], Loss: 6.2923\n",
      "Epoch [1/5], Step [8625/136675], Loss: 6.0479\n",
      "Epoch [1/5], Step [8700/136675], Loss: 5.9765\n",
      "Epoch [1/5], Step [8775/136675], Loss: 6.1798\n",
      "Epoch [1/5], Step [8850/136675], Loss: 6.4708\n",
      "Epoch [1/5], Step [8925/136675], Loss: 5.9780\n",
      "Epoch [1/5], Step [9000/136675], Loss: 6.1677\n",
      "Epoch [1/5], Step [9075/136675], Loss: 6.0757\n",
      "Epoch [1/5], Step [9150/136675], Loss: 5.7706\n",
      "Epoch [1/5], Step [9225/136675], Loss: 5.6565\n",
      "Epoch [1/5], Step [9300/136675], Loss: 6.3404\n",
      "Epoch [1/5], Step [9375/136675], Loss: 6.0054\n",
      "Epoch [1/5], Step [9450/136675], Loss: 6.1289\n",
      "Epoch [1/5], Step [9525/136675], Loss: 5.8878\n",
      "Epoch [1/5], Step [9600/136675], Loss: 6.0793\n",
      "Epoch [1/5], Step [9675/136675], Loss: 6.0082\n",
      "Epoch [1/5], Step [9750/136675], Loss: 5.9834\n",
      "Epoch [1/5], Step [9825/136675], Loss: 6.1061\n",
      "Epoch [1/5], Step [9900/136675], Loss: 5.7953\n",
      "Epoch [1/5], Step [9975/136675], Loss: 5.9381\n",
      "Validation perplexity: 374.02976824842017\n",
      "Epoch [1/5], Step [10050/136675], Loss: 6.0401\n",
      "Epoch [1/5], Step [10125/136675], Loss: 6.0660\n",
      "Epoch [1/5], Step [10200/136675], Loss: 5.8705\n",
      "Epoch [1/5], Step [10275/136675], Loss: 6.0845\n",
      "Epoch [1/5], Step [10350/136675], Loss: 5.7483\n",
      "Epoch [1/5], Step [10425/136675], Loss: 5.6193\n",
      "Epoch [1/5], Step [10500/136675], Loss: 6.1146\n",
      "Epoch [1/5], Step [10575/136675], Loss: 6.0451\n",
      "Epoch [1/5], Step [10650/136675], Loss: 5.8175\n",
      "Epoch [1/5], Step [10725/136675], Loss: 6.1414\n",
      "Epoch [1/5], Step [10800/136675], Loss: 6.2531\n",
      "Epoch [1/5], Step [10875/136675], Loss: 5.8671\n",
      "Epoch [1/5], Step [10950/136675], Loss: 6.0394\n",
      "Epoch [1/5], Step [11025/136675], Loss: 6.0406\n",
      "Epoch [1/5], Step [11100/136675], Loss: 6.0946\n",
      "Epoch [1/5], Step [11175/136675], Loss: 6.0828\n",
      "Epoch [1/5], Step [11250/136675], Loss: 6.1403\n",
      "Epoch [1/5], Step [11325/136675], Loss: 6.2083\n",
      "Epoch [1/5], Step [11400/136675], Loss: 5.8569\n",
      "Epoch [1/5], Step [11475/136675], Loss: 5.9812\n",
      "Epoch [1/5], Step [11550/136675], Loss: 6.3038\n",
      "Epoch [1/5], Step [11625/136675], Loss: 5.6161\n",
      "Epoch [1/5], Step [11700/136675], Loss: 6.0117\n",
      "Epoch [1/5], Step [11775/136675], Loss: 6.1001\n",
      "Epoch [1/5], Step [11850/136675], Loss: 5.7936\n",
      "Epoch [1/5], Step [11925/136675], Loss: 5.8858\n",
      "Epoch [1/5], Step [12000/136675], Loss: 6.0451\n",
      "Epoch [1/5], Step [12075/136675], Loss: 5.9452\n",
      "Epoch [1/5], Step [12150/136675], Loss: 5.9513\n",
      "Epoch [1/5], Step [12225/136675], Loss: 5.8481\n",
      "Epoch [1/5], Step [12300/136675], Loss: 5.4125\n",
      "Epoch [1/5], Step [12375/136675], Loss: 6.0359\n",
      "Epoch [1/5], Step [12450/136675], Loss: 6.1123\n",
      "Epoch [1/5], Step [12525/136675], Loss: 6.1690\n",
      "Epoch [1/5], Step [12600/136675], Loss: 6.0525\n",
      "Epoch [1/5], Step [12675/136675], Loss: 5.9297\n",
      "Epoch [1/5], Step [12750/136675], Loss: 6.0309\n",
      "Epoch [1/5], Step [12825/136675], Loss: 5.9924\n",
      "Epoch [1/5], Step [12900/136675], Loss: 6.1843\n",
      "Epoch [1/5], Step [12975/136675], Loss: 6.0472\n",
      "Epoch [1/5], Step [13050/136675], Loss: 5.9119\n",
      "Epoch [1/5], Step [13125/136675], Loss: 5.8826\n",
      "Epoch [1/5], Step [13200/136675], Loss: 5.9603\n",
      "Epoch [1/5], Step [13275/136675], Loss: 6.1008\n",
      "Epoch [1/5], Step [13350/136675], Loss: 5.4602\n",
      "Epoch [1/5], Step [13425/136675], Loss: 5.7096\n",
      "Epoch [1/5], Step [13500/136675], Loss: 5.8485\n",
      "Epoch [1/5], Step [13575/136675], Loss: 5.9306\n",
      "Epoch [1/5], Step [13650/136675], Loss: 5.7892\n",
      "Epoch [1/5], Step [13725/136675], Loss: 5.9658\n",
      "Epoch [1/5], Step [13800/136675], Loss: 5.8755\n",
      "Epoch [1/5], Step [13875/136675], Loss: 6.2172\n",
      "Epoch [1/5], Step [13950/136675], Loss: 5.8891\n",
      "Epoch [1/5], Step [14025/136675], Loss: 6.1389\n",
      "Epoch [1/5], Step [14100/136675], Loss: 6.1074\n",
      "Epoch [1/5], Step [14175/136675], Loss: 5.5968\n",
      "Epoch [1/5], Step [14250/136675], Loss: 6.0301\n",
      "Epoch [1/5], Step [14325/136675], Loss: 6.2455\n",
      "Epoch [1/5], Step [14400/136675], Loss: 5.6605\n",
      "Epoch [1/5], Step [14475/136675], Loss: 5.9095\n",
      "Epoch [1/5], Step [14550/136675], Loss: 5.7643\n",
      "Epoch [1/5], Step [14625/136675], Loss: 5.6870\n",
      "Epoch [1/5], Step [14700/136675], Loss: 5.9485\n",
      "Epoch [1/5], Step [14775/136675], Loss: 5.3260\n",
      "Epoch [1/5], Step [14850/136675], Loss: 6.0169\n",
      "Epoch [1/5], Step [14925/136675], Loss: 6.2280\n",
      "Epoch [1/5], Step [15000/136675], Loss: 6.3017\n",
      "Epoch [1/5], Step [15075/136675], Loss: 6.4492\n",
      "Epoch [1/5], Step [15150/136675], Loss: 5.9290\n",
      "Epoch [1/5], Step [15225/136675], Loss: 5.9714\n",
      "Epoch [1/5], Step [15300/136675], Loss: 5.7528\n",
      "Epoch [1/5], Step [15375/136675], Loss: 5.6466\n",
      "Epoch [1/5], Step [15450/136675], Loss: 5.8938\n",
      "Epoch [1/5], Step [15525/136675], Loss: 5.7526\n",
      "Epoch [1/5], Step [15600/136675], Loss: 5.7157\n",
      "Epoch [1/5], Step [15675/136675], Loss: 6.0888\n",
      "Epoch [1/5], Step [15750/136675], Loss: 6.2304\n",
      "Epoch [1/5], Step [15825/136675], Loss: 6.1507\n",
      "Epoch [1/5], Step [15900/136675], Loss: 5.7691\n",
      "Epoch [1/5], Step [15975/136675], Loss: 6.1407\n",
      "Epoch [1/5], Step [16050/136675], Loss: 5.8837\n",
      "Epoch [1/5], Step [16125/136675], Loss: 6.0261\n",
      "Epoch [1/5], Step [16200/136675], Loss: 5.9541\n",
      "Epoch [1/5], Step [16275/136675], Loss: 5.4995\n",
      "Epoch [1/5], Step [16350/136675], Loss: 5.9401\n",
      "Epoch [1/5], Step [16425/136675], Loss: 5.5518\n",
      "Epoch [1/5], Step [16500/136675], Loss: 5.6518\n",
      "Epoch [1/5], Step [16575/136675], Loss: 5.6091\n",
      "Epoch [1/5], Step [16650/136675], Loss: 5.7723\n",
      "Epoch [1/5], Step [16725/136675], Loss: 5.8883\n",
      "Epoch [1/5], Step [16800/136675], Loss: 5.7167\n",
      "Epoch [1/5], Step [16875/136675], Loss: 5.6687\n",
      "Epoch [1/5], Step [16950/136675], Loss: 6.0694\n",
      "Epoch [1/5], Step [17025/136675], Loss: 6.0192\n",
      "Epoch [1/5], Step [17100/136675], Loss: 5.5927\n",
      "Epoch [1/5], Step [17175/136675], Loss: 5.8200\n",
      "Epoch [1/5], Step [17250/136675], Loss: 5.7160\n",
      "Epoch [1/5], Step [17325/136675], Loss: 5.7949\n",
      "Epoch [1/5], Step [17400/136675], Loss: 5.9733\n",
      "Epoch [1/5], Step [17475/136675], Loss: 5.9615\n",
      "Epoch [1/5], Step [17550/136675], Loss: 5.6009\n",
      "Epoch [1/5], Step [17625/136675], Loss: 5.6207\n",
      "Epoch [1/5], Step [17700/136675], Loss: 5.7918\n",
      "Epoch [1/5], Step [17775/136675], Loss: 5.8582\n",
      "Epoch [1/5], Step [17850/136675], Loss: 5.8636\n",
      "Epoch [1/5], Step [17925/136675], Loss: 5.6707\n",
      "Epoch [1/5], Step [18000/136675], Loss: 5.8054\n",
      "Epoch [1/5], Step [18075/136675], Loss: 5.6789\n",
      "Epoch [1/5], Step [18150/136675], Loss: 6.0657\n",
      "Epoch [1/5], Step [18225/136675], Loss: 5.8573\n",
      "Epoch [1/5], Step [18300/136675], Loss: 6.0513\n",
      "Epoch [1/5], Step [18375/136675], Loss: 6.1251\n",
      "Epoch [1/5], Step [18450/136675], Loss: 5.4721\n",
      "Epoch [1/5], Step [18525/136675], Loss: 5.7292\n",
      "Epoch [1/5], Step [18600/136675], Loss: 5.7988\n",
      "Epoch [1/5], Step [18675/136675], Loss: 6.3061\n",
      "Epoch [1/5], Step [18750/136675], Loss: 5.8552\n",
      "Epoch [1/5], Step [18825/136675], Loss: 5.9722\n",
      "Epoch [1/5], Step [18900/136675], Loss: 5.9400\n",
      "Epoch [1/5], Step [18975/136675], Loss: 5.7090\n",
      "Epoch [1/5], Step [19050/136675], Loss: 5.7516\n",
      "Epoch [1/5], Step [19125/136675], Loss: 6.1199\n",
      "Epoch [1/5], Step [19200/136675], Loss: 5.7673\n",
      "Epoch [1/5], Step [19275/136675], Loss: 6.1877\n",
      "Epoch [1/5], Step [19350/136675], Loss: 5.6865\n",
      "Epoch [1/5], Step [19425/136675], Loss: 5.7675\n",
      "Epoch [1/5], Step [19500/136675], Loss: 5.6687\n",
      "Epoch [1/5], Step [19575/136675], Loss: 5.9918\n",
      "Epoch [1/5], Step [19650/136675], Loss: 5.8454\n",
      "Epoch [1/5], Step [19725/136675], Loss: 5.9634\n",
      "Epoch [1/5], Step [19800/136675], Loss: 6.0075\n",
      "Epoch [1/5], Step [19875/136675], Loss: 5.6452\n",
      "Epoch [1/5], Step [19950/136675], Loss: 6.2714\n",
      "Validation perplexity: 286.007933186517\n",
      "Epoch [1/5], Step [20025/136675], Loss: 5.7851\n",
      "Epoch [1/5], Step [20100/136675], Loss: 5.8810\n",
      "Epoch [1/5], Step [20175/136675], Loss: 5.9191\n",
      "Epoch [1/5], Step [20250/136675], Loss: 5.7907\n",
      "Epoch [1/5], Step [20325/136675], Loss: 5.8615\n",
      "Epoch [1/5], Step [20400/136675], Loss: 6.0459\n",
      "Epoch [1/5], Step [20475/136675], Loss: 5.6992\n",
      "Epoch [1/5], Step [20550/136675], Loss: 5.8459\n",
      "Epoch [1/5], Step [20625/136675], Loss: 5.6970\n",
      "Epoch [1/5], Step [20700/136675], Loss: 5.8259\n",
      "Epoch [1/5], Step [20775/136675], Loss: 5.5342\n",
      "Epoch [1/5], Step [20850/136675], Loss: 5.7636\n",
      "Epoch [1/5], Step [20925/136675], Loss: 6.1388\n",
      "Epoch [1/5], Step [21000/136675], Loss: 6.0715\n",
      "Epoch [1/5], Step [21075/136675], Loss: 5.5790\n",
      "Epoch [1/5], Step [21150/136675], Loss: 5.9840\n",
      "Epoch [1/5], Step [21225/136675], Loss: 6.0430\n",
      "Epoch [1/5], Step [21300/136675], Loss: 5.6142\n",
      "Epoch [1/5], Step [21375/136675], Loss: 5.6670\n",
      "Epoch [1/5], Step [21450/136675], Loss: 5.5715\n",
      "Epoch [1/5], Step [21525/136675], Loss: 5.7283\n",
      "Epoch [1/5], Step [21600/136675], Loss: 5.9849\n",
      "Epoch [1/5], Step [21675/136675], Loss: 6.0426\n",
      "Epoch [1/5], Step [21750/136675], Loss: 5.5533\n",
      "Epoch [1/5], Step [21825/136675], Loss: 5.8434\n",
      "Epoch [1/5], Step [21900/136675], Loss: 5.7838\n",
      "Epoch [1/5], Step [21975/136675], Loss: 5.9999\n",
      "Epoch [1/5], Step [22050/136675], Loss: 5.7622\n",
      "Epoch [1/5], Step [22125/136675], Loss: 5.6121\n",
      "Epoch [1/5], Step [22200/136675], Loss: 5.8314\n",
      "Epoch [1/5], Step [22275/136675], Loss: 5.8257\n",
      "Epoch [1/5], Step [22350/136675], Loss: 5.6762\n",
      "Epoch [1/5], Step [22425/136675], Loss: 5.9742\n",
      "Epoch [1/5], Step [22500/136675], Loss: 5.8850\n",
      "Epoch [1/5], Step [22575/136675], Loss: 5.6704\n",
      "Epoch [1/5], Step [22650/136675], Loss: 5.9445\n",
      "Epoch [1/5], Step [22725/136675], Loss: 5.4633\n",
      "Epoch [1/5], Step [22800/136675], Loss: 5.6129\n",
      "Epoch [1/5], Step [22875/136675], Loss: 5.5018\n",
      "Epoch [1/5], Step [22950/136675], Loss: 5.5057\n",
      "Epoch [1/5], Step [23025/136675], Loss: 5.7727\n",
      "Epoch [1/5], Step [23100/136675], Loss: 5.4402\n",
      "Epoch [1/5], Step [23175/136675], Loss: 5.7115\n",
      "Epoch [1/5], Step [23250/136675], Loss: 5.5474\n",
      "Epoch [1/5], Step [23325/136675], Loss: 5.9195\n",
      "Epoch [1/5], Step [23400/136675], Loss: 5.6624\n",
      "Epoch [1/5], Step [23475/136675], Loss: 5.5956\n",
      "Epoch [1/5], Step [23550/136675], Loss: 6.0612\n",
      "Epoch [1/5], Step [23625/136675], Loss: 5.7930\n",
      "Epoch [1/5], Step [23700/136675], Loss: 5.9800\n",
      "Epoch [1/5], Step [23775/136675], Loss: 5.8726\n",
      "Epoch [1/5], Step [23850/136675], Loss: 5.6977\n",
      "Epoch [1/5], Step [23925/136675], Loss: 5.6409\n",
      "Epoch [1/5], Step [24000/136675], Loss: 5.7268\n",
      "Epoch [1/5], Step [24075/136675], Loss: 5.7766\n",
      "Epoch [1/5], Step [24150/136675], Loss: 5.7430\n",
      "Epoch [1/5], Step [24225/136675], Loss: 5.7715\n",
      "Epoch [1/5], Step [24300/136675], Loss: 5.7512\n",
      "Epoch [1/5], Step [24375/136675], Loss: 5.6564\n",
      "Epoch [1/5], Step [24450/136675], Loss: 5.8667\n",
      "Epoch [1/5], Step [24525/136675], Loss: 5.7574\n",
      "Epoch [1/5], Step [24600/136675], Loss: 6.0571\n",
      "Epoch [1/5], Step [24675/136675], Loss: 5.6008\n",
      "Epoch [1/5], Step [24750/136675], Loss: 5.6398\n",
      "Epoch [1/5], Step [24825/136675], Loss: 5.8820\n",
      "Epoch [1/5], Step [24900/136675], Loss: 5.8117\n",
      "Epoch [1/5], Step [24975/136675], Loss: 5.7178\n",
      "Epoch [1/5], Step [25050/136675], Loss: 5.6477\n",
      "Epoch [1/5], Step [25125/136675], Loss: 5.6115\n",
      "Epoch [1/5], Step [25200/136675], Loss: 5.6887\n",
      "Epoch [1/5], Step [25275/136675], Loss: 5.7036\n",
      "Epoch [1/5], Step [25350/136675], Loss: 5.5201\n",
      "Epoch [1/5], Step [25425/136675], Loss: 5.5217\n",
      "Epoch [1/5], Step [25500/136675], Loss: 5.6708\n",
      "Epoch [1/5], Step [25575/136675], Loss: 5.7775\n",
      "Epoch [1/5], Step [25650/136675], Loss: 5.7359\n",
      "Epoch [1/5], Step [25725/136675], Loss: 5.4240\n",
      "Epoch [1/5], Step [25800/136675], Loss: 5.3888\n",
      "Epoch [1/5], Step [25875/136675], Loss: 5.7582\n",
      "Epoch [1/5], Step [25950/136675], Loss: 5.7407\n",
      "Epoch [1/5], Step [26025/136675], Loss: 5.6061\n",
      "Epoch [1/5], Step [26100/136675], Loss: 5.6943\n",
      "Epoch [1/5], Step [26175/136675], Loss: 5.7916\n",
      "Epoch [1/5], Step [26250/136675], Loss: 5.6045\n",
      "Epoch [1/5], Step [26325/136675], Loss: 5.5884\n",
      "Epoch [1/5], Step [26400/136675], Loss: 5.6191\n",
      "Epoch [1/5], Step [26475/136675], Loss: 5.5220\n",
      "Epoch [1/5], Step [26550/136675], Loss: 5.5457\n",
      "Epoch [1/5], Step [26625/136675], Loss: 5.5576\n",
      "Epoch [1/5], Step [26700/136675], Loss: 5.8512\n",
      "Epoch [1/5], Step [26775/136675], Loss: 5.6198\n",
      "Epoch [1/5], Step [26850/136675], Loss: 5.5640\n",
      "Epoch [1/5], Step [26925/136675], Loss: 5.6418\n",
      "Epoch [1/5], Step [27000/136675], Loss: 5.5145\n",
      "Epoch [1/5], Step [27075/136675], Loss: 5.9800\n",
      "Epoch [1/5], Step [27150/136675], Loss: 5.6761\n",
      "Epoch [1/5], Step [27225/136675], Loss: 6.0306\n",
      "Epoch [1/5], Step [27300/136675], Loss: 5.6005\n",
      "Epoch [1/5], Step [27375/136675], Loss: 5.4751\n",
      "Epoch [1/5], Step [27450/136675], Loss: 5.3971\n",
      "Epoch [1/5], Step [27525/136675], Loss: 5.6176\n",
      "Epoch [1/5], Step [27600/136675], Loss: 5.6985\n",
      "Epoch [1/5], Step [27675/136675], Loss: 5.6726\n",
      "Epoch [1/5], Step [27750/136675], Loss: 5.6997\n",
      "Epoch [1/5], Step [27825/136675], Loss: 5.6652\n",
      "Epoch [1/5], Step [27900/136675], Loss: 5.5490\n",
      "Epoch [1/5], Step [27975/136675], Loss: 5.7402\n",
      "Epoch [1/5], Step [28050/136675], Loss: 5.5321\n",
      "Epoch [1/5], Step [28125/136675], Loss: 5.8050\n",
      "Epoch [1/5], Step [28200/136675], Loss: 5.4383\n",
      "Epoch [1/5], Step [28275/136675], Loss: 5.6131\n",
      "Epoch [1/5], Step [28350/136675], Loss: 5.7448\n",
      "Epoch [1/5], Step [28425/136675], Loss: 5.6700\n",
      "Epoch [1/5], Step [28500/136675], Loss: 5.9278\n",
      "Epoch [1/5], Step [28575/136675], Loss: 5.7817\n",
      "Epoch [1/5], Step [28650/136675], Loss: 5.7838\n",
      "Epoch [1/5], Step [28725/136675], Loss: 5.7852\n",
      "Epoch [1/5], Step [28800/136675], Loss: 5.9191\n",
      "Epoch [1/5], Step [28875/136675], Loss: 6.1605\n",
      "Epoch [1/5], Step [28950/136675], Loss: 5.6050\n",
      "Epoch [1/5], Step [29025/136675], Loss: 5.5526\n",
      "Epoch [1/5], Step [29100/136675], Loss: 5.7120\n",
      "Epoch [1/5], Step [29175/136675], Loss: 5.8592\n",
      "Epoch [1/5], Step [29250/136675], Loss: 5.7467\n",
      "Epoch [1/5], Step [29325/136675], Loss: 5.4710\n",
      "Epoch [1/5], Step [29400/136675], Loss: 5.3906\n",
      "Epoch [1/5], Step [29475/136675], Loss: 5.5660\n",
      "Epoch [1/5], Step [29550/136675], Loss: 5.2125\n",
      "Epoch [1/5], Step [29625/136675], Loss: 5.8629\n",
      "Epoch [1/5], Step [29700/136675], Loss: 5.5175\n",
      "Epoch [1/5], Step [29775/136675], Loss: 5.8470\n",
      "Epoch [1/5], Step [29850/136675], Loss: 5.7934\n",
      "Epoch [1/5], Step [29925/136675], Loss: 5.7463\n",
      "Epoch [1/5], Step [30000/136675], Loss: 5.7404\n",
      "Validation perplexity: 249.75200715367475\n",
      "Epoch [1/5], Step [30075/136675], Loss: 5.7578\n",
      "Epoch [1/5], Step [30150/136675], Loss: 5.7579\n",
      "Epoch [1/5], Step [30225/136675], Loss: 5.7689\n",
      "Epoch [1/5], Step [30300/136675], Loss: 5.5629\n",
      "Epoch [1/5], Step [30375/136675], Loss: 5.5952\n",
      "Epoch [1/5], Step [30450/136675], Loss: 5.8263\n",
      "Epoch [1/5], Step [30525/136675], Loss: 5.8996\n",
      "Epoch [1/5], Step [30600/136675], Loss: 5.5680\n",
      "Epoch [1/5], Step [30675/136675], Loss: 5.7227\n",
      "Epoch [1/5], Step [30750/136675], Loss: 5.5864\n",
      "Epoch [1/5], Step [30825/136675], Loss: 5.7657\n",
      "Epoch [1/5], Step [30900/136675], Loss: 5.6159\n",
      "Epoch [1/5], Step [30975/136675], Loss: 5.4655\n",
      "Epoch [1/5], Step [31050/136675], Loss: 5.5187\n",
      "Epoch [1/5], Step [31125/136675], Loss: 5.7457\n",
      "Epoch [1/5], Step [31200/136675], Loss: 6.1666\n",
      "Epoch [1/5], Step [31275/136675], Loss: 5.6443\n",
      "Epoch [1/5], Step [31350/136675], Loss: 5.4382\n",
      "Epoch [1/5], Step [31425/136675], Loss: 5.6786\n",
      "Epoch [1/5], Step [31500/136675], Loss: 5.5296\n",
      "Epoch [1/5], Step [31575/136675], Loss: 5.5320\n",
      "Epoch [1/5], Step [31650/136675], Loss: 5.5512\n",
      "Epoch [1/5], Step [31725/136675], Loss: 5.4640\n",
      "Epoch [1/5], Step [31800/136675], Loss: 5.7686\n",
      "Epoch [1/5], Step [31875/136675], Loss: 5.7516\n",
      "Epoch [1/5], Step [31950/136675], Loss: 5.3029\n",
      "Epoch [1/5], Step [32025/136675], Loss: 5.9721\n",
      "Epoch [1/5], Step [32100/136675], Loss: 5.7153\n",
      "Epoch [1/5], Step [32175/136675], Loss: 5.6566\n",
      "Epoch [1/5], Step [32250/136675], Loss: 5.6376\n",
      "Epoch [1/5], Step [32325/136675], Loss: 5.8605\n",
      "Epoch [1/5], Step [32400/136675], Loss: 5.4961\n",
      "Epoch [1/5], Step [32475/136675], Loss: 5.6906\n",
      "Epoch [1/5], Step [32550/136675], Loss: 5.7230\n",
      "Epoch [1/5], Step [32625/136675], Loss: 5.6254\n",
      "Epoch [1/5], Step [32700/136675], Loss: 5.7414\n",
      "Epoch [1/5], Step [32775/136675], Loss: 5.5628\n",
      "Epoch [1/5], Step [32850/136675], Loss: 5.4916\n",
      "Epoch [1/5], Step [32925/136675], Loss: 5.8050\n",
      "Epoch [1/5], Step [33000/136675], Loss: 5.4972\n",
      "Epoch [1/5], Step [33075/136675], Loss: 5.5825\n",
      "Epoch [1/5], Step [33150/136675], Loss: 5.8862\n",
      "Epoch [1/5], Step [33225/136675], Loss: 5.4513\n",
      "Epoch [1/5], Step [33300/136675], Loss: 5.6588\n",
      "Epoch [1/5], Step [33375/136675], Loss: 5.8812\n",
      "Epoch [1/5], Step [33450/136675], Loss: 5.8866\n",
      "Epoch [1/5], Step [33525/136675], Loss: 5.5904\n",
      "Epoch [1/5], Step [33600/136675], Loss: 5.6301\n",
      "Epoch [1/5], Step [33675/136675], Loss: 5.6455\n",
      "Epoch [1/5], Step [33750/136675], Loss: 5.4763\n",
      "Epoch [1/5], Step [33825/136675], Loss: 5.8323\n",
      "Epoch [1/5], Step [33900/136675], Loss: 5.8965\n",
      "Epoch [1/5], Step [33975/136675], Loss: 5.9033\n",
      "Epoch [1/5], Step [34050/136675], Loss: 5.8628\n",
      "Epoch [1/5], Step [34125/136675], Loss: 5.5369\n",
      "Epoch [1/5], Step [34200/136675], Loss: 5.6487\n",
      "Epoch [1/5], Step [34275/136675], Loss: 5.5107\n",
      "Epoch [1/5], Step [34350/136675], Loss: 5.6759\n",
      "Epoch [1/5], Step [34425/136675], Loss: 5.7817\n",
      "Epoch [1/5], Step [34500/136675], Loss: 5.9875\n",
      "Epoch [1/5], Step [34575/136675], Loss: 5.2035\n",
      "Epoch [1/5], Step [34650/136675], Loss: 5.5801\n",
      "Epoch [1/5], Step [34725/136675], Loss: 5.5278\n",
      "Epoch [1/5], Step [34800/136675], Loss: 5.5976\n",
      "Epoch [1/5], Step [34875/136675], Loss: 5.7750\n",
      "Epoch [1/5], Step [34950/136675], Loss: 5.5144\n",
      "Epoch [1/5], Step [35025/136675], Loss: 5.9220\n",
      "Epoch [1/5], Step [35100/136675], Loss: 5.8274\n",
      "Epoch [1/5], Step [35175/136675], Loss: 5.5703\n",
      "Epoch [1/5], Step [35250/136675], Loss: 5.7072\n",
      "Epoch [1/5], Step [35325/136675], Loss: 5.6334\n",
      "Epoch [1/5], Step [35400/136675], Loss: 5.5680\n",
      "Epoch [1/5], Step [35475/136675], Loss: 5.6330\n",
      "Epoch [1/5], Step [35550/136675], Loss: 5.6510\n",
      "Epoch [1/5], Step [35625/136675], Loss: 6.0621\n",
      "Epoch [1/5], Step [35700/136675], Loss: 5.7981\n",
      "Epoch [1/5], Step [35775/136675], Loss: 6.3143\n",
      "Epoch [1/5], Step [35850/136675], Loss: 5.9054\n",
      "Epoch [1/5], Step [35925/136675], Loss: 5.4634\n",
      "Epoch [1/5], Step [36000/136675], Loss: 5.9850\n",
      "Epoch [1/5], Step [36075/136675], Loss: 5.4401\n",
      "Epoch [1/5], Step [36150/136675], Loss: 5.7056\n",
      "Epoch [1/5], Step [36225/136675], Loss: 5.7980\n",
      "Epoch [1/5], Step [36300/136675], Loss: 5.6198\n",
      "Epoch [1/5], Step [36375/136675], Loss: 5.8636\n",
      "Epoch [1/5], Step [36450/136675], Loss: 5.7277\n",
      "Epoch [1/5], Step [36525/136675], Loss: 5.8431\n",
      "Epoch [1/5], Step [36600/136675], Loss: 5.6276\n",
      "Epoch [1/5], Step [36675/136675], Loss: 5.5339\n",
      "Epoch [1/5], Step [36750/136675], Loss: 5.5227\n",
      "Epoch [1/5], Step [36825/136675], Loss: 5.5675\n",
      "Epoch [1/5], Step [36900/136675], Loss: 5.4733\n",
      "Epoch [1/5], Step [36975/136675], Loss: 5.6257\n",
      "Epoch [1/5], Step [37050/136675], Loss: 5.3005\n",
      "Epoch [1/5], Step [37125/136675], Loss: 5.5861\n",
      "Epoch [1/5], Step [37200/136675], Loss: 5.5857\n",
      "Epoch [1/5], Step [37275/136675], Loss: 5.8520\n",
      "Epoch [1/5], Step [37350/136675], Loss: 5.4948\n",
      "Epoch [1/5], Step [37425/136675], Loss: 5.5715\n",
      "Epoch [1/5], Step [37500/136675], Loss: 5.4669\n",
      "Epoch [1/5], Step [37575/136675], Loss: 5.9111\n",
      "Epoch [1/5], Step [37650/136675], Loss: 5.8537\n",
      "Epoch [1/5], Step [37725/136675], Loss: 5.5876\n",
      "Epoch [1/5], Step [37800/136675], Loss: 5.5639\n",
      "Epoch [1/5], Step [37875/136675], Loss: 5.3572\n",
      "Epoch [1/5], Step [37950/136675], Loss: 5.7057\n",
      "Epoch [1/5], Step [38025/136675], Loss: 6.0359\n",
      "Epoch [1/5], Step [38100/136675], Loss: 5.5212\n",
      "Epoch [1/5], Step [38175/136675], Loss: 5.7231\n",
      "Epoch [1/5], Step [38250/136675], Loss: 5.6570\n",
      "Epoch [1/5], Step [38325/136675], Loss: 5.8968\n",
      "Epoch [1/5], Step [38400/136675], Loss: 5.7019\n",
      "Epoch [1/5], Step [38475/136675], Loss: 5.4393\n",
      "Epoch [1/5], Step [38550/136675], Loss: 5.9840\n",
      "Epoch [1/5], Step [38625/136675], Loss: 5.6922\n",
      "Epoch [1/5], Step [38700/136675], Loss: 5.4633\n",
      "Epoch [1/5], Step [38775/136675], Loss: 5.8006\n",
      "Epoch [1/5], Step [38850/136675], Loss: 5.5789\n",
      "Epoch [1/5], Step [38925/136675], Loss: 5.6235\n",
      "Epoch [1/5], Step [39000/136675], Loss: 5.9564\n",
      "Epoch [1/5], Step [39075/136675], Loss: 5.7508\n",
      "Epoch [1/5], Step [39150/136675], Loss: 5.6037\n",
      "Epoch [1/5], Step [39225/136675], Loss: 5.4814\n",
      "Epoch [1/5], Step [39300/136675], Loss: 5.6886\n",
      "Epoch [1/5], Step [39375/136675], Loss: 5.6309\n",
      "Epoch [1/5], Step [39450/136675], Loss: 5.4185\n",
      "Epoch [1/5], Step [39525/136675], Loss: 5.6181\n",
      "Epoch [1/5], Step [39600/136675], Loss: 5.8870\n",
      "Epoch [1/5], Step [39675/136675], Loss: 5.5659\n",
      "Epoch [1/5], Step [39750/136675], Loss: 5.4627\n",
      "Epoch [1/5], Step [39825/136675], Loss: 5.4755\n",
      "Epoch [1/5], Step [39900/136675], Loss: 5.3873\n",
      "Epoch [1/5], Step [39975/136675], Loss: 5.7358\n",
      "Validation perplexity: 231.58820716427576\n",
      "Epoch [1/5], Step [40050/136675], Loss: 5.4846\n",
      "Epoch [1/5], Step [40125/136675], Loss: 5.7550\n",
      "Epoch [1/5], Step [40200/136675], Loss: 5.4603\n",
      "Epoch [1/5], Step [40275/136675], Loss: 5.7580\n",
      "Epoch [1/5], Step [40350/136675], Loss: 5.7001\n",
      "Epoch [1/5], Step [40425/136675], Loss: 5.5292\n",
      "Epoch [1/5], Step [40500/136675], Loss: 5.3239\n",
      "Epoch [1/5], Step [40575/136675], Loss: 5.5183\n",
      "Epoch [1/5], Step [40650/136675], Loss: 5.4379\n",
      "Epoch [1/5], Step [40725/136675], Loss: 5.8910\n",
      "Epoch [1/5], Step [40800/136675], Loss: 5.8366\n",
      "Epoch [1/5], Step [40875/136675], Loss: 5.2739\n",
      "Epoch [1/5], Step [40950/136675], Loss: 5.6451\n",
      "Epoch [1/5], Step [41025/136675], Loss: 5.6000\n",
      "Epoch [1/5], Step [41100/136675], Loss: 5.8432\n",
      "Epoch [1/5], Step [41175/136675], Loss: 5.6523\n",
      "Epoch [1/5], Step [41250/136675], Loss: 5.6792\n",
      "Epoch [1/5], Step [41325/136675], Loss: 6.0382\n",
      "Epoch [1/5], Step [41400/136675], Loss: 5.3664\n",
      "Epoch [1/5], Step [41475/136675], Loss: 5.6289\n",
      "Epoch [1/5], Step [41550/136675], Loss: 5.4191\n",
      "Epoch [1/5], Step [41625/136675], Loss: 5.8661\n",
      "Epoch [1/5], Step [41700/136675], Loss: 5.6996\n",
      "Epoch [1/5], Step [41775/136675], Loss: 5.6828\n",
      "Epoch [1/5], Step [41850/136675], Loss: 5.4086\n",
      "Epoch [1/5], Step [41925/136675], Loss: 5.7130\n",
      "Epoch [1/5], Step [42000/136675], Loss: 5.6742\n",
      "Epoch [1/5], Step [42075/136675], Loss: 5.6872\n",
      "Epoch [1/5], Step [42150/136675], Loss: 5.5271\n",
      "Epoch [1/5], Step [42225/136675], Loss: 5.6077\n",
      "Epoch [1/5], Step [42300/136675], Loss: 5.6124\n",
      "Epoch [1/5], Step [42375/136675], Loss: 5.6212\n",
      "Epoch [1/5], Step [42450/136675], Loss: 5.5726\n",
      "Epoch [1/5], Step [42525/136675], Loss: 5.4278\n",
      "Epoch [1/5], Step [42600/136675], Loss: 5.4719\n",
      "Epoch [1/5], Step [42675/136675], Loss: 5.9556\n",
      "Epoch [1/5], Step [42750/136675], Loss: 5.6241\n",
      "Epoch [1/5], Step [42825/136675], Loss: 5.7545\n",
      "Epoch [1/5], Step [42900/136675], Loss: 5.3230\n",
      "Epoch [1/5], Step [42975/136675], Loss: 5.7190\n",
      "Epoch [1/5], Step [43050/136675], Loss: 5.3397\n",
      "Epoch [1/5], Step [43125/136675], Loss: 5.7271\n",
      "Epoch [1/5], Step [43200/136675], Loss: 5.8863\n",
      "Epoch [1/5], Step [43275/136675], Loss: 5.4194\n",
      "Epoch [1/5], Step [43350/136675], Loss: 5.3574\n",
      "Epoch [1/5], Step [43425/136675], Loss: 4.9528\n",
      "Epoch [1/5], Step [43500/136675], Loss: 5.6625\n",
      "Epoch [1/5], Step [43575/136675], Loss: 5.4840\n",
      "Epoch [1/5], Step [43650/136675], Loss: 5.4042\n",
      "Epoch [1/5], Step [43725/136675], Loss: 5.6687\n",
      "Epoch [1/5], Step [43800/136675], Loss: 5.7039\n",
      "Epoch [1/5], Step [43875/136675], Loss: 5.6036\n",
      "Epoch [1/5], Step [43950/136675], Loss: 5.5600\n",
      "Epoch [1/5], Step [44025/136675], Loss: 5.5998\n",
      "Epoch [1/5], Step [44100/136675], Loss: 5.2982\n",
      "Epoch [1/5], Step [44175/136675], Loss: 5.5778\n",
      "Epoch [1/5], Step [44250/136675], Loss: 5.5238\n",
      "Epoch [1/5], Step [44325/136675], Loss: 5.6781\n",
      "Epoch [1/5], Step [44400/136675], Loss: 5.5050\n",
      "Epoch [1/5], Step [44475/136675], Loss: 5.6638\n",
      "Epoch [1/5], Step [44550/136675], Loss: 5.6255\n",
      "Epoch [1/5], Step [44625/136675], Loss: 5.7401\n",
      "Epoch [1/5], Step [44700/136675], Loss: 5.6432\n",
      "Epoch [1/5], Step [44775/136675], Loss: 5.4830\n",
      "Epoch [1/5], Step [44850/136675], Loss: 5.5948\n",
      "Epoch [1/5], Step [44925/136675], Loss: 5.5955\n",
      "Epoch [1/5], Step [45000/136675], Loss: 5.8461\n",
      "Epoch [1/5], Step [45075/136675], Loss: 5.0658\n",
      "Epoch [1/5], Step [45150/136675], Loss: 5.5614\n",
      "Epoch [1/5], Step [45225/136675], Loss: 5.6431\n",
      "Epoch [1/5], Step [45300/136675], Loss: 5.3348\n",
      "Epoch [1/5], Step [45375/136675], Loss: 5.8318\n",
      "Epoch [1/5], Step [45450/136675], Loss: 5.5682\n",
      "Epoch [1/5], Step [45525/136675], Loss: 5.5244\n",
      "Epoch [1/5], Step [45600/136675], Loss: 5.7157\n",
      "Epoch [1/5], Step [45675/136675], Loss: 5.4730\n",
      "Epoch [1/5], Step [45750/136675], Loss: 5.6365\n",
      "Epoch [1/5], Step [45825/136675], Loss: 5.6364\n",
      "Epoch [1/5], Step [45900/136675], Loss: 5.1032\n",
      "Epoch [1/5], Step [45975/136675], Loss: 5.3625\n",
      "Epoch [1/5], Step [46050/136675], Loss: 5.8055\n",
      "Epoch [1/5], Step [46125/136675], Loss: 5.6446\n",
      "Epoch [1/5], Step [46200/136675], Loss: 5.1608\n",
      "Epoch [1/5], Step [46275/136675], Loss: 5.9102\n",
      "Epoch [1/5], Step [46350/136675], Loss: 5.3545\n",
      "Epoch [1/5], Step [46425/136675], Loss: 5.5902\n",
      "Epoch [1/5], Step [46500/136675], Loss: 5.5321\n",
      "Epoch [1/5], Step [46575/136675], Loss: 5.7116\n",
      "Epoch [1/5], Step [46650/136675], Loss: 5.4217\n",
      "Epoch [1/5], Step [46725/136675], Loss: 5.6625\n",
      "Epoch [1/5], Step [46800/136675], Loss: 5.8501\n",
      "Epoch [1/5], Step [46875/136675], Loss: 5.5152\n",
      "Epoch [1/5], Step [46950/136675], Loss: 5.7653\n",
      "Epoch [1/5], Step [47025/136675], Loss: 5.4262\n",
      "Epoch [1/5], Step [47100/136675], Loss: 5.5674\n",
      "Epoch [1/5], Step [47175/136675], Loss: 5.6243\n",
      "Epoch [1/5], Step [47250/136675], Loss: 5.6091\n",
      "Epoch [1/5], Step [47325/136675], Loss: 5.4233\n",
      "Epoch [1/5], Step [47400/136675], Loss: 5.4200\n",
      "Epoch [1/5], Step [47475/136675], Loss: 5.6958\n",
      "Epoch [1/5], Step [47550/136675], Loss: 5.5521\n",
      "Epoch [1/5], Step [47625/136675], Loss: 5.6102\n",
      "Epoch [1/5], Step [47700/136675], Loss: 5.8014\n",
      "Epoch [1/5], Step [47775/136675], Loss: 5.7703\n",
      "Epoch [1/5], Step [47850/136675], Loss: 5.6058\n",
      "Epoch [1/5], Step [47925/136675], Loss: 5.9030\n",
      "Epoch [1/5], Step [48000/136675], Loss: 5.3382\n",
      "Epoch [1/5], Step [48075/136675], Loss: 5.5361\n",
      "Epoch [1/5], Step [48150/136675], Loss: 5.6607\n",
      "Epoch [1/5], Step [48225/136675], Loss: 5.7142\n",
      "Epoch [1/5], Step [48300/136675], Loss: 5.7631\n",
      "Epoch [1/5], Step [48375/136675], Loss: 5.7086\n",
      "Epoch [1/5], Step [48450/136675], Loss: 5.5121\n",
      "Epoch [1/5], Step [48525/136675], Loss: 5.5836\n",
      "Epoch [1/5], Step [48600/136675], Loss: 5.6411\n",
      "Epoch [1/5], Step [48675/136675], Loss: 5.4824\n",
      "Epoch [1/5], Step [48750/136675], Loss: 5.5307\n",
      "Epoch [1/5], Step [48825/136675], Loss: 5.5254\n",
      "Epoch [1/5], Step [48900/136675], Loss: 5.7949\n",
      "Epoch [1/5], Step [48975/136675], Loss: 5.6618\n",
      "Epoch [1/5], Step [49050/136675], Loss: 5.5870\n",
      "Epoch [1/5], Step [49125/136675], Loss: 5.9494\n",
      "Epoch [1/5], Step [49200/136675], Loss: 5.4946\n",
      "Epoch [1/5], Step [49275/136675], Loss: 5.5951\n",
      "Epoch [1/5], Step [49350/136675], Loss: 5.3743\n",
      "Epoch [1/5], Step [49425/136675], Loss: 5.4586\n",
      "Epoch [1/5], Step [49500/136675], Loss: 5.3863\n",
      "Epoch [1/5], Step [49575/136675], Loss: 5.5077\n",
      "Epoch [1/5], Step [49650/136675], Loss: 5.4117\n",
      "Epoch [1/5], Step [49725/136675], Loss: 5.2841\n",
      "Epoch [1/5], Step [49800/136675], Loss: 5.7310\n",
      "Epoch [1/5], Step [49875/136675], Loss: 5.5818\n",
      "Epoch [1/5], Step [49950/136675], Loss: 5.7225\n",
      "Validation perplexity: 219.12650881854742\n",
      "Epoch [1/5], Step [50025/136675], Loss: 5.4316\n",
      "Epoch [1/5], Step [50100/136675], Loss: 5.4929\n",
      "Epoch [1/5], Step [50175/136675], Loss: 5.3519\n",
      "Epoch [1/5], Step [50250/136675], Loss: 5.4601\n",
      "Epoch [1/5], Step [50325/136675], Loss: 5.7778\n",
      "Epoch [1/5], Step [50400/136675], Loss: 5.2794\n",
      "Epoch [1/5], Step [50475/136675], Loss: 5.5927\n",
      "Epoch [1/5], Step [50550/136675], Loss: 5.6886\n",
      "Epoch [1/5], Step [50625/136675], Loss: 5.4619\n",
      "Epoch [1/5], Step [50700/136675], Loss: 5.7389\n",
      "Epoch [1/5], Step [50775/136675], Loss: 5.5096\n",
      "Epoch [1/5], Step [50850/136675], Loss: 5.6660\n",
      "Epoch [1/5], Step [50925/136675], Loss: 5.6491\n",
      "Epoch [1/5], Step [51000/136675], Loss: 5.5452\n",
      "Epoch [1/5], Step [51075/136675], Loss: 5.3790\n",
      "Epoch [1/5], Step [51150/136675], Loss: 5.6924\n",
      "Epoch [1/5], Step [51225/136675], Loss: 5.4600\n",
      "Epoch [1/5], Step [51300/136675], Loss: 5.6381\n",
      "Epoch [1/5], Step [51375/136675], Loss: 5.4710\n",
      "Epoch [1/5], Step [51450/136675], Loss: 5.5047\n",
      "Epoch [1/5], Step [51525/136675], Loss: 5.5706\n",
      "Epoch [1/5], Step [51600/136675], Loss: 5.4725\n",
      "Epoch [1/5], Step [51675/136675], Loss: 5.3719\n",
      "Epoch [1/5], Step [51750/136675], Loss: 5.5157\n",
      "Epoch [1/5], Step [51825/136675], Loss: 5.4701\n",
      "Epoch [1/5], Step [51900/136675], Loss: 5.6507\n",
      "Epoch [1/5], Step [51975/136675], Loss: 5.7155\n",
      "Epoch [1/5], Step [52050/136675], Loss: 5.3878\n",
      "Epoch [1/5], Step [52125/136675], Loss: 5.5504\n",
      "Epoch [1/5], Step [52200/136675], Loss: 5.5723\n",
      "Epoch [1/5], Step [52275/136675], Loss: 5.3314\n",
      "Epoch [1/5], Step [52350/136675], Loss: 5.5073\n",
      "Epoch [1/5], Step [52425/136675], Loss: 5.9306\n",
      "Epoch [1/5], Step [52500/136675], Loss: 5.2971\n",
      "Epoch [1/5], Step [52575/136675], Loss: 5.3159\n",
      "Epoch [1/5], Step [52650/136675], Loss: 5.3849\n",
      "Epoch [1/5], Step [52725/136675], Loss: 5.8014\n",
      "Epoch [1/5], Step [52800/136675], Loss: 5.5379\n",
      "Epoch [1/5], Step [52875/136675], Loss: 5.7162\n",
      "Epoch [1/5], Step [52950/136675], Loss: 5.6147\n",
      "Epoch [1/5], Step [53025/136675], Loss: 5.5737\n",
      "Epoch [1/5], Step [53100/136675], Loss: 5.5887\n",
      "Epoch [1/5], Step [53175/136675], Loss: 5.5024\n",
      "Epoch [1/5], Step [53250/136675], Loss: 5.4944\n",
      "Epoch [1/5], Step [53325/136675], Loss: 5.4799\n",
      "Epoch [1/5], Step [53400/136675], Loss: 5.5572\n",
      "Epoch [1/5], Step [53475/136675], Loss: 5.5239\n",
      "Epoch [1/5], Step [53550/136675], Loss: 5.5782\n",
      "Epoch [1/5], Step [53625/136675], Loss: 5.2630\n",
      "Epoch [1/5], Step [53700/136675], Loss: 5.5656\n",
      "Epoch [1/5], Step [53775/136675], Loss: 5.4204\n",
      "Epoch [1/5], Step [53850/136675], Loss: 5.5826\n",
      "Epoch [1/5], Step [53925/136675], Loss: 5.6414\n",
      "Epoch [1/5], Step [54000/136675], Loss: 5.8319\n",
      "Epoch [1/5], Step [54075/136675], Loss: 5.7648\n",
      "Epoch [1/5], Step [54150/136675], Loss: 5.4596\n",
      "Epoch [1/5], Step [54225/136675], Loss: 5.4981\n",
      "Epoch [1/5], Step [54300/136675], Loss: 5.5077\n",
      "Epoch [1/5], Step [54375/136675], Loss: 5.6468\n",
      "Epoch [1/5], Step [54450/136675], Loss: 5.4410\n",
      "Epoch [1/5], Step [54525/136675], Loss: 5.6773\n",
      "Epoch [1/5], Step [54600/136675], Loss: 5.5390\n",
      "Epoch [1/5], Step [54675/136675], Loss: 5.3223\n",
      "Epoch [1/5], Step [54750/136675], Loss: 5.4636\n",
      "Epoch [1/5], Step [54825/136675], Loss: 5.5552\n",
      "Epoch [1/5], Step [54900/136675], Loss: 5.3698\n",
      "Epoch [1/5], Step [54975/136675], Loss: 5.6413\n",
      "Epoch [1/5], Step [55050/136675], Loss: 5.5694\n",
      "Epoch [1/5], Step [55125/136675], Loss: 5.3803\n",
      "Epoch [1/5], Step [55200/136675], Loss: 5.3140\n",
      "Epoch [1/5], Step [55275/136675], Loss: 5.3368\n",
      "Epoch [1/5], Step [55350/136675], Loss: 5.6681\n",
      "Epoch [1/5], Step [55425/136675], Loss: 5.5169\n",
      "Epoch [1/5], Step [55500/136675], Loss: 5.4442\n",
      "Epoch [1/5], Step [55575/136675], Loss: 5.3398\n",
      "Epoch [1/5], Step [55650/136675], Loss: 5.6100\n",
      "Epoch [1/5], Step [55725/136675], Loss: 5.3752\n",
      "Epoch [1/5], Step [55800/136675], Loss: 5.5094\n",
      "Epoch [1/5], Step [55875/136675], Loss: 5.5783\n",
      "Epoch [1/5], Step [55950/136675], Loss: 5.8395\n",
      "Epoch [1/5], Step [56025/136675], Loss: 5.4326\n",
      "Epoch [1/5], Step [56100/136675], Loss: 5.1782\n",
      "Epoch [1/5], Step [56175/136675], Loss: 5.5039\n",
      "Epoch [1/5], Step [56250/136675], Loss: 5.6099\n",
      "Epoch [1/5], Step [56325/136675], Loss: 5.2986\n",
      "Epoch [1/5], Step [56400/136675], Loss: 5.4593\n",
      "Epoch [1/5], Step [56475/136675], Loss: 5.3386\n",
      "Epoch [1/5], Step [56550/136675], Loss: 5.7519\n",
      "Epoch [1/5], Step [56625/136675], Loss: 5.7582\n",
      "Epoch [1/5], Step [56700/136675], Loss: 5.5020\n",
      "Epoch [1/5], Step [56775/136675], Loss: 5.5067\n",
      "Epoch [1/5], Step [56850/136675], Loss: 5.2974\n",
      "Epoch [1/5], Step [56925/136675], Loss: 5.7867\n",
      "Epoch [1/5], Step [57000/136675], Loss: 5.3637\n",
      "Epoch [1/5], Step [57075/136675], Loss: 5.4432\n",
      "Epoch [1/5], Step [57150/136675], Loss: 5.6513\n",
      "Epoch [1/5], Step [57225/136675], Loss: 5.6976\n",
      "Epoch [1/5], Step [57300/136675], Loss: 5.2851\n",
      "Epoch [1/5], Step [57375/136675], Loss: 5.4743\n",
      "Epoch [1/5], Step [57450/136675], Loss: 5.3761\n",
      "Epoch [1/5], Step [57525/136675], Loss: 5.4120\n",
      "Epoch [1/5], Step [57600/136675], Loss: 5.4548\n",
      "Epoch [1/5], Step [57675/136675], Loss: 5.6768\n",
      "Epoch [1/5], Step [57750/136675], Loss: 5.5745\n",
      "Epoch [1/5], Step [57825/136675], Loss: 5.6264\n",
      "Epoch [1/5], Step [57900/136675], Loss: 5.2179\n",
      "Epoch [1/5], Step [57975/136675], Loss: 5.3501\n",
      "Epoch [1/5], Step [58050/136675], Loss: 5.3581\n",
      "Epoch [1/5], Step [58125/136675], Loss: 5.4774\n",
      "Epoch [1/5], Step [58200/136675], Loss: 5.4595\n",
      "Epoch [1/5], Step [58275/136675], Loss: 5.5475\n",
      "Epoch [1/5], Step [58350/136675], Loss: 5.9552\n",
      "Epoch [1/5], Step [58425/136675], Loss: 5.5005\n",
      "Epoch [1/5], Step [58500/136675], Loss: 5.7641\n",
      "Epoch [1/5], Step [58575/136675], Loss: 5.5645\n",
      "Epoch [1/5], Step [58650/136675], Loss: 5.2881\n",
      "Epoch [1/5], Step [58725/136675], Loss: 5.2190\n",
      "Epoch [1/5], Step [58800/136675], Loss: 5.5316\n",
      "Epoch [1/5], Step [58875/136675], Loss: 5.5260\n",
      "Epoch [1/5], Step [58950/136675], Loss: 5.4484\n",
      "Epoch [1/5], Step [59025/136675], Loss: 5.4047\n",
      "Epoch [1/5], Step [59100/136675], Loss: 5.6000\n",
      "Epoch [1/5], Step [59175/136675], Loss: 5.6624\n",
      "Epoch [1/5], Step [59250/136675], Loss: 5.1285\n",
      "Epoch [1/5], Step [59325/136675], Loss: 5.4654\n",
      "Epoch [1/5], Step [59400/136675], Loss: 5.5300\n",
      "Epoch [1/5], Step [59475/136675], Loss: 5.4848\n",
      "Epoch [1/5], Step [59550/136675], Loss: 5.7031\n",
      "Epoch [1/5], Step [59625/136675], Loss: 5.5800\n",
      "Epoch [1/5], Step [59700/136675], Loss: 5.5366\n",
      "Epoch [1/5], Step [59775/136675], Loss: 5.1617\n",
      "Epoch [1/5], Step [59850/136675], Loss: 5.6256\n",
      "Epoch [1/5], Step [59925/136675], Loss: 5.2922\n",
      "Epoch [1/5], Step [60000/136675], Loss: 5.8272\n",
      "Validation perplexity: 207.46181042895267\n",
      "Epoch [1/5], Step [60075/136675], Loss: 5.6009\n",
      "Epoch [1/5], Step [60150/136675], Loss: 5.8287\n",
      "Epoch [1/5], Step [60225/136675], Loss: 5.6968\n",
      "Epoch [1/5], Step [60300/136675], Loss: 5.6228\n",
      "Epoch [1/5], Step [60375/136675], Loss: 4.9191\n",
      "Epoch [1/5], Step [60450/136675], Loss: 5.9518\n",
      "Epoch [1/5], Step [60525/136675], Loss: 5.1688\n",
      "Epoch [1/5], Step [60600/136675], Loss: 5.4944\n",
      "Epoch [1/5], Step [60675/136675], Loss: 5.5151\n",
      "Epoch [1/5], Step [60750/136675], Loss: 5.5994\n",
      "Epoch [1/5], Step [60825/136675], Loss: 5.0740\n",
      "Epoch [1/5], Step [60900/136675], Loss: 5.3798\n",
      "Epoch [1/5], Step [60975/136675], Loss: 5.5731\n",
      "Epoch [1/5], Step [61050/136675], Loss: 5.6033\n",
      "Epoch [1/5], Step [61125/136675], Loss: 5.2873\n",
      "Epoch [1/5], Step [61200/136675], Loss: 5.5066\n",
      "Epoch [1/5], Step [61275/136675], Loss: 5.4203\n",
      "Epoch [1/5], Step [61350/136675], Loss: 5.0907\n",
      "Epoch [1/5], Step [61425/136675], Loss: 5.7006\n",
      "Epoch [1/5], Step [61500/136675], Loss: 5.3073\n",
      "Epoch [1/5], Step [61575/136675], Loss: 5.5926\n",
      "Epoch [1/5], Step [61650/136675], Loss: 5.4548\n",
      "Epoch [1/5], Step [61725/136675], Loss: 5.6501\n",
      "Epoch [1/5], Step [61800/136675], Loss: 5.4917\n",
      "Epoch [1/5], Step [61875/136675], Loss: 5.1487\n",
      "Epoch [1/5], Step [61950/136675], Loss: 5.5848\n",
      "Epoch [1/5], Step [62025/136675], Loss: 5.5290\n",
      "Epoch [1/5], Step [62100/136675], Loss: 5.4446\n",
      "Epoch [1/5], Step [62175/136675], Loss: 5.3192\n",
      "Epoch [1/5], Step [62250/136675], Loss: 5.4328\n",
      "Epoch [1/5], Step [62325/136675], Loss: 5.4894\n",
      "Epoch [1/5], Step [62400/136675], Loss: 5.7578\n",
      "Epoch [1/5], Step [62475/136675], Loss: 5.5180\n",
      "Epoch [1/5], Step [62550/136675], Loss: 5.6504\n",
      "Epoch [1/5], Step [62625/136675], Loss: 5.4092\n",
      "Epoch [1/5], Step [62700/136675], Loss: 5.2927\n",
      "Epoch [1/5], Step [62775/136675], Loss: 5.5135\n",
      "Epoch [1/5], Step [62850/136675], Loss: 5.6398\n",
      "Epoch [1/5], Step [62925/136675], Loss: 5.6497\n",
      "Epoch [1/5], Step [63000/136675], Loss: 5.9314\n",
      "Epoch [1/5], Step [63075/136675], Loss: 5.6146\n",
      "Epoch [1/5], Step [63150/136675], Loss: 5.6274\n",
      "Epoch [1/5], Step [63225/136675], Loss: 5.4333\n",
      "Epoch [1/5], Step [63300/136675], Loss: 5.2659\n",
      "Epoch [1/5], Step [63375/136675], Loss: 5.4980\n",
      "Epoch [1/5], Step [63450/136675], Loss: 5.5513\n",
      "Epoch [1/5], Step [63525/136675], Loss: 5.7201\n",
      "Epoch [1/5], Step [63600/136675], Loss: 5.5644\n",
      "Epoch [1/5], Step [63675/136675], Loss: 5.3182\n",
      "Epoch [1/5], Step [63750/136675], Loss: 5.7485\n",
      "Epoch [1/5], Step [63825/136675], Loss: 5.5772\n",
      "Epoch [1/5], Step [63900/136675], Loss: 5.4506\n",
      "Epoch [1/5], Step [63975/136675], Loss: 5.4536\n",
      "Epoch [1/5], Step [64050/136675], Loss: 5.5513\n",
      "Epoch [1/5], Step [64125/136675], Loss: 5.5685\n",
      "Epoch [1/5], Step [64200/136675], Loss: 5.6481\n",
      "Epoch [1/5], Step [64275/136675], Loss: 5.4120\n",
      "Epoch [1/5], Step [64350/136675], Loss: 5.6490\n",
      "Epoch [1/5], Step [64425/136675], Loss: 5.4595\n",
      "Epoch [1/5], Step [64500/136675], Loss: 5.6126\n",
      "Epoch [1/5], Step [64575/136675], Loss: 5.3626\n",
      "Epoch [1/5], Step [64650/136675], Loss: 5.5223\n",
      "Epoch [1/5], Step [64725/136675], Loss: 5.6651\n",
      "Epoch [1/5], Step [64800/136675], Loss: 5.6082\n",
      "Epoch [1/5], Step [64875/136675], Loss: 5.4963\n",
      "Epoch [1/5], Step [64950/136675], Loss: 5.6243\n",
      "Epoch [1/5], Step [65025/136675], Loss: 5.3826\n",
      "Epoch [1/5], Step [65100/136675], Loss: 5.5653\n",
      "Epoch [1/5], Step [65175/136675], Loss: 5.4838\n",
      "Epoch [1/5], Step [65250/136675], Loss: 5.2264\n",
      "Epoch [1/5], Step [65325/136675], Loss: 5.4571\n",
      "Epoch [1/5], Step [65400/136675], Loss: 5.5724\n",
      "Epoch [1/5], Step [65475/136675], Loss: 5.4948\n",
      "Epoch [1/5], Step [65550/136675], Loss: 5.3680\n",
      "Epoch [1/5], Step [65625/136675], Loss: 5.3643\n",
      "Epoch [1/5], Step [65700/136675], Loss: 5.4427\n",
      "Epoch [1/5], Step [65775/136675], Loss: 5.7057\n",
      "Epoch [1/5], Step [65850/136675], Loss: 5.6230\n",
      "Epoch [1/5], Step [65925/136675], Loss: 5.4495\n",
      "Epoch [1/5], Step [66000/136675], Loss: 5.2663\n",
      "Epoch [1/5], Step [66075/136675], Loss: 5.7019\n",
      "Epoch [1/5], Step [66150/136675], Loss: 5.7621\n",
      "Epoch [1/5], Step [66225/136675], Loss: 5.4403\n",
      "Epoch [1/5], Step [66300/136675], Loss: 5.4338\n",
      "Epoch [1/5], Step [66375/136675], Loss: 5.3553\n",
      "Epoch [1/5], Step [66450/136675], Loss: 5.3515\n",
      "Epoch [1/5], Step [66525/136675], Loss: 5.4842\n",
      "Epoch [1/5], Step [66600/136675], Loss: 5.4381\n",
      "Epoch [1/5], Step [66675/136675], Loss: 5.6146\n",
      "Epoch [1/5], Step [66750/136675], Loss: 5.4265\n",
      "Epoch [1/5], Step [66825/136675], Loss: 5.3299\n",
      "Epoch [1/5], Step [66900/136675], Loss: 5.7276\n",
      "Epoch [1/5], Step [66975/136675], Loss: 5.7285\n",
      "Epoch [1/5], Step [67050/136675], Loss: 5.5519\n",
      "Epoch [1/5], Step [67125/136675], Loss: 5.3475\n",
      "Epoch [1/5], Step [67200/136675], Loss: 5.5793\n",
      "Epoch [1/5], Step [67275/136675], Loss: 5.1620\n",
      "Epoch [1/5], Step [67350/136675], Loss: 5.8447\n",
      "Epoch [1/5], Step [67425/136675], Loss: 5.4584\n",
      "Epoch [1/5], Step [67500/136675], Loss: 5.2548\n",
      "Epoch [1/5], Step [67575/136675], Loss: 5.2999\n",
      "Epoch [1/5], Step [67650/136675], Loss: 5.3417\n",
      "Epoch [1/5], Step [67725/136675], Loss: 5.3322\n",
      "Epoch [1/5], Step [67800/136675], Loss: 5.3468\n",
      "Epoch [1/5], Step [67875/136675], Loss: 5.8044\n",
      "Epoch [1/5], Step [67950/136675], Loss: 5.4814\n",
      "Epoch [1/5], Step [68025/136675], Loss: 5.5544\n",
      "Epoch [1/5], Step [68100/136675], Loss: 5.3853\n",
      "Epoch [1/5], Step [68175/136675], Loss: 5.1874\n",
      "Epoch [1/5], Step [68250/136675], Loss: 5.3006\n",
      "Epoch [1/5], Step [68325/136675], Loss: 5.6242\n",
      "Epoch [1/5], Step [68400/136675], Loss: 5.5672\n",
      "Epoch [1/5], Step [68475/136675], Loss: 5.3170\n",
      "Epoch [1/5], Step [68550/136675], Loss: 5.5638\n",
      "Epoch [1/5], Step [68625/136675], Loss: 5.4772\n",
      "Epoch [1/5], Step [68700/136675], Loss: 5.3479\n",
      "Epoch [1/5], Step [68775/136675], Loss: 5.3927\n",
      "Epoch [1/5], Step [68850/136675], Loss: 5.3501\n",
      "Epoch [1/5], Step [68925/136675], Loss: 5.4247\n",
      "Epoch [1/5], Step [69000/136675], Loss: 5.0590\n",
      "Epoch [1/5], Step [69075/136675], Loss: 5.3977\n",
      "Epoch [1/5], Step [69150/136675], Loss: 5.3788\n",
      "Epoch [1/5], Step [69225/136675], Loss: 5.3657\n",
      "Epoch [1/5], Step [69300/136675], Loss: 5.6690\n",
      "Epoch [1/5], Step [69375/136675], Loss: 5.1369\n",
      "Epoch [1/5], Step [69450/136675], Loss: 5.5636\n",
      "Epoch [1/5], Step [69525/136675], Loss: 5.3541\n",
      "Epoch [1/5], Step [69600/136675], Loss: 5.2734\n",
      "Epoch [1/5], Step [69675/136675], Loss: 5.4715\n",
      "Epoch [1/5], Step [69750/136675], Loss: 5.7057\n",
      "Epoch [1/5], Step [69825/136675], Loss: 5.4838\n",
      "Epoch [1/5], Step [69900/136675], Loss: 5.6869\n",
      "Epoch [1/5], Step [69975/136675], Loss: 5.4849\n",
      "Validation perplexity: 201.02657794765534\n",
      "Epoch [1/5], Step [70050/136675], Loss: 5.5033\n",
      "Epoch [1/5], Step [70125/136675], Loss: 5.5108\n",
      "Epoch [1/5], Step [70200/136675], Loss: 5.5816\n",
      "Epoch [1/5], Step [70275/136675], Loss: 5.3567\n",
      "Epoch [1/5], Step [70350/136675], Loss: 5.3560\n",
      "Epoch [1/5], Step [70425/136675], Loss: 5.3500\n",
      "Epoch [1/5], Step [70500/136675], Loss: 5.3357\n",
      "Epoch [1/5], Step [70575/136675], Loss: 5.5509\n",
      "Epoch [1/5], Step [70650/136675], Loss: 5.6242\n",
      "Epoch [1/5], Step [70725/136675], Loss: 5.2405\n",
      "Epoch [1/5], Step [70800/136675], Loss: 5.6762\n",
      "Epoch [1/5], Step [70875/136675], Loss: 5.2731\n",
      "Epoch [1/5], Step [70950/136675], Loss: 5.5420\n",
      "Epoch [1/5], Step [71025/136675], Loss: 5.4429\n",
      "Epoch [1/5], Step [71100/136675], Loss: 5.7358\n",
      "Epoch [1/5], Step [71175/136675], Loss: 5.2998\n",
      "Epoch [1/5], Step [71250/136675], Loss: 5.1934\n",
      "Epoch [1/5], Step [71325/136675], Loss: 5.5002\n",
      "Epoch [1/5], Step [71400/136675], Loss: 5.6405\n",
      "Epoch [1/5], Step [71475/136675], Loss: 5.4785\n",
      "Epoch [1/5], Step [71550/136675], Loss: 5.4831\n",
      "Epoch [1/5], Step [71625/136675], Loss: 5.4688\n",
      "Epoch [1/5], Step [71700/136675], Loss: 5.3597\n",
      "Epoch [1/5], Step [71775/136675], Loss: 5.3212\n",
      "Epoch [1/5], Step [71850/136675], Loss: 5.6413\n",
      "Epoch [1/5], Step [71925/136675], Loss: 5.5953\n",
      "Epoch [1/5], Step [72000/136675], Loss: 5.7716\n",
      "Epoch [1/5], Step [72075/136675], Loss: 5.6491\n",
      "Epoch [1/5], Step [72150/136675], Loss: 5.6765\n",
      "Epoch [1/5], Step [72225/136675], Loss: 5.2457\n",
      "Epoch [1/5], Step [72300/136675], Loss: 5.7953\n",
      "Epoch [1/5], Step [72375/136675], Loss: 5.3886\n",
      "Epoch [1/5], Step [72450/136675], Loss: 5.8087\n",
      "Epoch [1/5], Step [72525/136675], Loss: 5.4205\n",
      "Epoch [1/5], Step [72600/136675], Loss: 5.7386\n",
      "Epoch [1/5], Step [72675/136675], Loss: 5.5821\n",
      "Epoch [1/5], Step [72750/136675], Loss: 5.4730\n",
      "Epoch [1/5], Step [72825/136675], Loss: 5.4811\n",
      "Epoch [1/5], Step [72900/136675], Loss: 5.4190\n",
      "Epoch [1/5], Step [72975/136675], Loss: 5.3654\n",
      "Epoch [1/5], Step [73050/136675], Loss: 5.4076\n",
      "Epoch [1/5], Step [73125/136675], Loss: 5.9948\n",
      "Epoch [1/5], Step [73200/136675], Loss: 5.4318\n",
      "Epoch [1/5], Step [73275/136675], Loss: 5.6143\n",
      "Epoch [1/5], Step [73350/136675], Loss: 5.4777\n",
      "Epoch [1/5], Step [73425/136675], Loss: 5.5387\n",
      "Epoch [1/5], Step [73500/136675], Loss: 5.5433\n",
      "Epoch [1/5], Step [73575/136675], Loss: 5.5477\n",
      "Epoch [1/5], Step [73650/136675], Loss: 5.6814\n",
      "Epoch [1/5], Step [73725/136675], Loss: 5.0602\n",
      "Epoch [1/5], Step [73800/136675], Loss: 5.5077\n",
      "Epoch [1/5], Step [73875/136675], Loss: 5.5356\n",
      "Epoch [1/5], Step [73950/136675], Loss: 5.2039\n",
      "Epoch [1/5], Step [74025/136675], Loss: 5.4827\n",
      "Epoch [1/5], Step [74100/136675], Loss: 5.3256\n",
      "Epoch [1/5], Step [74175/136675], Loss: 5.2728\n",
      "Epoch [1/5], Step [74250/136675], Loss: 5.4294\n",
      "Epoch [1/5], Step [74325/136675], Loss: 5.3966\n",
      "Epoch [1/5], Step [74400/136675], Loss: 5.7047\n",
      "Epoch [1/5], Step [74475/136675], Loss: 5.0605\n",
      "Epoch [1/5], Step [74550/136675], Loss: 5.2921\n",
      "Epoch [1/5], Step [74625/136675], Loss: 5.7675\n",
      "Epoch [1/5], Step [74700/136675], Loss: 5.3615\n",
      "Epoch [1/5], Step [74775/136675], Loss: 5.7223\n",
      "Epoch [1/5], Step [74850/136675], Loss: 5.4613\n",
      "Epoch [1/5], Step [74925/136675], Loss: 5.6266\n",
      "Epoch [1/5], Step [75000/136675], Loss: 5.3786\n",
      "Epoch [1/5], Step [75075/136675], Loss: 5.3799\n",
      "Epoch [1/5], Step [75150/136675], Loss: 5.4990\n",
      "Epoch [1/5], Step [75225/136675], Loss: 5.2428\n",
      "Epoch [1/5], Step [75300/136675], Loss: 5.3807\n",
      "Epoch [1/5], Step [75375/136675], Loss: 5.4564\n",
      "Epoch [1/5], Step [75450/136675], Loss: 5.5770\n",
      "Epoch [1/5], Step [75525/136675], Loss: 5.5533\n",
      "Epoch [1/5], Step [75600/136675], Loss: 5.6893\n",
      "Epoch [1/5], Step [75675/136675], Loss: 5.6552\n",
      "Epoch [1/5], Step [75750/136675], Loss: 5.4673\n",
      "Epoch [1/5], Step [75825/136675], Loss: 5.5171\n",
      "Epoch [1/5], Step [75900/136675], Loss: 5.4402\n",
      "Epoch [1/5], Step [75975/136675], Loss: 5.4662\n",
      "Epoch [1/5], Step [76050/136675], Loss: 5.4789\n",
      "Epoch [1/5], Step [76125/136675], Loss: 5.4718\n",
      "Epoch [1/5], Step [76200/136675], Loss: 5.5638\n",
      "Epoch [1/5], Step [76275/136675], Loss: 5.6261\n",
      "Epoch [1/5], Step [76350/136675], Loss: 5.2838\n",
      "Epoch [1/5], Step [76425/136675], Loss: 5.2711\n",
      "Epoch [1/5], Step [76500/136675], Loss: 5.7444\n",
      "Epoch [1/5], Step [76575/136675], Loss: 5.6181\n",
      "Epoch [1/5], Step [76650/136675], Loss: 5.7151\n",
      "Epoch [1/5], Step [76725/136675], Loss: 5.5615\n",
      "Epoch [1/5], Step [76800/136675], Loss: 5.5298\n",
      "Epoch [1/5], Step [76875/136675], Loss: 5.7092\n",
      "Epoch [1/5], Step [76950/136675], Loss: 5.2015\n",
      "Epoch [1/5], Step [77025/136675], Loss: 5.4876\n",
      "Epoch [1/5], Step [77100/136675], Loss: 5.1056\n",
      "Epoch [1/5], Step [77175/136675], Loss: 5.4745\n",
      "Epoch [1/5], Step [77250/136675], Loss: 5.3999\n",
      "Epoch [1/5], Step [77325/136675], Loss: 5.3205\n",
      "Epoch [1/5], Step [77400/136675], Loss: 5.7993\n",
      "Epoch [1/5], Step [77475/136675], Loss: 5.3888\n",
      "Epoch [1/5], Step [77550/136675], Loss: 5.1659\n",
      "Epoch [1/5], Step [77625/136675], Loss: 5.3310\n",
      "Epoch [1/5], Step [77700/136675], Loss: 5.2689\n",
      "Epoch [1/5], Step [77775/136675], Loss: 5.5777\n",
      "Epoch [1/5], Step [77850/136675], Loss: 5.4850\n",
      "Epoch [1/5], Step [77925/136675], Loss: 5.5255\n",
      "Epoch [1/5], Step [78000/136675], Loss: 5.6116\n",
      "Epoch [1/5], Step [78075/136675], Loss: 5.4341\n",
      "Epoch [1/5], Step [78150/136675], Loss: 5.7295\n",
      "Epoch [1/5], Step [78225/136675], Loss: 5.3405\n",
      "Epoch [1/5], Step [78300/136675], Loss: 5.3061\n",
      "Epoch [1/5], Step [78375/136675], Loss: 5.5121\n",
      "Epoch [1/5], Step [78450/136675], Loss: 5.3853\n",
      "Epoch [1/5], Step [78525/136675], Loss: 5.3294\n",
      "Epoch [1/5], Step [78600/136675], Loss: 5.3355\n",
      "Epoch [1/5], Step [78675/136675], Loss: 5.3228\n",
      "Epoch [1/5], Step [78750/136675], Loss: 5.2574\n",
      "Epoch [1/5], Step [78825/136675], Loss: 5.3016\n",
      "Epoch [1/5], Step [78900/136675], Loss: 5.7235\n",
      "Epoch [1/5], Step [78975/136675], Loss: 5.4204\n",
      "Epoch [1/5], Step [79050/136675], Loss: 5.7135\n",
      "Epoch [1/5], Step [79125/136675], Loss: 5.5931\n",
      "Epoch [1/5], Step [79200/136675], Loss: 5.6719\n",
      "Epoch [1/5], Step [79275/136675], Loss: 5.5684\n",
      "Epoch [1/5], Step [79350/136675], Loss: 5.6471\n",
      "Epoch [1/5], Step [79425/136675], Loss: 5.2988\n",
      "Epoch [1/5], Step [79500/136675], Loss: 5.3724\n",
      "Epoch [1/5], Step [79575/136675], Loss: 5.4539\n",
      "Epoch [1/5], Step [79650/136675], Loss: 5.4772\n",
      "Epoch [1/5], Step [79725/136675], Loss: 5.3355\n",
      "Epoch [1/5], Step [79800/136675], Loss: 5.4759\n",
      "Epoch [1/5], Step [79875/136675], Loss: 5.5323\n",
      "Epoch [1/5], Step [79950/136675], Loss: 5.6090\n",
      "Validation perplexity: 197.0474923678599\n",
      "Epoch [1/5], Step [80025/136675], Loss: 5.7285\n",
      "Epoch [1/5], Step [80100/136675], Loss: 5.7821\n",
      "Epoch [1/5], Step [80175/136675], Loss: 5.4650\n",
      "Epoch [1/5], Step [80250/136675], Loss: 5.6184\n",
      "Epoch [1/5], Step [80325/136675], Loss: 5.5123\n",
      "Epoch [1/5], Step [80400/136675], Loss: 5.4490\n",
      "Epoch [1/5], Step [80475/136675], Loss: 5.5795\n",
      "Epoch [1/5], Step [80550/136675], Loss: 5.2501\n",
      "Epoch [1/5], Step [80625/136675], Loss: 5.9631\n",
      "Epoch [1/5], Step [80700/136675], Loss: 5.3048\n",
      "Epoch [1/5], Step [80775/136675], Loss: 5.4460\n",
      "Epoch [1/5], Step [80850/136675], Loss: 5.5893\n",
      "Epoch [1/5], Step [80925/136675], Loss: 5.4565\n",
      "Epoch [1/5], Step [81000/136675], Loss: 5.4935\n",
      "Epoch [1/5], Step [81075/136675], Loss: 5.5112\n",
      "Epoch [1/5], Step [81150/136675], Loss: 5.1778\n",
      "Epoch [1/5], Step [81225/136675], Loss: 5.3749\n",
      "Epoch [1/5], Step [81300/136675], Loss: 5.8708\n",
      "Epoch [1/5], Step [81375/136675], Loss: 5.6076\n",
      "Epoch [1/5], Step [81450/136675], Loss: 5.1365\n",
      "Epoch [1/5], Step [81525/136675], Loss: 5.4677\n",
      "Epoch [1/5], Step [81600/136675], Loss: 5.3282\n",
      "Epoch [1/5], Step [81675/136675], Loss: 5.3711\n",
      "Epoch [1/5], Step [81750/136675], Loss: 5.7209\n",
      "Epoch [1/5], Step [81825/136675], Loss: 5.4112\n",
      "Epoch [1/5], Step [81900/136675], Loss: 5.4004\n",
      "Epoch [1/5], Step [81975/136675], Loss: 5.5409\n",
      "Epoch [1/5], Step [82050/136675], Loss: 5.4127\n",
      "Epoch [1/5], Step [82125/136675], Loss: 5.2762\n",
      "Epoch [1/5], Step [82200/136675], Loss: 5.5858\n",
      "Epoch [1/5], Step [82275/136675], Loss: 5.6697\n",
      "Epoch [1/5], Step [82350/136675], Loss: 5.2761\n",
      "Epoch [1/5], Step [82425/136675], Loss: 5.3283\n",
      "Epoch [1/5], Step [82500/136675], Loss: 5.4448\n",
      "Epoch [1/5], Step [82575/136675], Loss: 5.3030\n",
      "Epoch [1/5], Step [82650/136675], Loss: 5.4740\n",
      "Epoch [1/5], Step [82725/136675], Loss: 5.2096\n",
      "Epoch [1/5], Step [82800/136675], Loss: 5.4978\n",
      "Epoch [1/5], Step [82875/136675], Loss: 5.1202\n",
      "Epoch [1/5], Step [82950/136675], Loss: 5.6681\n",
      "Epoch [1/5], Step [83025/136675], Loss: 5.4301\n",
      "Epoch [1/5], Step [83100/136675], Loss: 5.8807\n",
      "Epoch [1/5], Step [83175/136675], Loss: 5.4028\n",
      "Epoch [1/5], Step [83250/136675], Loss: 5.0284\n",
      "Epoch [1/5], Step [83325/136675], Loss: 5.5270\n",
      "Epoch [1/5], Step [83400/136675], Loss: 5.6731\n",
      "Epoch [1/5], Step [83475/136675], Loss: 5.6693\n",
      "Epoch [1/5], Step [83550/136675], Loss: 5.4583\n",
      "Epoch [1/5], Step [83625/136675], Loss: 5.0381\n",
      "Epoch [1/5], Step [83700/136675], Loss: 5.3042\n",
      "Epoch [1/5], Step [83775/136675], Loss: 5.5985\n",
      "Epoch [1/5], Step [83850/136675], Loss: 5.4844\n",
      "Epoch [1/5], Step [83925/136675], Loss: 5.5266\n",
      "Epoch [1/5], Step [84000/136675], Loss: 5.7016\n",
      "Epoch [1/5], Step [84075/136675], Loss: 5.3027\n",
      "Epoch [1/5], Step [84150/136675], Loss: 5.6006\n",
      "Epoch [1/5], Step [84225/136675], Loss: 5.1037\n",
      "Epoch [1/5], Step [84300/136675], Loss: 5.3136\n",
      "Epoch [1/5], Step [84375/136675], Loss: 5.2124\n",
      "Epoch [1/5], Step [84450/136675], Loss: 5.2506\n",
      "Epoch [1/5], Step [84525/136675], Loss: 5.4492\n",
      "Epoch [1/5], Step [84600/136675], Loss: 5.4993\n",
      "Epoch [1/5], Step [84675/136675], Loss: 5.7162\n",
      "Epoch [1/5], Step [84750/136675], Loss: 5.4352\n",
      "Epoch [1/5], Step [84825/136675], Loss: 5.3938\n",
      "Epoch [1/5], Step [84900/136675], Loss: 5.4389\n",
      "Epoch [1/5], Step [84975/136675], Loss: 5.3628\n",
      "Epoch [1/5], Step [85050/136675], Loss: 5.7459\n",
      "Epoch [1/5], Step [85125/136675], Loss: 5.3368\n",
      "Epoch [1/5], Step [85200/136675], Loss: 5.4383\n",
      "Epoch [1/5], Step [85275/136675], Loss: 5.2320\n",
      "Epoch [1/5], Step [85350/136675], Loss: 5.5801\n",
      "Epoch [1/5], Step [85425/136675], Loss: 5.3040\n",
      "Epoch [1/5], Step [85500/136675], Loss: 5.4249\n",
      "Epoch [1/5], Step [85575/136675], Loss: 5.3496\n",
      "Epoch [1/5], Step [85650/136675], Loss: 5.6098\n",
      "Epoch [1/5], Step [85725/136675], Loss: 5.6776\n",
      "Epoch [1/5], Step [85800/136675], Loss: 5.2257\n",
      "Epoch [1/5], Step [85875/136675], Loss: 5.4608\n",
      "Epoch [1/5], Step [85950/136675], Loss: 5.3404\n",
      "Epoch [1/5], Step [86025/136675], Loss: 5.4629\n",
      "Epoch [1/5], Step [86100/136675], Loss: 5.5057\n",
      "Epoch [1/5], Step [86175/136675], Loss: 5.2022\n",
      "Epoch [1/5], Step [86250/136675], Loss: 5.5521\n",
      "Epoch [1/5], Step [86325/136675], Loss: 5.4239\n",
      "Epoch [1/5], Step [86400/136675], Loss: 5.5515\n",
      "Epoch [1/5], Step [86475/136675], Loss: 5.3333\n",
      "Epoch [1/5], Step [86550/136675], Loss: 5.4271\n",
      "Epoch [1/5], Step [86625/136675], Loss: 5.3154\n",
      "Epoch [1/5], Step [86700/136675], Loss: 5.5619\n",
      "Epoch [1/5], Step [86775/136675], Loss: 5.7948\n",
      "Epoch [1/5], Step [86850/136675], Loss: 5.6207\n",
      "Epoch [1/5], Step [86925/136675], Loss: 5.5801\n",
      "Epoch [1/5], Step [87000/136675], Loss: 5.5192\n",
      "Epoch [1/5], Step [87075/136675], Loss: 5.1707\n",
      "Epoch [1/5], Step [87150/136675], Loss: 5.4082\n",
      "Epoch [1/5], Step [87225/136675], Loss: 5.3405\n",
      "Epoch [1/5], Step [87300/136675], Loss: 5.2841\n",
      "Epoch [1/5], Step [87375/136675], Loss: 5.6479\n",
      "Epoch [1/5], Step [87450/136675], Loss: 5.6173\n",
      "Epoch [1/5], Step [87525/136675], Loss: 5.5323\n",
      "Epoch [1/5], Step [87600/136675], Loss: 5.4396\n",
      "Epoch [1/5], Step [87675/136675], Loss: 5.4420\n",
      "Epoch [1/5], Step [87750/136675], Loss: 5.4565\n",
      "Epoch [1/5], Step [87825/136675], Loss: 5.2731\n",
      "Epoch [1/5], Step [87900/136675], Loss: 5.5836\n",
      "Epoch [1/5], Step [87975/136675], Loss: 5.4640\n",
      "Epoch [1/5], Step [88050/136675], Loss: 5.5705\n",
      "Epoch [1/5], Step [88125/136675], Loss: 5.6954\n",
      "Epoch [1/5], Step [88200/136675], Loss: 5.0002\n",
      "Epoch [1/5], Step [88275/136675], Loss: 5.2496\n",
      "Epoch [1/5], Step [88350/136675], Loss: 5.4959\n",
      "Epoch [1/5], Step [88425/136675], Loss: 5.5249\n",
      "Epoch [1/5], Step [88500/136675], Loss: 4.9680\n",
      "Epoch [1/5], Step [88575/136675], Loss: 5.6125\n",
      "Epoch [1/5], Step [88650/136675], Loss: 5.3376\n",
      "Epoch [1/5], Step [88725/136675], Loss: 5.4661\n",
      "Epoch [1/5], Step [88800/136675], Loss: 5.5528\n",
      "Epoch [1/5], Step [88875/136675], Loss: 5.3853\n",
      "Epoch [1/5], Step [88950/136675], Loss: 5.4014\n",
      "Epoch [1/5], Step [89025/136675], Loss: 5.5944\n",
      "Epoch [1/5], Step [89100/136675], Loss: 5.6741\n",
      "Epoch [1/5], Step [89175/136675], Loss: 5.4200\n",
      "Epoch [1/5], Step [89250/136675], Loss: 5.4903\n",
      "Epoch [1/5], Step [89325/136675], Loss: 5.6934\n",
      "Epoch [1/5], Step [89400/136675], Loss: 5.6653\n",
      "Epoch [1/5], Step [89475/136675], Loss: 5.5936\n",
      "Epoch [1/5], Step [89550/136675], Loss: 5.6124\n",
      "Epoch [1/5], Step [89625/136675], Loss: 5.6765\n",
      "Epoch [1/5], Step [89700/136675], Loss: 5.1386\n",
      "Epoch [1/5], Step [89775/136675], Loss: 5.5362\n",
      "Epoch [1/5], Step [89850/136675], Loss: 5.4353\n",
      "Epoch [1/5], Step [89925/136675], Loss: 5.4017\n",
      "Epoch [1/5], Step [90000/136675], Loss: 5.3129\n",
      "Validation perplexity: 191.80860010192504\n",
      "Epoch [1/5], Step [90075/136675], Loss: 5.0996\n",
      "Epoch [1/5], Step [90150/136675], Loss: 5.3899\n",
      "Epoch [1/5], Step [90225/136675], Loss: 5.5053\n",
      "Epoch [1/5], Step [90300/136675], Loss: 4.7272\n",
      "Epoch [1/5], Step [90375/136675], Loss: 4.9637\n",
      "Epoch [1/5], Step [90450/136675], Loss: 5.3584\n",
      "Epoch [1/5], Step [90525/136675], Loss: 5.7810\n",
      "Epoch [1/5], Step [90600/136675], Loss: 5.0372\n",
      "Epoch [1/5], Step [90675/136675], Loss: 5.3736\n",
      "Epoch [1/5], Step [90750/136675], Loss: 5.3038\n",
      "Epoch [1/5], Step [90825/136675], Loss: 5.2276\n",
      "Epoch [1/5], Step [90900/136675], Loss: 5.4796\n",
      "Epoch [1/5], Step [90975/136675], Loss: 5.7548\n",
      "Epoch [1/5], Step [91050/136675], Loss: 5.2891\n",
      "Epoch [1/5], Step [91125/136675], Loss: 5.7096\n",
      "Epoch [1/5], Step [91200/136675], Loss: 5.2937\n",
      "Epoch [1/5], Step [91275/136675], Loss: 5.3077\n",
      "Epoch [1/5], Step [91350/136675], Loss: 5.4718\n",
      "Epoch [1/5], Step [91425/136675], Loss: 5.7467\n",
      "Epoch [1/5], Step [91500/136675], Loss: 5.3596\n",
      "Epoch [1/5], Step [91575/136675], Loss: 5.3732\n",
      "Epoch [1/5], Step [91650/136675], Loss: 5.4503\n",
      "Epoch [1/5], Step [91725/136675], Loss: 5.5253\n",
      "Epoch [1/5], Step [91800/136675], Loss: 5.6590\n",
      "Epoch [1/5], Step [91875/136675], Loss: 5.3521\n",
      "Epoch [1/5], Step [91950/136675], Loss: 5.5919\n",
      "Epoch [1/5], Step [92025/136675], Loss: 5.3251\n",
      "Epoch [1/5], Step [92100/136675], Loss: 5.1407\n",
      "Epoch [1/5], Step [92175/136675], Loss: 5.2755\n",
      "Epoch [1/5], Step [92250/136675], Loss: 5.5568\n",
      "Epoch [1/5], Step [92325/136675], Loss: 5.4325\n",
      "Epoch [1/5], Step [92400/136675], Loss: 5.6078\n",
      "Epoch [1/5], Step [92475/136675], Loss: 5.3097\n",
      "Epoch [1/5], Step [92550/136675], Loss: 5.5355\n",
      "Epoch [1/5], Step [92625/136675], Loss: 5.4197\n",
      "Epoch [1/5], Step [92700/136675], Loss: 5.6097\n",
      "Epoch [1/5], Step [92775/136675], Loss: 5.1342\n",
      "Epoch [1/5], Step [92850/136675], Loss: 5.4139\n",
      "Epoch [1/5], Step [92925/136675], Loss: 5.5931\n",
      "Epoch [1/5], Step [93000/136675], Loss: 5.5125\n",
      "Epoch [1/5], Step [93075/136675], Loss: 5.4787\n",
      "Epoch [1/5], Step [93150/136675], Loss: 5.5285\n",
      "Epoch [1/5], Step [93225/136675], Loss: 5.4935\n",
      "Epoch [1/5], Step [93300/136675], Loss: 5.5799\n",
      "Epoch [1/5], Step [93375/136675], Loss: 5.5226\n",
      "Epoch [1/5], Step [93450/136675], Loss: 5.6363\n",
      "Epoch [1/5], Step [93525/136675], Loss: 5.6804\n",
      "Epoch [1/5], Step [93600/136675], Loss: 5.4891\n",
      "Epoch [1/5], Step [93675/136675], Loss: 5.4454\n",
      "Epoch [1/5], Step [93750/136675], Loss: 5.3235\n",
      "Epoch [1/5], Step [93825/136675], Loss: 5.6686\n",
      "Epoch [1/5], Step [93900/136675], Loss: 5.3060\n",
      "Epoch [1/5], Step [93975/136675], Loss: 5.2444\n",
      "Epoch [1/5], Step [94050/136675], Loss: 5.2845\n",
      "Epoch [1/5], Step [94125/136675], Loss: 5.4971\n",
      "Epoch [1/5], Step [94200/136675], Loss: 5.4969\n",
      "Epoch [1/5], Step [94275/136675], Loss: 5.6461\n",
      "Epoch [1/5], Step [94350/136675], Loss: 5.3053\n",
      "Epoch [1/5], Step [94425/136675], Loss: 5.4787\n",
      "Epoch [1/5], Step [94500/136675], Loss: 5.2145\n",
      "Epoch [1/5], Step [94575/136675], Loss: 5.2831\n",
      "Epoch [1/5], Step [94650/136675], Loss: 5.4167\n",
      "Epoch [1/5], Step [94725/136675], Loss: 4.9849\n",
      "Epoch [1/5], Step [94800/136675], Loss: 5.4903\n",
      "Epoch [1/5], Step [94875/136675], Loss: 5.3979\n",
      "Epoch [1/5], Step [94950/136675], Loss: 5.4354\n",
      "Epoch [1/5], Step [95025/136675], Loss: 5.6866\n",
      "Epoch [1/5], Step [95100/136675], Loss: 5.2909\n",
      "Epoch [1/5], Step [95175/136675], Loss: 5.4284\n",
      "Epoch [1/5], Step [95250/136675], Loss: 5.0630\n",
      "Epoch [1/5], Step [95325/136675], Loss: 5.5682\n",
      "Epoch [1/5], Step [95400/136675], Loss: 5.1579\n",
      "Epoch [1/5], Step [95475/136675], Loss: 5.7221\n",
      "Epoch [1/5], Step [95550/136675], Loss: 5.2065\n",
      "Epoch [1/5], Step [95625/136675], Loss: 5.4214\n",
      "Epoch [1/5], Step [95700/136675], Loss: 5.0838\n",
      "Epoch [1/5], Step [95775/136675], Loss: 5.4162\n",
      "Epoch [1/5], Step [95850/136675], Loss: 5.5167\n",
      "Epoch [1/5], Step [95925/136675], Loss: 5.2981\n",
      "Epoch [1/5], Step [96000/136675], Loss: 5.4205\n",
      "Epoch [1/5], Step [96075/136675], Loss: 5.6162\n",
      "Epoch [1/5], Step [96150/136675], Loss: 5.6287\n",
      "Epoch [1/5], Step [96225/136675], Loss: 5.3054\n",
      "Epoch [1/5], Step [96300/136675], Loss: 5.8560\n",
      "Epoch [1/5], Step [96375/136675], Loss: 5.3865\n",
      "Epoch [1/5], Step [96450/136675], Loss: 5.4804\n",
      "Epoch [1/5], Step [96525/136675], Loss: 5.5792\n",
      "Epoch [1/5], Step [96600/136675], Loss: 5.4504\n",
      "Epoch [1/5], Step [96675/136675], Loss: 5.4776\n",
      "Epoch [1/5], Step [96750/136675], Loss: 5.2792\n",
      "Epoch [1/5], Step [96825/136675], Loss: 5.6400\n",
      "Epoch [1/5], Step [96900/136675], Loss: 5.1765\n",
      "Epoch [1/5], Step [96975/136675], Loss: 5.3510\n",
      "Epoch [1/5], Step [97050/136675], Loss: 5.3195\n",
      "Epoch [1/5], Step [97125/136675], Loss: 5.4696\n",
      "Epoch [1/5], Step [97200/136675], Loss: 5.5951\n",
      "Epoch [1/5], Step [97275/136675], Loss: 5.3998\n",
      "Epoch [1/5], Step [97350/136675], Loss: 5.5685\n",
      "Epoch [1/5], Step [97425/136675], Loss: 5.5713\n",
      "Epoch [1/5], Step [97500/136675], Loss: 5.0117\n",
      "Epoch [1/5], Step [97575/136675], Loss: 5.3451\n",
      "Epoch [1/5], Step [97650/136675], Loss: 5.3077\n",
      "Epoch [1/5], Step [97725/136675], Loss: 5.5634\n",
      "Epoch [1/5], Step [97800/136675], Loss: 5.5025\n",
      "Epoch [1/5], Step [97875/136675], Loss: 5.4053\n",
      "Epoch [1/5], Step [97950/136675], Loss: 5.2259\n",
      "Epoch [1/5], Step [98025/136675], Loss: 5.7930\n",
      "Epoch [1/5], Step [98100/136675], Loss: 5.5049\n",
      "Epoch [1/5], Step [98175/136675], Loss: 5.4893\n",
      "Epoch [1/5], Step [98250/136675], Loss: 5.3597\n",
      "Epoch [1/5], Step [98325/136675], Loss: 5.3690\n",
      "Epoch [1/5], Step [98400/136675], Loss: 5.4520\n",
      "Epoch [1/5], Step [98475/136675], Loss: 5.5101\n",
      "Epoch [1/5], Step [98550/136675], Loss: 5.2995\n",
      "Epoch [1/5], Step [98625/136675], Loss: 5.4427\n",
      "Epoch [1/5], Step [98700/136675], Loss: 5.4178\n",
      "Epoch [1/5], Step [98775/136675], Loss: 5.3261\n",
      "Epoch [1/5], Step [98850/136675], Loss: 5.1354\n",
      "Epoch [1/5], Step [98925/136675], Loss: 5.5233\n",
      "Epoch [1/5], Step [99000/136675], Loss: 5.6004\n",
      "Epoch [1/5], Step [99075/136675], Loss: 5.4225\n",
      "Epoch [1/5], Step [99150/136675], Loss: 5.5188\n",
      "Epoch [1/5], Step [99225/136675], Loss: 4.9990\n",
      "Epoch [1/5], Step [99300/136675], Loss: 5.6879\n",
      "Epoch [1/5], Step [99375/136675], Loss: 5.4853\n",
      "Epoch [1/5], Step [99450/136675], Loss: 5.1790\n",
      "Epoch [1/5], Step [99525/136675], Loss: 5.6788\n",
      "Epoch [1/5], Step [99600/136675], Loss: 5.3792\n",
      "Epoch [1/5], Step [99675/136675], Loss: 5.1897\n",
      "Epoch [1/5], Step [99750/136675], Loss: 5.3537\n",
      "Epoch [1/5], Step [99825/136675], Loss: 5.4632\n",
      "Epoch [1/5], Step [99900/136675], Loss: 5.3098\n",
      "Epoch [1/5], Step [99975/136675], Loss: 5.7167\n",
      "Validation perplexity: 188.30214852582114\n",
      "Epoch [1/5], Step [100050/136675], Loss: 5.4884\n",
      "Epoch [1/5], Step [100125/136675], Loss: 5.6935\n",
      "Epoch [1/5], Step [100200/136675], Loss: 5.6432\n",
      "Epoch [1/5], Step [100275/136675], Loss: 5.3782\n",
      "Epoch [1/5], Step [100350/136675], Loss: 5.2618\n",
      "Epoch [1/5], Step [100425/136675], Loss: 5.2287\n",
      "Epoch [1/5], Step [100500/136675], Loss: 5.2610\n",
      "Epoch [1/5], Step [100575/136675], Loss: 5.2744\n",
      "Epoch [1/5], Step [100650/136675], Loss: 5.6663\n",
      "Epoch [1/5], Step [100725/136675], Loss: 5.4676\n",
      "Epoch [1/5], Step [100800/136675], Loss: 5.4870\n",
      "Epoch [1/5], Step [100875/136675], Loss: 5.3445\n",
      "Epoch [1/5], Step [100950/136675], Loss: 5.7259\n",
      "Epoch [1/5], Step [101025/136675], Loss: 5.4590\n",
      "Epoch [1/5], Step [101100/136675], Loss: 5.2487\n",
      "Epoch [1/5], Step [101175/136675], Loss: 5.4210\n",
      "Epoch [1/5], Step [101250/136675], Loss: 5.4357\n",
      "Epoch [1/5], Step [101325/136675], Loss: 5.3777\n",
      "Epoch [1/5], Step [101400/136675], Loss: 5.3910\n",
      "Epoch [1/5], Step [101475/136675], Loss: 5.4362\n",
      "Epoch [1/5], Step [101550/136675], Loss: 5.1768\n",
      "Epoch [1/5], Step [101625/136675], Loss: 5.3372\n",
      "Epoch [1/5], Step [101700/136675], Loss: 5.4039\n",
      "Epoch [1/5], Step [101775/136675], Loss: 5.4183\n",
      "Epoch [1/5], Step [101850/136675], Loss: 5.0991\n",
      "Epoch [1/5], Step [101925/136675], Loss: 5.7339\n",
      "Epoch [1/5], Step [102000/136675], Loss: 5.3755\n",
      "Epoch [1/5], Step [102075/136675], Loss: 5.5832\n",
      "Epoch [1/5], Step [102150/136675], Loss: 5.4564\n",
      "Epoch [1/5], Step [102225/136675], Loss: 5.6191\n",
      "Epoch [1/5], Step [102300/136675], Loss: 5.3904\n",
      "Epoch [1/5], Step [102375/136675], Loss: 5.4414\n",
      "Epoch [1/5], Step [102450/136675], Loss: 5.9937\n",
      "Epoch [1/5], Step [102525/136675], Loss: 5.2362\n",
      "Epoch [1/5], Step [102600/136675], Loss: 5.3454\n",
      "Epoch [1/5], Step [102675/136675], Loss: 5.5602\n",
      "Epoch [1/5], Step [102750/136675], Loss: 5.1911\n",
      "Epoch [1/5], Step [102825/136675], Loss: 5.2363\n",
      "Epoch [1/5], Step [102900/136675], Loss: 5.3957\n",
      "Epoch [1/5], Step [102975/136675], Loss: 5.1588\n",
      "Epoch [1/5], Step [103050/136675], Loss: 5.8524\n",
      "Epoch [1/5], Step [103125/136675], Loss: 5.5459\n",
      "Epoch [1/5], Step [103200/136675], Loss: 5.5799\n",
      "Epoch [1/5], Step [103275/136675], Loss: 5.4181\n",
      "Epoch [1/5], Step [103350/136675], Loss: 5.3916\n",
      "Epoch [1/5], Step [103425/136675], Loss: 5.2437\n",
      "Epoch [1/5], Step [103500/136675], Loss: 5.5844\n",
      "Epoch [1/5], Step [103575/136675], Loss: 5.8471\n",
      "Epoch [1/5], Step [103650/136675], Loss: 5.3623\n",
      "Epoch [1/5], Step [103725/136675], Loss: 5.2516\n",
      "Epoch [1/5], Step [103800/136675], Loss: 4.9300\n",
      "Epoch [1/5], Step [103875/136675], Loss: 5.5118\n",
      "Epoch [1/5], Step [103950/136675], Loss: 5.3851\n",
      "Epoch [1/5], Step [104025/136675], Loss: 5.8175\n",
      "Epoch [1/5], Step [104100/136675], Loss: 5.5894\n",
      "Epoch [1/5], Step [104175/136675], Loss: 5.3979\n",
      "Epoch [1/5], Step [104250/136675], Loss: 5.5984\n",
      "Epoch [1/5], Step [104325/136675], Loss: 5.3927\n",
      "Epoch [1/5], Step [104400/136675], Loss: 5.7674\n",
      "Epoch [1/5], Step [104475/136675], Loss: 5.3464\n",
      "Epoch [1/5], Step [104550/136675], Loss: 5.3456\n",
      "Epoch [1/5], Step [104625/136675], Loss: 5.4666\n",
      "Epoch [1/5], Step [104700/136675], Loss: 5.1159\n",
      "Epoch [1/5], Step [104775/136675], Loss: 5.4071\n",
      "Epoch [1/5], Step [104850/136675], Loss: 5.5467\n",
      "Epoch [1/5], Step [104925/136675], Loss: 5.3474\n",
      "Epoch [1/5], Step [105000/136675], Loss: 5.1806\n",
      "Epoch [1/5], Step [105075/136675], Loss: 5.5861\n",
      "Epoch [1/5], Step [105150/136675], Loss: 5.6465\n",
      "Epoch [1/5], Step [105225/136675], Loss: 5.3719\n",
      "Epoch [1/5], Step [105300/136675], Loss: 5.1730\n",
      "Epoch [1/5], Step [105375/136675], Loss: 5.3928\n",
      "Epoch [1/5], Step [105450/136675], Loss: 5.6018\n",
      "Epoch [1/5], Step [105525/136675], Loss: 5.8212\n",
      "Epoch [1/5], Step [105600/136675], Loss: 5.3867\n",
      "Epoch [1/5], Step [105675/136675], Loss: 5.4489\n",
      "Epoch [1/5], Step [105750/136675], Loss: 5.5652\n",
      "Epoch [1/5], Step [105825/136675], Loss: 5.3976\n",
      "Epoch [1/5], Step [105900/136675], Loss: 5.6461\n",
      "Epoch [1/5], Step [105975/136675], Loss: 5.1706\n",
      "Epoch [1/5], Step [106050/136675], Loss: 5.2749\n",
      "Epoch [1/5], Step [106125/136675], Loss: 5.0950\n",
      "Epoch [1/5], Step [106200/136675], Loss: 5.0586\n",
      "Epoch [1/5], Step [106275/136675], Loss: 5.1684\n",
      "Epoch [1/5], Step [106350/136675], Loss: 5.3465\n",
      "Epoch [1/5], Step [106425/136675], Loss: 5.4207\n",
      "Epoch [1/5], Step [106500/136675], Loss: 5.3778\n",
      "Epoch [1/5], Step [106575/136675], Loss: 5.7084\n",
      "Epoch [1/5], Step [106650/136675], Loss: 5.5204\n",
      "Epoch [1/5], Step [106725/136675], Loss: 5.5659\n",
      "Epoch [1/5], Step [106800/136675], Loss: 5.4847\n",
      "Epoch [1/5], Step [106875/136675], Loss: 5.2612\n",
      "Epoch [1/5], Step [106950/136675], Loss: 5.3050\n",
      "Epoch [1/5], Step [107025/136675], Loss: 5.3606\n",
      "Epoch [1/5], Step [107100/136675], Loss: 5.2389\n",
      "Epoch [1/5], Step [107175/136675], Loss: 5.4999\n",
      "Epoch [1/5], Step [107250/136675], Loss: 5.3134\n",
      "Epoch [1/5], Step [107325/136675], Loss: 5.0635\n",
      "Epoch [1/5], Step [107400/136675], Loss: 5.4969\n",
      "Epoch [1/5], Step [107475/136675], Loss: 5.2330\n",
      "Epoch [1/5], Step [107550/136675], Loss: 5.4756\n",
      "Epoch [1/5], Step [107625/136675], Loss: 5.3668\n",
      "Epoch [1/5], Step [107700/136675], Loss: 5.2543\n",
      "Epoch [1/5], Step [107775/136675], Loss: 5.5744\n",
      "Epoch [1/5], Step [107850/136675], Loss: 5.4558\n",
      "Epoch [1/5], Step [107925/136675], Loss: 5.2863\n",
      "Epoch [1/5], Step [108000/136675], Loss: 5.6367\n",
      "Epoch [1/5], Step [108075/136675], Loss: 5.3611\n",
      "Epoch [1/5], Step [108150/136675], Loss: 5.7552\n",
      "Epoch [1/5], Step [108225/136675], Loss: 5.2779\n",
      "Epoch [1/5], Step [108300/136675], Loss: 5.4866\n",
      "Epoch [1/5], Step [108375/136675], Loss: 5.2068\n",
      "Epoch [1/5], Step [108450/136675], Loss: 5.1753\n",
      "Epoch [1/5], Step [108525/136675], Loss: 5.3270\n",
      "Epoch [1/5], Step [108600/136675], Loss: 5.4078\n",
      "Epoch [1/5], Step [108675/136675], Loss: 5.5398\n",
      "Epoch [1/5], Step [108750/136675], Loss: 5.4706\n",
      "Epoch [1/5], Step [108825/136675], Loss: 5.5061\n",
      "Epoch [1/5], Step [108900/136675], Loss: 4.9953\n",
      "Epoch [1/5], Step [108975/136675], Loss: 5.3818\n",
      "Epoch [1/5], Step [109050/136675], Loss: 5.3502\n",
      "Epoch [1/5], Step [109125/136675], Loss: 4.9423\n",
      "Epoch [1/5], Step [109200/136675], Loss: 5.4268\n",
      "Epoch [1/5], Step [109275/136675], Loss: 5.4159\n",
      "Epoch [1/5], Step [109350/136675], Loss: 5.1798\n",
      "Epoch [1/5], Step [109425/136675], Loss: 5.3172\n",
      "Epoch [1/5], Step [109500/136675], Loss: 5.5069\n",
      "Epoch [1/5], Step [109575/136675], Loss: 5.6391\n",
      "Epoch [1/5], Step [109650/136675], Loss: 5.3040\n",
      "Epoch [1/5], Step [109725/136675], Loss: 5.4431\n",
      "Epoch [1/5], Step [109800/136675], Loss: 5.4675\n",
      "Epoch [1/5], Step [109875/136675], Loss: 5.3832\n",
      "Epoch [1/5], Step [109950/136675], Loss: 5.2241\n",
      "Validation perplexity: 186.56695413476731\n",
      "Epoch [1/5], Step [110025/136675], Loss: 5.4699\n",
      "Epoch [1/5], Step [110100/136675], Loss: 5.3779\n",
      "Epoch [1/5], Step [110175/136675], Loss: 5.3744\n",
      "Epoch [1/5], Step [110250/136675], Loss: 5.5131\n",
      "Epoch [1/5], Step [110325/136675], Loss: 5.6049\n",
      "Epoch [1/5], Step [110400/136675], Loss: 5.2719\n",
      "Epoch [1/5], Step [110475/136675], Loss: 5.3611\n",
      "Epoch [1/5], Step [110550/136675], Loss: 5.3713\n",
      "Epoch [1/5], Step [110625/136675], Loss: 5.3465\n",
      "Epoch [1/5], Step [110700/136675], Loss: 5.3020\n",
      "Epoch [1/5], Step [110775/136675], Loss: 5.4768\n",
      "Epoch [1/5], Step [110850/136675], Loss: 5.1306\n",
      "Epoch [1/5], Step [110925/136675], Loss: 5.3620\n",
      "Epoch [1/5], Step [111000/136675], Loss: 5.4843\n",
      "Epoch [1/5], Step [111075/136675], Loss: 5.0702\n",
      "Epoch [1/5], Step [111150/136675], Loss: 5.3210\n",
      "Epoch [1/5], Step [111225/136675], Loss: 5.4740\n",
      "Epoch [1/5], Step [111300/136675], Loss: 5.5685\n",
      "Epoch [1/5], Step [111375/136675], Loss: 5.7397\n",
      "Epoch [1/5], Step [111450/136675], Loss: 5.4808\n",
      "Epoch [1/5], Step [111525/136675], Loss: 5.5962\n",
      "Epoch [1/5], Step [111600/136675], Loss: 5.4348\n",
      "Epoch [1/5], Step [111675/136675], Loss: 5.6109\n",
      "Epoch [1/5], Step [111750/136675], Loss: 5.6148\n",
      "Epoch [1/5], Step [111825/136675], Loss: 5.2797\n",
      "Epoch [1/5], Step [111900/136675], Loss: 5.1856\n",
      "Epoch [1/5], Step [111975/136675], Loss: 5.5999\n",
      "Epoch [1/5], Step [112050/136675], Loss: 5.4823\n",
      "Epoch [1/5], Step [112125/136675], Loss: 5.4504\n",
      "Epoch [1/5], Step [112200/136675], Loss: 5.3259\n",
      "Epoch [1/5], Step [112275/136675], Loss: 5.3406\n",
      "Epoch [1/5], Step [112350/136675], Loss: 5.9319\n",
      "Epoch [1/5], Step [112425/136675], Loss: 5.6147\n",
      "Epoch [1/5], Step [112500/136675], Loss: 5.2743\n",
      "Epoch [1/5], Step [112575/136675], Loss: 5.5925\n",
      "Epoch [1/5], Step [112650/136675], Loss: 5.3282\n",
      "Epoch [1/5], Step [112725/136675], Loss: 4.9343\n",
      "Epoch [1/5], Step [112800/136675], Loss: 5.5066\n",
      "Epoch [1/5], Step [112875/136675], Loss: 5.4363\n",
      "Epoch [1/5], Step [112950/136675], Loss: 5.1997\n",
      "Epoch [1/5], Step [113025/136675], Loss: 5.4623\n",
      "Epoch [1/5], Step [113100/136675], Loss: 5.3007\n",
      "Epoch [1/5], Step [113175/136675], Loss: 4.8072\n",
      "Epoch [1/5], Step [113250/136675], Loss: 5.4327\n",
      "Epoch [1/5], Step [113325/136675], Loss: 5.2197\n",
      "Epoch [1/5], Step [113400/136675], Loss: 5.6435\n",
      "Epoch [1/5], Step [113475/136675], Loss: 5.8763\n",
      "Epoch [1/5], Step [113550/136675], Loss: 5.2005\n",
      "Epoch [1/5], Step [113625/136675], Loss: 5.2211\n",
      "Epoch [1/5], Step [113700/136675], Loss: 5.0829\n",
      "Epoch [1/5], Step [113775/136675], Loss: 4.9040\n",
      "Epoch [1/5], Step [113850/136675], Loss: 5.5503\n",
      "Epoch [1/5], Step [113925/136675], Loss: 5.0792\n",
      "Epoch [1/5], Step [114000/136675], Loss: 5.5996\n",
      "Epoch [1/5], Step [114075/136675], Loss: 5.8632\n",
      "Epoch [1/5], Step [114150/136675], Loss: 5.0755\n",
      "Epoch [1/5], Step [114225/136675], Loss: 5.3475\n",
      "Epoch [1/5], Step [114300/136675], Loss: 5.4732\n",
      "Epoch [1/5], Step [114375/136675], Loss: 5.4083\n",
      "Epoch [1/5], Step [114450/136675], Loss: 5.7100\n",
      "Epoch [1/5], Step [114525/136675], Loss: 5.3514\n",
      "Epoch [1/5], Step [114600/136675], Loss: 5.5226\n",
      "Epoch [1/5], Step [114675/136675], Loss: 5.3118\n",
      "Epoch [1/5], Step [114750/136675], Loss: 5.6270\n",
      "Epoch [1/5], Step [114825/136675], Loss: 5.4840\n",
      "Epoch [1/5], Step [114900/136675], Loss: 5.4086\n",
      "Epoch [1/5], Step [114975/136675], Loss: 5.1464\n",
      "Epoch [1/5], Step [115050/136675], Loss: 5.7646\n",
      "Epoch [1/5], Step [115125/136675], Loss: 5.4334\n",
      "Epoch [1/5], Step [115200/136675], Loss: 5.6581\n",
      "Epoch [1/5], Step [115275/136675], Loss: 5.5480\n",
      "Epoch [1/5], Step [115350/136675], Loss: 4.7354\n",
      "Epoch [1/5], Step [115425/136675], Loss: 5.1485\n",
      "Epoch [1/5], Step [115500/136675], Loss: 5.5294\n",
      "Epoch [1/5], Step [115575/136675], Loss: 5.1429\n",
      "Epoch [1/5], Step [115650/136675], Loss: 5.5132\n",
      "Epoch [1/5], Step [115725/136675], Loss: 5.3322\n",
      "Epoch [1/5], Step [115800/136675], Loss: 5.5803\n",
      "Epoch [1/5], Step [115875/136675], Loss: 5.3793\n",
      "Epoch [1/5], Step [115950/136675], Loss: 5.5669\n",
      "Epoch [1/5], Step [116025/136675], Loss: 5.7101\n",
      "Epoch [1/5], Step [116100/136675], Loss: 5.6815\n",
      "Epoch [1/5], Step [116175/136675], Loss: 5.5565\n",
      "Epoch [1/5], Step [116250/136675], Loss: 5.4613\n",
      "Epoch [1/5], Step [116325/136675], Loss: 5.6730\n",
      "Epoch [1/5], Step [116400/136675], Loss: 5.7555\n",
      "Epoch [1/5], Step [116475/136675], Loss: 5.6861\n",
      "Epoch [1/5], Step [116550/136675], Loss: 5.4335\n",
      "Epoch [1/5], Step [116625/136675], Loss: 5.5262\n",
      "Epoch [1/5], Step [116700/136675], Loss: 5.2535\n",
      "Epoch [1/5], Step [116775/136675], Loss: 5.4255\n",
      "Epoch [1/5], Step [116850/136675], Loss: 5.7108\n",
      "Epoch [1/5], Step [116925/136675], Loss: 5.4854\n",
      "Epoch [1/5], Step [117000/136675], Loss: 5.5607\n",
      "Epoch [1/5], Step [117075/136675], Loss: 5.4179\n",
      "Epoch [1/5], Step [117150/136675], Loss: 5.4702\n",
      "Epoch [1/5], Step [117225/136675], Loss: 5.2917\n",
      "Epoch [1/5], Step [117300/136675], Loss: 5.4048\n",
      "Epoch [1/5], Step [117375/136675], Loss: 5.2279\n",
      "Epoch [1/5], Step [117450/136675], Loss: 5.3302\n",
      "Epoch [1/5], Step [117525/136675], Loss: 5.2859\n",
      "Epoch [1/5], Step [117600/136675], Loss: 5.3387\n",
      "Epoch [1/5], Step [117675/136675], Loss: 5.6410\n",
      "Epoch [1/5], Step [117750/136675], Loss: 5.7764\n",
      "Epoch [1/5], Step [117825/136675], Loss: 5.2653\n",
      "Epoch [1/5], Step [117900/136675], Loss: 5.3090\n",
      "Epoch [1/5], Step [117975/136675], Loss: 5.3612\n",
      "Epoch [1/5], Step [118050/136675], Loss: 5.2914\n",
      "Epoch [1/5], Step [118125/136675], Loss: 5.6499\n",
      "Epoch [1/5], Step [118200/136675], Loss: 5.5635\n",
      "Epoch [1/5], Step [118275/136675], Loss: 5.2700\n",
      "Epoch [1/5], Step [118350/136675], Loss: 5.3881\n",
      "Epoch [1/5], Step [118425/136675], Loss: 5.5467\n",
      "Epoch [1/5], Step [118500/136675], Loss: 5.4144\n",
      "Epoch [1/5], Step [118575/136675], Loss: 5.4772\n",
      "Epoch [1/5], Step [118650/136675], Loss: 5.7555\n",
      "Epoch [1/5], Step [118725/136675], Loss: 5.0634\n",
      "Epoch [1/5], Step [118800/136675], Loss: 5.3450\n",
      "Epoch [1/5], Step [118875/136675], Loss: 5.2091\n",
      "Epoch [1/5], Step [118950/136675], Loss: 5.3739\n",
      "Epoch [1/5], Step [119025/136675], Loss: 5.2928\n",
      "Epoch [1/5], Step [119100/136675], Loss: 5.5286\n",
      "Epoch [1/5], Step [119175/136675], Loss: 5.2338\n",
      "Epoch [1/5], Step [119250/136675], Loss: 5.2474\n",
      "Epoch [1/5], Step [119325/136675], Loss: 5.2225\n",
      "Epoch [1/5], Step [119400/136675], Loss: 5.1682\n",
      "Epoch [1/5], Step [119475/136675], Loss: 5.4011\n",
      "Epoch [1/5], Step [119550/136675], Loss: 5.4940\n",
      "Epoch [1/5], Step [119625/136675], Loss: 5.3710\n",
      "Epoch [1/5], Step [119700/136675], Loss: 5.5586\n",
      "Epoch [1/5], Step [119775/136675], Loss: 5.1854\n",
      "Epoch [1/5], Step [119850/136675], Loss: 5.1640\n",
      "Epoch [1/5], Step [119925/136675], Loss: 5.2008\n",
      "Epoch [1/5], Step [120000/136675], Loss: 5.4092\n",
      "Validation perplexity: 182.8925889120841\n",
      "Epoch [1/5], Step [120075/136675], Loss: 5.1856\n",
      "Epoch [1/5], Step [120150/136675], Loss: 5.0065\n",
      "Epoch [1/5], Step [120225/136675], Loss: 5.5012\n",
      "Epoch [1/5], Step [120300/136675], Loss: 5.6717\n",
      "Epoch [1/5], Step [120375/136675], Loss: 5.0583\n",
      "Epoch [1/5], Step [120450/136675], Loss: 5.3741\n",
      "Epoch [1/5], Step [120525/136675], Loss: 5.4733\n",
      "Epoch [1/5], Step [120600/136675], Loss: 5.5814\n",
      "Epoch [1/5], Step [120675/136675], Loss: 5.6014\n",
      "Epoch [1/5], Step [120750/136675], Loss: 5.7055\n",
      "Epoch [1/5], Step [120825/136675], Loss: 5.2937\n",
      "Epoch [1/5], Step [120900/136675], Loss: 5.0192\n",
      "Epoch [1/5], Step [120975/136675], Loss: 5.1831\n",
      "Epoch [1/5], Step [121050/136675], Loss: 5.4712\n",
      "Epoch [1/5], Step [121125/136675], Loss: 5.2935\n",
      "Epoch [1/5], Step [121200/136675], Loss: 5.6298\n",
      "Epoch [1/5], Step [121275/136675], Loss: 5.0500\n",
      "Epoch [1/5], Step [121350/136675], Loss: 5.6847\n",
      "Epoch [1/5], Step [121425/136675], Loss: 5.3385\n",
      "Epoch [1/5], Step [121500/136675], Loss: 5.6656\n",
      "Epoch [1/5], Step [121575/136675], Loss: 5.3697\n",
      "Epoch [1/5], Step [121650/136675], Loss: 5.1656\n",
      "Epoch [1/5], Step [121725/136675], Loss: 5.2809\n",
      "Epoch [1/5], Step [121800/136675], Loss: 5.4578\n",
      "Epoch [1/5], Step [121875/136675], Loss: 5.4661\n",
      "Epoch [1/5], Step [121950/136675], Loss: 5.3172\n",
      "Epoch [1/5], Step [122025/136675], Loss: 5.5066\n",
      "Epoch [1/5], Step [122100/136675], Loss: 4.9432\n",
      "Epoch [1/5], Step [122175/136675], Loss: 5.4552\n",
      "Epoch [1/5], Step [122250/136675], Loss: 5.6204\n",
      "Epoch [1/5], Step [122325/136675], Loss: 5.7034\n",
      "Epoch [1/5], Step [122400/136675], Loss: 5.2472\n",
      "Epoch [1/5], Step [122475/136675], Loss: 5.3718\n",
      "Epoch [1/5], Step [122550/136675], Loss: 5.4871\n",
      "Epoch [1/5], Step [122625/136675], Loss: 5.4656\n",
      "Epoch [1/5], Step [122700/136675], Loss: 5.5021\n",
      "Epoch [1/5], Step [122775/136675], Loss: 5.3823\n",
      "Epoch [1/5], Step [122850/136675], Loss: 5.4947\n",
      "Epoch [1/5], Step [122925/136675], Loss: 5.2643\n",
      "Epoch [1/5], Step [123000/136675], Loss: 5.3385\n",
      "Epoch [1/5], Step [123075/136675], Loss: 5.1820\n",
      "Epoch [1/5], Step [123150/136675], Loss: 5.3439\n",
      "Epoch [1/5], Step [123225/136675], Loss: 5.3860\n",
      "Epoch [1/5], Step [123300/136675], Loss: 5.4800\n",
      "Epoch [1/5], Step [123375/136675], Loss: 5.2873\n",
      "Epoch [1/5], Step [123450/136675], Loss: 5.3836\n",
      "Epoch [1/5], Step [123525/136675], Loss: 5.6872\n",
      "Epoch [1/5], Step [123600/136675], Loss: 5.2014\n",
      "Epoch [1/5], Step [123675/136675], Loss: 5.2801\n",
      "Epoch [1/5], Step [123750/136675], Loss: 5.3569\n",
      "Epoch [1/5], Step [123825/136675], Loss: 5.4045\n",
      "Epoch [1/5], Step [123900/136675], Loss: 5.5389\n",
      "Epoch [1/5], Step [123975/136675], Loss: 5.1908\n",
      "Epoch [1/5], Step [124050/136675], Loss: 5.4771\n",
      "Epoch [1/5], Step [124125/136675], Loss: 5.3440\n",
      "Epoch [1/5], Step [124200/136675], Loss: 5.7096\n",
      "Epoch [1/5], Step [124275/136675], Loss: 5.6433\n",
      "Epoch [1/5], Step [124350/136675], Loss: 5.7985\n",
      "Epoch [1/5], Step [124425/136675], Loss: 5.3088\n",
      "Epoch [1/5], Step [124500/136675], Loss: 5.0517\n",
      "Epoch [1/5], Step [124575/136675], Loss: 5.5040\n",
      "Epoch [1/5], Step [124650/136675], Loss: 5.3726\n",
      "Epoch [1/5], Step [124725/136675], Loss: 5.2758\n",
      "Epoch [1/5], Step [124800/136675], Loss: 5.3659\n",
      "Epoch [1/5], Step [124875/136675], Loss: 5.4563\n",
      "Epoch [1/5], Step [124950/136675], Loss: 5.5246\n",
      "Epoch [1/5], Step [125025/136675], Loss: 5.3097\n",
      "Epoch [1/5], Step [125100/136675], Loss: 5.2838\n",
      "Epoch [1/5], Step [125175/136675], Loss: 5.2353\n",
      "Epoch [1/5], Step [125250/136675], Loss: 5.4443\n",
      "Epoch [1/5], Step [125325/136675], Loss: 5.3300\n",
      "Epoch [1/5], Step [125400/136675], Loss: 5.7713\n",
      "Epoch [1/5], Step [125475/136675], Loss: 5.2059\n",
      "Epoch [1/5], Step [125550/136675], Loss: 5.3917\n",
      "Epoch [1/5], Step [125625/136675], Loss: 5.4519\n",
      "Epoch [1/5], Step [125700/136675], Loss: 5.5796\n",
      "Epoch [1/5], Step [125775/136675], Loss: 5.2760\n",
      "Epoch [1/5], Step [125850/136675], Loss: 5.2136\n",
      "Epoch [1/5], Step [125925/136675], Loss: 5.7363\n",
      "Epoch [1/5], Step [126000/136675], Loss: 5.4210\n",
      "Epoch [1/5], Step [126075/136675], Loss: 5.4861\n",
      "Epoch [1/5], Step [126150/136675], Loss: 5.2110\n",
      "Epoch [1/5], Step [126225/136675], Loss: 5.3794\n",
      "Epoch [1/5], Step [126300/136675], Loss: 5.2912\n",
      "Epoch [1/5], Step [126375/136675], Loss: 5.3685\n",
      "Epoch [1/5], Step [126450/136675], Loss: 5.4996\n",
      "Epoch [1/5], Step [126525/136675], Loss: 5.4897\n",
      "Epoch [1/5], Step [126600/136675], Loss: 5.3893\n",
      "Epoch [1/5], Step [126675/136675], Loss: 5.4641\n",
      "Epoch [1/5], Step [126750/136675], Loss: 5.5254\n",
      "Epoch [1/5], Step [126825/136675], Loss: 5.4496\n",
      "Epoch [1/5], Step [126900/136675], Loss: 5.5047\n",
      "Epoch [1/5], Step [126975/136675], Loss: 5.2324\n",
      "Epoch [1/5], Step [127050/136675], Loss: 5.3200\n",
      "Epoch [1/5], Step [127125/136675], Loss: 5.3733\n",
      "Epoch [1/5], Step [127200/136675], Loss: 5.5895\n",
      "Epoch [1/5], Step [127275/136675], Loss: 5.2150\n",
      "Epoch [1/5], Step [127350/136675], Loss: 5.5490\n",
      "Epoch [1/5], Step [127425/136675], Loss: 5.4653\n",
      "Epoch [1/5], Step [127500/136675], Loss: 5.3739\n",
      "Epoch [1/5], Step [127575/136675], Loss: 5.5553\n",
      "Epoch [1/5], Step [127650/136675], Loss: 5.4950\n",
      "Epoch [1/5], Step [127725/136675], Loss: 5.4666\n",
      "Epoch [1/5], Step [127800/136675], Loss: 5.5605\n",
      "Epoch [1/5], Step [127875/136675], Loss: 5.4718\n",
      "Epoch [1/5], Step [127950/136675], Loss: 5.5416\n",
      "Epoch [1/5], Step [128025/136675], Loss: 5.2425\n",
      "Epoch [1/5], Step [128100/136675], Loss: 5.3171\n",
      "Epoch [1/5], Step [128175/136675], Loss: 5.4343\n",
      "Epoch [1/5], Step [128250/136675], Loss: 5.3473\n",
      "Epoch [1/5], Step [128325/136675], Loss: 5.6955\n",
      "Epoch [1/5], Step [128400/136675], Loss: 5.2798\n",
      "Epoch [1/5], Step [128475/136675], Loss: 5.4461\n",
      "Epoch [1/5], Step [128550/136675], Loss: 5.4159\n",
      "Epoch [1/5], Step [128625/136675], Loss: 5.4196\n",
      "Epoch [1/5], Step [128700/136675], Loss: 5.4764\n",
      "Epoch [1/5], Step [128775/136675], Loss: 4.9606\n",
      "Epoch [1/5], Step [128850/136675], Loss: 5.3937\n",
      "Epoch [1/5], Step [128925/136675], Loss: 5.2917\n",
      "Epoch [1/5], Step [129000/136675], Loss: 5.2089\n",
      "Epoch [1/5], Step [129075/136675], Loss: 5.2555\n",
      "Epoch [1/5], Step [129150/136675], Loss: 5.1599\n",
      "Epoch [1/5], Step [129225/136675], Loss: 5.3507\n",
      "Epoch [1/5], Step [129300/136675], Loss: 5.6053\n",
      "Epoch [1/5], Step [129375/136675], Loss: 5.4784\n",
      "Epoch [1/5], Step [129450/136675], Loss: 5.3028\n",
      "Epoch [1/5], Step [129525/136675], Loss: 5.2934\n",
      "Epoch [1/5], Step [129600/136675], Loss: 5.5709\n",
      "Epoch [1/5], Step [129675/136675], Loss: 5.4672\n",
      "Epoch [1/5], Step [129750/136675], Loss: 5.5198\n",
      "Epoch [1/5], Step [129825/136675], Loss: 5.4179\n",
      "Epoch [1/5], Step [129900/136675], Loss: 5.3388\n",
      "Epoch [1/5], Step [129975/136675], Loss: 5.3451\n",
      "Validation perplexity: 182.32691344991642\n",
      "Epoch [1/5], Step [130050/136675], Loss: 5.2809\n",
      "Epoch [1/5], Step [130125/136675], Loss: 5.2589\n",
      "Epoch [1/5], Step [130200/136675], Loss: 5.3608\n",
      "Epoch [1/5], Step [130275/136675], Loss: 5.4916\n",
      "Epoch [1/5], Step [130350/136675], Loss: 5.5218\n",
      "Epoch [1/5], Step [130425/136675], Loss: 5.4622\n",
      "Epoch [1/5], Step [130500/136675], Loss: 5.2954\n",
      "Epoch [1/5], Step [130575/136675], Loss: 5.4900\n",
      "Epoch [1/5], Step [130650/136675], Loss: 5.4725\n",
      "Epoch [1/5], Step [130725/136675], Loss: 5.5341\n",
      "Epoch [1/5], Step [130800/136675], Loss: 4.8845\n",
      "Epoch [1/5], Step [130875/136675], Loss: 5.5348\n",
      "Epoch [1/5], Step [130950/136675], Loss: 5.4477\n",
      "Epoch [1/5], Step [131025/136675], Loss: 5.0801\n",
      "Epoch [1/5], Step [131100/136675], Loss: 5.2670\n",
      "Epoch [1/5], Step [131175/136675], Loss: 5.3045\n",
      "Epoch [1/5], Step [131250/136675], Loss: 5.5709\n",
      "Epoch [1/5], Step [131325/136675], Loss: 5.4402\n",
      "Epoch [1/5], Step [131400/136675], Loss: 5.2627\n",
      "Epoch [1/5], Step [131475/136675], Loss: 5.3807\n",
      "Epoch [1/5], Step [131550/136675], Loss: 5.1616\n",
      "Epoch [1/5], Step [131625/136675], Loss: 5.3667\n",
      "Epoch [1/5], Step [131700/136675], Loss: 5.4110\n",
      "Epoch [1/5], Step [131775/136675], Loss: 5.4247\n",
      "Epoch [1/5], Step [131850/136675], Loss: 5.5003\n",
      "Epoch [1/5], Step [131925/136675], Loss: 5.4110\n",
      "Epoch [1/5], Step [132000/136675], Loss: 5.3072\n",
      "Epoch [1/5], Step [132075/136675], Loss: 5.1565\n",
      "Epoch [1/5], Step [132150/136675], Loss: 5.4253\n",
      "Epoch [1/5], Step [132225/136675], Loss: 4.9970\n",
      "Epoch [1/5], Step [132300/136675], Loss: 5.2919\n",
      "Epoch [1/5], Step [132375/136675], Loss: 5.0285\n",
      "Epoch [1/5], Step [132450/136675], Loss: 5.3179\n",
      "Epoch [1/5], Step [132525/136675], Loss: 5.3486\n",
      "Epoch [1/5], Step [132600/136675], Loss: 5.5468\n",
      "Epoch [1/5], Step [132675/136675], Loss: 5.3667\n",
      "Epoch [1/5], Step [132750/136675], Loss: 5.5460\n",
      "Epoch [1/5], Step [132825/136675], Loss: 5.6019\n",
      "Epoch [1/5], Step [132900/136675], Loss: 5.4853\n",
      "Epoch [1/5], Step [132975/136675], Loss: 4.9659\n",
      "Epoch [1/5], Step [133050/136675], Loss: 5.4852\n",
      "Epoch [1/5], Step [133125/136675], Loss: 5.4297\n",
      "Epoch [1/5], Step [133200/136675], Loss: 5.3797\n",
      "Epoch [1/5], Step [133275/136675], Loss: 5.6335\n",
      "Epoch [1/5], Step [133350/136675], Loss: 5.3269\n",
      "Epoch [1/5], Step [133425/136675], Loss: 5.5372\n",
      "Epoch [1/5], Step [133500/136675], Loss: 5.5715\n",
      "Epoch [1/5], Step [133575/136675], Loss: 5.2816\n",
      "Epoch [1/5], Step [133650/136675], Loss: 5.4008\n",
      "Epoch [1/5], Step [133725/136675], Loss: 5.2574\n",
      "Epoch [1/5], Step [133800/136675], Loss: 5.3298\n",
      "Epoch [1/5], Step [133875/136675], Loss: 5.5259\n",
      "Epoch [1/5], Step [133950/136675], Loss: 5.2938\n",
      "Epoch [1/5], Step [134025/136675], Loss: 5.4548\n",
      "Epoch [1/5], Step [134100/136675], Loss: 5.3196\n",
      "Epoch [1/5], Step [134175/136675], Loss: 5.2204\n",
      "Epoch [1/5], Step [134250/136675], Loss: 5.7110\n",
      "Epoch [1/5], Step [134325/136675], Loss: 5.3652\n",
      "Epoch [1/5], Step [134400/136675], Loss: 5.4089\n",
      "Epoch [1/5], Step [134475/136675], Loss: 5.7258\n",
      "Epoch [1/5], Step [134550/136675], Loss: 5.4373\n",
      "Epoch [1/5], Step [134625/136675], Loss: 5.1110\n",
      "Epoch [1/5], Step [134700/136675], Loss: 5.3923\n",
      "Epoch [1/5], Step [134775/136675], Loss: 5.5728\n",
      "Epoch [1/5], Step [134850/136675], Loss: 5.0376\n",
      "Epoch [1/5], Step [134925/136675], Loss: 5.2057\n",
      "Epoch [1/5], Step [135000/136675], Loss: 5.6999\n",
      "Epoch [1/5], Step [135075/136675], Loss: 5.0836\n",
      "Epoch [1/5], Step [135150/136675], Loss: 5.5329\n",
      "Epoch [1/5], Step [135225/136675], Loss: 5.2641\n",
      "Epoch [1/5], Step [135300/136675], Loss: 5.3965\n",
      "Epoch [1/5], Step [135375/136675], Loss: 5.6393\n",
      "Epoch [1/5], Step [135450/136675], Loss: 5.2384\n",
      "Epoch [1/5], Step [135525/136675], Loss: 5.6201\n",
      "Epoch [1/5], Step [135600/136675], Loss: 5.6768\n",
      "Epoch [1/5], Step [135675/136675], Loss: 5.5280\n",
      "Epoch [1/5], Step [135750/136675], Loss: 5.2562\n",
      "Epoch [1/5], Step [135825/136675], Loss: 5.4965\n",
      "Epoch [1/5], Step [135900/136675], Loss: 5.4587\n",
      "Epoch [1/5], Step [135975/136675], Loss: 5.3810\n",
      "Epoch [1/5], Step [136050/136675], Loss: 5.2472\n",
      "Epoch [1/5], Step [136125/136675], Loss: 5.4801\n",
      "Epoch [1/5], Step [136200/136675], Loss: 5.5650\n",
      "Epoch [1/5], Step [136275/136675], Loss: 5.2996\n",
      "Epoch [1/5], Step [136350/136675], Loss: 5.2195\n",
      "Epoch [1/5], Step [136425/136675], Loss: 5.3537\n",
      "Epoch [1/5], Step [136500/136675], Loss: 5.2863\n",
      "Epoch [1/5], Step [136575/136675], Loss: 5.5127\n",
      "Epoch [1/5], Step [136650/136675], Loss: 5.6035\n",
      "Epoch [1/5] Average Loss: 5.6028, Perplexity: 271.20\n",
      "Epoch [2/5], Step [0/136675], Loss: 5.3491\n",
      "Validation perplexity: 177.7938611521836\n",
      "Epoch [2/5], Step [75/136675], Loss: 5.3080\n",
      "Epoch [2/5], Step [150/136675], Loss: 5.2636\n",
      "Epoch [2/5], Step [225/136675], Loss: 5.7478\n",
      "Epoch [2/5], Step [300/136675], Loss: 5.5321\n",
      "Epoch [2/5], Step [375/136675], Loss: 5.2942\n",
      "Epoch [2/5], Step [450/136675], Loss: 5.6681\n",
      "Epoch [2/5], Step [525/136675], Loss: 5.3265\n",
      "Epoch [2/5], Step [600/136675], Loss: 5.1504\n",
      "Epoch [2/5], Step [675/136675], Loss: 5.4480\n",
      "Epoch [2/5], Step [750/136675], Loss: 5.1167\n",
      "Epoch [2/5], Step [825/136675], Loss: 5.1003\n",
      "Epoch [2/5], Step [900/136675], Loss: 5.5363\n",
      "Epoch [2/5], Step [975/136675], Loss: 5.0772\n",
      "Epoch [2/5], Step [1050/136675], Loss: 5.5038\n",
      "Epoch [2/5], Step [1125/136675], Loss: 5.2487\n",
      "Epoch [2/5], Step [1200/136675], Loss: 5.4861\n",
      "Epoch [2/5], Step [1275/136675], Loss: 5.1679\n",
      "Epoch [2/5], Step [1350/136675], Loss: 5.0719\n",
      "Epoch [2/5], Step [1425/136675], Loss: 5.0162\n",
      "Epoch [2/5], Step [1500/136675], Loss: 5.3058\n",
      "Epoch [2/5], Step [1575/136675], Loss: 5.3990\n",
      "Epoch [2/5], Step [1650/136675], Loss: 5.3855\n",
      "Epoch [2/5], Step [1725/136675], Loss: 4.9130\n",
      "Epoch [2/5], Step [1800/136675], Loss: 5.4724\n",
      "Epoch [2/5], Step [1875/136675], Loss: 5.3362\n",
      "Epoch [2/5], Step [1950/136675], Loss: 5.3318\n",
      "Epoch [2/5], Step [2025/136675], Loss: 5.1421\n",
      "Epoch [2/5], Step [2100/136675], Loss: 5.4093\n",
      "Epoch [2/5], Step [2175/136675], Loss: 5.4293\n",
      "Epoch [2/5], Step [2250/136675], Loss: 5.3807\n",
      "Epoch [2/5], Step [2325/136675], Loss: 5.3658\n",
      "Epoch [2/5], Step [2400/136675], Loss: 5.3168\n",
      "Epoch [2/5], Step [2475/136675], Loss: 5.7540\n",
      "Epoch [2/5], Step [2550/136675], Loss: 5.4368\n",
      "Epoch [2/5], Step [2625/136675], Loss: 5.3295\n",
      "Epoch [2/5], Step [2700/136675], Loss: 5.4528\n",
      "Epoch [2/5], Step [2775/136675], Loss: 5.3487\n",
      "Epoch [2/5], Step [2850/136675], Loss: 5.2154\n",
      "Epoch [2/5], Step [2925/136675], Loss: 5.4714\n",
      "Epoch [2/5], Step [3000/136675], Loss: 5.6222\n",
      "Epoch [2/5], Step [3075/136675], Loss: 5.6004\n",
      "Epoch [2/5], Step [3150/136675], Loss: 5.2780\n",
      "Epoch [2/5], Step [3225/136675], Loss: 5.3731\n",
      "Epoch [2/5], Step [3300/136675], Loss: 5.3689\n",
      "Epoch [2/5], Step [3375/136675], Loss: 5.1919\n",
      "Epoch [2/5], Step [3450/136675], Loss: 5.6052\n",
      "Epoch [2/5], Step [3525/136675], Loss: 5.3446\n",
      "Epoch [2/5], Step [3600/136675], Loss: 5.4214\n",
      "Epoch [2/5], Step [3675/136675], Loss: 4.9311\n",
      "Epoch [2/5], Step [3750/136675], Loss: 5.5531\n",
      "Epoch [2/5], Step [3825/136675], Loss: 5.1056\n",
      "Epoch [2/5], Step [3900/136675], Loss: 5.4506\n",
      "Epoch [2/5], Step [3975/136675], Loss: 5.5498\n",
      "Epoch [2/5], Step [4050/136675], Loss: 5.6404\n",
      "Epoch [2/5], Step [4125/136675], Loss: 5.6842\n",
      "Epoch [2/5], Step [4200/136675], Loss: 5.4401\n",
      "Epoch [2/5], Step [4275/136675], Loss: 5.2794\n",
      "Epoch [2/5], Step [4350/136675], Loss: 5.2583\n",
      "Epoch [2/5], Step [4425/136675], Loss: 5.4411\n",
      "Epoch [2/5], Step [4500/136675], Loss: 5.1900\n",
      "Epoch [2/5], Step [4575/136675], Loss: 5.2655\n",
      "Epoch [2/5], Step [4650/136675], Loss: 5.3336\n",
      "Epoch [2/5], Step [4725/136675], Loss: 5.4920\n",
      "Epoch [2/5], Step [4800/136675], Loss: 5.2560\n",
      "Epoch [2/5], Step [4875/136675], Loss: 5.0750\n",
      "Epoch [2/5], Step [4950/136675], Loss: 5.3951\n",
      "Epoch [2/5], Step [5025/136675], Loss: 5.7615\n",
      "Epoch [2/5], Step [5100/136675], Loss: 5.5073\n",
      "Epoch [2/5], Step [5175/136675], Loss: 5.3300\n",
      "Epoch [2/5], Step [5250/136675], Loss: 5.3576\n",
      "Epoch [2/5], Step [5325/136675], Loss: 5.4802\n",
      "Epoch [2/5], Step [5400/136675], Loss: 5.2810\n",
      "Epoch [2/5], Step [5475/136675], Loss: 5.4285\n",
      "Epoch [2/5], Step [5550/136675], Loss: 5.6050\n",
      "Epoch [2/5], Step [5625/136675], Loss: 5.5123\n",
      "Epoch [2/5], Step [5700/136675], Loss: 5.6517\n",
      "Epoch [2/5], Step [5775/136675], Loss: 5.2429\n",
      "Epoch [2/5], Step [5850/136675], Loss: 5.1818\n",
      "Epoch [2/5], Step [5925/136675], Loss: 5.3364\n",
      "Epoch [2/5], Step [6000/136675], Loss: 5.4451\n",
      "Epoch [2/5], Step [6075/136675], Loss: 5.3409\n",
      "Epoch [2/5], Step [6150/136675], Loss: 5.5166\n",
      "Epoch [2/5], Step [6225/136675], Loss: 5.6214\n",
      "Epoch [2/5], Step [6300/136675], Loss: 5.5240\n",
      "Epoch [2/5], Step [6375/136675], Loss: 5.3558\n",
      "Epoch [2/5], Step [6450/136675], Loss: 5.2857\n",
      "Epoch [2/5], Step [6525/136675], Loss: 5.4660\n",
      "Epoch [2/5], Step [6600/136675], Loss: 5.4715\n",
      "Epoch [2/5], Step [6675/136675], Loss: 5.4315\n",
      "Epoch [2/5], Step [6750/136675], Loss: 5.4587\n",
      "Epoch [2/5], Step [6825/136675], Loss: 5.4302\n",
      "Epoch [2/5], Step [6900/136675], Loss: 4.9703\n",
      "Epoch [2/5], Step [6975/136675], Loss: 5.4772\n",
      "Epoch [2/5], Step [7050/136675], Loss: 5.3630\n",
      "Epoch [2/5], Step [7125/136675], Loss: 5.3533\n",
      "Epoch [2/5], Step [7200/136675], Loss: 5.1154\n",
      "Epoch [2/5], Step [7275/136675], Loss: 5.5196\n",
      "Epoch [2/5], Step [7350/136675], Loss: 5.5487\n",
      "Epoch [2/5], Step [7425/136675], Loss: 5.4551\n",
      "Epoch [2/5], Step [7500/136675], Loss: 5.2625\n",
      "Epoch [2/5], Step [7575/136675], Loss: 5.3741\n",
      "Epoch [2/5], Step [7650/136675], Loss: 5.1998\n",
      "Epoch [2/5], Step [7725/136675], Loss: 5.4208\n",
      "Epoch [2/5], Step [7800/136675], Loss: 5.3559\n",
      "Epoch [2/5], Step [7875/136675], Loss: 5.4594\n",
      "Epoch [2/5], Step [7950/136675], Loss: 5.2524\n",
      "Epoch [2/5], Step [8025/136675], Loss: 5.3700\n",
      "Epoch [2/5], Step [8100/136675], Loss: 5.4106\n",
      "Epoch [2/5], Step [8175/136675], Loss: 5.3087\n",
      "Epoch [2/5], Step [8250/136675], Loss: 5.2692\n",
      "Epoch [2/5], Step [8325/136675], Loss: 5.2227\n",
      "Epoch [2/5], Step [8400/136675], Loss: 5.1905\n",
      "Epoch [2/5], Step [8475/136675], Loss: 5.3566\n",
      "Epoch [2/5], Step [8550/136675], Loss: 5.1945\n",
      "Epoch [2/5], Step [8625/136675], Loss: 5.4686\n",
      "Epoch [2/5], Step [8700/136675], Loss: 5.3566\n",
      "Epoch [2/5], Step [8775/136675], Loss: 5.2421\n",
      "Epoch [2/5], Step [8850/136675], Loss: 5.4548\n",
      "Epoch [2/5], Step [8925/136675], Loss: 5.1707\n",
      "Epoch [2/5], Step [9000/136675], Loss: 5.4384\n",
      "Epoch [2/5], Step [9075/136675], Loss: 5.3937\n",
      "Epoch [2/5], Step [9150/136675], Loss: 5.5366\n",
      "Epoch [2/5], Step [9225/136675], Loss: 5.4480\n",
      "Epoch [2/5], Step [9300/136675], Loss: 5.3314\n",
      "Epoch [2/5], Step [9375/136675], Loss: 4.6680\n",
      "Epoch [2/5], Step [9450/136675], Loss: 5.4754\n",
      "Epoch [2/5], Step [9525/136675], Loss: 5.2494\n",
      "Epoch [2/5], Step [9600/136675], Loss: 5.2608\n",
      "Epoch [2/5], Step [9675/136675], Loss: 5.2356\n",
      "Epoch [2/5], Step [9750/136675], Loss: 5.2928\n",
      "Epoch [2/5], Step [9825/136675], Loss: 5.4165\n",
      "Epoch [2/5], Step [9900/136675], Loss: 5.0611\n",
      "Epoch [2/5], Step [9975/136675], Loss: 5.5333\n",
      "Validation perplexity: 178.2653343722933\n",
      "Epoch [2/5], Step [10050/136675], Loss: 5.4714\n",
      "Epoch [2/5], Step [10125/136675], Loss: 5.1877\n",
      "Epoch [2/5], Step [10200/136675], Loss: 5.2653\n",
      "Epoch [2/5], Step [10275/136675], Loss: 5.4622\n",
      "Epoch [2/5], Step [10350/136675], Loss: 5.6113\n",
      "Epoch [2/5], Step [10425/136675], Loss: 5.3192\n",
      "Epoch [2/5], Step [10500/136675], Loss: 5.2399\n",
      "Epoch [2/5], Step [10575/136675], Loss: 5.1219\n",
      "Epoch [2/5], Step [10650/136675], Loss: 5.3602\n",
      "Epoch [2/5], Step [10725/136675], Loss: 5.4348\n",
      "Epoch [2/5], Step [10800/136675], Loss: 5.3726\n",
      "Epoch [2/5], Step [10875/136675], Loss: 5.4833\n",
      "Epoch [2/5], Step [10950/136675], Loss: 5.1868\n",
      "Epoch [2/5], Step [11025/136675], Loss: 5.5765\n",
      "Epoch [2/5], Step [11100/136675], Loss: 5.0846\n",
      "Epoch [2/5], Step [11175/136675], Loss: 5.3285\n",
      "Epoch [2/5], Step [11250/136675], Loss: 5.3207\n",
      "Epoch [2/5], Step [11325/136675], Loss: 5.5881\n",
      "Epoch [2/5], Step [11400/136675], Loss: 5.5494\n",
      "Epoch [2/5], Step [11475/136675], Loss: 5.1515\n",
      "Epoch [2/5], Step [11550/136675], Loss: 5.4857\n",
      "Epoch [2/5], Step [11625/136675], Loss: 5.6088\n",
      "Epoch [2/5], Step [11700/136675], Loss: 5.5744\n",
      "Epoch [2/5], Step [11775/136675], Loss: 5.7691\n",
      "Epoch [2/5], Step [11850/136675], Loss: 5.2308\n",
      "Epoch [2/5], Step [11925/136675], Loss: 5.3528\n",
      "Epoch [2/5], Step [12000/136675], Loss: 5.3812\n",
      "Epoch [2/5], Step [12075/136675], Loss: 5.4568\n",
      "Epoch [2/5], Step [12150/136675], Loss: 5.2269\n",
      "Epoch [2/5], Step [12225/136675], Loss: 5.1258\n",
      "Epoch [2/5], Step [12300/136675], Loss: 5.2876\n",
      "Epoch [2/5], Step [12375/136675], Loss: 5.2960\n",
      "Epoch [2/5], Step [12450/136675], Loss: 5.3311\n",
      "Epoch [2/5], Step [12525/136675], Loss: 5.1065\n",
      "Epoch [2/5], Step [12600/136675], Loss: 5.4675\n",
      "Epoch [2/5], Step [12675/136675], Loss: 5.3523\n",
      "Epoch [2/5], Step [12750/136675], Loss: 5.2643\n",
      "Epoch [2/5], Step [12825/136675], Loss: 5.1889\n",
      "Epoch [2/5], Step [12900/136675], Loss: 5.4270\n",
      "Epoch [2/5], Step [12975/136675], Loss: 5.3557\n",
      "Epoch [2/5], Step [13050/136675], Loss: 5.5932\n",
      "Epoch [2/5], Step [13125/136675], Loss: 5.3961\n",
      "Epoch [2/5], Step [13200/136675], Loss: 5.4503\n",
      "Epoch [2/5], Step [13275/136675], Loss: 5.1027\n",
      "Epoch [2/5], Step [13350/136675], Loss: 5.3851\n",
      "Epoch [2/5], Step [13425/136675], Loss: 5.3597\n",
      "Epoch [2/5], Step [13500/136675], Loss: 5.3662\n",
      "Epoch [2/5], Step [13575/136675], Loss: 5.5807\n",
      "Epoch [2/5], Step [13650/136675], Loss: 5.3849\n",
      "Epoch [2/5], Step [13725/136675], Loss: 5.5857\n",
      "Epoch [2/5], Step [13800/136675], Loss: 5.3781\n",
      "Epoch [2/5], Step [13875/136675], Loss: 5.4218\n",
      "Epoch [2/5], Step [13950/136675], Loss: 5.5067\n",
      "Epoch [2/5], Step [14025/136675], Loss: 5.6971\n",
      "Epoch [2/5], Step [14100/136675], Loss: 5.5494\n",
      "Epoch [2/5], Step [14175/136675], Loss: 5.4446\n",
      "Epoch [2/5], Step [14250/136675], Loss: 5.4742\n",
      "Epoch [2/5], Step [14325/136675], Loss: 5.2280\n",
      "Epoch [2/5], Step [14400/136675], Loss: 5.4895\n",
      "Epoch [2/5], Step [14475/136675], Loss: 5.2303\n",
      "Epoch [2/5], Step [14550/136675], Loss: 5.1364\n",
      "Epoch [2/5], Step [14625/136675], Loss: 5.5426\n",
      "Epoch [2/5], Step [14700/136675], Loss: 5.3541\n",
      "Epoch [2/5], Step [14775/136675], Loss: 5.4689\n",
      "Epoch [2/5], Step [14850/136675], Loss: 5.3021\n",
      "Epoch [2/5], Step [14925/136675], Loss: 5.1817\n",
      "Epoch [2/5], Step [15000/136675], Loss: 5.2702\n",
      "Epoch [2/5], Step [15075/136675], Loss: 5.4017\n",
      "Epoch [2/5], Step [15150/136675], Loss: 5.7138\n",
      "Epoch [2/5], Step [15225/136675], Loss: 5.4598\n",
      "Epoch [2/5], Step [15300/136675], Loss: 5.3305\n",
      "Epoch [2/5], Step [15375/136675], Loss: 5.7075\n",
      "Epoch [2/5], Step [15450/136675], Loss: 5.4234\n",
      "Epoch [2/5], Step [15525/136675], Loss: 5.3032\n",
      "Epoch [2/5], Step [15600/136675], Loss: 5.2929\n",
      "Epoch [2/5], Step [15675/136675], Loss: 5.4935\n",
      "Epoch [2/5], Step [15750/136675], Loss: 5.2301\n",
      "Epoch [2/5], Step [15825/136675], Loss: 5.5190\n",
      "Epoch [2/5], Step [15900/136675], Loss: 5.6212\n",
      "Epoch [2/5], Step [15975/136675], Loss: 5.4112\n",
      "Epoch [2/5], Step [16050/136675], Loss: 5.6293\n",
      "Epoch [2/5], Step [16125/136675], Loss: 5.5938\n",
      "Epoch [2/5], Step [16200/136675], Loss: 5.6865\n",
      "Epoch [2/5], Step [16275/136675], Loss: 5.3521\n",
      "Epoch [2/5], Step [16350/136675], Loss: 5.3346\n",
      "Epoch [2/5], Step [16425/136675], Loss: 5.8186\n",
      "Epoch [2/5], Step [16500/136675], Loss: 5.5540\n",
      "Epoch [2/5], Step [16575/136675], Loss: 5.1863\n",
      "Epoch [2/5], Step [16650/136675], Loss: 4.9572\n",
      "Epoch [2/5], Step [16725/136675], Loss: 5.4130\n",
      "Epoch [2/5], Step [16800/136675], Loss: 5.3932\n",
      "Epoch [2/5], Step [16875/136675], Loss: 5.5476\n",
      "Epoch [2/5], Step [16950/136675], Loss: 5.5603\n",
      "Epoch [2/5], Step [17025/136675], Loss: 5.5236\n",
      "Epoch [2/5], Step [17100/136675], Loss: 5.1096\n",
      "Epoch [2/5], Step [17175/136675], Loss: 5.3253\n",
      "Epoch [2/5], Step [17250/136675], Loss: 5.2457\n",
      "Epoch [2/5], Step [17325/136675], Loss: 5.4027\n",
      "Epoch [2/5], Step [17400/136675], Loss: 5.3350\n",
      "Epoch [2/5], Step [17475/136675], Loss: 5.9096\n",
      "Epoch [2/5], Step [17550/136675], Loss: 5.4158\n",
      "Epoch [2/5], Step [17625/136675], Loss: 5.4368\n",
      "Epoch [2/5], Step [17700/136675], Loss: 5.3524\n",
      "Epoch [2/5], Step [17775/136675], Loss: 5.3426\n",
      "Epoch [2/5], Step [17850/136675], Loss: 5.6038\n",
      "Epoch [2/5], Step [17925/136675], Loss: 5.4047\n",
      "Epoch [2/5], Step [18000/136675], Loss: 5.5466\n",
      "Epoch [2/5], Step [18075/136675], Loss: 5.2972\n",
      "Epoch [2/5], Step [18150/136675], Loss: 5.2835\n",
      "Epoch [2/5], Step [18225/136675], Loss: 5.0890\n",
      "Epoch [2/5], Step [18300/136675], Loss: 5.3427\n",
      "Epoch [2/5], Step [18375/136675], Loss: 5.4732\n",
      "Epoch [2/5], Step [18450/136675], Loss: 5.2212\n",
      "Epoch [2/5], Step [18525/136675], Loss: 5.0864\n",
      "Epoch [2/5], Step [18600/136675], Loss: 5.5156\n",
      "Epoch [2/5], Step [18675/136675], Loss: 5.4142\n",
      "Epoch [2/5], Step [18750/136675], Loss: 5.3669\n",
      "Epoch [2/5], Step [18825/136675], Loss: 5.1792\n",
      "Epoch [2/5], Step [18900/136675], Loss: 5.4681\n",
      "Epoch [2/5], Step [18975/136675], Loss: 5.2470\n",
      "Epoch [2/5], Step [19050/136675], Loss: 5.0076\n",
      "Epoch [2/5], Step [19125/136675], Loss: 5.5072\n",
      "Epoch [2/5], Step [19200/136675], Loss: 5.2871\n",
      "Epoch [2/5], Step [19275/136675], Loss: 5.4469\n",
      "Epoch [2/5], Step [19350/136675], Loss: 5.3095\n",
      "Epoch [2/5], Step [19425/136675], Loss: 5.4051\n",
      "Epoch [2/5], Step [19500/136675], Loss: 5.4309\n",
      "Epoch [2/5], Step [19575/136675], Loss: 5.3926\n",
      "Epoch [2/5], Step [19650/136675], Loss: 5.3948\n",
      "Epoch [2/5], Step [19725/136675], Loss: 5.2250\n",
      "Epoch [2/5], Step [19800/136675], Loss: 5.3116\n",
      "Epoch [2/5], Step [19875/136675], Loss: 5.1836\n",
      "Epoch [2/5], Step [19950/136675], Loss: 5.3710\n",
      "Validation perplexity: 174.698705280969\n",
      "Epoch [2/5], Step [20025/136675], Loss: 5.4592\n",
      "Epoch [2/5], Step [20100/136675], Loss: 5.1539\n",
      "Epoch [2/5], Step [20175/136675], Loss: 4.9523\n",
      "Epoch [2/5], Step [20250/136675], Loss: 5.5636\n",
      "Epoch [2/5], Step [20325/136675], Loss: 5.2208\n",
      "Epoch [2/5], Step [20400/136675], Loss: 5.0793\n",
      "Epoch [2/5], Step [20475/136675], Loss: 5.4566\n",
      "Epoch [2/5], Step [20550/136675], Loss: 5.4993\n",
      "Epoch [2/5], Step [20625/136675], Loss: 5.2164\n",
      "Epoch [2/5], Step [20700/136675], Loss: 5.3752\n",
      "Epoch [2/5], Step [20775/136675], Loss: 5.3454\n",
      "Epoch [2/5], Step [20850/136675], Loss: 5.1929\n",
      "Epoch [2/5], Step [20925/136675], Loss: 5.2232\n",
      "Epoch [2/5], Step [21000/136675], Loss: 5.6273\n",
      "Epoch [2/5], Step [21075/136675], Loss: 5.3734\n",
      "Epoch [2/5], Step [21150/136675], Loss: 5.3426\n",
      "Epoch [2/5], Step [21225/136675], Loss: 5.2018\n",
      "Epoch [2/5], Step [21300/136675], Loss: 5.4717\n",
      "Epoch [2/5], Step [21375/136675], Loss: 5.1232\n",
      "Epoch [2/5], Step [21450/136675], Loss: 5.5069\n",
      "Epoch [2/5], Step [21525/136675], Loss: 5.7462\n",
      "Epoch [2/5], Step [21600/136675], Loss: 5.2936\n",
      "Epoch [2/5], Step [21675/136675], Loss: 5.4953\n",
      "Epoch [2/5], Step [21750/136675], Loss: 5.6628\n",
      "Epoch [2/5], Step [21825/136675], Loss: 5.3637\n",
      "Epoch [2/5], Step [21900/136675], Loss: 5.4130\n",
      "Epoch [2/5], Step [21975/136675], Loss: 5.4737\n",
      "Epoch [2/5], Step [22050/136675], Loss: 5.4124\n",
      "Epoch [2/5], Step [22125/136675], Loss: 5.2780\n",
      "Epoch [2/5], Step [22200/136675], Loss: 5.5393\n",
      "Epoch [2/5], Step [22275/136675], Loss: 5.3679\n",
      "Epoch [2/5], Step [22350/136675], Loss: 5.5541\n",
      "Epoch [2/5], Step [22425/136675], Loss: 5.2485\n",
      "Epoch [2/5], Step [22500/136675], Loss: 5.4529\n",
      "Epoch [2/5], Step [22575/136675], Loss: 5.3578\n",
      "Epoch [2/5], Step [22650/136675], Loss: 5.3668\n",
      "Epoch [2/5], Step [22725/136675], Loss: 5.3492\n",
      "Epoch [2/5], Step [22800/136675], Loss: 5.5305\n",
      "Epoch [2/5], Step [22875/136675], Loss: 5.4966\n",
      "Epoch [2/5], Step [22950/136675], Loss: 5.4310\n",
      "Epoch [2/5], Step [23025/136675], Loss: 5.1918\n",
      "Epoch [2/5], Step [23100/136675], Loss: 5.3891\n",
      "Epoch [2/5], Step [23175/136675], Loss: 5.2936\n",
      "Epoch [2/5], Step [23250/136675], Loss: 5.3719\n",
      "Epoch [2/5], Step [23325/136675], Loss: 5.5532\n",
      "Epoch [2/5], Step [23400/136675], Loss: 5.2954\n",
      "Epoch [2/5], Step [23475/136675], Loss: 5.4393\n",
      "Epoch [2/5], Step [23550/136675], Loss: 5.4557\n",
      "Epoch [2/5], Step [23625/136675], Loss: 5.2426\n",
      "Epoch [2/5], Step [23700/136675], Loss: 5.6362\n",
      "Epoch [2/5], Step [23775/136675], Loss: 5.2428\n",
      "Epoch [2/5], Step [23850/136675], Loss: 5.0487\n",
      "Epoch [2/5], Step [23925/136675], Loss: 5.4954\n",
      "Epoch [2/5], Step [24000/136675], Loss: 5.2349\n",
      "Epoch [2/5], Step [24075/136675], Loss: 5.1591\n",
      "Epoch [2/5], Step [24150/136675], Loss: 5.2969\n",
      "Epoch [2/5], Step [24225/136675], Loss: 5.8960\n",
      "Epoch [2/5], Step [24300/136675], Loss: 5.4837\n",
      "Epoch [2/5], Step [24375/136675], Loss: 5.6950\n",
      "Epoch [2/5], Step [24450/136675], Loss: 5.3890\n",
      "Epoch [2/5], Step [24525/136675], Loss: 5.2159\n",
      "Epoch [2/5], Step [24600/136675], Loss: 5.4499\n",
      "Epoch [2/5], Step [24675/136675], Loss: 5.4472\n",
      "Epoch [2/5], Step [24750/136675], Loss: 5.5854\n",
      "Epoch [2/5], Step [24825/136675], Loss: 5.5419\n",
      "Epoch [2/5], Step [24900/136675], Loss: 5.2024\n",
      "Epoch [2/5], Step [24975/136675], Loss: 5.4391\n",
      "Epoch [2/5], Step [25050/136675], Loss: 5.5716\n",
      "Epoch [2/5], Step [25125/136675], Loss: 5.6733\n",
      "Epoch [2/5], Step [25200/136675], Loss: 5.5697\n",
      "Epoch [2/5], Step [25275/136675], Loss: 5.1188\n",
      "Epoch [2/5], Step [25350/136675], Loss: 5.5785\n",
      "Epoch [2/5], Step [25425/136675], Loss: 5.2046\n",
      "Epoch [2/5], Step [25500/136675], Loss: 5.3051\n",
      "Epoch [2/5], Step [25575/136675], Loss: 5.4492\n",
      "Epoch [2/5], Step [25650/136675], Loss: 5.6760\n",
      "Epoch [2/5], Step [25725/136675], Loss: 5.4644\n",
      "Epoch [2/5], Step [25800/136675], Loss: 5.1407\n",
      "Epoch [2/5], Step [25875/136675], Loss: 4.9534\n",
      "Epoch [2/5], Step [25950/136675], Loss: 5.3926\n",
      "Epoch [2/5], Step [26025/136675], Loss: 5.3501\n",
      "Epoch [2/5], Step [26100/136675], Loss: 5.4812\n",
      "Epoch [2/5], Step [26175/136675], Loss: 5.4319\n",
      "Epoch [2/5], Step [26250/136675], Loss: 5.4598\n",
      "Epoch [2/5], Step [26325/136675], Loss: 5.7687\n",
      "Epoch [2/5], Step [26400/136675], Loss: 5.2475\n",
      "Epoch [2/5], Step [26475/136675], Loss: 5.3895\n",
      "Epoch [2/5], Step [26550/136675], Loss: 5.1923\n",
      "Epoch [2/5], Step [26625/136675], Loss: 5.3870\n",
      "Epoch [2/5], Step [26700/136675], Loss: 5.4105\n",
      "Epoch [2/5], Step [26775/136675], Loss: 5.3581\n",
      "Epoch [2/5], Step [26850/136675], Loss: 5.5962\n",
      "Epoch [2/5], Step [26925/136675], Loss: 5.3786\n",
      "Epoch [2/5], Step [27000/136675], Loss: 4.8992\n",
      "Epoch [2/5], Step [27075/136675], Loss: 5.1452\n",
      "Epoch [2/5], Step [27150/136675], Loss: 5.6347\n",
      "Epoch [2/5], Step [27225/136675], Loss: 5.4991\n",
      "Epoch [2/5], Step [27300/136675], Loss: 5.4515\n",
      "Epoch [2/5], Step [27375/136675], Loss: 5.4725\n",
      "Epoch [2/5], Step [27450/136675], Loss: 5.2971\n",
      "Epoch [2/5], Step [27525/136675], Loss: 5.3696\n",
      "Epoch [2/5], Step [27600/136675], Loss: 5.2450\n",
      "Epoch [2/5], Step [27675/136675], Loss: 5.4887\n",
      "Epoch [2/5], Step [27750/136675], Loss: 5.3102\n",
      "Epoch [2/5], Step [27825/136675], Loss: 5.4617\n",
      "Epoch [2/5], Step [27900/136675], Loss: 5.1740\n",
      "Epoch [2/5], Step [27975/136675], Loss: 5.4559\n",
      "Epoch [2/5], Step [28050/136675], Loss: 5.3381\n",
      "Epoch [2/5], Step [28125/136675], Loss: 5.2963\n",
      "Epoch [2/5], Step [28200/136675], Loss: 5.3149\n",
      "Epoch [2/5], Step [28275/136675], Loss: 5.5746\n",
      "Epoch [2/5], Step [28350/136675], Loss: 5.1982\n",
      "Epoch [2/5], Step [28425/136675], Loss: 5.5676\n",
      "Epoch [2/5], Step [28500/136675], Loss: 5.2642\n",
      "Epoch [2/5], Step [28575/136675], Loss: 5.2832\n",
      "Epoch [2/5], Step [28650/136675], Loss: 5.1551\n",
      "Epoch [2/5], Step [28725/136675], Loss: 5.1005\n",
      "Epoch [2/5], Step [28800/136675], Loss: 5.3341\n",
      "Epoch [2/5], Step [28875/136675], Loss: 5.3118\n",
      "Epoch [2/5], Step [28950/136675], Loss: 5.4529\n",
      "Epoch [2/5], Step [29025/136675], Loss: 5.4036\n",
      "Epoch [2/5], Step [29100/136675], Loss: 5.3819\n",
      "Epoch [2/5], Step [29175/136675], Loss: 5.3784\n",
      "Epoch [2/5], Step [29250/136675], Loss: 5.1800\n",
      "Epoch [2/5], Step [29325/136675], Loss: 5.4984\n",
      "Epoch [2/5], Step [29400/136675], Loss: 5.3731\n",
      "Epoch [2/5], Step [29475/136675], Loss: 5.1966\n",
      "Epoch [2/5], Step [29550/136675], Loss: 5.5309\n",
      "Epoch [2/5], Step [29625/136675], Loss: 5.0912\n",
      "Epoch [2/5], Step [29700/136675], Loss: 5.5261\n",
      "Epoch [2/5], Step [29775/136675], Loss: 5.2857\n",
      "Epoch [2/5], Step [29850/136675], Loss: 5.4617\n",
      "Epoch [2/5], Step [29925/136675], Loss: 5.3136\n",
      "Epoch [2/5], Step [30000/136675], Loss: 5.4336\n",
      "Validation perplexity: 174.30732811615275\n",
      "Epoch [2/5], Step [30075/136675], Loss: 5.4992\n",
      "Epoch [2/5], Step [30150/136675], Loss: 5.3590\n",
      "Epoch [2/5], Step [30225/136675], Loss: 5.2056\n",
      "Epoch [2/5], Step [30300/136675], Loss: 5.2144\n",
      "Epoch [2/5], Step [30375/136675], Loss: 5.5242\n",
      "Epoch [2/5], Step [30450/136675], Loss: 5.5322\n",
      "Epoch [2/5], Step [30525/136675], Loss: 5.2253\n",
      "Epoch [2/5], Step [30600/136675], Loss: 5.3580\n",
      "Epoch [2/5], Step [30675/136675], Loss: 5.4363\n",
      "Epoch [2/5], Step [30750/136675], Loss: 5.1910\n",
      "Epoch [2/5], Step [30825/136675], Loss: 5.7170\n",
      "Epoch [2/5], Step [30900/136675], Loss: 5.1939\n",
      "Epoch [2/5], Step [30975/136675], Loss: 5.2573\n",
      "Epoch [2/5], Step [31050/136675], Loss: 5.3947\n",
      "Epoch [2/5], Step [31125/136675], Loss: 5.4170\n",
      "Epoch [2/5], Step [31200/136675], Loss: 5.2955\n",
      "Epoch [2/5], Step [31275/136675], Loss: 5.4951\n",
      "Epoch [2/5], Step [31350/136675], Loss: 5.7418\n",
      "Epoch [2/5], Step [31425/136675], Loss: 5.5001\n",
      "Epoch [2/5], Step [31500/136675], Loss: 5.4857\n",
      "Epoch [2/5], Step [31575/136675], Loss: 5.2629\n",
      "Epoch [2/5], Step [31650/136675], Loss: 5.5178\n",
      "Epoch [2/5], Step [31725/136675], Loss: 5.4950\n",
      "Epoch [2/5], Step [31800/136675], Loss: 5.4074\n",
      "Epoch [2/5], Step [31875/136675], Loss: 5.4973\n",
      "Epoch [2/5], Step [31950/136675], Loss: 5.3421\n",
      "Epoch [2/5], Step [32025/136675], Loss: 5.4548\n",
      "Epoch [2/5], Step [32100/136675], Loss: 5.5679\n",
      "Epoch [2/5], Step [32175/136675], Loss: 5.3933\n",
      "Epoch [2/5], Step [32250/136675], Loss: 5.4013\n",
      "Epoch [2/5], Step [32325/136675], Loss: 5.5877\n",
      "Epoch [2/5], Step [32400/136675], Loss: 5.5909\n",
      "Epoch [2/5], Step [32475/136675], Loss: 5.1895\n",
      "Epoch [2/5], Step [32550/136675], Loss: 5.2637\n",
      "Epoch [2/5], Step [32625/136675], Loss: 5.4696\n",
      "Epoch [2/5], Step [32700/136675], Loss: 5.3596\n",
      "Epoch [2/5], Step [32775/136675], Loss: 5.3276\n",
      "Epoch [2/5], Step [32850/136675], Loss: 5.5829\n",
      "Epoch [2/5], Step [32925/136675], Loss: 5.4581\n",
      "Epoch [2/5], Step [33000/136675], Loss: 5.4368\n",
      "Epoch [2/5], Step [33075/136675], Loss: 5.3438\n",
      "Epoch [2/5], Step [33150/136675], Loss: 5.5357\n",
      "Epoch [2/5], Step [33225/136675], Loss: 5.3034\n",
      "Epoch [2/5], Step [33300/136675], Loss: 5.0925\n",
      "Epoch [2/5], Step [33375/136675], Loss: 5.1517\n",
      "Epoch [2/5], Step [33450/136675], Loss: 5.5457\n",
      "Epoch [2/5], Step [33525/136675], Loss: 5.2733\n",
      "Epoch [2/5], Step [33600/136675], Loss: 5.4120\n",
      "Epoch [2/5], Step [33675/136675], Loss: 5.4244\n",
      "Epoch [2/5], Step [33750/136675], Loss: 5.1267\n",
      "Epoch [2/5], Step [33825/136675], Loss: 5.0768\n",
      "Epoch [2/5], Step [33900/136675], Loss: 5.4949\n",
      "Epoch [2/5], Step [33975/136675], Loss: 5.1435\n",
      "Epoch [2/5], Step [34050/136675], Loss: 5.1806\n",
      "Epoch [2/5], Step [34125/136675], Loss: 5.2715\n",
      "Epoch [2/5], Step [34200/136675], Loss: 5.3383\n",
      "Epoch [2/5], Step [34275/136675], Loss: 5.2759\n",
      "Epoch [2/5], Step [34350/136675], Loss: 5.3197\n",
      "Epoch [2/5], Step [34425/136675], Loss: 5.9291\n",
      "Epoch [2/5], Step [34500/136675], Loss: 5.3695\n",
      "Epoch [2/5], Step [34575/136675], Loss: 5.3778\n",
      "Epoch [2/5], Step [34650/136675], Loss: 5.6057\n",
      "Epoch [2/5], Step [34725/136675], Loss: 5.5824\n",
      "Epoch [2/5], Step [34800/136675], Loss: 5.4150\n",
      "Epoch [2/5], Step [34875/136675], Loss: 5.3516\n",
      "Epoch [2/5], Step [34950/136675], Loss: 5.4139\n",
      "Epoch [2/5], Step [35025/136675], Loss: 5.4267\n",
      "Epoch [2/5], Step [35100/136675], Loss: 5.0557\n",
      "Epoch [2/5], Step [35175/136675], Loss: 5.3280\n",
      "Epoch [2/5], Step [35250/136675], Loss: 5.2392\n",
      "Epoch [2/5], Step [35325/136675], Loss: 5.1907\n",
      "Epoch [2/5], Step [35400/136675], Loss: 5.3992\n",
      "Epoch [2/5], Step [35475/136675], Loss: 5.5234\n",
      "Epoch [2/5], Step [35550/136675], Loss: 5.3789\n",
      "Epoch [2/5], Step [35625/136675], Loss: 5.2688\n",
      "Epoch [2/5], Step [35700/136675], Loss: 5.3574\n",
      "Epoch [2/5], Step [35775/136675], Loss: 5.3984\n",
      "Epoch [2/5], Step [35850/136675], Loss: 5.4588\n",
      "Epoch [2/5], Step [35925/136675], Loss: 5.3588\n",
      "Epoch [2/5], Step [36000/136675], Loss: 5.4005\n",
      "Epoch [2/5], Step [36075/136675], Loss: 5.0913\n",
      "Epoch [2/5], Step [36150/136675], Loss: 5.0798\n",
      "Epoch [2/5], Step [36225/136675], Loss: 5.7536\n",
      "Epoch [2/5], Step [36300/136675], Loss: 5.1506\n",
      "Epoch [2/5], Step [36375/136675], Loss: 5.3486\n",
      "Epoch [2/5], Step [36450/136675], Loss: 5.3729\n",
      "Epoch [2/5], Step [36525/136675], Loss: 5.3628\n",
      "Epoch [2/5], Step [36600/136675], Loss: 5.1724\n",
      "Epoch [2/5], Step [36675/136675], Loss: 5.3065\n",
      "Epoch [2/5], Step [36750/136675], Loss: 5.2183\n",
      "Epoch [2/5], Step [36825/136675], Loss: 5.1109\n",
      "Epoch [2/5], Step [36900/136675], Loss: 5.0932\n",
      "Epoch [2/5], Step [36975/136675], Loss: 5.2805\n",
      "Epoch [2/5], Step [37050/136675], Loss: 5.2548\n",
      "Epoch [2/5], Step [37125/136675], Loss: 5.3535\n",
      "Epoch [2/5], Step [37200/136675], Loss: 5.3040\n",
      "Epoch [2/5], Step [37275/136675], Loss: 5.4847\n",
      "Epoch [2/5], Step [37350/136675], Loss: 5.4612\n",
      "Epoch [2/5], Step [37425/136675], Loss: 5.2859\n",
      "Epoch [2/5], Step [37500/136675], Loss: 5.2271\n",
      "Epoch [2/5], Step [37575/136675], Loss: 5.3641\n",
      "Epoch [2/5], Step [37650/136675], Loss: 5.3365\n",
      "Epoch [2/5], Step [37725/136675], Loss: 5.3535\n",
      "Epoch [2/5], Step [37800/136675], Loss: 5.8298\n",
      "Epoch [2/5], Step [37875/136675], Loss: 5.6170\n",
      "Epoch [2/5], Step [37950/136675], Loss: 5.0901\n",
      "Epoch [2/5], Step [38025/136675], Loss: 5.3784\n",
      "Epoch [2/5], Step [38100/136675], Loss: 5.3155\n",
      "Epoch [2/5], Step [38175/136675], Loss: 4.9602\n",
      "Epoch [2/5], Step [38250/136675], Loss: 5.1110\n",
      "Epoch [2/5], Step [38325/136675], Loss: 5.4623\n",
      "Epoch [2/5], Step [38400/136675], Loss: 5.4533\n",
      "Epoch [2/5], Step [38475/136675], Loss: 5.4220\n",
      "Epoch [2/5], Step [38550/136675], Loss: 5.4909\n",
      "Epoch [2/5], Step [38625/136675], Loss: 5.0618\n",
      "Epoch [2/5], Step [38700/136675], Loss: 5.5961\n",
      "Epoch [2/5], Step [38775/136675], Loss: 5.5067\n",
      "Epoch [2/5], Step [38850/136675], Loss: 5.0192\n",
      "Epoch [2/5], Step [38925/136675], Loss: 5.6472\n",
      "Epoch [2/5], Step [39000/136675], Loss: 5.4142\n",
      "Epoch [2/5], Step [39075/136675], Loss: 4.9796\n",
      "Epoch [2/5], Step [39150/136675], Loss: 5.1983\n",
      "Epoch [2/5], Step [39225/136675], Loss: 5.1950\n",
      "Epoch [2/5], Step [39300/136675], Loss: 5.3575\n",
      "Epoch [2/5], Step [39375/136675], Loss: 5.4133\n",
      "Epoch [2/5], Step [39450/136675], Loss: 5.5378\n",
      "Epoch [2/5], Step [39525/136675], Loss: 5.4300\n",
      "Epoch [2/5], Step [39600/136675], Loss: 5.5902\n",
      "Epoch [2/5], Step [39675/136675], Loss: 5.3822\n",
      "Epoch [2/5], Step [39750/136675], Loss: 5.5798\n",
      "Epoch [2/5], Step [39825/136675], Loss: 5.1241\n",
      "Epoch [2/5], Step [39900/136675], Loss: 5.1682\n",
      "Epoch [2/5], Step [39975/136675], Loss: 5.2300\n",
      "Validation perplexity: 173.67840874071732\n",
      "Epoch [2/5], Step [40050/136675], Loss: 5.3747\n",
      "Epoch [2/5], Step [40125/136675], Loss: 5.5803\n",
      "Epoch [2/5], Step [40200/136675], Loss: 5.1293\n",
      "Epoch [2/5], Step [40275/136675], Loss: 5.5648\n",
      "Epoch [2/5], Step [40350/136675], Loss: 5.2321\n",
      "Epoch [2/5], Step [40425/136675], Loss: 5.5925\n",
      "Epoch [2/5], Step [40500/136675], Loss: 5.3220\n",
      "Epoch [2/5], Step [40575/136675], Loss: 5.2143\n",
      "Epoch [2/5], Step [40650/136675], Loss: 5.3666\n",
      "Epoch [2/5], Step [40725/136675], Loss: 5.2185\n",
      "Epoch [2/5], Step [40800/136675], Loss: 5.4763\n",
      "Epoch [2/5], Step [40875/136675], Loss: 5.5716\n",
      "Epoch [2/5], Step [40950/136675], Loss: 5.1269\n",
      "Epoch [2/5], Step [41025/136675], Loss: 4.9656\n",
      "Epoch [2/5], Step [41100/136675], Loss: 5.6744\n",
      "Epoch [2/5], Step [41175/136675], Loss: 5.0157\n",
      "Epoch [2/5], Step [41250/136675], Loss: 5.2496\n",
      "Epoch [2/5], Step [41325/136675], Loss: 5.2696\n",
      "Epoch [2/5], Step [41400/136675], Loss: 5.3834\n",
      "Epoch [2/5], Step [41475/136675], Loss: 5.5593\n",
      "Epoch [2/5], Step [41550/136675], Loss: 5.3717\n",
      "Epoch [2/5], Step [41625/136675], Loss: 5.3531\n",
      "Epoch [2/5], Step [41700/136675], Loss: 5.3346\n",
      "Epoch [2/5], Step [41775/136675], Loss: 5.3159\n",
      "Epoch [2/5], Step [41850/136675], Loss: 5.3882\n",
      "Epoch [2/5], Step [41925/136675], Loss: 5.2785\n",
      "Epoch [2/5], Step [42000/136675], Loss: 5.3745\n",
      "Epoch [2/5], Step [42075/136675], Loss: 5.2658\n",
      "Epoch [2/5], Step [42150/136675], Loss: 5.5858\n",
      "Epoch [2/5], Step [42225/136675], Loss: 5.0709\n",
      "Epoch [2/5], Step [42300/136675], Loss: 5.5849\n",
      "Epoch [2/5], Step [42375/136675], Loss: 5.0166\n",
      "Epoch [2/5], Step [42450/136675], Loss: 5.3773\n",
      "Epoch [2/5], Step [42525/136675], Loss: 5.3725\n",
      "Epoch [2/5], Step [42600/136675], Loss: 5.6216\n",
      "Epoch [2/5], Step [42675/136675], Loss: 5.4054\n",
      "Epoch [2/5], Step [42750/136675], Loss: 5.4028\n",
      "Epoch [2/5], Step [42825/136675], Loss: 5.3939\n",
      "Epoch [2/5], Step [42900/136675], Loss: 5.2821\n",
      "Epoch [2/5], Step [42975/136675], Loss: 5.2138\n",
      "Epoch [2/5], Step [43050/136675], Loss: 5.0085\n",
      "Epoch [2/5], Step [43125/136675], Loss: 5.5749\n",
      "Epoch [2/5], Step [43200/136675], Loss: 5.2413\n",
      "Epoch [2/5], Step [43275/136675], Loss: 5.3443\n",
      "Epoch [2/5], Step [43350/136675], Loss: 5.2913\n",
      "Epoch [2/5], Step [43425/136675], Loss: 5.6462\n",
      "Epoch [2/5], Step [43500/136675], Loss: 5.3594\n",
      "Epoch [2/5], Step [43575/136675], Loss: 5.2640\n",
      "Epoch [2/5], Step [43650/136675], Loss: 5.2724\n",
      "Epoch [2/5], Step [43725/136675], Loss: 5.3370\n",
      "Epoch [2/5], Step [43800/136675], Loss: 5.2577\n",
      "Epoch [2/5], Step [43875/136675], Loss: 5.4063\n",
      "Epoch [2/5], Step [43950/136675], Loss: 5.0514\n",
      "Epoch [2/5], Step [44025/136675], Loss: 5.6061\n",
      "Epoch [2/5], Step [44100/136675], Loss: 5.2328\n",
      "Epoch [2/5], Step [44175/136675], Loss: 5.1488\n",
      "Epoch [2/5], Step [44250/136675], Loss: 5.4547\n",
      "Epoch [2/5], Step [44325/136675], Loss: 5.3860\n",
      "Epoch [2/5], Step [44400/136675], Loss: 5.2140\n",
      "Epoch [2/5], Step [44475/136675], Loss: 5.2682\n",
      "Epoch [2/5], Step [44550/136675], Loss: 5.4665\n",
      "Epoch [2/5], Step [44625/136675], Loss: 5.5438\n",
      "Epoch [2/5], Step [44700/136675], Loss: 5.3045\n",
      "Epoch [2/5], Step [44775/136675], Loss: 5.0543\n",
      "Epoch [2/5], Step [44850/136675], Loss: 5.2610\n",
      "Epoch [2/5], Step [44925/136675], Loss: 5.3866\n",
      "Epoch [2/5], Step [45000/136675], Loss: 5.0203\n",
      "Epoch [2/5], Step [45075/136675], Loss: 5.7577\n",
      "Epoch [2/5], Step [45150/136675], Loss: 5.2399\n",
      "Epoch [2/5], Step [45225/136675], Loss: 5.2433\n",
      "Epoch [2/5], Step [45300/136675], Loss: 5.1266\n",
      "Epoch [2/5], Step [45375/136675], Loss: 5.4526\n",
      "Epoch [2/5], Step [45450/136675], Loss: 5.3596\n",
      "Epoch [2/5], Step [45525/136675], Loss: 5.5211\n",
      "Epoch [2/5], Step [45600/136675], Loss: 5.4314\n",
      "Epoch [2/5], Step [45675/136675], Loss: 5.2068\n",
      "Epoch [2/5], Step [45750/136675], Loss: 5.3768\n",
      "Epoch [2/5], Step [45825/136675], Loss: 5.3597\n",
      "Epoch [2/5], Step [45900/136675], Loss: 5.2558\n",
      "Epoch [2/5], Step [45975/136675], Loss: 5.3926\n",
      "Epoch [2/5], Step [46050/136675], Loss: 5.6769\n",
      "Epoch [2/5], Step [46125/136675], Loss: 5.3042\n",
      "Epoch [2/5], Step [46200/136675], Loss: 5.3092\n",
      "Epoch [2/5], Step [46275/136675], Loss: 5.2965\n",
      "Epoch [2/5], Step [46350/136675], Loss: 5.3538\n",
      "Epoch [2/5], Step [46425/136675], Loss: 5.4253\n",
      "Epoch [2/5], Step [46500/136675], Loss: 5.3844\n",
      "Epoch [2/5], Step [46575/136675], Loss: 5.4110\n",
      "Epoch [2/5], Step [46650/136675], Loss: 5.5765\n",
      "Epoch [2/5], Step [46725/136675], Loss: 5.3373\n",
      "Epoch [2/5], Step [46800/136675], Loss: 5.6372\n",
      "Epoch [2/5], Step [46875/136675], Loss: 5.1795\n",
      "Epoch [2/5], Step [46950/136675], Loss: 5.4727\n",
      "Epoch [2/5], Step [47025/136675], Loss: 5.0981\n",
      "Epoch [2/5], Step [47100/136675], Loss: 5.3247\n",
      "Epoch [2/5], Step [47175/136675], Loss: 5.2190\n",
      "Epoch [2/5], Step [47250/136675], Loss: 5.3284\n",
      "Epoch [2/5], Step [47325/136675], Loss: 5.2570\n",
      "Epoch [2/5], Step [47400/136675], Loss: 5.2444\n",
      "Epoch [2/5], Step [47475/136675], Loss: 5.0499\n",
      "Epoch [2/5], Step [47550/136675], Loss: 5.5697\n",
      "Epoch [2/5], Step [47625/136675], Loss: 5.2026\n",
      "Epoch [2/5], Step [47700/136675], Loss: 5.5645\n",
      "Epoch [2/5], Step [47775/136675], Loss: 5.4797\n",
      "Epoch [2/5], Step [47850/136675], Loss: 5.5576\n",
      "Epoch [2/5], Step [47925/136675], Loss: 5.3443\n",
      "Epoch [2/5], Step [48000/136675], Loss: 5.3478\n",
      "Epoch [2/5], Step [48075/136675], Loss: 5.3322\n",
      "Epoch [2/5], Step [48150/136675], Loss: 5.4464\n",
      "Epoch [2/5], Step [48225/136675], Loss: 5.5118\n",
      "Epoch [2/5], Step [48300/136675], Loss: 5.4109\n",
      "Epoch [2/5], Step [48375/136675], Loss: 5.6256\n",
      "Epoch [2/5], Step [48450/136675], Loss: 5.2350\n",
      "Epoch [2/5], Step [48525/136675], Loss: 5.8327\n",
      "Epoch [2/5], Step [48600/136675], Loss: 5.0719\n",
      "Epoch [2/5], Step [48675/136675], Loss: 5.3802\n",
      "Epoch [2/5], Step [48750/136675], Loss: 5.4459\n",
      "Epoch [2/5], Step [48825/136675], Loss: 5.5889\n",
      "Epoch [2/5], Step [48900/136675], Loss: 5.4571\n",
      "Epoch [2/5], Step [48975/136675], Loss: 5.4583\n",
      "Epoch [2/5], Step [49050/136675], Loss: 5.1814\n",
      "Epoch [2/5], Step [49125/136675], Loss: 5.2395\n",
      "Epoch [2/5], Step [49200/136675], Loss: 5.4063\n",
      "Epoch [2/5], Step [49275/136675], Loss: 5.2530\n",
      "Epoch [2/5], Step [49350/136675], Loss: 5.4845\n",
      "Epoch [2/5], Step [49425/136675], Loss: 5.7741\n",
      "Epoch [2/5], Step [49500/136675], Loss: 5.2143\n",
      "Epoch [2/5], Step [49575/136675], Loss: 5.5356\n",
      "Epoch [2/5], Step [49650/136675], Loss: 5.5163\n",
      "Epoch [2/5], Step [49725/136675], Loss: 5.4353\n",
      "Epoch [2/5], Step [49800/136675], Loss: 5.5617\n",
      "Epoch [2/5], Step [49875/136675], Loss: 5.2423\n",
      "Epoch [2/5], Step [49950/136675], Loss: 5.4925\n",
      "Validation perplexity: 172.09146589098316\n",
      "Epoch [2/5], Step [50025/136675], Loss: 5.3464\n",
      "Epoch [2/5], Step [50100/136675], Loss: 5.4695\n",
      "Epoch [2/5], Step [50175/136675], Loss: 5.6812\n",
      "Epoch [2/5], Step [50250/136675], Loss: 5.4153\n",
      "Epoch [2/5], Step [50325/136675], Loss: 5.4612\n",
      "Epoch [2/5], Step [50400/136675], Loss: 5.3408\n",
      "Epoch [2/5], Step [50475/136675], Loss: 5.3467\n",
      "Epoch [2/5], Step [50550/136675], Loss: 5.3857\n",
      "Epoch [2/5], Step [50625/136675], Loss: 5.5603\n",
      "Epoch [2/5], Step [50700/136675], Loss: 5.3897\n",
      "Epoch [2/5], Step [50775/136675], Loss: 5.1933\n",
      "Epoch [2/5], Step [50850/136675], Loss: 5.5144\n",
      "Epoch [2/5], Step [50925/136675], Loss: 5.5706\n",
      "Epoch [2/5], Step [51000/136675], Loss: 5.3457\n",
      "Epoch [2/5], Step [51075/136675], Loss: 5.3990\n",
      "Epoch [2/5], Step [51150/136675], Loss: 5.4323\n",
      "Epoch [2/5], Step [51225/136675], Loss: 5.6397\n",
      "Epoch [2/5], Step [51300/136675], Loss: 5.2492\n",
      "Epoch [2/5], Step [51375/136675], Loss: 5.2510\n",
      "Epoch [2/5], Step [51450/136675], Loss: 5.3749\n",
      "Epoch [2/5], Step [51525/136675], Loss: 5.3048\n",
      "Epoch [2/5], Step [51600/136675], Loss: 5.4924\n",
      "Epoch [2/5], Step [51675/136675], Loss: 5.3754\n",
      "Epoch [2/5], Step [51750/136675], Loss: 5.2394\n",
      "Epoch [2/5], Step [51825/136675], Loss: 5.0327\n",
      "Epoch [2/5], Step [51900/136675], Loss: 4.9972\n",
      "Epoch [2/5], Step [51975/136675], Loss: 5.6263\n",
      "Epoch [2/5], Step [52050/136675], Loss: 5.1234\n",
      "Epoch [2/5], Step [52125/136675], Loss: 5.3178\n",
      "Epoch [2/5], Step [52200/136675], Loss: 5.2519\n",
      "Epoch [2/5], Step [52275/136675], Loss: 5.2555\n",
      "Epoch [2/5], Step [52350/136675], Loss: 5.1442\n",
      "Epoch [2/5], Step [52425/136675], Loss: 5.3636\n",
      "Epoch [2/5], Step [52500/136675], Loss: 5.2157\n",
      "Epoch [2/5], Step [52575/136675], Loss: 5.3654\n",
      "Epoch [2/5], Step [52650/136675], Loss: 5.4039\n",
      "Epoch [2/5], Step [52725/136675], Loss: 5.6197\n",
      "Epoch [2/5], Step [52800/136675], Loss: 5.3084\n",
      "Epoch [2/5], Step [52875/136675], Loss: 5.2598\n",
      "Epoch [2/5], Step [52950/136675], Loss: 5.3492\n",
      "Epoch [2/5], Step [53025/136675], Loss: 5.2868\n",
      "Epoch [2/5], Step [53100/136675], Loss: 5.6788\n",
      "Epoch [2/5], Step [53175/136675], Loss: 5.0752\n",
      "Epoch [2/5], Step [53250/136675], Loss: 5.2270\n",
      "Epoch [2/5], Step [53325/136675], Loss: 5.6670\n",
      "Epoch [2/5], Step [53400/136675], Loss: 5.2847\n",
      "Epoch [2/5], Step [53475/136675], Loss: 5.2454\n",
      "Epoch [2/5], Step [53550/136675], Loss: 5.6248\n",
      "Epoch [2/5], Step [53625/136675], Loss: 5.3288\n",
      "Epoch [2/5], Step [53700/136675], Loss: 5.5002\n",
      "Epoch [2/5], Step [53775/136675], Loss: 5.0932\n",
      "Epoch [2/5], Step [53850/136675], Loss: 5.5348\n",
      "Epoch [2/5], Step [53925/136675], Loss: 6.0167\n",
      "Epoch [2/5], Step [54000/136675], Loss: 5.5716\n",
      "Epoch [2/5], Step [54075/136675], Loss: 5.2528\n",
      "Epoch [2/5], Step [54150/136675], Loss: 5.1736\n",
      "Epoch [2/5], Step [54225/136675], Loss: 5.4632\n",
      "Epoch [2/5], Step [54300/136675], Loss: 5.0817\n",
      "Epoch [2/5], Step [54375/136675], Loss: 5.0960\n",
      "Epoch [2/5], Step [54450/136675], Loss: 5.4728\n",
      "Epoch [2/5], Step [54525/136675], Loss: 5.4652\n",
      "Epoch [2/5], Step [54600/136675], Loss: 5.4644\n",
      "Epoch [2/5], Step [54675/136675], Loss: 5.1769\n",
      "Epoch [2/5], Step [54750/136675], Loss: 5.2756\n",
      "Epoch [2/5], Step [54825/136675], Loss: 5.4824\n",
      "Epoch [2/5], Step [54900/136675], Loss: 5.2754\n",
      "Epoch [2/5], Step [54975/136675], Loss: 5.6109\n",
      "Epoch [2/5], Step [55050/136675], Loss: 5.3818\n",
      "Epoch [2/5], Step [55125/136675], Loss: 5.1261\n",
      "Epoch [2/5], Step [55200/136675], Loss: 5.4681\n",
      "Epoch [2/5], Step [55275/136675], Loss: 5.8861\n",
      "Epoch [2/5], Step [55350/136675], Loss: 5.1345\n",
      "Epoch [2/5], Step [55425/136675], Loss: 5.4482\n",
      "Epoch [2/5], Step [55500/136675], Loss: 5.3685\n",
      "Epoch [2/5], Step [55575/136675], Loss: 5.2528\n",
      "Epoch [2/5], Step [55650/136675], Loss: 5.6677\n",
      "Epoch [2/5], Step [55725/136675], Loss: 5.5091\n",
      "Epoch [2/5], Step [55800/136675], Loss: 5.1645\n",
      "Epoch [2/5], Step [55875/136675], Loss: 5.6202\n",
      "Epoch [2/5], Step [55950/136675], Loss: 5.4210\n",
      "Epoch [2/5], Step [56025/136675], Loss: 5.2100\n",
      "Epoch [2/5], Step [56100/136675], Loss: 5.0171\n",
      "Epoch [2/5], Step [56175/136675], Loss: 5.2095\n",
      "Epoch [2/5], Step [56250/136675], Loss: 5.1613\n",
      "Epoch [2/5], Step [56325/136675], Loss: 5.2496\n",
      "Epoch [2/5], Step [56400/136675], Loss: 5.5017\n",
      "Epoch [2/5], Step [56475/136675], Loss: 5.3668\n",
      "Epoch [2/5], Step [56550/136675], Loss: 5.0540\n",
      "Epoch [2/5], Step [56625/136675], Loss: 5.1601\n",
      "Epoch [2/5], Step [56700/136675], Loss: 5.4980\n",
      "Epoch [2/5], Step [56775/136675], Loss: 5.3362\n",
      "Epoch [2/5], Step [56850/136675], Loss: 5.5251\n",
      "Epoch [2/5], Step [56925/136675], Loss: 5.6601\n",
      "Epoch [2/5], Step [57000/136675], Loss: 5.3062\n",
      "Epoch [2/5], Step [57075/136675], Loss: 5.3199\n",
      "Epoch [2/5], Step [57150/136675], Loss: 5.4471\n",
      "Epoch [2/5], Step [57225/136675], Loss: 5.7672\n",
      "Epoch [2/5], Step [57300/136675], Loss: 4.9647\n",
      "Epoch [2/5], Step [57375/136675], Loss: 5.4828\n",
      "Epoch [2/5], Step [57450/136675], Loss: 5.2491\n",
      "Epoch [2/5], Step [57525/136675], Loss: 5.4629\n",
      "Epoch [2/5], Step [57600/136675], Loss: 5.3868\n",
      "Epoch [2/5], Step [57675/136675], Loss: 5.0813\n",
      "Epoch [2/5], Step [57750/136675], Loss: 5.6639\n",
      "Epoch [2/5], Step [57825/136675], Loss: 5.3064\n",
      "Epoch [2/5], Step [57900/136675], Loss: 5.0669\n",
      "Epoch [2/5], Step [57975/136675], Loss: 5.1100\n",
      "Epoch [2/5], Step [58050/136675], Loss: 5.1075\n",
      "Epoch [2/5], Step [58125/136675], Loss: 5.4711\n",
      "Epoch [2/5], Step [58200/136675], Loss: 5.0427\n",
      "Epoch [2/5], Step [58275/136675], Loss: 5.2484\n",
      "Epoch [2/5], Step [58350/136675], Loss: 5.0780\n",
      "Epoch [2/5], Step [58425/136675], Loss: 5.1660\n",
      "Epoch [2/5], Step [58500/136675], Loss: 5.0966\n",
      "Epoch [2/5], Step [58575/136675], Loss: 5.2042\n",
      "Epoch [2/5], Step [58650/136675], Loss: 5.6126\n",
      "Epoch [2/5], Step [58725/136675], Loss: 5.4391\n",
      "Epoch [2/5], Step [58800/136675], Loss: 5.3709\n",
      "Epoch [2/5], Step [58875/136675], Loss: 5.0999\n",
      "Epoch [2/5], Step [58950/136675], Loss: 5.5024\n",
      "Epoch [2/5], Step [59025/136675], Loss: 5.1447\n",
      "Epoch [2/5], Step [59100/136675], Loss: 5.0509\n",
      "Epoch [2/5], Step [59175/136675], Loss: 5.3632\n",
      "Epoch [2/5], Step [59250/136675], Loss: 5.3244\n",
      "Epoch [2/5], Step [59325/136675], Loss: 5.3684\n",
      "Epoch [2/5], Step [59400/136675], Loss: 5.4983\n",
      "Epoch [2/5], Step [59475/136675], Loss: 5.3353\n",
      "Epoch [2/5], Step [59550/136675], Loss: 5.3041\n",
      "Epoch [2/5], Step [59625/136675], Loss: 5.3984\n",
      "Epoch [2/5], Step [59700/136675], Loss: 5.3376\n",
      "Epoch [2/5], Step [59775/136675], Loss: 5.1757\n",
      "Epoch [2/5], Step [59850/136675], Loss: 5.1296\n",
      "Epoch [2/5], Step [59925/136675], Loss: 5.1892\n",
      "Epoch [2/5], Step [60000/136675], Loss: 5.5650\n",
      "Validation perplexity: 170.68447535424633\n",
      "Epoch [2/5], Step [60075/136675], Loss: 5.4397\n",
      "Epoch [2/5], Step [60150/136675], Loss: 5.2809\n",
      "Epoch [2/5], Step [60225/136675], Loss: 5.3258\n",
      "Epoch [2/5], Step [60300/136675], Loss: 5.4459\n",
      "Epoch [2/5], Step [60375/136675], Loss: 5.3901\n",
      "Epoch [2/5], Step [60450/136675], Loss: 5.2703\n",
      "Epoch [2/5], Step [60525/136675], Loss: 5.3881\n",
      "Epoch [2/5], Step [60600/136675], Loss: 5.0320\n",
      "Epoch [2/5], Step [60675/136675], Loss: 4.8750\n",
      "Epoch [2/5], Step [60750/136675], Loss: 5.4639\n",
      "Epoch [2/5], Step [60825/136675], Loss: 5.3250\n",
      "Epoch [2/5], Step [60900/136675], Loss: 5.6430\n",
      "Epoch [2/5], Step [60975/136675], Loss: 5.2132\n",
      "Epoch [2/5], Step [61050/136675], Loss: 5.1667\n",
      "Epoch [2/5], Step [61125/136675], Loss: 5.3254\n",
      "Epoch [2/5], Step [61200/136675], Loss: 5.5599\n",
      "Epoch [2/5], Step [61275/136675], Loss: 5.3743\n",
      "Epoch [2/5], Step [61350/136675], Loss: 5.1297\n",
      "Epoch [2/5], Step [61425/136675], Loss: 5.3454\n",
      "Epoch [2/5], Step [61500/136675], Loss: 5.4807\n",
      "Epoch [2/5], Step [61575/136675], Loss: 5.3334\n",
      "Epoch [2/5], Step [61650/136675], Loss: 5.3210\n",
      "Epoch [2/5], Step [61725/136675], Loss: 5.4763\n",
      "Epoch [2/5], Step [61800/136675], Loss: 5.1201\n",
      "Epoch [2/5], Step [61875/136675], Loss: 5.2727\n",
      "Epoch [2/5], Step [61950/136675], Loss: 5.2632\n",
      "Epoch [2/5], Step [62025/136675], Loss: 5.3770\n",
      "Epoch [2/5], Step [62100/136675], Loss: 5.3652\n",
      "Epoch [2/5], Step [62175/136675], Loss: 5.2495\n",
      "Epoch [2/5], Step [62250/136675], Loss: 5.3978\n",
      "Epoch [2/5], Step [62325/136675], Loss: 5.5117\n",
      "Epoch [2/5], Step [62400/136675], Loss: 5.4849\n",
      "Epoch [2/5], Step [62475/136675], Loss: 5.3977\n",
      "Epoch [2/5], Step [62550/136675], Loss: 5.5440\n",
      "Epoch [2/5], Step [62625/136675], Loss: 5.3304\n",
      "Epoch [2/5], Step [62700/136675], Loss: 5.6529\n",
      "Epoch [2/5], Step [62775/136675], Loss: 5.5800\n",
      "Epoch [2/5], Step [62850/136675], Loss: 5.2394\n",
      "Epoch [2/5], Step [62925/136675], Loss: 5.0894\n",
      "Epoch [2/5], Step [63000/136675], Loss: 5.6504\n",
      "Epoch [2/5], Step [63075/136675], Loss: 5.3808\n",
      "Epoch [2/5], Step [63150/136675], Loss: 5.3144\n",
      "Epoch [2/5], Step [63225/136675], Loss: 5.3070\n",
      "Epoch [2/5], Step [63300/136675], Loss: 5.3637\n",
      "Epoch [2/5], Step [63375/136675], Loss: 5.3020\n",
      "Epoch [2/5], Step [63450/136675], Loss: 5.1700\n",
      "Epoch [2/5], Step [63525/136675], Loss: 5.2491\n",
      "Epoch [2/5], Step [63600/136675], Loss: 5.0268\n",
      "Epoch [2/5], Step [63675/136675], Loss: 5.6903\n",
      "Epoch [2/5], Step [63750/136675], Loss: 5.4155\n",
      "Epoch [2/5], Step [63825/136675], Loss: 5.2403\n",
      "Epoch [2/5], Step [63900/136675], Loss: 5.0261\n",
      "Epoch [2/5], Step [63975/136675], Loss: 5.1503\n",
      "Epoch [2/5], Step [64050/136675], Loss: 5.0691\n",
      "Epoch [2/5], Step [64125/136675], Loss: 5.2768\n",
      "Epoch [2/5], Step [64200/136675], Loss: 5.1935\n",
      "Epoch [2/5], Step [64275/136675], Loss: 5.3665\n",
      "Epoch [2/5], Step [64350/136675], Loss: 5.4272\n",
      "Epoch [2/5], Step [64425/136675], Loss: 5.2756\n",
      "Epoch [2/5], Step [64500/136675], Loss: 5.3856\n",
      "Epoch [2/5], Step [64575/136675], Loss: 5.4679\n",
      "Epoch [2/5], Step [64650/136675], Loss: 5.1141\n",
      "Epoch [2/5], Step [64725/136675], Loss: 5.3962\n",
      "Epoch [2/5], Step [64800/136675], Loss: 5.5491\n",
      "Epoch [2/5], Step [64875/136675], Loss: 5.3049\n",
      "Epoch [2/5], Step [64950/136675], Loss: 5.5714\n",
      "Epoch [2/5], Step [65025/136675], Loss: 5.2697\n",
      "Epoch [2/5], Step [65100/136675], Loss: 5.3525\n",
      "Epoch [2/5], Step [65175/136675], Loss: 5.0568\n",
      "Epoch [2/5], Step [65250/136675], Loss: 5.1370\n",
      "Epoch [2/5], Step [65325/136675], Loss: 5.6238\n",
      "Epoch [2/5], Step [65400/136675], Loss: 5.1801\n",
      "Epoch [2/5], Step [65475/136675], Loss: 5.4714\n",
      "Epoch [2/5], Step [65550/136675], Loss: 5.4089\n",
      "Epoch [2/5], Step [65625/136675], Loss: 5.3324\n",
      "Epoch [2/5], Step [65700/136675], Loss: 5.2643\n",
      "Epoch [2/5], Step [65775/136675], Loss: 5.3172\n",
      "Epoch [2/5], Step [65850/136675], Loss: 5.4462\n",
      "Epoch [2/5], Step [65925/136675], Loss: 5.0346\n",
      "Epoch [2/5], Step [66000/136675], Loss: 5.4100\n",
      "Epoch [2/5], Step [66075/136675], Loss: 5.2968\n",
      "Epoch [2/5], Step [66150/136675], Loss: 5.5532\n",
      "Epoch [2/5], Step [66225/136675], Loss: 5.2960\n",
      "Epoch [2/5], Step [66300/136675], Loss: 4.9282\n",
      "Epoch [2/5], Step [66375/136675], Loss: 5.4936\n",
      "Epoch [2/5], Step [66450/136675], Loss: 5.2685\n",
      "Epoch [2/5], Step [66525/136675], Loss: 5.5144\n",
      "Epoch [2/5], Step [66600/136675], Loss: 5.5972\n",
      "Epoch [2/5], Step [66675/136675], Loss: 5.1215\n",
      "Epoch [2/5], Step [66750/136675], Loss: 5.5059\n",
      "Epoch [2/5], Step [66825/136675], Loss: 5.5930\n",
      "Epoch [2/5], Step [66900/136675], Loss: 5.3551\n",
      "Epoch [2/5], Step [66975/136675], Loss: 5.3312\n",
      "Epoch [2/5], Step [67050/136675], Loss: 5.2563\n",
      "Epoch [2/5], Step [67125/136675], Loss: 5.5414\n",
      "Epoch [2/5], Step [67200/136675], Loss: 5.3016\n",
      "Epoch [2/5], Step [67275/136675], Loss: 5.4079\n",
      "Epoch [2/5], Step [67350/136675], Loss: 5.2341\n",
      "Epoch [2/5], Step [67425/136675], Loss: 5.4764\n",
      "Epoch [2/5], Step [67500/136675], Loss: 5.6372\n",
      "Epoch [2/5], Step [67575/136675], Loss: 5.6010\n",
      "Epoch [2/5], Step [67650/136675], Loss: 5.3812\n",
      "Epoch [2/5], Step [67725/136675], Loss: 5.2092\n",
      "Epoch [2/5], Step [67800/136675], Loss: 5.2556\n",
      "Epoch [2/5], Step [67875/136675], Loss: 5.2257\n",
      "Epoch [2/5], Step [67950/136675], Loss: 5.1996\n",
      "Epoch [2/5], Step [68025/136675], Loss: 5.2655\n",
      "Epoch [2/5], Step [68100/136675], Loss: 5.3197\n",
      "Epoch [2/5], Step [68175/136675], Loss: 5.5502\n",
      "Epoch [2/5], Step [68250/136675], Loss: 4.9527\n",
      "Epoch [2/5], Step [68325/136675], Loss: 5.2040\n",
      "Epoch [2/5], Step [68400/136675], Loss: 5.6202\n",
      "Epoch [2/5], Step [68475/136675], Loss: 5.7701\n",
      "Epoch [2/5], Step [68550/136675], Loss: 5.2354\n",
      "Epoch [2/5], Step [68625/136675], Loss: 5.6522\n",
      "Epoch [2/5], Step [68700/136675], Loss: 5.2143\n",
      "Epoch [2/5], Step [68775/136675], Loss: 5.1134\n",
      "Epoch [2/5], Step [68850/136675], Loss: 5.2386\n",
      "Epoch [2/5], Step [68925/136675], Loss: 4.9970\n",
      "Epoch [2/5], Step [69000/136675], Loss: 5.3285\n",
      "Epoch [2/5], Step [69075/136675], Loss: 5.4101\n",
      "Epoch [2/5], Step [69150/136675], Loss: 4.8466\n",
      "Epoch [2/5], Step [69225/136675], Loss: 5.2783\n",
      "Epoch [2/5], Step [69300/136675], Loss: 5.3983\n",
      "Epoch [2/5], Step [69375/136675], Loss: 5.3723\n",
      "Epoch [2/5], Step [69450/136675], Loss: 5.2588\n",
      "Epoch [2/5], Step [69525/136675], Loss: 5.5085\n",
      "Epoch [2/5], Step [69600/136675], Loss: 5.5909\n",
      "Epoch [2/5], Step [69675/136675], Loss: 5.2076\n",
      "Epoch [2/5], Step [69750/136675], Loss: 5.5478\n",
      "Epoch [2/5], Step [69825/136675], Loss: 5.2381\n",
      "Epoch [2/5], Step [69900/136675], Loss: 5.6730\n",
      "Epoch [2/5], Step [69975/136675], Loss: 5.5804\n",
      "Validation perplexity: 169.96554545205078\n",
      "Epoch [2/5], Step [70050/136675], Loss: 5.1673\n",
      "Epoch [2/5], Step [70125/136675], Loss: 5.3051\n",
      "Epoch [2/5], Step [70200/136675], Loss: 5.6561\n",
      "Epoch [2/5], Step [70275/136675], Loss: 5.4159\n",
      "Epoch [2/5], Step [70350/136675], Loss: 5.6316\n",
      "Epoch [2/5], Step [70425/136675], Loss: 5.4696\n",
      "Epoch [2/5], Step [70500/136675], Loss: 5.4351\n",
      "Epoch [2/5], Step [70575/136675], Loss: 5.1638\n",
      "Epoch [2/5], Step [70650/136675], Loss: 5.0683\n",
      "Epoch [2/5], Step [70725/136675], Loss: 5.5457\n",
      "Epoch [2/5], Step [70800/136675], Loss: 5.6024\n",
      "Epoch [2/5], Step [70875/136675], Loss: 5.2971\n",
      "Epoch [2/5], Step [70950/136675], Loss: 5.3703\n",
      "Epoch [2/5], Step [71025/136675], Loss: 5.1497\n",
      "Epoch [2/5], Step [71100/136675], Loss: 5.2014\n",
      "Epoch [2/5], Step [71175/136675], Loss: 5.1960\n",
      "Epoch [2/5], Step [71250/136675], Loss: 5.3895\n",
      "Epoch [2/5], Step [71325/136675], Loss: 5.1205\n",
      "Epoch [2/5], Step [71400/136675], Loss: 5.4712\n",
      "Epoch [2/5], Step [71475/136675], Loss: 5.3174\n",
      "Epoch [2/5], Step [71550/136675], Loss: 5.4055\n",
      "Epoch [2/5], Step [71625/136675], Loss: 5.3570\n",
      "Epoch [2/5], Step [71700/136675], Loss: 5.2460\n",
      "Epoch [2/5], Step [71775/136675], Loss: 5.3462\n",
      "Epoch [2/5], Step [71850/136675], Loss: 5.2666\n",
      "Epoch [2/5], Step [71925/136675], Loss: 5.1496\n",
      "Epoch [2/5], Step [72000/136675], Loss: 5.7562\n",
      "Epoch [2/5], Step [72075/136675], Loss: 5.5181\n",
      "Epoch [2/5], Step [72150/136675], Loss: 5.5187\n",
      "Epoch [2/5], Step [72225/136675], Loss: 5.4320\n",
      "Epoch [2/5], Step [72300/136675], Loss: 5.4288\n",
      "Epoch [2/5], Step [72375/136675], Loss: 5.4897\n",
      "Epoch [2/5], Step [72450/136675], Loss: 5.1595\n",
      "Epoch [2/5], Step [72525/136675], Loss: 5.5156\n",
      "Epoch [2/5], Step [72600/136675], Loss: 5.1548\n",
      "Epoch [2/5], Step [72675/136675], Loss: 5.4274\n",
      "Epoch [2/5], Step [72750/136675], Loss: 5.4045\n",
      "Epoch [2/5], Step [72825/136675], Loss: 5.5617\n",
      "Epoch [2/5], Step [72900/136675], Loss: 5.5523\n",
      "Epoch [2/5], Step [72975/136675], Loss: 5.5220\n",
      "Epoch [2/5], Step [73050/136675], Loss: 5.0120\n",
      "Epoch [2/5], Step [73125/136675], Loss: 5.4138\n",
      "Epoch [2/5], Step [73200/136675], Loss: 5.4550\n",
      "Epoch [2/5], Step [73275/136675], Loss: 5.2998\n",
      "Epoch [2/5], Step [73350/136675], Loss: 5.4020\n",
      "Epoch [2/5], Step [73425/136675], Loss: 5.1555\n",
      "Epoch [2/5], Step [73500/136675], Loss: 4.9806\n",
      "Epoch [2/5], Step [73575/136675], Loss: 5.4902\n",
      "Epoch [2/5], Step [73650/136675], Loss: 5.1155\n",
      "Epoch [2/5], Step [73725/136675], Loss: 5.2307\n",
      "Epoch [2/5], Step [73800/136675], Loss: 5.5684\n",
      "Epoch [2/5], Step [73875/136675], Loss: 5.2165\n",
      "Epoch [2/5], Step [73950/136675], Loss: 5.6774\n",
      "Epoch [2/5], Step [74025/136675], Loss: 5.3806\n",
      "Epoch [2/5], Step [74100/136675], Loss: 5.1794\n",
      "Epoch [2/5], Step [74175/136675], Loss: 5.2946\n",
      "Epoch [2/5], Step [74250/136675], Loss: 5.4278\n",
      "Epoch [2/5], Step [74325/136675], Loss: 5.2658\n",
      "Epoch [2/5], Step [74400/136675], Loss: 5.1648\n",
      "Epoch [2/5], Step [74475/136675], Loss: 5.0357\n",
      "Epoch [2/5], Step [74550/136675], Loss: 5.4700\n",
      "Epoch [2/5], Step [74625/136675], Loss: 5.3244\n",
      "Epoch [2/5], Step [74700/136675], Loss: 5.4957\n",
      "Epoch [2/5], Step [74775/136675], Loss: 5.2558\n",
      "Epoch [2/5], Step [74850/136675], Loss: 5.4026\n",
      "Epoch [2/5], Step [74925/136675], Loss: 5.3089\n",
      "Epoch [2/5], Step [75000/136675], Loss: 5.5942\n",
      "Epoch [2/5], Step [75075/136675], Loss: 5.4443\n",
      "Epoch [2/5], Step [75150/136675], Loss: 5.4759\n",
      "Epoch [2/5], Step [75225/136675], Loss: 5.5902\n",
      "Epoch [2/5], Step [75300/136675], Loss: 5.1980\n",
      "Epoch [2/5], Step [75375/136675], Loss: 5.4833\n",
      "Epoch [2/5], Step [75450/136675], Loss: 5.2378\n",
      "Epoch [2/5], Step [75525/136675], Loss: 5.5435\n",
      "Epoch [2/5], Step [75600/136675], Loss: 4.7357\n",
      "Epoch [2/5], Step [75675/136675], Loss: 5.4425\n",
      "Epoch [2/5], Step [75750/136675], Loss: 5.5331\n",
      "Epoch [2/5], Step [75825/136675], Loss: 5.1941\n",
      "Epoch [2/5], Step [75900/136675], Loss: 4.9791\n",
      "Epoch [2/5], Step [75975/136675], Loss: 5.2374\n",
      "Epoch [2/5], Step [76050/136675], Loss: 5.6315\n",
      "Epoch [2/5], Step [76125/136675], Loss: 5.2300\n",
      "Epoch [2/5], Step [76200/136675], Loss: 5.6176\n",
      "Epoch [2/5], Step [76275/136675], Loss: 5.1153\n",
      "Epoch [2/5], Step [76350/136675], Loss: 5.6483\n",
      "Epoch [2/5], Step [76425/136675], Loss: 5.2330\n",
      "Epoch [2/5], Step [76500/136675], Loss: 5.3638\n",
      "Epoch [2/5], Step [76575/136675], Loss: 4.6747\n",
      "Epoch [2/5], Step [76650/136675], Loss: 5.5295\n",
      "Epoch [2/5], Step [76725/136675], Loss: 5.5563\n",
      "Epoch [2/5], Step [76800/136675], Loss: 5.5177\n",
      "Epoch [2/5], Step [76875/136675], Loss: 5.4886\n",
      "Epoch [2/5], Step [76950/136675], Loss: 5.5141\n",
      "Epoch [2/5], Step [77025/136675], Loss: 5.5484\n",
      "Epoch [2/5], Step [77100/136675], Loss: 5.1974\n",
      "Epoch [2/5], Step [77175/136675], Loss: 5.3790\n",
      "Epoch [2/5], Step [77250/136675], Loss: 5.6199\n",
      "Epoch [2/5], Step [77325/136675], Loss: 5.4207\n",
      "Epoch [2/5], Step [77400/136675], Loss: 5.1908\n",
      "Epoch [2/5], Step [77475/136675], Loss: 5.1833\n",
      "Epoch [2/5], Step [77550/136675], Loss: 5.3597\n",
      "Epoch [2/5], Step [77625/136675], Loss: 5.5327\n",
      "Epoch [2/5], Step [77700/136675], Loss: 5.3023\n",
      "Epoch [2/5], Step [77775/136675], Loss: 5.5030\n",
      "Epoch [2/5], Step [77850/136675], Loss: 5.3377\n",
      "Epoch [2/5], Step [77925/136675], Loss: 5.2638\n",
      "Epoch [2/5], Step [78000/136675], Loss: 5.2528\n",
      "Epoch [2/5], Step [78075/136675], Loss: 5.3631\n",
      "Epoch [2/5], Step [78150/136675], Loss: 5.2612\n",
      "Epoch [2/5], Step [78225/136675], Loss: 5.4746\n",
      "Epoch [2/5], Step [78300/136675], Loss: 5.2719\n",
      "Epoch [2/5], Step [78375/136675], Loss: 5.1184\n",
      "Epoch [2/5], Step [78450/136675], Loss: 5.2466\n",
      "Epoch [2/5], Step [78525/136675], Loss: 5.3607\n",
      "Epoch [2/5], Step [78600/136675], Loss: 5.0794\n",
      "Epoch [2/5], Step [78675/136675], Loss: 5.4052\n",
      "Epoch [2/5], Step [78750/136675], Loss: 5.0934\n",
      "Epoch [2/5], Step [78825/136675], Loss: 5.1595\n",
      "Epoch [2/5], Step [78900/136675], Loss: 5.1891\n",
      "Epoch [2/5], Step [78975/136675], Loss: 5.3203\n",
      "Epoch [2/5], Step [79050/136675], Loss: 5.3664\n",
      "Epoch [2/5], Step [79125/136675], Loss: 5.1621\n",
      "Epoch [2/5], Step [79200/136675], Loss: 5.4601\n",
      "Epoch [2/5], Step [79275/136675], Loss: 5.3807\n",
      "Epoch [2/5], Step [79350/136675], Loss: 5.1595\n",
      "Epoch [2/5], Step [79425/136675], Loss: 5.3443\n",
      "Epoch [2/5], Step [79500/136675], Loss: 5.4412\n",
      "Epoch [2/5], Step [79575/136675], Loss: 5.4854\n",
      "Epoch [2/5], Step [79650/136675], Loss: 5.2252\n",
      "Epoch [2/5], Step [79725/136675], Loss: 5.3699\n",
      "Epoch [2/5], Step [79800/136675], Loss: 5.4072\n",
      "Epoch [2/5], Step [79875/136675], Loss: 5.2390\n",
      "Epoch [2/5], Step [79950/136675], Loss: 5.3029\n",
      "Validation perplexity: 168.84606657433145\n",
      "Epoch [2/5], Step [80025/136675], Loss: 5.2627\n",
      "Epoch [2/5], Step [80100/136675], Loss: 5.6261\n",
      "Epoch [2/5], Step [80175/136675], Loss: 5.2625\n",
      "Epoch [2/5], Step [80250/136675], Loss: 4.7156\n",
      "Epoch [2/5], Step [80325/136675], Loss: 5.1582\n",
      "Epoch [2/5], Step [80400/136675], Loss: 5.4377\n",
      "Epoch [2/5], Step [80475/136675], Loss: 5.5901\n",
      "Epoch [2/5], Step [80550/136675], Loss: 5.4323\n",
      "Epoch [2/5], Step [80625/136675], Loss: 5.1821\n",
      "Epoch [2/5], Step [80700/136675], Loss: 5.4147\n",
      "Epoch [2/5], Step [80775/136675], Loss: 5.5454\n",
      "Epoch [2/5], Step [80850/136675], Loss: 5.3838\n",
      "Epoch [2/5], Step [80925/136675], Loss: 5.3896\n",
      "Epoch [2/5], Step [81000/136675], Loss: 5.4566\n",
      "Epoch [2/5], Step [81075/136675], Loss: 5.1913\n",
      "Epoch [2/5], Step [81150/136675], Loss: 5.5794\n",
      "Epoch [2/5], Step [81225/136675], Loss: 5.3710\n",
      "Epoch [2/5], Step [81300/136675], Loss: 5.5509\n",
      "Epoch [2/5], Step [81375/136675], Loss: 5.5095\n",
      "Epoch [2/5], Step [81450/136675], Loss: 5.2662\n",
      "Epoch [2/5], Step [81525/136675], Loss: 5.0115\n",
      "Epoch [2/5], Step [81600/136675], Loss: 5.5349\n",
      "Epoch [2/5], Step [81675/136675], Loss: 5.6531\n",
      "Epoch [2/5], Step [81750/136675], Loss: 4.9133\n",
      "Epoch [2/5], Step [81825/136675], Loss: 5.2854\n",
      "Epoch [2/5], Step [81900/136675], Loss: 5.3094\n",
      "Epoch [2/5], Step [81975/136675], Loss: 5.3917\n",
      "Epoch [2/5], Step [82050/136675], Loss: 5.3383\n",
      "Epoch [2/5], Step [82125/136675], Loss: 5.3405\n",
      "Epoch [2/5], Step [82200/136675], Loss: 5.2734\n",
      "Epoch [2/5], Step [82275/136675], Loss: 5.0422\n",
      "Epoch [2/5], Step [82350/136675], Loss: 5.4234\n",
      "Epoch [2/5], Step [82425/136675], Loss: 5.6800\n",
      "Epoch [2/5], Step [82500/136675], Loss: 5.5476\n",
      "Epoch [2/5], Step [82575/136675], Loss: 5.4113\n",
      "Epoch [2/5], Step [82650/136675], Loss: 5.2546\n",
      "Epoch [2/5], Step [82725/136675], Loss: 5.8223\n",
      "Epoch [2/5], Step [82800/136675], Loss: 5.4410\n",
      "Epoch [2/5], Step [82875/136675], Loss: 5.4295\n",
      "Epoch [2/5], Step [82950/136675], Loss: 5.3872\n",
      "Epoch [2/5], Step [83025/136675], Loss: 5.2894\n",
      "Epoch [2/5], Step [83100/136675], Loss: 5.1049\n",
      "Epoch [2/5], Step [83175/136675], Loss: 5.6946\n",
      "Epoch [2/5], Step [83250/136675], Loss: 5.4201\n",
      "Epoch [2/5], Step [83325/136675], Loss: 5.0808\n",
      "Epoch [2/5], Step [83400/136675], Loss: 5.4011\n",
      "Epoch [2/5], Step [83475/136675], Loss: 5.3666\n",
      "Epoch [2/5], Step [83550/136675], Loss: 5.4340\n",
      "Epoch [2/5], Step [83625/136675], Loss: 4.9857\n",
      "Epoch [2/5], Step [83700/136675], Loss: 5.5779\n",
      "Epoch [2/5], Step [83775/136675], Loss: 5.4893\n",
      "Epoch [2/5], Step [83850/136675], Loss: 5.2017\n",
      "Epoch [2/5], Step [83925/136675], Loss: 5.5661\n",
      "Epoch [2/5], Step [84000/136675], Loss: 5.2746\n",
      "Epoch [2/5], Step [84075/136675], Loss: 5.6059\n",
      "Epoch [2/5], Step [84150/136675], Loss: 5.2386\n",
      "Epoch [2/5], Step [84225/136675], Loss: 5.3117\n",
      "Epoch [2/5], Step [84300/136675], Loss: 5.1551\n",
      "Epoch [2/5], Step [84375/136675], Loss: 5.2221\n",
      "Epoch [2/5], Step [84450/136675], Loss: 5.6131\n",
      "Epoch [2/5], Step [84525/136675], Loss: 5.3662\n",
      "Epoch [2/5], Step [84600/136675], Loss: 5.6946\n",
      "Epoch [2/5], Step [84675/136675], Loss: 5.3307\n",
      "Epoch [2/5], Step [84750/136675], Loss: 5.2670\n",
      "Epoch [2/5], Step [84825/136675], Loss: 5.2226\n",
      "Epoch [2/5], Step [84900/136675], Loss: 5.5714\n",
      "Epoch [2/5], Step [84975/136675], Loss: 5.4489\n",
      "Epoch [2/5], Step [85050/136675], Loss: 5.2242\n",
      "Epoch [2/5], Step [85125/136675], Loss: 5.4187\n",
      "Epoch [2/5], Step [85200/136675], Loss: 5.4416\n",
      "Epoch [2/5], Step [85275/136675], Loss: 5.7632\n",
      "Epoch [2/5], Step [85350/136675], Loss: 5.5380\n",
      "Epoch [2/5], Step [85425/136675], Loss: 5.1503\n",
      "Epoch [2/5], Step [85500/136675], Loss: 5.1378\n",
      "Epoch [2/5], Step [85575/136675], Loss: 5.3971\n",
      "Epoch [2/5], Step [85650/136675], Loss: 5.3477\n",
      "Epoch [2/5], Step [85725/136675], Loss: 5.0992\n",
      "Epoch [2/5], Step [85800/136675], Loss: 5.4023\n",
      "Epoch [2/5], Step [85875/136675], Loss: 5.6299\n",
      "Epoch [2/5], Step [85950/136675], Loss: 5.2659\n",
      "Epoch [2/5], Step [86025/136675], Loss: 5.3369\n",
      "Epoch [2/5], Step [86100/136675], Loss: 5.5754\n",
      "Epoch [2/5], Step [86175/136675], Loss: 5.3335\n",
      "Epoch [2/5], Step [86250/136675], Loss: 5.0857\n",
      "Epoch [2/5], Step [86325/136675], Loss: 5.4427\n",
      "Epoch [2/5], Step [86400/136675], Loss: 5.4828\n",
      "Epoch [2/5], Step [86475/136675], Loss: 5.2820\n",
      "Epoch [2/5], Step [86550/136675], Loss: 4.9704\n",
      "Epoch [2/5], Step [86625/136675], Loss: 5.4158\n",
      "Epoch [2/5], Step [86700/136675], Loss: 5.4036\n",
      "Epoch [2/5], Step [86775/136675], Loss: 5.3705\n",
      "Epoch [2/5], Step [86850/136675], Loss: 5.2447\n",
      "Epoch [2/5], Step [86925/136675], Loss: 5.5267\n",
      "Epoch [2/5], Step [87000/136675], Loss: 5.3601\n",
      "Epoch [2/5], Step [87075/136675], Loss: 5.5476\n",
      "Epoch [2/5], Step [87150/136675], Loss: 5.3675\n",
      "Epoch [2/5], Step [87225/136675], Loss: 5.1733\n",
      "Epoch [2/5], Step [87300/136675], Loss: 5.3884\n",
      "Epoch [2/5], Step [87375/136675], Loss: 5.7061\n",
      "Epoch [2/5], Step [87450/136675], Loss: 5.4239\n",
      "Epoch [2/5], Step [87525/136675], Loss: 5.1459\n",
      "Epoch [2/5], Step [87600/136675], Loss: 5.1761\n",
      "Epoch [2/5], Step [87675/136675], Loss: 5.1176\n",
      "Epoch [2/5], Step [87750/136675], Loss: 5.6053\n",
      "Epoch [2/5], Step [87825/136675], Loss: 5.4883\n",
      "Epoch [2/5], Step [87900/136675], Loss: 5.7661\n",
      "Epoch [2/5], Step [87975/136675], Loss: 5.3644\n",
      "Epoch [2/5], Step [88050/136675], Loss: 5.4540\n",
      "Epoch [2/5], Step [88125/136675], Loss: 5.3173\n",
      "Epoch [2/5], Step [88200/136675], Loss: 5.2148\n",
      "Epoch [2/5], Step [88275/136675], Loss: 5.5687\n",
      "Epoch [2/5], Step [88350/136675], Loss: 5.3681\n",
      "Epoch [2/5], Step [88425/136675], Loss: 5.2022\n",
      "Epoch [2/5], Step [88500/136675], Loss: 5.0205\n",
      "Epoch [2/5], Step [88575/136675], Loss: 5.2132\n",
      "Epoch [2/5], Step [88650/136675], Loss: 5.3853\n",
      "Epoch [2/5], Step [88725/136675], Loss: 5.0188\n",
      "Epoch [2/5], Step [88800/136675], Loss: 5.3773\n",
      "Epoch [2/5], Step [88875/136675], Loss: 5.3719\n",
      "Epoch [2/5], Step [88950/136675], Loss: 5.3190\n",
      "Epoch [2/5], Step [89025/136675], Loss: 5.2293\n",
      "Epoch [2/5], Step [89100/136675], Loss: 5.4301\n",
      "Epoch [2/5], Step [89175/136675], Loss: 5.3299\n",
      "Epoch [2/5], Step [89250/136675], Loss: 5.1889\n",
      "Epoch [2/5], Step [89325/136675], Loss: 5.0276\n",
      "Epoch [2/5], Step [89400/136675], Loss: 5.4772\n",
      "Epoch [2/5], Step [89475/136675], Loss: 5.2080\n",
      "Epoch [2/5], Step [89550/136675], Loss: 5.4330\n",
      "Epoch [2/5], Step [89625/136675], Loss: 5.3006\n",
      "Epoch [2/5], Step [89700/136675], Loss: 5.2946\n",
      "Epoch [2/5], Step [89775/136675], Loss: 4.9617\n",
      "Epoch [2/5], Step [89850/136675], Loss: 5.2270\n",
      "Epoch [2/5], Step [89925/136675], Loss: 5.1433\n",
      "Epoch [2/5], Step [90000/136675], Loss: 5.1182\n",
      "Validation perplexity: 167.4644337550494\n",
      "Epoch [2/5], Step [90075/136675], Loss: 5.3632\n",
      "Epoch [2/5], Step [90150/136675], Loss: 5.4559\n",
      "Epoch [2/5], Step [90225/136675], Loss: 5.4958\n",
      "Epoch [2/5], Step [90300/136675], Loss: 5.5550\n",
      "Epoch [2/5], Step [90375/136675], Loss: 5.4933\n",
      "Epoch [2/5], Step [90450/136675], Loss: 5.1741\n",
      "Epoch [2/5], Step [90525/136675], Loss: 5.3224\n",
      "Epoch [2/5], Step [90600/136675], Loss: 5.2718\n",
      "Epoch [2/5], Step [90675/136675], Loss: 5.0838\n",
      "Epoch [2/5], Step [90750/136675], Loss: 5.4386\n",
      "Epoch [2/5], Step [90825/136675], Loss: 5.3167\n",
      "Epoch [2/5], Step [90900/136675], Loss: 5.4972\n",
      "Epoch [2/5], Step [90975/136675], Loss: 5.3555\n",
      "Epoch [2/5], Step [91050/136675], Loss: 5.5776\n",
      "Epoch [2/5], Step [91125/136675], Loss: 5.5125\n",
      "Epoch [2/5], Step [91200/136675], Loss: 5.4509\n",
      "Epoch [2/5], Step [91275/136675], Loss: 5.3453\n",
      "Epoch [2/5], Step [91350/136675], Loss: 5.6566\n",
      "Epoch [2/5], Step [91425/136675], Loss: 5.6335\n",
      "Epoch [2/5], Step [91500/136675], Loss: 5.2677\n",
      "Epoch [2/5], Step [91575/136675], Loss: 5.4003\n",
      "Epoch [2/5], Step [91650/136675], Loss: 5.4386\n",
      "Epoch [2/5], Step [91725/136675], Loss: 5.2929\n",
      "Epoch [2/5], Step [91800/136675], Loss: 5.5013\n",
      "Epoch [2/5], Step [91875/136675], Loss: 5.1837\n",
      "Epoch [2/5], Step [91950/136675], Loss: 5.2684\n",
      "Epoch [2/5], Step [92025/136675], Loss: 5.3103\n",
      "Epoch [2/5], Step [92100/136675], Loss: 5.4194\n",
      "Epoch [2/5], Step [92175/136675], Loss: 5.1679\n",
      "Epoch [2/5], Step [92250/136675], Loss: 5.0563\n",
      "Epoch [2/5], Step [92325/136675], Loss: 5.1889\n",
      "Epoch [2/5], Step [92400/136675], Loss: 5.4788\n",
      "Epoch [2/5], Step [92475/136675], Loss: 5.1416\n",
      "Epoch [2/5], Step [92550/136675], Loss: 5.3383\n",
      "Epoch [2/5], Step [92625/136675], Loss: 5.1410\n",
      "Epoch [2/5], Step [92700/136675], Loss: 5.3814\n",
      "Epoch [2/5], Step [92775/136675], Loss: 5.1449\n",
      "Epoch [2/5], Step [92850/136675], Loss: 5.1239\n",
      "Epoch [2/5], Step [92925/136675], Loss: 5.3133\n",
      "Epoch [2/5], Step [93000/136675], Loss: 5.3561\n",
      "Epoch [2/5], Step [93075/136675], Loss: 5.4386\n",
      "Epoch [2/5], Step [93150/136675], Loss: 5.5566\n",
      "Epoch [2/5], Step [93225/136675], Loss: 5.5043\n",
      "Epoch [2/5], Step [93300/136675], Loss: 5.2283\n",
      "Epoch [2/5], Step [93375/136675], Loss: 5.0581\n",
      "Epoch [2/5], Step [93450/136675], Loss: 5.3697\n",
      "Epoch [2/5], Step [93525/136675], Loss: 5.3376\n",
      "Epoch [2/5], Step [93600/136675], Loss: 5.3587\n",
      "Epoch [2/5], Step [93675/136675], Loss: 4.9905\n",
      "Epoch [2/5], Step [93750/136675], Loss: 5.0859\n",
      "Epoch [2/5], Step [93825/136675], Loss: 5.5163\n",
      "Epoch [2/5], Step [93900/136675], Loss: 5.2151\n",
      "Epoch [2/5], Step [93975/136675], Loss: 4.9380\n",
      "Epoch [2/5], Step [94050/136675], Loss: 5.0719\n",
      "Epoch [2/5], Step [94125/136675], Loss: 5.4129\n",
      "Epoch [2/5], Step [94200/136675], Loss: 5.4246\n",
      "Epoch [2/5], Step [94275/136675], Loss: 5.4205\n",
      "Epoch [2/5], Step [94350/136675], Loss: 5.3058\n",
      "Epoch [2/5], Step [94425/136675], Loss: 4.8599\n",
      "Epoch [2/5], Step [94500/136675], Loss: 5.3015\n",
      "Epoch [2/5], Step [94575/136675], Loss: 5.0958\n",
      "Epoch [2/5], Step [94650/136675], Loss: 5.3087\n",
      "Epoch [2/5], Step [94725/136675], Loss: 5.4259\n",
      "Epoch [2/5], Step [94800/136675], Loss: 5.4428\n",
      "Epoch [2/5], Step [94875/136675], Loss: 5.5028\n",
      "Epoch [2/5], Step [94950/136675], Loss: 5.2617\n",
      "Epoch [2/5], Step [95025/136675], Loss: 5.3342\n",
      "Epoch [2/5], Step [95100/136675], Loss: 5.0299\n",
      "Epoch [2/5], Step [95175/136675], Loss: 5.4271\n",
      "Epoch [2/5], Step [95250/136675], Loss: 5.1974\n",
      "Epoch [2/5], Step [95325/136675], Loss: 5.0129\n",
      "Epoch [2/5], Step [95400/136675], Loss: 5.0101\n",
      "Epoch [2/5], Step [95475/136675], Loss: 5.1341\n",
      "Epoch [2/5], Step [95550/136675], Loss: 5.4308\n",
      "Epoch [2/5], Step [95625/136675], Loss: 5.2962\n",
      "Epoch [2/5], Step [95700/136675], Loss: 5.3287\n",
      "Epoch [2/5], Step [95775/136675], Loss: 5.2409\n",
      "Epoch [2/5], Step [95850/136675], Loss: 5.5329\n",
      "Epoch [2/5], Step [95925/136675], Loss: 5.4807\n",
      "Epoch [2/5], Step [96000/136675], Loss: 5.6414\n",
      "Epoch [2/5], Step [96075/136675], Loss: 5.3096\n",
      "Epoch [2/5], Step [96150/136675], Loss: 5.3792\n",
      "Epoch [2/5], Step [96225/136675], Loss: 5.7024\n",
      "Epoch [2/5], Step [96300/136675], Loss: 5.5968\n",
      "Epoch [2/5], Step [96375/136675], Loss: 5.4018\n",
      "Epoch [2/5], Step [96450/136675], Loss: 5.4117\n",
      "Epoch [2/5], Step [96525/136675], Loss: 5.2127\n",
      "Epoch [2/5], Step [96600/136675], Loss: 5.1741\n",
      "Epoch [2/5], Step [96675/136675], Loss: 5.6555\n",
      "Epoch [2/5], Step [96750/136675], Loss: 5.3962\n",
      "Epoch [2/5], Step [96825/136675], Loss: 5.3398\n",
      "Epoch [2/5], Step [96900/136675], Loss: 5.4590\n",
      "Epoch [2/5], Step [96975/136675], Loss: 5.1848\n",
      "Epoch [2/5], Step [97050/136675], Loss: 5.2385\n",
      "Epoch [2/5], Step [97125/136675], Loss: 5.1062\n",
      "Epoch [2/5], Step [97200/136675], Loss: 5.1550\n",
      "Epoch [2/5], Step [97275/136675], Loss: 5.2774\n",
      "Epoch [2/5], Step [97350/136675], Loss: 5.2381\n",
      "Epoch [2/5], Step [97425/136675], Loss: 5.1635\n",
      "Epoch [2/5], Step [97500/136675], Loss: 4.8522\n",
      "Epoch [2/5], Step [97575/136675], Loss: 5.3044\n",
      "Epoch [2/5], Step [97650/136675], Loss: 5.4121\n",
      "Epoch [2/5], Step [97725/136675], Loss: 5.5052\n",
      "Epoch [2/5], Step [97800/136675], Loss: 5.6753\n",
      "Epoch [2/5], Step [97875/136675], Loss: 5.0931\n",
      "Epoch [2/5], Step [97950/136675], Loss: 5.4301\n",
      "Epoch [2/5], Step [98025/136675], Loss: 5.3436\n",
      "Epoch [2/5], Step [98100/136675], Loss: 5.6449\n",
      "Epoch [2/5], Step [98175/136675], Loss: 5.4934\n",
      "Epoch [2/5], Step [98250/136675], Loss: 5.1212\n",
      "Epoch [2/5], Step [98325/136675], Loss: 5.5568\n",
      "Epoch [2/5], Step [98400/136675], Loss: 5.0770\n",
      "Epoch [2/5], Step [98475/136675], Loss: 5.3470\n",
      "Epoch [2/5], Step [98550/136675], Loss: 5.0483\n",
      "Epoch [2/5], Step [98625/136675], Loss: 5.1842\n",
      "Epoch [2/5], Step [98700/136675], Loss: 5.5215\n",
      "Epoch [2/5], Step [98775/136675], Loss: 5.6313\n",
      "Epoch [2/5], Step [98850/136675], Loss: 5.3415\n",
      "Epoch [2/5], Step [98925/136675], Loss: 5.4938\n",
      "Epoch [2/5], Step [99000/136675], Loss: 5.0526\n",
      "Epoch [2/5], Step [99075/136675], Loss: 5.3766\n",
      "Epoch [2/5], Step [99150/136675], Loss: 5.2704\n",
      "Epoch [2/5], Step [99225/136675], Loss: 5.5709\n",
      "Epoch [2/5], Step [99300/136675], Loss: 5.5550\n",
      "Epoch [2/5], Step [99375/136675], Loss: 5.3201\n",
      "Epoch [2/5], Step [99450/136675], Loss: 5.3252\n",
      "Epoch [2/5], Step [99525/136675], Loss: 5.0636\n",
      "Epoch [2/5], Step [99600/136675], Loss: 5.4052\n",
      "Epoch [2/5], Step [99675/136675], Loss: 5.1828\n",
      "Epoch [2/5], Step [99750/136675], Loss: 5.5464\n",
      "Epoch [2/5], Step [99825/136675], Loss: 5.3781\n",
      "Epoch [2/5], Step [99900/136675], Loss: 5.3724\n",
      "Epoch [2/5], Step [99975/136675], Loss: 5.4304\n",
      "Validation perplexity: 166.7556675961334\n",
      "Epoch [2/5], Step [100050/136675], Loss: 5.4342\n",
      "Epoch [2/5], Step [100125/136675], Loss: 5.5360\n",
      "Epoch [2/5], Step [100200/136675], Loss: 5.3057\n",
      "Epoch [2/5], Step [100275/136675], Loss: 5.5501\n",
      "Epoch [2/5], Step [100350/136675], Loss: 5.3579\n",
      "Epoch [2/5], Step [100425/136675], Loss: 5.0813\n",
      "Epoch [2/5], Step [100500/136675], Loss: 5.4611\n",
      "Epoch [2/5], Step [100575/136675], Loss: 5.4939\n",
      "Epoch [2/5], Step [100650/136675], Loss: 5.0471\n",
      "Epoch [2/5], Step [100725/136675], Loss: 5.1582\n",
      "Epoch [2/5], Step [100800/136675], Loss: 5.1489\n",
      "Epoch [2/5], Step [100875/136675], Loss: 5.4293\n",
      "Epoch [2/5], Step [100950/136675], Loss: 5.3716\n",
      "Epoch [2/5], Step [101025/136675], Loss: 5.4892\n",
      "Epoch [2/5], Step [101100/136675], Loss: 5.2255\n",
      "Epoch [2/5], Step [101175/136675], Loss: 5.1451\n",
      "Epoch [2/5], Step [101250/136675], Loss: 5.1388\n",
      "Epoch [2/5], Step [101325/136675], Loss: 5.2631\n",
      "Epoch [2/5], Step [101400/136675], Loss: 5.3480\n",
      "Epoch [2/5], Step [101475/136675], Loss: 5.5306\n",
      "Epoch [2/5], Step [101550/136675], Loss: 5.3315\n",
      "Epoch [2/5], Step [101625/136675], Loss: 5.3208\n",
      "Epoch [2/5], Step [101700/136675], Loss: 5.2419\n",
      "Epoch [2/5], Step [101775/136675], Loss: 5.2851\n",
      "Epoch [2/5], Step [101850/136675], Loss: 5.2281\n",
      "Epoch [2/5], Step [101925/136675], Loss: 5.2815\n",
      "Epoch [2/5], Step [102000/136675], Loss: 5.4105\n",
      "Epoch [2/5], Step [102075/136675], Loss: 5.4407\n",
      "Epoch [2/5], Step [102150/136675], Loss: 5.2895\n",
      "Epoch [2/5], Step [102225/136675], Loss: 5.4393\n",
      "Epoch [2/5], Step [102300/136675], Loss: 5.4584\n",
      "Epoch [2/5], Step [102375/136675], Loss: 5.2532\n",
      "Epoch [2/5], Step [102450/136675], Loss: 5.4937\n",
      "Epoch [2/5], Step [102525/136675], Loss: 5.2240\n",
      "Epoch [2/5], Step [102600/136675], Loss: 5.6553\n",
      "Epoch [2/5], Step [102675/136675], Loss: 5.2632\n",
      "Epoch [2/5], Step [102750/136675], Loss: 5.4758\n",
      "Epoch [2/5], Step [102825/136675], Loss: 5.2736\n",
      "Epoch [2/5], Step [102900/136675], Loss: 5.4063\n",
      "Epoch [2/5], Step [102975/136675], Loss: 5.1381\n",
      "Epoch [2/5], Step [103050/136675], Loss: 5.3656\n",
      "Epoch [2/5], Step [103125/136675], Loss: 5.1692\n",
      "Epoch [2/5], Step [103200/136675], Loss: 5.5822\n",
      "Epoch [2/5], Step [103275/136675], Loss: 4.9355\n",
      "Epoch [2/5], Step [103350/136675], Loss: 5.2036\n",
      "Epoch [2/5], Step [103425/136675], Loss: 5.5409\n",
      "Epoch [2/5], Step [103500/136675], Loss: 5.3783\n",
      "Epoch [2/5], Step [103575/136675], Loss: 5.3639\n",
      "Epoch [2/5], Step [103650/136675], Loss: 5.6216\n",
      "Epoch [2/5], Step [103725/136675], Loss: 5.5065\n",
      "Epoch [2/5], Step [103800/136675], Loss: 5.4207\n",
      "Epoch [2/5], Step [103875/136675], Loss: 5.2543\n",
      "Epoch [2/5], Step [103950/136675], Loss: 5.3191\n",
      "Epoch [2/5], Step [104025/136675], Loss: 5.2804\n",
      "Epoch [2/5], Step [104100/136675], Loss: 5.4345\n",
      "Epoch [2/5], Step [104175/136675], Loss: 5.3142\n",
      "Epoch [2/5], Step [104250/136675], Loss: 5.3028\n",
      "Epoch [2/5], Step [104325/136675], Loss: 5.2976\n",
      "Epoch [2/5], Step [104400/136675], Loss: 5.0760\n",
      "Epoch [2/5], Step [104475/136675], Loss: 5.4359\n",
      "Epoch [2/5], Step [104550/136675], Loss: 5.6881\n",
      "Epoch [2/5], Step [104625/136675], Loss: 5.3028\n",
      "Epoch [2/5], Step [104700/136675], Loss: 5.2923\n",
      "Epoch [2/5], Step [104775/136675], Loss: 5.4087\n",
      "Epoch [2/5], Step [104850/136675], Loss: 5.3521\n",
      "Epoch [2/5], Step [104925/136675], Loss: 5.1624\n",
      "Epoch [2/5], Step [105000/136675], Loss: 5.2231\n",
      "Epoch [2/5], Step [105075/136675], Loss: 5.4894\n",
      "Epoch [2/5], Step [105150/136675], Loss: 5.3991\n",
      "Epoch [2/5], Step [105225/136675], Loss: 5.4774\n",
      "Epoch [2/5], Step [105300/136675], Loss: 5.2919\n",
      "Epoch [2/5], Step [105375/136675], Loss: 5.0671\n",
      "Epoch [2/5], Step [105450/136675], Loss: 5.3981\n",
      "Epoch [2/5], Step [105525/136675], Loss: 5.2577\n",
      "Epoch [2/5], Step [105600/136675], Loss: 5.4336\n",
      "Epoch [2/5], Step [105675/136675], Loss: 5.3846\n",
      "Epoch [2/5], Step [105750/136675], Loss: 5.4856\n",
      "Epoch [2/5], Step [105825/136675], Loss: 5.3902\n",
      "Epoch [2/5], Step [105900/136675], Loss: 5.1492\n",
      "Epoch [2/5], Step [105975/136675], Loss: 5.4780\n",
      "Epoch [2/5], Step [106050/136675], Loss: 5.5260\n",
      "Epoch [2/5], Step [106125/136675], Loss: 5.6121\n",
      "Epoch [2/5], Step [106200/136675], Loss: 5.4321\n",
      "Epoch [2/5], Step [106275/136675], Loss: 5.4433\n",
      "Epoch [2/5], Step [106350/136675], Loss: 5.2716\n",
      "Epoch [2/5], Step [106425/136675], Loss: 5.1842\n",
      "Epoch [2/5], Step [106500/136675], Loss: 5.3178\n",
      "Epoch [2/5], Step [106575/136675], Loss: 5.1822\n",
      "Epoch [2/5], Step [106650/136675], Loss: 5.3781\n",
      "Epoch [2/5], Step [106725/136675], Loss: 5.5294\n",
      "Epoch [2/5], Step [106800/136675], Loss: 5.2456\n",
      "Epoch [2/5], Step [106875/136675], Loss: 5.3449\n",
      "Epoch [2/5], Step [106950/136675], Loss: 5.4482\n",
      "Epoch [2/5], Step [107025/136675], Loss: 5.2267\n",
      "Epoch [2/5], Step [107100/136675], Loss: 5.2589\n",
      "Epoch [2/5], Step [107175/136675], Loss: 5.2708\n",
      "Epoch [2/5], Step [107250/136675], Loss: 5.3914\n",
      "Epoch [2/5], Step [107325/136675], Loss: 5.6013\n",
      "Epoch [2/5], Step [107400/136675], Loss: 5.2490\n",
      "Epoch [2/5], Step [107475/136675], Loss: 5.5928\n",
      "Epoch [2/5], Step [107550/136675], Loss: 5.3856\n",
      "Epoch [2/5], Step [107625/136675], Loss: 5.4013\n",
      "Epoch [2/5], Step [107700/136675], Loss: 5.1889\n",
      "Epoch [2/5], Step [107775/136675], Loss: 5.4063\n",
      "Epoch [2/5], Step [107850/136675], Loss: 5.5472\n",
      "Epoch [2/5], Step [107925/136675], Loss: 5.4022\n",
      "Epoch [2/5], Step [108000/136675], Loss: 5.4214\n",
      "Epoch [2/5], Step [108075/136675], Loss: 5.3112\n",
      "Epoch [2/5], Step [108150/136675], Loss: 5.3973\n",
      "Epoch [2/5], Step [108225/136675], Loss: 5.2313\n",
      "Epoch [2/5], Step [108300/136675], Loss: 5.2222\n",
      "Epoch [2/5], Step [108375/136675], Loss: 5.2490\n",
      "Epoch [2/5], Step [108450/136675], Loss: 5.1063\n",
      "Epoch [2/5], Step [108525/136675], Loss: 5.1964\n",
      "Epoch [2/5], Step [108600/136675], Loss: 5.6190\n",
      "Epoch [2/5], Step [108675/136675], Loss: 5.5958\n",
      "Epoch [2/5], Step [108750/136675], Loss: 5.4566\n",
      "Epoch [2/5], Step [108825/136675], Loss: 5.6539\n",
      "Epoch [2/5], Step [108900/136675], Loss: 5.3756\n",
      "Epoch [2/5], Step [108975/136675], Loss: 5.3819\n",
      "Epoch [2/5], Step [109050/136675], Loss: 5.3012\n",
      "Epoch [2/5], Step [109125/136675], Loss: 5.4128\n",
      "Epoch [2/5], Step [109200/136675], Loss: 5.2599\n",
      "Epoch [2/5], Step [109275/136675], Loss: 5.4400\n",
      "Epoch [2/5], Step [109350/136675], Loss: 5.3953\n",
      "Epoch [2/5], Step [109425/136675], Loss: 5.2775\n",
      "Epoch [2/5], Step [109500/136675], Loss: 5.2891\n",
      "Epoch [2/5], Step [109575/136675], Loss: 5.6202\n",
      "Epoch [2/5], Step [109650/136675], Loss: 5.1044\n",
      "Epoch [2/5], Step [109725/136675], Loss: 5.1567\n",
      "Epoch [2/5], Step [109800/136675], Loss: 5.1855\n",
      "Epoch [2/5], Step [109875/136675], Loss: 5.0971\n",
      "Epoch [2/5], Step [109950/136675], Loss: 5.2274\n",
      "Validation perplexity: 165.89316371800723\n",
      "Epoch [2/5], Step [110025/136675], Loss: 5.0989\n",
      "Epoch [2/5], Step [110100/136675], Loss: 5.3638\n",
      "Epoch [2/5], Step [110175/136675], Loss: 5.4203\n",
      "Epoch [2/5], Step [110250/136675], Loss: 5.4781\n",
      "Epoch [2/5], Step [110325/136675], Loss: 5.1794\n",
      "Epoch [2/5], Step [110400/136675], Loss: 5.2463\n",
      "Epoch [2/5], Step [110475/136675], Loss: 5.2767\n",
      "Epoch [2/5], Step [110550/136675], Loss: 5.4032\n",
      "Epoch [2/5], Step [110625/136675], Loss: 5.2866\n",
      "Epoch [2/5], Step [110700/136675], Loss: 5.1979\n",
      "Epoch [2/5], Step [110775/136675], Loss: 5.5032\n",
      "Epoch [2/5], Step [110850/136675], Loss: 5.4970\n",
      "Epoch [2/5], Step [110925/136675], Loss: 5.6622\n",
      "Epoch [2/5], Step [111000/136675], Loss: 4.8763\n",
      "Epoch [2/5], Step [111075/136675], Loss: 5.4049\n",
      "Epoch [2/5], Step [111150/136675], Loss: 5.5142\n",
      "Epoch [2/5], Step [111225/136675], Loss: 4.9976\n",
      "Epoch [2/5], Step [111300/136675], Loss: 5.4005\n",
      "Epoch [2/5], Step [111375/136675], Loss: 5.1365\n",
      "Epoch [2/5], Step [111450/136675], Loss: 5.3010\n",
      "Epoch [2/5], Step [111525/136675], Loss: 5.1702\n",
      "Epoch [2/5], Step [111600/136675], Loss: 5.8947\n",
      "Epoch [2/5], Step [111675/136675], Loss: 5.3381\n",
      "Epoch [2/5], Step [111750/136675], Loss: 5.2853\n",
      "Epoch [2/5], Step [111825/136675], Loss: 5.0533\n",
      "Epoch [2/5], Step [111900/136675], Loss: 5.4095\n",
      "Epoch [2/5], Step [111975/136675], Loss: 5.5206\n",
      "Epoch [2/5], Step [112050/136675], Loss: 5.4256\n",
      "Epoch [2/5], Step [112125/136675], Loss: 5.3228\n",
      "Epoch [2/5], Step [112200/136675], Loss: 5.2664\n",
      "Epoch [2/5], Step [112275/136675], Loss: 5.2946\n",
      "Epoch [2/5], Step [112350/136675], Loss: 5.3953\n",
      "Epoch [2/5], Step [112425/136675], Loss: 5.2119\n",
      "Epoch [2/5], Step [112500/136675], Loss: 5.1001\n",
      "Epoch [2/5], Step [112575/136675], Loss: 5.1948\n",
      "Epoch [2/5], Step [112650/136675], Loss: 5.4933\n",
      "Epoch [2/5], Step [112725/136675], Loss: 5.4222\n",
      "Epoch [2/5], Step [112800/136675], Loss: 5.3981\n",
      "Epoch [2/5], Step [112875/136675], Loss: 5.2191\n",
      "Epoch [2/5], Step [112950/136675], Loss: 5.5569\n",
      "Epoch [2/5], Step [113025/136675], Loss: 5.4739\n",
      "Epoch [2/5], Step [113100/136675], Loss: 5.0545\n",
      "Epoch [2/5], Step [113175/136675], Loss: 5.4570\n",
      "Epoch [2/5], Step [113250/136675], Loss: 5.2396\n",
      "Epoch [2/5], Step [113325/136675], Loss: 5.3134\n",
      "Epoch [2/5], Step [113400/136675], Loss: 5.5350\n",
      "Epoch [2/5], Step [113475/136675], Loss: 5.4416\n",
      "Epoch [2/5], Step [113550/136675], Loss: 5.1424\n",
      "Epoch [2/5], Step [113625/136675], Loss: 5.3536\n",
      "Epoch [2/5], Step [113700/136675], Loss: 5.4453\n",
      "Epoch [2/5], Step [113775/136675], Loss: 5.3763\n",
      "Epoch [2/5], Step [113850/136675], Loss: 5.1509\n",
      "Epoch [2/5], Step [113925/136675], Loss: 5.3424\n",
      "Epoch [2/5], Step [114000/136675], Loss: 5.4502\n",
      "Epoch [2/5], Step [114075/136675], Loss: 5.4082\n",
      "Epoch [2/5], Step [114150/136675], Loss: 5.6954\n",
      "Epoch [2/5], Step [114225/136675], Loss: 5.0470\n",
      "Epoch [2/5], Step [114300/136675], Loss: 5.1098\n",
      "Epoch [2/5], Step [114375/136675], Loss: 5.1548\n",
      "Epoch [2/5], Step [114450/136675], Loss: 5.2464\n",
      "Epoch [2/5], Step [114525/136675], Loss: 5.2638\n",
      "Epoch [2/5], Step [114600/136675], Loss: 5.2220\n",
      "Epoch [2/5], Step [114675/136675], Loss: 5.2113\n",
      "Epoch [2/5], Step [114750/136675], Loss: 5.4528\n",
      "Epoch [2/5], Step [114825/136675], Loss: 5.2067\n",
      "Epoch [2/5], Step [114900/136675], Loss: 5.1716\n",
      "Epoch [2/5], Step [114975/136675], Loss: 5.4300\n",
      "Epoch [2/5], Step [115050/136675], Loss: 5.3787\n",
      "Epoch [2/5], Step [115125/136675], Loss: 5.3536\n",
      "Epoch [2/5], Step [115200/136675], Loss: 5.2148\n",
      "Epoch [2/5], Step [115275/136675], Loss: 5.3949\n",
      "Epoch [2/5], Step [115350/136675], Loss: 5.3324\n",
      "Epoch [2/5], Step [115425/136675], Loss: 5.2896\n",
      "Epoch [2/5], Step [115500/136675], Loss: 5.3938\n",
      "Epoch [2/5], Step [115575/136675], Loss: 5.1822\n",
      "Epoch [2/5], Step [115650/136675], Loss: 5.1166\n",
      "Epoch [2/5], Step [115725/136675], Loss: 5.1623\n",
      "Epoch [2/5], Step [115800/136675], Loss: 5.2695\n",
      "Epoch [2/5], Step [115875/136675], Loss: 5.3082\n",
      "Epoch [2/5], Step [115950/136675], Loss: 5.1460\n",
      "Epoch [2/5], Step [116025/136675], Loss: 5.3774\n",
      "Epoch [2/5], Step [116100/136675], Loss: 5.6200\n",
      "Epoch [2/5], Step [116175/136675], Loss: 5.4303\n",
      "Epoch [2/5], Step [116250/136675], Loss: 5.3702\n",
      "Epoch [2/5], Step [116325/136675], Loss: 5.2282\n",
      "Epoch [2/5], Step [116400/136675], Loss: 5.8102\n",
      "Epoch [2/5], Step [116475/136675], Loss: 5.1360\n",
      "Epoch [2/5], Step [116550/136675], Loss: 5.2591\n",
      "Epoch [2/5], Step [116625/136675], Loss: 5.4798\n",
      "Epoch [2/5], Step [116700/136675], Loss: 5.5976\n",
      "Epoch [2/5], Step [116775/136675], Loss: 5.5147\n",
      "Epoch [2/5], Step [116850/136675], Loss: 5.0320\n",
      "Epoch [2/5], Step [116925/136675], Loss: 5.4808\n",
      "Epoch [2/5], Step [117000/136675], Loss: 5.5086\n",
      "Epoch [2/5], Step [117075/136675], Loss: 5.5864\n",
      "Epoch [2/5], Step [117150/136675], Loss: 5.3397\n",
      "Epoch [2/5], Step [117225/136675], Loss: 5.5854\n",
      "Epoch [2/5], Step [117300/136675], Loss: 5.5049\n",
      "Epoch [2/5], Step [117375/136675], Loss: 5.2751\n",
      "Epoch [2/5], Step [117450/136675], Loss: 5.1816\n",
      "Epoch [2/5], Step [117525/136675], Loss: 5.1196\n",
      "Epoch [2/5], Step [117600/136675], Loss: 5.1589\n",
      "Epoch [2/5], Step [117675/136675], Loss: 5.3805\n",
      "Epoch [2/5], Step [117750/136675], Loss: 5.4254\n",
      "Epoch [2/5], Step [117825/136675], Loss: 5.2941\n",
      "Epoch [2/5], Step [117900/136675], Loss: 5.1950\n",
      "Epoch [2/5], Step [117975/136675], Loss: 5.4806\n",
      "Epoch [2/5], Step [118050/136675], Loss: 5.3098\n",
      "Epoch [2/5], Step [118125/136675], Loss: 5.1004\n",
      "Epoch [2/5], Step [118200/136675], Loss: 5.2648\n",
      "Epoch [2/5], Step [118275/136675], Loss: 5.3674\n",
      "Epoch [2/5], Step [118350/136675], Loss: 5.3364\n",
      "Epoch [2/5], Step [118425/136675], Loss: 5.2455\n",
      "Epoch [2/5], Step [118500/136675], Loss: 5.2061\n",
      "Epoch [2/5], Step [118575/136675], Loss: 4.8762\n",
      "Epoch [2/5], Step [118650/136675], Loss: 5.1386\n",
      "Epoch [2/5], Step [118725/136675], Loss: 5.5931\n",
      "Epoch [2/5], Step [118800/136675], Loss: 5.4015\n",
      "Epoch [2/5], Step [118875/136675], Loss: 5.3749\n",
      "Epoch [2/5], Step [118950/136675], Loss: 5.3566\n",
      "Epoch [2/5], Step [119025/136675], Loss: 5.1215\n",
      "Epoch [2/5], Step [119100/136675], Loss: 5.2685\n",
      "Epoch [2/5], Step [119175/136675], Loss: 5.4219\n",
      "Epoch [2/5], Step [119250/136675], Loss: 5.5223\n",
      "Epoch [2/5], Step [119325/136675], Loss: 5.2423\n",
      "Epoch [2/5], Step [119400/136675], Loss: 5.2290\n",
      "Epoch [2/5], Step [119475/136675], Loss: 5.3834\n",
      "Epoch [2/5], Step [119550/136675], Loss: 5.5626\n",
      "Epoch [2/5], Step [119625/136675], Loss: 5.1294\n",
      "Epoch [2/5], Step [119700/136675], Loss: 5.4451\n",
      "Epoch [2/5], Step [119775/136675], Loss: 5.1133\n",
      "Epoch [2/5], Step [119850/136675], Loss: 5.6094\n",
      "Epoch [2/5], Step [119925/136675], Loss: 5.0468\n",
      "Epoch [2/5], Step [120000/136675], Loss: 5.1760\n",
      "Validation perplexity: 166.45538344528225\n",
      "Epoch [2/5], Step [120075/136675], Loss: 5.3076\n",
      "Epoch [2/5], Step [120150/136675], Loss: 5.1704\n",
      "Epoch [2/5], Step [120225/136675], Loss: 5.4560\n",
      "Epoch [2/5], Step [120300/136675], Loss: 5.2596\n",
      "Epoch [2/5], Step [120375/136675], Loss: 5.0952\n",
      "Epoch [2/5], Step [120450/136675], Loss: 5.4135\n",
      "Epoch [2/5], Step [120525/136675], Loss: 5.3378\n",
      "Epoch [2/5], Step [120600/136675], Loss: 5.4396\n",
      "Epoch [2/5], Step [120675/136675], Loss: 5.1831\n",
      "Epoch [2/5], Step [120750/136675], Loss: 5.2593\n",
      "Epoch [2/5], Step [120825/136675], Loss: 4.8659\n",
      "Epoch [2/5], Step [120900/136675], Loss: 5.3491\n",
      "Epoch [2/5], Step [120975/136675], Loss: 5.2080\n",
      "Epoch [2/5], Step [121050/136675], Loss: 5.3020\n",
      "Epoch [2/5], Step [121125/136675], Loss: 5.2687\n",
      "Epoch [2/5], Step [121200/136675], Loss: 5.1729\n",
      "Epoch [2/5], Step [121275/136675], Loss: 5.1272\n",
      "Epoch [2/5], Step [121350/136675], Loss: 5.1564\n",
      "Epoch [2/5], Step [121425/136675], Loss: 5.3709\n",
      "Epoch [2/5], Step [121500/136675], Loss: 5.5995\n",
      "Epoch [2/5], Step [121575/136675], Loss: 5.1886\n",
      "Epoch [2/5], Step [121650/136675], Loss: 5.4563\n",
      "Epoch [2/5], Step [121725/136675], Loss: 5.2935\n",
      "Epoch [2/5], Step [121800/136675], Loss: 5.1413\n",
      "Epoch [2/5], Step [121875/136675], Loss: 5.3902\n",
      "Epoch [2/5], Step [121950/136675], Loss: 5.2512\n",
      "Epoch [2/5], Step [122025/136675], Loss: 5.4370\n",
      "Epoch [2/5], Step [122100/136675], Loss: 5.5880\n",
      "Epoch [2/5], Step [122175/136675], Loss: 5.4058\n",
      "Epoch [2/5], Step [122250/136675], Loss: 5.3569\n",
      "Epoch [2/5], Step [122325/136675], Loss: 5.1316\n",
      "Epoch [2/5], Step [122400/136675], Loss: 5.2452\n",
      "Epoch [2/5], Step [122475/136675], Loss: 5.2949\n",
      "Epoch [2/5], Step [122550/136675], Loss: 5.2956\n",
      "Epoch [2/5], Step [122625/136675], Loss: 5.2137\n",
      "Epoch [2/5], Step [122700/136675], Loss: 5.2095\n",
      "Epoch [2/5], Step [122775/136675], Loss: 5.1429\n",
      "Epoch [2/5], Step [122850/136675], Loss: 5.4805\n",
      "Epoch [2/5], Step [122925/136675], Loss: 5.0307\n",
      "Epoch [2/5], Step [123000/136675], Loss: 5.1995\n",
      "Epoch [2/5], Step [123075/136675], Loss: 5.2410\n",
      "Epoch [2/5], Step [123150/136675], Loss: 5.2156\n",
      "Epoch [2/5], Step [123225/136675], Loss: 5.1910\n",
      "Epoch [2/5], Step [123300/136675], Loss: 5.4615\n",
      "Epoch [2/5], Step [123375/136675], Loss: 5.3321\n",
      "Epoch [2/5], Step [123450/136675], Loss: 5.1539\n",
      "Epoch [2/5], Step [123525/136675], Loss: 5.5706\n",
      "Epoch [2/5], Step [123600/136675], Loss: 5.1242\n",
      "Epoch [2/5], Step [123675/136675], Loss: 5.3969\n",
      "Epoch [2/5], Step [123750/136675], Loss: 5.0508\n",
      "Epoch [2/5], Step [123825/136675], Loss: 5.4080\n",
      "Epoch [2/5], Step [123900/136675], Loss: 5.3024\n",
      "Epoch [2/5], Step [123975/136675], Loss: 5.1277\n",
      "Epoch [2/5], Step [124050/136675], Loss: 5.4137\n",
      "Epoch [2/5], Step [124125/136675], Loss: 5.4621\n",
      "Epoch [2/5], Step [124200/136675], Loss: 5.3160\n",
      "Epoch [2/5], Step [124275/136675], Loss: 5.2982\n",
      "Epoch [2/5], Step [124350/136675], Loss: 5.5664\n",
      "Epoch [2/5], Step [124425/136675], Loss: 5.1131\n",
      "Epoch [2/5], Step [124500/136675], Loss: 4.9670\n",
      "Epoch [2/5], Step [124575/136675], Loss: 4.9994\n",
      "Epoch [2/5], Step [124650/136675], Loss: 5.3924\n",
      "Epoch [2/5], Step [124725/136675], Loss: 5.4294\n",
      "Epoch [2/5], Step [124800/136675], Loss: 5.2564\n",
      "Epoch [2/5], Step [124875/136675], Loss: 5.2911\n",
      "Epoch [2/5], Step [124950/136675], Loss: 5.0417\n",
      "Epoch [2/5], Step [125025/136675], Loss: 5.2788\n",
      "Epoch [2/5], Step [125100/136675], Loss: 5.0957\n",
      "Epoch [2/5], Step [125175/136675], Loss: 5.1056\n",
      "Epoch [2/5], Step [125250/136675], Loss: 5.2860\n",
      "Epoch [2/5], Step [125325/136675], Loss: 5.6127\n",
      "Epoch [2/5], Step [125400/136675], Loss: 5.1357\n",
      "Epoch [2/5], Step [125475/136675], Loss: 5.3026\n",
      "Epoch [2/5], Step [125550/136675], Loss: 5.3788\n",
      "Epoch [2/5], Step [125625/136675], Loss: 5.2600\n",
      "Epoch [2/5], Step [125700/136675], Loss: 5.3643\n",
      "Epoch [2/5], Step [125775/136675], Loss: 5.1002\n",
      "Epoch [2/5], Step [125850/136675], Loss: 5.2476\n",
      "Epoch [2/5], Step [125925/136675], Loss: 5.5605\n",
      "Epoch [2/5], Step [126000/136675], Loss: 5.6508\n",
      "Epoch [2/5], Step [126075/136675], Loss: 5.3656\n",
      "Epoch [2/5], Step [126150/136675], Loss: 5.2612\n",
      "Epoch [2/5], Step [126225/136675], Loss: 5.5521\n",
      "Epoch [2/5], Step [126300/136675], Loss: 5.3025\n",
      "Epoch [2/5], Step [126375/136675], Loss: 4.6718\n",
      "Epoch [2/5], Step [126450/136675], Loss: 5.0953\n",
      "Epoch [2/5], Step [126525/136675], Loss: 5.4743\n",
      "Epoch [2/5], Step [126600/136675], Loss: 5.4894\n",
      "Epoch [2/5], Step [126675/136675], Loss: 5.1915\n",
      "Epoch [2/5], Step [126750/136675], Loss: 5.4550\n",
      "Epoch [2/5], Step [126825/136675], Loss: 5.5192\n",
      "Epoch [2/5], Step [126900/136675], Loss: 5.2346\n",
      "Epoch [2/5], Step [126975/136675], Loss: 5.5132\n",
      "Epoch [2/5], Step [127050/136675], Loss: 5.3656\n",
      "Epoch [2/5], Step [127125/136675], Loss: 5.4303\n",
      "Epoch [2/5], Step [127200/136675], Loss: 5.3042\n",
      "Epoch [2/5], Step [127275/136675], Loss: 5.3601\n",
      "Epoch [2/5], Step [127350/136675], Loss: 5.3112\n",
      "Epoch [2/5], Step [127425/136675], Loss: 5.2847\n",
      "Epoch [2/5], Step [127500/136675], Loss: 5.6632\n",
      "Epoch [2/5], Step [127575/136675], Loss: 5.3569\n",
      "Epoch [2/5], Step [127650/136675], Loss: 5.1797\n",
      "Epoch [2/5], Step [127725/136675], Loss: 5.2761\n",
      "Epoch [2/5], Step [127800/136675], Loss: 4.9837\n",
      "Epoch [2/5], Step [127875/136675], Loss: 4.9849\n",
      "Epoch [2/5], Step [127950/136675], Loss: 5.2747\n",
      "Epoch [2/5], Step [128025/136675], Loss: 5.3017\n",
      "Epoch [2/5], Step [128100/136675], Loss: 4.9837\n",
      "Epoch [2/5], Step [128175/136675], Loss: 5.2520\n",
      "Epoch [2/5], Step [128250/136675], Loss: 5.3128\n",
      "Epoch [2/5], Step [128325/136675], Loss: 5.3118\n",
      "Epoch [2/5], Step [128400/136675], Loss: 5.4183\n",
      "Epoch [2/5], Step [128475/136675], Loss: 5.2944\n",
      "Epoch [2/5], Step [128550/136675], Loss: 5.2708\n",
      "Epoch [2/5], Step [128625/136675], Loss: 5.1813\n",
      "Epoch [2/5], Step [128700/136675], Loss: 5.7095\n",
      "Epoch [2/5], Step [128775/136675], Loss: 5.0050\n",
      "Epoch [2/5], Step [128850/136675], Loss: 5.3155\n",
      "Epoch [2/5], Step [128925/136675], Loss: 5.2682\n",
      "Epoch [2/5], Step [129000/136675], Loss: 5.5620\n",
      "Epoch [2/5], Step [129075/136675], Loss: 5.5120\n",
      "Epoch [2/5], Step [129150/136675], Loss: 5.2124\n",
      "Epoch [2/5], Step [129225/136675], Loss: 5.1582\n",
      "Epoch [2/5], Step [129300/136675], Loss: 5.4889\n",
      "Epoch [2/5], Step [129375/136675], Loss: 5.1153\n",
      "Epoch [2/5], Step [129450/136675], Loss: 5.1386\n",
      "Epoch [2/5], Step [129525/136675], Loss: 5.1598\n",
      "Epoch [2/5], Step [129600/136675], Loss: 5.1733\n",
      "Epoch [2/5], Step [129675/136675], Loss: 5.4051\n",
      "Epoch [2/5], Step [129750/136675], Loss: 5.4547\n",
      "Epoch [2/5], Step [129825/136675], Loss: 5.0456\n",
      "Epoch [2/5], Step [129900/136675], Loss: 5.2385\n",
      "Epoch [2/5], Step [129975/136675], Loss: 5.4501\n",
      "Validation perplexity: 164.14517772517308\n",
      "Epoch [2/5], Step [130050/136675], Loss: 5.0480\n",
      "Epoch [2/5], Step [130125/136675], Loss: 5.3155\n",
      "Epoch [2/5], Step [130200/136675], Loss: 5.3289\n",
      "Epoch [2/5], Step [130275/136675], Loss: 5.3203\n",
      "Epoch [2/5], Step [130350/136675], Loss: 5.3452\n",
      "Epoch [2/5], Step [130425/136675], Loss: 5.3237\n",
      "Epoch [2/5], Step [130500/136675], Loss: 5.4695\n",
      "Epoch [2/5], Step [130575/136675], Loss: 5.3363\n",
      "Epoch [2/5], Step [130650/136675], Loss: 5.2195\n",
      "Epoch [2/5], Step [130725/136675], Loss: 5.2587\n",
      "Epoch [2/5], Step [130800/136675], Loss: 5.4476\n",
      "Epoch [2/5], Step [130875/136675], Loss: 5.2801\n",
      "Epoch [2/5], Step [130950/136675], Loss: 5.1156\n",
      "Epoch [2/5], Step [131025/136675], Loss: 5.3801\n",
      "Epoch [2/5], Step [131100/136675], Loss: 5.4657\n",
      "Epoch [2/5], Step [131175/136675], Loss: 5.4266\n",
      "Epoch [2/5], Step [131250/136675], Loss: 5.4750\n",
      "Epoch [2/5], Step [131325/136675], Loss: 5.1892\n",
      "Epoch [2/5], Step [131400/136675], Loss: 5.5493\n",
      "Epoch [2/5], Step [131475/136675], Loss: 5.3597\n",
      "Epoch [2/5], Step [131550/136675], Loss: 5.6340\n",
      "Epoch [2/5], Step [131625/136675], Loss: 5.4445\n",
      "Epoch [2/5], Step [131700/136675], Loss: 5.1068\n",
      "Epoch [2/5], Step [131775/136675], Loss: 5.3137\n",
      "Epoch [2/5], Step [131850/136675], Loss: 5.5357\n",
      "Epoch [2/5], Step [131925/136675], Loss: 5.5415\n",
      "Epoch [2/5], Step [132000/136675], Loss: 4.9973\n",
      "Epoch [2/5], Step [132075/136675], Loss: 5.0770\n",
      "Epoch [2/5], Step [132150/136675], Loss: 5.1658\n",
      "Epoch [2/5], Step [132225/136675], Loss: 5.4456\n",
      "Epoch [2/5], Step [132300/136675], Loss: 5.4082\n",
      "Epoch [2/5], Step [132375/136675], Loss: 5.6183\n",
      "Epoch [2/5], Step [132450/136675], Loss: 5.2641\n",
      "Epoch [2/5], Step [132525/136675], Loss: 5.3925\n",
      "Epoch [2/5], Step [132600/136675], Loss: 5.0573\n",
      "Epoch [2/5], Step [132675/136675], Loss: 5.4547\n",
      "Epoch [2/5], Step [132750/136675], Loss: 5.2262\n",
      "Epoch [2/5], Step [132825/136675], Loss: 5.4239\n",
      "Epoch [2/5], Step [132900/136675], Loss: 5.2869\n",
      "Epoch [2/5], Step [132975/136675], Loss: 5.5071\n",
      "Epoch [2/5], Step [133050/136675], Loss: 5.3336\n",
      "Epoch [2/5], Step [133125/136675], Loss: 5.4236\n",
      "Epoch [2/5], Step [133200/136675], Loss: 5.1861\n",
      "Epoch [2/5], Step [133275/136675], Loss: 5.5991\n",
      "Epoch [2/5], Step [133350/136675], Loss: 5.4149\n",
      "Epoch [2/5], Step [133425/136675], Loss: 5.3573\n",
      "Epoch [2/5], Step [133500/136675], Loss: 5.0762\n",
      "Epoch [2/5], Step [133575/136675], Loss: 5.4821\n",
      "Epoch [2/5], Step [133650/136675], Loss: 5.4444\n",
      "Epoch [2/5], Step [133725/136675], Loss: 5.0064\n",
      "Epoch [2/5], Step [133800/136675], Loss: 5.6354\n",
      "Epoch [2/5], Step [133875/136675], Loss: 5.1187\n",
      "Epoch [2/5], Step [133950/136675], Loss: 5.2748\n",
      "Epoch [2/5], Step [134025/136675], Loss: 5.0801\n",
      "Epoch [2/5], Step [134100/136675], Loss: 5.2639\n",
      "Epoch [2/5], Step [134175/136675], Loss: 5.2742\n",
      "Epoch [2/5], Step [134250/136675], Loss: 5.4887\n",
      "Epoch [2/5], Step [134325/136675], Loss: 5.2028\n",
      "Epoch [2/5], Step [134400/136675], Loss: 5.5279\n",
      "Epoch [2/5], Step [134475/136675], Loss: 5.2790\n",
      "Epoch [2/5], Step [134550/136675], Loss: 5.0239\n",
      "Epoch [2/5], Step [134625/136675], Loss: 5.3422\n",
      "Epoch [2/5], Step [134700/136675], Loss: 5.1470\n",
      "Epoch [2/5], Step [134775/136675], Loss: 5.4153\n",
      "Epoch [2/5], Step [134850/136675], Loss: 5.4858\n",
      "Epoch [2/5], Step [134925/136675], Loss: 5.4760\n",
      "Epoch [2/5], Step [135000/136675], Loss: 5.0307\n",
      "Epoch [2/5], Step [135075/136675], Loss: 5.5556\n",
      "Epoch [2/5], Step [135150/136675], Loss: 5.1890\n",
      "Epoch [2/5], Step [135225/136675], Loss: 5.5743\n",
      "Epoch [2/5], Step [135300/136675], Loss: 5.6166\n",
      "Epoch [2/5], Step [135375/136675], Loss: 5.3657\n",
      "Epoch [2/5], Step [135450/136675], Loss: 5.2729\n",
      "Epoch [2/5], Step [135525/136675], Loss: 5.3366\n",
      "Epoch [2/5], Step [135600/136675], Loss: 5.4314\n",
      "Epoch [2/5], Step [135675/136675], Loss: 5.3127\n",
      "Epoch [2/5], Step [135750/136675], Loss: 5.3493\n",
      "Epoch [2/5], Step [135825/136675], Loss: 5.3526\n",
      "Epoch [2/5], Step [135900/136675], Loss: 5.4956\n",
      "Epoch [2/5], Step [135975/136675], Loss: 5.5425\n",
      "Epoch [2/5], Step [136050/136675], Loss: 5.2998\n",
      "Epoch [2/5], Step [136125/136675], Loss: 5.4875\n",
      "Epoch [2/5], Step [136200/136675], Loss: 5.6652\n",
      "Epoch [2/5], Step [136275/136675], Loss: 5.5093\n",
      "Epoch [2/5], Step [136350/136675], Loss: 5.3477\n",
      "Epoch [2/5], Step [136425/136675], Loss: 5.6322\n",
      "Epoch [2/5], Step [136500/136675], Loss: 5.0718\n",
      "Epoch [2/5], Step [136575/136675], Loss: 5.1651\n",
      "Epoch [2/5], Step [136650/136675], Loss: 5.4489\n",
      "Epoch [2/5] Average Loss: 5.3481, Perplexity: 210.20\n",
      "Epoch [3/5], Step [0/136675], Loss: 5.3914\n",
      "Validation perplexity: 165.4459009294448\n",
      "Epoch [3/5], Step [75/136675], Loss: 5.4852\n",
      "Epoch [3/5], Step [150/136675], Loss: 5.2064\n",
      "Epoch [3/5], Step [225/136675], Loss: 5.5403\n",
      "Epoch [3/5], Step [300/136675], Loss: 4.7969\n",
      "Epoch [3/5], Step [375/136675], Loss: 5.4567\n",
      "Epoch [3/5], Step [450/136675], Loss: 5.4198\n",
      "Epoch [3/5], Step [525/136675], Loss: 5.3849\n",
      "Epoch [3/5], Step [600/136675], Loss: 5.1577\n",
      "Epoch [3/5], Step [675/136675], Loss: 5.2283\n",
      "Epoch [3/5], Step [750/136675], Loss: 5.3722\n",
      "Epoch [3/5], Step [825/136675], Loss: 5.0283\n",
      "Epoch [3/5], Step [900/136675], Loss: 4.9685\n",
      "Epoch [3/5], Step [975/136675], Loss: 5.1070\n",
      "Epoch [3/5], Step [1050/136675], Loss: 5.4437\n",
      "Epoch [3/5], Step [1125/136675], Loss: 5.0760\n",
      "Epoch [3/5], Step [1200/136675], Loss: 5.5821\n",
      "Epoch [3/5], Step [1275/136675], Loss: 5.2759\n",
      "Epoch [3/5], Step [1350/136675], Loss: 5.4454\n",
      "Epoch [3/5], Step [1425/136675], Loss: 5.4531\n",
      "Epoch [3/5], Step [1500/136675], Loss: 5.3297\n",
      "Epoch [3/5], Step [1575/136675], Loss: 5.4447\n",
      "Epoch [3/5], Step [1650/136675], Loss: 5.1781\n",
      "Epoch [3/5], Step [1725/136675], Loss: 5.0190\n",
      "Epoch [3/5], Step [1800/136675], Loss: 5.5188\n",
      "Epoch [3/5], Step [1875/136675], Loss: 5.0795\n",
      "Epoch [3/5], Step [1950/136675], Loss: 5.1407\n",
      "Epoch [3/5], Step [2025/136675], Loss: 5.2895\n",
      "Epoch [3/5], Step [2100/136675], Loss: 5.1500\n",
      "Epoch [3/5], Step [2175/136675], Loss: 5.4254\n",
      "Epoch [3/5], Step [2250/136675], Loss: 5.2782\n",
      "Epoch [3/5], Step [2325/136675], Loss: 5.2139\n",
      "Epoch [3/5], Step [2400/136675], Loss: 5.4968\n",
      "Epoch [3/5], Step [2475/136675], Loss: 5.3208\n",
      "Epoch [3/5], Step [2550/136675], Loss: 5.3546\n",
      "Epoch [3/5], Step [2625/136675], Loss: 5.3985\n",
      "Epoch [3/5], Step [2700/136675], Loss: 5.6520\n",
      "Epoch [3/5], Step [2775/136675], Loss: 5.4457\n",
      "Epoch [3/5], Step [2850/136675], Loss: 5.2310\n",
      "Epoch [3/5], Step [2925/136675], Loss: 5.1878\n",
      "Epoch [3/5], Step [3000/136675], Loss: 4.9568\n",
      "Epoch [3/5], Step [3075/136675], Loss: 5.5830\n",
      "Epoch [3/5], Step [3150/136675], Loss: 5.3203\n",
      "Epoch [3/5], Step [3225/136675], Loss: 5.2625\n",
      "Epoch [3/5], Step [3300/136675], Loss: 5.1407\n",
      "Epoch [3/5], Step [3375/136675], Loss: 5.2979\n",
      "Epoch [3/5], Step [3450/136675], Loss: 5.4735\n",
      "Epoch [3/5], Step [3525/136675], Loss: 5.5539\n",
      "Epoch [3/5], Step [3600/136675], Loss: 5.3280\n",
      "Epoch [3/5], Step [3675/136675], Loss: 5.2640\n",
      "Epoch [3/5], Step [3750/136675], Loss: 5.3413\n",
      "Epoch [3/5], Step [3825/136675], Loss: 5.4123\n",
      "Epoch [3/5], Step [3900/136675], Loss: 5.3314\n",
      "Epoch [3/5], Step [3975/136675], Loss: 5.1647\n",
      "Epoch [3/5], Step [4050/136675], Loss: 5.3238\n",
      "Epoch [3/5], Step [4125/136675], Loss: 5.1320\n",
      "Epoch [3/5], Step [4200/136675], Loss: 5.3285\n",
      "Epoch [3/5], Step [4275/136675], Loss: 4.9760\n",
      "Epoch [3/5], Step [4350/136675], Loss: 5.2543\n",
      "Epoch [3/5], Step [4425/136675], Loss: 5.0184\n",
      "Epoch [3/5], Step [4500/136675], Loss: 5.2924\n",
      "Epoch [3/5], Step [4575/136675], Loss: 5.3216\n",
      "Epoch [3/5], Step [4650/136675], Loss: 5.2152\n",
      "Epoch [3/5], Step [4725/136675], Loss: 5.4483\n",
      "Epoch [3/5], Step [4800/136675], Loss: 5.2654\n",
      "Epoch [3/5], Step [4875/136675], Loss: 5.2383\n",
      "Epoch [3/5], Step [4950/136675], Loss: 5.4635\n",
      "Epoch [3/5], Step [5025/136675], Loss: 5.3893\n",
      "Epoch [3/5], Step [5100/136675], Loss: 5.1717\n",
      "Epoch [3/5], Step [5175/136675], Loss: 5.3591\n",
      "Epoch [3/5], Step [5250/136675], Loss: 5.0023\n",
      "Epoch [3/5], Step [5325/136675], Loss: 5.6201\n",
      "Epoch [3/5], Step [5400/136675], Loss: 5.3052\n",
      "Epoch [3/5], Step [5475/136675], Loss: 5.3602\n",
      "Epoch [3/5], Step [5550/136675], Loss: 5.4614\n",
      "Epoch [3/5], Step [5625/136675], Loss: 5.2885\n",
      "Epoch [3/5], Step [5700/136675], Loss: 5.3411\n",
      "Epoch [3/5], Step [5775/136675], Loss: 5.7353\n",
      "Epoch [3/5], Step [5850/136675], Loss: 4.9307\n",
      "Epoch [3/5], Step [5925/136675], Loss: 5.3498\n",
      "Epoch [3/5], Step [6000/136675], Loss: 5.5939\n",
      "Epoch [3/5], Step [6075/136675], Loss: 5.2220\n",
      "Epoch [3/5], Step [6150/136675], Loss: 5.3393\n",
      "Epoch [3/5], Step [6225/136675], Loss: 5.4406\n",
      "Epoch [3/5], Step [6300/136675], Loss: 5.6138\n",
      "Epoch [3/5], Step [6375/136675], Loss: 5.0935\n",
      "Epoch [3/5], Step [6450/136675], Loss: 5.3285\n",
      "Epoch [3/5], Step [6525/136675], Loss: 5.2625\n",
      "Epoch [3/5], Step [6600/136675], Loss: 5.2770\n",
      "Epoch [3/5], Step [6675/136675], Loss: 5.1607\n",
      "Epoch [3/5], Step [6750/136675], Loss: 5.3374\n",
      "Epoch [3/5], Step [6825/136675], Loss: 5.3878\n",
      "Epoch [3/5], Step [6900/136675], Loss: 5.4245\n",
      "Epoch [3/5], Step [6975/136675], Loss: 5.3021\n",
      "Epoch [3/5], Step [7050/136675], Loss: 5.2884\n",
      "Epoch [3/5], Step [7125/136675], Loss: 5.4019\n",
      "Epoch [3/5], Step [7200/136675], Loss: 5.3982\n",
      "Epoch [3/5], Step [7275/136675], Loss: 5.6685\n",
      "Epoch [3/5], Step [7350/136675], Loss: 5.4645\n",
      "Epoch [3/5], Step [7425/136675], Loss: 5.2459\n",
      "Epoch [3/5], Step [7500/136675], Loss: 5.5970\n",
      "Epoch [3/5], Step [7575/136675], Loss: 5.4665\n",
      "Epoch [3/5], Step [7650/136675], Loss: 5.3929\n",
      "Epoch [3/5], Step [7725/136675], Loss: 5.4024\n",
      "Epoch [3/5], Step [7800/136675], Loss: 5.3242\n",
      "Epoch [3/5], Step [7875/136675], Loss: 5.2901\n",
      "Epoch [3/5], Step [7950/136675], Loss: 5.1409\n",
      "Epoch [3/5], Step [8025/136675], Loss: 5.6525\n",
      "Epoch [3/5], Step [8100/136675], Loss: 5.1420\n",
      "Epoch [3/5], Step [8175/136675], Loss: 5.1427\n",
      "Epoch [3/5], Step [8250/136675], Loss: 5.0683\n",
      "Epoch [3/5], Step [8325/136675], Loss: 5.2885\n",
      "Epoch [3/5], Step [8400/136675], Loss: 5.1326\n",
      "Epoch [3/5], Step [8475/136675], Loss: 5.3905\n",
      "Epoch [3/5], Step [8550/136675], Loss: 5.2269\n",
      "Epoch [3/5], Step [8625/136675], Loss: 5.3993\n",
      "Epoch [3/5], Step [8700/136675], Loss: 5.1940\n",
      "Epoch [3/5], Step [8775/136675], Loss: 5.3669\n",
      "Epoch [3/5], Step [8850/136675], Loss: 5.5649\n",
      "Epoch [3/5], Step [8925/136675], Loss: 5.2847\n",
      "Epoch [3/5], Step [9000/136675], Loss: 5.0476\n",
      "Epoch [3/5], Step [9075/136675], Loss: 5.2760\n",
      "Epoch [3/5], Step [9150/136675], Loss: 5.4228\n",
      "Epoch [3/5], Step [9225/136675], Loss: 5.4724\n",
      "Epoch [3/5], Step [9300/136675], Loss: 5.3904\n",
      "Epoch [3/5], Step [9375/136675], Loss: 5.3988\n",
      "Epoch [3/5], Step [9450/136675], Loss: 5.2335\n",
      "Epoch [3/5], Step [9525/136675], Loss: 5.0500\n",
      "Epoch [3/5], Step [9600/136675], Loss: 5.3156\n",
      "Epoch [3/5], Step [9675/136675], Loss: 5.2494\n",
      "Epoch [3/5], Step [9750/136675], Loss: 5.4225\n",
      "Epoch [3/5], Step [9825/136675], Loss: 5.5378\n",
      "Epoch [3/5], Step [9900/136675], Loss: 5.1125\n",
      "Epoch [3/5], Step [9975/136675], Loss: 5.3887\n",
      "Validation perplexity: 164.05039401940576\n",
      "Epoch [3/5], Step [10050/136675], Loss: 5.4941\n",
      "Epoch [3/5], Step [10125/136675], Loss: 5.3939\n",
      "Epoch [3/5], Step [10200/136675], Loss: 5.4602\n",
      "Epoch [3/5], Step [10275/136675], Loss: 5.3701\n",
      "Epoch [3/5], Step [10350/136675], Loss: 5.3117\n",
      "Epoch [3/5], Step [10425/136675], Loss: 5.3295\n",
      "Epoch [3/5], Step [10500/136675], Loss: 5.3810\n",
      "Epoch [3/5], Step [10575/136675], Loss: 5.4673\n",
      "Epoch [3/5], Step [10650/136675], Loss: 5.3244\n",
      "Epoch [3/5], Step [10725/136675], Loss: 5.6393\n",
      "Epoch [3/5], Step [10800/136675], Loss: 5.2304\n",
      "Epoch [3/5], Step [10875/136675], Loss: 5.6171\n",
      "Epoch [3/5], Step [10950/136675], Loss: 5.2311\n",
      "Epoch [3/5], Step [11025/136675], Loss: 5.8488\n",
      "Epoch [3/5], Step [11100/136675], Loss: 5.3955\n",
      "Epoch [3/5], Step [11175/136675], Loss: 5.1836\n",
      "Epoch [3/5], Step [11250/136675], Loss: 5.4314\n",
      "Epoch [3/5], Step [11325/136675], Loss: 5.3598\n",
      "Epoch [3/5], Step [11400/136675], Loss: 5.1522\n",
      "Epoch [3/5], Step [11475/136675], Loss: 5.1803\n",
      "Epoch [3/5], Step [11550/136675], Loss: 5.3934\n",
      "Epoch [3/5], Step [11625/136675], Loss: 5.6303\n",
      "Epoch [3/5], Step [11700/136675], Loss: 5.2505\n",
      "Epoch [3/5], Step [11775/136675], Loss: 5.2699\n",
      "Epoch [3/5], Step [11850/136675], Loss: 5.7347\n",
      "Epoch [3/5], Step [11925/136675], Loss: 5.3384\n",
      "Epoch [3/5], Step [12000/136675], Loss: 5.5841\n",
      "Epoch [3/5], Step [12075/136675], Loss: 5.2857\n",
      "Epoch [3/5], Step [12150/136675], Loss: 5.1270\n",
      "Epoch [3/5], Step [12225/136675], Loss: 5.5811\n",
      "Epoch [3/5], Step [12300/136675], Loss: 5.2654\n",
      "Epoch [3/5], Step [12375/136675], Loss: 5.6896\n",
      "Epoch [3/5], Step [12450/136675], Loss: 5.2111\n",
      "Epoch [3/5], Step [12525/136675], Loss: 5.4668\n",
      "Epoch [3/5], Step [12600/136675], Loss: 5.4527\n",
      "Epoch [3/5], Step [12675/136675], Loss: 5.4186\n",
      "Epoch [3/5], Step [12750/136675], Loss: 5.4389\n",
      "Epoch [3/5], Step [12825/136675], Loss: 5.1164\n",
      "Epoch [3/5], Step [12900/136675], Loss: 5.1540\n",
      "Epoch [3/5], Step [12975/136675], Loss: 5.1711\n",
      "Epoch [3/5], Step [13050/136675], Loss: 5.4302\n",
      "Epoch [3/5], Step [13125/136675], Loss: 5.5158\n",
      "Epoch [3/5], Step [13200/136675], Loss: 4.9151\n",
      "Epoch [3/5], Step [13275/136675], Loss: 5.0002\n",
      "Epoch [3/5], Step [13350/136675], Loss: 5.3603\n",
      "Epoch [3/5], Step [13425/136675], Loss: 5.3408\n",
      "Epoch [3/5], Step [13500/136675], Loss: 5.2435\n",
      "Epoch [3/5], Step [13575/136675], Loss: 5.1310\n",
      "Epoch [3/5], Step [13650/136675], Loss: 5.4915\n",
      "Epoch [3/5], Step [13725/136675], Loss: 5.5681\n",
      "Epoch [3/5], Step [13800/136675], Loss: 5.4480\n",
      "Epoch [3/5], Step [13875/136675], Loss: 5.2178\n",
      "Epoch [3/5], Step [13950/136675], Loss: 5.4828\n",
      "Epoch [3/5], Step [14025/136675], Loss: 5.2438\n",
      "Epoch [3/5], Step [14100/136675], Loss: 5.4943\n",
      "Epoch [3/5], Step [14175/136675], Loss: 4.8979\n",
      "Epoch [3/5], Step [14250/136675], Loss: 5.3361\n",
      "Epoch [3/5], Step [14325/136675], Loss: 5.4003\n",
      "Epoch [3/5], Step [14400/136675], Loss: 5.3166\n",
      "Epoch [3/5], Step [14475/136675], Loss: 5.1902\n",
      "Epoch [3/5], Step [14550/136675], Loss: 5.4985\n",
      "Epoch [3/5], Step [14625/136675], Loss: 5.0805\n",
      "Epoch [3/5], Step [14700/136675], Loss: 5.2287\n",
      "Epoch [3/5], Step [14775/136675], Loss: 5.2840\n",
      "Epoch [3/5], Step [14850/136675], Loss: 5.0642\n",
      "Epoch [3/5], Step [14925/136675], Loss: 5.0180\n",
      "Epoch [3/5], Step [15000/136675], Loss: 5.4041\n",
      "Epoch [3/5], Step [15075/136675], Loss: 5.4655\n",
      "Epoch [3/5], Step [15150/136675], Loss: 5.2495\n",
      "Epoch [3/5], Step [15225/136675], Loss: 5.4109\n",
      "Epoch [3/5], Step [15300/136675], Loss: 5.1548\n",
      "Epoch [3/5], Step [15375/136675], Loss: 5.4917\n",
      "Epoch [3/5], Step [15450/136675], Loss: 5.0046\n",
      "Epoch [3/5], Step [15525/136675], Loss: 5.3787\n",
      "Epoch [3/5], Step [15600/136675], Loss: 5.4650\n",
      "Epoch [3/5], Step [15675/136675], Loss: 5.3941\n",
      "Epoch [3/5], Step [15750/136675], Loss: 5.4413\n",
      "Epoch [3/5], Step [15825/136675], Loss: 5.3256\n",
      "Epoch [3/5], Step [15900/136675], Loss: 5.4802\n",
      "Epoch [3/5], Step [15975/136675], Loss: 5.5969\n",
      "Epoch [3/5], Step [16050/136675], Loss: 5.4519\n",
      "Epoch [3/5], Step [16125/136675], Loss: 5.2906\n",
      "Epoch [3/5], Step [16200/136675], Loss: 5.3279\n",
      "Epoch [3/5], Step [16275/136675], Loss: 5.1856\n",
      "Epoch [3/5], Step [16350/136675], Loss: 5.3887\n",
      "Epoch [3/5], Step [16425/136675], Loss: 5.2972\n",
      "Epoch [3/5], Step [16500/136675], Loss: 5.2794\n",
      "Epoch [3/5], Step [16575/136675], Loss: 5.2374\n",
      "Epoch [3/5], Step [16650/136675], Loss: 5.3738\n",
      "Epoch [3/5], Step [16725/136675], Loss: 5.3652\n",
      "Epoch [3/5], Step [16800/136675], Loss: 5.4484\n",
      "Epoch [3/5], Step [16875/136675], Loss: 5.4171\n",
      "Epoch [3/5], Step [16950/136675], Loss: 5.4158\n",
      "Epoch [3/5], Step [17025/136675], Loss: 4.8419\n",
      "Epoch [3/5], Step [17100/136675], Loss: 5.6202\n",
      "Epoch [3/5], Step [17175/136675], Loss: 5.2668\n",
      "Epoch [3/5], Step [17250/136675], Loss: 5.2630\n",
      "Epoch [3/5], Step [17325/136675], Loss: 5.5863\n",
      "Epoch [3/5], Step [17400/136675], Loss: 5.3326\n",
      "Epoch [3/5], Step [17475/136675], Loss: 5.4081\n",
      "Epoch [3/5], Step [17550/136675], Loss: 5.3928\n",
      "Epoch [3/5], Step [17625/136675], Loss: 5.2364\n",
      "Epoch [3/5], Step [17700/136675], Loss: 5.3069\n",
      "Epoch [3/5], Step [17775/136675], Loss: 5.2544\n",
      "Epoch [3/5], Step [17850/136675], Loss: 5.3628\n",
      "Epoch [3/5], Step [17925/136675], Loss: 5.3821\n",
      "Epoch [3/5], Step [18000/136675], Loss: 5.2189\n",
      "Epoch [3/5], Step [18075/136675], Loss: 5.4643\n",
      "Epoch [3/5], Step [18150/136675], Loss: 5.3639\n",
      "Epoch [3/5], Step [18225/136675], Loss: 5.3311\n",
      "Epoch [3/5], Step [18300/136675], Loss: 5.2738\n",
      "Epoch [3/5], Step [18375/136675], Loss: 5.4305\n",
      "Epoch [3/5], Step [18450/136675], Loss: 5.7628\n",
      "Epoch [3/5], Step [18525/136675], Loss: 5.4257\n",
      "Epoch [3/5], Step [18600/136675], Loss: 5.4760\n",
      "Epoch [3/5], Step [18675/136675], Loss: 5.2649\n",
      "Epoch [3/5], Step [18750/136675], Loss: 5.4960\n",
      "Epoch [3/5], Step [18825/136675], Loss: 5.6891\n",
      "Epoch [3/5], Step [18900/136675], Loss: 5.3889\n",
      "Epoch [3/5], Step [18975/136675], Loss: 5.3326\n",
      "Epoch [3/5], Step [19050/136675], Loss: 5.1170\n",
      "Epoch [3/5], Step [19125/136675], Loss: 5.3523\n",
      "Epoch [3/5], Step [19200/136675], Loss: 5.6046\n",
      "Epoch [3/5], Step [19275/136675], Loss: 5.1303\n",
      "Epoch [3/5], Step [19350/136675], Loss: 5.4883\n",
      "Epoch [3/5], Step [19425/136675], Loss: 5.1728\n",
      "Epoch [3/5], Step [19500/136675], Loss: 5.2869\n",
      "Epoch [3/5], Step [19575/136675], Loss: 5.1465\n",
      "Epoch [3/5], Step [19650/136675], Loss: 5.0932\n",
      "Epoch [3/5], Step [19725/136675], Loss: 5.3566\n",
      "Epoch [3/5], Step [19800/136675], Loss: 5.4758\n",
      "Epoch [3/5], Step [19875/136675], Loss: 5.6094\n",
      "Epoch [3/5], Step [19950/136675], Loss: 5.1430\n",
      "Validation perplexity: 162.56393150838858\n",
      "Epoch [3/5], Step [20025/136675], Loss: 5.2966\n",
      "Epoch [3/5], Step [20100/136675], Loss: 5.1852\n",
      "Epoch [3/5], Step [20175/136675], Loss: 5.4328\n",
      "Epoch [3/5], Step [20250/136675], Loss: 5.4954\n",
      "Epoch [3/5], Step [20325/136675], Loss: 5.4349\n",
      "Epoch [3/5], Step [20400/136675], Loss: 5.4456\n",
      "Epoch [3/5], Step [20475/136675], Loss: 5.3563\n",
      "Epoch [3/5], Step [20550/136675], Loss: 5.5702\n",
      "Epoch [3/5], Step [20625/136675], Loss: 5.3547\n",
      "Epoch [3/5], Step [20700/136675], Loss: 5.3729\n",
      "Epoch [3/5], Step [20775/136675], Loss: 5.5388\n",
      "Epoch [3/5], Step [20850/136675], Loss: 5.2996\n",
      "Epoch [3/5], Step [20925/136675], Loss: 5.5155\n",
      "Epoch [3/5], Step [21000/136675], Loss: 5.3712\n",
      "Epoch [3/5], Step [21075/136675], Loss: 5.1035\n",
      "Epoch [3/5], Step [21150/136675], Loss: 5.2575\n",
      "Epoch [3/5], Step [21225/136675], Loss: 5.4660\n",
      "Epoch [3/5], Step [21300/136675], Loss: 5.3243\n",
      "Epoch [3/5], Step [21375/136675], Loss: 5.4101\n",
      "Epoch [3/5], Step [21450/136675], Loss: 5.3792\n",
      "Epoch [3/5], Step [21525/136675], Loss: 5.1229\n",
      "Epoch [3/5], Step [21600/136675], Loss: 5.2980\n",
      "Epoch [3/5], Step [21675/136675], Loss: 5.2147\n",
      "Epoch [3/5], Step [21750/136675], Loss: 5.5389\n",
      "Epoch [3/5], Step [21825/136675], Loss: 5.3917\n",
      "Epoch [3/5], Step [21900/136675], Loss: 4.9621\n",
      "Epoch [3/5], Step [21975/136675], Loss: 5.1850\n",
      "Epoch [3/5], Step [22050/136675], Loss: 5.4283\n",
      "Epoch [3/5], Step [22125/136675], Loss: 5.5246\n",
      "Epoch [3/5], Step [22200/136675], Loss: 5.0401\n",
      "Epoch [3/5], Step [22275/136675], Loss: 5.3587\n",
      "Epoch [3/5], Step [22350/136675], Loss: 5.4881\n",
      "Epoch [3/5], Step [22425/136675], Loss: 5.6393\n",
      "Epoch [3/5], Step [22500/136675], Loss: 5.0795\n",
      "Epoch [3/5], Step [22575/136675], Loss: 5.2542\n",
      "Epoch [3/5], Step [22650/136675], Loss: 5.1107\n",
      "Epoch [3/5], Step [22725/136675], Loss: 5.3301\n",
      "Epoch [3/5], Step [22800/136675], Loss: 5.3794\n",
      "Epoch [3/5], Step [22875/136675], Loss: 5.3562\n",
      "Epoch [3/5], Step [22950/136675], Loss: 5.4216\n",
      "Epoch [3/5], Step [23025/136675], Loss: 5.5675\n",
      "Epoch [3/5], Step [23100/136675], Loss: 5.4781\n",
      "Epoch [3/5], Step [23175/136675], Loss: 5.4353\n",
      "Epoch [3/5], Step [23250/136675], Loss: 5.4169\n",
      "Epoch [3/5], Step [23325/136675], Loss: 5.4741\n",
      "Epoch [3/5], Step [23400/136675], Loss: 5.3557\n",
      "Epoch [3/5], Step [23475/136675], Loss: 5.5448\n",
      "Epoch [3/5], Step [23550/136675], Loss: 5.5390\n",
      "Epoch [3/5], Step [23625/136675], Loss: 5.5249\n",
      "Epoch [3/5], Step [23700/136675], Loss: 5.3606\n",
      "Epoch [3/5], Step [23775/136675], Loss: 5.2890\n",
      "Epoch [3/5], Step [23850/136675], Loss: 5.2262\n",
      "Epoch [3/5], Step [23925/136675], Loss: 5.0783\n",
      "Epoch [3/5], Step [24000/136675], Loss: 5.3303\n",
      "Epoch [3/5], Step [24075/136675], Loss: 5.3344\n",
      "Epoch [3/5], Step [24150/136675], Loss: 5.4093\n",
      "Epoch [3/5], Step [24225/136675], Loss: 5.2777\n",
      "Epoch [3/5], Step [24300/136675], Loss: 4.8323\n",
      "Epoch [3/5], Step [24375/136675], Loss: 5.4205\n",
      "Epoch [3/5], Step [24450/136675], Loss: 5.5307\n",
      "Epoch [3/5], Step [24525/136675], Loss: 5.1860\n",
      "Epoch [3/5], Step [24600/136675], Loss: 5.3665\n",
      "Epoch [3/5], Step [24675/136675], Loss: 5.1749\n",
      "Epoch [3/5], Step [24750/136675], Loss: 5.4075\n",
      "Epoch [3/5], Step [24825/136675], Loss: 5.3013\n",
      "Epoch [3/5], Step [24900/136675], Loss: 5.2014\n",
      "Epoch [3/5], Step [24975/136675], Loss: 5.1109\n",
      "Epoch [3/5], Step [25050/136675], Loss: 5.1633\n",
      "Epoch [3/5], Step [25125/136675], Loss: 5.2876\n",
      "Epoch [3/5], Step [25200/136675], Loss: 5.2950\n",
      "Epoch [3/5], Step [25275/136675], Loss: 5.0152\n",
      "Epoch [3/5], Step [25350/136675], Loss: 5.0865\n",
      "Epoch [3/5], Step [25425/136675], Loss: 5.0901\n",
      "Epoch [3/5], Step [25500/136675], Loss: 5.4492\n",
      "Epoch [3/5], Step [25575/136675], Loss: 5.1328\n",
      "Epoch [3/5], Step [25650/136675], Loss: 5.1795\n",
      "Epoch [3/5], Step [25725/136675], Loss: 5.1407\n",
      "Epoch [3/5], Step [25800/136675], Loss: 5.0822\n",
      "Epoch [3/5], Step [25875/136675], Loss: 5.4644\n",
      "Epoch [3/5], Step [25950/136675], Loss: 5.4181\n",
      "Epoch [3/5], Step [26025/136675], Loss: 5.4030\n",
      "Epoch [3/5], Step [26100/136675], Loss: 5.3468\n",
      "Epoch [3/5], Step [26175/136675], Loss: 5.2156\n",
      "Epoch [3/5], Step [26250/136675], Loss: 5.2006\n",
      "Epoch [3/5], Step [26325/136675], Loss: 4.9915\n",
      "Epoch [3/5], Step [26400/136675], Loss: 5.1046\n",
      "Epoch [3/5], Step [26475/136675], Loss: 5.4193\n",
      "Epoch [3/5], Step [26550/136675], Loss: 4.9957\n",
      "Epoch [3/5], Step [26625/136675], Loss: 5.3545\n",
      "Epoch [3/5], Step [26700/136675], Loss: 5.3473\n",
      "Epoch [3/5], Step [26775/136675], Loss: 5.1595\n",
      "Epoch [3/5], Step [26850/136675], Loss: 5.0635\n",
      "Epoch [3/5], Step [26925/136675], Loss: 5.1214\n",
      "Epoch [3/5], Step [27000/136675], Loss: 5.4001\n",
      "Epoch [3/5], Step [27075/136675], Loss: 5.4620\n",
      "Epoch [3/5], Step [27150/136675], Loss: 5.3094\n",
      "Epoch [3/5], Step [27225/136675], Loss: 5.4542\n",
      "Epoch [3/5], Step [27300/136675], Loss: 5.4354\n",
      "Epoch [3/5], Step [27375/136675], Loss: 5.5705\n",
      "Epoch [3/5], Step [27450/136675], Loss: 5.3715\n",
      "Epoch [3/5], Step [27525/136675], Loss: 5.2619\n",
      "Epoch [3/5], Step [27600/136675], Loss: 5.2684\n",
      "Epoch [3/5], Step [27675/136675], Loss: 5.2651\n",
      "Epoch [3/5], Step [27750/136675], Loss: 5.4775\n",
      "Epoch [3/5], Step [27825/136675], Loss: 5.3845\n",
      "Epoch [3/5], Step [27900/136675], Loss: 5.2810\n",
      "Epoch [3/5], Step [27975/136675], Loss: 5.4582\n",
      "Epoch [3/5], Step [28050/136675], Loss: 5.1380\n",
      "Epoch [3/5], Step [28125/136675], Loss: 5.2856\n",
      "Epoch [3/5], Step [28200/136675], Loss: 5.5063\n",
      "Epoch [3/5], Step [28275/136675], Loss: 5.4233\n",
      "Epoch [3/5], Step [28350/136675], Loss: 5.4668\n",
      "Epoch [3/5], Step [28425/136675], Loss: 4.8527\n",
      "Epoch [3/5], Step [28500/136675], Loss: 5.2361\n",
      "Epoch [3/5], Step [28575/136675], Loss: 5.4799\n",
      "Epoch [3/5], Step [28650/136675], Loss: 5.3529\n",
      "Epoch [3/5], Step [28725/136675], Loss: 5.1811\n",
      "Epoch [3/5], Step [28800/136675], Loss: 5.5934\n",
      "Epoch [3/5], Step [28875/136675], Loss: 5.3723\n",
      "Epoch [3/5], Step [28950/136675], Loss: 5.5018\n",
      "Epoch [3/5], Step [29025/136675], Loss: 5.3735\n",
      "Epoch [3/5], Step [29100/136675], Loss: 5.1566\n",
      "Epoch [3/5], Step [29175/136675], Loss: 5.3869\n",
      "Epoch [3/5], Step [29250/136675], Loss: 5.2259\n",
      "Epoch [3/5], Step [29325/136675], Loss: 4.9700\n",
      "Epoch [3/5], Step [29400/136675], Loss: 5.1213\n",
      "Epoch [3/5], Step [29475/136675], Loss: 5.4088\n",
      "Epoch [3/5], Step [29550/136675], Loss: 4.9156\n",
      "Epoch [3/5], Step [29625/136675], Loss: 5.1002\n",
      "Epoch [3/5], Step [29700/136675], Loss: 5.4198\n",
      "Epoch [3/5], Step [29775/136675], Loss: 5.2105\n",
      "Epoch [3/5], Step [29850/136675], Loss: 5.2903\n",
      "Epoch [3/5], Step [29925/136675], Loss: 5.2842\n",
      "Epoch [3/5], Step [30000/136675], Loss: 5.5133\n",
      "Validation perplexity: 162.65726465392316\n",
      "Epoch [3/5], Step [30075/136675], Loss: 5.4377\n",
      "Epoch [3/5], Step [30150/136675], Loss: 5.2824\n",
      "Epoch [3/5], Step [30225/136675], Loss: 5.1032\n",
      "Epoch [3/5], Step [30300/136675], Loss: 5.1431\n",
      "Epoch [3/5], Step [30375/136675], Loss: 5.3687\n",
      "Epoch [3/5], Step [30450/136675], Loss: 5.5313\n",
      "Epoch [3/5], Step [30525/136675], Loss: 5.3148\n",
      "Epoch [3/5], Step [30600/136675], Loss: 5.2337\n",
      "Epoch [3/5], Step [30675/136675], Loss: 5.5599\n",
      "Epoch [3/5], Step [30750/136675], Loss: 5.3887\n",
      "Epoch [3/5], Step [30825/136675], Loss: 5.7151\n",
      "Epoch [3/5], Step [30900/136675], Loss: 5.6127\n",
      "Epoch [3/5], Step [30975/136675], Loss: 5.3445\n",
      "Epoch [3/5], Step [31050/136675], Loss: 5.3696\n",
      "Epoch [3/5], Step [31125/136675], Loss: 5.1820\n",
      "Epoch [3/5], Step [31200/136675], Loss: 5.1661\n",
      "Epoch [3/5], Step [31275/136675], Loss: 5.3459\n",
      "Epoch [3/5], Step [31350/136675], Loss: 5.6817\n",
      "Epoch [3/5], Step [31425/136675], Loss: 5.2889\n",
      "Epoch [3/5], Step [31500/136675], Loss: 5.5779\n",
      "Epoch [3/5], Step [31575/136675], Loss: 5.2695\n",
      "Epoch [3/5], Step [31650/136675], Loss: 5.3223\n",
      "Epoch [3/5], Step [31725/136675], Loss: 5.3730\n",
      "Epoch [3/5], Step [31800/136675], Loss: 5.2110\n",
      "Epoch [3/5], Step [31875/136675], Loss: 4.9800\n",
      "Epoch [3/5], Step [31950/136675], Loss: 5.3157\n",
      "Epoch [3/5], Step [32025/136675], Loss: 5.6278\n",
      "Epoch [3/5], Step [32100/136675], Loss: 5.3469\n",
      "Epoch [3/5], Step [32175/136675], Loss: 5.4335\n",
      "Epoch [3/5], Step [32250/136675], Loss: 5.2177\n",
      "Epoch [3/5], Step [32325/136675], Loss: 5.2936\n",
      "Epoch [3/5], Step [32400/136675], Loss: 5.1153\n",
      "Epoch [3/5], Step [32475/136675], Loss: 5.2886\n",
      "Epoch [3/5], Step [32550/136675], Loss: 5.3582\n",
      "Epoch [3/5], Step [32625/136675], Loss: 5.2367\n",
      "Epoch [3/5], Step [32700/136675], Loss: 5.2082\n",
      "Epoch [3/5], Step [32775/136675], Loss: 4.8407\n",
      "Epoch [3/5], Step [32850/136675], Loss: 5.0776\n",
      "Epoch [3/5], Step [32925/136675], Loss: 5.4226\n",
      "Epoch [3/5], Step [33000/136675], Loss: 5.2887\n",
      "Epoch [3/5], Step [33075/136675], Loss: 5.2309\n",
      "Epoch [3/5], Step [33150/136675], Loss: 5.3347\n",
      "Epoch [3/5], Step [33225/136675], Loss: 5.4052\n",
      "Epoch [3/5], Step [33300/136675], Loss: 5.3862\n",
      "Epoch [3/5], Step [33375/136675], Loss: 5.4748\n",
      "Epoch [3/5], Step [33450/136675], Loss: 5.1478\n",
      "Epoch [3/5], Step [33525/136675], Loss: 5.2967\n",
      "Epoch [3/5], Step [33600/136675], Loss: 5.7195\n",
      "Epoch [3/5], Step [33675/136675], Loss: 5.1904\n",
      "Epoch [3/5], Step [33750/136675], Loss: 5.3092\n",
      "Epoch [3/5], Step [33825/136675], Loss: 5.4120\n",
      "Epoch [3/5], Step [33900/136675], Loss: 5.4982\n",
      "Epoch [3/5], Step [33975/136675], Loss: 5.1585\n",
      "Epoch [3/5], Step [34050/136675], Loss: 5.3685\n",
      "Epoch [3/5], Step [34125/136675], Loss: 5.1007\n",
      "Epoch [3/5], Step [34200/136675], Loss: 5.2148\n",
      "Epoch [3/5], Step [34275/136675], Loss: 5.2257\n",
      "Epoch [3/5], Step [34350/136675], Loss: 5.1556\n",
      "Epoch [3/5], Step [34425/136675], Loss: 5.5849\n",
      "Epoch [3/5], Step [34500/136675], Loss: 5.3308\n",
      "Epoch [3/5], Step [34575/136675], Loss: 5.2631\n",
      "Epoch [3/5], Step [34650/136675], Loss: 5.6922\n",
      "Epoch [3/5], Step [34725/136675], Loss: 5.2526\n",
      "Epoch [3/5], Step [34800/136675], Loss: 5.3584\n",
      "Epoch [3/5], Step [34875/136675], Loss: 5.2859\n",
      "Epoch [3/5], Step [34950/136675], Loss: 5.0194\n",
      "Epoch [3/5], Step [35025/136675], Loss: 5.3578\n",
      "Epoch [3/5], Step [35100/136675], Loss: 5.2338\n",
      "Epoch [3/5], Step [35175/136675], Loss: 5.0286\n",
      "Epoch [3/5], Step [35250/136675], Loss: 5.3954\n",
      "Epoch [3/5], Step [35325/136675], Loss: 5.2392\n",
      "Epoch [3/5], Step [35400/136675], Loss: 5.4022\n",
      "Epoch [3/5], Step [35475/136675], Loss: 5.1698\n",
      "Epoch [3/5], Step [35550/136675], Loss: 5.5721\n",
      "Epoch [3/5], Step [35625/136675], Loss: 5.3187\n",
      "Epoch [3/5], Step [35700/136675], Loss: 5.1421\n",
      "Epoch [3/5], Step [35775/136675], Loss: 5.6547\n",
      "Epoch [3/5], Step [35850/136675], Loss: 5.7483\n",
      "Epoch [3/5], Step [35925/136675], Loss: 5.3516\n",
      "Epoch [3/5], Step [36000/136675], Loss: 5.4046\n",
      "Epoch [3/5], Step [36075/136675], Loss: 5.2237\n",
      "Epoch [3/5], Step [36150/136675], Loss: 5.7021\n",
      "Epoch [3/5], Step [36225/136675], Loss: 5.0723\n",
      "Epoch [3/5], Step [36300/136675], Loss: 5.3796\n",
      "Epoch [3/5], Step [36375/136675], Loss: 5.4493\n",
      "Epoch [3/5], Step [36450/136675], Loss: 5.5093\n",
      "Epoch [3/5], Step [36525/136675], Loss: 5.0889\n",
      "Epoch [3/5], Step [36600/136675], Loss: 5.2356\n",
      "Epoch [3/5], Step [36675/136675], Loss: 5.3344\n",
      "Epoch [3/5], Step [36750/136675], Loss: 5.1743\n",
      "Epoch [3/5], Step [36825/136675], Loss: 5.2891\n",
      "Epoch [3/5], Step [36900/136675], Loss: 5.2050\n",
      "Epoch [3/5], Step [36975/136675], Loss: 5.4623\n",
      "Epoch [3/5], Step [37050/136675], Loss: 5.2243\n",
      "Epoch [3/5], Step [37125/136675], Loss: 5.3685\n",
      "Epoch [3/5], Step [37200/136675], Loss: 4.9827\n",
      "Epoch [3/5], Step [37275/136675], Loss: 5.4499\n",
      "Epoch [3/5], Step [37350/136675], Loss: 5.2552\n",
      "Epoch [3/5], Step [37425/136675], Loss: 5.1618\n",
      "Epoch [3/5], Step [37500/136675], Loss: 4.9332\n",
      "Epoch [3/5], Step [37575/136675], Loss: 5.0929\n",
      "Epoch [3/5], Step [37650/136675], Loss: 4.9924\n",
      "Epoch [3/5], Step [37725/136675], Loss: 5.2537\n",
      "Epoch [3/5], Step [37800/136675], Loss: 5.2514\n",
      "Epoch [3/5], Step [37875/136675], Loss: 5.2409\n",
      "Epoch [3/5], Step [37950/136675], Loss: 5.4487\n",
      "Epoch [3/5], Step [38025/136675], Loss: 5.5487\n",
      "Epoch [3/5], Step [38100/136675], Loss: 5.2937\n",
      "Epoch [3/5], Step [38175/136675], Loss: 5.4387\n",
      "Epoch [3/5], Step [38250/136675], Loss: 5.4883\n",
      "Epoch [3/5], Step [38325/136675], Loss: 5.6143\n",
      "Epoch [3/5], Step [38400/136675], Loss: 5.3203\n",
      "Epoch [3/5], Step [38475/136675], Loss: 5.4156\n",
      "Epoch [3/5], Step [38550/136675], Loss: 5.3978\n",
      "Epoch [3/5], Step [38625/136675], Loss: 5.3408\n",
      "Epoch [3/5], Step [38700/136675], Loss: 5.3583\n",
      "Epoch [3/5], Step [38775/136675], Loss: 5.3555\n",
      "Epoch [3/5], Step [38850/136675], Loss: 5.3136\n",
      "Epoch [3/5], Step [38925/136675], Loss: 5.4127\n",
      "Epoch [3/5], Step [39000/136675], Loss: 5.3191\n",
      "Epoch [3/5], Step [39075/136675], Loss: 5.1644\n",
      "Epoch [3/5], Step [39150/136675], Loss: 5.3677\n",
      "Epoch [3/5], Step [39225/136675], Loss: 5.3326\n",
      "Epoch [3/5], Step [39300/136675], Loss: 5.1873\n",
      "Epoch [3/5], Step [39375/136675], Loss: 5.1971\n",
      "Epoch [3/5], Step [39450/136675], Loss: 5.3046\n",
      "Epoch [3/5], Step [39525/136675], Loss: 5.0125\n",
      "Epoch [3/5], Step [39600/136675], Loss: 5.2008\n",
      "Epoch [3/5], Step [39675/136675], Loss: 5.0683\n",
      "Epoch [3/5], Step [39750/136675], Loss: 5.2119\n",
      "Epoch [3/5], Step [39825/136675], Loss: 5.5136\n",
      "Epoch [3/5], Step [39900/136675], Loss: 5.6163\n",
      "Epoch [3/5], Step [39975/136675], Loss: 5.2626\n",
      "Validation perplexity: 162.48273482237207\n",
      "Epoch [3/5], Step [40050/136675], Loss: 5.2319\n",
      "Epoch [3/5], Step [40125/136675], Loss: 5.4074\n",
      "Epoch [3/5], Step [40200/136675], Loss: 5.1223\n",
      "Epoch [3/5], Step [40275/136675], Loss: 5.3474\n",
      "Epoch [3/5], Step [40350/136675], Loss: 5.0844\n",
      "Epoch [3/5], Step [40425/136675], Loss: 5.3918\n",
      "Epoch [3/5], Step [40500/136675], Loss: 5.4087\n",
      "Epoch [3/5], Step [40575/136675], Loss: 4.9506\n",
      "Epoch [3/5], Step [40650/136675], Loss: 5.1453\n",
      "Epoch [3/5], Step [40725/136675], Loss: 5.3550\n",
      "Epoch [3/5], Step [40800/136675], Loss: 4.9176\n",
      "Epoch [3/5], Step [40875/136675], Loss: 4.9588\n",
      "Epoch [3/5], Step [40950/136675], Loss: 5.1356\n",
      "Epoch [3/5], Step [41025/136675], Loss: 5.2881\n",
      "Epoch [3/5], Step [41100/136675], Loss: 5.2030\n",
      "Epoch [3/5], Step [41175/136675], Loss: 5.3303\n",
      "Epoch [3/5], Step [41250/136675], Loss: 5.2737\n",
      "Epoch [3/5], Step [41325/136675], Loss: 5.0546\n",
      "Epoch [3/5], Step [41400/136675], Loss: 5.3062\n",
      "Epoch [3/5], Step [41475/136675], Loss: 5.5093\n",
      "Epoch [3/5], Step [41550/136675], Loss: 4.9562\n",
      "Epoch [3/5], Step [41625/136675], Loss: 5.3887\n",
      "Epoch [3/5], Step [41700/136675], Loss: 5.4638\n",
      "Epoch [3/5], Step [41775/136675], Loss: 5.3078\n",
      "Epoch [3/5], Step [41850/136675], Loss: 4.9621\n",
      "Epoch [3/5], Step [41925/136675], Loss: 5.2127\n",
      "Epoch [3/5], Step [42000/136675], Loss: 5.1940\n",
      "Epoch [3/5], Step [42075/136675], Loss: 5.4354\n",
      "Epoch [3/5], Step [42150/136675], Loss: 5.6359\n",
      "Epoch [3/5], Step [42225/136675], Loss: 5.4189\n",
      "Epoch [3/5], Step [42300/136675], Loss: 5.2265\n",
      "Epoch [3/5], Step [42375/136675], Loss: 5.3408\n",
      "Epoch [3/5], Step [42450/136675], Loss: 5.2875\n",
      "Epoch [3/5], Step [42525/136675], Loss: 5.3741\n",
      "Epoch [3/5], Step [42600/136675], Loss: 5.6028\n",
      "Epoch [3/5], Step [42675/136675], Loss: 5.4024\n",
      "Epoch [3/5], Step [42750/136675], Loss: 5.2552\n",
      "Epoch [3/5], Step [42825/136675], Loss: 5.2395\n",
      "Epoch [3/5], Step [42900/136675], Loss: 5.1701\n",
      "Epoch [3/5], Step [42975/136675], Loss: 5.0087\n",
      "Epoch [3/5], Step [43050/136675], Loss: 5.4820\n",
      "Epoch [3/5], Step [43125/136675], Loss: 5.5189\n",
      "Epoch [3/5], Step [43200/136675], Loss: 5.3736\n",
      "Epoch [3/5], Step [43275/136675], Loss: 5.6311\n",
      "Epoch [3/5], Step [43350/136675], Loss: 5.4075\n",
      "Epoch [3/5], Step [43425/136675], Loss: 5.2570\n",
      "Epoch [3/5], Step [43500/136675], Loss: 5.0835\n",
      "Epoch [3/5], Step [43575/136675], Loss: 5.3390\n",
      "Epoch [3/5], Step [43650/136675], Loss: 5.0611\n",
      "Epoch [3/5], Step [43725/136675], Loss: 5.1574\n",
      "Epoch [3/5], Step [43800/136675], Loss: 5.3100\n",
      "Epoch [3/5], Step [43875/136675], Loss: 5.3423\n",
      "Epoch [3/5], Step [43950/136675], Loss: 5.2890\n",
      "Epoch [3/5], Step [44025/136675], Loss: 5.4241\n",
      "Epoch [3/5], Step [44100/136675], Loss: 5.2539\n",
      "Epoch [3/5], Step [44175/136675], Loss: 5.0062\n",
      "Epoch [3/5], Step [44250/136675], Loss: 5.2023\n",
      "Epoch [3/5], Step [44325/136675], Loss: 5.5581\n",
      "Epoch [3/5], Step [44400/136675], Loss: 5.3656\n",
      "Epoch [3/5], Step [44475/136675], Loss: 5.3643\n",
      "Epoch [3/5], Step [44550/136675], Loss: 5.5544\n",
      "Epoch [3/5], Step [44625/136675], Loss: 5.2575\n",
      "Epoch [3/5], Step [44700/136675], Loss: 5.1448\n",
      "Epoch [3/5], Step [44775/136675], Loss: 5.3371\n",
      "Epoch [3/5], Step [44850/136675], Loss: 5.4480\n",
      "Epoch [3/5], Step [44925/136675], Loss: 5.2796\n",
      "Epoch [3/5], Step [45000/136675], Loss: 5.4007\n",
      "Epoch [3/5], Step [45075/136675], Loss: 5.5204\n",
      "Epoch [3/5], Step [45150/136675], Loss: 5.5188\n",
      "Epoch [3/5], Step [45225/136675], Loss: 5.2336\n",
      "Epoch [3/5], Step [45300/136675], Loss: 5.2329\n",
      "Epoch [3/5], Step [45375/136675], Loss: 5.5481\n",
      "Epoch [3/5], Step [45450/136675], Loss: 5.3514\n",
      "Epoch [3/5], Step [45525/136675], Loss: 4.9530\n",
      "Epoch [3/5], Step [45600/136675], Loss: 5.4009\n",
      "Epoch [3/5], Step [45675/136675], Loss: 5.5437\n",
      "Epoch [3/5], Step [45750/136675], Loss: 4.8445\n",
      "Epoch [3/5], Step [45825/136675], Loss: 5.2840\n",
      "Epoch [3/5], Step [45900/136675], Loss: 5.3295\n",
      "Epoch [3/5], Step [45975/136675], Loss: 5.2749\n",
      "Epoch [3/5], Step [46050/136675], Loss: 5.2503\n",
      "Epoch [3/5], Step [46125/136675], Loss: 4.9975\n",
      "Epoch [3/5], Step [46200/136675], Loss: 5.2055\n",
      "Epoch [3/5], Step [46275/136675], Loss: 5.4694\n",
      "Epoch [3/5], Step [46350/136675], Loss: 5.1815\n",
      "Epoch [3/5], Step [46425/136675], Loss: 5.3978\n",
      "Epoch [3/5], Step [46500/136675], Loss: 5.3378\n",
      "Epoch [3/5], Step [46575/136675], Loss: 5.2460\n",
      "Epoch [3/5], Step [46650/136675], Loss: 5.4071\n",
      "Epoch [3/5], Step [46725/136675], Loss: 5.0989\n",
      "Epoch [3/5], Step [46800/136675], Loss: 5.2419\n",
      "Epoch [3/5], Step [46875/136675], Loss: 5.3791\n",
      "Epoch [3/5], Step [46950/136675], Loss: 5.1669\n",
      "Epoch [3/5], Step [47025/136675], Loss: 5.4446\n",
      "Epoch [3/5], Step [47100/136675], Loss: 5.2456\n",
      "Epoch [3/5], Step [47175/136675], Loss: 5.4136\n",
      "Epoch [3/5], Step [47250/136675], Loss: 5.2447\n",
      "Epoch [3/5], Step [47325/136675], Loss: 5.1929\n",
      "Epoch [3/5], Step [47400/136675], Loss: 4.9782\n",
      "Epoch [3/5], Step [47475/136675], Loss: 5.5006\n",
      "Epoch [3/5], Step [47550/136675], Loss: 5.1406\n",
      "Epoch [3/5], Step [47625/136675], Loss: 5.6509\n",
      "Epoch [3/5], Step [47700/136675], Loss: 4.9541\n",
      "Epoch [3/5], Step [47775/136675], Loss: 5.6125\n",
      "Epoch [3/5], Step [47850/136675], Loss: 4.9679\n",
      "Epoch [3/5], Step [47925/136675], Loss: 5.2208\n",
      "Epoch [3/5], Step [48000/136675], Loss: 5.0517\n",
      "Epoch [3/5], Step [48075/136675], Loss: 5.2405\n",
      "Epoch [3/5], Step [48150/136675], Loss: 5.5191\n",
      "Epoch [3/5], Step [48225/136675], Loss: 5.4447\n",
      "Epoch [3/5], Step [48300/136675], Loss: 5.5757\n",
      "Epoch [3/5], Step [48375/136675], Loss: 5.3046\n",
      "Epoch [3/5], Step [48450/136675], Loss: 5.4757\n",
      "Epoch [3/5], Step [48525/136675], Loss: 5.2287\n",
      "Epoch [3/5], Step [48600/136675], Loss: 5.2502\n",
      "Epoch [3/5], Step [48675/136675], Loss: 5.2523\n",
      "Epoch [3/5], Step [48750/136675], Loss: 5.3872\n",
      "Epoch [3/5], Step [48825/136675], Loss: 5.5276\n",
      "Epoch [3/5], Step [48900/136675], Loss: 5.3652\n",
      "Epoch [3/5], Step [48975/136675], Loss: 5.3418\n",
      "Epoch [3/5], Step [49050/136675], Loss: 5.5721\n",
      "Epoch [3/5], Step [49125/136675], Loss: 5.6092\n",
      "Epoch [3/5], Step [49200/136675], Loss: 5.2536\n",
      "Epoch [3/5], Step [49275/136675], Loss: 5.2305\n",
      "Epoch [3/5], Step [49350/136675], Loss: 5.4148\n",
      "Epoch [3/5], Step [49425/136675], Loss: 5.5713\n",
      "Epoch [3/5], Step [49500/136675], Loss: 5.2628\n",
      "Epoch [3/5], Step [49575/136675], Loss: 5.1175\n",
      "Epoch [3/5], Step [49650/136675], Loss: 5.2768\n",
      "Epoch [3/5], Step [49725/136675], Loss: 5.2177\n",
      "Epoch [3/5], Step [49800/136675], Loss: 5.4368\n",
      "Epoch [3/5], Step [49875/136675], Loss: 5.3785\n",
      "Epoch [3/5], Step [49950/136675], Loss: 5.6279\n",
      "Validation perplexity: 161.6729522979531\n",
      "Epoch [3/5], Step [50025/136675], Loss: 5.3010\n",
      "Epoch [3/5], Step [50100/136675], Loss: 5.1933\n",
      "Epoch [3/5], Step [50175/136675], Loss: 5.5807\n",
      "Epoch [3/5], Step [50250/136675], Loss: 5.4300\n",
      "Epoch [3/5], Step [50325/136675], Loss: 5.2136\n",
      "Epoch [3/5], Step [50400/136675], Loss: 5.2230\n",
      "Epoch [3/5], Step [50475/136675], Loss: 5.3943\n",
      "Epoch [3/5], Step [50550/136675], Loss: 5.3606\n",
      "Epoch [3/5], Step [50625/136675], Loss: 5.1648\n",
      "Epoch [3/5], Step [50700/136675], Loss: 5.2593\n",
      "Epoch [3/5], Step [50775/136675], Loss: 5.1828\n",
      "Epoch [3/5], Step [50850/136675], Loss: 5.4008\n",
      "Epoch [3/5], Step [50925/136675], Loss: 5.0577\n",
      "Epoch [3/5], Step [51000/136675], Loss: 5.2567\n",
      "Epoch [3/5], Step [51075/136675], Loss: 4.9634\n",
      "Epoch [3/5], Step [51150/136675], Loss: 5.3176\n",
      "Epoch [3/5], Step [51225/136675], Loss: 5.3329\n",
      "Epoch [3/5], Step [51300/136675], Loss: 5.6244\n",
      "Epoch [3/5], Step [51375/136675], Loss: 5.4502\n",
      "Epoch [3/5], Step [51450/136675], Loss: 5.3683\n",
      "Epoch [3/5], Step [51525/136675], Loss: 5.2583\n",
      "Epoch [3/5], Step [51600/136675], Loss: 5.0094\n",
      "Epoch [3/5], Step [51675/136675], Loss: 4.9979\n",
      "Epoch [3/5], Step [51750/136675], Loss: 5.4539\n",
      "Epoch [3/5], Step [51825/136675], Loss: 5.6238\n",
      "Epoch [3/5], Step [51900/136675], Loss: 5.4231\n",
      "Epoch [3/5], Step [51975/136675], Loss: 5.1498\n",
      "Epoch [3/5], Step [52050/136675], Loss: 5.2988\n",
      "Epoch [3/5], Step [52125/136675], Loss: 5.3826\n",
      "Epoch [3/5], Step [52200/136675], Loss: 5.3506\n",
      "Epoch [3/5], Step [52275/136675], Loss: 4.9037\n",
      "Epoch [3/5], Step [52350/136675], Loss: 5.1032\n",
      "Epoch [3/5], Step [52425/136675], Loss: 5.7464\n",
      "Epoch [3/5], Step [52500/136675], Loss: 5.4985\n",
      "Epoch [3/5], Step [52575/136675], Loss: 5.1440\n",
      "Epoch [3/5], Step [52650/136675], Loss: 5.1682\n",
      "Epoch [3/5], Step [52725/136675], Loss: 5.3123\n",
      "Epoch [3/5], Step [52800/136675], Loss: 5.2958\n",
      "Epoch [3/5], Step [52875/136675], Loss: 5.3808\n",
      "Epoch [3/5], Step [52950/136675], Loss: 5.3616\n",
      "Epoch [3/5], Step [53025/136675], Loss: 5.2335\n",
      "Epoch [3/5], Step [53100/136675], Loss: 5.3651\n",
      "Epoch [3/5], Step [53175/136675], Loss: 5.1099\n",
      "Epoch [3/5], Step [53250/136675], Loss: 5.3409\n",
      "Epoch [3/5], Step [53325/136675], Loss: 5.0117\n",
      "Epoch [3/5], Step [53400/136675], Loss: 5.2054\n",
      "Epoch [3/5], Step [53475/136675], Loss: 5.7739\n",
      "Epoch [3/5], Step [53550/136675], Loss: 5.3090\n",
      "Epoch [3/5], Step [53625/136675], Loss: 5.5715\n",
      "Epoch [3/5], Step [53700/136675], Loss: 5.1840\n",
      "Epoch [3/5], Step [53775/136675], Loss: 5.5112\n",
      "Epoch [3/5], Step [53850/136675], Loss: 5.2573\n",
      "Epoch [3/5], Step [53925/136675], Loss: 5.4795\n",
      "Epoch [3/5], Step [54000/136675], Loss: 5.1467\n",
      "Epoch [3/5], Step [54075/136675], Loss: 5.0271\n",
      "Epoch [3/5], Step [54150/136675], Loss: 5.4173\n",
      "Epoch [3/5], Step [54225/136675], Loss: 5.0422\n",
      "Epoch [3/5], Step [54300/136675], Loss: 5.4640\n",
      "Epoch [3/5], Step [54375/136675], Loss: 5.3880\n",
      "Epoch [3/5], Step [54450/136675], Loss: 4.9936\n",
      "Epoch [3/5], Step [54525/136675], Loss: 5.0757\n",
      "Epoch [3/5], Step [54600/136675], Loss: 5.3937\n",
      "Epoch [3/5], Step [54675/136675], Loss: 5.3960\n",
      "Epoch [3/5], Step [54750/136675], Loss: 5.2362\n",
      "Epoch [3/5], Step [54825/136675], Loss: 5.3531\n",
      "Epoch [3/5], Step [54900/136675], Loss: 5.2849\n",
      "Epoch [3/5], Step [54975/136675], Loss: 5.3102\n",
      "Epoch [3/5], Step [55050/136675], Loss: 5.2546\n",
      "Epoch [3/5], Step [55125/136675], Loss: 4.9588\n",
      "Epoch [3/5], Step [55200/136675], Loss: 5.7790\n",
      "Epoch [3/5], Step [55275/136675], Loss: 5.2671\n",
      "Epoch [3/5], Step [55350/136675], Loss: 5.0608\n",
      "Epoch [3/5], Step [55425/136675], Loss: 5.4736\n",
      "Epoch [3/5], Step [55500/136675], Loss: 5.2077\n",
      "Epoch [3/5], Step [55575/136675], Loss: 5.0355\n",
      "Epoch [3/5], Step [55650/136675], Loss: 5.2742\n",
      "Epoch [3/5], Step [55725/136675], Loss: 5.3310\n",
      "Epoch [3/5], Step [55800/136675], Loss: 5.6572\n",
      "Epoch [3/5], Step [55875/136675], Loss: 5.4497\n",
      "Epoch [3/5], Step [55950/136675], Loss: 5.2156\n",
      "Epoch [3/5], Step [56025/136675], Loss: 5.3440\n",
      "Epoch [3/5], Step [56100/136675], Loss: 5.4154\n",
      "Epoch [3/5], Step [56175/136675], Loss: 5.3655\n",
      "Epoch [3/5], Step [56250/136675], Loss: 5.3183\n",
      "Epoch [3/5], Step [56325/136675], Loss: 5.2516\n",
      "Epoch [3/5], Step [56400/136675], Loss: 5.2798\n",
      "Epoch [3/5], Step [56475/136675], Loss: 5.3688\n",
      "Epoch [3/5], Step [56550/136675], Loss: 5.6463\n",
      "Epoch [3/5], Step [56625/136675], Loss: 5.3081\n",
      "Epoch [3/5], Step [56700/136675], Loss: 5.1500\n",
      "Epoch [3/5], Step [56775/136675], Loss: 4.9985\n",
      "Epoch [3/5], Step [56850/136675], Loss: 5.4592\n",
      "Epoch [3/5], Step [56925/136675], Loss: 5.5231\n",
      "Epoch [3/5], Step [57000/136675], Loss: 4.9317\n",
      "Epoch [3/5], Step [57075/136675], Loss: 5.3644\n",
      "Epoch [3/5], Step [57150/136675], Loss: 5.1688\n",
      "Epoch [3/5], Step [57225/136675], Loss: 4.9460\n",
      "Epoch [3/5], Step [57300/136675], Loss: 5.1417\n",
      "Epoch [3/5], Step [57375/136675], Loss: 5.3909\n",
      "Epoch [3/5], Step [57450/136675], Loss: 5.4078\n",
      "Epoch [3/5], Step [57525/136675], Loss: 5.4363\n",
      "Epoch [3/5], Step [57600/136675], Loss: 5.3345\n",
      "Epoch [3/5], Step [57675/136675], Loss: 5.2677\n",
      "Epoch [3/5], Step [57750/136675], Loss: 5.4597\n",
      "Epoch [3/5], Step [57825/136675], Loss: 5.2418\n",
      "Epoch [3/5], Step [57900/136675], Loss: 5.3478\n",
      "Epoch [3/5], Step [57975/136675], Loss: 5.3326\n",
      "Epoch [3/5], Step [58050/136675], Loss: 5.3506\n",
      "Epoch [3/5], Step [58125/136675], Loss: 5.4502\n",
      "Epoch [3/5], Step [58200/136675], Loss: 5.2210\n",
      "Epoch [3/5], Step [58275/136675], Loss: 5.3181\n",
      "Epoch [3/5], Step [58350/136675], Loss: 5.2235\n",
      "Epoch [3/5], Step [58425/136675], Loss: 5.3656\n",
      "Epoch [3/5], Step [58500/136675], Loss: 5.4721\n",
      "Epoch [3/5], Step [58575/136675], Loss: 5.6010\n",
      "Epoch [3/5], Step [58650/136675], Loss: 5.2448\n",
      "Epoch [3/5], Step [58725/136675], Loss: 5.1528\n",
      "Epoch [3/5], Step [58800/136675], Loss: 5.4925\n",
      "Epoch [3/5], Step [58875/136675], Loss: 5.4576\n",
      "Epoch [3/5], Step [58950/136675], Loss: 5.2765\n",
      "Epoch [3/5], Step [59025/136675], Loss: 5.3282\n",
      "Epoch [3/5], Step [59100/136675], Loss: 5.1898\n",
      "Epoch [3/5], Step [59175/136675], Loss: 5.0996\n",
      "Epoch [3/5], Step [59250/136675], Loss: 5.1954\n",
      "Epoch [3/5], Step [59325/136675], Loss: 5.1200\n",
      "Epoch [3/5], Step [59400/136675], Loss: 5.3105\n",
      "Epoch [3/5], Step [59475/136675], Loss: 5.1377\n",
      "Epoch [3/5], Step [59550/136675], Loss: 5.1470\n",
      "Epoch [3/5], Step [59625/136675], Loss: 5.2018\n",
      "Epoch [3/5], Step [59700/136675], Loss: 5.2145\n",
      "Epoch [3/5], Step [59775/136675], Loss: 5.3742\n",
      "Epoch [3/5], Step [59850/136675], Loss: 5.0872\n",
      "Epoch [3/5], Step [59925/136675], Loss: 5.3023\n",
      "Epoch [3/5], Step [60000/136675], Loss: 5.0819\n",
      "Validation perplexity: 160.82979488522417\n",
      "Epoch [3/5], Step [60075/136675], Loss: 5.4398\n",
      "Epoch [3/5], Step [60150/136675], Loss: 4.8769\n",
      "Epoch [3/5], Step [60225/136675], Loss: 5.6057\n",
      "Epoch [3/5], Step [60300/136675], Loss: 5.4585\n",
      "Epoch [3/5], Step [60375/136675], Loss: 5.3585\n",
      "Epoch [3/5], Step [60450/136675], Loss: 5.1904\n",
      "Epoch [3/5], Step [60525/136675], Loss: 5.4634\n",
      "Epoch [3/5], Step [60600/136675], Loss: 5.1558\n",
      "Epoch [3/5], Step [60675/136675], Loss: 5.5250\n",
      "Epoch [3/5], Step [60750/136675], Loss: 5.2501\n",
      "Epoch [3/5], Step [60825/136675], Loss: 5.2837\n",
      "Epoch [3/5], Step [60900/136675], Loss: 5.2942\n",
      "Epoch [3/5], Step [60975/136675], Loss: 5.3648\n",
      "Epoch [3/5], Step [61050/136675], Loss: 5.2944\n",
      "Epoch [3/5], Step [61125/136675], Loss: 4.8948\n",
      "Epoch [3/5], Step [61200/136675], Loss: 5.4788\n",
      "Epoch [3/5], Step [61275/136675], Loss: 5.0121\n",
      "Epoch [3/5], Step [61350/136675], Loss: 5.3737\n",
      "Epoch [3/5], Step [61425/136675], Loss: 5.2459\n",
      "Epoch [3/5], Step [61500/136675], Loss: 5.3360\n",
      "Epoch [3/5], Step [61575/136675], Loss: 5.5220\n",
      "Epoch [3/5], Step [61650/136675], Loss: 5.2402\n",
      "Epoch [3/5], Step [61725/136675], Loss: 5.1064\n",
      "Epoch [3/5], Step [61800/136675], Loss: 5.6339\n",
      "Epoch [3/5], Step [61875/136675], Loss: 5.2020\n",
      "Epoch [3/5], Step [61950/136675], Loss: 5.0864\n",
      "Epoch [3/5], Step [62025/136675], Loss: 5.4014\n",
      "Epoch [3/5], Step [62100/136675], Loss: 5.2456\n",
      "Epoch [3/5], Step [62175/136675], Loss: 5.0177\n",
      "Epoch [3/5], Step [62250/136675], Loss: 5.4672\n",
      "Epoch [3/5], Step [62325/136675], Loss: 5.2055\n",
      "Epoch [3/5], Step [62400/136675], Loss: 5.2897\n",
      "Epoch [3/5], Step [62475/136675], Loss: 4.9572\n",
      "Epoch [3/5], Step [62550/136675], Loss: 5.1358\n",
      "Epoch [3/5], Step [62625/136675], Loss: 5.2085\n",
      "Epoch [3/5], Step [62700/136675], Loss: 5.3838\n",
      "Epoch [3/5], Step [62775/136675], Loss: 5.1867\n",
      "Epoch [3/5], Step [62850/136675], Loss: 5.1456\n",
      "Epoch [3/5], Step [62925/136675], Loss: 5.3265\n",
      "Epoch [3/5], Step [63000/136675], Loss: 5.4540\n",
      "Epoch [3/5], Step [63075/136675], Loss: 5.4508\n",
      "Epoch [3/5], Step [63150/136675], Loss: 5.2097\n",
      "Epoch [3/5], Step [63225/136675], Loss: 5.0441\n",
      "Epoch [3/5], Step [63300/136675], Loss: 5.1516\n",
      "Epoch [3/5], Step [63375/136675], Loss: 5.2279\n",
      "Epoch [3/5], Step [63450/136675], Loss: 5.1552\n",
      "Epoch [3/5], Step [63525/136675], Loss: 5.4507\n",
      "Epoch [3/5], Step [63600/136675], Loss: 5.4455\n",
      "Epoch [3/5], Step [63675/136675], Loss: 5.2843\n",
      "Epoch [3/5], Step [63750/136675], Loss: 5.4959\n",
      "Epoch [3/5], Step [63825/136675], Loss: 5.2457\n",
      "Epoch [3/5], Step [63900/136675], Loss: 5.5095\n",
      "Epoch [3/5], Step [63975/136675], Loss: 5.4478\n",
      "Epoch [3/5], Step [64050/136675], Loss: 5.2747\n",
      "Epoch [3/5], Step [64125/136675], Loss: 5.2152\n",
      "Epoch [3/5], Step [64200/136675], Loss: 5.2357\n",
      "Epoch [3/5], Step [64275/136675], Loss: 5.4172\n",
      "Epoch [3/5], Step [64350/136675], Loss: 5.1969\n",
      "Epoch [3/5], Step [64425/136675], Loss: 5.0157\n",
      "Epoch [3/5], Step [64500/136675], Loss: 5.2357\n",
      "Epoch [3/5], Step [64575/136675], Loss: 5.4019\n",
      "Epoch [3/5], Step [64650/136675], Loss: 5.2417\n",
      "Epoch [3/5], Step [64725/136675], Loss: 5.2434\n",
      "Epoch [3/5], Step [64800/136675], Loss: 5.2097\n",
      "Epoch [3/5], Step [64875/136675], Loss: 5.1944\n",
      "Epoch [3/5], Step [64950/136675], Loss: 5.5609\n",
      "Epoch [3/5], Step [65025/136675], Loss: 5.2253\n",
      "Epoch [3/5], Step [65100/136675], Loss: 5.1434\n",
      "Epoch [3/5], Step [65175/136675], Loss: 5.4729\n",
      "Epoch [3/5], Step [65250/136675], Loss: 5.3633\n",
      "Epoch [3/5], Step [65325/136675], Loss: 5.5276\n",
      "Epoch [3/5], Step [65400/136675], Loss: 4.9338\n",
      "Epoch [3/5], Step [65475/136675], Loss: 5.1286\n",
      "Epoch [3/5], Step [65550/136675], Loss: 5.4365\n",
      "Epoch [3/5], Step [65625/136675], Loss: 5.1071\n",
      "Epoch [3/5], Step [65700/136675], Loss: 5.2248\n",
      "Epoch [3/5], Step [65775/136675], Loss: 5.2560\n",
      "Epoch [3/5], Step [65850/136675], Loss: 5.5051\n",
      "Epoch [3/5], Step [65925/136675], Loss: 5.4399\n",
      "Epoch [3/5], Step [66000/136675], Loss: 5.1612\n",
      "Epoch [3/5], Step [66075/136675], Loss: 5.2522\n",
      "Epoch [3/5], Step [66150/136675], Loss: 5.5732\n",
      "Epoch [3/5], Step [66225/136675], Loss: 5.1432\n",
      "Epoch [3/5], Step [66300/136675], Loss: 5.5185\n",
      "Epoch [3/5], Step [66375/136675], Loss: 4.9644\n",
      "Epoch [3/5], Step [66450/136675], Loss: 5.5009\n",
      "Epoch [3/5], Step [66525/136675], Loss: 5.3897\n",
      "Epoch [3/5], Step [66600/136675], Loss: 5.5354\n",
      "Epoch [3/5], Step [66675/136675], Loss: 5.7627\n",
      "Epoch [3/5], Step [66750/136675], Loss: 5.5343\n",
      "Epoch [3/5], Step [66825/136675], Loss: 5.2190\n",
      "Epoch [3/5], Step [66900/136675], Loss: 5.4172\n",
      "Epoch [3/5], Step [66975/136675], Loss: 5.3680\n",
      "Epoch [3/5], Step [67050/136675], Loss: 5.4271\n",
      "Epoch [3/5], Step [67125/136675], Loss: 5.1454\n",
      "Epoch [3/5], Step [67200/136675], Loss: 5.5667\n",
      "Epoch [3/5], Step [67275/136675], Loss: 5.5015\n",
      "Epoch [3/5], Step [67350/136675], Loss: 5.4741\n",
      "Epoch [3/5], Step [67425/136675], Loss: 4.9608\n",
      "Epoch [3/5], Step [67500/136675], Loss: 5.2795\n",
      "Epoch [3/5], Step [67575/136675], Loss: 4.8780\n",
      "Epoch [3/5], Step [67650/136675], Loss: 5.2746\n",
      "Epoch [3/5], Step [67725/136675], Loss: 5.2611\n",
      "Epoch [3/5], Step [67800/136675], Loss: 5.5857\n",
      "Epoch [3/5], Step [67875/136675], Loss: 5.4060\n",
      "Epoch [3/5], Step [67950/136675], Loss: 5.6458\n",
      "Epoch [3/5], Step [68025/136675], Loss: 5.5675\n",
      "Epoch [3/5], Step [68100/136675], Loss: 5.4384\n",
      "Epoch [3/5], Step [68175/136675], Loss: 5.3516\n",
      "Epoch [3/5], Step [68250/136675], Loss: 5.0435\n",
      "Epoch [3/5], Step [68325/136675], Loss: 5.1318\n",
      "Epoch [3/5], Step [68400/136675], Loss: 5.1316\n",
      "Epoch [3/5], Step [68475/136675], Loss: 5.4589\n",
      "Epoch [3/5], Step [68550/136675], Loss: 5.2736\n",
      "Epoch [3/5], Step [68625/136675], Loss: 5.2393\n",
      "Epoch [3/5], Step [68700/136675], Loss: 5.4720\n",
      "Epoch [3/5], Step [68775/136675], Loss: 5.4381\n",
      "Epoch [3/5], Step [68850/136675], Loss: 5.4259\n",
      "Epoch [3/5], Step [68925/136675], Loss: 5.2004\n",
      "Epoch [3/5], Step [69000/136675], Loss: 5.1506\n",
      "Epoch [3/5], Step [69075/136675], Loss: 5.4049\n",
      "Epoch [3/5], Step [69150/136675], Loss: 5.2938\n",
      "Epoch [3/5], Step [69225/136675], Loss: 4.6597\n",
      "Epoch [3/5], Step [69300/136675], Loss: 5.2067\n",
      "Epoch [3/5], Step [69375/136675], Loss: 5.4875\n",
      "Epoch [3/5], Step [69450/136675], Loss: 5.3207\n",
      "Epoch [3/5], Step [69525/136675], Loss: 5.3325\n",
      "Epoch [3/5], Step [69600/136675], Loss: 5.2393\n",
      "Epoch [3/5], Step [69675/136675], Loss: 4.9625\n",
      "Epoch [3/5], Step [69750/136675], Loss: 5.3544\n",
      "Epoch [3/5], Step [69825/136675], Loss: 5.6171\n",
      "Epoch [3/5], Step [69900/136675], Loss: 4.8767\n",
      "Epoch [3/5], Step [69975/136675], Loss: 5.2658\n",
      "Validation perplexity: 160.56393026975252\n",
      "Epoch [3/5], Step [70050/136675], Loss: 5.2159\n",
      "Epoch [3/5], Step [70125/136675], Loss: 5.0507\n",
      "Epoch [3/5], Step [70200/136675], Loss: 4.9305\n",
      "Epoch [3/5], Step [70275/136675], Loss: 5.5106\n",
      "Epoch [3/5], Step [70350/136675], Loss: 5.4666\n",
      "Epoch [3/5], Step [70425/136675], Loss: 5.2201\n",
      "Epoch [3/5], Step [70500/136675], Loss: 5.0525\n",
      "Epoch [3/5], Step [70575/136675], Loss: 5.5119\n",
      "Epoch [3/5], Step [70650/136675], Loss: 5.4642\n",
      "Epoch [3/5], Step [70725/136675], Loss: 5.0855\n",
      "Epoch [3/5], Step [70800/136675], Loss: 5.2540\n",
      "Epoch [3/5], Step [70875/136675], Loss: 5.2014\n",
      "Epoch [3/5], Step [70950/136675], Loss: 5.2433\n",
      "Epoch [3/5], Step [71025/136675], Loss: 5.4687\n",
      "Epoch [3/5], Step [71100/136675], Loss: 5.3735\n",
      "Epoch [3/5], Step [71175/136675], Loss: 5.4898\n",
      "Epoch [3/5], Step [71250/136675], Loss: 5.0746\n",
      "Epoch [3/5], Step [71325/136675], Loss: 5.3679\n",
      "Epoch [3/5], Step [71400/136675], Loss: 5.3835\n",
      "Epoch [3/5], Step [71475/136675], Loss: 5.7730\n",
      "Epoch [3/5], Step [71550/136675], Loss: 5.3228\n",
      "Epoch [3/5], Step [71625/136675], Loss: 5.1454\n",
      "Epoch [3/5], Step [71700/136675], Loss: 5.0808\n",
      "Epoch [3/5], Step [71775/136675], Loss: 5.2030\n",
      "Epoch [3/5], Step [71850/136675], Loss: 5.5735\n",
      "Epoch [3/5], Step [71925/136675], Loss: 5.3518\n",
      "Epoch [3/5], Step [72000/136675], Loss: 5.2430\n",
      "Epoch [3/5], Step [72075/136675], Loss: 5.0470\n",
      "Epoch [3/5], Step [72150/136675], Loss: 5.4815\n",
      "Epoch [3/5], Step [72225/136675], Loss: 5.3479\n",
      "Epoch [3/5], Step [72300/136675], Loss: 5.3415\n",
      "Epoch [3/5], Step [72375/136675], Loss: 5.3649\n",
      "Epoch [3/5], Step [72450/136675], Loss: 5.4099\n",
      "Epoch [3/5], Step [72525/136675], Loss: 5.2976\n",
      "Epoch [3/5], Step [72600/136675], Loss: 5.3419\n",
      "Epoch [3/5], Step [72675/136675], Loss: 5.2775\n",
      "Epoch [3/5], Step [72750/136675], Loss: 5.3445\n",
      "Epoch [3/5], Step [72825/136675], Loss: 5.3256\n",
      "Epoch [3/5], Step [72900/136675], Loss: 5.3283\n",
      "Epoch [3/5], Step [72975/136675], Loss: 5.3821\n",
      "Epoch [3/5], Step [73050/136675], Loss: 5.3471\n",
      "Epoch [3/5], Step [73125/136675], Loss: 5.4832\n",
      "Epoch [3/5], Step [73200/136675], Loss: 5.1792\n",
      "Epoch [3/5], Step [73275/136675], Loss: 5.2839\n",
      "Epoch [3/5], Step [73350/136675], Loss: 5.1075\n",
      "Epoch [3/5], Step [73425/136675], Loss: 5.1289\n",
      "Epoch [3/5], Step [73500/136675], Loss: 5.3053\n",
      "Epoch [3/5], Step [73575/136675], Loss: 5.3116\n",
      "Epoch [3/5], Step [73650/136675], Loss: 4.8875\n",
      "Epoch [3/5], Step [73725/136675], Loss: 5.4863\n",
      "Epoch [3/5], Step [73800/136675], Loss: 5.0772\n",
      "Epoch [3/5], Step [73875/136675], Loss: 5.1509\n",
      "Epoch [3/5], Step [73950/136675], Loss: 5.2875\n",
      "Epoch [3/5], Step [74025/136675], Loss: 5.0985\n",
      "Epoch [3/5], Step [74100/136675], Loss: 5.0120\n",
      "Epoch [3/5], Step [74175/136675], Loss: 5.5536\n",
      "Epoch [3/5], Step [74250/136675], Loss: 5.3053\n",
      "Epoch [3/5], Step [74325/136675], Loss: 5.3072\n",
      "Epoch [3/5], Step [74400/136675], Loss: 5.4184\n",
      "Epoch [3/5], Step [74475/136675], Loss: 5.0738\n",
      "Epoch [3/5], Step [74550/136675], Loss: 5.5144\n",
      "Epoch [3/5], Step [74625/136675], Loss: 5.2844\n",
      "Epoch [3/5], Step [74700/136675], Loss: 5.2855\n",
      "Epoch [3/5], Step [74775/136675], Loss: 5.2265\n",
      "Epoch [3/5], Step [74850/136675], Loss: 5.1550\n",
      "Epoch [3/5], Step [74925/136675], Loss: 5.4176\n",
      "Epoch [3/5], Step [75000/136675], Loss: 5.4970\n",
      "Epoch [3/5], Step [75075/136675], Loss: 5.0900\n",
      "Epoch [3/5], Step [75150/136675], Loss: 5.5980\n",
      "Epoch [3/5], Step [75225/136675], Loss: 4.8991\n",
      "Epoch [3/5], Step [75300/136675], Loss: 5.2325\n",
      "Epoch [3/5], Step [75375/136675], Loss: 5.4521\n",
      "Epoch [3/5], Step [75450/136675], Loss: 5.0591\n",
      "Epoch [3/5], Step [75525/136675], Loss: 5.2380\n",
      "Epoch [3/5], Step [75600/136675], Loss: 5.4232\n",
      "Epoch [3/5], Step [75675/136675], Loss: 5.4279\n",
      "Epoch [3/5], Step [75750/136675], Loss: 4.9908\n",
      "Epoch [3/5], Step [75825/136675], Loss: 5.3914\n",
      "Epoch [3/5], Step [75900/136675], Loss: 5.3864\n",
      "Epoch [3/5], Step [75975/136675], Loss: 5.0689\n",
      "Epoch [3/5], Step [76050/136675], Loss: 5.4669\n",
      "Epoch [3/5], Step [76125/136675], Loss: 5.3079\n",
      "Epoch [3/5], Step [76200/136675], Loss: 5.4246\n",
      "Epoch [3/5], Step [76275/136675], Loss: 5.4125\n",
      "Epoch [3/5], Step [76350/136675], Loss: 5.4201\n",
      "Epoch [3/5], Step [76425/136675], Loss: 5.3670\n",
      "Epoch [3/5], Step [76500/136675], Loss: 5.1420\n",
      "Epoch [3/5], Step [76575/136675], Loss: 5.3321\n",
      "Epoch [3/5], Step [76650/136675], Loss: 5.1027\n",
      "Epoch [3/5], Step [76725/136675], Loss: 5.3433\n",
      "Epoch [3/5], Step [76800/136675], Loss: 5.1314\n",
      "Epoch [3/5], Step [76875/136675], Loss: 5.3033\n",
      "Epoch [3/5], Step [76950/136675], Loss: 5.1987\n",
      "Epoch [3/5], Step [77025/136675], Loss: 5.2424\n",
      "Epoch [3/5], Step [77100/136675], Loss: 5.3187\n",
      "Epoch [3/5], Step [77175/136675], Loss: 5.0799\n",
      "Epoch [3/5], Step [77250/136675], Loss: 5.4409\n",
      "Epoch [3/5], Step [77325/136675], Loss: 5.5055\n",
      "Epoch [3/5], Step [77400/136675], Loss: 5.1469\n",
      "Epoch [3/5], Step [77475/136675], Loss: 5.4624\n",
      "Epoch [3/5], Step [77550/136675], Loss: 5.1357\n",
      "Epoch [3/5], Step [77625/136675], Loss: 5.2890\n",
      "Epoch [3/5], Step [77700/136675], Loss: 5.1473\n",
      "Epoch [3/5], Step [77775/136675], Loss: 5.4943\n",
      "Epoch [3/5], Step [77850/136675], Loss: 5.0444\n",
      "Epoch [3/5], Step [77925/136675], Loss: 5.3736\n",
      "Epoch [3/5], Step [78000/136675], Loss: 5.3289\n",
      "Epoch [3/5], Step [78075/136675], Loss: 5.3361\n",
      "Epoch [3/5], Step [78150/136675], Loss: 5.1238\n",
      "Epoch [3/5], Step [78225/136675], Loss: 5.1840\n",
      "Epoch [3/5], Step [78300/136675], Loss: 4.9861\n",
      "Epoch [3/5], Step [78375/136675], Loss: 5.6723\n",
      "Epoch [3/5], Step [78450/136675], Loss: 5.2967\n",
      "Epoch [3/5], Step [78525/136675], Loss: 4.9643\n",
      "Epoch [3/5], Step [78600/136675], Loss: 4.8775\n",
      "Epoch [3/5], Step [78675/136675], Loss: 5.0757\n",
      "Epoch [3/5], Step [78750/136675], Loss: 5.4563\n",
      "Epoch [3/5], Step [78825/136675], Loss: 5.4327\n",
      "Epoch [3/5], Step [78900/136675], Loss: 5.0846\n",
      "Epoch [3/5], Step [78975/136675], Loss: 5.2636\n",
      "Epoch [3/5], Step [79050/136675], Loss: 5.0046\n",
      "Epoch [3/5], Step [79125/136675], Loss: 5.1604\n",
      "Epoch [3/5], Step [79200/136675], Loss: 5.3728\n",
      "Epoch [3/5], Step [79275/136675], Loss: 5.3431\n",
      "Epoch [3/5], Step [79350/136675], Loss: 5.4017\n",
      "Epoch [3/5], Step [79425/136675], Loss: 5.3470\n",
      "Epoch [3/5], Step [79500/136675], Loss: 5.1659\n",
      "Epoch [3/5], Step [79575/136675], Loss: 5.2675\n",
      "Epoch [3/5], Step [79650/136675], Loss: 5.1710\n",
      "Epoch [3/5], Step [79725/136675], Loss: 5.4983\n",
      "Epoch [3/5], Step [79800/136675], Loss: 5.1921\n",
      "Epoch [3/5], Step [79875/136675], Loss: 5.5016\n",
      "Epoch [3/5], Step [79950/136675], Loss: 5.5605\n",
      "Validation perplexity: 160.52412717485205\n",
      "Epoch [3/5], Step [80025/136675], Loss: 5.2385\n",
      "Epoch [3/5], Step [80100/136675], Loss: 5.4623\n",
      "Epoch [3/5], Step [80175/136675], Loss: 5.0172\n",
      "Epoch [3/5], Step [80250/136675], Loss: 5.2020\n",
      "Epoch [3/5], Step [80325/136675], Loss: 5.3076\n",
      "Epoch [3/5], Step [80400/136675], Loss: 5.4124\n",
      "Epoch [3/5], Step [80475/136675], Loss: 5.1683\n",
      "Epoch [3/5], Step [80550/136675], Loss: 5.1985\n",
      "Epoch [3/5], Step [80625/136675], Loss: 5.2227\n",
      "Epoch [3/5], Step [80700/136675], Loss: 5.3086\n",
      "Epoch [3/5], Step [80775/136675], Loss: 5.1307\n",
      "Epoch [3/5], Step [80850/136675], Loss: 5.4316\n",
      "Epoch [3/5], Step [80925/136675], Loss: 5.5470\n",
      "Epoch [3/5], Step [81000/136675], Loss: 5.2676\n",
      "Epoch [3/5], Step [81075/136675], Loss: 5.4020\n",
      "Epoch [3/5], Step [81150/136675], Loss: 5.1611\n",
      "Epoch [3/5], Step [81225/136675], Loss: 5.5167\n",
      "Epoch [3/5], Step [81300/136675], Loss: 5.2331\n",
      "Epoch [3/5], Step [81375/136675], Loss: 5.4101\n",
      "Epoch [3/5], Step [81450/136675], Loss: 5.4549\n",
      "Epoch [3/5], Step [81525/136675], Loss: 5.3847\n",
      "Epoch [3/5], Step [81600/136675], Loss: 5.5063\n",
      "Epoch [3/5], Step [81675/136675], Loss: 5.1766\n",
      "Epoch [3/5], Step [81750/136675], Loss: 4.8757\n",
      "Epoch [3/5], Step [81825/136675], Loss: 5.3018\n",
      "Epoch [3/5], Step [81900/136675], Loss: 5.0072\n",
      "Epoch [3/5], Step [81975/136675], Loss: 5.3241\n",
      "Epoch [3/5], Step [82050/136675], Loss: 5.1162\n",
      "Epoch [3/5], Step [82125/136675], Loss: 5.2235\n",
      "Epoch [3/5], Step [82200/136675], Loss: 5.4864\n",
      "Epoch [3/5], Step [82275/136675], Loss: 5.3617\n",
      "Epoch [3/5], Step [82350/136675], Loss: 5.0853\n",
      "Epoch [3/5], Step [82425/136675], Loss: 5.4976\n",
      "Epoch [3/5], Step [82500/136675], Loss: 5.2746\n",
      "Epoch [3/5], Step [82575/136675], Loss: 5.3858\n",
      "Epoch [3/5], Step [82650/136675], Loss: 5.2336\n",
      "Epoch [3/5], Step [82725/136675], Loss: 5.8104\n",
      "Epoch [3/5], Step [82800/136675], Loss: 5.2857\n",
      "Epoch [3/5], Step [82875/136675], Loss: 4.9920\n",
      "Epoch [3/5], Step [82950/136675], Loss: 5.4869\n",
      "Epoch [3/5], Step [83025/136675], Loss: 5.3191\n",
      "Epoch [3/5], Step [83100/136675], Loss: 5.0093\n",
      "Epoch [3/5], Step [83175/136675], Loss: 5.4161\n",
      "Epoch [3/5], Step [83250/136675], Loss: 5.2517\n",
      "Epoch [3/5], Step [83325/136675], Loss: 5.2715\n",
      "Epoch [3/5], Step [83400/136675], Loss: 5.4274\n",
      "Epoch [3/5], Step [83475/136675], Loss: 5.0774\n",
      "Epoch [3/5], Step [83550/136675], Loss: 5.1433\n",
      "Epoch [3/5], Step [83625/136675], Loss: 5.3971\n",
      "Epoch [3/5], Step [83700/136675], Loss: 5.4617\n",
      "Epoch [3/5], Step [83775/136675], Loss: 5.1927\n",
      "Epoch [3/5], Step [83850/136675], Loss: 5.6213\n",
      "Epoch [3/5], Step [83925/136675], Loss: 5.3675\n",
      "Epoch [3/5], Step [84000/136675], Loss: 5.4022\n",
      "Epoch [3/5], Step [84075/136675], Loss: 5.4871\n",
      "Epoch [3/5], Step [84150/136675], Loss: 5.2502\n",
      "Epoch [3/5], Step [84225/136675], Loss: 5.3738\n",
      "Epoch [3/5], Step [84300/136675], Loss: 5.1268\n",
      "Epoch [3/5], Step [84375/136675], Loss: 5.3842\n",
      "Epoch [3/5], Step [84450/136675], Loss: 5.3977\n",
      "Epoch [3/5], Step [84525/136675], Loss: 5.2142\n",
      "Epoch [3/5], Step [84600/136675], Loss: 5.4910\n",
      "Epoch [3/5], Step [84675/136675], Loss: 5.4387\n",
      "Epoch [3/5], Step [84750/136675], Loss: 5.2043\n",
      "Epoch [3/5], Step [84825/136675], Loss: 5.4100\n",
      "Epoch [3/5], Step [84900/136675], Loss: 5.1681\n",
      "Epoch [3/5], Step [84975/136675], Loss: 5.3133\n",
      "Epoch [3/5], Step [85050/136675], Loss: 5.0518\n",
      "Epoch [3/5], Step [85125/136675], Loss: 5.4357\n",
      "Epoch [3/5], Step [85200/136675], Loss: 5.1469\n",
      "Epoch [3/5], Step [85275/136675], Loss: 5.1417\n",
      "Epoch [3/5], Step [85350/136675], Loss: 5.2006\n",
      "Epoch [3/5], Step [85425/136675], Loss: 5.7125\n",
      "Epoch [3/5], Step [85500/136675], Loss: 5.4115\n",
      "Epoch [3/5], Step [85575/136675], Loss: 5.1481\n",
      "Epoch [3/5], Step [85650/136675], Loss: 5.3549\n",
      "Epoch [3/5], Step [85725/136675], Loss: 4.9486\n",
      "Epoch [3/5], Step [85800/136675], Loss: 5.0720\n",
      "Epoch [3/5], Step [85875/136675], Loss: 5.4303\n",
      "Epoch [3/5], Step [85950/136675], Loss: 5.4324\n",
      "Epoch [3/5], Step [86025/136675], Loss: 5.4914\n",
      "Epoch [3/5], Step [86100/136675], Loss: 5.5483\n",
      "Epoch [3/5], Step [86175/136675], Loss: 5.0944\n",
      "Epoch [3/5], Step [86250/136675], Loss: 5.3449\n",
      "Epoch [3/5], Step [86325/136675], Loss: 5.4853\n",
      "Epoch [3/5], Step [86400/136675], Loss: 5.3500\n",
      "Epoch [3/5], Step [86475/136675], Loss: 5.4395\n",
      "Epoch [3/5], Step [86550/136675], Loss: 5.4486\n",
      "Epoch [3/5], Step [86625/136675], Loss: 5.1866\n",
      "Epoch [3/5], Step [86700/136675], Loss: 5.2685\n",
      "Epoch [3/5], Step [86775/136675], Loss: 5.5506\n",
      "Epoch [3/5], Step [86850/136675], Loss: 5.5955\n",
      "Epoch [3/5], Step [86925/136675], Loss: 5.2500\n",
      "Epoch [3/5], Step [87000/136675], Loss: 4.9900\n",
      "Epoch [3/5], Step [87075/136675], Loss: 5.3158\n",
      "Epoch [3/5], Step [87150/136675], Loss: 5.2861\n",
      "Epoch [3/5], Step [87225/136675], Loss: 5.3620\n",
      "Epoch [3/5], Step [87300/136675], Loss: 5.0585\n",
      "Epoch [3/5], Step [87375/136675], Loss: 5.5221\n",
      "Epoch [3/5], Step [87450/136675], Loss: 5.2432\n",
      "Epoch [3/5], Step [87525/136675], Loss: 5.2990\n",
      "Epoch [3/5], Step [87600/136675], Loss: 5.4082\n",
      "Epoch [3/5], Step [87675/136675], Loss: 5.4659\n",
      "Epoch [3/5], Step [87750/136675], Loss: 5.4426\n",
      "Epoch [3/5], Step [87825/136675], Loss: 5.1599\n",
      "Epoch [3/5], Step [87900/136675], Loss: 5.4765\n",
      "Epoch [3/5], Step [87975/136675], Loss: 5.3007\n",
      "Epoch [3/5], Step [88050/136675], Loss: 5.1514\n",
      "Epoch [3/5], Step [88125/136675], Loss: 5.4803\n",
      "Epoch [3/5], Step [88200/136675], Loss: 5.3618\n",
      "Epoch [3/5], Step [88275/136675], Loss: 5.0621\n",
      "Epoch [3/5], Step [88350/136675], Loss: 5.4016\n",
      "Epoch [3/5], Step [88425/136675], Loss: 5.1138\n",
      "Epoch [3/5], Step [88500/136675], Loss: 5.4387\n",
      "Epoch [3/5], Step [88575/136675], Loss: 5.4879\n",
      "Epoch [3/5], Step [88650/136675], Loss: 5.5330\n",
      "Epoch [3/5], Step [88725/136675], Loss: 5.2538\n",
      "Epoch [3/5], Step [88800/136675], Loss: 5.3284\n",
      "Epoch [3/5], Step [88875/136675], Loss: 5.3147\n",
      "Epoch [3/5], Step [88950/136675], Loss: 5.0959\n",
      "Epoch [3/5], Step [89025/136675], Loss: 5.1762\n",
      "Epoch [3/5], Step [89100/136675], Loss: 5.3417\n",
      "Epoch [3/5], Step [89175/136675], Loss: 5.4249\n",
      "Epoch [3/5], Step [89250/136675], Loss: 5.4477\n",
      "Epoch [3/5], Step [89325/136675], Loss: 5.2813\n",
      "Epoch [3/5], Step [89400/136675], Loss: 5.3097\n",
      "Epoch [3/5], Step [89475/136675], Loss: 5.3067\n",
      "Epoch [3/5], Step [89550/136675], Loss: 5.6501\n",
      "Epoch [3/5], Step [89625/136675], Loss: 5.4328\n",
      "Epoch [3/5], Step [89700/136675], Loss: 5.2545\n",
      "Epoch [3/5], Step [89775/136675], Loss: 5.3922\n",
      "Epoch [3/5], Step [89850/136675], Loss: 5.3506\n",
      "Epoch [3/5], Step [89925/136675], Loss: 5.5316\n",
      "Epoch [3/5], Step [90000/136675], Loss: 5.5850\n",
      "Validation perplexity: 159.72150168012868\n",
      "Epoch [3/5], Step [90075/136675], Loss: 5.5368\n",
      "Epoch [3/5], Step [90150/136675], Loss: 5.1518\n",
      "Epoch [3/5], Step [90225/136675], Loss: 5.1275\n",
      "Epoch [3/5], Step [90300/136675], Loss: 5.3846\n",
      "Epoch [3/5], Step [90375/136675], Loss: 5.1004\n",
      "Epoch [3/5], Step [90450/136675], Loss: 5.0962\n",
      "Epoch [3/5], Step [90525/136675], Loss: 5.3524\n",
      "Epoch [3/5], Step [90600/136675], Loss: 5.1591\n",
      "Epoch [3/5], Step [90675/136675], Loss: 5.3773\n",
      "Epoch [3/5], Step [90750/136675], Loss: 5.0439\n",
      "Epoch [3/5], Step [90825/136675], Loss: 5.2256\n",
      "Epoch [3/5], Step [90900/136675], Loss: 5.4178\n",
      "Epoch [3/5], Step [90975/136675], Loss: 5.2186\n",
      "Epoch [3/5], Step [91050/136675], Loss: 5.5371\n",
      "Epoch [3/5], Step [91125/136675], Loss: 5.7179\n",
      "Epoch [3/5], Step [91200/136675], Loss: 5.8179\n",
      "Epoch [3/5], Step [91275/136675], Loss: 5.3638\n",
      "Epoch [3/5], Step [91350/136675], Loss: 5.2719\n",
      "Epoch [3/5], Step [91425/136675], Loss: 4.9495\n",
      "Epoch [3/5], Step [91500/136675], Loss: 5.1280\n",
      "Epoch [3/5], Step [91575/136675], Loss: 4.8669\n",
      "Epoch [3/5], Step [91650/136675], Loss: 5.4929\n",
      "Epoch [3/5], Step [91725/136675], Loss: 5.5703\n",
      "Epoch [3/5], Step [91800/136675], Loss: 5.4083\n",
      "Epoch [3/5], Step [91875/136675], Loss: 5.0956\n",
      "Epoch [3/5], Step [91950/136675], Loss: 5.3872\n",
      "Epoch [3/5], Step [92025/136675], Loss: 5.5162\n",
      "Epoch [3/5], Step [92100/136675], Loss: 5.4811\n",
      "Epoch [3/5], Step [92175/136675], Loss: 5.2698\n",
      "Epoch [3/5], Step [92250/136675], Loss: 5.2263\n",
      "Epoch [3/5], Step [92325/136675], Loss: 5.5551\n",
      "Epoch [3/5], Step [92400/136675], Loss: 5.0156\n",
      "Epoch [3/5], Step [92475/136675], Loss: 5.2005\n",
      "Epoch [3/5], Step [92550/136675], Loss: 5.1379\n",
      "Epoch [3/5], Step [92625/136675], Loss: 5.7140\n",
      "Epoch [3/5], Step [92700/136675], Loss: 5.5390\n",
      "Epoch [3/5], Step [92775/136675], Loss: 5.0962\n",
      "Epoch [3/5], Step [92850/136675], Loss: 5.2248\n",
      "Epoch [3/5], Step [92925/136675], Loss: 5.1443\n",
      "Epoch [3/5], Step [93000/136675], Loss: 5.4482\n",
      "Epoch [3/5], Step [93075/136675], Loss: 5.5236\n",
      "Epoch [3/5], Step [93150/136675], Loss: 4.7678\n",
      "Epoch [3/5], Step [93225/136675], Loss: 5.2566\n",
      "Epoch [3/5], Step [93300/136675], Loss: 5.2618\n",
      "Epoch [3/5], Step [93375/136675], Loss: 5.2436\n",
      "Epoch [3/5], Step [93450/136675], Loss: 5.1802\n",
      "Epoch [3/5], Step [93525/136675], Loss: 5.3360\n",
      "Epoch [3/5], Step [93600/136675], Loss: 5.6296\n",
      "Epoch [3/5], Step [93675/136675], Loss: 5.1382\n",
      "Epoch [3/5], Step [93750/136675], Loss: 5.2427\n",
      "Epoch [3/5], Step [93825/136675], Loss: 5.4238\n",
      "Epoch [3/5], Step [93900/136675], Loss: 5.5053\n",
      "Epoch [3/5], Step [93975/136675], Loss: 4.9941\n",
      "Epoch [3/5], Step [94050/136675], Loss: 5.1280\n",
      "Epoch [3/5], Step [94125/136675], Loss: 5.5457\n",
      "Epoch [3/5], Step [94200/136675], Loss: 5.3286\n",
      "Epoch [3/5], Step [94275/136675], Loss: 5.2183\n",
      "Epoch [3/5], Step [94350/136675], Loss: 5.2875\n",
      "Epoch [3/5], Step [94425/136675], Loss: 5.6034\n",
      "Epoch [3/5], Step [94500/136675], Loss: 5.1896\n",
      "Epoch [3/5], Step [94575/136675], Loss: 5.1246\n",
      "Epoch [3/5], Step [94650/136675], Loss: 5.1537\n",
      "Epoch [3/5], Step [94725/136675], Loss: 5.1434\n",
      "Epoch [3/5], Step [94800/136675], Loss: 5.4586\n",
      "Epoch [3/5], Step [94875/136675], Loss: 5.3980\n",
      "Epoch [3/5], Step [94950/136675], Loss: 5.2838\n",
      "Epoch [3/5], Step [95025/136675], Loss: 5.4440\n",
      "Epoch [3/5], Step [95100/136675], Loss: 5.3632\n",
      "Epoch [3/5], Step [95175/136675], Loss: 5.1410\n",
      "Epoch [3/5], Step [95250/136675], Loss: 5.5513\n",
      "Epoch [3/5], Step [95325/136675], Loss: 5.2128\n",
      "Epoch [3/5], Step [95400/136675], Loss: 5.1645\n",
      "Epoch [3/5], Step [95475/136675], Loss: 5.4731\n",
      "Epoch [3/5], Step [95550/136675], Loss: 5.3026\n",
      "Epoch [3/5], Step [95625/136675], Loss: 5.4006\n",
      "Epoch [3/5], Step [95700/136675], Loss: 5.4359\n",
      "Epoch [3/5], Step [95775/136675], Loss: 5.1753\n",
      "Epoch [3/5], Step [95850/136675], Loss: 5.1354\n",
      "Epoch [3/5], Step [95925/136675], Loss: 5.1827\n",
      "Epoch [3/5], Step [96000/136675], Loss: 5.0943\n",
      "Epoch [3/5], Step [96075/136675], Loss: 5.2826\n",
      "Epoch [3/5], Step [96150/136675], Loss: 5.3787\n",
      "Epoch [3/5], Step [96225/136675], Loss: 5.2359\n",
      "Epoch [3/5], Step [96300/136675], Loss: 5.3310\n",
      "Epoch [3/5], Step [96375/136675], Loss: 5.5222\n",
      "Epoch [3/5], Step [96450/136675], Loss: 5.1580\n",
      "Epoch [3/5], Step [96525/136675], Loss: 5.2867\n",
      "Epoch [3/5], Step [96600/136675], Loss: 5.2695\n",
      "Epoch [3/5], Step [96675/136675], Loss: 4.9374\n",
      "Epoch [3/5], Step [96750/136675], Loss: 5.2847\n",
      "Epoch [3/5], Step [96825/136675], Loss: 5.3403\n",
      "Epoch [3/5], Step [96900/136675], Loss: 5.1996\n",
      "Epoch [3/5], Step [96975/136675], Loss: 5.4471\n",
      "Epoch [3/5], Step [97050/136675], Loss: 5.3756\n",
      "Epoch [3/5], Step [97125/136675], Loss: 5.3161\n",
      "Epoch [3/5], Step [97200/136675], Loss: 5.0517\n",
      "Epoch [3/5], Step [97275/136675], Loss: 5.4324\n",
      "Epoch [3/5], Step [97350/136675], Loss: 5.4925\n",
      "Epoch [3/5], Step [97425/136675], Loss: 5.2068\n",
      "Epoch [3/5], Step [97500/136675], Loss: 5.1361\n",
      "Epoch [3/5], Step [97575/136675], Loss: 5.7632\n",
      "Epoch [3/5], Step [97650/136675], Loss: 5.2627\n",
      "Epoch [3/5], Step [97725/136675], Loss: 5.2991\n",
      "Epoch [3/5], Step [97800/136675], Loss: 5.2140\n",
      "Epoch [3/5], Step [97875/136675], Loss: 5.3433\n",
      "Epoch [3/5], Step [97950/136675], Loss: 5.4257\n",
      "Epoch [3/5], Step [98025/136675], Loss: 5.6671\n",
      "Epoch [3/5], Step [98100/136675], Loss: 5.3903\n",
      "Epoch [3/5], Step [98175/136675], Loss: 5.1922\n",
      "Epoch [3/5], Step [98250/136675], Loss: 5.1780\n",
      "Epoch [3/5], Step [98325/136675], Loss: 5.6946\n",
      "Epoch [3/5], Step [98400/136675], Loss: 5.3970\n",
      "Epoch [3/5], Step [98475/136675], Loss: 5.5397\n",
      "Epoch [3/5], Step [98550/136675], Loss: 5.2351\n",
      "Epoch [3/5], Step [98625/136675], Loss: 5.1554\n",
      "Epoch [3/5], Step [98700/136675], Loss: 5.0202\n",
      "Epoch [3/5], Step [98775/136675], Loss: 5.5809\n",
      "Epoch [3/5], Step [98850/136675], Loss: 5.4045\n",
      "Epoch [3/5], Step [98925/136675], Loss: 5.3029\n",
      "Epoch [3/5], Step [99000/136675], Loss: 5.3072\n",
      "Epoch [3/5], Step [99075/136675], Loss: 5.2583\n",
      "Epoch [3/5], Step [99150/136675], Loss: 5.2417\n",
      "Epoch [3/5], Step [99225/136675], Loss: 5.3073\n",
      "Epoch [3/5], Step [99300/136675], Loss: 5.3819\n",
      "Epoch [3/5], Step [99375/136675], Loss: 5.0174\n",
      "Epoch [3/5], Step [99450/136675], Loss: 4.8628\n",
      "Epoch [3/5], Step [99525/136675], Loss: 5.1994\n",
      "Epoch [3/5], Step [99600/136675], Loss: 5.2915\n",
      "Epoch [3/5], Step [99675/136675], Loss: 5.6349\n",
      "Epoch [3/5], Step [99750/136675], Loss: 5.3144\n",
      "Epoch [3/5], Step [99825/136675], Loss: 5.2357\n",
      "Epoch [3/5], Step [99900/136675], Loss: 5.1243\n",
      "Epoch [3/5], Step [99975/136675], Loss: 5.1788\n",
      "Validation perplexity: 159.636715959362\n",
      "Epoch [3/5], Step [100050/136675], Loss: 5.0544\n",
      "Epoch [3/5], Step [100125/136675], Loss: 5.4346\n",
      "Epoch [3/5], Step [100200/136675], Loss: 5.3115\n",
      "Epoch [3/5], Step [100275/136675], Loss: 5.4535\n",
      "Epoch [3/5], Step [100350/136675], Loss: 5.5985\n",
      "Epoch [3/5], Step [100425/136675], Loss: 5.3883\n",
      "Epoch [3/5], Step [100500/136675], Loss: 5.2354\n",
      "Epoch [3/5], Step [100575/136675], Loss: 5.0925\n",
      "Epoch [3/5], Step [100650/136675], Loss: 5.2633\n",
      "Epoch [3/5], Step [100725/136675], Loss: 5.2076\n",
      "Epoch [3/5], Step [100800/136675], Loss: 5.3859\n",
      "Epoch [3/5], Step [100875/136675], Loss: 5.3923\n",
      "Epoch [3/5], Step [100950/136675], Loss: 5.4187\n",
      "Epoch [3/5], Step [101025/136675], Loss: 5.1885\n",
      "Epoch [3/5], Step [101100/136675], Loss: 5.4327\n",
      "Epoch [3/5], Step [101175/136675], Loss: 5.1368\n",
      "Epoch [3/5], Step [101250/136675], Loss: 5.2944\n",
      "Epoch [3/5], Step [101325/136675], Loss: 5.4522\n",
      "Epoch [3/5], Step [101400/136675], Loss: 5.0224\n",
      "Epoch [3/5], Step [101475/136675], Loss: 5.0785\n",
      "Epoch [3/5], Step [101550/136675], Loss: 5.3766\n",
      "Epoch [3/5], Step [101625/136675], Loss: 5.1741\n",
      "Epoch [3/5], Step [101700/136675], Loss: 5.5058\n",
      "Epoch [3/5], Step [101775/136675], Loss: 5.3406\n",
      "Epoch [3/5], Step [101850/136675], Loss: 5.3036\n",
      "Epoch [3/5], Step [101925/136675], Loss: 4.9815\n",
      "Epoch [3/5], Step [102000/136675], Loss: 5.6157\n",
      "Epoch [3/5], Step [102075/136675], Loss: 5.3064\n",
      "Epoch [3/5], Step [102150/136675], Loss: 5.3516\n",
      "Epoch [3/5], Step [102225/136675], Loss: 5.4389\n",
      "Epoch [3/5], Step [102300/136675], Loss: 5.7323\n",
      "Epoch [3/5], Step [102375/136675], Loss: 4.7033\n",
      "Epoch [3/5], Step [102450/136675], Loss: 5.2348\n",
      "Epoch [3/5], Step [102525/136675], Loss: 5.3418\n",
      "Epoch [3/5], Step [102600/136675], Loss: 5.2740\n",
      "Epoch [3/5], Step [102675/136675], Loss: 5.3448\n",
      "Epoch [3/5], Step [102750/136675], Loss: 5.4405\n",
      "Epoch [3/5], Step [102825/136675], Loss: 5.3953\n",
      "Epoch [3/5], Step [102900/136675], Loss: 5.4306\n",
      "Epoch [3/5], Step [102975/136675], Loss: 5.3636\n",
      "Epoch [3/5], Step [103050/136675], Loss: 5.1588\n",
      "Epoch [3/5], Step [103125/136675], Loss: 5.2230\n",
      "Epoch [3/5], Step [103200/136675], Loss: 5.0715\n",
      "Epoch [3/5], Step [103275/136675], Loss: 5.3403\n",
      "Epoch [3/5], Step [103350/136675], Loss: 5.0374\n",
      "Epoch [3/5], Step [103425/136675], Loss: 5.2043\n",
      "Epoch [3/5], Step [103500/136675], Loss: 5.1059\n",
      "Epoch [3/5], Step [103575/136675], Loss: 5.2935\n",
      "Epoch [3/5], Step [103650/136675], Loss: 5.1223\n",
      "Epoch [3/5], Step [103725/136675], Loss: 5.3625\n",
      "Epoch [3/5], Step [103800/136675], Loss: 5.0847\n",
      "Epoch [3/5], Step [103875/136675], Loss: 5.1794\n",
      "Epoch [3/5], Step [103950/136675], Loss: 5.3536\n",
      "Epoch [3/5], Step [104025/136675], Loss: 5.0583\n",
      "Epoch [3/5], Step [104100/136675], Loss: 5.3695\n",
      "Epoch [3/5], Step [104175/136675], Loss: 5.0153\n",
      "Epoch [3/5], Step [104250/136675], Loss: 5.1194\n",
      "Epoch [3/5], Step [104325/136675], Loss: 5.4481\n",
      "Epoch [3/5], Step [104400/136675], Loss: 5.3663\n",
      "Epoch [3/5], Step [104475/136675], Loss: 4.9435\n",
      "Epoch [3/5], Step [104550/136675], Loss: 5.3188\n",
      "Epoch [3/5], Step [104625/136675], Loss: 5.6415\n",
      "Epoch [3/5], Step [104700/136675], Loss: 5.2902\n",
      "Epoch [3/5], Step [104775/136675], Loss: 4.9719\n",
      "Epoch [3/5], Step [104850/136675], Loss: 5.3954\n",
      "Epoch [3/5], Step [104925/136675], Loss: 5.5137\n",
      "Epoch [3/5], Step [105000/136675], Loss: 5.2827\n",
      "Epoch [3/5], Step [105075/136675], Loss: 5.1669\n",
      "Epoch [3/5], Step [105150/136675], Loss: 5.2023\n",
      "Epoch [3/5], Step [105225/136675], Loss: 5.0968\n",
      "Epoch [3/5], Step [105300/136675], Loss: 5.4893\n",
      "Epoch [3/5], Step [105375/136675], Loss: 5.0578\n",
      "Epoch [3/5], Step [105450/136675], Loss: 5.3961\n",
      "Epoch [3/5], Step [105525/136675], Loss: 5.1696\n",
      "Epoch [3/5], Step [105600/136675], Loss: 5.1558\n",
      "Epoch [3/5], Step [105675/136675], Loss: 5.1947\n",
      "Epoch [3/5], Step [105750/136675], Loss: 5.2865\n",
      "Epoch [3/5], Step [105825/136675], Loss: 5.4122\n",
      "Epoch [3/5], Step [105900/136675], Loss: 5.3605\n",
      "Epoch [3/5], Step [105975/136675], Loss: 5.5286\n",
      "Epoch [3/5], Step [106050/136675], Loss: 5.3345\n",
      "Epoch [3/5], Step [106125/136675], Loss: 5.2735\n",
      "Epoch [3/5], Step [106200/136675], Loss: 5.1343\n",
      "Epoch [3/5], Step [106275/136675], Loss: 5.3514\n",
      "Epoch [3/5], Step [106350/136675], Loss: 5.3155\n",
      "Epoch [3/5], Step [106425/136675], Loss: 5.1456\n",
      "Epoch [3/5], Step [106500/136675], Loss: 5.3002\n",
      "Epoch [3/5], Step [106575/136675], Loss: 5.4010\n",
      "Epoch [3/5], Step [106650/136675], Loss: 5.4072\n",
      "Epoch [3/5], Step [106725/136675], Loss: 5.0637\n",
      "Epoch [3/5], Step [106800/136675], Loss: 5.1881\n",
      "Epoch [3/5], Step [106875/136675], Loss: 5.3300\n",
      "Epoch [3/5], Step [106950/136675], Loss: 5.0298\n",
      "Epoch [3/5], Step [107025/136675], Loss: 5.4680\n",
      "Epoch [3/5], Step [107100/136675], Loss: 5.2712\n",
      "Epoch [3/5], Step [107175/136675], Loss: 5.2172\n",
      "Epoch [3/5], Step [107250/136675], Loss: 5.4296\n",
      "Epoch [3/5], Step [107325/136675], Loss: 5.0439\n",
      "Epoch [3/5], Step [107400/136675], Loss: 5.1278\n",
      "Epoch [3/5], Step [107475/136675], Loss: 5.0945\n",
      "Epoch [3/5], Step [107550/136675], Loss: 5.3156\n",
      "Epoch [3/5], Step [107625/136675], Loss: 5.2813\n",
      "Epoch [3/5], Step [107700/136675], Loss: 5.2640\n",
      "Epoch [3/5], Step [107775/136675], Loss: 5.2033\n",
      "Epoch [3/5], Step [107850/136675], Loss: 4.9802\n",
      "Epoch [3/5], Step [107925/136675], Loss: 5.2221\n",
      "Epoch [3/5], Step [108000/136675], Loss: 5.1561\n",
      "Epoch [3/5], Step [108075/136675], Loss: 5.3699\n",
      "Epoch [3/5], Step [108150/136675], Loss: 5.2011\n",
      "Epoch [3/5], Step [108225/136675], Loss: 5.3403\n",
      "Epoch [3/5], Step [108300/136675], Loss: 5.1640\n",
      "Epoch [3/5], Step [108375/136675], Loss: 5.4964\n",
      "Epoch [3/5], Step [108450/136675], Loss: 5.2014\n",
      "Epoch [3/5], Step [108525/136675], Loss: 5.4614\n",
      "Epoch [3/5], Step [108600/136675], Loss: 5.2853\n",
      "Epoch [3/5], Step [108675/136675], Loss: 5.1888\n",
      "Epoch [3/5], Step [108750/136675], Loss: 5.3278\n",
      "Epoch [3/5], Step [108825/136675], Loss: 5.3906\n",
      "Epoch [3/5], Step [108900/136675], Loss: 5.5582\n",
      "Epoch [3/5], Step [108975/136675], Loss: 5.2852\n",
      "Epoch [3/5], Step [109050/136675], Loss: 5.3776\n",
      "Epoch [3/5], Step [109125/136675], Loss: 5.0837\n",
      "Epoch [3/5], Step [109200/136675], Loss: 5.3429\n",
      "Epoch [3/5], Step [109275/136675], Loss: 5.2235\n",
      "Epoch [3/5], Step [109350/136675], Loss: 4.9923\n",
      "Epoch [3/5], Step [109425/136675], Loss: 5.2397\n",
      "Epoch [3/5], Step [109500/136675], Loss: 5.2647\n",
      "Epoch [3/5], Step [109575/136675], Loss: 5.3713\n",
      "Epoch [3/5], Step [109650/136675], Loss: 5.3944\n",
      "Epoch [3/5], Step [109725/136675], Loss: 5.3872\n",
      "Epoch [3/5], Step [109800/136675], Loss: 5.0569\n",
      "Epoch [3/5], Step [109875/136675], Loss: 5.4347\n",
      "Epoch [3/5], Step [109950/136675], Loss: 5.3869\n",
      "Validation perplexity: 161.29291777924564\n",
      "Epoch [3/5], Step [110025/136675], Loss: 5.2238\n",
      "Epoch [3/5], Step [110100/136675], Loss: 5.2035\n",
      "Epoch [3/5], Step [110175/136675], Loss: 5.2613\n",
      "Epoch [3/5], Step [110250/136675], Loss: 5.1513\n",
      "Epoch [3/5], Step [110325/136675], Loss: 5.1137\n",
      "Epoch [3/5], Step [110400/136675], Loss: 5.0553\n",
      "Epoch [3/5], Step [110475/136675], Loss: 5.3117\n",
      "Epoch [3/5], Step [110550/136675], Loss: 5.1964\n",
      "Epoch [3/5], Step [110625/136675], Loss: 5.3921\n",
      "Epoch [3/5], Step [110700/136675], Loss: 5.2107\n",
      "Epoch [3/5], Step [110775/136675], Loss: 5.2515\n",
      "Epoch [3/5], Step [110850/136675], Loss: 5.3543\n",
      "Epoch [3/5], Step [110925/136675], Loss: 5.4636\n",
      "Epoch [3/5], Step [111000/136675], Loss: 5.1224\n",
      "Epoch [3/5], Step [111075/136675], Loss: 5.2235\n",
      "Epoch [3/5], Step [111150/136675], Loss: 5.3990\n",
      "Epoch [3/5], Step [111225/136675], Loss: 5.0947\n",
      "Epoch [3/5], Step [111300/136675], Loss: 5.0101\n",
      "Epoch [3/5], Step [111375/136675], Loss: 5.1547\n",
      "Epoch [3/5], Step [111450/136675], Loss: 5.1329\n",
      "Epoch [3/5], Step [111525/136675], Loss: 5.1386\n",
      "Epoch [3/5], Step [111600/136675], Loss: 5.4921\n",
      "Epoch [3/5], Step [111675/136675], Loss: 5.4931\n",
      "Epoch [3/5], Step [111750/136675], Loss: 5.2163\n",
      "Epoch [3/5], Step [111825/136675], Loss: 5.5831\n",
      "Epoch [3/5], Step [111900/136675], Loss: 5.3964\n",
      "Epoch [3/5], Step [111975/136675], Loss: 5.0696\n",
      "Epoch [3/5], Step [112050/136675], Loss: 5.2462\n",
      "Epoch [3/5], Step [112125/136675], Loss: 5.3597\n",
      "Epoch [3/5], Step [112200/136675], Loss: 5.2124\n",
      "Epoch [3/5], Step [112275/136675], Loss: 5.3470\n",
      "Epoch [3/5], Step [112350/136675], Loss: 5.1642\n",
      "Epoch [3/5], Step [112425/136675], Loss: 5.1829\n",
      "Epoch [3/5], Step [112500/136675], Loss: 5.2377\n",
      "Epoch [3/5], Step [112575/136675], Loss: 5.3516\n",
      "Epoch [3/5], Step [112650/136675], Loss: 5.5103\n",
      "Epoch [3/5], Step [112725/136675], Loss: 5.1843\n",
      "Epoch [3/5], Step [112800/136675], Loss: 5.2007\n",
      "Epoch [3/5], Step [112875/136675], Loss: 5.6228\n",
      "Epoch [3/5], Step [112950/136675], Loss: 5.4791\n",
      "Epoch [3/5], Step [113025/136675], Loss: 5.2879\n",
      "Epoch [3/5], Step [113100/136675], Loss: 5.3678\n",
      "Epoch [3/5], Step [113175/136675], Loss: 5.0885\n",
      "Epoch [3/5], Step [113250/136675], Loss: 5.1952\n",
      "Epoch [3/5], Step [113325/136675], Loss: 5.2545\n",
      "Epoch [3/5], Step [113400/136675], Loss: 5.3183\n",
      "Epoch [3/5], Step [113475/136675], Loss: 5.2745\n",
      "Epoch [3/5], Step [113550/136675], Loss: 5.2599\n",
      "Epoch [3/5], Step [113625/136675], Loss: 5.0570\n",
      "Epoch [3/5], Step [113700/136675], Loss: 5.5227\n",
      "Epoch [3/5], Step [113775/136675], Loss: 5.2063\n",
      "Epoch [3/5], Step [113850/136675], Loss: 5.1533\n",
      "Epoch [3/5], Step [113925/136675], Loss: 5.1777\n",
      "Epoch [3/5], Step [114000/136675], Loss: 5.3482\n",
      "Epoch [3/5], Step [114075/136675], Loss: 5.4363\n",
      "Epoch [3/5], Step [114150/136675], Loss: 5.4041\n",
      "Epoch [3/5], Step [114225/136675], Loss: 5.4328\n",
      "Epoch [3/5], Step [114300/136675], Loss: 5.0567\n",
      "Epoch [3/5], Step [114375/136675], Loss: 5.4041\n",
      "Epoch [3/5], Step [114450/136675], Loss: 5.2829\n",
      "Epoch [3/5], Step [114525/136675], Loss: 5.3260\n",
      "Epoch [3/5], Step [114600/136675], Loss: 5.2856\n",
      "Epoch [3/5], Step [114675/136675], Loss: 5.3322\n",
      "Epoch [3/5], Step [114750/136675], Loss: 5.6122\n",
      "Epoch [3/5], Step [114825/136675], Loss: 5.1212\n",
      "Epoch [3/5], Step [114900/136675], Loss: 5.2337\n",
      "Epoch [3/5], Step [114975/136675], Loss: 5.2956\n",
      "Epoch [3/5], Step [115050/136675], Loss: 5.3948\n",
      "Epoch [3/5], Step [115125/136675], Loss: 5.1652\n",
      "Epoch [3/5], Step [115200/136675], Loss: 5.1332\n",
      "Epoch [3/5], Step [115275/136675], Loss: 5.2423\n",
      "Epoch [3/5], Step [115350/136675], Loss: 5.2777\n",
      "Epoch [3/5], Step [115425/136675], Loss: 5.2356\n",
      "Epoch [3/5], Step [115500/136675], Loss: 5.4922\n",
      "Epoch [3/5], Step [115575/136675], Loss: 5.1480\n",
      "Epoch [3/5], Step [115650/136675], Loss: 5.2805\n",
      "Epoch [3/5], Step [115725/136675], Loss: 5.4924\n",
      "Epoch [3/5], Step [115800/136675], Loss: 5.3838\n",
      "Epoch [3/5], Step [115875/136675], Loss: 5.3953\n",
      "Epoch [3/5], Step [115950/136675], Loss: 5.3751\n",
      "Epoch [3/5], Step [116025/136675], Loss: 5.0794\n",
      "Epoch [3/5], Step [116100/136675], Loss: 5.2661\n",
      "Epoch [3/5], Step [116175/136675], Loss: 5.4507\n",
      "Epoch [3/5], Step [116250/136675], Loss: 4.9636\n",
      "Epoch [3/5], Step [116325/136675], Loss: 5.3123\n",
      "Epoch [3/5], Step [116400/136675], Loss: 5.0583\n",
      "Epoch [3/5], Step [116475/136675], Loss: 5.0783\n",
      "Epoch [3/5], Step [116550/136675], Loss: 5.2537\n",
      "Epoch [3/5], Step [116625/136675], Loss: 5.1536\n",
      "Epoch [3/5], Step [116700/136675], Loss: 5.0317\n",
      "Epoch [3/5], Step [116775/136675], Loss: 5.2505\n",
      "Epoch [3/5], Step [116850/136675], Loss: 5.2015\n",
      "Epoch [3/5], Step [116925/136675], Loss: 5.2514\n",
      "Epoch [3/5], Step [117000/136675], Loss: 5.3083\n",
      "Epoch [3/5], Step [117075/136675], Loss: 5.3668\n",
      "Epoch [3/5], Step [117150/136675], Loss: 5.4644\n",
      "Epoch [3/5], Step [117225/136675], Loss: 5.3577\n",
      "Epoch [3/5], Step [117300/136675], Loss: 5.0317\n",
      "Epoch [3/5], Step [117375/136675], Loss: 5.2014\n",
      "Epoch [3/5], Step [117450/136675], Loss: 5.5298\n",
      "Epoch [3/5], Step [117525/136675], Loss: 5.4510\n",
      "Epoch [3/5], Step [117600/136675], Loss: 5.1386\n",
      "Epoch [3/5], Step [117675/136675], Loss: 5.7313\n",
      "Epoch [3/5], Step [117750/136675], Loss: 5.2956\n",
      "Epoch [3/5], Step [117825/136675], Loss: 5.3750\n",
      "Epoch [3/5], Step [117900/136675], Loss: 5.1727\n",
      "Epoch [3/5], Step [117975/136675], Loss: 5.5331\n",
      "Epoch [3/5], Step [118050/136675], Loss: 5.2621\n",
      "Epoch [3/5], Step [118125/136675], Loss: 5.4245\n",
      "Epoch [3/5], Step [118200/136675], Loss: 5.2675\n",
      "Epoch [3/5], Step [118275/136675], Loss: 5.2520\n",
      "Epoch [3/5], Step [118350/136675], Loss: 5.5171\n",
      "Epoch [3/5], Step [118425/136675], Loss: 5.3965\n",
      "Epoch [3/5], Step [118500/136675], Loss: 5.3206\n",
      "Epoch [3/5], Step [118575/136675], Loss: 5.5557\n",
      "Epoch [3/5], Step [118650/136675], Loss: 4.9596\n",
      "Epoch [3/5], Step [118725/136675], Loss: 5.1816\n",
      "Epoch [3/5], Step [118800/136675], Loss: 5.3276\n",
      "Epoch [3/5], Step [118875/136675], Loss: 5.2468\n",
      "Epoch [3/5], Step [118950/136675], Loss: 5.1672\n",
      "Epoch [3/5], Step [119025/136675], Loss: 5.1228\n",
      "Epoch [3/5], Step [119100/136675], Loss: 5.3895\n",
      "Epoch [3/5], Step [119175/136675], Loss: 5.1702\n",
      "Epoch [3/5], Step [119250/136675], Loss: 5.4980\n",
      "Epoch [3/5], Step [119325/136675], Loss: 5.2705\n",
      "Epoch [3/5], Step [119400/136675], Loss: 5.1659\n",
      "Epoch [3/5], Step [119475/136675], Loss: 5.5449\n",
      "Epoch [3/5], Step [119550/136675], Loss: 5.2497\n",
      "Epoch [3/5], Step [119625/136675], Loss: 5.0924\n",
      "Epoch [3/5], Step [119700/136675], Loss: 5.1333\n",
      "Epoch [3/5], Step [119775/136675], Loss: 5.4775\n",
      "Epoch [3/5], Step [119850/136675], Loss: 5.2670\n",
      "Epoch [3/5], Step [119925/136675], Loss: 5.2883\n",
      "Epoch [3/5], Step [120000/136675], Loss: 5.6910\n",
      "Validation perplexity: 158.56733958109734\n",
      "Epoch [3/5], Step [120075/136675], Loss: 5.6926\n",
      "Epoch [3/5], Step [120150/136675], Loss: 5.3119\n",
      "Epoch [3/5], Step [120225/136675], Loss: 5.1552\n",
      "Epoch [3/5], Step [120300/136675], Loss: 4.7739\n",
      "Epoch [3/5], Step [120375/136675], Loss: 5.1053\n",
      "Epoch [3/5], Step [120450/136675], Loss: 5.1372\n",
      "Epoch [3/5], Step [120525/136675], Loss: 5.0920\n",
      "Epoch [3/5], Step [120600/136675], Loss: 4.9308\n",
      "Epoch [3/5], Step [120675/136675], Loss: 5.3106\n",
      "Epoch [3/5], Step [120750/136675], Loss: 5.1023\n",
      "Epoch [3/5], Step [120825/136675], Loss: 5.3231\n",
      "Epoch [3/5], Step [120900/136675], Loss: 5.5221\n",
      "Epoch [3/5], Step [120975/136675], Loss: 5.3652\n",
      "Epoch [3/5], Step [121050/136675], Loss: 5.2336\n",
      "Epoch [3/5], Step [121125/136675], Loss: 5.4461\n",
      "Epoch [3/5], Step [121200/136675], Loss: 5.3695\n",
      "Epoch [3/5], Step [121275/136675], Loss: 5.4409\n",
      "Epoch [3/5], Step [121350/136675], Loss: 5.3098\n",
      "Epoch [3/5], Step [121425/136675], Loss: 5.2324\n",
      "Epoch [3/5], Step [121500/136675], Loss: 5.3598\n",
      "Epoch [3/5], Step [121575/136675], Loss: 5.3434\n",
      "Epoch [3/5], Step [121650/136675], Loss: 5.2667\n",
      "Epoch [3/5], Step [121725/136675], Loss: 5.0240\n",
      "Epoch [3/5], Step [121800/136675], Loss: 5.2364\n",
      "Epoch [3/5], Step [121875/136675], Loss: 5.0857\n",
      "Epoch [3/5], Step [121950/136675], Loss: 5.3411\n",
      "Epoch [3/5], Step [122025/136675], Loss: 5.6012\n",
      "Epoch [3/5], Step [122100/136675], Loss: 5.2578\n",
      "Epoch [3/5], Step [122175/136675], Loss: 5.1749\n",
      "Epoch [3/5], Step [122250/136675], Loss: 5.5344\n",
      "Epoch [3/5], Step [122325/136675], Loss: 5.3668\n",
      "Epoch [3/5], Step [122400/136675], Loss: 5.3607\n",
      "Epoch [3/5], Step [122475/136675], Loss: 5.2048\n",
      "Epoch [3/5], Step [122550/136675], Loss: 5.2998\n",
      "Epoch [3/5], Step [122625/136675], Loss: 5.5496\n",
      "Epoch [3/5], Step [122700/136675], Loss: 5.4764\n",
      "Epoch [3/5], Step [122775/136675], Loss: 5.2218\n",
      "Epoch [3/5], Step [122850/136675], Loss: 5.0195\n",
      "Epoch [3/5], Step [122925/136675], Loss: 5.2276\n",
      "Epoch [3/5], Step [123000/136675], Loss: 5.4087\n",
      "Epoch [3/5], Step [123075/136675], Loss: 5.3595\n",
      "Epoch [3/5], Step [123150/136675], Loss: 5.1415\n",
      "Epoch [3/5], Step [123225/136675], Loss: 5.3196\n",
      "Epoch [3/5], Step [123300/136675], Loss: 5.4035\n",
      "Epoch [3/5], Step [123375/136675], Loss: 5.2120\n",
      "Epoch [3/5], Step [123450/136675], Loss: 5.4373\n",
      "Epoch [3/5], Step [123525/136675], Loss: 5.4404\n",
      "Epoch [3/5], Step [123600/136675], Loss: 5.4778\n",
      "Epoch [3/5], Step [123675/136675], Loss: 4.9014\n",
      "Epoch [3/5], Step [123750/136675], Loss: 5.1001\n",
      "Epoch [3/5], Step [123825/136675], Loss: 5.2979\n",
      "Epoch [3/5], Step [123900/136675], Loss: 5.2084\n",
      "Epoch [3/5], Step [123975/136675], Loss: 5.2038\n",
      "Epoch [3/5], Step [124050/136675], Loss: 5.1880\n",
      "Epoch [3/5], Step [124125/136675], Loss: 5.2367\n",
      "Epoch [3/5], Step [124200/136675], Loss: 5.3556\n",
      "Epoch [3/5], Step [124275/136675], Loss: 5.6027\n",
      "Epoch [3/5], Step [124350/136675], Loss: 5.3410\n",
      "Epoch [3/5], Step [124425/136675], Loss: 5.2014\n",
      "Epoch [3/5], Step [124500/136675], Loss: 5.2165\n",
      "Epoch [3/5], Step [124575/136675], Loss: 5.1643\n",
      "Epoch [3/5], Step [124650/136675], Loss: 4.8809\n",
      "Epoch [3/5], Step [124725/136675], Loss: 5.2225\n",
      "Epoch [3/5], Step [124800/136675], Loss: 5.4777\n",
      "Epoch [3/5], Step [124875/136675], Loss: 5.4460\n",
      "Epoch [3/5], Step [124950/136675], Loss: 5.0811\n",
      "Epoch [3/5], Step [125025/136675], Loss: 5.4073\n",
      "Epoch [3/5], Step [125100/136675], Loss: 5.5695\n",
      "Epoch [3/5], Step [125175/136675], Loss: 5.3137\n",
      "Epoch [3/5], Step [125250/136675], Loss: 5.7628\n",
      "Epoch [3/5], Step [125325/136675], Loss: 5.0445\n",
      "Epoch [3/5], Step [125400/136675], Loss: 5.2686\n",
      "Epoch [3/5], Step [125475/136675], Loss: 5.4868\n",
      "Epoch [3/5], Step [125550/136675], Loss: 5.4394\n",
      "Epoch [3/5], Step [125625/136675], Loss: 5.4299\n",
      "Epoch [3/5], Step [125700/136675], Loss: 5.4204\n",
      "Epoch [3/5], Step [125775/136675], Loss: 5.3110\n",
      "Epoch [3/5], Step [125850/136675], Loss: 5.1881\n",
      "Epoch [3/5], Step [125925/136675], Loss: 5.1654\n",
      "Epoch [3/5], Step [126000/136675], Loss: 5.2791\n",
      "Epoch [3/5], Step [126075/136675], Loss: 5.2106\n",
      "Epoch [3/5], Step [126150/136675], Loss: 5.1628\n",
      "Epoch [3/5], Step [126225/136675], Loss: 5.1233\n",
      "Epoch [3/5], Step [126300/136675], Loss: 5.2324\n",
      "Epoch [3/5], Step [126375/136675], Loss: 5.1249\n",
      "Epoch [3/5], Step [126450/136675], Loss: 5.3545\n",
      "Epoch [3/5], Step [126525/136675], Loss: 5.1020\n",
      "Epoch [3/5], Step [126600/136675], Loss: 5.2330\n",
      "Epoch [3/5], Step [126675/136675], Loss: 5.6008\n",
      "Epoch [3/5], Step [126750/136675], Loss: 5.5562\n",
      "Epoch [3/5], Step [126825/136675], Loss: 5.3936\n",
      "Epoch [3/5], Step [126900/136675], Loss: 5.2850\n",
      "Epoch [3/5], Step [126975/136675], Loss: 5.4530\n",
      "Epoch [3/5], Step [127050/136675], Loss: 5.1522\n",
      "Epoch [3/5], Step [127125/136675], Loss: 5.1276\n",
      "Epoch [3/5], Step [127200/136675], Loss: 5.4348\n",
      "Epoch [3/5], Step [127275/136675], Loss: 5.1840\n",
      "Epoch [3/5], Step [127350/136675], Loss: 5.2429\n",
      "Epoch [3/5], Step [127425/136675], Loss: 5.3164\n",
      "Epoch [3/5], Step [127500/136675], Loss: 5.7391\n",
      "Epoch [3/5], Step [127575/136675], Loss: 5.3705\n",
      "Epoch [3/5], Step [127650/136675], Loss: 5.1620\n",
      "Epoch [3/5], Step [127725/136675], Loss: 5.2697\n",
      "Epoch [3/5], Step [127800/136675], Loss: 5.2981\n",
      "Epoch [3/5], Step [127875/136675], Loss: 5.3599\n",
      "Epoch [3/5], Step [127950/136675], Loss: 5.1588\n",
      "Epoch [3/5], Step [128025/136675], Loss: 5.3510\n",
      "Epoch [3/5], Step [128100/136675], Loss: 5.0976\n",
      "Epoch [3/5], Step [128175/136675], Loss: 5.6892\n",
      "Epoch [3/5], Step [128250/136675], Loss: 5.3520\n",
      "Epoch [3/5], Step [128325/136675], Loss: 5.3287\n",
      "Epoch [3/5], Step [128400/136675], Loss: 5.2623\n",
      "Epoch [3/5], Step [128475/136675], Loss: 5.1941\n",
      "Epoch [3/5], Step [128550/136675], Loss: 5.1288\n",
      "Epoch [3/5], Step [128625/136675], Loss: 5.5406\n",
      "Epoch [3/5], Step [128700/136675], Loss: 5.7190\n",
      "Epoch [3/5], Step [128775/136675], Loss: 5.5425\n",
      "Epoch [3/5], Step [128850/136675], Loss: 5.2688\n",
      "Epoch [3/5], Step [128925/136675], Loss: 5.5042\n",
      "Epoch [3/5], Step [129000/136675], Loss: 5.3591\n",
      "Epoch [3/5], Step [129075/136675], Loss: 5.0329\n",
      "Epoch [3/5], Step [129150/136675], Loss: 5.1606\n",
      "Epoch [3/5], Step [129225/136675], Loss: 5.0002\n",
      "Epoch [3/5], Step [129300/136675], Loss: 5.1850\n",
      "Epoch [3/5], Step [129375/136675], Loss: 5.1260\n",
      "Epoch [3/5], Step [129450/136675], Loss: 5.7005\n",
      "Epoch [3/5], Step [129525/136675], Loss: 5.4125\n",
      "Epoch [3/5], Step [129600/136675], Loss: 5.2113\n",
      "Epoch [3/5], Step [129675/136675], Loss: 5.1729\n",
      "Epoch [3/5], Step [129750/136675], Loss: 5.0854\n",
      "Epoch [3/5], Step [129825/136675], Loss: 5.2828\n",
      "Epoch [3/5], Step [129900/136675], Loss: 5.3322\n",
      "Epoch [3/5], Step [129975/136675], Loss: 5.3394\n",
      "Validation perplexity: 158.28128929571474\n",
      "Epoch [3/5], Step [130050/136675], Loss: 5.2904\n",
      "Epoch [3/5], Step [130125/136675], Loss: 5.0941\n",
      "Epoch [3/5], Step [130200/136675], Loss: 5.2458\n",
      "Epoch [3/5], Step [130275/136675], Loss: 5.3113\n",
      "Epoch [3/5], Step [130350/136675], Loss: 5.2576\n",
      "Epoch [3/5], Step [130425/136675], Loss: 5.3895\n",
      "Epoch [3/5], Step [130500/136675], Loss: 5.1877\n",
      "Epoch [3/5], Step [130575/136675], Loss: 4.9210\n",
      "Epoch [3/5], Step [130650/136675], Loss: 5.2820\n",
      "Epoch [3/5], Step [130725/136675], Loss: 5.4107\n",
      "Epoch [3/5], Step [130800/136675], Loss: 5.3452\n",
      "Epoch [3/5], Step [130875/136675], Loss: 5.3988\n",
      "Epoch [3/5], Step [130950/136675], Loss: 5.0328\n",
      "Epoch [3/5], Step [131025/136675], Loss: 5.0791\n",
      "Epoch [3/5], Step [131100/136675], Loss: 5.3750\n",
      "Epoch [3/5], Step [131175/136675], Loss: 5.5652\n",
      "Epoch [3/5], Step [131250/136675], Loss: 5.2943\n",
      "Epoch [3/5], Step [131325/136675], Loss: 5.3147\n",
      "Epoch [3/5], Step [131400/136675], Loss: 5.1507\n",
      "Epoch [3/5], Step [131475/136675], Loss: 5.3366\n",
      "Epoch [3/5], Step [131550/136675], Loss: 5.2195\n",
      "Epoch [3/5], Step [131625/136675], Loss: 5.4415\n",
      "Epoch [3/5], Step [131700/136675], Loss: 5.4915\n",
      "Epoch [3/5], Step [131775/136675], Loss: 5.0937\n",
      "Epoch [3/5], Step [131850/136675], Loss: 5.3773\n",
      "Epoch [3/5], Step [131925/136675], Loss: 5.2284\n",
      "Epoch [3/5], Step [132000/136675], Loss: 5.3701\n",
      "Epoch [3/5], Step [132075/136675], Loss: 5.7365\n",
      "Epoch [3/5], Step [132150/136675], Loss: 5.5136\n",
      "Epoch [3/5], Step [132225/136675], Loss: 5.6023\n",
      "Epoch [3/5], Step [132300/136675], Loss: 5.2600\n",
      "Epoch [3/5], Step [132375/136675], Loss: 5.2381\n",
      "Epoch [3/5], Step [132450/136675], Loss: 4.8804\n",
      "Epoch [3/5], Step [132525/136675], Loss: 5.4213\n",
      "Epoch [3/5], Step [132600/136675], Loss: 5.5661\n",
      "Epoch [3/5], Step [132675/136675], Loss: 5.3683\n",
      "Epoch [3/5], Step [132750/136675], Loss: 4.9642\n",
      "Epoch [3/5], Step [132825/136675], Loss: 5.2400\n",
      "Epoch [3/5], Step [132900/136675], Loss: 5.1964\n",
      "Epoch [3/5], Step [132975/136675], Loss: 4.9923\n",
      "Epoch [3/5], Step [133050/136675], Loss: 5.2917\n",
      "Epoch [3/5], Step [133125/136675], Loss: 5.2774\n",
      "Epoch [3/5], Step [133200/136675], Loss: 5.3031\n",
      "Epoch [3/5], Step [133275/136675], Loss: 5.3055\n",
      "Epoch [3/5], Step [133350/136675], Loss: 5.3012\n",
      "Epoch [3/5], Step [133425/136675], Loss: 4.8924\n",
      "Epoch [3/5], Step [133500/136675], Loss: 5.3257\n",
      "Epoch [3/5], Step [133575/136675], Loss: 5.3909\n",
      "Epoch [3/5], Step [133650/136675], Loss: 5.4774\n",
      "Epoch [3/5], Step [133725/136675], Loss: 5.1948\n",
      "Epoch [3/5], Step [133800/136675], Loss: 5.1464\n",
      "Epoch [3/5], Step [133875/136675], Loss: 5.2166\n",
      "Epoch [3/5], Step [133950/136675], Loss: 5.3626\n",
      "Epoch [3/5], Step [134025/136675], Loss: 5.3915\n",
      "Epoch [3/5], Step [134100/136675], Loss: 5.3634\n",
      "Epoch [3/5], Step [134175/136675], Loss: 5.4968\n",
      "Epoch [3/5], Step [134250/136675], Loss: 5.2402\n",
      "Epoch [3/5], Step [134325/136675], Loss: 5.5327\n",
      "Epoch [3/5], Step [134400/136675], Loss: 5.0806\n",
      "Epoch [3/5], Step [134475/136675], Loss: 5.4276\n",
      "Epoch [3/5], Step [134550/136675], Loss: 5.2333\n",
      "Epoch [3/5], Step [134625/136675], Loss: 5.3410\n",
      "Epoch [3/5], Step [134700/136675], Loss: 5.1053\n",
      "Epoch [3/5], Step [134775/136675], Loss: 5.2726\n",
      "Epoch [3/5], Step [134850/136675], Loss: 5.3402\n",
      "Epoch [3/5], Step [134925/136675], Loss: 5.2435\n",
      "Epoch [3/5], Step [135000/136675], Loss: 5.7950\n",
      "Epoch [3/5], Step [135075/136675], Loss: 5.3906\n",
      "Epoch [3/5], Step [135150/136675], Loss: 5.2030\n",
      "Epoch [3/5], Step [135225/136675], Loss: 5.2905\n",
      "Epoch [3/5], Step [135300/136675], Loss: 5.3635\n",
      "Epoch [3/5], Step [135375/136675], Loss: 5.2255\n",
      "Epoch [3/5], Step [135450/136675], Loss: 5.1640\n",
      "Epoch [3/5], Step [135525/136675], Loss: 4.9410\n",
      "Epoch [3/5], Step [135600/136675], Loss: 5.4505\n",
      "Epoch [3/5], Step [135675/136675], Loss: 5.0604\n",
      "Epoch [3/5], Step [135750/136675], Loss: 5.4315\n",
      "Epoch [3/5], Step [135825/136675], Loss: 5.2012\n",
      "Epoch [3/5], Step [135900/136675], Loss: 5.4216\n",
      "Epoch [3/5], Step [135975/136675], Loss: 5.4383\n",
      "Epoch [3/5], Step [136050/136675], Loss: 5.3007\n",
      "Epoch [3/5], Step [136125/136675], Loss: 5.2930\n",
      "Epoch [3/5], Step [136200/136675], Loss: 5.4932\n",
      "Epoch [3/5], Step [136275/136675], Loss: 5.3737\n",
      "Epoch [3/5], Step [136350/136675], Loss: 5.3722\n",
      "Epoch [3/5], Step [136425/136675], Loss: 5.5064\n",
      "Epoch [3/5], Step [136500/136675], Loss: 5.2247\n",
      "Epoch [3/5], Step [136575/136675], Loss: 5.1933\n",
      "Epoch [3/5], Step [136650/136675], Loss: 5.0004\n",
      "Epoch [3/5] Average Loss: 5.3044, Perplexity: 201.21\n",
      "Epoch [4/5], Step [0/136675], Loss: 5.5139\n",
      "Validation perplexity: 158.94518957685546\n",
      "Epoch [4/5], Step [75/136675], Loss: 5.3394\n",
      "Epoch [4/5], Step [150/136675], Loss: 5.0378\n",
      "Epoch [4/5], Step [225/136675], Loss: 5.0178\n",
      "Epoch [4/5], Step [300/136675], Loss: 5.2407\n",
      "Epoch [4/5], Step [375/136675], Loss: 5.3312\n",
      "Epoch [4/5], Step [450/136675], Loss: 5.2027\n",
      "Epoch [4/5], Step [525/136675], Loss: 5.2224\n",
      "Epoch [4/5], Step [600/136675], Loss: 5.2161\n",
      "Epoch [4/5], Step [675/136675], Loss: 5.2365\n",
      "Epoch [4/5], Step [750/136675], Loss: 4.9627\n",
      "Epoch [4/5], Step [825/136675], Loss: 5.2961\n",
      "Epoch [4/5], Step [900/136675], Loss: 5.3901\n",
      "Epoch [4/5], Step [975/136675], Loss: 5.3119\n",
      "Epoch [4/5], Step [1050/136675], Loss: 5.3468\n",
      "Epoch [4/5], Step [1125/136675], Loss: 4.9570\n",
      "Epoch [4/5], Step [1200/136675], Loss: 5.5250\n",
      "Epoch [4/5], Step [1275/136675], Loss: 5.5743\n",
      "Epoch [4/5], Step [1350/136675], Loss: 5.3199\n",
      "Epoch [4/5], Step [1425/136675], Loss: 5.3170\n",
      "Epoch [4/5], Step [1500/136675], Loss: 5.1510\n",
      "Epoch [4/5], Step [1575/136675], Loss: 5.0687\n",
      "Epoch [4/5], Step [1650/136675], Loss: 5.5557\n",
      "Epoch [4/5], Step [1725/136675], Loss: 5.0717\n",
      "Epoch [4/5], Step [1800/136675], Loss: 5.1897\n",
      "Epoch [4/5], Step [1875/136675], Loss: 5.2559\n",
      "Epoch [4/5], Step [1950/136675], Loss: 5.1641\n",
      "Epoch [4/5], Step [2025/136675], Loss: 5.4740\n",
      "Epoch [4/5], Step [2100/136675], Loss: 5.4896\n",
      "Epoch [4/5], Step [2175/136675], Loss: 5.1850\n",
      "Epoch [4/5], Step [2250/136675], Loss: 5.0032\n",
      "Epoch [4/5], Step [2325/136675], Loss: 5.1091\n",
      "Epoch [4/5], Step [2400/136675], Loss: 5.1660\n",
      "Epoch [4/5], Step [2475/136675], Loss: 5.1551\n",
      "Epoch [4/5], Step [2550/136675], Loss: 5.3165\n",
      "Epoch [4/5], Step [2625/136675], Loss: 5.2062\n",
      "Epoch [4/5], Step [2700/136675], Loss: 5.1249\n",
      "Epoch [4/5], Step [2775/136675], Loss: 5.4012\n",
      "Epoch [4/5], Step [2850/136675], Loss: 5.1102\n",
      "Epoch [4/5], Step [2925/136675], Loss: 5.3144\n",
      "Epoch [4/5], Step [3000/136675], Loss: 5.2806\n",
      "Epoch [4/5], Step [3075/136675], Loss: 5.4398\n",
      "Epoch [4/5], Step [3150/136675], Loss: 4.9342\n",
      "Epoch [4/5], Step [3225/136675], Loss: 5.0336\n",
      "Epoch [4/5], Step [3300/136675], Loss: 5.0741\n",
      "Epoch [4/5], Step [3375/136675], Loss: 5.3960\n",
      "Epoch [4/5], Step [3450/136675], Loss: 5.4678\n",
      "Epoch [4/5], Step [3525/136675], Loss: 5.2099\n",
      "Epoch [4/5], Step [3600/136675], Loss: 5.1479\n",
      "Epoch [4/5], Step [3675/136675], Loss: 5.1871\n",
      "Epoch [4/5], Step [3750/136675], Loss: 5.0300\n",
      "Epoch [4/5], Step [3825/136675], Loss: 5.1011\n",
      "Epoch [4/5], Step [3900/136675], Loss: 5.4724\n",
      "Epoch [4/5], Step [3975/136675], Loss: 5.5116\n",
      "Epoch [4/5], Step [4050/136675], Loss: 5.3701\n",
      "Epoch [4/5], Step [4125/136675], Loss: 5.4734\n",
      "Epoch [4/5], Step [4200/136675], Loss: 5.3783\n",
      "Epoch [4/5], Step [4275/136675], Loss: 5.2589\n",
      "Epoch [4/5], Step [4350/136675], Loss: 5.3673\n",
      "Epoch [4/5], Step [4425/136675], Loss: 5.2829\n",
      "Epoch [4/5], Step [4500/136675], Loss: 5.4160\n",
      "Epoch [4/5], Step [4575/136675], Loss: 5.4106\n",
      "Epoch [4/5], Step [4650/136675], Loss: 5.2764\n",
      "Epoch [4/5], Step [4725/136675], Loss: 5.2123\n",
      "Epoch [4/5], Step [4800/136675], Loss: 5.0666\n",
      "Epoch [4/5], Step [4875/136675], Loss: 5.3850\n",
      "Epoch [4/5], Step [4950/136675], Loss: 5.3617\n",
      "Epoch [4/5], Step [5025/136675], Loss: 4.9665\n",
      "Epoch [4/5], Step [5100/136675], Loss: 5.2001\n",
      "Epoch [4/5], Step [5175/136675], Loss: 5.3607\n",
      "Epoch [4/5], Step [5250/136675], Loss: 5.3655\n",
      "Epoch [4/5], Step [5325/136675], Loss: 5.0550\n",
      "Epoch [4/5], Step [5400/136675], Loss: 4.6343\n",
      "Epoch [4/5], Step [5475/136675], Loss: 5.4291\n",
      "Epoch [4/5], Step [5550/136675], Loss: 5.4123\n",
      "Epoch [4/5], Step [5625/136675], Loss: 5.5580\n",
      "Epoch [4/5], Step [5700/136675], Loss: 5.0821\n",
      "Epoch [4/5], Step [5775/136675], Loss: 5.3470\n",
      "Epoch [4/5], Step [5850/136675], Loss: 5.1421\n",
      "Epoch [4/5], Step [5925/136675], Loss: 5.4189\n",
      "Epoch [4/5], Step [6000/136675], Loss: 5.5464\n",
      "Epoch [4/5], Step [6075/136675], Loss: 5.3011\n",
      "Epoch [4/5], Step [6150/136675], Loss: 5.2047\n",
      "Epoch [4/5], Step [6225/136675], Loss: 5.5609\n",
      "Epoch [4/5], Step [6300/136675], Loss: 5.4344\n",
      "Epoch [4/5], Step [6375/136675], Loss: 5.3153\n",
      "Epoch [4/5], Step [6450/136675], Loss: 5.0601\n",
      "Epoch [4/5], Step [6525/136675], Loss: 5.1787\n",
      "Epoch [4/5], Step [6600/136675], Loss: 5.4057\n",
      "Epoch [4/5], Step [6675/136675], Loss: 5.4422\n",
      "Epoch [4/5], Step [6750/136675], Loss: 5.6100\n",
      "Epoch [4/5], Step [6825/136675], Loss: 5.3325\n",
      "Epoch [4/5], Step [6900/136675], Loss: 5.3550\n",
      "Epoch [4/5], Step [6975/136675], Loss: 5.1751\n",
      "Epoch [4/5], Step [7050/136675], Loss: 5.4437\n",
      "Epoch [4/5], Step [7125/136675], Loss: 5.0871\n",
      "Epoch [4/5], Step [7200/136675], Loss: 5.5058\n",
      "Epoch [4/5], Step [7275/136675], Loss: 5.5927\n",
      "Epoch [4/5], Step [7350/136675], Loss: 5.1875\n",
      "Epoch [4/5], Step [7425/136675], Loss: 4.7927\n",
      "Epoch [4/5], Step [7500/136675], Loss: 5.1640\n",
      "Epoch [4/5], Step [7575/136675], Loss: 5.2119\n",
      "Epoch [4/5], Step [7650/136675], Loss: 5.2430\n",
      "Epoch [4/5], Step [7725/136675], Loss: 5.2231\n",
      "Epoch [4/5], Step [7800/136675], Loss: 5.4254\n",
      "Epoch [4/5], Step [7875/136675], Loss: 5.2825\n",
      "Epoch [4/5], Step [7950/136675], Loss: 5.4391\n",
      "Epoch [4/5], Step [8025/136675], Loss: 5.3789\n",
      "Epoch [4/5], Step [8100/136675], Loss: 5.2101\n",
      "Epoch [4/5], Step [8175/136675], Loss: 5.4879\n",
      "Epoch [4/5], Step [8250/136675], Loss: 5.5175\n",
      "Epoch [4/5], Step [8325/136675], Loss: 5.6228\n",
      "Epoch [4/5], Step [8400/136675], Loss: 5.5284\n",
      "Epoch [4/5], Step [8475/136675], Loss: 5.4944\n",
      "Epoch [4/5], Step [8550/136675], Loss: 5.4030\n",
      "Epoch [4/5], Step [8625/136675], Loss: 4.9786\n",
      "Epoch [4/5], Step [8700/136675], Loss: 5.2873\n",
      "Epoch [4/5], Step [8775/136675], Loss: 5.3077\n",
      "Epoch [4/5], Step [8850/136675], Loss: 5.3019\n",
      "Epoch [4/5], Step [8925/136675], Loss: 5.1424\n",
      "Epoch [4/5], Step [9000/136675], Loss: 5.3839\n",
      "Epoch [4/5], Step [9075/136675], Loss: 5.3031\n",
      "Epoch [4/5], Step [9150/136675], Loss: 5.3021\n",
      "Epoch [4/5], Step [9225/136675], Loss: 5.4328\n",
      "Epoch [4/5], Step [9300/136675], Loss: 5.3433\n",
      "Epoch [4/5], Step [9375/136675], Loss: 5.1787\n",
      "Epoch [4/5], Step [9450/136675], Loss: 5.3225\n",
      "Epoch [4/5], Step [9525/136675], Loss: 5.0712\n",
      "Epoch [4/5], Step [9600/136675], Loss: 5.0054\n",
      "Epoch [4/5], Step [9675/136675], Loss: 5.2386\n",
      "Epoch [4/5], Step [9750/136675], Loss: 5.4801\n",
      "Epoch [4/5], Step [9825/136675], Loss: 5.3386\n",
      "Epoch [4/5], Step [9900/136675], Loss: 5.3172\n",
      "Epoch [4/5], Step [9975/136675], Loss: 5.2913\n",
      "Validation perplexity: 157.51186903038763\n",
      "Epoch [4/5], Step [10050/136675], Loss: 5.3657\n",
      "Epoch [4/5], Step [10125/136675], Loss: 5.2233\n",
      "Epoch [4/5], Step [10200/136675], Loss: 5.7779\n",
      "Epoch [4/5], Step [10275/136675], Loss: 5.4622\n",
      "Epoch [4/5], Step [10350/136675], Loss: 5.4822\n",
      "Epoch [4/5], Step [10425/136675], Loss: 5.4388\n",
      "Epoch [4/5], Step [10500/136675], Loss: 5.2680\n",
      "Epoch [4/5], Step [10575/136675], Loss: 5.2314\n",
      "Epoch [4/5], Step [10650/136675], Loss: 5.6026\n",
      "Epoch [4/5], Step [10725/136675], Loss: 5.3242\n",
      "Epoch [4/5], Step [10800/136675], Loss: 5.3445\n",
      "Epoch [4/5], Step [10875/136675], Loss: 5.2761\n",
      "Epoch [4/5], Step [10950/136675], Loss: 5.3581\n",
      "Epoch [4/5], Step [11025/136675], Loss: 5.3827\n",
      "Epoch [4/5], Step [11100/136675], Loss: 5.0102\n",
      "Epoch [4/5], Step [11175/136675], Loss: 5.3679\n",
      "Epoch [4/5], Step [11250/136675], Loss: 5.1473\n",
      "Epoch [4/5], Step [11325/136675], Loss: 5.3924\n",
      "Epoch [4/5], Step [11400/136675], Loss: 5.3444\n",
      "Epoch [4/5], Step [11475/136675], Loss: 5.0851\n",
      "Epoch [4/5], Step [11550/136675], Loss: 5.5745\n",
      "Epoch [4/5], Step [11625/136675], Loss: 5.2202\n",
      "Epoch [4/5], Step [11700/136675], Loss: 5.4276\n",
      "Epoch [4/5], Step [11775/136675], Loss: 5.2454\n",
      "Epoch [4/5], Step [11850/136675], Loss: 5.4478\n",
      "Epoch [4/5], Step [11925/136675], Loss: 5.2931\n",
      "Epoch [4/5], Step [12000/136675], Loss: 5.2331\n",
      "Epoch [4/5], Step [12075/136675], Loss: 5.2124\n",
      "Epoch [4/5], Step [12150/136675], Loss: 5.5156\n",
      "Epoch [4/5], Step [12225/136675], Loss: 5.2939\n",
      "Epoch [4/5], Step [12300/136675], Loss: 5.3835\n",
      "Epoch [4/5], Step [12375/136675], Loss: 5.1695\n",
      "Epoch [4/5], Step [12450/136675], Loss: 5.2480\n",
      "Epoch [4/5], Step [12525/136675], Loss: 5.2179\n",
      "Epoch [4/5], Step [12600/136675], Loss: 5.4340\n",
      "Epoch [4/5], Step [12675/136675], Loss: 5.4529\n",
      "Epoch [4/5], Step [12750/136675], Loss: 5.4307\n",
      "Epoch [4/5], Step [12825/136675], Loss: 5.6046\n",
      "Epoch [4/5], Step [12900/136675], Loss: 5.4841\n",
      "Epoch [4/5], Step [12975/136675], Loss: 4.8334\n",
      "Epoch [4/5], Step [13050/136675], Loss: 5.1798\n",
      "Epoch [4/5], Step [13125/136675], Loss: 5.0305\n",
      "Epoch [4/5], Step [13200/136675], Loss: 5.3101\n",
      "Epoch [4/5], Step [13275/136675], Loss: 5.4805\n",
      "Epoch [4/5], Step [13350/136675], Loss: 5.1871\n",
      "Epoch [4/5], Step [13425/136675], Loss: 5.5165\n",
      "Epoch [4/5], Step [13500/136675], Loss: 5.1559\n",
      "Epoch [4/5], Step [13575/136675], Loss: 5.4383\n",
      "Epoch [4/5], Step [13650/136675], Loss: 5.4153\n",
      "Epoch [4/5], Step [13725/136675], Loss: 4.9762\n",
      "Epoch [4/5], Step [13800/136675], Loss: 5.2566\n",
      "Epoch [4/5], Step [13875/136675], Loss: 5.3278\n",
      "Epoch [4/5], Step [13950/136675], Loss: 5.2926\n",
      "Epoch [4/5], Step [14025/136675], Loss: 5.3043\n",
      "Epoch [4/5], Step [14100/136675], Loss: 5.7749\n",
      "Epoch [4/5], Step [14175/136675], Loss: 5.3200\n",
      "Epoch [4/5], Step [14250/136675], Loss: 5.0812\n",
      "Epoch [4/5], Step [14325/136675], Loss: 5.4605\n",
      "Epoch [4/5], Step [14400/136675], Loss: 5.2446\n",
      "Epoch [4/5], Step [14475/136675], Loss: 4.9996\n",
      "Epoch [4/5], Step [14550/136675], Loss: 5.1718\n",
      "Epoch [4/5], Step [14625/136675], Loss: 5.3535\n",
      "Epoch [4/5], Step [14700/136675], Loss: 5.3733\n",
      "Epoch [4/5], Step [14775/136675], Loss: 5.4038\n",
      "Epoch [4/5], Step [14850/136675], Loss: 5.2590\n",
      "Epoch [4/5], Step [14925/136675], Loss: 5.4625\n",
      "Epoch [4/5], Step [15000/136675], Loss: 5.4841\n",
      "Epoch [4/5], Step [15075/136675], Loss: 5.2566\n",
      "Epoch [4/5], Step [15150/136675], Loss: 5.1610\n",
      "Epoch [4/5], Step [15225/136675], Loss: 5.2465\n",
      "Epoch [4/5], Step [15300/136675], Loss: 5.3829\n",
      "Epoch [4/5], Step [15375/136675], Loss: 5.1751\n",
      "Epoch [4/5], Step [15450/136675], Loss: 5.4408\n",
      "Epoch [4/5], Step [15525/136675], Loss: 5.2768\n",
      "Epoch [4/5], Step [15600/136675], Loss: 5.2617\n",
      "Epoch [4/5], Step [15675/136675], Loss: 5.3305\n",
      "Epoch [4/5], Step [15750/136675], Loss: 4.9952\n",
      "Epoch [4/5], Step [15825/136675], Loss: 5.2714\n",
      "Epoch [4/5], Step [15900/136675], Loss: 5.2453\n",
      "Epoch [4/5], Step [15975/136675], Loss: 5.1428\n",
      "Epoch [4/5], Step [16050/136675], Loss: 5.1266\n",
      "Epoch [4/5], Step [16125/136675], Loss: 5.3076\n",
      "Epoch [4/5], Step [16200/136675], Loss: 5.3558\n",
      "Epoch [4/5], Step [16275/136675], Loss: 5.2763\n",
      "Epoch [4/5], Step [16350/136675], Loss: 5.5239\n",
      "Epoch [4/5], Step [16425/136675], Loss: 5.6925\n",
      "Epoch [4/5], Step [16500/136675], Loss: 5.1935\n",
      "Epoch [4/5], Step [16575/136675], Loss: 5.0245\n",
      "Epoch [4/5], Step [16650/136675], Loss: 5.2834\n",
      "Epoch [4/5], Step [16725/136675], Loss: 5.3677\n",
      "Epoch [4/5], Step [16800/136675], Loss: 5.4525\n",
      "Epoch [4/5], Step [16875/136675], Loss: 5.4870\n",
      "Epoch [4/5], Step [16950/136675], Loss: 5.3995\n",
      "Epoch [4/5], Step [17025/136675], Loss: 5.4650\n",
      "Epoch [4/5], Step [17100/136675], Loss: 5.3912\n",
      "Epoch [4/5], Step [17175/136675], Loss: 5.1184\n",
      "Epoch [4/5], Step [17250/136675], Loss: 5.3243\n",
      "Epoch [4/5], Step [17325/136675], Loss: 5.2748\n",
      "Epoch [4/5], Step [17400/136675], Loss: 5.5638\n",
      "Epoch [4/5], Step [17475/136675], Loss: 5.2286\n",
      "Epoch [4/5], Step [17550/136675], Loss: 5.2482\n",
      "Epoch [4/5], Step [17625/136675], Loss: 5.1073\n",
      "Epoch [4/5], Step [17700/136675], Loss: 5.1812\n",
      "Epoch [4/5], Step [17775/136675], Loss: 5.2887\n",
      "Epoch [4/5], Step [17850/136675], Loss: 5.2494\n",
      "Epoch [4/5], Step [17925/136675], Loss: 5.3748\n",
      "Epoch [4/5], Step [18000/136675], Loss: 5.0142\n",
      "Epoch [4/5], Step [18075/136675], Loss: 5.2711\n",
      "Epoch [4/5], Step [18150/136675], Loss: 5.2748\n",
      "Epoch [4/5], Step [18225/136675], Loss: 5.5013\n",
      "Epoch [4/5], Step [18300/136675], Loss: 5.5055\n",
      "Epoch [4/5], Step [18375/136675], Loss: 5.3870\n",
      "Epoch [4/5], Step [18450/136675], Loss: 5.4316\n",
      "Epoch [4/5], Step [18525/136675], Loss: 5.2539\n",
      "Epoch [4/5], Step [18600/136675], Loss: 4.9970\n",
      "Epoch [4/5], Step [18675/136675], Loss: 5.1104\n",
      "Epoch [4/5], Step [18750/136675], Loss: 5.0804\n",
      "Epoch [4/5], Step [18825/136675], Loss: 5.7485\n",
      "Epoch [4/5], Step [18900/136675], Loss: 5.2603\n",
      "Epoch [4/5], Step [18975/136675], Loss: 5.2026\n",
      "Epoch [4/5], Step [19050/136675], Loss: 5.1986\n",
      "Epoch [4/5], Step [19125/136675], Loss: 5.3933\n",
      "Epoch [4/5], Step [19200/136675], Loss: 5.6596\n",
      "Epoch [4/5], Step [19275/136675], Loss: 5.3925\n",
      "Epoch [4/5], Step [19350/136675], Loss: 5.1986\n",
      "Epoch [4/5], Step [19425/136675], Loss: 5.2822\n",
      "Epoch [4/5], Step [19500/136675], Loss: 5.1861\n",
      "Epoch [4/5], Step [19575/136675], Loss: 5.3364\n",
      "Epoch [4/5], Step [19650/136675], Loss: 5.2072\n",
      "Epoch [4/5], Step [19725/136675], Loss: 5.4120\n",
      "Epoch [4/5], Step [19800/136675], Loss: 5.4814\n",
      "Epoch [4/5], Step [19875/136675], Loss: 5.2602\n",
      "Epoch [4/5], Step [19950/136675], Loss: 5.2812\n",
      "Validation perplexity: 157.64449363281923\n",
      "Epoch [4/5], Step [20025/136675], Loss: 5.1877\n",
      "Epoch [4/5], Step [20100/136675], Loss: 5.2710\n",
      "Epoch [4/5], Step [20175/136675], Loss: 5.2257\n",
      "Epoch [4/5], Step [20250/136675], Loss: 5.0739\n",
      "Epoch [4/5], Step [20325/136675], Loss: 5.1790\n",
      "Epoch [4/5], Step [20400/136675], Loss: 5.1006\n",
      "Epoch [4/5], Step [20475/136675], Loss: 5.3464\n",
      "Epoch [4/5], Step [20550/136675], Loss: 5.1908\n",
      "Epoch [4/5], Step [20625/136675], Loss: 5.1599\n",
      "Epoch [4/5], Step [20700/136675], Loss: 5.3231\n",
      "Epoch [4/5], Step [20775/136675], Loss: 5.1914\n",
      "Epoch [4/5], Step [20850/136675], Loss: 5.4629\n",
      "Epoch [4/5], Step [20925/136675], Loss: 5.4919\n",
      "Epoch [4/5], Step [21000/136675], Loss: 5.3959\n",
      "Epoch [4/5], Step [21075/136675], Loss: 5.2745\n",
      "Epoch [4/5], Step [21150/136675], Loss: 5.0411\n",
      "Epoch [4/5], Step [21225/136675], Loss: 5.3020\n",
      "Epoch [4/5], Step [21300/136675], Loss: 5.4032\n",
      "Epoch [4/5], Step [21375/136675], Loss: 5.1006\n",
      "Epoch [4/5], Step [21450/136675], Loss: 4.9588\n",
      "Epoch [4/5], Step [21525/136675], Loss: 5.4588\n",
      "Epoch [4/5], Step [21600/136675], Loss: 5.2368\n",
      "Epoch [4/5], Step [21675/136675], Loss: 5.3590\n",
      "Epoch [4/5], Step [21750/136675], Loss: 5.5126\n",
      "Epoch [4/5], Step [21825/136675], Loss: 5.3675\n",
      "Epoch [4/5], Step [21900/136675], Loss: 5.0475\n",
      "Epoch [4/5], Step [21975/136675], Loss: 5.1896\n",
      "Epoch [4/5], Step [22050/136675], Loss: 5.4816\n",
      "Epoch [4/5], Step [22125/136675], Loss: 5.3233\n",
      "Epoch [4/5], Step [22200/136675], Loss: 5.2742\n",
      "Epoch [4/5], Step [22275/136675], Loss: 5.4529\n",
      "Epoch [4/5], Step [22350/136675], Loss: 5.2487\n",
      "Epoch [4/5], Step [22425/136675], Loss: 5.4558\n",
      "Epoch [4/5], Step [22500/136675], Loss: 5.3962\n",
      "Epoch [4/5], Step [22575/136675], Loss: 5.5732\n",
      "Epoch [4/5], Step [22650/136675], Loss: 4.8481\n",
      "Epoch [4/5], Step [22725/136675], Loss: 5.4515\n",
      "Epoch [4/5], Step [22800/136675], Loss: 5.4561\n",
      "Epoch [4/5], Step [22875/136675], Loss: 5.1997\n",
      "Epoch [4/5], Step [22950/136675], Loss: 5.3476\n",
      "Epoch [4/5], Step [23025/136675], Loss: 5.2515\n",
      "Epoch [4/5], Step [23100/136675], Loss: 5.5921\n",
      "Epoch [4/5], Step [23175/136675], Loss: 5.3224\n",
      "Epoch [4/5], Step [23250/136675], Loss: 5.1333\n",
      "Epoch [4/5], Step [23325/136675], Loss: 5.2524\n",
      "Epoch [4/5], Step [23400/136675], Loss: 5.1923\n",
      "Epoch [4/5], Step [23475/136675], Loss: 5.3069\n",
      "Epoch [4/5], Step [23550/136675], Loss: 4.8005\n",
      "Epoch [4/5], Step [23625/136675], Loss: 5.0974\n",
      "Epoch [4/5], Step [23700/136675], Loss: 5.3789\n",
      "Epoch [4/5], Step [23775/136675], Loss: 5.0671\n",
      "Epoch [4/5], Step [23850/136675], Loss: 5.3351\n",
      "Epoch [4/5], Step [23925/136675], Loss: 5.0189\n",
      "Epoch [4/5], Step [24000/136675], Loss: 5.3455\n",
      "Epoch [4/5], Step [24075/136675], Loss: 5.1336\n",
      "Epoch [4/5], Step [24150/136675], Loss: 4.8874\n",
      "Epoch [4/5], Step [24225/136675], Loss: 5.4058\n",
      "Epoch [4/5], Step [24300/136675], Loss: 5.6543\n",
      "Epoch [4/5], Step [24375/136675], Loss: 5.2003\n",
      "Epoch [4/5], Step [24450/136675], Loss: 5.3278\n",
      "Epoch [4/5], Step [24525/136675], Loss: 5.0690\n",
      "Epoch [4/5], Step [24600/136675], Loss: 5.1697\n",
      "Epoch [4/5], Step [24675/136675], Loss: 5.2399\n",
      "Epoch [4/5], Step [24750/136675], Loss: 5.2096\n",
      "Epoch [4/5], Step [24825/136675], Loss: 5.4359\n",
      "Epoch [4/5], Step [24900/136675], Loss: 5.4469\n",
      "Epoch [4/5], Step [24975/136675], Loss: 5.3942\n",
      "Epoch [4/5], Step [25050/136675], Loss: 5.3315\n",
      "Epoch [4/5], Step [25125/136675], Loss: 5.0974\n",
      "Epoch [4/5], Step [25200/136675], Loss: 5.3288\n",
      "Epoch [4/5], Step [25275/136675], Loss: 5.1638\n",
      "Epoch [4/5], Step [25350/136675], Loss: 5.4147\n",
      "Epoch [4/5], Step [25425/136675], Loss: 4.9796\n",
      "Epoch [4/5], Step [25500/136675], Loss: 5.2747\n",
      "Epoch [4/5], Step [25575/136675], Loss: 5.1406\n",
      "Epoch [4/5], Step [25650/136675], Loss: 5.2344\n",
      "Epoch [4/5], Step [25725/136675], Loss: 5.5050\n",
      "Epoch [4/5], Step [25800/136675], Loss: 5.6681\n",
      "Epoch [4/5], Step [25875/136675], Loss: 5.3139\n",
      "Epoch [4/5], Step [25950/136675], Loss: 5.5208\n",
      "Epoch [4/5], Step [26025/136675], Loss: 5.5195\n",
      "Epoch [4/5], Step [26100/136675], Loss: 5.4108\n",
      "Epoch [4/5], Step [26175/136675], Loss: 5.5773\n",
      "Epoch [4/5], Step [26250/136675], Loss: 5.2561\n",
      "Epoch [4/5], Step [26325/136675], Loss: 5.0518\n",
      "Epoch [4/5], Step [26400/136675], Loss: 4.8712\n",
      "Epoch [4/5], Step [26475/136675], Loss: 5.2250\n",
      "Epoch [4/5], Step [26550/136675], Loss: 5.3334\n",
      "Epoch [4/5], Step [26625/136675], Loss: 5.3019\n",
      "Epoch [4/5], Step [26700/136675], Loss: 5.7194\n",
      "Epoch [4/5], Step [26775/136675], Loss: 4.8031\n",
      "Epoch [4/5], Step [26850/136675], Loss: 5.1659\n",
      "Epoch [4/5], Step [26925/136675], Loss: 5.1446\n",
      "Epoch [4/5], Step [27000/136675], Loss: 5.3827\n",
      "Epoch [4/5], Step [27075/136675], Loss: 5.2710\n",
      "Epoch [4/5], Step [27150/136675], Loss: 5.2808\n",
      "Epoch [4/5], Step [27225/136675], Loss: 5.0444\n",
      "Epoch [4/5], Step [27300/136675], Loss: 5.2296\n",
      "Epoch [4/5], Step [27375/136675], Loss: 5.5019\n",
      "Epoch [4/5], Step [27450/136675], Loss: 5.4034\n",
      "Epoch [4/5], Step [27525/136675], Loss: 5.6125\n",
      "Epoch [4/5], Step [27600/136675], Loss: 5.4603\n",
      "Epoch [4/5], Step [27675/136675], Loss: 5.3950\n",
      "Epoch [4/5], Step [27750/136675], Loss: 5.3155\n",
      "Epoch [4/5], Step [27825/136675], Loss: 5.1154\n",
      "Epoch [4/5], Step [27900/136675], Loss: 4.9670\n",
      "Epoch [4/5], Step [27975/136675], Loss: 5.2984\n",
      "Epoch [4/5], Step [28050/136675], Loss: 5.0137\n",
      "Epoch [4/5], Step [28125/136675], Loss: 5.6269\n",
      "Epoch [4/5], Step [28200/136675], Loss: 5.3561\n",
      "Epoch [4/5], Step [28275/136675], Loss: 5.2784\n",
      "Epoch [4/5], Step [28350/136675], Loss: 5.2829\n",
      "Epoch [4/5], Step [28425/136675], Loss: 5.3214\n",
      "Epoch [4/5], Step [28500/136675], Loss: 5.3690\n",
      "Epoch [4/5], Step [28575/136675], Loss: 5.4802\n",
      "Epoch [4/5], Step [28650/136675], Loss: 5.0936\n",
      "Epoch [4/5], Step [28725/136675], Loss: 5.1868\n",
      "Epoch [4/5], Step [28800/136675], Loss: 5.4526\n",
      "Epoch [4/5], Step [28875/136675], Loss: 5.3361\n",
      "Epoch [4/5], Step [28950/136675], Loss: 5.5294\n",
      "Epoch [4/5], Step [29025/136675], Loss: 4.9931\n",
      "Epoch [4/5], Step [29100/136675], Loss: 5.4468\n",
      "Epoch [4/5], Step [29175/136675], Loss: 5.4707\n",
      "Epoch [4/5], Step [29250/136675], Loss: 5.3594\n",
      "Epoch [4/5], Step [29325/136675], Loss: 4.9958\n",
      "Epoch [4/5], Step [29400/136675], Loss: 5.5464\n",
      "Epoch [4/5], Step [29475/136675], Loss: 5.4410\n",
      "Epoch [4/5], Step [29550/136675], Loss: 5.3441\n",
      "Epoch [4/5], Step [29625/136675], Loss: 5.4817\n",
      "Epoch [4/5], Step [29700/136675], Loss: 5.4525\n",
      "Epoch [4/5], Step [29775/136675], Loss: 5.2061\n",
      "Epoch [4/5], Step [29850/136675], Loss: 5.0876\n",
      "Epoch [4/5], Step [29925/136675], Loss: 5.3863\n",
      "Epoch [4/5], Step [30000/136675], Loss: 4.8933\n",
      "Validation perplexity: 157.88331486801022\n",
      "Epoch [4/5], Step [30075/136675], Loss: 5.5252\n",
      "Epoch [4/5], Step [30150/136675], Loss: 5.5637\n",
      "Epoch [4/5], Step [30225/136675], Loss: 5.3218\n",
      "Epoch [4/5], Step [30300/136675], Loss: 5.3802\n",
      "Epoch [4/5], Step [30375/136675], Loss: 5.3361\n",
      "Epoch [4/5], Step [30450/136675], Loss: 4.7402\n",
      "Epoch [4/5], Step [30525/136675], Loss: 5.2297\n",
      "Epoch [4/5], Step [30600/136675], Loss: 5.1943\n",
      "Epoch [4/5], Step [30675/136675], Loss: 4.8288\n",
      "Epoch [4/5], Step [30750/136675], Loss: 5.2252\n",
      "Epoch [4/5], Step [30825/136675], Loss: 5.2553\n",
      "Epoch [4/5], Step [30900/136675], Loss: 5.1278\n",
      "Epoch [4/5], Step [30975/136675], Loss: 5.5683\n",
      "Epoch [4/5], Step [31050/136675], Loss: 5.2058\n",
      "Epoch [4/5], Step [31125/136675], Loss: 5.3196\n",
      "Epoch [4/5], Step [31200/136675], Loss: 5.3794\n",
      "Epoch [4/5], Step [31275/136675], Loss: 5.0968\n",
      "Epoch [4/5], Step [31350/136675], Loss: 5.3302\n",
      "Epoch [4/5], Step [31425/136675], Loss: 5.2007\n",
      "Epoch [4/5], Step [31500/136675], Loss: 5.2011\n",
      "Epoch [4/5], Step [31575/136675], Loss: 5.5964\n",
      "Epoch [4/5], Step [31650/136675], Loss: 4.9257\n",
      "Epoch [4/5], Step [31725/136675], Loss: 5.0320\n",
      "Epoch [4/5], Step [31800/136675], Loss: 5.1733\n",
      "Epoch [4/5], Step [31875/136675], Loss: 5.2048\n",
      "Epoch [4/5], Step [31950/136675], Loss: 5.2647\n",
      "Epoch [4/5], Step [32025/136675], Loss: 4.9339\n",
      "Epoch [4/5], Step [32100/136675], Loss: 5.1548\n",
      "Epoch [4/5], Step [32175/136675], Loss: 5.3099\n",
      "Epoch [4/5], Step [32250/136675], Loss: 5.4183\n",
      "Epoch [4/5], Step [32325/136675], Loss: 4.9412\n",
      "Epoch [4/5], Step [32400/136675], Loss: 5.2631\n",
      "Epoch [4/5], Step [32475/136675], Loss: 5.2517\n",
      "Epoch [4/5], Step [32550/136675], Loss: 5.1140\n",
      "Epoch [4/5], Step [32625/136675], Loss: 5.4330\n",
      "Epoch [4/5], Step [32700/136675], Loss: 5.2525\n",
      "Epoch [4/5], Step [32775/136675], Loss: 5.3212\n",
      "Epoch [4/5], Step [32850/136675], Loss: 5.0080\n",
      "Epoch [4/5], Step [32925/136675], Loss: 5.2616\n",
      "Epoch [4/5], Step [33000/136675], Loss: 5.2039\n",
      "Epoch [4/5], Step [33075/136675], Loss: 5.1662\n",
      "Epoch [4/5], Step [33150/136675], Loss: 5.2904\n",
      "Epoch [4/5], Step [33225/136675], Loss: 5.3936\n",
      "Epoch [4/5], Step [33300/136675], Loss: 5.0753\n",
      "Epoch [4/5], Step [33375/136675], Loss: 5.3752\n",
      "Epoch [4/5], Step [33450/136675], Loss: 5.1221\n",
      "Epoch [4/5], Step [33525/136675], Loss: 5.5875\n",
      "Epoch [4/5], Step [33600/136675], Loss: 5.3403\n",
      "Epoch [4/5], Step [33675/136675], Loss: 5.2357\n",
      "Epoch [4/5], Step [33750/136675], Loss: 5.3552\n",
      "Epoch [4/5], Step [33825/136675], Loss: 5.5958\n",
      "Epoch [4/5], Step [33900/136675], Loss: 5.3197\n",
      "Epoch [4/5], Step [33975/136675], Loss: 5.2831\n",
      "Epoch [4/5], Step [34050/136675], Loss: 5.3905\n",
      "Epoch [4/5], Step [34125/136675], Loss: 5.5895\n",
      "Epoch [4/5], Step [34200/136675], Loss: 4.9536\n",
      "Epoch [4/5], Step [34275/136675], Loss: 4.9128\n",
      "Epoch [4/5], Step [34350/136675], Loss: 5.2962\n",
      "Epoch [4/5], Step [34425/136675], Loss: 5.2688\n",
      "Epoch [4/5], Step [34500/136675], Loss: 5.4922\n",
      "Epoch [4/5], Step [34575/136675], Loss: 5.6556\n",
      "Epoch [4/5], Step [34650/136675], Loss: 5.3059\n",
      "Epoch [4/5], Step [34725/136675], Loss: 5.4339\n",
      "Epoch [4/5], Step [34800/136675], Loss: 5.2361\n",
      "Epoch [4/5], Step [34875/136675], Loss: 5.1981\n",
      "Epoch [4/5], Step [34950/136675], Loss: 5.2511\n",
      "Epoch [4/5], Step [35025/136675], Loss: 5.4063\n",
      "Epoch [4/5], Step [35100/136675], Loss: 5.3198\n",
      "Epoch [4/5], Step [35175/136675], Loss: 5.3350\n",
      "Epoch [4/5], Step [35250/136675], Loss: 5.2119\n",
      "Epoch [4/5], Step [35325/136675], Loss: 5.2432\n",
      "Epoch [4/5], Step [35400/136675], Loss: 5.7338\n",
      "Epoch [4/5], Step [35475/136675], Loss: 5.3893\n",
      "Epoch [4/5], Step [35550/136675], Loss: 5.5916\n",
      "Epoch [4/5], Step [35625/136675], Loss: 5.3357\n",
      "Epoch [4/5], Step [35700/136675], Loss: 5.1514\n",
      "Epoch [4/5], Step [35775/136675], Loss: 5.2300\n",
      "Epoch [4/5], Step [35850/136675], Loss: 5.4041\n",
      "Epoch [4/5], Step [35925/136675], Loss: 5.3959\n",
      "Epoch [4/5], Step [36000/136675], Loss: 5.3242\n",
      "Epoch [4/5], Step [36075/136675], Loss: 5.4977\n",
      "Epoch [4/5], Step [36150/136675], Loss: 5.6002\n",
      "Epoch [4/5], Step [36225/136675], Loss: 5.3768\n",
      "Epoch [4/5], Step [36300/136675], Loss: 5.3603\n",
      "Epoch [4/5], Step [36375/136675], Loss: 5.0342\n",
      "Epoch [4/5], Step [36450/136675], Loss: 5.2046\n",
      "Epoch [4/5], Step [36525/136675], Loss: 5.4906\n",
      "Epoch [4/5], Step [36600/136675], Loss: 5.5309\n",
      "Epoch [4/5], Step [36675/136675], Loss: 5.2888\n",
      "Epoch [4/5], Step [36750/136675], Loss: 5.2080\n",
      "Epoch [4/5], Step [36825/136675], Loss: 5.3932\n",
      "Epoch [4/5], Step [36900/136675], Loss: 5.1818\n",
      "Epoch [4/5], Step [36975/136675], Loss: 5.2760\n",
      "Epoch [4/5], Step [37050/136675], Loss: 5.1065\n",
      "Epoch [4/5], Step [37125/136675], Loss: 5.4881\n",
      "Epoch [4/5], Step [37200/136675], Loss: 5.3084\n",
      "Epoch [4/5], Step [37275/136675], Loss: 5.3599\n",
      "Epoch [4/5], Step [37350/136675], Loss: 5.3615\n",
      "Epoch [4/5], Step [37425/136675], Loss: 5.2272\n",
      "Epoch [4/5], Step [37500/136675], Loss: 5.6178\n",
      "Epoch [4/5], Step [37575/136675], Loss: 5.1253\n",
      "Epoch [4/5], Step [37650/136675], Loss: 5.3771\n",
      "Epoch [4/5], Step [37725/136675], Loss: 5.2332\n",
      "Epoch [4/5], Step [37800/136675], Loss: 5.4500\n",
      "Epoch [4/5], Step [37875/136675], Loss: 5.0805\n",
      "Epoch [4/5], Step [37950/136675], Loss: 5.4543\n",
      "Epoch [4/5], Step [38025/136675], Loss: 5.5471\n",
      "Epoch [4/5], Step [38100/136675], Loss: 5.2900\n",
      "Epoch [4/5], Step [38175/136675], Loss: 5.0501\n",
      "Epoch [4/5], Step [38250/136675], Loss: 5.3665\n",
      "Epoch [4/5], Step [38325/136675], Loss: 5.1319\n",
      "Epoch [4/5], Step [38400/136675], Loss: 5.3126\n",
      "Epoch [4/5], Step [38475/136675], Loss: 5.5997\n",
      "Epoch [4/5], Step [38550/136675], Loss: 5.5212\n",
      "Epoch [4/5], Step [38625/136675], Loss: 4.8573\n",
      "Epoch [4/5], Step [38700/136675], Loss: 5.3804\n",
      "Epoch [4/5], Step [38775/136675], Loss: 5.4151\n",
      "Epoch [4/5], Step [38850/136675], Loss: 5.4704\n",
      "Epoch [4/5], Step [38925/136675], Loss: 5.1261\n",
      "Epoch [4/5], Step [39000/136675], Loss: 5.3717\n",
      "Epoch [4/5], Step [39075/136675], Loss: 5.0885\n",
      "Epoch [4/5], Step [39150/136675], Loss: 5.4114\n",
      "Epoch [4/5], Step [39225/136675], Loss: 5.0633\n",
      "Epoch [4/5], Step [39300/136675], Loss: 5.4299\n",
      "Epoch [4/5], Step [39375/136675], Loss: 5.4388\n",
      "Epoch [4/5], Step [39450/136675], Loss: 5.5484\n",
      "Epoch [4/5], Step [39525/136675], Loss: 5.4752\n",
      "Epoch [4/5], Step [39600/136675], Loss: 5.1656\n",
      "Epoch [4/5], Step [39675/136675], Loss: 5.4462\n",
      "Epoch [4/5], Step [39750/136675], Loss: 5.2099\n",
      "Epoch [4/5], Step [39825/136675], Loss: 4.9090\n",
      "Epoch [4/5], Step [39900/136675], Loss: 5.2605\n",
      "Epoch [4/5], Step [39975/136675], Loss: 5.2364\n",
      "Validation perplexity: 157.24171216516103\n",
      "Epoch [4/5], Step [40050/136675], Loss: 5.5628\n",
      "Epoch [4/5], Step [40125/136675], Loss: 5.4425\n",
      "Epoch [4/5], Step [40200/136675], Loss: 5.4287\n",
      "Epoch [4/5], Step [40275/136675], Loss: 5.3957\n",
      "Epoch [4/5], Step [40350/136675], Loss: 5.0816\n",
      "Epoch [4/5], Step [40425/136675], Loss: 5.1285\n",
      "Epoch [4/5], Step [40500/136675], Loss: 5.1593\n",
      "Epoch [4/5], Step [40575/136675], Loss: 5.3003\n",
      "Epoch [4/5], Step [40650/136675], Loss: 5.3391\n",
      "Epoch [4/5], Step [40725/136675], Loss: 5.7943\n",
      "Epoch [4/5], Step [40800/136675], Loss: 5.5322\n",
      "Epoch [4/5], Step [40875/136675], Loss: 5.3610\n",
      "Epoch [4/5], Step [40950/136675], Loss: 5.2677\n",
      "Epoch [4/5], Step [41025/136675], Loss: 5.1701\n",
      "Epoch [4/5], Step [41100/136675], Loss: 5.4406\n",
      "Epoch [4/5], Step [41175/136675], Loss: 5.2524\n",
      "Epoch [4/5], Step [41250/136675], Loss: 4.9622\n",
      "Epoch [4/5], Step [41325/136675], Loss: 5.4331\n",
      "Epoch [4/5], Step [41400/136675], Loss: 5.2663\n",
      "Epoch [4/5], Step [41475/136675], Loss: 5.6066\n",
      "Epoch [4/5], Step [41550/136675], Loss: 5.3499\n",
      "Epoch [4/5], Step [41625/136675], Loss: 5.0620\n",
      "Epoch [4/5], Step [41700/136675], Loss: 5.3067\n",
      "Epoch [4/5], Step [41775/136675], Loss: 5.5362\n",
      "Epoch [4/5], Step [41850/136675], Loss: 5.2243\n",
      "Epoch [4/5], Step [41925/136675], Loss: 5.4957\n",
      "Epoch [4/5], Step [42000/136675], Loss: 5.4099\n",
      "Epoch [4/5], Step [42075/136675], Loss: 5.2864\n",
      "Epoch [4/5], Step [42150/136675], Loss: 4.9556\n",
      "Epoch [4/5], Step [42225/136675], Loss: 5.5237\n",
      "Epoch [4/5], Step [42300/136675], Loss: 5.2134\n",
      "Epoch [4/5], Step [42375/136675], Loss: 5.1137\n",
      "Epoch [4/5], Step [42450/136675], Loss: 5.3158\n",
      "Epoch [4/5], Step [42525/136675], Loss: 5.6162\n",
      "Epoch [4/5], Step [42600/136675], Loss: 5.0492\n",
      "Epoch [4/5], Step [42675/136675], Loss: 5.5231\n",
      "Epoch [4/5], Step [42750/136675], Loss: 5.0071\n",
      "Epoch [4/5], Step [42825/136675], Loss: 5.2319\n",
      "Epoch [4/5], Step [42900/136675], Loss: 5.4671\n",
      "Epoch [4/5], Step [42975/136675], Loss: 5.0092\n",
      "Epoch [4/5], Step [43050/136675], Loss: 5.0624\n",
      "Epoch [4/5], Step [43125/136675], Loss: 5.3454\n",
      "Epoch [4/5], Step [43200/136675], Loss: 5.5298\n",
      "Epoch [4/5], Step [43275/136675], Loss: 5.5187\n",
      "Epoch [4/5], Step [43350/136675], Loss: 5.2312\n",
      "Epoch [4/5], Step [43425/136675], Loss: 5.0351\n",
      "Epoch [4/5], Step [43500/136675], Loss: 5.1038\n",
      "Epoch [4/5], Step [43575/136675], Loss: 5.3383\n",
      "Epoch [4/5], Step [43650/136675], Loss: 5.0984\n",
      "Epoch [4/5], Step [43725/136675], Loss: 5.1477\n",
      "Epoch [4/5], Step [43800/136675], Loss: 5.0992\n",
      "Epoch [4/5], Step [43875/136675], Loss: 4.9261\n",
      "Epoch [4/5], Step [43950/136675], Loss: 5.2386\n",
      "Epoch [4/5], Step [44025/136675], Loss: 5.1544\n",
      "Epoch [4/5], Step [44100/136675], Loss: 5.1506\n",
      "Epoch [4/5], Step [44175/136675], Loss: 5.2436\n",
      "Epoch [4/5], Step [44250/136675], Loss: 5.0974\n",
      "Epoch [4/5], Step [44325/136675], Loss: 5.5726\n",
      "Epoch [4/5], Step [44400/136675], Loss: 5.4134\n",
      "Epoch [4/5], Step [44475/136675], Loss: 5.3506\n",
      "Epoch [4/5], Step [44550/136675], Loss: 5.3021\n",
      "Epoch [4/5], Step [44625/136675], Loss: 5.4110\n",
      "Epoch [4/5], Step [44700/136675], Loss: 5.1867\n",
      "Epoch [4/5], Step [44775/136675], Loss: 5.3545\n",
      "Epoch [4/5], Step [44850/136675], Loss: 5.2415\n",
      "Epoch [4/5], Step [44925/136675], Loss: 5.4760\n",
      "Epoch [4/5], Step [45000/136675], Loss: 5.4258\n",
      "Epoch [4/5], Step [45075/136675], Loss: 5.5089\n",
      "Epoch [4/5], Step [45150/136675], Loss: 5.4275\n",
      "Epoch [4/5], Step [45225/136675], Loss: 5.2643\n",
      "Epoch [4/5], Step [45300/136675], Loss: 5.4756\n",
      "Epoch [4/5], Step [45375/136675], Loss: 5.3189\n",
      "Epoch [4/5], Step [45450/136675], Loss: 5.2425\n",
      "Epoch [4/5], Step [45525/136675], Loss: 5.3193\n",
      "Epoch [4/5], Step [45600/136675], Loss: 5.4767\n",
      "Epoch [4/5], Step [45675/136675], Loss: 5.3344\n",
      "Epoch [4/5], Step [45750/136675], Loss: 5.2787\n",
      "Epoch [4/5], Step [45825/136675], Loss: 5.2991\n",
      "Epoch [4/5], Step [45900/136675], Loss: 5.1231\n",
      "Epoch [4/5], Step [45975/136675], Loss: 5.2467\n",
      "Epoch [4/5], Step [46050/136675], Loss: 5.5262\n",
      "Epoch [4/5], Step [46125/136675], Loss: 5.2493\n",
      "Epoch [4/5], Step [46200/136675], Loss: 5.3645\n",
      "Epoch [4/5], Step [46275/136675], Loss: 5.1280\n",
      "Epoch [4/5], Step [46350/136675], Loss: 5.6293\n",
      "Epoch [4/5], Step [46425/136675], Loss: 5.1709\n",
      "Epoch [4/5], Step [46500/136675], Loss: 5.4046\n",
      "Epoch [4/5], Step [46575/136675], Loss: 5.4209\n",
      "Epoch [4/5], Step [46650/136675], Loss: 5.3703\n",
      "Epoch [4/5], Step [46725/136675], Loss: 5.2598\n",
      "Epoch [4/5], Step [46800/136675], Loss: 5.2687\n",
      "Epoch [4/5], Step [46875/136675], Loss: 5.7000\n",
      "Epoch [4/5], Step [46950/136675], Loss: 5.4313\n",
      "Epoch [4/5], Step [47025/136675], Loss: 5.5724\n",
      "Epoch [4/5], Step [47100/136675], Loss: 5.4040\n",
      "Epoch [4/5], Step [47175/136675], Loss: 5.6959\n",
      "Epoch [4/5], Step [47250/136675], Loss: 5.1298\n",
      "Epoch [4/5], Step [47325/136675], Loss: 5.1325\n",
      "Epoch [4/5], Step [47400/136675], Loss: 5.4027\n",
      "Epoch [4/5], Step [47475/136675], Loss: 5.1544\n",
      "Epoch [4/5], Step [47550/136675], Loss: 5.4029\n",
      "Epoch [4/5], Step [47625/136675], Loss: 5.5071\n",
      "Epoch [4/5], Step [47700/136675], Loss: 5.3908\n",
      "Epoch [4/5], Step [47775/136675], Loss: 5.3893\n",
      "Epoch [4/5], Step [47850/136675], Loss: 5.5169\n",
      "Epoch [4/5], Step [47925/136675], Loss: 5.4714\n",
      "Epoch [4/5], Step [48000/136675], Loss: 5.3641\n",
      "Epoch [4/5], Step [48075/136675], Loss: 5.4181\n",
      "Epoch [4/5], Step [48150/136675], Loss: 5.2953\n",
      "Epoch [4/5], Step [48225/136675], Loss: 5.3858\n",
      "Epoch [4/5], Step [48300/136675], Loss: 5.2615\n",
      "Epoch [4/5], Step [48375/136675], Loss: 5.4093\n",
      "Epoch [4/5], Step [48450/136675], Loss: 4.9741\n",
      "Epoch [4/5], Step [48525/136675], Loss: 5.2230\n",
      "Epoch [4/5], Step [48600/136675], Loss: 5.2061\n",
      "Epoch [4/5], Step [48675/136675], Loss: 5.4292\n",
      "Epoch [4/5], Step [48750/136675], Loss: 5.1404\n",
      "Epoch [4/5], Step [48825/136675], Loss: 5.2497\n",
      "Epoch [4/5], Step [48900/136675], Loss: 5.1729\n",
      "Epoch [4/5], Step [48975/136675], Loss: 5.2076\n",
      "Epoch [4/5], Step [49050/136675], Loss: 5.4369\n",
      "Epoch [4/5], Step [49125/136675], Loss: 5.1102\n",
      "Epoch [4/5], Step [49200/136675], Loss: 5.4372\n",
      "Epoch [4/5], Step [49275/136675], Loss: 5.5021\n",
      "Epoch [4/5], Step [49350/136675], Loss: 4.8371\n",
      "Epoch [4/5], Step [49425/136675], Loss: 5.1758\n",
      "Epoch [4/5], Step [49500/136675], Loss: 5.1622\n",
      "Epoch [4/5], Step [49575/136675], Loss: 5.1832\n",
      "Epoch [4/5], Step [49650/136675], Loss: 5.1713\n",
      "Epoch [4/5], Step [49725/136675], Loss: 5.3188\n",
      "Epoch [4/5], Step [49800/136675], Loss: 5.4399\n",
      "Epoch [4/5], Step [49875/136675], Loss: 5.2420\n",
      "Epoch [4/5], Step [49950/136675], Loss: 5.5323\n",
      "Validation perplexity: 160.1518974170643\n",
      "Epoch [4/5], Step [50025/136675], Loss: 5.1831\n",
      "Epoch [4/5], Step [50100/136675], Loss: 5.4123\n",
      "Epoch [4/5], Step [50175/136675], Loss: 5.1547\n",
      "Epoch [4/5], Step [50250/136675], Loss: 5.0457\n",
      "Epoch [4/5], Step [50325/136675], Loss: 5.0458\n",
      "Epoch [4/5], Step [50400/136675], Loss: 5.1433\n",
      "Epoch [4/5], Step [50475/136675], Loss: 5.2284\n",
      "Epoch [4/5], Step [50550/136675], Loss: 5.0856\n",
      "Epoch [4/5], Step [50625/136675], Loss: 5.3981\n",
      "Epoch [4/5], Step [50700/136675], Loss: 5.1996\n",
      "Epoch [4/5], Step [50775/136675], Loss: 5.3087\n",
      "Epoch [4/5], Step [50850/136675], Loss: 5.2938\n",
      "Epoch [4/5], Step [50925/136675], Loss: 5.2874\n",
      "Epoch [4/5], Step [51000/136675], Loss: 5.2582\n",
      "Epoch [4/5], Step [51075/136675], Loss: 5.1006\n",
      "Epoch [4/5], Step [51150/136675], Loss: 5.2307\n",
      "Epoch [4/5], Step [51225/136675], Loss: 5.5483\n",
      "Epoch [4/5], Step [51300/136675], Loss: 5.1529\n",
      "Epoch [4/5], Step [51375/136675], Loss: 5.5034\n",
      "Epoch [4/5], Step [51450/136675], Loss: 5.5910\n",
      "Epoch [4/5], Step [51525/136675], Loss: 5.2681\n",
      "Epoch [4/5], Step [51600/136675], Loss: 5.3834\n",
      "Epoch [4/5], Step [51675/136675], Loss: 5.3415\n",
      "Epoch [4/5], Step [51750/136675], Loss: 5.1772\n",
      "Epoch [4/5], Step [51825/136675], Loss: 5.2931\n",
      "Epoch [4/5], Step [51900/136675], Loss: 5.4469\n",
      "Epoch [4/5], Step [51975/136675], Loss: 5.6828\n",
      "Epoch [4/5], Step [52050/136675], Loss: 5.1852\n",
      "Epoch [4/5], Step [52125/136675], Loss: 5.0642\n",
      "Epoch [4/5], Step [52200/136675], Loss: 5.1699\n",
      "Epoch [4/5], Step [52275/136675], Loss: 5.2669\n",
      "Epoch [4/5], Step [52350/136675], Loss: 5.1905\n",
      "Epoch [4/5], Step [52425/136675], Loss: 5.2690\n",
      "Epoch [4/5], Step [52500/136675], Loss: 5.4805\n",
      "Epoch [4/5], Step [52575/136675], Loss: 5.0622\n",
      "Epoch [4/5], Step [52650/136675], Loss: 5.0940\n",
      "Epoch [4/5], Step [52725/136675], Loss: 5.3077\n",
      "Epoch [4/5], Step [52800/136675], Loss: 5.0409\n",
      "Epoch [4/5], Step [52875/136675], Loss: 5.0751\n",
      "Epoch [4/5], Step [52950/136675], Loss: 5.0415\n",
      "Epoch [4/5], Step [53025/136675], Loss: 5.0246\n",
      "Epoch [4/5], Step [53100/136675], Loss: 5.3015\n",
      "Epoch [4/5], Step [53175/136675], Loss: 5.4023\n",
      "Epoch [4/5], Step [53250/136675], Loss: 5.5224\n",
      "Epoch [4/5], Step [53325/136675], Loss: 5.2652\n",
      "Epoch [4/5], Step [53400/136675], Loss: 5.3456\n",
      "Epoch [4/5], Step [53475/136675], Loss: 5.4063\n",
      "Epoch [4/5], Step [53550/136675], Loss: 5.5363\n",
      "Epoch [4/5], Step [53625/136675], Loss: 5.1971\n",
      "Epoch [4/5], Step [53700/136675], Loss: 5.1780\n",
      "Epoch [4/5], Step [53775/136675], Loss: 5.4324\n",
      "Epoch [4/5], Step [53850/136675], Loss: 5.4087\n",
      "Epoch [4/5], Step [53925/136675], Loss: 5.4756\n",
      "Epoch [4/5], Step [54000/136675], Loss: 5.1015\n",
      "Epoch [4/5], Step [54075/136675], Loss: 5.3619\n",
      "Epoch [4/5], Step [54150/136675], Loss: 5.1636\n",
      "Epoch [4/5], Step [54225/136675], Loss: 5.4320\n",
      "Epoch [4/5], Step [54300/136675], Loss: 5.4479\n",
      "Epoch [4/5], Step [54375/136675], Loss: 5.5163\n",
      "Epoch [4/5], Step [54450/136675], Loss: 5.1410\n",
      "Epoch [4/5], Step [54525/136675], Loss: 5.4153\n",
      "Epoch [4/5], Step [54600/136675], Loss: 4.9988\n",
      "Epoch [4/5], Step [54675/136675], Loss: 5.2676\n",
      "Epoch [4/5], Step [54750/136675], Loss: 4.8840\n",
      "Epoch [4/5], Step [54825/136675], Loss: 5.3852\n",
      "Epoch [4/5], Step [54900/136675], Loss: 5.1133\n",
      "Epoch [4/5], Step [54975/136675], Loss: 5.2026\n",
      "Epoch [4/5], Step [55050/136675], Loss: 5.2834\n",
      "Epoch [4/5], Step [55125/136675], Loss: 5.3444\n",
      "Epoch [4/5], Step [55200/136675], Loss: 5.2156\n",
      "Epoch [4/5], Step [55275/136675], Loss: 5.5135\n",
      "Epoch [4/5], Step [55350/136675], Loss: 5.2085\n",
      "Epoch [4/5], Step [55425/136675], Loss: 5.4153\n",
      "Epoch [4/5], Step [55500/136675], Loss: 5.4003\n",
      "Epoch [4/5], Step [55575/136675], Loss: 5.3694\n",
      "Epoch [4/5], Step [55650/136675], Loss: 5.2946\n",
      "Epoch [4/5], Step [55725/136675], Loss: 5.0275\n",
      "Epoch [4/5], Step [55800/136675], Loss: 5.3635\n",
      "Epoch [4/5], Step [55875/136675], Loss: 4.7952\n",
      "Epoch [4/5], Step [55950/136675], Loss: 5.3845\n",
      "Epoch [4/5], Step [56025/136675], Loss: 5.6621\n",
      "Epoch [4/5], Step [56100/136675], Loss: 5.1782\n",
      "Epoch [4/5], Step [56175/136675], Loss: 5.1935\n",
      "Epoch [4/5], Step [56250/136675], Loss: 5.4703\n",
      "Epoch [4/5], Step [56325/136675], Loss: 5.4727\n",
      "Epoch [4/5], Step [56400/136675], Loss: 5.0698\n",
      "Epoch [4/5], Step [56475/136675], Loss: 5.4441\n",
      "Epoch [4/5], Step [56550/136675], Loss: 5.0115\n",
      "Epoch [4/5], Step [56625/136675], Loss: 5.4355\n",
      "Epoch [4/5], Step [56700/136675], Loss: 5.3820\n",
      "Epoch [4/5], Step [56775/136675], Loss: 5.3917\n",
      "Epoch [4/5], Step [56850/136675], Loss: 4.9526\n",
      "Epoch [4/5], Step [56925/136675], Loss: 5.1567\n",
      "Epoch [4/5], Step [57000/136675], Loss: 5.3414\n",
      "Epoch [4/5], Step [57075/136675], Loss: 5.5855\n",
      "Epoch [4/5], Step [57150/136675], Loss: 5.4304\n",
      "Epoch [4/5], Step [57225/136675], Loss: 5.1085\n",
      "Epoch [4/5], Step [57300/136675], Loss: 5.6467\n",
      "Epoch [4/5], Step [57375/136675], Loss: 5.3094\n",
      "Epoch [4/5], Step [57450/136675], Loss: 4.9122\n",
      "Epoch [4/5], Step [57525/136675], Loss: 5.3961\n",
      "Epoch [4/5], Step [57600/136675], Loss: 5.2009\n",
      "Epoch [4/5], Step [57675/136675], Loss: 5.3059\n",
      "Epoch [4/5], Step [57750/136675], Loss: 5.6146\n",
      "Epoch [4/5], Step [57825/136675], Loss: 5.3513\n",
      "Epoch [4/5], Step [57900/136675], Loss: 5.3287\n",
      "Epoch [4/5], Step [57975/136675], Loss: 5.7843\n",
      "Epoch [4/5], Step [58050/136675], Loss: 4.9099\n",
      "Epoch [4/5], Step [58125/136675], Loss: 5.5778\n",
      "Epoch [4/5], Step [58200/136675], Loss: 5.5535\n",
      "Epoch [4/5], Step [58275/136675], Loss: 5.2159\n",
      "Epoch [4/5], Step [58350/136675], Loss: 5.1986\n",
      "Epoch [4/5], Step [58425/136675], Loss: 5.3423\n",
      "Epoch [4/5], Step [58500/136675], Loss: 5.4857\n",
      "Epoch [4/5], Step [58575/136675], Loss: 5.2398\n",
      "Epoch [4/5], Step [58650/136675], Loss: 5.2305\n",
      "Epoch [4/5], Step [58725/136675], Loss: 5.0441\n",
      "Epoch [4/5], Step [58800/136675], Loss: 5.6088\n",
      "Epoch [4/5], Step [58875/136675], Loss: 5.2066\n",
      "Epoch [4/5], Step [58950/136675], Loss: 5.4621\n",
      "Epoch [4/5], Step [59025/136675], Loss: 5.0888\n",
      "Epoch [4/5], Step [59100/136675], Loss: 5.4479\n",
      "Epoch [4/5], Step [59175/136675], Loss: 4.9717\n",
      "Epoch [4/5], Step [59250/136675], Loss: 5.1701\n",
      "Epoch [4/5], Step [59325/136675], Loss: 5.2293\n",
      "Epoch [4/5], Step [59400/136675], Loss: 5.4140\n",
      "Epoch [4/5], Step [59475/136675], Loss: 5.3057\n",
      "Epoch [4/5], Step [59550/136675], Loss: 5.1811\n",
      "Epoch [4/5], Step [59625/136675], Loss: 5.1873\n",
      "Epoch [4/5], Step [59700/136675], Loss: 5.0605\n",
      "Epoch [4/5], Step [59775/136675], Loss: 5.0865\n",
      "Epoch [4/5], Step [59850/136675], Loss: 5.4503\n",
      "Epoch [4/5], Step [59925/136675], Loss: 5.2208\n",
      "Epoch [4/5], Step [60000/136675], Loss: 4.9842\n",
      "Validation perplexity: 157.31401870616077\n",
      "Epoch [4/5], Step [60075/136675], Loss: 5.3319\n",
      "Epoch [4/5], Step [60150/136675], Loss: 5.1862\n",
      "Epoch [4/5], Step [60225/136675], Loss: 5.1358\n",
      "Epoch [4/5], Step [60300/136675], Loss: 5.4071\n",
      "Epoch [4/5], Step [60375/136675], Loss: 5.2903\n",
      "Epoch [4/5], Step [60450/136675], Loss: 5.2516\n",
      "Epoch [4/5], Step [60525/136675], Loss: 5.1807\n",
      "Epoch [4/5], Step [60600/136675], Loss: 5.6564\n",
      "Epoch [4/5], Step [60675/136675], Loss: 5.4156\n",
      "Epoch [4/5], Step [60750/136675], Loss: 5.0610\n",
      "Epoch [4/5], Step [60825/136675], Loss: 5.4047\n",
      "Epoch [4/5], Step [60900/136675], Loss: 5.3772\n",
      "Epoch [4/5], Step [60975/136675], Loss: 5.2370\n",
      "Epoch [4/5], Step [61050/136675], Loss: 5.4954\n",
      "Epoch [4/5], Step [61125/136675], Loss: 5.1852\n",
      "Epoch [4/5], Step [61200/136675], Loss: 5.4965\n",
      "Epoch [4/5], Step [61275/136675], Loss: 5.1511\n",
      "Epoch [4/5], Step [61350/136675], Loss: 5.2954\n",
      "Epoch [4/5], Step [61425/136675], Loss: 5.2709\n",
      "Epoch [4/5], Step [61500/136675], Loss: 5.3284\n",
      "Epoch [4/5], Step [61575/136675], Loss: 5.4266\n",
      "Epoch [4/5], Step [61650/136675], Loss: 5.1792\n",
      "Epoch [4/5], Step [61725/136675], Loss: 5.1367\n",
      "Epoch [4/5], Step [61800/136675], Loss: 4.9133\n",
      "Epoch [4/5], Step [61875/136675], Loss: 5.2231\n",
      "Epoch [4/5], Step [61950/136675], Loss: 5.2443\n",
      "Epoch [4/5], Step [62025/136675], Loss: 5.5116\n",
      "Epoch [4/5], Step [62100/136675], Loss: 5.5541\n",
      "Epoch [4/5], Step [62175/136675], Loss: 5.1368\n",
      "Epoch [4/5], Step [62250/136675], Loss: 5.2031\n",
      "Epoch [4/5], Step [62325/136675], Loss: 5.0775\n",
      "Epoch [4/5], Step [62400/136675], Loss: 5.0427\n",
      "Epoch [4/5], Step [62475/136675], Loss: 5.3473\n",
      "Epoch [4/5], Step [62550/136675], Loss: 5.1907\n",
      "Epoch [4/5], Step [62625/136675], Loss: 5.1523\n",
      "Epoch [4/5], Step [62700/136675], Loss: 5.3361\n",
      "Epoch [4/5], Step [62775/136675], Loss: 5.3267\n",
      "Epoch [4/5], Step [62850/136675], Loss: 5.5177\n",
      "Epoch [4/5], Step [62925/136675], Loss: 5.3715\n",
      "Epoch [4/5], Step [63000/136675], Loss: 5.0338\n",
      "Epoch [4/5], Step [63075/136675], Loss: 5.2742\n",
      "Epoch [4/5], Step [63150/136675], Loss: 5.1948\n",
      "Epoch [4/5], Step [63225/136675], Loss: 5.0466\n",
      "Epoch [4/5], Step [63300/136675], Loss: 5.3976\n",
      "Epoch [4/5], Step [63375/136675], Loss: 5.1551\n",
      "Epoch [4/5], Step [63450/136675], Loss: 5.4010\n",
      "Epoch [4/5], Step [63525/136675], Loss: 5.3392\n",
      "Epoch [4/5], Step [63600/136675], Loss: 5.1912\n",
      "Epoch [4/5], Step [63675/136675], Loss: 5.2809\n",
      "Epoch [4/5], Step [63750/136675], Loss: 5.3149\n",
      "Epoch [4/5], Step [63825/136675], Loss: 5.2142\n",
      "Epoch [4/5], Step [63900/136675], Loss: 5.3182\n",
      "Epoch [4/5], Step [63975/136675], Loss: 5.1926\n",
      "Epoch [4/5], Step [64050/136675], Loss: 5.0279\n",
      "Epoch [4/5], Step [64125/136675], Loss: 5.3454\n",
      "Epoch [4/5], Step [64200/136675], Loss: 5.4609\n",
      "Epoch [4/5], Step [64275/136675], Loss: 5.1775\n",
      "Epoch [4/5], Step [64350/136675], Loss: 5.2067\n",
      "Epoch [4/5], Step [64425/136675], Loss: 5.5736\n",
      "Epoch [4/5], Step [64500/136675], Loss: 5.2363\n",
      "Epoch [4/5], Step [64575/136675], Loss: 5.4018\n",
      "Epoch [4/5], Step [64650/136675], Loss: 5.2441\n",
      "Epoch [4/5], Step [64725/136675], Loss: 5.1030\n",
      "Epoch [4/5], Step [64800/136675], Loss: 5.3704\n",
      "Epoch [4/5], Step [64875/136675], Loss: 5.3662\n",
      "Epoch [4/5], Step [64950/136675], Loss: 5.0330\n",
      "Epoch [4/5], Step [65025/136675], Loss: 5.2483\n",
      "Epoch [4/5], Step [65100/136675], Loss: 4.8881\n",
      "Epoch [4/5], Step [65175/136675], Loss: 5.2932\n",
      "Epoch [4/5], Step [65250/136675], Loss: 5.5188\n",
      "Epoch [4/5], Step [65325/136675], Loss: 5.3016\n",
      "Epoch [4/5], Step [65400/136675], Loss: 5.2241\n",
      "Epoch [4/5], Step [65475/136675], Loss: 5.5367\n",
      "Epoch [4/5], Step [65550/136675], Loss: 5.2849\n",
      "Epoch [4/5], Step [65625/136675], Loss: 5.3444\n",
      "Epoch [4/5], Step [65700/136675], Loss: 5.4843\n",
      "Epoch [4/5], Step [65775/136675], Loss: 5.3182\n",
      "Epoch [4/5], Step [65850/136675], Loss: 5.3571\n",
      "Epoch [4/5], Step [65925/136675], Loss: 5.0754\n",
      "Epoch [4/5], Step [66000/136675], Loss: 5.0456\n",
      "Epoch [4/5], Step [66075/136675], Loss: 5.5131\n",
      "Epoch [4/5], Step [66150/136675], Loss: 5.5309\n",
      "Epoch [4/5], Step [66225/136675], Loss: 5.2484\n",
      "Epoch [4/5], Step [66300/136675], Loss: 5.4263\n",
      "Epoch [4/5], Step [66375/136675], Loss: 5.2367\n",
      "Epoch [4/5], Step [66450/136675], Loss: 5.0737\n",
      "Epoch [4/5], Step [66525/136675], Loss: 5.5863\n",
      "Epoch [4/5], Step [66600/136675], Loss: 5.4057\n",
      "Epoch [4/5], Step [66675/136675], Loss: 5.1990\n",
      "Epoch [4/5], Step [66750/136675], Loss: 5.3097\n",
      "Epoch [4/5], Step [66825/136675], Loss: 5.4316\n",
      "Epoch [4/5], Step [66900/136675], Loss: 5.0671\n",
      "Epoch [4/5], Step [66975/136675], Loss: 5.4091\n",
      "Epoch [4/5], Step [67050/136675], Loss: 5.3779\n",
      "Epoch [4/5], Step [67125/136675], Loss: 5.0494\n",
      "Epoch [4/5], Step [67200/136675], Loss: 5.2226\n",
      "Epoch [4/5], Step [67275/136675], Loss: 5.4414\n",
      "Epoch [4/5], Step [67350/136675], Loss: 5.4624\n",
      "Epoch [4/5], Step [67425/136675], Loss: 5.0667\n",
      "Epoch [4/5], Step [67500/136675], Loss: 5.7205\n",
      "Epoch [4/5], Step [67575/136675], Loss: 5.5983\n",
      "Epoch [4/5], Step [67650/136675], Loss: 5.4289\n",
      "Epoch [4/5], Step [67725/136675], Loss: 5.3603\n",
      "Epoch [4/5], Step [67800/136675], Loss: 5.2304\n",
      "Epoch [4/5], Step [67875/136675], Loss: 5.2513\n",
      "Epoch [4/5], Step [67950/136675], Loss: 5.1350\n",
      "Epoch [4/5], Step [68025/136675], Loss: 5.5580\n",
      "Epoch [4/5], Step [68100/136675], Loss: 5.0976\n",
      "Epoch [4/5], Step [68175/136675], Loss: 5.5145\n",
      "Epoch [4/5], Step [68250/136675], Loss: 5.1639\n",
      "Epoch [4/5], Step [68325/136675], Loss: 5.1400\n",
      "Epoch [4/5], Step [68400/136675], Loss: 5.3390\n",
      "Epoch [4/5], Step [68475/136675], Loss: 5.3836\n",
      "Epoch [4/5], Step [68550/136675], Loss: 5.5131\n",
      "Epoch [4/5], Step [68625/136675], Loss: 5.2843\n",
      "Epoch [4/5], Step [68700/136675], Loss: 5.2215\n",
      "Epoch [4/5], Step [68775/136675], Loss: 5.1642\n",
      "Epoch [4/5], Step [68850/136675], Loss: 5.0862\n",
      "Epoch [4/5], Step [68925/136675], Loss: 5.3371\n",
      "Epoch [4/5], Step [69000/136675], Loss: 5.2732\n",
      "Epoch [4/5], Step [69075/136675], Loss: 5.1824\n",
      "Epoch [4/5], Step [69150/136675], Loss: 5.2934\n",
      "Epoch [4/5], Step [69225/136675], Loss: 5.2461\n",
      "Epoch [4/5], Step [69300/136675], Loss: 5.1112\n",
      "Epoch [4/5], Step [69375/136675], Loss: 5.3791\n",
      "Epoch [4/5], Step [69450/136675], Loss: 5.4347\n",
      "Epoch [4/5], Step [69525/136675], Loss: 4.9654\n",
      "Epoch [4/5], Step [69600/136675], Loss: 5.6425\n",
      "Epoch [4/5], Step [69675/136675], Loss: 5.3066\n",
      "Epoch [4/5], Step [69750/136675], Loss: 4.9346\n",
      "Epoch [4/5], Step [69825/136675], Loss: 4.9130\n",
      "Epoch [4/5], Step [69900/136675], Loss: 5.2514\n",
      "Epoch [4/5], Step [69975/136675], Loss: 5.4383\n",
      "Validation perplexity: 155.61586204136427\n",
      "Epoch [4/5], Step [70050/136675], Loss: 5.2705\n",
      "Epoch [4/5], Step [70125/136675], Loss: 5.2423\n",
      "Epoch [4/5], Step [70200/136675], Loss: 5.4929\n",
      "Epoch [4/5], Step [70275/136675], Loss: 5.2379\n",
      "Epoch [4/5], Step [70350/136675], Loss: 5.4715\n",
      "Epoch [4/5], Step [70425/136675], Loss: 5.1316\n",
      "Epoch [4/5], Step [70500/136675], Loss: 5.0342\n",
      "Epoch [4/5], Step [70575/136675], Loss: 5.2808\n",
      "Epoch [4/5], Step [70650/136675], Loss: 5.2183\n",
      "Epoch [4/5], Step [70725/136675], Loss: 5.3478\n",
      "Epoch [4/5], Step [70800/136675], Loss: 5.2091\n",
      "Epoch [4/5], Step [70875/136675], Loss: 5.4364\n",
      "Epoch [4/5], Step [70950/136675], Loss: 5.2537\n",
      "Epoch [4/5], Step [71025/136675], Loss: 5.5211\n",
      "Epoch [4/5], Step [71100/136675], Loss: 5.3303\n",
      "Epoch [4/5], Step [71175/136675], Loss: 5.3476\n",
      "Epoch [4/5], Step [71250/136675], Loss: 5.0268\n",
      "Epoch [4/5], Step [71325/136675], Loss: 5.2551\n",
      "Epoch [4/5], Step [71400/136675], Loss: 5.1158\n",
      "Epoch [4/5], Step [71475/136675], Loss: 5.2869\n",
      "Epoch [4/5], Step [71550/136675], Loss: 5.0994\n",
      "Epoch [4/5], Step [71625/136675], Loss: 5.4540\n",
      "Epoch [4/5], Step [71700/136675], Loss: 5.0840\n",
      "Epoch [4/5], Step [71775/136675], Loss: 5.4607\n",
      "Epoch [4/5], Step [71850/136675], Loss: 5.3932\n",
      "Epoch [4/5], Step [71925/136675], Loss: 5.4243\n",
      "Epoch [4/5], Step [72000/136675], Loss: 5.3021\n",
      "Epoch [4/5], Step [72075/136675], Loss: 5.2600\n",
      "Epoch [4/5], Step [72150/136675], Loss: 5.3793\n",
      "Epoch [4/5], Step [72225/136675], Loss: 5.1621\n",
      "Epoch [4/5], Step [72300/136675], Loss: 5.4437\n",
      "Epoch [4/5], Step [72375/136675], Loss: 5.3302\n",
      "Epoch [4/5], Step [72450/136675], Loss: 5.0878\n",
      "Epoch [4/5], Step [72525/136675], Loss: 5.3495\n",
      "Epoch [4/5], Step [72600/136675], Loss: 5.0688\n",
      "Epoch [4/5], Step [72675/136675], Loss: 5.1520\n",
      "Epoch [4/5], Step [72750/136675], Loss: 5.3112\n",
      "Epoch [4/5], Step [72825/136675], Loss: 5.2556\n",
      "Epoch [4/5], Step [72900/136675], Loss: 5.3712\n",
      "Epoch [4/5], Step [72975/136675], Loss: 5.1766\n",
      "Epoch [4/5], Step [73050/136675], Loss: 5.2656\n",
      "Epoch [4/5], Step [73125/136675], Loss: 5.3304\n",
      "Epoch [4/5], Step [73200/136675], Loss: 5.1843\n",
      "Epoch [4/5], Step [73275/136675], Loss: 5.2079\n",
      "Epoch [4/5], Step [73350/136675], Loss: 5.4291\n",
      "Epoch [4/5], Step [73425/136675], Loss: 5.2133\n",
      "Epoch [4/5], Step [73500/136675], Loss: 5.4604\n",
      "Epoch [4/5], Step [73575/136675], Loss: 5.1871\n",
      "Epoch [4/5], Step [73650/136675], Loss: 5.1629\n",
      "Epoch [4/5], Step [73725/136675], Loss: 5.2655\n",
      "Epoch [4/5], Step [73800/136675], Loss: 5.5793\n",
      "Epoch [4/5], Step [73875/136675], Loss: 5.1241\n",
      "Epoch [4/5], Step [73950/136675], Loss: 5.0771\n",
      "Epoch [4/5], Step [74025/136675], Loss: 5.2673\n",
      "Epoch [4/5], Step [74100/136675], Loss: 5.3638\n",
      "Epoch [4/5], Step [74175/136675], Loss: 5.0826\n",
      "Epoch [4/5], Step [74250/136675], Loss: 5.2883\n",
      "Epoch [4/5], Step [74325/136675], Loss: 5.2510\n",
      "Epoch [4/5], Step [74400/136675], Loss: 5.2787\n",
      "Epoch [4/5], Step [74475/136675], Loss: 5.0724\n",
      "Epoch [4/5], Step [74550/136675], Loss: 5.1347\n",
      "Epoch [4/5], Step [74625/136675], Loss: 5.2473\n",
      "Epoch [4/5], Step [74700/136675], Loss: 5.3700\n",
      "Epoch [4/5], Step [74775/136675], Loss: 5.1673\n",
      "Epoch [4/5], Step [74850/136675], Loss: 5.2037\n",
      "Epoch [4/5], Step [74925/136675], Loss: 5.5532\n",
      "Epoch [4/5], Step [75000/136675], Loss: 5.5498\n",
      "Epoch [4/5], Step [75075/136675], Loss: 4.9368\n",
      "Epoch [4/5], Step [75150/136675], Loss: 4.8970\n",
      "Epoch [4/5], Step [75225/136675], Loss: 5.2604\n",
      "Epoch [4/5], Step [75300/136675], Loss: 5.1754\n",
      "Epoch [4/5], Step [75375/136675], Loss: 5.3310\n",
      "Epoch [4/5], Step [75450/136675], Loss: 5.3296\n",
      "Epoch [4/5], Step [75525/136675], Loss: 5.1522\n",
      "Epoch [4/5], Step [75600/136675], Loss: 5.3889\n",
      "Epoch [4/5], Step [75675/136675], Loss: 5.3611\n",
      "Epoch [4/5], Step [75750/136675], Loss: 5.1330\n",
      "Epoch [4/5], Step [75825/136675], Loss: 4.9907\n",
      "Epoch [4/5], Step [75900/136675], Loss: 5.1839\n",
      "Epoch [4/5], Step [75975/136675], Loss: 5.4471\n",
      "Epoch [4/5], Step [76050/136675], Loss: 5.3202\n",
      "Epoch [4/5], Step [76125/136675], Loss: 4.9955\n",
      "Epoch [4/5], Step [76200/136675], Loss: 5.3239\n",
      "Epoch [4/5], Step [76275/136675], Loss: 5.4401\n",
      "Epoch [4/5], Step [76350/136675], Loss: 5.3396\n",
      "Epoch [4/5], Step [76425/136675], Loss: 5.3948\n",
      "Epoch [4/5], Step [76500/136675], Loss: 5.2689\n",
      "Epoch [4/5], Step [76575/136675], Loss: 5.2412\n",
      "Epoch [4/5], Step [76650/136675], Loss: 5.4098\n",
      "Epoch [4/5], Step [76725/136675], Loss: 5.3374\n",
      "Epoch [4/5], Step [76800/136675], Loss: 5.2845\n",
      "Epoch [4/5], Step [76875/136675], Loss: 5.1961\n",
      "Epoch [4/5], Step [76950/136675], Loss: 5.1540\n",
      "Epoch [4/5], Step [77025/136675], Loss: 5.4340\n",
      "Epoch [4/5], Step [77100/136675], Loss: 5.4895\n",
      "Epoch [4/5], Step [77175/136675], Loss: 5.4349\n",
      "Epoch [4/5], Step [77250/136675], Loss: 5.0807\n",
      "Epoch [4/5], Step [77325/136675], Loss: 5.3919\n",
      "Epoch [4/5], Step [77400/136675], Loss: 5.2857\n",
      "Epoch [4/5], Step [77475/136675], Loss: 5.3742\n",
      "Epoch [4/5], Step [77550/136675], Loss: 5.4350\n",
      "Epoch [4/5], Step [77625/136675], Loss: 5.1631\n",
      "Epoch [4/5], Step [77700/136675], Loss: 5.3597\n",
      "Epoch [4/5], Step [77775/136675], Loss: 5.2761\n",
      "Epoch [4/5], Step [77850/136675], Loss: 5.1306\n",
      "Epoch [4/5], Step [77925/136675], Loss: 5.4061\n",
      "Epoch [4/5], Step [78000/136675], Loss: 5.2954\n",
      "Epoch [4/5], Step [78075/136675], Loss: 5.4701\n",
      "Epoch [4/5], Step [78150/136675], Loss: 5.4293\n",
      "Epoch [4/5], Step [78225/136675], Loss: 5.0649\n",
      "Epoch [4/5], Step [78300/136675], Loss: 5.3893\n",
      "Epoch [4/5], Step [78375/136675], Loss: 5.0389\n",
      "Epoch [4/5], Step [78450/136675], Loss: 5.3763\n",
      "Epoch [4/5], Step [78525/136675], Loss: 5.2678\n",
      "Epoch [4/5], Step [78600/136675], Loss: 5.0689\n",
      "Epoch [4/5], Step [78675/136675], Loss: 5.2671\n",
      "Epoch [4/5], Step [78750/136675], Loss: 5.1470\n",
      "Epoch [4/5], Step [78825/136675], Loss: 5.3539\n",
      "Epoch [4/5], Step [78900/136675], Loss: 5.4437\n",
      "Epoch [4/5], Step [78975/136675], Loss: 5.5315\n",
      "Epoch [4/5], Step [79050/136675], Loss: 5.1320\n",
      "Epoch [4/5], Step [79125/136675], Loss: 5.1383\n",
      "Epoch [4/5], Step [79200/136675], Loss: 5.6790\n",
      "Epoch [4/5], Step [79275/136675], Loss: 5.3215\n",
      "Epoch [4/5], Step [79350/136675], Loss: 5.0861\n",
      "Epoch [4/5], Step [79425/136675], Loss: 5.5763\n",
      "Epoch [4/5], Step [79500/136675], Loss: 5.4059\n",
      "Epoch [4/5], Step [79575/136675], Loss: 5.1754\n",
      "Epoch [4/5], Step [79650/136675], Loss: 5.1809\n",
      "Epoch [4/5], Step [79725/136675], Loss: 5.0288\n",
      "Epoch [4/5], Step [79800/136675], Loss: 5.3513\n",
      "Epoch [4/5], Step [79875/136675], Loss: 5.3348\n",
      "Epoch [4/5], Step [79950/136675], Loss: 5.4522\n",
      "Validation perplexity: 157.19332716379435\n",
      "Epoch [4/5], Step [80025/136675], Loss: 5.0689\n",
      "Epoch [4/5], Step [80100/136675], Loss: 4.9166\n",
      "Epoch [4/5], Step [80175/136675], Loss: 5.2774\n",
      "Epoch [4/5], Step [80250/136675], Loss: 5.2066\n",
      "Epoch [4/5], Step [80325/136675], Loss: 5.0473\n",
      "Epoch [4/5], Step [80400/136675], Loss: 5.2350\n",
      "Epoch [4/5], Step [80475/136675], Loss: 5.2455\n",
      "Epoch [4/5], Step [80550/136675], Loss: 5.2361\n",
      "Epoch [4/5], Step [80625/136675], Loss: 5.1481\n",
      "Epoch [4/5], Step [80700/136675], Loss: 5.2596\n",
      "Epoch [4/5], Step [80775/136675], Loss: 5.3064\n",
      "Epoch [4/5], Step [80850/136675], Loss: 5.4544\n",
      "Epoch [4/5], Step [80925/136675], Loss: 5.3896\n",
      "Epoch [4/5], Step [81000/136675], Loss: 5.4852\n",
      "Epoch [4/5], Step [81075/136675], Loss: 4.9441\n",
      "Epoch [4/5], Step [81150/136675], Loss: 5.3409\n",
      "Epoch [4/5], Step [81225/136675], Loss: 5.3470\n",
      "Epoch [4/5], Step [81300/136675], Loss: 5.4814\n",
      "Epoch [4/5], Step [81375/136675], Loss: 5.4075\n",
      "Epoch [4/5], Step [81450/136675], Loss: 5.1877\n",
      "Epoch [4/5], Step [81525/136675], Loss: 5.1105\n",
      "Epoch [4/5], Step [81600/136675], Loss: 5.4551\n",
      "Epoch [4/5], Step [81675/136675], Loss: 5.4315\n",
      "Epoch [4/5], Step [81750/136675], Loss: 5.2537\n",
      "Epoch [4/5], Step [81825/136675], Loss: 4.9943\n",
      "Epoch [4/5], Step [81900/136675], Loss: 5.3662\n",
      "Epoch [4/5], Step [81975/136675], Loss: 5.3811\n",
      "Epoch [4/5], Step [82050/136675], Loss: 4.7436\n",
      "Epoch [4/5], Step [82125/136675], Loss: 5.3535\n",
      "Epoch [4/5], Step [82200/136675], Loss: 5.3700\n",
      "Epoch [4/5], Step [82275/136675], Loss: 5.5957\n",
      "Epoch [4/5], Step [82350/136675], Loss: 5.4396\n",
      "Epoch [4/5], Step [82425/136675], Loss: 5.5491\n",
      "Epoch [4/5], Step [82500/136675], Loss: 5.1527\n",
      "Epoch [4/5], Step [82575/136675], Loss: 5.0230\n",
      "Epoch [4/5], Step [82650/136675], Loss: 5.0187\n",
      "Epoch [4/5], Step [82725/136675], Loss: 5.3035\n",
      "Epoch [4/5], Step [82800/136675], Loss: 5.2327\n",
      "Epoch [4/5], Step [82875/136675], Loss: 5.2339\n",
      "Epoch [4/5], Step [82950/136675], Loss: 5.1548\n",
      "Epoch [4/5], Step [83025/136675], Loss: 5.2871\n",
      "Epoch [4/5], Step [83100/136675], Loss: 4.9093\n",
      "Epoch [4/5], Step [83175/136675], Loss: 5.1166\n",
      "Epoch [4/5], Step [83250/136675], Loss: 5.1030\n",
      "Epoch [4/5], Step [83325/136675], Loss: 5.2737\n",
      "Epoch [4/5], Step [83400/136675], Loss: 5.5102\n",
      "Epoch [4/5], Step [83475/136675], Loss: 5.4935\n",
      "Epoch [4/5], Step [83550/136675], Loss: 5.4342\n",
      "Epoch [4/5], Step [83625/136675], Loss: 4.8934\n",
      "Epoch [4/5], Step [83700/136675], Loss: 5.1369\n",
      "Epoch [4/5], Step [83775/136675], Loss: 5.5229\n",
      "Epoch [4/5], Step [83850/136675], Loss: 5.1830\n",
      "Epoch [4/5], Step [83925/136675], Loss: 5.4266\n",
      "Epoch [4/5], Step [84000/136675], Loss: 5.3388\n",
      "Epoch [4/5], Step [84075/136675], Loss: 5.6477\n",
      "Epoch [4/5], Step [84150/136675], Loss: 5.3052\n",
      "Epoch [4/5], Step [84225/136675], Loss: 5.2506\n",
      "Epoch [4/5], Step [84300/136675], Loss: 5.1276\n",
      "Epoch [4/5], Step [84375/136675], Loss: 5.3147\n",
      "Epoch [4/5], Step [84450/136675], Loss: 4.8229\n",
      "Epoch [4/5], Step [84525/136675], Loss: 5.3810\n",
      "Epoch [4/5], Step [84600/136675], Loss: 5.3553\n",
      "Epoch [4/5], Step [84675/136675], Loss: 5.6585\n",
      "Epoch [4/5], Step [84750/136675], Loss: 5.2109\n",
      "Epoch [4/5], Step [84825/136675], Loss: 5.3876\n",
      "Epoch [4/5], Step [84900/136675], Loss: 5.1284\n",
      "Epoch [4/5], Step [84975/136675], Loss: 5.4115\n",
      "Epoch [4/5], Step [85050/136675], Loss: 5.2438\n",
      "Epoch [4/5], Step [85125/136675], Loss: 5.1592\n",
      "Epoch [4/5], Step [85200/136675], Loss: 5.1084\n",
      "Epoch [4/5], Step [85275/136675], Loss: 5.2427\n",
      "Epoch [4/5], Step [85350/136675], Loss: 5.0561\n",
      "Epoch [4/5], Step [85425/136675], Loss: 5.1227\n",
      "Epoch [4/5], Step [85500/136675], Loss: 5.6227\n",
      "Epoch [4/5], Step [85575/136675], Loss: 5.1748\n",
      "Epoch [4/5], Step [85650/136675], Loss: 5.0656\n",
      "Epoch [4/5], Step [85725/136675], Loss: 5.4474\n",
      "Epoch [4/5], Step [85800/136675], Loss: 5.3687\n",
      "Epoch [4/5], Step [85875/136675], Loss: 5.2006\n",
      "Epoch [4/5], Step [85950/136675], Loss: 5.0976\n",
      "Epoch [4/5], Step [86025/136675], Loss: 5.4061\n",
      "Epoch [4/5], Step [86100/136675], Loss: 5.1139\n",
      "Epoch [4/5], Step [86175/136675], Loss: 5.3420\n",
      "Epoch [4/5], Step [86250/136675], Loss: 5.2746\n",
      "Epoch [4/5], Step [86325/136675], Loss: 5.1592\n",
      "Epoch [4/5], Step [86400/136675], Loss: 5.2431\n",
      "Epoch [4/5], Step [86475/136675], Loss: 5.2701\n",
      "Epoch [4/5], Step [86550/136675], Loss: 5.3601\n",
      "Epoch [4/5], Step [86625/136675], Loss: 4.9135\n",
      "Epoch [4/5], Step [86700/136675], Loss: 5.1172\n",
      "Epoch [4/5], Step [86775/136675], Loss: 5.2560\n",
      "Epoch [4/5], Step [86850/136675], Loss: 5.1894\n",
      "Epoch [4/5], Step [86925/136675], Loss: 5.2716\n",
      "Epoch [4/5], Step [87000/136675], Loss: 5.0399\n",
      "Epoch [4/5], Step [87075/136675], Loss: 5.3411\n",
      "Epoch [4/5], Step [87150/136675], Loss: 5.4729\n",
      "Epoch [4/5], Step [87225/136675], Loss: 5.1822\n",
      "Epoch [4/5], Step [87300/136675], Loss: 5.1139\n",
      "Epoch [4/5], Step [87375/136675], Loss: 5.3716\n",
      "Epoch [4/5], Step [87450/136675], Loss: 5.3373\n",
      "Epoch [4/5], Step [87525/136675], Loss: 5.2975\n",
      "Epoch [4/5], Step [87600/136675], Loss: 5.2794\n",
      "Epoch [4/5], Step [87675/136675], Loss: 5.1858\n",
      "Epoch [4/5], Step [87750/136675], Loss: 5.2332\n",
      "Epoch [4/5], Step [87825/136675], Loss: 5.5468\n",
      "Epoch [4/5], Step [87900/136675], Loss: 5.3114\n",
      "Epoch [4/5], Step [87975/136675], Loss: 5.1656\n",
      "Epoch [4/5], Step [88050/136675], Loss: 5.6491\n",
      "Epoch [4/5], Step [88125/136675], Loss: 5.3909\n",
      "Epoch [4/5], Step [88200/136675], Loss: 5.0479\n",
      "Epoch [4/5], Step [88275/136675], Loss: 5.2002\n",
      "Epoch [4/5], Step [88350/136675], Loss: 5.1844\n",
      "Epoch [4/5], Step [88425/136675], Loss: 4.9868\n",
      "Epoch [4/5], Step [88500/136675], Loss: 5.4571\n",
      "Epoch [4/5], Step [88575/136675], Loss: 5.4319\n",
      "Epoch [4/5], Step [88650/136675], Loss: 5.0737\n",
      "Epoch [4/5], Step [88725/136675], Loss: 5.2017\n",
      "Epoch [4/5], Step [88800/136675], Loss: 5.2160\n",
      "Epoch [4/5], Step [88875/136675], Loss: 5.0202\n",
      "Epoch [4/5], Step [88950/136675], Loss: 5.4291\n",
      "Epoch [4/5], Step [89025/136675], Loss: 5.3891\n",
      "Epoch [4/5], Step [89100/136675], Loss: 5.3591\n",
      "Epoch [4/5], Step [89175/136675], Loss: 5.5120\n",
      "Epoch [4/5], Step [89250/136675], Loss: 5.1003\n",
      "Epoch [4/5], Step [89325/136675], Loss: 5.3191\n",
      "Epoch [4/5], Step [89400/136675], Loss: 5.2673\n",
      "Epoch [4/5], Step [89475/136675], Loss: 5.1374\n",
      "Epoch [4/5], Step [89550/136675], Loss: 5.5075\n",
      "Epoch [4/5], Step [89625/136675], Loss: 5.2859\n",
      "Epoch [4/5], Step [89700/136675], Loss: 5.2042\n",
      "Epoch [4/5], Step [89775/136675], Loss: 5.3225\n",
      "Epoch [4/5], Step [89850/136675], Loss: 5.0863\n",
      "Epoch [4/5], Step [89925/136675], Loss: 5.4610\n",
      "Epoch [4/5], Step [90000/136675], Loss: 5.4137\n",
      "Validation perplexity: 156.75115189565068\n",
      "Epoch [4/5], Step [90075/136675], Loss: 5.4572\n",
      "Epoch [4/5], Step [90150/136675], Loss: 5.2455\n",
      "Epoch [4/5], Step [90225/136675], Loss: 5.2580\n",
      "Epoch [4/5], Step [90300/136675], Loss: 5.1212\n",
      "Epoch [4/5], Step [90375/136675], Loss: 5.5445\n",
      "Epoch [4/5], Step [90450/136675], Loss: 5.4124\n",
      "Epoch [4/5], Step [90525/136675], Loss: 5.4054\n",
      "Epoch [4/5], Step [90600/136675], Loss: 5.2746\n",
      "Epoch [4/5], Step [90675/136675], Loss: 5.2441\n",
      "Epoch [4/5], Step [90750/136675], Loss: 5.1787\n",
      "Epoch [4/5], Step [90825/136675], Loss: 5.4793\n",
      "Epoch [4/5], Step [90900/136675], Loss: 5.4166\n",
      "Epoch [4/5], Step [90975/136675], Loss: 5.1164\n",
      "Epoch [4/5], Step [91050/136675], Loss: 5.4913\n",
      "Epoch [4/5], Step [91125/136675], Loss: 5.2308\n",
      "Epoch [4/5], Step [91200/136675], Loss: 5.3598\n",
      "Epoch [4/5], Step [91275/136675], Loss: 5.5757\n",
      "Epoch [4/5], Step [91350/136675], Loss: 5.1825\n",
      "Epoch [4/5], Step [91425/136675], Loss: 5.5735\n",
      "Epoch [4/5], Step [91500/136675], Loss: 5.3286\n",
      "Epoch [4/5], Step [91575/136675], Loss: 5.3500\n",
      "Epoch [4/5], Step [91650/136675], Loss: 5.7389\n",
      "Epoch [4/5], Step [91725/136675], Loss: 5.2176\n",
      "Epoch [4/5], Step [91800/136675], Loss: 5.0522\n",
      "Epoch [4/5], Step [91875/136675], Loss: 5.3809\n",
      "Epoch [4/5], Step [91950/136675], Loss: 4.8069\n",
      "Epoch [4/5], Step [92025/136675], Loss: 5.5570\n",
      "Epoch [4/5], Step [92100/136675], Loss: 5.2234\n",
      "Epoch [4/5], Step [92175/136675], Loss: 5.2979\n",
      "Epoch [4/5], Step [92250/136675], Loss: 4.8892\n",
      "Epoch [4/5], Step [92325/136675], Loss: 5.1447\n",
      "Epoch [4/5], Step [92400/136675], Loss: 5.6170\n",
      "Epoch [4/5], Step [92475/136675], Loss: 5.5078\n",
      "Epoch [4/5], Step [92550/136675], Loss: 5.2733\n",
      "Epoch [4/5], Step [92625/136675], Loss: 5.0191\n",
      "Epoch [4/5], Step [92700/136675], Loss: 5.3127\n",
      "Epoch [4/5], Step [92775/136675], Loss: 5.4229\n",
      "Epoch [4/5], Step [92850/136675], Loss: 5.5875\n",
      "Epoch [4/5], Step [92925/136675], Loss: 5.0705\n",
      "Epoch [4/5], Step [93000/136675], Loss: 5.1600\n",
      "Epoch [4/5], Step [93075/136675], Loss: 5.3952\n",
      "Epoch [4/5], Step [93150/136675], Loss: 5.3493\n",
      "Epoch [4/5], Step [93225/136675], Loss: 5.3852\n",
      "Epoch [4/5], Step [93300/136675], Loss: 5.1593\n",
      "Epoch [4/5], Step [93375/136675], Loss: 5.4004\n",
      "Epoch [4/5], Step [93450/136675], Loss: 5.0810\n",
      "Epoch [4/5], Step [93525/136675], Loss: 5.1727\n",
      "Epoch [4/5], Step [93600/136675], Loss: 5.4455\n",
      "Epoch [4/5], Step [93675/136675], Loss: 5.3853\n",
      "Epoch [4/5], Step [93750/136675], Loss: 5.0955\n",
      "Epoch [4/5], Step [93825/136675], Loss: 5.2645\n",
      "Epoch [4/5], Step [93900/136675], Loss: 5.3488\n",
      "Epoch [4/5], Step [93975/136675], Loss: 5.5092\n",
      "Epoch [4/5], Step [94050/136675], Loss: 5.3813\n",
      "Epoch [4/5], Step [94125/136675], Loss: 4.9955\n",
      "Epoch [4/5], Step [94200/136675], Loss: 5.6388\n",
      "Epoch [4/5], Step [94275/136675], Loss: 5.1525\n",
      "Epoch [4/5], Step [94350/136675], Loss: 5.4664\n",
      "Epoch [4/5], Step [94425/136675], Loss: 5.5006\n",
      "Epoch [4/5], Step [94500/136675], Loss: 5.0425\n",
      "Epoch [4/5], Step [94575/136675], Loss: 5.5462\n",
      "Epoch [4/5], Step [94650/136675], Loss: 5.3752\n",
      "Epoch [4/5], Step [94725/136675], Loss: 5.2191\n",
      "Epoch [4/5], Step [94800/136675], Loss: 5.3917\n",
      "Epoch [4/5], Step [94875/136675], Loss: 5.1945\n",
      "Epoch [4/5], Step [94950/136675], Loss: 5.3480\n",
      "Epoch [4/5], Step [95025/136675], Loss: 5.0436\n",
      "Epoch [4/5], Step [95100/136675], Loss: 5.2888\n",
      "Epoch [4/5], Step [95175/136675], Loss: 5.3855\n",
      "Epoch [4/5], Step [95250/136675], Loss: 5.4210\n",
      "Epoch [4/5], Step [95325/136675], Loss: 5.1633\n",
      "Epoch [4/5], Step [95400/136675], Loss: 5.2875\n",
      "Epoch [4/5], Step [95475/136675], Loss: 5.1206\n",
      "Epoch [4/5], Step [95550/136675], Loss: 5.3546\n",
      "Epoch [4/5], Step [95625/136675], Loss: 5.0634\n",
      "Epoch [4/5], Step [95700/136675], Loss: 5.2850\n",
      "Epoch [4/5], Step [95775/136675], Loss: 5.0214\n",
      "Epoch [4/5], Step [95850/136675], Loss: 5.5062\n",
      "Epoch [4/5], Step [95925/136675], Loss: 5.2603\n",
      "Epoch [4/5], Step [96000/136675], Loss: 4.9826\n",
      "Epoch [4/5], Step [96075/136675], Loss: 5.2470\n",
      "Epoch [4/5], Step [96150/136675], Loss: 5.3896\n",
      "Epoch [4/5], Step [96225/136675], Loss: 5.6768\n",
      "Epoch [4/5], Step [96300/136675], Loss: 5.2235\n",
      "Epoch [4/5], Step [96375/136675], Loss: 5.1624\n",
      "Epoch [4/5], Step [96450/136675], Loss: 5.5461\n",
      "Epoch [4/5], Step [96525/136675], Loss: 5.5417\n",
      "Epoch [4/5], Step [96600/136675], Loss: 5.6698\n",
      "Epoch [4/5], Step [96675/136675], Loss: 5.5045\n",
      "Epoch [4/5], Step [96750/136675], Loss: 5.0556\n",
      "Epoch [4/5], Step [96825/136675], Loss: 5.2046\n",
      "Epoch [4/5], Step [96900/136675], Loss: 5.4300\n",
      "Epoch [4/5], Step [96975/136675], Loss: 5.2400\n",
      "Epoch [4/5], Step [97050/136675], Loss: 5.4016\n",
      "Epoch [4/5], Step [97125/136675], Loss: 5.4918\n",
      "Epoch [4/5], Step [97200/136675], Loss: 5.1697\n",
      "Epoch [4/5], Step [97275/136675], Loss: 5.2453\n",
      "Epoch [4/5], Step [97350/136675], Loss: 4.9441\n",
      "Epoch [4/5], Step [97425/136675], Loss: 5.2521\n",
      "Epoch [4/5], Step [97500/136675], Loss: 5.5578\n",
      "Epoch [4/5], Step [97575/136675], Loss: 5.4119\n",
      "Epoch [4/5], Step [97650/136675], Loss: 5.4833\n",
      "Epoch [4/5], Step [97725/136675], Loss: 5.5525\n",
      "Epoch [4/5], Step [97800/136675], Loss: 5.1944\n",
      "Epoch [4/5], Step [97875/136675], Loss: 5.4492\n",
      "Epoch [4/5], Step [97950/136675], Loss: 5.2197\n",
      "Epoch [4/5], Step [98025/136675], Loss: 5.2427\n",
      "Epoch [4/5], Step [98100/136675], Loss: 5.3163\n",
      "Epoch [4/5], Step [98175/136675], Loss: 5.6105\n",
      "Epoch [4/5], Step [98250/136675], Loss: 5.2200\n",
      "Epoch [4/5], Step [98325/136675], Loss: 5.4258\n",
      "Epoch [4/5], Step [98400/136675], Loss: 5.0993\n",
      "Epoch [4/5], Step [98475/136675], Loss: 5.2027\n",
      "Epoch [4/5], Step [98550/136675], Loss: 5.4612\n",
      "Epoch [4/5], Step [98625/136675], Loss: 5.3915\n",
      "Epoch [4/5], Step [98700/136675], Loss: 5.1177\n",
      "Epoch [4/5], Step [98775/136675], Loss: 5.3242\n",
      "Epoch [4/5], Step [98850/136675], Loss: 5.2109\n",
      "Epoch [4/5], Step [98925/136675], Loss: 5.3122\n",
      "Epoch [4/5], Step [99000/136675], Loss: 5.0982\n",
      "Epoch [4/5], Step [99075/136675], Loss: 5.2750\n",
      "Epoch [4/5], Step [99150/136675], Loss: 5.4016\n",
      "Epoch [4/5], Step [99225/136675], Loss: 5.1898\n",
      "Epoch [4/5], Step [99300/136675], Loss: 5.0112\n",
      "Epoch [4/5], Step [99375/136675], Loss: 5.1927\n",
      "Epoch [4/5], Step [99450/136675], Loss: 5.3407\n",
      "Epoch [4/5], Step [99525/136675], Loss: 5.2209\n",
      "Epoch [4/5], Step [99600/136675], Loss: 5.3815\n",
      "Epoch [4/5], Step [99675/136675], Loss: 5.1908\n",
      "Epoch [4/5], Step [99750/136675], Loss: 5.1094\n",
      "Epoch [4/5], Step [99825/136675], Loss: 5.3799\n",
      "Epoch [4/5], Step [99900/136675], Loss: 5.4596\n",
      "Epoch [4/5], Step [99975/136675], Loss: 5.3100\n",
      "Validation perplexity: 154.934256348326\n",
      "Epoch [4/5], Step [100050/136675], Loss: 5.2386\n",
      "Epoch [4/5], Step [100125/136675], Loss: 5.3250\n",
      "Epoch [4/5], Step [100200/136675], Loss: 5.3945\n",
      "Epoch [4/5], Step [100275/136675], Loss: 5.2165\n",
      "Epoch [4/5], Step [100350/136675], Loss: 5.5052\n",
      "Epoch [4/5], Step [100425/136675], Loss: 5.4266\n",
      "Epoch [4/5], Step [100500/136675], Loss: 5.1330\n",
      "Epoch [4/5], Step [100575/136675], Loss: 5.2351\n",
      "Epoch [4/5], Step [100650/136675], Loss: 5.3410\n",
      "Epoch [4/5], Step [100725/136675], Loss: 5.0542\n",
      "Epoch [4/5], Step [100800/136675], Loss: 5.3412\n",
      "Epoch [4/5], Step [100875/136675], Loss: 5.3888\n",
      "Epoch [4/5], Step [100950/136675], Loss: 5.2541\n",
      "Epoch [4/5], Step [101025/136675], Loss: 5.0998\n",
      "Epoch [4/5], Step [101100/136675], Loss: 5.2738\n",
      "Epoch [4/5], Step [101175/136675], Loss: 5.1125\n",
      "Epoch [4/5], Step [101250/136675], Loss: 5.0613\n",
      "Epoch [4/5], Step [101325/136675], Loss: 5.0135\n",
      "Epoch [4/5], Step [101400/136675], Loss: 5.3377\n",
      "Epoch [4/5], Step [101475/136675], Loss: 5.4929\n",
      "Epoch [4/5], Step [101550/136675], Loss: 5.4291\n",
      "Epoch [4/5], Step [101625/136675], Loss: 5.2973\n",
      "Epoch [4/5], Step [101700/136675], Loss: 5.2985\n",
      "Epoch [4/5], Step [101775/136675], Loss: 5.2536\n",
      "Epoch [4/5], Step [101850/136675], Loss: 5.4328\n",
      "Epoch [4/5], Step [101925/136675], Loss: 5.2069\n",
      "Epoch [4/5], Step [102000/136675], Loss: 5.2328\n",
      "Epoch [4/5], Step [102075/136675], Loss: 5.1512\n",
      "Epoch [4/5], Step [102150/136675], Loss: 5.4047\n",
      "Epoch [4/5], Step [102225/136675], Loss: 5.3482\n",
      "Epoch [4/5], Step [102300/136675], Loss: 5.2221\n",
      "Epoch [4/5], Step [102375/136675], Loss: 5.4237\n",
      "Epoch [4/5], Step [102450/136675], Loss: 5.2400\n",
      "Epoch [4/5], Step [102525/136675], Loss: 5.2891\n",
      "Epoch [4/5], Step [102600/136675], Loss: 5.2918\n",
      "Epoch [4/5], Step [102675/136675], Loss: 5.2569\n",
      "Epoch [4/5], Step [102750/136675], Loss: 5.3819\n",
      "Epoch [4/5], Step [102825/136675], Loss: 5.3503\n",
      "Epoch [4/5], Step [102900/136675], Loss: 5.2191\n",
      "Epoch [4/5], Step [102975/136675], Loss: 5.2517\n",
      "Epoch [4/5], Step [103050/136675], Loss: 5.4194\n",
      "Epoch [4/5], Step [103125/136675], Loss: 5.2545\n",
      "Epoch [4/5], Step [103200/136675], Loss: 5.5580\n",
      "Epoch [4/5], Step [103275/136675], Loss: 5.1483\n",
      "Epoch [4/5], Step [103350/136675], Loss: 4.9411\n",
      "Epoch [4/5], Step [103425/136675], Loss: 5.1559\n",
      "Epoch [4/5], Step [103500/136675], Loss: 5.3018\n",
      "Epoch [4/5], Step [103575/136675], Loss: 5.1880\n",
      "Epoch [4/5], Step [103650/136675], Loss: 5.3532\n",
      "Epoch [4/5], Step [103725/136675], Loss: 5.5470\n",
      "Epoch [4/5], Step [103800/136675], Loss: 5.1361\n",
      "Epoch [4/5], Step [103875/136675], Loss: 5.3804\n",
      "Epoch [4/5], Step [103950/136675], Loss: 5.3079\n",
      "Epoch [4/5], Step [104025/136675], Loss: 5.3884\n",
      "Epoch [4/5], Step [104100/136675], Loss: 5.4950\n",
      "Epoch [4/5], Step [104175/136675], Loss: 5.3660\n",
      "Epoch [4/5], Step [104250/136675], Loss: 5.0952\n",
      "Epoch [4/5], Step [104325/136675], Loss: 5.3935\n",
      "Epoch [4/5], Step [104400/136675], Loss: 5.4121\n",
      "Epoch [4/5], Step [104475/136675], Loss: 5.3436\n",
      "Epoch [4/5], Step [104550/136675], Loss: 5.1391\n",
      "Epoch [4/5], Step [104625/136675], Loss: 5.1692\n",
      "Epoch [4/5], Step [104700/136675], Loss: 5.2786\n",
      "Epoch [4/5], Step [104775/136675], Loss: 5.1626\n",
      "Epoch [4/5], Step [104850/136675], Loss: 5.3704\n",
      "Epoch [4/5], Step [104925/136675], Loss: 5.3085\n",
      "Epoch [4/5], Step [105000/136675], Loss: 5.1938\n",
      "Epoch [4/5], Step [105075/136675], Loss: 4.8571\n",
      "Epoch [4/5], Step [105150/136675], Loss: 5.0552\n",
      "Epoch [4/5], Step [105225/136675], Loss: 5.4274\n",
      "Epoch [4/5], Step [105300/136675], Loss: 5.3288\n",
      "Epoch [4/5], Step [105375/136675], Loss: 5.4210\n",
      "Epoch [4/5], Step [105450/136675], Loss: 5.2464\n",
      "Epoch [4/5], Step [105525/136675], Loss: 5.3642\n",
      "Epoch [4/5], Step [105600/136675], Loss: 5.3876\n",
      "Epoch [4/5], Step [105675/136675], Loss: 5.5472\n",
      "Epoch [4/5], Step [105750/136675], Loss: 5.5500\n",
      "Epoch [4/5], Step [105825/136675], Loss: 5.3243\n",
      "Epoch [4/5], Step [105900/136675], Loss: 5.3854\n",
      "Epoch [4/5], Step [105975/136675], Loss: 5.2552\n",
      "Epoch [4/5], Step [106050/136675], Loss: 5.2786\n",
      "Epoch [4/5], Step [106125/136675], Loss: 5.0666\n",
      "Epoch [4/5], Step [106200/136675], Loss: 5.2988\n",
      "Epoch [4/5], Step [106275/136675], Loss: 5.7156\n",
      "Epoch [4/5], Step [106350/136675], Loss: 5.5345\n",
      "Epoch [4/5], Step [106425/136675], Loss: 5.5007\n",
      "Epoch [4/5], Step [106500/136675], Loss: 5.2720\n",
      "Epoch [4/5], Step [106575/136675], Loss: 5.2423\n",
      "Epoch [4/5], Step [106650/136675], Loss: 5.5157\n",
      "Epoch [4/5], Step [106725/136675], Loss: 5.4019\n",
      "Epoch [4/5], Step [106800/136675], Loss: 4.9197\n",
      "Epoch [4/5], Step [106875/136675], Loss: 5.4073\n",
      "Epoch [4/5], Step [106950/136675], Loss: 5.3622\n",
      "Epoch [4/5], Step [107025/136675], Loss: 5.3932\n",
      "Epoch [4/5], Step [107100/136675], Loss: 5.1843\n",
      "Epoch [4/5], Step [107175/136675], Loss: 5.4360\n",
      "Epoch [4/5], Step [107250/136675], Loss: 5.2481\n",
      "Epoch [4/5], Step [107325/136675], Loss: 5.5620\n",
      "Epoch [4/5], Step [107400/136675], Loss: 5.0452\n",
      "Epoch [4/5], Step [107475/136675], Loss: 5.3191\n",
      "Epoch [4/5], Step [107550/136675], Loss: 5.1439\n",
      "Epoch [4/5], Step [107625/136675], Loss: 5.4303\n",
      "Epoch [4/5], Step [107700/136675], Loss: 5.2607\n",
      "Epoch [4/5], Step [107775/136675], Loss: 5.0884\n",
      "Epoch [4/5], Step [107850/136675], Loss: 5.2093\n",
      "Epoch [4/5], Step [107925/136675], Loss: 5.4534\n",
      "Epoch [4/5], Step [108000/136675], Loss: 4.8663\n",
      "Epoch [4/5], Step [108075/136675], Loss: 4.9821\n",
      "Epoch [4/5], Step [108150/136675], Loss: 5.3275\n",
      "Epoch [4/5], Step [108225/136675], Loss: 5.0715\n",
      "Epoch [4/5], Step [108300/136675], Loss: 5.3836\n",
      "Epoch [4/5], Step [108375/136675], Loss: 5.3985\n",
      "Epoch [4/5], Step [108450/136675], Loss: 5.1324\n",
      "Epoch [4/5], Step [108525/136675], Loss: 5.4340\n",
      "Epoch [4/5], Step [108600/136675], Loss: 5.4500\n",
      "Epoch [4/5], Step [108675/136675], Loss: 5.1946\n",
      "Epoch [4/5], Step [108750/136675], Loss: 5.2468\n",
      "Epoch [4/5], Step [108825/136675], Loss: 5.1652\n",
      "Epoch [4/5], Step [108900/136675], Loss: 5.1513\n",
      "Epoch [4/5], Step [108975/136675], Loss: 5.1301\n",
      "Epoch [4/5], Step [109050/136675], Loss: 5.0859\n",
      "Epoch [4/5], Step [109125/136675], Loss: 5.2673\n",
      "Epoch [4/5], Step [109200/136675], Loss: 5.3709\n",
      "Epoch [4/5], Step [109275/136675], Loss: 5.4240\n",
      "Epoch [4/5], Step [109350/136675], Loss: 5.2086\n",
      "Epoch [4/5], Step [109425/136675], Loss: 5.1622\n",
      "Epoch [4/5], Step [109500/136675], Loss: 5.2848\n",
      "Epoch [4/5], Step [109575/136675], Loss: 5.5157\n",
      "Epoch [4/5], Step [109650/136675], Loss: 5.2350\n",
      "Epoch [4/5], Step [109725/136675], Loss: 5.1845\n",
      "Epoch [4/5], Step [109800/136675], Loss: 5.2354\n",
      "Epoch [4/5], Step [109875/136675], Loss: 5.5344\n",
      "Epoch [4/5], Step [109950/136675], Loss: 5.3458\n",
      "Validation perplexity: 156.42351561041315\n",
      "Epoch [4/5], Step [110025/136675], Loss: 5.1063\n",
      "Epoch [4/5], Step [110100/136675], Loss: 5.1238\n",
      "Epoch [4/5], Step [110175/136675], Loss: 5.2650\n",
      "Epoch [4/5], Step [110250/136675], Loss: 5.3733\n",
      "Epoch [4/5], Step [110325/136675], Loss: 5.2524\n",
      "Epoch [4/5], Step [110400/136675], Loss: 5.4246\n",
      "Epoch [4/5], Step [110475/136675], Loss: 5.3714\n",
      "Epoch [4/5], Step [110550/136675], Loss: 5.1629\n",
      "Epoch [4/5], Step [110625/136675], Loss: 4.9314\n",
      "Epoch [4/5], Step [110700/136675], Loss: 5.4704\n",
      "Epoch [4/5], Step [110775/136675], Loss: 5.2376\n",
      "Epoch [4/5], Step [110850/136675], Loss: 5.4599\n",
      "Epoch [4/5], Step [110925/136675], Loss: 5.3249\n",
      "Epoch [4/5], Step [111000/136675], Loss: 5.5580\n",
      "Epoch [4/5], Step [111075/136675], Loss: 5.2112\n",
      "Epoch [4/5], Step [111150/136675], Loss: 5.3840\n",
      "Epoch [4/5], Step [111225/136675], Loss: 5.5072\n",
      "Epoch [4/5], Step [111300/136675], Loss: 5.4278\n",
      "Epoch [4/5], Step [111375/136675], Loss: 4.9000\n",
      "Epoch [4/5], Step [111450/136675], Loss: 5.1835\n",
      "Epoch [4/5], Step [111525/136675], Loss: 5.1133\n",
      "Epoch [4/5], Step [111600/136675], Loss: 5.1045\n",
      "Epoch [4/5], Step [111675/136675], Loss: 5.5132\n",
      "Epoch [4/5], Step [111750/136675], Loss: 4.9688\n",
      "Epoch [4/5], Step [111825/136675], Loss: 5.2819\n",
      "Epoch [4/5], Step [111900/136675], Loss: 5.1914\n",
      "Epoch [4/5], Step [111975/136675], Loss: 5.2179\n",
      "Epoch [4/5], Step [112050/136675], Loss: 5.1326\n",
      "Epoch [4/5], Step [112125/136675], Loss: 5.3784\n",
      "Epoch [4/5], Step [112200/136675], Loss: 5.6157\n",
      "Epoch [4/5], Step [112275/136675], Loss: 5.3856\n",
      "Epoch [4/5], Step [112350/136675], Loss: 5.4844\n",
      "Epoch [4/5], Step [112425/136675], Loss: 4.9804\n",
      "Epoch [4/5], Step [112500/136675], Loss: 5.3389\n",
      "Epoch [4/5], Step [112575/136675], Loss: 5.0699\n",
      "Epoch [4/5], Step [112650/136675], Loss: 5.3066\n",
      "Epoch [4/5], Step [112725/136675], Loss: 5.2662\n",
      "Epoch [4/5], Step [112800/136675], Loss: 5.2485\n",
      "Epoch [4/5], Step [112875/136675], Loss: 5.2830\n",
      "Epoch [4/5], Step [112950/136675], Loss: 5.2904\n",
      "Epoch [4/5], Step [113025/136675], Loss: 4.7625\n",
      "Epoch [4/5], Step [113100/136675], Loss: 5.1345\n",
      "Epoch [4/5], Step [113175/136675], Loss: 5.3018\n",
      "Epoch [4/5], Step [113250/136675], Loss: 5.2308\n",
      "Epoch [4/5], Step [113325/136675], Loss: 5.3827\n",
      "Epoch [4/5], Step [113400/136675], Loss: 5.3876\n",
      "Epoch [4/5], Step [113475/136675], Loss: 5.2946\n",
      "Epoch [4/5], Step [113550/136675], Loss: 5.3383\n",
      "Epoch [4/5], Step [113625/136675], Loss: 5.4509\n",
      "Epoch [4/5], Step [113700/136675], Loss: 5.2111\n",
      "Epoch [4/5], Step [113775/136675], Loss: 5.2422\n",
      "Epoch [4/5], Step [113850/136675], Loss: 5.4349\n",
      "Epoch [4/5], Step [113925/136675], Loss: 5.2322\n",
      "Epoch [4/5], Step [114000/136675], Loss: 5.3300\n",
      "Epoch [4/5], Step [114075/136675], Loss: 5.0772\n",
      "Epoch [4/5], Step [114150/136675], Loss: 5.5708\n",
      "Epoch [4/5], Step [114225/136675], Loss: 5.2911\n",
      "Epoch [4/5], Step [114300/136675], Loss: 5.1784\n",
      "Epoch [4/5], Step [114375/136675], Loss: 5.2750\n",
      "Epoch [4/5], Step [114450/136675], Loss: 5.3120\n",
      "Epoch [4/5], Step [114525/136675], Loss: 5.2803\n",
      "Epoch [4/5], Step [114600/136675], Loss: 5.1604\n",
      "Epoch [4/5], Step [114675/136675], Loss: 5.0101\n",
      "Epoch [4/5], Step [114750/136675], Loss: 5.1531\n",
      "Epoch [4/5], Step [114825/136675], Loss: 5.3236\n",
      "Epoch [4/5], Step [114900/136675], Loss: 5.0558\n",
      "Epoch [4/5], Step [114975/136675], Loss: 5.3974\n",
      "Epoch [4/5], Step [115050/136675], Loss: 5.0892\n",
      "Epoch [4/5], Step [115125/136675], Loss: 5.2148\n",
      "Epoch [4/5], Step [115200/136675], Loss: 5.4057\n",
      "Epoch [4/5], Step [115275/136675], Loss: 5.3672\n",
      "Epoch [4/5], Step [115350/136675], Loss: 5.3868\n",
      "Epoch [4/5], Step [115425/136675], Loss: 5.4700\n",
      "Epoch [4/5], Step [115500/136675], Loss: 5.3093\n",
      "Epoch [4/5], Step [115575/136675], Loss: 5.1339\n",
      "Epoch [4/5], Step [115650/136675], Loss: 5.5286\n",
      "Epoch [4/5], Step [115725/136675], Loss: 5.1088\n",
      "Epoch [4/5], Step [115800/136675], Loss: 5.1017\n",
      "Epoch [4/5], Step [115875/136675], Loss: 5.5192\n",
      "Epoch [4/5], Step [115950/136675], Loss: 5.1464\n",
      "Epoch [4/5], Step [116025/136675], Loss: 4.9461\n",
      "Epoch [4/5], Step [116100/136675], Loss: 5.0256\n",
      "Epoch [4/5], Step [116175/136675], Loss: 5.5168\n",
      "Epoch [4/5], Step [116250/136675], Loss: 5.3527\n",
      "Epoch [4/5], Step [116325/136675], Loss: 5.1795\n",
      "Epoch [4/5], Step [116400/136675], Loss: 4.8968\n",
      "Epoch [4/5], Step [116475/136675], Loss: 5.4575\n",
      "Epoch [4/5], Step [116550/136675], Loss: 5.5189\n",
      "Epoch [4/5], Step [116625/136675], Loss: 5.3637\n",
      "Epoch [4/5], Step [116700/136675], Loss: 5.2066\n",
      "Epoch [4/5], Step [116775/136675], Loss: 5.5616\n",
      "Epoch [4/5], Step [116850/136675], Loss: 5.1458\n",
      "Epoch [4/5], Step [116925/136675], Loss: 5.5810\n",
      "Epoch [4/5], Step [117000/136675], Loss: 5.3655\n",
      "Epoch [4/5], Step [117075/136675], Loss: 5.3937\n",
      "Epoch [4/5], Step [117150/136675], Loss: 5.3412\n",
      "Epoch [4/5], Step [117225/136675], Loss: 5.3443\n",
      "Epoch [4/5], Step [117300/136675], Loss: 5.1647\n",
      "Epoch [4/5], Step [117375/136675], Loss: 5.1467\n",
      "Epoch [4/5], Step [117450/136675], Loss: 5.5779\n",
      "Epoch [4/5], Step [117525/136675], Loss: 5.1868\n",
      "Epoch [4/5], Step [117600/136675], Loss: 5.4362\n",
      "Epoch [4/5], Step [117675/136675], Loss: 5.3561\n",
      "Epoch [4/5], Step [117750/136675], Loss: 5.1086\n",
      "Epoch [4/5], Step [117825/136675], Loss: 5.1043\n",
      "Epoch [4/5], Step [117900/136675], Loss: 5.6128\n",
      "Epoch [4/5], Step [117975/136675], Loss: 5.2794\n",
      "Epoch [4/5], Step [118050/136675], Loss: 5.2937\n",
      "Epoch [4/5], Step [118125/136675], Loss: 5.5078\n",
      "Epoch [4/5], Step [118200/136675], Loss: 5.0638\n",
      "Epoch [4/5], Step [118275/136675], Loss: 5.3732\n",
      "Epoch [4/5], Step [118350/136675], Loss: 5.1501\n",
      "Epoch [4/5], Step [118425/136675], Loss: 5.2731\n",
      "Epoch [4/5], Step [118500/136675], Loss: 4.9622\n",
      "Epoch [4/5], Step [118575/136675], Loss: 5.4855\n",
      "Epoch [4/5], Step [118650/136675], Loss: 5.2580\n",
      "Epoch [4/5], Step [118725/136675], Loss: 5.5535\n",
      "Epoch [4/5], Step [118800/136675], Loss: 5.6440\n",
      "Epoch [4/5], Step [118875/136675], Loss: 5.3164\n",
      "Epoch [4/5], Step [118950/136675], Loss: 5.1546\n",
      "Epoch [4/5], Step [119025/136675], Loss: 5.1566\n",
      "Epoch [4/5], Step [119100/136675], Loss: 5.2070\n",
      "Epoch [4/5], Step [119175/136675], Loss: 5.4840\n",
      "Epoch [4/5], Step [119250/136675], Loss: 5.2375\n",
      "Epoch [4/5], Step [119325/136675], Loss: 5.4793\n",
      "Epoch [4/5], Step [119400/136675], Loss: 5.4485\n",
      "Epoch [4/5], Step [119475/136675], Loss: 5.2938\n",
      "Epoch [4/5], Step [119550/136675], Loss: 5.0958\n",
      "Epoch [4/5], Step [119625/136675], Loss: 5.4552\n",
      "Epoch [4/5], Step [119700/136675], Loss: 5.3227\n",
      "Epoch [4/5], Step [119775/136675], Loss: 5.4209\n",
      "Epoch [4/5], Step [119850/136675], Loss: 5.3257\n",
      "Epoch [4/5], Step [119925/136675], Loss: 5.4380\n",
      "Epoch [4/5], Step [120000/136675], Loss: 5.4207\n",
      "Validation perplexity: 156.10148576467193\n",
      "Epoch [4/5], Step [120075/136675], Loss: 5.3845\n",
      "Epoch [4/5], Step [120150/136675], Loss: 5.3118\n",
      "Epoch [4/5], Step [120225/136675], Loss: 5.1475\n",
      "Epoch [4/5], Step [120300/136675], Loss: 5.3657\n",
      "Epoch [4/5], Step [120375/136675], Loss: 5.0887\n",
      "Epoch [4/5], Step [120450/136675], Loss: 5.3615\n",
      "Epoch [4/5], Step [120525/136675], Loss: 5.2228\n",
      "Epoch [4/5], Step [120600/136675], Loss: 5.6797\n",
      "Epoch [4/5], Step [120675/136675], Loss: 5.2462\n",
      "Epoch [4/5], Step [120750/136675], Loss: 5.4080\n",
      "Epoch [4/5], Step [120825/136675], Loss: 5.6121\n",
      "Epoch [4/5], Step [120900/136675], Loss: 5.2495\n",
      "Epoch [4/5], Step [120975/136675], Loss: 5.3927\n",
      "Epoch [4/5], Step [121050/136675], Loss: 5.3111\n",
      "Epoch [4/5], Step [121125/136675], Loss: 4.8947\n",
      "Epoch [4/5], Step [121200/136675], Loss: 5.3400\n",
      "Epoch [4/5], Step [121275/136675], Loss: 5.2009\n",
      "Epoch [4/5], Step [121350/136675], Loss: 5.1572\n",
      "Epoch [4/5], Step [121425/136675], Loss: 5.5185\n",
      "Epoch [4/5], Step [121500/136675], Loss: 5.1029\n",
      "Epoch [4/5], Step [121575/136675], Loss: 5.7073\n",
      "Epoch [4/5], Step [121650/136675], Loss: 5.0543\n",
      "Epoch [4/5], Step [121725/136675], Loss: 5.1129\n",
      "Epoch [4/5], Step [121800/136675], Loss: 5.0558\n",
      "Epoch [4/5], Step [121875/136675], Loss: 5.2728\n",
      "Epoch [4/5], Step [121950/136675], Loss: 4.9705\n",
      "Epoch [4/5], Step [122025/136675], Loss: 5.1106\n",
      "Epoch [4/5], Step [122100/136675], Loss: 5.3559\n",
      "Epoch [4/5], Step [122175/136675], Loss: 5.4044\n",
      "Epoch [4/5], Step [122250/136675], Loss: 5.5462\n",
      "Epoch [4/5], Step [122325/136675], Loss: 5.1504\n",
      "Epoch [4/5], Step [122400/136675], Loss: 5.2594\n",
      "Epoch [4/5], Step [122475/136675], Loss: 5.0893\n",
      "Epoch [4/5], Step [122550/136675], Loss: 5.2893\n",
      "Epoch [4/5], Step [122625/136675], Loss: 5.4810\n",
      "Epoch [4/5], Step [122700/136675], Loss: 5.2910\n",
      "Epoch [4/5], Step [122775/136675], Loss: 5.4184\n",
      "Epoch [4/5], Step [122850/136675], Loss: 5.2968\n",
      "Epoch [4/5], Step [122925/136675], Loss: 5.0284\n",
      "Epoch [4/5], Step [123000/136675], Loss: 5.6332\n",
      "Epoch [4/5], Step [123075/136675], Loss: 5.2897\n",
      "Epoch [4/5], Step [123150/136675], Loss: 5.4109\n",
      "Epoch [4/5], Step [123225/136675], Loss: 5.3295\n",
      "Epoch [4/5], Step [123300/136675], Loss: 5.3368\n",
      "Epoch [4/5], Step [123375/136675], Loss: 5.4461\n",
      "Epoch [4/5], Step [123450/136675], Loss: 5.4137\n",
      "Epoch [4/5], Step [123525/136675], Loss: 5.3691\n",
      "Epoch [4/5], Step [123600/136675], Loss: 5.0140\n",
      "Epoch [4/5], Step [123675/136675], Loss: 5.4695\n",
      "Epoch [4/5], Step [123750/136675], Loss: 5.2516\n",
      "Epoch [4/5], Step [123825/136675], Loss: 5.4143\n",
      "Epoch [4/5], Step [123900/136675], Loss: 5.2485\n",
      "Epoch [4/5], Step [123975/136675], Loss: 4.8664\n",
      "Epoch [4/5], Step [124050/136675], Loss: 5.1866\n",
      "Epoch [4/5], Step [124125/136675], Loss: 5.4717\n",
      "Epoch [4/5], Step [124200/136675], Loss: 5.1360\n",
      "Epoch [4/5], Step [124275/136675], Loss: 5.2198\n",
      "Epoch [4/5], Step [124350/136675], Loss: 5.3637\n",
      "Epoch [4/5], Step [124425/136675], Loss: 5.2460\n",
      "Epoch [4/5], Step [124500/136675], Loss: 5.2865\n",
      "Epoch [4/5], Step [124575/136675], Loss: 5.4221\n",
      "Epoch [4/5], Step [124650/136675], Loss: 4.9912\n",
      "Epoch [4/5], Step [124725/136675], Loss: 5.5783\n",
      "Epoch [4/5], Step [124800/136675], Loss: 5.0918\n",
      "Epoch [4/5], Step [124875/136675], Loss: 5.2367\n",
      "Epoch [4/5], Step [124950/136675], Loss: 5.3061\n",
      "Epoch [4/5], Step [125025/136675], Loss: 5.3779\n",
      "Epoch [4/5], Step [125100/136675], Loss: 5.2558\n",
      "Epoch [4/5], Step [125175/136675], Loss: 5.6350\n",
      "Epoch [4/5], Step [125250/136675], Loss: 4.9926\n",
      "Epoch [4/5], Step [125325/136675], Loss: 5.2261\n",
      "Epoch [4/5], Step [125400/136675], Loss: 5.5833\n",
      "Epoch [4/5], Step [125475/136675], Loss: 4.9761\n",
      "Epoch [4/5], Step [125550/136675], Loss: 4.9326\n",
      "Epoch [4/5], Step [125625/136675], Loss: 5.0534\n",
      "Epoch [4/5], Step [125700/136675], Loss: 5.1904\n",
      "Epoch [4/5], Step [125775/136675], Loss: 5.2519\n",
      "Epoch [4/5], Step [125850/136675], Loss: 5.2870\n",
      "Epoch [4/5], Step [125925/136675], Loss: 5.0948\n",
      "Epoch [4/5], Step [126000/136675], Loss: 5.1229\n",
      "Epoch [4/5], Step [126075/136675], Loss: 5.2567\n",
      "Epoch [4/5], Step [126150/136675], Loss: 5.1420\n",
      "Epoch [4/5], Step [126225/136675], Loss: 5.1265\n",
      "Epoch [4/5], Step [126300/136675], Loss: 5.1349\n",
      "Epoch [4/5], Step [126375/136675], Loss: 5.2360\n",
      "Epoch [4/5], Step [126450/136675], Loss: 5.0649\n",
      "Epoch [4/5], Step [126525/136675], Loss: 5.2095\n",
      "Epoch [4/5], Step [126600/136675], Loss: 5.3605\n",
      "Epoch [4/5], Step [126675/136675], Loss: 5.5643\n",
      "Epoch [4/5], Step [126750/136675], Loss: 5.5847\n",
      "Epoch [4/5], Step [126825/136675], Loss: 5.5725\n",
      "Epoch [4/5], Step [126900/136675], Loss: 5.1810\n",
      "Epoch [4/5], Step [126975/136675], Loss: 5.1348\n",
      "Epoch [4/5], Step [127050/136675], Loss: 5.3179\n",
      "Epoch [4/5], Step [127125/136675], Loss: 5.3129\n",
      "Epoch [4/5], Step [127200/136675], Loss: 5.3378\n",
      "Epoch [4/5], Step [127275/136675], Loss: 5.1867\n",
      "Epoch [4/5], Step [127350/136675], Loss: 5.2635\n",
      "Epoch [4/5], Step [127425/136675], Loss: 5.2212\n",
      "Epoch [4/5], Step [127500/136675], Loss: 5.3015\n",
      "Epoch [4/5], Step [127575/136675], Loss: 5.0959\n",
      "Epoch [4/5], Step [127650/136675], Loss: 5.3297\n",
      "Epoch [4/5], Step [127725/136675], Loss: 5.0723\n",
      "Epoch [4/5], Step [127800/136675], Loss: 5.3967\n",
      "Epoch [4/5], Step [127875/136675], Loss: 5.2689\n",
      "Epoch [4/5], Step [127950/136675], Loss: 5.7226\n",
      "Epoch [4/5], Step [128025/136675], Loss: 5.3271\n",
      "Epoch [4/5], Step [128100/136675], Loss: 5.4006\n",
      "Epoch [4/5], Step [128175/136675], Loss: 5.3692\n",
      "Epoch [4/5], Step [128250/136675], Loss: 5.0905\n",
      "Epoch [4/5], Step [128325/136675], Loss: 4.9653\n",
      "Epoch [4/5], Step [128400/136675], Loss: 5.4509\n",
      "Epoch [4/5], Step [128475/136675], Loss: 5.3112\n",
      "Epoch [4/5], Step [128550/136675], Loss: 5.2341\n",
      "Epoch [4/5], Step [128625/136675], Loss: 5.1169\n",
      "Epoch [4/5], Step [128700/136675], Loss: 5.1314\n",
      "Epoch [4/5], Step [128775/136675], Loss: 5.4032\n",
      "Epoch [4/5], Step [128850/136675], Loss: 5.4426\n",
      "Epoch [4/5], Step [128925/136675], Loss: 5.0550\n",
      "Epoch [4/5], Step [129000/136675], Loss: 5.1700\n",
      "Epoch [4/5], Step [129075/136675], Loss: 5.1372\n",
      "Epoch [4/5], Step [129150/136675], Loss: 5.4468\n",
      "Epoch [4/5], Step [129225/136675], Loss: 5.0832\n",
      "Epoch [4/5], Step [129300/136675], Loss: 5.4352\n",
      "Epoch [4/5], Step [129375/136675], Loss: 5.2070\n",
      "Epoch [4/5], Step [129450/136675], Loss: 5.3546\n",
      "Epoch [4/5], Step [129525/136675], Loss: 5.2768\n",
      "Epoch [4/5], Step [129600/136675], Loss: 5.1747\n",
      "Epoch [4/5], Step [129675/136675], Loss: 5.3562\n",
      "Epoch [4/5], Step [129750/136675], Loss: 5.1028\n",
      "Epoch [4/5], Step [129825/136675], Loss: 5.0835\n",
      "Epoch [4/5], Step [129900/136675], Loss: 5.2924\n",
      "Epoch [4/5], Step [129975/136675], Loss: 5.2572\n",
      "Validation perplexity: 155.9723208629131\n",
      "Epoch [4/5], Step [130050/136675], Loss: 5.0885\n",
      "Epoch [4/5], Step [130125/136675], Loss: 5.2956\n",
      "Epoch [4/5], Step [130200/136675], Loss: 5.1860\n",
      "Epoch [4/5], Step [130275/136675], Loss: 5.2473\n",
      "Epoch [4/5], Step [130350/136675], Loss: 5.3440\n",
      "Epoch [4/5], Step [130425/136675], Loss: 5.0467\n",
      "Epoch [4/5], Step [130500/136675], Loss: 5.5374\n",
      "Epoch [4/5], Step [130575/136675], Loss: 4.9510\n",
      "Epoch [4/5], Step [130650/136675], Loss: 5.2674\n",
      "Epoch [4/5], Step [130725/136675], Loss: 5.1072\n",
      "Epoch [4/5], Step [130800/136675], Loss: 5.4643\n",
      "Epoch [4/5], Step [130875/136675], Loss: 5.1585\n",
      "Epoch [4/5], Step [130950/136675], Loss: 5.2713\n",
      "Epoch [4/5], Step [131025/136675], Loss: 5.5533\n",
      "Epoch [4/5], Step [131100/136675], Loss: 5.2831\n",
      "Epoch [4/5], Step [131175/136675], Loss: 4.9897\n",
      "Epoch [4/5], Step [131250/136675], Loss: 5.1628\n",
      "Epoch [4/5], Step [131325/136675], Loss: 5.5649\n",
      "Epoch [4/5], Step [131400/136675], Loss: 5.6134\n",
      "Epoch [4/5], Step [131475/136675], Loss: 5.1386\n",
      "Epoch [4/5], Step [131550/136675], Loss: 5.2488\n",
      "Epoch [4/5], Step [131625/136675], Loss: 5.5340\n",
      "Epoch [4/5], Step [131700/136675], Loss: 5.4573\n",
      "Epoch [4/5], Step [131775/136675], Loss: 5.0379\n",
      "Epoch [4/5], Step [131850/136675], Loss: 5.2166\n",
      "Epoch [4/5], Step [131925/136675], Loss: 5.1871\n",
      "Epoch [4/5], Step [132000/136675], Loss: 5.3341\n",
      "Epoch [4/5], Step [132075/136675], Loss: 5.3481\n",
      "Epoch [4/5], Step [132150/136675], Loss: 5.4618\n",
      "Epoch [4/5], Step [132225/136675], Loss: 5.0826\n",
      "Epoch [4/5], Step [132300/136675], Loss: 5.2888\n",
      "Epoch [4/5], Step [132375/136675], Loss: 5.2996\n",
      "Epoch [4/5], Step [132450/136675], Loss: 5.2698\n",
      "Epoch [4/5], Step [132525/136675], Loss: 5.5708\n",
      "Epoch [4/5], Step [132600/136675], Loss: 5.3439\n",
      "Epoch [4/5], Step [132675/136675], Loss: 5.0143\n",
      "Epoch [4/5], Step [132750/136675], Loss: 5.5583\n",
      "Epoch [4/5], Step [132825/136675], Loss: 5.4677\n",
      "Epoch [4/5], Step [132900/136675], Loss: 5.2001\n",
      "Epoch [4/5], Step [132975/136675], Loss: 5.3360\n",
      "Epoch [4/5], Step [133050/136675], Loss: 5.3507\n",
      "Epoch [4/5], Step [133125/136675], Loss: 5.2559\n",
      "Epoch [4/5], Step [133200/136675], Loss: 5.2422\n",
      "Epoch [4/5], Step [133275/136675], Loss: 5.3417\n",
      "Epoch [4/5], Step [133350/136675], Loss: 5.1990\n",
      "Epoch [4/5], Step [133425/136675], Loss: 5.2052\n",
      "Epoch [4/5], Step [133500/136675], Loss: 5.1815\n",
      "Epoch [4/5], Step [133575/136675], Loss: 4.9349\n",
      "Epoch [4/5], Step [133650/136675], Loss: 5.1053\n",
      "Epoch [4/5], Step [133725/136675], Loss: 5.2409\n",
      "Epoch [4/5], Step [133800/136675], Loss: 5.3670\n",
      "Epoch [4/5], Step [133875/136675], Loss: 4.9329\n",
      "Epoch [4/5], Step [133950/136675], Loss: 4.9150\n",
      "Epoch [4/5], Step [134025/136675], Loss: 5.3600\n",
      "Epoch [4/5], Step [134100/136675], Loss: 5.3405\n",
      "Epoch [4/5], Step [134175/136675], Loss: 5.3006\n",
      "Epoch [4/5], Step [134250/136675], Loss: 5.1572\n",
      "Epoch [4/5], Step [134325/136675], Loss: 5.3358\n",
      "Epoch [4/5], Step [134400/136675], Loss: 5.3580\n",
      "Epoch [4/5], Step [134475/136675], Loss: 4.6715\n",
      "Epoch [4/5], Step [134550/136675], Loss: 5.0580\n",
      "Epoch [4/5], Step [134625/136675], Loss: 5.3771\n",
      "Epoch [4/5], Step [134700/136675], Loss: 5.2794\n",
      "Epoch [4/5], Step [134775/136675], Loss: 5.2456\n",
      "Epoch [4/5], Step [134850/136675], Loss: 5.2795\n",
      "Epoch [4/5], Step [134925/136675], Loss: 5.2388\n",
      "Epoch [4/5], Step [135000/136675], Loss: 5.3324\n",
      "Epoch [4/5], Step [135075/136675], Loss: 5.1144\n",
      "Epoch [4/5], Step [135150/136675], Loss: 5.1188\n",
      "Epoch [4/5], Step [135225/136675], Loss: 5.1754\n",
      "Epoch [4/5], Step [135300/136675], Loss: 5.4209\n",
      "Epoch [4/5], Step [135375/136675], Loss: 5.3418\n",
      "Epoch [4/5], Step [135450/136675], Loss: 5.1360\n",
      "Epoch [4/5], Step [135525/136675], Loss: 5.3994\n",
      "Epoch [4/5], Step [135600/136675], Loss: 5.3357\n",
      "Epoch [4/5], Step [135675/136675], Loss: 5.3301\n",
      "Epoch [4/5], Step [135750/136675], Loss: 5.3154\n",
      "Epoch [4/5], Step [135825/136675], Loss: 5.5086\n",
      "Epoch [4/5], Step [135900/136675], Loss: 5.2100\n",
      "Epoch [4/5], Step [135975/136675], Loss: 5.1089\n",
      "Epoch [4/5], Step [136050/136675], Loss: 5.3536\n",
      "Epoch [4/5], Step [136125/136675], Loss: 5.5923\n",
      "Epoch [4/5], Step [136200/136675], Loss: 5.1978\n",
      "Epoch [4/5], Step [136275/136675], Loss: 5.3931\n",
      "Epoch [4/5], Step [136350/136675], Loss: 5.2571\n",
      "Epoch [4/5], Step [136425/136675], Loss: 5.1256\n",
      "Epoch [4/5], Step [136500/136675], Loss: 5.2535\n",
      "Epoch [4/5], Step [136575/136675], Loss: 5.3602\n",
      "Epoch [4/5], Step [136650/136675], Loss: 5.0586\n",
      "Epoch [4/5] Average Loss: 5.2847, Perplexity: 197.29\n",
      "Epoch [5/5], Step [0/136675], Loss: 5.0332\n",
      "Validation perplexity: 154.92239027982075\n",
      "Epoch [5/5], Step [75/136675], Loss: 5.1168\n",
      "Epoch [5/5], Step [150/136675], Loss: 5.5819\n",
      "Epoch [5/5], Step [225/136675], Loss: 5.1567\n",
      "Epoch [5/5], Step [300/136675], Loss: 5.1566\n",
      "Epoch [5/5], Step [375/136675], Loss: 5.3792\n",
      "Epoch [5/5], Step [450/136675], Loss: 5.4170\n",
      "Epoch [5/5], Step [525/136675], Loss: 5.5590\n",
      "Epoch [5/5], Step [600/136675], Loss: 5.0270\n",
      "Epoch [5/5], Step [675/136675], Loss: 5.1174\n",
      "Epoch [5/5], Step [750/136675], Loss: 5.4751\n",
      "Epoch [5/5], Step [825/136675], Loss: 5.3435\n",
      "Epoch [5/5], Step [900/136675], Loss: 5.0492\n",
      "Epoch [5/5], Step [975/136675], Loss: 5.4650\n",
      "Epoch [5/5], Step [1050/136675], Loss: 5.3873\n",
      "Epoch [5/5], Step [1125/136675], Loss: 5.3676\n",
      "Epoch [5/5], Step [1200/136675], Loss: 5.4891\n",
      "Epoch [5/5], Step [1275/136675], Loss: 5.3765\n",
      "Epoch [5/5], Step [1350/136675], Loss: 5.2369\n",
      "Epoch [5/5], Step [1425/136675], Loss: 5.3053\n",
      "Epoch [5/5], Step [1500/136675], Loss: 5.3110\n",
      "Epoch [5/5], Step [1575/136675], Loss: 5.4925\n",
      "Epoch [5/5], Step [1650/136675], Loss: 5.1210\n",
      "Epoch [5/5], Step [1725/136675], Loss: 5.1471\n",
      "Epoch [5/5], Step [1800/136675], Loss: 4.7765\n",
      "Epoch [5/5], Step [1875/136675], Loss: 5.1649\n",
      "Epoch [5/5], Step [1950/136675], Loss: 5.0657\n",
      "Epoch [5/5], Step [2025/136675], Loss: 5.3654\n",
      "Epoch [5/5], Step [2100/136675], Loss: 5.0723\n",
      "Epoch [5/5], Step [2175/136675], Loss: 5.3032\n",
      "Epoch [5/5], Step [2250/136675], Loss: 5.3054\n",
      "Epoch [5/5], Step [2325/136675], Loss: 5.2638\n",
      "Epoch [5/5], Step [2400/136675], Loss: 5.1007\n",
      "Epoch [5/5], Step [2475/136675], Loss: 5.2585\n",
      "Epoch [5/5], Step [2550/136675], Loss: 5.4193\n",
      "Epoch [5/5], Step [2625/136675], Loss: 5.2641\n",
      "Epoch [5/5], Step [2700/136675], Loss: 5.2065\n",
      "Epoch [5/5], Step [2775/136675], Loss: 5.2039\n",
      "Epoch [5/5], Step [2850/136675], Loss: 5.4332\n",
      "Epoch [5/5], Step [2925/136675], Loss: 5.1324\n",
      "Epoch [5/5], Step [3000/136675], Loss: 5.2807\n",
      "Epoch [5/5], Step [3075/136675], Loss: 5.5612\n",
      "Epoch [5/5], Step [3150/136675], Loss: 5.1381\n",
      "Epoch [5/5], Step [3225/136675], Loss: 5.3249\n",
      "Epoch [5/5], Step [3300/136675], Loss: 5.3453\n",
      "Epoch [5/5], Step [3375/136675], Loss: 5.2609\n",
      "Epoch [5/5], Step [3450/136675], Loss: 5.4762\n",
      "Epoch [5/5], Step [3525/136675], Loss: 5.2214\n",
      "Epoch [5/5], Step [3600/136675], Loss: 5.3617\n",
      "Epoch [5/5], Step [3675/136675], Loss: 5.4528\n",
      "Epoch [5/5], Step [3750/136675], Loss: 5.2482\n",
      "Epoch [5/5], Step [3825/136675], Loss: 5.0825\n",
      "Epoch [5/5], Step [3900/136675], Loss: 5.2707\n",
      "Epoch [5/5], Step [3975/136675], Loss: 5.4442\n",
      "Epoch [5/5], Step [4050/136675], Loss: 5.1307\n",
      "Epoch [5/5], Step [4125/136675], Loss: 5.3093\n",
      "Epoch [5/5], Step [4200/136675], Loss: 5.0964\n",
      "Epoch [5/5], Step [4275/136675], Loss: 5.1880\n",
      "Epoch [5/5], Step [4350/136675], Loss: 5.4384\n",
      "Epoch [5/5], Step [4425/136675], Loss: 5.2961\n",
      "Epoch [5/5], Step [4500/136675], Loss: 5.2793\n",
      "Epoch [5/5], Step [4575/136675], Loss: 5.3374\n",
      "Epoch [5/5], Step [4650/136675], Loss: 5.0075\n",
      "Epoch [5/5], Step [4725/136675], Loss: 5.2926\n",
      "Epoch [5/5], Step [4800/136675], Loss: 5.2888\n",
      "Epoch [5/5], Step [4875/136675], Loss: 5.1814\n",
      "Epoch [5/5], Step [4950/136675], Loss: 5.3574\n",
      "Epoch [5/5], Step [5025/136675], Loss: 5.5661\n",
      "Epoch [5/5], Step [5100/136675], Loss: 5.1578\n",
      "Epoch [5/5], Step [5175/136675], Loss: 5.4842\n",
      "Epoch [5/5], Step [5250/136675], Loss: 5.1900\n",
      "Epoch [5/5], Step [5325/136675], Loss: 5.3019\n",
      "Epoch [5/5], Step [5400/136675], Loss: 5.4638\n",
      "Epoch [5/5], Step [5475/136675], Loss: 5.4098\n",
      "Epoch [5/5], Step [5550/136675], Loss: 5.3326\n",
      "Epoch [5/5], Step [5625/136675], Loss: 5.6153\n",
      "Epoch [5/5], Step [5700/136675], Loss: 5.0743\n",
      "Epoch [5/5], Step [5775/136675], Loss: 5.2606\n",
      "Epoch [5/5], Step [5850/136675], Loss: 5.1982\n",
      "Epoch [5/5], Step [5925/136675], Loss: 5.2653\n",
      "Epoch [5/5], Step [6000/136675], Loss: 5.1427\n",
      "Epoch [5/5], Step [6075/136675], Loss: 5.2720\n",
      "Epoch [5/5], Step [6150/136675], Loss: 5.0442\n",
      "Epoch [5/5], Step [6225/136675], Loss: 5.1967\n",
      "Epoch [5/5], Step [6300/136675], Loss: 5.4364\n",
      "Epoch [5/5], Step [6375/136675], Loss: 5.3063\n",
      "Epoch [5/5], Step [6450/136675], Loss: 5.5363\n",
      "Epoch [5/5], Step [6525/136675], Loss: 5.2077\n",
      "Epoch [5/5], Step [6600/136675], Loss: 5.4585\n",
      "Epoch [5/5], Step [6675/136675], Loss: 5.2280\n",
      "Epoch [5/5], Step [6750/136675], Loss: 5.4203\n",
      "Epoch [5/5], Step [6825/136675], Loss: 5.1862\n",
      "Epoch [5/5], Step [6900/136675], Loss: 4.9678\n",
      "Epoch [5/5], Step [6975/136675], Loss: 5.4041\n",
      "Epoch [5/5], Step [7050/136675], Loss: 5.3839\n",
      "Epoch [5/5], Step [7125/136675], Loss: 5.2084\n",
      "Epoch [5/5], Step [7200/136675], Loss: 5.2863\n",
      "Epoch [5/5], Step [7275/136675], Loss: 5.4661\n",
      "Epoch [5/5], Step [7350/136675], Loss: 5.5427\n",
      "Epoch [5/5], Step [7425/136675], Loss: 5.5204\n",
      "Epoch [5/5], Step [7500/136675], Loss: 5.0869\n",
      "Epoch [5/5], Step [7575/136675], Loss: 5.0308\n",
      "Epoch [5/5], Step [7650/136675], Loss: 5.3179\n",
      "Epoch [5/5], Step [7725/136675], Loss: 4.8615\n",
      "Epoch [5/5], Step [7800/136675], Loss: 5.2278\n",
      "Epoch [5/5], Step [7875/136675], Loss: 5.1466\n",
      "Epoch [5/5], Step [7950/136675], Loss: 5.3216\n",
      "Epoch [5/5], Step [8025/136675], Loss: 5.1469\n",
      "Epoch [5/5], Step [8100/136675], Loss: 5.6643\n",
      "Epoch [5/5], Step [8175/136675], Loss: 5.6544\n",
      "Epoch [5/5], Step [8250/136675], Loss: 5.2223\n",
      "Epoch [5/5], Step [8325/136675], Loss: 5.7794\n",
      "Epoch [5/5], Step [8400/136675], Loss: 5.1596\n",
      "Epoch [5/5], Step [8475/136675], Loss: 5.0674\n",
      "Epoch [5/5], Step [8550/136675], Loss: 5.1382\n",
      "Epoch [5/5], Step [8625/136675], Loss: 5.3213\n",
      "Epoch [5/5], Step [8700/136675], Loss: 5.2570\n",
      "Epoch [5/5], Step [8775/136675], Loss: 5.3043\n",
      "Epoch [5/5], Step [8850/136675], Loss: 5.1198\n",
      "Epoch [5/5], Step [8925/136675], Loss: 5.3731\n",
      "Epoch [5/5], Step [9000/136675], Loss: 5.4271\n",
      "Epoch [5/5], Step [9075/136675], Loss: 5.1279\n",
      "Epoch [5/5], Step [9150/136675], Loss: 5.2581\n",
      "Epoch [5/5], Step [9225/136675], Loss: 5.3176\n",
      "Epoch [5/5], Step [9300/136675], Loss: 5.3203\n",
      "Epoch [5/5], Step [9375/136675], Loss: 5.1383\n",
      "Epoch [5/5], Step [9450/136675], Loss: 4.9299\n",
      "Epoch [5/5], Step [9525/136675], Loss: 5.2222\n",
      "Epoch [5/5], Step [9600/136675], Loss: 5.3355\n",
      "Epoch [5/5], Step [9675/136675], Loss: 5.3186\n",
      "Epoch [5/5], Step [9750/136675], Loss: 5.5448\n",
      "Epoch [5/5], Step [9825/136675], Loss: 5.2851\n",
      "Epoch [5/5], Step [9900/136675], Loss: 5.5763\n",
      "Epoch [5/5], Step [9975/136675], Loss: 5.1782\n",
      "Validation perplexity: 154.5150222978431\n",
      "Epoch [5/5], Step [10050/136675], Loss: 5.2377\n",
      "Epoch [5/5], Step [10125/136675], Loss: 5.3405\n",
      "Epoch [5/5], Step [10200/136675], Loss: 5.3824\n",
      "Epoch [5/5], Step [10275/136675], Loss: 5.5846\n",
      "Epoch [5/5], Step [10350/136675], Loss: 5.4138\n",
      "Epoch [5/5], Step [10425/136675], Loss: 4.9598\n",
      "Epoch [5/5], Step [10500/136675], Loss: 5.1012\n",
      "Epoch [5/5], Step [10575/136675], Loss: 5.1952\n",
      "Epoch [5/5], Step [10650/136675], Loss: 5.1233\n",
      "Epoch [5/5], Step [10725/136675], Loss: 5.2789\n",
      "Epoch [5/5], Step [10800/136675], Loss: 5.1289\n",
      "Epoch [5/5], Step [10875/136675], Loss: 5.2407\n",
      "Epoch [5/5], Step [10950/136675], Loss: 5.3707\n",
      "Epoch [5/5], Step [11025/136675], Loss: 5.3894\n",
      "Epoch [5/5], Step [11100/136675], Loss: 4.9969\n",
      "Epoch [5/5], Step [11175/136675], Loss: 5.0965\n",
      "Epoch [5/5], Step [11250/136675], Loss: 5.1866\n",
      "Epoch [5/5], Step [11325/136675], Loss: 4.8562\n",
      "Epoch [5/5], Step [11400/136675], Loss: 5.1497\n",
      "Epoch [5/5], Step [11475/136675], Loss: 5.5174\n",
      "Epoch [5/5], Step [11550/136675], Loss: 5.2045\n",
      "Epoch [5/5], Step [11625/136675], Loss: 5.4459\n",
      "Epoch [5/5], Step [11700/136675], Loss: 5.6735\n",
      "Epoch [5/5], Step [11775/136675], Loss: 5.4125\n",
      "Epoch [5/5], Step [11850/136675], Loss: 5.4193\n",
      "Epoch [5/5], Step [11925/136675], Loss: 5.4189\n",
      "Epoch [5/5], Step [12000/136675], Loss: 5.4586\n",
      "Epoch [5/5], Step [12075/136675], Loss: 5.2455\n",
      "Epoch [5/5], Step [12150/136675], Loss: 5.5570\n",
      "Epoch [5/5], Step [12225/136675], Loss: 5.0191\n",
      "Epoch [5/5], Step [12300/136675], Loss: 5.1167\n",
      "Epoch [5/5], Step [12375/136675], Loss: 5.3568\n",
      "Epoch [5/5], Step [12450/136675], Loss: 5.2131\n",
      "Epoch [5/5], Step [12525/136675], Loss: 5.0296\n",
      "Epoch [5/5], Step [12600/136675], Loss: 5.0876\n",
      "Epoch [5/5], Step [12675/136675], Loss: 5.0402\n",
      "Epoch [5/5], Step [12750/136675], Loss: 5.2809\n",
      "Epoch [5/5], Step [12825/136675], Loss: 5.1022\n",
      "Epoch [5/5], Step [12900/136675], Loss: 5.1436\n",
      "Epoch [5/5], Step [12975/136675], Loss: 4.9097\n",
      "Epoch [5/5], Step [13050/136675], Loss: 5.3452\n",
      "Epoch [5/5], Step [13125/136675], Loss: 5.3264\n",
      "Epoch [5/5], Step [13200/136675], Loss: 5.6290\n",
      "Epoch [5/5], Step [13275/136675], Loss: 5.2453\n",
      "Epoch [5/5], Step [13350/136675], Loss: 5.1601\n",
      "Epoch [5/5], Step [13425/136675], Loss: 4.9071\n",
      "Epoch [5/5], Step [13500/136675], Loss: 5.3602\n",
      "Epoch [5/5], Step [13575/136675], Loss: 5.0731\n",
      "Epoch [5/5], Step [13650/136675], Loss: 5.3025\n",
      "Epoch [5/5], Step [13725/136675], Loss: 5.3000\n",
      "Epoch [5/5], Step [13800/136675], Loss: 5.2500\n",
      "Epoch [5/5], Step [13875/136675], Loss: 5.3355\n",
      "Epoch [5/5], Step [13950/136675], Loss: 5.3559\n",
      "Epoch [5/5], Step [14025/136675], Loss: 5.3902\n",
      "Epoch [5/5], Step [14100/136675], Loss: 5.6564\n",
      "Epoch [5/5], Step [14175/136675], Loss: 5.3217\n",
      "Epoch [5/5], Step [14250/136675], Loss: 5.3435\n",
      "Epoch [5/5], Step [14325/136675], Loss: 5.0869\n",
      "Epoch [5/5], Step [14400/136675], Loss: 5.3740\n",
      "Epoch [5/5], Step [14475/136675], Loss: 5.2335\n",
      "Epoch [5/5], Step [14550/136675], Loss: 5.2245\n",
      "Epoch [5/5], Step [14625/136675], Loss: 5.5073\n",
      "Epoch [5/5], Step [14700/136675], Loss: 5.1188\n",
      "Epoch [5/5], Step [14775/136675], Loss: 5.0783\n",
      "Epoch [5/5], Step [14850/136675], Loss: 5.2012\n",
      "Epoch [5/5], Step [14925/136675], Loss: 5.0962\n",
      "Epoch [5/5], Step [15000/136675], Loss: 5.0863\n",
      "Epoch [5/5], Step [15075/136675], Loss: 4.8565\n",
      "Epoch [5/5], Step [15150/136675], Loss: 5.2330\n",
      "Epoch [5/5], Step [15225/136675], Loss: 5.3206\n",
      "Epoch [5/5], Step [15300/136675], Loss: 5.3880\n",
      "Epoch [5/5], Step [15375/136675], Loss: 5.3636\n",
      "Epoch [5/5], Step [15450/136675], Loss: 5.1630\n",
      "Epoch [5/5], Step [15525/136675], Loss: 5.3476\n",
      "Epoch [5/5], Step [15600/136675], Loss: 5.5994\n",
      "Epoch [5/5], Step [15675/136675], Loss: 5.2674\n",
      "Epoch [5/5], Step [15750/136675], Loss: 5.1432\n",
      "Epoch [5/5], Step [15825/136675], Loss: 5.6192\n",
      "Epoch [5/5], Step [15900/136675], Loss: 5.2899\n",
      "Epoch [5/5], Step [15975/136675], Loss: 5.2253\n",
      "Epoch [5/5], Step [16050/136675], Loss: 5.2167\n",
      "Epoch [5/5], Step [16125/136675], Loss: 5.2177\n",
      "Epoch [5/5], Step [16200/136675], Loss: 5.4876\n",
      "Epoch [5/5], Step [16275/136675], Loss: 5.3859\n",
      "Epoch [5/5], Step [16350/136675], Loss: 5.6435\n",
      "Epoch [5/5], Step [16425/136675], Loss: 5.2081\n",
      "Epoch [5/5], Step [16500/136675], Loss: 5.1105\n",
      "Epoch [5/5], Step [16575/136675], Loss: 5.2656\n",
      "Epoch [5/5], Step [16650/136675], Loss: 5.5144\n",
      "Epoch [5/5], Step [16725/136675], Loss: 5.2801\n",
      "Epoch [5/5], Step [16800/136675], Loss: 4.9958\n",
      "Epoch [5/5], Step [16875/136675], Loss: 5.1209\n",
      "Epoch [5/5], Step [16950/136675], Loss: 5.3171\n",
      "Epoch [5/5], Step [17025/136675], Loss: 5.6031\n",
      "Epoch [5/5], Step [17100/136675], Loss: 5.1890\n",
      "Epoch [5/5], Step [17175/136675], Loss: 5.0864\n",
      "Epoch [5/5], Step [17250/136675], Loss: 5.1125\n",
      "Epoch [5/5], Step [17325/136675], Loss: 5.1023\n",
      "Epoch [5/5], Step [17400/136675], Loss: 5.4529\n",
      "Epoch [5/5], Step [17475/136675], Loss: 5.1887\n",
      "Epoch [5/5], Step [17550/136675], Loss: 5.1028\n",
      "Epoch [5/5], Step [17625/136675], Loss: 5.3765\n",
      "Epoch [5/5], Step [17700/136675], Loss: 5.3105\n",
      "Epoch [5/5], Step [17775/136675], Loss: 4.8132\n",
      "Epoch [5/5], Step [17850/136675], Loss: 5.3341\n",
      "Epoch [5/5], Step [17925/136675], Loss: 5.2541\n",
      "Epoch [5/5], Step [18000/136675], Loss: 5.3688\n",
      "Epoch [5/5], Step [18075/136675], Loss: 5.4903\n",
      "Epoch [5/5], Step [18150/136675], Loss: 5.1969\n",
      "Epoch [5/5], Step [18225/136675], Loss: 5.0477\n",
      "Epoch [5/5], Step [18300/136675], Loss: 5.4053\n",
      "Epoch [5/5], Step [18375/136675], Loss: 5.4292\n",
      "Epoch [5/5], Step [18450/136675], Loss: 5.2994\n",
      "Epoch [5/5], Step [18525/136675], Loss: 5.2121\n",
      "Epoch [5/5], Step [18600/136675], Loss: 5.4247\n",
      "Epoch [5/5], Step [18675/136675], Loss: 5.2141\n",
      "Epoch [5/5], Step [18750/136675], Loss: 5.2779\n",
      "Epoch [5/5], Step [18825/136675], Loss: 5.2596\n",
      "Epoch [5/5], Step [18900/136675], Loss: 5.5388\n",
      "Epoch [5/5], Step [18975/136675], Loss: 5.4558\n",
      "Epoch [5/5], Step [19050/136675], Loss: 5.2288\n",
      "Epoch [5/5], Step [19125/136675], Loss: 5.1505\n",
      "Epoch [5/5], Step [19200/136675], Loss: 4.9291\n",
      "Epoch [5/5], Step [19275/136675], Loss: 5.0883\n",
      "Epoch [5/5], Step [19350/136675], Loss: 5.1303\n",
      "Epoch [5/5], Step [19425/136675], Loss: 5.2761\n",
      "Epoch [5/5], Step [19500/136675], Loss: 5.6459\n",
      "Epoch [5/5], Step [19575/136675], Loss: 5.3143\n",
      "Epoch [5/5], Step [19650/136675], Loss: 4.8953\n",
      "Epoch [5/5], Step [19725/136675], Loss: 5.1164\n",
      "Epoch [5/5], Step [19800/136675], Loss: 5.5198\n",
      "Epoch [5/5], Step [19875/136675], Loss: 5.4870\n",
      "Epoch [5/5], Step [19950/136675], Loss: 5.1232\n",
      "Validation perplexity: 155.6457316256732\n",
      "Epoch [5/5], Step [20025/136675], Loss: 5.1883\n",
      "Epoch [5/5], Step [20100/136675], Loss: 5.3500\n",
      "Epoch [5/5], Step [20175/136675], Loss: 4.9968\n",
      "Epoch [5/5], Step [20250/136675], Loss: 5.2668\n",
      "Epoch [5/5], Step [20325/136675], Loss: 5.4495\n",
      "Epoch [5/5], Step [20400/136675], Loss: 5.1006\n",
      "Epoch [5/5], Step [20475/136675], Loss: 5.4889\n",
      "Epoch [5/5], Step [20550/136675], Loss: 5.3054\n",
      "Epoch [5/5], Step [20625/136675], Loss: 5.3907\n",
      "Epoch [5/5], Step [20700/136675], Loss: 5.1712\n",
      "Epoch [5/5], Step [20775/136675], Loss: 5.0521\n",
      "Epoch [5/5], Step [20850/136675], Loss: 5.3240\n",
      "Epoch [5/5], Step [20925/136675], Loss: 5.2225\n",
      "Epoch [5/5], Step [21000/136675], Loss: 5.4162\n",
      "Epoch [5/5], Step [21075/136675], Loss: 5.3181\n",
      "Epoch [5/5], Step [21150/136675], Loss: 5.3251\n",
      "Epoch [5/5], Step [21225/136675], Loss: 4.9458\n",
      "Epoch [5/5], Step [21300/136675], Loss: 5.0329\n",
      "Epoch [5/5], Step [21375/136675], Loss: 5.4080\n",
      "Epoch [5/5], Step [21450/136675], Loss: 5.3216\n",
      "Epoch [5/5], Step [21525/136675], Loss: 4.9711\n",
      "Epoch [5/5], Step [21600/136675], Loss: 5.5182\n",
      "Epoch [5/5], Step [21675/136675], Loss: 5.3557\n",
      "Epoch [5/5], Step [21750/136675], Loss: 5.1200\n",
      "Epoch [5/5], Step [21825/136675], Loss: 5.3631\n",
      "Epoch [5/5], Step [21900/136675], Loss: 4.9952\n",
      "Epoch [5/5], Step [21975/136675], Loss: 4.9777\n",
      "Epoch [5/5], Step [22050/136675], Loss: 5.2450\n",
      "Epoch [5/5], Step [22125/136675], Loss: 5.5495\n",
      "Epoch [5/5], Step [22200/136675], Loss: 5.0684\n",
      "Epoch [5/5], Step [22275/136675], Loss: 5.3010\n",
      "Epoch [5/5], Step [22350/136675], Loss: 5.2728\n",
      "Epoch [5/5], Step [22425/136675], Loss: 5.4782\n",
      "Epoch [5/5], Step [22500/136675], Loss: 5.1593\n",
      "Epoch [5/5], Step [22575/136675], Loss: 5.2967\n",
      "Epoch [5/5], Step [22650/136675], Loss: 4.7320\n",
      "Epoch [5/5], Step [22725/136675], Loss: 4.9675\n",
      "Epoch [5/5], Step [22800/136675], Loss: 5.2292\n",
      "Epoch [5/5], Step [22875/136675], Loss: 5.3270\n",
      "Epoch [5/5], Step [22950/136675], Loss: 5.0467\n",
      "Epoch [5/5], Step [23025/136675], Loss: 5.3681\n",
      "Epoch [5/5], Step [23100/136675], Loss: 5.3306\n",
      "Epoch [5/5], Step [23175/136675], Loss: 5.3572\n",
      "Epoch [5/5], Step [23250/136675], Loss: 5.3531\n",
      "Epoch [5/5], Step [23325/136675], Loss: 5.3419\n",
      "Epoch [5/5], Step [23400/136675], Loss: 5.1399\n",
      "Epoch [5/5], Step [23475/136675], Loss: 5.1894\n",
      "Epoch [5/5], Step [23550/136675], Loss: 5.4087\n",
      "Epoch [5/5], Step [23625/136675], Loss: 5.1822\n",
      "Epoch [5/5], Step [23700/136675], Loss: 5.2059\n",
      "Epoch [5/5], Step [23775/136675], Loss: 5.2687\n",
      "Epoch [5/5], Step [23850/136675], Loss: 5.4102\n",
      "Epoch [5/5], Step [23925/136675], Loss: 5.3545\n",
      "Epoch [5/5], Step [24000/136675], Loss: 5.4249\n",
      "Epoch [5/5], Step [24075/136675], Loss: 4.9804\n",
      "Epoch [5/5], Step [24150/136675], Loss: 4.9932\n",
      "Epoch [5/5], Step [24225/136675], Loss: 5.3228\n",
      "Epoch [5/5], Step [24300/136675], Loss: 5.4415\n",
      "Epoch [5/5], Step [24375/136675], Loss: 5.3662\n",
      "Epoch [5/5], Step [24450/136675], Loss: 5.0761\n",
      "Epoch [5/5], Step [24525/136675], Loss: 5.3554\n",
      "Epoch [5/5], Step [24600/136675], Loss: 4.9958\n",
      "Epoch [5/5], Step [24675/136675], Loss: 5.3976\n",
      "Epoch [5/5], Step [24750/136675], Loss: 5.1209\n",
      "Epoch [5/5], Step [24825/136675], Loss: 5.0862\n",
      "Epoch [5/5], Step [24900/136675], Loss: 5.3301\n",
      "Epoch [5/5], Step [24975/136675], Loss: 5.1840\n",
      "Epoch [5/5], Step [25050/136675], Loss: 5.2404\n",
      "Epoch [5/5], Step [25125/136675], Loss: 5.1412\n",
      "Epoch [5/5], Step [25200/136675], Loss: 5.3179\n",
      "Epoch [5/5], Step [25275/136675], Loss: 5.3076\n",
      "Epoch [5/5], Step [25350/136675], Loss: 5.3522\n",
      "Epoch [5/5], Step [25425/136675], Loss: 5.2208\n",
      "Epoch [5/5], Step [25500/136675], Loss: 5.4028\n",
      "Epoch [5/5], Step [25575/136675], Loss: 5.5812\n",
      "Epoch [5/5], Step [25650/136675], Loss: 5.5842\n",
      "Epoch [5/5], Step [25725/136675], Loss: 5.5352\n",
      "Epoch [5/5], Step [25800/136675], Loss: 5.0086\n",
      "Epoch [5/5], Step [25875/136675], Loss: 5.4225\n",
      "Epoch [5/5], Step [25950/136675], Loss: 5.3957\n",
      "Epoch [5/5], Step [26025/136675], Loss: 5.0535\n",
      "Epoch [5/5], Step [26100/136675], Loss: 5.2075\n",
      "Epoch [5/5], Step [26175/136675], Loss: 5.3544\n",
      "Epoch [5/5], Step [26250/136675], Loss: 5.2846\n",
      "Epoch [5/5], Step [26325/136675], Loss: 5.2455\n",
      "Epoch [5/5], Step [26400/136675], Loss: 5.2701\n",
      "Epoch [5/5], Step [26475/136675], Loss: 4.8428\n",
      "Epoch [5/5], Step [26550/136675], Loss: 5.3836\n",
      "Epoch [5/5], Step [26625/136675], Loss: 5.2895\n",
      "Epoch [5/5], Step [26700/136675], Loss: 5.3401\n",
      "Epoch [5/5], Step [26775/136675], Loss: 5.1752\n",
      "Epoch [5/5], Step [26850/136675], Loss: 5.1592\n",
      "Epoch [5/5], Step [26925/136675], Loss: 5.2556\n",
      "Epoch [5/5], Step [27000/136675], Loss: 5.3200\n",
      "Epoch [5/5], Step [27075/136675], Loss: 5.2828\n",
      "Epoch [5/5], Step [27150/136675], Loss: 4.8974\n",
      "Epoch [5/5], Step [27225/136675], Loss: 5.5935\n",
      "Epoch [5/5], Step [27300/136675], Loss: 5.3173\n",
      "Epoch [5/5], Step [27375/136675], Loss: 5.5009\n",
      "Epoch [5/5], Step [27450/136675], Loss: 5.4875\n",
      "Epoch [5/5], Step [27525/136675], Loss: 5.4391\n",
      "Epoch [5/5], Step [27600/136675], Loss: 5.4918\n",
      "Epoch [5/5], Step [27675/136675], Loss: 5.4035\n",
      "Epoch [5/5], Step [27750/136675], Loss: 5.1552\n",
      "Epoch [5/5], Step [27825/136675], Loss: 5.2774\n",
      "Epoch [5/5], Step [27900/136675], Loss: 4.8653\n",
      "Epoch [5/5], Step [27975/136675], Loss: 5.2330\n",
      "Epoch [5/5], Step [28050/136675], Loss: 5.3092\n",
      "Epoch [5/5], Step [28125/136675], Loss: 5.2130\n",
      "Epoch [5/5], Step [28200/136675], Loss: 5.5515\n",
      "Epoch [5/5], Step [28275/136675], Loss: 5.4953\n",
      "Epoch [5/5], Step [28350/136675], Loss: 5.2878\n",
      "Epoch [5/5], Step [28425/136675], Loss: 5.2742\n",
      "Epoch [5/5], Step [28500/136675], Loss: 5.1961\n",
      "Epoch [5/5], Step [28575/136675], Loss: 5.4663\n",
      "Epoch [5/5], Step [28650/136675], Loss: 5.1388\n",
      "Epoch [5/5], Step [28725/136675], Loss: 5.1489\n",
      "Epoch [5/5], Step [28800/136675], Loss: 5.2736\n",
      "Epoch [5/5], Step [28875/136675], Loss: 5.0582\n",
      "Epoch [5/5], Step [28950/136675], Loss: 5.1803\n",
      "Epoch [5/5], Step [29025/136675], Loss: 5.4761\n",
      "Epoch [5/5], Step [29100/136675], Loss: 5.4711\n",
      "Epoch [5/5], Step [29175/136675], Loss: 5.2524\n",
      "Epoch [5/5], Step [29250/136675], Loss: 5.3085\n",
      "Epoch [5/5], Step [29325/136675], Loss: 5.4319\n",
      "Epoch [5/5], Step [29400/136675], Loss: 5.2445\n",
      "Epoch [5/5], Step [29475/136675], Loss: 5.5645\n",
      "Epoch [5/5], Step [29550/136675], Loss: 5.1099\n",
      "Epoch [5/5], Step [29625/136675], Loss: 5.2560\n",
      "Epoch [5/5], Step [29700/136675], Loss: 5.3446\n",
      "Epoch [5/5], Step [29775/136675], Loss: 5.3358\n",
      "Epoch [5/5], Step [29850/136675], Loss: 5.5739\n",
      "Epoch [5/5], Step [29925/136675], Loss: 5.7438\n",
      "Epoch [5/5], Step [30000/136675], Loss: 5.3533\n",
      "Validation perplexity: 156.3588852302087\n",
      "Epoch [5/5], Step [30075/136675], Loss: 5.6366\n",
      "Epoch [5/5], Step [30150/136675], Loss: 5.4190\n",
      "Epoch [5/5], Step [30225/136675], Loss: 5.3381\n",
      "Epoch [5/5], Step [30300/136675], Loss: 5.1004\n",
      "Epoch [5/5], Step [30375/136675], Loss: 5.1562\n",
      "Epoch [5/5], Step [30450/136675], Loss: 5.3105\n",
      "Epoch [5/5], Step [30525/136675], Loss: 5.5395\n",
      "Epoch [5/5], Step [30600/136675], Loss: 5.4254\n",
      "Epoch [5/5], Step [30675/136675], Loss: 5.3250\n",
      "Epoch [5/5], Step [30750/136675], Loss: 5.2679\n",
      "Epoch [5/5], Step [30825/136675], Loss: 5.4439\n",
      "Epoch [5/5], Step [30900/136675], Loss: 5.3233\n",
      "Epoch [5/5], Step [30975/136675], Loss: 5.5223\n",
      "Epoch [5/5], Step [31050/136675], Loss: 5.0911\n",
      "Epoch [5/5], Step [31125/136675], Loss: 5.3929\n",
      "Epoch [5/5], Step [31200/136675], Loss: 5.2148\n",
      "Epoch [5/5], Step [31275/136675], Loss: 5.3864\n",
      "Epoch [5/5], Step [31350/136675], Loss: 5.4545\n",
      "Epoch [5/5], Step [31425/136675], Loss: 5.4190\n",
      "Epoch [5/5], Step [31500/136675], Loss: 5.3845\n",
      "Epoch [5/5], Step [31575/136675], Loss: 4.8672\n",
      "Epoch [5/5], Step [31650/136675], Loss: 5.1664\n",
      "Epoch [5/5], Step [31725/136675], Loss: 5.2509\n",
      "Epoch [5/5], Step [31800/136675], Loss: 5.0646\n",
      "Epoch [5/5], Step [31875/136675], Loss: 5.1154\n",
      "Epoch [5/5], Step [31950/136675], Loss: 5.0665\n",
      "Epoch [5/5], Step [32025/136675], Loss: 5.5075\n",
      "Epoch [5/5], Step [32100/136675], Loss: 5.4974\n",
      "Epoch [5/5], Step [32175/136675], Loss: 5.2568\n",
      "Epoch [5/5], Step [32250/136675], Loss: 5.0783\n",
      "Epoch [5/5], Step [32325/136675], Loss: 5.3536\n",
      "Epoch [5/5], Step [32400/136675], Loss: 5.0013\n",
      "Epoch [5/5], Step [32475/136675], Loss: 5.7173\n",
      "Epoch [5/5], Step [32550/136675], Loss: 5.5093\n",
      "Epoch [5/5], Step [32625/136675], Loss: 5.3405\n",
      "Epoch [5/5], Step [32700/136675], Loss: 5.2725\n",
      "Epoch [5/5], Step [32775/136675], Loss: 5.5465\n",
      "Epoch [5/5], Step [32850/136675], Loss: 4.9230\n",
      "Epoch [5/5], Step [32925/136675], Loss: 5.0169\n",
      "Epoch [5/5], Step [33000/136675], Loss: 5.1119\n",
      "Epoch [5/5], Step [33075/136675], Loss: 5.5456\n",
      "Epoch [5/5], Step [33150/136675], Loss: 5.7139\n",
      "Epoch [5/5], Step [33225/136675], Loss: 5.5539\n",
      "Epoch [5/5], Step [33300/136675], Loss: 5.4383\n",
      "Epoch [5/5], Step [33375/136675], Loss: 5.3345\n",
      "Epoch [5/5], Step [33450/136675], Loss: 5.4471\n",
      "Epoch [5/5], Step [33525/136675], Loss: 5.4288\n",
      "Epoch [5/5], Step [33600/136675], Loss: 5.1082\n",
      "Epoch [5/5], Step [33675/136675], Loss: 5.5017\n",
      "Epoch [5/5], Step [33750/136675], Loss: 5.7048\n",
      "Epoch [5/5], Step [33825/136675], Loss: 5.4526\n",
      "Epoch [5/5], Step [33900/136675], Loss: 4.9526\n",
      "Epoch [5/5], Step [33975/136675], Loss: 5.1957\n",
      "Epoch [5/5], Step [34050/136675], Loss: 5.3299\n",
      "Epoch [5/5], Step [34125/136675], Loss: 5.1933\n",
      "Epoch [5/5], Step [34200/136675], Loss: 5.4788\n",
      "Epoch [5/5], Step [34275/136675], Loss: 5.3667\n",
      "Epoch [5/5], Step [34350/136675], Loss: 5.2713\n",
      "Epoch [5/5], Step [34425/136675], Loss: 5.1828\n",
      "Epoch [5/5], Step [34500/136675], Loss: 5.1446\n",
      "Epoch [5/5], Step [34575/136675], Loss: 5.3211\n",
      "Epoch [5/5], Step [34650/136675], Loss: 5.2500\n",
      "Epoch [5/5], Step [34725/136675], Loss: 5.5094\n",
      "Epoch [5/5], Step [34800/136675], Loss: 5.2435\n",
      "Epoch [5/5], Step [34875/136675], Loss: 5.1034\n",
      "Epoch [5/5], Step [34950/136675], Loss: 5.2074\n",
      "Epoch [5/5], Step [35025/136675], Loss: 5.3279\n",
      "Epoch [5/5], Step [35100/136675], Loss: 5.4199\n",
      "Epoch [5/5], Step [35175/136675], Loss: 5.4543\n",
      "Epoch [5/5], Step [35250/136675], Loss: 5.2766\n",
      "Epoch [5/5], Step [35325/136675], Loss: 5.2961\n",
      "Epoch [5/5], Step [35400/136675], Loss: 5.3687\n",
      "Epoch [5/5], Step [35475/136675], Loss: 5.2433\n",
      "Epoch [5/5], Step [35550/136675], Loss: 5.4993\n",
      "Epoch [5/5], Step [35625/136675], Loss: 5.3244\n",
      "Epoch [5/5], Step [35700/136675], Loss: 5.1875\n",
      "Epoch [5/5], Step [35775/136675], Loss: 5.3048\n",
      "Epoch [5/5], Step [35850/136675], Loss: 5.5691\n",
      "Epoch [5/5], Step [35925/136675], Loss: 5.3987\n",
      "Epoch [5/5], Step [36000/136675], Loss: 5.1805\n",
      "Epoch [5/5], Step [36075/136675], Loss: 5.2312\n",
      "Epoch [5/5], Step [36150/136675], Loss: 5.2592\n",
      "Epoch [5/5], Step [36225/136675], Loss: 5.2146\n",
      "Epoch [5/5], Step [36300/136675], Loss: 5.1045\n",
      "Epoch [5/5], Step [36375/136675], Loss: 5.2902\n",
      "Epoch [5/5], Step [36450/136675], Loss: 5.4138\n",
      "Epoch [5/5], Step [36525/136675], Loss: 5.5161\n",
      "Epoch [5/5], Step [36600/136675], Loss: 5.0736\n",
      "Epoch [5/5], Step [36675/136675], Loss: 5.2370\n",
      "Epoch [5/5], Step [36750/136675], Loss: 5.2656\n",
      "Epoch [5/5], Step [36825/136675], Loss: 5.4137\n",
      "Epoch [5/5], Step [36900/136675], Loss: 5.2387\n",
      "Epoch [5/5], Step [36975/136675], Loss: 5.1882\n",
      "Epoch [5/5], Step [37050/136675], Loss: 5.1534\n",
      "Epoch [5/5], Step [37125/136675], Loss: 5.3688\n",
      "Epoch [5/5], Step [37200/136675], Loss: 5.3157\n",
      "Epoch [5/5], Step [37275/136675], Loss: 5.3012\n",
      "Epoch [5/5], Step [37350/136675], Loss: 5.2157\n",
      "Epoch [5/5], Step [37425/136675], Loss: 5.2552\n",
      "Epoch [5/5], Step [37500/136675], Loss: 5.1022\n",
      "Epoch [5/5], Step [37575/136675], Loss: 5.1746\n",
      "Epoch [5/5], Step [37650/136675], Loss: 5.4153\n",
      "Epoch [5/5], Step [37725/136675], Loss: 4.7847\n",
      "Epoch [5/5], Step [37800/136675], Loss: 5.5056\n",
      "Epoch [5/5], Step [37875/136675], Loss: 5.3271\n",
      "Epoch [5/5], Step [37950/136675], Loss: 5.3468\n",
      "Epoch [5/5], Step [38025/136675], Loss: 5.0877\n",
      "Epoch [5/5], Step [38100/136675], Loss: 5.1643\n",
      "Epoch [5/5], Step [38175/136675], Loss: 5.4033\n",
      "Epoch [5/5], Step [38250/136675], Loss: 4.9678\n",
      "Epoch [5/5], Step [38325/136675], Loss: 5.4020\n",
      "Epoch [5/5], Step [38400/136675], Loss: 5.5446\n",
      "Epoch [5/5], Step [38475/136675], Loss: 5.1982\n",
      "Epoch [5/5], Step [38550/136675], Loss: 5.7395\n",
      "Epoch [5/5], Step [38625/136675], Loss: 5.2590\n",
      "Epoch [5/5], Step [38700/136675], Loss: 5.3263\n",
      "Epoch [5/5], Step [38775/136675], Loss: 5.4200\n",
      "Epoch [5/5], Step [38850/136675], Loss: 5.6775\n",
      "Epoch [5/5], Step [38925/136675], Loss: 5.2960\n",
      "Epoch [5/5], Step [39000/136675], Loss: 5.0223\n",
      "Epoch [5/5], Step [39075/136675], Loss: 5.0493\n",
      "Epoch [5/5], Step [39150/136675], Loss: 5.2637\n",
      "Epoch [5/5], Step [39225/136675], Loss: 5.3956\n",
      "Epoch [5/5], Step [39300/136675], Loss: 5.1456\n",
      "Epoch [5/5], Step [39375/136675], Loss: 5.3898\n",
      "Epoch [5/5], Step [39450/136675], Loss: 5.2317\n",
      "Epoch [5/5], Step [39525/136675], Loss: 5.3815\n",
      "Epoch [5/5], Step [39600/136675], Loss: 5.3063\n",
      "Epoch [5/5], Step [39675/136675], Loss: 5.4691\n",
      "Epoch [5/5], Step [39750/136675], Loss: 5.4718\n",
      "Epoch [5/5], Step [39825/136675], Loss: 5.2187\n",
      "Epoch [5/5], Step [39900/136675], Loss: 5.3630\n",
      "Epoch [5/5], Step [39975/136675], Loss: 5.3475\n",
      "Validation perplexity: 155.24856321678132\n",
      "Epoch [5/5], Step [40050/136675], Loss: 5.8203\n",
      "Epoch [5/5], Step [40125/136675], Loss: 5.1286\n",
      "Epoch [5/5], Step [40200/136675], Loss: 5.4126\n",
      "Epoch [5/5], Step [40275/136675], Loss: 4.7800\n",
      "Epoch [5/5], Step [40350/136675], Loss: 5.5407\n",
      "Epoch [5/5], Step [40425/136675], Loss: 5.5730\n",
      "Epoch [5/5], Step [40500/136675], Loss: 5.2688\n",
      "Epoch [5/5], Step [40575/136675], Loss: 5.4654\n",
      "Epoch [5/5], Step [40650/136675], Loss: 5.1969\n",
      "Epoch [5/5], Step [40725/136675], Loss: 5.2023\n",
      "Epoch [5/5], Step [40800/136675], Loss: 5.1881\n",
      "Epoch [5/5], Step [40875/136675], Loss: 5.3481\n",
      "Epoch [5/5], Step [40950/136675], Loss: 5.4918\n",
      "Epoch [5/5], Step [41025/136675], Loss: 5.1502\n",
      "Epoch [5/5], Step [41100/136675], Loss: 5.3318\n",
      "Epoch [5/5], Step [41175/136675], Loss: 5.7970\n",
      "Epoch [5/5], Step [41250/136675], Loss: 5.2970\n",
      "Epoch [5/5], Step [41325/136675], Loss: 5.2726\n",
      "Epoch [5/5], Step [41400/136675], Loss: 4.9019\n",
      "Epoch [5/5], Step [41475/136675], Loss: 5.4159\n",
      "Epoch [5/5], Step [41550/136675], Loss: 5.2436\n",
      "Epoch [5/5], Step [41625/136675], Loss: 5.2379\n",
      "Epoch [5/5], Step [41700/136675], Loss: 4.9187\n",
      "Epoch [5/5], Step [41775/136675], Loss: 5.1958\n",
      "Epoch [5/5], Step [41850/136675], Loss: 5.2271\n",
      "Epoch [5/5], Step [41925/136675], Loss: 5.1067\n",
      "Epoch [5/5], Step [42000/136675], Loss: 5.3760\n",
      "Epoch [5/5], Step [42075/136675], Loss: 5.3369\n",
      "Epoch [5/5], Step [42150/136675], Loss: 5.2666\n",
      "Epoch [5/5], Step [42225/136675], Loss: 5.3296\n",
      "Epoch [5/5], Step [42300/136675], Loss: 5.4597\n",
      "Epoch [5/5], Step [42375/136675], Loss: 5.6380\n",
      "Epoch [5/5], Step [42450/136675], Loss: 5.2724\n",
      "Epoch [5/5], Step [42525/136675], Loss: 5.2559\n",
      "Epoch [5/5], Step [42600/136675], Loss: 5.2209\n",
      "Epoch [5/5], Step [42675/136675], Loss: 5.4021\n",
      "Epoch [5/5], Step [42750/136675], Loss: 5.4051\n",
      "Epoch [5/5], Step [42825/136675], Loss: 5.5831\n",
      "Epoch [5/5], Step [42900/136675], Loss: 5.4193\n",
      "Epoch [5/5], Step [42975/136675], Loss: 5.3991\n",
      "Epoch [5/5], Step [43050/136675], Loss: 5.3416\n",
      "Epoch [5/5], Step [43125/136675], Loss: 5.2919\n",
      "Epoch [5/5], Step [43200/136675], Loss: 5.2094\n",
      "Epoch [5/5], Step [43275/136675], Loss: 5.4574\n",
      "Epoch [5/5], Step [43350/136675], Loss: 5.4134\n",
      "Epoch [5/5], Step [43425/136675], Loss: 5.4903\n",
      "Epoch [5/5], Step [43500/136675], Loss: 4.9902\n",
      "Epoch [5/5], Step [43575/136675], Loss: 4.9069\n",
      "Epoch [5/5], Step [43650/136675], Loss: 5.4081\n",
      "Epoch [5/5], Step [43725/136675], Loss: 4.8845\n",
      "Epoch [5/5], Step [43800/136675], Loss: 5.3588\n",
      "Epoch [5/5], Step [43875/136675], Loss: 5.2663\n",
      "Epoch [5/5], Step [43950/136675], Loss: 5.5128\n",
      "Epoch [5/5], Step [44025/136675], Loss: 5.4147\n",
      "Epoch [5/5], Step [44100/136675], Loss: 5.1876\n",
      "Epoch [5/5], Step [44175/136675], Loss: 5.4961\n",
      "Epoch [5/5], Step [44250/136675], Loss: 5.2998\n",
      "Epoch [5/5], Step [44325/136675], Loss: 5.4912\n",
      "Epoch [5/5], Step [44400/136675], Loss: 5.2717\n",
      "Epoch [5/5], Step [44475/136675], Loss: 5.4246\n",
      "Epoch [5/5], Step [44550/136675], Loss: 5.3386\n",
      "Epoch [5/5], Step [44625/136675], Loss: 5.0899\n",
      "Epoch [5/5], Step [44700/136675], Loss: 5.0659\n",
      "Epoch [5/5], Step [44775/136675], Loss: 5.0843\n",
      "Epoch [5/5], Step [44850/136675], Loss: 5.1381\n",
      "Epoch [5/5], Step [44925/136675], Loss: 5.2043\n",
      "Epoch [5/5], Step [45000/136675], Loss: 5.4389\n",
      "Epoch [5/5], Step [45075/136675], Loss: 5.1140\n",
      "Epoch [5/5], Step [45150/136675], Loss: 5.3446\n",
      "Epoch [5/5], Step [45225/136675], Loss: 5.3618\n",
      "Epoch [5/5], Step [45300/136675], Loss: 5.4810\n",
      "Epoch [5/5], Step [45375/136675], Loss: 5.0593\n",
      "Epoch [5/5], Step [45450/136675], Loss: 5.1692\n",
      "Epoch [5/5], Step [45525/136675], Loss: 5.2700\n",
      "Epoch [5/5], Step [45600/136675], Loss: 5.5065\n",
      "Epoch [5/5], Step [45675/136675], Loss: 5.1528\n",
      "Epoch [5/5], Step [45750/136675], Loss: 5.4385\n",
      "Epoch [5/5], Step [45825/136675], Loss: 4.9169\n",
      "Epoch [5/5], Step [45900/136675], Loss: 5.2425\n",
      "Epoch [5/5], Step [45975/136675], Loss: 5.3461\n",
      "Epoch [5/5], Step [46050/136675], Loss: 5.5263\n",
      "Epoch [5/5], Step [46125/136675], Loss: 5.1715\n",
      "Epoch [5/5], Step [46200/136675], Loss: 5.3026\n",
      "Epoch [5/5], Step [46275/136675], Loss: 5.4062\n",
      "Epoch [5/5], Step [46350/136675], Loss: 5.1430\n",
      "Epoch [5/5], Step [46425/136675], Loss: 5.3332\n",
      "Epoch [5/5], Step [46500/136675], Loss: 4.8965\n",
      "Epoch [5/5], Step [46575/136675], Loss: 5.4004\n",
      "Epoch [5/5], Step [46650/136675], Loss: 5.2283\n",
      "Epoch [5/5], Step [46725/136675], Loss: 5.3079\n",
      "Epoch [5/5], Step [46800/136675], Loss: 5.4479\n",
      "Epoch [5/5], Step [46875/136675], Loss: 5.1354\n",
      "Epoch [5/5], Step [46950/136675], Loss: 5.3066\n",
      "Epoch [5/5], Step [47025/136675], Loss: 5.3395\n",
      "Epoch [5/5], Step [47100/136675], Loss: 5.2265\n",
      "Epoch [5/5], Step [47175/136675], Loss: 5.6169\n",
      "Epoch [5/5], Step [47250/136675], Loss: 5.2049\n",
      "Epoch [5/5], Step [47325/136675], Loss: 5.1636\n",
      "Epoch [5/5], Step [47400/136675], Loss: 5.4066\n",
      "Epoch [5/5], Step [47475/136675], Loss: 4.9981\n",
      "Epoch [5/5], Step [47550/136675], Loss: 5.0046\n",
      "Epoch [5/5], Step [47625/136675], Loss: 5.1002\n",
      "Epoch [5/5], Step [47700/136675], Loss: 5.0362\n",
      "Epoch [5/5], Step [47775/136675], Loss: 5.1726\n",
      "Epoch [5/5], Step [47850/136675], Loss: 5.2329\n",
      "Epoch [5/5], Step [47925/136675], Loss: 5.0427\n",
      "Epoch [5/5], Step [48000/136675], Loss: 5.4457\n",
      "Epoch [5/5], Step [48075/136675], Loss: 5.3996\n",
      "Epoch [5/5], Step [48150/136675], Loss: 5.3371\n",
      "Epoch [5/5], Step [48225/136675], Loss: 5.5804\n",
      "Epoch [5/5], Step [48300/136675], Loss: 5.1318\n",
      "Epoch [5/5], Step [48375/136675], Loss: 5.4893\n",
      "Epoch [5/5], Step [48450/136675], Loss: 5.5095\n",
      "Epoch [5/5], Step [48525/136675], Loss: 5.3902\n",
      "Epoch [5/5], Step [48600/136675], Loss: 5.5857\n",
      "Epoch [5/5], Step [48675/136675], Loss: 4.9073\n",
      "Epoch [5/5], Step [48750/136675], Loss: 4.9786\n",
      "Epoch [5/5], Step [48825/136675], Loss: 5.2982\n",
      "Epoch [5/5], Step [48900/136675], Loss: 5.4455\n",
      "Epoch [5/5], Step [48975/136675], Loss: 5.2997\n",
      "Epoch [5/5], Step [49050/136675], Loss: 5.1604\n",
      "Epoch [5/5], Step [49125/136675], Loss: 5.3231\n",
      "Epoch [5/5], Step [49200/136675], Loss: 5.6868\n",
      "Epoch [5/5], Step [49275/136675], Loss: 5.0763\n",
      "Epoch [5/5], Step [49350/136675], Loss: 5.1820\n",
      "Epoch [5/5], Step [49425/136675], Loss: 5.4178\n",
      "Epoch [5/5], Step [49500/136675], Loss: 5.4925\n",
      "Epoch [5/5], Step [49575/136675], Loss: 5.4352\n",
      "Epoch [5/5], Step [49650/136675], Loss: 5.3565\n",
      "Epoch [5/5], Step [49725/136675], Loss: 5.6618\n",
      "Epoch [5/5], Step [49800/136675], Loss: 5.1842\n",
      "Epoch [5/5], Step [49875/136675], Loss: 5.2343\n",
      "Epoch [5/5], Step [49950/136675], Loss: 5.1235\n",
      "Validation perplexity: 154.66677822643032\n",
      "Epoch [5/5], Step [50025/136675], Loss: 5.1845\n",
      "Epoch [5/5], Step [50100/136675], Loss: 5.3452\n",
      "Epoch [5/5], Step [50175/136675], Loss: 5.2488\n",
      "Epoch [5/5], Step [50250/136675], Loss: 5.3374\n",
      "Epoch [5/5], Step [50325/136675], Loss: 5.5854\n",
      "Epoch [5/5], Step [50400/136675], Loss: 5.1718\n",
      "Epoch [5/5], Step [50475/136675], Loss: 5.2736\n",
      "Epoch [5/5], Step [50550/136675], Loss: 5.0928\n",
      "Epoch [5/5], Step [50625/136675], Loss: 5.1617\n",
      "Epoch [5/5], Step [50700/136675], Loss: 5.3026\n",
      "Epoch [5/5], Step [50775/136675], Loss: 5.2215\n",
      "Epoch [5/5], Step [50850/136675], Loss: 5.1334\n",
      "Epoch [5/5], Step [50925/136675], Loss: 5.2548\n",
      "Epoch [5/5], Step [51000/136675], Loss: 5.1733\n",
      "Epoch [5/5], Step [51075/136675], Loss: 5.0891\n",
      "Epoch [5/5], Step [51150/136675], Loss: 5.2638\n",
      "Epoch [5/5], Step [51225/136675], Loss: 5.2290\n",
      "Epoch [5/5], Step [51300/136675], Loss: 5.1484\n",
      "Epoch [5/5], Step [51375/136675], Loss: 5.1814\n",
      "Epoch [5/5], Step [51450/136675], Loss: 5.3929\n",
      "Epoch [5/5], Step [51525/136675], Loss: 5.4377\n",
      "Epoch [5/5], Step [51600/136675], Loss: 5.1582\n",
      "Epoch [5/5], Step [51675/136675], Loss: 5.4077\n",
      "Epoch [5/5], Step [51750/136675], Loss: 5.4225\n",
      "Epoch [5/5], Step [51825/136675], Loss: 5.2194\n",
      "Epoch [5/5], Step [51900/136675], Loss: 5.4083\n",
      "Epoch [5/5], Step [51975/136675], Loss: 5.1601\n",
      "Epoch [5/5], Step [52050/136675], Loss: 5.4669\n",
      "Epoch [5/5], Step [52125/136675], Loss: 5.1403\n",
      "Epoch [5/5], Step [52200/136675], Loss: 5.2264\n",
      "Epoch [5/5], Step [52275/136675], Loss: 4.9845\n",
      "Epoch [5/5], Step [52350/136675], Loss: 5.4331\n",
      "Epoch [5/5], Step [52425/136675], Loss: 5.3938\n",
      "Epoch [5/5], Step [52500/136675], Loss: 5.4114\n",
      "Epoch [5/5], Step [52575/136675], Loss: 4.9558\n",
      "Epoch [5/5], Step [52650/136675], Loss: 5.2226\n",
      "Epoch [5/5], Step [52725/136675], Loss: 5.0378\n",
      "Epoch [5/5], Step [52800/136675], Loss: 5.3322\n",
      "Epoch [5/5], Step [52875/136675], Loss: 5.2708\n",
      "Epoch [5/5], Step [52950/136675], Loss: 5.1464\n",
      "Epoch [5/5], Step [53025/136675], Loss: 5.5947\n",
      "Epoch [5/5], Step [53100/136675], Loss: 5.6392\n",
      "Epoch [5/5], Step [53175/136675], Loss: 5.3020\n",
      "Epoch [5/5], Step [53250/136675], Loss: 5.3853\n",
      "Epoch [5/5], Step [53325/136675], Loss: 4.9886\n",
      "Epoch [5/5], Step [53400/136675], Loss: 5.3143\n",
      "Epoch [5/5], Step [53475/136675], Loss: 5.2535\n",
      "Epoch [5/5], Step [53550/136675], Loss: 5.2596\n",
      "Epoch [5/5], Step [53625/136675], Loss: 5.3413\n",
      "Epoch [5/5], Step [53700/136675], Loss: 5.3530\n",
      "Epoch [5/5], Step [53775/136675], Loss: 4.9828\n",
      "Epoch [5/5], Step [53850/136675], Loss: 5.2580\n",
      "Epoch [5/5], Step [53925/136675], Loss: 5.3568\n",
      "Epoch [5/5], Step [54000/136675], Loss: 5.0933\n",
      "Epoch [5/5], Step [54075/136675], Loss: 5.5870\n",
      "Epoch [5/5], Step [54150/136675], Loss: 5.0878\n",
      "Epoch [5/5], Step [54225/136675], Loss: 5.2551\n",
      "Epoch [5/5], Step [54300/136675], Loss: 5.2278\n",
      "Epoch [5/5], Step [54375/136675], Loss: 5.1628\n",
      "Epoch [5/5], Step [54450/136675], Loss: 5.3663\n",
      "Epoch [5/5], Step [54525/136675], Loss: 5.4382\n",
      "Epoch [5/5], Step [54600/136675], Loss: 4.7625\n",
      "Epoch [5/5], Step [54675/136675], Loss: 5.4636\n",
      "Epoch [5/5], Step [54750/136675], Loss: 5.5686\n",
      "Epoch [5/5], Step [54825/136675], Loss: 5.3005\n",
      "Epoch [5/5], Step [54900/136675], Loss: 5.0007\n",
      "Epoch [5/5], Step [54975/136675], Loss: 5.1738\n",
      "Epoch [5/5], Step [55050/136675], Loss: 5.1868\n",
      "Epoch [5/5], Step [55125/136675], Loss: 5.4476\n",
      "Epoch [5/5], Step [55200/136675], Loss: 5.3761\n",
      "Epoch [5/5], Step [55275/136675], Loss: 5.1347\n",
      "Epoch [5/5], Step [55350/136675], Loss: 5.2454\n",
      "Epoch [5/5], Step [55425/136675], Loss: 5.2348\n",
      "Epoch [5/5], Step [55500/136675], Loss: 5.4737\n",
      "Epoch [5/5], Step [55575/136675], Loss: 5.2627\n",
      "Epoch [5/5], Step [55650/136675], Loss: 5.3221\n",
      "Epoch [5/5], Step [55725/136675], Loss: 5.3482\n",
      "Epoch [5/5], Step [55800/136675], Loss: 5.3710\n",
      "Epoch [5/5], Step [55875/136675], Loss: 5.1661\n",
      "Epoch [5/5], Step [55950/136675], Loss: 5.2695\n",
      "Epoch [5/5], Step [56025/136675], Loss: 5.5416\n",
      "Epoch [5/5], Step [56100/136675], Loss: 5.2296\n",
      "Epoch [5/5], Step [56175/136675], Loss: 5.4601\n",
      "Epoch [5/5], Step [56250/136675], Loss: 5.1732\n",
      "Epoch [5/5], Step [56325/136675], Loss: 5.3161\n",
      "Epoch [5/5], Step [56400/136675], Loss: 5.3290\n",
      "Epoch [5/5], Step [56475/136675], Loss: 5.3882\n",
      "Epoch [5/5], Step [56550/136675], Loss: 5.5680\n",
      "Epoch [5/5], Step [56625/136675], Loss: 5.4161\n",
      "Epoch [5/5], Step [56700/136675], Loss: 4.9433\n",
      "Epoch [5/5], Step [56775/136675], Loss: 5.4394\n",
      "Epoch [5/5], Step [56850/136675], Loss: 5.0601\n",
      "Epoch [5/5], Step [56925/136675], Loss: 5.1736\n",
      "Epoch [5/5], Step [57000/136675], Loss: 5.1875\n",
      "Epoch [5/5], Step [57075/136675], Loss: 5.4488\n",
      "Epoch [5/5], Step [57150/136675], Loss: 5.2061\n",
      "Epoch [5/5], Step [57225/136675], Loss: 5.4461\n",
      "Epoch [5/5], Step [57300/136675], Loss: 5.2787\n",
      "Epoch [5/5], Step [57375/136675], Loss: 5.2037\n",
      "Epoch [5/5], Step [57450/136675], Loss: 4.9910\n",
      "Epoch [5/5], Step [57525/136675], Loss: 5.4587\n",
      "Epoch [5/5], Step [57600/136675], Loss: 5.3129\n",
      "Epoch [5/5], Step [57675/136675], Loss: 5.2593\n",
      "Epoch [5/5], Step [57750/136675], Loss: 5.1842\n",
      "Epoch [5/5], Step [57825/136675], Loss: 5.3488\n",
      "Epoch [5/5], Step [57900/136675], Loss: 5.3425\n",
      "Epoch [5/5], Step [57975/136675], Loss: 5.1756\n",
      "Epoch [5/5], Step [58050/136675], Loss: 4.9799\n",
      "Epoch [5/5], Step [58125/136675], Loss: 5.3249\n",
      "Epoch [5/5], Step [58200/136675], Loss: 5.3877\n",
      "Epoch [5/5], Step [58275/136675], Loss: 4.9697\n",
      "Epoch [5/5], Step [58350/136675], Loss: 5.3388\n",
      "Epoch [5/5], Step [58425/136675], Loss: 5.2791\n",
      "Epoch [5/5], Step [58500/136675], Loss: 5.2798\n",
      "Epoch [5/5], Step [58575/136675], Loss: 4.9788\n",
      "Epoch [5/5], Step [58650/136675], Loss: 5.1602\n",
      "Epoch [5/5], Step [58725/136675], Loss: 4.9175\n",
      "Epoch [5/5], Step [58800/136675], Loss: 5.4571\n",
      "Epoch [5/5], Step [58875/136675], Loss: 5.3833\n",
      "Epoch [5/5], Step [58950/136675], Loss: 5.2496\n",
      "Epoch [5/5], Step [59025/136675], Loss: 5.1547\n",
      "Epoch [5/5], Step [59100/136675], Loss: 5.2887\n",
      "Epoch [5/5], Step [59175/136675], Loss: 5.4079\n",
      "Epoch [5/5], Step [59250/136675], Loss: 5.0676\n",
      "Epoch [5/5], Step [59325/136675], Loss: 5.2000\n",
      "Epoch [5/5], Step [59400/136675], Loss: 4.9976\n",
      "Epoch [5/5], Step [59475/136675], Loss: 5.5643\n",
      "Epoch [5/5], Step [59550/136675], Loss: 5.3100\n",
      "Epoch [5/5], Step [59625/136675], Loss: 5.4320\n",
      "Epoch [5/5], Step [59700/136675], Loss: 5.4248\n",
      "Epoch [5/5], Step [59775/136675], Loss: 5.2267\n",
      "Epoch [5/5], Step [59850/136675], Loss: 5.2613\n",
      "Epoch [5/5], Step [59925/136675], Loss: 5.4170\n",
      "Epoch [5/5], Step [60000/136675], Loss: 5.1266\n",
      "Validation perplexity: 154.71752708214012\n",
      "Epoch [5/5], Step [60075/136675], Loss: 5.1826\n",
      "Epoch [5/5], Step [60150/136675], Loss: 5.6267\n",
      "Epoch [5/5], Step [60225/136675], Loss: 5.1406\n",
      "Epoch [5/5], Step [60300/136675], Loss: 5.3361\n",
      "Epoch [5/5], Step [60375/136675], Loss: 5.4031\n",
      "Epoch [5/5], Step [60450/136675], Loss: 5.3811\n",
      "Epoch [5/5], Step [60525/136675], Loss: 5.3624\n",
      "Epoch [5/5], Step [60600/136675], Loss: 5.4837\n",
      "Epoch [5/5], Step [60675/136675], Loss: 5.0638\n",
      "Epoch [5/5], Step [60750/136675], Loss: 5.1369\n",
      "Epoch [5/5], Step [60825/136675], Loss: 5.3528\n",
      "Epoch [5/5], Step [60900/136675], Loss: 5.4289\n",
      "Epoch [5/5], Step [60975/136675], Loss: 5.5472\n",
      "Epoch [5/5], Step [61050/136675], Loss: 5.4237\n",
      "Epoch [5/5], Step [61125/136675], Loss: 5.0371\n",
      "Epoch [5/5], Step [61200/136675], Loss: 5.6465\n",
      "Epoch [5/5], Step [61275/136675], Loss: 5.2688\n",
      "Epoch [5/5], Step [61350/136675], Loss: 5.3648\n",
      "Epoch [5/5], Step [61425/136675], Loss: 5.4815\n",
      "Epoch [5/5], Step [61500/136675], Loss: 5.3432\n",
      "Epoch [5/5], Step [61575/136675], Loss: 5.1838\n",
      "Epoch [5/5], Step [61650/136675], Loss: 5.3348\n",
      "Epoch [5/5], Step [61725/136675], Loss: 5.1672\n",
      "Epoch [5/5], Step [61800/136675], Loss: 5.4598\n",
      "Epoch [5/5], Step [61875/136675], Loss: 5.5028\n",
      "Epoch [5/5], Step [61950/136675], Loss: 5.1755\n",
      "Epoch [5/5], Step [62025/136675], Loss: 5.4395\n",
      "Epoch [5/5], Step [62100/136675], Loss: 5.5158\n",
      "Epoch [5/5], Step [62175/136675], Loss: 5.4120\n",
      "Epoch [5/5], Step [62250/136675], Loss: 5.2510\n",
      "Epoch [5/5], Step [62325/136675], Loss: 5.4538\n",
      "Epoch [5/5], Step [62400/136675], Loss: 5.4662\n",
      "Epoch [5/5], Step [62475/136675], Loss: 5.4492\n",
      "Epoch [5/5], Step [62550/136675], Loss: 5.3309\n",
      "Epoch [5/5], Step [62625/136675], Loss: 5.0317\n",
      "Epoch [5/5], Step [62700/136675], Loss: 5.2753\n",
      "Epoch [5/5], Step [62775/136675], Loss: 5.2842\n",
      "Epoch [5/5], Step [62850/136675], Loss: 5.0257\n",
      "Epoch [5/5], Step [62925/136675], Loss: 5.0770\n",
      "Epoch [5/5], Step [63000/136675], Loss: 4.9445\n",
      "Epoch [5/5], Step [63075/136675], Loss: 5.1143\n",
      "Epoch [5/5], Step [63150/136675], Loss: 5.3591\n",
      "Epoch [5/5], Step [63225/136675], Loss: 5.2608\n",
      "Epoch [5/5], Step [63300/136675], Loss: 5.2081\n",
      "Epoch [5/5], Step [63375/136675], Loss: 5.3895\n",
      "Epoch [5/5], Step [63450/136675], Loss: 5.4156\n",
      "Epoch [5/5], Step [63525/136675], Loss: 5.4341\n",
      "Epoch [5/5], Step [63600/136675], Loss: 5.1455\n",
      "Epoch [5/5], Step [63675/136675], Loss: 5.2023\n",
      "Epoch [5/5], Step [63750/136675], Loss: 5.1044\n",
      "Epoch [5/5], Step [63825/136675], Loss: 5.2450\n",
      "Epoch [5/5], Step [63900/136675], Loss: 5.2731\n",
      "Epoch [5/5], Step [63975/136675], Loss: 5.2994\n",
      "Epoch [5/5], Step [64050/136675], Loss: 5.3834\n",
      "Epoch [5/5], Step [64125/136675], Loss: 5.4126\n",
      "Epoch [5/5], Step [64200/136675], Loss: 5.3115\n",
      "Epoch [5/5], Step [64275/136675], Loss: 5.1034\n",
      "Epoch [5/5], Step [64350/136675], Loss: 5.2942\n",
      "Epoch [5/5], Step [64425/136675], Loss: 5.1583\n",
      "Epoch [5/5], Step [64500/136675], Loss: 4.9993\n",
      "Epoch [5/5], Step [64575/136675], Loss: 5.2890\n",
      "Epoch [5/5], Step [64650/136675], Loss: 5.4106\n",
      "Epoch [5/5], Step [64725/136675], Loss: 5.3243\n",
      "Epoch [5/5], Step [64800/136675], Loss: 5.2218\n",
      "Epoch [5/5], Step [64875/136675], Loss: 5.3620\n",
      "Epoch [5/5], Step [64950/136675], Loss: 5.2027\n",
      "Epoch [5/5], Step [65025/136675], Loss: 5.0176\n",
      "Epoch [5/5], Step [65100/136675], Loss: 5.3134\n",
      "Epoch [5/5], Step [65175/136675], Loss: 5.1298\n",
      "Epoch [5/5], Step [65250/136675], Loss: 5.0083\n",
      "Epoch [5/5], Step [65325/136675], Loss: 5.1686\n",
      "Epoch [5/5], Step [65400/136675], Loss: 5.1052\n",
      "Epoch [5/5], Step [65475/136675], Loss: 5.2659\n",
      "Epoch [5/5], Step [65550/136675], Loss: 5.3258\n",
      "Epoch [5/5], Step [65625/136675], Loss: 5.5673\n",
      "Epoch [5/5], Step [65700/136675], Loss: 5.4307\n",
      "Epoch [5/5], Step [65775/136675], Loss: 5.2518\n",
      "Epoch [5/5], Step [65850/136675], Loss: 5.2074\n",
      "Epoch [5/5], Step [65925/136675], Loss: 5.0867\n",
      "Epoch [5/5], Step [66000/136675], Loss: 5.1708\n",
      "Epoch [5/5], Step [66075/136675], Loss: 5.3867\n",
      "Epoch [5/5], Step [66150/136675], Loss: 5.0363\n",
      "Epoch [5/5], Step [66225/136675], Loss: 5.3343\n",
      "Epoch [5/5], Step [66300/136675], Loss: 5.4638\n",
      "Epoch [5/5], Step [66375/136675], Loss: 5.0624\n",
      "Epoch [5/5], Step [66450/136675], Loss: 5.3068\n",
      "Epoch [5/5], Step [66525/136675], Loss: 5.5173\n",
      "Epoch [5/5], Step [66600/136675], Loss: 5.2255\n",
      "Epoch [5/5], Step [66675/136675], Loss: 5.4855\n",
      "Epoch [5/5], Step [66750/136675], Loss: 5.3431\n",
      "Epoch [5/5], Step [66825/136675], Loss: 5.2560\n",
      "Epoch [5/5], Step [66900/136675], Loss: 5.3571\n",
      "Epoch [5/5], Step [66975/136675], Loss: 5.0376\n",
      "Epoch [5/5], Step [67050/136675], Loss: 4.9791\n",
      "Epoch [5/5], Step [67125/136675], Loss: 5.2974\n",
      "Epoch [5/5], Step [67200/136675], Loss: 5.0742\n",
      "Epoch [5/5], Step [67275/136675], Loss: 5.1959\n",
      "Epoch [5/5], Step [67350/136675], Loss: 5.0055\n",
      "Epoch [5/5], Step [67425/136675], Loss: 4.9894\n",
      "Epoch [5/5], Step [67500/136675], Loss: 5.2994\n",
      "Epoch [5/5], Step [67575/136675], Loss: 5.3532\n",
      "Epoch [5/5], Step [67650/136675], Loss: 5.1678\n",
      "Epoch [5/5], Step [67725/136675], Loss: 5.5515\n",
      "Epoch [5/5], Step [67800/136675], Loss: 5.2073\n",
      "Epoch [5/5], Step [67875/136675], Loss: 5.4137\n",
      "Epoch [5/5], Step [67950/136675], Loss: 5.4422\n",
      "Epoch [5/5], Step [68025/136675], Loss: 5.3180\n",
      "Epoch [5/5], Step [68100/136675], Loss: 5.2218\n",
      "Epoch [5/5], Step [68175/136675], Loss: 5.6093\n",
      "Epoch [5/5], Step [68250/136675], Loss: 5.3645\n",
      "Epoch [5/5], Step [68325/136675], Loss: 5.3368\n",
      "Epoch [5/5], Step [68400/136675], Loss: 5.5088\n",
      "Epoch [5/5], Step [68475/136675], Loss: 5.2365\n",
      "Epoch [5/5], Step [68550/136675], Loss: 5.3451\n",
      "Epoch [5/5], Step [68625/136675], Loss: 5.3369\n",
      "Epoch [5/5], Step [68700/136675], Loss: 5.5009\n",
      "Epoch [5/5], Step [68775/136675], Loss: 5.0417\n",
      "Epoch [5/5], Step [68850/136675], Loss: 5.3960\n",
      "Epoch [5/5], Step [68925/136675], Loss: 5.2175\n",
      "Epoch [5/5], Step [69000/136675], Loss: 5.0880\n",
      "Epoch [5/5], Step [69075/136675], Loss: 5.4010\n",
      "Epoch [5/5], Step [69150/136675], Loss: 5.5422\n",
      "Epoch [5/5], Step [69225/136675], Loss: 5.7514\n",
      "Epoch [5/5], Step [69300/136675], Loss: 5.1391\n",
      "Epoch [5/5], Step [69375/136675], Loss: 5.1954\n",
      "Epoch [5/5], Step [69450/136675], Loss: 5.3506\n",
      "Epoch [5/5], Step [69525/136675], Loss: 5.2963\n",
      "Epoch [5/5], Step [69600/136675], Loss: 5.3092\n",
      "Epoch [5/5], Step [69675/136675], Loss: 5.1919\n",
      "Epoch [5/5], Step [69750/136675], Loss: 5.3712\n",
      "Epoch [5/5], Step [69825/136675], Loss: 5.2212\n",
      "Epoch [5/5], Step [69900/136675], Loss: 5.0675\n",
      "Epoch [5/5], Step [69975/136675], Loss: 5.5256\n",
      "Validation perplexity: 153.71117465378046\n",
      "Epoch [5/5], Step [70050/136675], Loss: 5.2107\n",
      "Epoch [5/5], Step [70125/136675], Loss: 5.4150\n",
      "Epoch [5/5], Step [70200/136675], Loss: 5.2261\n",
      "Epoch [5/5], Step [70275/136675], Loss: 5.2242\n",
      "Epoch [5/5], Step [70350/136675], Loss: 5.0897\n",
      "Epoch [5/5], Step [70425/136675], Loss: 5.4153\n",
      "Epoch [5/5], Step [70500/136675], Loss: 5.1739\n",
      "Epoch [5/5], Step [70575/136675], Loss: 5.0027\n",
      "Epoch [5/5], Step [70650/136675], Loss: 5.1447\n",
      "Epoch [5/5], Step [70725/136675], Loss: 5.1693\n",
      "Epoch [5/5], Step [70800/136675], Loss: 5.1627\n",
      "Epoch [5/5], Step [70875/136675], Loss: 5.2958\n",
      "Epoch [5/5], Step [70950/136675], Loss: 5.1616\n",
      "Epoch [5/5], Step [71025/136675], Loss: 5.3542\n",
      "Epoch [5/5], Step [71100/136675], Loss: 5.0580\n",
      "Epoch [5/5], Step [71175/136675], Loss: 5.1842\n",
      "Epoch [5/5], Step [71250/136675], Loss: 5.2176\n",
      "Epoch [5/5], Step [71325/136675], Loss: 5.3088\n",
      "Epoch [5/5], Step [71400/136675], Loss: 5.4751\n",
      "Epoch [5/5], Step [71475/136675], Loss: 5.1479\n",
      "Epoch [5/5], Step [71550/136675], Loss: 5.5873\n",
      "Epoch [5/5], Step [71625/136675], Loss: 5.5443\n",
      "Epoch [5/5], Step [71700/136675], Loss: 5.5080\n",
      "Epoch [5/5], Step [71775/136675], Loss: 5.3638\n",
      "Epoch [5/5], Step [71850/136675], Loss: 5.3176\n",
      "Epoch [5/5], Step [71925/136675], Loss: 4.9812\n",
      "Epoch [5/5], Step [72000/136675], Loss: 5.4747\n",
      "Epoch [5/5], Step [72075/136675], Loss: 5.1914\n",
      "Epoch [5/5], Step [72150/136675], Loss: 4.9650\n",
      "Epoch [5/5], Step [72225/136675], Loss: 5.4778\n",
      "Epoch [5/5], Step [72300/136675], Loss: 5.3310\n",
      "Epoch [5/5], Step [72375/136675], Loss: 5.2323\n",
      "Epoch [5/5], Step [72450/136675], Loss: 5.4239\n",
      "Epoch [5/5], Step [72525/136675], Loss: 5.2924\n",
      "Epoch [5/5], Step [72600/136675], Loss: 5.1881\n",
      "Epoch [5/5], Step [72675/136675], Loss: 5.2616\n",
      "Epoch [5/5], Step [72750/136675], Loss: 5.1311\n",
      "Epoch [5/5], Step [72825/136675], Loss: 5.1696\n",
      "Epoch [5/5], Step [72900/136675], Loss: 5.1598\n",
      "Epoch [5/5], Step [72975/136675], Loss: 5.1233\n",
      "Epoch [5/5], Step [73050/136675], Loss: 5.3238\n",
      "Epoch [5/5], Step [73125/136675], Loss: 5.4056\n",
      "Epoch [5/5], Step [73200/136675], Loss: 5.0623\n",
      "Epoch [5/5], Step [73275/136675], Loss: 5.2969\n",
      "Epoch [5/5], Step [73350/136675], Loss: 5.1052\n",
      "Epoch [5/5], Step [73425/136675], Loss: 5.2426\n",
      "Epoch [5/5], Step [73500/136675], Loss: 5.1267\n",
      "Epoch [5/5], Step [73575/136675], Loss: 5.2724\n",
      "Epoch [5/5], Step [73650/136675], Loss: 5.2289\n",
      "Epoch [5/5], Step [73725/136675], Loss: 5.4231\n",
      "Epoch [5/5], Step [73800/136675], Loss: 5.3187\n",
      "Epoch [5/5], Step [73875/136675], Loss: 5.2494\n",
      "Epoch [5/5], Step [73950/136675], Loss: 5.0860\n",
      "Epoch [5/5], Step [74025/136675], Loss: 5.0235\n",
      "Epoch [5/5], Step [74100/136675], Loss: 5.2001\n",
      "Epoch [5/5], Step [74175/136675], Loss: 5.5183\n",
      "Epoch [5/5], Step [74250/136675], Loss: 5.3114\n",
      "Epoch [5/5], Step [74325/136675], Loss: 5.0934\n",
      "Epoch [5/5], Step [74400/136675], Loss: 5.4225\n",
      "Epoch [5/5], Step [74475/136675], Loss: 5.6256\n",
      "Epoch [5/5], Step [74550/136675], Loss: 5.3783\n",
      "Epoch [5/5], Step [74625/136675], Loss: 5.4139\n",
      "Epoch [5/5], Step [74700/136675], Loss: 5.3408\n",
      "Epoch [5/5], Step [74775/136675], Loss: 5.0357\n",
      "Epoch [5/5], Step [74850/136675], Loss: 5.2226\n",
      "Epoch [5/5], Step [74925/136675], Loss: 5.2627\n",
      "Epoch [5/5], Step [75000/136675], Loss: 5.3521\n",
      "Epoch [5/5], Step [75075/136675], Loss: 5.1873\n",
      "Epoch [5/5], Step [75150/136675], Loss: 5.5732\n",
      "Epoch [5/5], Step [75225/136675], Loss: 5.3984\n",
      "Epoch [5/5], Step [75300/136675], Loss: 4.8660\n",
      "Epoch [5/5], Step [75375/136675], Loss: 5.3051\n",
      "Epoch [5/5], Step [75450/136675], Loss: 5.1918\n",
      "Epoch [5/5], Step [75525/136675], Loss: 5.4675\n",
      "Epoch [5/5], Step [75600/136675], Loss: 5.1782\n",
      "Epoch [5/5], Step [75675/136675], Loss: 5.0762\n",
      "Epoch [5/5], Step [75750/136675], Loss: 5.3588\n",
      "Epoch [5/5], Step [75825/136675], Loss: 5.4531\n",
      "Epoch [5/5], Step [75900/136675], Loss: 5.0140\n",
      "Epoch [5/5], Step [75975/136675], Loss: 5.1152\n",
      "Epoch [5/5], Step [76050/136675], Loss: 5.2396\n",
      "Epoch [5/5], Step [76125/136675], Loss: 5.3846\n",
      "Epoch [5/5], Step [76200/136675], Loss: 5.3451\n",
      "Epoch [5/5], Step [76275/136675], Loss: 5.4623\n",
      "Epoch [5/5], Step [76350/136675], Loss: 5.4464\n",
      "Epoch [5/5], Step [76425/136675], Loss: 5.3269\n",
      "Epoch [5/5], Step [76500/136675], Loss: 5.4689\n",
      "Epoch [5/5], Step [76575/136675], Loss: 5.2536\n",
      "Epoch [5/5], Step [76650/136675], Loss: 5.3803\n",
      "Epoch [5/5], Step [76725/136675], Loss: 5.2593\n",
      "Epoch [5/5], Step [76800/136675], Loss: 5.0814\n",
      "Epoch [5/5], Step [76875/136675], Loss: 5.1298\n",
      "Epoch [5/5], Step [76950/136675], Loss: 5.2648\n",
      "Epoch [5/5], Step [77025/136675], Loss: 5.3886\n",
      "Epoch [5/5], Step [77100/136675], Loss: 4.9988\n",
      "Epoch [5/5], Step [77175/136675], Loss: 5.5566\n",
      "Epoch [5/5], Step [77250/136675], Loss: 5.1988\n",
      "Epoch [5/5], Step [77325/136675], Loss: 5.2571\n",
      "Epoch [5/5], Step [77400/136675], Loss: 5.3592\n",
      "Epoch [5/5], Step [77475/136675], Loss: 5.1353\n",
      "Epoch [5/5], Step [77550/136675], Loss: 5.2312\n",
      "Epoch [5/5], Step [77625/136675], Loss: 5.3212\n",
      "Epoch [5/5], Step [77700/136675], Loss: 5.0534\n",
      "Epoch [5/5], Step [77775/136675], Loss: 5.1522\n",
      "Epoch [5/5], Step [77850/136675], Loss: 5.0398\n",
      "Epoch [5/5], Step [77925/136675], Loss: 5.3622\n",
      "Epoch [5/5], Step [78000/136675], Loss: 5.1277\n",
      "Epoch [5/5], Step [78075/136675], Loss: 4.9263\n",
      "Epoch [5/5], Step [78150/136675], Loss: 5.1272\n",
      "Epoch [5/5], Step [78225/136675], Loss: 5.4588\n",
      "Epoch [5/5], Step [78300/136675], Loss: 5.0817\n",
      "Epoch [5/5], Step [78375/136675], Loss: 5.3361\n",
      "Epoch [5/5], Step [78450/136675], Loss: 5.2727\n",
      "Epoch [5/5], Step [78525/136675], Loss: 5.2404\n",
      "Epoch [5/5], Step [78600/136675], Loss: 5.4022\n",
      "Epoch [5/5], Step [78675/136675], Loss: 5.2020\n",
      "Epoch [5/5], Step [78750/136675], Loss: 5.3104\n",
      "Epoch [5/5], Step [78825/136675], Loss: 5.4003\n",
      "Epoch [5/5], Step [78900/136675], Loss: 5.3444\n",
      "Epoch [5/5], Step [78975/136675], Loss: 5.0772\n",
      "Epoch [5/5], Step [79050/136675], Loss: 5.1011\n",
      "Epoch [5/5], Step [79125/136675], Loss: 5.4216\n",
      "Epoch [5/5], Step [79200/136675], Loss: 5.3124\n",
      "Epoch [5/5], Step [79275/136675], Loss: 5.4016\n",
      "Epoch [5/5], Step [79350/136675], Loss: 5.0737\n",
      "Epoch [5/5], Step [79425/136675], Loss: 5.0691\n",
      "Epoch [5/5], Step [79500/136675], Loss: 5.5653\n",
      "Epoch [5/5], Step [79575/136675], Loss: 5.3350\n",
      "Epoch [5/5], Step [79650/136675], Loss: 5.4429\n",
      "Epoch [5/5], Step [79725/136675], Loss: 5.0038\n",
      "Epoch [5/5], Step [79800/136675], Loss: 4.9953\n",
      "Epoch [5/5], Step [79875/136675], Loss: 5.3557\n",
      "Epoch [5/5], Step [79950/136675], Loss: 5.0845\n",
      "Validation perplexity: 153.70793878028306\n",
      "Epoch [5/5], Step [80025/136675], Loss: 5.3159\n",
      "Epoch [5/5], Step [80100/136675], Loss: 5.4343\n",
      "Epoch [5/5], Step [80175/136675], Loss: 5.2668\n",
      "Epoch [5/5], Step [80250/136675], Loss: 5.2714\n",
      "Epoch [5/5], Step [80325/136675], Loss: 5.2575\n",
      "Epoch [5/5], Step [80400/136675], Loss: 5.2784\n",
      "Epoch [5/5], Step [80475/136675], Loss: 5.3974\n",
      "Epoch [5/5], Step [80550/136675], Loss: 5.2832\n",
      "Epoch [5/5], Step [80625/136675], Loss: 5.1903\n",
      "Epoch [5/5], Step [80700/136675], Loss: 5.4300\n",
      "Epoch [5/5], Step [80775/136675], Loss: 5.4451\n",
      "Epoch [5/5], Step [80850/136675], Loss: 5.3762\n",
      "Epoch [5/5], Step [80925/136675], Loss: 5.6289\n",
      "Epoch [5/5], Step [81000/136675], Loss: 5.5971\n",
      "Epoch [5/5], Step [81075/136675], Loss: 5.3616\n",
      "Epoch [5/5], Step [81150/136675], Loss: 5.3956\n",
      "Epoch [5/5], Step [81225/136675], Loss: 5.0806\n",
      "Epoch [5/5], Step [81300/136675], Loss: 5.6600\n",
      "Epoch [5/5], Step [81375/136675], Loss: 5.2634\n",
      "Epoch [5/5], Step [81450/136675], Loss: 5.3508\n",
      "Epoch [5/5], Step [81525/136675], Loss: 5.3332\n",
      "Epoch [5/5], Step [81600/136675], Loss: 5.1546\n",
      "Epoch [5/5], Step [81675/136675], Loss: 5.1429\n",
      "Epoch [5/5], Step [81750/136675], Loss: 5.3976\n",
      "Epoch [5/5], Step [81825/136675], Loss: 5.3834\n",
      "Epoch [5/5], Step [81900/136675], Loss: 5.3798\n",
      "Epoch [5/5], Step [81975/136675], Loss: 5.4854\n",
      "Epoch [5/5], Step [82050/136675], Loss: 5.1835\n",
      "Epoch [5/5], Step [82125/136675], Loss: 5.3013\n",
      "Epoch [5/5], Step [82200/136675], Loss: 5.3197\n",
      "Epoch [5/5], Step [82275/136675], Loss: 5.3041\n",
      "Epoch [5/5], Step [82350/136675], Loss: 5.4234\n",
      "Epoch [5/5], Step [82425/136675], Loss: 5.1768\n",
      "Epoch [5/5], Step [82500/136675], Loss: 5.1147\n",
      "Epoch [5/5], Step [82575/136675], Loss: 5.4752\n",
      "Epoch [5/5], Step [82650/136675], Loss: 5.2372\n",
      "Epoch [5/5], Step [82725/136675], Loss: 4.9781\n",
      "Epoch [5/5], Step [82800/136675], Loss: 5.1533\n",
      "Epoch [5/5], Step [82875/136675], Loss: 5.3843\n",
      "Epoch [5/5], Step [82950/136675], Loss: 4.9621\n",
      "Epoch [5/5], Step [83025/136675], Loss: 4.9525\n",
      "Epoch [5/5], Step [83100/136675], Loss: 5.1642\n",
      "Epoch [5/5], Step [83175/136675], Loss: 5.4672\n",
      "Epoch [5/5], Step [83250/136675], Loss: 5.3617\n",
      "Epoch [5/5], Step [83325/136675], Loss: 5.1303\n",
      "Epoch [5/5], Step [83400/136675], Loss: 5.1390\n",
      "Epoch [5/5], Step [83475/136675], Loss: 4.9837\n",
      "Epoch [5/5], Step [83550/136675], Loss: 5.3347\n",
      "Epoch [5/5], Step [83625/136675], Loss: 5.2075\n",
      "Epoch [5/5], Step [83700/136675], Loss: 4.8291\n",
      "Epoch [5/5], Step [83775/136675], Loss: 5.6574\n",
      "Epoch [5/5], Step [83850/136675], Loss: 5.0493\n",
      "Epoch [5/5], Step [83925/136675], Loss: 5.1251\n",
      "Epoch [5/5], Step [84000/136675], Loss: 5.0779\n",
      "Epoch [5/5], Step [84075/136675], Loss: 5.4058\n",
      "Epoch [5/5], Step [84150/136675], Loss: 5.3597\n",
      "Epoch [5/5], Step [84225/136675], Loss: 5.3571\n",
      "Epoch [5/5], Step [84300/136675], Loss: 5.0379\n",
      "Epoch [5/5], Step [84375/136675], Loss: 5.4233\n",
      "Epoch [5/5], Step [84450/136675], Loss: 5.2493\n",
      "Epoch [5/5], Step [84525/136675], Loss: 5.5578\n",
      "Epoch [5/5], Step [84600/136675], Loss: 5.4366\n",
      "Epoch [5/5], Step [84675/136675], Loss: 5.0747\n",
      "Epoch [5/5], Step [84750/136675], Loss: 5.3472\n",
      "Epoch [5/5], Step [84825/136675], Loss: 5.2176\n",
      "Epoch [5/5], Step [84900/136675], Loss: 5.2519\n",
      "Epoch [5/5], Step [84975/136675], Loss: 5.4727\n",
      "Epoch [5/5], Step [85050/136675], Loss: 5.3524\n",
      "Epoch [5/5], Step [85125/136675], Loss: 5.6546\n",
      "Epoch [5/5], Step [85200/136675], Loss: 5.3466\n",
      "Epoch [5/5], Step [85275/136675], Loss: 5.3383\n",
      "Epoch [5/5], Step [85350/136675], Loss: 5.0551\n",
      "Epoch [5/5], Step [85425/136675], Loss: 5.0883\n",
      "Epoch [5/5], Step [85500/136675], Loss: 5.4520\n",
      "Epoch [5/5], Step [85575/136675], Loss: 5.1470\n",
      "Epoch [5/5], Step [85650/136675], Loss: 5.0345\n",
      "Epoch [5/5], Step [85725/136675], Loss: 5.1019\n",
      "Epoch [5/5], Step [85800/136675], Loss: 5.1964\n",
      "Epoch [5/5], Step [85875/136675], Loss: 5.1638\n",
      "Epoch [5/5], Step [85950/136675], Loss: 5.3461\n",
      "Epoch [5/5], Step [86025/136675], Loss: 5.2951\n",
      "Epoch [5/5], Step [86100/136675], Loss: 5.3767\n",
      "Epoch [5/5], Step [86175/136675], Loss: 5.3579\n",
      "Epoch [5/5], Step [86250/136675], Loss: 5.2741\n",
      "Epoch [5/5], Step [86325/136675], Loss: 4.7908\n",
      "Epoch [5/5], Step [86400/136675], Loss: 5.3599\n",
      "Epoch [5/5], Step [86475/136675], Loss: 5.5605\n",
      "Epoch [5/5], Step [86550/136675], Loss: 5.3755\n",
      "Epoch [5/5], Step [86625/136675], Loss: 5.2871\n",
      "Epoch [5/5], Step [86700/136675], Loss: 4.9884\n",
      "Epoch [5/5], Step [86775/136675], Loss: 5.3064\n",
      "Epoch [5/5], Step [86850/136675], Loss: 5.5118\n",
      "Epoch [5/5], Step [86925/136675], Loss: 5.2614\n",
      "Epoch [5/5], Step [87000/136675], Loss: 5.1452\n",
      "Epoch [5/5], Step [87075/136675], Loss: 5.1973\n",
      "Epoch [5/5], Step [87150/136675], Loss: 5.2048\n",
      "Epoch [5/5], Step [87225/136675], Loss: 5.2390\n",
      "Epoch [5/5], Step [87300/136675], Loss: 5.2999\n",
      "Epoch [5/5], Step [87375/136675], Loss: 5.1581\n",
      "Epoch [5/5], Step [87450/136675], Loss: 5.6611\n",
      "Epoch [5/5], Step [87525/136675], Loss: 5.5639\n",
      "Epoch [5/5], Step [87600/136675], Loss: 5.3098\n",
      "Epoch [5/5], Step [87675/136675], Loss: 5.2297\n",
      "Epoch [5/5], Step [87750/136675], Loss: 5.3968\n",
      "Epoch [5/5], Step [87825/136675], Loss: 5.2671\n",
      "Epoch [5/5], Step [87900/136675], Loss: 5.5708\n",
      "Epoch [5/5], Step [87975/136675], Loss: 5.2312\n",
      "Epoch [5/5], Step [88050/136675], Loss: 5.2982\n",
      "Epoch [5/5], Step [88125/136675], Loss: 5.3574\n",
      "Epoch [5/5], Step [88200/136675], Loss: 5.3770\n",
      "Epoch [5/5], Step [88275/136675], Loss: 5.4560\n",
      "Epoch [5/5], Step [88350/136675], Loss: 5.1871\n",
      "Epoch [5/5], Step [88425/136675], Loss: 5.0271\n",
      "Epoch [5/5], Step [88500/136675], Loss: 5.3092\n",
      "Epoch [5/5], Step [88575/136675], Loss: 4.9410\n",
      "Epoch [5/5], Step [88650/136675], Loss: 5.1251\n",
      "Epoch [5/5], Step [88725/136675], Loss: 4.9739\n",
      "Epoch [5/5], Step [88800/136675], Loss: 5.2277\n",
      "Epoch [5/5], Step [88875/136675], Loss: 5.5176\n",
      "Epoch [5/5], Step [88950/136675], Loss: 5.1796\n",
      "Epoch [5/5], Step [89025/136675], Loss: 5.2174\n",
      "Epoch [5/5], Step [89100/136675], Loss: 5.2752\n",
      "Epoch [5/5], Step [89175/136675], Loss: 5.1023\n",
      "Epoch [5/5], Step [89250/136675], Loss: 5.2827\n",
      "Epoch [5/5], Step [89325/136675], Loss: 5.0528\n",
      "Epoch [5/5], Step [89400/136675], Loss: 5.4646\n",
      "Epoch [5/5], Step [89475/136675], Loss: 5.4012\n",
      "Epoch [5/5], Step [89550/136675], Loss: 5.4696\n",
      "Epoch [5/5], Step [89625/136675], Loss: 4.8354\n",
      "Epoch [5/5], Step [89700/136675], Loss: 5.2545\n",
      "Epoch [5/5], Step [89775/136675], Loss: 5.2960\n",
      "Epoch [5/5], Step [89850/136675], Loss: 5.6196\n",
      "Epoch [5/5], Step [89925/136675], Loss: 5.3195\n",
      "Epoch [5/5], Step [90000/136675], Loss: 5.2431\n",
      "Validation perplexity: 154.82729024596944\n",
      "Epoch [5/5], Step [90075/136675], Loss: 5.2475\n",
      "Epoch [5/5], Step [90150/136675], Loss: 4.9463\n",
      "Epoch [5/5], Step [90225/136675], Loss: 5.3922\n",
      "Epoch [5/5], Step [90300/136675], Loss: 5.0671\n",
      "Epoch [5/5], Step [90375/136675], Loss: 5.3938\n",
      "Epoch [5/5], Step [90450/136675], Loss: 5.0014\n",
      "Epoch [5/5], Step [90525/136675], Loss: 5.1990\n",
      "Epoch [5/5], Step [90600/136675], Loss: 5.0886\n",
      "Epoch [5/5], Step [90675/136675], Loss: 5.5596\n",
      "Epoch [5/5], Step [90750/136675], Loss: 5.1898\n",
      "Epoch [5/5], Step [90825/136675], Loss: 5.1834\n",
      "Epoch [5/5], Step [90900/136675], Loss: 5.3999\n",
      "Epoch [5/5], Step [90975/136675], Loss: 5.2643\n",
      "Epoch [5/5], Step [91050/136675], Loss: 5.0455\n",
      "Epoch [5/5], Step [91125/136675], Loss: 5.5008\n",
      "Epoch [5/5], Step [91200/136675], Loss: 5.4092\n",
      "Epoch [5/5], Step [91275/136675], Loss: 5.2364\n",
      "Epoch [5/5], Step [91350/136675], Loss: 5.0211\n",
      "Epoch [5/5], Step [91425/136675], Loss: 5.3361\n",
      "Epoch [5/5], Step [91500/136675], Loss: 5.0808\n",
      "Epoch [5/5], Step [91575/136675], Loss: 5.1170\n",
      "Epoch [5/5], Step [91650/136675], Loss: 5.0539\n",
      "Epoch [5/5], Step [91725/136675], Loss: 5.2960\n",
      "Epoch [5/5], Step [91800/136675], Loss: 5.2800\n",
      "Epoch [5/5], Step [91875/136675], Loss: 5.2384\n",
      "Epoch [5/5], Step [91950/136675], Loss: 5.4273\n",
      "Epoch [5/5], Step [92025/136675], Loss: 5.1115\n",
      "Epoch [5/5], Step [92100/136675], Loss: 5.3763\n",
      "Epoch [5/5], Step [92175/136675], Loss: 5.0699\n",
      "Epoch [5/5], Step [92250/136675], Loss: 5.2219\n",
      "Epoch [5/5], Step [92325/136675], Loss: 5.3157\n",
      "Epoch [5/5], Step [92400/136675], Loss: 5.3132\n",
      "Epoch [5/5], Step [92475/136675], Loss: 4.9253\n",
      "Epoch [5/5], Step [92550/136675], Loss: 5.1007\n",
      "Epoch [5/5], Step [92625/136675], Loss: 5.4257\n",
      "Epoch [5/5], Step [92700/136675], Loss: 4.9595\n",
      "Epoch [5/5], Step [92775/136675], Loss: 5.2222\n",
      "Epoch [5/5], Step [92850/136675], Loss: 5.3701\n",
      "Epoch [5/5], Step [92925/136675], Loss: 5.5030\n",
      "Epoch [5/5], Step [93000/136675], Loss: 5.5096\n",
      "Epoch [5/5], Step [93075/136675], Loss: 5.5687\n",
      "Epoch [5/5], Step [93150/136675], Loss: 5.3763\n",
      "Epoch [5/5], Step [93225/136675], Loss: 5.2729\n",
      "Epoch [5/5], Step [93300/136675], Loss: 5.1163\n",
      "Epoch [5/5], Step [93375/136675], Loss: 5.4329\n",
      "Epoch [5/5], Step [93450/136675], Loss: 5.1460\n",
      "Epoch [5/5], Step [93525/136675], Loss: 5.6261\n",
      "Epoch [5/5], Step [93600/136675], Loss: 5.3400\n",
      "Epoch [5/5], Step [93675/136675], Loss: 5.2870\n",
      "Epoch [5/5], Step [93750/136675], Loss: 5.4525\n",
      "Epoch [5/5], Step [93825/136675], Loss: 5.2949\n",
      "Epoch [5/5], Step [93900/136675], Loss: 5.0597\n",
      "Epoch [5/5], Step [93975/136675], Loss: 5.1505\n",
      "Epoch [5/5], Step [94050/136675], Loss: 5.1648\n",
      "Epoch [5/5], Step [94125/136675], Loss: 5.3610\n",
      "Epoch [5/5], Step [94200/136675], Loss: 5.5460\n",
      "Epoch [5/5], Step [94275/136675], Loss: 5.4768\n",
      "Epoch [5/5], Step [94350/136675], Loss: 4.9898\n",
      "Epoch [5/5], Step [94425/136675], Loss: 5.3152\n",
      "Epoch [5/5], Step [94500/136675], Loss: 5.3207\n",
      "Epoch [5/5], Step [94575/136675], Loss: 5.0154\n",
      "Epoch [5/5], Step [94650/136675], Loss: 5.4501\n",
      "Epoch [5/5], Step [94725/136675], Loss: 5.0711\n",
      "Epoch [5/5], Step [94800/136675], Loss: 5.1699\n",
      "Epoch [5/5], Step [94875/136675], Loss: 5.2671\n",
      "Epoch [5/5], Step [94950/136675], Loss: 5.1581\n",
      "Epoch [5/5], Step [95025/136675], Loss: 5.4381\n",
      "Epoch [5/5], Step [95100/136675], Loss: 5.0478\n",
      "Epoch [5/5], Step [95175/136675], Loss: 5.0937\n",
      "Epoch [5/5], Step [95250/136675], Loss: 5.1701\n",
      "Epoch [5/5], Step [95325/136675], Loss: 5.3190\n",
      "Epoch [5/5], Step [95400/136675], Loss: 5.2929\n",
      "Epoch [5/5], Step [95475/136675], Loss: 5.6216\n",
      "Epoch [5/5], Step [95550/136675], Loss: 5.3772\n",
      "Epoch [5/5], Step [95625/136675], Loss: 5.2006\n",
      "Epoch [5/5], Step [95700/136675], Loss: 5.5939\n",
      "Epoch [5/5], Step [95775/136675], Loss: 5.5368\n",
      "Epoch [5/5], Step [95850/136675], Loss: 5.0736\n",
      "Epoch [5/5], Step [95925/136675], Loss: 5.2222\n",
      "Epoch [5/5], Step [96000/136675], Loss: 5.2991\n",
      "Epoch [5/5], Step [96075/136675], Loss: 5.3758\n",
      "Epoch [5/5], Step [96150/136675], Loss: 5.2451\n",
      "Epoch [5/5], Step [96225/136675], Loss: 5.2541\n",
      "Epoch [5/5], Step [96300/136675], Loss: 5.3481\n",
      "Epoch [5/5], Step [96375/136675], Loss: 5.4245\n",
      "Epoch [5/5], Step [96450/136675], Loss: 5.0416\n",
      "Epoch [5/5], Step [96525/136675], Loss: 5.3405\n",
      "Epoch [5/5], Step [96600/136675], Loss: 5.1053\n",
      "Epoch [5/5], Step [96675/136675], Loss: 5.3247\n",
      "Epoch [5/5], Step [96750/136675], Loss: 5.3013\n",
      "Epoch [5/5], Step [96825/136675], Loss: 5.2159\n",
      "Epoch [5/5], Step [96900/136675], Loss: 5.3094\n",
      "Epoch [5/5], Step [96975/136675], Loss: 5.4026\n",
      "Epoch [5/5], Step [97050/136675], Loss: 5.4381\n",
      "Epoch [5/5], Step [97125/136675], Loss: 5.1690\n",
      "Epoch [5/5], Step [97200/136675], Loss: 5.4327\n",
      "Epoch [5/5], Step [97275/136675], Loss: 5.2565\n",
      "Epoch [5/5], Step [97350/136675], Loss: 5.2823\n",
      "Epoch [5/5], Step [97425/136675], Loss: 5.1656\n",
      "Epoch [5/5], Step [97500/136675], Loss: 5.3322\n",
      "Epoch [5/5], Step [97575/136675], Loss: 5.3811\n",
      "Epoch [5/5], Step [97650/136675], Loss: 5.2716\n",
      "Epoch [5/5], Step [97725/136675], Loss: 5.4425\n",
      "Epoch [5/5], Step [97800/136675], Loss: 5.4991\n",
      "Epoch [5/5], Step [97875/136675], Loss: 5.2762\n",
      "Epoch [5/5], Step [97950/136675], Loss: 5.1543\n",
      "Epoch [5/5], Step [98025/136675], Loss: 5.0860\n",
      "Epoch [5/5], Step [98100/136675], Loss: 5.1952\n",
      "Epoch [5/5], Step [98175/136675], Loss: 5.3760\n",
      "Epoch [5/5], Step [98250/136675], Loss: 4.9496\n",
      "Epoch [5/5], Step [98325/136675], Loss: 5.2339\n",
      "Epoch [5/5], Step [98400/136675], Loss: 5.3654\n",
      "Epoch [5/5], Step [98475/136675], Loss: 5.3083\n",
      "Epoch [5/5], Step [98550/136675], Loss: 5.2495\n",
      "Epoch [5/5], Step [98625/136675], Loss: 5.3242\n",
      "Epoch [5/5], Step [98700/136675], Loss: 5.4851\n",
      "Epoch [5/5], Step [98775/136675], Loss: 5.4848\n",
      "Epoch [5/5], Step [98850/136675], Loss: 4.9922\n",
      "Epoch [5/5], Step [98925/136675], Loss: 4.9425\n",
      "Epoch [5/5], Step [99000/136675], Loss: 5.2188\n",
      "Epoch [5/5], Step [99075/136675], Loss: 5.3237\n",
      "Epoch [5/5], Step [99150/136675], Loss: 5.0894\n",
      "Epoch [5/5], Step [99225/136675], Loss: 5.3087\n",
      "Epoch [5/5], Step [99300/136675], Loss: 5.4608\n",
      "Epoch [5/5], Step [99375/136675], Loss: 5.2977\n",
      "Epoch [5/5], Step [99450/136675], Loss: 5.0449\n",
      "Epoch [5/5], Step [99525/136675], Loss: 5.3347\n",
      "Epoch [5/5], Step [99600/136675], Loss: 5.6877\n",
      "Epoch [5/5], Step [99675/136675], Loss: 5.1852\n",
      "Epoch [5/5], Step [99750/136675], Loss: 5.2978\n",
      "Epoch [5/5], Step [99825/136675], Loss: 5.3187\n",
      "Epoch [5/5], Step [99900/136675], Loss: 5.1223\n",
      "Epoch [5/5], Step [99975/136675], Loss: 5.1422\n",
      "Validation perplexity: 154.6111922331841\n",
      "Epoch [5/5], Step [100050/136675], Loss: 5.4158\n",
      "Epoch [5/5], Step [100125/136675], Loss: 5.0845\n",
      "Epoch [5/5], Step [100200/136675], Loss: 5.2713\n",
      "Epoch [5/5], Step [100275/136675], Loss: 5.5146\n",
      "Epoch [5/5], Step [100350/136675], Loss: 5.3117\n",
      "Epoch [5/5], Step [100425/136675], Loss: 5.4799\n",
      "Epoch [5/5], Step [100500/136675], Loss: 5.3893\n",
      "Epoch [5/5], Step [100575/136675], Loss: 5.2450\n",
      "Epoch [5/5], Step [100650/136675], Loss: 5.5914\n",
      "Epoch [5/5], Step [100725/136675], Loss: 5.1902\n",
      "Epoch [5/5], Step [100800/136675], Loss: 5.4790\n",
      "Epoch [5/5], Step [100875/136675], Loss: 5.0381\n",
      "Epoch [5/5], Step [100950/136675], Loss: 5.5825\n",
      "Epoch [5/5], Step [101025/136675], Loss: 5.4412\n",
      "Epoch [5/5], Step [101100/136675], Loss: 5.2085\n",
      "Epoch [5/5], Step [101175/136675], Loss: 5.3232\n",
      "Epoch [5/5], Step [101250/136675], Loss: 5.2852\n",
      "Epoch [5/5], Step [101325/136675], Loss: 5.4470\n",
      "Epoch [5/5], Step [101400/136675], Loss: 5.2468\n",
      "Epoch [5/5], Step [101475/136675], Loss: 5.0856\n",
      "Epoch [5/5], Step [101550/136675], Loss: 5.0645\n",
      "Epoch [5/5], Step [101625/136675], Loss: 5.1708\n",
      "Epoch [5/5], Step [101700/136675], Loss: 5.4079\n",
      "Epoch [5/5], Step [101775/136675], Loss: 5.3972\n",
      "Epoch [5/5], Step [101850/136675], Loss: 5.0195\n",
      "Epoch [5/5], Step [101925/136675], Loss: 5.4503\n",
      "Epoch [5/5], Step [102000/136675], Loss: 5.2816\n",
      "Epoch [5/5], Step [102075/136675], Loss: 5.1653\n",
      "Epoch [5/5], Step [102150/136675], Loss: 5.2386\n",
      "Epoch [5/5], Step [102225/136675], Loss: 5.0135\n",
      "Epoch [5/5], Step [102300/136675], Loss: 5.4947\n",
      "Epoch [5/5], Step [102375/136675], Loss: 5.4475\n",
      "Epoch [5/5], Step [102450/136675], Loss: 5.3205\n",
      "Epoch [5/5], Step [102525/136675], Loss: 5.2699\n",
      "Epoch [5/5], Step [102600/136675], Loss: 5.4202\n",
      "Epoch [5/5], Step [102675/136675], Loss: 5.2603\n",
      "Epoch [5/5], Step [102750/136675], Loss: 5.2986\n",
      "Epoch [5/5], Step [102825/136675], Loss: 5.1123\n",
      "Epoch [5/5], Step [102900/136675], Loss: 5.1529\n",
      "Epoch [5/5], Step [102975/136675], Loss: 5.4969\n",
      "Epoch [5/5], Step [103050/136675], Loss: 5.2001\n",
      "Epoch [5/5], Step [103125/136675], Loss: 5.1539\n",
      "Epoch [5/5], Step [103200/136675], Loss: 5.6130\n",
      "Epoch [5/5], Step [103275/136675], Loss: 5.3180\n",
      "Epoch [5/5], Step [103350/136675], Loss: 5.2381\n",
      "Epoch [5/5], Step [103425/136675], Loss: 5.4037\n",
      "Epoch [5/5], Step [103500/136675], Loss: 5.2473\n",
      "Epoch [5/5], Step [103575/136675], Loss: 5.4126\n",
      "Epoch [5/5], Step [103650/136675], Loss: 4.9825\n",
      "Epoch [5/5], Step [103725/136675], Loss: 5.3076\n",
      "Epoch [5/5], Step [103800/136675], Loss: 5.2502\n",
      "Epoch [5/5], Step [103875/136675], Loss: 5.3688\n",
      "Epoch [5/5], Step [103950/136675], Loss: 5.0318\n",
      "Epoch [5/5], Step [104025/136675], Loss: 5.3326\n",
      "Epoch [5/5], Step [104100/136675], Loss: 5.2962\n",
      "Epoch [5/5], Step [104175/136675], Loss: 5.2947\n",
      "Epoch [5/5], Step [104250/136675], Loss: 5.0029\n",
      "Epoch [5/5], Step [104325/136675], Loss: 5.0557\n",
      "Epoch [5/5], Step [104400/136675], Loss: 5.3633\n",
      "Epoch [5/5], Step [104475/136675], Loss: 5.2046\n",
      "Epoch [5/5], Step [104550/136675], Loss: 5.1576\n",
      "Epoch [5/5], Step [104625/136675], Loss: 5.2262\n",
      "Epoch [5/5], Step [104700/136675], Loss: 5.3099\n",
      "Epoch [5/5], Step [104775/136675], Loss: 4.9566\n",
      "Epoch [5/5], Step [104850/136675], Loss: 4.9921\n",
      "Epoch [5/5], Step [104925/136675], Loss: 5.3282\n",
      "Epoch [5/5], Step [105000/136675], Loss: 5.0812\n",
      "Epoch [5/5], Step [105075/136675], Loss: 5.5555\n",
      "Epoch [5/5], Step [105150/136675], Loss: 5.3366\n",
      "Epoch [5/5], Step [105225/136675], Loss: 5.4593\n",
      "Epoch [5/5], Step [105300/136675], Loss: 5.1677\n",
      "Epoch [5/5], Step [105375/136675], Loss: 4.9779\n",
      "Epoch [5/5], Step [105450/136675], Loss: 4.9533\n",
      "Epoch [5/5], Step [105525/136675], Loss: 5.0431\n",
      "Epoch [5/5], Step [105600/136675], Loss: 5.0689\n",
      "Epoch [5/5], Step [105675/136675], Loss: 5.2915\n",
      "Epoch [5/5], Step [105750/136675], Loss: 5.0715\n",
      "Epoch [5/5], Step [105825/136675], Loss: 5.2692\n",
      "Epoch [5/5], Step [105900/136675], Loss: 5.4489\n",
      "Epoch [5/5], Step [105975/136675], Loss: 5.1388\n",
      "Epoch [5/5], Step [106050/136675], Loss: 5.2620\n",
      "Epoch [5/5], Step [106125/136675], Loss: 5.4412\n",
      "Epoch [5/5], Step [106200/136675], Loss: 5.1158\n",
      "Epoch [5/5], Step [106275/136675], Loss: 5.2071\n",
      "Epoch [5/5], Step [106350/136675], Loss: 5.1907\n",
      "Epoch [5/5], Step [106425/136675], Loss: 5.1049\n",
      "Epoch [5/5], Step [106500/136675], Loss: 5.2664\n",
      "Epoch [5/5], Step [106575/136675], Loss: 5.0363\n",
      "Epoch [5/5], Step [106650/136675], Loss: 5.1616\n",
      "Epoch [5/5], Step [106725/136675], Loss: 5.1600\n",
      "Epoch [5/5], Step [106800/136675], Loss: 5.2807\n",
      "Epoch [5/5], Step [106875/136675], Loss: 5.0195\n",
      "Epoch [5/5], Step [106950/136675], Loss: 5.3252\n",
      "Epoch [5/5], Step [107025/136675], Loss: 5.3296\n",
      "Epoch [5/5], Step [107100/136675], Loss: 5.3187\n",
      "Epoch [5/5], Step [107175/136675], Loss: 5.3286\n",
      "Epoch [5/5], Step [107250/136675], Loss: 5.3346\n",
      "Epoch [5/5], Step [107325/136675], Loss: 5.2661\n",
      "Epoch [5/5], Step [107400/136675], Loss: 5.1538\n",
      "Epoch [5/5], Step [107475/136675], Loss: 5.4077\n",
      "Epoch [5/5], Step [107550/136675], Loss: 5.3415\n",
      "Epoch [5/5], Step [107625/136675], Loss: 5.1501\n",
      "Epoch [5/5], Step [107700/136675], Loss: 5.3156\n",
      "Epoch [5/5], Step [107775/136675], Loss: 5.3141\n",
      "Epoch [5/5], Step [107850/136675], Loss: 5.2524\n",
      "Epoch [5/5], Step [107925/136675], Loss: 5.3144\n",
      "Epoch [5/5], Step [108000/136675], Loss: 5.4173\n",
      "Epoch [5/5], Step [108075/136675], Loss: 5.5227\n",
      "Epoch [5/5], Step [108150/136675], Loss: 5.3079\n",
      "Epoch [5/5], Step [108225/136675], Loss: 5.5388\n",
      "Epoch [5/5], Step [108300/136675], Loss: 5.4139\n",
      "Epoch [5/5], Step [108375/136675], Loss: 5.1542\n",
      "Epoch [5/5], Step [108450/136675], Loss: 5.1515\n",
      "Epoch [5/5], Step [108525/136675], Loss: 5.2842\n",
      "Epoch [5/5], Step [108600/136675], Loss: 5.3486\n",
      "Epoch [5/5], Step [108675/136675], Loss: 5.4244\n",
      "Epoch [5/5], Step [108750/136675], Loss: 5.1396\n",
      "Epoch [5/5], Step [108825/136675], Loss: 5.3913\n",
      "Epoch [5/5], Step [108900/136675], Loss: 5.3040\n",
      "Epoch [5/5], Step [108975/136675], Loss: 5.3301\n",
      "Epoch [5/5], Step [109050/136675], Loss: 5.4220\n",
      "Epoch [5/5], Step [109125/136675], Loss: 5.1969\n",
      "Epoch [5/5], Step [109200/136675], Loss: 5.3659\n",
      "Epoch [5/5], Step [109275/136675], Loss: 5.1405\n",
      "Epoch [5/5], Step [109350/136675], Loss: 5.4605\n",
      "Epoch [5/5], Step [109425/136675], Loss: 5.5281\n",
      "Epoch [5/5], Step [109500/136675], Loss: 5.4306\n",
      "Epoch [5/5], Step [109575/136675], Loss: 5.4721\n",
      "Epoch [5/5], Step [109650/136675], Loss: 4.9329\n",
      "Epoch [5/5], Step [109725/136675], Loss: 5.5739\n",
      "Epoch [5/5], Step [109800/136675], Loss: 5.4287\n",
      "Epoch [5/5], Step [109875/136675], Loss: 5.6783\n",
      "Epoch [5/5], Step [109950/136675], Loss: 5.3303\n",
      "Validation perplexity: 154.4111099913749\n",
      "Epoch [5/5], Step [110025/136675], Loss: 5.2139\n",
      "Epoch [5/5], Step [110100/136675], Loss: 4.9573\n",
      "Epoch [5/5], Step [110175/136675], Loss: 5.4772\n",
      "Epoch [5/5], Step [110250/136675], Loss: 5.2855\n",
      "Epoch [5/5], Step [110325/136675], Loss: 5.3901\n",
      "Epoch [5/5], Step [110400/136675], Loss: 5.1426\n",
      "Epoch [5/5], Step [110475/136675], Loss: 5.3685\n",
      "Epoch [5/5], Step [110550/136675], Loss: 5.2094\n",
      "Epoch [5/5], Step [110625/136675], Loss: 5.4707\n",
      "Epoch [5/5], Step [110700/136675], Loss: 5.3411\n",
      "Epoch [5/5], Step [110775/136675], Loss: 5.2011\n",
      "Epoch [5/5], Step [110850/136675], Loss: 5.0034\n",
      "Epoch [5/5], Step [110925/136675], Loss: 5.5726\n",
      "Epoch [5/5], Step [111000/136675], Loss: 5.1031\n",
      "Epoch [5/5], Step [111075/136675], Loss: 5.6816\n",
      "Epoch [5/5], Step [111150/136675], Loss: 5.3881\n",
      "Epoch [5/5], Step [111225/136675], Loss: 5.2226\n",
      "Epoch [5/5], Step [111300/136675], Loss: 5.2756\n",
      "Epoch [5/5], Step [111375/136675], Loss: 5.3793\n",
      "Epoch [5/5], Step [111450/136675], Loss: 5.3818\n",
      "Epoch [5/5], Step [111525/136675], Loss: 5.2618\n",
      "Epoch [5/5], Step [111600/136675], Loss: 5.1519\n",
      "Epoch [5/5], Step [111675/136675], Loss: 5.1499\n",
      "Epoch [5/5], Step [111750/136675], Loss: 5.1904\n",
      "Epoch [5/5], Step [111825/136675], Loss: 5.5285\n",
      "Epoch [5/5], Step [111900/136675], Loss: 4.8261\n",
      "Epoch [5/5], Step [111975/136675], Loss: 5.1438\n",
      "Epoch [5/5], Step [112050/136675], Loss: 5.2042\n",
      "Epoch [5/5], Step [112125/136675], Loss: 5.1352\n",
      "Epoch [5/5], Step [112200/136675], Loss: 5.2631\n",
      "Epoch [5/5], Step [112275/136675], Loss: 4.8707\n",
      "Epoch [5/5], Step [112350/136675], Loss: 5.3477\n",
      "Epoch [5/5], Step [112425/136675], Loss: 4.9910\n",
      "Epoch [5/5], Step [112500/136675], Loss: 5.2263\n",
      "Epoch [5/5], Step [112575/136675], Loss: 5.3445\n",
      "Epoch [5/5], Step [112650/136675], Loss: 5.2681\n",
      "Epoch [5/5], Step [112725/136675], Loss: 5.5921\n",
      "Epoch [5/5], Step [112800/136675], Loss: 4.9898\n",
      "Epoch [5/5], Step [112875/136675], Loss: 5.4293\n",
      "Epoch [5/5], Step [112950/136675], Loss: 5.2343\n",
      "Epoch [5/5], Step [113025/136675], Loss: 5.5497\n",
      "Epoch [5/5], Step [113100/136675], Loss: 5.4904\n",
      "Epoch [5/5], Step [113175/136675], Loss: 5.4157\n",
      "Epoch [5/5], Step [113250/136675], Loss: 5.2179\n",
      "Epoch [5/5], Step [113325/136675], Loss: 5.5180\n",
      "Epoch [5/5], Step [113400/136675], Loss: 5.0654\n",
      "Epoch [5/5], Step [113475/136675], Loss: 5.2471\n",
      "Epoch [5/5], Step [113550/136675], Loss: 5.3329\n",
      "Epoch [5/5], Step [113625/136675], Loss: 5.2635\n",
      "Epoch [5/5], Step [113700/136675], Loss: 5.3751\n",
      "Epoch [5/5], Step [113775/136675], Loss: 5.3567\n",
      "Epoch [5/5], Step [113850/136675], Loss: 5.1976\n",
      "Epoch [5/5], Step [113925/136675], Loss: 5.3815\n",
      "Epoch [5/5], Step [114000/136675], Loss: 5.5921\n",
      "Epoch [5/5], Step [114075/136675], Loss: 5.3048\n",
      "Epoch [5/5], Step [114150/136675], Loss: 5.5434\n",
      "Epoch [5/5], Step [114225/136675], Loss: 5.3879\n",
      "Epoch [5/5], Step [114300/136675], Loss: 5.2436\n",
      "Epoch [5/5], Step [114375/136675], Loss: 5.0791\n",
      "Epoch [5/5], Step [114450/136675], Loss: 5.3417\n",
      "Epoch [5/5], Step [114525/136675], Loss: 5.5056\n",
      "Epoch [5/5], Step [114600/136675], Loss: 5.2305\n",
      "Epoch [5/5], Step [114675/136675], Loss: 5.1903\n",
      "Epoch [5/5], Step [114750/136675], Loss: 5.0939\n",
      "Epoch [5/5], Step [114825/136675], Loss: 5.2857\n",
      "Epoch [5/5], Step [114900/136675], Loss: 5.5813\n",
      "Epoch [5/5], Step [114975/136675], Loss: 5.2628\n",
      "Epoch [5/5], Step [115050/136675], Loss: 5.5346\n",
      "Epoch [5/5], Step [115125/136675], Loss: 5.2526\n",
      "Epoch [5/5], Step [115200/136675], Loss: 5.1602\n",
      "Epoch [5/5], Step [115275/136675], Loss: 5.1915\n",
      "Epoch [5/5], Step [115350/136675], Loss: 4.8652\n",
      "Epoch [5/5], Step [115425/136675], Loss: 5.2638\n",
      "Epoch [5/5], Step [115500/136675], Loss: 5.2199\n",
      "Epoch [5/5], Step [115575/136675], Loss: 5.4690\n",
      "Epoch [5/5], Step [115650/136675], Loss: 5.2368\n",
      "Epoch [5/5], Step [115725/136675], Loss: 5.2427\n",
      "Epoch [5/5], Step [115800/136675], Loss: 5.2861\n",
      "Epoch [5/5], Step [115875/136675], Loss: 4.9385\n",
      "Epoch [5/5], Step [115950/136675], Loss: 5.3072\n",
      "Epoch [5/5], Step [116025/136675], Loss: 5.4213\n",
      "Epoch [5/5], Step [116100/136675], Loss: 5.4504\n",
      "Epoch [5/5], Step [116175/136675], Loss: 5.3647\n",
      "Epoch [5/5], Step [116250/136675], Loss: 5.3283\n",
      "Epoch [5/5], Step [116325/136675], Loss: 5.0720\n",
      "Epoch [5/5], Step [116400/136675], Loss: 5.4826\n",
      "Epoch [5/5], Step [116475/136675], Loss: 5.1471\n",
      "Epoch [5/5], Step [116550/136675], Loss: 4.9765\n",
      "Epoch [5/5], Step [116625/136675], Loss: 5.2670\n",
      "Epoch [5/5], Step [116700/136675], Loss: 5.1494\n",
      "Epoch [5/5], Step [116775/136675], Loss: 5.1533\n",
      "Epoch [5/5], Step [116850/136675], Loss: 5.3660\n",
      "Epoch [5/5], Step [116925/136675], Loss: 5.3870\n",
      "Epoch [5/5], Step [117000/136675], Loss: 5.0686\n",
      "Epoch [5/5], Step [117075/136675], Loss: 5.3529\n",
      "Epoch [5/5], Step [117150/136675], Loss: 5.5341\n",
      "Epoch [5/5], Step [117225/136675], Loss: 5.6345\n",
      "Epoch [5/5], Step [117300/136675], Loss: 5.2068\n",
      "Epoch [5/5], Step [117375/136675], Loss: 5.1102\n",
      "Epoch [5/5], Step [117450/136675], Loss: 5.4354\n",
      "Epoch [5/5], Step [117525/136675], Loss: 5.5492\n",
      "Epoch [5/5], Step [117600/136675], Loss: 5.2342\n",
      "Epoch [5/5], Step [117675/136675], Loss: 5.3156\n",
      "Epoch [5/5], Step [117750/136675], Loss: 5.0707\n",
      "Epoch [5/5], Step [117825/136675], Loss: 5.2186\n",
      "Epoch [5/5], Step [117900/136675], Loss: 5.0604\n",
      "Epoch [5/5], Step [117975/136675], Loss: 5.5632\n",
      "Epoch [5/5], Step [118050/136675], Loss: 5.4630\n",
      "Epoch [5/5], Step [118125/136675], Loss: 4.8663\n",
      "Epoch [5/5], Step [118200/136675], Loss: 5.2718\n",
      "Epoch [5/5], Step [118275/136675], Loss: 5.1431\n",
      "Epoch [5/5], Step [118350/136675], Loss: 5.3172\n",
      "Epoch [5/5], Step [118425/136675], Loss: 5.2461\n",
      "Epoch [5/5], Step [118500/136675], Loss: 5.1243\n",
      "Epoch [5/5], Step [118575/136675], Loss: 5.1758\n",
      "Epoch [5/5], Step [118650/136675], Loss: 5.3842\n",
      "Epoch [5/5], Step [118725/136675], Loss: 5.2612\n",
      "Epoch [5/5], Step [118800/136675], Loss: 5.3810\n",
      "Epoch [5/5], Step [118875/136675], Loss: 5.3723\n",
      "Epoch [5/5], Step [118950/136675], Loss: 4.9044\n",
      "Epoch [5/5], Step [119025/136675], Loss: 5.3061\n",
      "Epoch [5/5], Step [119100/136675], Loss: 5.2299\n",
      "Epoch [5/5], Step [119175/136675], Loss: 5.4293\n",
      "Epoch [5/5], Step [119250/136675], Loss: 5.0558\n",
      "Epoch [5/5], Step [119325/136675], Loss: 5.1220\n",
      "Epoch [5/5], Step [119400/136675], Loss: 5.5448\n",
      "Epoch [5/5], Step [119475/136675], Loss: 5.3022\n",
      "Epoch [5/5], Step [119550/136675], Loss: 5.1121\n",
      "Epoch [5/5], Step [119625/136675], Loss: 5.1836\n",
      "Epoch [5/5], Step [119700/136675], Loss: 5.2582\n",
      "Epoch [5/5], Step [119775/136675], Loss: 5.3879\n",
      "Epoch [5/5], Step [119850/136675], Loss: 5.0871\n",
      "Epoch [5/5], Step [119925/136675], Loss: 5.2595\n",
      "Epoch [5/5], Step [120000/136675], Loss: 5.3315\n",
      "Validation perplexity: 153.32846791159193\n",
      "Epoch [5/5], Step [120075/136675], Loss: 5.3954\n",
      "Epoch [5/5], Step [120150/136675], Loss: 5.2834\n",
      "Epoch [5/5], Step [120225/136675], Loss: 5.3279\n",
      "Epoch [5/5], Step [120300/136675], Loss: 5.1805\n",
      "Epoch [5/5], Step [120375/136675], Loss: 5.1683\n",
      "Epoch [5/5], Step [120450/136675], Loss: 5.1375\n",
      "Epoch [5/5], Step [120525/136675], Loss: 5.2691\n",
      "Epoch [5/5], Step [120600/136675], Loss: 5.4539\n",
      "Epoch [5/5], Step [120675/136675], Loss: 5.4216\n",
      "Epoch [5/5], Step [120750/136675], Loss: 5.1388\n",
      "Epoch [5/5], Step [120825/136675], Loss: 5.0351\n",
      "Epoch [5/5], Step [120900/136675], Loss: 5.2343\n",
      "Epoch [5/5], Step [120975/136675], Loss: 5.5644\n",
      "Epoch [5/5], Step [121050/136675], Loss: 4.7091\n",
      "Epoch [5/5], Step [121125/136675], Loss: 5.0757\n",
      "Epoch [5/5], Step [121200/136675], Loss: 5.5733\n",
      "Epoch [5/5], Step [121275/136675], Loss: 5.3930\n",
      "Epoch [5/5], Step [121350/136675], Loss: 5.2811\n",
      "Epoch [5/5], Step [121425/136675], Loss: 5.0489\n",
      "Epoch [5/5], Step [121500/136675], Loss: 5.2200\n",
      "Epoch [5/5], Step [121575/136675], Loss: 5.2047\n",
      "Epoch [5/5], Step [121650/136675], Loss: 5.1756\n",
      "Epoch [5/5], Step [121725/136675], Loss: 5.2369\n",
      "Epoch [5/5], Step [121800/136675], Loss: 5.2402\n",
      "Epoch [5/5], Step [121875/136675], Loss: 5.1156\n",
      "Epoch [5/5], Step [121950/136675], Loss: 5.1409\n",
      "Epoch [5/5], Step [122025/136675], Loss: 5.1033\n",
      "Epoch [5/5], Step [122100/136675], Loss: 5.2982\n",
      "Epoch [5/5], Step [122175/136675], Loss: 5.6534\n",
      "Epoch [5/5], Step [122250/136675], Loss: 5.4886\n",
      "Epoch [5/5], Step [122325/136675], Loss: 5.4647\n",
      "Epoch [5/5], Step [122400/136675], Loss: 5.3697\n",
      "Epoch [5/5], Step [122475/136675], Loss: 5.2121\n",
      "Epoch [5/5], Step [122550/136675], Loss: 5.3677\n",
      "Epoch [5/5], Step [122625/136675], Loss: 5.7193\n",
      "Epoch [5/5], Step [122700/136675], Loss: 5.5313\n",
      "Epoch [5/5], Step [122775/136675], Loss: 4.9241\n",
      "Epoch [5/5], Step [122850/136675], Loss: 5.2849\n",
      "Epoch [5/5], Step [122925/136675], Loss: 5.3869\n",
      "Epoch [5/5], Step [123000/136675], Loss: 5.2342\n",
      "Epoch [5/5], Step [123075/136675], Loss: 5.6477\n",
      "Epoch [5/5], Step [123150/136675], Loss: 5.3240\n",
      "Epoch [5/5], Step [123225/136675], Loss: 5.2546\n",
      "Epoch [5/5], Step [123300/136675], Loss: 5.5410\n",
      "Epoch [5/5], Step [123375/136675], Loss: 5.2856\n",
      "Epoch [5/5], Step [123450/136675], Loss: 5.2950\n",
      "Epoch [5/5], Step [123525/136675], Loss: 5.2858\n",
      "Epoch [5/5], Step [123600/136675], Loss: 5.2341\n",
      "Epoch [5/5], Step [123675/136675], Loss: 5.1626\n",
      "Epoch [5/5], Step [123750/136675], Loss: 5.5182\n",
      "Epoch [5/5], Step [123825/136675], Loss: 5.1863\n",
      "Epoch [5/5], Step [123900/136675], Loss: 5.2667\n",
      "Epoch [5/5], Step [123975/136675], Loss: 5.3458\n",
      "Epoch [5/5], Step [124050/136675], Loss: 5.4547\n",
      "Epoch [5/5], Step [124125/136675], Loss: 5.2469\n",
      "Epoch [5/5], Step [124200/136675], Loss: 5.1294\n",
      "Epoch [5/5], Step [124275/136675], Loss: 5.3043\n",
      "Epoch [5/5], Step [124350/136675], Loss: 5.4020\n",
      "Epoch [5/5], Step [124425/136675], Loss: 5.5354\n",
      "Epoch [5/5], Step [124500/136675], Loss: 5.4641\n",
      "Epoch [5/5], Step [124575/136675], Loss: 5.2633\n",
      "Epoch [5/5], Step [124650/136675], Loss: 5.1571\n",
      "Epoch [5/5], Step [124725/136675], Loss: 5.1249\n",
      "Epoch [5/5], Step [124800/136675], Loss: 5.1843\n",
      "Epoch [5/5], Step [124875/136675], Loss: 5.5377\n",
      "Epoch [5/5], Step [124950/136675], Loss: 5.1979\n",
      "Epoch [5/5], Step [125025/136675], Loss: 5.0431\n",
      "Epoch [5/5], Step [125100/136675], Loss: 5.1666\n",
      "Epoch [5/5], Step [125175/136675], Loss: 5.2992\n",
      "Epoch [5/5], Step [125250/136675], Loss: 5.3649\n",
      "Epoch [5/5], Step [125325/136675], Loss: 5.2135\n",
      "Epoch [5/5], Step [125400/136675], Loss: 5.4079\n",
      "Epoch [5/5], Step [125475/136675], Loss: 5.3454\n",
      "Epoch [5/5], Step [125550/136675], Loss: 5.5468\n",
      "Epoch [5/5], Step [125625/136675], Loss: 5.1708\n",
      "Epoch [5/5], Step [125700/136675], Loss: 5.3882\n",
      "Epoch [5/5], Step [125775/136675], Loss: 5.3906\n",
      "Epoch [5/5], Step [125850/136675], Loss: 5.3745\n",
      "Epoch [5/5], Step [125925/136675], Loss: 5.1646\n",
      "Epoch [5/5], Step [126000/136675], Loss: 5.2549\n",
      "Epoch [5/5], Step [126075/136675], Loss: 5.0991\n",
      "Epoch [5/5], Step [126150/136675], Loss: 5.6007\n",
      "Epoch [5/5], Step [126225/136675], Loss: 5.2703\n",
      "Epoch [5/5], Step [126300/136675], Loss: 5.0592\n",
      "Epoch [5/5], Step [126375/136675], Loss: 5.4986\n",
      "Epoch [5/5], Step [126450/136675], Loss: 5.4376\n",
      "Epoch [5/5], Step [126525/136675], Loss: 5.1802\n",
      "Epoch [5/5], Step [126600/136675], Loss: 5.4585\n",
      "Epoch [5/5], Step [126675/136675], Loss: 5.2849\n",
      "Epoch [5/5], Step [126750/136675], Loss: 5.2235\n",
      "Epoch [5/5], Step [126825/136675], Loss: 5.2887\n",
      "Epoch [5/5], Step [126900/136675], Loss: 5.1114\n",
      "Epoch [5/5], Step [126975/136675], Loss: 5.2211\n",
      "Epoch [5/5], Step [127050/136675], Loss: 5.4176\n",
      "Epoch [5/5], Step [127125/136675], Loss: 5.0875\n",
      "Epoch [5/5], Step [127200/136675], Loss: 5.3294\n",
      "Epoch [5/5], Step [127275/136675], Loss: 5.2160\n",
      "Epoch [5/5], Step [127350/136675], Loss: 5.0161\n",
      "Epoch [5/5], Step [127425/136675], Loss: 5.1629\n",
      "Epoch [5/5], Step [127500/136675], Loss: 5.3564\n",
      "Epoch [5/5], Step [127575/136675], Loss: 5.0376\n",
      "Epoch [5/5], Step [127650/136675], Loss: 5.2738\n",
      "Epoch [5/5], Step [127725/136675], Loss: 5.4185\n",
      "Epoch [5/5], Step [127800/136675], Loss: 5.4218\n",
      "Epoch [5/5], Step [127875/136675], Loss: 5.2388\n",
      "Epoch [5/5], Step [127950/136675], Loss: 5.2463\n",
      "Epoch [5/5], Step [128025/136675], Loss: 5.2074\n",
      "Epoch [5/5], Step [128100/136675], Loss: 5.4205\n",
      "Epoch [5/5], Step [128175/136675], Loss: 5.2732\n",
      "Epoch [5/5], Step [128250/136675], Loss: 5.2935\n",
      "Epoch [5/5], Step [128325/136675], Loss: 5.0035\n",
      "Epoch [5/5], Step [128400/136675], Loss: 5.2402\n",
      "Epoch [5/5], Step [128475/136675], Loss: 5.3018\n",
      "Epoch [5/5], Step [128550/136675], Loss: 5.1311\n",
      "Epoch [5/5], Step [128625/136675], Loss: 5.3185\n",
      "Epoch [5/5], Step [128700/136675], Loss: 5.4156\n",
      "Epoch [5/5], Step [128775/136675], Loss: 5.3505\n",
      "Epoch [5/5], Step [128850/136675], Loss: 5.1228\n",
      "Epoch [5/5], Step [128925/136675], Loss: 5.3378\n",
      "Epoch [5/5], Step [129000/136675], Loss: 5.2168\n",
      "Epoch [5/5], Step [129075/136675], Loss: 5.0066\n",
      "Epoch [5/5], Step [129150/136675], Loss: 5.3883\n",
      "Epoch [5/5], Step [129225/136675], Loss: 5.6960\n",
      "Epoch [5/5], Step [129300/136675], Loss: 5.2890\n",
      "Epoch [5/5], Step [129375/136675], Loss: 5.4205\n",
      "Epoch [5/5], Step [129450/136675], Loss: 5.1138\n",
      "Epoch [5/5], Step [129525/136675], Loss: 5.1688\n",
      "Epoch [5/5], Step [129600/136675], Loss: 5.0963\n",
      "Epoch [5/5], Step [129675/136675], Loss: 5.1030\n",
      "Epoch [5/5], Step [129750/136675], Loss: 5.6894\n",
      "Epoch [5/5], Step [129825/136675], Loss: 5.1468\n",
      "Epoch [5/5], Step [129900/136675], Loss: 5.4728\n",
      "Epoch [5/5], Step [129975/136675], Loss: 5.2076\n",
      "Validation perplexity: 153.82618076906294\n",
      "Epoch [5/5], Step [130050/136675], Loss: 5.1647\n",
      "Epoch [5/5], Step [130125/136675], Loss: 5.5862\n",
      "Epoch [5/5], Step [130200/136675], Loss: 5.3600\n",
      "Epoch [5/5], Step [130275/136675], Loss: 5.5904\n",
      "Epoch [5/5], Step [130350/136675], Loss: 5.2843\n",
      "Epoch [5/5], Step [130425/136675], Loss: 5.4570\n",
      "Epoch [5/5], Step [130500/136675], Loss: 5.4618\n",
      "Epoch [5/5], Step [130575/136675], Loss: 5.2662\n",
      "Epoch [5/5], Step [130650/136675], Loss: 5.3508\n",
      "Epoch [5/5], Step [130725/136675], Loss: 5.3517\n",
      "Epoch [5/5], Step [130800/136675], Loss: 5.5033\n",
      "Epoch [5/5], Step [130875/136675], Loss: 5.1374\n",
      "Epoch [5/5], Step [130950/136675], Loss: 5.6210\n",
      "Epoch [5/5], Step [131025/136675], Loss: 5.1931\n",
      "Epoch [5/5], Step [131100/136675], Loss: 5.6640\n",
      "Epoch [5/5], Step [131175/136675], Loss: 5.2229\n",
      "Epoch [5/5], Step [131250/136675], Loss: 5.4690\n",
      "Epoch [5/5], Step [131325/136675], Loss: 5.2453\n",
      "Epoch [5/5], Step [131400/136675], Loss: 5.3912\n",
      "Epoch [5/5], Step [131475/136675], Loss: 5.0283\n",
      "Epoch [5/5], Step [131550/136675], Loss: 5.1464\n",
      "Epoch [5/5], Step [131625/136675], Loss: 5.5077\n",
      "Epoch [5/5], Step [131700/136675], Loss: 5.3537\n",
      "Epoch [5/5], Step [131775/136675], Loss: 5.2232\n",
      "Epoch [5/5], Step [131850/136675], Loss: 5.2516\n",
      "Epoch [5/5], Step [131925/136675], Loss: 5.2945\n",
      "Epoch [5/5], Step [132000/136675], Loss: 5.2712\n",
      "Epoch [5/5], Step [132075/136675], Loss: 5.4624\n",
      "Epoch [5/5], Step [132150/136675], Loss: 5.1938\n",
      "Epoch [5/5], Step [132225/136675], Loss: 5.1956\n",
      "Epoch [5/5], Step [132300/136675], Loss: 5.1777\n",
      "Epoch [5/5], Step [132375/136675], Loss: 5.2876\n",
      "Epoch [5/5], Step [132450/136675], Loss: 5.4679\n",
      "Epoch [5/5], Step [132525/136675], Loss: 5.3839\n",
      "Epoch [5/5], Step [132600/136675], Loss: 5.6052\n",
      "Epoch [5/5], Step [132675/136675], Loss: 5.1458\n",
      "Epoch [5/5], Step [132750/136675], Loss: 5.0913\n",
      "Epoch [5/5], Step [132825/136675], Loss: 5.4462\n",
      "Epoch [5/5], Step [132900/136675], Loss: 4.9976\n",
      "Epoch [5/5], Step [132975/136675], Loss: 5.3253\n",
      "Epoch [5/5], Step [133050/136675], Loss: 5.1705\n",
      "Epoch [5/5], Step [133125/136675], Loss: 5.4391\n",
      "Epoch [5/5], Step [133200/136675], Loss: 5.6268\n",
      "Epoch [5/5], Step [133275/136675], Loss: 5.3945\n",
      "Epoch [5/5], Step [133350/136675], Loss: 5.3163\n",
      "Epoch [5/5], Step [133425/136675], Loss: 5.0023\n",
      "Epoch [5/5], Step [133500/136675], Loss: 5.3706\n",
      "Epoch [5/5], Step [133575/136675], Loss: 5.7365\n",
      "Epoch [5/5], Step [133650/136675], Loss: 5.0913\n",
      "Epoch [5/5], Step [133725/136675], Loss: 5.1816\n",
      "Epoch [5/5], Step [133800/136675], Loss: 5.3447\n",
      "Epoch [5/5], Step [133875/136675], Loss: 5.1977\n",
      "Epoch [5/5], Step [133950/136675], Loss: 5.2918\n",
      "Epoch [5/5], Step [134025/136675], Loss: 5.5852\n",
      "Epoch [5/5], Step [134100/136675], Loss: 5.1059\n",
      "Epoch [5/5], Step [134175/136675], Loss: 5.2526\n",
      "Epoch [5/5], Step [134250/136675], Loss: 5.3637\n",
      "Epoch [5/5], Step [134325/136675], Loss: 5.3110\n",
      "Epoch [5/5], Step [134400/136675], Loss: 5.3392\n",
      "Epoch [5/5], Step [134475/136675], Loss: 5.0659\n",
      "Epoch [5/5], Step [134550/136675], Loss: 5.0688\n",
      "Epoch [5/5], Step [134625/136675], Loss: 5.2383\n",
      "Epoch [5/5], Step [134700/136675], Loss: 5.2453\n",
      "Epoch [5/5], Step [134775/136675], Loss: 5.0902\n",
      "Epoch [5/5], Step [134850/136675], Loss: 5.2372\n",
      "Epoch [5/5], Step [134925/136675], Loss: 5.4956\n",
      "Epoch [5/5], Step [135000/136675], Loss: 5.2327\n",
      "Epoch [5/5], Step [135075/136675], Loss: 5.2679\n",
      "Epoch [5/5], Step [135150/136675], Loss: 5.0416\n",
      "Epoch [5/5], Step [135225/136675], Loss: 5.4259\n",
      "Epoch [5/5], Step [135300/136675], Loss: 5.2795\n",
      "Epoch [5/5], Step [135375/136675], Loss: 5.3469\n",
      "Epoch [5/5], Step [135450/136675], Loss: 5.0748\n",
      "Epoch [5/5], Step [135525/136675], Loss: 5.6674\n",
      "Epoch [5/5], Step [135600/136675], Loss: 5.1095\n",
      "Epoch [5/5], Step [135675/136675], Loss: 5.3674\n",
      "Epoch [5/5], Step [135750/136675], Loss: 5.0417\n",
      "Epoch [5/5], Step [135825/136675], Loss: 5.4462\n",
      "Epoch [5/5], Step [135900/136675], Loss: 5.4021\n",
      "Epoch [5/5], Step [135975/136675], Loss: 5.4305\n",
      "Epoch [5/5], Step [136050/136675], Loss: 5.4629\n",
      "Epoch [5/5], Step [136125/136675], Loss: 5.1378\n",
      "Epoch [5/5], Step [136200/136675], Loss: 5.5234\n",
      "Epoch [5/5], Step [136275/136675], Loss: 5.3488\n",
      "Epoch [5/5], Step [136350/136675], Loss: 5.5199\n",
      "Epoch [5/5], Step [136425/136675], Loss: 5.2890\n",
      "Epoch [5/5], Step [136500/136675], Loss: 5.3422\n",
      "Epoch [5/5], Step [136575/136675], Loss: 5.2810\n",
      "Epoch [5/5], Step [136650/136675], Loss: 5.1556\n",
      "Epoch [5/5] Average Loss: 5.2742, Perplexity: 195.23\n"
     ]
    }
   ],
   "source": [
    "from src.trainComplete import TrainComplete\n",
    "from src.attentionModel import LanguageModelWithAttention\n",
    "\n",
    "trainclass = TrainComplete(text_path = text_path,path_to_save_folder= path_to_save_folder,tokenizer = tokenizer,\n",
    "                           allowed_special=False, is_attention_training = True)\n",
    "\n",
    "\n",
    "context_length = 32  # Increased context size\n",
    "embedding_dim = 128\n",
    "attention_dim = 64\n",
    "hidden_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "model = LanguageModelWithAttention(\n",
    "    vocab_size, embedding_dim, attention_dim, context_length, hidden_dim, num_heads, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "trainclass.train(model,\n",
    "              vocab_size,device,raw_text,\"pers_attention_standard_dropout_batchsize16_ep5_eval10000\",\n",
    "                print_every=75,evaluate_every=10000,optimizer=None,criterion=None,\n",
    "              batch_size = 16,\n",
    "              embedding_dim = embedding_dim,\n",
    "              context_length = context_length,\n",
    "              num_epochs = 5\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34208be6-bb03-4f20-a4a2-7ad0591f29d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create Dataset 2720000 / 2733504Started Training\n",
      "Epoch [1/5], Step [0/34168], Loss: 10.3165\n",
      "Validation perplexity: 29934.56014135121\n",
      "Epoch [1/5], Step [75/34168], Loss: 7.3190\n",
      "Epoch [1/5], Step [150/34168], Loss: 7.2938\n",
      "Epoch [1/5], Step [225/34168], Loss: 7.2164\n",
      "Epoch [1/5], Step [300/34168], Loss: 7.2242\n",
      "Epoch [1/5], Step [375/34168], Loss: 7.2208\n",
      "Epoch [1/5], Step [450/34168], Loss: 6.9309\n",
      "Epoch [1/5], Step [525/34168], Loss: 6.8232\n",
      "Epoch [1/5], Step [600/34168], Loss: 6.9315\n",
      "Epoch [1/5], Step [675/34168], Loss: 6.8030\n",
      "Epoch [1/5], Step [750/34168], Loss: 6.7433\n",
      "Epoch [1/5], Step [825/34168], Loss: 6.6874\n",
      "Epoch [1/5], Step [900/34168], Loss: 6.7487\n",
      "Epoch [1/5], Step [975/34168], Loss: 6.5829\n",
      "Epoch [1/5], Step [1050/34168], Loss: 6.6490\n",
      "Epoch [1/5], Step [1125/34168], Loss: 6.6424\n",
      "Epoch [1/5], Step [1200/34168], Loss: 6.4952\n",
      "Epoch [1/5], Step [1275/34168], Loss: 6.6095\n",
      "Epoch [1/5], Step [1350/34168], Loss: 6.6045\n",
      "Epoch [1/5], Step [1425/34168], Loss: 6.4220\n",
      "Epoch [1/5], Step [1500/34168], Loss: 6.6255\n",
      "Epoch [1/5], Step [1575/34168], Loss: 6.3558\n",
      "Epoch [1/5], Step [1650/34168], Loss: 6.3009\n",
      "Epoch [1/5], Step [1725/34168], Loss: 6.3973\n",
      "Epoch [1/5], Step [1800/34168], Loss: 6.3992\n",
      "Epoch [1/5], Step [1875/34168], Loss: 6.3833\n",
      "Epoch [1/5], Step [1950/34168], Loss: 6.3595\n",
      "Epoch [1/5], Step [2025/34168], Loss: 6.5596\n",
      "Epoch [1/5], Step [2100/34168], Loss: 6.3103\n",
      "Epoch [1/5], Step [2175/34168], Loss: 6.2512\n",
      "Epoch [1/5], Step [2250/34168], Loss: 6.2853\n",
      "Epoch [1/5], Step [2325/34168], Loss: 6.1557\n",
      "Epoch [1/5], Step [2400/34168], Loss: 6.1914\n",
      "Epoch [1/5], Step [2475/34168], Loss: 6.3676\n",
      "Epoch [1/5], Step [2550/34168], Loss: 6.1271\n",
      "Epoch [1/5], Step [2625/34168], Loss: 6.1312\n",
      "Epoch [1/5], Step [2700/34168], Loss: 6.2299\n",
      "Epoch [1/5], Step [2775/34168], Loss: 6.3311\n",
      "Epoch [1/5], Step [2850/34168], Loss: 6.2268\n",
      "Epoch [1/5], Step [2925/34168], Loss: 6.1662\n",
      "Epoch [1/5], Step [3000/34168], Loss: 6.0218\n",
      "Epoch [1/5], Step [3075/34168], Loss: 6.0075\n",
      "Epoch [1/5], Step [3150/34168], Loss: 6.1250\n",
      "Epoch [1/5], Step [3225/34168], Loss: 5.9259\n",
      "Epoch [1/5], Step [3300/34168], Loss: 5.8831\n",
      "Epoch [1/5], Step [3375/34168], Loss: 5.9889\n",
      "Epoch [1/5], Step [3450/34168], Loss: 6.1411\n",
      "Epoch [1/5], Step [3525/34168], Loss: 6.1171\n",
      "Epoch [1/5], Step [3600/34168], Loss: 6.2305\n",
      "Epoch [1/5], Step [3675/34168], Loss: 6.0517\n",
      "Epoch [1/5], Step [3750/34168], Loss: 6.2380\n",
      "Epoch [1/5], Step [3825/34168], Loss: 6.0790\n",
      "Epoch [1/5], Step [3900/34168], Loss: 5.9949\n",
      "Epoch [1/5], Step [3975/34168], Loss: 6.2172\n",
      "Epoch [1/5], Step [4050/34168], Loss: 5.9780\n",
      "Epoch [1/5], Step [4125/34168], Loss: 5.9695\n",
      "Epoch [1/5], Step [4200/34168], Loss: 6.1152\n",
      "Epoch [1/5], Step [4275/34168], Loss: 6.0418\n",
      "Epoch [1/5], Step [4350/34168], Loss: 5.9818\n",
      "Epoch [1/5], Step [4425/34168], Loss: 5.9612\n",
      "Epoch [1/5], Step [4500/34168], Loss: 6.0923\n",
      "Epoch [1/5], Step [4575/34168], Loss: 6.0303\n",
      "Epoch [1/5], Step [4650/34168], Loss: 6.0695\n",
      "Epoch [1/5], Step [4725/34168], Loss: 5.9921\n",
      "Epoch [1/5], Step [4800/34168], Loss: 6.0526\n",
      "Epoch [1/5], Step [4875/34168], Loss: 5.9953\n",
      "Epoch [1/5], Step [4950/34168], Loss: 6.0318\n",
      "Epoch [1/5], Step [5025/34168], Loss: 5.8072\n",
      "Epoch [1/5], Step [5100/34168], Loss: 6.0080\n",
      "Epoch [1/5], Step [5175/34168], Loss: 5.8586\n",
      "Epoch [1/5], Step [5250/34168], Loss: 5.9249\n",
      "Epoch [1/5], Step [5325/34168], Loss: 5.9468\n",
      "Epoch [1/5], Step [5400/34168], Loss: 5.9037\n",
      "Epoch [1/5], Step [5475/34168], Loss: 6.0976\n",
      "Epoch [1/5], Step [5550/34168], Loss: 5.9167\n",
      "Epoch [1/5], Step [5625/34168], Loss: 5.8572\n",
      "Epoch [1/5], Step [5700/34168], Loss: 5.9324\n",
      "Epoch [1/5], Step [5775/34168], Loss: 5.9539\n",
      "Epoch [1/5], Step [5850/34168], Loss: 6.0225\n",
      "Epoch [1/5], Step [5925/34168], Loss: 5.8866\n",
      "Epoch [1/5], Step [6000/34168], Loss: 5.9010\n",
      "Epoch [1/5], Step [6075/34168], Loss: 5.9317\n",
      "Epoch [1/5], Step [6150/34168], Loss: 5.8445\n",
      "Epoch [1/5], Step [6225/34168], Loss: 5.8916\n",
      "Epoch [1/5], Step [6300/34168], Loss: 5.8106\n",
      "Epoch [1/5], Step [6375/34168], Loss: 5.8690\n",
      "Epoch [1/5], Step [6450/34168], Loss: 5.8820\n",
      "Epoch [1/5], Step [6525/34168], Loss: 5.8221\n",
      "Epoch [1/5], Step [6600/34168], Loss: 5.7576\n",
      "Epoch [1/5], Step [6675/34168], Loss: 5.7595\n",
      "Epoch [1/5], Step [6750/34168], Loss: 5.9840\n",
      "Epoch [1/5], Step [6825/34168], Loss: 5.7080\n",
      "Epoch [1/5], Step [6900/34168], Loss: 5.9314\n",
      "Epoch [1/5], Step [6975/34168], Loss: 5.9431\n",
      "Epoch [1/5], Step [7050/34168], Loss: 5.7012\n",
      "Epoch [1/5], Step [7125/34168], Loss: 5.9405\n",
      "Epoch [1/5], Step [7200/34168], Loss: 5.9203\n",
      "Epoch [1/5], Step [7275/34168], Loss: 5.8673\n",
      "Epoch [1/5], Step [7350/34168], Loss: 5.7671\n",
      "Epoch [1/5], Step [7425/34168], Loss: 5.7747\n",
      "Epoch [1/5], Step [7500/34168], Loss: 5.9216\n",
      "Epoch [1/5], Step [7575/34168], Loss: 5.7269\n",
      "Epoch [1/5], Step [7650/34168], Loss: 5.7424\n",
      "Epoch [1/5], Step [7725/34168], Loss: 5.8805\n",
      "Epoch [1/5], Step [7800/34168], Loss: 5.7475\n",
      "Epoch [1/5], Step [7875/34168], Loss: 5.8092\n",
      "Epoch [1/5], Step [7950/34168], Loss: 5.7843\n",
      "Epoch [1/5], Step [8025/34168], Loss: 5.9290\n",
      "Epoch [1/5], Step [8100/34168], Loss: 5.7379\n",
      "Epoch [1/5], Step [8175/34168], Loss: 5.8394\n",
      "Epoch [1/5], Step [8250/34168], Loss: 5.6035\n",
      "Epoch [1/5], Step [8325/34168], Loss: 5.7470\n",
      "Epoch [1/5], Step [8400/34168], Loss: 5.5935\n",
      "Epoch [1/5], Step [8475/34168], Loss: 5.5873\n",
      "Epoch [1/5], Step [8550/34168], Loss: 5.6021\n",
      "Epoch [1/5], Step [8625/34168], Loss: 5.6538\n",
      "Epoch [1/5], Step [8700/34168], Loss: 5.6552\n",
      "Epoch [1/5], Step [8775/34168], Loss: 5.7071\n",
      "Epoch [1/5], Step [8850/34168], Loss: 5.5984\n",
      "Epoch [1/5], Step [8925/34168], Loss: 5.6955\n",
      "Epoch [1/5], Step [9000/34168], Loss: 5.7611\n",
      "Epoch [1/5], Step [9075/34168], Loss: 5.8877\n",
      "Epoch [1/5], Step [9150/34168], Loss: 5.6121\n",
      "Epoch [1/5], Step [9225/34168], Loss: 5.7349\n",
      "Epoch [1/5], Step [9300/34168], Loss: 5.5406\n",
      "Epoch [1/5], Step [9375/34168], Loss: 5.7070\n",
      "Epoch [1/5], Step [9450/34168], Loss: 5.7612\n",
      "Epoch [1/5], Step [9525/34168], Loss: 5.7754\n",
      "Epoch [1/5], Step [9600/34168], Loss: 5.7281\n",
      "Epoch [1/5], Step [9675/34168], Loss: 5.6104\n",
      "Epoch [1/5], Step [9750/34168], Loss: 5.6629\n",
      "Epoch [1/5], Step [9825/34168], Loss: 5.5343\n",
      "Epoch [1/5], Step [9900/34168], Loss: 5.7058\n",
      "Epoch [1/5], Step [9975/34168], Loss: 5.6895\n",
      "Validation perplexity: 246.24646429281609\n",
      "Epoch [1/5], Step [10050/34168], Loss: 5.6676\n",
      "Epoch [1/5], Step [10125/34168], Loss: 5.6656\n",
      "Epoch [1/5], Step [10200/34168], Loss: 5.5431\n",
      "Epoch [1/5], Step [10275/34168], Loss: 5.6764\n",
      "Epoch [1/5], Step [10350/34168], Loss: 5.6336\n",
      "Epoch [1/5], Step [10425/34168], Loss: 5.7643\n",
      "Epoch [1/5], Step [10500/34168], Loss: 5.6725\n",
      "Epoch [1/5], Step [10575/34168], Loss: 5.7184\n",
      "Epoch [1/5], Step [10650/34168], Loss: 5.7216\n",
      "Epoch [1/5], Step [10725/34168], Loss: 5.6378\n",
      "Epoch [1/5], Step [10800/34168], Loss: 5.6798\n",
      "Epoch [1/5], Step [10875/34168], Loss: 5.5445\n",
      "Epoch [1/5], Step [10950/34168], Loss: 5.6024\n",
      "Epoch [1/5], Step [11025/34168], Loss: 5.7254\n",
      "Epoch [1/5], Step [11100/34168], Loss: 5.5525\n",
      "Epoch [1/5], Step [11175/34168], Loss: 5.7201\n",
      "Epoch [1/5], Step [11250/34168], Loss: 5.5667\n",
      "Epoch [1/5], Step [11325/34168], Loss: 5.5719\n",
      "Epoch [1/5], Step [11400/34168], Loss: 5.4917\n",
      "Epoch [1/5], Step [11475/34168], Loss: 5.6528\n",
      "Epoch [1/5], Step [11550/34168], Loss: 5.5769\n",
      "Epoch [1/5], Step [11625/34168], Loss: 5.7597\n",
      "Epoch [1/5], Step [11700/34168], Loss: 5.7230\n",
      "Epoch [1/5], Step [11775/34168], Loss: 5.5866\n",
      "Epoch [1/5], Step [11850/34168], Loss: 5.4957\n",
      "Epoch [1/5], Step [11925/34168], Loss: 5.5870\n",
      "Epoch [1/5], Step [12000/34168], Loss: 5.4510\n",
      "Epoch [1/5], Step [12075/34168], Loss: 5.7092\n",
      "Epoch [1/5], Step [12150/34168], Loss: 5.8225\n",
      "Epoch [1/5], Step [12225/34168], Loss: 5.6337\n",
      "Epoch [1/5], Step [12300/34168], Loss: 5.4056\n",
      "Epoch [1/5], Step [12375/34168], Loss: 5.5214\n",
      "Epoch [1/5], Step [12450/34168], Loss: 5.6251\n",
      "Epoch [1/5], Step [12525/34168], Loss: 5.5978\n",
      "Epoch [1/5], Step [12600/34168], Loss: 5.7104\n",
      "Epoch [1/5], Step [12675/34168], Loss: 5.5675\n",
      "Epoch [1/5], Step [12750/34168], Loss: 5.4893\n",
      "Epoch [1/5], Step [12825/34168], Loss: 5.6527\n",
      "Epoch [1/5], Step [12900/34168], Loss: 5.4851\n",
      "Epoch [1/5], Step [12975/34168], Loss: 5.4907\n",
      "Epoch [1/5], Step [13050/34168], Loss: 5.6991\n",
      "Epoch [1/5], Step [13125/34168], Loss: 5.6025\n",
      "Epoch [1/5], Step [13200/34168], Loss: 5.5715\n",
      "Epoch [1/5], Step [13275/34168], Loss: 5.5829\n",
      "Epoch [1/5], Step [13350/34168], Loss: 5.7260\n",
      "Epoch [1/5], Step [13425/34168], Loss: 5.3872\n",
      "Epoch [1/5], Step [13500/34168], Loss: 5.6211\n",
      "Epoch [1/5], Step [13575/34168], Loss: 5.5090\n",
      "Epoch [1/5], Step [13650/34168], Loss: 5.5000\n",
      "Epoch [1/5], Step [13725/34168], Loss: 5.4947\n",
      "Epoch [1/5], Step [13800/34168], Loss: 5.4660\n",
      "Epoch [1/5], Step [13875/34168], Loss: 5.5201\n",
      "Epoch [1/5], Step [13950/34168], Loss: 5.4447\n",
      "Epoch [1/5], Step [14025/34168], Loss: 5.6390\n",
      "Epoch [1/5], Step [14100/34168], Loss: 5.5698\n",
      "Epoch [1/5], Step [14175/34168], Loss: 5.5645\n",
      "Epoch [1/5], Step [14250/34168], Loss: 5.6272\n",
      "Epoch [1/5], Step [14325/34168], Loss: 5.5347\n",
      "Epoch [1/5], Step [14400/34168], Loss: 5.7052\n",
      "Epoch [1/5], Step [14475/34168], Loss: 5.5570\n",
      "Epoch [1/5], Step [14550/34168], Loss: 5.4955\n",
      "Epoch [1/5], Step [14625/34168], Loss: 5.5135\n",
      "Epoch [1/5], Step [14700/34168], Loss: 5.7324\n",
      "Epoch [1/5], Step [14775/34168], Loss: 5.5556\n",
      "Epoch [1/5], Step [14850/34168], Loss: 5.5442\n",
      "Epoch [1/5], Step [14925/34168], Loss: 5.5589\n",
      "Epoch [1/5], Step [15000/34168], Loss: 5.6276\n",
      "Epoch [1/5], Step [15075/34168], Loss: 5.3773\n",
      "Epoch [1/5], Step [15150/34168], Loss: 5.4561\n",
      "Epoch [1/5], Step [15225/34168], Loss: 5.4975\n",
      "Epoch [1/5], Step [15300/34168], Loss: 5.4592\n",
      "Epoch [1/5], Step [15375/34168], Loss: 5.4896\n",
      "Epoch [1/5], Step [15450/34168], Loss: 5.5210\n",
      "Epoch [1/5], Step [15525/34168], Loss: 5.5963\n",
      "Epoch [1/5], Step [15600/34168], Loss: 5.7553\n",
      "Epoch [1/5], Step [15675/34168], Loss: 5.5607\n",
      "Epoch [1/5], Step [15750/34168], Loss: 5.4364\n",
      "Epoch [1/5], Step [15825/34168], Loss: 5.4436\n",
      "Epoch [1/5], Step [15900/34168], Loss: 5.5811\n",
      "Epoch [1/5], Step [15975/34168], Loss: 5.3983\n",
      "Epoch [1/5], Step [16050/34168], Loss: 5.6905\n",
      "Epoch [1/5], Step [16125/34168], Loss: 5.6193\n",
      "Epoch [1/5], Step [16200/34168], Loss: 5.5787\n",
      "Epoch [1/5], Step [16275/34168], Loss: 5.4577\n",
      "Epoch [1/5], Step [16350/34168], Loss: 5.3364\n",
      "Epoch [1/5], Step [16425/34168], Loss: 5.5085\n",
      "Epoch [1/5], Step [16500/34168], Loss: 5.4853\n",
      "Epoch [1/5], Step [16575/34168], Loss: 5.5536\n",
      "Epoch [1/5], Step [16650/34168], Loss: 5.4429\n",
      "Epoch [1/5], Step [16725/34168], Loss: 5.5506\n",
      "Epoch [1/5], Step [16800/34168], Loss: 5.5213\n",
      "Epoch [1/5], Step [16875/34168], Loss: 5.5448\n",
      "Epoch [1/5], Step [16950/34168], Loss: 5.6716\n",
      "Epoch [1/5], Step [17025/34168], Loss: 5.5299\n",
      "Epoch [1/5], Step [17100/34168], Loss: 5.4893\n",
      "Epoch [1/5], Step [17175/34168], Loss: 5.4860\n",
      "Epoch [1/5], Step [17250/34168], Loss: 5.3367\n",
      "Epoch [1/5], Step [17325/34168], Loss: 5.4814\n",
      "Epoch [1/5], Step [17400/34168], Loss: 5.4943\n",
      "Epoch [1/5], Step [17475/34168], Loss: 5.4126\n",
      "Epoch [1/5], Step [17550/34168], Loss: 5.5147\n",
      "Epoch [1/5], Step [17625/34168], Loss: 5.5313\n",
      "Epoch [1/5], Step [17700/34168], Loss: 5.4899\n",
      "Epoch [1/5], Step [17775/34168], Loss: 5.5223\n",
      "Epoch [1/5], Step [17850/34168], Loss: 5.3998\n",
      "Epoch [1/5], Step [17925/34168], Loss: 5.5589\n",
      "Epoch [1/5], Step [18000/34168], Loss: 5.5301\n",
      "Epoch [1/5], Step [18075/34168], Loss: 5.4008\n",
      "Epoch [1/5], Step [18150/34168], Loss: 5.3427\n",
      "Epoch [1/5], Step [18225/34168], Loss: 5.3889\n",
      "Epoch [1/5], Step [18300/34168], Loss: 5.4684\n",
      "Epoch [1/5], Step [18375/34168], Loss: 5.5050\n",
      "Epoch [1/5], Step [18450/34168], Loss: 5.5101\n",
      "Epoch [1/5], Step [18525/34168], Loss: 5.5900\n",
      "Epoch [1/5], Step [18600/34168], Loss: 5.5148\n",
      "Epoch [1/5], Step [18675/34168], Loss: 5.3848\n",
      "Epoch [1/5], Step [18750/34168], Loss: 5.3908\n",
      "Epoch [1/5], Step [18825/34168], Loss: 5.5319\n",
      "Epoch [1/5], Step [18900/34168], Loss: 5.4464\n",
      "Epoch [1/5], Step [18975/34168], Loss: 5.4600\n",
      "Epoch [1/5], Step [19050/34168], Loss: 5.4534\n",
      "Epoch [1/5], Step [19125/34168], Loss: 5.4521\n",
      "Epoch [1/5], Step [19200/34168], Loss: 5.3763\n",
      "Epoch [1/5], Step [19275/34168], Loss: 5.4755\n",
      "Epoch [1/5], Step [19350/34168], Loss: 5.5681\n",
      "Epoch [1/5], Step [19425/34168], Loss: 5.5004\n",
      "Epoch [1/5], Step [19500/34168], Loss: 5.3997\n",
      "Epoch [1/5], Step [19575/34168], Loss: 5.4157\n",
      "Epoch [1/5], Step [19650/34168], Loss: 5.4876\n",
      "Epoch [1/5], Step [19725/34168], Loss: 5.4775\n",
      "Epoch [1/5], Step [19800/34168], Loss: 5.4122\n",
      "Epoch [1/5], Step [19875/34168], Loss: 5.2924\n",
      "Epoch [1/5], Step [19950/34168], Loss: 5.5406\n",
      "Validation perplexity: 193.01441444018724\n",
      "Epoch [1/5], Step [20025/34168], Loss: 5.3426\n",
      "Epoch [1/5], Step [20100/34168], Loss: 5.5603\n",
      "Epoch [1/5], Step [20175/34168], Loss: 5.7646\n",
      "Epoch [1/5], Step [20250/34168], Loss: 5.5240\n",
      "Epoch [1/5], Step [20325/34168], Loss: 5.5914\n",
      "Epoch [1/5], Step [20400/34168], Loss: 5.5610\n",
      "Epoch [1/5], Step [20475/34168], Loss: 5.4374\n",
      "Epoch [1/5], Step [20550/34168], Loss: 5.5058\n",
      "Epoch [1/5], Step [20625/34168], Loss: 5.5267\n",
      "Epoch [1/5], Step [20700/34168], Loss: 5.4365\n",
      "Epoch [1/5], Step [20775/34168], Loss: 5.5322\n",
      "Epoch [1/5], Step [20850/34168], Loss: 5.2736\n",
      "Epoch [1/5], Step [20925/34168], Loss: 5.4425\n",
      "Epoch [1/5], Step [21000/34168], Loss: 5.4150\n",
      "Epoch [1/5], Step [21075/34168], Loss: 5.4208\n",
      "Epoch [1/5], Step [21150/34168], Loss: 5.2012\n",
      "Epoch [1/5], Step [21225/34168], Loss: 5.4706\n",
      "Epoch [1/5], Step [21300/34168], Loss: 5.5505\n",
      "Epoch [1/5], Step [21375/34168], Loss: 5.5068\n",
      "Epoch [1/5], Step [21450/34168], Loss: 5.5256\n",
      "Epoch [1/5], Step [21525/34168], Loss: 5.3782\n",
      "Epoch [1/5], Step [21600/34168], Loss: 5.2747\n",
      "Epoch [1/5], Step [21675/34168], Loss: 5.4942\n",
      "Epoch [1/5], Step [21750/34168], Loss: 5.3340\n",
      "Epoch [1/5], Step [21825/34168], Loss: 5.6253\n",
      "Epoch [1/5], Step [21900/34168], Loss: 5.3810\n",
      "Epoch [1/5], Step [21975/34168], Loss: 5.6618\n",
      "Epoch [1/5], Step [22050/34168], Loss: 5.3914\n",
      "Epoch [1/5], Step [22125/34168], Loss: 5.4564\n",
      "Epoch [1/5], Step [22200/34168], Loss: 5.4669\n",
      "Epoch [1/5], Step [22275/34168], Loss: 5.3783\n",
      "Epoch [1/5], Step [22350/34168], Loss: 5.5277\n",
      "Epoch [1/5], Step [22425/34168], Loss: 5.3558\n",
      "Epoch [1/5], Step [22500/34168], Loss: 5.4828\n",
      "Epoch [1/5], Step [22575/34168], Loss: 5.5550\n",
      "Epoch [1/5], Step [22650/34168], Loss: 5.3919\n",
      "Epoch [1/5], Step [22725/34168], Loss: 5.5343\n",
      "Epoch [1/5], Step [22800/34168], Loss: 5.4535\n",
      "Epoch [1/5], Step [22875/34168], Loss: 5.2041\n",
      "Epoch [1/5], Step [22950/34168], Loss: 5.4455\n",
      "Epoch [1/5], Step [23025/34168], Loss: 5.4493\n",
      "Epoch [1/5], Step [23100/34168], Loss: 5.3598\n",
      "Epoch [1/5], Step [23175/34168], Loss: 5.3709\n",
      "Epoch [1/5], Step [23250/34168], Loss: 5.2594\n",
      "Epoch [1/5], Step [23325/34168], Loss: 5.3662\n",
      "Epoch [1/5], Step [23400/34168], Loss: 5.2435\n",
      "Epoch [1/5], Step [23475/34168], Loss: 5.2956\n",
      "Epoch [1/5], Step [23550/34168], Loss: 5.3829\n",
      "Epoch [1/5], Step [23625/34168], Loss: 5.4993\n",
      "Epoch [1/5], Step [23700/34168], Loss: 5.3867\n",
      "Epoch [1/5], Step [23775/34168], Loss: 5.3660\n",
      "Epoch [1/5], Step [23850/34168], Loss: 5.5255\n",
      "Epoch [1/5], Step [23925/34168], Loss: 5.4810\n",
      "Epoch [1/5], Step [24000/34168], Loss: 5.3380\n",
      "Epoch [1/5], Step [24075/34168], Loss: 5.5270\n",
      "Epoch [1/5], Step [24150/34168], Loss: 5.4185\n",
      "Epoch [1/5], Step [24225/34168], Loss: 5.2473\n",
      "Epoch [1/5], Step [24300/34168], Loss: 5.3006\n",
      "Epoch [1/5], Step [24375/34168], Loss: 5.4616\n",
      "Epoch [1/5], Step [24450/34168], Loss: 5.2555\n",
      "Epoch [1/5], Step [24525/34168], Loss: 5.3417\n",
      "Epoch [1/5], Step [24600/34168], Loss: 5.4287\n",
      "Epoch [1/5], Step [24675/34168], Loss: 5.3680\n",
      "Epoch [1/5], Step [24750/34168], Loss: 5.4591\n",
      "Epoch [1/5], Step [24825/34168], Loss: 5.3194\n",
      "Epoch [1/5], Step [24900/34168], Loss: 5.4659\n",
      "Epoch [1/5], Step [24975/34168], Loss: 5.4680\n",
      "Epoch [1/5], Step [25050/34168], Loss: 5.4695\n",
      "Epoch [1/5], Step [25125/34168], Loss: 5.3755\n",
      "Epoch [1/5], Step [25200/34168], Loss: 5.4769\n",
      "Epoch [1/5], Step [25275/34168], Loss: 5.4232\n",
      "Epoch [1/5], Step [25350/34168], Loss: 5.3651\n",
      "Epoch [1/5], Step [25425/34168], Loss: 5.5184\n",
      "Epoch [1/5], Step [25500/34168], Loss: 5.3190\n",
      "Epoch [1/5], Step [25575/34168], Loss: 5.3832\n",
      "Epoch [1/5], Step [25650/34168], Loss: 5.4467\n",
      "Epoch [1/5], Step [25725/34168], Loss: 5.3623\n",
      "Epoch [1/5], Step [25800/34168], Loss: 5.4339\n",
      "Epoch [1/5], Step [25875/34168], Loss: 5.4156\n",
      "Epoch [1/5], Step [25950/34168], Loss: 5.3899\n",
      "Epoch [1/5], Step [26025/34168], Loss: 5.4243\n",
      "Epoch [1/5], Step [26100/34168], Loss: 5.3863\n",
      "Epoch [1/5], Step [26175/34168], Loss: 5.5227\n",
      "Epoch [1/5], Step [26250/34168], Loss: 5.2071\n",
      "Epoch [1/5], Step [26325/34168], Loss: 5.4180\n",
      "Epoch [1/5], Step [26400/34168], Loss: 5.3579\n",
      "Epoch [1/5], Step [26475/34168], Loss: 5.3642\n",
      "Epoch [1/5], Step [26550/34168], Loss: 5.1705\n",
      "Epoch [1/5], Step [26625/34168], Loss: 5.5724\n",
      "Epoch [1/5], Step [26700/34168], Loss: 5.4728\n",
      "Epoch [1/5], Step [26775/34168], Loss: 5.4340\n",
      "Epoch [1/5], Step [26850/34168], Loss: 5.4041\n",
      "Epoch [1/5], Step [26925/34168], Loss: 5.3456\n",
      "Epoch [1/5], Step [27000/34168], Loss: 5.5003\n",
      "Epoch [1/5], Step [27075/34168], Loss: 5.2958\n",
      "Epoch [1/5], Step [27150/34168], Loss: 5.4302\n",
      "Epoch [1/5], Step [27225/34168], Loss: 5.5582\n",
      "Epoch [1/5], Step [27300/34168], Loss: 5.3854\n",
      "Epoch [1/5], Step [27375/34168], Loss: 5.2666\n",
      "Epoch [1/5], Step [27450/34168], Loss: 5.4725\n",
      "Epoch [1/5], Step [27525/34168], Loss: 5.5356\n",
      "Epoch [1/5], Step [27600/34168], Loss: 5.4590\n",
      "Epoch [1/5], Step [27675/34168], Loss: 5.5039\n",
      "Epoch [1/5], Step [27750/34168], Loss: 5.3999\n",
      "Epoch [1/5], Step [27825/34168], Loss: 5.4057\n",
      "Epoch [1/5], Step [27900/34168], Loss: 5.2600\n",
      "Epoch [1/5], Step [27975/34168], Loss: 5.3577\n",
      "Epoch [1/5], Step [28050/34168], Loss: 5.3792\n",
      "Epoch [1/5], Step [28125/34168], Loss: 5.3929\n",
      "Epoch [1/5], Step [28200/34168], Loss: 5.3008\n",
      "Epoch [1/5], Step [28275/34168], Loss: 5.2899\n",
      "Epoch [1/5], Step [28350/34168], Loss: 5.4416\n",
      "Epoch [1/5], Step [28425/34168], Loss: 5.3953\n",
      "Epoch [1/5], Step [28500/34168], Loss: 5.4470\n",
      "Epoch [1/5], Step [28575/34168], Loss: 5.3547\n",
      "Epoch [1/5], Step [28650/34168], Loss: 5.3067\n",
      "Epoch [1/5], Step [28725/34168], Loss: 5.2644\n",
      "Epoch [1/5], Step [28800/34168], Loss: 5.2879\n",
      "Epoch [1/5], Step [28875/34168], Loss: 5.3248\n",
      "Epoch [1/5], Step [28950/34168], Loss: 5.3623\n",
      "Epoch [1/5], Step [29025/34168], Loss: 5.4020\n",
      "Epoch [1/5], Step [29100/34168], Loss: 5.6384\n",
      "Epoch [1/5], Step [29175/34168], Loss: 5.3745\n",
      "Epoch [1/5], Step [29250/34168], Loss: 5.4336\n",
      "Epoch [1/5], Step [29325/34168], Loss: 5.4582\n",
      "Epoch [1/5], Step [29400/34168], Loss: 5.4232\n",
      "Epoch [1/5], Step [29475/34168], Loss: 5.3961\n",
      "Epoch [1/5], Step [29550/34168], Loss: 5.3139\n",
      "Epoch [1/5], Step [29625/34168], Loss: 5.3562\n",
      "Epoch [1/5], Step [29700/34168], Loss: 5.3101\n",
      "Epoch [1/5], Step [29775/34168], Loss: 5.3648\n",
      "Epoch [1/5], Step [29850/34168], Loss: 5.2822\n",
      "Epoch [1/5], Step [29925/34168], Loss: 5.3971\n",
      "Epoch [1/5], Step [30000/34168], Loss: 5.3055\n",
      "Validation perplexity: 171.64497239218136\n",
      "Epoch [1/5], Step [30075/34168], Loss: 5.2783\n",
      "Epoch [1/5], Step [30150/34168], Loss: 5.3814\n",
      "Epoch [1/5], Step [30225/34168], Loss: 5.3515\n",
      "Epoch [1/5], Step [30300/34168], Loss: 5.4209\n",
      "Epoch [1/5], Step [30375/34168], Loss: 5.2847\n",
      "Epoch [1/5], Step [30450/34168], Loss: 5.3245\n",
      "Epoch [1/5], Step [30525/34168], Loss: 5.4012\n",
      "Epoch [1/5], Step [30600/34168], Loss: 5.3695\n",
      "Epoch [1/5], Step [30675/34168], Loss: 5.3960\n",
      "Epoch [1/5], Step [30750/34168], Loss: 5.3661\n",
      "Epoch [1/5], Step [30825/34168], Loss: 5.3899\n",
      "Epoch [1/5], Step [30900/34168], Loss: 5.2130\n",
      "Epoch [1/5], Step [30975/34168], Loss: 5.2429\n",
      "Epoch [1/5], Step [31050/34168], Loss: 5.2944\n",
      "Epoch [1/5], Step [31125/34168], Loss: 5.2004\n",
      "Epoch [1/5], Step [31200/34168], Loss: 5.3439\n",
      "Epoch [1/5], Step [31275/34168], Loss: 5.3854\n",
      "Epoch [1/5], Step [31350/34168], Loss: 5.3072\n",
      "Epoch [1/5], Step [31425/34168], Loss: 5.3673\n",
      "Epoch [1/5], Step [31500/34168], Loss: 5.4903\n",
      "Epoch [1/5], Step [31575/34168], Loss: 5.2468\n",
      "Epoch [1/5], Step [31650/34168], Loss: 5.3640\n",
      "Epoch [1/5], Step [31725/34168], Loss: 5.4872\n",
      "Epoch [1/5], Step [31800/34168], Loss: 5.4155\n",
      "Epoch [1/5], Step [31875/34168], Loss: 5.4342\n",
      "Epoch [1/5], Step [31950/34168], Loss: 5.3810\n",
      "Epoch [1/5], Step [32025/34168], Loss: 5.4671\n",
      "Epoch [1/5], Step [32100/34168], Loss: 5.2542\n",
      "Epoch [1/5], Step [32175/34168], Loss: 5.3643\n",
      "Epoch [1/5], Step [32250/34168], Loss: 5.2888\n",
      "Epoch [1/5], Step [32325/34168], Loss: 5.3066\n",
      "Epoch [1/5], Step [32400/34168], Loss: 5.2043\n",
      "Epoch [1/5], Step [32475/34168], Loss: 5.2056\n",
      "Epoch [1/5], Step [32550/34168], Loss: 5.3686\n",
      "Epoch [1/5], Step [32625/34168], Loss: 5.4146\n",
      "Epoch [1/5], Step [32700/34168], Loss: 5.3579\n",
      "Epoch [1/5], Step [32775/34168], Loss: 5.3603\n",
      "Epoch [1/5], Step [32850/34168], Loss: 5.2043\n",
      "Epoch [1/5], Step [32925/34168], Loss: 5.3992\n",
      "Epoch [1/5], Step [33000/34168], Loss: 5.3490\n",
      "Epoch [1/5], Step [33075/34168], Loss: 5.3166\n",
      "Epoch [1/5], Step [33150/34168], Loss: 5.1784\n",
      "Epoch [1/5], Step [33225/34168], Loss: 5.3528\n",
      "Epoch [1/5], Step [33300/34168], Loss: 5.3389\n",
      "Epoch [1/5], Step [33375/34168], Loss: 5.2135\n",
      "Epoch [1/5], Step [33450/34168], Loss: 5.2868\n",
      "Epoch [1/5], Step [33525/34168], Loss: 5.3393\n",
      "Epoch [1/5], Step [33600/34168], Loss: 5.3038\n",
      "Epoch [1/5], Step [33675/34168], Loss: 5.2840\n",
      "Epoch [1/5], Step [33750/34168], Loss: 5.3118\n",
      "Epoch [1/5], Step [33825/34168], Loss: 5.2840\n",
      "Epoch [1/5], Step [33900/34168], Loss: 5.3553\n",
      "Epoch [1/5], Step [33975/34168], Loss: 5.3575\n",
      "Epoch [1/5], Step [34050/34168], Loss: 5.3698\n",
      "Epoch [1/5], Step [34125/34168], Loss: 5.2921\n",
      "Epoch [1/5] Average Loss: 5.6425, Perplexity: 282.17\n",
      "Epoch [2/5], Step [0/34168], Loss: 5.2595\n",
      "Validation perplexity: 165.42873159243746\n",
      "Epoch [2/5], Step [75/34168], Loss: 5.1986\n",
      "Epoch [2/5], Step [150/34168], Loss: 5.3192\n",
      "Epoch [2/5], Step [225/34168], Loss: 5.3841\n",
      "Epoch [2/5], Step [300/34168], Loss: 5.2510\n",
      "Epoch [2/5], Step [375/34168], Loss: 5.3476\n",
      "Epoch [2/5], Step [450/34168], Loss: 5.4473\n",
      "Epoch [2/5], Step [525/34168], Loss: 5.3385\n",
      "Epoch [2/5], Step [600/34168], Loss: 5.5258\n",
      "Epoch [2/5], Step [675/34168], Loss: 5.3307\n",
      "Epoch [2/5], Step [750/34168], Loss: 5.1963\n",
      "Epoch [2/5], Step [825/34168], Loss: 5.4774\n",
      "Epoch [2/5], Step [900/34168], Loss: 5.3264\n",
      "Epoch [2/5], Step [975/34168], Loss: 5.2150\n",
      "Epoch [2/5], Step [1050/34168], Loss: 5.1063\n",
      "Epoch [2/5], Step [1125/34168], Loss: 5.3564\n",
      "Epoch [2/5], Step [1200/34168], Loss: 5.1439\n",
      "Epoch [2/5], Step [1275/34168], Loss: 5.3425\n",
      "Epoch [2/5], Step [1350/34168], Loss: 5.3244\n",
      "Epoch [2/5], Step [1425/34168], Loss: 5.3127\n",
      "Epoch [2/5], Step [1500/34168], Loss: 5.3516\n",
      "Epoch [2/5], Step [1575/34168], Loss: 5.4519\n",
      "Epoch [2/5], Step [1650/34168], Loss: 5.4347\n",
      "Epoch [2/5], Step [1725/34168], Loss: 5.3571\n",
      "Epoch [2/5], Step [1800/34168], Loss: 5.2169\n",
      "Epoch [2/5], Step [1875/34168], Loss: 5.2115\n",
      "Epoch [2/5], Step [1950/34168], Loss: 5.3770\n",
      "Epoch [2/5], Step [2025/34168], Loss: 5.1798\n",
      "Epoch [2/5], Step [2100/34168], Loss: 5.3277\n",
      "Epoch [2/5], Step [2175/34168], Loss: 5.4188\n",
      "Epoch [2/5], Step [2250/34168], Loss: 5.2811\n",
      "Epoch [2/5], Step [2325/34168], Loss: 5.2894\n",
      "Epoch [2/5], Step [2400/34168], Loss: 5.3489\n",
      "Epoch [2/5], Step [2475/34168], Loss: 5.3444\n",
      "Epoch [2/5], Step [2550/34168], Loss: 5.3616\n",
      "Epoch [2/5], Step [2625/34168], Loss: 5.4920\n",
      "Epoch [2/5], Step [2700/34168], Loss: 5.2613\n",
      "Epoch [2/5], Step [2775/34168], Loss: 5.3335\n",
      "Epoch [2/5], Step [2850/34168], Loss: 5.2625\n",
      "Epoch [2/5], Step [2925/34168], Loss: 5.1772\n",
      "Epoch [2/5], Step [3000/34168], Loss: 5.3508\n",
      "Epoch [2/5], Step [3075/34168], Loss: 5.3826\n",
      "Epoch [2/5], Step [3150/34168], Loss: 5.3107\n",
      "Epoch [2/5], Step [3225/34168], Loss: 5.2241\n",
      "Epoch [2/5], Step [3300/34168], Loss: 5.3119\n",
      "Epoch [2/5], Step [3375/34168], Loss: 5.3183\n",
      "Epoch [2/5], Step [3450/34168], Loss: 5.2793\n",
      "Epoch [2/5], Step [3525/34168], Loss: 5.4085\n",
      "Epoch [2/5], Step [3600/34168], Loss: 5.2547\n",
      "Epoch [2/5], Step [3675/34168], Loss: 5.2926\n",
      "Epoch [2/5], Step [3750/34168], Loss: 5.3493\n",
      "Epoch [2/5], Step [3825/34168], Loss: 5.3757\n",
      "Epoch [2/5], Step [3900/34168], Loss: 5.3898\n",
      "Epoch [2/5], Step [3975/34168], Loss: 5.2471\n",
      "Epoch [2/5], Step [4050/34168], Loss: 5.5018\n",
      "Epoch [2/5], Step [4125/34168], Loss: 5.2033\n",
      "Epoch [2/5], Step [4200/34168], Loss: 5.2568\n",
      "Epoch [2/5], Step [4275/34168], Loss: 5.3776\n",
      "Epoch [2/5], Step [4350/34168], Loss: 5.2830\n",
      "Epoch [2/5], Step [4425/34168], Loss: 5.4723\n",
      "Epoch [2/5], Step [4500/34168], Loss: 5.2600\n",
      "Epoch [2/5], Step [4575/34168], Loss: 5.3697\n",
      "Epoch [2/5], Step [4650/34168], Loss: 5.2876\n",
      "Epoch [2/5], Step [4725/34168], Loss: 5.2708\n",
      "Epoch [2/5], Step [4800/34168], Loss: 5.3312\n",
      "Epoch [2/5], Step [4875/34168], Loss: 5.2280\n",
      "Epoch [2/5], Step [4950/34168], Loss: 5.2462\n",
      "Epoch [2/5], Step [5025/34168], Loss: 5.2745\n",
      "Epoch [2/5], Step [5100/34168], Loss: 5.3333\n",
      "Epoch [2/5], Step [5175/34168], Loss: 5.4571\n",
      "Epoch [2/5], Step [5250/34168], Loss: 5.0132\n",
      "Epoch [2/5], Step [5325/34168], Loss: 5.2993\n",
      "Epoch [2/5], Step [5400/34168], Loss: 5.2892\n",
      "Epoch [2/5], Step [5475/34168], Loss: 5.3011\n",
      "Epoch [2/5], Step [5550/34168], Loss: 5.0594\n",
      "Epoch [2/5], Step [5625/34168], Loss: 5.2070\n",
      "Epoch [2/5], Step [5700/34168], Loss: 5.2903\n",
      "Epoch [2/5], Step [5775/34168], Loss: 5.2526\n",
      "Epoch [2/5], Step [5850/34168], Loss: 5.1814\n",
      "Epoch [2/5], Step [5925/34168], Loss: 5.1940\n",
      "Epoch [2/5], Step [6000/34168], Loss: 5.2266\n",
      "Epoch [2/5], Step [6075/34168], Loss: 5.3254\n",
      "Epoch [2/5], Step [6150/34168], Loss: 5.2050\n",
      "Epoch [2/5], Step [6225/34168], Loss: 5.2231\n",
      "Epoch [2/5], Step [6300/34168], Loss: 5.3584\n",
      "Epoch [2/5], Step [6375/34168], Loss: 5.3809\n",
      "Epoch [2/5], Step [6450/34168], Loss: 5.2725\n",
      "Epoch [2/5], Step [6525/34168], Loss: 5.2093\n",
      "Epoch [2/5], Step [6600/34168], Loss: 5.2907\n",
      "Epoch [2/5], Step [6675/34168], Loss: 5.2356\n",
      "Epoch [2/5], Step [6750/34168], Loss: 5.2279\n",
      "Epoch [2/5], Step [6825/34168], Loss: 5.2694\n",
      "Epoch [2/5], Step [6900/34168], Loss: 5.3821\n",
      "Epoch [2/5], Step [6975/34168], Loss: 5.0467\n",
      "Epoch [2/5], Step [7050/34168], Loss: 5.2056\n",
      "Epoch [2/5], Step [7125/34168], Loss: 5.2275\n",
      "Epoch [2/5], Step [7200/34168], Loss: 5.2830\n",
      "Epoch [2/5], Step [7275/34168], Loss: 5.3120\n",
      "Epoch [2/5], Step [7350/34168], Loss: 5.2350\n",
      "Epoch [2/5], Step [7425/34168], Loss: 5.3504\n",
      "Epoch [2/5], Step [7500/34168], Loss: 5.2858\n",
      "Epoch [2/5], Step [7575/34168], Loss: 5.2082\n",
      "Epoch [2/5], Step [7650/34168], Loss: 5.2790\n",
      "Epoch [2/5], Step [7725/34168], Loss: 5.1931\n",
      "Epoch [2/5], Step [7800/34168], Loss: 5.4898\n",
      "Epoch [2/5], Step [7875/34168], Loss: 5.2055\n",
      "Epoch [2/5], Step [7950/34168], Loss: 5.3144\n",
      "Epoch [2/5], Step [8025/34168], Loss: 5.2359\n",
      "Epoch [2/5], Step [8100/34168], Loss: 5.2806\n",
      "Epoch [2/5], Step [8175/34168], Loss: 5.3018\n",
      "Epoch [2/5], Step [8250/34168], Loss: 5.3725\n",
      "Epoch [2/5], Step [8325/34168], Loss: 5.2545\n",
      "Epoch [2/5], Step [8400/34168], Loss: 5.2650\n",
      "Epoch [2/5], Step [8475/34168], Loss: 5.2529\n",
      "Epoch [2/5], Step [8550/34168], Loss: 5.2451\n",
      "Epoch [2/5], Step [8625/34168], Loss: 5.3351\n",
      "Epoch [2/5], Step [8700/34168], Loss: 5.2471\n",
      "Epoch [2/5], Step [8775/34168], Loss: 5.2578\n",
      "Epoch [2/5], Step [8850/34168], Loss: 5.3073\n",
      "Epoch [2/5], Step [8925/34168], Loss: 5.2646\n",
      "Epoch [2/5], Step [9000/34168], Loss: 5.1909\n",
      "Epoch [2/5], Step [9075/34168], Loss: 5.1790\n",
      "Epoch [2/5], Step [9150/34168], Loss: 5.2793\n",
      "Epoch [2/5], Step [9225/34168], Loss: 5.2362\n",
      "Epoch [2/5], Step [9300/34168], Loss: 5.0712\n",
      "Epoch [2/5], Step [9375/34168], Loss: 5.4551\n",
      "Epoch [2/5], Step [9450/34168], Loss: 5.3497\n",
      "Epoch [2/5], Step [9525/34168], Loss: 5.2036\n",
      "Epoch [2/5], Step [9600/34168], Loss: 5.3168\n",
      "Epoch [2/5], Step [9675/34168], Loss: 5.2141\n",
      "Epoch [2/5], Step [9750/34168], Loss: 5.2861\n",
      "Epoch [2/5], Step [9825/34168], Loss: 5.2240\n",
      "Epoch [2/5], Step [9900/34168], Loss: 5.3731\n",
      "Epoch [2/5], Step [9975/34168], Loss: 5.2941\n",
      "Validation perplexity: 154.66313583383766\n",
      "Epoch [2/5], Step [10050/34168], Loss: 5.4075\n",
      "Epoch [2/5], Step [10125/34168], Loss: 5.3402\n",
      "Epoch [2/5], Step [10200/34168], Loss: 5.3405\n",
      "Epoch [2/5], Step [10275/34168], Loss: 5.3575\n",
      "Epoch [2/5], Step [10350/34168], Loss: 5.3404\n",
      "Epoch [2/5], Step [10425/34168], Loss: 5.1816\n",
      "Epoch [2/5], Step [10500/34168], Loss: 5.2791\n",
      "Epoch [2/5], Step [10575/34168], Loss: 5.2477\n",
      "Epoch [2/5], Step [10650/34168], Loss: 5.2611\n",
      "Epoch [2/5], Step [10725/34168], Loss: 5.3147\n",
      "Epoch [2/5], Step [10800/34168], Loss: 5.3390\n",
      "Epoch [2/5], Step [10875/34168], Loss: 5.2286\n",
      "Epoch [2/5], Step [10950/34168], Loss: 5.3489\n",
      "Epoch [2/5], Step [11025/34168], Loss: 5.2841\n",
      "Epoch [2/5], Step [11100/34168], Loss: 5.3045\n",
      "Epoch [2/5], Step [11175/34168], Loss: 5.0936\n",
      "Epoch [2/5], Step [11250/34168], Loss: 5.4209\n",
      "Epoch [2/5], Step [11325/34168], Loss: 5.0465\n",
      "Epoch [2/5], Step [11400/34168], Loss: 5.2525\n",
      "Epoch [2/5], Step [11475/34168], Loss: 5.2012\n",
      "Epoch [2/5], Step [11550/34168], Loss: 5.3741\n",
      "Epoch [2/5], Step [11625/34168], Loss: 5.3765\n",
      "Epoch [2/5], Step [11700/34168], Loss: 5.3013\n",
      "Epoch [2/5], Step [11775/34168], Loss: 5.1334\n",
      "Epoch [2/5], Step [11850/34168], Loss: 5.2010\n",
      "Epoch [2/5], Step [11925/34168], Loss: 5.1020\n",
      "Epoch [2/5], Step [12000/34168], Loss: 5.3215\n",
      "Epoch [2/5], Step [12075/34168], Loss: 5.3339\n",
      "Epoch [2/5], Step [12150/34168], Loss: 5.1877\n",
      "Epoch [2/5], Step [12225/34168], Loss: 5.2836\n",
      "Epoch [2/5], Step [12300/34168], Loss: 5.1857\n",
      "Epoch [2/5], Step [12375/34168], Loss: 5.1151\n",
      "Epoch [2/5], Step [12450/34168], Loss: 5.2482\n",
      "Epoch [2/5], Step [12525/34168], Loss: 5.2217\n",
      "Epoch [2/5], Step [12600/34168], Loss: 5.1682\n",
      "Epoch [2/5], Step [12675/34168], Loss: 5.1680\n",
      "Epoch [2/5], Step [12750/34168], Loss: 5.1330\n",
      "Epoch [2/5], Step [12825/34168], Loss: 5.3359\n",
      "Epoch [2/5], Step [12900/34168], Loss: 5.2522\n",
      "Epoch [2/5], Step [12975/34168], Loss: 5.2063\n",
      "Epoch [2/5], Step [13050/34168], Loss: 5.2608\n",
      "Epoch [2/5], Step [13125/34168], Loss: 5.3183\n",
      "Epoch [2/5], Step [13200/34168], Loss: 5.1342\n",
      "Epoch [2/5], Step [13275/34168], Loss: 5.1960\n",
      "Epoch [2/5], Step [13350/34168], Loss: 5.1979\n",
      "Epoch [2/5], Step [13425/34168], Loss: 5.1601\n",
      "Epoch [2/5], Step [13500/34168], Loss: 5.2617\n",
      "Epoch [2/5], Step [13575/34168], Loss: 5.3261\n",
      "Epoch [2/5], Step [13650/34168], Loss: 5.3461\n",
      "Epoch [2/5], Step [13725/34168], Loss: 5.0939\n",
      "Epoch [2/5], Step [13800/34168], Loss: 5.4089\n",
      "Epoch [2/5], Step [13875/34168], Loss: 5.2740\n",
      "Epoch [2/5], Step [13950/34168], Loss: 5.2442\n",
      "Epoch [2/5], Step [14025/34168], Loss: 5.3690\n",
      "Epoch [2/5], Step [14100/34168], Loss: 5.2697\n",
      "Epoch [2/5], Step [14175/34168], Loss: 5.0361\n",
      "Epoch [2/5], Step [14250/34168], Loss: 5.2217\n",
      "Epoch [2/5], Step [14325/34168], Loss: 5.2407\n",
      "Epoch [2/5], Step [14400/34168], Loss: 5.2383\n",
      "Epoch [2/5], Step [14475/34168], Loss: 5.0103\n",
      "Epoch [2/5], Step [14550/34168], Loss: 5.3444\n",
      "Epoch [2/5], Step [14625/34168], Loss: 5.1688\n",
      "Epoch [2/5], Step [14700/34168], Loss: 5.0913\n",
      "Epoch [2/5], Step [14775/34168], Loss: 5.2568\n",
      "Epoch [2/5], Step [14850/34168], Loss: 5.1858\n",
      "Epoch [2/5], Step [14925/34168], Loss: 5.3700\n",
      "Epoch [2/5], Step [15000/34168], Loss: 5.3482\n",
      "Epoch [2/5], Step [15075/34168], Loss: 5.1491\n",
      "Epoch [2/5], Step [15150/34168], Loss: 5.2434\n",
      "Epoch [2/5], Step [15225/34168], Loss: 5.1472\n",
      "Epoch [2/5], Step [15300/34168], Loss: 5.2515\n",
      "Epoch [2/5], Step [15375/34168], Loss: 5.1016\n",
      "Epoch [2/5], Step [15450/34168], Loss: 5.1676\n",
      "Epoch [2/5], Step [15525/34168], Loss: 5.2511\n",
      "Epoch [2/5], Step [15600/34168], Loss: 5.2991\n",
      "Epoch [2/5], Step [15675/34168], Loss: 5.2479\n",
      "Epoch [2/5], Step [15750/34168], Loss: 5.1638\n",
      "Epoch [2/5], Step [15825/34168], Loss: 5.3470\n",
      "Epoch [2/5], Step [15900/34168], Loss: 5.2360\n",
      "Epoch [2/5], Step [15975/34168], Loss: 5.3057\n",
      "Epoch [2/5], Step [16050/34168], Loss: 5.3809\n",
      "Epoch [2/5], Step [16125/34168], Loss: 5.2694\n",
      "Epoch [2/5], Step [16200/34168], Loss: 5.2135\n",
      "Epoch [2/5], Step [16275/34168], Loss: 5.1888\n",
      "Epoch [2/5], Step [16350/34168], Loss: 5.2753\n",
      "Epoch [2/5], Step [16425/34168], Loss: 5.3707\n",
      "Epoch [2/5], Step [16500/34168], Loss: 5.1863\n",
      "Epoch [2/5], Step [16575/34168], Loss: 5.2861\n",
      "Epoch [2/5], Step [16650/34168], Loss: 5.2069\n",
      "Epoch [2/5], Step [16725/34168], Loss: 5.2821\n",
      "Epoch [2/5], Step [16800/34168], Loss: 5.3312\n",
      "Epoch [2/5], Step [16875/34168], Loss: 5.2887\n",
      "Epoch [2/5], Step [16950/34168], Loss: 5.2907\n",
      "Epoch [2/5], Step [17025/34168], Loss: 5.2686\n",
      "Epoch [2/5], Step [17100/34168], Loss: 5.2547\n",
      "Epoch [2/5], Step [17175/34168], Loss: 5.2331\n",
      "Epoch [2/5], Step [17250/34168], Loss: 5.1247\n",
      "Epoch [2/5], Step [17325/34168], Loss: 5.2259\n",
      "Epoch [2/5], Step [17400/34168], Loss: 5.2591\n",
      "Epoch [2/5], Step [17475/34168], Loss: 5.3883\n",
      "Epoch [2/5], Step [17550/34168], Loss: 5.2431\n",
      "Epoch [2/5], Step [17625/34168], Loss: 5.3763\n",
      "Epoch [2/5], Step [17700/34168], Loss: 5.2399\n",
      "Epoch [2/5], Step [17775/34168], Loss: 5.2481\n",
      "Epoch [2/5], Step [17850/34168], Loss: 5.2665\n",
      "Epoch [2/5], Step [17925/34168], Loss: 5.1329\n",
      "Epoch [2/5], Step [18000/34168], Loss: 5.2553\n",
      "Epoch [2/5], Step [18075/34168], Loss: 5.2802\n",
      "Epoch [2/5], Step [18150/34168], Loss: 5.2752\n",
      "Epoch [2/5], Step [18225/34168], Loss: 5.3137\n",
      "Epoch [2/5], Step [18300/34168], Loss: 5.2483\n",
      "Epoch [2/5], Step [18375/34168], Loss: 5.3022\n",
      "Epoch [2/5], Step [18450/34168], Loss: 5.2916\n",
      "Epoch [2/5], Step [18525/34168], Loss: 5.0590\n",
      "Epoch [2/5], Step [18600/34168], Loss: 5.1887\n",
      "Epoch [2/5], Step [18675/34168], Loss: 5.3661\n",
      "Epoch [2/5], Step [18750/34168], Loss: 5.3418\n",
      "Epoch [2/5], Step [18825/34168], Loss: 5.1012\n",
      "Epoch [2/5], Step [18900/34168], Loss: 5.3316\n",
      "Epoch [2/5], Step [18975/34168], Loss: 5.2809\n",
      "Epoch [2/5], Step [19050/34168], Loss: 5.4219\n",
      "Epoch [2/5], Step [19125/34168], Loss: 5.2223\n",
      "Epoch [2/5], Step [19200/34168], Loss: 5.0853\n",
      "Epoch [2/5], Step [19275/34168], Loss: 5.2287\n",
      "Epoch [2/5], Step [19350/34168], Loss: 5.2096\n",
      "Epoch [2/5], Step [19425/34168], Loss: 5.3392\n",
      "Epoch [2/5], Step [19500/34168], Loss: 5.1708\n",
      "Epoch [2/5], Step [19575/34168], Loss: 5.1132\n",
      "Epoch [2/5], Step [19650/34168], Loss: 5.2100\n",
      "Epoch [2/5], Step [19725/34168], Loss: 5.2271\n",
      "Epoch [2/5], Step [19800/34168], Loss: 5.2357\n",
      "Epoch [2/5], Step [19875/34168], Loss: 5.3055\n",
      "Epoch [2/5], Step [19950/34168], Loss: 5.1315\n",
      "Validation perplexity: 147.76683844124932\n",
      "Epoch [2/5], Step [20025/34168], Loss: 5.2973\n",
      "Epoch [2/5], Step [20100/34168], Loss: 5.2266\n",
      "Epoch [2/5], Step [20175/34168], Loss: 5.1733\n",
      "Epoch [2/5], Step [20250/34168], Loss: 5.1507\n",
      "Epoch [2/5], Step [20325/34168], Loss: 5.2061\n",
      "Epoch [2/5], Step [20400/34168], Loss: 5.1646\n",
      "Epoch [2/5], Step [20475/34168], Loss: 5.2706\n",
      "Epoch [2/5], Step [20550/34168], Loss: 5.3345\n",
      "Epoch [2/5], Step [20625/34168], Loss: 5.3554\n",
      "Epoch [2/5], Step [20700/34168], Loss: 5.1959\n",
      "Epoch [2/5], Step [20775/34168], Loss: 5.1881\n",
      "Epoch [2/5], Step [20850/34168], Loss: 5.0498\n",
      "Epoch [2/5], Step [20925/34168], Loss: 5.1535\n",
      "Epoch [2/5], Step [21000/34168], Loss: 5.1364\n",
      "Epoch [2/5], Step [21075/34168], Loss: 5.3311\n",
      "Epoch [2/5], Step [21150/34168], Loss: 5.2415\n",
      "Epoch [2/5], Step [21225/34168], Loss: 5.3240\n",
      "Epoch [2/5], Step [21300/34168], Loss: 5.2539\n",
      "Epoch [2/5], Step [21375/34168], Loss: 5.1593\n",
      "Epoch [2/5], Step [21450/34168], Loss: 5.1853\n",
      "Epoch [2/5], Step [21525/34168], Loss: 5.2713\n",
      "Epoch [2/5], Step [21600/34168], Loss: 5.1370\n",
      "Epoch [2/5], Step [21675/34168], Loss: 5.2298\n",
      "Epoch [2/5], Step [21750/34168], Loss: 5.1752\n",
      "Epoch [2/5], Step [21825/34168], Loss: 5.1457\n",
      "Epoch [2/5], Step [21900/34168], Loss: 5.2750\n",
      "Epoch [2/5], Step [21975/34168], Loss: 5.2098\n",
      "Epoch [2/5], Step [22050/34168], Loss: 5.1377\n",
      "Epoch [2/5], Step [22125/34168], Loss: 5.3145\n",
      "Epoch [2/5], Step [22200/34168], Loss: 5.3371\n",
      "Epoch [2/5], Step [22275/34168], Loss: 5.1655\n",
      "Epoch [2/5], Step [22350/34168], Loss: 5.1145\n",
      "Epoch [2/5], Step [22425/34168], Loss: 5.2269\n",
      "Epoch [2/5], Step [22500/34168], Loss: 5.3539\n",
      "Epoch [2/5], Step [22575/34168], Loss: 5.2023\n",
      "Epoch [2/5], Step [22650/34168], Loss: 5.1671\n",
      "Epoch [2/5], Step [22725/34168], Loss: 5.2590\n",
      "Epoch [2/5], Step [22800/34168], Loss: 5.1897\n",
      "Epoch [2/5], Step [22875/34168], Loss: 5.1433\n",
      "Epoch [2/5], Step [22950/34168], Loss: 5.2821\n",
      "Epoch [2/5], Step [23025/34168], Loss: 5.2025\n",
      "Epoch [2/5], Step [23100/34168], Loss: 5.3171\n",
      "Epoch [2/5], Step [23175/34168], Loss: 5.2131\n",
      "Epoch [2/5], Step [23250/34168], Loss: 5.1545\n",
      "Epoch [2/5], Step [23325/34168], Loss: 5.0472\n",
      "Epoch [2/5], Step [23400/34168], Loss: 5.3891\n",
      "Epoch [2/5], Step [23475/34168], Loss: 5.3384\n",
      "Epoch [2/5], Step [23550/34168], Loss: 5.1510\n",
      "Epoch [2/5], Step [23625/34168], Loss: 5.1231\n",
      "Epoch [2/5], Step [23700/34168], Loss: 5.2719\n",
      "Epoch [2/5], Step [23775/34168], Loss: 5.2750\n",
      "Epoch [2/5], Step [23850/34168], Loss: 5.1171\n",
      "Epoch [2/5], Step [23925/34168], Loss: 5.1146\n",
      "Epoch [2/5], Step [24000/34168], Loss: 5.1655\n",
      "Epoch [2/5], Step [24075/34168], Loss: 5.2736\n",
      "Epoch [2/5], Step [24150/34168], Loss: 5.0636\n",
      "Epoch [2/5], Step [24225/34168], Loss: 5.1736\n",
      "Epoch [2/5], Step [24300/34168], Loss: 5.1650\n",
      "Epoch [2/5], Step [24375/34168], Loss: 5.3143\n",
      "Epoch [2/5], Step [24450/34168], Loss: 5.2509\n",
      "Epoch [2/5], Step [24525/34168], Loss: 5.3624\n",
      "Epoch [2/5], Step [24600/34168], Loss: 5.1253\n",
      "Epoch [2/5], Step [24675/34168], Loss: 5.2029\n",
      "Epoch [2/5], Step [24750/34168], Loss: 5.0746\n",
      "Epoch [2/5], Step [24825/34168], Loss: 5.3957\n",
      "Epoch [2/5], Step [24900/34168], Loss: 5.1133\n",
      "Epoch [2/5], Step [24975/34168], Loss: 5.2264\n",
      "Epoch [2/5], Step [25050/34168], Loss: 5.1356\n",
      "Epoch [2/5], Step [25125/34168], Loss: 5.3275\n",
      "Epoch [2/5], Step [25200/34168], Loss: 5.0640\n",
      "Epoch [2/5], Step [25275/34168], Loss: 5.2493\n",
      "Epoch [2/5], Step [25350/34168], Loss: 5.2114\n",
      "Epoch [2/5], Step [25425/34168], Loss: 5.1793\n",
      "Epoch [2/5], Step [25500/34168], Loss: 5.2251\n",
      "Epoch [2/5], Step [25575/34168], Loss: 5.2083\n",
      "Epoch [2/5], Step [25650/34168], Loss: 5.3878\n",
      "Epoch [2/5], Step [25725/34168], Loss: 5.1356\n",
      "Epoch [2/5], Step [25800/34168], Loss: 5.2373\n",
      "Epoch [2/5], Step [25875/34168], Loss: 5.3827\n",
      "Epoch [2/5], Step [25950/34168], Loss: 5.1246\n",
      "Epoch [2/5], Step [26025/34168], Loss: 5.2796\n",
      "Epoch [2/5], Step [26100/34168], Loss: 5.2143\n",
      "Epoch [2/5], Step [26175/34168], Loss: 5.2245\n",
      "Epoch [2/5], Step [26250/34168], Loss: 5.0627\n",
      "Epoch [2/5], Step [26325/34168], Loss: 5.0856\n",
      "Epoch [2/5], Step [26400/34168], Loss: 5.2619\n",
      "Epoch [2/5], Step [26475/34168], Loss: 5.2686\n",
      "Epoch [2/5], Step [26550/34168], Loss: 5.1286\n",
      "Epoch [2/5], Step [26625/34168], Loss: 5.2684\n",
      "Epoch [2/5], Step [26700/34168], Loss: 5.1694\n",
      "Epoch [2/5], Step [26775/34168], Loss: 5.2244\n",
      "Epoch [2/5], Step [26850/34168], Loss: 5.2252\n",
      "Epoch [2/5], Step [26925/34168], Loss: 5.0910\n",
      "Epoch [2/5], Step [27000/34168], Loss: 5.2924\n",
      "Epoch [2/5], Step [27075/34168], Loss: 5.2350\n",
      "Epoch [2/5], Step [27150/34168], Loss: 5.1781\n",
      "Epoch [2/5], Step [27225/34168], Loss: 5.1286\n",
      "Epoch [2/5], Step [27300/34168], Loss: 5.3077\n",
      "Epoch [2/5], Step [27375/34168], Loss: 5.2446\n",
      "Epoch [2/5], Step [27450/34168], Loss: 5.1342\n",
      "Epoch [2/5], Step [27525/34168], Loss: 5.3280\n",
      "Epoch [2/5], Step [27600/34168], Loss: 5.3470\n",
      "Epoch [2/5], Step [27675/34168], Loss: 5.2901\n",
      "Epoch [2/5], Step [27750/34168], Loss: 5.1526\n",
      "Epoch [2/5], Step [27825/34168], Loss: 5.3435\n",
      "Epoch [2/5], Step [27900/34168], Loss: 5.2383\n",
      "Epoch [2/5], Step [27975/34168], Loss: 5.2298\n",
      "Epoch [2/5], Step [28050/34168], Loss: 5.1753\n",
      "Epoch [2/5], Step [28125/34168], Loss: 5.1655\n",
      "Epoch [2/5], Step [28200/34168], Loss: 4.9833\n",
      "Epoch [2/5], Step [28275/34168], Loss: 5.1472\n",
      "Epoch [2/5], Step [28350/34168], Loss: 5.1685\n",
      "Epoch [2/5], Step [28425/34168], Loss: 5.2856\n",
      "Epoch [2/5], Step [28500/34168], Loss: 5.2497\n",
      "Epoch [2/5], Step [28575/34168], Loss: 5.0577\n",
      "Epoch [2/5], Step [28650/34168], Loss: 5.1858\n",
      "Epoch [2/5], Step [28725/34168], Loss: 5.2319\n",
      "Epoch [2/5], Step [28800/34168], Loss: 5.1561\n",
      "Epoch [2/5], Step [28875/34168], Loss: 5.2269\n",
      "Epoch [2/5], Step [28950/34168], Loss: 5.0934\n",
      "Epoch [2/5], Step [29025/34168], Loss: 5.1689\n",
      "Epoch [2/5], Step [29100/34168], Loss: 5.3172\n",
      "Epoch [2/5], Step [29175/34168], Loss: 5.3227\n",
      "Epoch [2/5], Step [29250/34168], Loss: 5.1342\n",
      "Epoch [2/5], Step [29325/34168], Loss: 5.3050\n",
      "Epoch [2/5], Step [29400/34168], Loss: 5.2974\n",
      "Epoch [2/5], Step [29475/34168], Loss: 5.4946\n",
      "Epoch [2/5], Step [29550/34168], Loss: 5.3055\n",
      "Epoch [2/5], Step [29625/34168], Loss: 5.0033\n",
      "Epoch [2/5], Step [29700/34168], Loss: 5.1335\n",
      "Epoch [2/5], Step [29775/34168], Loss: 5.1918\n",
      "Epoch [2/5], Step [29850/34168], Loss: 5.1635\n",
      "Epoch [2/5], Step [29925/34168], Loss: 5.2026\n",
      "Epoch [2/5], Step [30000/34168], Loss: 5.2786\n",
      "Validation perplexity: 142.30137394036737\n",
      "Epoch [2/5], Step [30075/34168], Loss: 5.2370\n",
      "Epoch [2/5], Step [30150/34168], Loss: 5.2511\n",
      "Epoch [2/5], Step [30225/34168], Loss: 5.1198\n",
      "Epoch [2/5], Step [30300/34168], Loss: 5.1939\n",
      "Epoch [2/5], Step [30375/34168], Loss: 5.2697\n",
      "Epoch [2/5], Step [30450/34168], Loss: 5.2035\n",
      "Epoch [2/5], Step [30525/34168], Loss: 5.1444\n",
      "Epoch [2/5], Step [30600/34168], Loss: 5.2113\n",
      "Epoch [2/5], Step [30675/34168], Loss: 5.4334\n",
      "Epoch [2/5], Step [30750/34168], Loss: 5.1103\n",
      "Epoch [2/5], Step [30825/34168], Loss: 5.2441\n",
      "Epoch [2/5], Step [30900/34168], Loss: 5.2577\n",
      "Epoch [2/5], Step [30975/34168], Loss: 5.2312\n",
      "Epoch [2/5], Step [31050/34168], Loss: 5.2342\n",
      "Epoch [2/5], Step [31125/34168], Loss: 5.1959\n",
      "Epoch [2/5], Step [31200/34168], Loss: 5.1221\n",
      "Epoch [2/5], Step [31275/34168], Loss: 5.1811\n",
      "Epoch [2/5], Step [31350/34168], Loss: 5.2062\n",
      "Epoch [2/5], Step [31425/34168], Loss: 5.0461\n",
      "Epoch [2/5], Step [31500/34168], Loss: 5.2297\n",
      "Epoch [2/5], Step [31575/34168], Loss: 5.1363\n",
      "Epoch [2/5], Step [31650/34168], Loss: 5.2881\n",
      "Epoch [2/5], Step [31725/34168], Loss: 5.2522\n",
      "Epoch [2/5], Step [31800/34168], Loss: 5.1025\n",
      "Epoch [2/5], Step [31875/34168], Loss: 5.1700\n",
      "Epoch [2/5], Step [31950/34168], Loss: 5.1046\n",
      "Epoch [2/5], Step [32025/34168], Loss: 5.1080\n",
      "Epoch [2/5], Step [32100/34168], Loss: 5.3003\n",
      "Epoch [2/5], Step [32175/34168], Loss: 5.1516\n",
      "Epoch [2/5], Step [32250/34168], Loss: 5.1773\n",
      "Epoch [2/5], Step [32325/34168], Loss: 5.2133\n",
      "Epoch [2/5], Step [32400/34168], Loss: 5.1451\n",
      "Epoch [2/5], Step [32475/34168], Loss: 5.2974\n",
      "Epoch [2/5], Step [32550/34168], Loss: 5.1985\n",
      "Epoch [2/5], Step [32625/34168], Loss: 5.1079\n",
      "Epoch [2/5], Step [32700/34168], Loss: 5.2342\n",
      "Epoch [2/5], Step [32775/34168], Loss: 5.1095\n",
      "Epoch [2/5], Step [32850/34168], Loss: 5.2824\n",
      "Epoch [2/5], Step [32925/34168], Loss: 5.2617\n",
      "Epoch [2/5], Step [33000/34168], Loss: 5.2357\n",
      "Epoch [2/5], Step [33075/34168], Loss: 5.1637\n",
      "Epoch [2/5], Step [33150/34168], Loss: 5.0231\n",
      "Epoch [2/5], Step [33225/34168], Loss: 5.1351\n",
      "Epoch [2/5], Step [33300/34168], Loss: 5.1080\n",
      "Epoch [2/5], Step [33375/34168], Loss: 5.2604\n",
      "Epoch [2/5], Step [33450/34168], Loss: 5.1510\n",
      "Epoch [2/5], Step [33525/34168], Loss: 5.0784\n",
      "Epoch [2/5], Step [33600/34168], Loss: 5.0948\n",
      "Epoch [2/5], Step [33675/34168], Loss: 5.2566\n",
      "Epoch [2/5], Step [33750/34168], Loss: 5.2100\n",
      "Epoch [2/5], Step [33825/34168], Loss: 5.2716\n",
      "Epoch [2/5], Step [33900/34168], Loss: 5.1220\n",
      "Epoch [2/5], Step [33975/34168], Loss: 5.1453\n",
      "Epoch [2/5], Step [34050/34168], Loss: 5.2801\n",
      "Epoch [2/5], Step [34125/34168], Loss: 5.0383\n",
      "Epoch [2/5] Average Loss: 5.2451, Perplexity: 189.64\n",
      "Epoch [3/5], Step [0/34168], Loss: 5.1975\n",
      "Validation perplexity: 140.93794202370046\n",
      "Epoch [3/5], Step [75/34168], Loss: 5.2020\n",
      "Epoch [3/5], Step [150/34168], Loss: 5.1261\n",
      "Epoch [3/5], Step [225/34168], Loss: 5.2213\n",
      "Epoch [3/5], Step [300/34168], Loss: 5.0542\n",
      "Epoch [3/5], Step [375/34168], Loss: 5.2079\n",
      "Epoch [3/5], Step [450/34168], Loss: 5.2671\n",
      "Epoch [3/5], Step [525/34168], Loss: 4.9838\n",
      "Epoch [3/5], Step [600/34168], Loss: 5.1594\n",
      "Epoch [3/5], Step [675/34168], Loss: 5.1780\n",
      "Epoch [3/5], Step [750/34168], Loss: 5.0953\n",
      "Epoch [3/5], Step [825/34168], Loss: 5.3598\n",
      "Epoch [3/5], Step [900/34168], Loss: 5.0832\n",
      "Epoch [3/5], Step [975/34168], Loss: 5.2990\n",
      "Epoch [3/5], Step [1050/34168], Loss: 5.3132\n",
      "Epoch [3/5], Step [1125/34168], Loss: 5.1037\n",
      "Epoch [3/5], Step [1200/34168], Loss: 5.0594\n",
      "Epoch [3/5], Step [1275/34168], Loss: 5.2262\n",
      "Epoch [3/5], Step [1350/34168], Loss: 5.2470\n",
      "Epoch [3/5], Step [1425/34168], Loss: 5.1372\n",
      "Epoch [3/5], Step [1500/34168], Loss: 5.2664\n",
      "Epoch [3/5], Step [1575/34168], Loss: 5.1799\n",
      "Epoch [3/5], Step [1650/34168], Loss: 5.1790\n",
      "Epoch [3/5], Step [1725/34168], Loss: 5.2037\n",
      "Epoch [3/5], Step [1800/34168], Loss: 5.0760\n",
      "Epoch [3/5], Step [1875/34168], Loss: 5.0771\n",
      "Epoch [3/5], Step [1950/34168], Loss: 5.2311\n",
      "Epoch [3/5], Step [2025/34168], Loss: 5.1968\n",
      "Epoch [3/5], Step [2100/34168], Loss: 5.2609\n",
      "Epoch [3/5], Step [2175/34168], Loss: 5.1043\n",
      "Epoch [3/5], Step [2250/34168], Loss: 5.2049\n",
      "Epoch [3/5], Step [2325/34168], Loss: 5.2037\n",
      "Epoch [3/5], Step [2400/34168], Loss: 5.1556\n",
      "Epoch [3/5], Step [2475/34168], Loss: 5.1858\n",
      "Epoch [3/5], Step [2550/34168], Loss: 5.2340\n",
      "Epoch [3/5], Step [2625/34168], Loss: 5.2553\n",
      "Epoch [3/5], Step [2700/34168], Loss: 5.0120\n",
      "Epoch [3/5], Step [2775/34168], Loss: 5.3950\n",
      "Epoch [3/5], Step [2850/34168], Loss: 5.2648\n",
      "Epoch [3/5], Step [2925/34168], Loss: 5.3429\n",
      "Epoch [3/5], Step [3000/34168], Loss: 5.2292\n",
      "Epoch [3/5], Step [3075/34168], Loss: 5.2266\n",
      "Epoch [3/5], Step [3150/34168], Loss: 5.3031\n",
      "Epoch [3/5], Step [3225/34168], Loss: 5.3200\n",
      "Epoch [3/5], Step [3300/34168], Loss: 5.0499\n",
      "Epoch [3/5], Step [3375/34168], Loss: 5.2681\n",
      "Epoch [3/5], Step [3450/34168], Loss: 5.1089\n",
      "Epoch [3/5], Step [3525/34168], Loss: 5.1252\n",
      "Epoch [3/5], Step [3600/34168], Loss: 5.2398\n",
      "Epoch [3/5], Step [3675/34168], Loss: 5.2512\n",
      "Epoch [3/5], Step [3750/34168], Loss: 5.3094\n",
      "Epoch [3/5], Step [3825/34168], Loss: 5.1014\n",
      "Epoch [3/5], Step [3900/34168], Loss: 5.1761\n",
      "Epoch [3/5], Step [3975/34168], Loss: 5.0588\n",
      "Epoch [3/5], Step [4050/34168], Loss: 5.1583\n",
      "Epoch [3/5], Step [4125/34168], Loss: 5.0936\n",
      "Epoch [3/5], Step [4200/34168], Loss: 5.2353\n",
      "Epoch [3/5], Step [4275/34168], Loss: 5.2210\n",
      "Epoch [3/5], Step [4350/34168], Loss: 5.0654\n",
      "Epoch [3/5], Step [4425/34168], Loss: 5.0664\n",
      "Epoch [3/5], Step [4500/34168], Loss: 5.4002\n",
      "Epoch [3/5], Step [4575/34168], Loss: 5.2734\n",
      "Epoch [3/5], Step [4650/34168], Loss: 5.3127\n",
      "Epoch [3/5], Step [4725/34168], Loss: 5.0966\n",
      "Epoch [3/5], Step [4800/34168], Loss: 5.1415\n",
      "Epoch [3/5], Step [4875/34168], Loss: 5.2780\n",
      "Epoch [3/5], Step [4950/34168], Loss: 5.0890\n",
      "Epoch [3/5], Step [5025/34168], Loss: 5.0556\n",
      "Epoch [3/5], Step [5100/34168], Loss: 5.1776\n",
      "Epoch [3/5], Step [5175/34168], Loss: 5.1763\n",
      "Epoch [3/5], Step [5250/34168], Loss: 5.1034\n",
      "Epoch [3/5], Step [5325/34168], Loss: 5.1795\n",
      "Epoch [3/5], Step [5400/34168], Loss: 5.2579\n",
      "Epoch [3/5], Step [5475/34168], Loss: 5.1603\n",
      "Epoch [3/5], Step [5550/34168], Loss: 5.1701\n",
      "Epoch [3/5], Step [5625/34168], Loss: 5.0817\n",
      "Epoch [3/5], Step [5700/34168], Loss: 5.1249\n",
      "Epoch [3/5], Step [5775/34168], Loss: 5.2585\n",
      "Epoch [3/5], Step [5850/34168], Loss: 5.1571\n",
      "Epoch [3/5], Step [5925/34168], Loss: 5.3032\n",
      "Epoch [3/5], Step [6000/34168], Loss: 5.1981\n",
      "Epoch [3/5], Step [6075/34168], Loss: 5.1220\n",
      "Epoch [3/5], Step [6150/34168], Loss: 5.2256\n",
      "Epoch [3/5], Step [6225/34168], Loss: 5.2324\n",
      "Epoch [3/5], Step [6300/34168], Loss: 5.0948\n",
      "Epoch [3/5], Step [6375/34168], Loss: 5.1094\n",
      "Epoch [3/5], Step [6450/34168], Loss: 5.2182\n",
      "Epoch [3/5], Step [6525/34168], Loss: 5.2366\n",
      "Epoch [3/5], Step [6600/34168], Loss: 5.2290\n",
      "Epoch [3/5], Step [6675/34168], Loss: 5.1413\n",
      "Epoch [3/5], Step [6750/34168], Loss: 5.1233\n",
      "Epoch [3/5], Step [6825/34168], Loss: 5.0507\n",
      "Epoch [3/5], Step [6900/34168], Loss: 5.1666\n",
      "Epoch [3/5], Step [6975/34168], Loss: 5.1015\n",
      "Epoch [3/5], Step [7050/34168], Loss: 5.0952\n",
      "Epoch [3/5], Step [7125/34168], Loss: 5.3398\n",
      "Epoch [3/5], Step [7200/34168], Loss: 5.2283\n",
      "Epoch [3/5], Step [7275/34168], Loss: 5.0649\n",
      "Epoch [3/5], Step [7350/34168], Loss: 5.2380\n",
      "Epoch [3/5], Step [7425/34168], Loss: 5.3104\n",
      "Epoch [3/5], Step [7500/34168], Loss: 5.1649\n",
      "Epoch [3/5], Step [7575/34168], Loss: 5.1967\n",
      "Epoch [3/5], Step [7650/34168], Loss: 5.2302\n",
      "Epoch [3/5], Step [7725/34168], Loss: 5.2154\n",
      "Epoch [3/5], Step [7800/34168], Loss: 5.0749\n",
      "Epoch [3/5], Step [7875/34168], Loss: 5.1577\n",
      "Epoch [3/5], Step [7950/34168], Loss: 5.0278\n",
      "Epoch [3/5], Step [8025/34168], Loss: 5.1842\n",
      "Epoch [3/5], Step [8100/34168], Loss: 5.1263\n",
      "Epoch [3/5], Step [8175/34168], Loss: 5.1745\n",
      "Epoch [3/5], Step [8250/34168], Loss: 5.1670\n",
      "Epoch [3/5], Step [8325/34168], Loss: 5.1323\n",
      "Epoch [3/5], Step [8400/34168], Loss: 5.2601\n",
      "Epoch [3/5], Step [8475/34168], Loss: 5.0818\n",
      "Epoch [3/5], Step [8550/34168], Loss: 5.1262\n",
      "Epoch [3/5], Step [8625/34168], Loss: 5.1730\n",
      "Epoch [3/5], Step [8700/34168], Loss: 5.2803\n",
      "Epoch [3/5], Step [8775/34168], Loss: 5.1864\n",
      "Epoch [3/5], Step [8850/34168], Loss: 5.3417\n",
      "Epoch [3/5], Step [8925/34168], Loss: 5.1268\n",
      "Epoch [3/5], Step [9000/34168], Loss: 5.0646\n",
      "Epoch [3/5], Step [9075/34168], Loss: 5.1389\n",
      "Epoch [3/5], Step [9150/34168], Loss: 5.0184\n",
      "Epoch [3/5], Step [9225/34168], Loss: 5.2349\n",
      "Epoch [3/5], Step [9300/34168], Loss: 5.2721\n",
      "Epoch [3/5], Step [9375/34168], Loss: 5.1316\n",
      "Epoch [3/5], Step [9450/34168], Loss: 5.2210\n",
      "Epoch [3/5], Step [9525/34168], Loss: 5.1936\n",
      "Epoch [3/5], Step [9600/34168], Loss: 5.2782\n",
      "Epoch [3/5], Step [9675/34168], Loss: 5.2015\n",
      "Epoch [3/5], Step [9750/34168], Loss: 5.0605\n",
      "Epoch [3/5], Step [9825/34168], Loss: 5.2259\n",
      "Epoch [3/5], Step [9900/34168], Loss: 4.9664\n",
      "Epoch [3/5], Step [9975/34168], Loss: 5.1510\n",
      "Validation perplexity: 136.6967614702794\n",
      "Epoch [3/5], Step [10050/34168], Loss: 5.3325\n",
      "Epoch [3/5], Step [10125/34168], Loss: 5.0977\n",
      "Epoch [3/5], Step [10200/34168], Loss: 4.9872\n",
      "Epoch [3/5], Step [10275/34168], Loss: 4.9601\n",
      "Epoch [3/5], Step [10350/34168], Loss: 5.2595\n",
      "Epoch [3/5], Step [10425/34168], Loss: 5.2604\n",
      "Epoch [3/5], Step [10500/34168], Loss: 5.1167\n",
      "Epoch [3/5], Step [10575/34168], Loss: 5.2070\n",
      "Epoch [3/5], Step [10650/34168], Loss: 5.0681\n",
      "Epoch [3/5], Step [10725/34168], Loss: 5.0588\n",
      "Epoch [3/5], Step [10800/34168], Loss: 4.9965\n",
      "Epoch [3/5], Step [10875/34168], Loss: 5.2981\n",
      "Epoch [3/5], Step [10950/34168], Loss: 5.2225\n",
      "Epoch [3/5], Step [11025/34168], Loss: 5.2563\n",
      "Epoch [3/5], Step [11100/34168], Loss: 5.1727\n",
      "Epoch [3/5], Step [11175/34168], Loss: 5.1494\n",
      "Epoch [3/5], Step [11250/34168], Loss: 5.1708\n",
      "Epoch [3/5], Step [11325/34168], Loss: 5.3379\n",
      "Epoch [3/5], Step [11400/34168], Loss: 5.0078\n",
      "Epoch [3/5], Step [11475/34168], Loss: 5.2008\n",
      "Epoch [3/5], Step [11550/34168], Loss: 5.1606\n",
      "Epoch [3/5], Step [11625/34168], Loss: 5.0711\n",
      "Epoch [3/5], Step [11700/34168], Loss: 5.2405\n",
      "Epoch [3/5], Step [11775/34168], Loss: 5.3164\n",
      "Epoch [3/5], Step [11850/34168], Loss: 5.1455\n",
      "Epoch [3/5], Step [11925/34168], Loss: 5.2343\n",
      "Epoch [3/5], Step [12000/34168], Loss: 5.0853\n",
      "Epoch [3/5], Step [12075/34168], Loss: 5.2590\n",
      "Epoch [3/5], Step [12150/34168], Loss: 4.9816\n",
      "Epoch [3/5], Step [12225/34168], Loss: 5.2831\n",
      "Epoch [3/5], Step [12300/34168], Loss: 5.2447\n",
      "Epoch [3/5], Step [12375/34168], Loss: 5.1598\n",
      "Epoch [3/5], Step [12450/34168], Loss: 5.1129\n",
      "Epoch [3/5], Step [12525/34168], Loss: 5.1813\n",
      "Epoch [3/5], Step [12600/34168], Loss: 5.0714\n",
      "Epoch [3/5], Step [12675/34168], Loss: 5.1464\n",
      "Epoch [3/5], Step [12750/34168], Loss: 5.0792\n",
      "Epoch [3/5], Step [12825/34168], Loss: 5.0889\n",
      "Epoch [3/5], Step [12900/34168], Loss: 5.2265\n",
      "Epoch [3/5], Step [12975/34168], Loss: 4.9064\n",
      "Epoch [3/5], Step [13050/34168], Loss: 5.2287\n",
      "Epoch [3/5], Step [13125/34168], Loss: 4.9974\n",
      "Epoch [3/5], Step [13200/34168], Loss: 5.0757\n",
      "Epoch [3/5], Step [13275/34168], Loss: 5.2376\n",
      "Epoch [3/5], Step [13350/34168], Loss: 5.0558\n",
      "Epoch [3/5], Step [13425/34168], Loss: 5.1531\n",
      "Epoch [3/5], Step [13500/34168], Loss: 5.0420\n",
      "Epoch [3/5], Step [13575/34168], Loss: 5.1780\n",
      "Epoch [3/5], Step [13650/34168], Loss: 4.9972\n",
      "Epoch [3/5], Step [13725/34168], Loss: 5.3397\n",
      "Epoch [3/5], Step [13800/34168], Loss: 5.1174\n",
      "Epoch [3/5], Step [13875/34168], Loss: 5.0958\n",
      "Epoch [3/5], Step [13950/34168], Loss: 5.2519\n",
      "Epoch [3/5], Step [14025/34168], Loss: 5.1812\n",
      "Epoch [3/5], Step [14100/34168], Loss: 5.1305\n",
      "Epoch [3/5], Step [14175/34168], Loss: 5.1320\n",
      "Epoch [3/5], Step [14250/34168], Loss: 5.0554\n",
      "Epoch [3/5], Step [14325/34168], Loss: 5.2457\n",
      "Epoch [3/5], Step [14400/34168], Loss: 5.3415\n",
      "Epoch [3/5], Step [14475/34168], Loss: 5.1184\n",
      "Epoch [3/5], Step [14550/34168], Loss: 5.1983\n",
      "Epoch [3/5], Step [14625/34168], Loss: 5.0508\n",
      "Epoch [3/5], Step [14700/34168], Loss: 5.2356\n",
      "Epoch [3/5], Step [14775/34168], Loss: 4.9678\n",
      "Epoch [3/5], Step [14850/34168], Loss: 5.0631\n",
      "Epoch [3/5], Step [14925/34168], Loss: 5.1895\n",
      "Epoch [3/5], Step [15000/34168], Loss: 5.1230\n",
      "Epoch [3/5], Step [15075/34168], Loss: 5.3265\n",
      "Epoch [3/5], Step [15150/34168], Loss: 5.1824\n",
      "Epoch [3/5], Step [15225/34168], Loss: 5.2195\n",
      "Epoch [3/5], Step [15300/34168], Loss: 5.1974\n",
      "Epoch [3/5], Step [15375/34168], Loss: 5.1581\n",
      "Epoch [3/5], Step [15450/34168], Loss: 5.1665\n",
      "Epoch [3/5], Step [15525/34168], Loss: 5.0596\n",
      "Epoch [3/5], Step [15600/34168], Loss: 5.2972\n",
      "Epoch [3/5], Step [15675/34168], Loss: 5.2116\n",
      "Epoch [3/5], Step [15750/34168], Loss: 5.2377\n",
      "Epoch [3/5], Step [15825/34168], Loss: 5.1729\n",
      "Epoch [3/5], Step [15900/34168], Loss: 5.1217\n",
      "Epoch [3/5], Step [15975/34168], Loss: 4.9526\n",
      "Epoch [3/5], Step [16050/34168], Loss: 5.1967\n",
      "Epoch [3/5], Step [16125/34168], Loss: 5.2617\n",
      "Epoch [3/5], Step [16200/34168], Loss: 5.2601\n",
      "Epoch [3/5], Step [16275/34168], Loss: 5.2416\n",
      "Epoch [3/5], Step [16350/34168], Loss: 5.1954\n",
      "Epoch [3/5], Step [16425/34168], Loss: 5.1490\n",
      "Epoch [3/5], Step [16500/34168], Loss: 5.0770\n",
      "Epoch [3/5], Step [16575/34168], Loss: 5.1489\n",
      "Epoch [3/5], Step [16650/34168], Loss: 5.0442\n",
      "Epoch [3/5], Step [16725/34168], Loss: 5.1089\n",
      "Epoch [3/5], Step [16800/34168], Loss: 5.1730\n",
      "Epoch [3/5], Step [16875/34168], Loss: 5.1122\n",
      "Epoch [3/5], Step [16950/34168], Loss: 5.2670\n",
      "Epoch [3/5], Step [17025/34168], Loss: 5.1871\n",
      "Epoch [3/5], Step [17100/34168], Loss: 5.0625\n",
      "Epoch [3/5], Step [17175/34168], Loss: 5.1975\n",
      "Epoch [3/5], Step [17250/34168], Loss: 5.1912\n",
      "Epoch [3/5], Step [17325/34168], Loss: 5.0676\n",
      "Epoch [3/5], Step [17400/34168], Loss: 5.2481\n",
      "Epoch [3/5], Step [17475/34168], Loss: 5.1777\n",
      "Epoch [3/5], Step [17550/34168], Loss: 5.1178\n",
      "Epoch [3/5], Step [17625/34168], Loss: 5.1754\n",
      "Epoch [3/5], Step [17700/34168], Loss: 5.0903\n",
      "Epoch [3/5], Step [17775/34168], Loss: 5.1480\n",
      "Epoch [3/5], Step [17850/34168], Loss: 5.0886\n",
      "Epoch [3/5], Step [17925/34168], Loss: 5.0893\n",
      "Epoch [3/5], Step [18000/34168], Loss: 5.3526\n",
      "Epoch [3/5], Step [18075/34168], Loss: 5.0853\n",
      "Epoch [3/5], Step [18150/34168], Loss: 4.9745\n",
      "Epoch [3/5], Step [18225/34168], Loss: 5.0808\n",
      "Epoch [3/5], Step [18300/34168], Loss: 5.2070\n",
      "Epoch [3/5], Step [18375/34168], Loss: 5.0826\n",
      "Epoch [3/5], Step [18450/34168], Loss: 5.2428\n",
      "Epoch [3/5], Step [18525/34168], Loss: 5.0634\n",
      "Epoch [3/5], Step [18600/34168], Loss: 5.0813\n",
      "Epoch [3/5], Step [18675/34168], Loss: 5.2044\n",
      "Epoch [3/5], Step [18750/34168], Loss: 5.1821\n",
      "Epoch [3/5], Step [18825/34168], Loss: 5.1591\n",
      "Epoch [3/5], Step [18900/34168], Loss: 5.1023\n",
      "Epoch [3/5], Step [18975/34168], Loss: 5.1141\n",
      "Epoch [3/5], Step [19050/34168], Loss: 5.2124\n",
      "Epoch [3/5], Step [19125/34168], Loss: 5.3151\n",
      "Epoch [3/5], Step [19200/34168], Loss: 5.0850\n",
      "Epoch [3/5], Step [19275/34168], Loss: 5.1172\n",
      "Epoch [3/5], Step [19350/34168], Loss: 5.3836\n",
      "Epoch [3/5], Step [19425/34168], Loss: 5.1311\n",
      "Epoch [3/5], Step [19500/34168], Loss: 5.2322\n",
      "Epoch [3/5], Step [19575/34168], Loss: 5.1353\n",
      "Epoch [3/5], Step [19650/34168], Loss: 5.0768\n",
      "Epoch [3/5], Step [19725/34168], Loss: 5.1447\n",
      "Epoch [3/5], Step [19800/34168], Loss: 5.3364\n",
      "Epoch [3/5], Step [19875/34168], Loss: 5.1392\n",
      "Epoch [3/5], Step [19950/34168], Loss: 4.9984\n",
      "Validation perplexity: 134.36278762866758\n",
      "Epoch [3/5], Step [20025/34168], Loss: 5.2757\n",
      "Epoch [3/5], Step [20100/34168], Loss: 5.0906\n",
      "Epoch [3/5], Step [20175/34168], Loss: 5.0941\n",
      "Epoch [3/5], Step [20250/34168], Loss: 5.2294\n",
      "Epoch [3/5], Step [20325/34168], Loss: 5.0932\n",
      "Epoch [3/5], Step [20400/34168], Loss: 5.0840\n",
      "Epoch [3/5], Step [20475/34168], Loss: 5.0751\n",
      "Epoch [3/5], Step [20550/34168], Loss: 5.1562\n",
      "Epoch [3/5], Step [20625/34168], Loss: 5.1262\n",
      "Epoch [3/5], Step [20700/34168], Loss: 5.1403\n",
      "Epoch [3/5], Step [20775/34168], Loss: 5.2207\n",
      "Epoch [3/5], Step [20850/34168], Loss: 5.0644\n",
      "Epoch [3/5], Step [20925/34168], Loss: 5.1621\n",
      "Epoch [3/5], Step [21000/34168], Loss: 5.1311\n",
      "Epoch [3/5], Step [21075/34168], Loss: 5.2081\n",
      "Epoch [3/5], Step [21150/34168], Loss: 5.1531\n",
      "Epoch [3/5], Step [21225/34168], Loss: 5.0490\n",
      "Epoch [3/5], Step [21300/34168], Loss: 5.0593\n",
      "Epoch [3/5], Step [21375/34168], Loss: 4.9864\n",
      "Epoch [3/5], Step [21450/34168], Loss: 5.2408\n",
      "Epoch [3/5], Step [21525/34168], Loss: 5.1470\n",
      "Epoch [3/5], Step [21600/34168], Loss: 5.0769\n",
      "Epoch [3/5], Step [21675/34168], Loss: 5.1937\n",
      "Epoch [3/5], Step [21750/34168], Loss: 5.0433\n",
      "Epoch [3/5], Step [21825/34168], Loss: 5.2098\n",
      "Epoch [3/5], Step [21900/34168], Loss: 5.1347\n",
      "Epoch [3/5], Step [21975/34168], Loss: 5.0052\n",
      "Epoch [3/5], Step [22050/34168], Loss: 5.0798\n",
      "Epoch [3/5], Step [22125/34168], Loss: 5.1579\n",
      "Epoch [3/5], Step [22200/34168], Loss: 5.1923\n",
      "Epoch [3/5], Step [22275/34168], Loss: 5.0913\n",
      "Epoch [3/5], Step [22350/34168], Loss: 5.2595\n",
      "Epoch [3/5], Step [22425/34168], Loss: 5.1383\n",
      "Epoch [3/5], Step [22500/34168], Loss: 5.1910\n",
      "Epoch [3/5], Step [22575/34168], Loss: 5.1461\n",
      "Epoch [3/5], Step [22650/34168], Loss: 5.2743\n",
      "Epoch [3/5], Step [22725/34168], Loss: 5.1553\n",
      "Epoch [3/5], Step [22800/34168], Loss: 5.0754\n",
      "Epoch [3/5], Step [22875/34168], Loss: 5.3118\n",
      "Epoch [3/5], Step [22950/34168], Loss: 5.1311\n",
      "Epoch [3/5], Step [23025/34168], Loss: 5.3119\n",
      "Epoch [3/5], Step [23100/34168], Loss: 5.0809\n",
      "Epoch [3/5], Step [23175/34168], Loss: 5.2279\n",
      "Epoch [3/5], Step [23250/34168], Loss: 5.0088\n",
      "Epoch [3/5], Step [23325/34168], Loss: 5.2006\n",
      "Epoch [3/5], Step [23400/34168], Loss: 5.2622\n",
      "Epoch [3/5], Step [23475/34168], Loss: 5.1487\n",
      "Epoch [3/5], Step [23550/34168], Loss: 5.1144\n",
      "Epoch [3/5], Step [23625/34168], Loss: 5.0939\n",
      "Epoch [3/5], Step [23700/34168], Loss: 5.1443\n",
      "Epoch [3/5], Step [23775/34168], Loss: 5.1144\n",
      "Epoch [3/5], Step [23850/34168], Loss: 5.1187\n",
      "Epoch [3/5], Step [23925/34168], Loss: 5.1601\n",
      "Epoch [3/5], Step [24000/34168], Loss: 5.2406\n",
      "Epoch [3/5], Step [24075/34168], Loss: 5.2010\n",
      "Epoch [3/5], Step [24150/34168], Loss: 5.1742\n",
      "Epoch [3/5], Step [24225/34168], Loss: 5.2365\n",
      "Epoch [3/5], Step [24300/34168], Loss: 5.0433\n",
      "Epoch [3/5], Step [24375/34168], Loss: 5.1997\n",
      "Epoch [3/5], Step [24450/34168], Loss: 4.8701\n",
      "Epoch [3/5], Step [24525/34168], Loss: 5.1837\n",
      "Epoch [3/5], Step [24600/34168], Loss: 5.1563\n",
      "Epoch [3/5], Step [24675/34168], Loss: 5.1415\n",
      "Epoch [3/5], Step [24750/34168], Loss: 5.2055\n",
      "Epoch [3/5], Step [24825/34168], Loss: 5.0147\n",
      "Epoch [3/5], Step [24900/34168], Loss: 5.0619\n",
      "Epoch [3/5], Step [24975/34168], Loss: 5.0404\n",
      "Epoch [3/5], Step [25050/34168], Loss: 5.0788\n",
      "Epoch [3/5], Step [25125/34168], Loss: 5.2875\n",
      "Epoch [3/5], Step [25200/34168], Loss: 5.1674\n",
      "Epoch [3/5], Step [25275/34168], Loss: 5.0954\n",
      "Epoch [3/5], Step [25350/34168], Loss: 5.0290\n",
      "Epoch [3/5], Step [25425/34168], Loss: 5.1150\n",
      "Epoch [3/5], Step [25500/34168], Loss: 5.2199\n",
      "Epoch [3/5], Step [25575/34168], Loss: 5.2288\n",
      "Epoch [3/5], Step [25650/34168], Loss: 5.2113\n",
      "Epoch [3/5], Step [25725/34168], Loss: 5.1427\n",
      "Epoch [3/5], Step [25800/34168], Loss: 5.0981\n",
      "Epoch [3/5], Step [25875/34168], Loss: 5.1465\n",
      "Epoch [3/5], Step [25950/34168], Loss: 5.0693\n",
      "Epoch [3/5], Step [26025/34168], Loss: 5.2414\n",
      "Epoch [3/5], Step [26100/34168], Loss: 5.2240\n",
      "Epoch [3/5], Step [26175/34168], Loss: 5.1151\n",
      "Epoch [3/5], Step [26250/34168], Loss: 5.0668\n",
      "Epoch [3/5], Step [26325/34168], Loss: 5.0724\n",
      "Epoch [3/5], Step [26400/34168], Loss: 5.0631\n",
      "Epoch [3/5], Step [26475/34168], Loss: 5.2310\n",
      "Epoch [3/5], Step [26550/34168], Loss: 5.2173\n",
      "Epoch [3/5], Step [26625/34168], Loss: 5.1409\n",
      "Epoch [3/5], Step [26700/34168], Loss: 5.1590\n",
      "Epoch [3/5], Step [26775/34168], Loss: 5.0428\n",
      "Epoch [3/5], Step [26850/34168], Loss: 5.1839\n",
      "Epoch [3/5], Step [26925/34168], Loss: 5.1349\n",
      "Epoch [3/5], Step [27000/34168], Loss: 5.2067\n",
      "Epoch [3/5], Step [27075/34168], Loss: 5.2822\n",
      "Epoch [3/5], Step [27150/34168], Loss: 5.1882\n",
      "Epoch [3/5], Step [27225/34168], Loss: 5.1082\n",
      "Epoch [3/5], Step [27300/34168], Loss: 5.1133\n",
      "Epoch [3/5], Step [27375/34168], Loss: 5.1404\n",
      "Epoch [3/5], Step [27450/34168], Loss: 5.1099\n",
      "Epoch [3/5], Step [27525/34168], Loss: 5.1420\n",
      "Epoch [3/5], Step [27600/34168], Loss: 5.1723\n",
      "Epoch [3/5], Step [27675/34168], Loss: 5.2741\n",
      "Epoch [3/5], Step [27750/34168], Loss: 5.1131\n",
      "Epoch [3/5], Step [27825/34168], Loss: 5.2401\n",
      "Epoch [3/5], Step [27900/34168], Loss: 5.0498\n",
      "Epoch [3/5], Step [27975/34168], Loss: 5.1484\n",
      "Epoch [3/5], Step [28050/34168], Loss: 5.0818\n",
      "Epoch [3/5], Step [28125/34168], Loss: 5.1699\n",
      "Epoch [3/5], Step [28200/34168], Loss: 5.0156\n",
      "Epoch [3/5], Step [28275/34168], Loss: 5.3300\n",
      "Epoch [3/5], Step [28350/34168], Loss: 5.1508\n",
      "Epoch [3/5], Step [28425/34168], Loss: 5.1609\n",
      "Epoch [3/5], Step [28500/34168], Loss: 5.1112\n",
      "Epoch [3/5], Step [28575/34168], Loss: 5.1578\n",
      "Epoch [3/5], Step [28650/34168], Loss: 5.0914\n",
      "Epoch [3/5], Step [28725/34168], Loss: 5.1127\n",
      "Epoch [3/5], Step [28800/34168], Loss: 5.0044\n",
      "Epoch [3/5], Step [28875/34168], Loss: 5.1766\n",
      "Epoch [3/5], Step [28950/34168], Loss: 5.0184\n",
      "Epoch [3/5], Step [29025/34168], Loss: 5.0687\n",
      "Epoch [3/5], Step [29100/34168], Loss: 5.1303\n",
      "Epoch [3/5], Step [29175/34168], Loss: 5.0747\n",
      "Epoch [3/5], Step [29250/34168], Loss: 5.1484\n",
      "Epoch [3/5], Step [29325/34168], Loss: 5.1795\n",
      "Epoch [3/5], Step [29400/34168], Loss: 5.1373\n",
      "Epoch [3/5], Step [29475/34168], Loss: 5.0348\n",
      "Epoch [3/5], Step [29550/34168], Loss: 5.1298\n",
      "Epoch [3/5], Step [29625/34168], Loss: 5.0586\n",
      "Epoch [3/5], Step [29700/34168], Loss: 5.2106\n",
      "Epoch [3/5], Step [29775/34168], Loss: 5.1969\n",
      "Epoch [3/5], Step [29850/34168], Loss: 5.0010\n",
      "Epoch [3/5], Step [29925/34168], Loss: 5.0710\n",
      "Epoch [3/5], Step [30000/34168], Loss: 5.0490\n",
      "Validation perplexity: 131.1442124161405\n",
      "Epoch [3/5], Step [30075/34168], Loss: 5.1656\n",
      "Epoch [3/5], Step [30150/34168], Loss: 5.0355\n",
      "Epoch [3/5], Step [30225/34168], Loss: 5.1871\n",
      "Epoch [3/5], Step [30300/34168], Loss: 4.9840\n",
      "Epoch [3/5], Step [30375/34168], Loss: 4.9961\n",
      "Epoch [3/5], Step [30450/34168], Loss: 5.1520\n",
      "Epoch [3/5], Step [30525/34168], Loss: 5.0347\n",
      "Epoch [3/5], Step [30600/34168], Loss: 5.0902\n",
      "Epoch [3/5], Step [30675/34168], Loss: 5.1537\n",
      "Epoch [3/5], Step [30750/34168], Loss: 5.0432\n",
      "Epoch [3/5], Step [30825/34168], Loss: 5.1330\n",
      "Epoch [3/5], Step [30900/34168], Loss: 5.1243\n",
      "Epoch [3/5], Step [30975/34168], Loss: 5.1441\n",
      "Epoch [3/5], Step [31050/34168], Loss: 5.0485\n",
      "Epoch [3/5], Step [31125/34168], Loss: 5.0697\n",
      "Epoch [3/5], Step [31200/34168], Loss: 5.1273\n",
      "Epoch [3/5], Step [31275/34168], Loss: 5.1347\n",
      "Epoch [3/5], Step [31350/34168], Loss: 4.9537\n",
      "Epoch [3/5], Step [31425/34168], Loss: 5.2579\n",
      "Epoch [3/5], Step [31500/34168], Loss: 5.1175\n",
      "Epoch [3/5], Step [31575/34168], Loss: 5.2308\n",
      "Epoch [3/5], Step [31650/34168], Loss: 5.1085\n",
      "Epoch [3/5], Step [31725/34168], Loss: 5.1165\n",
      "Epoch [3/5], Step [31800/34168], Loss: 5.0947\n",
      "Epoch [3/5], Step [31875/34168], Loss: 5.2372\n",
      "Epoch [3/5], Step [31950/34168], Loss: 5.1478\n",
      "Epoch [3/5], Step [32025/34168], Loss: 5.1445\n",
      "Epoch [3/5], Step [32100/34168], Loss: 5.1084\n",
      "Epoch [3/5], Step [32175/34168], Loss: 5.2364\n",
      "Epoch [3/5], Step [32250/34168], Loss: 5.1179\n",
      "Epoch [3/5], Step [32325/34168], Loss: 5.0169\n",
      "Epoch [3/5], Step [32400/34168], Loss: 5.1145\n",
      "Epoch [3/5], Step [32475/34168], Loss: 5.1792\n",
      "Epoch [3/5], Step [32550/34168], Loss: 5.0784\n",
      "Epoch [3/5], Step [32625/34168], Loss: 5.1360\n",
      "Epoch [3/5], Step [32700/34168], Loss: 5.1497\n",
      "Epoch [3/5], Step [32775/34168], Loss: 5.1119\n",
      "Epoch [3/5], Step [32850/34168], Loss: 5.1230\n",
      "Epoch [3/5], Step [32925/34168], Loss: 5.1355\n",
      "Epoch [3/5], Step [33000/34168], Loss: 5.2474\n",
      "Epoch [3/5], Step [33075/34168], Loss: 5.1389\n",
      "Epoch [3/5], Step [33150/34168], Loss: 5.1498\n",
      "Epoch [3/5], Step [33225/34168], Loss: 5.2431\n",
      "Epoch [3/5], Step [33300/34168], Loss: 5.0208\n",
      "Epoch [3/5], Step [33375/34168], Loss: 5.2600\n",
      "Epoch [3/5], Step [33450/34168], Loss: 5.1101\n",
      "Epoch [3/5], Step [33525/34168], Loss: 5.0605\n",
      "Epoch [3/5], Step [33600/34168], Loss: 5.0761\n",
      "Epoch [3/5], Step [33675/34168], Loss: 5.3439\n",
      "Epoch [3/5], Step [33750/34168], Loss: 4.9683\n",
      "Epoch [3/5], Step [33825/34168], Loss: 5.2456\n",
      "Epoch [3/5], Step [33900/34168], Loss: 4.9776\n",
      "Epoch [3/5], Step [33975/34168], Loss: 5.1352\n",
      "Epoch [3/5], Step [34050/34168], Loss: 5.0706\n",
      "Epoch [3/5], Step [34125/34168], Loss: 5.0137\n",
      "Epoch [3/5] Average Loss: 5.1528, Perplexity: 172.92\n",
      "Epoch [4/5], Step [0/34168], Loss: 5.2991\n",
      "Validation perplexity: 130.51396210941533\n",
      "Epoch [4/5], Step [75/34168], Loss: 5.2509\n",
      "Epoch [4/5], Step [150/34168], Loss: 5.1743\n",
      "Epoch [4/5], Step [225/34168], Loss: 5.0137\n",
      "Epoch [4/5], Step [300/34168], Loss: 5.1186\n",
      "Epoch [4/5], Step [375/34168], Loss: 5.1797\n",
      "Epoch [4/5], Step [450/34168], Loss: 5.0996\n",
      "Epoch [4/5], Step [525/34168], Loss: 5.0847\n",
      "Epoch [4/5], Step [600/34168], Loss: 5.1907\n",
      "Epoch [4/5], Step [675/34168], Loss: 5.1171\n",
      "Epoch [4/5], Step [750/34168], Loss: 5.0996\n",
      "Epoch [4/5], Step [825/34168], Loss: 5.2363\n",
      "Epoch [4/5], Step [900/34168], Loss: 5.0630\n",
      "Epoch [4/5], Step [975/34168], Loss: 5.1137\n",
      "Epoch [4/5], Step [1050/34168], Loss: 5.1219\n",
      "Epoch [4/5], Step [1125/34168], Loss: 4.9765\n",
      "Epoch [4/5], Step [1200/34168], Loss: 5.2380\n",
      "Epoch [4/5], Step [1275/34168], Loss: 5.0503\n",
      "Epoch [4/5], Step [1350/34168], Loss: 5.2304\n",
      "Epoch [4/5], Step [1425/34168], Loss: 5.0413\n",
      "Epoch [4/5], Step [1500/34168], Loss: 5.1503\n",
      "Epoch [4/5], Step [1575/34168], Loss: 5.0720\n",
      "Epoch [4/5], Step [1650/34168], Loss: 5.2761\n",
      "Epoch [4/5], Step [1725/34168], Loss: 5.1481\n",
      "Epoch [4/5], Step [1800/34168], Loss: 5.0717\n",
      "Epoch [4/5], Step [1875/34168], Loss: 5.1426\n",
      "Epoch [4/5], Step [1950/34168], Loss: 5.0959\n",
      "Epoch [4/5], Step [2025/34168], Loss: 5.2540\n",
      "Epoch [4/5], Step [2100/34168], Loss: 5.1475\n",
      "Epoch [4/5], Step [2175/34168], Loss: 4.9364\n",
      "Epoch [4/5], Step [2250/34168], Loss: 5.1511\n",
      "Epoch [4/5], Step [2325/34168], Loss: 5.1002\n",
      "Epoch [4/5], Step [2400/34168], Loss: 5.0948\n",
      "Epoch [4/5], Step [2475/34168], Loss: 5.0712\n",
      "Epoch [4/5], Step [2550/34168], Loss: 5.1464\n",
      "Epoch [4/5], Step [2625/34168], Loss: 5.1754\n",
      "Epoch [4/5], Step [2700/34168], Loss: 5.0310\n",
      "Epoch [4/5], Step [2775/34168], Loss: 5.0579\n",
      "Epoch [4/5], Step [2850/34168], Loss: 5.1528\n",
      "Epoch [4/5], Step [2925/34168], Loss: 5.1041\n",
      "Epoch [4/5], Step [3000/34168], Loss: 4.9867\n",
      "Epoch [4/5], Step [3075/34168], Loss: 5.0716\n",
      "Epoch [4/5], Step [3150/34168], Loss: 5.2728\n",
      "Epoch [4/5], Step [3225/34168], Loss: 5.0359\n",
      "Epoch [4/5], Step [3300/34168], Loss: 5.0817\n",
      "Epoch [4/5], Step [3375/34168], Loss: 5.1007\n",
      "Epoch [4/5], Step [3450/34168], Loss: 5.0508\n",
      "Epoch [4/5], Step [3525/34168], Loss: 4.9945\n",
      "Epoch [4/5], Step [3600/34168], Loss: 5.0584\n",
      "Epoch [4/5], Step [3675/34168], Loss: 5.0411\n",
      "Epoch [4/5], Step [3750/34168], Loss: 5.1618\n",
      "Epoch [4/5], Step [3825/34168], Loss: 5.3040\n",
      "Epoch [4/5], Step [3900/34168], Loss: 5.1525\n",
      "Epoch [4/5], Step [3975/34168], Loss: 5.1327\n",
      "Epoch [4/5], Step [4050/34168], Loss: 5.1435\n",
      "Epoch [4/5], Step [4125/34168], Loss: 5.2147\n",
      "Epoch [4/5], Step [4200/34168], Loss: 5.2244\n",
      "Epoch [4/5], Step [4275/34168], Loss: 5.0988\n",
      "Epoch [4/5], Step [4350/34168], Loss: 5.0149\n",
      "Epoch [4/5], Step [4425/34168], Loss: 5.0631\n",
      "Epoch [4/5], Step [4500/34168], Loss: 4.9972\n",
      "Epoch [4/5], Step [4575/34168], Loss: 5.1385\n",
      "Epoch [4/5], Step [4650/34168], Loss: 5.0695\n",
      "Epoch [4/5], Step [4725/34168], Loss: 5.2266\n",
      "Epoch [4/5], Step [4800/34168], Loss: 5.1195\n",
      "Epoch [4/5], Step [4875/34168], Loss: 5.0477\n",
      "Epoch [4/5], Step [4950/34168], Loss: 5.1962\n",
      "Epoch [4/5], Step [5025/34168], Loss: 5.1921\n",
      "Epoch [4/5], Step [5100/34168], Loss: 5.0716\n",
      "Epoch [4/5], Step [5175/34168], Loss: 5.0171\n",
      "Epoch [4/5], Step [5250/34168], Loss: 5.1321\n",
      "Epoch [4/5], Step [5325/34168], Loss: 5.1067\n",
      "Epoch [4/5], Step [5400/34168], Loss: 5.1945\n",
      "Epoch [4/5], Step [5475/34168], Loss: 5.1700\n",
      "Epoch [4/5], Step [5550/34168], Loss: 5.1213\n",
      "Epoch [4/5], Step [5625/34168], Loss: 5.0214\n",
      "Epoch [4/5], Step [5700/34168], Loss: 5.1092\n",
      "Epoch [4/5], Step [5775/34168], Loss: 5.0596\n",
      "Epoch [4/5], Step [5850/34168], Loss: 5.1576\n",
      "Epoch [4/5], Step [5925/34168], Loss: 5.0432\n",
      "Epoch [4/5], Step [6000/34168], Loss: 5.1091\n",
      "Epoch [4/5], Step [6075/34168], Loss: 5.1841\n",
      "Epoch [4/5], Step [6150/34168], Loss: 5.1687\n",
      "Epoch [4/5], Step [6225/34168], Loss: 5.0771\n",
      "Epoch [4/5], Step [6300/34168], Loss: 5.0356\n",
      "Epoch [4/5], Step [6375/34168], Loss: 5.1464\n",
      "Epoch [4/5], Step [6450/34168], Loss: 5.0670\n",
      "Epoch [4/5], Step [6525/34168], Loss: 5.1562\n",
      "Epoch [4/5], Step [6600/34168], Loss: 5.0750\n",
      "Epoch [4/5], Step [6675/34168], Loss: 5.1098\n",
      "Epoch [4/5], Step [6750/34168], Loss: 5.0853\n",
      "Epoch [4/5], Step [6825/34168], Loss: 5.1858\n",
      "Epoch [4/5], Step [6900/34168], Loss: 5.2136\n",
      "Epoch [4/5], Step [6975/34168], Loss: 5.1798\n",
      "Epoch [4/5], Step [7050/34168], Loss: 5.1315\n",
      "Epoch [4/5], Step [7125/34168], Loss: 5.2252\n",
      "Epoch [4/5], Step [7200/34168], Loss: 5.0093\n",
      "Epoch [4/5], Step [7275/34168], Loss: 5.0563\n",
      "Epoch [4/5], Step [7350/34168], Loss: 5.0932\n",
      "Epoch [4/5], Step [7425/34168], Loss: 5.2102\n",
      "Epoch [4/5], Step [7500/34168], Loss: 5.0244\n",
      "Epoch [4/5], Step [7575/34168], Loss: 5.1058\n",
      "Epoch [4/5], Step [7650/34168], Loss: 5.3202\n",
      "Epoch [4/5], Step [7725/34168], Loss: 5.1805\n",
      "Epoch [4/5], Step [7800/34168], Loss: 5.1966\n",
      "Epoch [4/5], Step [7875/34168], Loss: 5.0006\n",
      "Epoch [4/5], Step [7950/34168], Loss: 5.0553\n",
      "Epoch [4/5], Step [8025/34168], Loss: 5.0339\n",
      "Epoch [4/5], Step [8100/34168], Loss: 5.1605\n",
      "Epoch [4/5], Step [8175/34168], Loss: 5.1859\n",
      "Epoch [4/5], Step [8250/34168], Loss: 5.1101\n",
      "Epoch [4/5], Step [8325/34168], Loss: 5.1407\n",
      "Epoch [4/5], Step [8400/34168], Loss: 5.1206\n",
      "Epoch [4/5], Step [8475/34168], Loss: 4.9957\n",
      "Epoch [4/5], Step [8550/34168], Loss: 5.0758\n",
      "Epoch [4/5], Step [8625/34168], Loss: 5.1555\n",
      "Epoch [4/5], Step [8700/34168], Loss: 5.1385\n",
      "Epoch [4/5], Step [8775/34168], Loss: 5.0829\n",
      "Epoch [4/5], Step [8850/34168], Loss: 5.0381\n",
      "Epoch [4/5], Step [8925/34168], Loss: 4.9805\n",
      "Epoch [4/5], Step [9000/34168], Loss: 5.0818\n",
      "Epoch [4/5], Step [9075/34168], Loss: 5.2060\n",
      "Epoch [4/5], Step [9150/34168], Loss: 5.2284\n",
      "Epoch [4/5], Step [9225/34168], Loss: 5.1165\n",
      "Epoch [4/5], Step [9300/34168], Loss: 5.0675\n",
      "Epoch [4/5], Step [9375/34168], Loss: 4.9783\n",
      "Epoch [4/5], Step [9450/34168], Loss: 4.9950\n",
      "Epoch [4/5], Step [9525/34168], Loss: 5.1377\n",
      "Epoch [4/5], Step [9600/34168], Loss: 5.1538\n",
      "Epoch [4/5], Step [9675/34168], Loss: 5.0585\n",
      "Epoch [4/5], Step [9750/34168], Loss: 5.1762\n",
      "Epoch [4/5], Step [9825/34168], Loss: 5.0997\n",
      "Epoch [4/5], Step [9900/34168], Loss: 5.3727\n",
      "Epoch [4/5], Step [9975/34168], Loss: 5.1002\n",
      "Validation perplexity: 128.37943520873338\n",
      "Epoch [4/5], Step [10050/34168], Loss: 5.1776\n",
      "Epoch [4/5], Step [10125/34168], Loss: 5.1218\n",
      "Epoch [4/5], Step [10200/34168], Loss: 5.1910\n",
      "Epoch [4/5], Step [10275/34168], Loss: 5.0550\n",
      "Epoch [4/5], Step [10350/34168], Loss: 5.3264\n",
      "Epoch [4/5], Step [10425/34168], Loss: 5.2379\n",
      "Epoch [4/5], Step [10500/34168], Loss: 5.1642\n",
      "Epoch [4/5], Step [10575/34168], Loss: 4.9871\n",
      "Epoch [4/5], Step [10650/34168], Loss: 5.0635\n",
      "Epoch [4/5], Step [10725/34168], Loss: 5.1445\n",
      "Epoch [4/5], Step [10800/34168], Loss: 4.9857\n",
      "Epoch [4/5], Step [10875/34168], Loss: 5.1718\n",
      "Epoch [4/5], Step [10950/34168], Loss: 5.0798\n",
      "Epoch [4/5], Step [11025/34168], Loss: 5.1493\n",
      "Epoch [4/5], Step [11100/34168], Loss: 5.1179\n",
      "Epoch [4/5], Step [11175/34168], Loss: 5.0819\n",
      "Epoch [4/5], Step [11250/34168], Loss: 5.0641\n",
      "Epoch [4/5], Step [11325/34168], Loss: 5.2676\n",
      "Epoch [4/5], Step [11400/34168], Loss: 5.2672\n",
      "Epoch [4/5], Step [11475/34168], Loss: 5.0865\n",
      "Epoch [4/5], Step [11550/34168], Loss: 5.1800\n",
      "Epoch [4/5], Step [11625/34168], Loss: 5.1509\n",
      "Epoch [4/5], Step [11700/34168], Loss: 5.0842\n",
      "Epoch [4/5], Step [11775/34168], Loss: 5.0991\n",
      "Epoch [4/5], Step [11850/34168], Loss: 5.0291\n",
      "Epoch [4/5], Step [11925/34168], Loss: 5.1903\n",
      "Epoch [4/5], Step [12000/34168], Loss: 5.1831\n",
      "Epoch [4/5], Step [12075/34168], Loss: 5.1477\n",
      "Epoch [4/5], Step [12150/34168], Loss: 4.9431\n",
      "Epoch [4/5], Step [12225/34168], Loss: 5.0866\n",
      "Epoch [4/5], Step [12300/34168], Loss: 5.0747\n",
      "Epoch [4/5], Step [12375/34168], Loss: 5.0717\n",
      "Epoch [4/5], Step [12450/34168], Loss: 5.1104\n",
      "Epoch [4/5], Step [12525/34168], Loss: 5.0208\n",
      "Epoch [4/5], Step [12600/34168], Loss: 5.1879\n",
      "Epoch [4/5], Step [12675/34168], Loss: 5.1798\n",
      "Epoch [4/5], Step [12750/34168], Loss: 5.0885\n",
      "Epoch [4/5], Step [12825/34168], Loss: 5.2120\n",
      "Epoch [4/5], Step [12900/34168], Loss: 5.1037\n",
      "Epoch [4/5], Step [12975/34168], Loss: 5.1194\n",
      "Epoch [4/5], Step [13050/34168], Loss: 5.0139\n",
      "Epoch [4/5], Step [13125/34168], Loss: 5.1240\n",
      "Epoch [4/5], Step [13200/34168], Loss: 5.0394\n",
      "Epoch [4/5], Step [13275/34168], Loss: 5.2097\n",
      "Epoch [4/5], Step [13350/34168], Loss: 5.2922\n",
      "Epoch [4/5], Step [13425/34168], Loss: 4.8664\n",
      "Epoch [4/5], Step [13500/34168], Loss: 5.1142\n",
      "Epoch [4/5], Step [13575/34168], Loss: 5.1280\n",
      "Epoch [4/5], Step [13650/34168], Loss: 5.0764\n",
      "Epoch [4/5], Step [13725/34168], Loss: 5.1043\n",
      "Epoch [4/5], Step [13800/34168], Loss: 5.1855\n",
      "Epoch [4/5], Step [13875/34168], Loss: 5.0989\n",
      "Epoch [4/5], Step [13950/34168], Loss: 5.1069\n",
      "Epoch [4/5], Step [14025/34168], Loss: 5.0913\n",
      "Epoch [4/5], Step [14100/34168], Loss: 5.1412\n",
      "Epoch [4/5], Step [14175/34168], Loss: 5.3036\n",
      "Epoch [4/5], Step [14250/34168], Loss: 5.1277\n",
      "Epoch [4/5], Step [14325/34168], Loss: 5.1485\n",
      "Epoch [4/5], Step [14400/34168], Loss: 5.1162\n",
      "Epoch [4/5], Step [14475/34168], Loss: 5.3265\n",
      "Epoch [4/5], Step [14550/34168], Loss: 5.1030\n",
      "Epoch [4/5], Step [14625/34168], Loss: 5.0988\n",
      "Epoch [4/5], Step [14700/34168], Loss: 5.1421\n",
      "Epoch [4/5], Step [14775/34168], Loss: 5.1249\n",
      "Epoch [4/5], Step [14850/34168], Loss: 5.0614\n",
      "Epoch [4/5], Step [14925/34168], Loss: 5.1529\n",
      "Epoch [4/5], Step [15000/34168], Loss: 5.0685\n",
      "Epoch [4/5], Step [15075/34168], Loss: 5.0679\n",
      "Epoch [4/5], Step [15150/34168], Loss: 5.0770\n",
      "Epoch [4/5], Step [15225/34168], Loss: 5.0311\n",
      "Epoch [4/5], Step [15300/34168], Loss: 5.1919\n",
      "Epoch [4/5], Step [15375/34168], Loss: 5.2047\n",
      "Epoch [4/5], Step [15450/34168], Loss: 5.0931\n",
      "Epoch [4/5], Step [15525/34168], Loss: 5.0826\n",
      "Epoch [4/5], Step [15600/34168], Loss: 5.0752\n",
      "Epoch [4/5], Step [15675/34168], Loss: 4.9335\n",
      "Epoch [4/5], Step [15750/34168], Loss: 5.0885\n",
      "Epoch [4/5], Step [15825/34168], Loss: 5.1588\n",
      "Epoch [4/5], Step [15900/34168], Loss: 5.1083\n",
      "Epoch [4/5], Step [15975/34168], Loss: 5.0168\n",
      "Epoch [4/5], Step [16050/34168], Loss: 4.9799\n",
      "Epoch [4/5], Step [16125/34168], Loss: 5.1311\n",
      "Epoch [4/5], Step [16200/34168], Loss: 5.1708\n",
      "Epoch [4/5], Step [16275/34168], Loss: 4.9083\n",
      "Epoch [4/5], Step [16350/34168], Loss: 5.2323\n",
      "Epoch [4/5], Step [16425/34168], Loss: 5.2153\n",
      "Epoch [4/5], Step [16500/34168], Loss: 5.0995\n",
      "Epoch [4/5], Step [16575/34168], Loss: 5.0817\n",
      "Epoch [4/5], Step [16650/34168], Loss: 5.1390\n",
      "Epoch [4/5], Step [16725/34168], Loss: 5.2264\n",
      "Epoch [4/5], Step [16800/34168], Loss: 5.0857\n",
      "Epoch [4/5], Step [16875/34168], Loss: 5.2207\n",
      "Epoch [4/5], Step [16950/34168], Loss: 5.0488\n",
      "Epoch [4/5], Step [17025/34168], Loss: 4.9628\n",
      "Epoch [4/5], Step [17100/34168], Loss: 5.1065\n",
      "Epoch [4/5], Step [17175/34168], Loss: 5.1184\n",
      "Epoch [4/5], Step [17250/34168], Loss: 5.1588\n",
      "Epoch [4/5], Step [17325/34168], Loss: 5.1363\n",
      "Epoch [4/5], Step [17400/34168], Loss: 5.2066\n",
      "Epoch [4/5], Step [17475/34168], Loss: 5.0858\n",
      "Epoch [4/5], Step [17550/34168], Loss: 5.1815\n",
      "Epoch [4/5], Step [17625/34168], Loss: 5.2149\n",
      "Epoch [4/5], Step [17700/34168], Loss: 5.2238\n",
      "Epoch [4/5], Step [17775/34168], Loss: 5.0054\n",
      "Epoch [4/5], Step [17850/34168], Loss: 5.0211\n",
      "Epoch [4/5], Step [17925/34168], Loss: 5.0782\n",
      "Epoch [4/5], Step [18000/34168], Loss: 5.0722\n",
      "Epoch [4/5], Step [18075/34168], Loss: 5.2065\n",
      "Epoch [4/5], Step [18150/34168], Loss: 5.0693\n",
      "Epoch [4/5], Step [18225/34168], Loss: 5.1543\n",
      "Epoch [4/5], Step [18300/34168], Loss: 5.0054\n",
      "Epoch [4/5], Step [18375/34168], Loss: 5.2286\n",
      "Epoch [4/5], Step [18450/34168], Loss: 5.0725\n",
      "Epoch [4/5], Step [18525/34168], Loss: 5.1063\n",
      "Epoch [4/5], Step [18600/34168], Loss: 5.0677\n",
      "Epoch [4/5], Step [18675/34168], Loss: 5.1249\n",
      "Epoch [4/5], Step [18750/34168], Loss: 4.9800\n",
      "Epoch [4/5], Step [18825/34168], Loss: 5.0764\n",
      "Epoch [4/5], Step [18900/34168], Loss: 5.1396\n",
      "Epoch [4/5], Step [18975/34168], Loss: 5.0709\n",
      "Epoch [4/5], Step [19050/34168], Loss: 5.2077\n",
      "Epoch [4/5], Step [19125/34168], Loss: 5.1205\n",
      "Epoch [4/5], Step [19200/34168], Loss: 5.2430\n",
      "Epoch [4/5], Step [19275/34168], Loss: 4.9640\n",
      "Epoch [4/5], Step [19350/34168], Loss: 5.0909\n",
      "Epoch [4/5], Step [19425/34168], Loss: 5.1958\n",
      "Epoch [4/5], Step [19500/34168], Loss: 5.0623\n",
      "Epoch [4/5], Step [19575/34168], Loss: 5.1113\n",
      "Epoch [4/5], Step [19650/34168], Loss: 5.2362\n",
      "Epoch [4/5], Step [19725/34168], Loss: 5.0937\n",
      "Epoch [4/5], Step [19800/34168], Loss: 4.9479\n",
      "Epoch [4/5], Step [19875/34168], Loss: 5.1002\n",
      "Epoch [4/5], Step [19950/34168], Loss: 5.0901\n",
      "Validation perplexity: 126.74301224488441\n",
      "Epoch [4/5], Step [20025/34168], Loss: 5.1108\n",
      "Epoch [4/5], Step [20100/34168], Loss: 5.2033\n",
      "Epoch [4/5], Step [20175/34168], Loss: 5.2170\n",
      "Epoch [4/5], Step [20250/34168], Loss: 5.1376\n",
      "Epoch [4/5], Step [20325/34168], Loss: 5.1317\n",
      "Epoch [4/5], Step [20400/34168], Loss: 5.1355\n",
      "Epoch [4/5], Step [20475/34168], Loss: 5.1187\n",
      "Epoch [4/5], Step [20550/34168], Loss: 5.0091\n",
      "Epoch [4/5], Step [20625/34168], Loss: 5.2401\n",
      "Epoch [4/5], Step [20700/34168], Loss: 5.0516\n",
      "Epoch [4/5], Step [20775/34168], Loss: 5.2017\n",
      "Epoch [4/5], Step [20850/34168], Loss: 4.9866\n",
      "Epoch [4/5], Step [20925/34168], Loss: 5.2482\n",
      "Epoch [4/5], Step [21000/34168], Loss: 5.0198\n",
      "Epoch [4/5], Step [21075/34168], Loss: 5.1029\n",
      "Epoch [4/5], Step [21150/34168], Loss: 5.0940\n",
      "Epoch [4/5], Step [21225/34168], Loss: 5.0725\n",
      "Epoch [4/5], Step [21300/34168], Loss: 5.1150\n",
      "Epoch [4/5], Step [21375/34168], Loss: 5.1478\n",
      "Epoch [4/5], Step [21450/34168], Loss: 5.0981\n",
      "Epoch [4/5], Step [21525/34168], Loss: 5.1811\n",
      "Epoch [4/5], Step [21600/34168], Loss: 5.0932\n",
      "Epoch [4/5], Step [21675/34168], Loss: 5.0275\n",
      "Epoch [4/5], Step [21750/34168], Loss: 5.0949\n",
      "Epoch [4/5], Step [21825/34168], Loss: 5.1130\n",
      "Epoch [4/5], Step [21900/34168], Loss: 5.0335\n",
      "Epoch [4/5], Step [21975/34168], Loss: 5.0020\n",
      "Epoch [4/5], Step [22050/34168], Loss: 5.2345\n",
      "Epoch [4/5], Step [22125/34168], Loss: 5.0398\n",
      "Epoch [4/5], Step [22200/34168], Loss: 5.0848\n",
      "Epoch [4/5], Step [22275/34168], Loss: 5.0621\n",
      "Epoch [4/5], Step [22350/34168], Loss: 4.9969\n",
      "Epoch [4/5], Step [22425/34168], Loss: 4.9826\n",
      "Epoch [4/5], Step [22500/34168], Loss: 5.2103\n",
      "Epoch [4/5], Step [22575/34168], Loss: 5.1939\n",
      "Epoch [4/5], Step [22650/34168], Loss: 5.1088\n",
      "Epoch [4/5], Step [22725/34168], Loss: 4.9469\n",
      "Epoch [4/5], Step [22800/34168], Loss: 5.2676\n",
      "Epoch [4/5], Step [22875/34168], Loss: 4.9567\n",
      "Epoch [4/5], Step [22950/34168], Loss: 5.0521\n",
      "Epoch [4/5], Step [23025/34168], Loss: 5.1879\n",
      "Epoch [4/5], Step [23100/34168], Loss: 5.0781\n",
      "Epoch [4/5], Step [23175/34168], Loss: 5.1039\n",
      "Epoch [4/5], Step [23250/34168], Loss: 5.0417\n",
      "Epoch [4/5], Step [23325/34168], Loss: 5.1452\n",
      "Epoch [4/5], Step [23400/34168], Loss: 5.1406\n",
      "Epoch [4/5], Step [23475/34168], Loss: 5.1232\n",
      "Epoch [4/5], Step [23550/34168], Loss: 5.2720\n",
      "Epoch [4/5], Step [23625/34168], Loss: 4.8896\n",
      "Epoch [4/5], Step [23700/34168], Loss: 5.2097\n",
      "Epoch [4/5], Step [23775/34168], Loss: 4.9718\n",
      "Epoch [4/5], Step [23850/34168], Loss: 5.2605\n",
      "Epoch [4/5], Step [23925/34168], Loss: 5.0423\n",
      "Epoch [4/5], Step [24000/34168], Loss: 5.2096\n",
      "Epoch [4/5], Step [24075/34168], Loss: 5.0500\n",
      "Epoch [4/5], Step [24150/34168], Loss: 5.0653\n",
      "Epoch [4/5], Step [24225/34168], Loss: 5.0003\n",
      "Epoch [4/5], Step [24300/34168], Loss: 5.2716\n",
      "Epoch [4/5], Step [24375/34168], Loss: 4.9669\n",
      "Epoch [4/5], Step [24450/34168], Loss: 5.1253\n",
      "Epoch [4/5], Step [24525/34168], Loss: 5.2135\n",
      "Epoch [4/5], Step [24600/34168], Loss: 5.0824\n",
      "Epoch [4/5], Step [24675/34168], Loss: 4.9819\n",
      "Epoch [4/5], Step [24750/34168], Loss: 5.0434\n",
      "Epoch [4/5], Step [24825/34168], Loss: 4.9431\n",
      "Epoch [4/5], Step [24900/34168], Loss: 5.0447\n",
      "Epoch [4/5], Step [24975/34168], Loss: 5.0597\n",
      "Epoch [4/5], Step [25050/34168], Loss: 5.1188\n",
      "Epoch [4/5], Step [25125/34168], Loss: 5.1796\n",
      "Epoch [4/5], Step [25200/34168], Loss: 5.0235\n",
      "Epoch [4/5], Step [25275/34168], Loss: 5.0451\n",
      "Epoch [4/5], Step [25350/34168], Loss: 4.9164\n",
      "Epoch [4/5], Step [25425/34168], Loss: 5.0913\n",
      "Epoch [4/5], Step [25500/34168], Loss: 5.2253\n",
      "Epoch [4/5], Step [25575/34168], Loss: 5.2948\n",
      "Epoch [4/5], Step [25650/34168], Loss: 5.1638\n",
      "Epoch [4/5], Step [25725/34168], Loss: 5.1236\n",
      "Epoch [4/5], Step [25800/34168], Loss: 5.1376\n",
      "Epoch [4/5], Step [25875/34168], Loss: 5.1042\n",
      "Epoch [4/5], Step [25950/34168], Loss: 5.0856\n",
      "Epoch [4/5], Step [26025/34168], Loss: 5.1084\n",
      "Epoch [4/5], Step [26100/34168], Loss: 5.1047\n",
      "Epoch [4/5], Step [26175/34168], Loss: 4.9722\n",
      "Epoch [4/5], Step [26250/34168], Loss: 5.1191\n",
      "Epoch [4/5], Step [26325/34168], Loss: 5.0650\n",
      "Epoch [4/5], Step [26400/34168], Loss: 5.0394\n",
      "Epoch [4/5], Step [26475/34168], Loss: 5.0982\n",
      "Epoch [4/5], Step [26550/34168], Loss: 5.1183\n",
      "Epoch [4/5], Step [26625/34168], Loss: 5.1837\n",
      "Epoch [4/5], Step [26700/34168], Loss: 5.2667\n",
      "Epoch [4/5], Step [26775/34168], Loss: 5.0248\n",
      "Epoch [4/5], Step [26850/34168], Loss: 5.1139\n",
      "Epoch [4/5], Step [26925/34168], Loss: 5.0657\n",
      "Epoch [4/5], Step [27000/34168], Loss: 5.1806\n",
      "Epoch [4/5], Step [27075/34168], Loss: 5.0928\n",
      "Epoch [4/5], Step [27150/34168], Loss: 5.0904\n",
      "Epoch [4/5], Step [27225/34168], Loss: 4.9659\n",
      "Epoch [4/5], Step [27300/34168], Loss: 5.0516\n",
      "Epoch [4/5], Step [27375/34168], Loss: 5.0841\n",
      "Epoch [4/5], Step [27450/34168], Loss: 4.9807\n",
      "Epoch [4/5], Step [27525/34168], Loss: 5.0034\n",
      "Epoch [4/5], Step [27600/34168], Loss: 5.0184\n",
      "Epoch [4/5], Step [27675/34168], Loss: 5.0731\n",
      "Epoch [4/5], Step [27750/34168], Loss: 5.0447\n",
      "Epoch [4/5], Step [27825/34168], Loss: 5.0680\n",
      "Epoch [4/5], Step [27900/34168], Loss: 5.0780\n",
      "Epoch [4/5], Step [27975/34168], Loss: 5.0383\n",
      "Epoch [4/5], Step [28050/34168], Loss: 5.0762\n",
      "Epoch [4/5], Step [28125/34168], Loss: 4.9544\n",
      "Epoch [4/5], Step [28200/34168], Loss: 5.1720\n",
      "Epoch [4/5], Step [28275/34168], Loss: 5.1110\n",
      "Epoch [4/5], Step [28350/34168], Loss: 5.0831\n",
      "Epoch [4/5], Step [28425/34168], Loss: 5.1484\n",
      "Epoch [4/5], Step [28500/34168], Loss: 5.1943\n",
      "Epoch [4/5], Step [28575/34168], Loss: 5.0383\n",
      "Epoch [4/5], Step [28650/34168], Loss: 5.0532\n",
      "Epoch [4/5], Step [28725/34168], Loss: 5.0859\n",
      "Epoch [4/5], Step [28800/34168], Loss: 5.2772\n",
      "Epoch [4/5], Step [28875/34168], Loss: 5.1663\n",
      "Epoch [4/5], Step [28950/34168], Loss: 5.0962\n",
      "Epoch [4/5], Step [29025/34168], Loss: 4.9481\n",
      "Epoch [4/5], Step [29100/34168], Loss: 5.1263\n",
      "Epoch [4/5], Step [29175/34168], Loss: 5.1810\n",
      "Epoch [4/5], Step [29250/34168], Loss: 5.0653\n",
      "Epoch [4/5], Step [29325/34168], Loss: 5.0285\n",
      "Epoch [4/5], Step [29400/34168], Loss: 4.9564\n",
      "Epoch [4/5], Step [29475/34168], Loss: 5.1775\n",
      "Epoch [4/5], Step [29550/34168], Loss: 5.1205\n",
      "Epoch [4/5], Step [29625/34168], Loss: 5.1139\n",
      "Epoch [4/5], Step [29700/34168], Loss: 5.0529\n",
      "Epoch [4/5], Step [29775/34168], Loss: 5.0653\n",
      "Epoch [4/5], Step [29850/34168], Loss: 4.9126\n",
      "Epoch [4/5], Step [29925/34168], Loss: 5.1451\n",
      "Epoch [4/5], Step [30000/34168], Loss: 4.9695\n",
      "Validation perplexity: 125.42211012681885\n",
      "Epoch [4/5], Step [30075/34168], Loss: 5.1976\n",
      "Epoch [4/5], Step [30150/34168], Loss: 5.0071\n",
      "Epoch [4/5], Step [30225/34168], Loss: 5.0467\n",
      "Epoch [4/5], Step [30300/34168], Loss: 4.9559\n",
      "Epoch [4/5], Step [30375/34168], Loss: 5.0462\n",
      "Epoch [4/5], Step [30450/34168], Loss: 5.2279\n",
      "Epoch [4/5], Step [30525/34168], Loss: 5.1348\n",
      "Epoch [4/5], Step [30600/34168], Loss: 5.1618\n",
      "Epoch [4/5], Step [30675/34168], Loss: 5.1678\n",
      "Epoch [4/5], Step [30750/34168], Loss: 5.0999\n",
      "Epoch [4/5], Step [30825/34168], Loss: 5.0992\n",
      "Epoch [4/5], Step [30900/34168], Loss: 5.0797\n",
      "Epoch [4/5], Step [30975/34168], Loss: 5.0532\n",
      "Epoch [4/5], Step [31050/34168], Loss: 5.0562\n",
      "Epoch [4/5], Step [31125/34168], Loss: 5.0736\n",
      "Epoch [4/5], Step [31200/34168], Loss: 5.1471\n",
      "Epoch [4/5], Step [31275/34168], Loss: 5.1653\n",
      "Epoch [4/5], Step [31350/34168], Loss: 5.0641\n",
      "Epoch [4/5], Step [31425/34168], Loss: 4.9344\n",
      "Epoch [4/5], Step [31500/34168], Loss: 4.9061\n",
      "Epoch [4/5], Step [31575/34168], Loss: 5.1065\n",
      "Epoch [4/5], Step [31650/34168], Loss: 5.1421\n",
      "Epoch [4/5], Step [31725/34168], Loss: 5.0118\n",
      "Epoch [4/5], Step [31800/34168], Loss: 5.0702\n",
      "Epoch [4/5], Step [31875/34168], Loss: 4.9857\n",
      "Epoch [4/5], Step [31950/34168], Loss: 5.0318\n",
      "Epoch [4/5], Step [32025/34168], Loss: 5.1058\n",
      "Epoch [4/5], Step [32100/34168], Loss: 4.9132\n",
      "Epoch [4/5], Step [32175/34168], Loss: 5.1622\n",
      "Epoch [4/5], Step [32250/34168], Loss: 5.2913\n",
      "Epoch [4/5], Step [32325/34168], Loss: 5.2214\n",
      "Epoch [4/5], Step [32400/34168], Loss: 5.0573\n",
      "Epoch [4/5], Step [32475/34168], Loss: 5.0575\n",
      "Epoch [4/5], Step [32550/34168], Loss: 4.9865\n",
      "Epoch [4/5], Step [32625/34168], Loss: 5.1527\n",
      "Epoch [4/5], Step [32700/34168], Loss: 5.1430\n",
      "Epoch [4/5], Step [32775/34168], Loss: 5.2466\n",
      "Epoch [4/5], Step [32850/34168], Loss: 5.0501\n",
      "Epoch [4/5], Step [32925/34168], Loss: 5.0716\n",
      "Epoch [4/5], Step [33000/34168], Loss: 5.1836\n",
      "Epoch [4/5], Step [33075/34168], Loss: 4.9892\n",
      "Epoch [4/5], Step [33150/34168], Loss: 5.1216\n",
      "Epoch [4/5], Step [33225/34168], Loss: 5.0725\n",
      "Epoch [4/5], Step [33300/34168], Loss: 5.2085\n",
      "Epoch [4/5], Step [33375/34168], Loss: 5.1170\n",
      "Epoch [4/5], Step [33450/34168], Loss: 5.0541\n",
      "Epoch [4/5], Step [33525/34168], Loss: 5.0525\n",
      "Epoch [4/5], Step [33600/34168], Loss: 5.1331\n",
      "Epoch [4/5], Step [33675/34168], Loss: 4.9416\n",
      "Epoch [4/5], Step [33750/34168], Loss: 5.1361\n",
      "Epoch [4/5], Step [33825/34168], Loss: 5.0811\n",
      "Epoch [4/5], Step [33900/34168], Loss: 5.1302\n",
      "Epoch [4/5], Step [33975/34168], Loss: 5.2381\n",
      "Epoch [4/5], Step [34050/34168], Loss: 5.0047\n",
      "Epoch [4/5], Step [34125/34168], Loss: 5.0697\n",
      "Epoch [4/5] Average Loss: 5.1054, Perplexity: 164.92\n",
      "Epoch [5/5], Step [0/34168], Loss: 5.1868\n",
      "Validation perplexity: 124.6718512553185\n",
      "Epoch [5/5], Step [75/34168], Loss: 4.9901\n",
      "Epoch [5/5], Step [150/34168], Loss: 5.1093\n",
      "Epoch [5/5], Step [225/34168], Loss: 5.0568\n",
      "Epoch [5/5], Step [300/34168], Loss: 5.1063\n",
      "Epoch [5/5], Step [375/34168], Loss: 5.1943\n",
      "Epoch [5/5], Step [450/34168], Loss: 4.9989\n",
      "Epoch [5/5], Step [525/34168], Loss: 5.0844\n",
      "Epoch [5/5], Step [600/34168], Loss: 5.0143\n",
      "Epoch [5/5], Step [675/34168], Loss: 5.0408\n",
      "Epoch [5/5], Step [750/34168], Loss: 5.0153\n",
      "Epoch [5/5], Step [825/34168], Loss: 4.9903\n",
      "Epoch [5/5], Step [900/34168], Loss: 4.8967\n",
      "Epoch [5/5], Step [975/34168], Loss: 5.0281\n",
      "Epoch [5/5], Step [1050/34168], Loss: 5.1813\n",
      "Epoch [5/5], Step [1125/34168], Loss: 5.1703\n",
      "Epoch [5/5], Step [1200/34168], Loss: 5.2423\n",
      "Epoch [5/5], Step [1275/34168], Loss: 4.9224\n",
      "Epoch [5/5], Step [1350/34168], Loss: 5.0771\n",
      "Epoch [5/5], Step [1425/34168], Loss: 5.2088\n",
      "Epoch [5/5], Step [1500/34168], Loss: 5.0751\n",
      "Epoch [5/5], Step [1575/34168], Loss: 5.1211\n",
      "Epoch [5/5], Step [1650/34168], Loss: 4.9886\n",
      "Epoch [5/5], Step [1725/34168], Loss: 5.2302\n",
      "Epoch [5/5], Step [1800/34168], Loss: 5.0177\n",
      "Epoch [5/5], Step [1875/34168], Loss: 5.0717\n",
      "Epoch [5/5], Step [1950/34168], Loss: 4.9907\n",
      "Epoch [5/5], Step [2025/34168], Loss: 5.2082\n",
      "Epoch [5/5], Step [2100/34168], Loss: 5.0821\n",
      "Epoch [5/5], Step [2175/34168], Loss: 5.0916\n",
      "Epoch [5/5], Step [2250/34168], Loss: 4.9771\n",
      "Epoch [5/5], Step [2325/34168], Loss: 5.1530\n",
      "Epoch [5/5], Step [2400/34168], Loss: 5.0307\n",
      "Epoch [5/5], Step [2475/34168], Loss: 5.1336\n",
      "Epoch [5/5], Step [2550/34168], Loss: 5.0048\n",
      "Epoch [5/5], Step [2625/34168], Loss: 5.0049\n",
      "Epoch [5/5], Step [2700/34168], Loss: 5.0944\n",
      "Epoch [5/5], Step [2775/34168], Loss: 5.2395\n",
      "Epoch [5/5], Step [2850/34168], Loss: 5.1387\n",
      "Epoch [5/5], Step [2925/34168], Loss: 5.2007\n",
      "Epoch [5/5], Step [3000/34168], Loss: 5.0882\n",
      "Epoch [5/5], Step [3075/34168], Loss: 5.1597\n",
      "Epoch [5/5], Step [3150/34168], Loss: 4.9922\n",
      "Epoch [5/5], Step [3225/34168], Loss: 5.0428\n",
      "Epoch [5/5], Step [3300/34168], Loss: 5.1408\n",
      "Epoch [5/5], Step [3375/34168], Loss: 4.9524\n",
      "Epoch [5/5], Step [3450/34168], Loss: 5.1729\n",
      "Epoch [5/5], Step [3525/34168], Loss: 5.0342\n",
      "Epoch [5/5], Step [3600/34168], Loss: 5.1256\n",
      "Epoch [5/5], Step [3675/34168], Loss: 5.0382\n",
      "Epoch [5/5], Step [3750/34168], Loss: 5.0500\n",
      "Epoch [5/5], Step [3825/34168], Loss: 5.1117\n",
      "Epoch [5/5], Step [3900/34168], Loss: 5.1248\n",
      "Epoch [5/5], Step [3975/34168], Loss: 5.1070\n",
      "Epoch [5/5], Step [4050/34168], Loss: 5.0642\n",
      "Epoch [5/5], Step [4125/34168], Loss: 5.1299\n",
      "Epoch [5/5], Step [4200/34168], Loss: 4.9338\n",
      "Epoch [5/5], Step [4275/34168], Loss: 4.9982\n",
      "Epoch [5/5], Step [4350/34168], Loss: 5.1009\n",
      "Epoch [5/5], Step [4425/34168], Loss: 4.9433\n",
      "Epoch [5/5], Step [4500/34168], Loss: 5.1839\n",
      "Epoch [5/5], Step [4575/34168], Loss: 5.0842\n",
      "Epoch [5/5], Step [4650/34168], Loss: 5.0228\n",
      "Epoch [5/5], Step [4725/34168], Loss: 5.1009\n",
      "Epoch [5/5], Step [4800/34168], Loss: 4.9892\n",
      "Epoch [5/5], Step [4875/34168], Loss: 5.1292\n",
      "Epoch [5/5], Step [4950/34168], Loss: 5.0703\n",
      "Epoch [5/5], Step [5025/34168], Loss: 5.0911\n",
      "Epoch [5/5], Step [5100/34168], Loss: 5.0169\n",
      "Epoch [5/5], Step [5175/34168], Loss: 5.1359\n",
      "Epoch [5/5], Step [5250/34168], Loss: 5.0487\n",
      "Epoch [5/5], Step [5325/34168], Loss: 5.0923\n",
      "Epoch [5/5], Step [5400/34168], Loss: 5.1375\n",
      "Epoch [5/5], Step [5475/34168], Loss: 5.0923\n",
      "Epoch [5/5], Step [5550/34168], Loss: 5.0495\n",
      "Epoch [5/5], Step [5625/34168], Loss: 5.1049\n",
      "Epoch [5/5], Step [5700/34168], Loss: 5.0617\n",
      "Epoch [5/5], Step [5775/34168], Loss: 5.1888\n",
      "Epoch [5/5], Step [5850/34168], Loss: 5.0542\n",
      "Epoch [5/5], Step [5925/34168], Loss: 5.1948\n",
      "Epoch [5/5], Step [6000/34168], Loss: 4.9260\n",
      "Epoch [5/5], Step [6075/34168], Loss: 4.9395\n",
      "Epoch [5/5], Step [6150/34168], Loss: 5.1066\n",
      "Epoch [5/5], Step [6225/34168], Loss: 4.9684\n",
      "Epoch [5/5], Step [6300/34168], Loss: 5.0110\n",
      "Epoch [5/5], Step [6375/34168], Loss: 5.1137\n",
      "Epoch [5/5], Step [6450/34168], Loss: 5.1060\n",
      "Epoch [5/5], Step [6525/34168], Loss: 5.1865\n",
      "Epoch [5/5], Step [6600/34168], Loss: 4.9667\n",
      "Epoch [5/5], Step [6675/34168], Loss: 5.1055\n",
      "Epoch [5/5], Step [6750/34168], Loss: 5.0289\n",
      "Epoch [5/5], Step [6825/34168], Loss: 5.0274\n",
      "Epoch [5/5], Step [6900/34168], Loss: 5.1155\n",
      "Epoch [5/5], Step [6975/34168], Loss: 5.1048\n",
      "Epoch [5/5], Step [7050/34168], Loss: 5.1943\n",
      "Epoch [5/5], Step [7125/34168], Loss: 5.1852\n",
      "Epoch [5/5], Step [7200/34168], Loss: 4.9705\n",
      "Epoch [5/5], Step [7275/34168], Loss: 5.2072\n",
      "Epoch [5/5], Step [7350/34168], Loss: 5.0828\n",
      "Epoch [5/5], Step [7425/34168], Loss: 5.0350\n",
      "Epoch [5/5], Step [7500/34168], Loss: 5.1090\n",
      "Epoch [5/5], Step [7575/34168], Loss: 5.0150\n",
      "Epoch [5/5], Step [7650/34168], Loss: 5.0911\n",
      "Epoch [5/5], Step [7725/34168], Loss: 4.9565\n",
      "Epoch [5/5], Step [7800/34168], Loss: 5.0202\n",
      "Epoch [5/5], Step [7875/34168], Loss: 5.0070\n",
      "Epoch [5/5], Step [7950/34168], Loss: 5.1231\n",
      "Epoch [5/5], Step [8025/34168], Loss: 5.1763\n",
      "Epoch [5/5], Step [8100/34168], Loss: 5.0035\n",
      "Epoch [5/5], Step [8175/34168], Loss: 5.1376\n",
      "Epoch [5/5], Step [8250/34168], Loss: 5.1312\n",
      "Epoch [5/5], Step [8325/34168], Loss: 5.1385\n",
      "Epoch [5/5], Step [8400/34168], Loss: 4.9379\n",
      "Epoch [5/5], Step [8475/34168], Loss: 5.1026\n",
      "Epoch [5/5], Step [8550/34168], Loss: 5.2204\n",
      "Epoch [5/5], Step [8625/34168], Loss: 5.2128\n",
      "Epoch [5/5], Step [8700/34168], Loss: 5.1280\n",
      "Epoch [5/5], Step [8775/34168], Loss: 4.9806\n",
      "Epoch [5/5], Step [8850/34168], Loss: 4.9304\n",
      "Epoch [5/5], Step [8925/34168], Loss: 4.9953\n",
      "Epoch [5/5], Step [9000/34168], Loss: 5.1337\n",
      "Epoch [5/5], Step [9075/34168], Loss: 5.1043\n",
      "Epoch [5/5], Step [9150/34168], Loss: 5.0701\n",
      "Epoch [5/5], Step [9225/34168], Loss: 5.1132\n",
      "Epoch [5/5], Step [9300/34168], Loss: 5.0927\n",
      "Epoch [5/5], Step [9375/34168], Loss: 4.9468\n",
      "Epoch [5/5], Step [9450/34168], Loss: 4.9565\n",
      "Epoch [5/5], Step [9525/34168], Loss: 5.0432\n",
      "Epoch [5/5], Step [9600/34168], Loss: 5.0949\n",
      "Epoch [5/5], Step [9675/34168], Loss: 5.0840\n",
      "Epoch [5/5], Step [9750/34168], Loss: 5.1276\n",
      "Epoch [5/5], Step [9825/34168], Loss: 5.0900\n",
      "Epoch [5/5], Step [9900/34168], Loss: 5.0869\n",
      "Epoch [5/5], Step [9975/34168], Loss: 5.0559\n",
      "Validation perplexity: 123.31998338763876\n",
      "Epoch [5/5], Step [10050/34168], Loss: 5.0520\n",
      "Epoch [5/5], Step [10125/34168], Loss: 5.1569\n",
      "Epoch [5/5], Step [10200/34168], Loss: 5.0491\n",
      "Epoch [5/5], Step [10275/34168], Loss: 5.0517\n",
      "Epoch [5/5], Step [10350/34168], Loss: 4.9523\n",
      "Epoch [5/5], Step [10425/34168], Loss: 5.2710\n",
      "Epoch [5/5], Step [10500/34168], Loss: 5.0443\n",
      "Epoch [5/5], Step [10575/34168], Loss: 5.1362\n",
      "Epoch [5/5], Step [10650/34168], Loss: 5.1182\n",
      "Epoch [5/5], Step [10725/34168], Loss: 5.0014\n",
      "Epoch [5/5], Step [10800/34168], Loss: 5.0591\n",
      "Epoch [5/5], Step [10875/34168], Loss: 5.0307\n",
      "Epoch [5/5], Step [10950/34168], Loss: 5.2183\n",
      "Epoch [5/5], Step [11025/34168], Loss: 5.2423\n",
      "Epoch [5/5], Step [11100/34168], Loss: 5.2032\n",
      "Epoch [5/5], Step [11175/34168], Loss: 5.0162\n",
      "Epoch [5/5], Step [11250/34168], Loss: 5.1329\n",
      "Epoch [5/5], Step [11325/34168], Loss: 5.0330\n",
      "Epoch [5/5], Step [11400/34168], Loss: 5.0639\n",
      "Epoch [5/5], Step [11475/34168], Loss: 5.1325\n",
      "Epoch [5/5], Step [11550/34168], Loss: 5.1230\n",
      "Epoch [5/5], Step [11625/34168], Loss: 5.1169\n",
      "Epoch [5/5], Step [11700/34168], Loss: 5.0729\n",
      "Epoch [5/5], Step [11775/34168], Loss: 5.0281\n",
      "Epoch [5/5], Step [11850/34168], Loss: 4.9513\n",
      "Epoch [5/5], Step [11925/34168], Loss: 5.1290\n",
      "Epoch [5/5], Step [12000/34168], Loss: 4.9723\n",
      "Epoch [5/5], Step [12075/34168], Loss: 5.0611\n",
      "Epoch [5/5], Step [12150/34168], Loss: 5.1062\n",
      "Epoch [5/5], Step [12225/34168], Loss: 5.0673\n",
      "Epoch [5/5], Step [12300/34168], Loss: 5.0340\n",
      "Epoch [5/5], Step [12375/34168], Loss: 5.1473\n",
      "Epoch [5/5], Step [12450/34168], Loss: 5.1672\n",
      "Epoch [5/5], Step [12525/34168], Loss: 5.0237\n",
      "Epoch [5/5], Step [12600/34168], Loss: 5.0830\n",
      "Epoch [5/5], Step [12675/34168], Loss: 5.1370\n",
      "Epoch [5/5], Step [12750/34168], Loss: 5.1546\n",
      "Epoch [5/5], Step [12825/34168], Loss: 5.0462\n",
      "Epoch [5/5], Step [12900/34168], Loss: 5.0519\n",
      "Epoch [5/5], Step [12975/34168], Loss: 5.0385\n",
      "Epoch [5/5], Step [13050/34168], Loss: 5.1100\n",
      "Epoch [5/5], Step [13125/34168], Loss: 5.0342\n",
      "Epoch [5/5], Step [13200/34168], Loss: 5.1510\n",
      "Epoch [5/5], Step [13275/34168], Loss: 5.0069\n",
      "Epoch [5/5], Step [13350/34168], Loss: 5.0821\n",
      "Epoch [5/5], Step [13425/34168], Loss: 5.1147\n",
      "Epoch [5/5], Step [13500/34168], Loss: 5.1504\n",
      "Epoch [5/5], Step [13575/34168], Loss: 5.1394\n",
      "Epoch [5/5], Step [13650/34168], Loss: 5.1109\n",
      "Epoch [5/5], Step [13725/34168], Loss: 5.0746\n",
      "Epoch [5/5], Step [13800/34168], Loss: 5.0238\n",
      "Epoch [5/5], Step [13875/34168], Loss: 4.9399\n",
      "Epoch [5/5], Step [13950/34168], Loss: 5.1211\n",
      "Epoch [5/5], Step [14025/34168], Loss: 4.9829\n",
      "Epoch [5/5], Step [14100/34168], Loss: 5.1980\n",
      "Epoch [5/5], Step [14175/34168], Loss: 5.0164\n",
      "Epoch [5/5], Step [14250/34168], Loss: 5.1412\n",
      "Epoch [5/5], Step [14325/34168], Loss: 4.9474\n",
      "Epoch [5/5], Step [14400/34168], Loss: 5.0759\n",
      "Epoch [5/5], Step [14475/34168], Loss: 5.1755\n",
      "Epoch [5/5], Step [14550/34168], Loss: 5.1501\n",
      "Epoch [5/5], Step [14625/34168], Loss: 5.0348\n",
      "Epoch [5/5], Step [14700/34168], Loss: 5.0756\n",
      "Epoch [5/5], Step [14775/34168], Loss: 5.0529\n",
      "Epoch [5/5], Step [14850/34168], Loss: 5.0145\n",
      "Epoch [5/5], Step [14925/34168], Loss: 5.0174\n",
      "Epoch [5/5], Step [15000/34168], Loss: 4.9565\n",
      "Epoch [5/5], Step [15075/34168], Loss: 5.1420\n",
      "Epoch [5/5], Step [15150/34168], Loss: 5.1190\n",
      "Epoch [5/5], Step [15225/34168], Loss: 5.0312\n",
      "Epoch [5/5], Step [15300/34168], Loss: 5.1522\n",
      "Epoch [5/5], Step [15375/34168], Loss: 5.0738\n",
      "Epoch [5/5], Step [15450/34168], Loss: 5.0779\n",
      "Epoch [5/5], Step [15525/34168], Loss: 5.1220\n",
      "Epoch [5/5], Step [15600/34168], Loss: 5.1202\n",
      "Epoch [5/5], Step [15675/34168], Loss: 5.0183\n",
      "Epoch [5/5], Step [15750/34168], Loss: 5.0203\n",
      "Epoch [5/5], Step [15825/34168], Loss: 5.0693\n",
      "Epoch [5/5], Step [15900/34168], Loss: 5.1401\n",
      "Epoch [5/5], Step [15975/34168], Loss: 5.1763\n",
      "Epoch [5/5], Step [16050/34168], Loss: 4.8815\n",
      "Epoch [5/5], Step [16125/34168], Loss: 4.9932\n",
      "Epoch [5/5], Step [16200/34168], Loss: 5.0965\n",
      "Epoch [5/5], Step [16275/34168], Loss: 5.0173\n",
      "Epoch [5/5], Step [16350/34168], Loss: 5.1589\n",
      "Epoch [5/5], Step [16425/34168], Loss: 5.1899\n",
      "Epoch [5/5], Step [16500/34168], Loss: 4.9882\n",
      "Epoch [5/5], Step [16575/34168], Loss: 5.0898\n",
      "Epoch [5/5], Step [16650/34168], Loss: 5.1624\n",
      "Epoch [5/5], Step [16725/34168], Loss: 4.9734\n",
      "Epoch [5/5], Step [16800/34168], Loss: 5.0136\n",
      "Epoch [5/5], Step [16875/34168], Loss: 5.0353\n",
      "Epoch [5/5], Step [16950/34168], Loss: 5.1591\n",
      "Epoch [5/5], Step [17025/34168], Loss: 5.0559\n",
      "Epoch [5/5], Step [17100/34168], Loss: 4.9522\n",
      "Epoch [5/5], Step [17175/34168], Loss: 5.1036\n",
      "Epoch [5/5], Step [17250/34168], Loss: 5.0129\n",
      "Epoch [5/5], Step [17325/34168], Loss: 5.1040\n",
      "Epoch [5/5], Step [17400/34168], Loss: 5.0595\n",
      "Epoch [5/5], Step [17475/34168], Loss: 5.0391\n",
      "Epoch [5/5], Step [17550/34168], Loss: 5.1419\n",
      "Epoch [5/5], Step [17625/34168], Loss: 5.1472\n",
      "Epoch [5/5], Step [17700/34168], Loss: 5.0044\n",
      "Epoch [5/5], Step [17775/34168], Loss: 5.2196\n",
      "Epoch [5/5], Step [17850/34168], Loss: 5.1370\n",
      "Epoch [5/5], Step [17925/34168], Loss: 5.0426\n",
      "Epoch [5/5], Step [18000/34168], Loss: 4.9539\n",
      "Epoch [5/5], Step [18075/34168], Loss: 4.9846\n",
      "Epoch [5/5], Step [18150/34168], Loss: 5.1738\n",
      "Epoch [5/5], Step [18225/34168], Loss: 5.0548\n",
      "Epoch [5/5], Step [18300/34168], Loss: 4.8840\n",
      "Epoch [5/5], Step [18375/34168], Loss: 5.0543\n",
      "Epoch [5/5], Step [18450/34168], Loss: 5.1563\n",
      "Epoch [5/5], Step [18525/34168], Loss: 4.9994\n",
      "Epoch [5/5], Step [18600/34168], Loss: 4.9677\n",
      "Epoch [5/5], Step [18675/34168], Loss: 5.1786\n",
      "Epoch [5/5], Step [18750/34168], Loss: 5.0784\n",
      "Epoch [5/5], Step [18825/34168], Loss: 5.2318\n",
      "Epoch [5/5], Step [18900/34168], Loss: 5.0526\n",
      "Epoch [5/5], Step [18975/34168], Loss: 5.1193\n",
      "Epoch [5/5], Step [19050/34168], Loss: 5.0863\n",
      "Epoch [5/5], Step [19125/34168], Loss: 5.0801\n",
      "Epoch [5/5], Step [19200/34168], Loss: 5.0622\n",
      "Epoch [5/5], Step [19275/34168], Loss: 5.0783\n",
      "Epoch [5/5], Step [19350/34168], Loss: 5.0290\n",
      "Epoch [5/5], Step [19425/34168], Loss: 5.0009\n",
      "Epoch [5/5], Step [19500/34168], Loss: 5.1151\n",
      "Epoch [5/5], Step [19575/34168], Loss: 5.0019\n",
      "Epoch [5/5], Step [19650/34168], Loss: 5.0455\n",
      "Epoch [5/5], Step [19725/34168], Loss: 5.0126\n",
      "Epoch [5/5], Step [19800/34168], Loss: 5.0511\n",
      "Epoch [5/5], Step [19875/34168], Loss: 5.0997\n",
      "Epoch [5/5], Step [19950/34168], Loss: 5.0640\n",
      "Validation perplexity: 122.35976407295217\n",
      "Epoch [5/5], Step [20025/34168], Loss: 5.1240\n",
      "Epoch [5/5], Step [20100/34168], Loss: 4.9189\n",
      "Epoch [5/5], Step [20175/34168], Loss: 5.0733\n",
      "Epoch [5/5], Step [20250/34168], Loss: 5.0034\n",
      "Epoch [5/5], Step [20325/34168], Loss: 5.0279\n",
      "Epoch [5/5], Step [20400/34168], Loss: 5.1368\n",
      "Epoch [5/5], Step [20475/34168], Loss: 5.1364\n",
      "Epoch [5/5], Step [20550/34168], Loss: 5.0049\n",
      "Epoch [5/5], Step [20625/34168], Loss: 5.0325\n",
      "Epoch [5/5], Step [20700/34168], Loss: 5.1733\n",
      "Epoch [5/5], Step [20775/34168], Loss: 5.0526\n",
      "Epoch [5/5], Step [20850/34168], Loss: 4.9647\n",
      "Epoch [5/5], Step [20925/34168], Loss: 5.1530\n",
      "Epoch [5/5], Step [21000/34168], Loss: 4.9982\n",
      "Epoch [5/5], Step [21075/34168], Loss: 5.0844\n",
      "Epoch [5/5], Step [21150/34168], Loss: 4.9023\n",
      "Epoch [5/5], Step [21225/34168], Loss: 5.0391\n",
      "Epoch [5/5], Step [21300/34168], Loss: 5.0466\n",
      "Epoch [5/5], Step [21375/34168], Loss: 5.1478\n",
      "Epoch [5/5], Step [21450/34168], Loss: 5.1734\n",
      "Epoch [5/5], Step [21525/34168], Loss: 5.0172\n",
      "Epoch [5/5], Step [21600/34168], Loss: 5.1996\n",
      "Epoch [5/5], Step [21675/34168], Loss: 5.0823\n",
      "Epoch [5/5], Step [21750/34168], Loss: 5.1313\n",
      "Epoch [5/5], Step [21825/34168], Loss: 5.0046\n",
      "Epoch [5/5], Step [21900/34168], Loss: 4.9894\n",
      "Epoch [5/5], Step [21975/34168], Loss: 5.0442\n",
      "Epoch [5/5], Step [22050/34168], Loss: 5.1249\n",
      "Epoch [5/5], Step [22125/34168], Loss: 5.1376\n",
      "Epoch [5/5], Step [22200/34168], Loss: 5.0775\n",
      "Epoch [5/5], Step [22275/34168], Loss: 5.1248\n",
      "Epoch [5/5], Step [22350/34168], Loss: 4.9996\n",
      "Epoch [5/5], Step [22425/34168], Loss: 5.0656\n",
      "Epoch [5/5], Step [22500/34168], Loss: 5.1434\n",
      "Epoch [5/5], Step [22575/34168], Loss: 4.9172\n",
      "Epoch [5/5], Step [22650/34168], Loss: 4.9554\n",
      "Epoch [5/5], Step [22725/34168], Loss: 5.1283\n",
      "Epoch [5/5], Step [22800/34168], Loss: 5.1680\n",
      "Epoch [5/5], Step [22875/34168], Loss: 4.9976\n",
      "Epoch [5/5], Step [22950/34168], Loss: 5.1582\n",
      "Epoch [5/5], Step [23025/34168], Loss: 5.2411\n",
      "Epoch [5/5], Step [23100/34168], Loss: 5.2033\n",
      "Epoch [5/5], Step [23175/34168], Loss: 5.0029\n",
      "Epoch [5/5], Step [23250/34168], Loss: 5.0987\n",
      "Epoch [5/5], Step [23325/34168], Loss: 5.0992\n",
      "Epoch [5/5], Step [23400/34168], Loss: 5.0620\n",
      "Epoch [5/5], Step [23475/34168], Loss: 5.0717\n",
      "Epoch [5/5], Step [23550/34168], Loss: 5.0633\n",
      "Epoch [5/5], Step [23625/34168], Loss: 5.1269\n",
      "Epoch [5/5], Step [23700/34168], Loss: 5.1243\n",
      "Epoch [5/5], Step [23775/34168], Loss: 5.0326\n",
      "Epoch [5/5], Step [23850/34168], Loss: 5.0889\n",
      "Epoch [5/5], Step [23925/34168], Loss: 5.0287\n",
      "Epoch [5/5], Step [24000/34168], Loss: 5.1589\n",
      "Epoch [5/5], Step [24075/34168], Loss: 5.1653\n",
      "Epoch [5/5], Step [24150/34168], Loss: 5.1064\n",
      "Epoch [5/5], Step [24225/34168], Loss: 5.2758\n",
      "Epoch [5/5], Step [24300/34168], Loss: 5.1612\n",
      "Epoch [5/5], Step [24375/34168], Loss: 5.1560\n",
      "Epoch [5/5], Step [24450/34168], Loss: 4.9317\n",
      "Epoch [5/5], Step [24525/34168], Loss: 5.2016\n",
      "Epoch [5/5], Step [24600/34168], Loss: 5.0986\n",
      "Epoch [5/5], Step [24675/34168], Loss: 5.0230\n",
      "Epoch [5/5], Step [24750/34168], Loss: 4.9976\n",
      "Epoch [5/5], Step [24825/34168], Loss: 5.0390\n",
      "Epoch [5/5], Step [24900/34168], Loss: 5.1384\n",
      "Epoch [5/5], Step [24975/34168], Loss: 4.9790\n",
      "Epoch [5/5], Step [25050/34168], Loss: 5.0957\n",
      "Epoch [5/5], Step [25125/34168], Loss: 5.0500\n",
      "Epoch [5/5], Step [25200/34168], Loss: 5.0463\n",
      "Epoch [5/5], Step [25275/34168], Loss: 5.1448\n",
      "Epoch [5/5], Step [25350/34168], Loss: 5.0712\n",
      "Epoch [5/5], Step [25425/34168], Loss: 4.9807\n",
      "Epoch [5/5], Step [25500/34168], Loss: 5.0839\n",
      "Epoch [5/5], Step [25575/34168], Loss: 5.1132\n",
      "Epoch [5/5], Step [25650/34168], Loss: 5.1005\n",
      "Epoch [5/5], Step [25725/34168], Loss: 5.0870\n",
      "Epoch [5/5], Step [25800/34168], Loss: 5.0768\n",
      "Epoch [5/5], Step [25875/34168], Loss: 5.0407\n",
      "Epoch [5/5], Step [25950/34168], Loss: 4.9100\n",
      "Epoch [5/5], Step [26025/34168], Loss: 5.1885\n",
      "Epoch [5/5], Step [26100/34168], Loss: 4.9093\n",
      "Epoch [5/5], Step [26175/34168], Loss: 5.1221\n",
      "Epoch [5/5], Step [26250/34168], Loss: 5.0806\n",
      "Epoch [5/5], Step [26325/34168], Loss: 5.0746\n",
      "Epoch [5/5], Step [26400/34168], Loss: 5.0201\n",
      "Epoch [5/5], Step [26475/34168], Loss: 5.0848\n",
      "Epoch [5/5], Step [26550/34168], Loss: 5.0145\n",
      "Epoch [5/5], Step [26625/34168], Loss: 5.0072\n",
      "Epoch [5/5], Step [26700/34168], Loss: 5.0487\n",
      "Epoch [5/5], Step [26775/34168], Loss: 5.0997\n",
      "Epoch [5/5], Step [26850/34168], Loss: 5.1540\n",
      "Epoch [5/5], Step [26925/34168], Loss: 5.0057\n",
      "Epoch [5/5], Step [27000/34168], Loss: 4.9750\n",
      "Epoch [5/5], Step [27075/34168], Loss: 5.1027\n",
      "Epoch [5/5], Step [27150/34168], Loss: 4.9946\n",
      "Epoch [5/5], Step [27225/34168], Loss: 5.1112\n",
      "Epoch [5/5], Step [27300/34168], Loss: 5.1680\n",
      "Epoch [5/5], Step [27375/34168], Loss: 5.1838\n",
      "Epoch [5/5], Step [27450/34168], Loss: 4.9726\n",
      "Epoch [5/5], Step [27525/34168], Loss: 5.1408\n",
      "Epoch [5/5], Step [27600/34168], Loss: 5.0976\n",
      "Epoch [5/5], Step [27675/34168], Loss: 5.0429\n",
      "Epoch [5/5], Step [27750/34168], Loss: 5.1347\n",
      "Epoch [5/5], Step [27825/34168], Loss: 5.0548\n",
      "Epoch [5/5], Step [27900/34168], Loss: 4.9051\n",
      "Epoch [5/5], Step [27975/34168], Loss: 5.2424\n",
      "Epoch [5/5], Step [28050/34168], Loss: 5.0407\n",
      "Epoch [5/5], Step [28125/34168], Loss: 5.1987\n",
      "Epoch [5/5], Step [28200/34168], Loss: 5.0978\n",
      "Epoch [5/5], Step [28275/34168], Loss: 5.0330\n",
      "Epoch [5/5], Step [28350/34168], Loss: 5.1426\n",
      "Epoch [5/5], Step [28425/34168], Loss: 4.9839\n",
      "Epoch [5/5], Step [28500/34168], Loss: 5.1297\n",
      "Epoch [5/5], Step [28575/34168], Loss: 5.0602\n",
      "Epoch [5/5], Step [28650/34168], Loss: 5.0746\n",
      "Epoch [5/5], Step [28725/34168], Loss: 5.2573\n",
      "Epoch [5/5], Step [28800/34168], Loss: 5.2755\n",
      "Epoch [5/5], Step [28875/34168], Loss: 5.0235\n",
      "Epoch [5/5], Step [28950/34168], Loss: 5.1087\n",
      "Epoch [5/5], Step [29025/34168], Loss: 5.0126\n",
      "Epoch [5/5], Step [29100/34168], Loss: 4.9912\n",
      "Epoch [5/5], Step [29175/34168], Loss: 5.0989\n",
      "Epoch [5/5], Step [29250/34168], Loss: 5.1181\n",
      "Epoch [5/5], Step [29325/34168], Loss: 5.0290\n",
      "Epoch [5/5], Step [29400/34168], Loss: 4.9832\n",
      "Epoch [5/5], Step [29475/34168], Loss: 5.1795\n",
      "Epoch [5/5], Step [29550/34168], Loss: 5.1680\n",
      "Epoch [5/5], Step [29625/34168], Loss: 4.9794\n",
      "Epoch [5/5], Step [29700/34168], Loss: 5.0594\n",
      "Epoch [5/5], Step [29775/34168], Loss: 4.9115\n",
      "Epoch [5/5], Step [29850/34168], Loss: 5.0701\n",
      "Epoch [5/5], Step [29925/34168], Loss: 4.9451\n",
      "Epoch [5/5], Step [30000/34168], Loss: 4.9963\n",
      "Validation perplexity: 120.80191713064477\n",
      "Epoch [5/5], Step [30075/34168], Loss: 5.1141\n",
      "Epoch [5/5], Step [30150/34168], Loss: 4.9187\n",
      "Epoch [5/5], Step [30225/34168], Loss: 5.0055\n",
      "Epoch [5/5], Step [30300/34168], Loss: 4.9513\n",
      "Epoch [5/5], Step [30375/34168], Loss: 5.2003\n",
      "Epoch [5/5], Step [30450/34168], Loss: 5.1291\n",
      "Epoch [5/5], Step [30525/34168], Loss: 4.9865\n",
      "Epoch [5/5], Step [30600/34168], Loss: 5.3489\n",
      "Epoch [5/5], Step [30675/34168], Loss: 5.1144\n",
      "Epoch [5/5], Step [30750/34168], Loss: 5.0077\n",
      "Epoch [5/5], Step [30825/34168], Loss: 5.0187\n",
      "Epoch [5/5], Step [30900/34168], Loss: 5.1498\n",
      "Epoch [5/5], Step [30975/34168], Loss: 5.1736\n",
      "Epoch [5/5], Step [31050/34168], Loss: 5.0458\n",
      "Epoch [5/5], Step [31125/34168], Loss: 5.1309\n",
      "Epoch [5/5], Step [31200/34168], Loss: 5.1084\n",
      "Epoch [5/5], Step [31275/34168], Loss: 4.9911\n",
      "Epoch [5/5], Step [31350/34168], Loss: 4.9488\n",
      "Epoch [5/5], Step [31425/34168], Loss: 5.1815\n",
      "Epoch [5/5], Step [31500/34168], Loss: 5.0621\n",
      "Epoch [5/5], Step [31575/34168], Loss: 4.9938\n",
      "Epoch [5/5], Step [31650/34168], Loss: 5.0225\n",
      "Epoch [5/5], Step [31725/34168], Loss: 5.0465\n",
      "Epoch [5/5], Step [31800/34168], Loss: 5.0284\n",
      "Epoch [5/5], Step [31875/34168], Loss: 5.1403\n",
      "Epoch [5/5], Step [31950/34168], Loss: 5.0745\n",
      "Epoch [5/5], Step [32025/34168], Loss: 5.0471\n",
      "Epoch [5/5], Step [32100/34168], Loss: 4.9974\n",
      "Epoch [5/5], Step [32175/34168], Loss: 5.0427\n",
      "Epoch [5/5], Step [32250/34168], Loss: 5.1172\n",
      "Epoch [5/5], Step [32325/34168], Loss: 4.9562\n",
      "Epoch [5/5], Step [32400/34168], Loss: 5.0629\n",
      "Epoch [5/5], Step [32475/34168], Loss: 5.0428\n",
      "Epoch [5/5], Step [32550/34168], Loss: 5.0317\n",
      "Epoch [5/5], Step [32625/34168], Loss: 4.9269\n",
      "Epoch [5/5], Step [32700/34168], Loss: 4.9887\n",
      "Epoch [5/5], Step [32775/34168], Loss: 5.0897\n",
      "Epoch [5/5], Step [32850/34168], Loss: 5.0453\n",
      "Epoch [5/5], Step [32925/34168], Loss: 5.1009\n",
      "Epoch [5/5], Step [33000/34168], Loss: 5.1049\n",
      "Epoch [5/5], Step [33075/34168], Loss: 5.1228\n",
      "Epoch [5/5], Step [33150/34168], Loss: 5.0791\n",
      "Epoch [5/5], Step [33225/34168], Loss: 5.0165\n",
      "Epoch [5/5], Step [33300/34168], Loss: 5.1076\n",
      "Epoch [5/5], Step [33375/34168], Loss: 4.9791\n",
      "Epoch [5/5], Step [33450/34168], Loss: 5.0689\n",
      "Epoch [5/5], Step [33525/34168], Loss: 5.1879\n",
      "Epoch [5/5], Step [33600/34168], Loss: 5.0969\n",
      "Epoch [5/5], Step [33675/34168], Loss: 5.0679\n",
      "Epoch [5/5], Step [33750/34168], Loss: 5.0029\n",
      "Epoch [5/5], Step [33825/34168], Loss: 5.1530\n",
      "Epoch [5/5], Step [33900/34168], Loss: 5.0465\n",
      "Epoch [5/5], Step [33975/34168], Loss: 5.0236\n",
      "Epoch [5/5], Step [34050/34168], Loss: 5.0700\n",
      "Epoch [5/5], Step [34125/34168], Loss: 5.0632\n",
      "Epoch [5/5] Average Loss: 5.0759, Perplexity: 160.11\n"
     ]
    }
   ],
   "source": [
    "from src.trainComplete import TrainComplete\n",
    "from src.attentionModel import LanguageModelWithAttention\n",
    "\n",
    "trainclass = TrainComplete(text_path = text_path,path_to_save_folder= path_to_save_folder,tokenizer = tokenizer,\n",
    "                           allowed_special=False, is_attention_training = True)\n",
    "\n",
    "\n",
    "context_length = 32  # Increased context size\n",
    "embedding_dim = 128\n",
    "attention_dim = 64\n",
    "hidden_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "model = LanguageModelWithAttention(\n",
    "    vocab_size, embedding_dim, attention_dim, context_length, hidden_dim, num_heads, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "trainclass.train(model,\n",
    "              vocab_size,device,raw_text,\"pers_attention_standard_dropout_batchsize64_ep5_eval10000\",\n",
    "                print_every=75,evaluate_every=10000,optimizer=None,criterion=None,\n",
    "              batch_size = 64,\n",
    "              embedding_dim = embedding_dim,\n",
    "              context_length = context_length,\n",
    "              num_epochs = 5\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0abdfa-b07b-424e-8e1c-e407834fe186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cab36-a678-40d6-b0e2-5264f644983c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
