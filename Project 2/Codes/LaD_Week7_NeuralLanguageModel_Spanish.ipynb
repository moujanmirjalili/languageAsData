{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3c5c90",
   "metadata": {},
   "source": [
    "# SpanishWeek 7 - Building and Improving a Simple Language Model\n",
    "\n",
    "Welcome back! In Week 6, we learned how to prepare textual data for training a language model. We generated input-target pairs using a DataLoader. This week, we'll build upon that foundation to implement and improve a simple neural network language model.\n",
    "\n",
    "This notebook was created by Qumeng Sun and Lisa Beinborn. It adapts parts from Sebastian Raschka's notebooks accompanying his book \"Build a Large Language Model (from Scratch)\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3aeb197-244c-42a7-8e70-8c240e6c091d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install 'torch>=2.0.1' 'jupyterlab>=4.0' 'tiktoken>=0.5.1' 'numpy>=1.25,<2.0' --user\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead8b6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.1.post300\n",
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from importlib.metadata import version\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4ed7a",
   "metadata": {},
   "source": [
    "## 1. Review of data preparation\n",
    "\n",
    "First, let's revisit how we prepared our data last week. We'll load the text data, tokenize it using the GPT-2 tokenizer, and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0651954-feb9-4437-93a8-e838c807ce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines replaced 30000\n"
     ]
    }
   ],
   "source": [
    "from src.helper  import get_cleaned_spanish_text_as_string\n",
    "text_path = \"content/spa_wikipedia_2021_30K-sentences.txt\"\n",
    "path_to_save_folder= \"model/train_data\"\n",
    "\n",
    "raw_text = get_cleaned_spanish_text_as_string(text_path)\n",
    "#enc_text = tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788d6b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines replaced 30000\n",
      "Total number of tokens: 1221649\n",
      "First 10 tokens: [1105, 390, 450, 22379, 390, 8235, 1658, 555, 64, 719]\n"
     ]
    }
   ],
   "source": [
    "# Load the text data\n",
    "raw_text = get_cleaned_spanish_text_as_string(text_path)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Tokenize the text\n",
    "token_ids = tokenizer.encode(raw_text)\n",
    "\n",
    "print(\"Total number of tokens:\", len(token_ids))\n",
    "print(\"First 10 tokens:\", token_ids[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ddcddc",
   "metadata": {},
   "source": [
    "## 2. Preparing dataset and dataloader\n",
    "\n",
    "We'll use the same `GPTDataset` class and `create_dataloader` function that we defined in Week 6 to generate input-target pairs where the target is the input sequence shifted by one token to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7e4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from src.dataset import GPTDataset\n",
    "from src.dataset import create_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9446d5b7",
   "metadata": {},
   "source": [
    "## 3. Training and evaluating a base model\n",
    "\n",
    "We'll start by defining and training a simplistic language model to understand the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf3e3d",
   "metadata": {},
   "source": [
    "### 3.1. Defining the model\n",
    "\n",
    "Our base model will consist of:\n",
    "- **Token Embeddings**: Convert token IDs to dense vectors.\n",
    "- **Positional Embeddings**: Incorporate positional information.\n",
    "- **Linear Layer**: Predict the next token in the sequence.\n",
    "\n",
    "We'll set an appropriate `context_length` during initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e43abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import SimpleLanguageModel\n",
    "\n",
    "#With Relu and two Linear layers\n",
    "from src.model import LanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34614df7",
   "metadata": {},
   "source": [
    "### 3.2. Setting up training parameters\n",
    "\n",
    "We'll initialize our model with an appropriate `context_length` and prepare for training.\n",
    "\n",
    "Check the torch documentation for the description of [CrossEntropyLoss](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#CrossEntropyLoss) and try to understand what it means that it \"is equivalent to applying LogSoftmax on an input, followed by NLLLoss.\"\n",
    "\n",
    "Check the documentation for the [AdamOptimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) and make sure you understand the role of the lr parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7829793b-09e1-47b7-879a-8d134a968446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 128\n",
    "context_length = 32  # Context size for training\n",
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "537273b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      " Create Dataset 1220000 / 1221617"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_dataloader(\n",
    "    raw_text, batch_size=batch_size, \n",
    "    context_length=context_length, shuffle=True\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleLanguageModel(vocab_size, embedding_dim, context_length).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters\n",
    "num_epochs = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f38944d4-9db5-4033-bdb1-7b9484c06e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 20\n",
    "evaluate_every = 3000 \n",
    "train_run_label = \"temp_simple\"\n",
    "data_loader = train_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0728c",
   "metadata": {},
   "source": [
    "### 3.3. Training the model\n",
    "\n",
    "Let's train a very simple model and monitor the loss. This will take a while. \n",
    "Make sure you understand every step of the code at least conceptually and consult the pytorch documentation. If the training process takes too long, test it with a smaller portion of the dataset and/or fewer epochs first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99a43d86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [0/7635], Loss: 11.1786\n",
      "Validation perplexity: 67887.43459883431\n",
      "Epoch [1/1], Step [20/7635], Loss: 10.3626\n",
      "Epoch [1/1], Step [40/7635], Loss: 9.5393\n",
      "Epoch [1/1], Step [60/7635], Loss: 8.6097\n",
      "Epoch [1/1], Step [80/7635], Loss: 7.6742\n",
      "Epoch [1/1], Step [100/7635], Loss: 6.9813\n",
      "Epoch [1/1], Step [120/7635], Loss: 6.5762\n",
      "Epoch [1/1], Step [140/7635], Loss: 6.2535\n",
      "Epoch [1/1], Step [160/7635], Loss: 6.0478\n",
      "Epoch [1/1], Step [180/7635], Loss: 5.7647\n",
      "Epoch [1/1], Step [200/7635], Loss: 5.7536\n",
      "Epoch [1/1], Step [220/7635], Loss: 5.6039\n",
      "Epoch [1/1], Step [240/7635], Loss: 5.6144\n",
      "Epoch [1/1], Step [260/7635], Loss: 5.4750\n",
      "Epoch [1/1], Step [280/7635], Loss: 5.3255\n",
      "Epoch [1/1], Step [300/7635], Loss: 5.3269\n",
      "Epoch [1/1], Step [320/7635], Loss: 5.2694\n",
      "Epoch [1/1], Step [340/7635], Loss: 5.2289\n",
      "Epoch [1/1], Step [360/7635], Loss: 5.0981\n",
      "Epoch [1/1], Step [380/7635], Loss: 5.2223\n",
      "Epoch [1/1], Step [400/7635], Loss: 5.2424\n",
      "Epoch [1/1], Step [420/7635], Loss: 5.1484\n",
      "Epoch [1/1], Step [440/7635], Loss: 5.1366\n",
      "Epoch [1/1], Step [460/7635], Loss: 5.0687\n",
      "Epoch [1/1], Step [480/7635], Loss: 4.8852\n",
      "Epoch [1/1], Step [500/7635], Loss: 4.8847\n",
      "Epoch [1/1], Step [520/7635], Loss: 4.9020\n",
      "Epoch [1/1], Step [540/7635], Loss: 4.8162\n",
      "Epoch [1/1], Step [560/7635], Loss: 4.8818\n",
      "Epoch [1/1], Step [580/7635], Loss: 4.9629\n",
      "Epoch [1/1], Step [600/7635], Loss: 4.8932\n",
      "Epoch [1/1], Step [620/7635], Loss: 4.8506\n",
      "Epoch [1/1], Step [640/7635], Loss: 4.7879\n",
      "Epoch [1/1], Step [660/7635], Loss: 4.8109\n",
      "Epoch [1/1], Step [680/7635], Loss: 4.7826\n",
      "Epoch [1/1], Step [700/7635], Loss: 4.6004\n",
      "Epoch [1/1], Step [720/7635], Loss: 4.7323\n",
      "Epoch [1/1], Step [740/7635], Loss: 4.7919\n",
      "Epoch [1/1], Step [760/7635], Loss: 4.8255\n",
      "Epoch [1/1], Step [780/7635], Loss: 4.6402\n",
      "Epoch [1/1], Step [800/7635], Loss: 4.6539\n",
      "Epoch [1/1], Step [820/7635], Loss: 4.6533\n",
      "Epoch [1/1], Step [840/7635], Loss: 4.6915\n",
      "Epoch [1/1], Step [860/7635], Loss: 4.6959\n",
      "Epoch [1/1], Step [880/7635], Loss: 4.5835\n",
      "Epoch [1/1], Step [900/7635], Loss: 4.6256\n",
      "Epoch [1/1], Step [920/7635], Loss: 4.5720\n",
      "Epoch [1/1], Step [940/7635], Loss: 4.6087\n",
      "Epoch [1/1], Step [960/7635], Loss: 4.5326\n",
      "Epoch [1/1], Step [980/7635], Loss: 4.5316\n",
      "Epoch [1/1], Step [1000/7635], Loss: 4.4928\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#(all_losses,train_losses,perplexities) = train(model,num_epochs,optimizer,criterion,data_loader,path_to_save_folder,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#                                               train_run_label,vocab_size,device,print_every)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m (all_losses,train_losses,perplexities,all_perplex) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpath_to_save_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_run_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mevaluate_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdev_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/vast-react/home/joris.hellwig/u13685/jupyterhub-gwdg/src/train.py:41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, optimizer, criterion, data_loader, path_to_save_folder, train_run_label, vocab_size, device, evaluate_every, dev_dataloader, print_every)\u001b[0m\n\u001b[1;32m     39\u001b[0m all_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_save_folder\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/temp_save\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m evaluate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     43\u001b[0m     perplexity_simple \u001b[38;5;241m=\u001b[39m evaluate(model, dev_dataloader,criterion,device,vocab_size)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 628\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:862\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    861\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 862\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# train(model,num_epochs,optimizer,criterion,data_loader,path_to_save_folder,train_run_label)\n",
    "#return (all_losses,train_losses,perplexities)\n",
    "from src.train import train\n",
    "#(all_losses,train_losses,perplexities) = train(model,num_epochs,optimizer,criterion,data_loader,path_to_save_folder,\n",
    "#                                               train_run_label,vocab_size,device,print_every)\n",
    "\n",
    "(all_losses,train_losses,perplexities,all_perplex) = train(model,num_epochs,optimizer,criterion,data_loader,path_to_save_folder,train_run_label,vocab_size,device,evaluate_every,dev_dataloader,print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5c60d7d-b4a0-4bae-9219-b08120f3976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To read old results \n",
    "#read_list_from_file(label,path_to_save_folder)\n",
    "from src.helper import read_list_from_file\n",
    "#train_losses=read_list_from_file(\"onlyLossesUntil3593\",path_to_save_folder)\n",
    "#perplexities= read_list_from_file(\"normal_model_perplexities\",path_to_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a0686aa-a320-4e62-8f2b-45f1cd520813",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_two\n\u001b[0;32m----> 2\u001b[0m plot_two(\u001b[43mall_losses\u001b[49m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss For all Steps\u001b[39m\u001b[38;5;124m\"\u001b[39m,train_losses,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mla2\u001b[39m\u001b[38;5;124m\"\u001b[39m,axLabel1\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSteps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_losses' is not defined"
     ]
    }
   ],
   "source": [
    "from src.plot import plot_two\n",
    "plot_two(all_losses,\"Loss For all Steps\",train_losses,\"la2\",axLabel1=(\"Steps\",\"Loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa1dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf0af7df",
   "metadata": {},
   "source": [
    "### 3.4. Evaluating the model\n",
    "\n",
    "Now, we'll compute the perplexity of our simplest model on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ff9410d-e23f-4036-bbb3-8f280a42b55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of base model: 50.65\n"
     ]
    }
   ],
   "source": [
    "from src.train import evaluate\n",
    "perplexity_simple = evaluate(model, dev_dataloader,criterion,device,vocab_size)\n",
    "print(f\"Perplexity of base model: {perplexity_simple:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124e2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cc0d402",
   "metadata": {},
   "source": [
    "## 4. Training with dropout\n",
    "\n",
    "To prevent overfitting and improve generalization, we'll test dropout as a regularization strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd5a2df",
   "metadata": {},
   "source": [
    "### 4.1. Adding dropout\n",
    "\n",
    "We'll modify our model to include a dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6bdc5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import RegularizedLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c182d5e3",
   "metadata": {},
   "source": [
    "### 4.2. Retraining the model with dropout\n",
    "\n",
    "We'll re-initialize the model and optimizer, then retrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef5c899f-0994-4913-98c0-aab357b12f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [0/7635], Loss: 11.2415\n",
      "Epoch [1/1], Step [1/7635], Loss: 11.2124\n",
      "Epoch [1/1], Step [2/7635], Loss: 11.1550\n",
      "Epoch [1/1], Step [3/7635], Loss: 11.1095\n",
      "Epoch [1/1], Step [4/7635], Loss: 11.0850\n",
      "Epoch [1/1], Step [5/7635], Loss: 11.0378\n",
      "Epoch [1/1], Step [6/7635], Loss: 11.0372\n",
      "Epoch [1/1], Step [7/7635], Loss: 10.9615\n",
      "Epoch [1/1], Step [8/7635], Loss: 10.8956\n",
      "Epoch [1/1], Step [9/7635], Loss: 10.8735\n",
      "Epoch [1/1], Step [10/7635], Loss: 10.7948\n",
      "Epoch [1/1], Step [11/7635], Loss: 10.7650\n",
      "Epoch [1/1], Step [12/7635], Loss: 10.7319\n",
      "Epoch [1/1], Step [13/7635], Loss: 10.7051\n",
      "Epoch [1/1], Step [14/7635], Loss: 10.6483\n",
      "Epoch [1/1], Step [15/7635], Loss: 10.5948\n",
      "Epoch [1/1], Step [16/7635], Loss: 10.5629\n",
      "Epoch [1/1], Step [17/7635], Loss: 10.5020\n",
      "Epoch [1/1], Step [18/7635], Loss: 10.4640\n",
      "Epoch [1/1], Step [19/7635], Loss: 10.4379\n",
      "Epoch [1/1], Step [20/7635], Loss: 10.4157\n",
      "Epoch [1/1], Step [21/7635], Loss: 10.3223\n",
      "Epoch [1/1], Step [22/7635], Loss: 10.3253\n",
      "Epoch [1/1], Step [23/7635], Loss: 10.2734\n",
      "Epoch [1/1], Step [24/7635], Loss: 10.2394\n",
      "Epoch [1/1], Step [25/7635], Loss: 10.1746\n",
      "Epoch [1/1], Step [26/7635], Loss: 10.1414\n",
      "Epoch [1/1], Step [27/7635], Loss: 10.1228\n",
      "Epoch [1/1], Step [28/7635], Loss: 10.0339\n",
      "Epoch [1/1], Step [29/7635], Loss: 10.0085\n",
      "Epoch [1/1], Step [30/7635], Loss: 9.9621\n",
      "Epoch [1/1], Step [31/7635], Loss: 9.9092\n",
      "Epoch [1/1], Step [32/7635], Loss: 9.8621\n",
      "Epoch [1/1], Step [33/7635], Loss: 9.7972\n",
      "Epoch [1/1], Step [34/7635], Loss: 9.8274\n",
      "Epoch [1/1], Step [35/7635], Loss: 9.7975\n",
      "Epoch [1/1], Step [36/7635], Loss: 9.7174\n",
      "Epoch [1/1], Step [37/7635], Loss: 9.6338\n",
      "Epoch [1/1], Step [38/7635], Loss: 9.5899\n",
      "Epoch [1/1], Step [39/7635], Loss: 9.5181\n",
      "Epoch [1/1], Step [40/7635], Loss: 9.4602\n",
      "Epoch [1/1], Step [41/7635], Loss: 9.4698\n",
      "Epoch [1/1], Step [42/7635], Loss: 9.3762\n",
      "Epoch [1/1], Step [43/7635], Loss: 9.3821\n",
      "Epoch [1/1], Step [44/7635], Loss: 9.3703\n",
      "Epoch [1/1], Step [45/7635], Loss: 9.2812\n",
      "Epoch [1/1], Step [46/7635], Loss: 9.1675\n",
      "Epoch [1/1], Step [47/7635], Loss: 9.2161\n",
      "Epoch [1/1], Step [48/7635], Loss: 9.1631\n",
      "Epoch [1/1], Step [49/7635], Loss: 9.0935\n",
      "Epoch [1/1], Step [50/7635], Loss: 9.0251\n",
      "Epoch [1/1], Step [51/7635], Loss: 8.9868\n",
      "Epoch [1/1], Step [52/7635], Loss: 9.0024\n",
      "Epoch [1/1], Step [53/7635], Loss: 8.8945\n",
      "Epoch [1/1], Step [54/7635], Loss: 8.8701\n",
      "Epoch [1/1], Step [55/7635], Loss: 8.7632\n",
      "Epoch [1/1], Step [56/7635], Loss: 8.7571\n",
      "Epoch [1/1], Step [57/7635], Loss: 8.7168\n",
      "Epoch [1/1], Step [58/7635], Loss: 8.6526\n",
      "Epoch [1/1], Step [59/7635], Loss: 8.5972\n",
      "Epoch [1/1], Step [60/7635], Loss: 8.5734\n",
      "Epoch [1/1], Step [61/7635], Loss: 8.5217\n",
      "Epoch [1/1], Step [62/7635], Loss: 8.4507\n",
      "Epoch [1/1], Step [63/7635], Loss: 8.4687\n",
      "Epoch [1/1], Step [64/7635], Loss: 8.3898\n",
      "Epoch [1/1], Step [65/7635], Loss: 8.3220\n",
      "Epoch [1/1], Step [66/7635], Loss: 8.2577\n",
      "Epoch [1/1], Step [67/7635], Loss: 8.2445\n",
      "Epoch [1/1], Step [68/7635], Loss: 8.1714\n",
      "Epoch [1/1], Step [69/7635], Loss: 8.1609\n",
      "Epoch [1/1], Step [70/7635], Loss: 8.0991\n",
      "Epoch [1/1], Step [71/7635], Loss: 8.1231\n",
      "Epoch [1/1], Step [72/7635], Loss: 8.0692\n",
      "Epoch [1/1], Step [73/7635], Loss: 8.0057\n",
      "Epoch [1/1], Step [74/7635], Loss: 7.9618\n",
      "Epoch [1/1], Step [75/7635], Loss: 7.9096\n",
      "Epoch [1/1], Step [76/7635], Loss: 7.9156\n",
      "Epoch [1/1], Step [77/7635], Loss: 7.8219\n",
      "Epoch [1/1], Step [78/7635], Loss: 7.8556\n",
      "Epoch [1/1], Step [79/7635], Loss: 7.7815\n",
      "Epoch [1/1], Step [80/7635], Loss: 7.6901\n",
      "Epoch [1/1], Step [81/7635], Loss: 7.6745\n",
      "Epoch [1/1], Step [82/7635], Loss: 7.5878\n",
      "Epoch [1/1], Step [83/7635], Loss: 7.5533\n",
      "Epoch [1/1], Step [84/7635], Loss: 7.5997\n",
      "Epoch [1/1], Step [85/7635], Loss: 7.5137\n",
      "Epoch [1/1], Step [86/7635], Loss: 7.4496\n",
      "Epoch [1/1], Step [87/7635], Loss: 7.5609\n",
      "Epoch [1/1], Step [88/7635], Loss: 7.4208\n",
      "Epoch [1/1], Step [89/7635], Loss: 7.3722\n",
      "Epoch [1/1], Step [90/7635], Loss: 7.3699\n",
      "Epoch [1/1], Step [91/7635], Loss: 7.3087\n",
      "Epoch [1/1], Step [92/7635], Loss: 7.3185\n",
      "Epoch [1/1], Step [93/7635], Loss: 7.2980\n",
      "Epoch [1/1], Step [94/7635], Loss: 7.2323\n",
      "Epoch [1/1], Step [95/7635], Loss: 7.1313\n",
      "Epoch [1/1], Step [96/7635], Loss: 7.2184\n",
      "Epoch [1/1], Step [97/7635], Loss: 7.1867\n",
      "Epoch [1/1], Step [98/7635], Loss: 7.1862\n",
      "Epoch [1/1], Step [99/7635], Loss: 7.0793\n",
      "Epoch [1/1], Step [100/7635], Loss: 7.0053\n",
      "Epoch [1/1], Step [101/7635], Loss: 7.0261\n",
      "Epoch [1/1], Step [102/7635], Loss: 7.0435\n",
      "Epoch [1/1], Step [103/7635], Loss: 7.0306\n",
      "Epoch [1/1], Step [104/7635], Loss: 6.9437\n",
      "Epoch [1/1], Step [105/7635], Loss: 6.8989\n",
      "Epoch [1/1], Step [106/7635], Loss: 6.8115\n",
      "Epoch [1/1], Step [107/7635], Loss: 6.8311\n",
      "Epoch [1/1], Step [108/7635], Loss: 6.8540\n",
      "Epoch [1/1], Step [109/7635], Loss: 6.9102\n",
      "Epoch [1/1], Step [110/7635], Loss: 6.7781\n",
      "Epoch [1/1], Step [111/7635], Loss: 6.8409\n",
      "Epoch [1/1], Step [112/7635], Loss: 6.7519\n",
      "Epoch [1/1], Step [113/7635], Loss: 6.7502\n",
      "Epoch [1/1], Step [114/7635], Loss: 6.7538\n",
      "Epoch [1/1], Step [115/7635], Loss: 6.7936\n",
      "Epoch [1/1], Step [116/7635], Loss: 6.6333\n",
      "Epoch [1/1], Step [117/7635], Loss: 6.7176\n",
      "Epoch [1/1], Step [118/7635], Loss: 6.7424\n",
      "Epoch [1/1], Step [119/7635], Loss: 6.6300\n",
      "Epoch [1/1], Step [120/7635], Loss: 6.6146\n",
      "Epoch [1/1], Step [121/7635], Loss: 6.5447\n",
      "Epoch [1/1], Step [122/7635], Loss: 6.6607\n",
      "Epoch [1/1], Step [123/7635], Loss: 6.6123\n",
      "Epoch [1/1], Step [124/7635], Loss: 6.5802\n",
      "Epoch [1/1], Step [125/7635], Loss: 6.5922\n",
      "Epoch [1/1], Step [126/7635], Loss: 6.5223\n",
      "Epoch [1/1], Step [127/7635], Loss: 6.5443\n",
      "Epoch [1/1], Step [128/7635], Loss: 6.5546\n",
      "Epoch [1/1], Step [129/7635], Loss: 6.5477\n",
      "Epoch [1/1], Step [130/7635], Loss: 6.4572\n",
      "Epoch [1/1], Step [131/7635], Loss: 6.4913\n",
      "Epoch [1/1], Step [132/7635], Loss: 6.4223\n",
      "Epoch [1/1], Step [133/7635], Loss: 6.4811\n",
      "Epoch [1/1], Step [134/7635], Loss: 6.3885\n",
      "Epoch [1/1], Step [135/7635], Loss: 6.4177\n",
      "Epoch [1/1], Step [136/7635], Loss: 6.3903\n",
      "Epoch [1/1], Step [137/7635], Loss: 6.4265\n",
      "Epoch [1/1], Step [138/7635], Loss: 6.4228\n",
      "Epoch [1/1], Step [139/7635], Loss: 6.3905\n",
      "Epoch [1/1], Step [140/7635], Loss: 6.3227\n",
      "Epoch [1/1], Step [141/7635], Loss: 6.3279\n",
      "Epoch [1/1], Step [142/7635], Loss: 6.3486\n",
      "Epoch [1/1], Step [143/7635], Loss: 6.3285\n",
      "Epoch [1/1], Step [144/7635], Loss: 6.2825\n",
      "Epoch [1/1], Step [145/7635], Loss: 6.3326\n",
      "Epoch [1/1], Step [146/7635], Loss: 6.3237\n",
      "Epoch [1/1], Step [147/7635], Loss: 6.4332\n",
      "Epoch [1/1], Step [148/7635], Loss: 6.2952\n",
      "Epoch [1/1], Step [149/7635], Loss: 6.2897\n",
      "Epoch [1/1], Step [150/7635], Loss: 6.3414\n",
      "Epoch [1/1], Step [151/7635], Loss: 6.2158\n",
      "Epoch [1/1], Step [152/7635], Loss: 6.3050\n",
      "Epoch [1/1], Step [153/7635], Loss: 6.2421\n",
      "Epoch [1/1], Step [154/7635], Loss: 6.1922\n",
      "Epoch [1/1], Step [155/7635], Loss: 6.2199\n",
      "Epoch [1/1], Step [156/7635], Loss: 6.2682\n",
      "Epoch [1/1], Step [157/7635], Loss: 6.2097\n",
      "Epoch [1/1], Step [158/7635], Loss: 6.2459\n",
      "Epoch [1/1], Step [159/7635], Loss: 6.1708\n",
      "Epoch [1/1], Step [160/7635], Loss: 6.1745\n",
      "Epoch [1/1], Step [161/7635], Loss: 6.1630\n",
      "Epoch [1/1], Step [162/7635], Loss: 6.1467\n",
      "Epoch [1/1], Step [163/7635], Loss: 6.1537\n",
      "Epoch [1/1], Step [164/7635], Loss: 6.1371\n",
      "Epoch [1/1], Step [165/7635], Loss: 6.0845\n",
      "Epoch [1/1], Step [166/7635], Loss: 6.1610\n",
      "Epoch [1/1], Step [167/7635], Loss: 6.1009\n",
      "Epoch [1/1], Step [168/7635], Loss: 6.1540\n",
      "Epoch [1/1], Step [169/7635], Loss: 6.1336\n",
      "Epoch [1/1], Step [170/7635], Loss: 6.1491\n",
      "Epoch [1/1], Step [171/7635], Loss: 6.0886\n",
      "Epoch [1/1], Step [172/7635], Loss: 6.0204\n",
      "Epoch [1/1], Step [173/7635], Loss: 6.1194\n",
      "Epoch [1/1], Step [174/7635], Loss: 6.0403\n",
      "Epoch [1/1], Step [175/7635], Loss: 6.0543\n",
      "Epoch [1/1], Step [176/7635], Loss: 6.0296\n",
      "Epoch [1/1], Step [177/7635], Loss: 6.0037\n",
      "Epoch [1/1], Step [178/7635], Loss: 6.0077\n",
      "Epoch [1/1], Step [179/7635], Loss: 6.1414\n",
      "Epoch [1/1], Step [180/7635], Loss: 6.0062\n",
      "Epoch [1/1], Step [181/7635], Loss: 6.0297\n",
      "Epoch [1/1], Step [182/7635], Loss: 5.9788\n",
      "Epoch [1/1], Step [183/7635], Loss: 5.9584\n",
      "Epoch [1/1], Step [184/7635], Loss: 6.1006\n",
      "Epoch [1/1], Step [185/7635], Loss: 6.0042\n",
      "Epoch [1/1], Step [186/7635], Loss: 5.9142\n",
      "Epoch [1/1], Step [187/7635], Loss: 6.0204\n",
      "Epoch [1/1], Step [188/7635], Loss: 6.0391\n",
      "Epoch [1/1], Step [189/7635], Loss: 5.8411\n",
      "Epoch [1/1], Step [190/7635], Loss: 6.0163\n",
      "Epoch [1/1], Step [191/7635], Loss: 5.9628\n",
      "Epoch [1/1], Step [192/7635], Loss: 6.0298\n",
      "Epoch [1/1], Step [193/7635], Loss: 6.0024\n",
      "Epoch [1/1], Step [194/7635], Loss: 5.9437\n",
      "Epoch [1/1], Step [195/7635], Loss: 5.8543\n",
      "Epoch [1/1], Step [196/7635], Loss: 5.9724\n",
      "Epoch [1/1], Step [197/7635], Loss: 5.9827\n",
      "Epoch [1/1], Step [198/7635], Loss: 5.8737\n",
      "Epoch [1/1], Step [199/7635], Loss: 5.8103\n",
      "Epoch [1/1], Step [200/7635], Loss: 5.9260\n",
      "Epoch [1/1], Step [201/7635], Loss: 6.0930\n",
      "Epoch [1/1], Step [202/7635], Loss: 5.9092\n",
      "Epoch [1/1], Step [203/7635], Loss: 5.8766\n",
      "Epoch [1/1], Step [204/7635], Loss: 5.8797\n",
      "Epoch [1/1], Step [205/7635], Loss: 5.8681\n",
      "Epoch [1/1], Step [206/7635], Loss: 5.8627\n",
      "Epoch [1/1], Step [207/7635], Loss: 5.9146\n",
      "Epoch [1/1], Step [208/7635], Loss: 5.9911\n",
      "Epoch [1/1], Step [209/7635], Loss: 5.8916\n",
      "Epoch [1/1], Step [210/7635], Loss: 5.9309\n",
      "Epoch [1/1], Step [211/7635], Loss: 5.9009\n",
      "Epoch [1/1], Step [212/7635], Loss: 5.8778\n",
      "Epoch [1/1], Step [213/7635], Loss: 5.8842\n",
      "Epoch [1/1], Step [214/7635], Loss: 5.8529\n",
      "Epoch [1/1], Step [215/7635], Loss: 5.8433\n",
      "Epoch [1/1], Step [216/7635], Loss: 5.8614\n",
      "Epoch [1/1], Step [217/7635], Loss: 5.8066\n",
      "Epoch [1/1], Step [218/7635], Loss: 5.7490\n",
      "Epoch [1/1], Step [219/7635], Loss: 5.8034\n",
      "Epoch [1/1], Step [220/7635], Loss: 5.8365\n",
      "Epoch [1/1], Step [221/7635], Loss: 5.9425\n",
      "Epoch [1/1], Step [222/7635], Loss: 5.7458\n",
      "Epoch [1/1], Step [223/7635], Loss: 5.8658\n",
      "Epoch [1/1], Step [224/7635], Loss: 5.9440\n",
      "Epoch [1/1], Step [225/7635], Loss: 5.8291\n",
      "Epoch [1/1], Step [226/7635], Loss: 5.7475\n",
      "Epoch [1/1], Step [227/7635], Loss: 5.8004\n",
      "Epoch [1/1], Step [228/7635], Loss: 5.7983\n",
      "Epoch [1/1], Step [229/7635], Loss: 5.9001\n",
      "Epoch [1/1], Step [230/7635], Loss: 5.7268\n",
      "Epoch [1/1], Step [231/7635], Loss: 5.7120\n",
      "Epoch [1/1], Step [232/7635], Loss: 5.8182\n",
      "Epoch [1/1], Step [233/7635], Loss: 5.7372\n",
      "Epoch [1/1], Step [234/7635], Loss: 5.7407\n",
      "Epoch [1/1], Step [235/7635], Loss: 5.7488\n",
      "Epoch [1/1], Step [236/7635], Loss: 5.7442\n",
      "Epoch [1/1], Step [237/7635], Loss: 5.7674\n",
      "Epoch [1/1], Step [238/7635], Loss: 5.8445\n",
      "Epoch [1/1], Step [239/7635], Loss: 5.7791\n",
      "Epoch [1/1], Step [240/7635], Loss: 5.8845\n",
      "Epoch [1/1], Step [241/7635], Loss: 5.6513\n",
      "Epoch [1/1], Step [242/7635], Loss: 5.8668\n",
      "Epoch [1/1], Step [243/7635], Loss: 5.7012\n",
      "Epoch [1/1], Step [244/7635], Loss: 5.8054\n",
      "Epoch [1/1], Step [245/7635], Loss: 5.8288\n",
      "Epoch [1/1], Step [246/7635], Loss: 5.6964\n",
      "Epoch [1/1], Step [247/7635], Loss: 5.8244\n",
      "Epoch [1/1], Step [248/7635], Loss: 5.8404\n",
      "Epoch [1/1], Step [249/7635], Loss: 5.7033\n",
      "Epoch [1/1], Step [250/7635], Loss: 5.8123\n",
      "Epoch [1/1], Step [251/7635], Loss: 5.6593\n",
      "Epoch [1/1], Step [252/7635], Loss: 5.8161\n",
      "Epoch [1/1], Step [253/7635], Loss: 5.7315\n",
      "Epoch [1/1], Step [254/7635], Loss: 5.7458\n",
      "Epoch [1/1], Step [255/7635], Loss: 5.6597\n",
      "Epoch [1/1], Step [256/7635], Loss: 5.6848\n",
      "Epoch [1/1], Step [257/7635], Loss: 5.6036\n",
      "Epoch [1/1], Step [258/7635], Loss: 5.6667\n",
      "Epoch [1/1], Step [259/7635], Loss: 5.7318\n",
      "Epoch [1/1], Step [260/7635], Loss: 5.7184\n",
      "Epoch [1/1], Step [261/7635], Loss: 5.6961\n",
      "Epoch [1/1], Step [262/7635], Loss: 5.6940\n",
      "Epoch [1/1], Step [263/7635], Loss: 5.7315\n",
      "Epoch [1/1], Step [264/7635], Loss: 5.6138\n",
      "Epoch [1/1], Step [265/7635], Loss: 5.6894\n",
      "Epoch [1/1], Step [266/7635], Loss: 5.6017\n",
      "Epoch [1/1], Step [267/7635], Loss: 5.8220\n",
      "Epoch [1/1], Step [268/7635], Loss: 5.6896\n",
      "Epoch [1/1], Step [269/7635], Loss: 5.6515\n",
      "Epoch [1/1], Step [270/7635], Loss: 5.6272\n",
      "Epoch [1/1], Step [271/7635], Loss: 5.8035\n",
      "Epoch [1/1], Step [272/7635], Loss: 5.6692\n",
      "Epoch [1/1], Step [273/7635], Loss: 5.7413\n",
      "Epoch [1/1], Step [274/7635], Loss: 5.6670\n",
      "Epoch [1/1], Step [275/7635], Loss: 5.6030\n",
      "Epoch [1/1], Step [276/7635], Loss: 5.6506\n",
      "Epoch [1/1], Step [277/7635], Loss: 5.6804\n",
      "Epoch [1/1], Step [278/7635], Loss: 5.5419\n",
      "Epoch [1/1], Step [279/7635], Loss: 5.6859\n",
      "Epoch [1/1], Step [280/7635], Loss: 5.5859\n",
      "Epoch [1/1], Step [281/7635], Loss: 5.7214\n",
      "Epoch [1/1], Step [282/7635], Loss: 5.6738\n",
      "Epoch [1/1], Step [283/7635], Loss: 5.5740\n",
      "Epoch [1/1], Step [284/7635], Loss: 5.6238\n",
      "Epoch [1/1], Step [285/7635], Loss: 5.6200\n",
      "Epoch [1/1], Step [286/7635], Loss: 5.5275\n",
      "Epoch [1/1], Step [287/7635], Loss: 5.7314\n",
      "Epoch [1/1], Step [288/7635], Loss: 5.6595\n",
      "Epoch [1/1], Step [289/7635], Loss: 5.5861\n",
      "Epoch [1/1], Step [290/7635], Loss: 5.5803\n",
      "Epoch [1/1], Step [291/7635], Loss: 5.5603\n",
      "Epoch [1/1], Step [292/7635], Loss: 5.4700\n",
      "Epoch [1/1], Step [293/7635], Loss: 5.6855\n",
      "Epoch [1/1], Step [294/7635], Loss: 5.6411\n",
      "Epoch [1/1], Step [295/7635], Loss: 5.5216\n",
      "Epoch [1/1], Step [296/7635], Loss: 5.6199\n",
      "Epoch [1/1], Step [297/7635], Loss: 5.6285\n",
      "Epoch [1/1], Step [298/7635], Loss: 5.6270\n",
      "Epoch [1/1], Step [299/7635], Loss: 5.6488\n",
      "Epoch [1/1], Step [300/7635], Loss: 5.5349\n",
      "Epoch [1/1], Step [301/7635], Loss: 5.5372\n",
      "Epoch [1/1], Step [302/7635], Loss: 5.6470\n",
      "Epoch [1/1], Step [303/7635], Loss: 5.5900\n",
      "Epoch [1/1], Step [304/7635], Loss: 5.5478\n",
      "Epoch [1/1], Step [305/7635], Loss: 5.5514\n",
      "Epoch [1/1], Step [306/7635], Loss: 5.5527\n",
      "Epoch [1/1], Step [307/7635], Loss: 5.5650\n",
      "Epoch [1/1], Step [308/7635], Loss: 5.5983\n",
      "Epoch [1/1], Step [309/7635], Loss: 5.4963\n",
      "Epoch [1/1], Step [310/7635], Loss: 5.6350\n",
      "Epoch [1/1], Step [311/7635], Loss: 5.5870\n",
      "Epoch [1/1], Step [312/7635], Loss: 5.5157\n",
      "Epoch [1/1], Step [313/7635], Loss: 5.6172\n",
      "Epoch [1/1], Step [314/7635], Loss: 5.5340\n",
      "Epoch [1/1], Step [315/7635], Loss: 5.5204\n",
      "Epoch [1/1], Step [316/7635], Loss: 5.6861\n",
      "Epoch [1/1], Step [317/7635], Loss: 5.6079\n",
      "Epoch [1/1], Step [318/7635], Loss: 5.5087\n",
      "Epoch [1/1], Step [319/7635], Loss: 5.4720\n",
      "Epoch [1/1], Step [320/7635], Loss: 5.5257\n",
      "Epoch [1/1], Step [321/7635], Loss: 5.6340\n",
      "Epoch [1/1], Step [322/7635], Loss: 5.5325\n",
      "Epoch [1/1], Step [323/7635], Loss: 5.4707\n",
      "Epoch [1/1], Step [324/7635], Loss: 5.6040\n",
      "Epoch [1/1], Step [325/7635], Loss: 5.5418\n",
      "Epoch [1/1], Step [326/7635], Loss: 5.5831\n",
      "Epoch [1/1], Step [327/7635], Loss: 5.4029\n",
      "Epoch [1/1], Step [328/7635], Loss: 5.4986\n",
      "Epoch [1/1], Step [329/7635], Loss: 5.4632\n",
      "Epoch [1/1], Step [330/7635], Loss: 5.4746\n",
      "Epoch [1/1], Step [331/7635], Loss: 5.6131\n",
      "Epoch [1/1], Step [332/7635], Loss: 5.5513\n",
      "Epoch [1/1], Step [333/7635], Loss: 5.4824\n",
      "Epoch [1/1], Step [334/7635], Loss: 5.4482\n",
      "Epoch [1/1], Step [335/7635], Loss: 5.5430\n",
      "Epoch [1/1], Step [336/7635], Loss: 5.4593\n",
      "Epoch [1/1], Step [337/7635], Loss: 5.5238\n",
      "Epoch [1/1], Step [338/7635], Loss: 5.4737\n",
      "Epoch [1/1], Step [339/7635], Loss: 5.5172\n",
      "Epoch [1/1], Step [340/7635], Loss: 5.4729\n",
      "Epoch [1/1], Step [341/7635], Loss: 5.4586\n",
      "Epoch [1/1], Step [342/7635], Loss: 5.4738\n",
      "Epoch [1/1], Step [343/7635], Loss: 5.4825\n",
      "Epoch [1/1], Step [344/7635], Loss: 5.4466\n",
      "Epoch [1/1], Step [345/7635], Loss: 5.4837\n",
      "Epoch [1/1], Step [346/7635], Loss: 5.4301\n",
      "Epoch [1/1], Step [347/7635], Loss: 5.5340\n",
      "Epoch [1/1], Step [348/7635], Loss: 5.4758\n",
      "Epoch [1/1], Step [349/7635], Loss: 5.4263\n",
      "Epoch [1/1], Step [350/7635], Loss: 5.4607\n",
      "Epoch [1/1], Step [351/7635], Loss: 5.5573\n",
      "Epoch [1/1], Step [352/7635], Loss: 5.3706\n",
      "Epoch [1/1], Step [353/7635], Loss: 5.4379\n",
      "Epoch [1/1], Step [354/7635], Loss: 5.4619\n",
      "Epoch [1/1], Step [355/7635], Loss: 5.5635\n",
      "Epoch [1/1], Step [356/7635], Loss: 5.4591\n",
      "Epoch [1/1], Step [357/7635], Loss: 5.6811\n",
      "Epoch [1/1], Step [358/7635], Loss: 5.4471\n",
      "Epoch [1/1], Step [359/7635], Loss: 5.4071\n",
      "Epoch [1/1], Step [360/7635], Loss: 5.4390\n",
      "Epoch [1/1], Step [361/7635], Loss: 5.4186\n",
      "Epoch [1/1], Step [362/7635], Loss: 5.5658\n",
      "Epoch [1/1], Step [363/7635], Loss: 5.3924\n",
      "Epoch [1/1], Step [364/7635], Loss: 5.4279\n",
      "Epoch [1/1], Step [365/7635], Loss: 5.4445\n",
      "Epoch [1/1], Step [366/7635], Loss: 5.5465\n",
      "Epoch [1/1], Step [367/7635], Loss: 5.5182\n",
      "Epoch [1/1], Step [368/7635], Loss: 5.4819\n",
      "Epoch [1/1], Step [369/7635], Loss: 5.4137\n",
      "Epoch [1/1], Step [370/7635], Loss: 5.4295\n",
      "Epoch [1/1], Step [371/7635], Loss: 5.4337\n",
      "Epoch [1/1], Step [372/7635], Loss: 5.4549\n",
      "Epoch [1/1], Step [373/7635], Loss: 5.4154\n",
      "Epoch [1/1], Step [374/7635], Loss: 5.4425\n",
      "Epoch [1/1], Step [375/7635], Loss: 5.3490\n",
      "Epoch [1/1], Step [376/7635], Loss: 5.4030\n",
      "Epoch [1/1], Step [377/7635], Loss: 5.3933\n",
      "Epoch [1/1], Step [378/7635], Loss: 5.3895\n",
      "Epoch [1/1], Step [379/7635], Loss: 5.3092\n",
      "Epoch [1/1], Step [380/7635], Loss: 5.5059\n",
      "Epoch [1/1], Step [381/7635], Loss: 5.3607\n",
      "Epoch [1/1], Step [382/7635], Loss: 5.3536\n",
      "Epoch [1/1], Step [383/7635], Loss: 5.4433\n",
      "Epoch [1/1], Step [384/7635], Loss: 5.4146\n",
      "Epoch [1/1], Step [385/7635], Loss: 5.4202\n",
      "Epoch [1/1], Step [386/7635], Loss: 5.4898\n",
      "Epoch [1/1], Step [387/7635], Loss: 5.3236\n",
      "Epoch [1/1], Step [388/7635], Loss: 5.3060\n",
      "Epoch [1/1], Step [389/7635], Loss: 5.4428\n",
      "Epoch [1/1], Step [390/7635], Loss: 5.3150\n",
      "Epoch [1/1], Step [391/7635], Loss: 5.4134\n",
      "Epoch [1/1], Step [392/7635], Loss: 5.5346\n",
      "Epoch [1/1], Step [393/7635], Loss: 5.3333\n",
      "Epoch [1/1], Step [394/7635], Loss: 5.4000\n",
      "Epoch [1/1], Step [395/7635], Loss: 5.5306\n",
      "Epoch [1/1], Step [396/7635], Loss: 5.4359\n",
      "Epoch [1/1], Step [397/7635], Loss: 5.2846\n",
      "Epoch [1/1], Step [398/7635], Loss: 5.3659\n",
      "Epoch [1/1], Step [399/7635], Loss: 5.3647\n",
      "Epoch [1/1], Step [400/7635], Loss: 5.4126\n",
      "Epoch [1/1], Step [401/7635], Loss: 5.4035\n",
      "Epoch [1/1], Step [402/7635], Loss: 5.4105\n",
      "Epoch [1/1], Step [403/7635], Loss: 5.4374\n",
      "Epoch [1/1], Step [404/7635], Loss: 5.3489\n",
      "Epoch [1/1], Step [405/7635], Loss: 5.3229\n",
      "Epoch [1/1], Step [406/7635], Loss: 5.3462\n",
      "Epoch [1/1], Step [407/7635], Loss: 5.5042\n",
      "Epoch [1/1], Step [408/7635], Loss: 5.3797\n",
      "Epoch [1/1], Step [409/7635], Loss: 5.3266\n",
      "Epoch [1/1], Step [410/7635], Loss: 5.4604\n",
      "Epoch [1/1], Step [411/7635], Loss: 5.4861\n",
      "Epoch [1/1], Step [412/7635], Loss: 5.4543\n",
      "Epoch [1/1], Step [413/7635], Loss: 5.3199\n",
      "Epoch [1/1], Step [414/7635], Loss: 5.3023\n",
      "Epoch [1/1], Step [415/7635], Loss: 5.3500\n",
      "Epoch [1/1], Step [416/7635], Loss: 5.2843\n",
      "Epoch [1/1], Step [417/7635], Loss: 5.3024\n",
      "Epoch [1/1], Step [418/7635], Loss: 5.3922\n",
      "Epoch [1/1], Step [419/7635], Loss: 5.3762\n",
      "Epoch [1/1], Step [420/7635], Loss: 5.3717\n",
      "Epoch [1/1], Step [421/7635], Loss: 5.2789\n",
      "Epoch [1/1], Step [422/7635], Loss: 5.4180\n",
      "Epoch [1/1], Step [423/7635], Loss: 5.2524\n",
      "Epoch [1/1], Step [424/7635], Loss: 5.2979\n",
      "Epoch [1/1], Step [425/7635], Loss: 5.4427\n",
      "Epoch [1/1], Step [426/7635], Loss: 5.3908\n",
      "Epoch [1/1], Step [427/7635], Loss: 5.3920\n",
      "Epoch [1/1], Step [428/7635], Loss: 5.3066\n",
      "Epoch [1/1], Step [429/7635], Loss: 5.3263\n",
      "Epoch [1/1], Step [430/7635], Loss: 5.2797\n",
      "Epoch [1/1], Step [431/7635], Loss: 5.4603\n",
      "Epoch [1/1], Step [432/7635], Loss: 5.2829\n",
      "Epoch [1/1], Step [433/7635], Loss: 5.2451\n",
      "Epoch [1/1], Step [434/7635], Loss: 5.3385\n",
      "Epoch [1/1], Step [435/7635], Loss: 5.3707\n",
      "Epoch [1/1], Step [436/7635], Loss: 5.3079\n",
      "Epoch [1/1], Step [437/7635], Loss: 5.3280\n",
      "Epoch [1/1], Step [438/7635], Loss: 5.2412\n",
      "Epoch [1/1], Step [439/7635], Loss: 5.4317\n",
      "Epoch [1/1], Step [440/7635], Loss: 5.3736\n",
      "Epoch [1/1], Step [441/7635], Loss: 5.3272\n",
      "Epoch [1/1], Step [442/7635], Loss: 5.3638\n",
      "Epoch [1/1], Step [443/7635], Loss: 5.2488\n",
      "Epoch [1/1], Step [444/7635], Loss: 5.3633\n",
      "Epoch [1/1], Step [445/7635], Loss: 5.3037\n",
      "Epoch [1/1], Step [446/7635], Loss: 5.4178\n",
      "Epoch [1/1], Step [447/7635], Loss: 5.3307\n",
      "Epoch [1/1], Step [448/7635], Loss: 5.3550\n",
      "Epoch [1/1], Step [449/7635], Loss: 5.2817\n",
      "Epoch [1/1], Step [450/7635], Loss: 5.3761\n",
      "Epoch [1/1], Step [451/7635], Loss: 5.2804\n",
      "Epoch [1/1], Step [452/7635], Loss: 5.3776\n",
      "Epoch [1/1], Step [453/7635], Loss: 5.3859\n",
      "Epoch [1/1], Step [454/7635], Loss: 5.2937\n",
      "Epoch [1/1], Step [455/7635], Loss: 5.2808\n",
      "Epoch [1/1], Step [456/7635], Loss: 5.3360\n",
      "Epoch [1/1], Step [457/7635], Loss: 5.2904\n",
      "Epoch [1/1], Step [458/7635], Loss: 5.1955\n",
      "Epoch [1/1], Step [459/7635], Loss: 5.2432\n",
      "Epoch [1/1], Step [460/7635], Loss: 5.2788\n",
      "Epoch [1/1], Step [461/7635], Loss: 5.3633\n",
      "Epoch [1/1], Step [462/7635], Loss: 5.2479\n",
      "Epoch [1/1], Step [463/7635], Loss: 5.4152\n",
      "Epoch [1/1], Step [464/7635], Loss: 5.3578\n",
      "Epoch [1/1], Step [465/7635], Loss: 5.2323\n",
      "Epoch [1/1], Step [466/7635], Loss: 5.2606\n",
      "Epoch [1/1], Step [467/7635], Loss: 5.2591\n",
      "Epoch [1/1], Step [468/7635], Loss: 5.2667\n",
      "Epoch [1/1], Step [469/7635], Loss: 5.3239\n",
      "Epoch [1/1], Step [470/7635], Loss: 5.3373\n",
      "Epoch [1/1], Step [471/7635], Loss: 5.2644\n",
      "Epoch [1/1], Step [472/7635], Loss: 5.3611\n",
      "Epoch [1/1], Step [473/7635], Loss: 5.3659\n",
      "Epoch [1/1], Step [474/7635], Loss: 5.3419\n",
      "Epoch [1/1], Step [475/7635], Loss: 5.2407\n",
      "Epoch [1/1], Step [476/7635], Loss: 5.2153\n",
      "Epoch [1/1], Step [477/7635], Loss: 5.2281\n",
      "Epoch [1/1], Step [478/7635], Loss: 5.2827\n",
      "Epoch [1/1], Step [479/7635], Loss: 5.2547\n",
      "Epoch [1/1], Step [480/7635], Loss: 5.2599\n",
      "Epoch [1/1], Step [481/7635], Loss: 5.2098\n",
      "Epoch [1/1], Step [482/7635], Loss: 5.1815\n",
      "Epoch [1/1], Step [483/7635], Loss: 5.2925\n",
      "Epoch [1/1], Step [484/7635], Loss: 5.1492\n",
      "Epoch [1/1], Step [485/7635], Loss: 5.2207\n",
      "Epoch [1/1], Step [486/7635], Loss: 5.2014\n",
      "Epoch [1/1], Step [487/7635], Loss: 5.2776\n",
      "Epoch [1/1], Step [488/7635], Loss: 5.1062\n",
      "Epoch [1/1], Step [489/7635], Loss: 5.2081\n",
      "Epoch [1/1], Step [490/7635], Loss: 5.3090\n",
      "Epoch [1/1], Step [491/7635], Loss: 5.3124\n",
      "Epoch [1/1], Step [492/7635], Loss: 5.1513\n",
      "Epoch [1/1], Step [493/7635], Loss: 5.1872\n",
      "Epoch [1/1], Step [494/7635], Loss: 5.1830\n",
      "Epoch [1/1], Step [495/7635], Loss: 5.3633\n",
      "Epoch [1/1], Step [496/7635], Loss: 5.2821\n",
      "Epoch [1/1], Step [497/7635], Loss: 5.3108\n",
      "Epoch [1/1], Step [498/7635], Loss: 5.3353\n",
      "Epoch [1/1], Step [499/7635], Loss: 5.2149\n",
      "Epoch [1/1], Step [500/7635], Loss: 5.1807\n",
      "Epoch [1/1], Step [501/7635], Loss: 5.2431\n",
      "Epoch [1/1], Step [502/7635], Loss: 5.1186\n",
      "Epoch [1/1], Step [503/7635], Loss: 5.2295\n",
      "Epoch [1/1], Step [504/7635], Loss: 5.0960\n",
      "Epoch [1/1], Step [505/7635], Loss: 5.2716\n",
      "Epoch [1/1], Step [506/7635], Loss: 5.1955\n",
      "Epoch [1/1], Step [507/7635], Loss: 5.2954\n",
      "Epoch [1/1], Step [508/7635], Loss: 5.2118\n",
      "Epoch [1/1], Step [509/7635], Loss: 5.1771\n",
      "Epoch [1/1], Step [510/7635], Loss: 5.1712\n",
      "Epoch [1/1], Step [511/7635], Loss: 5.2423\n",
      "Epoch [1/1], Step [512/7635], Loss: 5.3047\n",
      "Epoch [1/1], Step [513/7635], Loss: 5.1920\n",
      "Epoch [1/1], Step [514/7635], Loss: 5.2539\n",
      "Epoch [1/1], Step [515/7635], Loss: 5.0587\n",
      "Epoch [1/1], Step [516/7635], Loss: 5.1262\n",
      "Epoch [1/1], Step [517/7635], Loss: 5.2265\n",
      "Epoch [1/1], Step [518/7635], Loss: 5.2666\n",
      "Epoch [1/1], Step [519/7635], Loss: 5.1488\n",
      "Epoch [1/1], Step [520/7635], Loss: 5.1449\n",
      "Epoch [1/1], Step [521/7635], Loss: 5.3841\n",
      "Epoch [1/1], Step [522/7635], Loss: 5.0867\n",
      "Epoch [1/1], Step [523/7635], Loss: 5.1511\n",
      "Epoch [1/1], Step [524/7635], Loss: 5.2788\n",
      "Epoch [1/1], Step [525/7635], Loss: 5.1712\n",
      "Epoch [1/1], Step [526/7635], Loss: 5.2447\n",
      "Epoch [1/1], Step [527/7635], Loss: 5.2452\n",
      "Epoch [1/1], Step [528/7635], Loss: 5.0497\n",
      "Epoch [1/1], Step [529/7635], Loss: 5.2331\n",
      "Epoch [1/1], Step [530/7635], Loss: 5.2536\n",
      "Epoch [1/1], Step [531/7635], Loss: 5.1672\n",
      "Epoch [1/1], Step [532/7635], Loss: 5.2345\n",
      "Epoch [1/1], Step [533/7635], Loss: 5.1833\n",
      "Epoch [1/1], Step [534/7635], Loss: 5.1977\n",
      "Epoch [1/1], Step [535/7635], Loss: 5.2948\n",
      "Epoch [1/1], Step [536/7635], Loss: 5.2085\n",
      "Epoch [1/1], Step [537/7635], Loss: 5.1927\n",
      "Epoch [1/1], Step [538/7635], Loss: 5.1126\n",
      "Epoch [1/1], Step [539/7635], Loss: 5.0962\n",
      "Epoch [1/1], Step [540/7635], Loss: 5.1626\n",
      "Epoch [1/1], Step [541/7635], Loss: 5.1774\n",
      "Epoch [1/1], Step [542/7635], Loss: 5.2316\n",
      "Epoch [1/1], Step [543/7635], Loss: 5.1921\n",
      "Epoch [1/1], Step [544/7635], Loss: 4.9335\n",
      "Epoch [1/1], Step [545/7635], Loss: 5.2074\n",
      "Epoch [1/1], Step [546/7635], Loss: 5.1276\n",
      "Epoch [1/1], Step [547/7635], Loss: 5.2351\n",
      "Epoch [1/1], Step [548/7635], Loss: 5.2359\n",
      "Epoch [1/1], Step [549/7635], Loss: 5.0974\n",
      "Epoch [1/1], Step [550/7635], Loss: 5.1429\n",
      "Epoch [1/1], Step [551/7635], Loss: 5.2342\n",
      "Epoch [1/1], Step [552/7635], Loss: 5.1559\n",
      "Epoch [1/1], Step [553/7635], Loss: 5.1940\n",
      "Epoch [1/1], Step [554/7635], Loss: 5.1129\n",
      "Epoch [1/1], Step [555/7635], Loss: 5.2508\n",
      "Epoch [1/1], Step [556/7635], Loss: 5.1295\n",
      "Epoch [1/1], Step [557/7635], Loss: 5.1727\n",
      "Epoch [1/1], Step [558/7635], Loss: 5.1198\n",
      "Epoch [1/1], Step [559/7635], Loss: 5.0957\n",
      "Epoch [1/1], Step [560/7635], Loss: 5.1097\n",
      "Epoch [1/1], Step [561/7635], Loss: 5.1318\n",
      "Epoch [1/1], Step [562/7635], Loss: 5.2736\n",
      "Epoch [1/1], Step [563/7635], Loss: 5.0498\n",
      "Epoch [1/1], Step [564/7635], Loss: 5.0601\n",
      "Epoch [1/1], Step [565/7635], Loss: 5.1145\n",
      "Epoch [1/1], Step [566/7635], Loss: 5.1913\n",
      "Epoch [1/1], Step [567/7635], Loss: 5.0954\n",
      "Epoch [1/1], Step [568/7635], Loss: 5.1221\n",
      "Epoch [1/1], Step [569/7635], Loss: 5.2058\n",
      "Epoch [1/1], Step [570/7635], Loss: 5.1418\n",
      "Epoch [1/1], Step [571/7635], Loss: 5.1311\n",
      "Epoch [1/1], Step [572/7635], Loss: 5.1864\n",
      "Epoch [1/1], Step [573/7635], Loss: 5.1243\n",
      "Epoch [1/1], Step [574/7635], Loss: 5.0967\n",
      "Epoch [1/1], Step [575/7635], Loss: 5.1110\n",
      "Epoch [1/1], Step [576/7635], Loss: 5.1735\n",
      "Epoch [1/1], Step [577/7635], Loss: 5.1257\n",
      "Epoch [1/1], Step [578/7635], Loss: 5.1873\n",
      "Epoch [1/1], Step [579/7635], Loss: 5.0837\n",
      "Epoch [1/1], Step [580/7635], Loss: 5.0862\n",
      "Epoch [1/1], Step [581/7635], Loss: 5.2083\n",
      "Epoch [1/1], Step [582/7635], Loss: 5.1211\n",
      "Epoch [1/1], Step [583/7635], Loss: 5.1426\n",
      "Epoch [1/1], Step [584/7635], Loss: 5.0388\n",
      "Epoch [1/1], Step [585/7635], Loss: 5.1770\n",
      "Epoch [1/1], Step [586/7635], Loss: 5.2038\n",
      "Epoch [1/1], Step [587/7635], Loss: 5.1980\n",
      "Epoch [1/1], Step [588/7635], Loss: 5.0847\n",
      "Epoch [1/1], Step [589/7635], Loss: 5.1926\n",
      "Epoch [1/1], Step [590/7635], Loss: 5.0847\n",
      "Epoch [1/1], Step [591/7635], Loss: 5.1029\n",
      "Epoch [1/1], Step [592/7635], Loss: 5.1636\n",
      "Epoch [1/1], Step [593/7635], Loss: 5.1989\n",
      "Epoch [1/1], Step [594/7635], Loss: 5.1027\n",
      "Epoch [1/1], Step [595/7635], Loss: 5.1665\n",
      "Epoch [1/1], Step [596/7635], Loss: 5.1599\n",
      "Epoch [1/1], Step [597/7635], Loss: 5.2007\n",
      "Epoch [1/1], Step [598/7635], Loss: 5.0947\n",
      "Epoch [1/1], Step [599/7635], Loss: 5.0716\n",
      "Epoch [1/1], Step [600/7635], Loss: 5.3665\n",
      "Epoch [1/1], Step [601/7635], Loss: 5.1129\n",
      "Epoch [1/1], Step [602/7635], Loss: 5.0970\n",
      "Epoch [1/1], Step [603/7635], Loss: 5.0307\n",
      "Epoch [1/1], Step [604/7635], Loss: 5.0456\n",
      "Epoch [1/1], Step [605/7635], Loss: 5.0707\n",
      "Epoch [1/1], Step [606/7635], Loss: 5.0922\n",
      "Epoch [1/1], Step [607/7635], Loss: 4.9962\n",
      "Epoch [1/1], Step [608/7635], Loss: 5.1115\n",
      "Epoch [1/1], Step [609/7635], Loss: 5.0917\n",
      "Epoch [1/1], Step [610/7635], Loss: 5.0631\n",
      "Epoch [1/1], Step [611/7635], Loss: 5.1111\n",
      "Epoch [1/1], Step [612/7635], Loss: 5.0130\n",
      "Epoch [1/1], Step [613/7635], Loss: 5.0503\n",
      "Epoch [1/1], Step [614/7635], Loss: 4.9885\n",
      "Epoch [1/1], Step [615/7635], Loss: 5.0240\n",
      "Epoch [1/1], Step [616/7635], Loss: 5.1077\n",
      "Epoch [1/1], Step [617/7635], Loss: 5.0472\n",
      "Epoch [1/1], Step [618/7635], Loss: 5.1501\n",
      "Epoch [1/1], Step [619/7635], Loss: 5.2192\n",
      "Epoch [1/1], Step [620/7635], Loss: 5.0186\n",
      "Epoch [1/1], Step [621/7635], Loss: 5.1389\n",
      "Epoch [1/1], Step [622/7635], Loss: 5.0958\n",
      "Epoch [1/1], Step [623/7635], Loss: 5.0646\n",
      "Epoch [1/1], Step [624/7635], Loss: 5.1466\n",
      "Epoch [1/1], Step [625/7635], Loss: 5.0071\n",
      "Epoch [1/1], Step [626/7635], Loss: 5.0511\n",
      "Epoch [1/1], Step [627/7635], Loss: 5.0483\n",
      "Epoch [1/1], Step [628/7635], Loss: 5.0474\n",
      "Epoch [1/1], Step [629/7635], Loss: 4.9631\n",
      "Epoch [1/1], Step [630/7635], Loss: 5.1230\n",
      "Epoch [1/1], Step [631/7635], Loss: 5.0500\n",
      "Epoch [1/1], Step [632/7635], Loss: 5.0828\n",
      "Epoch [1/1], Step [633/7635], Loss: 5.1631\n",
      "Epoch [1/1], Step [634/7635], Loss: 5.0639\n",
      "Epoch [1/1], Step [635/7635], Loss: 5.2521\n",
      "Epoch [1/1], Step [636/7635], Loss: 5.1134\n",
      "Epoch [1/1], Step [637/7635], Loss: 5.0651\n",
      "Epoch [1/1], Step [638/7635], Loss: 5.0211\n",
      "Epoch [1/1], Step [639/7635], Loss: 5.0280\n",
      "Epoch [1/1], Step [640/7635], Loss: 5.0773\n",
      "Epoch [1/1], Step [641/7635], Loss: 5.0034\n",
      "Epoch [1/1], Step [642/7635], Loss: 4.9882\n",
      "Epoch [1/1], Step [643/7635], Loss: 5.0895\n",
      "Epoch [1/1], Step [644/7635], Loss: 5.0361\n",
      "Epoch [1/1], Step [645/7635], Loss: 5.0653\n",
      "Epoch [1/1], Step [646/7635], Loss: 5.1223\n",
      "Epoch [1/1], Step [647/7635], Loss: 5.0523\n",
      "Epoch [1/1], Step [648/7635], Loss: 5.1162\n",
      "Epoch [1/1], Step [649/7635], Loss: 5.0878\n",
      "Epoch [1/1], Step [650/7635], Loss: 5.0948\n",
      "Epoch [1/1], Step [651/7635], Loss: 5.0983\n",
      "Epoch [1/1], Step [652/7635], Loss: 5.1484\n",
      "Epoch [1/1], Step [653/7635], Loss: 5.0061\n",
      "Epoch [1/1], Step [654/7635], Loss: 5.0426\n",
      "Epoch [1/1], Step [655/7635], Loss: 5.1103\n",
      "Epoch [1/1], Step [656/7635], Loss: 4.9545\n",
      "Epoch [1/1], Step [657/7635], Loss: 5.0066\n",
      "Epoch [1/1], Step [658/7635], Loss: 5.0169\n",
      "Epoch [1/1], Step [659/7635], Loss: 5.1134\n",
      "Epoch [1/1], Step [660/7635], Loss: 5.0981\n",
      "Epoch [1/1], Step [661/7635], Loss: 4.9785\n",
      "Epoch [1/1], Step [662/7635], Loss: 5.1237\n",
      "Epoch [1/1], Step [663/7635], Loss: 5.1082\n",
      "Epoch [1/1], Step [664/7635], Loss: 5.0624\n",
      "Epoch [1/1], Step [665/7635], Loss: 5.0789\n",
      "Epoch [1/1], Step [666/7635], Loss: 5.0230\n",
      "Epoch [1/1], Step [667/7635], Loss: 4.9717\n",
      "Epoch [1/1], Step [668/7635], Loss: 5.0919\n",
      "Epoch [1/1], Step [669/7635], Loss: 4.9845\n",
      "Epoch [1/1], Step [670/7635], Loss: 5.0854\n",
      "Epoch [1/1], Step [671/7635], Loss: 4.9324\n",
      "Epoch [1/1], Step [672/7635], Loss: 5.0210\n",
      "Epoch [1/1], Step [673/7635], Loss: 5.0554\n",
      "Epoch [1/1], Step [674/7635], Loss: 5.1559\n",
      "Epoch [1/1], Step [675/7635], Loss: 5.0244\n",
      "Epoch [1/1], Step [676/7635], Loss: 5.0462\n",
      "Epoch [1/1], Step [677/7635], Loss: 5.0104\n",
      "Epoch [1/1], Step [678/7635], Loss: 4.9640\n",
      "Epoch [1/1], Step [679/7635], Loss: 5.0670\n",
      "Epoch [1/1], Step [680/7635], Loss: 4.9322\n",
      "Epoch [1/1], Step [681/7635], Loss: 5.0302\n",
      "Epoch [1/1], Step [682/7635], Loss: 4.9328\n",
      "Epoch [1/1], Step [683/7635], Loss: 4.9953\n",
      "Epoch [1/1], Step [684/7635], Loss: 4.9445\n",
      "Epoch [1/1], Step [685/7635], Loss: 5.0594\n",
      "Epoch [1/1], Step [686/7635], Loss: 5.0627\n",
      "Epoch [1/1], Step [687/7635], Loss: 5.0636\n",
      "Epoch [1/1], Step [688/7635], Loss: 5.0310\n",
      "Epoch [1/1], Step [689/7635], Loss: 5.0251\n",
      "Epoch [1/1], Step [690/7635], Loss: 5.0330\n",
      "Epoch [1/1], Step [691/7635], Loss: 4.9663\n",
      "Epoch [1/1], Step [692/7635], Loss: 5.1038\n",
      "Epoch [1/1], Step [693/7635], Loss: 5.0125\n",
      "Epoch [1/1], Step [694/7635], Loss: 5.0148\n",
      "Epoch [1/1], Step [695/7635], Loss: 4.9959\n",
      "Epoch [1/1], Step [696/7635], Loss: 5.1749\n",
      "Epoch [1/1], Step [697/7635], Loss: 4.9187\n",
      "Epoch [1/1], Step [698/7635], Loss: 5.0551\n",
      "Epoch [1/1], Step [699/7635], Loss: 5.0082\n",
      "Epoch [1/1], Step [700/7635], Loss: 4.9334\n",
      "Epoch [1/1], Step [701/7635], Loss: 4.9680\n",
      "Epoch [1/1], Step [702/7635], Loss: 5.0376\n",
      "Epoch [1/1], Step [703/7635], Loss: 5.0497\n",
      "Epoch [1/1], Step [704/7635], Loss: 5.0519\n",
      "Epoch [1/1], Step [705/7635], Loss: 4.9780\n",
      "Epoch [1/1], Step [706/7635], Loss: 4.9905\n",
      "Epoch [1/1], Step [707/7635], Loss: 4.9591\n",
      "Epoch [1/1], Step [708/7635], Loss: 5.0854\n",
      "Epoch [1/1], Step [709/7635], Loss: 4.9496\n",
      "Epoch [1/1], Step [710/7635], Loss: 5.1534\n",
      "Epoch [1/1], Step [711/7635], Loss: 4.9729\n",
      "Epoch [1/1], Step [712/7635], Loss: 4.9874\n",
      "Epoch [1/1], Step [713/7635], Loss: 4.9561\n",
      "Epoch [1/1], Step [714/7635], Loss: 5.0693\n",
      "Epoch [1/1], Step [715/7635], Loss: 5.0591\n",
      "Epoch [1/1], Step [716/7635], Loss: 5.0316\n",
      "Epoch [1/1], Step [717/7635], Loss: 5.0026\n",
      "Epoch [1/1], Step [718/7635], Loss: 4.9404\n",
      "Epoch [1/1], Step [719/7635], Loss: 5.0343\n",
      "Epoch [1/1], Step [720/7635], Loss: 5.0716\n",
      "Epoch [1/1], Step [721/7635], Loss: 4.8805\n",
      "Epoch [1/1], Step [722/7635], Loss: 5.0151\n",
      "Epoch [1/1], Step [723/7635], Loss: 5.0100\n",
      "Epoch [1/1], Step [724/7635], Loss: 5.1711\n",
      "Epoch [1/1], Step [725/7635], Loss: 4.9664\n",
      "Epoch [1/1], Step [726/7635], Loss: 5.0405\n",
      "Epoch [1/1], Step [727/7635], Loss: 5.0717\n",
      "Epoch [1/1], Step [728/7635], Loss: 5.0354\n",
      "Epoch [1/1], Step [729/7635], Loss: 4.9649\n",
      "Epoch [1/1], Step [730/7635], Loss: 4.9602\n",
      "Epoch [1/1], Step [731/7635], Loss: 5.1411\n",
      "Epoch [1/1], Step [732/7635], Loss: 4.8986\n",
      "Epoch [1/1], Step [733/7635], Loss: 4.8744\n",
      "Epoch [1/1], Step [734/7635], Loss: 4.9852\n",
      "Epoch [1/1], Step [735/7635], Loss: 5.0176\n",
      "Epoch [1/1], Step [736/7635], Loss: 4.9466\n",
      "Epoch [1/1], Step [737/7635], Loss: 5.0858\n",
      "Epoch [1/1], Step [738/7635], Loss: 4.9746\n",
      "Epoch [1/1], Step [739/7635], Loss: 4.9885\n",
      "Epoch [1/1], Step [740/7635], Loss: 4.8408\n",
      "Epoch [1/1], Step [741/7635], Loss: 4.9467\n",
      "Epoch [1/1], Step [742/7635], Loss: 4.9581\n",
      "Epoch [1/1], Step [743/7635], Loss: 4.9176\n",
      "Epoch [1/1], Step [744/7635], Loss: 4.9569\n",
      "Epoch [1/1], Step [745/7635], Loss: 4.9921\n",
      "Epoch [1/1], Step [746/7635], Loss: 4.8912\n",
      "Epoch [1/1], Step [747/7635], Loss: 4.9831\n",
      "Epoch [1/1], Step [748/7635], Loss: 5.0404\n",
      "Epoch [1/1], Step [749/7635], Loss: 5.1067\n",
      "Epoch [1/1], Step [750/7635], Loss: 4.9869\n",
      "Epoch [1/1], Step [751/7635], Loss: 4.9416\n",
      "Epoch [1/1], Step [752/7635], Loss: 5.0592\n",
      "Epoch [1/1], Step [753/7635], Loss: 5.0511\n",
      "Epoch [1/1], Step [754/7635], Loss: 4.9787\n",
      "Epoch [1/1], Step [755/7635], Loss: 5.0475\n",
      "Epoch [1/1], Step [756/7635], Loss: 4.9521\n",
      "Epoch [1/1], Step [757/7635], Loss: 5.0995\n",
      "Epoch [1/1], Step [758/7635], Loss: 4.9834\n",
      "Epoch [1/1], Step [759/7635], Loss: 4.9542\n",
      "Epoch [1/1], Step [760/7635], Loss: 4.8983\n",
      "Epoch [1/1], Step [761/7635], Loss: 5.0123\n",
      "Epoch [1/1], Step [762/7635], Loss: 4.9222\n",
      "Epoch [1/1], Step [763/7635], Loss: 5.0308\n",
      "Epoch [1/1], Step [764/7635], Loss: 4.9474\n",
      "Epoch [1/1], Step [765/7635], Loss: 5.0214\n",
      "Epoch [1/1], Step [766/7635], Loss: 4.9747\n",
      "Epoch [1/1], Step [767/7635], Loss: 4.9522\n",
      "Epoch [1/1], Step [768/7635], Loss: 4.8949\n",
      "Epoch [1/1], Step [769/7635], Loss: 4.8689\n",
      "Epoch [1/1], Step [770/7635], Loss: 4.9640\n",
      "Epoch [1/1], Step [771/7635], Loss: 4.9660\n",
      "Epoch [1/1], Step [772/7635], Loss: 4.9197\n",
      "Epoch [1/1], Step [773/7635], Loss: 5.0072\n",
      "Epoch [1/1], Step [774/7635], Loss: 4.9104\n",
      "Epoch [1/1], Step [775/7635], Loss: 5.0056\n",
      "Epoch [1/1], Step [776/7635], Loss: 4.9019\n",
      "Epoch [1/1], Step [777/7635], Loss: 4.9001\n",
      "Epoch [1/1], Step [778/7635], Loss: 4.9722\n",
      "Epoch [1/1], Step [779/7635], Loss: 5.1241\n",
      "Epoch [1/1], Step [780/7635], Loss: 4.9775\n",
      "Epoch [1/1], Step [781/7635], Loss: 4.8723\n",
      "Epoch [1/1], Step [782/7635], Loss: 4.9991\n",
      "Epoch [1/1], Step [783/7635], Loss: 4.9582\n",
      "Epoch [1/1], Step [784/7635], Loss: 5.0069\n",
      "Epoch [1/1], Step [785/7635], Loss: 4.9607\n",
      "Epoch [1/1], Step [786/7635], Loss: 4.9006\n",
      "Epoch [1/1], Step [787/7635], Loss: 4.9422\n",
      "Epoch [1/1], Step [788/7635], Loss: 4.9005\n",
      "Epoch [1/1], Step [789/7635], Loss: 4.9659\n",
      "Epoch [1/1], Step [790/7635], Loss: 4.8266\n",
      "Epoch [1/1], Step [791/7635], Loss: 5.0045\n",
      "Epoch [1/1], Step [792/7635], Loss: 4.9910\n",
      "Epoch [1/1], Step [793/7635], Loss: 5.0463\n",
      "Epoch [1/1], Step [794/7635], Loss: 4.9463\n",
      "Epoch [1/1], Step [795/7635], Loss: 5.0513\n",
      "Epoch [1/1], Step [796/7635], Loss: 4.9662\n",
      "Epoch [1/1], Step [797/7635], Loss: 4.7801\n",
      "Epoch [1/1], Step [798/7635], Loss: 4.9699\n",
      "Epoch [1/1], Step [799/7635], Loss: 4.9807\n",
      "Epoch [1/1], Step [800/7635], Loss: 4.9058\n",
      "Epoch [1/1], Step [801/7635], Loss: 4.9281\n",
      "Epoch [1/1], Step [802/7635], Loss: 5.0949\n",
      "Epoch [1/1], Step [803/7635], Loss: 5.0023\n",
      "Epoch [1/1], Step [804/7635], Loss: 4.8944\n",
      "Epoch [1/1], Step [805/7635], Loss: 4.8631\n",
      "Epoch [1/1], Step [806/7635], Loss: 4.9945\n",
      "Epoch [1/1], Step [807/7635], Loss: 4.8809\n",
      "Epoch [1/1], Step [808/7635], Loss: 4.9519\n",
      "Epoch [1/1], Step [809/7635], Loss: 4.9411\n",
      "Epoch [1/1], Step [810/7635], Loss: 4.9134\n",
      "Epoch [1/1], Step [811/7635], Loss: 4.9031\n",
      "Epoch [1/1], Step [812/7635], Loss: 4.8727\n",
      "Epoch [1/1], Step [813/7635], Loss: 4.9239\n",
      "Epoch [1/1], Step [814/7635], Loss: 4.9316\n",
      "Epoch [1/1], Step [815/7635], Loss: 4.9780\n",
      "Epoch [1/1], Step [816/7635], Loss: 4.8683\n",
      "Epoch [1/1], Step [817/7635], Loss: 4.8434\n",
      "Epoch [1/1], Step [818/7635], Loss: 5.1370\n",
      "Epoch [1/1], Step [819/7635], Loss: 4.8130\n",
      "Epoch [1/1], Step [820/7635], Loss: 4.8975\n",
      "Epoch [1/1], Step [821/7635], Loss: 4.8426\n",
      "Epoch [1/1], Step [822/7635], Loss: 4.8996\n",
      "Epoch [1/1], Step [823/7635], Loss: 4.8417\n",
      "Epoch [1/1], Step [824/7635], Loss: 4.8949\n",
      "Epoch [1/1], Step [825/7635], Loss: 4.9609\n",
      "Epoch [1/1], Step [826/7635], Loss: 4.8032\n",
      "Epoch [1/1], Step [827/7635], Loss: 4.8806\n",
      "Epoch [1/1], Step [828/7635], Loss: 4.9449\n",
      "Epoch [1/1], Step [829/7635], Loss: 4.9233\n",
      "Epoch [1/1], Step [830/7635], Loss: 4.9112\n",
      "Epoch [1/1], Step [831/7635], Loss: 4.9562\n",
      "Epoch [1/1], Step [832/7635], Loss: 4.8218\n",
      "Epoch [1/1], Step [833/7635], Loss: 4.9637\n",
      "Epoch [1/1], Step [834/7635], Loss: 4.9536\n",
      "Epoch [1/1], Step [835/7635], Loss: 4.9460\n",
      "Epoch [1/1], Step [836/7635], Loss: 4.9016\n",
      "Epoch [1/1], Step [837/7635], Loss: 4.9304\n",
      "Epoch [1/1], Step [838/7635], Loss: 4.8735\n",
      "Epoch [1/1], Step [839/7635], Loss: 4.8317\n",
      "Epoch [1/1], Step [840/7635], Loss: 4.9700\n",
      "Epoch [1/1], Step [841/7635], Loss: 4.9717\n",
      "Epoch [1/1], Step [842/7635], Loss: 4.8733\n",
      "Epoch [1/1], Step [843/7635], Loss: 4.9515\n",
      "Epoch [1/1], Step [844/7635], Loss: 4.9881\n",
      "Epoch [1/1], Step [845/7635], Loss: 4.8053\n",
      "Epoch [1/1], Step [846/7635], Loss: 4.9135\n",
      "Epoch [1/1], Step [847/7635], Loss: 4.9869\n",
      "Epoch [1/1], Step [848/7635], Loss: 4.9190\n",
      "Epoch [1/1], Step [849/7635], Loss: 4.9900\n",
      "Epoch [1/1], Step [850/7635], Loss: 4.8817\n",
      "Epoch [1/1], Step [851/7635], Loss: 4.9110\n",
      "Epoch [1/1], Step [852/7635], Loss: 4.8371\n",
      "Epoch [1/1], Step [853/7635], Loss: 4.9304\n",
      "Epoch [1/1], Step [854/7635], Loss: 4.8891\n",
      "Epoch [1/1], Step [855/7635], Loss: 4.8896\n",
      "Epoch [1/1], Step [856/7635], Loss: 4.8931\n",
      "Epoch [1/1], Step [857/7635], Loss: 4.9374\n",
      "Epoch [1/1], Step [858/7635], Loss: 4.8794\n",
      "Epoch [1/1], Step [859/7635], Loss: 4.9020\n",
      "Epoch [1/1], Step [860/7635], Loss: 4.9641\n",
      "Epoch [1/1], Step [861/7635], Loss: 4.9115\n",
      "Epoch [1/1], Step [862/7635], Loss: 4.8757\n",
      "Epoch [1/1], Step [863/7635], Loss: 4.8413\n",
      "Epoch [1/1], Step [864/7635], Loss: 4.9443\n",
      "Epoch [1/1], Step [865/7635], Loss: 4.8971\n",
      "Epoch [1/1], Step [866/7635], Loss: 4.8532\n",
      "Epoch [1/1], Step [867/7635], Loss: 4.9086\n",
      "Epoch [1/1], Step [868/7635], Loss: 4.8698\n",
      "Epoch [1/1], Step [869/7635], Loss: 4.8425\n",
      "Epoch [1/1], Step [870/7635], Loss: 4.8917\n",
      "Epoch [1/1], Step [871/7635], Loss: 4.7981\n",
      "Epoch [1/1], Step [872/7635], Loss: 4.9151\n",
      "Epoch [1/1], Step [873/7635], Loss: 4.9671\n",
      "Epoch [1/1], Step [874/7635], Loss: 4.8515\n",
      "Epoch [1/1], Step [875/7635], Loss: 4.8295\n",
      "Epoch [1/1], Step [876/7635], Loss: 4.7613\n",
      "Epoch [1/1], Step [877/7635], Loss: 4.9217\n",
      "Epoch [1/1], Step [878/7635], Loss: 4.7570\n",
      "Epoch [1/1], Step [879/7635], Loss: 4.8626\n",
      "Epoch [1/1], Step [880/7635], Loss: 4.9061\n",
      "Epoch [1/1], Step [881/7635], Loss: 4.8584\n",
      "Epoch [1/1], Step [882/7635], Loss: 4.8504\n",
      "Epoch [1/1], Step [883/7635], Loss: 4.9634\n",
      "Epoch [1/1], Step [884/7635], Loss: 4.8377\n",
      "Epoch [1/1], Step [885/7635], Loss: 4.9536\n",
      "Epoch [1/1], Step [886/7635], Loss: 4.9087\n",
      "Epoch [1/1], Step [887/7635], Loss: 4.7544\n",
      "Epoch [1/1], Step [888/7635], Loss: 4.9696\n",
      "Epoch [1/1], Step [889/7635], Loss: 4.9095\n",
      "Epoch [1/1], Step [890/7635], Loss: 4.8282\n",
      "Epoch [1/1], Step [891/7635], Loss: 4.8284\n",
      "Epoch [1/1], Step [892/7635], Loss: 4.9174\n",
      "Epoch [1/1], Step [893/7635], Loss: 4.9240\n",
      "Epoch [1/1], Step [894/7635], Loss: 4.8739\n",
      "Epoch [1/1], Step [895/7635], Loss: 4.8775\n",
      "Epoch [1/1], Step [896/7635], Loss: 4.8790\n",
      "Epoch [1/1], Step [897/7635], Loss: 4.8495\n",
      "Epoch [1/1], Step [898/7635], Loss: 4.8105\n",
      "Epoch [1/1], Step [899/7635], Loss: 4.8700\n",
      "Epoch [1/1], Step [900/7635], Loss: 4.9262\n",
      "Epoch [1/1], Step [901/7635], Loss: 4.8362\n",
      "Epoch [1/1], Step [902/7635], Loss: 5.0026\n",
      "Epoch [1/1], Step [903/7635], Loss: 4.9214\n",
      "Epoch [1/1], Step [904/7635], Loss: 4.8373\n",
      "Epoch [1/1], Step [905/7635], Loss: 4.9132\n",
      "Epoch [1/1], Step [906/7635], Loss: 4.8731\n",
      "Epoch [1/1], Step [907/7635], Loss: 4.8626\n",
      "Epoch [1/1], Step [908/7635], Loss: 4.9075\n",
      "Epoch [1/1], Step [909/7635], Loss: 4.9411\n",
      "Epoch [1/1], Step [910/7635], Loss: 4.9178\n",
      "Epoch [1/1], Step [911/7635], Loss: 4.8743\n",
      "Epoch [1/1], Step [912/7635], Loss: 4.8512\n",
      "Epoch [1/1], Step [913/7635], Loss: 4.9473\n",
      "Epoch [1/1], Step [914/7635], Loss: 4.8199\n",
      "Epoch [1/1], Step [915/7635], Loss: 4.8695\n",
      "Epoch [1/1], Step [916/7635], Loss: 4.8434\n",
      "Epoch [1/1], Step [917/7635], Loss: 4.8448\n",
      "Epoch [1/1], Step [918/7635], Loss: 4.8932\n",
      "Epoch [1/1], Step [919/7635], Loss: 4.8806\n",
      "Epoch [1/1], Step [920/7635], Loss: 4.6750\n",
      "Epoch [1/1], Step [921/7635], Loss: 4.7744\n",
      "Epoch [1/1], Step [922/7635], Loss: 4.7913\n",
      "Epoch [1/1], Step [923/7635], Loss: 4.9050\n",
      "Epoch [1/1], Step [924/7635], Loss: 4.8504\n",
      "Epoch [1/1], Step [925/7635], Loss: 4.9450\n",
      "Epoch [1/1], Step [926/7635], Loss: 4.8770\n",
      "Epoch [1/1], Step [927/7635], Loss: 4.8086\n",
      "Epoch [1/1], Step [928/7635], Loss: 4.8059\n",
      "Epoch [1/1], Step [929/7635], Loss: 4.9118\n",
      "Epoch [1/1], Step [930/7635], Loss: 4.9696\n",
      "Epoch [1/1], Step [931/7635], Loss: 4.8403\n",
      "Epoch [1/1], Step [932/7635], Loss: 4.9214\n",
      "Epoch [1/1], Step [933/7635], Loss: 4.7991\n",
      "Epoch [1/1], Step [934/7635], Loss: 4.7946\n",
      "Epoch [1/1], Step [935/7635], Loss: 4.8844\n",
      "Epoch [1/1], Step [936/7635], Loss: 4.9177\n",
      "Epoch [1/1], Step [937/7635], Loss: 4.8646\n",
      "Epoch [1/1], Step [938/7635], Loss: 4.8823\n",
      "Epoch [1/1], Step [939/7635], Loss: 4.7638\n",
      "Epoch [1/1], Step [940/7635], Loss: 4.8740\n",
      "Epoch [1/1], Step [941/7635], Loss: 4.9284\n",
      "Epoch [1/1], Step [942/7635], Loss: 4.8572\n",
      "Epoch [1/1], Step [943/7635], Loss: 4.8184\n",
      "Epoch [1/1], Step [944/7635], Loss: 4.8556\n",
      "Epoch [1/1], Step [945/7635], Loss: 4.9987\n",
      "Epoch [1/1], Step [946/7635], Loss: 4.7557\n",
      "Epoch [1/1], Step [947/7635], Loss: 4.7618\n",
      "Epoch [1/1], Step [948/7635], Loss: 4.8513\n",
      "Epoch [1/1], Step [949/7635], Loss: 4.8956\n",
      "Epoch [1/1], Step [950/7635], Loss: 4.7925\n",
      "Epoch [1/1], Step [951/7635], Loss: 4.8216\n",
      "Epoch [1/1], Step [952/7635], Loss: 4.8946\n",
      "Epoch [1/1], Step [953/7635], Loss: 4.8376\n",
      "Epoch [1/1], Step [954/7635], Loss: 4.8888\n",
      "Epoch [1/1], Step [955/7635], Loss: 4.8760\n",
      "Epoch [1/1], Step [956/7635], Loss: 4.8933\n",
      "Epoch [1/1], Step [957/7635], Loss: 4.8844\n",
      "Epoch [1/1], Step [958/7635], Loss: 4.7521\n",
      "Epoch [1/1], Step [959/7635], Loss: 4.7908\n",
      "Epoch [1/1], Step [960/7635], Loss: 4.8062\n",
      "Epoch [1/1], Step [961/7635], Loss: 4.8134\n",
      "Epoch [1/1], Step [962/7635], Loss: 4.8067\n",
      "Epoch [1/1], Step [963/7635], Loss: 4.7623\n",
      "Epoch [1/1], Step [964/7635], Loss: 4.8298\n",
      "Epoch [1/1], Step [965/7635], Loss: 4.8521\n",
      "Epoch [1/1], Step [966/7635], Loss: 4.8708\n",
      "Epoch [1/1], Step [967/7635], Loss: 4.9154\n",
      "Epoch [1/1], Step [968/7635], Loss: 4.8865\n",
      "Epoch [1/1], Step [969/7635], Loss: 4.8697\n",
      "Epoch [1/1], Step [970/7635], Loss: 4.8035\n",
      "Epoch [1/1], Step [971/7635], Loss: 4.8279\n",
      "Epoch [1/1], Step [972/7635], Loss: 4.8061\n",
      "Epoch [1/1], Step [973/7635], Loss: 4.6748\n",
      "Epoch [1/1], Step [974/7635], Loss: 4.8414\n",
      "Epoch [1/1], Step [975/7635], Loss: 4.9117\n",
      "Epoch [1/1], Step [976/7635], Loss: 4.8023\n",
      "Epoch [1/1], Step [977/7635], Loss: 4.8505\n",
      "Epoch [1/1], Step [978/7635], Loss: 4.7872\n",
      "Epoch [1/1], Step [979/7635], Loss: 4.8617\n",
      "Epoch [1/1], Step [980/7635], Loss: 4.7785\n",
      "Epoch [1/1], Step [981/7635], Loss: 4.9819\n",
      "Epoch [1/1], Step [982/7635], Loss: 4.8024\n",
      "Epoch [1/1], Step [983/7635], Loss: 4.8553\n",
      "Epoch [1/1], Step [984/7635], Loss: 4.8456\n",
      "Epoch [1/1], Step [985/7635], Loss: 4.8327\n",
      "Epoch [1/1], Step [986/7635], Loss: 4.7089\n",
      "Epoch [1/1], Step [987/7635], Loss: 4.8521\n",
      "Epoch [1/1], Step [988/7635], Loss: 4.7355\n",
      "Epoch [1/1], Step [989/7635], Loss: 4.7348\n",
      "Epoch [1/1], Step [990/7635], Loss: 4.8341\n",
      "Epoch [1/1], Step [991/7635], Loss: 4.8365\n",
      "Epoch [1/1], Step [992/7635], Loss: 4.8823\n",
      "Epoch [1/1], Step [993/7635], Loss: 4.8620\n",
      "Epoch [1/1], Step [994/7635], Loss: 4.8346\n",
      "Epoch [1/1], Step [995/7635], Loss: 4.8602\n",
      "Epoch [1/1], Step [996/7635], Loss: 4.7646\n",
      "Epoch [1/1], Step [997/7635], Loss: 4.7855\n",
      "Epoch [1/1], Step [998/7635], Loss: 4.8506\n",
      "Epoch [1/1], Step [999/7635], Loss: 4.7567\n",
      "Epoch [1/1], Step [1000/7635], Loss: 4.7727\n",
      "Epoch [1/1], Step [1001/7635], Loss: 4.7757\n",
      "Epoch [1/1], Step [1002/7635], Loss: 4.7994\n",
      "Epoch [1/1], Step [1003/7635], Loss: 4.8086\n",
      "Epoch [1/1], Step [1004/7635], Loss: 4.8331\n",
      "Epoch [1/1], Step [1005/7635], Loss: 4.7394\n",
      "Epoch [1/1], Step [1006/7635], Loss: 4.6706\n",
      "Epoch [1/1], Step [1007/7635], Loss: 4.8770\n",
      "Epoch [1/1], Step [1008/7635], Loss: 4.8206\n",
      "Epoch [1/1], Step [1009/7635], Loss: 4.7953\n",
      "Epoch [1/1], Step [1010/7635], Loss: 4.7629\n",
      "Epoch [1/1], Step [1011/7635], Loss: 4.8845\n",
      "Epoch [1/1], Step [1012/7635], Loss: 4.8741\n",
      "Epoch [1/1], Step [1013/7635], Loss: 4.8586\n",
      "Epoch [1/1], Step [1014/7635], Loss: 4.7468\n",
      "Epoch [1/1], Step [1015/7635], Loss: 4.7982\n",
      "Epoch [1/1], Step [1016/7635], Loss: 4.8603\n",
      "Epoch [1/1], Step [1017/7635], Loss: 4.8488\n",
      "Epoch [1/1], Step [1018/7635], Loss: 4.7703\n",
      "Epoch [1/1], Step [1019/7635], Loss: 4.7732\n",
      "Epoch [1/1], Step [1020/7635], Loss: 4.7296\n",
      "Epoch [1/1], Step [1021/7635], Loss: 4.8102\n",
      "Epoch [1/1], Step [1022/7635], Loss: 4.9254\n",
      "Epoch [1/1], Step [1023/7635], Loss: 4.8755\n",
      "Epoch [1/1], Step [1024/7635], Loss: 4.7302\n",
      "Epoch [1/1], Step [1025/7635], Loss: 4.8098\n",
      "Epoch [1/1], Step [1026/7635], Loss: 4.8318\n",
      "Epoch [1/1], Step [1027/7635], Loss: 4.7311\n",
      "Epoch [1/1], Step [1028/7635], Loss: 4.8072\n",
      "Epoch [1/1], Step [1029/7635], Loss: 4.7590\n",
      "Epoch [1/1], Step [1030/7635], Loss: 4.7338\n",
      "Epoch [1/1], Step [1031/7635], Loss: 4.7866\n",
      "Epoch [1/1], Step [1032/7635], Loss: 4.7728\n",
      "Epoch [1/1], Step [1033/7635], Loss: 4.8129\n",
      "Epoch [1/1], Step [1034/7635], Loss: 4.7875\n",
      "Epoch [1/1], Step [1035/7635], Loss: 4.7467\n",
      "Epoch [1/1], Step [1036/7635], Loss: 4.7869\n",
      "Epoch [1/1], Step [1037/7635], Loss: 4.7914\n",
      "Epoch [1/1], Step [1038/7635], Loss: 4.7797\n",
      "Epoch [1/1], Step [1039/7635], Loss: 4.8193\n",
      "Epoch [1/1], Step [1040/7635], Loss: 4.8412\n",
      "Epoch [1/1], Step [1041/7635], Loss: 4.7913\n",
      "Epoch [1/1], Step [1042/7635], Loss: 4.8033\n",
      "Epoch [1/1], Step [1043/7635], Loss: 4.6763\n",
      "Epoch [1/1], Step [1044/7635], Loss: 4.7856\n",
      "Epoch [1/1], Step [1045/7635], Loss: 4.7731\n",
      "Epoch [1/1], Step [1046/7635], Loss: 4.6936\n",
      "Epoch [1/1], Step [1047/7635], Loss: 4.7572\n",
      "Epoch [1/1], Step [1048/7635], Loss: 4.8262\n",
      "Epoch [1/1], Step [1049/7635], Loss: 4.8333\n",
      "Epoch [1/1], Step [1050/7635], Loss: 4.8153\n",
      "Epoch [1/1], Step [1051/7635], Loss: 4.8178\n",
      "Epoch [1/1], Step [1052/7635], Loss: 4.7113\n",
      "Epoch [1/1], Step [1053/7635], Loss: 4.8196\n",
      "Epoch [1/1], Step [1054/7635], Loss: 4.7725\n",
      "Epoch [1/1], Step [1055/7635], Loss: 4.7652\n",
      "Epoch [1/1], Step [1056/7635], Loss: 4.7626\n",
      "Epoch [1/1], Step [1057/7635], Loss: 4.7810\n",
      "Epoch [1/1], Step [1058/7635], Loss: 4.7669\n",
      "Epoch [1/1], Step [1059/7635], Loss: 4.7698\n",
      "Epoch [1/1], Step [1060/7635], Loss: 4.7746\n",
      "Epoch [1/1], Step [1061/7635], Loss: 4.8436\n",
      "Epoch [1/1], Step [1062/7635], Loss: 4.7888\n",
      "Epoch [1/1], Step [1063/7635], Loss: 4.8202\n",
      "Epoch [1/1], Step [1064/7635], Loss: 4.7909\n",
      "Epoch [1/1], Step [1065/7635], Loss: 4.8276\n",
      "Epoch [1/1], Step [1066/7635], Loss: 4.7448\n",
      "Epoch [1/1], Step [1067/7635], Loss: 4.7504\n",
      "Epoch [1/1], Step [1068/7635], Loss: 4.8034\n",
      "Epoch [1/1], Step [1069/7635], Loss: 4.6675\n",
      "Epoch [1/1], Step [1070/7635], Loss: 4.7155\n",
      "Epoch [1/1], Step [1071/7635], Loss: 4.7895\n",
      "Epoch [1/1], Step [1072/7635], Loss: 4.8492\n",
      "Epoch [1/1], Step [1073/7635], Loss: 4.7405\n",
      "Epoch [1/1], Step [1074/7635], Loss: 4.7808\n",
      "Epoch [1/1], Step [1075/7635], Loss: 4.7284\n",
      "Epoch [1/1], Step [1076/7635], Loss: 4.8856\n",
      "Epoch [1/1], Step [1077/7635], Loss: 4.7959\n",
      "Epoch [1/1], Step [1078/7635], Loss: 4.8779\n",
      "Epoch [1/1], Step [1079/7635], Loss: 4.7272\n",
      "Epoch [1/1], Step [1080/7635], Loss: 4.7567\n",
      "Epoch [1/1], Step [1081/7635], Loss: 4.7720\n",
      "Epoch [1/1], Step [1082/7635], Loss: 4.7909\n",
      "Epoch [1/1], Step [1083/7635], Loss: 4.7290\n",
      "Epoch [1/1], Step [1084/7635], Loss: 4.7906\n",
      "Epoch [1/1], Step [1085/7635], Loss: 4.6922\n",
      "Epoch [1/1], Step [1086/7635], Loss: 4.7662\n",
      "Epoch [1/1], Step [1087/7635], Loss: 4.7050\n",
      "Epoch [1/1], Step [1088/7635], Loss: 4.7361\n",
      "Epoch [1/1], Step [1089/7635], Loss: 4.7057\n",
      "Epoch [1/1], Step [1090/7635], Loss: 4.8505\n",
      "Epoch [1/1], Step [1091/7635], Loss: 4.8348\n",
      "Epoch [1/1], Step [1092/7635], Loss: 4.7321\n",
      "Epoch [1/1], Step [1093/7635], Loss: 4.7533\n",
      "Epoch [1/1], Step [1094/7635], Loss: 4.7435\n",
      "Epoch [1/1], Step [1095/7635], Loss: 4.7837\n",
      "Epoch [1/1], Step [1096/7635], Loss: 4.8394\n",
      "Epoch [1/1], Step [1097/7635], Loss: 4.7963\n",
      "Epoch [1/1], Step [1098/7635], Loss: 4.7536\n",
      "Epoch [1/1], Step [1099/7635], Loss: 4.7823\n",
      "Epoch [1/1], Step [1100/7635], Loss: 4.8072\n",
      "Epoch [1/1], Step [1101/7635], Loss: 4.7884\n",
      "Epoch [1/1], Step [1102/7635], Loss: 4.8515\n",
      "Epoch [1/1], Step [1103/7635], Loss: 4.7440\n",
      "Epoch [1/1], Step [1104/7635], Loss: 4.7174\n",
      "Epoch [1/1], Step [1105/7635], Loss: 4.7980\n",
      "Epoch [1/1], Step [1106/7635], Loss: 4.7808\n",
      "Epoch [1/1], Step [1107/7635], Loss: 4.7291\n",
      "Epoch [1/1], Step [1108/7635], Loss: 4.7572\n",
      "Epoch [1/1], Step [1109/7635], Loss: 4.7845\n",
      "Epoch [1/1], Step [1110/7635], Loss: 4.8075\n",
      "Epoch [1/1], Step [1111/7635], Loss: 4.6847\n",
      "Epoch [1/1], Step [1112/7635], Loss: 4.6865\n",
      "Epoch [1/1], Step [1113/7635], Loss: 4.7039\n",
      "Epoch [1/1], Step [1114/7635], Loss: 4.7220\n",
      "Epoch [1/1], Step [1115/7635], Loss: 4.8750\n",
      "Epoch [1/1], Step [1116/7635], Loss: 4.7250\n",
      "Epoch [1/1], Step [1117/7635], Loss: 4.6850\n",
      "Epoch [1/1], Step [1118/7635], Loss: 4.6064\n",
      "Epoch [1/1], Step [1119/7635], Loss: 4.7682\n",
      "Epoch [1/1], Step [1120/7635], Loss: 4.7444\n",
      "Epoch [1/1], Step [1121/7635], Loss: 4.7140\n",
      "Epoch [1/1], Step [1122/7635], Loss: 4.7470\n",
      "Epoch [1/1], Step [1123/7635], Loss: 4.7077\n",
      "Epoch [1/1], Step [1124/7635], Loss: 4.8085\n",
      "Epoch [1/1], Step [1125/7635], Loss: 4.7367\n",
      "Epoch [1/1], Step [1126/7635], Loss: 4.6067\n",
      "Epoch [1/1], Step [1127/7635], Loss: 4.7604\n",
      "Epoch [1/1], Step [1128/7635], Loss: 4.6979\n",
      "Epoch [1/1], Step [1129/7635], Loss: 4.7244\n",
      "Epoch [1/1], Step [1130/7635], Loss: 4.7526\n",
      "Epoch [1/1], Step [1131/7635], Loss: 4.7839\n",
      "Epoch [1/1], Step [1132/7635], Loss: 4.8543\n",
      "Epoch [1/1], Step [1133/7635], Loss: 4.8588\n",
      "Epoch [1/1], Step [1134/7635], Loss: 4.7495\n",
      "Epoch [1/1], Step [1135/7635], Loss: 4.7001\n",
      "Epoch [1/1], Step [1136/7635], Loss: 4.8130\n",
      "Epoch [1/1], Step [1137/7635], Loss: 4.7182\n",
      "Epoch [1/1], Step [1138/7635], Loss: 4.7092\n",
      "Epoch [1/1], Step [1139/7635], Loss: 4.8097\n",
      "Epoch [1/1], Step [1140/7635], Loss: 4.8274\n",
      "Epoch [1/1], Step [1141/7635], Loss: 4.7639\n",
      "Epoch [1/1], Step [1142/7635], Loss: 4.6894\n",
      "Epoch [1/1], Step [1143/7635], Loss: 4.7232\n",
      "Epoch [1/1], Step [1144/7635], Loss: 4.7255\n",
      "Epoch [1/1], Step [1145/7635], Loss: 4.8000\n",
      "Epoch [1/1], Step [1146/7635], Loss: 4.7911\n",
      "Epoch [1/1], Step [1147/7635], Loss: 4.6601\n",
      "Epoch [1/1], Step [1148/7635], Loss: 4.7852\n",
      "Epoch [1/1], Step [1149/7635], Loss: 4.8650\n",
      "Epoch [1/1], Step [1150/7635], Loss: 4.6940\n",
      "Epoch [1/1], Step [1151/7635], Loss: 4.6954\n",
      "Epoch [1/1], Step [1152/7635], Loss: 4.7992\n",
      "Epoch [1/1], Step [1153/7635], Loss: 4.5953\n",
      "Epoch [1/1], Step [1154/7635], Loss: 4.6942\n",
      "Epoch [1/1], Step [1155/7635], Loss: 4.8256\n",
      "Epoch [1/1], Step [1156/7635], Loss: 4.7390\n",
      "Epoch [1/1], Step [1157/7635], Loss: 4.7109\n",
      "Epoch [1/1], Step [1158/7635], Loss: 4.7079\n",
      "Epoch [1/1], Step [1159/7635], Loss: 4.7132\n",
      "Epoch [1/1], Step [1160/7635], Loss: 4.7379\n",
      "Epoch [1/1], Step [1161/7635], Loss: 4.7016\n",
      "Epoch [1/1], Step [1162/7635], Loss: 4.7862\n",
      "Epoch [1/1], Step [1163/7635], Loss: 4.7197\n",
      "Epoch [1/1], Step [1164/7635], Loss: 4.7641\n",
      "Epoch [1/1], Step [1165/7635], Loss: 4.7591\n",
      "Epoch [1/1], Step [1166/7635], Loss: 4.7072\n",
      "Epoch [1/1], Step [1167/7635], Loss: 4.6168\n",
      "Epoch [1/1], Step [1168/7635], Loss: 4.7506\n",
      "Epoch [1/1], Step [1169/7635], Loss: 4.8056\n",
      "Epoch [1/1], Step [1170/7635], Loss: 4.7009\n",
      "Epoch [1/1], Step [1171/7635], Loss: 4.7605\n",
      "Epoch [1/1], Step [1172/7635], Loss: 4.7443\n",
      "Epoch [1/1], Step [1173/7635], Loss: 4.7875\n",
      "Epoch [1/1], Step [1174/7635], Loss: 4.7668\n",
      "Epoch [1/1], Step [1175/7635], Loss: 4.7230\n",
      "Epoch [1/1], Step [1176/7635], Loss: 4.7375\n",
      "Epoch [1/1], Step [1177/7635], Loss: 4.7535\n",
      "Epoch [1/1], Step [1178/7635], Loss: 4.6884\n",
      "Epoch [1/1], Step [1179/7635], Loss: 4.7485\n",
      "Epoch [1/1], Step [1180/7635], Loss: 4.6745\n",
      "Epoch [1/1], Step [1181/7635], Loss: 4.6763\n",
      "Epoch [1/1], Step [1182/7635], Loss: 4.6452\n",
      "Epoch [1/1], Step [1183/7635], Loss: 4.7153\n",
      "Epoch [1/1], Step [1184/7635], Loss: 4.6907\n",
      "Epoch [1/1], Step [1185/7635], Loss: 4.7148\n",
      "Epoch [1/1], Step [1186/7635], Loss: 4.6528\n",
      "Epoch [1/1], Step [1187/7635], Loss: 4.7089\n",
      "Epoch [1/1], Step [1188/7635], Loss: 4.7657\n",
      "Epoch [1/1], Step [1189/7635], Loss: 4.6455\n",
      "Epoch [1/1], Step [1190/7635], Loss: 4.6252\n",
      "Epoch [1/1], Step [1191/7635], Loss: 4.5831\n",
      "Epoch [1/1], Step [1192/7635], Loss: 4.7575\n",
      "Epoch [1/1], Step [1193/7635], Loss: 4.6905\n",
      "Epoch [1/1], Step [1194/7635], Loss: 4.6962\n",
      "Epoch [1/1], Step [1195/7635], Loss: 4.8093\n",
      "Epoch [1/1], Step [1196/7635], Loss: 4.7188\n",
      "Epoch [1/1], Step [1197/7635], Loss: 4.7848\n",
      "Epoch [1/1], Step [1198/7635], Loss: 4.7335\n",
      "Epoch [1/1], Step [1199/7635], Loss: 4.6111\n",
      "Epoch [1/1], Step [1200/7635], Loss: 4.7626\n",
      "Epoch [1/1], Step [1201/7635], Loss: 4.8094\n",
      "Epoch [1/1], Step [1202/7635], Loss: 4.7768\n",
      "Epoch [1/1], Step [1203/7635], Loss: 4.6970\n",
      "Epoch [1/1], Step [1204/7635], Loss: 4.6339\n",
      "Epoch [1/1], Step [1205/7635], Loss: 4.6235\n",
      "Epoch [1/1], Step [1206/7635], Loss: 4.5570\n",
      "Epoch [1/1], Step [1207/7635], Loss: 4.6723\n",
      "Epoch [1/1], Step [1208/7635], Loss: 4.8021\n",
      "Epoch [1/1], Step [1209/7635], Loss: 4.6151\n",
      "Epoch [1/1], Step [1210/7635], Loss: 4.7618\n",
      "Epoch [1/1], Step [1211/7635], Loss: 4.7326\n",
      "Epoch [1/1], Step [1212/7635], Loss: 4.7031\n",
      "Epoch [1/1], Step [1213/7635], Loss: 4.6595\n",
      "Epoch [1/1], Step [1214/7635], Loss: 4.7003\n",
      "Epoch [1/1], Step [1215/7635], Loss: 4.7150\n",
      "Epoch [1/1], Step [1216/7635], Loss: 4.7111\n",
      "Epoch [1/1], Step [1217/7635], Loss: 4.7668\n",
      "Epoch [1/1], Step [1218/7635], Loss: 4.7691\n",
      "Epoch [1/1], Step [1219/7635], Loss: 4.7282\n",
      "Epoch [1/1], Step [1220/7635], Loss: 4.7968\n",
      "Epoch [1/1], Step [1221/7635], Loss: 4.6415\n",
      "Epoch [1/1], Step [1222/7635], Loss: 4.7868\n",
      "Epoch [1/1], Step [1223/7635], Loss: 4.6854\n",
      "Epoch [1/1], Step [1224/7635], Loss: 4.6543\n",
      "Epoch [1/1], Step [1225/7635], Loss: 4.7504\n",
      "Epoch [1/1], Step [1226/7635], Loss: 4.5913\n",
      "Epoch [1/1], Step [1227/7635], Loss: 4.7464\n",
      "Epoch [1/1], Step [1228/7635], Loss: 4.6874\n",
      "Epoch [1/1], Step [1229/7635], Loss: 4.6490\n",
      "Epoch [1/1], Step [1230/7635], Loss: 4.6303\n",
      "Epoch [1/1], Step [1231/7635], Loss: 4.6954\n",
      "Epoch [1/1], Step [1232/7635], Loss: 4.6066\n",
      "Epoch [1/1], Step [1233/7635], Loss: 4.5927\n",
      "Epoch [1/1], Step [1234/7635], Loss: 4.6919\n",
      "Epoch [1/1], Step [1235/7635], Loss: 4.6781\n",
      "Epoch [1/1], Step [1236/7635], Loss: 4.6760\n",
      "Epoch [1/1], Step [1237/7635], Loss: 4.7172\n",
      "Epoch [1/1], Step [1238/7635], Loss: 4.6895\n",
      "Epoch [1/1], Step [1239/7635], Loss: 4.7026\n",
      "Epoch [1/1], Step [1240/7635], Loss: 4.8321\n",
      "Epoch [1/1], Step [1241/7635], Loss: 4.7871\n",
      "Epoch [1/1], Step [1242/7635], Loss: 4.7393\n",
      "Epoch [1/1], Step [1243/7635], Loss: 4.6794\n",
      "Epoch [1/1], Step [1244/7635], Loss: 4.7268\n",
      "Epoch [1/1], Step [1245/7635], Loss: 4.7481\n",
      "Epoch [1/1], Step [1246/7635], Loss: 4.5796\n",
      "Epoch [1/1], Step [1247/7635], Loss: 4.7201\n",
      "Epoch [1/1], Step [1248/7635], Loss: 4.6330\n",
      "Epoch [1/1], Step [1249/7635], Loss: 4.6970\n",
      "Epoch [1/1], Step [1250/7635], Loss: 4.6720\n",
      "Epoch [1/1], Step [1251/7635], Loss: 4.7257\n",
      "Epoch [1/1], Step [1252/7635], Loss: 4.7456\n",
      "Epoch [1/1], Step [1253/7635], Loss: 4.7493\n",
      "Epoch [1/1], Step [1254/7635], Loss: 4.7565\n",
      "Epoch [1/1], Step [1255/7635], Loss: 4.6559\n",
      "Epoch [1/1], Step [1256/7635], Loss: 4.7023\n",
      "Epoch [1/1], Step [1257/7635], Loss: 4.6040\n",
      "Epoch [1/1], Step [1258/7635], Loss: 4.6436\n",
      "Epoch [1/1], Step [1259/7635], Loss: 4.6841\n",
      "Epoch [1/1], Step [1260/7635], Loss: 4.6423\n",
      "Epoch [1/1], Step [1261/7635], Loss: 4.7397\n",
      "Epoch [1/1], Step [1262/7635], Loss: 4.7132\n",
      "Epoch [1/1], Step [1263/7635], Loss: 4.6343\n",
      "Epoch [1/1], Step [1264/7635], Loss: 4.7875\n",
      "Epoch [1/1], Step [1265/7635], Loss: 4.5837\n",
      "Epoch [1/1], Step [1266/7635], Loss: 4.7014\n",
      "Epoch [1/1], Step [1267/7635], Loss: 4.7515\n",
      "Epoch [1/1], Step [1268/7635], Loss: 4.6900\n",
      "Epoch [1/1], Step [1269/7635], Loss: 4.5850\n",
      "Epoch [1/1], Step [1270/7635], Loss: 4.6523\n",
      "Epoch [1/1], Step [1271/7635], Loss: 4.7415\n",
      "Epoch [1/1], Step [1272/7635], Loss: 4.6401\n",
      "Epoch [1/1], Step [1273/7635], Loss: 4.6808\n",
      "Epoch [1/1], Step [1274/7635], Loss: 4.7318\n",
      "Epoch [1/1], Step [1275/7635], Loss: 4.7093\n",
      "Epoch [1/1], Step [1276/7635], Loss: 4.7644\n",
      "Epoch [1/1], Step [1277/7635], Loss: 4.7300\n",
      "Epoch [1/1], Step [1278/7635], Loss: 4.7061\n",
      "Epoch [1/1], Step [1279/7635], Loss: 4.7355\n",
      "Epoch [1/1], Step [1280/7635], Loss: 4.7394\n",
      "Epoch [1/1], Step [1281/7635], Loss: 4.6138\n",
      "Epoch [1/1], Step [1282/7635], Loss: 4.7030\n",
      "Epoch [1/1], Step [1283/7635], Loss: 4.6877\n",
      "Epoch [1/1], Step [1284/7635], Loss: 4.6496\n",
      "Epoch [1/1], Step [1285/7635], Loss: 4.6679\n",
      "Epoch [1/1], Step [1286/7635], Loss: 4.6802\n",
      "Epoch [1/1], Step [1287/7635], Loss: 4.7105\n",
      "Epoch [1/1], Step [1288/7635], Loss: 4.5429\n",
      "Epoch [1/1], Step [1289/7635], Loss: 4.6047\n",
      "Epoch [1/1], Step [1290/7635], Loss: 4.6372\n",
      "Epoch [1/1], Step [1291/7635], Loss: 4.6230\n",
      "Epoch [1/1], Step [1292/7635], Loss: 4.7922\n",
      "Epoch [1/1], Step [1293/7635], Loss: 4.7412\n",
      "Epoch [1/1], Step [1294/7635], Loss: 4.7658\n",
      "Epoch [1/1], Step [1295/7635], Loss: 4.7358\n",
      "Epoch [1/1], Step [1296/7635], Loss: 4.6516\n",
      "Epoch [1/1], Step [1297/7635], Loss: 4.5724\n",
      "Epoch [1/1], Step [1298/7635], Loss: 4.6042\n",
      "Epoch [1/1], Step [1299/7635], Loss: 4.7155\n",
      "Epoch [1/1], Step [1300/7635], Loss: 4.6384\n",
      "Epoch [1/1], Step [1301/7635], Loss: 4.7234\n",
      "Epoch [1/1], Step [1302/7635], Loss: 4.6561\n",
      "Epoch [1/1], Step [1303/7635], Loss: 4.6600\n",
      "Epoch [1/1], Step [1304/7635], Loss: 4.6063\n",
      "Epoch [1/1], Step [1305/7635], Loss: 4.8039\n",
      "Epoch [1/1], Step [1306/7635], Loss: 4.7213\n",
      "Epoch [1/1], Step [1307/7635], Loss: 4.7536\n",
      "Epoch [1/1], Step [1308/7635], Loss: 4.5959\n",
      "Epoch [1/1], Step [1309/7635], Loss: 4.7318\n",
      "Epoch [1/1], Step [1310/7635], Loss: 4.6715\n",
      "Epoch [1/1], Step [1311/7635], Loss: 4.6685\n",
      "Epoch [1/1], Step [1312/7635], Loss: 4.6437\n",
      "Epoch [1/1], Step [1313/7635], Loss: 4.6707\n",
      "Epoch [1/1], Step [1314/7635], Loss: 4.6343\n",
      "Epoch [1/1], Step [1315/7635], Loss: 4.7256\n",
      "Epoch [1/1], Step [1316/7635], Loss: 4.6873\n",
      "Epoch [1/1], Step [1317/7635], Loss: 4.6794\n",
      "Epoch [1/1], Step [1318/7635], Loss: 4.7538\n",
      "Epoch [1/1], Step [1319/7635], Loss: 4.7017\n",
      "Epoch [1/1], Step [1320/7635], Loss: 4.7057\n",
      "Epoch [1/1], Step [1321/7635], Loss: 4.6624\n",
      "Epoch [1/1], Step [1322/7635], Loss: 4.6405\n",
      "Epoch [1/1], Step [1323/7635], Loss: 4.7152\n",
      "Epoch [1/1], Step [1324/7635], Loss: 4.6794\n",
      "Epoch [1/1], Step [1325/7635], Loss: 4.5424\n",
      "Epoch [1/1], Step [1326/7635], Loss: 4.6466\n",
      "Epoch [1/1], Step [1327/7635], Loss: 4.6136\n",
      "Epoch [1/1], Step [1328/7635], Loss: 4.6881\n",
      "Epoch [1/1], Step [1329/7635], Loss: 4.7179\n",
      "Epoch [1/1], Step [1330/7635], Loss: 4.6924\n",
      "Epoch [1/1], Step [1331/7635], Loss: 4.6986\n",
      "Epoch [1/1], Step [1332/7635], Loss: 4.6293\n",
      "Epoch [1/1], Step [1333/7635], Loss: 4.7317\n",
      "Epoch [1/1], Step [1334/7635], Loss: 4.7350\n",
      "Epoch [1/1], Step [1335/7635], Loss: 4.6738\n",
      "Epoch [1/1], Step [1336/7635], Loss: 4.6564\n",
      "Epoch [1/1], Step [1337/7635], Loss: 4.6354\n",
      "Epoch [1/1], Step [1338/7635], Loss: 4.6581\n",
      "Epoch [1/1], Step [1339/7635], Loss: 4.6969\n",
      "Epoch [1/1], Step [1340/7635], Loss: 4.6061\n",
      "Epoch [1/1], Step [1341/7635], Loss: 4.6427\n",
      "Epoch [1/1], Step [1342/7635], Loss: 4.7593\n",
      "Epoch [1/1], Step [1343/7635], Loss: 4.6282\n",
      "Epoch [1/1], Step [1344/7635], Loss: 4.6704\n",
      "Epoch [1/1], Step [1345/7635], Loss: 4.7472\n",
      "Epoch [1/1], Step [1346/7635], Loss: 4.7541\n",
      "Epoch [1/1], Step [1347/7635], Loss: 4.5361\n",
      "Epoch [1/1], Step [1348/7635], Loss: 4.6060\n",
      "Epoch [1/1], Step [1349/7635], Loss: 4.7329\n",
      "Epoch [1/1], Step [1350/7635], Loss: 4.6412\n",
      "Epoch [1/1], Step [1351/7635], Loss: 4.6624\n",
      "Epoch [1/1], Step [1352/7635], Loss: 4.6609\n",
      "Epoch [1/1], Step [1353/7635], Loss: 4.6388\n",
      "Epoch [1/1], Step [1354/7635], Loss: 4.6269\n",
      "Epoch [1/1], Step [1355/7635], Loss: 4.7557\n",
      "Epoch [1/1], Step [1356/7635], Loss: 4.6552\n",
      "Epoch [1/1], Step [1357/7635], Loss: 4.6465\n",
      "Epoch [1/1], Step [1358/7635], Loss: 4.6360\n",
      "Epoch [1/1], Step [1359/7635], Loss: 4.7049\n",
      "Epoch [1/1], Step [1360/7635], Loss: 4.5607\n",
      "Epoch [1/1], Step [1361/7635], Loss: 4.6814\n",
      "Epoch [1/1], Step [1362/7635], Loss: 4.5903\n",
      "Epoch [1/1], Step [1363/7635], Loss: 4.6558\n",
      "Epoch [1/1], Step [1364/7635], Loss: 4.6759\n",
      "Epoch [1/1], Step [1365/7635], Loss: 4.6950\n",
      "Epoch [1/1], Step [1366/7635], Loss: 4.7181\n",
      "Epoch [1/1], Step [1367/7635], Loss: 4.7387\n",
      "Epoch [1/1], Step [1368/7635], Loss: 4.6601\n",
      "Epoch [1/1], Step [1369/7635], Loss: 4.6458\n",
      "Epoch [1/1], Step [1370/7635], Loss: 4.5991\n",
      "Epoch [1/1], Step [1371/7635], Loss: 4.6513\n",
      "Epoch [1/1], Step [1372/7635], Loss: 4.6299\n",
      "Epoch [1/1], Step [1373/7635], Loss: 4.6261\n",
      "Epoch [1/1], Step [1374/7635], Loss: 4.7099\n",
      "Epoch [1/1], Step [1375/7635], Loss: 4.7173\n",
      "Epoch [1/1], Step [1376/7635], Loss: 4.6220\n",
      "Epoch [1/1], Step [1377/7635], Loss: 4.6178\n",
      "Epoch [1/1], Step [1378/7635], Loss: 4.6592\n",
      "Epoch [1/1], Step [1379/7635], Loss: 4.5423\n",
      "Epoch [1/1], Step [1380/7635], Loss: 4.5513\n",
      "Epoch [1/1], Step [1381/7635], Loss: 4.7440\n",
      "Epoch [1/1], Step [1382/7635], Loss: 4.6056\n",
      "Epoch [1/1], Step [1383/7635], Loss: 4.6382\n",
      "Epoch [1/1], Step [1384/7635], Loss: 4.5649\n",
      "Epoch [1/1], Step [1385/7635], Loss: 4.6739\n",
      "Epoch [1/1], Step [1386/7635], Loss: 4.5661\n",
      "Epoch [1/1], Step [1387/7635], Loss: 4.6441\n",
      "Epoch [1/1], Step [1388/7635], Loss: 4.5539\n",
      "Epoch [1/1], Step [1389/7635], Loss: 4.6199\n",
      "Epoch [1/1], Step [1390/7635], Loss: 4.6063\n",
      "Epoch [1/1], Step [1391/7635], Loss: 4.6098\n",
      "Epoch [1/1], Step [1392/7635], Loss: 4.5961\n",
      "Epoch [1/1], Step [1393/7635], Loss: 4.4803\n",
      "Epoch [1/1], Step [1394/7635], Loss: 4.6245\n",
      "Epoch [1/1], Step [1395/7635], Loss: 4.7029\n",
      "Epoch [1/1], Step [1396/7635], Loss: 4.6852\n",
      "Epoch [1/1], Step [1397/7635], Loss: 4.6473\n",
      "Epoch [1/1], Step [1398/7635], Loss: 4.7030\n",
      "Epoch [1/1], Step [1399/7635], Loss: 4.5913\n",
      "Epoch [1/1], Step [1400/7635], Loss: 4.6065\n",
      "Epoch [1/1], Step [1401/7635], Loss: 4.6392\n",
      "Epoch [1/1], Step [1402/7635], Loss: 4.6086\n",
      "Epoch [1/1], Step [1403/7635], Loss: 4.5266\n",
      "Epoch [1/1], Step [1404/7635], Loss: 4.5662\n",
      "Epoch [1/1], Step [1405/7635], Loss: 4.6064\n",
      "Epoch [1/1], Step [1406/7635], Loss: 4.6352\n",
      "Epoch [1/1], Step [1407/7635], Loss: 4.6223\n",
      "Epoch [1/1], Step [1408/7635], Loss: 4.6255\n",
      "Epoch [1/1], Step [1409/7635], Loss: 4.6470\n",
      "Epoch [1/1], Step [1410/7635], Loss: 4.5656\n",
      "Epoch [1/1], Step [1411/7635], Loss: 4.6579\n",
      "Epoch [1/1], Step [1412/7635], Loss: 4.6700\n",
      "Epoch [1/1], Step [1413/7635], Loss: 4.5748\n",
      "Epoch [1/1], Step [1414/7635], Loss: 4.6644\n",
      "Epoch [1/1], Step [1415/7635], Loss: 4.6320\n",
      "Epoch [1/1], Step [1416/7635], Loss: 4.6499\n",
      "Epoch [1/1], Step [1417/7635], Loss: 4.6029\n",
      "Epoch [1/1], Step [1418/7635], Loss: 4.5657\n",
      "Epoch [1/1], Step [1419/7635], Loss: 4.7063\n",
      "Epoch [1/1], Step [1420/7635], Loss: 4.5747\n",
      "Epoch [1/1], Step [1421/7635], Loss: 4.6985\n",
      "Epoch [1/1], Step [1422/7635], Loss: 4.6436\n",
      "Epoch [1/1], Step [1423/7635], Loss: 4.6010\n",
      "Epoch [1/1], Step [1424/7635], Loss: 4.6105\n",
      "Epoch [1/1], Step [1425/7635], Loss: 4.6300\n",
      "Epoch [1/1], Step [1426/7635], Loss: 4.5759\n",
      "Epoch [1/1], Step [1427/7635], Loss: 4.6264\n",
      "Epoch [1/1], Step [1428/7635], Loss: 4.6543\n",
      "Epoch [1/1], Step [1429/7635], Loss: 4.6541\n",
      "Epoch [1/1], Step [1430/7635], Loss: 4.6234\n",
      "Epoch [1/1], Step [1431/7635], Loss: 4.6123\n",
      "Epoch [1/1], Step [1432/7635], Loss: 4.6004\n",
      "Epoch [1/1], Step [1433/7635], Loss: 4.6998\n",
      "Epoch [1/1], Step [1434/7635], Loss: 4.6063\n",
      "Epoch [1/1], Step [1435/7635], Loss: 4.6632\n",
      "Epoch [1/1], Step [1436/7635], Loss: 4.7476\n",
      "Epoch [1/1], Step [1437/7635], Loss: 4.6121\n",
      "Epoch [1/1], Step [1438/7635], Loss: 4.6209\n",
      "Epoch [1/1], Step [1439/7635], Loss: 4.6394\n",
      "Epoch [1/1], Step [1440/7635], Loss: 4.6552\n",
      "Epoch [1/1], Step [1441/7635], Loss: 4.6271\n",
      "Epoch [1/1], Step [1442/7635], Loss: 4.6248\n",
      "Epoch [1/1], Step [1443/7635], Loss: 4.6070\n",
      "Epoch [1/1], Step [1444/7635], Loss: 4.6458\n",
      "Epoch [1/1], Step [1445/7635], Loss: 4.6479\n",
      "Epoch [1/1], Step [1446/7635], Loss: 4.6250\n",
      "Epoch [1/1], Step [1447/7635], Loss: 4.6950\n",
      "Epoch [1/1], Step [1448/7635], Loss: 4.6151\n",
      "Epoch [1/1], Step [1449/7635], Loss: 4.6376\n",
      "Epoch [1/1], Step [1450/7635], Loss: 4.6880\n",
      "Epoch [1/1], Step [1451/7635], Loss: 4.7368\n",
      "Epoch [1/1], Step [1452/7635], Loss: 4.5751\n",
      "Epoch [1/1], Step [1453/7635], Loss: 4.6272\n",
      "Epoch [1/1], Step [1454/7635], Loss: 4.6816\n",
      "Epoch [1/1], Step [1455/7635], Loss: 4.6709\n",
      "Epoch [1/1], Step [1456/7635], Loss: 4.5482\n",
      "Epoch [1/1], Step [1457/7635], Loss: 4.5729\n",
      "Epoch [1/1], Step [1458/7635], Loss: 4.5646\n",
      "Epoch [1/1], Step [1459/7635], Loss: 4.7100\n",
      "Epoch [1/1], Step [1460/7635], Loss: 4.6090\n",
      "Epoch [1/1], Step [1461/7635], Loss: 4.6474\n",
      "Epoch [1/1], Step [1462/7635], Loss: 4.5620\n",
      "Epoch [1/1], Step [1463/7635], Loss: 4.6614\n",
      "Epoch [1/1], Step [1464/7635], Loss: 4.6487\n",
      "Epoch [1/1], Step [1465/7635], Loss: 4.5301\n",
      "Epoch [1/1], Step [1466/7635], Loss: 4.5940\n",
      "Epoch [1/1], Step [1467/7635], Loss: 4.5516\n",
      "Epoch [1/1], Step [1468/7635], Loss: 4.6250\n",
      "Epoch [1/1], Step [1469/7635], Loss: 4.5389\n",
      "Epoch [1/1], Step [1470/7635], Loss: 4.6173\n",
      "Epoch [1/1], Step [1471/7635], Loss: 4.6278\n",
      "Epoch [1/1], Step [1472/7635], Loss: 4.6211\n",
      "Epoch [1/1], Step [1473/7635], Loss: 4.5465\n",
      "Epoch [1/1], Step [1474/7635], Loss: 4.5897\n",
      "Epoch [1/1], Step [1475/7635], Loss: 4.6147\n",
      "Epoch [1/1], Step [1476/7635], Loss: 4.5122\n",
      "Epoch [1/1], Step [1477/7635], Loss: 4.5869\n",
      "Epoch [1/1], Step [1478/7635], Loss: 4.6311\n",
      "Epoch [1/1], Step [1479/7635], Loss: 4.6684\n",
      "Epoch [1/1], Step [1480/7635], Loss: 4.5823\n",
      "Epoch [1/1], Step [1481/7635], Loss: 4.5820\n",
      "Epoch [1/1], Step [1482/7635], Loss: 4.6419\n",
      "Epoch [1/1], Step [1483/7635], Loss: 4.6197\n",
      "Epoch [1/1], Step [1484/7635], Loss: 4.6175\n",
      "Epoch [1/1], Step [1485/7635], Loss: 4.5338\n",
      "Epoch [1/1], Step [1486/7635], Loss: 4.6502\n",
      "Epoch [1/1], Step [1487/7635], Loss: 4.7314\n",
      "Epoch [1/1], Step [1488/7635], Loss: 4.6477\n",
      "Epoch [1/1], Step [1489/7635], Loss: 4.5920\n",
      "Epoch [1/1], Step [1490/7635], Loss: 4.5503\n",
      "Epoch [1/1], Step [1491/7635], Loss: 4.5251\n",
      "Epoch [1/1], Step [1492/7635], Loss: 4.5446\n",
      "Epoch [1/1], Step [1493/7635], Loss: 4.6313\n",
      "Epoch [1/1], Step [1494/7635], Loss: 4.6821\n",
      "Epoch [1/1], Step [1495/7635], Loss: 4.6527\n",
      "Epoch [1/1], Step [1496/7635], Loss: 4.5947\n",
      "Epoch [1/1], Step [1497/7635], Loss: 4.6421\n",
      "Epoch [1/1], Step [1498/7635], Loss: 4.6862\n",
      "Epoch [1/1], Step [1499/7635], Loss: 4.6185\n",
      "Epoch [1/1], Step [1500/7635], Loss: 4.5805\n",
      "Epoch [1/1], Step [1501/7635], Loss: 4.5933\n",
      "Epoch [1/1], Step [1502/7635], Loss: 4.6235\n",
      "Epoch [1/1], Step [1503/7635], Loss: 4.5033\n",
      "Epoch [1/1], Step [1504/7635], Loss: 4.6349\n",
      "Epoch [1/1], Step [1505/7635], Loss: 4.5627\n",
      "Epoch [1/1], Step [1506/7635], Loss: 4.6511\n",
      "Epoch [1/1], Step [1507/7635], Loss: 4.6098\n",
      "Epoch [1/1], Step [1508/7635], Loss: 4.6235\n",
      "Epoch [1/1], Step [1509/7635], Loss: 4.6659\n",
      "Epoch [1/1], Step [1510/7635], Loss: 4.7211\n",
      "Epoch [1/1], Step [1511/7635], Loss: 4.8211\n",
      "Epoch [1/1], Step [1512/7635], Loss: 4.6837\n",
      "Epoch [1/1], Step [1513/7635], Loss: 4.6723\n",
      "Epoch [1/1], Step [1514/7635], Loss: 4.6524\n",
      "Epoch [1/1], Step [1515/7635], Loss: 4.5109\n",
      "Epoch [1/1], Step [1516/7635], Loss: 4.6811\n",
      "Epoch [1/1], Step [1517/7635], Loss: 4.6025\n",
      "Epoch [1/1], Step [1518/7635], Loss: 4.6249\n",
      "Epoch [1/1], Step [1519/7635], Loss: 4.5249\n",
      "Epoch [1/1], Step [1520/7635], Loss: 4.6127\n",
      "Epoch [1/1], Step [1521/7635], Loss: 4.5671\n",
      "Epoch [1/1], Step [1522/7635], Loss: 4.6100\n",
      "Epoch [1/1], Step [1523/7635], Loss: 4.5822\n",
      "Epoch [1/1], Step [1524/7635], Loss: 4.5431\n",
      "Epoch [1/1], Step [1525/7635], Loss: 4.6260\n",
      "Epoch [1/1], Step [1526/7635], Loss: 4.6450\n",
      "Epoch [1/1], Step [1527/7635], Loss: 4.5123\n",
      "Epoch [1/1], Step [1528/7635], Loss: 4.6256\n",
      "Epoch [1/1], Step [1529/7635], Loss: 4.5322\n",
      "Epoch [1/1], Step [1530/7635], Loss: 4.4751\n",
      "Epoch [1/1], Step [1531/7635], Loss: 4.5958\n",
      "Epoch [1/1], Step [1532/7635], Loss: 4.7068\n",
      "Epoch [1/1], Step [1533/7635], Loss: 4.6248\n",
      "Epoch [1/1], Step [1534/7635], Loss: 4.5962\n",
      "Epoch [1/1], Step [1535/7635], Loss: 4.5450\n",
      "Epoch [1/1], Step [1536/7635], Loss: 4.6407\n",
      "Epoch [1/1], Step [1537/7635], Loss: 4.6051\n",
      "Epoch [1/1], Step [1538/7635], Loss: 4.5418\n",
      "Epoch [1/1], Step [1539/7635], Loss: 4.6256\n",
      "Epoch [1/1], Step [1540/7635], Loss: 4.5162\n",
      "Epoch [1/1], Step [1541/7635], Loss: 4.5475\n",
      "Epoch [1/1], Step [1542/7635], Loss: 4.5415\n",
      "Epoch [1/1], Step [1543/7635], Loss: 4.6196\n",
      "Epoch [1/1], Step [1544/7635], Loss: 4.5545\n",
      "Epoch [1/1], Step [1545/7635], Loss: 4.6529\n",
      "Epoch [1/1], Step [1546/7635], Loss: 4.7114\n",
      "Epoch [1/1], Step [1547/7635], Loss: 4.5788\n",
      "Epoch [1/1], Step [1548/7635], Loss: 4.5146\n",
      "Epoch [1/1], Step [1549/7635], Loss: 4.5546\n",
      "Epoch [1/1], Step [1550/7635], Loss: 4.5842\n",
      "Epoch [1/1], Step [1551/7635], Loss: 4.5684\n",
      "Epoch [1/1], Step [1552/7635], Loss: 4.5173\n",
      "Epoch [1/1], Step [1553/7635], Loss: 4.5480\n",
      "Epoch [1/1], Step [1554/7635], Loss: 4.5619\n",
      "Epoch [1/1], Step [1555/7635], Loss: 4.5698\n",
      "Epoch [1/1], Step [1556/7635], Loss: 4.8025\n",
      "Epoch [1/1], Step [1557/7635], Loss: 4.6444\n",
      "Epoch [1/1], Step [1558/7635], Loss: 4.5945\n",
      "Epoch [1/1], Step [1559/7635], Loss: 4.6318\n",
      "Epoch [1/1], Step [1560/7635], Loss: 4.6922\n",
      "Epoch [1/1], Step [1561/7635], Loss: 4.6197\n",
      "Epoch [1/1], Step [1562/7635], Loss: 4.5917\n",
      "Epoch [1/1], Step [1563/7635], Loss: 4.5584\n",
      "Epoch [1/1], Step [1564/7635], Loss: 4.5482\n",
      "Epoch [1/1], Step [1565/7635], Loss: 4.5890\n",
      "Epoch [1/1], Step [1566/7635], Loss: 4.4957\n",
      "Epoch [1/1], Step [1567/7635], Loss: 4.5925\n",
      "Epoch [1/1], Step [1568/7635], Loss: 4.6297\n",
      "Epoch [1/1], Step [1569/7635], Loss: 4.5821\n",
      "Epoch [1/1], Step [1570/7635], Loss: 4.6927\n",
      "Epoch [1/1], Step [1571/7635], Loss: 4.5290\n",
      "Epoch [1/1], Step [1572/7635], Loss: 4.6464\n",
      "Epoch [1/1], Step [1573/7635], Loss: 4.6106\n",
      "Epoch [1/1], Step [1574/7635], Loss: 4.5328\n",
      "Epoch [1/1], Step [1575/7635], Loss: 4.5437\n",
      "Epoch [1/1], Step [1576/7635], Loss: 4.5445\n",
      "Epoch [1/1], Step [1577/7635], Loss: 4.5756\n",
      "Epoch [1/1], Step [1578/7635], Loss: 4.5876\n",
      "Epoch [1/1], Step [1579/7635], Loss: 4.4349\n",
      "Epoch [1/1], Step [1580/7635], Loss: 4.6352\n",
      "Epoch [1/1], Step [1581/7635], Loss: 4.5618\n",
      "Epoch [1/1], Step [1582/7635], Loss: 4.6778\n",
      "Epoch [1/1], Step [1583/7635], Loss: 4.5635\n",
      "Epoch [1/1], Step [1584/7635], Loss: 4.5708\n",
      "Epoch [1/1], Step [1585/7635], Loss: 4.6333\n",
      "Epoch [1/1], Step [1586/7635], Loss: 4.6118\n",
      "Epoch [1/1], Step [1587/7635], Loss: 4.6083\n",
      "Epoch [1/1], Step [1588/7635], Loss: 4.5416\n",
      "Epoch [1/1], Step [1589/7635], Loss: 4.5938\n",
      "Epoch [1/1], Step [1590/7635], Loss: 4.5165\n",
      "Epoch [1/1], Step [1591/7635], Loss: 4.6543\n",
      "Epoch [1/1], Step [1592/7635], Loss: 4.5551\n",
      "Epoch [1/1], Step [1593/7635], Loss: 4.5476\n",
      "Epoch [1/1], Step [1594/7635], Loss: 4.5850\n",
      "Epoch [1/1], Step [1595/7635], Loss: 4.5611\n",
      "Epoch [1/1], Step [1596/7635], Loss: 4.5228\n",
      "Epoch [1/1], Step [1597/7635], Loss: 4.4895\n",
      "Epoch [1/1], Step [1598/7635], Loss: 4.5154\n",
      "Epoch [1/1], Step [1599/7635], Loss: 4.5107\n",
      "Epoch [1/1], Step [1600/7635], Loss: 4.6887\n",
      "Epoch [1/1], Step [1601/7635], Loss: 4.5254\n",
      "Epoch [1/1], Step [1602/7635], Loss: 4.6446\n",
      "Epoch [1/1], Step [1603/7635], Loss: 4.5887\n",
      "Epoch [1/1], Step [1604/7635], Loss: 4.5460\n",
      "Epoch [1/1], Step [1605/7635], Loss: 4.5568\n",
      "Epoch [1/1], Step [1606/7635], Loss: 4.6253\n",
      "Epoch [1/1], Step [1607/7635], Loss: 4.5360\n",
      "Epoch [1/1], Step [1608/7635], Loss: 4.5788\n",
      "Epoch [1/1], Step [1609/7635], Loss: 4.6472\n",
      "Epoch [1/1], Step [1610/7635], Loss: 4.4913\n",
      "Epoch [1/1], Step [1611/7635], Loss: 4.5576\n",
      "Epoch [1/1], Step [1612/7635], Loss: 4.5129\n",
      "Epoch [1/1], Step [1613/7635], Loss: 4.6203\n",
      "Epoch [1/1], Step [1614/7635], Loss: 4.6126\n",
      "Epoch [1/1], Step [1615/7635], Loss: 4.6638\n",
      "Epoch [1/1], Step [1616/7635], Loss: 4.6464\n",
      "Epoch [1/1], Step [1617/7635], Loss: 4.5616\n",
      "Epoch [1/1], Step [1618/7635], Loss: 4.6389\n",
      "Epoch [1/1], Step [1619/7635], Loss: 4.6244\n",
      "Epoch [1/1], Step [1620/7635], Loss: 4.5446\n",
      "Epoch [1/1], Step [1621/7635], Loss: 4.5538\n",
      "Epoch [1/1], Step [1622/7635], Loss: 4.6467\n",
      "Epoch [1/1], Step [1623/7635], Loss: 4.5256\n",
      "Epoch [1/1], Step [1624/7635], Loss: 4.5701\n",
      "Epoch [1/1], Step [1625/7635], Loss: 4.6477\n",
      "Epoch [1/1], Step [1626/7635], Loss: 4.5526\n",
      "Epoch [1/1], Step [1627/7635], Loss: 4.5603\n",
      "Epoch [1/1], Step [1628/7635], Loss: 4.5283\n",
      "Epoch [1/1], Step [1629/7635], Loss: 4.6046\n",
      "Epoch [1/1], Step [1630/7635], Loss: 4.4552\n",
      "Epoch [1/1], Step [1631/7635], Loss: 4.5354\n",
      "Epoch [1/1], Step [1632/7635], Loss: 4.5149\n",
      "Epoch [1/1], Step [1633/7635], Loss: 4.5631\n",
      "Epoch [1/1], Step [1634/7635], Loss: 4.5258\n",
      "Epoch [1/1], Step [1635/7635], Loss: 4.6292\n",
      "Epoch [1/1], Step [1636/7635], Loss: 4.6178\n",
      "Epoch [1/1], Step [1637/7635], Loss: 4.5702\n",
      "Epoch [1/1], Step [1638/7635], Loss: 4.5960\n",
      "Epoch [1/1], Step [1639/7635], Loss: 4.5865\n",
      "Epoch [1/1], Step [1640/7635], Loss: 4.6065\n",
      "Epoch [1/1], Step [1641/7635], Loss: 4.5644\n",
      "Epoch [1/1], Step [1642/7635], Loss: 4.6732\n",
      "Epoch [1/1], Step [1643/7635], Loss: 4.6019\n",
      "Epoch [1/1], Step [1644/7635], Loss: 4.5890\n",
      "Epoch [1/1], Step [1645/7635], Loss: 4.6153\n",
      "Epoch [1/1], Step [1646/7635], Loss: 4.5582\n",
      "Epoch [1/1], Step [1647/7635], Loss: 4.5942\n",
      "Epoch [1/1], Step [1648/7635], Loss: 4.5698\n",
      "Epoch [1/1], Step [1649/7635], Loss: 4.6146\n",
      "Epoch [1/1], Step [1650/7635], Loss: 4.6522\n",
      "Epoch [1/1], Step [1651/7635], Loss: 4.5191\n",
      "Epoch [1/1], Step [1652/7635], Loss: 4.5893\n",
      "Epoch [1/1], Step [1653/7635], Loss: 4.5656\n",
      "Epoch [1/1], Step [1654/7635], Loss: 4.5923\n",
      "Epoch [1/1], Step [1655/7635], Loss: 4.5515\n",
      "Epoch [1/1], Step [1656/7635], Loss: 4.4866\n",
      "Epoch [1/1], Step [1657/7635], Loss: 4.6434\n",
      "Epoch [1/1], Step [1658/7635], Loss: 4.6733\n",
      "Epoch [1/1], Step [1659/7635], Loss: 4.5540\n",
      "Epoch [1/1], Step [1660/7635], Loss: 4.5790\n",
      "Epoch [1/1], Step [1661/7635], Loss: 4.6158\n",
      "Epoch [1/1], Step [1662/7635], Loss: 4.6549\n",
      "Epoch [1/1], Step [1663/7635], Loss: 4.5712\n",
      "Epoch [1/1], Step [1664/7635], Loss: 4.5103\n",
      "Epoch [1/1], Step [1665/7635], Loss: 4.7192\n",
      "Epoch [1/1], Step [1666/7635], Loss: 4.5773\n",
      "Epoch [1/1], Step [1667/7635], Loss: 4.5954\n",
      "Epoch [1/1], Step [1668/7635], Loss: 4.5868\n",
      "Epoch [1/1], Step [1669/7635], Loss: 4.5472\n",
      "Epoch [1/1], Step [1670/7635], Loss: 4.5951\n",
      "Epoch [1/1], Step [1671/7635], Loss: 4.5844\n",
      "Epoch [1/1], Step [1672/7635], Loss: 4.6265\n",
      "Epoch [1/1], Step [1673/7635], Loss: 4.6402\n",
      "Epoch [1/1], Step [1674/7635], Loss: 4.4998\n",
      "Epoch [1/1], Step [1675/7635], Loss: 4.5670\n",
      "Epoch [1/1], Step [1676/7635], Loss: 4.5755\n",
      "Epoch [1/1], Step [1677/7635], Loss: 4.5154\n",
      "Epoch [1/1], Step [1678/7635], Loss: 4.6287\n",
      "Epoch [1/1], Step [1679/7635], Loss: 4.5836\n",
      "Epoch [1/1], Step [1680/7635], Loss: 4.5894\n",
      "Epoch [1/1], Step [1681/7635], Loss: 4.5226\n",
      "Epoch [1/1], Step [1682/7635], Loss: 4.6156\n",
      "Epoch [1/1], Step [1683/7635], Loss: 4.5430\n",
      "Epoch [1/1], Step [1684/7635], Loss: 4.5319\n",
      "Epoch [1/1], Step [1685/7635], Loss: 4.5513\n",
      "Epoch [1/1], Step [1686/7635], Loss: 4.5607\n",
      "Epoch [1/1], Step [1687/7635], Loss: 4.5579\n",
      "Epoch [1/1], Step [1688/7635], Loss: 4.5015\n",
      "Epoch [1/1], Step [1689/7635], Loss: 4.6014\n",
      "Epoch [1/1], Step [1690/7635], Loss: 4.5628\n",
      "Epoch [1/1], Step [1691/7635], Loss: 4.4484\n",
      "Epoch [1/1], Step [1692/7635], Loss: 4.5574\n",
      "Epoch [1/1], Step [1693/7635], Loss: 4.5390\n",
      "Epoch [1/1], Step [1694/7635], Loss: 4.5893\n",
      "Epoch [1/1], Step [1695/7635], Loss: 4.5520\n",
      "Epoch [1/1], Step [1696/7635], Loss: 4.4811\n",
      "Epoch [1/1], Step [1697/7635], Loss: 4.6303\n",
      "Epoch [1/1], Step [1698/7635], Loss: 4.5466\n",
      "Epoch [1/1], Step [1699/7635], Loss: 4.5960\n",
      "Epoch [1/1], Step [1700/7635], Loss: 4.5396\n",
      "Epoch [1/1], Step [1701/7635], Loss: 4.5854\n",
      "Epoch [1/1], Step [1702/7635], Loss: 4.4794\n",
      "Epoch [1/1], Step [1703/7635], Loss: 4.4724\n",
      "Epoch [1/1], Step [1704/7635], Loss: 4.5376\n",
      "Epoch [1/1], Step [1705/7635], Loss: 4.5326\n",
      "Epoch [1/1], Step [1706/7635], Loss: 4.5945\n",
      "Epoch [1/1], Step [1707/7635], Loss: 4.6050\n",
      "Epoch [1/1], Step [1708/7635], Loss: 4.5593\n",
      "Epoch [1/1], Step [1709/7635], Loss: 4.4919\n",
      "Epoch [1/1], Step [1710/7635], Loss: 4.5545\n",
      "Epoch [1/1], Step [1711/7635], Loss: 4.5141\n",
      "Epoch [1/1], Step [1712/7635], Loss: 4.5375\n",
      "Epoch [1/1], Step [1713/7635], Loss: 4.5823\n",
      "Epoch [1/1], Step [1714/7635], Loss: 4.6026\n",
      "Epoch [1/1], Step [1715/7635], Loss: 4.5938\n",
      "Epoch [1/1], Step [1716/7635], Loss: 4.5598\n",
      "Epoch [1/1], Step [1717/7635], Loss: 4.5922\n",
      "Epoch [1/1], Step [1718/7635], Loss: 4.4565\n",
      "Epoch [1/1], Step [1719/7635], Loss: 4.5241\n",
      "Epoch [1/1], Step [1720/7635], Loss: 4.5676\n",
      "Epoch [1/1], Step [1721/7635], Loss: 4.5132\n",
      "Epoch [1/1], Step [1722/7635], Loss: 4.4639\n",
      "Epoch [1/1], Step [1723/7635], Loss: 4.4942\n",
      "Epoch [1/1], Step [1724/7635], Loss: 4.5857\n",
      "Epoch [1/1], Step [1725/7635], Loss: 4.5229\n",
      "Epoch [1/1], Step [1726/7635], Loss: 4.5579\n",
      "Epoch [1/1], Step [1727/7635], Loss: 4.5808\n",
      "Epoch [1/1], Step [1728/7635], Loss: 4.5764\n",
      "Epoch [1/1], Step [1729/7635], Loss: 4.5840\n",
      "Epoch [1/1], Step [1730/7635], Loss: 4.5532\n",
      "Epoch [1/1], Step [1731/7635], Loss: 4.5777\n",
      "Epoch [1/1], Step [1732/7635], Loss: 4.5432\n",
      "Epoch [1/1], Step [1733/7635], Loss: 4.5011\n",
      "Epoch [1/1], Step [1734/7635], Loss: 4.5693\n",
      "Epoch [1/1], Step [1735/7635], Loss: 4.6103\n",
      "Epoch [1/1], Step [1736/7635], Loss: 4.5638\n",
      "Epoch [1/1], Step [1737/7635], Loss: 4.5491\n",
      "Epoch [1/1], Step [1738/7635], Loss: 4.6284\n",
      "Epoch [1/1], Step [1739/7635], Loss: 4.5395\n",
      "Epoch [1/1], Step [1740/7635], Loss: 4.4714\n",
      "Epoch [1/1], Step [1741/7635], Loss: 4.4953\n",
      "Epoch [1/1], Step [1742/7635], Loss: 4.5501\n",
      "Epoch [1/1], Step [1743/7635], Loss: 4.6608\n",
      "Epoch [1/1], Step [1744/7635], Loss: 4.5596\n",
      "Epoch [1/1], Step [1745/7635], Loss: 4.5267\n",
      "Epoch [1/1], Step [1746/7635], Loss: 4.5148\n",
      "Epoch [1/1], Step [1747/7635], Loss: 4.4876\n",
      "Epoch [1/1], Step [1748/7635], Loss: 4.5939\n",
      "Epoch [1/1], Step [1749/7635], Loss: 4.5184\n",
      "Epoch [1/1], Step [1750/7635], Loss: 4.4973\n",
      "Epoch [1/1], Step [1751/7635], Loss: 4.5551\n",
      "Epoch [1/1], Step [1752/7635], Loss: 4.5253\n",
      "Epoch [1/1], Step [1753/7635], Loss: 4.4595\n",
      "Epoch [1/1], Step [1754/7635], Loss: 4.4894\n",
      "Epoch [1/1], Step [1755/7635], Loss: 4.5310\n",
      "Epoch [1/1], Step [1756/7635], Loss: 4.5218\n",
      "Epoch [1/1], Step [1757/7635], Loss: 4.5063\n",
      "Epoch [1/1], Step [1758/7635], Loss: 4.5509\n",
      "Epoch [1/1], Step [1759/7635], Loss: 4.4984\n",
      "Epoch [1/1], Step [1760/7635], Loss: 4.4405\n",
      "Epoch [1/1], Step [1761/7635], Loss: 4.5539\n",
      "Epoch [1/1], Step [1762/7635], Loss: 4.6337\n",
      "Epoch [1/1], Step [1763/7635], Loss: 4.6889\n",
      "Epoch [1/1], Step [1764/7635], Loss: 4.5604\n",
      "Epoch [1/1], Step [1765/7635], Loss: 4.5169\n",
      "Epoch [1/1], Step [1766/7635], Loss: 4.5735\n",
      "Epoch [1/1], Step [1767/7635], Loss: 4.4702\n",
      "Epoch [1/1], Step [1768/7635], Loss: 4.6078\n",
      "Epoch [1/1], Step [1769/7635], Loss: 4.5726\n",
      "Epoch [1/1], Step [1770/7635], Loss: 4.5147\n",
      "Epoch [1/1], Step [1771/7635], Loss: 4.4530\n",
      "Epoch [1/1], Step [1772/7635], Loss: 4.4731\n",
      "Epoch [1/1], Step [1773/7635], Loss: 4.5761\n",
      "Epoch [1/1], Step [1774/7635], Loss: 4.6235\n",
      "Epoch [1/1], Step [1775/7635], Loss: 4.5377\n",
      "Epoch [1/1], Step [1776/7635], Loss: 4.4527\n",
      "Epoch [1/1], Step [1777/7635], Loss: 4.4475\n",
      "Epoch [1/1], Step [1778/7635], Loss: 4.5414\n",
      "Epoch [1/1], Step [1779/7635], Loss: 4.4415\n",
      "Epoch [1/1], Step [1780/7635], Loss: 4.5045\n",
      "Epoch [1/1], Step [1781/7635], Loss: 4.5105\n",
      "Epoch [1/1], Step [1782/7635], Loss: 4.5123\n",
      "Epoch [1/1], Step [1783/7635], Loss: 4.5685\n",
      "Epoch [1/1], Step [1784/7635], Loss: 4.4802\n",
      "Epoch [1/1], Step [1785/7635], Loss: 4.5390\n",
      "Epoch [1/1], Step [1786/7635], Loss: 4.5980\n",
      "Epoch [1/1], Step [1787/7635], Loss: 4.4931\n",
      "Epoch [1/1], Step [1788/7635], Loss: 4.4328\n",
      "Epoch [1/1], Step [1789/7635], Loss: 4.4172\n",
      "Epoch [1/1], Step [1790/7635], Loss: 4.5482\n",
      "Epoch [1/1], Step [1791/7635], Loss: 4.5284\n",
      "Epoch [1/1], Step [1792/7635], Loss: 4.5803\n",
      "Epoch [1/1], Step [1793/7635], Loss: 4.4920\n",
      "Epoch [1/1], Step [1794/7635], Loss: 4.5244\n",
      "Epoch [1/1], Step [1795/7635], Loss: 4.5821\n",
      "Epoch [1/1], Step [1796/7635], Loss: 4.4260\n",
      "Epoch [1/1], Step [1797/7635], Loss: 4.5120\n",
      "Epoch [1/1], Step [1798/7635], Loss: 4.5680\n",
      "Epoch [1/1], Step [1799/7635], Loss: 4.4439\n",
      "Epoch [1/1], Step [1800/7635], Loss: 4.4859\n",
      "Epoch [1/1], Step [1801/7635], Loss: 4.5406\n",
      "Epoch [1/1], Step [1802/7635], Loss: 4.5363\n",
      "Epoch [1/1], Step [1803/7635], Loss: 4.5619\n",
      "Epoch [1/1], Step [1804/7635], Loss: 4.5525\n",
      "Epoch [1/1], Step [1805/7635], Loss: 4.6274\n",
      "Epoch [1/1], Step [1806/7635], Loss: 4.5117\n",
      "Epoch [1/1], Step [1807/7635], Loss: 4.5179\n",
      "Epoch [1/1], Step [1808/7635], Loss: 4.5687\n",
      "Epoch [1/1], Step [1809/7635], Loss: 4.5686\n",
      "Epoch [1/1], Step [1810/7635], Loss: 4.5344\n",
      "Epoch [1/1], Step [1811/7635], Loss: 4.5413\n",
      "Epoch [1/1], Step [1812/7635], Loss: 4.5210\n",
      "Epoch [1/1], Step [1813/7635], Loss: 4.4937\n",
      "Epoch [1/1], Step [1814/7635], Loss: 4.5332\n",
      "Epoch [1/1], Step [1815/7635], Loss: 4.4888\n",
      "Epoch [1/1], Step [1816/7635], Loss: 4.5511\n",
      "Epoch [1/1], Step [1817/7635], Loss: 4.6110\n",
      "Epoch [1/1], Step [1818/7635], Loss: 4.5274\n",
      "Epoch [1/1], Step [1819/7635], Loss: 4.6057\n",
      "Epoch [1/1], Step [1820/7635], Loss: 4.5614\n",
      "Epoch [1/1], Step [1821/7635], Loss: 4.5118\n",
      "Epoch [1/1], Step [1822/7635], Loss: 4.5465\n",
      "Epoch [1/1], Step [1823/7635], Loss: 4.5989\n",
      "Epoch [1/1], Step [1824/7635], Loss: 4.4904\n",
      "Epoch [1/1], Step [1825/7635], Loss: 4.4835\n",
      "Epoch [1/1], Step [1826/7635], Loss: 4.4882\n",
      "Epoch [1/1], Step [1827/7635], Loss: 4.4885\n",
      "Epoch [1/1], Step [1828/7635], Loss: 4.5131\n",
      "Epoch [1/1], Step [1829/7635], Loss: 4.4886\n",
      "Epoch [1/1], Step [1830/7635], Loss: 4.5683\n",
      "Epoch [1/1], Step [1831/7635], Loss: 4.5966\n",
      "Epoch [1/1], Step [1832/7635], Loss: 4.5815\n",
      "Epoch [1/1], Step [1833/7635], Loss: 4.5928\n",
      "Epoch [1/1], Step [1834/7635], Loss: 4.4743\n",
      "Epoch [1/1], Step [1835/7635], Loss: 4.4847\n",
      "Epoch [1/1], Step [1836/7635], Loss: 4.4689\n",
      "Epoch [1/1], Step [1837/7635], Loss: 4.5536\n",
      "Epoch [1/1], Step [1838/7635], Loss: 4.4664\n",
      "Epoch [1/1], Step [1839/7635], Loss: 4.5601\n",
      "Epoch [1/1], Step [1840/7635], Loss: 4.5037\n",
      "Epoch [1/1], Step [1841/7635], Loss: 4.6012\n",
      "Epoch [1/1], Step [1842/7635], Loss: 4.4651\n",
      "Epoch [1/1], Step [1843/7635], Loss: 4.5140\n",
      "Epoch [1/1], Step [1844/7635], Loss: 4.5294\n",
      "Epoch [1/1], Step [1845/7635], Loss: 4.4936\n",
      "Epoch [1/1], Step [1846/7635], Loss: 4.5016\n",
      "Epoch [1/1], Step [1847/7635], Loss: 4.5564\n",
      "Epoch [1/1], Step [1848/7635], Loss: 4.5549\n",
      "Epoch [1/1], Step [1849/7635], Loss: 4.5393\n",
      "Epoch [1/1], Step [1850/7635], Loss: 4.6505\n",
      "Epoch [1/1], Step [1851/7635], Loss: 4.5890\n",
      "Epoch [1/1], Step [1852/7635], Loss: 4.4435\n",
      "Epoch [1/1], Step [1853/7635], Loss: 4.5326\n",
      "Epoch [1/1], Step [1854/7635], Loss: 4.4812\n",
      "Epoch [1/1], Step [1855/7635], Loss: 4.6084\n",
      "Epoch [1/1], Step [1856/7635], Loss: 4.4801\n",
      "Epoch [1/1], Step [1857/7635], Loss: 4.5653\n",
      "Epoch [1/1], Step [1858/7635], Loss: 4.5722\n",
      "Epoch [1/1], Step [1859/7635], Loss: 4.4406\n",
      "Epoch [1/1], Step [1860/7635], Loss: 4.5512\n",
      "Epoch [1/1], Step [1861/7635], Loss: 4.4827\n",
      "Epoch [1/1], Step [1862/7635], Loss: 4.5747\n",
      "Epoch [1/1], Step [1863/7635], Loss: 4.5487\n",
      "Epoch [1/1], Step [1864/7635], Loss: 4.5446\n",
      "Epoch [1/1], Step [1865/7635], Loss: 4.5979\n",
      "Epoch [1/1], Step [1866/7635], Loss: 4.4829\n",
      "Epoch [1/1], Step [1867/7635], Loss: 4.5897\n",
      "Epoch [1/1], Step [1868/7635], Loss: 4.5008\n",
      "Epoch [1/1], Step [1869/7635], Loss: 4.5223\n",
      "Epoch [1/1], Step [1870/7635], Loss: 4.5796\n",
      "Epoch [1/1], Step [1871/7635], Loss: 4.4626\n",
      "Epoch [1/1], Step [1872/7635], Loss: 4.6187\n",
      "Epoch [1/1], Step [1873/7635], Loss: 4.5047\n",
      "Epoch [1/1], Step [1874/7635], Loss: 4.5238\n",
      "Epoch [1/1], Step [1875/7635], Loss: 4.5221\n",
      "Epoch [1/1], Step [1876/7635], Loss: 4.5336\n",
      "Epoch [1/1], Step [1877/7635], Loss: 4.5303\n",
      "Epoch [1/1], Step [1878/7635], Loss: 4.5502\n",
      "Epoch [1/1], Step [1879/7635], Loss: 4.5220\n",
      "Epoch [1/1], Step [1880/7635], Loss: 4.4788\n",
      "Epoch [1/1], Step [1881/7635], Loss: 4.4964\n",
      "Epoch [1/1], Step [1882/7635], Loss: 4.3558\n",
      "Epoch [1/1], Step [1883/7635], Loss: 4.5686\n",
      "Epoch [1/1], Step [1884/7635], Loss: 4.5817\n",
      "Epoch [1/1], Step [1885/7635], Loss: 4.5317\n",
      "Epoch [1/1], Step [1886/7635], Loss: 4.4302\n",
      "Epoch [1/1], Step [1887/7635], Loss: 4.4326\n",
      "Epoch [1/1], Step [1888/7635], Loss: 4.5539\n",
      "Epoch [1/1], Step [1889/7635], Loss: 4.6054\n",
      "Epoch [1/1], Step [1890/7635], Loss: 4.4049\n",
      "Epoch [1/1], Step [1891/7635], Loss: 4.5567\n",
      "Epoch [1/1], Step [1892/7635], Loss: 4.4314\n",
      "Epoch [1/1], Step [1893/7635], Loss: 4.4606\n",
      "Epoch [1/1], Step [1894/7635], Loss: 4.5247\n",
      "Epoch [1/1], Step [1895/7635], Loss: 4.4993\n",
      "Epoch [1/1], Step [1896/7635], Loss: 4.5293\n",
      "Epoch [1/1], Step [1897/7635], Loss: 4.5349\n",
      "Epoch [1/1], Step [1898/7635], Loss: 4.4547\n",
      "Epoch [1/1], Step [1899/7635], Loss: 4.5281\n",
      "Epoch [1/1], Step [1900/7635], Loss: 4.4583\n",
      "Epoch [1/1], Step [1901/7635], Loss: 4.5285\n",
      "Epoch [1/1], Step [1902/7635], Loss: 4.5537\n",
      "Epoch [1/1], Step [1903/7635], Loss: 4.5396\n",
      "Epoch [1/1], Step [1904/7635], Loss: 4.5780\n",
      "Epoch [1/1], Step [1905/7635], Loss: 4.5685\n",
      "Epoch [1/1], Step [1906/7635], Loss: 4.5455\n",
      "Epoch [1/1], Step [1907/7635], Loss: 4.4851\n",
      "Epoch [1/1], Step [1908/7635], Loss: 4.4897\n",
      "Epoch [1/1], Step [1909/7635], Loss: 4.5159\n",
      "Epoch [1/1], Step [1910/7635], Loss: 4.5568\n",
      "Epoch [1/1], Step [1911/7635], Loss: 4.4494\n",
      "Epoch [1/1], Step [1912/7635], Loss: 4.6122\n",
      "Epoch [1/1], Step [1913/7635], Loss: 4.5168\n",
      "Epoch [1/1], Step [1914/7635], Loss: 4.4486\n",
      "Epoch [1/1], Step [1915/7635], Loss: 4.5369\n",
      "Epoch [1/1], Step [1916/7635], Loss: 4.5148\n",
      "Epoch [1/1], Step [1917/7635], Loss: 4.5713\n",
      "Epoch [1/1], Step [1918/7635], Loss: 4.5287\n",
      "Epoch [1/1], Step [1919/7635], Loss: 4.5596\n",
      "Epoch [1/1], Step [1920/7635], Loss: 4.5760\n",
      "Epoch [1/1], Step [1921/7635], Loss: 4.5032\n",
      "Epoch [1/1], Step [1922/7635], Loss: 4.4467\n",
      "Epoch [1/1], Step [1923/7635], Loss: 4.6030\n",
      "Epoch [1/1], Step [1924/7635], Loss: 4.4885\n",
      "Epoch [1/1], Step [1925/7635], Loss: 4.6206\n",
      "Epoch [1/1], Step [1926/7635], Loss: 4.4724\n",
      "Epoch [1/1], Step [1927/7635], Loss: 4.5116\n",
      "Epoch [1/1], Step [1928/7635], Loss: 4.5180\n",
      "Epoch [1/1], Step [1929/7635], Loss: 4.4973\n",
      "Epoch [1/1], Step [1930/7635], Loss: 4.4092\n",
      "Epoch [1/1], Step [1931/7635], Loss: 4.4440\n",
      "Epoch [1/1], Step [1932/7635], Loss: 4.4711\n",
      "Epoch [1/1], Step [1933/7635], Loss: 4.4666\n",
      "Epoch [1/1], Step [1934/7635], Loss: 4.5395\n",
      "Epoch [1/1], Step [1935/7635], Loss: 4.4106\n",
      "Epoch [1/1], Step [1936/7635], Loss: 4.4920\n",
      "Epoch [1/1], Step [1937/7635], Loss: 4.5222\n",
      "Epoch [1/1], Step [1938/7635], Loss: 4.4454\n",
      "Epoch [1/1], Step [1939/7635], Loss: 4.4933\n",
      "Epoch [1/1], Step [1940/7635], Loss: 4.4601\n",
      "Epoch [1/1], Step [1941/7635], Loss: 4.4521\n",
      "Epoch [1/1], Step [1942/7635], Loss: 4.5962\n",
      "Epoch [1/1], Step [1943/7635], Loss: 4.4885\n",
      "Epoch [1/1], Step [1944/7635], Loss: 4.5289\n",
      "Epoch [1/1], Step [1945/7635], Loss: 4.5264\n",
      "Epoch [1/1], Step [1946/7635], Loss: 4.4762\n",
      "Epoch [1/1], Step [1947/7635], Loss: 4.4311\n",
      "Epoch [1/1], Step [1948/7635], Loss: 4.4951\n",
      "Epoch [1/1], Step [1949/7635], Loss: 4.3760\n",
      "Epoch [1/1], Step [1950/7635], Loss: 4.4972\n",
      "Epoch [1/1], Step [1951/7635], Loss: 4.3890\n",
      "Epoch [1/1], Step [1952/7635], Loss: 4.4639\n",
      "Epoch [1/1], Step [1953/7635], Loss: 4.5530\n",
      "Epoch [1/1], Step [1954/7635], Loss: 4.4763\n",
      "Epoch [1/1], Step [1955/7635], Loss: 4.5252\n",
      "Epoch [1/1], Step [1956/7635], Loss: 4.5040\n",
      "Epoch [1/1], Step [1957/7635], Loss: 4.5040\n",
      "Epoch [1/1], Step [1958/7635], Loss: 4.4714\n",
      "Epoch [1/1], Step [1959/7635], Loss: 4.4582\n",
      "Epoch [1/1], Step [1960/7635], Loss: 4.4408\n",
      "Epoch [1/1], Step [1961/7635], Loss: 4.4353\n",
      "Epoch [1/1], Step [1962/7635], Loss: 4.4963\n",
      "Epoch [1/1], Step [1963/7635], Loss: 4.5024\n",
      "Epoch [1/1], Step [1964/7635], Loss: 4.4591\n",
      "Epoch [1/1], Step [1965/7635], Loss: 4.4748\n",
      "Epoch [1/1], Step [1966/7635], Loss: 4.5138\n",
      "Epoch [1/1], Step [1967/7635], Loss: 4.5086\n",
      "Epoch [1/1], Step [1968/7635], Loss: 4.4530\n",
      "Epoch [1/1], Step [1969/7635], Loss: 4.5493\n",
      "Epoch [1/1], Step [1970/7635], Loss: 4.4635\n",
      "Epoch [1/1], Step [1971/7635], Loss: 4.4510\n",
      "Epoch [1/1], Step [1972/7635], Loss: 4.4977\n",
      "Epoch [1/1], Step [1973/7635], Loss: 4.5477\n",
      "Epoch [1/1], Step [1974/7635], Loss: 4.4715\n",
      "Epoch [1/1], Step [1975/7635], Loss: 4.4884\n",
      "Epoch [1/1], Step [1976/7635], Loss: 4.4842\n",
      "Epoch [1/1], Step [1977/7635], Loss: 4.4928\n",
      "Epoch [1/1], Step [1978/7635], Loss: 4.5393\n",
      "Epoch [1/1], Step [1979/7635], Loss: 4.4527\n",
      "Epoch [1/1], Step [1980/7635], Loss: 4.5909\n",
      "Epoch [1/1], Step [1981/7635], Loss: 4.4449\n",
      "Epoch [1/1], Step [1982/7635], Loss: 4.4929\n",
      "Epoch [1/1], Step [1983/7635], Loss: 4.4006\n",
      "Epoch [1/1], Step [1984/7635], Loss: 4.5311\n",
      "Epoch [1/1], Step [1985/7635], Loss: 4.4784\n",
      "Epoch [1/1], Step [1986/7635], Loss: 4.4315\n",
      "Epoch [1/1], Step [1987/7635], Loss: 4.5300\n",
      "Epoch [1/1], Step [1988/7635], Loss: 4.4173\n",
      "Epoch [1/1], Step [1989/7635], Loss: 4.5482\n",
      "Epoch [1/1], Step [1990/7635], Loss: 4.5899\n",
      "Epoch [1/1], Step [1991/7635], Loss: 4.5425\n",
      "Epoch [1/1], Step [1992/7635], Loss: 4.5680\n",
      "Epoch [1/1], Step [1993/7635], Loss: 4.4693\n",
      "Epoch [1/1], Step [1994/7635], Loss: 4.5641\n",
      "Epoch [1/1], Step [1995/7635], Loss: 4.4078\n",
      "Epoch [1/1], Step [1996/7635], Loss: 4.4656\n",
      "Epoch [1/1], Step [1997/7635], Loss: 4.4444\n",
      "Epoch [1/1], Step [1998/7635], Loss: 4.4580\n",
      "Epoch [1/1], Step [1999/7635], Loss: 4.3707\n",
      "Epoch [1/1], Step [2000/7635], Loss: 4.4733\n",
      "Epoch [1/1], Step [2001/7635], Loss: 4.5042\n",
      "Epoch [1/1], Step [2002/7635], Loss: 4.4458\n",
      "Epoch [1/1], Step [2003/7635], Loss: 4.4581\n",
      "Epoch [1/1], Step [2004/7635], Loss: 4.4173\n",
      "Epoch [1/1], Step [2005/7635], Loss: 4.4096\n",
      "Epoch [1/1], Step [2006/7635], Loss: 4.3766\n",
      "Epoch [1/1], Step [2007/7635], Loss: 4.4540\n",
      "Epoch [1/1], Step [2008/7635], Loss: 4.5058\n",
      "Epoch [1/1], Step [2009/7635], Loss: 4.6271\n",
      "Epoch [1/1], Step [2010/7635], Loss: 4.4379\n",
      "Epoch [1/1], Step [2011/7635], Loss: 4.5322\n",
      "Epoch [1/1], Step [2012/7635], Loss: 4.5457\n",
      "Epoch [1/1], Step [2013/7635], Loss: 4.5197\n",
      "Epoch [1/1], Step [2014/7635], Loss: 4.4644\n",
      "Epoch [1/1], Step [2015/7635], Loss: 4.5429\n",
      "Epoch [1/1], Step [2016/7635], Loss: 4.4582\n",
      "Epoch [1/1], Step [2017/7635], Loss: 4.5558\n",
      "Epoch [1/1], Step [2018/7635], Loss: 4.3668\n",
      "Epoch [1/1], Step [2019/7635], Loss: 4.5191\n",
      "Epoch [1/1], Step [2020/7635], Loss: 4.5024\n",
      "Epoch [1/1], Step [2021/7635], Loss: 4.4627\n",
      "Epoch [1/1], Step [2022/7635], Loss: 4.4447\n",
      "Epoch [1/1], Step [2023/7635], Loss: 4.5472\n",
      "Epoch [1/1], Step [2024/7635], Loss: 4.3555\n",
      "Epoch [1/1], Step [2025/7635], Loss: 4.5225\n",
      "Epoch [1/1], Step [2026/7635], Loss: 4.5034\n",
      "Epoch [1/1], Step [2027/7635], Loss: 4.3353\n",
      "Epoch [1/1], Step [2028/7635], Loss: 4.4681\n",
      "Epoch [1/1], Step [2029/7635], Loss: 4.4496\n",
      "Epoch [1/1], Step [2030/7635], Loss: 4.4898\n",
      "Epoch [1/1], Step [2031/7635], Loss: 4.4957\n",
      "Epoch [1/1], Step [2032/7635], Loss: 4.5182\n",
      "Epoch [1/1], Step [2033/7635], Loss: 4.5120\n",
      "Epoch [1/1], Step [2034/7635], Loss: 4.5621\n",
      "Epoch [1/1], Step [2035/7635], Loss: 4.5180\n",
      "Epoch [1/1], Step [2036/7635], Loss: 4.5709\n",
      "Epoch [1/1], Step [2037/7635], Loss: 4.4852\n",
      "Epoch [1/1], Step [2038/7635], Loss: 4.5358\n",
      "Epoch [1/1], Step [2039/7635], Loss: 4.4904\n",
      "Epoch [1/1], Step [2040/7635], Loss: 4.4870\n",
      "Epoch [1/1], Step [2041/7635], Loss: 4.4941\n",
      "Epoch [1/1], Step [2042/7635], Loss: 4.4531\n",
      "Epoch [1/1], Step [2043/7635], Loss: 4.4352\n",
      "Epoch [1/1], Step [2044/7635], Loss: 4.4521\n",
      "Epoch [1/1], Step [2045/7635], Loss: 4.5370\n",
      "Epoch [1/1], Step [2046/7635], Loss: 4.4218\n",
      "Epoch [1/1], Step [2047/7635], Loss: 4.4672\n",
      "Epoch [1/1], Step [2048/7635], Loss: 4.4840\n",
      "Epoch [1/1], Step [2049/7635], Loss: 4.4058\n",
      "Epoch [1/1], Step [2050/7635], Loss: 4.4566\n",
      "Epoch [1/1], Step [2051/7635], Loss: 4.5573\n",
      "Epoch [1/1], Step [2052/7635], Loss: 4.4816\n",
      "Epoch [1/1], Step [2053/7635], Loss: 4.4285\n",
      "Epoch [1/1], Step [2054/7635], Loss: 4.5499\n",
      "Epoch [1/1], Step [2055/7635], Loss: 4.3864\n",
      "Epoch [1/1], Step [2056/7635], Loss: 4.5036\n",
      "Epoch [1/1], Step [2057/7635], Loss: 4.3502\n",
      "Epoch [1/1], Step [2058/7635], Loss: 4.4210\n",
      "Epoch [1/1], Step [2059/7635], Loss: 4.4648\n",
      "Epoch [1/1], Step [2060/7635], Loss: 4.3963\n",
      "Epoch [1/1], Step [2061/7635], Loss: 4.4333\n",
      "Epoch [1/1], Step [2062/7635], Loss: 4.5943\n",
      "Epoch [1/1], Step [2063/7635], Loss: 4.5680\n",
      "Epoch [1/1], Step [2064/7635], Loss: 4.4651\n",
      "Epoch [1/1], Step [2065/7635], Loss: 4.4824\n",
      "Epoch [1/1], Step [2066/7635], Loss: 4.6716\n",
      "Epoch [1/1], Step [2067/7635], Loss: 4.5196\n",
      "Epoch [1/1], Step [2068/7635], Loss: 4.5228\n",
      "Epoch [1/1], Step [2069/7635], Loss: 4.5329\n",
      "Epoch [1/1], Step [2070/7635], Loss: 4.4837\n",
      "Epoch [1/1], Step [2071/7635], Loss: 4.4522\n",
      "Epoch [1/1], Step [2072/7635], Loss: 4.4768\n",
      "Epoch [1/1], Step [2073/7635], Loss: 4.4546\n",
      "Epoch [1/1], Step [2074/7635], Loss: 4.5936\n",
      "Epoch [1/1], Step [2075/7635], Loss: 4.4257\n",
      "Epoch [1/1], Step [2076/7635], Loss: 4.5442\n",
      "Epoch [1/1], Step [2077/7635], Loss: 4.4771\n",
      "Epoch [1/1], Step [2078/7635], Loss: 4.4916\n",
      "Epoch [1/1], Step [2079/7635], Loss: 4.4382\n",
      "Epoch [1/1], Step [2080/7635], Loss: 4.4401\n",
      "Epoch [1/1], Step [2081/7635], Loss: 4.4730\n",
      "Epoch [1/1], Step [2082/7635], Loss: 4.4634\n",
      "Epoch [1/1], Step [2083/7635], Loss: 4.4054\n",
      "Epoch [1/1], Step [2084/7635], Loss: 4.4460\n",
      "Epoch [1/1], Step [2085/7635], Loss: 4.4443\n",
      "Epoch [1/1], Step [2086/7635], Loss: 4.4591\n",
      "Epoch [1/1], Step [2087/7635], Loss: 4.5608\n",
      "Epoch [1/1], Step [2088/7635], Loss: 4.4621\n",
      "Epoch [1/1], Step [2089/7635], Loss: 4.3655\n",
      "Epoch [1/1], Step [2090/7635], Loss: 4.4471\n",
      "Epoch [1/1], Step [2091/7635], Loss: 4.4361\n",
      "Epoch [1/1], Step [2092/7635], Loss: 4.4040\n",
      "Epoch [1/1], Step [2093/7635], Loss: 4.4633\n",
      "Epoch [1/1], Step [2094/7635], Loss: 4.5338\n",
      "Epoch [1/1], Step [2095/7635], Loss: 4.3800\n",
      "Epoch [1/1], Step [2096/7635], Loss: 4.4940\n",
      "Epoch [1/1], Step [2097/7635], Loss: 4.4346\n",
      "Epoch [1/1], Step [2098/7635], Loss: 4.4396\n",
      "Epoch [1/1], Step [2099/7635], Loss: 4.5038\n",
      "Epoch [1/1], Step [2100/7635], Loss: 4.5291\n",
      "Epoch [1/1], Step [2101/7635], Loss: 4.3059\n",
      "Epoch [1/1], Step [2102/7635], Loss: 4.4375\n",
      "Epoch [1/1], Step [2103/7635], Loss: 4.5157\n",
      "Epoch [1/1], Step [2104/7635], Loss: 4.4432\n",
      "Epoch [1/1], Step [2105/7635], Loss: 4.4607\n",
      "Epoch [1/1], Step [2106/7635], Loss: 4.4150\n",
      "Epoch [1/1], Step [2107/7635], Loss: 4.5349\n",
      "Epoch [1/1], Step [2108/7635], Loss: 4.4652\n",
      "Epoch [1/1], Step [2109/7635], Loss: 4.3771\n",
      "Epoch [1/1], Step [2110/7635], Loss: 4.4968\n",
      "Epoch [1/1], Step [2111/7635], Loss: 4.4515\n",
      "Epoch [1/1], Step [2112/7635], Loss: 4.4850\n",
      "Epoch [1/1], Step [2113/7635], Loss: 4.4057\n",
      "Epoch [1/1], Step [2114/7635], Loss: 4.4549\n",
      "Epoch [1/1], Step [2115/7635], Loss: 4.4067\n",
      "Epoch [1/1], Step [2116/7635], Loss: 4.4112\n",
      "Epoch [1/1], Step [2117/7635], Loss: 4.4320\n",
      "Epoch [1/1], Step [2118/7635], Loss: 4.5279\n",
      "Epoch [1/1], Step [2119/7635], Loss: 4.5239\n",
      "Epoch [1/1], Step [2120/7635], Loss: 4.4949\n",
      "Epoch [1/1], Step [2121/7635], Loss: 4.4430\n",
      "Epoch [1/1], Step [2122/7635], Loss: 4.5842\n",
      "Epoch [1/1], Step [2123/7635], Loss: 4.5195\n",
      "Epoch [1/1], Step [2124/7635], Loss: 4.4845\n",
      "Epoch [1/1], Step [2125/7635], Loss: 4.3993\n",
      "Epoch [1/1], Step [2126/7635], Loss: 4.4427\n",
      "Epoch [1/1], Step [2127/7635], Loss: 4.5415\n",
      "Epoch [1/1], Step [2128/7635], Loss: 4.4139\n",
      "Epoch [1/1], Step [2129/7635], Loss: 4.4230\n",
      "Epoch [1/1], Step [2130/7635], Loss: 4.4396\n",
      "Epoch [1/1], Step [2131/7635], Loss: 4.4325\n",
      "Epoch [1/1], Step [2132/7635], Loss: 4.5467\n",
      "Epoch [1/1], Step [2133/7635], Loss: 4.5331\n",
      "Epoch [1/1], Step [2134/7635], Loss: 4.4476\n",
      "Epoch [1/1], Step [2135/7635], Loss: 4.4574\n",
      "Epoch [1/1], Step [2136/7635], Loss: 4.4790\n",
      "Epoch [1/1], Step [2137/7635], Loss: 4.4896\n",
      "Epoch [1/1], Step [2138/7635], Loss: 4.3779\n",
      "Epoch [1/1], Step [2139/7635], Loss: 4.3993\n",
      "Epoch [1/1], Step [2140/7635], Loss: 4.4729\n",
      "Epoch [1/1], Step [2141/7635], Loss: 4.4947\n",
      "Epoch [1/1], Step [2142/7635], Loss: 4.5040\n",
      "Epoch [1/1], Step [2143/7635], Loss: 4.4433\n",
      "Epoch [1/1], Step [2144/7635], Loss: 4.4961\n",
      "Epoch [1/1], Step [2145/7635], Loss: 4.5417\n",
      "Epoch [1/1], Step [2146/7635], Loss: 4.3946\n",
      "Epoch [1/1], Step [2147/7635], Loss: 4.4959\n",
      "Epoch [1/1], Step [2148/7635], Loss: 4.4626\n",
      "Epoch [1/1], Step [2149/7635], Loss: 4.4534\n",
      "Epoch [1/1], Step [2150/7635], Loss: 4.4329\n",
      "Epoch [1/1], Step [2151/7635], Loss: 4.4856\n",
      "Epoch [1/1], Step [2152/7635], Loss: 4.4156\n",
      "Epoch [1/1], Step [2153/7635], Loss: 4.4895\n",
      "Epoch [1/1], Step [2154/7635], Loss: 4.3968\n",
      "Epoch [1/1], Step [2155/7635], Loss: 4.5044\n",
      "Epoch [1/1], Step [2156/7635], Loss: 4.5477\n",
      "Epoch [1/1], Step [2157/7635], Loss: 4.3936\n",
      "Epoch [1/1], Step [2158/7635], Loss: 4.4636\n",
      "Epoch [1/1], Step [2159/7635], Loss: 4.5739\n",
      "Epoch [1/1], Step [2160/7635], Loss: 4.4260\n",
      "Epoch [1/1], Step [2161/7635], Loss: 4.5533\n",
      "Epoch [1/1], Step [2162/7635], Loss: 4.5697\n",
      "Epoch [1/1], Step [2163/7635], Loss: 4.5162\n",
      "Epoch [1/1], Step [2164/7635], Loss: 4.4110\n",
      "Epoch [1/1], Step [2165/7635], Loss: 4.4923\n",
      "Epoch [1/1], Step [2166/7635], Loss: 4.5427\n",
      "Epoch [1/1], Step [2167/7635], Loss: 4.4745\n",
      "Epoch [1/1], Step [2168/7635], Loss: 4.4043\n",
      "Epoch [1/1], Step [2169/7635], Loss: 4.4452\n",
      "Epoch [1/1], Step [2170/7635], Loss: 4.3564\n",
      "Epoch [1/1], Step [2171/7635], Loss: 4.4550\n",
      "Epoch [1/1], Step [2172/7635], Loss: 4.4619\n",
      "Epoch [1/1], Step [2173/7635], Loss: 4.3986\n",
      "Epoch [1/1], Step [2174/7635], Loss: 4.4762\n",
      "Epoch [1/1], Step [2175/7635], Loss: 4.3961\n",
      "Epoch [1/1], Step [2176/7635], Loss: 4.4157\n",
      "Epoch [1/1], Step [2177/7635], Loss: 4.5179\n",
      "Epoch [1/1], Step [2178/7635], Loss: 4.4713\n",
      "Epoch [1/1], Step [2179/7635], Loss: 4.4070\n",
      "Epoch [1/1], Step [2180/7635], Loss: 4.4347\n",
      "Epoch [1/1], Step [2181/7635], Loss: 4.4912\n",
      "Epoch [1/1], Step [2182/7635], Loss: 4.4573\n",
      "Epoch [1/1], Step [2183/7635], Loss: 4.4272\n",
      "Epoch [1/1], Step [2184/7635], Loss: 4.3668\n",
      "Epoch [1/1], Step [2185/7635], Loss: 4.5139\n",
      "Epoch [1/1], Step [2186/7635], Loss: 4.4906\n",
      "Epoch [1/1], Step [2187/7635], Loss: 4.5034\n",
      "Epoch [1/1], Step [2188/7635], Loss: 4.4903\n",
      "Epoch [1/1], Step [2189/7635], Loss: 4.4275\n",
      "Epoch [1/1], Step [2190/7635], Loss: 4.4151\n",
      "Epoch [1/1], Step [2191/7635], Loss: 4.4432\n",
      "Epoch [1/1], Step [2192/7635], Loss: 4.5119\n",
      "Epoch [1/1], Step [2193/7635], Loss: 4.3500\n",
      "Epoch [1/1], Step [2194/7635], Loss: 4.4956\n",
      "Epoch [1/1], Step [2195/7635], Loss: 4.4467\n",
      "Epoch [1/1], Step [2196/7635], Loss: 4.4064\n",
      "Epoch [1/1], Step [2197/7635], Loss: 4.5307\n",
      "Epoch [1/1], Step [2198/7635], Loss: 4.4892\n",
      "Epoch [1/1], Step [2199/7635], Loss: 4.3822\n",
      "Epoch [1/1], Step [2200/7635], Loss: 4.5475\n",
      "Epoch [1/1], Step [2201/7635], Loss: 4.4440\n",
      "Epoch [1/1], Step [2202/7635], Loss: 4.4243\n",
      "Epoch [1/1], Step [2203/7635], Loss: 4.3847\n",
      "Epoch [1/1], Step [2204/7635], Loss: 4.4186\n",
      "Epoch [1/1], Step [2205/7635], Loss: 4.4142\n",
      "Epoch [1/1], Step [2206/7635], Loss: 4.4310\n",
      "Epoch [1/1], Step [2207/7635], Loss: 4.4576\n",
      "Epoch [1/1], Step [2208/7635], Loss: 4.4934\n",
      "Epoch [1/1], Step [2209/7635], Loss: 4.4644\n",
      "Epoch [1/1], Step [2210/7635], Loss: 4.4389\n",
      "Epoch [1/1], Step [2211/7635], Loss: 4.4940\n",
      "Epoch [1/1], Step [2212/7635], Loss: 4.5110\n",
      "Epoch [1/1], Step [2213/7635], Loss: 4.4538\n",
      "Epoch [1/1], Step [2214/7635], Loss: 4.4730\n",
      "Epoch [1/1], Step [2215/7635], Loss: 4.4094\n",
      "Epoch [1/1], Step [2216/7635], Loss: 4.4431\n",
      "Epoch [1/1], Step [2217/7635], Loss: 4.4784\n",
      "Epoch [1/1], Step [2218/7635], Loss: 4.4443\n",
      "Epoch [1/1], Step [2219/7635], Loss: 4.4825\n",
      "Epoch [1/1], Step [2220/7635], Loss: 4.4264\n",
      "Epoch [1/1], Step [2221/7635], Loss: 4.3777\n",
      "Epoch [1/1], Step [2222/7635], Loss: 4.4727\n",
      "Epoch [1/1], Step [2223/7635], Loss: 4.4087\n",
      "Epoch [1/1], Step [2224/7635], Loss: 4.4332\n",
      "Epoch [1/1], Step [2225/7635], Loss: 4.4480\n",
      "Epoch [1/1], Step [2226/7635], Loss: 4.4710\n",
      "Epoch [1/1], Step [2227/7635], Loss: 4.5061\n",
      "Epoch [1/1], Step [2228/7635], Loss: 4.4749\n",
      "Epoch [1/1], Step [2229/7635], Loss: 4.4490\n",
      "Epoch [1/1], Step [2230/7635], Loss: 4.4462\n",
      "Epoch [1/1], Step [2231/7635], Loss: 4.4680\n",
      "Epoch [1/1], Step [2232/7635], Loss: 4.4427\n",
      "Epoch [1/1], Step [2233/7635], Loss: 4.4796\n",
      "Epoch [1/1], Step [2234/7635], Loss: 4.5477\n",
      "Epoch [1/1], Step [2235/7635], Loss: 4.5223\n",
      "Epoch [1/1], Step [2236/7635], Loss: 4.4490\n",
      "Epoch [1/1], Step [2237/7635], Loss: 4.4174\n",
      "Epoch [1/1], Step [2238/7635], Loss: 4.3752\n",
      "Epoch [1/1], Step [2239/7635], Loss: 4.3877\n",
      "Epoch [1/1], Step [2240/7635], Loss: 4.4357\n",
      "Epoch [1/1], Step [2241/7635], Loss: 4.4351\n",
      "Epoch [1/1], Step [2242/7635], Loss: 4.3791\n",
      "Epoch [1/1], Step [2243/7635], Loss: 4.5161\n",
      "Epoch [1/1], Step [2244/7635], Loss: 4.4998\n",
      "Epoch [1/1], Step [2245/7635], Loss: 4.4644\n",
      "Epoch [1/1], Step [2246/7635], Loss: 4.3597\n",
      "Epoch [1/1], Step [2247/7635], Loss: 4.4629\n",
      "Epoch [1/1], Step [2248/7635], Loss: 4.5292\n",
      "Epoch [1/1], Step [2249/7635], Loss: 4.4384\n",
      "Epoch [1/1], Step [2250/7635], Loss: 4.4070\n",
      "Epoch [1/1], Step [2251/7635], Loss: 4.4453\n",
      "Epoch [1/1], Step [2252/7635], Loss: 4.3913\n",
      "Epoch [1/1], Step [2253/7635], Loss: 4.4357\n",
      "Epoch [1/1], Step [2254/7635], Loss: 4.4367\n",
      "Epoch [1/1], Step [2255/7635], Loss: 4.5077\n",
      "Epoch [1/1], Step [2256/7635], Loss: 4.4635\n",
      "Epoch [1/1], Step [2257/7635], Loss: 4.4389\n",
      "Epoch [1/1], Step [2258/7635], Loss: 4.3518\n",
      "Epoch [1/1], Step [2259/7635], Loss: 4.4469\n",
      "Epoch [1/1], Step [2260/7635], Loss: 4.4427\n",
      "Epoch [1/1], Step [2261/7635], Loss: 4.4373\n",
      "Epoch [1/1], Step [2262/7635], Loss: 4.4713\n",
      "Epoch [1/1], Step [2263/7635], Loss: 4.4092\n",
      "Epoch [1/1], Step [2264/7635], Loss: 4.3795\n",
      "Epoch [1/1], Step [2265/7635], Loss: 4.3982\n",
      "Epoch [1/1], Step [2266/7635], Loss: 4.4927\n",
      "Epoch [1/1], Step [2267/7635], Loss: 4.4031\n",
      "Epoch [1/1], Step [2268/7635], Loss: 4.4592\n",
      "Epoch [1/1], Step [2269/7635], Loss: 4.5364\n",
      "Epoch [1/1], Step [2270/7635], Loss: 4.3267\n",
      "Epoch [1/1], Step [2271/7635], Loss: 4.4584\n",
      "Epoch [1/1], Step [2272/7635], Loss: 4.5182\n",
      "Epoch [1/1], Step [2273/7635], Loss: 4.3508\n",
      "Epoch [1/1], Step [2274/7635], Loss: 4.3970\n",
      "Epoch [1/1], Step [2275/7635], Loss: 4.3966\n",
      "Epoch [1/1], Step [2276/7635], Loss: 4.4294\n",
      "Epoch [1/1], Step [2277/7635], Loss: 4.4985\n",
      "Epoch [1/1], Step [2278/7635], Loss: 4.4614\n",
      "Epoch [1/1], Step [2279/7635], Loss: 4.3882\n",
      "Epoch [1/1], Step [2280/7635], Loss: 4.4529\n",
      "Epoch [1/1], Step [2281/7635], Loss: 4.3678\n",
      "Epoch [1/1], Step [2282/7635], Loss: 4.4858\n",
      "Epoch [1/1], Step [2283/7635], Loss: 4.4279\n",
      "Epoch [1/1], Step [2284/7635], Loss: 4.4157\n",
      "Epoch [1/1], Step [2285/7635], Loss: 4.4158\n",
      "Epoch [1/1], Step [2286/7635], Loss: 4.4518\n",
      "Epoch [1/1], Step [2287/7635], Loss: 4.4824\n",
      "Epoch [1/1], Step [2288/7635], Loss: 4.5162\n",
      "Epoch [1/1], Step [2289/7635], Loss: 4.5571\n",
      "Epoch [1/1], Step [2290/7635], Loss: 4.4149\n",
      "Epoch [1/1], Step [2291/7635], Loss: 4.3124\n",
      "Epoch [1/1], Step [2292/7635], Loss: 4.4754\n",
      "Epoch [1/1], Step [2293/7635], Loss: 4.4369\n",
      "Epoch [1/1], Step [2294/7635], Loss: 4.3818\n",
      "Epoch [1/1], Step [2295/7635], Loss: 4.4423\n",
      "Epoch [1/1], Step [2296/7635], Loss: 4.4424\n",
      "Epoch [1/1], Step [2297/7635], Loss: 4.3662\n",
      "Epoch [1/1], Step [2298/7635], Loss: 4.3984\n",
      "Epoch [1/1], Step [2299/7635], Loss: 4.3317\n",
      "Epoch [1/1], Step [2300/7635], Loss: 4.4437\n",
      "Epoch [1/1], Step [2301/7635], Loss: 4.4432\n",
      "Epoch [1/1], Step [2302/7635], Loss: 4.4595\n",
      "Epoch [1/1], Step [2303/7635], Loss: 4.5163\n",
      "Epoch [1/1], Step [2304/7635], Loss: 4.4324\n",
      "Epoch [1/1], Step [2305/7635], Loss: 4.4780\n",
      "Epoch [1/1], Step [2306/7635], Loss: 4.3170\n",
      "Epoch [1/1], Step [2307/7635], Loss: 4.4645\n",
      "Epoch [1/1], Step [2308/7635], Loss: 4.3436\n",
      "Epoch [1/1], Step [2309/7635], Loss: 4.5126\n",
      "Epoch [1/1], Step [2310/7635], Loss: 4.4898\n",
      "Epoch [1/1], Step [2311/7635], Loss: 4.4590\n",
      "Epoch [1/1], Step [2312/7635], Loss: 4.4511\n",
      "Epoch [1/1], Step [2313/7635], Loss: 4.4127\n",
      "Epoch [1/1], Step [2314/7635], Loss: 4.4517\n",
      "Epoch [1/1], Step [2315/7635], Loss: 4.3823\n",
      "Epoch [1/1], Step [2316/7635], Loss: 4.4809\n",
      "Epoch [1/1], Step [2317/7635], Loss: 4.4248\n",
      "Epoch [1/1], Step [2318/7635], Loss: 4.4056\n",
      "Epoch [1/1], Step [2319/7635], Loss: 4.4227\n",
      "Epoch [1/1], Step [2320/7635], Loss: 4.4779\n",
      "Epoch [1/1], Step [2321/7635], Loss: 4.4192\n",
      "Epoch [1/1], Step [2322/7635], Loss: 4.4641\n",
      "Epoch [1/1], Step [2323/7635], Loss: 4.3771\n",
      "Epoch [1/1], Step [2324/7635], Loss: 4.4250\n",
      "Epoch [1/1], Step [2325/7635], Loss: 4.5056\n",
      "Epoch [1/1], Step [2326/7635], Loss: 4.4889\n",
      "Epoch [1/1], Step [2327/7635], Loss: 4.4151\n",
      "Epoch [1/1], Step [2328/7635], Loss: 4.4353\n",
      "Epoch [1/1], Step [2329/7635], Loss: 4.4393\n",
      "Epoch [1/1], Step [2330/7635], Loss: 4.4172\n",
      "Epoch [1/1], Step [2331/7635], Loss: 4.3430\n",
      "Epoch [1/1], Step [2332/7635], Loss: 4.3953\n",
      "Epoch [1/1], Step [2333/7635], Loss: 4.4252\n",
      "Epoch [1/1], Step [2334/7635], Loss: 4.4533\n",
      "Epoch [1/1], Step [2335/7635], Loss: 4.4475\n",
      "Epoch [1/1], Step [2336/7635], Loss: 4.4770\n",
      "Epoch [1/1], Step [2337/7635], Loss: 4.5027\n",
      "Epoch [1/1], Step [2338/7635], Loss: 4.3765\n",
      "Epoch [1/1], Step [2339/7635], Loss: 4.3760\n",
      "Epoch [1/1], Step [2340/7635], Loss: 4.5073\n",
      "Epoch [1/1], Step [2341/7635], Loss: 4.4939\n",
      "Epoch [1/1], Step [2342/7635], Loss: 4.5337\n",
      "Epoch [1/1], Step [2343/7635], Loss: 4.4598\n",
      "Epoch [1/1], Step [2344/7635], Loss: 4.4065\n",
      "Epoch [1/1], Step [2345/7635], Loss: 4.4124\n",
      "Epoch [1/1], Step [2346/7635], Loss: 4.4529\n",
      "Epoch [1/1], Step [2347/7635], Loss: 4.4003\n",
      "Epoch [1/1], Step [2348/7635], Loss: 4.3711\n",
      "Epoch [1/1], Step [2349/7635], Loss: 4.3683\n",
      "Epoch [1/1], Step [2350/7635], Loss: 4.4060\n",
      "Epoch [1/1], Step [2351/7635], Loss: 4.3062\n",
      "Epoch [1/1], Step [2352/7635], Loss: 4.4436\n",
      "Epoch [1/1], Step [2353/7635], Loss: 4.4428\n",
      "Epoch [1/1], Step [2354/7635], Loss: 4.3501\n",
      "Epoch [1/1], Step [2355/7635], Loss: 4.3798\n",
      "Epoch [1/1], Step [2356/7635], Loss: 4.5009\n",
      "Epoch [1/1], Step [2357/7635], Loss: 4.4345\n",
      "Epoch [1/1], Step [2358/7635], Loss: 4.4346\n",
      "Epoch [1/1], Step [2359/7635], Loss: 4.4421\n",
      "Epoch [1/1], Step [2360/7635], Loss: 4.3548\n",
      "Epoch [1/1], Step [2361/7635], Loss: 4.4620\n",
      "Epoch [1/1], Step [2362/7635], Loss: 4.4144\n",
      "Epoch [1/1], Step [2363/7635], Loss: 4.4399\n",
      "Epoch [1/1], Step [2364/7635], Loss: 4.4258\n",
      "Epoch [1/1], Step [2365/7635], Loss: 4.3686\n",
      "Epoch [1/1], Step [2366/7635], Loss: 4.4455\n",
      "Epoch [1/1], Step [2367/7635], Loss: 4.3817\n",
      "Epoch [1/1], Step [2368/7635], Loss: 4.4090\n",
      "Epoch [1/1], Step [2369/7635], Loss: 4.4227\n",
      "Epoch [1/1], Step [2370/7635], Loss: 4.3855\n",
      "Epoch [1/1], Step [2371/7635], Loss: 4.3750\n",
      "Epoch [1/1], Step [2372/7635], Loss: 4.4312\n",
      "Epoch [1/1], Step [2373/7635], Loss: 4.4256\n",
      "Epoch [1/1], Step [2374/7635], Loss: 4.4578\n",
      "Epoch [1/1], Step [2375/7635], Loss: 4.4595\n",
      "Epoch [1/1], Step [2376/7635], Loss: 4.3645\n",
      "Epoch [1/1], Step [2377/7635], Loss: 4.4318\n",
      "Epoch [1/1], Step [2378/7635], Loss: 4.4910\n",
      "Epoch [1/1], Step [2379/7635], Loss: 4.4217\n",
      "Epoch [1/1], Step [2380/7635], Loss: 4.4682\n",
      "Epoch [1/1], Step [2381/7635], Loss: 4.3679\n",
      "Epoch [1/1], Step [2382/7635], Loss: 4.3990\n",
      "Epoch [1/1], Step [2383/7635], Loss: 4.3713\n",
      "Epoch [1/1], Step [2384/7635], Loss: 4.2462\n",
      "Epoch [1/1], Step [2385/7635], Loss: 4.3897\n",
      "Epoch [1/1], Step [2386/7635], Loss: 4.4401\n",
      "Epoch [1/1], Step [2387/7635], Loss: 4.4053\n",
      "Epoch [1/1], Step [2388/7635], Loss: 4.4335\n",
      "Epoch [1/1], Step [2389/7635], Loss: 4.4563\n",
      "Epoch [1/1], Step [2390/7635], Loss: 4.3653\n",
      "Epoch [1/1], Step [2391/7635], Loss: 4.4266\n",
      "Epoch [1/1], Step [2392/7635], Loss: 4.4880\n",
      "Epoch [1/1], Step [2393/7635], Loss: 4.4165\n",
      "Epoch [1/1], Step [2394/7635], Loss: 4.3771\n",
      "Epoch [1/1], Step [2395/7635], Loss: 4.4062\n",
      "Epoch [1/1], Step [2396/7635], Loss: 4.4588\n",
      "Epoch [1/1], Step [2397/7635], Loss: 4.4291\n",
      "Epoch [1/1], Step [2398/7635], Loss: 4.4450\n",
      "Epoch [1/1], Step [2399/7635], Loss: 4.3232\n",
      "Epoch [1/1], Step [2400/7635], Loss: 4.4071\n",
      "Epoch [1/1], Step [2401/7635], Loss: 4.4017\n",
      "Epoch [1/1], Step [2402/7635], Loss: 4.4569\n",
      "Epoch [1/1], Step [2403/7635], Loss: 4.3833\n",
      "Epoch [1/1], Step [2404/7635], Loss: 4.4344\n",
      "Epoch [1/1], Step [2405/7635], Loss: 4.4085\n",
      "Epoch [1/1], Step [2406/7635], Loss: 4.4213\n",
      "Epoch [1/1], Step [2407/7635], Loss: 4.3683\n",
      "Epoch [1/1], Step [2408/7635], Loss: 4.3229\n",
      "Epoch [1/1], Step [2409/7635], Loss: 4.4733\n",
      "Epoch [1/1], Step [2410/7635], Loss: 4.5042\n",
      "Epoch [1/1], Step [2411/7635], Loss: 4.4355\n",
      "Epoch [1/1], Step [2412/7635], Loss: 4.4810\n",
      "Epoch [1/1], Step [2413/7635], Loss: 4.3711\n",
      "Epoch [1/1], Step [2414/7635], Loss: 4.4362\n",
      "Epoch [1/1], Step [2415/7635], Loss: 4.4889\n",
      "Epoch [1/1], Step [2416/7635], Loss: 4.4796\n",
      "Epoch [1/1], Step [2417/7635], Loss: 4.3893\n",
      "Epoch [1/1], Step [2418/7635], Loss: 4.4065\n",
      "Epoch [1/1], Step [2419/7635], Loss: 4.3433\n",
      "Epoch [1/1], Step [2420/7635], Loss: 4.5131\n",
      "Epoch [1/1], Step [2421/7635], Loss: 4.3789\n",
      "Epoch [1/1], Step [2422/7635], Loss: 4.3852\n",
      "Epoch [1/1], Step [2423/7635], Loss: 4.3837\n",
      "Epoch [1/1], Step [2424/7635], Loss: 4.4783\n",
      "Epoch [1/1], Step [2425/7635], Loss: 4.3849\n",
      "Epoch [1/1], Step [2426/7635], Loss: 4.3956\n",
      "Epoch [1/1], Step [2427/7635], Loss: 4.4536\n",
      "Epoch [1/1], Step [2428/7635], Loss: 4.3587\n",
      "Epoch [1/1], Step [2429/7635], Loss: 4.3766\n",
      "Epoch [1/1], Step [2430/7635], Loss: 4.4418\n",
      "Epoch [1/1], Step [2431/7635], Loss: 4.4565\n",
      "Epoch [1/1], Step [2432/7635], Loss: 4.3750\n",
      "Epoch [1/1], Step [2433/7635], Loss: 4.4186\n",
      "Epoch [1/1], Step [2434/7635], Loss: 4.3531\n",
      "Epoch [1/1], Step [2435/7635], Loss: 4.4058\n",
      "Epoch [1/1], Step [2436/7635], Loss: 4.3392\n",
      "Epoch [1/1], Step [2437/7635], Loss: 4.3271\n",
      "Epoch [1/1], Step [2438/7635], Loss: 4.4223\n",
      "Epoch [1/1], Step [2439/7635], Loss: 4.4214\n",
      "Epoch [1/1], Step [2440/7635], Loss: 4.4353\n",
      "Epoch [1/1], Step [2441/7635], Loss: 4.3616\n",
      "Epoch [1/1], Step [2442/7635], Loss: 4.4516\n",
      "Epoch [1/1], Step [2443/7635], Loss: 4.4815\n",
      "Epoch [1/1], Step [2444/7635], Loss: 4.3762\n",
      "Epoch [1/1], Step [2445/7635], Loss: 4.3391\n",
      "Epoch [1/1], Step [2446/7635], Loss: 4.2666\n",
      "Epoch [1/1], Step [2447/7635], Loss: 4.4326\n",
      "Epoch [1/1], Step [2448/7635], Loss: 4.4264\n",
      "Epoch [1/1], Step [2449/7635], Loss: 4.3796\n",
      "Epoch [1/1], Step [2450/7635], Loss: 4.3739\n",
      "Epoch [1/1], Step [2451/7635], Loss: 4.3235\n",
      "Epoch [1/1], Step [2452/7635], Loss: 4.4270\n",
      "Epoch [1/1], Step [2453/7635], Loss: 4.4516\n",
      "Epoch [1/1], Step [2454/7635], Loss: 4.3818\n",
      "Epoch [1/1], Step [2455/7635], Loss: 4.4648\n",
      "Epoch [1/1], Step [2456/7635], Loss: 4.3917\n",
      "Epoch [1/1], Step [2457/7635], Loss: 4.4087\n",
      "Epoch [1/1], Step [2458/7635], Loss: 4.4432\n",
      "Epoch [1/1], Step [2459/7635], Loss: 4.3536\n",
      "Epoch [1/1], Step [2460/7635], Loss: 4.3359\n",
      "Epoch [1/1], Step [2461/7635], Loss: 4.4397\n",
      "Epoch [1/1], Step [2462/7635], Loss: 4.5233\n",
      "Epoch [1/1], Step [2463/7635], Loss: 4.4455\n",
      "Epoch [1/1], Step [2464/7635], Loss: 4.3584\n",
      "Epoch [1/1], Step [2465/7635], Loss: 4.3880\n",
      "Epoch [1/1], Step [2466/7635], Loss: 4.4022\n",
      "Epoch [1/1], Step [2467/7635], Loss: 4.4710\n",
      "Epoch [1/1], Step [2468/7635], Loss: 4.3630\n",
      "Epoch [1/1], Step [2469/7635], Loss: 4.4725\n",
      "Epoch [1/1], Step [2470/7635], Loss: 4.4155\n",
      "Epoch [1/1], Step [2471/7635], Loss: 4.4919\n",
      "Epoch [1/1], Step [2472/7635], Loss: 4.3912\n",
      "Epoch [1/1], Step [2473/7635], Loss: 4.3842\n",
      "Epoch [1/1], Step [2474/7635], Loss: 4.3260\n",
      "Epoch [1/1], Step [2475/7635], Loss: 4.3998\n",
      "Epoch [1/1], Step [2476/7635], Loss: 4.4485\n",
      "Epoch [1/1], Step [2477/7635], Loss: 4.5001\n",
      "Epoch [1/1], Step [2478/7635], Loss: 4.3798\n",
      "Epoch [1/1], Step [2479/7635], Loss: 4.5254\n",
      "Epoch [1/1], Step [2480/7635], Loss: 4.4038\n",
      "Epoch [1/1], Step [2481/7635], Loss: 4.3568\n",
      "Epoch [1/1], Step [2482/7635], Loss: 4.3865\n",
      "Epoch [1/1], Step [2483/7635], Loss: 4.4314\n",
      "Epoch [1/1], Step [2484/7635], Loss: 4.3440\n",
      "Epoch [1/1], Step [2485/7635], Loss: 4.3178\n",
      "Epoch [1/1], Step [2486/7635], Loss: 4.4907\n",
      "Epoch [1/1], Step [2487/7635], Loss: 4.3966\n",
      "Epoch [1/1], Step [2488/7635], Loss: 4.3528\n",
      "Epoch [1/1], Step [2489/7635], Loss: 4.4068\n",
      "Epoch [1/1], Step [2490/7635], Loss: 4.4119\n",
      "Epoch [1/1], Step [2491/7635], Loss: 4.4123\n",
      "Epoch [1/1], Step [2492/7635], Loss: 4.3283\n",
      "Epoch [1/1], Step [2493/7635], Loss: 4.3184\n",
      "Epoch [1/1], Step [2494/7635], Loss: 4.4210\n",
      "Epoch [1/1], Step [2495/7635], Loss: 4.3656\n",
      "Epoch [1/1], Step [2496/7635], Loss: 4.4702\n",
      "Epoch [1/1], Step [2497/7635], Loss: 4.3503\n",
      "Epoch [1/1], Step [2498/7635], Loss: 4.3683\n",
      "Epoch [1/1], Step [2499/7635], Loss: 4.3589\n",
      "Epoch [1/1], Step [2500/7635], Loss: 4.4766\n",
      "Epoch [1/1], Step [2501/7635], Loss: 4.3893\n",
      "Epoch [1/1], Step [2502/7635], Loss: 4.4790\n",
      "Epoch [1/1], Step [2503/7635], Loss: 4.3921\n",
      "Epoch [1/1], Step [2504/7635], Loss: 4.4243\n",
      "Epoch [1/1], Step [2505/7635], Loss: 4.3452\n",
      "Epoch [1/1], Step [2506/7635], Loss: 4.4243\n",
      "Epoch [1/1], Step [2507/7635], Loss: 4.3640\n",
      "Epoch [1/1], Step [2508/7635], Loss: 4.4195\n",
      "Epoch [1/1], Step [2509/7635], Loss: 4.3816\n",
      "Epoch [1/1], Step [2510/7635], Loss: 4.3424\n",
      "Epoch [1/1], Step [2511/7635], Loss: 4.3443\n",
      "Epoch [1/1], Step [2512/7635], Loss: 4.3946\n",
      "Epoch [1/1], Step [2513/7635], Loss: 4.4604\n",
      "Epoch [1/1], Step [2514/7635], Loss: 4.4417\n",
      "Epoch [1/1], Step [2515/7635], Loss: 4.3130\n",
      "Epoch [1/1], Step [2516/7635], Loss: 4.3626\n",
      "Epoch [1/1], Step [2517/7635], Loss: 4.3721\n",
      "Epoch [1/1], Step [2518/7635], Loss: 4.4364\n",
      "Epoch [1/1], Step [2519/7635], Loss: 4.3084\n",
      "Epoch [1/1], Step [2520/7635], Loss: 4.5599\n",
      "Epoch [1/1], Step [2521/7635], Loss: 4.4101\n",
      "Epoch [1/1], Step [2522/7635], Loss: 4.4396\n",
      "Epoch [1/1], Step [2523/7635], Loss: 4.3558\n",
      "Epoch [1/1], Step [2524/7635], Loss: 4.4577\n",
      "Epoch [1/1], Step [2525/7635], Loss: 4.3813\n",
      "Epoch [1/1], Step [2526/7635], Loss: 4.4182\n",
      "Epoch [1/1], Step [2527/7635], Loss: 4.4605\n",
      "Epoch [1/1], Step [2528/7635], Loss: 4.4797\n",
      "Epoch [1/1], Step [2529/7635], Loss: 4.3549\n",
      "Epoch [1/1], Step [2530/7635], Loss: 4.3804\n",
      "Epoch [1/1], Step [2531/7635], Loss: 4.4011\n",
      "Epoch [1/1], Step [2532/7635], Loss: 4.4809\n",
      "Epoch [1/1], Step [2533/7635], Loss: 4.3543\n",
      "Epoch [1/1], Step [2534/7635], Loss: 4.2770\n",
      "Epoch [1/1], Step [2535/7635], Loss: 4.4418\n",
      "Epoch [1/1], Step [2536/7635], Loss: 4.4999\n",
      "Epoch [1/1], Step [2537/7635], Loss: 4.3990\n",
      "Epoch [1/1], Step [2538/7635], Loss: 4.3128\n",
      "Epoch [1/1], Step [2539/7635], Loss: 4.4534\n",
      "Epoch [1/1], Step [2540/7635], Loss: 4.4545\n",
      "Epoch [1/1], Step [2541/7635], Loss: 4.3843\n",
      "Epoch [1/1], Step [2542/7635], Loss: 4.3437\n",
      "Epoch [1/1], Step [2543/7635], Loss: 4.2722\n",
      "Epoch [1/1], Step [2544/7635], Loss: 4.3785\n",
      "Epoch [1/1], Step [2545/7635], Loss: 4.4059\n",
      "Epoch [1/1], Step [2546/7635], Loss: 4.3345\n",
      "Epoch [1/1], Step [2547/7635], Loss: 4.4142\n",
      "Epoch [1/1], Step [2548/7635], Loss: 4.3644\n",
      "Epoch [1/1], Step [2549/7635], Loss: 4.4177\n",
      "Epoch [1/1], Step [2550/7635], Loss: 4.3535\n",
      "Epoch [1/1], Step [2551/7635], Loss: 4.3683\n",
      "Epoch [1/1], Step [2552/7635], Loss: 4.3581\n",
      "Epoch [1/1], Step [2553/7635], Loss: 4.3947\n",
      "Epoch [1/1], Step [2554/7635], Loss: 4.4420\n",
      "Epoch [1/1], Step [2555/7635], Loss: 4.4059\n",
      "Epoch [1/1], Step [2556/7635], Loss: 4.3925\n",
      "Epoch [1/1], Step [2557/7635], Loss: 4.4458\n",
      "Epoch [1/1], Step [2558/7635], Loss: 4.3014\n",
      "Epoch [1/1], Step [2559/7635], Loss: 4.4176\n",
      "Epoch [1/1], Step [2560/7635], Loss: 4.3743\n",
      "Epoch [1/1], Step [2561/7635], Loss: 4.3494\n",
      "Epoch [1/1], Step [2562/7635], Loss: 4.3761\n",
      "Epoch [1/1], Step [2563/7635], Loss: 4.4074\n",
      "Epoch [1/1], Step [2564/7635], Loss: 4.4047\n",
      "Epoch [1/1], Step [2565/7635], Loss: 4.4561\n",
      "Epoch [1/1], Step [2566/7635], Loss: 4.4459\n",
      "Epoch [1/1], Step [2567/7635], Loss: 4.3516\n",
      "Epoch [1/1], Step [2568/7635], Loss: 4.4340\n",
      "Epoch [1/1], Step [2569/7635], Loss: 4.4175\n",
      "Epoch [1/1], Step [2570/7635], Loss: 4.5013\n",
      "Epoch [1/1], Step [2571/7635], Loss: 4.3911\n",
      "Epoch [1/1], Step [2572/7635], Loss: 4.4031\n",
      "Epoch [1/1], Step [2573/7635], Loss: 4.3833\n",
      "Epoch [1/1], Step [2574/7635], Loss: 4.3482\n",
      "Epoch [1/1], Step [2575/7635], Loss: 4.5049\n",
      "Epoch [1/1], Step [2576/7635], Loss: 4.3772\n",
      "Epoch [1/1], Step [2577/7635], Loss: 4.3520\n",
      "Epoch [1/1], Step [2578/7635], Loss: 4.3922\n",
      "Epoch [1/1], Step [2579/7635], Loss: 4.4602\n",
      "Epoch [1/1], Step [2580/7635], Loss: 4.3405\n",
      "Epoch [1/1], Step [2581/7635], Loss: 4.4169\n",
      "Epoch [1/1], Step [2582/7635], Loss: 4.4059\n",
      "Epoch [1/1], Step [2583/7635], Loss: 4.3496\n",
      "Epoch [1/1], Step [2584/7635], Loss: 4.4578\n",
      "Epoch [1/1], Step [2585/7635], Loss: 4.3474\n",
      "Epoch [1/1], Step [2586/7635], Loss: 4.3491\n",
      "Epoch [1/1], Step [2587/7635], Loss: 4.3109\n",
      "Epoch [1/1], Step [2588/7635], Loss: 4.4015\n",
      "Epoch [1/1], Step [2589/7635], Loss: 4.3723\n",
      "Epoch [1/1], Step [2590/7635], Loss: 4.3027\n",
      "Epoch [1/1], Step [2591/7635], Loss: 4.3845\n",
      "Epoch [1/1], Step [2592/7635], Loss: 4.4328\n",
      "Epoch [1/1], Step [2593/7635], Loss: 4.3368\n",
      "Epoch [1/1], Step [2594/7635], Loss: 4.3690\n",
      "Epoch [1/1], Step [2595/7635], Loss: 4.3683\n",
      "Epoch [1/1], Step [2596/7635], Loss: 4.3685\n",
      "Epoch [1/1], Step [2597/7635], Loss: 4.3337\n",
      "Epoch [1/1], Step [2598/7635], Loss: 4.4311\n",
      "Epoch [1/1], Step [2599/7635], Loss: 4.3101\n",
      "Epoch [1/1], Step [2600/7635], Loss: 4.5170\n",
      "Epoch [1/1], Step [2601/7635], Loss: 4.4162\n",
      "Epoch [1/1], Step [2602/7635], Loss: 4.4467\n",
      "Epoch [1/1], Step [2603/7635], Loss: 4.4719\n",
      "Epoch [1/1], Step [2604/7635], Loss: 4.4795\n",
      "Epoch [1/1], Step [2605/7635], Loss: 4.3763\n",
      "Epoch [1/1], Step [2606/7635], Loss: 4.4173\n",
      "Epoch [1/1], Step [2607/7635], Loss: 4.4422\n",
      "Epoch [1/1], Step [2608/7635], Loss: 4.4386\n",
      "Epoch [1/1], Step [2609/7635], Loss: 4.4153\n",
      "Epoch [1/1], Step [2610/7635], Loss: 4.3753\n",
      "Epoch [1/1], Step [2611/7635], Loss: 4.4136\n",
      "Epoch [1/1], Step [2612/7635], Loss: 4.3720\n",
      "Epoch [1/1], Step [2613/7635], Loss: 4.3547\n",
      "Epoch [1/1], Step [2614/7635], Loss: 4.4926\n",
      "Epoch [1/1], Step [2615/7635], Loss: 4.3953\n",
      "Epoch [1/1], Step [2616/7635], Loss: 4.3457\n",
      "Epoch [1/1], Step [2617/7635], Loss: 4.3679\n",
      "Epoch [1/1], Step [2618/7635], Loss: 4.4060\n",
      "Epoch [1/1], Step [2619/7635], Loss: 4.3819\n",
      "Epoch [1/1], Step [2620/7635], Loss: 4.3488\n",
      "Epoch [1/1], Step [2621/7635], Loss: 4.3731\n",
      "Epoch [1/1], Step [2622/7635], Loss: 4.4079\n",
      "Epoch [1/1], Step [2623/7635], Loss: 4.3964\n",
      "Epoch [1/1], Step [2624/7635], Loss: 4.4068\n",
      "Epoch [1/1], Step [2625/7635], Loss: 4.3365\n",
      "Epoch [1/1], Step [2626/7635], Loss: 4.3769\n",
      "Epoch [1/1], Step [2627/7635], Loss: 4.2980\n",
      "Epoch [1/1], Step [2628/7635], Loss: 4.3214\n",
      "Epoch [1/1], Step [2629/7635], Loss: 4.3431\n",
      "Epoch [1/1], Step [2630/7635], Loss: 4.3599\n",
      "Epoch [1/1], Step [2631/7635], Loss: 4.3072\n",
      "Epoch [1/1], Step [2632/7635], Loss: 4.4351\n",
      "Epoch [1/1], Step [2633/7635], Loss: 4.4518\n",
      "Epoch [1/1], Step [2634/7635], Loss: 4.4138\n",
      "Epoch [1/1], Step [2635/7635], Loss: 4.4260\n",
      "Epoch [1/1], Step [2636/7635], Loss: 4.2748\n",
      "Epoch [1/1], Step [2637/7635], Loss: 4.3525\n",
      "Epoch [1/1], Step [2638/7635], Loss: 4.3955\n",
      "Epoch [1/1], Step [2639/7635], Loss: 4.3623\n",
      "Epoch [1/1], Step [2640/7635], Loss: 4.4063\n",
      "Epoch [1/1], Step [2641/7635], Loss: 4.3588\n",
      "Epoch [1/1], Step [2642/7635], Loss: 4.4556\n",
      "Epoch [1/1], Step [2643/7635], Loss: 4.3836\n",
      "Epoch [1/1], Step [2644/7635], Loss: 4.3049\n",
      "Epoch [1/1], Step [2645/7635], Loss: 4.4908\n",
      "Epoch [1/1], Step [2646/7635], Loss: 4.4035\n",
      "Epoch [1/1], Step [2647/7635], Loss: 4.3715\n",
      "Epoch [1/1], Step [2648/7635], Loss: 4.3031\n",
      "Epoch [1/1], Step [2649/7635], Loss: 4.4123\n",
      "Epoch [1/1], Step [2650/7635], Loss: 4.2940\n",
      "Epoch [1/1], Step [2651/7635], Loss: 4.3440\n",
      "Epoch [1/1], Step [2652/7635], Loss: 4.3635\n",
      "Epoch [1/1], Step [2653/7635], Loss: 4.2244\n",
      "Epoch [1/1], Step [2654/7635], Loss: 4.4407\n",
      "Epoch [1/1], Step [2655/7635], Loss: 4.3946\n",
      "Epoch [1/1], Step [2656/7635], Loss: 4.4324\n",
      "Epoch [1/1], Step [2657/7635], Loss: 4.3055\n",
      "Epoch [1/1], Step [2658/7635], Loss: 4.4019\n",
      "Epoch [1/1], Step [2659/7635], Loss: 4.4022\n",
      "Epoch [1/1], Step [2660/7635], Loss: 4.4090\n",
      "Epoch [1/1], Step [2661/7635], Loss: 4.3682\n",
      "Epoch [1/1], Step [2662/7635], Loss: 4.3956\n",
      "Epoch [1/1], Step [2663/7635], Loss: 4.3703\n",
      "Epoch [1/1], Step [2664/7635], Loss: 4.4348\n",
      "Epoch [1/1], Step [2665/7635], Loss: 4.3872\n",
      "Epoch [1/1], Step [2666/7635], Loss: 4.3765\n",
      "Epoch [1/1], Step [2667/7635], Loss: 4.3578\n",
      "Epoch [1/1], Step [2668/7635], Loss: 4.3139\n",
      "Epoch [1/1], Step [2669/7635], Loss: 4.3601\n",
      "Epoch [1/1], Step [2670/7635], Loss: 4.4146\n",
      "Epoch [1/1], Step [2671/7635], Loss: 4.3676\n",
      "Epoch [1/1], Step [2672/7635], Loss: 4.2995\n",
      "Epoch [1/1], Step [2673/7635], Loss: 4.3622\n",
      "Epoch [1/1], Step [2674/7635], Loss: 4.3935\n",
      "Epoch [1/1], Step [2675/7635], Loss: 4.3662\n",
      "Epoch [1/1], Step [2676/7635], Loss: 4.3201\n",
      "Epoch [1/1], Step [2677/7635], Loss: 4.3516\n",
      "Epoch [1/1], Step [2678/7635], Loss: 4.4414\n",
      "Epoch [1/1], Step [2679/7635], Loss: 4.4007\n",
      "Epoch [1/1], Step [2680/7635], Loss: 4.4332\n",
      "Epoch [1/1], Step [2681/7635], Loss: 4.4167\n",
      "Epoch [1/1], Step [2682/7635], Loss: 4.4054\n",
      "Epoch [1/1], Step [2683/7635], Loss: 4.4180\n",
      "Epoch [1/1], Step [2684/7635], Loss: 4.3078\n",
      "Epoch [1/1], Step [2685/7635], Loss: 4.3271\n",
      "Epoch [1/1], Step [2686/7635], Loss: 4.2965\n",
      "Epoch [1/1], Step [2687/7635], Loss: 4.3259\n",
      "Epoch [1/1], Step [2688/7635], Loss: 4.3294\n",
      "Epoch [1/1], Step [2689/7635], Loss: 4.3623\n",
      "Epoch [1/1], Step [2690/7635], Loss: 4.3626\n",
      "Epoch [1/1], Step [2691/7635], Loss: 4.3739\n",
      "Epoch [1/1], Step [2692/7635], Loss: 4.3031\n",
      "Epoch [1/1], Step [2693/7635], Loss: 4.2450\n",
      "Epoch [1/1], Step [2694/7635], Loss: 4.4442\n",
      "Epoch [1/1], Step [2695/7635], Loss: 4.3925\n",
      "Epoch [1/1], Step [2696/7635], Loss: 4.3053\n",
      "Epoch [1/1], Step [2697/7635], Loss: 4.3886\n",
      "Epoch [1/1], Step [2698/7635], Loss: 4.3582\n",
      "Epoch [1/1], Step [2699/7635], Loss: 4.3274\n",
      "Epoch [1/1], Step [2700/7635], Loss: 4.4347\n",
      "Epoch [1/1], Step [2701/7635], Loss: 4.3345\n",
      "Epoch [1/1], Step [2702/7635], Loss: 4.4782\n",
      "Epoch [1/1], Step [2703/7635], Loss: 4.4603\n",
      "Epoch [1/1], Step [2704/7635], Loss: 4.4565\n",
      "Epoch [1/1], Step [2705/7635], Loss: 4.3688\n",
      "Epoch [1/1], Step [2706/7635], Loss: 4.3863\n",
      "Epoch [1/1], Step [2707/7635], Loss: 4.4169\n",
      "Epoch [1/1], Step [2708/7635], Loss: 4.3931\n",
      "Epoch [1/1], Step [2709/7635], Loss: 4.3532\n",
      "Epoch [1/1], Step [2710/7635], Loss: 4.3823\n",
      "Epoch [1/1], Step [2711/7635], Loss: 4.3907\n",
      "Epoch [1/1], Step [2712/7635], Loss: 4.2755\n",
      "Epoch [1/1], Step [2713/7635], Loss: 4.3427\n",
      "Epoch [1/1], Step [2714/7635], Loss: 4.4480\n",
      "Epoch [1/1], Step [2715/7635], Loss: 4.3516\n",
      "Epoch [1/1], Step [2716/7635], Loss: 4.4007\n",
      "Epoch [1/1], Step [2717/7635], Loss: 4.3462\n",
      "Epoch [1/1], Step [2718/7635], Loss: 4.3842\n",
      "Epoch [1/1], Step [2719/7635], Loss: 4.3817\n",
      "Epoch [1/1], Step [2720/7635], Loss: 4.3580\n",
      "Epoch [1/1], Step [2721/7635], Loss: 4.3157\n",
      "Epoch [1/1], Step [2722/7635], Loss: 4.3920\n",
      "Epoch [1/1], Step [2723/7635], Loss: 4.4139\n",
      "Epoch [1/1], Step [2724/7635], Loss: 4.3693\n",
      "Epoch [1/1], Step [2725/7635], Loss: 4.4151\n",
      "Epoch [1/1], Step [2726/7635], Loss: 4.2871\n",
      "Epoch [1/1], Step [2727/7635], Loss: 4.3051\n",
      "Epoch [1/1], Step [2728/7635], Loss: 4.5033\n",
      "Epoch [1/1], Step [2729/7635], Loss: 4.3638\n",
      "Epoch [1/1], Step [2730/7635], Loss: 4.3945\n",
      "Epoch [1/1], Step [2731/7635], Loss: 4.3232\n",
      "Epoch [1/1], Step [2732/7635], Loss: 4.3406\n",
      "Epoch [1/1], Step [2733/7635], Loss: 4.3344\n",
      "Epoch [1/1], Step [2734/7635], Loss: 4.4048\n",
      "Epoch [1/1], Step [2735/7635], Loss: 4.4121\n",
      "Epoch [1/1], Step [2736/7635], Loss: 4.4432\n",
      "Epoch [1/1], Step [2737/7635], Loss: 4.4958\n",
      "Epoch [1/1], Step [2738/7635], Loss: 4.4104\n",
      "Epoch [1/1], Step [2739/7635], Loss: 4.3220\n",
      "Epoch [1/1], Step [2740/7635], Loss: 4.4299\n",
      "Epoch [1/1], Step [2741/7635], Loss: 4.4158\n",
      "Epoch [1/1], Step [2742/7635], Loss: 4.3975\n",
      "Epoch [1/1], Step [2743/7635], Loss: 4.3292\n",
      "Epoch [1/1], Step [2744/7635], Loss: 4.4438\n",
      "Epoch [1/1], Step [2745/7635], Loss: 4.4241\n",
      "Epoch [1/1], Step [2746/7635], Loss: 4.4199\n",
      "Epoch [1/1], Step [2747/7635], Loss: 4.4439\n",
      "Epoch [1/1], Step [2748/7635], Loss: 4.4003\n",
      "Epoch [1/1], Step [2749/7635], Loss: 4.3233\n",
      "Epoch [1/1], Step [2750/7635], Loss: 4.3388\n",
      "Epoch [1/1], Step [2751/7635], Loss: 4.3238\n",
      "Epoch [1/1], Step [2752/7635], Loss: 4.3366\n",
      "Epoch [1/1], Step [2753/7635], Loss: 4.2746\n",
      "Epoch [1/1], Step [2754/7635], Loss: 4.4221\n",
      "Epoch [1/1], Step [2755/7635], Loss: 4.3882\n",
      "Epoch [1/1], Step [2756/7635], Loss: 4.3596\n",
      "Epoch [1/1], Step [2757/7635], Loss: 4.4829\n",
      "Epoch [1/1], Step [2758/7635], Loss: 4.3803\n",
      "Epoch [1/1], Step [2759/7635], Loss: 4.3858\n",
      "Epoch [1/1], Step [2760/7635], Loss: 4.3204\n",
      "Epoch [1/1], Step [2761/7635], Loss: 4.3182\n",
      "Epoch [1/1], Step [2762/7635], Loss: 4.3435\n",
      "Epoch [1/1], Step [2763/7635], Loss: 4.4262\n",
      "Epoch [1/1], Step [2764/7635], Loss: 4.3750\n",
      "Epoch [1/1], Step [2765/7635], Loss: 4.4334\n",
      "Epoch [1/1], Step [2766/7635], Loss: 4.3644\n",
      "Epoch [1/1], Step [2767/7635], Loss: 4.3421\n",
      "Epoch [1/1], Step [2768/7635], Loss: 4.3233\n",
      "Epoch [1/1], Step [2769/7635], Loss: 4.3904\n",
      "Epoch [1/1], Step [2770/7635], Loss: 4.3379\n",
      "Epoch [1/1], Step [2771/7635], Loss: 4.3339\n",
      "Epoch [1/1], Step [2772/7635], Loss: 4.3300\n",
      "Epoch [1/1], Step [2773/7635], Loss: 4.3850\n",
      "Epoch [1/1], Step [2774/7635], Loss: 4.3524\n",
      "Epoch [1/1], Step [2775/7635], Loss: 4.3889\n",
      "Epoch [1/1], Step [2776/7635], Loss: 4.3628\n",
      "Epoch [1/1], Step [2777/7635], Loss: 4.3319\n",
      "Epoch [1/1], Step [2778/7635], Loss: 4.5001\n",
      "Epoch [1/1], Step [2779/7635], Loss: 4.3297\n",
      "Epoch [1/1], Step [2780/7635], Loss: 4.3616\n",
      "Epoch [1/1], Step [2781/7635], Loss: 4.3726\n",
      "Epoch [1/1], Step [2782/7635], Loss: 4.3087\n",
      "Epoch [1/1], Step [2783/7635], Loss: 4.3444\n",
      "Epoch [1/1], Step [2784/7635], Loss: 4.3661\n",
      "Epoch [1/1], Step [2785/7635], Loss: 4.2961\n",
      "Epoch [1/1], Step [2786/7635], Loss: 4.3593\n",
      "Epoch [1/1], Step [2787/7635], Loss: 4.3356\n",
      "Epoch [1/1], Step [2788/7635], Loss: 4.3338\n",
      "Epoch [1/1], Step [2789/7635], Loss: 4.4424\n",
      "Epoch [1/1], Step [2790/7635], Loss: 4.3489\n",
      "Epoch [1/1], Step [2791/7635], Loss: 4.4224\n",
      "Epoch [1/1], Step [2792/7635], Loss: 4.4198\n",
      "Epoch [1/1], Step [2793/7635], Loss: 4.3173\n",
      "Epoch [1/1], Step [2794/7635], Loss: 4.4026\n",
      "Epoch [1/1], Step [2795/7635], Loss: 4.3692\n",
      "Epoch [1/1], Step [2796/7635], Loss: 4.4835\n",
      "Epoch [1/1], Step [2797/7635], Loss: 4.3617\n",
      "Epoch [1/1], Step [2798/7635], Loss: 4.3043\n",
      "Epoch [1/1], Step [2799/7635], Loss: 4.3565\n",
      "Epoch [1/1], Step [2800/7635], Loss: 4.2978\n",
      "Epoch [1/1], Step [2801/7635], Loss: 4.3462\n",
      "Epoch [1/1], Step [2802/7635], Loss: 4.3842\n",
      "Epoch [1/1], Step [2803/7635], Loss: 4.3388\n",
      "Epoch [1/1], Step [2804/7635], Loss: 4.3389\n",
      "Epoch [1/1], Step [2805/7635], Loss: 4.3347\n",
      "Epoch [1/1], Step [2806/7635], Loss: 4.3086\n",
      "Epoch [1/1], Step [2807/7635], Loss: 4.2750\n",
      "Epoch [1/1], Step [2808/7635], Loss: 4.3526\n",
      "Epoch [1/1], Step [2809/7635], Loss: 4.3888\n",
      "Epoch [1/1], Step [2810/7635], Loss: 4.3481\n",
      "Epoch [1/1], Step [2811/7635], Loss: 4.3565\n",
      "Epoch [1/1], Step [2812/7635], Loss: 4.3406\n",
      "Epoch [1/1], Step [2813/7635], Loss: 4.3439\n",
      "Epoch [1/1], Step [2814/7635], Loss: 4.3237\n",
      "Epoch [1/1], Step [2815/7635], Loss: 4.3502\n",
      "Epoch [1/1], Step [2816/7635], Loss: 4.2784\n",
      "Epoch [1/1], Step [2817/7635], Loss: 4.3827\n",
      "Epoch [1/1], Step [2818/7635], Loss: 4.3452\n",
      "Epoch [1/1], Step [2819/7635], Loss: 4.4344\n",
      "Epoch [1/1], Step [2820/7635], Loss: 4.3563\n",
      "Epoch [1/1], Step [2821/7635], Loss: 4.2906\n",
      "Epoch [1/1], Step [2822/7635], Loss: 4.3568\n",
      "Epoch [1/1], Step [2823/7635], Loss: 4.4085\n",
      "Epoch [1/1], Step [2824/7635], Loss: 4.3861\n",
      "Epoch [1/1], Step [2825/7635], Loss: 4.3553\n",
      "Epoch [1/1], Step [2826/7635], Loss: 4.2949\n",
      "Epoch [1/1], Step [2827/7635], Loss: 4.3314\n",
      "Epoch [1/1], Step [2828/7635], Loss: 4.3673\n",
      "Epoch [1/1], Step [2829/7635], Loss: 4.3211\n",
      "Epoch [1/1], Step [2830/7635], Loss: 4.4808\n",
      "Epoch [1/1], Step [2831/7635], Loss: 4.3631\n",
      "Epoch [1/1], Step [2832/7635], Loss: 4.3309\n",
      "Epoch [1/1], Step [2833/7635], Loss: 4.2917\n",
      "Epoch [1/1], Step [2834/7635], Loss: 4.3793\n",
      "Epoch [1/1], Step [2835/7635], Loss: 4.3215\n",
      "Epoch [1/1], Step [2836/7635], Loss: 4.2930\n",
      "Epoch [1/1], Step [2837/7635], Loss: 4.3427\n",
      "Epoch [1/1], Step [2838/7635], Loss: 4.3600\n",
      "Epoch [1/1], Step [2839/7635], Loss: 4.4177\n",
      "Epoch [1/1], Step [2840/7635], Loss: 4.3417\n",
      "Epoch [1/1], Step [2841/7635], Loss: 4.3442\n",
      "Epoch [1/1], Step [2842/7635], Loss: 4.3844\n",
      "Epoch [1/1], Step [2843/7635], Loss: 4.3061\n",
      "Epoch [1/1], Step [2844/7635], Loss: 4.3471\n",
      "Epoch [1/1], Step [2845/7635], Loss: 4.3291\n",
      "Epoch [1/1], Step [2846/7635], Loss: 4.3098\n",
      "Epoch [1/1], Step [2847/7635], Loss: 4.3515\n",
      "Epoch [1/1], Step [2848/7635], Loss: 4.3097\n",
      "Epoch [1/1], Step [2849/7635], Loss: 4.3647\n",
      "Epoch [1/1], Step [2850/7635], Loss: 4.2868\n",
      "Epoch [1/1], Step [2851/7635], Loss: 4.3846\n",
      "Epoch [1/1], Step [2852/7635], Loss: 4.3691\n",
      "Epoch [1/1], Step [2853/7635], Loss: 4.3989\n",
      "Epoch [1/1], Step [2854/7635], Loss: 4.4459\n",
      "Epoch [1/1], Step [2855/7635], Loss: 4.4714\n",
      "Epoch [1/1], Step [2856/7635], Loss: 4.3590\n",
      "Epoch [1/1], Step [2857/7635], Loss: 4.2835\n",
      "Epoch [1/1], Step [2858/7635], Loss: 4.3086\n",
      "Epoch [1/1], Step [2859/7635], Loss: 4.3573\n",
      "Epoch [1/1], Step [2860/7635], Loss: 4.4069\n",
      "Epoch [1/1], Step [2861/7635], Loss: 4.3157\n",
      "Epoch [1/1], Step [2862/7635], Loss: 4.4236\n",
      "Epoch [1/1], Step [2863/7635], Loss: 4.2875\n",
      "Epoch [1/1], Step [2864/7635], Loss: 4.3165\n",
      "Epoch [1/1], Step [2865/7635], Loss: 4.3970\n",
      "Epoch [1/1], Step [2866/7635], Loss: 4.3901\n",
      "Epoch [1/1], Step [2867/7635], Loss: 4.3004\n",
      "Epoch [1/1], Step [2868/7635], Loss: 4.2587\n",
      "Epoch [1/1], Step [2869/7635], Loss: 4.3844\n",
      "Epoch [1/1], Step [2870/7635], Loss: 4.3054\n",
      "Epoch [1/1], Step [2871/7635], Loss: 4.4296\n",
      "Epoch [1/1], Step [2872/7635], Loss: 4.3848\n",
      "Epoch [1/1], Step [2873/7635], Loss: 4.4198\n",
      "Epoch [1/1], Step [2874/7635], Loss: 4.3230\n",
      "Epoch [1/1], Step [2875/7635], Loss: 4.3531\n",
      "Epoch [1/1], Step [2876/7635], Loss: 4.3709\n",
      "Epoch [1/1], Step [2877/7635], Loss: 4.4434\n",
      "Epoch [1/1], Step [2878/7635], Loss: 4.3924\n",
      "Epoch [1/1], Step [2879/7635], Loss: 4.3037\n",
      "Epoch [1/1], Step [2880/7635], Loss: 4.3573\n",
      "Epoch [1/1], Step [2881/7635], Loss: 4.3530\n",
      "Epoch [1/1], Step [2882/7635], Loss: 4.3027\n",
      "Epoch [1/1], Step [2883/7635], Loss: 4.4010\n",
      "Epoch [1/1], Step [2884/7635], Loss: 4.3350\n",
      "Epoch [1/1], Step [2885/7635], Loss: 4.4268\n",
      "Epoch [1/1], Step [2886/7635], Loss: 4.3704\n",
      "Epoch [1/1], Step [2887/7635], Loss: 4.2516\n",
      "Epoch [1/1], Step [2888/7635], Loss: 4.3239\n",
      "Epoch [1/1], Step [2889/7635], Loss: 4.3157\n",
      "Epoch [1/1], Step [2890/7635], Loss: 4.4027\n",
      "Epoch [1/1], Step [2891/7635], Loss: 4.2802\n",
      "Epoch [1/1], Step [2892/7635], Loss: 4.3407\n",
      "Epoch [1/1], Step [2893/7635], Loss: 4.2963\n",
      "Epoch [1/1], Step [2894/7635], Loss: 4.3537\n",
      "Epoch [1/1], Step [2895/7635], Loss: 4.3542\n",
      "Epoch [1/1], Step [2896/7635], Loss: 4.3540\n",
      "Epoch [1/1], Step [2897/7635], Loss: 4.3845\n",
      "Epoch [1/1], Step [2898/7635], Loss: 4.3848\n",
      "Epoch [1/1], Step [2899/7635], Loss: 4.4751\n",
      "Epoch [1/1], Step [2900/7635], Loss: 4.3298\n",
      "Epoch [1/1], Step [2901/7635], Loss: 4.3759\n",
      "Epoch [1/1], Step [2902/7635], Loss: 4.3460\n",
      "Epoch [1/1], Step [2903/7635], Loss: 4.4165\n",
      "Epoch [1/1], Step [2904/7635], Loss: 4.3788\n",
      "Epoch [1/1], Step [2905/7635], Loss: 4.3176\n",
      "Epoch [1/1], Step [2906/7635], Loss: 4.3077\n",
      "Epoch [1/1], Step [2907/7635], Loss: 4.3665\n",
      "Epoch [1/1], Step [2908/7635], Loss: 4.3598\n",
      "Epoch [1/1], Step [2909/7635], Loss: 4.3369\n",
      "Epoch [1/1], Step [2910/7635], Loss: 4.3307\n",
      "Epoch [1/1], Step [2911/7635], Loss: 4.3029\n",
      "Epoch [1/1], Step [2912/7635], Loss: 4.4305\n",
      "Epoch [1/1], Step [2913/7635], Loss: 4.3941\n",
      "Epoch [1/1], Step [2914/7635], Loss: 4.3841\n",
      "Epoch [1/1], Step [2915/7635], Loss: 4.2932\n",
      "Epoch [1/1], Step [2916/7635], Loss: 4.3699\n",
      "Epoch [1/1], Step [2917/7635], Loss: 4.3514\n",
      "Epoch [1/1], Step [2918/7635], Loss: 4.3187\n",
      "Epoch [1/1], Step [2919/7635], Loss: 4.3308\n",
      "Epoch [1/1], Step [2920/7635], Loss: 4.3702\n",
      "Epoch [1/1], Step [2921/7635], Loss: 4.4030\n",
      "Epoch [1/1], Step [2922/7635], Loss: 4.3145\n",
      "Epoch [1/1], Step [2923/7635], Loss: 4.4072\n",
      "Epoch [1/1], Step [2924/7635], Loss: 4.3284\n",
      "Epoch [1/1], Step [2925/7635], Loss: 4.3066\n",
      "Epoch [1/1], Step [2926/7635], Loss: 4.3550\n",
      "Epoch [1/1], Step [2927/7635], Loss: 4.3395\n",
      "Epoch [1/1], Step [2928/7635], Loss: 4.2900\n",
      "Epoch [1/1], Step [2929/7635], Loss: 4.3099\n",
      "Epoch [1/1], Step [2930/7635], Loss: 4.3862\n",
      "Epoch [1/1], Step [2931/7635], Loss: 4.3115\n",
      "Epoch [1/1], Step [2932/7635], Loss: 4.3108\n",
      "Epoch [1/1], Step [2933/7635], Loss: 4.4148\n",
      "Epoch [1/1], Step [2934/7635], Loss: 4.3439\n",
      "Epoch [1/1], Step [2935/7635], Loss: 4.3052\n",
      "Epoch [1/1], Step [2936/7635], Loss: 4.3101\n",
      "Epoch [1/1], Step [2937/7635], Loss: 4.3449\n",
      "Epoch [1/1], Step [2938/7635], Loss: 4.3961\n",
      "Epoch [1/1], Step [2939/7635], Loss: 4.4244\n",
      "Epoch [1/1], Step [2940/7635], Loss: 4.3065\n",
      "Epoch [1/1], Step [2941/7635], Loss: 4.4187\n",
      "Epoch [1/1], Step [2942/7635], Loss: 4.3573\n",
      "Epoch [1/1], Step [2943/7635], Loss: 4.3145\n",
      "Epoch [1/1], Step [2944/7635], Loss: 4.3668\n",
      "Epoch [1/1], Step [2945/7635], Loss: 4.3259\n",
      "Epoch [1/1], Step [2946/7635], Loss: 4.2447\n",
      "Epoch [1/1], Step [2947/7635], Loss: 4.3537\n",
      "Epoch [1/1], Step [2948/7635], Loss: 4.3038\n",
      "Epoch [1/1], Step [2949/7635], Loss: 4.4510\n",
      "Epoch [1/1], Step [2950/7635], Loss: 4.2841\n",
      "Epoch [1/1], Step [2951/7635], Loss: 4.3746\n",
      "Epoch [1/1], Step [2952/7635], Loss: 4.3680\n",
      "Epoch [1/1], Step [2953/7635], Loss: 4.3219\n",
      "Epoch [1/1], Step [2954/7635], Loss: 4.3356\n",
      "Epoch [1/1], Step [2955/7635], Loss: 4.2798\n",
      "Epoch [1/1], Step [2956/7635], Loss: 4.3561\n",
      "Epoch [1/1], Step [2957/7635], Loss: 4.3281\n",
      "Epoch [1/1], Step [2958/7635], Loss: 4.3607\n",
      "Epoch [1/1], Step [2959/7635], Loss: 4.2610\n",
      "Epoch [1/1], Step [2960/7635], Loss: 4.3060\n",
      "Epoch [1/1], Step [2961/7635], Loss: 4.2850\n",
      "Epoch [1/1], Step [2962/7635], Loss: 4.3452\n",
      "Epoch [1/1], Step [2963/7635], Loss: 4.3171\n",
      "Epoch [1/1], Step [2964/7635], Loss: 4.3916\n",
      "Epoch [1/1], Step [2965/7635], Loss: 4.4168\n",
      "Epoch [1/1], Step [2966/7635], Loss: 4.3449\n",
      "Epoch [1/1], Step [2967/7635], Loss: 4.3584\n",
      "Epoch [1/1], Step [2968/7635], Loss: 4.3142\n",
      "Epoch [1/1], Step [2969/7635], Loss: 4.3491\n",
      "Epoch [1/1], Step [2970/7635], Loss: 4.3537\n",
      "Epoch [1/1], Step [2971/7635], Loss: 4.4272\n",
      "Epoch [1/1], Step [2972/7635], Loss: 4.3623\n",
      "Epoch [1/1], Step [2973/7635], Loss: 4.3330\n",
      "Epoch [1/1], Step [2974/7635], Loss: 4.3278\n",
      "Epoch [1/1], Step [2975/7635], Loss: 4.3868\n",
      "Epoch [1/1], Step [2976/7635], Loss: 4.2697\n",
      "Epoch [1/1], Step [2977/7635], Loss: 4.3560\n",
      "Epoch [1/1], Step [2978/7635], Loss: 4.3396\n",
      "Epoch [1/1], Step [2979/7635], Loss: 4.2909\n",
      "Epoch [1/1], Step [2980/7635], Loss: 4.3915\n",
      "Epoch [1/1], Step [2981/7635], Loss: 4.3556\n",
      "Epoch [1/1], Step [2982/7635], Loss: 4.2893\n",
      "Epoch [1/1], Step [2983/7635], Loss: 4.3709\n",
      "Epoch [1/1], Step [2984/7635], Loss: 4.3368\n",
      "Epoch [1/1], Step [2985/7635], Loss: 4.3198\n",
      "Epoch [1/1], Step [2986/7635], Loss: 4.3322\n",
      "Epoch [1/1], Step [2987/7635], Loss: 4.3398\n",
      "Epoch [1/1], Step [2988/7635], Loss: 4.3202\n",
      "Epoch [1/1], Step [2989/7635], Loss: 4.2990\n",
      "Epoch [1/1], Step [2990/7635], Loss: 4.3907\n",
      "Epoch [1/1], Step [2991/7635], Loss: 4.3099\n",
      "Epoch [1/1], Step [2992/7635], Loss: 4.3170\n",
      "Epoch [1/1], Step [2993/7635], Loss: 4.2925\n",
      "Epoch [1/1], Step [2994/7635], Loss: 4.2834\n",
      "Epoch [1/1], Step [2995/7635], Loss: 4.3703\n",
      "Epoch [1/1], Step [2996/7635], Loss: 4.3562\n",
      "Epoch [1/1], Step [2997/7635], Loss: 4.3428\n",
      "Epoch [1/1], Step [2998/7635], Loss: 4.3903\n",
      "Epoch [1/1], Step [2999/7635], Loss: 4.3431\n",
      "Epoch [1/1], Step [3000/7635], Loss: 4.4857\n",
      "Epoch [1/1], Step [3001/7635], Loss: 4.3193\n",
      "Epoch [1/1], Step [3002/7635], Loss: 4.3488\n",
      "Epoch [1/1], Step [3003/7635], Loss: 4.2912\n",
      "Epoch [1/1], Step [3004/7635], Loss: 4.2902\n",
      "Epoch [1/1], Step [3005/7635], Loss: 4.3515\n",
      "Epoch [1/1], Step [3006/7635], Loss: 4.3405\n",
      "Epoch [1/1], Step [3007/7635], Loss: 4.3056\n",
      "Epoch [1/1], Step [3008/7635], Loss: 4.3155\n",
      "Epoch [1/1], Step [3009/7635], Loss: 4.4036\n",
      "Epoch [1/1], Step [3010/7635], Loss: 4.3211\n",
      "Epoch [1/1], Step [3011/7635], Loss: 4.2909\n",
      "Epoch [1/1], Step [3012/7635], Loss: 4.4150\n",
      "Epoch [1/1], Step [3013/7635], Loss: 4.3883\n",
      "Epoch [1/1], Step [3014/7635], Loss: 4.3329\n",
      "Epoch [1/1], Step [3015/7635], Loss: 4.2758\n",
      "Epoch [1/1], Step [3016/7635], Loss: 4.3353\n",
      "Epoch [1/1], Step [3017/7635], Loss: 4.3351\n",
      "Epoch [1/1], Step [3018/7635], Loss: 4.3296\n",
      "Epoch [1/1], Step [3019/7635], Loss: 4.2830\n",
      "Epoch [1/1], Step [3020/7635], Loss: 4.3271\n",
      "Epoch [1/1], Step [3021/7635], Loss: 4.2938\n",
      "Epoch [1/1], Step [3022/7635], Loss: 4.3252\n",
      "Epoch [1/1], Step [3023/7635], Loss: 4.3631\n",
      "Epoch [1/1], Step [3024/7635], Loss: 4.2979\n",
      "Epoch [1/1], Step [3025/7635], Loss: 4.3308\n",
      "Epoch [1/1], Step [3026/7635], Loss: 4.3331\n",
      "Epoch [1/1], Step [3027/7635], Loss: 4.3661\n",
      "Epoch [1/1], Step [3028/7635], Loss: 4.3223\n",
      "Epoch [1/1], Step [3029/7635], Loss: 4.3621\n",
      "Epoch [1/1], Step [3030/7635], Loss: 4.3973\n",
      "Epoch [1/1], Step [3031/7635], Loss: 4.2850\n",
      "Epoch [1/1], Step [3032/7635], Loss: 4.3268\n",
      "Epoch [1/1], Step [3033/7635], Loss: 4.3580\n",
      "Epoch [1/1], Step [3034/7635], Loss: 4.3319\n",
      "Epoch [1/1], Step [3035/7635], Loss: 4.2889\n",
      "Epoch [1/1], Step [3036/7635], Loss: 4.3733\n",
      "Epoch [1/1], Step [3037/7635], Loss: 4.3599\n",
      "Epoch [1/1], Step [3038/7635], Loss: 4.4060\n",
      "Epoch [1/1], Step [3039/7635], Loss: 4.3897\n",
      "Epoch [1/1], Step [3040/7635], Loss: 4.4180\n",
      "Epoch [1/1], Step [3041/7635], Loss: 4.3391\n",
      "Epoch [1/1], Step [3042/7635], Loss: 4.3693\n",
      "Epoch [1/1], Step [3043/7635], Loss: 4.3921\n",
      "Epoch [1/1], Step [3044/7635], Loss: 4.3458\n",
      "Epoch [1/1], Step [3045/7635], Loss: 4.2880\n",
      "Epoch [1/1], Step [3046/7635], Loss: 4.3458\n",
      "Epoch [1/1], Step [3047/7635], Loss: 4.3559\n",
      "Epoch [1/1], Step [3048/7635], Loss: 4.3142\n",
      "Epoch [1/1], Step [3049/7635], Loss: 4.3265\n",
      "Epoch [1/1], Step [3050/7635], Loss: 4.2672\n",
      "Epoch [1/1], Step [3051/7635], Loss: 4.3330\n",
      "Epoch [1/1], Step [3052/7635], Loss: 4.3200\n",
      "Epoch [1/1], Step [3053/7635], Loss: 4.3081\n",
      "Epoch [1/1], Step [3054/7635], Loss: 4.3926\n",
      "Epoch [1/1], Step [3055/7635], Loss: 4.2853\n",
      "Epoch [1/1], Step [3056/7635], Loss: 4.2813\n",
      "Epoch [1/1], Step [3057/7635], Loss: 4.3757\n",
      "Epoch [1/1], Step [3058/7635], Loss: 4.2788\n",
      "Epoch [1/1], Step [3059/7635], Loss: 4.2955\n",
      "Epoch [1/1], Step [3060/7635], Loss: 4.2106\n",
      "Epoch [1/1], Step [3061/7635], Loss: 4.3401\n",
      "Epoch [1/1], Step [3062/7635], Loss: 4.3729\n",
      "Epoch [1/1], Step [3063/7635], Loss: 4.3926\n",
      "Epoch [1/1], Step [3064/7635], Loss: 4.2986\n",
      "Epoch [1/1], Step [3065/7635], Loss: 4.2800\n",
      "Epoch [1/1], Step [3066/7635], Loss: 4.3179\n",
      "Epoch [1/1], Step [3067/7635], Loss: 4.3113\n",
      "Epoch [1/1], Step [3068/7635], Loss: 4.3584\n",
      "Epoch [1/1], Step [3069/7635], Loss: 4.3441\n",
      "Epoch [1/1], Step [3070/7635], Loss: 4.3103\n",
      "Epoch [1/1], Step [3071/7635], Loss: 4.3460\n",
      "Epoch [1/1], Step [3072/7635], Loss: 4.3238\n",
      "Epoch [1/1], Step [3073/7635], Loss: 4.3292\n",
      "Epoch [1/1], Step [3074/7635], Loss: 4.3266\n",
      "Epoch [1/1], Step [3075/7635], Loss: 4.3218\n",
      "Epoch [1/1], Step [3076/7635], Loss: 4.3460\n",
      "Epoch [1/1], Step [3077/7635], Loss: 4.2195\n",
      "Epoch [1/1], Step [3078/7635], Loss: 4.3555\n",
      "Epoch [1/1], Step [3079/7635], Loss: 4.2675\n",
      "Epoch [1/1], Step [3080/7635], Loss: 4.3641\n",
      "Epoch [1/1], Step [3081/7635], Loss: 4.3493\n",
      "Epoch [1/1], Step [3082/7635], Loss: 4.2861\n",
      "Epoch [1/1], Step [3083/7635], Loss: 4.3940\n",
      "Epoch [1/1], Step [3084/7635], Loss: 4.3582\n",
      "Epoch [1/1], Step [3085/7635], Loss: 4.3312\n",
      "Epoch [1/1], Step [3086/7635], Loss: 4.2424\n",
      "Epoch [1/1], Step [3087/7635], Loss: 4.3209\n",
      "Epoch [1/1], Step [3088/7635], Loss: 4.4214\n",
      "Epoch [1/1], Step [3089/7635], Loss: 4.3351\n",
      "Epoch [1/1], Step [3090/7635], Loss: 4.3266\n",
      "Epoch [1/1], Step [3091/7635], Loss: 4.3595\n",
      "Epoch [1/1], Step [3092/7635], Loss: 4.3672\n",
      "Epoch [1/1], Step [3093/7635], Loss: 4.2679\n",
      "Epoch [1/1], Step [3094/7635], Loss: 4.3266\n",
      "Epoch [1/1], Step [3095/7635], Loss: 4.3897\n",
      "Epoch [1/1], Step [3096/7635], Loss: 4.3513\n",
      "Epoch [1/1], Step [3097/7635], Loss: 4.3065\n",
      "Epoch [1/1], Step [3098/7635], Loss: 4.2695\n",
      "Epoch [1/1], Step [3099/7635], Loss: 4.3763\n",
      "Epoch [1/1], Step [3100/7635], Loss: 4.3447\n",
      "Epoch [1/1], Step [3101/7635], Loss: 4.3018\n",
      "Epoch [1/1], Step [3102/7635], Loss: 4.3751\n",
      "Epoch [1/1], Step [3103/7635], Loss: 4.3587\n",
      "Epoch [1/1], Step [3104/7635], Loss: 4.3325\n",
      "Epoch [1/1], Step [3105/7635], Loss: 4.3532\n",
      "Epoch [1/1], Step [3106/7635], Loss: 4.2742\n",
      "Epoch [1/1], Step [3107/7635], Loss: 4.3031\n",
      "Epoch [1/1], Step [3108/7635], Loss: 4.3701\n",
      "Epoch [1/1], Step [3109/7635], Loss: 4.3104\n",
      "Epoch [1/1], Step [3110/7635], Loss: 4.2669\n",
      "Epoch [1/1], Step [3111/7635], Loss: 4.2969\n",
      "Epoch [1/1], Step [3112/7635], Loss: 4.3767\n",
      "Epoch [1/1], Step [3113/7635], Loss: 4.3104\n",
      "Epoch [1/1], Step [3114/7635], Loss: 4.3377\n",
      "Epoch [1/1], Step [3115/7635], Loss: 4.3313\n",
      "Epoch [1/1], Step [3116/7635], Loss: 4.3043\n",
      "Epoch [1/1], Step [3117/7635], Loss: 4.2583\n",
      "Epoch [1/1], Step [3118/7635], Loss: 4.3563\n",
      "Epoch [1/1], Step [3119/7635], Loss: 4.3277\n",
      "Epoch [1/1], Step [3120/7635], Loss: 4.2418\n",
      "Epoch [1/1], Step [3121/7635], Loss: 4.3313\n",
      "Epoch [1/1], Step [3122/7635], Loss: 4.3964\n",
      "Epoch [1/1], Step [3123/7635], Loss: 4.3207\n",
      "Epoch [1/1], Step [3124/7635], Loss: 4.3431\n",
      "Epoch [1/1], Step [3125/7635], Loss: 4.3442\n",
      "Epoch [1/1], Step [3126/7635], Loss: 4.3424\n",
      "Epoch [1/1], Step [3127/7635], Loss: 4.2744\n",
      "Epoch [1/1], Step [3128/7635], Loss: 4.3266\n",
      "Epoch [1/1], Step [3129/7635], Loss: 4.3355\n",
      "Epoch [1/1], Step [3130/7635], Loss: 4.3567\n",
      "Epoch [1/1], Step [3131/7635], Loss: 4.2925\n",
      "Epoch [1/1], Step [3132/7635], Loss: 4.3488\n",
      "Epoch [1/1], Step [3133/7635], Loss: 4.3591\n",
      "Epoch [1/1], Step [3134/7635], Loss: 4.2686\n",
      "Epoch [1/1], Step [3135/7635], Loss: 4.2993\n",
      "Epoch [1/1], Step [3136/7635], Loss: 4.2635\n",
      "Epoch [1/1], Step [3137/7635], Loss: 4.3180\n",
      "Epoch [1/1], Step [3138/7635], Loss: 4.3166\n",
      "Epoch [1/1], Step [3139/7635], Loss: 4.3205\n",
      "Epoch [1/1], Step [3140/7635], Loss: 4.3867\n",
      "Epoch [1/1], Step [3141/7635], Loss: 4.3243\n",
      "Epoch [1/1], Step [3142/7635], Loss: 4.3376\n",
      "Epoch [1/1], Step [3143/7635], Loss: 4.2727\n",
      "Epoch [1/1], Step [3144/7635], Loss: 4.3593\n",
      "Epoch [1/1], Step [3145/7635], Loss: 4.2237\n",
      "Epoch [1/1], Step [3146/7635], Loss: 4.3485\n",
      "Epoch [1/1], Step [3147/7635], Loss: 4.3853\n",
      "Epoch [1/1], Step [3148/7635], Loss: 4.2061\n",
      "Epoch [1/1], Step [3149/7635], Loss: 4.3348\n",
      "Epoch [1/1], Step [3150/7635], Loss: 4.3399\n",
      "Epoch [1/1], Step [3151/7635], Loss: 4.2462\n",
      "Epoch [1/1], Step [3152/7635], Loss: 4.3811\n",
      "Epoch [1/1], Step [3153/7635], Loss: 4.3769\n",
      "Epoch [1/1], Step [3154/7635], Loss: 4.3750\n",
      "Epoch [1/1], Step [3155/7635], Loss: 4.3271\n",
      "Epoch [1/1], Step [3156/7635], Loss: 4.3740\n",
      "Epoch [1/1], Step [3157/7635], Loss: 4.3906\n",
      "Epoch [1/1], Step [3158/7635], Loss: 4.3764\n",
      "Epoch [1/1], Step [3159/7635], Loss: 4.2803\n",
      "Epoch [1/1], Step [3160/7635], Loss: 4.2999\n",
      "Epoch [1/1], Step [3161/7635], Loss: 4.3060\n",
      "Epoch [1/1], Step [3162/7635], Loss: 4.3014\n",
      "Epoch [1/1], Step [3163/7635], Loss: 4.3326\n",
      "Epoch [1/1], Step [3164/7635], Loss: 4.3861\n",
      "Epoch [1/1], Step [3165/7635], Loss: 4.2369\n",
      "Epoch [1/1], Step [3166/7635], Loss: 4.3841\n",
      "Epoch [1/1], Step [3167/7635], Loss: 4.3470\n",
      "Epoch [1/1], Step [3168/7635], Loss: 4.3904\n",
      "Epoch [1/1], Step [3169/7635], Loss: 4.2874\n",
      "Epoch [1/1], Step [3170/7635], Loss: 4.2791\n",
      "Epoch [1/1], Step [3171/7635], Loss: 4.2969\n",
      "Epoch [1/1], Step [3172/7635], Loss: 4.3774\n",
      "Epoch [1/1], Step [3173/7635], Loss: 4.3645\n",
      "Epoch [1/1], Step [3174/7635], Loss: 4.2344\n",
      "Epoch [1/1], Step [3175/7635], Loss: 4.2335\n",
      "Epoch [1/1], Step [3176/7635], Loss: 4.2873\n",
      "Epoch [1/1], Step [3177/7635], Loss: 4.2089\n",
      "Epoch [1/1], Step [3178/7635], Loss: 4.3304\n",
      "Epoch [1/1], Step [3179/7635], Loss: 4.4098\n",
      "Epoch [1/1], Step [3180/7635], Loss: 4.3431\n",
      "Epoch [1/1], Step [3181/7635], Loss: 4.2903\n",
      "Epoch [1/1], Step [3182/7635], Loss: 4.3272\n",
      "Epoch [1/1], Step [3183/7635], Loss: 4.3031\n",
      "Epoch [1/1], Step [3184/7635], Loss: 4.3327\n",
      "Epoch [1/1], Step [3185/7635], Loss: 4.2446\n",
      "Epoch [1/1], Step [3186/7635], Loss: 4.3907\n",
      "Epoch [1/1], Step [3187/7635], Loss: 4.2572\n",
      "Epoch [1/1], Step [3188/7635], Loss: 4.3728\n",
      "Epoch [1/1], Step [3189/7635], Loss: 4.3387\n",
      "Epoch [1/1], Step [3190/7635], Loss: 4.3532\n",
      "Epoch [1/1], Step [3191/7635], Loss: 4.3586\n",
      "Epoch [1/1], Step [3192/7635], Loss: 4.3014\n",
      "Epoch [1/1], Step [3193/7635], Loss: 4.2659\n",
      "Epoch [1/1], Step [3194/7635], Loss: 4.3780\n",
      "Epoch [1/1], Step [3195/7635], Loss: 4.3634\n",
      "Epoch [1/1], Step [3196/7635], Loss: 4.3444\n",
      "Epoch [1/1], Step [3197/7635], Loss: 4.2847\n",
      "Epoch [1/1], Step [3198/7635], Loss: 4.3419\n",
      "Epoch [1/1], Step [3199/7635], Loss: 4.2897\n",
      "Epoch [1/1], Step [3200/7635], Loss: 4.3027\n",
      "Epoch [1/1], Step [3201/7635], Loss: 4.3722\n",
      "Epoch [1/1], Step [3202/7635], Loss: 4.4386\n",
      "Epoch [1/1], Step [3203/7635], Loss: 4.2820\n",
      "Epoch [1/1], Step [3204/7635], Loss: 4.3416\n",
      "Epoch [1/1], Step [3205/7635], Loss: 4.3074\n",
      "Epoch [1/1], Step [3206/7635], Loss: 4.3510\n",
      "Epoch [1/1], Step [3207/7635], Loss: 4.2365\n",
      "Epoch [1/1], Step [3208/7635], Loss: 4.3714\n",
      "Epoch [1/1], Step [3209/7635], Loss: 4.3588\n",
      "Epoch [1/1], Step [3210/7635], Loss: 4.3717\n",
      "Epoch [1/1], Step [3211/7635], Loss: 4.3237\n",
      "Epoch [1/1], Step [3212/7635], Loss: 4.3293\n",
      "Epoch [1/1], Step [3213/7635], Loss: 4.3203\n",
      "Epoch [1/1], Step [3214/7635], Loss: 4.2046\n",
      "Epoch [1/1], Step [3215/7635], Loss: 4.2930\n",
      "Epoch [1/1], Step [3216/7635], Loss: 4.3136\n",
      "Epoch [1/1], Step [3217/7635], Loss: 4.3252\n",
      "Epoch [1/1], Step [3218/7635], Loss: 4.3363\n",
      "Epoch [1/1], Step [3219/7635], Loss: 4.2991\n",
      "Epoch [1/1], Step [3220/7635], Loss: 4.2887\n",
      "Epoch [1/1], Step [3221/7635], Loss: 4.3487\n",
      "Epoch [1/1], Step [3222/7635], Loss: 4.4204\n",
      "Epoch [1/1], Step [3223/7635], Loss: 4.3118\n",
      "Epoch [1/1], Step [3224/7635], Loss: 4.3014\n",
      "Epoch [1/1], Step [3225/7635], Loss: 4.2933\n",
      "Epoch [1/1], Step [3226/7635], Loss: 4.2721\n",
      "Epoch [1/1], Step [3227/7635], Loss: 4.2276\n",
      "Epoch [1/1], Step [3228/7635], Loss: 4.3126\n",
      "Epoch [1/1], Step [3229/7635], Loss: 4.2676\n",
      "Epoch [1/1], Step [3230/7635], Loss: 4.2987\n",
      "Epoch [1/1], Step [3231/7635], Loss: 4.3003\n",
      "Epoch [1/1], Step [3232/7635], Loss: 4.1993\n",
      "Epoch [1/1], Step [3233/7635], Loss: 4.3765\n",
      "Epoch [1/1], Step [3234/7635], Loss: 4.2728\n",
      "Epoch [1/1], Step [3235/7635], Loss: 4.3301\n",
      "Epoch [1/1], Step [3236/7635], Loss: 4.2798\n",
      "Epoch [1/1], Step [3237/7635], Loss: 4.3820\n",
      "Epoch [1/1], Step [3238/7635], Loss: 4.3201\n",
      "Epoch [1/1], Step [3239/7635], Loss: 4.3527\n",
      "Epoch [1/1], Step [3240/7635], Loss: 4.3069\n",
      "Epoch [1/1], Step [3241/7635], Loss: 4.3245\n",
      "Epoch [1/1], Step [3242/7635], Loss: 4.2749\n",
      "Epoch [1/1], Step [3243/7635], Loss: 4.3904\n",
      "Epoch [1/1], Step [3244/7635], Loss: 4.3560\n",
      "Epoch [1/1], Step [3245/7635], Loss: 4.2913\n",
      "Epoch [1/1], Step [3246/7635], Loss: 4.3286\n",
      "Epoch [1/1], Step [3247/7635], Loss: 4.3677\n",
      "Epoch [1/1], Step [3248/7635], Loss: 4.3075\n",
      "Epoch [1/1], Step [3249/7635], Loss: 4.2854\n",
      "Epoch [1/1], Step [3250/7635], Loss: 4.2162\n",
      "Epoch [1/1], Step [3251/7635], Loss: 4.2504\n",
      "Epoch [1/1], Step [3252/7635], Loss: 4.2954\n",
      "Epoch [1/1], Step [3253/7635], Loss: 4.3596\n",
      "Epoch [1/1], Step [3254/7635], Loss: 4.2615\n",
      "Epoch [1/1], Step [3255/7635], Loss: 4.3445\n",
      "Epoch [1/1], Step [3256/7635], Loss: 4.2855\n",
      "Epoch [1/1], Step [3257/7635], Loss: 4.2562\n",
      "Epoch [1/1], Step [3258/7635], Loss: 4.2702\n",
      "Epoch [1/1], Step [3259/7635], Loss: 4.2918\n",
      "Epoch [1/1], Step [3260/7635], Loss: 4.3377\n",
      "Epoch [1/1], Step [3261/7635], Loss: 4.2957\n",
      "Epoch [1/1], Step [3262/7635], Loss: 4.3488\n",
      "Epoch [1/1], Step [3263/7635], Loss: 4.2892\n",
      "Epoch [1/1], Step [3264/7635], Loss: 4.2489\n",
      "Epoch [1/1], Step [3265/7635], Loss: 4.3410\n",
      "Epoch [1/1], Step [3266/7635], Loss: 4.3002\n",
      "Epoch [1/1], Step [3267/7635], Loss: 4.3568\n",
      "Epoch [1/1], Step [3268/7635], Loss: 4.3072\n",
      "Epoch [1/1], Step [3269/7635], Loss: 4.2948\n",
      "Epoch [1/1], Step [3270/7635], Loss: 4.3074\n",
      "Epoch [1/1], Step [3271/7635], Loss: 4.2412\n",
      "Epoch [1/1], Step [3272/7635], Loss: 4.3751\n",
      "Epoch [1/1], Step [3273/7635], Loss: 4.2332\n",
      "Epoch [1/1], Step [3274/7635], Loss: 4.3214\n",
      "Epoch [1/1], Step [3275/7635], Loss: 4.2308\n",
      "Epoch [1/1], Step [3276/7635], Loss: 4.2604\n",
      "Epoch [1/1], Step [3277/7635], Loss: 4.3223\n",
      "Epoch [1/1], Step [3278/7635], Loss: 4.2887\n",
      "Epoch [1/1], Step [3279/7635], Loss: 4.3013\n",
      "Epoch [1/1], Step [3280/7635], Loss: 4.2271\n",
      "Epoch [1/1], Step [3281/7635], Loss: 4.3497\n",
      "Epoch [1/1], Step [3282/7635], Loss: 4.2962\n",
      "Epoch [1/1], Step [3283/7635], Loss: 4.2963\n",
      "Epoch [1/1], Step [3284/7635], Loss: 4.3747\n",
      "Epoch [1/1], Step [3285/7635], Loss: 4.3385\n",
      "Epoch [1/1], Step [3286/7635], Loss: 4.2519\n",
      "Epoch [1/1], Step [3287/7635], Loss: 4.2568\n",
      "Epoch [1/1], Step [3288/7635], Loss: 4.2601\n",
      "Epoch [1/1], Step [3289/7635], Loss: 4.2696\n",
      "Epoch [1/1], Step [3290/7635], Loss: 4.3008\n",
      "Epoch [1/1], Step [3291/7635], Loss: 4.3154\n",
      "Epoch [1/1], Step [3292/7635], Loss: 4.2623\n",
      "Epoch [1/1], Step [3293/7635], Loss: 4.3223\n",
      "Epoch [1/1], Step [3294/7635], Loss: 4.2922\n",
      "Epoch [1/1], Step [3295/7635], Loss: 4.3396\n",
      "Epoch [1/1], Step [3296/7635], Loss: 4.3369\n",
      "Epoch [1/1], Step [3297/7635], Loss: 4.3637\n",
      "Epoch [1/1], Step [3298/7635], Loss: 4.3891\n",
      "Epoch [1/1], Step [3299/7635], Loss: 4.2116\n",
      "Epoch [1/1], Step [3300/7635], Loss: 4.2678\n",
      "Epoch [1/1], Step [3301/7635], Loss: 4.3635\n",
      "Epoch [1/1], Step [3302/7635], Loss: 4.3031\n",
      "Epoch [1/1], Step [3303/7635], Loss: 4.3138\n",
      "Epoch [1/1], Step [3304/7635], Loss: 4.2309\n",
      "Epoch [1/1], Step [3305/7635], Loss: 4.2782\n",
      "Epoch [1/1], Step [3306/7635], Loss: 4.2791\n",
      "Epoch [1/1], Step [3307/7635], Loss: 4.2440\n",
      "Epoch [1/1], Step [3308/7635], Loss: 4.2569\n",
      "Epoch [1/1], Step [3309/7635], Loss: 4.3377\n",
      "Epoch [1/1], Step [3310/7635], Loss: 4.3434\n",
      "Epoch [1/1], Step [3311/7635], Loss: 4.2456\n",
      "Epoch [1/1], Step [3312/7635], Loss: 4.2620\n",
      "Epoch [1/1], Step [3313/7635], Loss: 4.2941\n",
      "Epoch [1/1], Step [3314/7635], Loss: 4.3435\n",
      "Epoch [1/1], Step [3315/7635], Loss: 4.2855\n",
      "Epoch [1/1], Step [3316/7635], Loss: 4.3470\n",
      "Epoch [1/1], Step [3317/7635], Loss: 4.2538\n",
      "Epoch [1/1], Step [3318/7635], Loss: 4.2273\n",
      "Epoch [1/1], Step [3319/7635], Loss: 4.2588\n",
      "Epoch [1/1], Step [3320/7635], Loss: 4.3578\n",
      "Epoch [1/1], Step [3321/7635], Loss: 4.2404\n",
      "Epoch [1/1], Step [3322/7635], Loss: 4.2755\n",
      "Epoch [1/1], Step [3323/7635], Loss: 4.2914\n",
      "Epoch [1/1], Step [3324/7635], Loss: 4.3054\n",
      "Epoch [1/1], Step [3325/7635], Loss: 4.2067\n",
      "Epoch [1/1], Step [3326/7635], Loss: 4.2067\n",
      "Epoch [1/1], Step [3327/7635], Loss: 4.3035\n",
      "Epoch [1/1], Step [3328/7635], Loss: 4.2770\n",
      "Epoch [1/1], Step [3329/7635], Loss: 4.3290\n",
      "Epoch [1/1], Step [3330/7635], Loss: 4.3661\n",
      "Epoch [1/1], Step [3331/7635], Loss: 4.2931\n",
      "Epoch [1/1], Step [3332/7635], Loss: 4.3791\n",
      "Epoch [1/1], Step [3333/7635], Loss: 4.3459\n",
      "Epoch [1/1], Step [3334/7635], Loss: 4.3198\n",
      "Epoch [1/1], Step [3335/7635], Loss: 4.3281\n",
      "Epoch [1/1], Step [3336/7635], Loss: 4.3200\n",
      "Epoch [1/1], Step [3337/7635], Loss: 4.4029\n",
      "Epoch [1/1], Step [3338/7635], Loss: 4.3117\n",
      "Epoch [1/1], Step [3339/7635], Loss: 4.3059\n",
      "Epoch [1/1], Step [3340/7635], Loss: 4.2843\n",
      "Epoch [1/1], Step [3341/7635], Loss: 4.2407\n",
      "Epoch [1/1], Step [3342/7635], Loss: 4.2435\n",
      "Epoch [1/1], Step [3343/7635], Loss: 4.2898\n",
      "Epoch [1/1], Step [3344/7635], Loss: 4.3781\n",
      "Epoch [1/1], Step [3345/7635], Loss: 4.2936\n",
      "Epoch [1/1], Step [3346/7635], Loss: 4.2467\n",
      "Epoch [1/1], Step [3347/7635], Loss: 4.3608\n",
      "Epoch [1/1], Step [3348/7635], Loss: 4.2544\n",
      "Epoch [1/1], Step [3349/7635], Loss: 4.3461\n",
      "Epoch [1/1], Step [3350/7635], Loss: 4.3138\n",
      "Epoch [1/1], Step [3351/7635], Loss: 4.4068\n",
      "Epoch [1/1], Step [3352/7635], Loss: 4.2907\n",
      "Epoch [1/1], Step [3353/7635], Loss: 4.2790\n",
      "Epoch [1/1], Step [3354/7635], Loss: 4.2564\n",
      "Epoch [1/1], Step [3355/7635], Loss: 4.4336\n",
      "Epoch [1/1], Step [3356/7635], Loss: 4.2139\n",
      "Epoch [1/1], Step [3357/7635], Loss: 4.3788\n",
      "Epoch [1/1], Step [3358/7635], Loss: 4.3544\n",
      "Epoch [1/1], Step [3359/7635], Loss: 4.3075\n",
      "Epoch [1/1], Step [3360/7635], Loss: 4.3465\n",
      "Epoch [1/1], Step [3361/7635], Loss: 4.3162\n",
      "Epoch [1/1], Step [3362/7635], Loss: 4.3089\n",
      "Epoch [1/1], Step [3363/7635], Loss: 4.3155\n",
      "Epoch [1/1], Step [3364/7635], Loss: 4.3436\n",
      "Epoch [1/1], Step [3365/7635], Loss: 4.2383\n",
      "Epoch [1/1], Step [3366/7635], Loss: 4.3111\n",
      "Epoch [1/1], Step [3367/7635], Loss: 4.3644\n",
      "Epoch [1/1], Step [3368/7635], Loss: 4.3248\n",
      "Epoch [1/1], Step [3369/7635], Loss: 4.3171\n",
      "Epoch [1/1], Step [3370/7635], Loss: 4.2989\n",
      "Epoch [1/1], Step [3371/7635], Loss: 4.3700\n",
      "Epoch [1/1], Step [3372/7635], Loss: 4.3368\n",
      "Epoch [1/1], Step [3373/7635], Loss: 4.2281\n",
      "Epoch [1/1], Step [3374/7635], Loss: 4.3560\n",
      "Epoch [1/1], Step [3375/7635], Loss: 4.2209\n",
      "Epoch [1/1], Step [3376/7635], Loss: 4.2932\n",
      "Epoch [1/1], Step [3377/7635], Loss: 4.3311\n",
      "Epoch [1/1], Step [3378/7635], Loss: 4.3194\n",
      "Epoch [1/1], Step [3379/7635], Loss: 4.2564\n",
      "Epoch [1/1], Step [3380/7635], Loss: 4.3284\n",
      "Epoch [1/1], Step [3381/7635], Loss: 4.2831\n",
      "Epoch [1/1], Step [3382/7635], Loss: 4.3084\n",
      "Epoch [1/1], Step [3383/7635], Loss: 4.3774\n",
      "Epoch [1/1], Step [3384/7635], Loss: 4.3664\n",
      "Epoch [1/1], Step [3385/7635], Loss: 4.2391\n",
      "Epoch [1/1], Step [3386/7635], Loss: 4.2531\n",
      "Epoch [1/1], Step [3387/7635], Loss: 4.3143\n",
      "Epoch [1/1], Step [3388/7635], Loss: 4.3730\n",
      "Epoch [1/1], Step [3389/7635], Loss: 4.3435\n",
      "Epoch [1/1], Step [3390/7635], Loss: 4.2846\n",
      "Epoch [1/1], Step [3391/7635], Loss: 4.3811\n",
      "Epoch [1/1], Step [3392/7635], Loss: 4.3052\n",
      "Epoch [1/1], Step [3393/7635], Loss: 4.2646\n",
      "Epoch [1/1], Step [3394/7635], Loss: 4.3046\n",
      "Epoch [1/1], Step [3395/7635], Loss: 4.3488\n",
      "Epoch [1/1], Step [3396/7635], Loss: 4.3510\n",
      "Epoch [1/1], Step [3397/7635], Loss: 4.3420\n",
      "Epoch [1/1], Step [3398/7635], Loss: 4.2861\n",
      "Epoch [1/1], Step [3399/7635], Loss: 4.2909\n",
      "Epoch [1/1], Step [3400/7635], Loss: 4.3285\n",
      "Epoch [1/1], Step [3401/7635], Loss: 4.3159\n",
      "Epoch [1/1], Step [3402/7635], Loss: 4.3589\n",
      "Epoch [1/1], Step [3403/7635], Loss: 4.3176\n",
      "Epoch [1/1], Step [3404/7635], Loss: 4.2080\n",
      "Epoch [1/1], Step [3405/7635], Loss: 4.2622\n",
      "Epoch [1/1], Step [3406/7635], Loss: 4.3351\n",
      "Epoch [1/1], Step [3407/7635], Loss: 4.2379\n",
      "Epoch [1/1], Step [3408/7635], Loss: 4.3371\n",
      "Epoch [1/1], Step [3409/7635], Loss: 4.2015\n",
      "Epoch [1/1], Step [3410/7635], Loss: 4.3514\n",
      "Epoch [1/1], Step [3411/7635], Loss: 4.2908\n",
      "Epoch [1/1], Step [3412/7635], Loss: 4.1886\n",
      "Epoch [1/1], Step [3413/7635], Loss: 4.2350\n",
      "Epoch [1/1], Step [3414/7635], Loss: 4.2890\n",
      "Epoch [1/1], Step [3415/7635], Loss: 4.2608\n",
      "Epoch [1/1], Step [3416/7635], Loss: 4.2929\n",
      "Epoch [1/1], Step [3417/7635], Loss: 4.2761\n",
      "Epoch [1/1], Step [3418/7635], Loss: 4.3901\n",
      "Epoch [1/1], Step [3419/7635], Loss: 4.2840\n",
      "Epoch [1/1], Step [3420/7635], Loss: 4.2747\n",
      "Epoch [1/1], Step [3421/7635], Loss: 4.2386\n",
      "Epoch [1/1], Step [3422/7635], Loss: 4.2079\n",
      "Epoch [1/1], Step [3423/7635], Loss: 4.3765\n",
      "Epoch [1/1], Step [3424/7635], Loss: 4.3732\n",
      "Epoch [1/1], Step [3425/7635], Loss: 4.2993\n",
      "Epoch [1/1], Step [3426/7635], Loss: 4.2904\n",
      "Epoch [1/1], Step [3427/7635], Loss: 4.3488\n",
      "Epoch [1/1], Step [3428/7635], Loss: 4.3422\n",
      "Epoch [1/1], Step [3429/7635], Loss: 4.2722\n",
      "Epoch [1/1], Step [3430/7635], Loss: 4.1954\n",
      "Epoch [1/1], Step [3431/7635], Loss: 4.3267\n",
      "Epoch [1/1], Step [3432/7635], Loss: 4.3629\n",
      "Epoch [1/1], Step [3433/7635], Loss: 4.2726\n",
      "Epoch [1/1], Step [3434/7635], Loss: 4.2477\n",
      "Epoch [1/1], Step [3435/7635], Loss: 4.3558\n",
      "Epoch [1/1], Step [3436/7635], Loss: 4.2243\n",
      "Epoch [1/1], Step [3437/7635], Loss: 4.3329\n",
      "Epoch [1/1], Step [3438/7635], Loss: 4.2510\n",
      "Epoch [1/1], Step [3439/7635], Loss: 4.2304\n",
      "Epoch [1/1], Step [3440/7635], Loss: 4.3131\n",
      "Epoch [1/1], Step [3441/7635], Loss: 4.3043\n",
      "Epoch [1/1], Step [3442/7635], Loss: 4.3021\n",
      "Epoch [1/1], Step [3443/7635], Loss: 4.3218\n",
      "Epoch [1/1], Step [3444/7635], Loss: 4.3016\n",
      "Epoch [1/1], Step [3445/7635], Loss: 4.3266\n",
      "Epoch [1/1], Step [3446/7635], Loss: 4.2252\n",
      "Epoch [1/1], Step [3447/7635], Loss: 4.3508\n",
      "Epoch [1/1], Step [3448/7635], Loss: 4.3038\n",
      "Epoch [1/1], Step [3449/7635], Loss: 4.3071\n",
      "Epoch [1/1], Step [3450/7635], Loss: 4.3091\n",
      "Epoch [1/1], Step [3451/7635], Loss: 4.3058\n",
      "Epoch [1/1], Step [3452/7635], Loss: 4.3812\n",
      "Epoch [1/1], Step [3453/7635], Loss: 4.3690\n",
      "Epoch [1/1], Step [3454/7635], Loss: 4.2388\n",
      "Epoch [1/1], Step [3455/7635], Loss: 4.3592\n",
      "Epoch [1/1], Step [3456/7635], Loss: 4.3412\n",
      "Epoch [1/1], Step [3457/7635], Loss: 4.1948\n",
      "Epoch [1/1], Step [3458/7635], Loss: 4.3489\n",
      "Epoch [1/1], Step [3459/7635], Loss: 4.3373\n",
      "Epoch [1/1], Step [3460/7635], Loss: 4.3318\n",
      "Epoch [1/1], Step [3461/7635], Loss: 4.3368\n",
      "Epoch [1/1], Step [3462/7635], Loss: 4.2305\n",
      "Epoch [1/1], Step [3463/7635], Loss: 4.2811\n",
      "Epoch [1/1], Step [3464/7635], Loss: 4.2973\n",
      "Epoch [1/1], Step [3465/7635], Loss: 4.2565\n",
      "Epoch [1/1], Step [3466/7635], Loss: 4.3033\n",
      "Epoch [1/1], Step [3467/7635], Loss: 4.2639\n",
      "Epoch [1/1], Step [3468/7635], Loss: 4.3021\n",
      "Epoch [1/1], Step [3469/7635], Loss: 4.3214\n",
      "Epoch [1/1], Step [3470/7635], Loss: 4.3422\n",
      "Epoch [1/1], Step [3471/7635], Loss: 4.2459\n",
      "Epoch [1/1], Step [3472/7635], Loss: 4.2984\n",
      "Epoch [1/1], Step [3473/7635], Loss: 4.3810\n",
      "Epoch [1/1], Step [3474/7635], Loss: 4.2604\n",
      "Epoch [1/1], Step [3475/7635], Loss: 4.2752\n",
      "Epoch [1/1], Step [3476/7635], Loss: 4.3009\n",
      "Epoch [1/1], Step [3477/7635], Loss: 4.3194\n",
      "Epoch [1/1], Step [3478/7635], Loss: 4.3117\n",
      "Epoch [1/1], Step [3479/7635], Loss: 4.2959\n",
      "Epoch [1/1], Step [3480/7635], Loss: 4.3130\n",
      "Epoch [1/1], Step [3481/7635], Loss: 4.3480\n",
      "Epoch [1/1], Step [3482/7635], Loss: 4.2617\n",
      "Epoch [1/1], Step [3483/7635], Loss: 4.3856\n",
      "Epoch [1/1], Step [3484/7635], Loss: 4.2570\n",
      "Epoch [1/1], Step [3485/7635], Loss: 4.2110\n",
      "Epoch [1/1], Step [3486/7635], Loss: 4.3041\n",
      "Epoch [1/1], Step [3487/7635], Loss: 4.2937\n",
      "Epoch [1/1], Step [3488/7635], Loss: 4.3141\n",
      "Epoch [1/1], Step [3489/7635], Loss: 4.2443\n",
      "Epoch [1/1], Step [3490/7635], Loss: 4.1609\n",
      "Epoch [1/1], Step [3491/7635], Loss: 4.3009\n",
      "Epoch [1/1], Step [3492/7635], Loss: 4.2747\n",
      "Epoch [1/1], Step [3493/7635], Loss: 4.3295\n",
      "Epoch [1/1], Step [3494/7635], Loss: 4.2208\n",
      "Epoch [1/1], Step [3495/7635], Loss: 4.2561\n",
      "Epoch [1/1], Step [3496/7635], Loss: 4.2360\n",
      "Epoch [1/1], Step [3497/7635], Loss: 4.2825\n",
      "Epoch [1/1], Step [3498/7635], Loss: 4.2592\n",
      "Epoch [1/1], Step [3499/7635], Loss: 4.2621\n",
      "Epoch [1/1], Step [3500/7635], Loss: 4.2494\n",
      "Epoch [1/1], Step [3501/7635], Loss: 4.2120\n",
      "Epoch [1/1], Step [3502/7635], Loss: 4.3214\n",
      "Epoch [1/1], Step [3503/7635], Loss: 4.2812\n",
      "Epoch [1/1], Step [3504/7635], Loss: 4.2523\n",
      "Epoch [1/1], Step [3505/7635], Loss: 4.2513\n",
      "Epoch [1/1], Step [3506/7635], Loss: 4.2754\n",
      "Epoch [1/1], Step [3507/7635], Loss: 4.2676\n",
      "Epoch [1/1], Step [3508/7635], Loss: 4.2690\n",
      "Epoch [1/1], Step [3509/7635], Loss: 4.3231\n",
      "Epoch [1/1], Step [3510/7635], Loss: 4.3054\n",
      "Epoch [1/1], Step [3511/7635], Loss: 4.3072\n",
      "Epoch [1/1], Step [3512/7635], Loss: 4.3313\n",
      "Epoch [1/1], Step [3513/7635], Loss: 4.2488\n",
      "Epoch [1/1], Step [3514/7635], Loss: 4.2594\n",
      "Epoch [1/1], Step [3515/7635], Loss: 4.2254\n",
      "Epoch [1/1], Step [3516/7635], Loss: 4.2703\n",
      "Epoch [1/1], Step [3517/7635], Loss: 4.2283\n",
      "Epoch [1/1], Step [3518/7635], Loss: 4.2476\n",
      "Epoch [1/1], Step [3519/7635], Loss: 4.3419\n",
      "Epoch [1/1], Step [3520/7635], Loss: 4.3180\n",
      "Epoch [1/1], Step [3521/7635], Loss: 4.2496\n",
      "Epoch [1/1], Step [3522/7635], Loss: 4.1696\n",
      "Epoch [1/1], Step [3523/7635], Loss: 4.2953\n",
      "Epoch [1/1], Step [3524/7635], Loss: 4.2870\n",
      "Epoch [1/1], Step [3525/7635], Loss: 4.3554\n",
      "Epoch [1/1], Step [3526/7635], Loss: 4.3434\n",
      "Epoch [1/1], Step [3527/7635], Loss: 4.3245\n",
      "Epoch [1/1], Step [3528/7635], Loss: 4.2302\n",
      "Epoch [1/1], Step [3529/7635], Loss: 4.3239\n",
      "Epoch [1/1], Step [3530/7635], Loss: 4.3520\n",
      "Epoch [1/1], Step [3531/7635], Loss: 4.3401\n",
      "Epoch [1/1], Step [3532/7635], Loss: 4.2959\n",
      "Epoch [1/1], Step [3533/7635], Loss: 4.2871\n",
      "Epoch [1/1], Step [3534/7635], Loss: 4.3079\n",
      "Epoch [1/1], Step [3535/7635], Loss: 4.3151\n",
      "Epoch [1/1], Step [3536/7635], Loss: 4.3558\n",
      "Epoch [1/1], Step [3537/7635], Loss: 4.2478\n",
      "Epoch [1/1], Step [3538/7635], Loss: 4.3436\n",
      "Epoch [1/1], Step [3539/7635], Loss: 4.2383\n",
      "Epoch [1/1], Step [3540/7635], Loss: 4.3354\n",
      "Epoch [1/1], Step [3541/7635], Loss: 4.3431\n",
      "Epoch [1/1], Step [3542/7635], Loss: 4.3094\n",
      "Epoch [1/1], Step [3543/7635], Loss: 4.2838\n",
      "Epoch [1/1], Step [3544/7635], Loss: 4.1414\n",
      "Epoch [1/1], Step [3545/7635], Loss: 4.2826\n",
      "Epoch [1/1], Step [3546/7635], Loss: 4.2891\n",
      "Epoch [1/1], Step [3547/7635], Loss: 4.3654\n",
      "Epoch [1/1], Step [3548/7635], Loss: 4.2590\n",
      "Epoch [1/1], Step [3549/7635], Loss: 4.1871\n",
      "Epoch [1/1], Step [3550/7635], Loss: 4.2288\n",
      "Epoch [1/1], Step [3551/7635], Loss: 4.2987\n",
      "Epoch [1/1], Step [3552/7635], Loss: 4.3009\n",
      "Epoch [1/1], Step [3553/7635], Loss: 4.2513\n",
      "Epoch [1/1], Step [3554/7635], Loss: 4.3712\n",
      "Epoch [1/1], Step [3555/7635], Loss: 4.2413\n",
      "Epoch [1/1], Step [3556/7635], Loss: 4.3441\n",
      "Epoch [1/1], Step [3557/7635], Loss: 4.3074\n",
      "Epoch [1/1], Step [3558/7635], Loss: 4.3922\n",
      "Epoch [1/1], Step [3559/7635], Loss: 4.2551\n",
      "Epoch [1/1], Step [3560/7635], Loss: 4.2542\n",
      "Epoch [1/1], Step [3561/7635], Loss: 4.3103\n",
      "Epoch [1/1], Step [3562/7635], Loss: 4.3528\n",
      "Epoch [1/1], Step [3563/7635], Loss: 4.2008\n",
      "Epoch [1/1], Step [3564/7635], Loss: 4.2383\n",
      "Epoch [1/1], Step [3565/7635], Loss: 4.2229\n",
      "Epoch [1/1], Step [3566/7635], Loss: 4.2617\n",
      "Epoch [1/1], Step [3567/7635], Loss: 4.2506\n",
      "Epoch [1/1], Step [3568/7635], Loss: 4.3244\n",
      "Epoch [1/1], Step [3569/7635], Loss: 4.2921\n",
      "Epoch [1/1], Step [3570/7635], Loss: 4.2671\n",
      "Epoch [1/1], Step [3571/7635], Loss: 4.3052\n",
      "Epoch [1/1], Step [3572/7635], Loss: 4.3290\n",
      "Epoch [1/1], Step [3573/7635], Loss: 4.2957\n",
      "Epoch [1/1], Step [3574/7635], Loss: 4.2496\n",
      "Epoch [1/1], Step [3575/7635], Loss: 4.1970\n",
      "Epoch [1/1], Step [3576/7635], Loss: 4.2957\n",
      "Epoch [1/1], Step [3577/7635], Loss: 4.1931\n",
      "Epoch [1/1], Step [3578/7635], Loss: 4.3479\n",
      "Epoch [1/1], Step [3579/7635], Loss: 4.2539\n",
      "Epoch [1/1], Step [3580/7635], Loss: 4.2590\n",
      "Epoch [1/1], Step [3581/7635], Loss: 4.3077\n",
      "Epoch [1/1], Step [3582/7635], Loss: 4.1968\n",
      "Epoch [1/1], Step [3583/7635], Loss: 4.2613\n",
      "Epoch [1/1], Step [3584/7635], Loss: 4.3063\n",
      "Epoch [1/1], Step [3585/7635], Loss: 4.2759\n",
      "Epoch [1/1], Step [3586/7635], Loss: 4.2432\n",
      "Epoch [1/1], Step [3587/7635], Loss: 4.3166\n",
      "Epoch [1/1], Step [3588/7635], Loss: 4.2468\n",
      "Epoch [1/1], Step [3589/7635], Loss: 4.1961\n",
      "Epoch [1/1], Step [3590/7635], Loss: 4.2919\n",
      "Epoch [1/1], Step [3591/7635], Loss: 4.3119\n",
      "Epoch [1/1], Step [3592/7635], Loss: 4.2718\n",
      "Epoch [1/1], Step [3593/7635], Loss: 4.2655\n",
      "Epoch [1/1], Step [3594/7635], Loss: 4.3009\n",
      "Epoch [1/1], Step [3595/7635], Loss: 4.3458\n",
      "Epoch [1/1], Step [3596/7635], Loss: 4.3572\n",
      "Epoch [1/1], Step [3597/7635], Loss: 4.3381\n",
      "Epoch [1/1], Step [3598/7635], Loss: 4.3561\n",
      "Epoch [1/1], Step [3599/7635], Loss: 4.3314\n",
      "Epoch [1/1], Step [3600/7635], Loss: 4.2930\n",
      "Epoch [1/1], Step [3601/7635], Loss: 4.2836\n",
      "Epoch [1/1], Step [3602/7635], Loss: 4.2960\n",
      "Epoch [1/1], Step [3603/7635], Loss: 4.3351\n",
      "Epoch [1/1], Step [3604/7635], Loss: 4.2218\n",
      "Epoch [1/1], Step [3605/7635], Loss: 4.3820\n",
      "Epoch [1/1], Step [3606/7635], Loss: 4.2632\n",
      "Epoch [1/1], Step [3607/7635], Loss: 4.3050\n",
      "Epoch [1/1], Step [3608/7635], Loss: 4.2617\n",
      "Epoch [1/1], Step [3609/7635], Loss: 4.2073\n",
      "Epoch [1/1], Step [3610/7635], Loss: 4.2693\n",
      "Epoch [1/1], Step [3611/7635], Loss: 4.3038\n",
      "Epoch [1/1], Step [3612/7635], Loss: 4.3079\n",
      "Epoch [1/1], Step [3613/7635], Loss: 4.1973\n",
      "Epoch [1/1], Step [3614/7635], Loss: 4.2781\n",
      "Epoch [1/1], Step [3615/7635], Loss: 4.2561\n",
      "Epoch [1/1], Step [3616/7635], Loss: 4.2525\n",
      "Epoch [1/1], Step [3617/7635], Loss: 4.2455\n",
      "Epoch [1/1], Step [3618/7635], Loss: 4.2694\n",
      "Epoch [1/1], Step [3619/7635], Loss: 4.2842\n",
      "Epoch [1/1], Step [3620/7635], Loss: 4.2779\n",
      "Epoch [1/1], Step [3621/7635], Loss: 4.1952\n",
      "Epoch [1/1], Step [3622/7635], Loss: 4.3116\n",
      "Epoch [1/1], Step [3623/7635], Loss: 4.2876\n",
      "Epoch [1/1], Step [3624/7635], Loss: 4.2290\n",
      "Epoch [1/1], Step [3625/7635], Loss: 4.2655\n",
      "Epoch [1/1], Step [3626/7635], Loss: 4.3030\n",
      "Epoch [1/1], Step [3627/7635], Loss: 4.1338\n",
      "Epoch [1/1], Step [3628/7635], Loss: 4.2388\n",
      "Epoch [1/1], Step [3629/7635], Loss: 4.2411\n",
      "Epoch [1/1], Step [3630/7635], Loss: 4.1968\n",
      "Epoch [1/1], Step [3631/7635], Loss: 4.2219\n",
      "Epoch [1/1], Step [3632/7635], Loss: 4.2820\n",
      "Epoch [1/1], Step [3633/7635], Loss: 4.3251\n",
      "Epoch [1/1], Step [3634/7635], Loss: 4.3685\n",
      "Epoch [1/1], Step [3635/7635], Loss: 4.2547\n",
      "Epoch [1/1], Step [3636/7635], Loss: 4.2908\n",
      "Epoch [1/1], Step [3637/7635], Loss: 4.2770\n",
      "Epoch [1/1], Step [3638/7635], Loss: 4.2352\n",
      "Epoch [1/1], Step [3639/7635], Loss: 4.3175\n",
      "Epoch [1/1], Step [3640/7635], Loss: 4.2053\n",
      "Epoch [1/1], Step [3641/7635], Loss: 4.3094\n",
      "Epoch [1/1], Step [3642/7635], Loss: 4.3048\n",
      "Epoch [1/1], Step [3643/7635], Loss: 4.2414\n",
      "Epoch [1/1], Step [3644/7635], Loss: 4.2465\n",
      "Epoch [1/1], Step [3645/7635], Loss: 4.2712\n",
      "Epoch [1/1], Step [3646/7635], Loss: 4.2554\n",
      "Epoch [1/1], Step [3647/7635], Loss: 4.2013\n",
      "Epoch [1/1], Step [3648/7635], Loss: 4.2668\n",
      "Epoch [1/1], Step [3649/7635], Loss: 4.2690\n",
      "Epoch [1/1], Step [3650/7635], Loss: 4.3550\n",
      "Epoch [1/1], Step [3651/7635], Loss: 4.1721\n",
      "Epoch [1/1], Step [3652/7635], Loss: 4.4158\n",
      "Epoch [1/1], Step [3653/7635], Loss: 4.2074\n",
      "Epoch [1/1], Step [3654/7635], Loss: 4.2858\n",
      "Epoch [1/1], Step [3655/7635], Loss: 4.2765\n",
      "Epoch [1/1], Step [3656/7635], Loss: 4.2584\n",
      "Epoch [1/1], Step [3657/7635], Loss: 4.2497\n",
      "Epoch [1/1], Step [3658/7635], Loss: 4.3088\n",
      "Epoch [1/1], Step [3659/7635], Loss: 4.2296\n",
      "Epoch [1/1], Step [3660/7635], Loss: 4.3208\n",
      "Epoch [1/1], Step [3661/7635], Loss: 4.2643\n",
      "Epoch [1/1], Step [3662/7635], Loss: 4.3047\n",
      "Epoch [1/1], Step [3663/7635], Loss: 4.2785\n",
      "Epoch [1/1], Step [3664/7635], Loss: 4.1833\n",
      "Epoch [1/1], Step [3665/7635], Loss: 4.2787\n",
      "Epoch [1/1], Step [3666/7635], Loss: 4.3442\n",
      "Epoch [1/1], Step [3667/7635], Loss: 4.3065\n",
      "Epoch [1/1], Step [3668/7635], Loss: 4.2292\n",
      "Epoch [1/1], Step [3669/7635], Loss: 4.2955\n",
      "Epoch [1/1], Step [3670/7635], Loss: 4.2827\n",
      "Epoch [1/1], Step [3671/7635], Loss: 4.2347\n",
      "Epoch [1/1], Step [3672/7635], Loss: 4.3170\n",
      "Epoch [1/1], Step [3673/7635], Loss: 4.1891\n",
      "Epoch [1/1], Step [3674/7635], Loss: 4.2770\n",
      "Epoch [1/1], Step [3675/7635], Loss: 4.2246\n",
      "Epoch [1/1], Step [3676/7635], Loss: 4.2595\n",
      "Epoch [1/1], Step [3677/7635], Loss: 4.2526\n",
      "Epoch [1/1], Step [3678/7635], Loss: 4.2766\n",
      "Epoch [1/1], Step [3679/7635], Loss: 4.3412\n",
      "Epoch [1/1], Step [3680/7635], Loss: 4.3061\n",
      "Epoch [1/1], Step [3681/7635], Loss: 4.2181\n",
      "Epoch [1/1], Step [3682/7635], Loss: 4.2527\n",
      "Epoch [1/1], Step [3683/7635], Loss: 4.2925\n",
      "Epoch [1/1], Step [3684/7635], Loss: 4.2584\n",
      "Epoch [1/1], Step [3685/7635], Loss: 4.2470\n",
      "Epoch [1/1], Step [3686/7635], Loss: 4.3201\n",
      "Epoch [1/1], Step [3687/7635], Loss: 4.2808\n",
      "Epoch [1/1], Step [3688/7635], Loss: 4.2222\n",
      "Epoch [1/1], Step [3689/7635], Loss: 4.1564\n",
      "Epoch [1/1], Step [3690/7635], Loss: 4.2874\n",
      "Epoch [1/1], Step [3691/7635], Loss: 4.3095\n",
      "Epoch [1/1], Step [3692/7635], Loss: 4.3095\n",
      "Epoch [1/1], Step [3693/7635], Loss: 4.2553\n",
      "Epoch [1/1], Step [3694/7635], Loss: 4.3059\n",
      "Epoch [1/1], Step [3695/7635], Loss: 4.2673\n",
      "Epoch [1/1], Step [3696/7635], Loss: 4.2646\n",
      "Epoch [1/1], Step [3697/7635], Loss: 4.2287\n",
      "Epoch [1/1], Step [3698/7635], Loss: 4.2625\n",
      "Epoch [1/1], Step [3699/7635], Loss: 4.2876\n",
      "Epoch [1/1], Step [3700/7635], Loss: 4.3135\n",
      "Epoch [1/1], Step [3701/7635], Loss: 4.2271\n",
      "Epoch [1/1], Step [3702/7635], Loss: 4.2524\n",
      "Epoch [1/1], Step [3703/7635], Loss: 4.2043\n",
      "Epoch [1/1], Step [3704/7635], Loss: 4.2983\n",
      "Epoch [1/1], Step [3705/7635], Loss: 4.3423\n",
      "Epoch [1/1], Step [3706/7635], Loss: 4.2250\n",
      "Epoch [1/1], Step [3707/7635], Loss: 4.2457\n",
      "Epoch [1/1], Step [3708/7635], Loss: 4.2987\n",
      "Epoch [1/1], Step [3709/7635], Loss: 4.2502\n",
      "Epoch [1/1], Step [3710/7635], Loss: 4.3367\n",
      "Epoch [1/1], Step [3711/7635], Loss: 4.3296\n",
      "Epoch [1/1], Step [3712/7635], Loss: 4.3299\n",
      "Epoch [1/1], Step [3713/7635], Loss: 4.2661\n",
      "Epoch [1/1], Step [3714/7635], Loss: 4.1828\n",
      "Epoch [1/1], Step [3715/7635], Loss: 4.3194\n",
      "Epoch [1/1], Step [3716/7635], Loss: 4.3155\n",
      "Epoch [1/1], Step [3717/7635], Loss: 4.3019\n",
      "Epoch [1/1], Step [3718/7635], Loss: 4.3216\n",
      "Epoch [1/1], Step [3719/7635], Loss: 4.3085\n",
      "Epoch [1/1], Step [3720/7635], Loss: 4.2773\n",
      "Epoch [1/1], Step [3721/7635], Loss: 4.3454\n",
      "Epoch [1/1], Step [3722/7635], Loss: 4.2281\n",
      "Epoch [1/1], Step [3723/7635], Loss: 4.2363\n",
      "Epoch [1/1], Step [3724/7635], Loss: 4.2065\n",
      "Epoch [1/1], Step [3725/7635], Loss: 4.2611\n",
      "Epoch [1/1], Step [3726/7635], Loss: 4.2443\n",
      "Epoch [1/1], Step [3727/7635], Loss: 4.2600\n",
      "Epoch [1/1], Step [3728/7635], Loss: 4.3465\n",
      "Epoch [1/1], Step [3729/7635], Loss: 4.2456\n",
      "Epoch [1/1], Step [3730/7635], Loss: 4.2573\n",
      "Epoch [1/1], Step [3731/7635], Loss: 4.1961\n",
      "Epoch [1/1], Step [3732/7635], Loss: 4.2486\n",
      "Epoch [1/1], Step [3733/7635], Loss: 4.1575\n",
      "Epoch [1/1], Step [3734/7635], Loss: 4.2741\n",
      "Epoch [1/1], Step [3735/7635], Loss: 4.2233\n",
      "Epoch [1/1], Step [3736/7635], Loss: 4.2690\n",
      "Epoch [1/1], Step [3737/7635], Loss: 4.2690\n",
      "Epoch [1/1], Step [3738/7635], Loss: 4.2742\n",
      "Epoch [1/1], Step [3739/7635], Loss: 4.2250\n",
      "Epoch [1/1], Step [3740/7635], Loss: 4.2379\n",
      "Epoch [1/1], Step [3741/7635], Loss: 4.3207\n",
      "Epoch [1/1], Step [3742/7635], Loss: 4.2453\n",
      "Epoch [1/1], Step [3743/7635], Loss: 4.2347\n",
      "Epoch [1/1], Step [3744/7635], Loss: 4.1959\n",
      "Epoch [1/1], Step [3745/7635], Loss: 4.2370\n",
      "Epoch [1/1], Step [3746/7635], Loss: 4.2255\n",
      "Epoch [1/1], Step [3747/7635], Loss: 4.2488\n",
      "Epoch [1/1], Step [3748/7635], Loss: 4.2068\n",
      "Epoch [1/1], Step [3749/7635], Loss: 4.2161\n",
      "Epoch [1/1], Step [3750/7635], Loss: 4.2946\n",
      "Epoch [1/1], Step [3751/7635], Loss: 4.2727\n",
      "Epoch [1/1], Step [3752/7635], Loss: 4.2417\n",
      "Epoch [1/1], Step [3753/7635], Loss: 4.3382\n",
      "Epoch [1/1], Step [3754/7635], Loss: 4.1496\n",
      "Epoch [1/1], Step [3755/7635], Loss: 4.2084\n",
      "Epoch [1/1], Step [3756/7635], Loss: 4.2629\n",
      "Epoch [1/1], Step [3757/7635], Loss: 4.2436\n",
      "Epoch [1/1], Step [3758/7635], Loss: 4.1852\n",
      "Epoch [1/1], Step [3759/7635], Loss: 4.2578\n",
      "Epoch [1/1], Step [3760/7635], Loss: 4.3085\n",
      "Epoch [1/1], Step [3761/7635], Loss: 4.2783\n",
      "Epoch [1/1], Step [3762/7635], Loss: 4.3441\n",
      "Epoch [1/1], Step [3763/7635], Loss: 4.2461\n",
      "Epoch [1/1], Step [3764/7635], Loss: 4.2317\n",
      "Epoch [1/1], Step [3765/7635], Loss: 4.3040\n",
      "Epoch [1/1], Step [3766/7635], Loss: 4.2888\n",
      "Epoch [1/1], Step [3767/7635], Loss: 4.3851\n",
      "Epoch [1/1], Step [3768/7635], Loss: 4.3465\n",
      "Epoch [1/1], Step [3769/7635], Loss: 4.3719\n",
      "Epoch [1/1], Step [3770/7635], Loss: 4.2519\n",
      "Epoch [1/1], Step [3771/7635], Loss: 4.2553\n",
      "Epoch [1/1], Step [3772/7635], Loss: 4.3337\n",
      "Epoch [1/1], Step [3773/7635], Loss: 4.2589\n",
      "Epoch [1/1], Step [3774/7635], Loss: 4.1974\n",
      "Epoch [1/1], Step [3775/7635], Loss: 4.2748\n",
      "Epoch [1/1], Step [3776/7635], Loss: 4.3298\n",
      "Epoch [1/1], Step [3777/7635], Loss: 4.3024\n",
      "Epoch [1/1], Step [3778/7635], Loss: 4.1668\n",
      "Epoch [1/1], Step [3779/7635], Loss: 4.3095\n",
      "Epoch [1/1], Step [3780/7635], Loss: 4.2425\n",
      "Epoch [1/1], Step [3781/7635], Loss: 4.2736\n",
      "Epoch [1/1], Step [3782/7635], Loss: 4.1965\n",
      "Epoch [1/1], Step [3783/7635], Loss: 4.2818\n",
      "Epoch [1/1], Step [3784/7635], Loss: 4.2612\n",
      "Epoch [1/1], Step [3785/7635], Loss: 4.2986\n",
      "Epoch [1/1], Step [3786/7635], Loss: 4.2621\n",
      "Epoch [1/1], Step [3787/7635], Loss: 4.1925\n",
      "Epoch [1/1], Step [3788/7635], Loss: 4.2660\n",
      "Epoch [1/1], Step [3789/7635], Loss: 4.2179\n",
      "Epoch [1/1], Step [3790/7635], Loss: 4.2382\n",
      "Epoch [1/1], Step [3791/7635], Loss: 4.2796\n",
      "Epoch [1/1], Step [3792/7635], Loss: 4.2601\n",
      "Epoch [1/1], Step [3793/7635], Loss: 4.1854\n",
      "Epoch [1/1], Step [3794/7635], Loss: 4.3915\n",
      "Epoch [1/1], Step [3795/7635], Loss: 4.1902\n",
      "Epoch [1/1], Step [3796/7635], Loss: 4.2688\n",
      "Epoch [1/1], Step [3797/7635], Loss: 4.2606\n",
      "Epoch [1/1], Step [3798/7635], Loss: 4.2897\n",
      "Epoch [1/1], Step [3799/7635], Loss: 4.3455\n",
      "Epoch [1/1], Step [3800/7635], Loss: 4.3962\n",
      "Epoch [1/1], Step [3801/7635], Loss: 4.2988\n",
      "Epoch [1/1], Step [3802/7635], Loss: 4.3012\n",
      "Epoch [1/1], Step [3803/7635], Loss: 4.2056\n",
      "Epoch [1/1], Step [3804/7635], Loss: 4.2209\n",
      "Epoch [1/1], Step [3805/7635], Loss: 4.2933\n",
      "Epoch [1/1], Step [3806/7635], Loss: 4.2686\n",
      "Epoch [1/1], Step [3807/7635], Loss: 4.3032\n",
      "Epoch [1/1], Step [3808/7635], Loss: 4.2922\n",
      "Epoch [1/1], Step [3809/7635], Loss: 4.2291\n",
      "Epoch [1/1], Step [3810/7635], Loss: 4.2862\n",
      "Epoch [1/1], Step [3811/7635], Loss: 4.1679\n",
      "Epoch [1/1], Step [3812/7635], Loss: 4.2367\n",
      "Epoch [1/1], Step [3813/7635], Loss: 4.2245\n",
      "Epoch [1/1], Step [3814/7635], Loss: 4.1943\n",
      "Epoch [1/1], Step [3815/7635], Loss: 4.3438\n",
      "Epoch [1/1], Step [3816/7635], Loss: 4.2606\n",
      "Epoch [1/1], Step [3817/7635], Loss: 4.2458\n",
      "Epoch [1/1], Step [3818/7635], Loss: 4.2533\n",
      "Epoch [1/1], Step [3819/7635], Loss: 4.2264\n",
      "Epoch [1/1], Step [3820/7635], Loss: 4.2743\n",
      "Epoch [1/1], Step [3821/7635], Loss: 4.2307\n",
      "Epoch [1/1], Step [3822/7635], Loss: 4.2531\n",
      "Epoch [1/1], Step [3823/7635], Loss: 4.2514\n",
      "Epoch [1/1], Step [3824/7635], Loss: 4.2541\n",
      "Epoch [1/1], Step [3825/7635], Loss: 4.2407\n",
      "Epoch [1/1], Step [3826/7635], Loss: 4.2511\n",
      "Epoch [1/1], Step [3827/7635], Loss: 4.2305\n",
      "Epoch [1/1], Step [3828/7635], Loss: 4.1934\n",
      "Epoch [1/1], Step [3829/7635], Loss: 4.2485\n",
      "Epoch [1/1], Step [3830/7635], Loss: 4.2659\n",
      "Epoch [1/1], Step [3831/7635], Loss: 4.2018\n",
      "Epoch [1/1], Step [3832/7635], Loss: 4.2904\n",
      "Epoch [1/1], Step [3833/7635], Loss: 4.2248\n",
      "Epoch [1/1], Step [3834/7635], Loss: 4.2317\n",
      "Epoch [1/1], Step [3835/7635], Loss: 4.2409\n",
      "Epoch [1/1], Step [3836/7635], Loss: 4.2866\n",
      "Epoch [1/1], Step [3837/7635], Loss: 4.2901\n",
      "Epoch [1/1], Step [3838/7635], Loss: 4.2342\n",
      "Epoch [1/1], Step [3839/7635], Loss: 4.2504\n",
      "Epoch [1/1], Step [3840/7635], Loss: 4.2929\n",
      "Epoch [1/1], Step [3841/7635], Loss: 4.2559\n",
      "Epoch [1/1], Step [3842/7635], Loss: 4.2517\n",
      "Epoch [1/1], Step [3843/7635], Loss: 4.2326\n",
      "Epoch [1/1], Step [3844/7635], Loss: 4.2836\n",
      "Epoch [1/1], Step [3845/7635], Loss: 4.2340\n",
      "Epoch [1/1], Step [3846/7635], Loss: 4.3506\n",
      "Epoch [1/1], Step [3847/7635], Loss: 4.3042\n",
      "Epoch [1/1], Step [3848/7635], Loss: 4.2536\n",
      "Epoch [1/1], Step [3849/7635], Loss: 4.2411\n",
      "Epoch [1/1], Step [3850/7635], Loss: 4.2634\n",
      "Epoch [1/1], Step [3851/7635], Loss: 4.2589\n",
      "Epoch [1/1], Step [3852/7635], Loss: 4.2098\n",
      "Epoch [1/1], Step [3853/7635], Loss: 4.2174\n",
      "Epoch [1/1], Step [3854/7635], Loss: 4.2608\n",
      "Epoch [1/1], Step [3855/7635], Loss: 4.2445\n",
      "Epoch [1/1], Step [3856/7635], Loss: 4.1906\n",
      "Epoch [1/1], Step [3857/7635], Loss: 4.2832\n",
      "Epoch [1/1], Step [3858/7635], Loss: 4.2303\n",
      "Epoch [1/1], Step [3859/7635], Loss: 4.2497\n",
      "Epoch [1/1], Step [3860/7635], Loss: 4.2387\n",
      "Epoch [1/1], Step [3861/7635], Loss: 4.2843\n",
      "Epoch [1/1], Step [3862/7635], Loss: 4.2255\n",
      "Epoch [1/1], Step [3863/7635], Loss: 4.2684\n",
      "Epoch [1/1], Step [3864/7635], Loss: 4.2291\n",
      "Epoch [1/1], Step [3865/7635], Loss: 4.2229\n",
      "Epoch [1/1], Step [3866/7635], Loss: 4.2793\n",
      "Epoch [1/1], Step [3867/7635], Loss: 4.1643\n",
      "Epoch [1/1], Step [3868/7635], Loss: 4.3304\n",
      "Epoch [1/1], Step [3869/7635], Loss: 4.2466\n",
      "Epoch [1/1], Step [3870/7635], Loss: 4.3160\n",
      "Epoch [1/1], Step [3871/7635], Loss: 4.2745\n",
      "Epoch [1/1], Step [3872/7635], Loss: 4.1787\n",
      "Epoch [1/1], Step [3873/7635], Loss: 4.1949\n",
      "Epoch [1/1], Step [3874/7635], Loss: 4.3198\n",
      "Epoch [1/1], Step [3875/7635], Loss: 4.2469\n",
      "Epoch [1/1], Step [3876/7635], Loss: 4.2912\n",
      "Epoch [1/1], Step [3877/7635], Loss: 4.2929\n",
      "Epoch [1/1], Step [3878/7635], Loss: 4.2343\n",
      "Epoch [1/1], Step [3879/7635], Loss: 4.3000\n",
      "Epoch [1/1], Step [3880/7635], Loss: 4.3131\n",
      "Epoch [1/1], Step [3881/7635], Loss: 4.3271\n",
      "Epoch [1/1], Step [3882/7635], Loss: 4.2184\n",
      "Epoch [1/1], Step [3883/7635], Loss: 4.1364\n",
      "Epoch [1/1], Step [3884/7635], Loss: 4.2475\n",
      "Epoch [1/1], Step [3885/7635], Loss: 4.1988\n",
      "Epoch [1/1], Step [3886/7635], Loss: 4.2397\n",
      "Epoch [1/1], Step [3887/7635], Loss: 4.2588\n",
      "Epoch [1/1], Step [3888/7635], Loss: 4.2958\n",
      "Epoch [1/1], Step [3889/7635], Loss: 4.1731\n",
      "Epoch [1/1], Step [3890/7635], Loss: 4.2932\n",
      "Epoch [1/1], Step [3891/7635], Loss: 4.2411\n",
      "Epoch [1/1], Step [3892/7635], Loss: 4.2718\n",
      "Epoch [1/1], Step [3893/7635], Loss: 4.2858\n",
      "Epoch [1/1], Step [3894/7635], Loss: 4.3186\n",
      "Epoch [1/1], Step [3895/7635], Loss: 4.1808\n",
      "Epoch [1/1], Step [3896/7635], Loss: 4.3287\n",
      "Epoch [1/1], Step [3897/7635], Loss: 4.1978\n",
      "Epoch [1/1], Step [3898/7635], Loss: 4.2405\n",
      "Epoch [1/1], Step [3899/7635], Loss: 4.2258\n",
      "Epoch [1/1], Step [3900/7635], Loss: 4.2844\n",
      "Epoch [1/1], Step [3901/7635], Loss: 4.3714\n",
      "Epoch [1/1], Step [3902/7635], Loss: 4.2589\n",
      "Epoch [1/1], Step [3903/7635], Loss: 4.1722\n",
      "Epoch [1/1], Step [3904/7635], Loss: 4.3169\n",
      "Epoch [1/1], Step [3905/7635], Loss: 4.2350\n",
      "Epoch [1/1], Step [3906/7635], Loss: 4.2925\n",
      "Epoch [1/1], Step [3907/7635], Loss: 4.2628\n",
      "Epoch [1/1], Step [3908/7635], Loss: 4.2392\n",
      "Epoch [1/1], Step [3909/7635], Loss: 4.2917\n",
      "Epoch [1/1], Step [3910/7635], Loss: 4.1938\n",
      "Epoch [1/1], Step [3911/7635], Loss: 4.2715\n",
      "Epoch [1/1], Step [3912/7635], Loss: 4.1882\n",
      "Epoch [1/1], Step [3913/7635], Loss: 4.1952\n",
      "Epoch [1/1], Step [3914/7635], Loss: 4.1894\n",
      "Epoch [1/1], Step [3915/7635], Loss: 4.2359\n",
      "Epoch [1/1], Step [3916/7635], Loss: 4.2518\n",
      "Epoch [1/1], Step [3917/7635], Loss: 4.2517\n",
      "Epoch [1/1], Step [3918/7635], Loss: 4.2234\n",
      "Epoch [1/1], Step [3919/7635], Loss: 4.2911\n",
      "Epoch [1/1], Step [3920/7635], Loss: 4.2069\n",
      "Epoch [1/1], Step [3921/7635], Loss: 4.2533\n",
      "Epoch [1/1], Step [3922/7635], Loss: 4.2778\n",
      "Epoch [1/1], Step [3923/7635], Loss: 4.2800\n",
      "Epoch [1/1], Step [3924/7635], Loss: 4.2739\n",
      "Epoch [1/1], Step [3925/7635], Loss: 4.2001\n",
      "Epoch [1/1], Step [3926/7635], Loss: 4.2679\n",
      "Epoch [1/1], Step [3927/7635], Loss: 4.2079\n",
      "Epoch [1/1], Step [3928/7635], Loss: 4.3375\n",
      "Epoch [1/1], Step [3929/7635], Loss: 4.2581\n",
      "Epoch [1/1], Step [3930/7635], Loss: 4.1644\n",
      "Epoch [1/1], Step [3931/7635], Loss: 4.1949\n",
      "Epoch [1/1], Step [3932/7635], Loss: 4.3427\n",
      "Epoch [1/1], Step [3933/7635], Loss: 4.2083\n",
      "Epoch [1/1], Step [3934/7635], Loss: 4.2701\n",
      "Epoch [1/1], Step [3935/7635], Loss: 4.2855\n",
      "Epoch [1/1], Step [3936/7635], Loss: 4.3501\n",
      "Epoch [1/1], Step [3937/7635], Loss: 4.2491\n",
      "Epoch [1/1], Step [3938/7635], Loss: 4.2649\n",
      "Epoch [1/1], Step [3939/7635], Loss: 4.1481\n",
      "Epoch [1/1], Step [3940/7635], Loss: 4.2548\n",
      "Epoch [1/1], Step [3941/7635], Loss: 4.2931\n",
      "Epoch [1/1], Step [3942/7635], Loss: 4.2608\n",
      "Epoch [1/1], Step [3943/7635], Loss: 4.2020\n",
      "Epoch [1/1], Step [3944/7635], Loss: 4.2230\n",
      "Epoch [1/1], Step [3945/7635], Loss: 4.3332\n",
      "Epoch [1/1], Step [3946/7635], Loss: 4.1964\n",
      "Epoch [1/1], Step [3947/7635], Loss: 4.2308\n",
      "Epoch [1/1], Step [3948/7635], Loss: 4.2085\n",
      "Epoch [1/1], Step [3949/7635], Loss: 4.2559\n",
      "Epoch [1/1], Step [3950/7635], Loss: 4.2805\n",
      "Epoch [1/1], Step [3951/7635], Loss: 4.2161\n",
      "Epoch [1/1], Step [3952/7635], Loss: 4.3162\n",
      "Epoch [1/1], Step [3953/7635], Loss: 4.2832\n",
      "Epoch [1/1], Step [3954/7635], Loss: 4.2703\n",
      "Epoch [1/1], Step [3955/7635], Loss: 4.2314\n",
      "Epoch [1/1], Step [3956/7635], Loss: 4.2908\n",
      "Epoch [1/1], Step [3957/7635], Loss: 4.2285\n",
      "Epoch [1/1], Step [3958/7635], Loss: 4.3129\n",
      "Epoch [1/1], Step [3959/7635], Loss: 4.2934\n",
      "Epoch [1/1], Step [3960/7635], Loss: 4.2065\n",
      "Epoch [1/1], Step [3961/7635], Loss: 4.1982\n",
      "Epoch [1/1], Step [3962/7635], Loss: 4.2539\n",
      "Epoch [1/1], Step [3963/7635], Loss: 4.1800\n",
      "Epoch [1/1], Step [3964/7635], Loss: 4.3097\n",
      "Epoch [1/1], Step [3965/7635], Loss: 4.2578\n",
      "Epoch [1/1], Step [3966/7635], Loss: 4.2795\n",
      "Epoch [1/1], Step [3967/7635], Loss: 4.1673\n",
      "Epoch [1/1], Step [3968/7635], Loss: 4.2398\n",
      "Epoch [1/1], Step [3969/7635], Loss: 4.1537\n",
      "Epoch [1/1], Step [3970/7635], Loss: 4.1865\n",
      "Epoch [1/1], Step [3971/7635], Loss: 4.1995\n",
      "Epoch [1/1], Step [3972/7635], Loss: 4.2151\n",
      "Epoch [1/1], Step [3973/7635], Loss: 4.3442\n",
      "Epoch [1/1], Step [3974/7635], Loss: 4.2755\n",
      "Epoch [1/1], Step [3975/7635], Loss: 4.2484\n",
      "Epoch [1/1], Step [3976/7635], Loss: 4.3091\n",
      "Epoch [1/1], Step [3977/7635], Loss: 4.1969\n",
      "Epoch [1/1], Step [3978/7635], Loss: 4.2121\n",
      "Epoch [1/1], Step [3979/7635], Loss: 4.2002\n",
      "Epoch [1/1], Step [3980/7635], Loss: 4.2445\n",
      "Epoch [1/1], Step [3981/7635], Loss: 4.2099\n",
      "Epoch [1/1], Step [3982/7635], Loss: 4.2270\n",
      "Epoch [1/1], Step [3983/7635], Loss: 4.2748\n",
      "Epoch [1/1], Step [3984/7635], Loss: 4.2556\n",
      "Epoch [1/1], Step [3985/7635], Loss: 4.2184\n",
      "Epoch [1/1], Step [3986/7635], Loss: 4.2603\n",
      "Epoch [1/1], Step [3987/7635], Loss: 4.2723\n",
      "Epoch [1/1], Step [3988/7635], Loss: 4.2202\n",
      "Epoch [1/1], Step [3989/7635], Loss: 4.2831\n",
      "Epoch [1/1], Step [3990/7635], Loss: 4.1674\n",
      "Epoch [1/1], Step [3991/7635], Loss: 4.2210\n",
      "Epoch [1/1], Step [3992/7635], Loss: 4.3279\n",
      "Epoch [1/1], Step [3993/7635], Loss: 4.3119\n",
      "Epoch [1/1], Step [3994/7635], Loss: 4.2347\n",
      "Epoch [1/1], Step [3995/7635], Loss: 4.2503\n",
      "Epoch [1/1], Step [3996/7635], Loss: 4.3513\n",
      "Epoch [1/1], Step [3997/7635], Loss: 4.3521\n",
      "Epoch [1/1], Step [3998/7635], Loss: 4.2631\n",
      "Epoch [1/1], Step [3999/7635], Loss: 4.2340\n",
      "Epoch [1/1], Step [4000/7635], Loss: 4.2750\n",
      "Epoch [1/1], Step [4001/7635], Loss: 4.2849\n",
      "Epoch [1/1], Step [4002/7635], Loss: 4.2906\n",
      "Epoch [1/1], Step [4003/7635], Loss: 4.3091\n",
      "Epoch [1/1], Step [4004/7635], Loss: 4.2977\n",
      "Epoch [1/1], Step [4005/7635], Loss: 4.2524\n",
      "Epoch [1/1], Step [4006/7635], Loss: 4.1973\n",
      "Epoch [1/1], Step [4007/7635], Loss: 4.2237\n",
      "Epoch [1/1], Step [4008/7635], Loss: 4.4026\n",
      "Epoch [1/1], Step [4009/7635], Loss: 4.2085\n",
      "Epoch [1/1], Step [4010/7635], Loss: 4.1779\n",
      "Epoch [1/1], Step [4011/7635], Loss: 4.1992\n",
      "Epoch [1/1], Step [4012/7635], Loss: 4.1729\n",
      "Epoch [1/1], Step [4013/7635], Loss: 4.2622\n",
      "Epoch [1/1], Step [4014/7635], Loss: 4.2906\n",
      "Epoch [1/1], Step [4015/7635], Loss: 4.1902\n",
      "Epoch [1/1], Step [4016/7635], Loss: 4.2175\n",
      "Epoch [1/1], Step [4017/7635], Loss: 4.2850\n",
      "Epoch [1/1], Step [4018/7635], Loss: 4.2241\n",
      "Epoch [1/1], Step [4019/7635], Loss: 4.2218\n",
      "Epoch [1/1], Step [4020/7635], Loss: 4.1702\n",
      "Epoch [1/1], Step [4021/7635], Loss: 4.2698\n",
      "Epoch [1/1], Step [4022/7635], Loss: 4.2480\n",
      "Epoch [1/1], Step [4023/7635], Loss: 4.3419\n",
      "Epoch [1/1], Step [4024/7635], Loss: 4.2771\n",
      "Epoch [1/1], Step [4025/7635], Loss: 4.1982\n",
      "Epoch [1/1], Step [4026/7635], Loss: 4.2823\n",
      "Epoch [1/1], Step [4027/7635], Loss: 4.2258\n",
      "Epoch [1/1], Step [4028/7635], Loss: 4.2660\n",
      "Epoch [1/1], Step [4029/7635], Loss: 4.2878\n",
      "Epoch [1/1], Step [4030/7635], Loss: 4.2482\n",
      "Epoch [1/1], Step [4031/7635], Loss: 4.1793\n",
      "Epoch [1/1], Step [4032/7635], Loss: 4.1972\n",
      "Epoch [1/1], Step [4033/7635], Loss: 4.2747\n",
      "Epoch [1/1], Step [4034/7635], Loss: 4.1990\n",
      "Epoch [1/1], Step [4035/7635], Loss: 4.2801\n",
      "Epoch [1/1], Step [4036/7635], Loss: 4.3460\n",
      "Epoch [1/1], Step [4037/7635], Loss: 4.1597\n",
      "Epoch [1/1], Step [4038/7635], Loss: 4.3210\n",
      "Epoch [1/1], Step [4039/7635], Loss: 4.2275\n",
      "Epoch [1/1], Step [4040/7635], Loss: 4.1977\n",
      "Epoch [1/1], Step [4041/7635], Loss: 4.1785\n",
      "Epoch [1/1], Step [4042/7635], Loss: 4.2473\n",
      "Epoch [1/1], Step [4043/7635], Loss: 4.2602\n",
      "Epoch [1/1], Step [4044/7635], Loss: 4.2015\n",
      "Epoch [1/1], Step [4045/7635], Loss: 4.2441\n",
      "Epoch [1/1], Step [4046/7635], Loss: 4.2744\n",
      "Epoch [1/1], Step [4047/7635], Loss: 4.2663\n",
      "Epoch [1/1], Step [4048/7635], Loss: 4.1670\n",
      "Epoch [1/1], Step [4049/7635], Loss: 4.2895\n",
      "Epoch [1/1], Step [4050/7635], Loss: 4.2412\n",
      "Epoch [1/1], Step [4051/7635], Loss: 4.2567\n",
      "Epoch [1/1], Step [4052/7635], Loss: 4.2494\n",
      "Epoch [1/1], Step [4053/7635], Loss: 4.2012\n",
      "Epoch [1/1], Step [4054/7635], Loss: 4.2844\n",
      "Epoch [1/1], Step [4055/7635], Loss: 4.2728\n",
      "Epoch [1/1], Step [4056/7635], Loss: 4.2055\n",
      "Epoch [1/1], Step [4057/7635], Loss: 4.2461\n",
      "Epoch [1/1], Step [4058/7635], Loss: 4.2674\n",
      "Epoch [1/1], Step [4059/7635], Loss: 4.2220\n",
      "Epoch [1/1], Step [4060/7635], Loss: 4.2736\n",
      "Epoch [1/1], Step [4061/7635], Loss: 4.2574\n",
      "Epoch [1/1], Step [4062/7635], Loss: 4.1899\n",
      "Epoch [1/1], Step [4063/7635], Loss: 4.2217\n",
      "Epoch [1/1], Step [4064/7635], Loss: 4.3329\n",
      "Epoch [1/1], Step [4065/7635], Loss: 4.2027\n",
      "Epoch [1/1], Step [4066/7635], Loss: 4.2180\n",
      "Epoch [1/1], Step [4067/7635], Loss: 4.2512\n",
      "Epoch [1/1], Step [4068/7635], Loss: 4.1028\n",
      "Epoch [1/1], Step [4069/7635], Loss: 4.1838\n",
      "Epoch [1/1], Step [4070/7635], Loss: 4.1885\n",
      "Epoch [1/1], Step [4071/7635], Loss: 4.2481\n",
      "Epoch [1/1], Step [4072/7635], Loss: 4.1974\n",
      "Epoch [1/1], Step [4073/7635], Loss: 4.2797\n",
      "Epoch [1/1], Step [4074/7635], Loss: 4.2886\n",
      "Epoch [1/1], Step [4075/7635], Loss: 4.2675\n",
      "Epoch [1/1], Step [4076/7635], Loss: 4.2308\n",
      "Epoch [1/1], Step [4077/7635], Loss: 4.3331\n",
      "Epoch [1/1], Step [4078/7635], Loss: 4.2330\n",
      "Epoch [1/1], Step [4079/7635], Loss: 4.3353\n",
      "Epoch [1/1], Step [4080/7635], Loss: 4.3283\n",
      "Epoch [1/1], Step [4081/7635], Loss: 4.2396\n",
      "Epoch [1/1], Step [4082/7635], Loss: 4.2402\n",
      "Epoch [1/1], Step [4083/7635], Loss: 4.2493\n",
      "Epoch [1/1], Step [4084/7635], Loss: 4.2246\n",
      "Epoch [1/1], Step [4085/7635], Loss: 4.2493\n",
      "Epoch [1/1], Step [4086/7635], Loss: 4.2163\n",
      "Epoch [1/1], Step [4087/7635], Loss: 4.2102\n",
      "Epoch [1/1], Step [4088/7635], Loss: 4.3174\n",
      "Epoch [1/1], Step [4089/7635], Loss: 4.1992\n",
      "Epoch [1/1], Step [4090/7635], Loss: 4.1940\n",
      "Epoch [1/1], Step [4091/7635], Loss: 4.2041\n",
      "Epoch [1/1], Step [4092/7635], Loss: 4.2553\n",
      "Epoch [1/1], Step [4093/7635], Loss: 4.2365\n",
      "Epoch [1/1], Step [4094/7635], Loss: 4.3113\n",
      "Epoch [1/1], Step [4095/7635], Loss: 4.2543\n",
      "Epoch [1/1], Step [4096/7635], Loss: 4.2376\n",
      "Epoch [1/1], Step [4097/7635], Loss: 4.2645\n",
      "Epoch [1/1], Step [4098/7635], Loss: 4.2138\n",
      "Epoch [1/1], Step [4099/7635], Loss: 4.1786\n",
      "Epoch [1/1], Step [4100/7635], Loss: 4.2112\n",
      "Epoch [1/1], Step [4101/7635], Loss: 4.2970\n",
      "Epoch [1/1], Step [4102/7635], Loss: 4.2045\n",
      "Epoch [1/1], Step [4103/7635], Loss: 4.1771\n",
      "Epoch [1/1], Step [4104/7635], Loss: 4.2478\n",
      "Epoch [1/1], Step [4105/7635], Loss: 4.2993\n",
      "Epoch [1/1], Step [4106/7635], Loss: 4.1659\n",
      "Epoch [1/1], Step [4107/7635], Loss: 4.1797\n",
      "Epoch [1/1], Step [4108/7635], Loss: 4.2442\n",
      "Epoch [1/1], Step [4109/7635], Loss: 4.2609\n",
      "Epoch [1/1], Step [4110/7635], Loss: 4.3252\n",
      "Epoch [1/1], Step [4111/7635], Loss: 4.2143\n",
      "Epoch [1/1], Step [4112/7635], Loss: 4.2791\n",
      "Epoch [1/1], Step [4113/7635], Loss: 4.2053\n",
      "Epoch [1/1], Step [4114/7635], Loss: 4.1802\n",
      "Epoch [1/1], Step [4115/7635], Loss: 4.2819\n",
      "Epoch [1/1], Step [4116/7635], Loss: 4.2299\n",
      "Epoch [1/1], Step [4117/7635], Loss: 4.2298\n",
      "Epoch [1/1], Step [4118/7635], Loss: 4.2116\n",
      "Epoch [1/1], Step [4119/7635], Loss: 4.1691\n",
      "Epoch [1/1], Step [4120/7635], Loss: 4.2455\n",
      "Epoch [1/1], Step [4121/7635], Loss: 4.1758\n",
      "Epoch [1/1], Step [4122/7635], Loss: 4.2805\n",
      "Epoch [1/1], Step [4123/7635], Loss: 4.2234\n",
      "Epoch [1/1], Step [4124/7635], Loss: 4.2789\n",
      "Epoch [1/1], Step [4125/7635], Loss: 4.1935\n",
      "Epoch [1/1], Step [4126/7635], Loss: 4.2580\n",
      "Epoch [1/1], Step [4127/7635], Loss: 4.2435\n",
      "Epoch [1/1], Step [4128/7635], Loss: 4.1638\n",
      "Epoch [1/1], Step [4129/7635], Loss: 4.2044\n",
      "Epoch [1/1], Step [4130/7635], Loss: 4.2702\n",
      "Epoch [1/1], Step [4131/7635], Loss: 4.1475\n",
      "Epoch [1/1], Step [4132/7635], Loss: 4.2224\n",
      "Epoch [1/1], Step [4133/7635], Loss: 4.3060\n",
      "Epoch [1/1], Step [4134/7635], Loss: 4.2084\n",
      "Epoch [1/1], Step [4135/7635], Loss: 4.3110\n",
      "Epoch [1/1], Step [4136/7635], Loss: 4.2653\n",
      "Epoch [1/1], Step [4137/7635], Loss: 4.2111\n",
      "Epoch [1/1], Step [4138/7635], Loss: 4.1932\n",
      "Epoch [1/1], Step [4139/7635], Loss: 4.1952\n",
      "Epoch [1/1], Step [4140/7635], Loss: 4.2488\n",
      "Epoch [1/1], Step [4141/7635], Loss: 4.3990\n",
      "Epoch [1/1], Step [4142/7635], Loss: 4.2569\n",
      "Epoch [1/1], Step [4143/7635], Loss: 4.2329\n",
      "Epoch [1/1], Step [4144/7635], Loss: 4.2651\n",
      "Epoch [1/1], Step [4145/7635], Loss: 4.2357\n",
      "Epoch [1/1], Step [4146/7635], Loss: 4.2719\n",
      "Epoch [1/1], Step [4147/7635], Loss: 4.2837\n",
      "Epoch [1/1], Step [4148/7635], Loss: 4.2089\n",
      "Epoch [1/1], Step [4149/7635], Loss: 4.2225\n",
      "Epoch [1/1], Step [4150/7635], Loss: 4.2002\n",
      "Epoch [1/1], Step [4151/7635], Loss: 4.2694\n",
      "Epoch [1/1], Step [4152/7635], Loss: 4.2912\n",
      "Epoch [1/1], Step [4153/7635], Loss: 4.2049\n",
      "Epoch [1/1], Step [4154/7635], Loss: 4.2596\n",
      "Epoch [1/1], Step [4155/7635], Loss: 4.1952\n",
      "Epoch [1/1], Step [4156/7635], Loss: 4.1382\n",
      "Epoch [1/1], Step [4157/7635], Loss: 4.2442\n",
      "Epoch [1/1], Step [4158/7635], Loss: 4.2725\n",
      "Epoch [1/1], Step [4159/7635], Loss: 4.1958\n",
      "Epoch [1/1], Step [4160/7635], Loss: 4.2024\n",
      "Epoch [1/1], Step [4161/7635], Loss: 4.2065\n",
      "Epoch [1/1], Step [4162/7635], Loss: 4.1931\n",
      "Epoch [1/1], Step [4163/7635], Loss: 4.2682\n",
      "Epoch [1/1], Step [4164/7635], Loss: 4.2888\n",
      "Epoch [1/1], Step [4165/7635], Loss: 4.2777\n",
      "Epoch [1/1], Step [4166/7635], Loss: 4.2165\n",
      "Epoch [1/1], Step [4167/7635], Loss: 4.2384\n",
      "Epoch [1/1], Step [4168/7635], Loss: 4.2349\n",
      "Epoch [1/1], Step [4169/7635], Loss: 4.1543\n",
      "Epoch [1/1], Step [4170/7635], Loss: 4.3169\n",
      "Epoch [1/1], Step [4171/7635], Loss: 4.2817\n",
      "Epoch [1/1], Step [4172/7635], Loss: 4.2012\n",
      "Epoch [1/1], Step [4173/7635], Loss: 4.2160\n",
      "Epoch [1/1], Step [4174/7635], Loss: 4.3066\n",
      "Epoch [1/1], Step [4175/7635], Loss: 4.2785\n",
      "Epoch [1/1], Step [4176/7635], Loss: 4.2170\n",
      "Epoch [1/1], Step [4177/7635], Loss: 4.2219\n",
      "Epoch [1/1], Step [4178/7635], Loss: 4.2016\n",
      "Epoch [1/1], Step [4179/7635], Loss: 4.2385\n",
      "Epoch [1/1], Step [4180/7635], Loss: 4.2602\n",
      "Epoch [1/1], Step [4181/7635], Loss: 4.2442\n",
      "Epoch [1/1], Step [4182/7635], Loss: 4.1672\n",
      "Epoch [1/1], Step [4183/7635], Loss: 4.2813\n",
      "Epoch [1/1], Step [4184/7635], Loss: 4.3060\n",
      "Epoch [1/1], Step [4185/7635], Loss: 4.2382\n",
      "Epoch [1/1], Step [4186/7635], Loss: 4.2891\n",
      "Epoch [1/1], Step [4187/7635], Loss: 4.1809\n",
      "Epoch [1/1], Step [4188/7635], Loss: 4.2260\n",
      "Epoch [1/1], Step [4189/7635], Loss: 4.2820\n",
      "Epoch [1/1], Step [4190/7635], Loss: 4.2356\n",
      "Epoch [1/1], Step [4191/7635], Loss: 4.2340\n",
      "Epoch [1/1], Step [4192/7635], Loss: 4.2702\n",
      "Epoch [1/1], Step [4193/7635], Loss: 4.2136\n",
      "Epoch [1/1], Step [4194/7635], Loss: 4.2263\n",
      "Epoch [1/1], Step [4195/7635], Loss: 4.3064\n",
      "Epoch [1/1], Step [4196/7635], Loss: 4.3090\n",
      "Epoch [1/1], Step [4197/7635], Loss: 4.1876\n",
      "Epoch [1/1], Step [4198/7635], Loss: 4.3265\n",
      "Epoch [1/1], Step [4199/7635], Loss: 4.2632\n",
      "Epoch [1/1], Step [4200/7635], Loss: 4.2412\n",
      "Epoch [1/1], Step [4201/7635], Loss: 4.2312\n",
      "Epoch [1/1], Step [4202/7635], Loss: 4.2627\n",
      "Epoch [1/1], Step [4203/7635], Loss: 4.2188\n",
      "Epoch [1/1], Step [4204/7635], Loss: 4.2888\n",
      "Epoch [1/1], Step [4205/7635], Loss: 4.2000\n",
      "Epoch [1/1], Step [4206/7635], Loss: 4.2716\n",
      "Epoch [1/1], Step [4207/7635], Loss: 4.1432\n",
      "Epoch [1/1], Step [4208/7635], Loss: 4.2681\n",
      "Epoch [1/1], Step [4209/7635], Loss: 4.2606\n",
      "Epoch [1/1], Step [4210/7635], Loss: 4.2481\n",
      "Epoch [1/1], Step [4211/7635], Loss: 4.1860\n",
      "Epoch [1/1], Step [4212/7635], Loss: 4.2809\n",
      "Epoch [1/1], Step [4213/7635], Loss: 4.2016\n",
      "Epoch [1/1], Step [4214/7635], Loss: 4.3227\n",
      "Epoch [1/1], Step [4215/7635], Loss: 4.1360\n",
      "Epoch [1/1], Step [4216/7635], Loss: 4.1797\n",
      "Epoch [1/1], Step [4217/7635], Loss: 4.1605\n",
      "Epoch [1/1], Step [4218/7635], Loss: 4.2696\n",
      "Epoch [1/1], Step [4219/7635], Loss: 4.2662\n",
      "Epoch [1/1], Step [4220/7635], Loss: 4.2712\n",
      "Epoch [1/1], Step [4221/7635], Loss: 4.2106\n",
      "Epoch [1/1], Step [4222/7635], Loss: 4.1648\n",
      "Epoch [1/1], Step [4223/7635], Loss: 4.2336\n",
      "Epoch [1/1], Step [4224/7635], Loss: 4.2376\n",
      "Epoch [1/1], Step [4225/7635], Loss: 4.1976\n",
      "Epoch [1/1], Step [4226/7635], Loss: 4.3126\n",
      "Epoch [1/1], Step [4227/7635], Loss: 4.2814\n",
      "Epoch [1/1], Step [4228/7635], Loss: 4.2274\n",
      "Epoch [1/1], Step [4229/7635], Loss: 4.2033\n",
      "Epoch [1/1], Step [4230/7635], Loss: 4.2744\n",
      "Epoch [1/1], Step [4231/7635], Loss: 4.2646\n",
      "Epoch [1/1], Step [4232/7635], Loss: 4.2590\n",
      "Epoch [1/1], Step [4233/7635], Loss: 4.2930\n",
      "Epoch [1/1], Step [4234/7635], Loss: 4.1575\n",
      "Epoch [1/1], Step [4235/7635], Loss: 4.1742\n",
      "Epoch [1/1], Step [4236/7635], Loss: 4.2649\n",
      "Epoch [1/1], Step [4237/7635], Loss: 4.2606\n",
      "Epoch [1/1], Step [4238/7635], Loss: 4.2491\n",
      "Epoch [1/1], Step [4239/7635], Loss: 4.2204\n",
      "Epoch [1/1], Step [4240/7635], Loss: 4.1932\n",
      "Epoch [1/1], Step [4241/7635], Loss: 4.2477\n",
      "Epoch [1/1], Step [4242/7635], Loss: 4.1781\n",
      "Epoch [1/1], Step [4243/7635], Loss: 4.2034\n",
      "Epoch [1/1], Step [4244/7635], Loss: 4.2124\n",
      "Epoch [1/1], Step [4245/7635], Loss: 4.2498\n",
      "Epoch [1/1], Step [4246/7635], Loss: 4.2354\n",
      "Epoch [1/1], Step [4247/7635], Loss: 4.2166\n",
      "Epoch [1/1], Step [4248/7635], Loss: 4.2629\n",
      "Epoch [1/1], Step [4249/7635], Loss: 4.1930\n",
      "Epoch [1/1], Step [4250/7635], Loss: 4.1810\n",
      "Epoch [1/1], Step [4251/7635], Loss: 4.2107\n",
      "Epoch [1/1], Step [4252/7635], Loss: 4.2428\n",
      "Epoch [1/1], Step [4253/7635], Loss: 4.2181\n",
      "Epoch [1/1], Step [4254/7635], Loss: 4.2335\n",
      "Epoch [1/1], Step [4255/7635], Loss: 4.2608\n",
      "Epoch [1/1], Step [4256/7635], Loss: 4.2356\n",
      "Epoch [1/1], Step [4257/7635], Loss: 4.1862\n",
      "Epoch [1/1], Step [4258/7635], Loss: 4.2320\n",
      "Epoch [1/1], Step [4259/7635], Loss: 4.2575\n",
      "Epoch [1/1], Step [4260/7635], Loss: 4.1668\n",
      "Epoch [1/1], Step [4261/7635], Loss: 4.2056\n",
      "Epoch [1/1], Step [4262/7635], Loss: 4.2099\n",
      "Epoch [1/1], Step [4263/7635], Loss: 4.2117\n",
      "Epoch [1/1], Step [4264/7635], Loss: 4.2982\n",
      "Epoch [1/1], Step [4265/7635], Loss: 4.1595\n",
      "Epoch [1/1], Step [4266/7635], Loss: 4.1815\n",
      "Epoch [1/1], Step [4267/7635], Loss: 4.1718\n",
      "Epoch [1/1], Step [4268/7635], Loss: 4.1939\n",
      "Epoch [1/1], Step [4269/7635], Loss: 4.2226\n",
      "Epoch [1/1], Step [4270/7635], Loss: 4.1794\n",
      "Epoch [1/1], Step [4271/7635], Loss: 4.2157\n",
      "Epoch [1/1], Step [4272/7635], Loss: 4.2194\n",
      "Epoch [1/1], Step [4273/7635], Loss: 4.2390\n",
      "Epoch [1/1], Step [4274/7635], Loss: 4.2774\n",
      "Epoch [1/1], Step [4275/7635], Loss: 4.1666\n",
      "Epoch [1/1], Step [4276/7635], Loss: 4.2059\n",
      "Epoch [1/1], Step [4277/7635], Loss: 4.1954\n",
      "Epoch [1/1], Step [4278/7635], Loss: 4.2754\n",
      "Epoch [1/1], Step [4279/7635], Loss: 4.3106\n",
      "Epoch [1/1], Step [4280/7635], Loss: 4.2440\n",
      "Epoch [1/1], Step [4281/7635], Loss: 4.2391\n",
      "Epoch [1/1], Step [4282/7635], Loss: 4.2229\n",
      "Epoch [1/1], Step [4283/7635], Loss: 4.2771\n",
      "Epoch [1/1], Step [4284/7635], Loss: 4.2345\n",
      "Epoch [1/1], Step [4285/7635], Loss: 4.2081\n",
      "Epoch [1/1], Step [4286/7635], Loss: 4.2339\n",
      "Epoch [1/1], Step [4287/7635], Loss: 4.1976\n",
      "Epoch [1/1], Step [4288/7635], Loss: 4.2240\n",
      "Epoch [1/1], Step [4289/7635], Loss: 4.1902\n",
      "Epoch [1/1], Step [4290/7635], Loss: 4.1467\n",
      "Epoch [1/1], Step [4291/7635], Loss: 4.1935\n",
      "Epoch [1/1], Step [4292/7635], Loss: 4.2074\n",
      "Epoch [1/1], Step [4293/7635], Loss: 4.2933\n",
      "Epoch [1/1], Step [4294/7635], Loss: 4.2442\n",
      "Epoch [1/1], Step [4295/7635], Loss: 4.1663\n",
      "Epoch [1/1], Step [4296/7635], Loss: 4.2798\n",
      "Epoch [1/1], Step [4297/7635], Loss: 4.2250\n",
      "Epoch [1/1], Step [4298/7635], Loss: 4.2675\n",
      "Epoch [1/1], Step [4299/7635], Loss: 4.1956\n",
      "Epoch [1/1], Step [4300/7635], Loss: 4.2610\n",
      "Epoch [1/1], Step [4301/7635], Loss: 4.1960\n",
      "Epoch [1/1], Step [4302/7635], Loss: 4.2280\n",
      "Epoch [1/1], Step [4303/7635], Loss: 4.1331\n",
      "Epoch [1/1], Step [4304/7635], Loss: 4.1751\n",
      "Epoch [1/1], Step [4305/7635], Loss: 4.3030\n",
      "Epoch [1/1], Step [4306/7635], Loss: 4.2524\n",
      "Epoch [1/1], Step [4307/7635], Loss: 4.2014\n",
      "Epoch [1/1], Step [4308/7635], Loss: 4.2117\n",
      "Epoch [1/1], Step [4309/7635], Loss: 4.2048\n",
      "Epoch [1/1], Step [4310/7635], Loss: 4.1842\n",
      "Epoch [1/1], Step [4311/7635], Loss: 4.2003\n",
      "Epoch [1/1], Step [4312/7635], Loss: 4.0888\n",
      "Epoch [1/1], Step [4313/7635], Loss: 4.2214\n",
      "Epoch [1/1], Step [4314/7635], Loss: 4.2097\n",
      "Epoch [1/1], Step [4315/7635], Loss: 4.2495\n",
      "Epoch [1/1], Step [4316/7635], Loss: 4.1455\n",
      "Epoch [1/1], Step [4317/7635], Loss: 4.2661\n",
      "Epoch [1/1], Step [4318/7635], Loss: 4.2297\n",
      "Epoch [1/1], Step [4319/7635], Loss: 4.2112\n",
      "Epoch [1/1], Step [4320/7635], Loss: 4.1999\n",
      "Epoch [1/1], Step [4321/7635], Loss: 4.1895\n",
      "Epoch [1/1], Step [4322/7635], Loss: 4.1812\n",
      "Epoch [1/1], Step [4323/7635], Loss: 4.1571\n",
      "Epoch [1/1], Step [4324/7635], Loss: 4.1710\n",
      "Epoch [1/1], Step [4325/7635], Loss: 4.2921\n",
      "Epoch [1/1], Step [4326/7635], Loss: 4.2697\n",
      "Epoch [1/1], Step [4327/7635], Loss: 4.1600\n",
      "Epoch [1/1], Step [4328/7635], Loss: 4.1829\n",
      "Epoch [1/1], Step [4329/7635], Loss: 4.1795\n",
      "Epoch [1/1], Step [4330/7635], Loss: 4.1656\n",
      "Epoch [1/1], Step [4331/7635], Loss: 4.2307\n",
      "Epoch [1/1], Step [4332/7635], Loss: 4.2119\n",
      "Epoch [1/1], Step [4333/7635], Loss: 4.1958\n",
      "Epoch [1/1], Step [4334/7635], Loss: 4.2557\n",
      "Epoch [1/1], Step [4335/7635], Loss: 4.1812\n",
      "Epoch [1/1], Step [4336/7635], Loss: 4.2149\n",
      "Epoch [1/1], Step [4337/7635], Loss: 4.2993\n",
      "Epoch [1/1], Step [4338/7635], Loss: 4.2104\n",
      "Epoch [1/1], Step [4339/7635], Loss: 4.1736\n",
      "Epoch [1/1], Step [4340/7635], Loss: 4.2203\n",
      "Epoch [1/1], Step [4341/7635], Loss: 4.1176\n",
      "Epoch [1/1], Step [4342/7635], Loss: 4.1553\n",
      "Epoch [1/1], Step [4343/7635], Loss: 4.1855\n",
      "Epoch [1/1], Step [4344/7635], Loss: 4.2090\n",
      "Epoch [1/1], Step [4345/7635], Loss: 4.2222\n",
      "Epoch [1/1], Step [4346/7635], Loss: 4.2935\n",
      "Epoch [1/1], Step [4347/7635], Loss: 4.2425\n",
      "Epoch [1/1], Step [4348/7635], Loss: 4.1624\n",
      "Epoch [1/1], Step [4349/7635], Loss: 4.2515\n",
      "Epoch [1/1], Step [4350/7635], Loss: 4.2560\n",
      "Epoch [1/1], Step [4351/7635], Loss: 4.1720\n",
      "Epoch [1/1], Step [4352/7635], Loss: 4.1727\n",
      "Epoch [1/1], Step [4353/7635], Loss: 4.1946\n",
      "Epoch [1/1], Step [4354/7635], Loss: 4.2656\n",
      "Epoch [1/1], Step [4355/7635], Loss: 4.2128\n",
      "Epoch [1/1], Step [4356/7635], Loss: 4.2238\n",
      "Epoch [1/1], Step [4357/7635], Loss: 4.2054\n",
      "Epoch [1/1], Step [4358/7635], Loss: 4.1660\n",
      "Epoch [1/1], Step [4359/7635], Loss: 4.2853\n",
      "Epoch [1/1], Step [4360/7635], Loss: 4.1847\n",
      "Epoch [1/1], Step [4361/7635], Loss: 4.1886\n",
      "Epoch [1/1], Step [4362/7635], Loss: 4.2078\n",
      "Epoch [1/1], Step [4363/7635], Loss: 4.1847\n",
      "Epoch [1/1], Step [4364/7635], Loss: 4.2070\n",
      "Epoch [1/1], Step [4365/7635], Loss: 4.1822\n",
      "Epoch [1/1], Step [4366/7635], Loss: 4.2388\n",
      "Epoch [1/1], Step [4367/7635], Loss: 4.1706\n",
      "Epoch [1/1], Step [4368/7635], Loss: 4.2884\n",
      "Epoch [1/1], Step [4369/7635], Loss: 4.2270\n",
      "Epoch [1/1], Step [4370/7635], Loss: 4.2208\n",
      "Epoch [1/1], Step [4371/7635], Loss: 4.2793\n",
      "Epoch [1/1], Step [4372/7635], Loss: 4.2279\n",
      "Epoch [1/1], Step [4373/7635], Loss: 4.2498\n",
      "Epoch [1/1], Step [4374/7635], Loss: 4.2668\n",
      "Epoch [1/1], Step [4375/7635], Loss: 4.2502\n",
      "Epoch [1/1], Step [4376/7635], Loss: 4.1797\n",
      "Epoch [1/1], Step [4377/7635], Loss: 4.2166\n",
      "Epoch [1/1], Step [4378/7635], Loss: 4.2624\n",
      "Epoch [1/1], Step [4379/7635], Loss: 4.1587\n",
      "Epoch [1/1], Step [4380/7635], Loss: 4.2102\n",
      "Epoch [1/1], Step [4381/7635], Loss: 4.2002\n",
      "Epoch [1/1], Step [4382/7635], Loss: 4.1685\n",
      "Epoch [1/1], Step [4383/7635], Loss: 4.2456\n",
      "Epoch [1/1], Step [4384/7635], Loss: 4.2466\n",
      "Epoch [1/1], Step [4385/7635], Loss: 4.2030\n",
      "Epoch [1/1], Step [4386/7635], Loss: 4.1703\n",
      "Epoch [1/1], Step [4387/7635], Loss: 4.1643\n",
      "Epoch [1/1], Step [4388/7635], Loss: 4.2210\n",
      "Epoch [1/1], Step [4389/7635], Loss: 4.1965\n",
      "Epoch [1/1], Step [4390/7635], Loss: 4.2601\n",
      "Epoch [1/1], Step [4391/7635], Loss: 4.2090\n",
      "Epoch [1/1], Step [4392/7635], Loss: 4.2919\n",
      "Epoch [1/1], Step [4393/7635], Loss: 4.2224\n",
      "Epoch [1/1], Step [4394/7635], Loss: 4.1735\n",
      "Epoch [1/1], Step [4395/7635], Loss: 4.1794\n",
      "Epoch [1/1], Step [4396/7635], Loss: 4.1861\n",
      "Epoch [1/1], Step [4397/7635], Loss: 4.3234\n",
      "Epoch [1/1], Step [4398/7635], Loss: 4.2413\n",
      "Epoch [1/1], Step [4399/7635], Loss: 4.2024\n",
      "Epoch [1/1], Step [4400/7635], Loss: 4.2030\n",
      "Epoch [1/1], Step [4401/7635], Loss: 4.1848\n",
      "Epoch [1/1], Step [4402/7635], Loss: 4.1850\n",
      "Epoch [1/1], Step [4403/7635], Loss: 4.1657\n",
      "Epoch [1/1], Step [4404/7635], Loss: 4.1743\n",
      "Epoch [1/1], Step [4405/7635], Loss: 4.1280\n",
      "Epoch [1/1], Step [4406/7635], Loss: 4.2002\n",
      "Epoch [1/1], Step [4407/7635], Loss: 4.1809\n",
      "Epoch [1/1], Step [4408/7635], Loss: 4.2034\n",
      "Epoch [1/1], Step [4409/7635], Loss: 4.2671\n",
      "Epoch [1/1], Step [4410/7635], Loss: 4.2162\n",
      "Epoch [1/1], Step [4411/7635], Loss: 4.1638\n",
      "Epoch [1/1], Step [4412/7635], Loss: 4.2183\n",
      "Epoch [1/1], Step [4413/7635], Loss: 4.1296\n",
      "Epoch [1/1], Step [4414/7635], Loss: 4.2915\n",
      "Epoch [1/1], Step [4415/7635], Loss: 4.2730\n",
      "Epoch [1/1], Step [4416/7635], Loss: 4.2239\n",
      "Epoch [1/1], Step [4417/7635], Loss: 4.2181\n",
      "Epoch [1/1], Step [4418/7635], Loss: 4.2069\n",
      "Epoch [1/1], Step [4419/7635], Loss: 4.1771\n",
      "Epoch [1/1], Step [4420/7635], Loss: 4.1798\n",
      "Epoch [1/1], Step [4421/7635], Loss: 4.2109\n",
      "Epoch [1/1], Step [4422/7635], Loss: 4.1929\n",
      "Epoch [1/1], Step [4423/7635], Loss: 4.1033\n",
      "Epoch [1/1], Step [4424/7635], Loss: 4.2030\n",
      "Epoch [1/1], Step [4425/7635], Loss: 4.1873\n",
      "Epoch [1/1], Step [4426/7635], Loss: 4.2131\n",
      "Epoch [1/1], Step [4427/7635], Loss: 4.2262\n",
      "Epoch [1/1], Step [4428/7635], Loss: 4.2780\n",
      "Epoch [1/1], Step [4429/7635], Loss: 4.2459\n",
      "Epoch [1/1], Step [4430/7635], Loss: 4.2288\n",
      "Epoch [1/1], Step [4431/7635], Loss: 4.1779\n",
      "Epoch [1/1], Step [4432/7635], Loss: 4.1954\n",
      "Epoch [1/1], Step [4433/7635], Loss: 4.2025\n",
      "Epoch [1/1], Step [4434/7635], Loss: 4.1744\n",
      "Epoch [1/1], Step [4435/7635], Loss: 4.2341\n",
      "Epoch [1/1], Step [4436/7635], Loss: 4.2196\n",
      "Epoch [1/1], Step [4437/7635], Loss: 4.2305\n",
      "Epoch [1/1], Step [4438/7635], Loss: 4.1379\n",
      "Epoch [1/1], Step [4439/7635], Loss: 4.2283\n",
      "Epoch [1/1], Step [4440/7635], Loss: 4.2322\n",
      "Epoch [1/1], Step [4441/7635], Loss: 4.2012\n",
      "Epoch [1/1], Step [4442/7635], Loss: 4.3145\n",
      "Epoch [1/1], Step [4443/7635], Loss: 4.1255\n",
      "Epoch [1/1], Step [4444/7635], Loss: 4.1865\n",
      "Epoch [1/1], Step [4445/7635], Loss: 4.2093\n",
      "Epoch [1/1], Step [4446/7635], Loss: 4.2700\n",
      "Epoch [1/1], Step [4447/7635], Loss: 4.1637\n",
      "Epoch [1/1], Step [4448/7635], Loss: 4.2753\n",
      "Epoch [1/1], Step [4449/7635], Loss: 4.2015\n",
      "Epoch [1/1], Step [4450/7635], Loss: 4.2708\n",
      "Epoch [1/1], Step [4451/7635], Loss: 4.1332\n",
      "Epoch [1/1], Step [4452/7635], Loss: 4.3034\n",
      "Epoch [1/1], Step [4453/7635], Loss: 4.2058\n",
      "Epoch [1/1], Step [4454/7635], Loss: 4.1508\n",
      "Epoch [1/1], Step [4455/7635], Loss: 4.1894\n",
      "Epoch [1/1], Step [4456/7635], Loss: 4.1810\n",
      "Epoch [1/1], Step [4457/7635], Loss: 4.1791\n",
      "Epoch [1/1], Step [4458/7635], Loss: 4.1671\n",
      "Epoch [1/1], Step [4459/7635], Loss: 4.1942\n",
      "Epoch [1/1], Step [4460/7635], Loss: 4.1630\n",
      "Epoch [1/1], Step [4461/7635], Loss: 4.2867\n",
      "Epoch [1/1], Step [4462/7635], Loss: 4.1499\n",
      "Epoch [1/1], Step [4463/7635], Loss: 4.1833\n",
      "Epoch [1/1], Step [4464/7635], Loss: 4.2674\n",
      "Epoch [1/1], Step [4465/7635], Loss: 4.1752\n",
      "Epoch [1/1], Step [4466/7635], Loss: 4.1840\n",
      "Epoch [1/1], Step [4467/7635], Loss: 4.2038\n",
      "Epoch [1/1], Step [4468/7635], Loss: 4.1976\n",
      "Epoch [1/1], Step [4469/7635], Loss: 4.1366\n",
      "Epoch [1/1], Step [4470/7635], Loss: 4.1569\n",
      "Epoch [1/1], Step [4471/7635], Loss: 4.2334\n",
      "Epoch [1/1], Step [4472/7635], Loss: 4.2656\n",
      "Epoch [1/1], Step [4473/7635], Loss: 4.2307\n",
      "Epoch [1/1], Step [4474/7635], Loss: 4.2515\n",
      "Epoch [1/1], Step [4475/7635], Loss: 4.2284\n",
      "Epoch [1/1], Step [4476/7635], Loss: 4.1483\n",
      "Epoch [1/1], Step [4477/7635], Loss: 4.2233\n",
      "Epoch [1/1], Step [4478/7635], Loss: 4.1392\n",
      "Epoch [1/1], Step [4479/7635], Loss: 4.2438\n",
      "Epoch [1/1], Step [4480/7635], Loss: 4.1905\n",
      "Epoch [1/1], Step [4481/7635], Loss: 4.2294\n",
      "Epoch [1/1], Step [4482/7635], Loss: 4.1575\n",
      "Epoch [1/1], Step [4483/7635], Loss: 4.2146\n",
      "Epoch [1/1], Step [4484/7635], Loss: 4.0919\n",
      "Epoch [1/1], Step [4485/7635], Loss: 4.1698\n",
      "Epoch [1/1], Step [4486/7635], Loss: 4.2717\n",
      "Epoch [1/1], Step [4487/7635], Loss: 4.2792\n",
      "Epoch [1/1], Step [4488/7635], Loss: 4.1728\n",
      "Epoch [1/1], Step [4489/7635], Loss: 4.2219\n",
      "Epoch [1/1], Step [4490/7635], Loss: 4.2065\n",
      "Epoch [1/1], Step [4491/7635], Loss: 4.2075\n",
      "Epoch [1/1], Step [4492/7635], Loss: 4.2680\n",
      "Epoch [1/1], Step [4493/7635], Loss: 4.2200\n",
      "Epoch [1/1], Step [4494/7635], Loss: 4.1954\n",
      "Epoch [1/1], Step [4495/7635], Loss: 4.2249\n",
      "Epoch [1/1], Step [4496/7635], Loss: 4.1889\n",
      "Epoch [1/1], Step [4497/7635], Loss: 4.2297\n",
      "Epoch [1/1], Step [4498/7635], Loss: 4.1887\n",
      "Epoch [1/1], Step [4499/7635], Loss: 4.2368\n",
      "Epoch [1/1], Step [4500/7635], Loss: 4.1762\n",
      "Epoch [1/1], Step [4501/7635], Loss: 4.1957\n",
      "Epoch [1/1], Step [4502/7635], Loss: 4.2447\n",
      "Epoch [1/1], Step [4503/7635], Loss: 4.2265\n",
      "Epoch [1/1], Step [4504/7635], Loss: 4.2089\n",
      "Epoch [1/1], Step [4505/7635], Loss: 4.1644\n",
      "Epoch [1/1], Step [4506/7635], Loss: 4.2416\n",
      "Epoch [1/1], Step [4507/7635], Loss: 4.2435\n",
      "Epoch [1/1], Step [4508/7635], Loss: 4.2329\n",
      "Epoch [1/1], Step [4509/7635], Loss: 4.2345\n",
      "Epoch [1/1], Step [4510/7635], Loss: 4.1608\n",
      "Epoch [1/1], Step [4511/7635], Loss: 4.2007\n",
      "Epoch [1/1], Step [4512/7635], Loss: 4.2424\n",
      "Epoch [1/1], Step [4513/7635], Loss: 4.2482\n",
      "Epoch [1/1], Step [4514/7635], Loss: 4.1816\n",
      "Epoch [1/1], Step [4515/7635], Loss: 4.2830\n",
      "Epoch [1/1], Step [4516/7635], Loss: 4.2578\n",
      "Epoch [1/1], Step [4517/7635], Loss: 4.3211\n",
      "Epoch [1/1], Step [4518/7635], Loss: 4.2031\n",
      "Epoch [1/1], Step [4519/7635], Loss: 4.2309\n",
      "Epoch [1/1], Step [4520/7635], Loss: 4.2514\n",
      "Epoch [1/1], Step [4521/7635], Loss: 4.1783\n",
      "Epoch [1/1], Step [4522/7635], Loss: 4.2367\n",
      "Epoch [1/1], Step [4523/7635], Loss: 4.2039\n",
      "Epoch [1/1], Step [4524/7635], Loss: 4.2702\n",
      "Epoch [1/1], Step [4525/7635], Loss: 4.2168\n",
      "Epoch [1/1], Step [4526/7635], Loss: 4.1770\n",
      "Epoch [1/1], Step [4527/7635], Loss: 4.2267\n",
      "Epoch [1/1], Step [4528/7635], Loss: 4.2114\n",
      "Epoch [1/1], Step [4529/7635], Loss: 4.2179\n",
      "Epoch [1/1], Step [4530/7635], Loss: 4.1779\n",
      "Epoch [1/1], Step [4531/7635], Loss: 4.2535\n",
      "Epoch [1/1], Step [4532/7635], Loss: 4.1170\n",
      "Epoch [1/1], Step [4533/7635], Loss: 4.1516\n",
      "Epoch [1/1], Step [4534/7635], Loss: 4.1260\n",
      "Epoch [1/1], Step [4535/7635], Loss: 4.2027\n",
      "Epoch [1/1], Step [4536/7635], Loss: 4.1187\n",
      "Epoch [1/1], Step [4537/7635], Loss: 4.2673\n",
      "Epoch [1/1], Step [4538/7635], Loss: 4.2321\n",
      "Epoch [1/1], Step [4539/7635], Loss: 4.2818\n",
      "Epoch [1/1], Step [4540/7635], Loss: 4.1677\n",
      "Epoch [1/1], Step [4541/7635], Loss: 4.1625\n",
      "Epoch [1/1], Step [4542/7635], Loss: 4.2234\n",
      "Epoch [1/1], Step [4543/7635], Loss: 4.1154\n",
      "Epoch [1/1], Step [4544/7635], Loss: 4.2318\n",
      "Epoch [1/1], Step [4545/7635], Loss: 4.2230\n",
      "Epoch [1/1], Step [4546/7635], Loss: 4.2609\n",
      "Epoch [1/1], Step [4547/7635], Loss: 4.2678\n",
      "Epoch [1/1], Step [4548/7635], Loss: 4.1441\n",
      "Epoch [1/1], Step [4549/7635], Loss: 4.1996\n",
      "Epoch [1/1], Step [4550/7635], Loss: 4.1866\n",
      "Epoch [1/1], Step [4551/7635], Loss: 4.2239\n",
      "Epoch [1/1], Step [4552/7635], Loss: 4.2418\n",
      "Epoch [1/1], Step [4553/7635], Loss: 4.2168\n",
      "Epoch [1/1], Step [4554/7635], Loss: 4.2268\n",
      "Epoch [1/1], Step [4555/7635], Loss: 4.1916\n",
      "Epoch [1/1], Step [4556/7635], Loss: 4.1894\n",
      "Epoch [1/1], Step [4557/7635], Loss: 4.2421\n",
      "Epoch [1/1], Step [4558/7635], Loss: 4.2680\n",
      "Epoch [1/1], Step [4559/7635], Loss: 4.2346\n",
      "Epoch [1/1], Step [4560/7635], Loss: 4.1827\n",
      "Epoch [1/1], Step [4561/7635], Loss: 4.2847\n",
      "Epoch [1/1], Step [4562/7635], Loss: 4.1807\n",
      "Epoch [1/1], Step [4563/7635], Loss: 4.1982\n",
      "Epoch [1/1], Step [4564/7635], Loss: 4.2128\n",
      "Epoch [1/1], Step [4565/7635], Loss: 4.1855\n",
      "Epoch [1/1], Step [4566/7635], Loss: 4.2367\n",
      "Epoch [1/1], Step [4567/7635], Loss: 4.1976\n",
      "Epoch [1/1], Step [4568/7635], Loss: 4.2094\n",
      "Epoch [1/1], Step [4569/7635], Loss: 4.1677\n",
      "Epoch [1/1], Step [4570/7635], Loss: 4.1938\n",
      "Epoch [1/1], Step [4571/7635], Loss: 4.1752\n",
      "Epoch [1/1], Step [4572/7635], Loss: 4.2817\n",
      "Epoch [1/1], Step [4573/7635], Loss: 4.2513\n",
      "Epoch [1/1], Step [4574/7635], Loss: 4.2368\n",
      "Epoch [1/1], Step [4575/7635], Loss: 4.2952\n",
      "Epoch [1/1], Step [4576/7635], Loss: 4.1954\n",
      "Epoch [1/1], Step [4577/7635], Loss: 4.1738\n",
      "Epoch [1/1], Step [4578/7635], Loss: 4.2126\n",
      "Epoch [1/1], Step [4579/7635], Loss: 4.1881\n",
      "Epoch [1/1], Step [4580/7635], Loss: 4.1457\n",
      "Epoch [1/1], Step [4581/7635], Loss: 4.2660\n",
      "Epoch [1/1], Step [4582/7635], Loss: 4.2493\n",
      "Epoch [1/1], Step [4583/7635], Loss: 4.1640\n",
      "Epoch [1/1], Step [4584/7635], Loss: 4.2035\n",
      "Epoch [1/1], Step [4585/7635], Loss: 4.2715\n",
      "Epoch [1/1], Step [4586/7635], Loss: 4.2233\n",
      "Epoch [1/1], Step [4587/7635], Loss: 4.2733\n",
      "Epoch [1/1], Step [4588/7635], Loss: 4.2327\n",
      "Epoch [1/1], Step [4589/7635], Loss: 4.1896\n",
      "Epoch [1/1], Step [4590/7635], Loss: 4.2557\n",
      "Epoch [1/1], Step [4591/7635], Loss: 4.2241\n",
      "Epoch [1/1], Step [4592/7635], Loss: 4.2322\n",
      "Epoch [1/1], Step [4593/7635], Loss: 4.2143\n",
      "Epoch [1/1], Step [4594/7635], Loss: 4.1014\n",
      "Epoch [1/1], Step [4595/7635], Loss: 4.1674\n",
      "Epoch [1/1], Step [4596/7635], Loss: 4.2443\n",
      "Epoch [1/1], Step [4597/7635], Loss: 4.2621\n",
      "Epoch [1/1], Step [4598/7635], Loss: 4.2012\n",
      "Epoch [1/1], Step [4599/7635], Loss: 4.1933\n",
      "Epoch [1/1], Step [4600/7635], Loss: 4.2031\n",
      "Epoch [1/1], Step [4601/7635], Loss: 4.1761\n",
      "Epoch [1/1], Step [4602/7635], Loss: 4.1879\n",
      "Epoch [1/1], Step [4603/7635], Loss: 4.1994\n",
      "Epoch [1/1], Step [4604/7635], Loss: 4.1604\n",
      "Epoch [1/1], Step [4605/7635], Loss: 4.1968\n",
      "Epoch [1/1], Step [4606/7635], Loss: 4.2267\n",
      "Epoch [1/1], Step [4607/7635], Loss: 4.2295\n",
      "Epoch [1/1], Step [4608/7635], Loss: 4.2190\n",
      "Epoch [1/1], Step [4609/7635], Loss: 4.2155\n",
      "Epoch [1/1], Step [4610/7635], Loss: 4.2043\n",
      "Epoch [1/1], Step [4611/7635], Loss: 4.1950\n",
      "Epoch [1/1], Step [4612/7635], Loss: 4.1608\n",
      "Epoch [1/1], Step [4613/7635], Loss: 4.2806\n",
      "Epoch [1/1], Step [4614/7635], Loss: 4.2233\n",
      "Epoch [1/1], Step [4615/7635], Loss: 4.1863\n",
      "Epoch [1/1], Step [4616/7635], Loss: 4.2820\n",
      "Epoch [1/1], Step [4617/7635], Loss: 4.1632\n",
      "Epoch [1/1], Step [4618/7635], Loss: 4.2028\n",
      "Epoch [1/1], Step [4619/7635], Loss: 4.1876\n",
      "Epoch [1/1], Step [4620/7635], Loss: 4.2354\n",
      "Epoch [1/1], Step [4621/7635], Loss: 4.2261\n",
      "Epoch [1/1], Step [4622/7635], Loss: 4.2010\n",
      "Epoch [1/1], Step [4623/7635], Loss: 4.1704\n",
      "Epoch [1/1], Step [4624/7635], Loss: 4.1506\n",
      "Epoch [1/1], Step [4625/7635], Loss: 4.1678\n",
      "Epoch [1/1], Step [4626/7635], Loss: 4.1482\n",
      "Epoch [1/1], Step [4627/7635], Loss: 4.2014\n",
      "Epoch [1/1], Step [4628/7635], Loss: 4.2149\n",
      "Epoch [1/1], Step [4629/7635], Loss: 4.1673\n",
      "Epoch [1/1], Step [4630/7635], Loss: 4.2424\n",
      "Epoch [1/1], Step [4631/7635], Loss: 4.1670\n",
      "Epoch [1/1], Step [4632/7635], Loss: 4.2399\n",
      "Epoch [1/1], Step [4633/7635], Loss: 4.2130\n",
      "Epoch [1/1], Step [4634/7635], Loss: 4.1711\n",
      "Epoch [1/1], Step [4635/7635], Loss: 4.2459\n",
      "Epoch [1/1], Step [4636/7635], Loss: 4.1898\n",
      "Epoch [1/1], Step [4637/7635], Loss: 4.2101\n",
      "Epoch [1/1], Step [4638/7635], Loss: 4.1782\n",
      "Epoch [1/1], Step [4639/7635], Loss: 4.1315\n",
      "Epoch [1/1], Step [4640/7635], Loss: 4.1332\n",
      "Epoch [1/1], Step [4641/7635], Loss: 4.1767\n",
      "Epoch [1/1], Step [4642/7635], Loss: 4.2015\n",
      "Epoch [1/1], Step [4643/7635], Loss: 4.2036\n",
      "Epoch [1/1], Step [4644/7635], Loss: 4.0901\n",
      "Epoch [1/1], Step [4645/7635], Loss: 4.2112\n",
      "Epoch [1/1], Step [4646/7635], Loss: 4.2014\n",
      "Epoch [1/1], Step [4647/7635], Loss: 4.1501\n",
      "Epoch [1/1], Step [4648/7635], Loss: 4.2303\n",
      "Epoch [1/1], Step [4649/7635], Loss: 4.1876\n",
      "Epoch [1/1], Step [4650/7635], Loss: 4.3276\n",
      "Epoch [1/1], Step [4651/7635], Loss: 4.2376\n",
      "Epoch [1/1], Step [4652/7635], Loss: 4.1582\n",
      "Epoch [1/1], Step [4653/7635], Loss: 4.1908\n",
      "Epoch [1/1], Step [4654/7635], Loss: 4.1710\n",
      "Epoch [1/1], Step [4655/7635], Loss: 4.1351\n",
      "Epoch [1/1], Step [4656/7635], Loss: 4.2092\n",
      "Epoch [1/1], Step [4657/7635], Loss: 4.2043\n",
      "Epoch [1/1], Step [4658/7635], Loss: 4.1662\n",
      "Epoch [1/1], Step [4659/7635], Loss: 4.2427\n",
      "Epoch [1/1], Step [4660/7635], Loss: 4.2494\n",
      "Epoch [1/1], Step [4661/7635], Loss: 4.1902\n",
      "Epoch [1/1], Step [4662/7635], Loss: 4.2315\n",
      "Epoch [1/1], Step [4663/7635], Loss: 4.2316\n",
      "Epoch [1/1], Step [4664/7635], Loss: 4.1762\n",
      "Epoch [1/1], Step [4665/7635], Loss: 4.1737\n",
      "Epoch [1/1], Step [4666/7635], Loss: 4.1778\n",
      "Epoch [1/1], Step [4667/7635], Loss: 4.2791\n",
      "Epoch [1/1], Step [4668/7635], Loss: 4.1912\n",
      "Epoch [1/1], Step [4669/7635], Loss: 4.2245\n",
      "Epoch [1/1], Step [4670/7635], Loss: 4.1532\n",
      "Epoch [1/1], Step [4671/7635], Loss: 4.1278\n",
      "Epoch [1/1], Step [4672/7635], Loss: 4.1900\n",
      "Epoch [1/1], Step [4673/7635], Loss: 4.2437\n",
      "Epoch [1/1], Step [4674/7635], Loss: 4.1946\n",
      "Epoch [1/1], Step [4675/7635], Loss: 4.1478\n",
      "Epoch [1/1], Step [4676/7635], Loss: 4.2342\n",
      "Epoch [1/1], Step [4677/7635], Loss: 4.1725\n",
      "Epoch [1/1], Step [4678/7635], Loss: 4.1768\n",
      "Epoch [1/1], Step [4679/7635], Loss: 4.1759\n",
      "Epoch [1/1], Step [4680/7635], Loss: 4.1534\n",
      "Epoch [1/1], Step [4681/7635], Loss: 4.2215\n",
      "Epoch [1/1], Step [4682/7635], Loss: 4.1645\n",
      "Epoch [1/1], Step [4683/7635], Loss: 4.2766\n",
      "Epoch [1/1], Step [4684/7635], Loss: 4.1782\n",
      "Epoch [1/1], Step [4685/7635], Loss: 4.2340\n",
      "Epoch [1/1], Step [4686/7635], Loss: 4.2385\n",
      "Epoch [1/1], Step [4687/7635], Loss: 4.2372\n",
      "Epoch [1/1], Step [4688/7635], Loss: 4.2207\n",
      "Epoch [1/1], Step [4689/7635], Loss: 4.2748\n",
      "Epoch [1/1], Step [4690/7635], Loss: 4.1477\n",
      "Epoch [1/1], Step [4691/7635], Loss: 4.1317\n",
      "Epoch [1/1], Step [4692/7635], Loss: 4.1331\n",
      "Epoch [1/1], Step [4693/7635], Loss: 4.1251\n",
      "Epoch [1/1], Step [4694/7635], Loss: 4.1698\n",
      "Epoch [1/1], Step [4695/7635], Loss: 4.2258\n",
      "Epoch [1/1], Step [4696/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [4697/7635], Loss: 4.2209\n",
      "Epoch [1/1], Step [4698/7635], Loss: 4.1160\n",
      "Epoch [1/1], Step [4699/7635], Loss: 4.2011\n",
      "Epoch [1/1], Step [4700/7635], Loss: 4.1384\n",
      "Epoch [1/1], Step [4701/7635], Loss: 4.2213\n",
      "Epoch [1/1], Step [4702/7635], Loss: 4.1796\n",
      "Epoch [1/1], Step [4703/7635], Loss: 4.2590\n",
      "Epoch [1/1], Step [4704/7635], Loss: 4.1564\n",
      "Epoch [1/1], Step [4705/7635], Loss: 4.1789\n",
      "Epoch [1/1], Step [4706/7635], Loss: 4.2132\n",
      "Epoch [1/1], Step [4707/7635], Loss: 4.2275\n",
      "Epoch [1/1], Step [4708/7635], Loss: 4.1349\n",
      "Epoch [1/1], Step [4709/7635], Loss: 4.1948\n",
      "Epoch [1/1], Step [4710/7635], Loss: 4.2204\n",
      "Epoch [1/1], Step [4711/7635], Loss: 4.1914\n",
      "Epoch [1/1], Step [4712/7635], Loss: 4.1676\n",
      "Epoch [1/1], Step [4713/7635], Loss: 4.2393\n",
      "Epoch [1/1], Step [4714/7635], Loss: 4.2107\n",
      "Epoch [1/1], Step [4715/7635], Loss: 4.1522\n",
      "Epoch [1/1], Step [4716/7635], Loss: 4.2226\n",
      "Epoch [1/1], Step [4717/7635], Loss: 4.2097\n",
      "Epoch [1/1], Step [4718/7635], Loss: 4.0976\n",
      "Epoch [1/1], Step [4719/7635], Loss: 4.1510\n",
      "Epoch [1/1], Step [4720/7635], Loss: 4.1619\n",
      "Epoch [1/1], Step [4721/7635], Loss: 4.2249\n",
      "Epoch [1/1], Step [4722/7635], Loss: 4.1675\n",
      "Epoch [1/1], Step [4723/7635], Loss: 4.1495\n",
      "Epoch [1/1], Step [4724/7635], Loss: 4.1899\n",
      "Epoch [1/1], Step [4725/7635], Loss: 4.2347\n",
      "Epoch [1/1], Step [4726/7635], Loss: 4.2417\n",
      "Epoch [1/1], Step [4727/7635], Loss: 4.1466\n",
      "Epoch [1/1], Step [4728/7635], Loss: 4.2006\n",
      "Epoch [1/1], Step [4729/7635], Loss: 4.2245\n",
      "Epoch [1/1], Step [4730/7635], Loss: 4.1769\n",
      "Epoch [1/1], Step [4731/7635], Loss: 4.1874\n",
      "Epoch [1/1], Step [4732/7635], Loss: 4.0798\n",
      "Epoch [1/1], Step [4733/7635], Loss: 4.1881\n",
      "Epoch [1/1], Step [4734/7635], Loss: 4.2079\n",
      "Epoch [1/1], Step [4735/7635], Loss: 4.1736\n",
      "Epoch [1/1], Step [4736/7635], Loss: 4.2272\n",
      "Epoch [1/1], Step [4737/7635], Loss: 4.1809\n",
      "Epoch [1/1], Step [4738/7635], Loss: 4.2090\n",
      "Epoch [1/1], Step [4739/7635], Loss: 4.1733\n",
      "Epoch [1/1], Step [4740/7635], Loss: 4.2185\n",
      "Epoch [1/1], Step [4741/7635], Loss: 4.1319\n",
      "Epoch [1/1], Step [4742/7635], Loss: 4.3179\n",
      "Epoch [1/1], Step [4743/7635], Loss: 4.2410\n",
      "Epoch [1/1], Step [4744/7635], Loss: 4.2413\n",
      "Epoch [1/1], Step [4745/7635], Loss: 4.1351\n",
      "Epoch [1/1], Step [4746/7635], Loss: 4.1917\n",
      "Epoch [1/1], Step [4747/7635], Loss: 4.1369\n",
      "Epoch [1/1], Step [4748/7635], Loss: 4.1451\n",
      "Epoch [1/1], Step [4749/7635], Loss: 4.1672\n",
      "Epoch [1/1], Step [4750/7635], Loss: 4.1600\n",
      "Epoch [1/1], Step [4751/7635], Loss: 4.2570\n",
      "Epoch [1/1], Step [4752/7635], Loss: 4.2112\n",
      "Epoch [1/1], Step [4753/7635], Loss: 4.2123\n",
      "Epoch [1/1], Step [4754/7635], Loss: 4.2571\n",
      "Epoch [1/1], Step [4755/7635], Loss: 4.1518\n",
      "Epoch [1/1], Step [4756/7635], Loss: 4.1817\n",
      "Epoch [1/1], Step [4757/7635], Loss: 4.1053\n",
      "Epoch [1/1], Step [4758/7635], Loss: 4.1813\n",
      "Epoch [1/1], Step [4759/7635], Loss: 4.1374\n",
      "Epoch [1/1], Step [4760/7635], Loss: 4.2331\n",
      "Epoch [1/1], Step [4761/7635], Loss: 4.2399\n",
      "Epoch [1/1], Step [4762/7635], Loss: 4.1837\n",
      "Epoch [1/1], Step [4763/7635], Loss: 4.2848\n",
      "Epoch [1/1], Step [4764/7635], Loss: 4.1932\n",
      "Epoch [1/1], Step [4765/7635], Loss: 4.1749\n",
      "Epoch [1/1], Step [4766/7635], Loss: 4.0935\n",
      "Epoch [1/1], Step [4767/7635], Loss: 4.2543\n",
      "Epoch [1/1], Step [4768/7635], Loss: 4.1903\n",
      "Epoch [1/1], Step [4769/7635], Loss: 4.2493\n",
      "Epoch [1/1], Step [4770/7635], Loss: 4.1912\n",
      "Epoch [1/1], Step [4771/7635], Loss: 4.2311\n",
      "Epoch [1/1], Step [4772/7635], Loss: 4.1895\n",
      "Epoch [1/1], Step [4773/7635], Loss: 4.1673\n",
      "Epoch [1/1], Step [4774/7635], Loss: 4.1250\n",
      "Epoch [1/1], Step [4775/7635], Loss: 4.2301\n",
      "Epoch [1/1], Step [4776/7635], Loss: 4.1824\n",
      "Epoch [1/1], Step [4777/7635], Loss: 4.1715\n",
      "Epoch [1/1], Step [4778/7635], Loss: 4.1377\n",
      "Epoch [1/1], Step [4779/7635], Loss: 4.2227\n",
      "Epoch [1/1], Step [4780/7635], Loss: 4.2143\n",
      "Epoch [1/1], Step [4781/7635], Loss: 4.1063\n",
      "Epoch [1/1], Step [4782/7635], Loss: 4.1997\n",
      "Epoch [1/1], Step [4783/7635], Loss: 4.2929\n",
      "Epoch [1/1], Step [4784/7635], Loss: 4.1624\n",
      "Epoch [1/1], Step [4785/7635], Loss: 4.2227\n",
      "Epoch [1/1], Step [4786/7635], Loss: 4.2676\n",
      "Epoch [1/1], Step [4787/7635], Loss: 4.2347\n",
      "Epoch [1/1], Step [4788/7635], Loss: 4.2146\n",
      "Epoch [1/1], Step [4789/7635], Loss: 4.2180\n",
      "Epoch [1/1], Step [4790/7635], Loss: 4.2192\n",
      "Epoch [1/1], Step [4791/7635], Loss: 4.2520\n",
      "Epoch [1/1], Step [4792/7635], Loss: 4.2144\n",
      "Epoch [1/1], Step [4793/7635], Loss: 4.2624\n",
      "Epoch [1/1], Step [4794/7635], Loss: 4.2844\n",
      "Epoch [1/1], Step [4795/7635], Loss: 4.2079\n",
      "Epoch [1/1], Step [4796/7635], Loss: 4.2068\n",
      "Epoch [1/1], Step [4797/7635], Loss: 4.2233\n",
      "Epoch [1/1], Step [4798/7635], Loss: 4.2509\n",
      "Epoch [1/1], Step [4799/7635], Loss: 4.1686\n",
      "Epoch [1/1], Step [4800/7635], Loss: 4.2534\n",
      "Epoch [1/1], Step [4801/7635], Loss: 4.1480\n",
      "Epoch [1/1], Step [4802/7635], Loss: 4.1498\n",
      "Epoch [1/1], Step [4803/7635], Loss: 4.2094\n",
      "Epoch [1/1], Step [4804/7635], Loss: 4.1735\n",
      "Epoch [1/1], Step [4805/7635], Loss: 4.2097\n",
      "Epoch [1/1], Step [4806/7635], Loss: 4.0600\n",
      "Epoch [1/1], Step [4807/7635], Loss: 4.1372\n",
      "Epoch [1/1], Step [4808/7635], Loss: 4.1806\n",
      "Epoch [1/1], Step [4809/7635], Loss: 4.2354\n",
      "Epoch [1/1], Step [4810/7635], Loss: 4.2384\n",
      "Epoch [1/1], Step [4811/7635], Loss: 4.2561\n",
      "Epoch [1/1], Step [4812/7635], Loss: 4.1422\n",
      "Epoch [1/1], Step [4813/7635], Loss: 4.2070\n",
      "Epoch [1/1], Step [4814/7635], Loss: 4.2502\n",
      "Epoch [1/1], Step [4815/7635], Loss: 4.2789\n",
      "Epoch [1/1], Step [4816/7635], Loss: 4.1544\n",
      "Epoch [1/1], Step [4817/7635], Loss: 4.1927\n",
      "Epoch [1/1], Step [4818/7635], Loss: 4.2176\n",
      "Epoch [1/1], Step [4819/7635], Loss: 4.2068\n",
      "Epoch [1/1], Step [4820/7635], Loss: 4.2267\n",
      "Epoch [1/1], Step [4821/7635], Loss: 4.2625\n",
      "Epoch [1/1], Step [4822/7635], Loss: 4.2326\n",
      "Epoch [1/1], Step [4823/7635], Loss: 4.2083\n",
      "Epoch [1/1], Step [4824/7635], Loss: 4.2775\n",
      "Epoch [1/1], Step [4825/7635], Loss: 4.1556\n",
      "Epoch [1/1], Step [4826/7635], Loss: 4.2783\n",
      "Epoch [1/1], Step [4827/7635], Loss: 4.2667\n",
      "Epoch [1/1], Step [4828/7635], Loss: 4.2924\n",
      "Epoch [1/1], Step [4829/7635], Loss: 4.2060\n",
      "Epoch [1/1], Step [4830/7635], Loss: 4.2155\n",
      "Epoch [1/1], Step [4831/7635], Loss: 4.1903\n",
      "Epoch [1/1], Step [4832/7635], Loss: 4.1497\n",
      "Epoch [1/1], Step [4833/7635], Loss: 4.2260\n",
      "Epoch [1/1], Step [4834/7635], Loss: 4.1815\n",
      "Epoch [1/1], Step [4835/7635], Loss: 4.1971\n",
      "Epoch [1/1], Step [4836/7635], Loss: 4.2263\n",
      "Epoch [1/1], Step [4837/7635], Loss: 4.2146\n",
      "Epoch [1/1], Step [4838/7635], Loss: 4.1883\n",
      "Epoch [1/1], Step [4839/7635], Loss: 4.1614\n",
      "Epoch [1/1], Step [4840/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [4841/7635], Loss: 4.1641\n",
      "Epoch [1/1], Step [4842/7635], Loss: 4.2297\n",
      "Epoch [1/1], Step [4843/7635], Loss: 4.2026\n",
      "Epoch [1/1], Step [4844/7635], Loss: 4.2805\n",
      "Epoch [1/1], Step [4845/7635], Loss: 4.1720\n",
      "Epoch [1/1], Step [4846/7635], Loss: 4.1343\n",
      "Epoch [1/1], Step [4847/7635], Loss: 4.2048\n",
      "Epoch [1/1], Step [4848/7635], Loss: 4.1549\n",
      "Epoch [1/1], Step [4849/7635], Loss: 4.1453\n",
      "Epoch [1/1], Step [4850/7635], Loss: 4.2507\n",
      "Epoch [1/1], Step [4851/7635], Loss: 4.2559\n",
      "Epoch [1/1], Step [4852/7635], Loss: 4.1486\n",
      "Epoch [1/1], Step [4853/7635], Loss: 4.1421\n",
      "Epoch [1/1], Step [4854/7635], Loss: 4.1738\n",
      "Epoch [1/1], Step [4855/7635], Loss: 4.2735\n",
      "Epoch [1/1], Step [4856/7635], Loss: 4.2559\n",
      "Epoch [1/1], Step [4857/7635], Loss: 4.1586\n",
      "Epoch [1/1], Step [4858/7635], Loss: 4.1969\n",
      "Epoch [1/1], Step [4859/7635], Loss: 4.2054\n",
      "Epoch [1/1], Step [4860/7635], Loss: 4.1607\n",
      "Epoch [1/1], Step [4861/7635], Loss: 4.1262\n",
      "Epoch [1/1], Step [4862/7635], Loss: 4.2111\n",
      "Epoch [1/1], Step [4863/7635], Loss: 4.1712\n",
      "Epoch [1/1], Step [4864/7635], Loss: 4.2333\n",
      "Epoch [1/1], Step [4865/7635], Loss: 4.1752\n",
      "Epoch [1/1], Step [4866/7635], Loss: 4.2120\n",
      "Epoch [1/1], Step [4867/7635], Loss: 4.2267\n",
      "Epoch [1/1], Step [4868/7635], Loss: 4.1217\n",
      "Epoch [1/1], Step [4869/7635], Loss: 4.2017\n",
      "Epoch [1/1], Step [4870/7635], Loss: 4.1577\n",
      "Epoch [1/1], Step [4871/7635], Loss: 4.2158\n",
      "Epoch [1/1], Step [4872/7635], Loss: 4.2360\n",
      "Epoch [1/1], Step [4873/7635], Loss: 4.1519\n",
      "Epoch [1/1], Step [4874/7635], Loss: 4.2008\n",
      "Epoch [1/1], Step [4875/7635], Loss: 4.0735\n",
      "Epoch [1/1], Step [4876/7635], Loss: 4.1559\n",
      "Epoch [1/1], Step [4877/7635], Loss: 4.1905\n",
      "Epoch [1/1], Step [4878/7635], Loss: 4.1603\n",
      "Epoch [1/1], Step [4879/7635], Loss: 4.1704\n",
      "Epoch [1/1], Step [4880/7635], Loss: 4.1435\n",
      "Epoch [1/1], Step [4881/7635], Loss: 4.1546\n",
      "Epoch [1/1], Step [4882/7635], Loss: 4.1590\n",
      "Epoch [1/1], Step [4883/7635], Loss: 4.2337\n",
      "Epoch [1/1], Step [4884/7635], Loss: 4.1640\n",
      "Epoch [1/1], Step [4885/7635], Loss: 4.2608\n",
      "Epoch [1/1], Step [4886/7635], Loss: 4.1803\n",
      "Epoch [1/1], Step [4887/7635], Loss: 4.2366\n",
      "Epoch [1/1], Step [4888/7635], Loss: 4.0827\n",
      "Epoch [1/1], Step [4889/7635], Loss: 4.2072\n",
      "Epoch [1/1], Step [4890/7635], Loss: 4.2526\n",
      "Epoch [1/1], Step [4891/7635], Loss: 4.1299\n",
      "Epoch [1/1], Step [4892/7635], Loss: 4.1448\n",
      "Epoch [1/1], Step [4893/7635], Loss: 4.1600\n",
      "Epoch [1/1], Step [4894/7635], Loss: 4.2727\n",
      "Epoch [1/1], Step [4895/7635], Loss: 4.1880\n",
      "Epoch [1/1], Step [4896/7635], Loss: 4.1029\n",
      "Epoch [1/1], Step [4897/7635], Loss: 4.1626\n",
      "Epoch [1/1], Step [4898/7635], Loss: 4.2427\n",
      "Epoch [1/1], Step [4899/7635], Loss: 4.2210\n",
      "Epoch [1/1], Step [4900/7635], Loss: 4.2191\n",
      "Epoch [1/1], Step [4901/7635], Loss: 4.1552\n",
      "Epoch [1/1], Step [4902/7635], Loss: 4.2060\n",
      "Epoch [1/1], Step [4903/7635], Loss: 4.2275\n",
      "Epoch [1/1], Step [4904/7635], Loss: 4.1884\n",
      "Epoch [1/1], Step [4905/7635], Loss: 4.1706\n",
      "Epoch [1/1], Step [4906/7635], Loss: 4.1678\n",
      "Epoch [1/1], Step [4907/7635], Loss: 4.1063\n",
      "Epoch [1/1], Step [4908/7635], Loss: 4.1950\n",
      "Epoch [1/1], Step [4909/7635], Loss: 4.1719\n",
      "Epoch [1/1], Step [4910/7635], Loss: 4.2021\n",
      "Epoch [1/1], Step [4911/7635], Loss: 4.2273\n",
      "Epoch [1/1], Step [4912/7635], Loss: 4.2449\n",
      "Epoch [1/1], Step [4913/7635], Loss: 4.1609\n",
      "Epoch [1/1], Step [4914/7635], Loss: 4.1721\n",
      "Epoch [1/1], Step [4915/7635], Loss: 4.1419\n",
      "Epoch [1/1], Step [4916/7635], Loss: 4.0668\n",
      "Epoch [1/1], Step [4917/7635], Loss: 4.2005\n",
      "Epoch [1/1], Step [4918/7635], Loss: 4.3266\n",
      "Epoch [1/1], Step [4919/7635], Loss: 4.1820\n",
      "Epoch [1/1], Step [4920/7635], Loss: 4.1695\n",
      "Epoch [1/1], Step [4921/7635], Loss: 4.1684\n",
      "Epoch [1/1], Step [4922/7635], Loss: 4.1376\n",
      "Epoch [1/1], Step [4923/7635], Loss: 4.1290\n",
      "Epoch [1/1], Step [4924/7635], Loss: 4.0752\n",
      "Epoch [1/1], Step [4925/7635], Loss: 4.1534\n",
      "Epoch [1/1], Step [4926/7635], Loss: 4.2188\n",
      "Epoch [1/1], Step [4927/7635], Loss: 4.1794\n",
      "Epoch [1/1], Step [4928/7635], Loss: 4.1238\n",
      "Epoch [1/1], Step [4929/7635], Loss: 4.1421\n",
      "Epoch [1/1], Step [4930/7635], Loss: 4.1455\n",
      "Epoch [1/1], Step [4931/7635], Loss: 4.1948\n",
      "Epoch [1/1], Step [4932/7635], Loss: 4.1830\n",
      "Epoch [1/1], Step [4933/7635], Loss: 4.1652\n",
      "Epoch [1/1], Step [4934/7635], Loss: 4.1865\n",
      "Epoch [1/1], Step [4935/7635], Loss: 4.1956\n",
      "Epoch [1/1], Step [4936/7635], Loss: 4.2215\n",
      "Epoch [1/1], Step [4937/7635], Loss: 4.1301\n",
      "Epoch [1/1], Step [4938/7635], Loss: 4.1873\n",
      "Epoch [1/1], Step [4939/7635], Loss: 4.2490\n",
      "Epoch [1/1], Step [4940/7635], Loss: 4.1997\n",
      "Epoch [1/1], Step [4941/7635], Loss: 4.2971\n",
      "Epoch [1/1], Step [4942/7635], Loss: 4.1816\n",
      "Epoch [1/1], Step [4943/7635], Loss: 4.2044\n",
      "Epoch [1/1], Step [4944/7635], Loss: 4.1900\n",
      "Epoch [1/1], Step [4945/7635], Loss: 4.1862\n",
      "Epoch [1/1], Step [4946/7635], Loss: 4.1786\n",
      "Epoch [1/1], Step [4947/7635], Loss: 4.2137\n",
      "Epoch [1/1], Step [4948/7635], Loss: 4.1616\n",
      "Epoch [1/1], Step [4949/7635], Loss: 4.1777\n",
      "Epoch [1/1], Step [4950/7635], Loss: 4.2425\n",
      "Epoch [1/1], Step [4951/7635], Loss: 4.1951\n",
      "Epoch [1/1], Step [4952/7635], Loss: 4.1704\n",
      "Epoch [1/1], Step [4953/7635], Loss: 4.1080\n",
      "Epoch [1/1], Step [4954/7635], Loss: 4.2052\n",
      "Epoch [1/1], Step [4955/7635], Loss: 4.1813\n",
      "Epoch [1/1], Step [4956/7635], Loss: 4.2483\n",
      "Epoch [1/1], Step [4957/7635], Loss: 4.2107\n",
      "Epoch [1/1], Step [4958/7635], Loss: 4.0827\n",
      "Epoch [1/1], Step [4959/7635], Loss: 4.2001\n",
      "Epoch [1/1], Step [4960/7635], Loss: 4.2600\n",
      "Epoch [1/1], Step [4961/7635], Loss: 4.2092\n",
      "Epoch [1/1], Step [4962/7635], Loss: 4.2451\n",
      "Epoch [1/1], Step [4963/7635], Loss: 4.2238\n",
      "Epoch [1/1], Step [4964/7635], Loss: 4.2090\n",
      "Epoch [1/1], Step [4965/7635], Loss: 4.1080\n",
      "Epoch [1/1], Step [4966/7635], Loss: 4.1588\n",
      "Epoch [1/1], Step [4967/7635], Loss: 4.2307\n",
      "Epoch [1/1], Step [4968/7635], Loss: 4.1677\n",
      "Epoch [1/1], Step [4969/7635], Loss: 4.1138\n",
      "Epoch [1/1], Step [4970/7635], Loss: 4.1918\n",
      "Epoch [1/1], Step [4971/7635], Loss: 4.1504\n",
      "Epoch [1/1], Step [4972/7635], Loss: 4.1726\n",
      "Epoch [1/1], Step [4973/7635], Loss: 4.1262\n",
      "Epoch [1/1], Step [4974/7635], Loss: 4.1894\n",
      "Epoch [1/1], Step [4975/7635], Loss: 4.1781\n",
      "Epoch [1/1], Step [4976/7635], Loss: 4.1752\n",
      "Epoch [1/1], Step [4977/7635], Loss: 4.1629\n",
      "Epoch [1/1], Step [4978/7635], Loss: 4.1337\n",
      "Epoch [1/1], Step [4979/7635], Loss: 4.1547\n",
      "Epoch [1/1], Step [4980/7635], Loss: 4.2268\n",
      "Epoch [1/1], Step [4981/7635], Loss: 4.1439\n",
      "Epoch [1/1], Step [4982/7635], Loss: 4.2472\n",
      "Epoch [1/1], Step [4983/7635], Loss: 4.1257\n",
      "Epoch [1/1], Step [4984/7635], Loss: 4.1944\n",
      "Epoch [1/1], Step [4985/7635], Loss: 4.2371\n",
      "Epoch [1/1], Step [4986/7635], Loss: 4.2338\n",
      "Epoch [1/1], Step [4987/7635], Loss: 4.1499\n",
      "Epoch [1/1], Step [4988/7635], Loss: 4.1634\n",
      "Epoch [1/1], Step [4989/7635], Loss: 4.2210\n",
      "Epoch [1/1], Step [4990/7635], Loss: 4.2346\n",
      "Epoch [1/1], Step [4991/7635], Loss: 4.1046\n",
      "Epoch [1/1], Step [4992/7635], Loss: 4.2784\n",
      "Epoch [1/1], Step [4993/7635], Loss: 4.1461\n",
      "Epoch [1/1], Step [4994/7635], Loss: 4.1560\n",
      "Epoch [1/1], Step [4995/7635], Loss: 4.1495\n",
      "Epoch [1/1], Step [4996/7635], Loss: 4.1769\n",
      "Epoch [1/1], Step [4997/7635], Loss: 4.2473\n",
      "Epoch [1/1], Step [4998/7635], Loss: 4.1577\n",
      "Epoch [1/1], Step [4999/7635], Loss: 4.1120\n",
      "Epoch [1/1], Step [5000/7635], Loss: 4.1412\n",
      "Epoch [1/1], Step [5001/7635], Loss: 4.0964\n",
      "Epoch [1/1], Step [5002/7635], Loss: 4.1673\n",
      "Epoch [1/1], Step [5003/7635], Loss: 4.1920\n",
      "Epoch [1/1], Step [5004/7635], Loss: 4.1697\n",
      "Epoch [1/1], Step [5005/7635], Loss: 4.1391\n",
      "Epoch [1/1], Step [5006/7635], Loss: 4.1836\n",
      "Epoch [1/1], Step [5007/7635], Loss: 4.1970\n",
      "Epoch [1/1], Step [5008/7635], Loss: 4.0844\n",
      "Epoch [1/1], Step [5009/7635], Loss: 4.1577\n",
      "Epoch [1/1], Step [5010/7635], Loss: 4.1416\n",
      "Epoch [1/1], Step [5011/7635], Loss: 4.1968\n",
      "Epoch [1/1], Step [5012/7635], Loss: 4.1502\n",
      "Epoch [1/1], Step [5013/7635], Loss: 4.1826\n",
      "Epoch [1/1], Step [5014/7635], Loss: 4.1472\n",
      "Epoch [1/1], Step [5015/7635], Loss: 4.2046\n",
      "Epoch [1/1], Step [5016/7635], Loss: 4.2489\n",
      "Epoch [1/1], Step [5017/7635], Loss: 4.1570\n",
      "Epoch [1/1], Step [5018/7635], Loss: 4.2388\n",
      "Epoch [1/1], Step [5019/7635], Loss: 4.2520\n",
      "Epoch [1/1], Step [5020/7635], Loss: 4.2038\n",
      "Epoch [1/1], Step [5021/7635], Loss: 4.2736\n",
      "Epoch [1/1], Step [5022/7635], Loss: 4.1935\n",
      "Epoch [1/1], Step [5023/7635], Loss: 4.1334\n",
      "Epoch [1/1], Step [5024/7635], Loss: 4.1734\n",
      "Epoch [1/1], Step [5025/7635], Loss: 4.2321\n",
      "Epoch [1/1], Step [5026/7635], Loss: 4.2641\n",
      "Epoch [1/1], Step [5027/7635], Loss: 4.1972\n",
      "Epoch [1/1], Step [5028/7635], Loss: 4.2036\n",
      "Epoch [1/1], Step [5029/7635], Loss: 4.1986\n",
      "Epoch [1/1], Step [5030/7635], Loss: 4.2526\n",
      "Epoch [1/1], Step [5031/7635], Loss: 4.2774\n",
      "Epoch [1/1], Step [5032/7635], Loss: 4.1423\n",
      "Epoch [1/1], Step [5033/7635], Loss: 4.1845\n",
      "Epoch [1/1], Step [5034/7635], Loss: 4.2410\n",
      "Epoch [1/1], Step [5035/7635], Loss: 4.1707\n",
      "Epoch [1/1], Step [5036/7635], Loss: 4.2801\n",
      "Epoch [1/1], Step [5037/7635], Loss: 4.1666\n",
      "Epoch [1/1], Step [5038/7635], Loss: 4.1747\n",
      "Epoch [1/1], Step [5039/7635], Loss: 4.1394\n",
      "Epoch [1/1], Step [5040/7635], Loss: 4.1537\n",
      "Epoch [1/1], Step [5041/7635], Loss: 4.1514\n",
      "Epoch [1/1], Step [5042/7635], Loss: 4.1739\n",
      "Epoch [1/1], Step [5043/7635], Loss: 4.2184\n",
      "Epoch [1/1], Step [5044/7635], Loss: 4.2178\n",
      "Epoch [1/1], Step [5045/7635], Loss: 4.1684\n",
      "Epoch [1/1], Step [5046/7635], Loss: 4.2313\n",
      "Epoch [1/1], Step [5047/7635], Loss: 4.1627\n",
      "Epoch [1/1], Step [5048/7635], Loss: 4.2052\n",
      "Epoch [1/1], Step [5049/7635], Loss: 4.2021\n",
      "Epoch [1/1], Step [5050/7635], Loss: 4.1482\n",
      "Epoch [1/1], Step [5051/7635], Loss: 4.1632\n",
      "Epoch [1/1], Step [5052/7635], Loss: 4.1760\n",
      "Epoch [1/1], Step [5053/7635], Loss: 4.1868\n",
      "Epoch [1/1], Step [5054/7635], Loss: 4.1758\n",
      "Epoch [1/1], Step [5055/7635], Loss: 4.2579\n",
      "Epoch [1/1], Step [5056/7635], Loss: 4.2158\n",
      "Epoch [1/1], Step [5057/7635], Loss: 4.2203\n",
      "Epoch [1/1], Step [5058/7635], Loss: 4.0968\n",
      "Epoch [1/1], Step [5059/7635], Loss: 4.1901\n",
      "Epoch [1/1], Step [5060/7635], Loss: 4.1598\n",
      "Epoch [1/1], Step [5061/7635], Loss: 4.1675\n",
      "Epoch [1/1], Step [5062/7635], Loss: 4.1215\n",
      "Epoch [1/1], Step [5063/7635], Loss: 4.2377\n",
      "Epoch [1/1], Step [5064/7635], Loss: 4.2034\n",
      "Epoch [1/1], Step [5065/7635], Loss: 4.1816\n",
      "Epoch [1/1], Step [5066/7635], Loss: 4.1944\n",
      "Epoch [1/1], Step [5067/7635], Loss: 4.1104\n",
      "Epoch [1/1], Step [5068/7635], Loss: 4.1224\n",
      "Epoch [1/1], Step [5069/7635], Loss: 4.1738\n",
      "Epoch [1/1], Step [5070/7635], Loss: 4.2846\n",
      "Epoch [1/1], Step [5071/7635], Loss: 4.1863\n",
      "Epoch [1/1], Step [5072/7635], Loss: 4.1477\n",
      "Epoch [1/1], Step [5073/7635], Loss: 4.1254\n",
      "Epoch [1/1], Step [5074/7635], Loss: 4.2149\n",
      "Epoch [1/1], Step [5075/7635], Loss: 4.1069\n",
      "Epoch [1/1], Step [5076/7635], Loss: 4.2143\n",
      "Epoch [1/1], Step [5077/7635], Loss: 4.1403\n",
      "Epoch [1/1], Step [5078/7635], Loss: 4.2356\n",
      "Epoch [1/1], Step [5079/7635], Loss: 4.1740\n",
      "Epoch [1/1], Step [5080/7635], Loss: 4.1922\n",
      "Epoch [1/1], Step [5081/7635], Loss: 4.1657\n",
      "Epoch [1/1], Step [5082/7635], Loss: 4.2864\n",
      "Epoch [1/1], Step [5083/7635], Loss: 4.2097\n",
      "Epoch [1/1], Step [5084/7635], Loss: 4.2376\n",
      "Epoch [1/1], Step [5085/7635], Loss: 4.1311\n",
      "Epoch [1/1], Step [5086/7635], Loss: 4.1334\n",
      "Epoch [1/1], Step [5087/7635], Loss: 4.2098\n",
      "Epoch [1/1], Step [5088/7635], Loss: 4.1162\n",
      "Epoch [1/1], Step [5089/7635], Loss: 4.1197\n",
      "Epoch [1/1], Step [5090/7635], Loss: 4.1486\n",
      "Epoch [1/1], Step [5091/7635], Loss: 4.1491\n",
      "Epoch [1/1], Step [5092/7635], Loss: 4.1243\n",
      "Epoch [1/1], Step [5093/7635], Loss: 4.1417\n",
      "Epoch [1/1], Step [5094/7635], Loss: 4.2314\n",
      "Epoch [1/1], Step [5095/7635], Loss: 4.2494\n",
      "Epoch [1/1], Step [5096/7635], Loss: 4.1478\n",
      "Epoch [1/1], Step [5097/7635], Loss: 4.1799\n",
      "Epoch [1/1], Step [5098/7635], Loss: 4.1609\n",
      "Epoch [1/1], Step [5099/7635], Loss: 4.1785\n",
      "Epoch [1/1], Step [5100/7635], Loss: 4.2976\n",
      "Epoch [1/1], Step [5101/7635], Loss: 4.1430\n",
      "Epoch [1/1], Step [5102/7635], Loss: 4.2215\n",
      "Epoch [1/1], Step [5103/7635], Loss: 4.1558\n",
      "Epoch [1/1], Step [5104/7635], Loss: 4.2284\n",
      "Epoch [1/1], Step [5105/7635], Loss: 4.1918\n",
      "Epoch [1/1], Step [5106/7635], Loss: 4.1543\n",
      "Epoch [1/1], Step [5107/7635], Loss: 4.2200\n",
      "Epoch [1/1], Step [5108/7635], Loss: 4.1046\n",
      "Epoch [1/1], Step [5109/7635], Loss: 4.1708\n",
      "Epoch [1/1], Step [5110/7635], Loss: 4.2092\n",
      "Epoch [1/1], Step [5111/7635], Loss: 4.2443\n",
      "Epoch [1/1], Step [5112/7635], Loss: 4.2126\n",
      "Epoch [1/1], Step [5113/7635], Loss: 4.1616\n",
      "Epoch [1/1], Step [5114/7635], Loss: 4.2193\n",
      "Epoch [1/1], Step [5115/7635], Loss: 4.2156\n",
      "Epoch [1/1], Step [5116/7635], Loss: 4.1629\n",
      "Epoch [1/1], Step [5117/7635], Loss: 4.0644\n",
      "Epoch [1/1], Step [5118/7635], Loss: 4.0636\n",
      "Epoch [1/1], Step [5119/7635], Loss: 4.2181\n",
      "Epoch [1/1], Step [5120/7635], Loss: 4.2022\n",
      "Epoch [1/1], Step [5121/7635], Loss: 4.1527\n",
      "Epoch [1/1], Step [5122/7635], Loss: 4.1714\n",
      "Epoch [1/1], Step [5123/7635], Loss: 4.1392\n",
      "Epoch [1/1], Step [5124/7635], Loss: 4.2296\n",
      "Epoch [1/1], Step [5125/7635], Loss: 4.1084\n",
      "Epoch [1/1], Step [5126/7635], Loss: 4.1950\n",
      "Epoch [1/1], Step [5127/7635], Loss: 4.2188\n",
      "Epoch [1/1], Step [5128/7635], Loss: 4.1648\n",
      "Epoch [1/1], Step [5129/7635], Loss: 4.1638\n",
      "Epoch [1/1], Step [5130/7635], Loss: 4.2223\n",
      "Epoch [1/1], Step [5131/7635], Loss: 4.1930\n",
      "Epoch [1/1], Step [5132/7635], Loss: 4.2049\n",
      "Epoch [1/1], Step [5133/7635], Loss: 4.2090\n",
      "Epoch [1/1], Step [5134/7635], Loss: 4.1696\n",
      "Epoch [1/1], Step [5135/7635], Loss: 4.1556\n",
      "Epoch [1/1], Step [5136/7635], Loss: 4.1868\n",
      "Epoch [1/1], Step [5137/7635], Loss: 4.2087\n",
      "Epoch [1/1], Step [5138/7635], Loss: 4.1177\n",
      "Epoch [1/1], Step [5139/7635], Loss: 4.1893\n",
      "Epoch [1/1], Step [5140/7635], Loss: 4.0785\n",
      "Epoch [1/1], Step [5141/7635], Loss: 4.2059\n",
      "Epoch [1/1], Step [5142/7635], Loss: 4.1830\n",
      "Epoch [1/1], Step [5143/7635], Loss: 4.2003\n",
      "Epoch [1/1], Step [5144/7635], Loss: 4.2268\n",
      "Epoch [1/1], Step [5145/7635], Loss: 4.1643\n",
      "Epoch [1/1], Step [5146/7635], Loss: 4.2413\n",
      "Epoch [1/1], Step [5147/7635], Loss: 4.2125\n",
      "Epoch [1/1], Step [5148/7635], Loss: 4.1358\n",
      "Epoch [1/1], Step [5149/7635], Loss: 4.1832\n",
      "Epoch [1/1], Step [5150/7635], Loss: 4.0959\n",
      "Epoch [1/1], Step [5151/7635], Loss: 4.1549\n",
      "Epoch [1/1], Step [5152/7635], Loss: 4.0960\n",
      "Epoch [1/1], Step [5153/7635], Loss: 4.1872\n",
      "Epoch [1/1], Step [5154/7635], Loss: 4.3028\n",
      "Epoch [1/1], Step [5155/7635], Loss: 4.1632\n",
      "Epoch [1/1], Step [5156/7635], Loss: 4.1536\n",
      "Epoch [1/1], Step [5157/7635], Loss: 4.1346\n",
      "Epoch [1/1], Step [5158/7635], Loss: 4.1702\n",
      "Epoch [1/1], Step [5159/7635], Loss: 4.1900\n",
      "Epoch [1/1], Step [5160/7635], Loss: 4.1860\n",
      "Epoch [1/1], Step [5161/7635], Loss: 4.1708\n",
      "Epoch [1/1], Step [5162/7635], Loss: 4.2151\n",
      "Epoch [1/1], Step [5163/7635], Loss: 4.1528\n",
      "Epoch [1/1], Step [5164/7635], Loss: 4.2080\n",
      "Epoch [1/1], Step [5165/7635], Loss: 4.1351\n",
      "Epoch [1/1], Step [5166/7635], Loss: 4.1693\n",
      "Epoch [1/1], Step [5167/7635], Loss: 4.1528\n",
      "Epoch [1/1], Step [5168/7635], Loss: 4.1941\n",
      "Epoch [1/1], Step [5169/7635], Loss: 4.1688\n",
      "Epoch [1/1], Step [5170/7635], Loss: 4.1581\n",
      "Epoch [1/1], Step [5171/7635], Loss: 4.1085\n",
      "Epoch [1/1], Step [5172/7635], Loss: 4.2241\n",
      "Epoch [1/1], Step [5173/7635], Loss: 4.1915\n",
      "Epoch [1/1], Step [5174/7635], Loss: 4.1825\n",
      "Epoch [1/1], Step [5175/7635], Loss: 4.0709\n",
      "Epoch [1/1], Step [5176/7635], Loss: 4.1648\n",
      "Epoch [1/1], Step [5177/7635], Loss: 4.1740\n",
      "Epoch [1/1], Step [5178/7635], Loss: 4.1754\n",
      "Epoch [1/1], Step [5179/7635], Loss: 4.1290\n",
      "Epoch [1/1], Step [5180/7635], Loss: 4.2486\n",
      "Epoch [1/1], Step [5181/7635], Loss: 4.1987\n",
      "Epoch [1/1], Step [5182/7635], Loss: 4.1578\n",
      "Epoch [1/1], Step [5183/7635], Loss: 4.1209\n",
      "Epoch [1/1], Step [5184/7635], Loss: 4.1640\n",
      "Epoch [1/1], Step [5185/7635], Loss: 4.1879\n",
      "Epoch [1/1], Step [5186/7635], Loss: 4.2077\n",
      "Epoch [1/1], Step [5187/7635], Loss: 4.1253\n",
      "Epoch [1/1], Step [5188/7635], Loss: 4.1671\n",
      "Epoch [1/1], Step [5189/7635], Loss: 4.2153\n",
      "Epoch [1/1], Step [5190/7635], Loss: 4.1519\n",
      "Epoch [1/1], Step [5191/7635], Loss: 4.1456\n",
      "Epoch [1/1], Step [5192/7635], Loss: 4.1392\n",
      "Epoch [1/1], Step [5193/7635], Loss: 4.2453\n",
      "Epoch [1/1], Step [5194/7635], Loss: 4.1086\n",
      "Epoch [1/1], Step [5195/7635], Loss: 4.1065\n",
      "Epoch [1/1], Step [5196/7635], Loss: 4.1394\n",
      "Epoch [1/1], Step [5197/7635], Loss: 4.1497\n",
      "Epoch [1/1], Step [5198/7635], Loss: 4.2206\n",
      "Epoch [1/1], Step [5199/7635], Loss: 4.1668\n",
      "Epoch [1/1], Step [5200/7635], Loss: 4.1660\n",
      "Epoch [1/1], Step [5201/7635], Loss: 4.1186\n",
      "Epoch [1/1], Step [5202/7635], Loss: 4.1849\n",
      "Epoch [1/1], Step [5203/7635], Loss: 4.2600\n",
      "Epoch [1/1], Step [5204/7635], Loss: 4.2412\n",
      "Epoch [1/1], Step [5205/7635], Loss: 4.1549\n",
      "Epoch [1/1], Step [5206/7635], Loss: 4.2248\n",
      "Epoch [1/1], Step [5207/7635], Loss: 4.1708\n",
      "Epoch [1/1], Step [5208/7635], Loss: 4.1960\n",
      "Epoch [1/1], Step [5209/7635], Loss: 4.2474\n",
      "Epoch [1/1], Step [5210/7635], Loss: 4.1096\n",
      "Epoch [1/1], Step [5211/7635], Loss: 4.1456\n",
      "Epoch [1/1], Step [5212/7635], Loss: 4.1163\n",
      "Epoch [1/1], Step [5213/7635], Loss: 4.2042\n",
      "Epoch [1/1], Step [5214/7635], Loss: 4.2200\n",
      "Epoch [1/1], Step [5215/7635], Loss: 4.1490\n",
      "Epoch [1/1], Step [5216/7635], Loss: 4.1833\n",
      "Epoch [1/1], Step [5217/7635], Loss: 4.1424\n",
      "Epoch [1/1], Step [5218/7635], Loss: 4.0994\n",
      "Epoch [1/1], Step [5219/7635], Loss: 4.1727\n",
      "Epoch [1/1], Step [5220/7635], Loss: 4.1581\n",
      "Epoch [1/1], Step [5221/7635], Loss: 4.1402\n",
      "Epoch [1/1], Step [5222/7635], Loss: 4.0694\n",
      "Epoch [1/1], Step [5223/7635], Loss: 4.1670\n",
      "Epoch [1/1], Step [5224/7635], Loss: 4.1821\n",
      "Epoch [1/1], Step [5225/7635], Loss: 4.2370\n",
      "Epoch [1/1], Step [5226/7635], Loss: 4.1862\n",
      "Epoch [1/1], Step [5227/7635], Loss: 4.1896\n",
      "Epoch [1/1], Step [5228/7635], Loss: 4.2061\n",
      "Epoch [1/1], Step [5229/7635], Loss: 4.1528\n",
      "Epoch [1/1], Step [5230/7635], Loss: 4.1977\n",
      "Epoch [1/1], Step [5231/7635], Loss: 4.1175\n",
      "Epoch [1/1], Step [5232/7635], Loss: 4.1551\n",
      "Epoch [1/1], Step [5233/7635], Loss: 4.2085\n",
      "Epoch [1/1], Step [5234/7635], Loss: 4.2124\n",
      "Epoch [1/1], Step [5235/7635], Loss: 4.1241\n",
      "Epoch [1/1], Step [5236/7635], Loss: 4.1847\n",
      "Epoch [1/1], Step [5237/7635], Loss: 4.1768\n",
      "Epoch [1/1], Step [5238/7635], Loss: 4.2128\n",
      "Epoch [1/1], Step [5239/7635], Loss: 4.1294\n",
      "Epoch [1/1], Step [5240/7635], Loss: 4.2000\n",
      "Epoch [1/1], Step [5241/7635], Loss: 4.2071\n",
      "Epoch [1/1], Step [5242/7635], Loss: 4.2199\n",
      "Epoch [1/1], Step [5243/7635], Loss: 4.1336\n",
      "Epoch [1/1], Step [5244/7635], Loss: 4.1670\n",
      "Epoch [1/1], Step [5245/7635], Loss: 4.1995\n",
      "Epoch [1/1], Step [5246/7635], Loss: 4.2167\n",
      "Epoch [1/1], Step [5247/7635], Loss: 4.1980\n",
      "Epoch [1/1], Step [5248/7635], Loss: 4.1529\n",
      "Epoch [1/1], Step [5249/7635], Loss: 4.2491\n",
      "Epoch [1/1], Step [5250/7635], Loss: 4.1626\n",
      "Epoch [1/1], Step [5251/7635], Loss: 4.1663\n",
      "Epoch [1/1], Step [5252/7635], Loss: 4.2116\n",
      "Epoch [1/1], Step [5253/7635], Loss: 4.2145\n",
      "Epoch [1/1], Step [5254/7635], Loss: 4.1640\n",
      "Epoch [1/1], Step [5255/7635], Loss: 4.1443\n",
      "Epoch [1/1], Step [5256/7635], Loss: 4.1739\n",
      "Epoch [1/1], Step [5257/7635], Loss: 4.2156\n",
      "Epoch [1/1], Step [5258/7635], Loss: 4.1033\n",
      "Epoch [1/1], Step [5259/7635], Loss: 4.1148\n",
      "Epoch [1/1], Step [5260/7635], Loss: 4.1775\n",
      "Epoch [1/1], Step [5261/7635], Loss: 4.1650\n",
      "Epoch [1/1], Step [5262/7635], Loss: 4.2054\n",
      "Epoch [1/1], Step [5263/7635], Loss: 4.2066\n",
      "Epoch [1/1], Step [5264/7635], Loss: 4.1023\n",
      "Epoch [1/1], Step [5265/7635], Loss: 4.2339\n",
      "Epoch [1/1], Step [5266/7635], Loss: 4.0938\n",
      "Epoch [1/1], Step [5267/7635], Loss: 4.1063\n",
      "Epoch [1/1], Step [5268/7635], Loss: 4.1946\n",
      "Epoch [1/1], Step [5269/7635], Loss: 4.1444\n",
      "Epoch [1/1], Step [5270/7635], Loss: 4.2319\n",
      "Epoch [1/1], Step [5271/7635], Loss: 4.2366\n",
      "Epoch [1/1], Step [5272/7635], Loss: 4.1547\n",
      "Epoch [1/1], Step [5273/7635], Loss: 4.1541\n",
      "Epoch [1/1], Step [5274/7635], Loss: 4.2244\n",
      "Epoch [1/1], Step [5275/7635], Loss: 4.2063\n",
      "Epoch [1/1], Step [5276/7635], Loss: 4.1857\n",
      "Epoch [1/1], Step [5277/7635], Loss: 4.1392\n",
      "Epoch [1/1], Step [5278/7635], Loss: 4.1890\n",
      "Epoch [1/1], Step [5279/7635], Loss: 4.1636\n",
      "Epoch [1/1], Step [5280/7635], Loss: 4.2162\n",
      "Epoch [1/1], Step [5281/7635], Loss: 4.1744\n",
      "Epoch [1/1], Step [5282/7635], Loss: 4.1583\n",
      "Epoch [1/1], Step [5283/7635], Loss: 4.0840\n",
      "Epoch [1/1], Step [5284/7635], Loss: 4.2026\n",
      "Epoch [1/1], Step [5285/7635], Loss: 4.2103\n",
      "Epoch [1/1], Step [5286/7635], Loss: 4.1270\n",
      "Epoch [1/1], Step [5287/7635], Loss: 4.1462\n",
      "Epoch [1/1], Step [5288/7635], Loss: 4.2230\n",
      "Epoch [1/1], Step [5289/7635], Loss: 4.1951\n",
      "Epoch [1/1], Step [5290/7635], Loss: 4.1031\n",
      "Epoch [1/1], Step [5291/7635], Loss: 4.1973\n",
      "Epoch [1/1], Step [5292/7635], Loss: 4.2241\n",
      "Epoch [1/1], Step [5293/7635], Loss: 4.1686\n",
      "Epoch [1/1], Step [5294/7635], Loss: 4.2552\n",
      "Epoch [1/1], Step [5295/7635], Loss: 4.1615\n",
      "Epoch [1/1], Step [5296/7635], Loss: 4.1467\n",
      "Epoch [1/1], Step [5297/7635], Loss: 4.1238\n",
      "Epoch [1/1], Step [5298/7635], Loss: 4.1652\n",
      "Epoch [1/1], Step [5299/7635], Loss: 4.1541\n",
      "Epoch [1/1], Step [5300/7635], Loss: 4.1609\n",
      "Epoch [1/1], Step [5301/7635], Loss: 4.0975\n",
      "Epoch [1/1], Step [5302/7635], Loss: 4.2300\n",
      "Epoch [1/1], Step [5303/7635], Loss: 4.1680\n",
      "Epoch [1/1], Step [5304/7635], Loss: 4.1196\n",
      "Epoch [1/1], Step [5305/7635], Loss: 4.1704\n",
      "Epoch [1/1], Step [5306/7635], Loss: 4.1223\n",
      "Epoch [1/1], Step [5307/7635], Loss: 4.1522\n",
      "Epoch [1/1], Step [5308/7635], Loss: 4.0788\n",
      "Epoch [1/1], Step [5309/7635], Loss: 4.2048\n",
      "Epoch [1/1], Step [5310/7635], Loss: 4.1514\n",
      "Epoch [1/1], Step [5311/7635], Loss: 4.2119\n",
      "Epoch [1/1], Step [5312/7635], Loss: 4.1704\n",
      "Epoch [1/1], Step [5313/7635], Loss: 4.1211\n",
      "Epoch [1/1], Step [5314/7635], Loss: 4.1461\n",
      "Epoch [1/1], Step [5315/7635], Loss: 4.2110\n",
      "Epoch [1/1], Step [5316/7635], Loss: 4.2145\n",
      "Epoch [1/1], Step [5317/7635], Loss: 4.2094\n",
      "Epoch [1/1], Step [5318/7635], Loss: 4.1879\n",
      "Epoch [1/1], Step [5319/7635], Loss: 4.0921\n",
      "Epoch [1/1], Step [5320/7635], Loss: 4.1678\n",
      "Epoch [1/1], Step [5321/7635], Loss: 4.1128\n",
      "Epoch [1/1], Step [5322/7635], Loss: 4.1403\n",
      "Epoch [1/1], Step [5323/7635], Loss: 4.1974\n",
      "Epoch [1/1], Step [5324/7635], Loss: 4.0574\n",
      "Epoch [1/1], Step [5325/7635], Loss: 4.1703\n",
      "Epoch [1/1], Step [5326/7635], Loss: 4.1685\n",
      "Epoch [1/1], Step [5327/7635], Loss: 4.1596\n",
      "Epoch [1/1], Step [5328/7635], Loss: 4.1377\n",
      "Epoch [1/1], Step [5329/7635], Loss: 4.1692\n",
      "Epoch [1/1], Step [5330/7635], Loss: 4.1519\n",
      "Epoch [1/1], Step [5331/7635], Loss: 4.1617\n",
      "Epoch [1/1], Step [5332/7635], Loss: 4.1067\n",
      "Epoch [1/1], Step [5333/7635], Loss: 4.1507\n",
      "Epoch [1/1], Step [5334/7635], Loss: 4.0649\n",
      "Epoch [1/1], Step [5335/7635], Loss: 4.1561\n",
      "Epoch [1/1], Step [5336/7635], Loss: 4.1723\n",
      "Epoch [1/1], Step [5337/7635], Loss: 4.1804\n",
      "Epoch [1/1], Step [5338/7635], Loss: 4.1461\n",
      "Epoch [1/1], Step [5339/7635], Loss: 4.2206\n",
      "Epoch [1/1], Step [5340/7635], Loss: 4.2396\n",
      "Epoch [1/1], Step [5341/7635], Loss: 4.1106\n",
      "Epoch [1/1], Step [5342/7635], Loss: 4.0888\n",
      "Epoch [1/1], Step [5343/7635], Loss: 4.2085\n",
      "Epoch [1/1], Step [5344/7635], Loss: 4.1767\n",
      "Epoch [1/1], Step [5345/7635], Loss: 4.1606\n",
      "Epoch [1/1], Step [5346/7635], Loss: 4.2108\n",
      "Epoch [1/1], Step [5347/7635], Loss: 4.1760\n",
      "Epoch [1/1], Step [5348/7635], Loss: 4.1037\n",
      "Epoch [1/1], Step [5349/7635], Loss: 4.1392\n",
      "Epoch [1/1], Step [5350/7635], Loss: 4.1248\n",
      "Epoch [1/1], Step [5351/7635], Loss: 4.1213\n",
      "Epoch [1/1], Step [5352/7635], Loss: 4.2001\n",
      "Epoch [1/1], Step [5353/7635], Loss: 4.2269\n",
      "Epoch [1/1], Step [5354/7635], Loss: 4.1950\n",
      "Epoch [1/1], Step [5355/7635], Loss: 4.1874\n",
      "Epoch [1/1], Step [5356/7635], Loss: 4.1546\n",
      "Epoch [1/1], Step [5357/7635], Loss: 4.1711\n",
      "Epoch [1/1], Step [5358/7635], Loss: 4.1517\n",
      "Epoch [1/1], Step [5359/7635], Loss: 4.1186\n",
      "Epoch [1/1], Step [5360/7635], Loss: 4.1605\n",
      "Epoch [1/1], Step [5361/7635], Loss: 4.2005\n",
      "Epoch [1/1], Step [5362/7635], Loss: 4.1843\n",
      "Epoch [1/1], Step [5363/7635], Loss: 4.1591\n",
      "Epoch [1/1], Step [5364/7635], Loss: 4.2019\n",
      "Epoch [1/1], Step [5365/7635], Loss: 4.1171\n",
      "Epoch [1/1], Step [5366/7635], Loss: 4.2124\n",
      "Epoch [1/1], Step [5367/7635], Loss: 4.1837\n",
      "Epoch [1/1], Step [5368/7635], Loss: 4.1100\n",
      "Epoch [1/1], Step [5369/7635], Loss: 4.1894\n",
      "Epoch [1/1], Step [5370/7635], Loss: 4.0488\n",
      "Epoch [1/1], Step [5371/7635], Loss: 4.1215\n",
      "Epoch [1/1], Step [5372/7635], Loss: 4.1961\n",
      "Epoch [1/1], Step [5373/7635], Loss: 4.1621\n",
      "Epoch [1/1], Step [5374/7635], Loss: 4.1172\n",
      "Epoch [1/1], Step [5375/7635], Loss: 4.0744\n",
      "Epoch [1/1], Step [5376/7635], Loss: 4.2265\n",
      "Epoch [1/1], Step [5377/7635], Loss: 4.2276\n",
      "Epoch [1/1], Step [5378/7635], Loss: 4.1793\n",
      "Epoch [1/1], Step [5379/7635], Loss: 4.1538\n",
      "Epoch [1/1], Step [5380/7635], Loss: 4.1562\n",
      "Epoch [1/1], Step [5381/7635], Loss: 4.0783\n",
      "Epoch [1/1], Step [5382/7635], Loss: 4.1250\n",
      "Epoch [1/1], Step [5383/7635], Loss: 4.1076\n",
      "Epoch [1/1], Step [5384/7635], Loss: 4.1343\n",
      "Epoch [1/1], Step [5385/7635], Loss: 4.1937\n",
      "Epoch [1/1], Step [5386/7635], Loss: 4.1783\n",
      "Epoch [1/1], Step [5387/7635], Loss: 4.1264\n",
      "Epoch [1/1], Step [5388/7635], Loss: 4.2032\n",
      "Epoch [1/1], Step [5389/7635], Loss: 4.1423\n",
      "Epoch [1/1], Step [5390/7635], Loss: 4.1483\n",
      "Epoch [1/1], Step [5391/7635], Loss: 4.1291\n",
      "Epoch [1/1], Step [5392/7635], Loss: 4.1416\n",
      "Epoch [1/1], Step [5393/7635], Loss: 4.1789\n",
      "Epoch [1/1], Step [5394/7635], Loss: 4.1703\n",
      "Epoch [1/1], Step [5395/7635], Loss: 4.2132\n",
      "Epoch [1/1], Step [5396/7635], Loss: 4.1379\n",
      "Epoch [1/1], Step [5397/7635], Loss: 4.1931\n",
      "Epoch [1/1], Step [5398/7635], Loss: 4.2011\n",
      "Epoch [1/1], Step [5399/7635], Loss: 4.1563\n",
      "Epoch [1/1], Step [5400/7635], Loss: 4.1789\n",
      "Epoch [1/1], Step [5401/7635], Loss: 4.1397\n",
      "Epoch [1/1], Step [5402/7635], Loss: 4.1462\n",
      "Epoch [1/1], Step [5403/7635], Loss: 4.2193\n",
      "Epoch [1/1], Step [5404/7635], Loss: 4.1484\n",
      "Epoch [1/1], Step [5405/7635], Loss: 4.1149\n",
      "Epoch [1/1], Step [5406/7635], Loss: 4.1671\n",
      "Epoch [1/1], Step [5407/7635], Loss: 4.2136\n",
      "Epoch [1/1], Step [5408/7635], Loss: 4.1724\n",
      "Epoch [1/1], Step [5409/7635], Loss: 4.1744\n",
      "Epoch [1/1], Step [5410/7635], Loss: 4.1637\n",
      "Epoch [1/1], Step [5411/7635], Loss: 4.1271\n",
      "Epoch [1/1], Step [5412/7635], Loss: 4.1681\n",
      "Epoch [1/1], Step [5413/7635], Loss: 4.1347\n",
      "Epoch [1/1], Step [5414/7635], Loss: 4.2288\n",
      "Epoch [1/1], Step [5415/7635], Loss: 4.1214\n",
      "Epoch [1/1], Step [5416/7635], Loss: 4.1416\n",
      "Epoch [1/1], Step [5417/7635], Loss: 4.1908\n",
      "Epoch [1/1], Step [5418/7635], Loss: 4.1341\n",
      "Epoch [1/1], Step [5419/7635], Loss: 4.1877\n",
      "Epoch [1/1], Step [5420/7635], Loss: 4.1292\n",
      "Epoch [1/1], Step [5421/7635], Loss: 4.1317\n",
      "Epoch [1/1], Step [5422/7635], Loss: 4.1674\n",
      "Epoch [1/1], Step [5423/7635], Loss: 4.1980\n",
      "Epoch [1/1], Step [5424/7635], Loss: 4.1657\n",
      "Epoch [1/1], Step [5425/7635], Loss: 4.1766\n",
      "Epoch [1/1], Step [5426/7635], Loss: 4.1601\n",
      "Epoch [1/1], Step [5427/7635], Loss: 4.0676\n",
      "Epoch [1/1], Step [5428/7635], Loss: 4.0421\n",
      "Epoch [1/1], Step [5429/7635], Loss: 4.1233\n",
      "Epoch [1/1], Step [5430/7635], Loss: 4.2123\n",
      "Epoch [1/1], Step [5431/7635], Loss: 4.1186\n",
      "Epoch [1/1], Step [5432/7635], Loss: 4.1288\n",
      "Epoch [1/1], Step [5433/7635], Loss: 4.2191\n",
      "Epoch [1/1], Step [5434/7635], Loss: 4.1941\n",
      "Epoch [1/1], Step [5435/7635], Loss: 4.1102\n",
      "Epoch [1/1], Step [5436/7635], Loss: 4.0800\n",
      "Epoch [1/1], Step [5437/7635], Loss: 4.1723\n",
      "Epoch [1/1], Step [5438/7635], Loss: 4.2022\n",
      "Epoch [1/1], Step [5439/7635], Loss: 4.1224\n",
      "Epoch [1/1], Step [5440/7635], Loss: 4.1891\n",
      "Epoch [1/1], Step [5441/7635], Loss: 4.1095\n",
      "Epoch [1/1], Step [5442/7635], Loss: 4.2081\n",
      "Epoch [1/1], Step [5443/7635], Loss: 4.1734\n",
      "Epoch [1/1], Step [5444/7635], Loss: 4.0791\n",
      "Epoch [1/1], Step [5445/7635], Loss: 4.2568\n",
      "Epoch [1/1], Step [5446/7635], Loss: 4.0987\n",
      "Epoch [1/1], Step [5447/7635], Loss: 4.1352\n",
      "Epoch [1/1], Step [5448/7635], Loss: 4.1585\n",
      "Epoch [1/1], Step [5449/7635], Loss: 4.1672\n",
      "Epoch [1/1], Step [5450/7635], Loss: 4.1841\n",
      "Epoch [1/1], Step [5451/7635], Loss: 4.1360\n",
      "Epoch [1/1], Step [5452/7635], Loss: 4.1924\n",
      "Epoch [1/1], Step [5453/7635], Loss: 4.1051\n",
      "Epoch [1/1], Step [5454/7635], Loss: 4.2733\n",
      "Epoch [1/1], Step [5455/7635], Loss: 4.1853\n",
      "Epoch [1/1], Step [5456/7635], Loss: 4.2455\n",
      "Epoch [1/1], Step [5457/7635], Loss: 4.1885\n",
      "Epoch [1/1], Step [5458/7635], Loss: 4.1714\n",
      "Epoch [1/1], Step [5459/7635], Loss: 4.1657\n",
      "Epoch [1/1], Step [5460/7635], Loss: 4.1869\n",
      "Epoch [1/1], Step [5461/7635], Loss: 4.1769\n",
      "Epoch [1/1], Step [5462/7635], Loss: 4.1845\n",
      "Epoch [1/1], Step [5463/7635], Loss: 4.2908\n",
      "Epoch [1/1], Step [5464/7635], Loss: 4.1787\n",
      "Epoch [1/1], Step [5465/7635], Loss: 4.1254\n",
      "Epoch [1/1], Step [5466/7635], Loss: 4.1740\n",
      "Epoch [1/1], Step [5467/7635], Loss: 4.1389\n",
      "Epoch [1/1], Step [5468/7635], Loss: 4.1055\n",
      "Epoch [1/1], Step [5469/7635], Loss: 4.1391\n",
      "Epoch [1/1], Step [5470/7635], Loss: 4.2286\n",
      "Epoch [1/1], Step [5471/7635], Loss: 4.1512\n",
      "Epoch [1/1], Step [5472/7635], Loss: 4.1452\n",
      "Epoch [1/1], Step [5473/7635], Loss: 4.1817\n",
      "Epoch [1/1], Step [5474/7635], Loss: 4.1368\n",
      "Epoch [1/1], Step [5475/7635], Loss: 4.2445\n",
      "Epoch [1/1], Step [5476/7635], Loss: 4.1807\n",
      "Epoch [1/1], Step [5477/7635], Loss: 4.1795\n",
      "Epoch [1/1], Step [5478/7635], Loss: 4.1959\n",
      "Epoch [1/1], Step [5479/7635], Loss: 4.2169\n",
      "Epoch [1/1], Step [5480/7635], Loss: 4.1977\n",
      "Epoch [1/1], Step [5481/7635], Loss: 4.1736\n",
      "Epoch [1/1], Step [5482/7635], Loss: 4.1704\n",
      "Epoch [1/1], Step [5483/7635], Loss: 4.1662\n",
      "Epoch [1/1], Step [5484/7635], Loss: 4.1553\n",
      "Epoch [1/1], Step [5485/7635], Loss: 4.1990\n",
      "Epoch [1/1], Step [5486/7635], Loss: 4.1230\n",
      "Epoch [1/1], Step [5487/7635], Loss: 4.1517\n",
      "Epoch [1/1], Step [5488/7635], Loss: 4.0981\n",
      "Epoch [1/1], Step [5489/7635], Loss: 4.0961\n",
      "Epoch [1/1], Step [5490/7635], Loss: 4.1547\n",
      "Epoch [1/1], Step [5491/7635], Loss: 4.1289\n",
      "Epoch [1/1], Step [5492/7635], Loss: 4.1765\n",
      "Epoch [1/1], Step [5493/7635], Loss: 4.0876\n",
      "Epoch [1/1], Step [5494/7635], Loss: 4.1767\n",
      "Epoch [1/1], Step [5495/7635], Loss: 4.1418\n",
      "Epoch [1/1], Step [5496/7635], Loss: 4.1593\n",
      "Epoch [1/1], Step [5497/7635], Loss: 4.1206\n",
      "Epoch [1/1], Step [5498/7635], Loss: 4.1955\n",
      "Epoch [1/1], Step [5499/7635], Loss: 4.1592\n",
      "Epoch [1/1], Step [5500/7635], Loss: 4.1638\n",
      "Epoch [1/1], Step [5501/7635], Loss: 4.2155\n",
      "Epoch [1/1], Step [5502/7635], Loss: 4.1558\n",
      "Epoch [1/1], Step [5503/7635], Loss: 4.1604\n",
      "Epoch [1/1], Step [5504/7635], Loss: 4.1406\n",
      "Epoch [1/1], Step [5505/7635], Loss: 4.1262\n",
      "Epoch [1/1], Step [5506/7635], Loss: 4.1862\n",
      "Epoch [1/1], Step [5507/7635], Loss: 4.1835\n",
      "Epoch [1/1], Step [5508/7635], Loss: 4.0695\n",
      "Epoch [1/1], Step [5509/7635], Loss: 4.1623\n",
      "Epoch [1/1], Step [5510/7635], Loss: 4.0865\n",
      "Epoch [1/1], Step [5511/7635], Loss: 4.2370\n",
      "Epoch [1/1], Step [5512/7635], Loss: 4.1628\n",
      "Epoch [1/1], Step [5513/7635], Loss: 4.1096\n",
      "Epoch [1/1], Step [5514/7635], Loss: 4.1519\n",
      "Epoch [1/1], Step [5515/7635], Loss: 4.1269\n",
      "Epoch [1/1], Step [5516/7635], Loss: 4.1868\n",
      "Epoch [1/1], Step [5517/7635], Loss: 4.2180\n",
      "Epoch [1/1], Step [5518/7635], Loss: 4.1737\n",
      "Epoch [1/1], Step [5519/7635], Loss: 4.1752\n",
      "Epoch [1/1], Step [5520/7635], Loss: 4.1184\n",
      "Epoch [1/1], Step [5521/7635], Loss: 4.2858\n",
      "Epoch [1/1], Step [5522/7635], Loss: 4.2267\n",
      "Epoch [1/1], Step [5523/7635], Loss: 4.1912\n",
      "Epoch [1/1], Step [5524/7635], Loss: 4.1559\n",
      "Epoch [1/1], Step [5525/7635], Loss: 4.0351\n",
      "Epoch [1/1], Step [5526/7635], Loss: 4.0903\n",
      "Epoch [1/1], Step [5527/7635], Loss: 4.1353\n",
      "Epoch [1/1], Step [5528/7635], Loss: 4.0756\n",
      "Epoch [1/1], Step [5529/7635], Loss: 4.1129\n",
      "Epoch [1/1], Step [5530/7635], Loss: 4.1208\n",
      "Epoch [1/1], Step [5531/7635], Loss: 4.1251\n",
      "Epoch [1/1], Step [5532/7635], Loss: 4.0893\n",
      "Epoch [1/1], Step [5533/7635], Loss: 4.1157\n",
      "Epoch [1/1], Step [5534/7635], Loss: 4.1971\n",
      "Epoch [1/1], Step [5535/7635], Loss: 4.1631\n",
      "Epoch [1/1], Step [5536/7635], Loss: 4.2315\n",
      "Epoch [1/1], Step [5537/7635], Loss: 4.1678\n",
      "Epoch [1/1], Step [5538/7635], Loss: 4.1215\n",
      "Epoch [1/1], Step [5539/7635], Loss: 4.2078\n",
      "Epoch [1/1], Step [5540/7635], Loss: 4.1741\n",
      "Epoch [1/1], Step [5541/7635], Loss: 4.1290\n",
      "Epoch [1/1], Step [5542/7635], Loss: 4.1672\n",
      "Epoch [1/1], Step [5543/7635], Loss: 4.1335\n",
      "Epoch [1/1], Step [5544/7635], Loss: 4.1747\n",
      "Epoch [1/1], Step [5545/7635], Loss: 4.1108\n",
      "Epoch [1/1], Step [5546/7635], Loss: 4.1263\n",
      "Epoch [1/1], Step [5547/7635], Loss: 4.1775\n",
      "Epoch [1/1], Step [5548/7635], Loss: 4.2000\n",
      "Epoch [1/1], Step [5549/7635], Loss: 4.1151\n",
      "Epoch [1/1], Step [5550/7635], Loss: 4.1860\n",
      "Epoch [1/1], Step [5551/7635], Loss: 4.1680\n",
      "Epoch [1/1], Step [5552/7635], Loss: 4.1963\n",
      "Epoch [1/1], Step [5553/7635], Loss: 4.1524\n",
      "Epoch [1/1], Step [5554/7635], Loss: 4.1372\n",
      "Epoch [1/1], Step [5555/7635], Loss: 4.1931\n",
      "Epoch [1/1], Step [5556/7635], Loss: 4.1294\n",
      "Epoch [1/1], Step [5557/7635], Loss: 4.2447\n",
      "Epoch [1/1], Step [5558/7635], Loss: 4.1652\n",
      "Epoch [1/1], Step [5559/7635], Loss: 4.1472\n",
      "Epoch [1/1], Step [5560/7635], Loss: 4.1322\n",
      "Epoch [1/1], Step [5561/7635], Loss: 4.2552\n",
      "Epoch [1/1], Step [5562/7635], Loss: 4.2170\n",
      "Epoch [1/1], Step [5563/7635], Loss: 4.1928\n",
      "Epoch [1/1], Step [5564/7635], Loss: 4.1551\n",
      "Epoch [1/1], Step [5565/7635], Loss: 4.1820\n",
      "Epoch [1/1], Step [5566/7635], Loss: 4.1823\n",
      "Epoch [1/1], Step [5567/7635], Loss: 4.1579\n",
      "Epoch [1/1], Step [5568/7635], Loss: 4.1074\n",
      "Epoch [1/1], Step [5569/7635], Loss: 4.1205\n",
      "Epoch [1/1], Step [5570/7635], Loss: 4.0645\n",
      "Epoch [1/1], Step [5571/7635], Loss: 4.1519\n",
      "Epoch [1/1], Step [5572/7635], Loss: 4.1687\n",
      "Epoch [1/1], Step [5573/7635], Loss: 4.1244\n",
      "Epoch [1/1], Step [5574/7635], Loss: 4.1469\n",
      "Epoch [1/1], Step [5575/7635], Loss: 4.0514\n",
      "Epoch [1/1], Step [5576/7635], Loss: 4.0973\n",
      "Epoch [1/1], Step [5577/7635], Loss: 4.1268\n",
      "Epoch [1/1], Step [5578/7635], Loss: 4.1691\n",
      "Epoch [1/1], Step [5579/7635], Loss: 4.1163\n",
      "Epoch [1/1], Step [5580/7635], Loss: 4.0993\n",
      "Epoch [1/1], Step [5581/7635], Loss: 4.1545\n",
      "Epoch [1/1], Step [5582/7635], Loss: 4.2291\n",
      "Epoch [1/1], Step [5583/7635], Loss: 4.1651\n",
      "Epoch [1/1], Step [5584/7635], Loss: 4.1986\n",
      "Epoch [1/1], Step [5585/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [5586/7635], Loss: 4.0892\n",
      "Epoch [1/1], Step [5587/7635], Loss: 4.1607\n",
      "Epoch [1/1], Step [5588/7635], Loss: 4.1293\n",
      "Epoch [1/1], Step [5589/7635], Loss: 4.1259\n",
      "Epoch [1/1], Step [5590/7635], Loss: 4.2113\n",
      "Epoch [1/1], Step [5591/7635], Loss: 4.1639\n",
      "Epoch [1/1], Step [5592/7635], Loss: 4.1165\n",
      "Epoch [1/1], Step [5593/7635], Loss: 4.0977\n",
      "Epoch [1/1], Step [5594/7635], Loss: 4.2314\n",
      "Epoch [1/1], Step [5595/7635], Loss: 4.1145\n",
      "Epoch [1/1], Step [5596/7635], Loss: 4.1356\n",
      "Epoch [1/1], Step [5597/7635], Loss: 4.0958\n",
      "Epoch [1/1], Step [5598/7635], Loss: 4.1888\n",
      "Epoch [1/1], Step [5599/7635], Loss: 4.2329\n",
      "Epoch [1/1], Step [5600/7635], Loss: 4.1883\n",
      "Epoch [1/1], Step [5601/7635], Loss: 4.2607\n",
      "Epoch [1/1], Step [5602/7635], Loss: 4.1511\n",
      "Epoch [1/1], Step [5603/7635], Loss: 4.1533\n",
      "Epoch [1/1], Step [5604/7635], Loss: 4.1315\n",
      "Epoch [1/1], Step [5605/7635], Loss: 4.1858\n",
      "Epoch [1/1], Step [5606/7635], Loss: 4.1681\n",
      "Epoch [1/1], Step [5607/7635], Loss: 4.1247\n",
      "Epoch [1/1], Step [5608/7635], Loss: 4.1397\n",
      "Epoch [1/1], Step [5609/7635], Loss: 4.1634\n",
      "Epoch [1/1], Step [5610/7635], Loss: 4.0890\n",
      "Epoch [1/1], Step [5611/7635], Loss: 4.1166\n",
      "Epoch [1/1], Step [5612/7635], Loss: 4.1580\n",
      "Epoch [1/1], Step [5613/7635], Loss: 4.1613\n",
      "Epoch [1/1], Step [5614/7635], Loss: 4.0881\n",
      "Epoch [1/1], Step [5615/7635], Loss: 4.1513\n",
      "Epoch [1/1], Step [5616/7635], Loss: 4.3011\n",
      "Epoch [1/1], Step [5617/7635], Loss: 4.2112\n",
      "Epoch [1/1], Step [5618/7635], Loss: 4.1737\n",
      "Epoch [1/1], Step [5619/7635], Loss: 4.1201\n",
      "Epoch [1/1], Step [5620/7635], Loss: 4.0928\n",
      "Epoch [1/1], Step [5621/7635], Loss: 4.1655\n",
      "Epoch [1/1], Step [5622/7635], Loss: 4.1097\n",
      "Epoch [1/1], Step [5623/7635], Loss: 4.2014\n",
      "Epoch [1/1], Step [5624/7635], Loss: 4.1214\n",
      "Epoch [1/1], Step [5625/7635], Loss: 4.1926\n",
      "Epoch [1/1], Step [5626/7635], Loss: 4.1947\n",
      "Epoch [1/1], Step [5627/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [5628/7635], Loss: 4.2042\n",
      "Epoch [1/1], Step [5629/7635], Loss: 4.0841\n",
      "Epoch [1/1], Step [5630/7635], Loss: 4.2409\n",
      "Epoch [1/1], Step [5631/7635], Loss: 4.1575\n",
      "Epoch [1/1], Step [5632/7635], Loss: 4.0890\n",
      "Epoch [1/1], Step [5633/7635], Loss: 4.1355\n",
      "Epoch [1/1], Step [5634/7635], Loss: 4.2195\n",
      "Epoch [1/1], Step [5635/7635], Loss: 4.1454\n",
      "Epoch [1/1], Step [5636/7635], Loss: 4.0872\n",
      "Epoch [1/1], Step [5637/7635], Loss: 4.1440\n",
      "Epoch [1/1], Step [5638/7635], Loss: 4.0849\n",
      "Epoch [1/1], Step [5639/7635], Loss: 4.1753\n",
      "Epoch [1/1], Step [5640/7635], Loss: 4.1340\n",
      "Epoch [1/1], Step [5641/7635], Loss: 4.1393\n",
      "Epoch [1/1], Step [5642/7635], Loss: 4.0838\n",
      "Epoch [1/1], Step [5643/7635], Loss: 4.1451\n",
      "Epoch [1/1], Step [5644/7635], Loss: 4.2463\n",
      "Epoch [1/1], Step [5645/7635], Loss: 4.1767\n",
      "Epoch [1/1], Step [5646/7635], Loss: 4.2026\n",
      "Epoch [1/1], Step [5647/7635], Loss: 4.1556\n",
      "Epoch [1/1], Step [5648/7635], Loss: 4.0119\n",
      "Epoch [1/1], Step [5649/7635], Loss: 4.1967\n",
      "Epoch [1/1], Step [5650/7635], Loss: 4.1747\n",
      "Epoch [1/1], Step [5651/7635], Loss: 4.0988\n",
      "Epoch [1/1], Step [5652/7635], Loss: 4.1253\n",
      "Epoch [1/1], Step [5653/7635], Loss: 4.1824\n",
      "Epoch [1/1], Step [5654/7635], Loss: 4.0954\n",
      "Epoch [1/1], Step [5655/7635], Loss: 4.1577\n",
      "Epoch [1/1], Step [5656/7635], Loss: 4.1588\n",
      "Epoch [1/1], Step [5657/7635], Loss: 4.2890\n",
      "Epoch [1/1], Step [5658/7635], Loss: 4.1178\n",
      "Epoch [1/1], Step [5659/7635], Loss: 4.0933\n",
      "Epoch [1/1], Step [5660/7635], Loss: 4.1232\n",
      "Epoch [1/1], Step [5661/7635], Loss: 4.1262\n",
      "Epoch [1/1], Step [5662/7635], Loss: 4.1328\n",
      "Epoch [1/1], Step [5663/7635], Loss: 4.1543\n",
      "Epoch [1/1], Step [5664/7635], Loss: 4.1809\n",
      "Epoch [1/1], Step [5665/7635], Loss: 4.1271\n",
      "Epoch [1/1], Step [5666/7635], Loss: 4.1867\n",
      "Epoch [1/1], Step [5667/7635], Loss: 4.0702\n",
      "Epoch [1/1], Step [5668/7635], Loss: 4.1926\n",
      "Epoch [1/1], Step [5669/7635], Loss: 4.1676\n",
      "Epoch [1/1], Step [5670/7635], Loss: 4.1280\n",
      "Epoch [1/1], Step [5671/7635], Loss: 4.2327\n",
      "Epoch [1/1], Step [5672/7635], Loss: 4.1393\n",
      "Epoch [1/1], Step [5673/7635], Loss: 4.1327\n",
      "Epoch [1/1], Step [5674/7635], Loss: 4.1270\n",
      "Epoch [1/1], Step [5675/7635], Loss: 4.1246\n",
      "Epoch [1/1], Step [5676/7635], Loss: 4.2059\n",
      "Epoch [1/1], Step [5677/7635], Loss: 4.1078\n",
      "Epoch [1/1], Step [5678/7635], Loss: 4.2001\n",
      "Epoch [1/1], Step [5679/7635], Loss: 4.1867\n",
      "Epoch [1/1], Step [5680/7635], Loss: 4.1745\n",
      "Epoch [1/1], Step [5681/7635], Loss: 4.0750\n",
      "Epoch [1/1], Step [5682/7635], Loss: 4.1305\n",
      "Epoch [1/1], Step [5683/7635], Loss: 4.1289\n",
      "Epoch [1/1], Step [5684/7635], Loss: 4.1882\n",
      "Epoch [1/1], Step [5685/7635], Loss: 4.1055\n",
      "Epoch [1/1], Step [5686/7635], Loss: 4.0925\n",
      "Epoch [1/1], Step [5687/7635], Loss: 4.1683\n",
      "Epoch [1/1], Step [5688/7635], Loss: 4.1490\n",
      "Epoch [1/1], Step [5689/7635], Loss: 4.1910\n",
      "Epoch [1/1], Step [5690/7635], Loss: 4.1707\n",
      "Epoch [1/1], Step [5691/7635], Loss: 4.1604\n",
      "Epoch [1/1], Step [5692/7635], Loss: 4.1640\n",
      "Epoch [1/1], Step [5693/7635], Loss: 4.1318\n",
      "Epoch [1/1], Step [5694/7635], Loss: 4.1008\n",
      "Epoch [1/1], Step [5695/7635], Loss: 4.1075\n",
      "Epoch [1/1], Step [5696/7635], Loss: 4.2067\n",
      "Epoch [1/1], Step [5697/7635], Loss: 4.2056\n",
      "Epoch [1/1], Step [5698/7635], Loss: 4.1190\n",
      "Epoch [1/1], Step [5699/7635], Loss: 4.1663\n",
      "Epoch [1/1], Step [5700/7635], Loss: 4.1937\n",
      "Epoch [1/1], Step [5701/7635], Loss: 4.0378\n",
      "Epoch [1/1], Step [5702/7635], Loss: 4.1568\n",
      "Epoch [1/1], Step [5703/7635], Loss: 4.0776\n",
      "Epoch [1/1], Step [5704/7635], Loss: 4.1725\n",
      "Epoch [1/1], Step [5705/7635], Loss: 4.1967\n",
      "Epoch [1/1], Step [5706/7635], Loss: 4.1888\n",
      "Epoch [1/1], Step [5707/7635], Loss: 4.0949\n",
      "Epoch [1/1], Step [5708/7635], Loss: 4.1337\n",
      "Epoch [1/1], Step [5709/7635], Loss: 4.2319\n",
      "Epoch [1/1], Step [5710/7635], Loss: 4.1723\n",
      "Epoch [1/1], Step [5711/7635], Loss: 4.1218\n",
      "Epoch [1/1], Step [5712/7635], Loss: 4.1796\n",
      "Epoch [1/1], Step [5713/7635], Loss: 4.1369\n",
      "Epoch [1/1], Step [5714/7635], Loss: 4.1157\n",
      "Epoch [1/1], Step [5715/7635], Loss: 4.0897\n",
      "Epoch [1/1], Step [5716/7635], Loss: 4.1434\n",
      "Epoch [1/1], Step [5717/7635], Loss: 4.1634\n",
      "Epoch [1/1], Step [5718/7635], Loss: 4.1567\n",
      "Epoch [1/1], Step [5719/7635], Loss: 4.0485\n",
      "Epoch [1/1], Step [5720/7635], Loss: 4.2087\n",
      "Epoch [1/1], Step [5721/7635], Loss: 4.1431\n",
      "Epoch [1/1], Step [5722/7635], Loss: 4.1624\n",
      "Epoch [1/1], Step [5723/7635], Loss: 4.1343\n",
      "Epoch [1/1], Step [5724/7635], Loss: 4.2521\n",
      "Epoch [1/1], Step [5725/7635], Loss: 4.1058\n",
      "Epoch [1/1], Step [5726/7635], Loss: 4.1124\n",
      "Epoch [1/1], Step [5727/7635], Loss: 4.1582\n",
      "Epoch [1/1], Step [5728/7635], Loss: 4.1642\n",
      "Epoch [1/1], Step [5729/7635], Loss: 4.0910\n",
      "Epoch [1/1], Step [5730/7635], Loss: 4.1781\n",
      "Epoch [1/1], Step [5731/7635], Loss: 4.2330\n",
      "Epoch [1/1], Step [5732/7635], Loss: 4.1451\n",
      "Epoch [1/1], Step [5733/7635], Loss: 4.1676\n",
      "Epoch [1/1], Step [5734/7635], Loss: 4.2747\n",
      "Epoch [1/1], Step [5735/7635], Loss: 4.1610\n",
      "Epoch [1/1], Step [5736/7635], Loss: 4.1183\n",
      "Epoch [1/1], Step [5737/7635], Loss: 4.1803\n",
      "Epoch [1/1], Step [5738/7635], Loss: 4.1197\n",
      "Epoch [1/1], Step [5739/7635], Loss: 4.1441\n",
      "Epoch [1/1], Step [5740/7635], Loss: 4.1991\n",
      "Epoch [1/1], Step [5741/7635], Loss: 4.1240\n",
      "Epoch [1/1], Step [5742/7635], Loss: 4.1907\n",
      "Epoch [1/1], Step [5743/7635], Loss: 4.1819\n",
      "Epoch [1/1], Step [5744/7635], Loss: 4.0526\n",
      "Epoch [1/1], Step [5745/7635], Loss: 4.0853\n",
      "Epoch [1/1], Step [5746/7635], Loss: 4.0846\n",
      "Epoch [1/1], Step [5747/7635], Loss: 4.1219\n",
      "Epoch [1/1], Step [5748/7635], Loss: 4.1485\n",
      "Epoch [1/1], Step [5749/7635], Loss: 4.2097\n",
      "Epoch [1/1], Step [5750/7635], Loss: 4.1997\n",
      "Epoch [1/1], Step [5751/7635], Loss: 4.1202\n",
      "Epoch [1/1], Step [5752/7635], Loss: 4.1554\n",
      "Epoch [1/1], Step [5753/7635], Loss: 4.1204\n",
      "Epoch [1/1], Step [5754/7635], Loss: 4.2229\n",
      "Epoch [1/1], Step [5755/7635], Loss: 4.1649\n",
      "Epoch [1/1], Step [5756/7635], Loss: 4.1830\n",
      "Epoch [1/1], Step [5757/7635], Loss: 4.1303\n",
      "Epoch [1/1], Step [5758/7635], Loss: 4.2130\n",
      "Epoch [1/1], Step [5759/7635], Loss: 4.1334\n",
      "Epoch [1/1], Step [5760/7635], Loss: 4.1731\n",
      "Epoch [1/1], Step [5761/7635], Loss: 4.1577\n",
      "Epoch [1/1], Step [5762/7635], Loss: 4.0785\n",
      "Epoch [1/1], Step [5763/7635], Loss: 4.1859\n",
      "Epoch [1/1], Step [5764/7635], Loss: 4.1218\n",
      "Epoch [1/1], Step [5765/7635], Loss: 4.1198\n",
      "Epoch [1/1], Step [5766/7635], Loss: 4.1649\n",
      "Epoch [1/1], Step [5767/7635], Loss: 4.2007\n",
      "Epoch [1/1], Step [5768/7635], Loss: 4.1949\n",
      "Epoch [1/1], Step [5769/7635], Loss: 4.1411\n",
      "Epoch [1/1], Step [5770/7635], Loss: 4.1279\n",
      "Epoch [1/1], Step [5771/7635], Loss: 4.1329\n",
      "Epoch [1/1], Step [5772/7635], Loss: 4.1253\n",
      "Epoch [1/1], Step [5773/7635], Loss: 4.1482\n",
      "Epoch [1/1], Step [5774/7635], Loss: 4.1425\n",
      "Epoch [1/1], Step [5775/7635], Loss: 4.1687\n",
      "Epoch [1/1], Step [5776/7635], Loss: 4.1058\n",
      "Epoch [1/1], Step [5777/7635], Loss: 4.1039\n",
      "Epoch [1/1], Step [5778/7635], Loss: 4.1217\n",
      "Epoch [1/1], Step [5779/7635], Loss: 4.1604\n",
      "Epoch [1/1], Step [5780/7635], Loss: 4.0891\n",
      "Epoch [1/1], Step [5781/7635], Loss: 4.1375\n",
      "Epoch [1/1], Step [5782/7635], Loss: 4.1126\n",
      "Epoch [1/1], Step [5783/7635], Loss: 4.1212\n",
      "Epoch [1/1], Step [5784/7635], Loss: 4.1381\n",
      "Epoch [1/1], Step [5785/7635], Loss: 4.0860\n",
      "Epoch [1/1], Step [5786/7635], Loss: 4.1112\n",
      "Epoch [1/1], Step [5787/7635], Loss: 4.1592\n",
      "Epoch [1/1], Step [5788/7635], Loss: 4.1251\n",
      "Epoch [1/1], Step [5789/7635], Loss: 4.1540\n",
      "Epoch [1/1], Step [5790/7635], Loss: 4.1822\n",
      "Epoch [1/1], Step [5791/7635], Loss: 4.1781\n",
      "Epoch [1/1], Step [5792/7635], Loss: 4.0893\n",
      "Epoch [1/1], Step [5793/7635], Loss: 4.1671\n",
      "Epoch [1/1], Step [5794/7635], Loss: 4.1654\n",
      "Epoch [1/1], Step [5795/7635], Loss: 4.1991\n",
      "Epoch [1/1], Step [5796/7635], Loss: 4.1609\n",
      "Epoch [1/1], Step [5797/7635], Loss: 4.1005\n",
      "Epoch [1/1], Step [5798/7635], Loss: 4.1468\n",
      "Epoch [1/1], Step [5799/7635], Loss: 4.1372\n",
      "Epoch [1/1], Step [5800/7635], Loss: 4.2263\n",
      "Epoch [1/1], Step [5801/7635], Loss: 4.2229\n",
      "Epoch [1/1], Step [5802/7635], Loss: 4.1373\n",
      "Epoch [1/1], Step [5803/7635], Loss: 4.1564\n",
      "Epoch [1/1], Step [5804/7635], Loss: 4.1524\n",
      "Epoch [1/1], Step [5805/7635], Loss: 4.1254\n",
      "Epoch [1/1], Step [5806/7635], Loss: 4.1742\n",
      "Epoch [1/1], Step [5807/7635], Loss: 4.1610\n",
      "Epoch [1/1], Step [5808/7635], Loss: 4.0810\n",
      "Epoch [1/1], Step [5809/7635], Loss: 4.1639\n",
      "Epoch [1/1], Step [5810/7635], Loss: 4.0932\n",
      "Epoch [1/1], Step [5811/7635], Loss: 4.1729\n",
      "Epoch [1/1], Step [5812/7635], Loss: 4.1255\n",
      "Epoch [1/1], Step [5813/7635], Loss: 4.1826\n",
      "Epoch [1/1], Step [5814/7635], Loss: 4.0902\n",
      "Epoch [1/1], Step [5815/7635], Loss: 4.1656\n",
      "Epoch [1/1], Step [5816/7635], Loss: 4.0746\n",
      "Epoch [1/1], Step [5817/7635], Loss: 4.1087\n",
      "Epoch [1/1], Step [5818/7635], Loss: 4.0925\n",
      "Epoch [1/1], Step [5819/7635], Loss: 4.1802\n",
      "Epoch [1/1], Step [5820/7635], Loss: 4.1267\n",
      "Epoch [1/1], Step [5821/7635], Loss: 4.1431\n",
      "Epoch [1/1], Step [5822/7635], Loss: 4.1384\n",
      "Epoch [1/1], Step [5823/7635], Loss: 4.1881\n",
      "Epoch [1/1], Step [5824/7635], Loss: 4.1274\n",
      "Epoch [1/1], Step [5825/7635], Loss: 4.1514\n",
      "Epoch [1/1], Step [5826/7635], Loss: 4.1912\n",
      "Epoch [1/1], Step [5827/7635], Loss: 4.1772\n",
      "Epoch [1/1], Step [5828/7635], Loss: 4.2119\n",
      "Epoch [1/1], Step [5829/7635], Loss: 4.2097\n",
      "Epoch [1/1], Step [5830/7635], Loss: 4.1474\n",
      "Epoch [1/1], Step [5831/7635], Loss: 4.0944\n",
      "Epoch [1/1], Step [5832/7635], Loss: 4.1122\n",
      "Epoch [1/1], Step [5833/7635], Loss: 4.1668\n",
      "Epoch [1/1], Step [5834/7635], Loss: 4.1715\n",
      "Epoch [1/1], Step [5835/7635], Loss: 4.1233\n",
      "Epoch [1/1], Step [5836/7635], Loss: 4.0796\n",
      "Epoch [1/1], Step [5837/7635], Loss: 4.1193\n",
      "Epoch [1/1], Step [5838/7635], Loss: 4.0949\n",
      "Epoch [1/1], Step [5839/7635], Loss: 4.1454\n",
      "Epoch [1/1], Step [5840/7635], Loss: 4.1447\n",
      "Epoch [1/1], Step [5841/7635], Loss: 4.0762\n",
      "Epoch [1/1], Step [5842/7635], Loss: 4.1674\n",
      "Epoch [1/1], Step [5843/7635], Loss: 4.0965\n",
      "Epoch [1/1], Step [5844/7635], Loss: 4.1329\n",
      "Epoch [1/1], Step [5845/7635], Loss: 4.1350\n",
      "Epoch [1/1], Step [5846/7635], Loss: 4.1512\n",
      "Epoch [1/1], Step [5847/7635], Loss: 4.1512\n",
      "Epoch [1/1], Step [5848/7635], Loss: 4.1639\n",
      "Epoch [1/1], Step [5849/7635], Loss: 4.0848\n",
      "Epoch [1/1], Step [5850/7635], Loss: 4.1687\n",
      "Epoch [1/1], Step [5851/7635], Loss: 4.1954\n",
      "Epoch [1/1], Step [5852/7635], Loss: 4.1443\n",
      "Epoch [1/1], Step [5853/7635], Loss: 4.1602\n",
      "Epoch [1/1], Step [5854/7635], Loss: 4.1184\n",
      "Epoch [1/1], Step [5855/7635], Loss: 4.1651\n",
      "Epoch [1/1], Step [5856/7635], Loss: 4.1546\n",
      "Epoch [1/1], Step [5857/7635], Loss: 4.0757\n",
      "Epoch [1/1], Step [5858/7635], Loss: 4.1781\n",
      "Epoch [1/1], Step [5859/7635], Loss: 4.1322\n",
      "Epoch [1/1], Step [5860/7635], Loss: 4.0943\n",
      "Epoch [1/1], Step [5861/7635], Loss: 4.1682\n",
      "Epoch [1/1], Step [5862/7635], Loss: 4.1016\n",
      "Epoch [1/1], Step [5863/7635], Loss: 4.1438\n",
      "Epoch [1/1], Step [5864/7635], Loss: 4.1326\n",
      "Epoch [1/1], Step [5865/7635], Loss: 4.2586\n",
      "Epoch [1/1], Step [5866/7635], Loss: 4.1306\n",
      "Epoch [1/1], Step [5867/7635], Loss: 4.1739\n",
      "Epoch [1/1], Step [5868/7635], Loss: 4.1531\n",
      "Epoch [1/1], Step [5869/7635], Loss: 4.2044\n",
      "Epoch [1/1], Step [5870/7635], Loss: 4.1497\n",
      "Epoch [1/1], Step [5871/7635], Loss: 4.0930\n",
      "Epoch [1/1], Step [5872/7635], Loss: 4.1365\n",
      "Epoch [1/1], Step [5873/7635], Loss: 4.1537\n",
      "Epoch [1/1], Step [5874/7635], Loss: 4.0288\n",
      "Epoch [1/1], Step [5875/7635], Loss: 4.0766\n",
      "Epoch [1/1], Step [5876/7635], Loss: 4.1249\n",
      "Epoch [1/1], Step [5877/7635], Loss: 4.1877\n",
      "Epoch [1/1], Step [5878/7635], Loss: 4.1361\n",
      "Epoch [1/1], Step [5879/7635], Loss: 4.0897\n",
      "Epoch [1/1], Step [5880/7635], Loss: 4.2213\n",
      "Epoch [1/1], Step [5881/7635], Loss: 4.1490\n",
      "Epoch [1/1], Step [5882/7635], Loss: 4.1209\n",
      "Epoch [1/1], Step [5883/7635], Loss: 4.0930\n",
      "Epoch [1/1], Step [5884/7635], Loss: 4.1898\n",
      "Epoch [1/1], Step [5885/7635], Loss: 4.0850\n",
      "Epoch [1/1], Step [5886/7635], Loss: 4.1678\n",
      "Epoch [1/1], Step [5887/7635], Loss: 4.2305\n",
      "Epoch [1/1], Step [5888/7635], Loss: 4.1708\n",
      "Epoch [1/1], Step [5889/7635], Loss: 4.1120\n",
      "Epoch [1/1], Step [5890/7635], Loss: 4.1692\n",
      "Epoch [1/1], Step [5891/7635], Loss: 4.1035\n",
      "Epoch [1/1], Step [5892/7635], Loss: 4.2727\n",
      "Epoch [1/1], Step [5893/7635], Loss: 4.1472\n",
      "Epoch [1/1], Step [5894/7635], Loss: 4.1181\n",
      "Epoch [1/1], Step [5895/7635], Loss: 4.1814\n",
      "Epoch [1/1], Step [5896/7635], Loss: 4.1970\n",
      "Epoch [1/1], Step [5897/7635], Loss: 4.1588\n",
      "Epoch [1/1], Step [5898/7635], Loss: 4.0773\n",
      "Epoch [1/1], Step [5899/7635], Loss: 4.1610\n",
      "Epoch [1/1], Step [5900/7635], Loss: 4.2129\n",
      "Epoch [1/1], Step [5901/7635], Loss: 4.1472\n",
      "Epoch [1/1], Step [5902/7635], Loss: 4.1830\n",
      "Epoch [1/1], Step [5903/7635], Loss: 4.2156\n",
      "Epoch [1/1], Step [5904/7635], Loss: 4.1280\n",
      "Epoch [1/1], Step [5905/7635], Loss: 4.1112\n",
      "Epoch [1/1], Step [5906/7635], Loss: 4.0841\n",
      "Epoch [1/1], Step [5907/7635], Loss: 4.1383\n",
      "Epoch [1/1], Step [5908/7635], Loss: 4.1441\n",
      "Epoch [1/1], Step [5909/7635], Loss: 4.1777\n",
      "Epoch [1/1], Step [5910/7635], Loss: 4.1101\n",
      "Epoch [1/1], Step [5911/7635], Loss: 4.0805\n",
      "Epoch [1/1], Step [5912/7635], Loss: 4.1363\n",
      "Epoch [1/1], Step [5913/7635], Loss: 4.0720\n",
      "Epoch [1/1], Step [5914/7635], Loss: 4.0686\n",
      "Epoch [1/1], Step [5915/7635], Loss: 4.1285\n",
      "Epoch [1/1], Step [5916/7635], Loss: 4.1668\n",
      "Epoch [1/1], Step [5917/7635], Loss: 4.1038\n",
      "Epoch [1/1], Step [5918/7635], Loss: 4.1343\n",
      "Epoch [1/1], Step [5919/7635], Loss: 4.1185\n",
      "Epoch [1/1], Step [5920/7635], Loss: 4.2476\n",
      "Epoch [1/1], Step [5921/7635], Loss: 4.1005\n",
      "Epoch [1/1], Step [5922/7635], Loss: 4.2326\n",
      "Epoch [1/1], Step [5923/7635], Loss: 4.0919\n",
      "Epoch [1/1], Step [5924/7635], Loss: 4.1127\n",
      "Epoch [1/1], Step [5925/7635], Loss: 4.1418\n",
      "Epoch [1/1], Step [5926/7635], Loss: 4.1836\n",
      "Epoch [1/1], Step [5927/7635], Loss: 4.1424\n",
      "Epoch [1/1], Step [5928/7635], Loss: 4.1468\n",
      "Epoch [1/1], Step [5929/7635], Loss: 4.1480\n",
      "Epoch [1/1], Step [5930/7635], Loss: 4.2099\n",
      "Epoch [1/1], Step [5931/7635], Loss: 4.2490\n",
      "Epoch [1/1], Step [5932/7635], Loss: 4.1499\n",
      "Epoch [1/1], Step [5933/7635], Loss: 4.1133\n",
      "Epoch [1/1], Step [5934/7635], Loss: 4.1928\n",
      "Epoch [1/1], Step [5935/7635], Loss: 4.1710\n",
      "Epoch [1/1], Step [5936/7635], Loss: 4.1639\n",
      "Epoch [1/1], Step [5937/7635], Loss: 4.1295\n",
      "Epoch [1/1], Step [5938/7635], Loss: 4.1526\n",
      "Epoch [1/1], Step [5939/7635], Loss: 4.0880\n",
      "Epoch [1/1], Step [5940/7635], Loss: 4.1158\n",
      "Epoch [1/1], Step [5941/7635], Loss: 4.0806\n",
      "Epoch [1/1], Step [5942/7635], Loss: 4.1162\n",
      "Epoch [1/1], Step [5943/7635], Loss: 4.1097\n",
      "Epoch [1/1], Step [5944/7635], Loss: 4.1416\n",
      "Epoch [1/1], Step [5945/7635], Loss: 4.2040\n",
      "Epoch [1/1], Step [5946/7635], Loss: 4.1228\n",
      "Epoch [1/1], Step [5947/7635], Loss: 4.1014\n",
      "Epoch [1/1], Step [5948/7635], Loss: 4.0947\n",
      "Epoch [1/1], Step [5949/7635], Loss: 4.0927\n",
      "Epoch [1/1], Step [5950/7635], Loss: 4.1599\n",
      "Epoch [1/1], Step [5951/7635], Loss: 4.1394\n",
      "Epoch [1/1], Step [5952/7635], Loss: 4.1277\n",
      "Epoch [1/1], Step [5953/7635], Loss: 4.1659\n",
      "Epoch [1/1], Step [5954/7635], Loss: 4.1258\n",
      "Epoch [1/1], Step [5955/7635], Loss: 4.0895\n",
      "Epoch [1/1], Step [5956/7635], Loss: 4.1391\n",
      "Epoch [1/1], Step [5957/7635], Loss: 4.1348\n",
      "Epoch [1/1], Step [5958/7635], Loss: 4.2079\n",
      "Epoch [1/1], Step [5959/7635], Loss: 4.1221\n",
      "Epoch [1/1], Step [5960/7635], Loss: 4.1411\n",
      "Epoch [1/1], Step [5961/7635], Loss: 4.1223\n",
      "Epoch [1/1], Step [5962/7635], Loss: 4.1531\n",
      "Epoch [1/1], Step [5963/7635], Loss: 4.1523\n",
      "Epoch [1/1], Step [5964/7635], Loss: 4.1415\n",
      "Epoch [1/1], Step [5965/7635], Loss: 4.1992\n",
      "Epoch [1/1], Step [5966/7635], Loss: 4.1906\n",
      "Epoch [1/1], Step [5967/7635], Loss: 4.0746\n",
      "Epoch [1/1], Step [5968/7635], Loss: 4.1825\n",
      "Epoch [1/1], Step [5969/7635], Loss: 4.1302\n",
      "Epoch [1/1], Step [5970/7635], Loss: 4.1150\n",
      "Epoch [1/1], Step [5971/7635], Loss: 4.1613\n",
      "Epoch [1/1], Step [5972/7635], Loss: 4.1909\n",
      "Epoch [1/1], Step [5973/7635], Loss: 4.1032\n",
      "Epoch [1/1], Step [5974/7635], Loss: 4.1154\n",
      "Epoch [1/1], Step [5975/7635], Loss: 4.0557\n",
      "Epoch [1/1], Step [5976/7635], Loss: 4.1547\n",
      "Epoch [1/1], Step [5977/7635], Loss: 4.1130\n",
      "Epoch [1/1], Step [5978/7635], Loss: 4.1452\n",
      "Epoch [1/1], Step [5979/7635], Loss: 4.1789\n",
      "Epoch [1/1], Step [5980/7635], Loss: 4.1594\n",
      "Epoch [1/1], Step [5981/7635], Loss: 4.0551\n",
      "Epoch [1/1], Step [5982/7635], Loss: 4.1509\n",
      "Epoch [1/1], Step [5983/7635], Loss: 4.1918\n",
      "Epoch [1/1], Step [5984/7635], Loss: 4.1199\n",
      "Epoch [1/1], Step [5985/7635], Loss: 4.1640\n",
      "Epoch [1/1], Step [5986/7635], Loss: 4.0686\n",
      "Epoch [1/1], Step [5987/7635], Loss: 4.0884\n",
      "Epoch [1/1], Step [5988/7635], Loss: 4.1624\n",
      "Epoch [1/1], Step [5989/7635], Loss: 4.0894\n",
      "Epoch [1/1], Step [5990/7635], Loss: 4.1547\n",
      "Epoch [1/1], Step [5991/7635], Loss: 4.1655\n",
      "Epoch [1/1], Step [5992/7635], Loss: 4.1570\n",
      "Epoch [1/1], Step [5993/7635], Loss: 4.0712\n",
      "Epoch [1/1], Step [5994/7635], Loss: 4.1695\n",
      "Epoch [1/1], Step [5995/7635], Loss: 4.1575\n",
      "Epoch [1/1], Step [5996/7635], Loss: 4.1107\n",
      "Epoch [1/1], Step [5997/7635], Loss: 4.1037\n",
      "Epoch [1/1], Step [5998/7635], Loss: 4.1290\n",
      "Epoch [1/1], Step [5999/7635], Loss: 4.1164\n",
      "Epoch [1/1], Step [6000/7635], Loss: 4.2049\n",
      "Epoch [1/1], Step [6001/7635], Loss: 4.1583\n",
      "Epoch [1/1], Step [6002/7635], Loss: 4.1412\n",
      "Epoch [1/1], Step [6003/7635], Loss: 4.1185\n",
      "Epoch [1/1], Step [6004/7635], Loss: 4.1051\n",
      "Epoch [1/1], Step [6005/7635], Loss: 4.0734\n",
      "Epoch [1/1], Step [6006/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [6007/7635], Loss: 4.1485\n",
      "Epoch [1/1], Step [6008/7635], Loss: 4.0528\n",
      "Epoch [1/1], Step [6009/7635], Loss: 4.1111\n",
      "Epoch [1/1], Step [6010/7635], Loss: 4.1604\n",
      "Epoch [1/1], Step [6011/7635], Loss: 4.1116\n",
      "Epoch [1/1], Step [6012/7635], Loss: 4.1161\n",
      "Epoch [1/1], Step [6013/7635], Loss: 4.1046\n",
      "Epoch [1/1], Step [6014/7635], Loss: 4.1103\n",
      "Epoch [1/1], Step [6015/7635], Loss: 4.0962\n",
      "Epoch [1/1], Step [6016/7635], Loss: 4.0842\n",
      "Epoch [1/1], Step [6017/7635], Loss: 4.1323\n",
      "Epoch [1/1], Step [6018/7635], Loss: 4.2023\n",
      "Epoch [1/1], Step [6019/7635], Loss: 4.0891\n",
      "Epoch [1/1], Step [6020/7635], Loss: 4.1208\n",
      "Epoch [1/1], Step [6021/7635], Loss: 4.1051\n",
      "Epoch [1/1], Step [6022/7635], Loss: 4.1468\n",
      "Epoch [1/1], Step [6023/7635], Loss: 4.1754\n",
      "Epoch [1/1], Step [6024/7635], Loss: 4.1236\n",
      "Epoch [1/1], Step [6025/7635], Loss: 4.1392\n",
      "Epoch [1/1], Step [6026/7635], Loss: 4.1057\n",
      "Epoch [1/1], Step [6027/7635], Loss: 4.1843\n",
      "Epoch [1/1], Step [6028/7635], Loss: 4.1465\n",
      "Epoch [1/1], Step [6029/7635], Loss: 4.2119\n",
      "Epoch [1/1], Step [6030/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [6031/7635], Loss: 4.1814\n",
      "Epoch [1/1], Step [6032/7635], Loss: 4.1055\n",
      "Epoch [1/1], Step [6033/7635], Loss: 4.0853\n",
      "Epoch [1/1], Step [6034/7635], Loss: 4.1412\n",
      "Epoch [1/1], Step [6035/7635], Loss: 4.1924\n",
      "Epoch [1/1], Step [6036/7635], Loss: 4.1829\n",
      "Epoch [1/1], Step [6037/7635], Loss: 4.1483\n",
      "Epoch [1/1], Step [6038/7635], Loss: 4.1731\n",
      "Epoch [1/1], Step [6039/7635], Loss: 4.1837\n",
      "Epoch [1/1], Step [6040/7635], Loss: 4.0655\n",
      "Epoch [1/1], Step [6041/7635], Loss: 4.1626\n",
      "Epoch [1/1], Step [6042/7635], Loss: 4.1655\n",
      "Epoch [1/1], Step [6043/7635], Loss: 4.1083\n",
      "Epoch [1/1], Step [6044/7635], Loss: 4.1554\n",
      "Epoch [1/1], Step [6045/7635], Loss: 4.1098\n",
      "Epoch [1/1], Step [6046/7635], Loss: 4.1428\n",
      "Epoch [1/1], Step [6047/7635], Loss: 4.1873\n",
      "Epoch [1/1], Step [6048/7635], Loss: 4.1560\n",
      "Epoch [1/1], Step [6049/7635], Loss: 4.0951\n",
      "Epoch [1/1], Step [6050/7635], Loss: 4.0744\n",
      "Epoch [1/1], Step [6051/7635], Loss: 4.1306\n",
      "Epoch [1/1], Step [6052/7635], Loss: 4.1099\n",
      "Epoch [1/1], Step [6053/7635], Loss: 4.1567\n",
      "Epoch [1/1], Step [6054/7635], Loss: 4.1237\n",
      "Epoch [1/1], Step [6055/7635], Loss: 4.0860\n",
      "Epoch [1/1], Step [6056/7635], Loss: 4.1138\n",
      "Epoch [1/1], Step [6057/7635], Loss: 4.1586\n",
      "Epoch [1/1], Step [6058/7635], Loss: 4.0798\n",
      "Epoch [1/1], Step [6059/7635], Loss: 4.0456\n",
      "Epoch [1/1], Step [6060/7635], Loss: 4.1884\n",
      "Epoch [1/1], Step [6061/7635], Loss: 4.0982\n",
      "Epoch [1/1], Step [6062/7635], Loss: 4.1056\n",
      "Epoch [1/1], Step [6063/7635], Loss: 4.1057\n",
      "Epoch [1/1], Step [6064/7635], Loss: 4.0630\n",
      "Epoch [1/1], Step [6065/7635], Loss: 4.0662\n",
      "Epoch [1/1], Step [6066/7635], Loss: 4.1572\n",
      "Epoch [1/1], Step [6067/7635], Loss: 4.1323\n",
      "Epoch [1/1], Step [6068/7635], Loss: 4.0813\n",
      "Epoch [1/1], Step [6069/7635], Loss: 4.1052\n",
      "Epoch [1/1], Step [6070/7635], Loss: 4.1646\n",
      "Epoch [1/1], Step [6071/7635], Loss: 4.1792\n",
      "Epoch [1/1], Step [6072/7635], Loss: 4.1242\n",
      "Epoch [1/1], Step [6073/7635], Loss: 4.0694\n",
      "Epoch [1/1], Step [6074/7635], Loss: 4.0748\n",
      "Epoch [1/1], Step [6075/7635], Loss: 4.1244\n",
      "Epoch [1/1], Step [6076/7635], Loss: 4.1300\n",
      "Epoch [1/1], Step [6077/7635], Loss: 4.1171\n",
      "Epoch [1/1], Step [6078/7635], Loss: 4.0686\n",
      "Epoch [1/1], Step [6079/7635], Loss: 4.2217\n",
      "Epoch [1/1], Step [6080/7635], Loss: 4.1560\n",
      "Epoch [1/1], Step [6081/7635], Loss: 4.1797\n",
      "Epoch [1/1], Step [6082/7635], Loss: 4.0980\n",
      "Epoch [1/1], Step [6083/7635], Loss: 4.1359\n",
      "Epoch [1/1], Step [6084/7635], Loss: 4.1045\n",
      "Epoch [1/1], Step [6085/7635], Loss: 4.1204\n",
      "Epoch [1/1], Step [6086/7635], Loss: 4.1688\n",
      "Epoch [1/1], Step [6087/7635], Loss: 4.1387\n",
      "Epoch [1/1], Step [6088/7635], Loss: 4.1252\n",
      "Epoch [1/1], Step [6089/7635], Loss: 4.0984\n",
      "Epoch [1/1], Step [6090/7635], Loss: 4.1354\n",
      "Epoch [1/1], Step [6091/7635], Loss: 4.1494\n",
      "Epoch [1/1], Step [6092/7635], Loss: 4.2144\n",
      "Epoch [1/1], Step [6093/7635], Loss: 4.1294\n",
      "Epoch [1/1], Step [6094/7635], Loss: 4.0986\n",
      "Epoch [1/1], Step [6095/7635], Loss: 4.1227\n",
      "Epoch [1/1], Step [6096/7635], Loss: 4.1461\n",
      "Epoch [1/1], Step [6097/7635], Loss: 4.1286\n",
      "Epoch [1/1], Step [6098/7635], Loss: 4.1168\n",
      "Epoch [1/1], Step [6099/7635], Loss: 4.1420\n",
      "Epoch [1/1], Step [6100/7635], Loss: 4.1065\n",
      "Epoch [1/1], Step [6101/7635], Loss: 4.0591\n",
      "Epoch [1/1], Step [6102/7635], Loss: 4.1026\n",
      "Epoch [1/1], Step [6103/7635], Loss: 4.1884\n",
      "Epoch [1/1], Step [6104/7635], Loss: 4.1320\n",
      "Epoch [1/1], Step [6105/7635], Loss: 4.1847\n",
      "Epoch [1/1], Step [6106/7635], Loss: 4.1997\n",
      "Epoch [1/1], Step [6107/7635], Loss: 4.1638\n",
      "Epoch [1/1], Step [6108/7635], Loss: 4.1709\n",
      "Epoch [1/1], Step [6109/7635], Loss: 4.0831\n",
      "Epoch [1/1], Step [6110/7635], Loss: 4.1082\n",
      "Epoch [1/1], Step [6111/7635], Loss: 4.1384\n",
      "Epoch [1/1], Step [6112/7635], Loss: 4.1272\n",
      "Epoch [1/1], Step [6113/7635], Loss: 4.1489\n",
      "Epoch [1/1], Step [6114/7635], Loss: 4.1231\n",
      "Epoch [1/1], Step [6115/7635], Loss: 4.1742\n",
      "Epoch [1/1], Step [6116/7635], Loss: 4.0657\n",
      "Epoch [1/1], Step [6117/7635], Loss: 4.1640\n",
      "Epoch [1/1], Step [6118/7635], Loss: 4.0932\n",
      "Epoch [1/1], Step [6119/7635], Loss: 4.1586\n",
      "Epoch [1/1], Step [6120/7635], Loss: 4.1167\n",
      "Epoch [1/1], Step [6121/7635], Loss: 4.0927\n",
      "Epoch [1/1], Step [6122/7635], Loss: 4.1383\n",
      "Epoch [1/1], Step [6123/7635], Loss: 4.1297\n",
      "Epoch [1/1], Step [6124/7635], Loss: 4.1690\n",
      "Epoch [1/1], Step [6125/7635], Loss: 4.1153\n",
      "Epoch [1/1], Step [6126/7635], Loss: 4.0362\n",
      "Epoch [1/1], Step [6127/7635], Loss: 4.0960\n",
      "Epoch [1/1], Step [6128/7635], Loss: 4.1674\n",
      "Epoch [1/1], Step [6129/7635], Loss: 4.1289\n",
      "Epoch [1/1], Step [6130/7635], Loss: 4.1090\n",
      "Epoch [1/1], Step [6131/7635], Loss: 4.1378\n",
      "Epoch [1/1], Step [6132/7635], Loss: 4.0596\n",
      "Epoch [1/1], Step [6133/7635], Loss: 4.1212\n",
      "Epoch [1/1], Step [6134/7635], Loss: 4.1293\n",
      "Epoch [1/1], Step [6135/7635], Loss: 4.0742\n",
      "Epoch [1/1], Step [6136/7635], Loss: 4.1274\n",
      "Epoch [1/1], Step [6137/7635], Loss: 4.1789\n",
      "Epoch [1/1], Step [6138/7635], Loss: 4.0899\n",
      "Epoch [1/1], Step [6139/7635], Loss: 4.0890\n",
      "Epoch [1/1], Step [6140/7635], Loss: 4.0967\n",
      "Epoch [1/1], Step [6141/7635], Loss: 4.1171\n",
      "Epoch [1/1], Step [6142/7635], Loss: 4.1171\n",
      "Epoch [1/1], Step [6143/7635], Loss: 4.0984\n",
      "Epoch [1/1], Step [6144/7635], Loss: 4.1317\n",
      "Epoch [1/1], Step [6145/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [6146/7635], Loss: 4.1212\n",
      "Epoch [1/1], Step [6147/7635], Loss: 4.1325\n",
      "Epoch [1/1], Step [6148/7635], Loss: 4.1577\n",
      "Epoch [1/1], Step [6149/7635], Loss: 4.0960\n",
      "Epoch [1/1], Step [6150/7635], Loss: 4.0707\n",
      "Epoch [1/1], Step [6151/7635], Loss: 4.1251\n",
      "Epoch [1/1], Step [6152/7635], Loss: 4.1015\n",
      "Epoch [1/1], Step [6153/7635], Loss: 4.1057\n",
      "Epoch [1/1], Step [6154/7635], Loss: 4.1055\n",
      "Epoch [1/1], Step [6155/7635], Loss: 4.0984\n",
      "Epoch [1/1], Step [6156/7635], Loss: 4.1948\n",
      "Epoch [1/1], Step [6157/7635], Loss: 4.1571\n",
      "Epoch [1/1], Step [6158/7635], Loss: 4.1123\n",
      "Epoch [1/1], Step [6159/7635], Loss: 4.1540\n",
      "Epoch [1/1], Step [6160/7635], Loss: 4.1117\n",
      "Epoch [1/1], Step [6161/7635], Loss: 4.0348\n",
      "Epoch [1/1], Step [6162/7635], Loss: 4.1140\n",
      "Epoch [1/1], Step [6163/7635], Loss: 4.1262\n",
      "Epoch [1/1], Step [6164/7635], Loss: 4.1252\n",
      "Epoch [1/1], Step [6165/7635], Loss: 4.2029\n",
      "Epoch [1/1], Step [6166/7635], Loss: 4.2202\n",
      "Epoch [1/1], Step [6167/7635], Loss: 4.1437\n",
      "Epoch [1/1], Step [6168/7635], Loss: 4.1080\n",
      "Epoch [1/1], Step [6169/7635], Loss: 4.1315\n",
      "Epoch [1/1], Step [6170/7635], Loss: 4.1021\n",
      "Epoch [1/1], Step [6171/7635], Loss: 4.0581\n",
      "Epoch [1/1], Step [6172/7635], Loss: 4.1145\n",
      "Epoch [1/1], Step [6173/7635], Loss: 4.2653\n",
      "Epoch [1/1], Step [6174/7635], Loss: 4.1038\n",
      "Epoch [1/1], Step [6175/7635], Loss: 4.0560\n",
      "Epoch [1/1], Step [6176/7635], Loss: 4.0790\n",
      "Epoch [1/1], Step [6177/7635], Loss: 4.1088\n",
      "Epoch [1/1], Step [6178/7635], Loss: 4.0675\n",
      "Epoch [1/1], Step [6179/7635], Loss: 4.1874\n",
      "Epoch [1/1], Step [6180/7635], Loss: 4.0735\n",
      "Epoch [1/1], Step [6181/7635], Loss: 4.0802\n",
      "Epoch [1/1], Step [6182/7635], Loss: 4.1464\n",
      "Epoch [1/1], Step [6183/7635], Loss: 4.1432\n",
      "Epoch [1/1], Step [6184/7635], Loss: 4.0677\n",
      "Epoch [1/1], Step [6185/7635], Loss: 4.0678\n",
      "Epoch [1/1], Step [6186/7635], Loss: 4.1570\n",
      "Epoch [1/1], Step [6187/7635], Loss: 4.1506\n",
      "Epoch [1/1], Step [6188/7635], Loss: 4.1186\n",
      "Epoch [1/1], Step [6189/7635], Loss: 4.0295\n",
      "Epoch [1/1], Step [6190/7635], Loss: 4.1198\n",
      "Epoch [1/1], Step [6191/7635], Loss: 4.1141\n",
      "Epoch [1/1], Step [6192/7635], Loss: 4.0809\n",
      "Epoch [1/1], Step [6193/7635], Loss: 4.1038\n",
      "Epoch [1/1], Step [6194/7635], Loss: 4.1751\n",
      "Epoch [1/1], Step [6195/7635], Loss: 4.1131\n",
      "Epoch [1/1], Step [6196/7635], Loss: 4.2123\n",
      "Epoch [1/1], Step [6197/7635], Loss: 4.0990\n",
      "Epoch [1/1], Step [6198/7635], Loss: 4.1109\n",
      "Epoch [1/1], Step [6199/7635], Loss: 4.1173\n",
      "Epoch [1/1], Step [6200/7635], Loss: 4.1033\n",
      "Epoch [1/1], Step [6201/7635], Loss: 4.0606\n",
      "Epoch [1/1], Step [6202/7635], Loss: 4.0856\n",
      "Epoch [1/1], Step [6203/7635], Loss: 4.2301\n",
      "Epoch [1/1], Step [6204/7635], Loss: 4.2229\n",
      "Epoch [1/1], Step [6205/7635], Loss: 4.1723\n",
      "Epoch [1/1], Step [6206/7635], Loss: 4.1852\n",
      "Epoch [1/1], Step [6207/7635], Loss: 4.2066\n",
      "Epoch [1/1], Step [6208/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [6209/7635], Loss: 4.1381\n",
      "Epoch [1/1], Step [6210/7635], Loss: 4.0748\n",
      "Epoch [1/1], Step [6211/7635], Loss: 4.1121\n",
      "Epoch [1/1], Step [6212/7635], Loss: 4.1297\n",
      "Epoch [1/1], Step [6213/7635], Loss: 4.0730\n",
      "Epoch [1/1], Step [6214/7635], Loss: 4.1114\n",
      "Epoch [1/1], Step [6215/7635], Loss: 4.1292\n",
      "Epoch [1/1], Step [6216/7635], Loss: 4.0805\n",
      "Epoch [1/1], Step [6217/7635], Loss: 4.0907\n",
      "Epoch [1/1], Step [6218/7635], Loss: 4.1291\n",
      "Epoch [1/1], Step [6219/7635], Loss: 4.1051\n",
      "Epoch [1/1], Step [6220/7635], Loss: 4.1408\n",
      "Epoch [1/1], Step [6221/7635], Loss: 4.1022\n",
      "Epoch [1/1], Step [6222/7635], Loss: 4.0793\n",
      "Epoch [1/1], Step [6223/7635], Loss: 4.1515\n",
      "Epoch [1/1], Step [6224/7635], Loss: 4.1206\n",
      "Epoch [1/1], Step [6225/7635], Loss: 4.1191\n",
      "Epoch [1/1], Step [6226/7635], Loss: 4.1662\n",
      "Epoch [1/1], Step [6227/7635], Loss: 4.1009\n",
      "Epoch [1/1], Step [6228/7635], Loss: 4.0942\n",
      "Epoch [1/1], Step [6229/7635], Loss: 4.1180\n",
      "Epoch [1/1], Step [6230/7635], Loss: 4.1601\n",
      "Epoch [1/1], Step [6231/7635], Loss: 4.1078\n",
      "Epoch [1/1], Step [6232/7635], Loss: 4.0933\n",
      "Epoch [1/1], Step [6233/7635], Loss: 4.0925\n",
      "Epoch [1/1], Step [6234/7635], Loss: 4.1359\n",
      "Epoch [1/1], Step [6235/7635], Loss: 4.1804\n",
      "Epoch [1/1], Step [6236/7635], Loss: 4.1160\n",
      "Epoch [1/1], Step [6237/7635], Loss: 4.1362\n",
      "Epoch [1/1], Step [6238/7635], Loss: 4.1442\n",
      "Epoch [1/1], Step [6239/7635], Loss: 4.0942\n",
      "Epoch [1/1], Step [6240/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [6241/7635], Loss: 4.1312\n",
      "Epoch [1/1], Step [6242/7635], Loss: 4.2093\n",
      "Epoch [1/1], Step [6243/7635], Loss: 4.1450\n",
      "Epoch [1/1], Step [6244/7635], Loss: 4.1853\n",
      "Epoch [1/1], Step [6245/7635], Loss: 4.1654\n",
      "Epoch [1/1], Step [6246/7635], Loss: 4.1370\n",
      "Epoch [1/1], Step [6247/7635], Loss: 4.1971\n",
      "Epoch [1/1], Step [6248/7635], Loss: 4.1489\n",
      "Epoch [1/1], Step [6249/7635], Loss: 4.1527\n",
      "Epoch [1/1], Step [6250/7635], Loss: 4.0831\n",
      "Epoch [1/1], Step [6251/7635], Loss: 4.1479\n",
      "Epoch [1/1], Step [6252/7635], Loss: 4.1529\n",
      "Epoch [1/1], Step [6253/7635], Loss: 4.1399\n",
      "Epoch [1/1], Step [6254/7635], Loss: 4.1207\n",
      "Epoch [1/1], Step [6255/7635], Loss: 4.1085\n",
      "Epoch [1/1], Step [6256/7635], Loss: 4.0748\n",
      "Epoch [1/1], Step [6257/7635], Loss: 4.1076\n",
      "Epoch [1/1], Step [6258/7635], Loss: 4.0917\n",
      "Epoch [1/1], Step [6259/7635], Loss: 4.1112\n",
      "Epoch [1/1], Step [6260/7635], Loss: 4.0417\n",
      "Epoch [1/1], Step [6261/7635], Loss: 4.1679\n",
      "Epoch [1/1], Step [6262/7635], Loss: 4.1144\n",
      "Epoch [1/1], Step [6263/7635], Loss: 4.1220\n",
      "Epoch [1/1], Step [6264/7635], Loss: 4.0839\n",
      "Epoch [1/1], Step [6265/7635], Loss: 4.1723\n",
      "Epoch [1/1], Step [6266/7635], Loss: 4.0778\n",
      "Epoch [1/1], Step [6267/7635], Loss: 4.0178\n",
      "Epoch [1/1], Step [6268/7635], Loss: 4.2100\n",
      "Epoch [1/1], Step [6269/7635], Loss: 4.2162\n",
      "Epoch [1/1], Step [6270/7635], Loss: 4.0724\n",
      "Epoch [1/1], Step [6271/7635], Loss: 4.1685\n",
      "Epoch [1/1], Step [6272/7635], Loss: 4.0433\n",
      "Epoch [1/1], Step [6273/7635], Loss: 4.1583\n",
      "Epoch [1/1], Step [6274/7635], Loss: 4.1447\n",
      "Epoch [1/1], Step [6275/7635], Loss: 4.1422\n",
      "Epoch [1/1], Step [6276/7635], Loss: 4.1571\n",
      "Epoch [1/1], Step [6277/7635], Loss: 4.0917\n",
      "Epoch [1/1], Step [6278/7635], Loss: 4.1062\n",
      "Epoch [1/1], Step [6279/7635], Loss: 4.1532\n",
      "Epoch [1/1], Step [6280/7635], Loss: 4.1274\n",
      "Epoch [1/1], Step [6281/7635], Loss: 4.1578\n",
      "Epoch [1/1], Step [6282/7635], Loss: 4.1763\n",
      "Epoch [1/1], Step [6283/7635], Loss: 4.0791\n",
      "Epoch [1/1], Step [6284/7635], Loss: 4.1266\n",
      "Epoch [1/1], Step [6285/7635], Loss: 4.1344\n",
      "Epoch [1/1], Step [6286/7635], Loss: 4.1713\n",
      "Epoch [1/1], Step [6287/7635], Loss: 4.1883\n",
      "Epoch [1/1], Step [6288/7635], Loss: 4.0670\n",
      "Epoch [1/1], Step [6289/7635], Loss: 4.0866\n",
      "Epoch [1/1], Step [6290/7635], Loss: 4.1788\n",
      "Epoch [1/1], Step [6291/7635], Loss: 4.0982\n",
      "Epoch [1/1], Step [6292/7635], Loss: 4.1457\n",
      "Epoch [1/1], Step [6293/7635], Loss: 4.1652\n",
      "Epoch [1/1], Step [6294/7635], Loss: 4.1169\n",
      "Epoch [1/1], Step [6295/7635], Loss: 4.1511\n",
      "Epoch [1/1], Step [6296/7635], Loss: 4.1712\n",
      "Epoch [1/1], Step [6297/7635], Loss: 4.1339\n",
      "Epoch [1/1], Step [6298/7635], Loss: 4.1823\n",
      "Epoch [1/1], Step [6299/7635], Loss: 4.1072\n",
      "Epoch [1/1], Step [6300/7635], Loss: 4.0516\n",
      "Epoch [1/1], Step [6301/7635], Loss: 4.1415\n",
      "Epoch [1/1], Step [6302/7635], Loss: 4.1123\n",
      "Epoch [1/1], Step [6303/7635], Loss: 4.1320\n",
      "Epoch [1/1], Step [6304/7635], Loss: 4.1518\n",
      "Epoch [1/1], Step [6305/7635], Loss: 4.1363\n",
      "Epoch [1/1], Step [6306/7635], Loss: 4.1339\n",
      "Epoch [1/1], Step [6307/7635], Loss: 4.0348\n",
      "Epoch [1/1], Step [6308/7635], Loss: 4.1892\n",
      "Epoch [1/1], Step [6309/7635], Loss: 4.0743\n",
      "Epoch [1/1], Step [6310/7635], Loss: 4.1622\n",
      "Epoch [1/1], Step [6311/7635], Loss: 4.1327\n",
      "Epoch [1/1], Step [6312/7635], Loss: 4.1940\n",
      "Epoch [1/1], Step [6313/7635], Loss: 4.1079\n",
      "Epoch [1/1], Step [6314/7635], Loss: 4.0595\n",
      "Epoch [1/1], Step [6315/7635], Loss: 4.1066\n",
      "Epoch [1/1], Step [6316/7635], Loss: 4.2209\n",
      "Epoch [1/1], Step [6317/7635], Loss: 4.1247\n",
      "Epoch [1/1], Step [6318/7635], Loss: 4.1446\n",
      "Epoch [1/1], Step [6319/7635], Loss: 4.1245\n",
      "Epoch [1/1], Step [6320/7635], Loss: 4.1041\n",
      "Epoch [1/1], Step [6321/7635], Loss: 4.1544\n",
      "Epoch [1/1], Step [6322/7635], Loss: 4.0505\n",
      "Epoch [1/1], Step [6323/7635], Loss: 4.1313\n",
      "Epoch [1/1], Step [6324/7635], Loss: 4.1201\n",
      "Epoch [1/1], Step [6325/7635], Loss: 4.1920\n",
      "Epoch [1/1], Step [6326/7635], Loss: 4.0462\n",
      "Epoch [1/1], Step [6327/7635], Loss: 4.0494\n",
      "Epoch [1/1], Step [6328/7635], Loss: 4.1007\n",
      "Epoch [1/1], Step [6329/7635], Loss: 4.1446\n",
      "Epoch [1/1], Step [6330/7635], Loss: 4.0907\n",
      "Epoch [1/1], Step [6331/7635], Loss: 4.1245\n",
      "Epoch [1/1], Step [6332/7635], Loss: 4.1357\n",
      "Epoch [1/1], Step [6333/7635], Loss: 4.0544\n",
      "Epoch [1/1], Step [6334/7635], Loss: 4.1123\n",
      "Epoch [1/1], Step [6335/7635], Loss: 4.1431\n",
      "Epoch [1/1], Step [6336/7635], Loss: 4.0971\n",
      "Epoch [1/1], Step [6337/7635], Loss: 4.1315\n",
      "Epoch [1/1], Step [6338/7635], Loss: 4.1057\n",
      "Epoch [1/1], Step [6339/7635], Loss: 4.0799\n",
      "Epoch [1/1], Step [6340/7635], Loss: 4.0728\n",
      "Epoch [1/1], Step [6341/7635], Loss: 4.0910\n",
      "Epoch [1/1], Step [6342/7635], Loss: 4.1265\n",
      "Epoch [1/1], Step [6343/7635], Loss: 4.1406\n",
      "Epoch [1/1], Step [6344/7635], Loss: 4.0245\n",
      "Epoch [1/1], Step [6345/7635], Loss: 4.1623\n",
      "Epoch [1/1], Step [6346/7635], Loss: 4.0847\n",
      "Epoch [1/1], Step [6347/7635], Loss: 4.1036\n",
      "Epoch [1/1], Step [6348/7635], Loss: 4.1072\n",
      "Epoch [1/1], Step [6349/7635], Loss: 4.1438\n",
      "Epoch [1/1], Step [6350/7635], Loss: 4.1458\n",
      "Epoch [1/1], Step [6351/7635], Loss: 4.0570\n",
      "Epoch [1/1], Step [6352/7635], Loss: 4.0700\n",
      "Epoch [1/1], Step [6353/7635], Loss: 4.1304\n",
      "Epoch [1/1], Step [6354/7635], Loss: 4.1199\n",
      "Epoch [1/1], Step [6355/7635], Loss: 4.1796\n",
      "Epoch [1/1], Step [6356/7635], Loss: 4.1346\n",
      "Epoch [1/1], Step [6357/7635], Loss: 4.1603\n",
      "Epoch [1/1], Step [6358/7635], Loss: 4.2079\n",
      "Epoch [1/1], Step [6359/7635], Loss: 4.1215\n",
      "Epoch [1/1], Step [6360/7635], Loss: 4.0718\n",
      "Epoch [1/1], Step [6361/7635], Loss: 4.1533\n",
      "Epoch [1/1], Step [6362/7635], Loss: 4.0990\n",
      "Epoch [1/1], Step [6363/7635], Loss: 4.1006\n",
      "Epoch [1/1], Step [6364/7635], Loss: 4.0970\n",
      "Epoch [1/1], Step [6365/7635], Loss: 4.1207\n",
      "Epoch [1/1], Step [6366/7635], Loss: 4.0706\n",
      "Epoch [1/1], Step [6367/7635], Loss: 4.1195\n",
      "Epoch [1/1], Step [6368/7635], Loss: 4.1867\n",
      "Epoch [1/1], Step [6369/7635], Loss: 4.1652\n",
      "Epoch [1/1], Step [6370/7635], Loss: 4.0564\n",
      "Epoch [1/1], Step [6371/7635], Loss: 4.0725\n",
      "Epoch [1/1], Step [6372/7635], Loss: 4.0479\n",
      "Epoch [1/1], Step [6373/7635], Loss: 4.1369\n",
      "Epoch [1/1], Step [6374/7635], Loss: 4.1360\n",
      "Epoch [1/1], Step [6375/7635], Loss: 4.0911\n",
      "Epoch [1/1], Step [6376/7635], Loss: 4.1729\n",
      "Epoch [1/1], Step [6377/7635], Loss: 4.0389\n",
      "Epoch [1/1], Step [6378/7635], Loss: 4.2400\n",
      "Epoch [1/1], Step [6379/7635], Loss: 4.0186\n",
      "Epoch [1/1], Step [6380/7635], Loss: 4.1364\n",
      "Epoch [1/1], Step [6381/7635], Loss: 4.1254\n",
      "Epoch [1/1], Step [6382/7635], Loss: 4.1357\n",
      "Epoch [1/1], Step [6383/7635], Loss: 4.1001\n",
      "Epoch [1/1], Step [6384/7635], Loss: 4.1623\n",
      "Epoch [1/1], Step [6385/7635], Loss: 4.1129\n",
      "Epoch [1/1], Step [6386/7635], Loss: 4.1286\n",
      "Epoch [1/1], Step [6387/7635], Loss: 4.1331\n",
      "Epoch [1/1], Step [6388/7635], Loss: 4.0991\n",
      "Epoch [1/1], Step [6389/7635], Loss: 4.1152\n",
      "Epoch [1/1], Step [6390/7635], Loss: 4.1145\n",
      "Epoch [1/1], Step [6391/7635], Loss: 4.1059\n",
      "Epoch [1/1], Step [6392/7635], Loss: 4.1697\n",
      "Epoch [1/1], Step [6393/7635], Loss: 4.1064\n",
      "Epoch [1/1], Step [6394/7635], Loss: 4.1297\n",
      "Epoch [1/1], Step [6395/7635], Loss: 4.0752\n",
      "Epoch [1/1], Step [6396/7635], Loss: 4.1409\n",
      "Epoch [1/1], Step [6397/7635], Loss: 4.1078\n",
      "Epoch [1/1], Step [6398/7635], Loss: 4.1439\n",
      "Epoch [1/1], Step [6399/7635], Loss: 4.1362\n",
      "Epoch [1/1], Step [6400/7635], Loss: 4.1736\n",
      "Epoch [1/1], Step [6401/7635], Loss: 4.2085\n",
      "Epoch [1/1], Step [6402/7635], Loss: 3.9304\n",
      "Epoch [1/1], Step [6403/7635], Loss: 4.0377\n",
      "Epoch [1/1], Step [6404/7635], Loss: 4.1665\n",
      "Epoch [1/1], Step [6405/7635], Loss: 4.0507\n",
      "Epoch [1/1], Step [6406/7635], Loss: 4.2177\n",
      "Epoch [1/1], Step [6407/7635], Loss: 4.0829\n",
      "Epoch [1/1], Step [6408/7635], Loss: 4.1796\n",
      "Epoch [1/1], Step [6409/7635], Loss: 4.0974\n",
      "Epoch [1/1], Step [6410/7635], Loss: 4.1414\n",
      "Epoch [1/1], Step [6411/7635], Loss: 4.1236\n",
      "Epoch [1/1], Step [6412/7635], Loss: 4.1591\n",
      "Epoch [1/1], Step [6413/7635], Loss: 4.2031\n",
      "Epoch [1/1], Step [6414/7635], Loss: 4.0771\n",
      "Epoch [1/1], Step [6415/7635], Loss: 4.1284\n",
      "Epoch [1/1], Step [6416/7635], Loss: 4.1368\n",
      "Epoch [1/1], Step [6417/7635], Loss: 4.1096\n",
      "Epoch [1/1], Step [6418/7635], Loss: 4.1115\n",
      "Epoch [1/1], Step [6419/7635], Loss: 4.0732\n",
      "Epoch [1/1], Step [6420/7635], Loss: 4.1811\n",
      "Epoch [1/1], Step [6421/7635], Loss: 4.1925\n",
      "Epoch [1/1], Step [6422/7635], Loss: 4.1740\n",
      "Epoch [1/1], Step [6423/7635], Loss: 4.1007\n",
      "Epoch [1/1], Step [6424/7635], Loss: 4.1374\n",
      "Epoch [1/1], Step [6425/7635], Loss: 4.1449\n",
      "Epoch [1/1], Step [6426/7635], Loss: 4.1386\n",
      "Epoch [1/1], Step [6427/7635], Loss: 4.0774\n",
      "Epoch [1/1], Step [6428/7635], Loss: 4.1987\n",
      "Epoch [1/1], Step [6429/7635], Loss: 4.0925\n",
      "Epoch [1/1], Step [6430/7635], Loss: 4.1412\n",
      "Epoch [1/1], Step [6431/7635], Loss: 4.1166\n",
      "Epoch [1/1], Step [6432/7635], Loss: 4.0678\n",
      "Epoch [1/1], Step [6433/7635], Loss: 4.0666\n",
      "Epoch [1/1], Step [6434/7635], Loss: 4.1702\n",
      "Epoch [1/1], Step [6435/7635], Loss: 4.1078\n",
      "Epoch [1/1], Step [6436/7635], Loss: 4.0903\n",
      "Epoch [1/1], Step [6437/7635], Loss: 4.1346\n",
      "Epoch [1/1], Step [6438/7635], Loss: 4.1016\n",
      "Epoch [1/1], Step [6439/7635], Loss: 4.0806\n",
      "Epoch [1/1], Step [6440/7635], Loss: 4.1106\n",
      "Epoch [1/1], Step [6441/7635], Loss: 4.1180\n",
      "Epoch [1/1], Step [6442/7635], Loss: 4.1068\n",
      "Epoch [1/1], Step [6443/7635], Loss: 4.1166\n",
      "Epoch [1/1], Step [6444/7635], Loss: 4.1843\n",
      "Epoch [1/1], Step [6445/7635], Loss: 4.0787\n",
      "Epoch [1/1], Step [6446/7635], Loss: 4.0618\n",
      "Epoch [1/1], Step [6447/7635], Loss: 4.1000\n",
      "Epoch [1/1], Step [6448/7635], Loss: 4.1419\n",
      "Epoch [1/1], Step [6449/7635], Loss: 4.1133\n",
      "Epoch [1/1], Step [6450/7635], Loss: 4.1520\n",
      "Epoch [1/1], Step [6451/7635], Loss: 4.1319\n",
      "Epoch [1/1], Step [6452/7635], Loss: 4.1424\n",
      "Epoch [1/1], Step [6453/7635], Loss: 4.1099\n",
      "Epoch [1/1], Step [6454/7635], Loss: 4.0748\n",
      "Epoch [1/1], Step [6455/7635], Loss: 4.2114\n",
      "Epoch [1/1], Step [6456/7635], Loss: 4.0582\n",
      "Epoch [1/1], Step [6457/7635], Loss: 4.0681\n",
      "Epoch [1/1], Step [6458/7635], Loss: 4.0713\n",
      "Epoch [1/1], Step [6459/7635], Loss: 4.1396\n",
      "Epoch [1/1], Step [6460/7635], Loss: 4.1265\n",
      "Epoch [1/1], Step [6461/7635], Loss: 4.1904\n",
      "Epoch [1/1], Step [6462/7635], Loss: 4.1445\n",
      "Epoch [1/1], Step [6463/7635], Loss: 4.0856\n",
      "Epoch [1/1], Step [6464/7635], Loss: 4.0236\n",
      "Epoch [1/1], Step [6465/7635], Loss: 4.1857\n",
      "Epoch [1/1], Step [6466/7635], Loss: 4.1795\n",
      "Epoch [1/1], Step [6467/7635], Loss: 4.1387\n",
      "Epoch [1/1], Step [6468/7635], Loss: 4.0912\n",
      "Epoch [1/1], Step [6469/7635], Loss: 4.1621\n",
      "Epoch [1/1], Step [6470/7635], Loss: 4.1854\n",
      "Epoch [1/1], Step [6471/7635], Loss: 4.0979\n",
      "Epoch [1/1], Step [6472/7635], Loss: 4.0440\n",
      "Epoch [1/1], Step [6473/7635], Loss: 4.0971\n",
      "Epoch [1/1], Step [6474/7635], Loss: 4.1408\n",
      "Epoch [1/1], Step [6475/7635], Loss: 4.0695\n",
      "Epoch [1/1], Step [6476/7635], Loss: 4.1496\n",
      "Epoch [1/1], Step [6477/7635], Loss: 4.1605\n",
      "Epoch [1/1], Step [6478/7635], Loss: 4.1133\n",
      "Epoch [1/1], Step [6479/7635], Loss: 4.0919\n",
      "Epoch [1/1], Step [6480/7635], Loss: 4.1279\n",
      "Epoch [1/1], Step [6481/7635], Loss: 4.0738\n",
      "Epoch [1/1], Step [6482/7635], Loss: 4.1024\n",
      "Epoch [1/1], Step [6483/7635], Loss: 4.0281\n",
      "Epoch [1/1], Step [6484/7635], Loss: 4.0933\n",
      "Epoch [1/1], Step [6485/7635], Loss: 4.1431\n",
      "Epoch [1/1], Step [6486/7635], Loss: 4.1157\n",
      "Epoch [1/1], Step [6487/7635], Loss: 4.1107\n",
      "Epoch [1/1], Step [6488/7635], Loss: 4.1282\n",
      "Epoch [1/1], Step [6489/7635], Loss: 4.0985\n",
      "Epoch [1/1], Step [6490/7635], Loss: 4.1765\n",
      "Epoch [1/1], Step [6491/7635], Loss: 4.1047\n",
      "Epoch [1/1], Step [6492/7635], Loss: 4.0791\n",
      "Epoch [1/1], Step [6493/7635], Loss: 4.2029\n",
      "Epoch [1/1], Step [6494/7635], Loss: 4.1067\n",
      "Epoch [1/1], Step [6495/7635], Loss: 4.0843\n",
      "Epoch [1/1], Step [6496/7635], Loss: 4.0467\n",
      "Epoch [1/1], Step [6497/7635], Loss: 4.2194\n",
      "Epoch [1/1], Step [6498/7635], Loss: 4.1443\n",
      "Epoch [1/1], Step [6499/7635], Loss: 4.1235\n",
      "Epoch [1/1], Step [6500/7635], Loss: 4.1630\n",
      "Epoch [1/1], Step [6501/7635], Loss: 4.0814\n",
      "Epoch [1/1], Step [6502/7635], Loss: 4.0532\n",
      "Epoch [1/1], Step [6503/7635], Loss: 4.1345\n",
      "Epoch [1/1], Step [6504/7635], Loss: 4.1568\n",
      "Epoch [1/1], Step [6505/7635], Loss: 4.1046\n",
      "Epoch [1/1], Step [6506/7635], Loss: 4.0916\n",
      "Epoch [1/1], Step [6507/7635], Loss: 4.1264\n",
      "Epoch [1/1], Step [6508/7635], Loss: 4.0606\n",
      "Epoch [1/1], Step [6509/7635], Loss: 4.1162\n",
      "Epoch [1/1], Step [6510/7635], Loss: 4.0976\n",
      "Epoch [1/1], Step [6511/7635], Loss: 4.1973\n",
      "Epoch [1/1], Step [6512/7635], Loss: 4.1334\n",
      "Epoch [1/1], Step [6513/7635], Loss: 4.0877\n",
      "Epoch [1/1], Step [6514/7635], Loss: 4.1334\n",
      "Epoch [1/1], Step [6515/7635], Loss: 4.1556\n",
      "Epoch [1/1], Step [6516/7635], Loss: 4.0978\n",
      "Epoch [1/1], Step [6517/7635], Loss: 4.1903\n",
      "Epoch [1/1], Step [6518/7635], Loss: 4.0763\n",
      "Epoch [1/1], Step [6519/7635], Loss: 4.1728\n",
      "Epoch [1/1], Step [6520/7635], Loss: 4.1339\n",
      "Epoch [1/1], Step [6521/7635], Loss: 4.1205\n",
      "Epoch [1/1], Step [6522/7635], Loss: 4.1072\n",
      "Epoch [1/1], Step [6523/7635], Loss: 4.1749\n",
      "Epoch [1/1], Step [6524/7635], Loss: 4.0539\n",
      "Epoch [1/1], Step [6525/7635], Loss: 4.1232\n",
      "Epoch [1/1], Step [6526/7635], Loss: 4.1386\n",
      "Epoch [1/1], Step [6527/7635], Loss: 4.0848\n",
      "Epoch [1/1], Step [6528/7635], Loss: 4.1229\n",
      "Epoch [1/1], Step [6529/7635], Loss: 4.1120\n",
      "Epoch [1/1], Step [6530/7635], Loss: 4.1238\n",
      "Epoch [1/1], Step [6531/7635], Loss: 4.0553\n",
      "Epoch [1/1], Step [6532/7635], Loss: 4.1304\n",
      "Epoch [1/1], Step [6533/7635], Loss: 4.2009\n",
      "Epoch [1/1], Step [6534/7635], Loss: 4.0982\n",
      "Epoch [1/1], Step [6535/7635], Loss: 4.0259\n",
      "Epoch [1/1], Step [6536/7635], Loss: 4.1506\n",
      "Epoch [1/1], Step [6537/7635], Loss: 4.0494\n",
      "Epoch [1/1], Step [6538/7635], Loss: 4.1876\n",
      "Epoch [1/1], Step [6539/7635], Loss: 4.2005\n",
      "Epoch [1/1], Step [6540/7635], Loss: 4.0786\n",
      "Epoch [1/1], Step [6541/7635], Loss: 4.1765\n",
      "Epoch [1/1], Step [6542/7635], Loss: 4.1503\n",
      "Epoch [1/1], Step [6543/7635], Loss: 4.2236\n",
      "Epoch [1/1], Step [6544/7635], Loss: 4.0391\n",
      "Epoch [1/1], Step [6545/7635], Loss: 4.1660\n",
      "Epoch [1/1], Step [6546/7635], Loss: 4.0868\n",
      "Epoch [1/1], Step [6547/7635], Loss: 4.1238\n",
      "Epoch [1/1], Step [6548/7635], Loss: 4.0998\n",
      "Epoch [1/1], Step [6549/7635], Loss: 4.1143\n",
      "Epoch [1/1], Step [6550/7635], Loss: 4.0826\n",
      "Epoch [1/1], Step [6551/7635], Loss: 4.1156\n",
      "Epoch [1/1], Step [6552/7635], Loss: 4.0748\n",
      "Epoch [1/1], Step [6553/7635], Loss: 4.1029\n",
      "Epoch [1/1], Step [6554/7635], Loss: 4.1033\n",
      "Epoch [1/1], Step [6555/7635], Loss: 4.1699\n",
      "Epoch [1/1], Step [6556/7635], Loss: 4.1122\n",
      "Epoch [1/1], Step [6557/7635], Loss: 4.1729\n",
      "Epoch [1/1], Step [6558/7635], Loss: 4.1683\n",
      "Epoch [1/1], Step [6559/7635], Loss: 4.0358\n",
      "Epoch [1/1], Step [6560/7635], Loss: 4.0140\n",
      "Epoch [1/1], Step [6561/7635], Loss: 4.1071\n",
      "Epoch [1/1], Step [6562/7635], Loss: 4.1396\n",
      "Epoch [1/1], Step [6563/7635], Loss: 4.0342\n",
      "Epoch [1/1], Step [6564/7635], Loss: 4.1559\n",
      "Epoch [1/1], Step [6565/7635], Loss: 4.0460\n",
      "Epoch [1/1], Step [6566/7635], Loss: 4.1628\n",
      "Epoch [1/1], Step [6567/7635], Loss: 4.0187\n",
      "Epoch [1/1], Step [6568/7635], Loss: 4.1763\n",
      "Epoch [1/1], Step [6569/7635], Loss: 4.0674\n",
      "Epoch [1/1], Step [6570/7635], Loss: 4.1515\n",
      "Epoch [1/1], Step [6571/7635], Loss: 4.1463\n",
      "Epoch [1/1], Step [6572/7635], Loss: 4.1307\n",
      "Epoch [1/1], Step [6573/7635], Loss: 4.1017\n",
      "Epoch [1/1], Step [6574/7635], Loss: 4.0886\n",
      "Epoch [1/1], Step [6575/7635], Loss: 4.1082\n",
      "Epoch [1/1], Step [6576/7635], Loss: 4.1057\n",
      "Epoch [1/1], Step [6577/7635], Loss: 4.1393\n",
      "Epoch [1/1], Step [6578/7635], Loss: 4.0588\n",
      "Epoch [1/1], Step [6579/7635], Loss: 4.0588\n",
      "Epoch [1/1], Step [6580/7635], Loss: 4.1183\n",
      "Epoch [1/1], Step [6581/7635], Loss: 4.1815\n",
      "Epoch [1/1], Step [6582/7635], Loss: 4.1188\n",
      "Epoch [1/1], Step [6583/7635], Loss: 4.1453\n",
      "Epoch [1/1], Step [6584/7635], Loss: 4.1567\n",
      "Epoch [1/1], Step [6585/7635], Loss: 4.0994\n",
      "Epoch [1/1], Step [6586/7635], Loss: 4.0860\n",
      "Epoch [1/1], Step [6587/7635], Loss: 4.1109\n",
      "Epoch [1/1], Step [6588/7635], Loss: 4.0857\n",
      "Epoch [1/1], Step [6589/7635], Loss: 4.1415\n",
      "Epoch [1/1], Step [6590/7635], Loss: 4.0826\n",
      "Epoch [1/1], Step [6591/7635], Loss: 4.1078\n",
      "Epoch [1/1], Step [6592/7635], Loss: 4.0571\n",
      "Epoch [1/1], Step [6593/7635], Loss: 4.1733\n",
      "Epoch [1/1], Step [6594/7635], Loss: 4.0690\n",
      "Epoch [1/1], Step [6595/7635], Loss: 4.1282\n",
      "Epoch [1/1], Step [6596/7635], Loss: 4.1062\n",
      "Epoch [1/1], Step [6597/7635], Loss: 4.1661\n",
      "Epoch [1/1], Step [6598/7635], Loss: 4.1564\n",
      "Epoch [1/1], Step [6599/7635], Loss: 4.0306\n",
      "Epoch [1/1], Step [6600/7635], Loss: 4.1412\n",
      "Epoch [1/1], Step [6601/7635], Loss: 4.1096\n",
      "Epoch [1/1], Step [6602/7635], Loss: 4.1076\n",
      "Epoch [1/1], Step [6603/7635], Loss: 4.0228\n",
      "Epoch [1/1], Step [6604/7635], Loss: 4.0972\n",
      "Epoch [1/1], Step [6605/7635], Loss: 4.1087\n",
      "Epoch [1/1], Step [6606/7635], Loss: 4.1032\n",
      "Epoch [1/1], Step [6607/7635], Loss: 4.1533\n",
      "Epoch [1/1], Step [6608/7635], Loss: 4.1175\n",
      "Epoch [1/1], Step [6609/7635], Loss: 4.0306\n",
      "Epoch [1/1], Step [6610/7635], Loss: 4.1021\n",
      "Epoch [1/1], Step [6611/7635], Loss: 4.0979\n",
      "Epoch [1/1], Step [6612/7635], Loss: 4.0490\n",
      "Epoch [1/1], Step [6613/7635], Loss: 4.1084\n",
      "Epoch [1/1], Step [6614/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [6615/7635], Loss: 4.1129\n",
      "Epoch [1/1], Step [6616/7635], Loss: 4.0634\n",
      "Epoch [1/1], Step [6617/7635], Loss: 4.0608\n",
      "Epoch [1/1], Step [6618/7635], Loss: 4.1492\n",
      "Epoch [1/1], Step [6619/7635], Loss: 4.1250\n",
      "Epoch [1/1], Step [6620/7635], Loss: 4.0326\n",
      "Epoch [1/1], Step [6621/7635], Loss: 4.1400\n",
      "Epoch [1/1], Step [6622/7635], Loss: 4.1172\n",
      "Epoch [1/1], Step [6623/7635], Loss: 4.1614\n",
      "Epoch [1/1], Step [6624/7635], Loss: 4.0844\n",
      "Epoch [1/1], Step [6625/7635], Loss: 4.0162\n",
      "Epoch [1/1], Step [6626/7635], Loss: 4.0988\n",
      "Epoch [1/1], Step [6627/7635], Loss: 4.1248\n",
      "Epoch [1/1], Step [6628/7635], Loss: 4.1404\n",
      "Epoch [1/1], Step [6629/7635], Loss: 4.0643\n",
      "Epoch [1/1], Step [6630/7635], Loss: 4.0892\n",
      "Epoch [1/1], Step [6631/7635], Loss: 4.1393\n",
      "Epoch [1/1], Step [6632/7635], Loss: 4.1088\n",
      "Epoch [1/1], Step [6633/7635], Loss: 4.1190\n",
      "Epoch [1/1], Step [6634/7635], Loss: 4.1207\n",
      "Epoch [1/1], Step [6635/7635], Loss: 4.1113\n",
      "Epoch [1/1], Step [6636/7635], Loss: 4.0689\n",
      "Epoch [1/1], Step [6637/7635], Loss: 4.0660\n",
      "Epoch [1/1], Step [6638/7635], Loss: 4.0674\n",
      "Epoch [1/1], Step [6639/7635], Loss: 4.1225\n",
      "Epoch [1/1], Step [6640/7635], Loss: 4.1907\n",
      "Epoch [1/1], Step [6641/7635], Loss: 4.1379\n",
      "Epoch [1/1], Step [6642/7635], Loss: 4.0669\n",
      "Epoch [1/1], Step [6643/7635], Loss: 4.1286\n",
      "Epoch [1/1], Step [6644/7635], Loss: 4.1199\n",
      "Epoch [1/1], Step [6645/7635], Loss: 4.0400\n",
      "Epoch [1/1], Step [6646/7635], Loss: 4.0712\n",
      "Epoch [1/1], Step [6647/7635], Loss: 4.0937\n",
      "Epoch [1/1], Step [6648/7635], Loss: 4.1214\n",
      "Epoch [1/1], Step [6649/7635], Loss: 4.1090\n",
      "Epoch [1/1], Step [6650/7635], Loss: 4.0518\n",
      "Epoch [1/1], Step [6651/7635], Loss: 4.1157\n",
      "Epoch [1/1], Step [6652/7635], Loss: 4.1477\n",
      "Epoch [1/1], Step [6653/7635], Loss: 4.0708\n",
      "Epoch [1/1], Step [6654/7635], Loss: 4.1440\n",
      "Epoch [1/1], Step [6655/7635], Loss: 4.1526\n",
      "Epoch [1/1], Step [6656/7635], Loss: 4.1521\n",
      "Epoch [1/1], Step [6657/7635], Loss: 4.1324\n",
      "Epoch [1/1], Step [6658/7635], Loss: 4.1284\n",
      "Epoch [1/1], Step [6659/7635], Loss: 4.0513\n",
      "Epoch [1/1], Step [6660/7635], Loss: 4.2331\n",
      "Epoch [1/1], Step [6661/7635], Loss: 4.1303\n",
      "Epoch [1/1], Step [6662/7635], Loss: 4.1541\n",
      "Epoch [1/1], Step [6663/7635], Loss: 4.0825\n",
      "Epoch [1/1], Step [6664/7635], Loss: 4.1322\n",
      "Epoch [1/1], Step [6665/7635], Loss: 4.1503\n",
      "Epoch [1/1], Step [6666/7635], Loss: 4.1196\n",
      "Epoch [1/1], Step [6667/7635], Loss: 4.1047\n",
      "Epoch [1/1], Step [6668/7635], Loss: 3.9923\n",
      "Epoch [1/1], Step [6669/7635], Loss: 4.1212\n",
      "Epoch [1/1], Step [6670/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [6671/7635], Loss: 4.1024\n",
      "Epoch [1/1], Step [6672/7635], Loss: 4.0924\n",
      "Epoch [1/1], Step [6673/7635], Loss: 4.0589\n",
      "Epoch [1/1], Step [6674/7635], Loss: 4.1471\n",
      "Epoch [1/1], Step [6675/7635], Loss: 4.1412\n",
      "Epoch [1/1], Step [6676/7635], Loss: 4.0379\n",
      "Epoch [1/1], Step [6677/7635], Loss: 4.1067\n",
      "Epoch [1/1], Step [6678/7635], Loss: 4.1432\n",
      "Epoch [1/1], Step [6679/7635], Loss: 4.1367\n",
      "Epoch [1/1], Step [6680/7635], Loss: 4.0243\n",
      "Epoch [1/1], Step [6681/7635], Loss: 4.0925\n",
      "Epoch [1/1], Step [6682/7635], Loss: 4.0735\n",
      "Epoch [1/1], Step [6683/7635], Loss: 4.0924\n",
      "Epoch [1/1], Step [6684/7635], Loss: 4.1007\n",
      "Epoch [1/1], Step [6685/7635], Loss: 4.1309\n",
      "Epoch [1/1], Step [6686/7635], Loss: 4.1495\n",
      "Epoch [1/1], Step [6687/7635], Loss: 4.0639\n",
      "Epoch [1/1], Step [6688/7635], Loss: 4.1046\n",
      "Epoch [1/1], Step [6689/7635], Loss: 4.0837\n",
      "Epoch [1/1], Step [6690/7635], Loss: 4.1477\n",
      "Epoch [1/1], Step [6691/7635], Loss: 4.1330\n",
      "Epoch [1/1], Step [6692/7635], Loss: 4.1223\n",
      "Epoch [1/1], Step [6693/7635], Loss: 4.1014\n",
      "Epoch [1/1], Step [6694/7635], Loss: 4.1556\n",
      "Epoch [1/1], Step [6695/7635], Loss: 4.0628\n",
      "Epoch [1/1], Step [6696/7635], Loss: 4.0387\n",
      "Epoch [1/1], Step [6697/7635], Loss: 4.1157\n",
      "Epoch [1/1], Step [6698/7635], Loss: 4.1249\n",
      "Epoch [1/1], Step [6699/7635], Loss: 4.0771\n",
      "Epoch [1/1], Step [6700/7635], Loss: 4.1142\n",
      "Epoch [1/1], Step [6701/7635], Loss: 4.1539\n",
      "Epoch [1/1], Step [6702/7635], Loss: 4.0737\n",
      "Epoch [1/1], Step [6703/7635], Loss: 4.0685\n",
      "Epoch [1/1], Step [6704/7635], Loss: 4.1493\n",
      "Epoch [1/1], Step [6705/7635], Loss: 4.1287\n",
      "Epoch [1/1], Step [6706/7635], Loss: 4.0667\n",
      "Epoch [1/1], Step [6707/7635], Loss: 4.0526\n",
      "Epoch [1/1], Step [6708/7635], Loss: 4.1207\n",
      "Epoch [1/1], Step [6709/7635], Loss: 4.1822\n",
      "Epoch [1/1], Step [6710/7635], Loss: 4.0683\n",
      "Epoch [1/1], Step [6711/7635], Loss: 4.1539\n",
      "Epoch [1/1], Step [6712/7635], Loss: 4.0394\n",
      "Epoch [1/1], Step [6713/7635], Loss: 4.0755\n",
      "Epoch [1/1], Step [6714/7635], Loss: 4.0770\n",
      "Epoch [1/1], Step [6715/7635], Loss: 4.0682\n",
      "Epoch [1/1], Step [6716/7635], Loss: 4.1050\n",
      "Epoch [1/1], Step [6717/7635], Loss: 4.1628\n",
      "Epoch [1/1], Step [6718/7635], Loss: 4.1423\n",
      "Epoch [1/1], Step [6719/7635], Loss: 4.0526\n",
      "Epoch [1/1], Step [6720/7635], Loss: 4.1041\n",
      "Epoch [1/1], Step [6721/7635], Loss: 4.1470\n",
      "Epoch [1/1], Step [6722/7635], Loss: 4.0296\n",
      "Epoch [1/1], Step [6723/7635], Loss: 4.1192\n",
      "Epoch [1/1], Step [6724/7635], Loss: 4.0538\n",
      "Epoch [1/1], Step [6725/7635], Loss: 4.1435\n",
      "Epoch [1/1], Step [6726/7635], Loss: 4.2157\n",
      "Epoch [1/1], Step [6727/7635], Loss: 4.1108\n",
      "Epoch [1/1], Step [6728/7635], Loss: 4.1319\n",
      "Epoch [1/1], Step [6729/7635], Loss: 4.0881\n",
      "Epoch [1/1], Step [6730/7635], Loss: 4.1259\n",
      "Epoch [1/1], Step [6731/7635], Loss: 4.1349\n",
      "Epoch [1/1], Step [6732/7635], Loss: 4.1143\n",
      "Epoch [1/1], Step [6733/7635], Loss: 4.0109\n",
      "Epoch [1/1], Step [6734/7635], Loss: 4.1507\n",
      "Epoch [1/1], Step [6735/7635], Loss: 4.1689\n",
      "Epoch [1/1], Step [6736/7635], Loss: 4.1072\n",
      "Epoch [1/1], Step [6737/7635], Loss: 4.1178\n",
      "Epoch [1/1], Step [6738/7635], Loss: 4.1161\n",
      "Epoch [1/1], Step [6739/7635], Loss: 4.1116\n",
      "Epoch [1/1], Step [6740/7635], Loss: 4.1072\n",
      "Epoch [1/1], Step [6741/7635], Loss: 4.0814\n",
      "Epoch [1/1], Step [6742/7635], Loss: 4.1234\n",
      "Epoch [1/1], Step [6743/7635], Loss: 4.0871\n",
      "Epoch [1/1], Step [6744/7635], Loss: 4.0748\n",
      "Epoch [1/1], Step [6745/7635], Loss: 4.0736\n",
      "Epoch [1/1], Step [6746/7635], Loss: 4.1454\n",
      "Epoch [1/1], Step [6747/7635], Loss: 4.0499\n",
      "Epoch [1/1], Step [6748/7635], Loss: 4.0869\n",
      "Epoch [1/1], Step [6749/7635], Loss: 4.2348\n",
      "Epoch [1/1], Step [6750/7635], Loss: 4.1468\n",
      "Epoch [1/1], Step [6751/7635], Loss: 3.9941\n",
      "Epoch [1/1], Step [6752/7635], Loss: 4.0420\n",
      "Epoch [1/1], Step [6753/7635], Loss: 4.1003\n",
      "Epoch [1/1], Step [6754/7635], Loss: 4.1394\n",
      "Epoch [1/1], Step [6755/7635], Loss: 4.1588\n",
      "Epoch [1/1], Step [6756/7635], Loss: 4.1098\n",
      "Epoch [1/1], Step [6757/7635], Loss: 4.2000\n",
      "Epoch [1/1], Step [6758/7635], Loss: 4.1126\n",
      "Epoch [1/1], Step [6759/7635], Loss: 4.0349\n",
      "Epoch [1/1], Step [6760/7635], Loss: 4.1015\n",
      "Epoch [1/1], Step [6761/7635], Loss: 4.0702\n",
      "Epoch [1/1], Step [6762/7635], Loss: 4.1253\n",
      "Epoch [1/1], Step [6763/7635], Loss: 4.0822\n",
      "Epoch [1/1], Step [6764/7635], Loss: 4.0817\n",
      "Epoch [1/1], Step [6765/7635], Loss: 4.1535\n",
      "Epoch [1/1], Step [6766/7635], Loss: 4.0204\n",
      "Epoch [1/1], Step [6767/7635], Loss: 4.0747\n",
      "Epoch [1/1], Step [6768/7635], Loss: 4.0882\n",
      "Epoch [1/1], Step [6769/7635], Loss: 4.1033\n",
      "Epoch [1/1], Step [6770/7635], Loss: 4.1180\n",
      "Epoch [1/1], Step [6771/7635], Loss: 4.0807\n",
      "Epoch [1/1], Step [6772/7635], Loss: 4.1774\n",
      "Epoch [1/1], Step [6773/7635], Loss: 4.0244\n",
      "Epoch [1/1], Step [6774/7635], Loss: 4.1021\n",
      "Epoch [1/1], Step [6775/7635], Loss: 4.0631\n",
      "Epoch [1/1], Step [6776/7635], Loss: 4.0588\n",
      "Epoch [1/1], Step [6777/7635], Loss: 4.0911\n",
      "Epoch [1/1], Step [6778/7635], Loss: 4.1078\n",
      "Epoch [1/1], Step [6779/7635], Loss: 4.1345\n",
      "Epoch [1/1], Step [6780/7635], Loss: 4.1249\n",
      "Epoch [1/1], Step [6781/7635], Loss: 4.1580\n",
      "Epoch [1/1], Step [6782/7635], Loss: 4.1354\n",
      "Epoch [1/1], Step [6783/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [6784/7635], Loss: 4.1553\n",
      "Epoch [1/1], Step [6785/7635], Loss: 4.1637\n",
      "Epoch [1/1], Step [6786/7635], Loss: 4.0811\n",
      "Epoch [1/1], Step [6787/7635], Loss: 4.0466\n",
      "Epoch [1/1], Step [6788/7635], Loss: 4.0569\n",
      "Epoch [1/1], Step [6789/7635], Loss: 4.1466\n",
      "Epoch [1/1], Step [6790/7635], Loss: 4.0851\n",
      "Epoch [1/1], Step [6791/7635], Loss: 4.1093\n",
      "Epoch [1/1], Step [6792/7635], Loss: 4.0271\n",
      "Epoch [1/1], Step [6793/7635], Loss: 4.2111\n",
      "Epoch [1/1], Step [6794/7635], Loss: 4.1191\n",
      "Epoch [1/1], Step [6795/7635], Loss: 4.0749\n",
      "Epoch [1/1], Step [6796/7635], Loss: 4.1332\n",
      "Epoch [1/1], Step [6797/7635], Loss: 4.0989\n",
      "Epoch [1/1], Step [6798/7635], Loss: 4.1006\n",
      "Epoch [1/1], Step [6799/7635], Loss: 4.0449\n",
      "Epoch [1/1], Step [6800/7635], Loss: 4.1213\n",
      "Epoch [1/1], Step [6801/7635], Loss: 4.0877\n",
      "Epoch [1/1], Step [6802/7635], Loss: 4.1059\n",
      "Epoch [1/1], Step [6803/7635], Loss: 4.0956\n",
      "Epoch [1/1], Step [6804/7635], Loss: 4.1204\n",
      "Epoch [1/1], Step [6805/7635], Loss: 4.0926\n",
      "Epoch [1/1], Step [6806/7635], Loss: 4.1015\n",
      "Epoch [1/1], Step [6807/7635], Loss: 4.1165\n",
      "Epoch [1/1], Step [6808/7635], Loss: 4.1414\n",
      "Epoch [1/1], Step [6809/7635], Loss: 4.1279\n",
      "Epoch [1/1], Step [6810/7635], Loss: 4.1415\n",
      "Epoch [1/1], Step [6811/7635], Loss: 4.1217\n",
      "Epoch [1/1], Step [6812/7635], Loss: 4.0914\n",
      "Epoch [1/1], Step [6813/7635], Loss: 4.1512\n",
      "Epoch [1/1], Step [6814/7635], Loss: 4.1945\n",
      "Epoch [1/1], Step [6815/7635], Loss: 4.1272\n",
      "Epoch [1/1], Step [6816/7635], Loss: 4.1534\n",
      "Epoch [1/1], Step [6817/7635], Loss: 4.1170\n",
      "Epoch [1/1], Step [6818/7635], Loss: 4.1354\n",
      "Epoch [1/1], Step [6819/7635], Loss: 4.1362\n",
      "Epoch [1/1], Step [6820/7635], Loss: 4.0985\n",
      "Epoch [1/1], Step [6821/7635], Loss: 4.1251\n",
      "Epoch [1/1], Step [6822/7635], Loss: 4.1357\n",
      "Epoch [1/1], Step [6823/7635], Loss: 4.1104\n",
      "Epoch [1/1], Step [6824/7635], Loss: 4.1057\n",
      "Epoch [1/1], Step [6825/7635], Loss: 4.0878\n",
      "Epoch [1/1], Step [6826/7635], Loss: 4.0443\n",
      "Epoch [1/1], Step [6827/7635], Loss: 4.0878\n",
      "Epoch [1/1], Step [6828/7635], Loss: 4.0986\n",
      "Epoch [1/1], Step [6829/7635], Loss: 4.0840\n",
      "Epoch [1/1], Step [6830/7635], Loss: 4.0638\n",
      "Epoch [1/1], Step [6831/7635], Loss: 4.0806\n",
      "Epoch [1/1], Step [6832/7635], Loss: 4.1816\n",
      "Epoch [1/1], Step [6833/7635], Loss: 4.0806\n",
      "Epoch [1/1], Step [6834/7635], Loss: 4.1450\n",
      "Epoch [1/1], Step [6835/7635], Loss: 4.0732\n",
      "Epoch [1/1], Step [6836/7635], Loss: 4.0831\n",
      "Epoch [1/1], Step [6837/7635], Loss: 4.1299\n",
      "Epoch [1/1], Step [6838/7635], Loss: 4.0720\n",
      "Epoch [1/1], Step [6839/7635], Loss: 4.1151\n",
      "Epoch [1/1], Step [6840/7635], Loss: 4.1190\n",
      "Epoch [1/1], Step [6841/7635], Loss: 4.0604\n",
      "Epoch [1/1], Step [6842/7635], Loss: 4.1248\n",
      "Epoch [1/1], Step [6843/7635], Loss: 4.1298\n",
      "Epoch [1/1], Step [6844/7635], Loss: 4.1589\n",
      "Epoch [1/1], Step [6845/7635], Loss: 4.0619\n",
      "Epoch [1/1], Step [6846/7635], Loss: 4.1354\n",
      "Epoch [1/1], Step [6847/7635], Loss: 4.1337\n",
      "Epoch [1/1], Step [6848/7635], Loss: 4.0631\n",
      "Epoch [1/1], Step [6849/7635], Loss: 4.0712\n",
      "Epoch [1/1], Step [6850/7635], Loss: 4.1133\n",
      "Epoch [1/1], Step [6851/7635], Loss: 4.1638\n",
      "Epoch [1/1], Step [6852/7635], Loss: 4.1101\n",
      "Epoch [1/1], Step [6853/7635], Loss: 4.0507\n",
      "Epoch [1/1], Step [6854/7635], Loss: 4.0678\n",
      "Epoch [1/1], Step [6855/7635], Loss: 4.0813\n",
      "Epoch [1/1], Step [6856/7635], Loss: 4.0254\n",
      "Epoch [1/1], Step [6857/7635], Loss: 4.0771\n",
      "Epoch [1/1], Step [6858/7635], Loss: 4.0537\n",
      "Epoch [1/1], Step [6859/7635], Loss: 4.1099\n",
      "Epoch [1/1], Step [6860/7635], Loss: 4.1251\n",
      "Epoch [1/1], Step [6861/7635], Loss: 4.1927\n",
      "Epoch [1/1], Step [6862/7635], Loss: 4.0827\n",
      "Epoch [1/1], Step [6863/7635], Loss: 4.1189\n",
      "Epoch [1/1], Step [6864/7635], Loss: 4.0707\n",
      "Epoch [1/1], Step [6865/7635], Loss: 4.1349\n",
      "Epoch [1/1], Step [6866/7635], Loss: 4.1040\n",
      "Epoch [1/1], Step [6867/7635], Loss: 4.0633\n",
      "Epoch [1/1], Step [6868/7635], Loss: 4.1061\n",
      "Epoch [1/1], Step [6869/7635], Loss: 4.1072\n",
      "Epoch [1/1], Step [6870/7635], Loss: 4.0852\n",
      "Epoch [1/1], Step [6871/7635], Loss: 4.1337\n",
      "Epoch [1/1], Step [6872/7635], Loss: 4.1148\n",
      "Epoch [1/1], Step [6873/7635], Loss: 4.1313\n",
      "Epoch [1/1], Step [6874/7635], Loss: 4.1713\n",
      "Epoch [1/1], Step [6875/7635], Loss: 4.0998\n",
      "Epoch [1/1], Step [6876/7635], Loss: 4.0539\n",
      "Epoch [1/1], Step [6877/7635], Loss: 4.0437\n",
      "Epoch [1/1], Step [6878/7635], Loss: 4.0437\n",
      "Epoch [1/1], Step [6879/7635], Loss: 4.0717\n",
      "Epoch [1/1], Step [6880/7635], Loss: 4.0351\n",
      "Epoch [1/1], Step [6881/7635], Loss: 4.0733\n",
      "Epoch [1/1], Step [6882/7635], Loss: 4.1209\n",
      "Epoch [1/1], Step [6883/7635], Loss: 4.0967\n",
      "Epoch [1/1], Step [6884/7635], Loss: 4.1580\n",
      "Epoch [1/1], Step [6885/7635], Loss: 4.1234\n",
      "Epoch [1/1], Step [6886/7635], Loss: 4.1326\n",
      "Epoch [1/1], Step [6887/7635], Loss: 4.0767\n",
      "Epoch [1/1], Step [6888/7635], Loss: 4.1462\n",
      "Epoch [1/1], Step [6889/7635], Loss: 4.0997\n",
      "Epoch [1/1], Step [6890/7635], Loss: 4.1265\n",
      "Epoch [1/1], Step [6891/7635], Loss: 3.9978\n",
      "Epoch [1/1], Step [6892/7635], Loss: 4.0139\n",
      "Epoch [1/1], Step [6893/7635], Loss: 4.0532\n",
      "Epoch [1/1], Step [6894/7635], Loss: 4.1861\n",
      "Epoch [1/1], Step [6895/7635], Loss: 4.1224\n",
      "Epoch [1/1], Step [6896/7635], Loss: 4.1617\n",
      "Epoch [1/1], Step [6897/7635], Loss: 4.0320\n",
      "Epoch [1/1], Step [6898/7635], Loss: 4.1093\n",
      "Epoch [1/1], Step [6899/7635], Loss: 4.1506\n",
      "Epoch [1/1], Step [6900/7635], Loss: 4.0732\n",
      "Epoch [1/1], Step [6901/7635], Loss: 4.0858\n",
      "Epoch [1/1], Step [6902/7635], Loss: 4.0930\n",
      "Epoch [1/1], Step [6903/7635], Loss: 4.1116\n",
      "Epoch [1/1], Step [6904/7635], Loss: 4.1104\n",
      "Epoch [1/1], Step [6905/7635], Loss: 4.0904\n",
      "Epoch [1/1], Step [6906/7635], Loss: 4.1425\n",
      "Epoch [1/1], Step [6907/7635], Loss: 4.1273\n",
      "Epoch [1/1], Step [6908/7635], Loss: 4.1120\n",
      "Epoch [1/1], Step [6909/7635], Loss: 4.1754\n",
      "Epoch [1/1], Step [6910/7635], Loss: 4.1813\n",
      "Epoch [1/1], Step [6911/7635], Loss: 4.1617\n",
      "Epoch [1/1], Step [6912/7635], Loss: 4.0382\n",
      "Epoch [1/1], Step [6913/7635], Loss: 4.1224\n",
      "Epoch [1/1], Step [6914/7635], Loss: 4.1712\n",
      "Epoch [1/1], Step [6915/7635], Loss: 4.1179\n",
      "Epoch [1/1], Step [6916/7635], Loss: 4.1615\n",
      "Epoch [1/1], Step [6917/7635], Loss: 4.1408\n",
      "Epoch [1/1], Step [6918/7635], Loss: 4.0982\n",
      "Epoch [1/1], Step [6919/7635], Loss: 4.0587\n",
      "Epoch [1/1], Step [6920/7635], Loss: 4.1217\n",
      "Epoch [1/1], Step [6921/7635], Loss: 4.0361\n",
      "Epoch [1/1], Step [6922/7635], Loss: 4.2255\n",
      "Epoch [1/1], Step [6923/7635], Loss: 4.0823\n",
      "Epoch [1/1], Step [6924/7635], Loss: 4.1252\n",
      "Epoch [1/1], Step [6925/7635], Loss: 4.0812\n",
      "Epoch [1/1], Step [6926/7635], Loss: 4.1169\n",
      "Epoch [1/1], Step [6927/7635], Loss: 4.1388\n",
      "Epoch [1/1], Step [6928/7635], Loss: 4.0628\n",
      "Epoch [1/1], Step [6929/7635], Loss: 4.1485\n",
      "Epoch [1/1], Step [6930/7635], Loss: 4.1331\n",
      "Epoch [1/1], Step [6931/7635], Loss: 4.0657\n",
      "Epoch [1/1], Step [6932/7635], Loss: 4.0256\n",
      "Epoch [1/1], Step [6933/7635], Loss: 4.1667\n",
      "Epoch [1/1], Step [6934/7635], Loss: 4.1312\n",
      "Epoch [1/1], Step [6935/7635], Loss: 4.0757\n",
      "Epoch [1/1], Step [6936/7635], Loss: 4.0702\n",
      "Epoch [1/1], Step [6937/7635], Loss: 4.0975\n",
      "Epoch [1/1], Step [6938/7635], Loss: 4.0644\n",
      "Epoch [1/1], Step [6939/7635], Loss: 4.1080\n",
      "Epoch [1/1], Step [6940/7635], Loss: 4.0927\n",
      "Epoch [1/1], Step [6941/7635], Loss: 4.1306\n",
      "Epoch [1/1], Step [6942/7635], Loss: 4.0843\n",
      "Epoch [1/1], Step [6943/7635], Loss: 4.1290\n",
      "Epoch [1/1], Step [6944/7635], Loss: 4.1652\n",
      "Epoch [1/1], Step [6945/7635], Loss: 4.1115\n",
      "Epoch [1/1], Step [6946/7635], Loss: 4.1330\n",
      "Epoch [1/1], Step [6947/7635], Loss: 4.1166\n",
      "Epoch [1/1], Step [6948/7635], Loss: 4.0795\n",
      "Epoch [1/1], Step [6949/7635], Loss: 4.1263\n",
      "Epoch [1/1], Step [6950/7635], Loss: 4.0694\n",
      "Epoch [1/1], Step [6951/7635], Loss: 4.1748\n",
      "Epoch [1/1], Step [6952/7635], Loss: 4.1479\n",
      "Epoch [1/1], Step [6953/7635], Loss: 4.1433\n",
      "Epoch [1/1], Step [6954/7635], Loss: 4.0300\n",
      "Epoch [1/1], Step [6955/7635], Loss: 4.0330\n",
      "Epoch [1/1], Step [6956/7635], Loss: 4.1717\n",
      "Epoch [1/1], Step [6957/7635], Loss: 4.1008\n",
      "Epoch [1/1], Step [6958/7635], Loss: 4.1684\n",
      "Epoch [1/1], Step [6959/7635], Loss: 4.1151\n",
      "Epoch [1/1], Step [6960/7635], Loss: 4.0620\n",
      "Epoch [1/1], Step [6961/7635], Loss: 4.1698\n",
      "Epoch [1/1], Step [6962/7635], Loss: 4.0906\n",
      "Epoch [1/1], Step [6963/7635], Loss: 4.1592\n",
      "Epoch [1/1], Step [6964/7635], Loss: 4.1377\n",
      "Epoch [1/1], Step [6965/7635], Loss: 4.0065\n",
      "Epoch [1/1], Step [6966/7635], Loss: 4.0792\n",
      "Epoch [1/1], Step [6967/7635], Loss: 4.1252\n",
      "Epoch [1/1], Step [6968/7635], Loss: 4.1193\n",
      "Epoch [1/1], Step [6969/7635], Loss: 4.1141\n",
      "Epoch [1/1], Step [6970/7635], Loss: 4.1441\n",
      "Epoch [1/1], Step [6971/7635], Loss: 3.9896\n",
      "Epoch [1/1], Step [6972/7635], Loss: 4.0775\n",
      "Epoch [1/1], Step [6973/7635], Loss: 4.1185\n",
      "Epoch [1/1], Step [6974/7635], Loss: 4.1144\n",
      "Epoch [1/1], Step [6975/7635], Loss: 4.1307\n",
      "Epoch [1/1], Step [6976/7635], Loss: 4.0627\n",
      "Epoch [1/1], Step [6977/7635], Loss: 4.0939\n",
      "Epoch [1/1], Step [6978/7635], Loss: 4.0299\n",
      "Epoch [1/1], Step [6979/7635], Loss: 4.0672\n",
      "Epoch [1/1], Step [6980/7635], Loss: 4.1069\n",
      "Epoch [1/1], Step [6981/7635], Loss: 4.0357\n",
      "Epoch [1/1], Step [6982/7635], Loss: 4.0491\n",
      "Epoch [1/1], Step [6983/7635], Loss: 4.0865\n",
      "Epoch [1/1], Step [6984/7635], Loss: 4.0983\n",
      "Epoch [1/1], Step [6985/7635], Loss: 4.0790\n",
      "Epoch [1/1], Step [6986/7635], Loss: 4.1168\n",
      "Epoch [1/1], Step [6987/7635], Loss: 4.0909\n",
      "Epoch [1/1], Step [6988/7635], Loss: 4.1367\n",
      "Epoch [1/1], Step [6989/7635], Loss: 4.1017\n",
      "Epoch [1/1], Step [6990/7635], Loss: 4.0587\n",
      "Epoch [1/1], Step [6991/7635], Loss: 4.1456\n",
      "Epoch [1/1], Step [6992/7635], Loss: 4.0375\n",
      "Epoch [1/1], Step [6993/7635], Loss: 4.0862\n",
      "Epoch [1/1], Step [6994/7635], Loss: 4.0818\n",
      "Epoch [1/1], Step [6995/7635], Loss: 4.1132\n",
      "Epoch [1/1], Step [6996/7635], Loss: 4.2268\n",
      "Epoch [1/1], Step [6997/7635], Loss: 4.0740\n",
      "Epoch [1/1], Step [6998/7635], Loss: 4.1312\n",
      "Epoch [1/1], Step [6999/7635], Loss: 4.0828\n",
      "Epoch [1/1], Step [7000/7635], Loss: 4.1250\n",
      "Epoch [1/1], Step [7001/7635], Loss: 4.0926\n",
      "Epoch [1/1], Step [7002/7635], Loss: 4.1071\n",
      "Epoch [1/1], Step [7003/7635], Loss: 4.0563\n",
      "Epoch [1/1], Step [7004/7635], Loss: 4.0851\n",
      "Epoch [1/1], Step [7005/7635], Loss: 4.0890\n",
      "Epoch [1/1], Step [7006/7635], Loss: 4.1500\n",
      "Epoch [1/1], Step [7007/7635], Loss: 4.1140\n",
      "Epoch [1/1], Step [7008/7635], Loss: 4.1355\n",
      "Epoch [1/1], Step [7009/7635], Loss: 4.0870\n",
      "Epoch [1/1], Step [7010/7635], Loss: 4.0964\n",
      "Epoch [1/1], Step [7011/7635], Loss: 4.0140\n",
      "Epoch [1/1], Step [7012/7635], Loss: 4.1761\n",
      "Epoch [1/1], Step [7013/7635], Loss: 4.0394\n",
      "Epoch [1/1], Step [7014/7635], Loss: 4.0773\n",
      "Epoch [1/1], Step [7015/7635], Loss: 4.1217\n",
      "Epoch [1/1], Step [7016/7635], Loss: 4.0841\n",
      "Epoch [1/1], Step [7017/7635], Loss: 4.1367\n",
      "Epoch [1/1], Step [7018/7635], Loss: 4.0420\n",
      "Epoch [1/1], Step [7019/7635], Loss: 4.1166\n",
      "Epoch [1/1], Step [7020/7635], Loss: 4.1022\n",
      "Epoch [1/1], Step [7021/7635], Loss: 4.0953\n",
      "Epoch [1/1], Step [7022/7635], Loss: 4.0941\n",
      "Epoch [1/1], Step [7023/7635], Loss: 4.1146\n",
      "Epoch [1/1], Step [7024/7635], Loss: 4.1483\n",
      "Epoch [1/1], Step [7025/7635], Loss: 4.0959\n",
      "Epoch [1/1], Step [7026/7635], Loss: 4.1002\n",
      "Epoch [1/1], Step [7027/7635], Loss: 4.1066\n",
      "Epoch [1/1], Step [7028/7635], Loss: 4.1117\n",
      "Epoch [1/1], Step [7029/7635], Loss: 4.0211\n",
      "Epoch [1/1], Step [7030/7635], Loss: 4.1032\n",
      "Epoch [1/1], Step [7031/7635], Loss: 4.1360\n",
      "Epoch [1/1], Step [7032/7635], Loss: 4.0671\n",
      "Epoch [1/1], Step [7033/7635], Loss: 4.0918\n",
      "Epoch [1/1], Step [7034/7635], Loss: 4.1159\n",
      "Epoch [1/1], Step [7035/7635], Loss: 4.0766\n",
      "Epoch [1/1], Step [7036/7635], Loss: 4.0337\n",
      "Epoch [1/1], Step [7037/7635], Loss: 4.0544\n",
      "Epoch [1/1], Step [7038/7635], Loss: 4.0520\n",
      "Epoch [1/1], Step [7039/7635], Loss: 4.0672\n",
      "Epoch [1/1], Step [7040/7635], Loss: 4.0706\n",
      "Epoch [1/1], Step [7041/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [7042/7635], Loss: 4.1761\n",
      "Epoch [1/1], Step [7043/7635], Loss: 4.0464\n",
      "Epoch [1/1], Step [7044/7635], Loss: 4.0448\n",
      "Epoch [1/1], Step [7045/7635], Loss: 4.0725\n",
      "Epoch [1/1], Step [7046/7635], Loss: 4.1120\n",
      "Epoch [1/1], Step [7047/7635], Loss: 4.1485\n",
      "Epoch [1/1], Step [7048/7635], Loss: 4.0445\n",
      "Epoch [1/1], Step [7049/7635], Loss: 4.1399\n",
      "Epoch [1/1], Step [7050/7635], Loss: 4.1271\n",
      "Epoch [1/1], Step [7051/7635], Loss: 4.1559\n",
      "Epoch [1/1], Step [7052/7635], Loss: 4.0619\n",
      "Epoch [1/1], Step [7053/7635], Loss: 4.1013\n",
      "Epoch [1/1], Step [7054/7635], Loss: 4.1353\n",
      "Epoch [1/1], Step [7055/7635], Loss: 3.9851\n",
      "Epoch [1/1], Step [7056/7635], Loss: 4.1298\n",
      "Epoch [1/1], Step [7057/7635], Loss: 4.1044\n",
      "Epoch [1/1], Step [7058/7635], Loss: 4.1077\n",
      "Epoch [1/1], Step [7059/7635], Loss: 4.1035\n",
      "Epoch [1/1], Step [7060/7635], Loss: 4.0740\n",
      "Epoch [1/1], Step [7061/7635], Loss: 4.0779\n",
      "Epoch [1/1], Step [7062/7635], Loss: 4.1576\n",
      "Epoch [1/1], Step [7063/7635], Loss: 4.0884\n",
      "Epoch [1/1], Step [7064/7635], Loss: 4.1217\n",
      "Epoch [1/1], Step [7065/7635], Loss: 4.0651\n",
      "Epoch [1/1], Step [7066/7635], Loss: 4.1063\n",
      "Epoch [1/1], Step [7067/7635], Loss: 4.1188\n",
      "Epoch [1/1], Step [7068/7635], Loss: 4.0665\n",
      "Epoch [1/1], Step [7069/7635], Loss: 4.0916\n",
      "Epoch [1/1], Step [7070/7635], Loss: 4.0655\n",
      "Epoch [1/1], Step [7071/7635], Loss: 4.1735\n",
      "Epoch [1/1], Step [7072/7635], Loss: 4.1124\n",
      "Epoch [1/1], Step [7073/7635], Loss: 4.0575\n",
      "Epoch [1/1], Step [7074/7635], Loss: 4.0587\n",
      "Epoch [1/1], Step [7075/7635], Loss: 4.1054\n",
      "Epoch [1/1], Step [7076/7635], Loss: 4.0968\n",
      "Epoch [1/1], Step [7077/7635], Loss: 4.1253\n",
      "Epoch [1/1], Step [7078/7635], Loss: 4.1446\n",
      "Epoch [1/1], Step [7079/7635], Loss: 4.0637\n",
      "Epoch [1/1], Step [7080/7635], Loss: 4.0947\n",
      "Epoch [1/1], Step [7081/7635], Loss: 4.0964\n",
      "Epoch [1/1], Step [7082/7635], Loss: 4.1507\n",
      "Epoch [1/1], Step [7083/7635], Loss: 4.1633\n",
      "Epoch [1/1], Step [7084/7635], Loss: 4.1804\n",
      "Epoch [1/1], Step [7085/7635], Loss: 4.0974\n",
      "Epoch [1/1], Step [7086/7635], Loss: 4.0330\n",
      "Epoch [1/1], Step [7087/7635], Loss: 4.1056\n",
      "Epoch [1/1], Step [7088/7635], Loss: 4.1581\n",
      "Epoch [1/1], Step [7089/7635], Loss: 4.0937\n",
      "Epoch [1/1], Step [7090/7635], Loss: 4.0560\n",
      "Epoch [1/1], Step [7091/7635], Loss: 4.1233\n",
      "Epoch [1/1], Step [7092/7635], Loss: 4.0872\n",
      "Epoch [1/1], Step [7093/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [7094/7635], Loss: 4.1145\n",
      "Epoch [1/1], Step [7095/7635], Loss: 4.0730\n",
      "Epoch [1/1], Step [7096/7635], Loss: 4.1829\n",
      "Epoch [1/1], Step [7097/7635], Loss: 4.1121\n",
      "Epoch [1/1], Step [7098/7635], Loss: 4.0396\n",
      "Epoch [1/1], Step [7099/7635], Loss: 4.1219\n",
      "Epoch [1/1], Step [7100/7635], Loss: 4.0711\n",
      "Epoch [1/1], Step [7101/7635], Loss: 4.1065\n",
      "Epoch [1/1], Step [7102/7635], Loss: 4.0273\n",
      "Epoch [1/1], Step [7103/7635], Loss: 4.0517\n",
      "Epoch [1/1], Step [7104/7635], Loss: 4.0417\n",
      "Epoch [1/1], Step [7105/7635], Loss: 4.1374\n",
      "Epoch [1/1], Step [7106/7635], Loss: 4.1751\n",
      "Epoch [1/1], Step [7107/7635], Loss: 4.0273\n",
      "Epoch [1/1], Step [7108/7635], Loss: 4.1022\n",
      "Epoch [1/1], Step [7109/7635], Loss: 4.0723\n",
      "Epoch [1/1], Step [7110/7635], Loss: 4.1537\n",
      "Epoch [1/1], Step [7111/7635], Loss: 4.0208\n",
      "Epoch [1/1], Step [7112/7635], Loss: 4.0874\n",
      "Epoch [1/1], Step [7113/7635], Loss: 4.2138\n",
      "Epoch [1/1], Step [7114/7635], Loss: 4.1044\n",
      "Epoch [1/1], Step [7115/7635], Loss: 4.1012\n",
      "Epoch [1/1], Step [7116/7635], Loss: 4.1391\n",
      "Epoch [1/1], Step [7117/7635], Loss: 4.1148\n",
      "Epoch [1/1], Step [7118/7635], Loss: 4.1261\n",
      "Epoch [1/1], Step [7119/7635], Loss: 4.0467\n",
      "Epoch [1/1], Step [7120/7635], Loss: 4.0828\n",
      "Epoch [1/1], Step [7121/7635], Loss: 4.1571\n",
      "Epoch [1/1], Step [7122/7635], Loss: 4.0827\n",
      "Epoch [1/1], Step [7123/7635], Loss: 4.0772\n",
      "Epoch [1/1], Step [7124/7635], Loss: 4.0487\n",
      "Epoch [1/1], Step [7125/7635], Loss: 4.0774\n",
      "Epoch [1/1], Step [7126/7635], Loss: 4.1049\n",
      "Epoch [1/1], Step [7127/7635], Loss: 4.1268\n",
      "Epoch [1/1], Step [7128/7635], Loss: 4.1332\n",
      "Epoch [1/1], Step [7129/7635], Loss: 4.0676\n",
      "Epoch [1/1], Step [7130/7635], Loss: 4.1723\n",
      "Epoch [1/1], Step [7131/7635], Loss: 4.1329\n",
      "Epoch [1/1], Step [7132/7635], Loss: 4.0731\n",
      "Epoch [1/1], Step [7133/7635], Loss: 4.1130\n",
      "Epoch [1/1], Step [7134/7635], Loss: 4.1719\n",
      "Epoch [1/1], Step [7135/7635], Loss: 4.0738\n",
      "Epoch [1/1], Step [7136/7635], Loss: 4.0937\n",
      "Epoch [1/1], Step [7137/7635], Loss: 4.1052\n",
      "Epoch [1/1], Step [7138/7635], Loss: 4.0197\n",
      "Epoch [1/1], Step [7139/7635], Loss: 4.1035\n",
      "Epoch [1/1], Step [7140/7635], Loss: 4.0775\n",
      "Epoch [1/1], Step [7141/7635], Loss: 4.0741\n",
      "Epoch [1/1], Step [7142/7635], Loss: 4.1223\n",
      "Epoch [1/1], Step [7143/7635], Loss: 4.0743\n",
      "Epoch [1/1], Step [7144/7635], Loss: 4.0789\n",
      "Epoch [1/1], Step [7145/7635], Loss: 4.1060\n",
      "Epoch [1/1], Step [7146/7635], Loss: 4.1000\n",
      "Epoch [1/1], Step [7147/7635], Loss: 4.1079\n",
      "Epoch [1/1], Step [7148/7635], Loss: 4.0674\n",
      "Epoch [1/1], Step [7149/7635], Loss: 4.0831\n",
      "Epoch [1/1], Step [7150/7635], Loss: 4.0230\n",
      "Epoch [1/1], Step [7151/7635], Loss: 4.0522\n",
      "Epoch [1/1], Step [7152/7635], Loss: 4.1415\n",
      "Epoch [1/1], Step [7153/7635], Loss: 4.0360\n",
      "Epoch [1/1], Step [7154/7635], Loss: 4.1122\n",
      "Epoch [1/1], Step [7155/7635], Loss: 4.1091\n",
      "Epoch [1/1], Step [7156/7635], Loss: 4.1102\n",
      "Epoch [1/1], Step [7157/7635], Loss: 4.0616\n",
      "Epoch [1/1], Step [7158/7635], Loss: 4.1766\n",
      "Epoch [1/1], Step [7159/7635], Loss: 4.1061\n",
      "Epoch [1/1], Step [7160/7635], Loss: 4.0706\n",
      "Epoch [1/1], Step [7161/7635], Loss: 4.0729\n",
      "Epoch [1/1], Step [7162/7635], Loss: 4.1093\n",
      "Epoch [1/1], Step [7163/7635], Loss: 4.1334\n",
      "Epoch [1/1], Step [7164/7635], Loss: 4.0660\n",
      "Epoch [1/1], Step [7165/7635], Loss: 4.0674\n",
      "Epoch [1/1], Step [7166/7635], Loss: 4.0069\n",
      "Epoch [1/1], Step [7167/7635], Loss: 4.0748\n",
      "Epoch [1/1], Step [7168/7635], Loss: 4.0843\n",
      "Epoch [1/1], Step [7169/7635], Loss: 4.0757\n",
      "Epoch [1/1], Step [7170/7635], Loss: 4.0731\n",
      "Epoch [1/1], Step [7171/7635], Loss: 4.1296\n",
      "Epoch [1/1], Step [7172/7635], Loss: 4.0532\n",
      "Epoch [1/1], Step [7173/7635], Loss: 4.0796\n",
      "Epoch [1/1], Step [7174/7635], Loss: 4.0902\n",
      "Epoch [1/1], Step [7175/7635], Loss: 4.1772\n",
      "Epoch [1/1], Step [7176/7635], Loss: 4.0185\n",
      "Epoch [1/1], Step [7177/7635], Loss: 4.1942\n",
      "Epoch [1/1], Step [7178/7635], Loss: 4.0074\n",
      "Epoch [1/1], Step [7179/7635], Loss: 4.1084\n",
      "Epoch [1/1], Step [7180/7635], Loss: 4.0361\n",
      "Epoch [1/1], Step [7181/7635], Loss: 4.0405\n",
      "Epoch [1/1], Step [7182/7635], Loss: 4.0654\n",
      "Epoch [1/1], Step [7183/7635], Loss: 4.0327\n",
      "Epoch [1/1], Step [7184/7635], Loss: 4.1308\n",
      "Epoch [1/1], Step [7185/7635], Loss: 4.0920\n",
      "Epoch [1/1], Step [7186/7635], Loss: 4.2446\n",
      "Epoch [1/1], Step [7187/7635], Loss: 4.0555\n",
      "Epoch [1/1], Step [7188/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [7189/7635], Loss: 4.1477\n",
      "Epoch [1/1], Step [7190/7635], Loss: 4.1046\n",
      "Epoch [1/1], Step [7191/7635], Loss: 4.1114\n",
      "Epoch [1/1], Step [7192/7635], Loss: 4.1189\n",
      "Epoch [1/1], Step [7193/7635], Loss: 4.0927\n",
      "Epoch [1/1], Step [7194/7635], Loss: 4.1010\n",
      "Epoch [1/1], Step [7195/7635], Loss: 4.1611\n",
      "Epoch [1/1], Step [7196/7635], Loss: 4.0778\n",
      "Epoch [1/1], Step [7197/7635], Loss: 4.1345\n",
      "Epoch [1/1], Step [7198/7635], Loss: 4.0708\n",
      "Epoch [1/1], Step [7199/7635], Loss: 4.1170\n",
      "Epoch [1/1], Step [7200/7635], Loss: 4.0786\n",
      "Epoch [1/1], Step [7201/7635], Loss: 4.1594\n",
      "Epoch [1/1], Step [7202/7635], Loss: 4.1006\n",
      "Epoch [1/1], Step [7203/7635], Loss: 4.1292\n",
      "Epoch [1/1], Step [7204/7635], Loss: 4.0571\n",
      "Epoch [1/1], Step [7205/7635], Loss: 4.0937\n",
      "Epoch [1/1], Step [7206/7635], Loss: 4.1092\n",
      "Epoch [1/1], Step [7207/7635], Loss: 4.0631\n",
      "Epoch [1/1], Step [7208/7635], Loss: 4.0852\n",
      "Epoch [1/1], Step [7209/7635], Loss: 4.1128\n",
      "Epoch [1/1], Step [7210/7635], Loss: 4.0837\n",
      "Epoch [1/1], Step [7211/7635], Loss: 4.1148\n",
      "Epoch [1/1], Step [7212/7635], Loss: 4.1050\n",
      "Epoch [1/1], Step [7213/7635], Loss: 4.1133\n",
      "Epoch [1/1], Step [7214/7635], Loss: 4.0928\n",
      "Epoch [1/1], Step [7215/7635], Loss: 4.1227\n",
      "Epoch [1/1], Step [7216/7635], Loss: 4.1216\n",
      "Epoch [1/1], Step [7217/7635], Loss: 4.0581\n",
      "Epoch [1/1], Step [7218/7635], Loss: 4.0931\n",
      "Epoch [1/1], Step [7219/7635], Loss: 4.0961\n",
      "Epoch [1/1], Step [7220/7635], Loss: 4.1010\n",
      "Epoch [1/1], Step [7221/7635], Loss: 4.0304\n",
      "Epoch [1/1], Step [7222/7635], Loss: 4.1039\n",
      "Epoch [1/1], Step [7223/7635], Loss: 4.0841\n",
      "Epoch [1/1], Step [7224/7635], Loss: 4.0104\n",
      "Epoch [1/1], Step [7225/7635], Loss: 4.1067\n",
      "Epoch [1/1], Step [7226/7635], Loss: 4.0693\n",
      "Epoch [1/1], Step [7227/7635], Loss: 4.2006\n",
      "Epoch [1/1], Step [7228/7635], Loss: 4.1020\n",
      "Epoch [1/1], Step [7229/7635], Loss: 4.1474\n",
      "Epoch [1/1], Step [7230/7635], Loss: 4.0809\n",
      "Epoch [1/1], Step [7231/7635], Loss: 4.0940\n",
      "Epoch [1/1], Step [7232/7635], Loss: 4.0651\n",
      "Epoch [1/1], Step [7233/7635], Loss: 4.1428\n",
      "Epoch [1/1], Step [7234/7635], Loss: 4.0526\n",
      "Epoch [1/1], Step [7235/7635], Loss: 4.0504\n",
      "Epoch [1/1], Step [7236/7635], Loss: 4.1381\n",
      "Epoch [1/1], Step [7237/7635], Loss: 4.0277\n",
      "Epoch [1/1], Step [7238/7635], Loss: 4.0835\n",
      "Epoch [1/1], Step [7239/7635], Loss: 4.0408\n",
      "Epoch [1/1], Step [7240/7635], Loss: 4.1461\n",
      "Epoch [1/1], Step [7241/7635], Loss: 4.0893\n",
      "Epoch [1/1], Step [7242/7635], Loss: 4.0868\n",
      "Epoch [1/1], Step [7243/7635], Loss: 4.0507\n",
      "Epoch [1/1], Step [7244/7635], Loss: 4.1478\n",
      "Epoch [1/1], Step [7245/7635], Loss: 4.0270\n",
      "Epoch [1/1], Step [7246/7635], Loss: 4.1020\n",
      "Epoch [1/1], Step [7247/7635], Loss: 4.1295\n",
      "Epoch [1/1], Step [7248/7635], Loss: 4.0941\n",
      "Epoch [1/1], Step [7249/7635], Loss: 4.1187\n",
      "Epoch [1/1], Step [7250/7635], Loss: 4.0969\n",
      "Epoch [1/1], Step [7251/7635], Loss: 4.0981\n",
      "Epoch [1/1], Step [7252/7635], Loss: 4.0815\n",
      "Epoch [1/1], Step [7253/7635], Loss: 4.0611\n",
      "Epoch [1/1], Step [7254/7635], Loss: 4.1345\n",
      "Epoch [1/1], Step [7255/7635], Loss: 4.0713\n",
      "Epoch [1/1], Step [7256/7635], Loss: 4.0510\n",
      "Epoch [1/1], Step [7257/7635], Loss: 4.1002\n",
      "Epoch [1/1], Step [7258/7635], Loss: 4.1764\n",
      "Epoch [1/1], Step [7259/7635], Loss: 4.0938\n",
      "Epoch [1/1], Step [7260/7635], Loss: 4.1131\n",
      "Epoch [1/1], Step [7261/7635], Loss: 4.0664\n",
      "Epoch [1/1], Step [7262/7635], Loss: 4.0772\n",
      "Epoch [1/1], Step [7263/7635], Loss: 4.1475\n",
      "Epoch [1/1], Step [7264/7635], Loss: 4.1609\n",
      "Epoch [1/1], Step [7265/7635], Loss: 4.1667\n",
      "Epoch [1/1], Step [7266/7635], Loss: 4.0479\n",
      "Epoch [1/1], Step [7267/7635], Loss: 4.0835\n",
      "Epoch [1/1], Step [7268/7635], Loss: 4.0590\n",
      "Epoch [1/1], Step [7269/7635], Loss: 4.0631\n",
      "Epoch [1/1], Step [7270/7635], Loss: 4.1594\n",
      "Epoch [1/1], Step [7271/7635], Loss: 4.1485\n",
      "Epoch [1/1], Step [7272/7635], Loss: 4.0531\n",
      "Epoch [1/1], Step [7273/7635], Loss: 4.1273\n",
      "Epoch [1/1], Step [7274/7635], Loss: 4.1250\n",
      "Epoch [1/1], Step [7275/7635], Loss: 4.1274\n",
      "Epoch [1/1], Step [7276/7635], Loss: 4.1516\n",
      "Epoch [1/1], Step [7277/7635], Loss: 4.1109\n",
      "Epoch [1/1], Step [7278/7635], Loss: 4.1377\n",
      "Epoch [1/1], Step [7279/7635], Loss: 4.1152\n",
      "Epoch [1/1], Step [7280/7635], Loss: 4.0511\n",
      "Epoch [1/1], Step [7281/7635], Loss: 4.0567\n",
      "Epoch [1/1], Step [7282/7635], Loss: 4.0953\n",
      "Epoch [1/1], Step [7283/7635], Loss: 4.0330\n",
      "Epoch [1/1], Step [7284/7635], Loss: 4.0806\n",
      "Epoch [1/1], Step [7285/7635], Loss: 4.1022\n",
      "Epoch [1/1], Step [7286/7635], Loss: 4.0703\n",
      "Epoch [1/1], Step [7287/7635], Loss: 4.1084\n",
      "Epoch [1/1], Step [7288/7635], Loss: 4.0942\n",
      "Epoch [1/1], Step [7289/7635], Loss: 4.1663\n",
      "Epoch [1/1], Step [7290/7635], Loss: 4.1368\n",
      "Epoch [1/1], Step [7291/7635], Loss: 4.0309\n",
      "Epoch [1/1], Step [7292/7635], Loss: 4.1291\n",
      "Epoch [1/1], Step [7293/7635], Loss: 4.0255\n",
      "Epoch [1/1], Step [7294/7635], Loss: 4.1407\n",
      "Epoch [1/1], Step [7295/7635], Loss: 4.0625\n",
      "Epoch [1/1], Step [7296/7635], Loss: 4.0782\n",
      "Epoch [1/1], Step [7297/7635], Loss: 4.0612\n",
      "Epoch [1/1], Step [7298/7635], Loss: 4.1370\n",
      "Epoch [1/1], Step [7299/7635], Loss: 4.0716\n",
      "Epoch [1/1], Step [7300/7635], Loss: 4.0376\n",
      "Epoch [1/1], Step [7301/7635], Loss: 4.0304\n",
      "Epoch [1/1], Step [7302/7635], Loss: 4.0557\n",
      "Epoch [1/1], Step [7303/7635], Loss: 4.0233\n",
      "Epoch [1/1], Step [7304/7635], Loss: 4.0776\n",
      "Epoch [1/1], Step [7305/7635], Loss: 4.0633\n",
      "Epoch [1/1], Step [7306/7635], Loss: 4.1495\n",
      "Epoch [1/1], Step [7307/7635], Loss: 4.1076\n",
      "Epoch [1/1], Step [7308/7635], Loss: 4.0738\n",
      "Epoch [1/1], Step [7309/7635], Loss: 4.1261\n",
      "Epoch [1/1], Step [7310/7635], Loss: 4.1066\n",
      "Epoch [1/1], Step [7311/7635], Loss: 4.1117\n",
      "Epoch [1/1], Step [7312/7635], Loss: 4.0562\n",
      "Epoch [1/1], Step [7313/7635], Loss: 4.0975\n",
      "Epoch [1/1], Step [7314/7635], Loss: 4.0778\n",
      "Epoch [1/1], Step [7315/7635], Loss: 4.0612\n",
      "Epoch [1/1], Step [7316/7635], Loss: 4.0035\n",
      "Epoch [1/1], Step [7317/7635], Loss: 4.0803\n",
      "Epoch [1/1], Step [7318/7635], Loss: 4.0399\n",
      "Epoch [1/1], Step [7319/7635], Loss: 4.1759\n",
      "Epoch [1/1], Step [7320/7635], Loss: 4.1125\n",
      "Epoch [1/1], Step [7321/7635], Loss: 4.1614\n",
      "Epoch [1/1], Step [7322/7635], Loss: 4.0481\n",
      "Epoch [1/1], Step [7323/7635], Loss: 4.0777\n",
      "Epoch [1/1], Step [7324/7635], Loss: 4.0651\n",
      "Epoch [1/1], Step [7325/7635], Loss: 4.1056\n",
      "Epoch [1/1], Step [7326/7635], Loss: 4.1275\n",
      "Epoch [1/1], Step [7327/7635], Loss: 4.0332\n",
      "Epoch [1/1], Step [7328/7635], Loss: 4.0673\n",
      "Epoch [1/1], Step [7329/7635], Loss: 4.1456\n",
      "Epoch [1/1], Step [7330/7635], Loss: 4.1429\n",
      "Epoch [1/1], Step [7331/7635], Loss: 4.0729\n",
      "Epoch [1/1], Step [7332/7635], Loss: 4.1784\n",
      "Epoch [1/1], Step [7333/7635], Loss: 4.1360\n",
      "Epoch [1/1], Step [7334/7635], Loss: 4.0972\n",
      "Epoch [1/1], Step [7335/7635], Loss: 4.1204\n",
      "Epoch [1/1], Step [7336/7635], Loss: 4.0481\n",
      "Epoch [1/1], Step [7337/7635], Loss: 4.0984\n",
      "Epoch [1/1], Step [7338/7635], Loss: 4.0717\n",
      "Epoch [1/1], Step [7339/7635], Loss: 4.0926\n",
      "Epoch [1/1], Step [7340/7635], Loss: 4.1248\n",
      "Epoch [1/1], Step [7341/7635], Loss: 4.1363\n",
      "Epoch [1/1], Step [7342/7635], Loss: 4.2091\n",
      "Epoch [1/1], Step [7343/7635], Loss: 4.1083\n",
      "Epoch [1/1], Step [7344/7635], Loss: 4.0305\n",
      "Epoch [1/1], Step [7345/7635], Loss: 4.0476\n",
      "Epoch [1/1], Step [7346/7635], Loss: 4.1672\n",
      "Epoch [1/1], Step [7347/7635], Loss: 4.0510\n",
      "Epoch [1/1], Step [7348/7635], Loss: 4.0983\n",
      "Epoch [1/1], Step [7349/7635], Loss: 4.1515\n",
      "Epoch [1/1], Step [7350/7635], Loss: 4.1016\n",
      "Epoch [1/1], Step [7351/7635], Loss: 4.1037\n",
      "Epoch [1/1], Step [7352/7635], Loss: 4.0199\n",
      "Epoch [1/1], Step [7353/7635], Loss: 4.0498\n",
      "Epoch [1/1], Step [7354/7635], Loss: 4.0848\n",
      "Epoch [1/1], Step [7355/7635], Loss: 4.0311\n",
      "Epoch [1/1], Step [7356/7635], Loss: 4.0671\n",
      "Epoch [1/1], Step [7357/7635], Loss: 4.1091\n",
      "Epoch [1/1], Step [7358/7635], Loss: 4.1014\n",
      "Epoch [1/1], Step [7359/7635], Loss: 4.0754\n",
      "Epoch [1/1], Step [7360/7635], Loss: 4.1539\n",
      "Epoch [1/1], Step [7361/7635], Loss: 4.1207\n",
      "Epoch [1/1], Step [7362/7635], Loss: 4.0798\n",
      "Epoch [1/1], Step [7363/7635], Loss: 4.1229\n",
      "Epoch [1/1], Step [7364/7635], Loss: 4.0837\n",
      "Epoch [1/1], Step [7365/7635], Loss: 4.0602\n",
      "Epoch [1/1], Step [7366/7635], Loss: 4.1089\n",
      "Epoch [1/1], Step [7367/7635], Loss: 4.0472\n",
      "Epoch [1/1], Step [7368/7635], Loss: 4.1410\n",
      "Epoch [1/1], Step [7369/7635], Loss: 4.1504\n",
      "Epoch [1/1], Step [7370/7635], Loss: 4.1461\n",
      "Epoch [1/1], Step [7371/7635], Loss: 4.0384\n",
      "Epoch [1/1], Step [7372/7635], Loss: 4.1534\n",
      "Epoch [1/1], Step [7373/7635], Loss: 4.0660\n",
      "Epoch [1/1], Step [7374/7635], Loss: 4.0338\n",
      "Epoch [1/1], Step [7375/7635], Loss: 4.1370\n",
      "Epoch [1/1], Step [7376/7635], Loss: 4.0916\n",
      "Epoch [1/1], Step [7377/7635], Loss: 4.0723\n",
      "Epoch [1/1], Step [7378/7635], Loss: 4.0821\n",
      "Epoch [1/1], Step [7379/7635], Loss: 4.0977\n",
      "Epoch [1/1], Step [7380/7635], Loss: 4.2311\n",
      "Epoch [1/1], Step [7381/7635], Loss: 4.1300\n",
      "Epoch [1/1], Step [7382/7635], Loss: 4.1517\n",
      "Epoch [1/1], Step [7383/7635], Loss: 4.1179\n",
      "Epoch [1/1], Step [7384/7635], Loss: 4.2153\n",
      "Epoch [1/1], Step [7385/7635], Loss: 4.0875\n",
      "Epoch [1/1], Step [7386/7635], Loss: 4.1309\n",
      "Epoch [1/1], Step [7387/7635], Loss: 4.0919\n",
      "Epoch [1/1], Step [7388/7635], Loss: 4.0385\n",
      "Epoch [1/1], Step [7389/7635], Loss: 4.1099\n",
      "Epoch [1/1], Step [7390/7635], Loss: 4.1369\n",
      "Epoch [1/1], Step [7391/7635], Loss: 4.1418\n",
      "Epoch [1/1], Step [7392/7635], Loss: 4.0275\n",
      "Epoch [1/1], Step [7393/7635], Loss: 4.1076\n",
      "Epoch [1/1], Step [7394/7635], Loss: 4.1056\n",
      "Epoch [1/1], Step [7395/7635], Loss: 4.1015\n",
      "Epoch [1/1], Step [7396/7635], Loss: 4.1734\n",
      "Epoch [1/1], Step [7397/7635], Loss: 3.9920\n",
      "Epoch [1/1], Step [7398/7635], Loss: 4.0365\n",
      "Epoch [1/1], Step [7399/7635], Loss: 4.1155\n",
      "Epoch [1/1], Step [7400/7635], Loss: 4.0979\n",
      "Epoch [1/1], Step [7401/7635], Loss: 4.1059\n",
      "Epoch [1/1], Step [7402/7635], Loss: 4.0995\n",
      "Epoch [1/1], Step [7403/7635], Loss: 4.0240\n",
      "Epoch [1/1], Step [7404/7635], Loss: 4.1063\n",
      "Epoch [1/1], Step [7405/7635], Loss: 4.0619\n",
      "Epoch [1/1], Step [7406/7635], Loss: 4.0915\n",
      "Epoch [1/1], Step [7407/7635], Loss: 4.0600\n",
      "Epoch [1/1], Step [7408/7635], Loss: 4.0926\n",
      "Epoch [1/1], Step [7409/7635], Loss: 4.0830\n",
      "Epoch [1/1], Step [7410/7635], Loss: 4.0379\n",
      "Epoch [1/1], Step [7411/7635], Loss: 4.1064\n",
      "Epoch [1/1], Step [7412/7635], Loss: 4.0964\n",
      "Epoch [1/1], Step [7413/7635], Loss: 4.0363\n",
      "Epoch [1/1], Step [7414/7635], Loss: 4.0999\n",
      "Epoch [1/1], Step [7415/7635], Loss: 4.0316\n",
      "Epoch [1/1], Step [7416/7635], Loss: 4.0978\n",
      "Epoch [1/1], Step [7417/7635], Loss: 4.1474\n",
      "Epoch [1/1], Step [7418/7635], Loss: 4.0605\n",
      "Epoch [1/1], Step [7419/7635], Loss: 4.0895\n",
      "Epoch [1/1], Step [7420/7635], Loss: 4.1266\n",
      "Epoch [1/1], Step [7421/7635], Loss: 4.0851\n",
      "Epoch [1/1], Step [7422/7635], Loss: 4.0709\n",
      "Epoch [1/1], Step [7423/7635], Loss: 4.0620\n",
      "Epoch [1/1], Step [7424/7635], Loss: 4.0673\n",
      "Epoch [1/1], Step [7425/7635], Loss: 4.0868\n",
      "Epoch [1/1], Step [7426/7635], Loss: 4.1159\n",
      "Epoch [1/1], Step [7427/7635], Loss: 4.1801\n",
      "Epoch [1/1], Step [7428/7635], Loss: 4.0858\n",
      "Epoch [1/1], Step [7429/7635], Loss: 4.0718\n",
      "Epoch [1/1], Step [7430/7635], Loss: 4.1064\n",
      "Epoch [1/1], Step [7431/7635], Loss: 4.0917\n",
      "Epoch [1/1], Step [7432/7635], Loss: 4.0399\n",
      "Epoch [1/1], Step [7433/7635], Loss: 4.0702\n",
      "Epoch [1/1], Step [7434/7635], Loss: 4.0038\n",
      "Epoch [1/1], Step [7435/7635], Loss: 4.0347\n",
      "Epoch [1/1], Step [7436/7635], Loss: 4.0978\n",
      "Epoch [1/1], Step [7437/7635], Loss: 4.0514\n",
      "Epoch [1/1], Step [7438/7635], Loss: 4.0923\n",
      "Epoch [1/1], Step [7439/7635], Loss: 4.0895\n",
      "Epoch [1/1], Step [7440/7635], Loss: 4.0858\n",
      "Epoch [1/1], Step [7441/7635], Loss: 4.1607\n",
      "Epoch [1/1], Step [7442/7635], Loss: 4.0163\n",
      "Epoch [1/1], Step [7443/7635], Loss: 4.0758\n",
      "Epoch [1/1], Step [7444/7635], Loss: 4.1074\n",
      "Epoch [1/1], Step [7445/7635], Loss: 4.0616\n",
      "Epoch [1/1], Step [7446/7635], Loss: 4.0661\n",
      "Epoch [1/1], Step [7447/7635], Loss: 4.0388\n",
      "Epoch [1/1], Step [7448/7635], Loss: 4.1168\n",
      "Epoch [1/1], Step [7449/7635], Loss: 4.0994\n",
      "Epoch [1/1], Step [7450/7635], Loss: 4.1371\n",
      "Epoch [1/1], Step [7451/7635], Loss: 4.0812\n",
      "Epoch [1/1], Step [7452/7635], Loss: 4.1033\n",
      "Epoch [1/1], Step [7453/7635], Loss: 4.0979\n",
      "Epoch [1/1], Step [7454/7635], Loss: 4.0334\n",
      "Epoch [1/1], Step [7455/7635], Loss: 4.0730\n",
      "Epoch [1/1], Step [7456/7635], Loss: 4.0769\n",
      "Epoch [1/1], Step [7457/7635], Loss: 4.0050\n",
      "Epoch [1/1], Step [7458/7635], Loss: 4.0352\n",
      "Epoch [1/1], Step [7459/7635], Loss: 4.0074\n",
      "Epoch [1/1], Step [7460/7635], Loss: 4.0339\n",
      "Epoch [1/1], Step [7461/7635], Loss: 4.0501\n",
      "Epoch [1/1], Step [7462/7635], Loss: 4.0707\n",
      "Epoch [1/1], Step [7463/7635], Loss: 4.1309\n",
      "Epoch [1/1], Step [7464/7635], Loss: 4.1178\n",
      "Epoch [1/1], Step [7465/7635], Loss: 4.0628\n",
      "Epoch [1/1], Step [7466/7635], Loss: 4.1307\n",
      "Epoch [1/1], Step [7467/7635], Loss: 4.1403\n",
      "Epoch [1/1], Step [7468/7635], Loss: 4.0755\n",
      "Epoch [1/1], Step [7469/7635], Loss: 4.1813\n",
      "Epoch [1/1], Step [7470/7635], Loss: 4.0830\n",
      "Epoch [1/1], Step [7471/7635], Loss: 4.0482\n",
      "Epoch [1/1], Step [7472/7635], Loss: 4.1660\n",
      "Epoch [1/1], Step [7473/7635], Loss: 4.1066\n",
      "Epoch [1/1], Step [7474/7635], Loss: 4.1001\n",
      "Epoch [1/1], Step [7475/7635], Loss: 4.0905\n",
      "Epoch [1/1], Step [7476/7635], Loss: 4.0456\n",
      "Epoch [1/1], Step [7477/7635], Loss: 4.1236\n",
      "Epoch [1/1], Step [7478/7635], Loss: 4.1344\n",
      "Epoch [1/1], Step [7479/7635], Loss: 4.1420\n",
      "Epoch [1/1], Step [7480/7635], Loss: 4.0948\n",
      "Epoch [1/1], Step [7481/7635], Loss: 4.0167\n",
      "Epoch [1/1], Step [7482/7635], Loss: 3.9966\n",
      "Epoch [1/1], Step [7483/7635], Loss: 4.1129\n",
      "Epoch [1/1], Step [7484/7635], Loss: 4.1503\n",
      "Epoch [1/1], Step [7485/7635], Loss: 4.0767\n",
      "Epoch [1/1], Step [7486/7635], Loss: 4.0513\n",
      "Epoch [1/1], Step [7487/7635], Loss: 4.1311\n",
      "Epoch [1/1], Step [7488/7635], Loss: 4.0095\n",
      "Epoch [1/1], Step [7489/7635], Loss: 4.1535\n",
      "Epoch [1/1], Step [7490/7635], Loss: 4.0341\n",
      "Epoch [1/1], Step [7491/7635], Loss: 4.0825\n",
      "Epoch [1/1], Step [7492/7635], Loss: 4.1035\n",
      "Epoch [1/1], Step [7493/7635], Loss: 4.0772\n",
      "Epoch [1/1], Step [7494/7635], Loss: 4.0919\n",
      "Epoch [1/1], Step [7495/7635], Loss: 4.1170\n",
      "Epoch [1/1], Step [7496/7635], Loss: 4.0892\n",
      "Epoch [1/1], Step [7497/7635], Loss: 4.0617\n",
      "Epoch [1/1], Step [7498/7635], Loss: 4.1111\n",
      "Epoch [1/1], Step [7499/7635], Loss: 4.0283\n",
      "Epoch [1/1], Step [7500/7635], Loss: 4.0539\n",
      "Epoch [1/1], Step [7501/7635], Loss: 4.0238\n",
      "Epoch [1/1], Step [7502/7635], Loss: 4.0808\n",
      "Epoch [1/1], Step [7503/7635], Loss: 4.0414\n",
      "Epoch [1/1], Step [7504/7635], Loss: 4.1032\n",
      "Epoch [1/1], Step [7505/7635], Loss: 4.1162\n",
      "Epoch [1/1], Step [7506/7635], Loss: 4.1266\n",
      "Epoch [1/1], Step [7507/7635], Loss: 4.0396\n",
      "Epoch [1/1], Step [7508/7635], Loss: 4.0769\n",
      "Epoch [1/1], Step [7509/7635], Loss: 4.0063\n",
      "Epoch [1/1], Step [7510/7635], Loss: 4.1090\n",
      "Epoch [1/1], Step [7511/7635], Loss: 4.0737\n",
      "Epoch [1/1], Step [7512/7635], Loss: 4.1094\n",
      "Epoch [1/1], Step [7513/7635], Loss: 4.0640\n",
      "Epoch [1/1], Step [7514/7635], Loss: 4.1058\n",
      "Epoch [1/1], Step [7515/7635], Loss: 4.0863\n",
      "Epoch [1/1], Step [7516/7635], Loss: 4.0453\n",
      "Epoch [1/1], Step [7517/7635], Loss: 4.0994\n",
      "Epoch [1/1], Step [7518/7635], Loss: 4.0766\n",
      "Epoch [1/1], Step [7519/7635], Loss: 4.0734\n",
      "Epoch [1/1], Step [7520/7635], Loss: 4.1520\n",
      "Epoch [1/1], Step [7521/7635], Loss: 4.1205\n",
      "Epoch [1/1], Step [7522/7635], Loss: 4.0773\n",
      "Epoch [1/1], Step [7523/7635], Loss: 4.0811\n",
      "Epoch [1/1], Step [7524/7635], Loss: 4.0446\n",
      "Epoch [1/1], Step [7525/7635], Loss: 4.0529\n",
      "Epoch [1/1], Step [7526/7635], Loss: 4.1088\n",
      "Epoch [1/1], Step [7527/7635], Loss: 4.0549\n",
      "Epoch [1/1], Step [7528/7635], Loss: 4.0703\n",
      "Epoch [1/1], Step [7529/7635], Loss: 4.0936\n",
      "Epoch [1/1], Step [7530/7635], Loss: 4.0772\n",
      "Epoch [1/1], Step [7531/7635], Loss: 4.0701\n",
      "Epoch [1/1], Step [7532/7635], Loss: 4.0978\n",
      "Epoch [1/1], Step [7533/7635], Loss: 4.1181\n",
      "Epoch [1/1], Step [7534/7635], Loss: 3.9991\n",
      "Epoch [1/1], Step [7535/7635], Loss: 4.0185\n",
      "Epoch [1/1], Step [7536/7635], Loss: 4.0614\n",
      "Epoch [1/1], Step [7537/7635], Loss: 4.0498\n",
      "Epoch [1/1], Step [7538/7635], Loss: 4.0940\n",
      "Epoch [1/1], Step [7539/7635], Loss: 4.1256\n",
      "Epoch [1/1], Step [7540/7635], Loss: 4.0154\n",
      "Epoch [1/1], Step [7541/7635], Loss: 4.0221\n",
      "Epoch [1/1], Step [7542/7635], Loss: 4.1411\n",
      "Epoch [1/1], Step [7543/7635], Loss: 4.0790\n",
      "Epoch [1/1], Step [7544/7635], Loss: 4.0233\n",
      "Epoch [1/1], Step [7545/7635], Loss: 4.1105\n",
      "Epoch [1/1], Step [7546/7635], Loss: 4.1577\n",
      "Epoch [1/1], Step [7547/7635], Loss: 4.1062\n",
      "Epoch [1/1], Step [7548/7635], Loss: 4.1485\n",
      "Epoch [1/1], Step [7549/7635], Loss: 4.1507\n",
      "Epoch [1/1], Step [7550/7635], Loss: 4.0031\n",
      "Epoch [1/1], Step [7551/7635], Loss: 4.0135\n",
      "Epoch [1/1], Step [7552/7635], Loss: 4.0661\n",
      "Epoch [1/1], Step [7553/7635], Loss: 4.0714\n",
      "Epoch [1/1], Step [7554/7635], Loss: 4.0916\n",
      "Epoch [1/1], Step [7555/7635], Loss: 4.0597\n",
      "Epoch [1/1], Step [7556/7635], Loss: 4.1057\n",
      "Epoch [1/1], Step [7557/7635], Loss: 4.0816\n",
      "Epoch [1/1], Step [7558/7635], Loss: 4.0794\n",
      "Epoch [1/1], Step [7559/7635], Loss: 4.1457\n",
      "Epoch [1/1], Step [7560/7635], Loss: 4.0660\n",
      "Epoch [1/1], Step [7561/7635], Loss: 4.0366\n",
      "Epoch [1/1], Step [7562/7635], Loss: 4.1168\n",
      "Epoch [1/1], Step [7563/7635], Loss: 4.0755\n",
      "Epoch [1/1], Step [7564/7635], Loss: 4.1620\n",
      "Epoch [1/1], Step [7565/7635], Loss: 4.1194\n",
      "Epoch [1/1], Step [7566/7635], Loss: 4.0904\n",
      "Epoch [1/1], Step [7567/7635], Loss: 4.0768\n",
      "Epoch [1/1], Step [7568/7635], Loss: 4.1258\n",
      "Epoch [1/1], Step [7569/7635], Loss: 4.0910\n",
      "Epoch [1/1], Step [7570/7635], Loss: 4.1442\n",
      "Epoch [1/1], Step [7571/7635], Loss: 4.0378\n",
      "Epoch [1/1], Step [7572/7635], Loss: 4.0749\n",
      "Epoch [1/1], Step [7573/7635], Loss: 4.0695\n",
      "Epoch [1/1], Step [7574/7635], Loss: 4.0409\n",
      "Epoch [1/1], Step [7575/7635], Loss: 4.1343\n",
      "Epoch [1/1], Step [7576/7635], Loss: 4.1092\n",
      "Epoch [1/1], Step [7577/7635], Loss: 4.1282\n",
      "Epoch [1/1], Step [7578/7635], Loss: 4.1308\n",
      "Epoch [1/1], Step [7579/7635], Loss: 4.0728\n",
      "Epoch [1/1], Step [7580/7635], Loss: 4.0399\n",
      "Epoch [1/1], Step [7581/7635], Loss: 4.0597\n",
      "Epoch [1/1], Step [7582/7635], Loss: 4.0812\n",
      "Epoch [1/1], Step [7583/7635], Loss: 3.9765\n",
      "Epoch [1/1], Step [7584/7635], Loss: 4.1053\n",
      "Epoch [1/1], Step [7585/7635], Loss: 4.0804\n",
      "Epoch [1/1], Step [7586/7635], Loss: 4.0319\n",
      "Epoch [1/1], Step [7587/7635], Loss: 4.1357\n",
      "Epoch [1/1], Step [7588/7635], Loss: 4.1253\n",
      "Epoch [1/1], Step [7589/7635], Loss: 4.1336\n",
      "Epoch [1/1], Step [7590/7635], Loss: 3.9895\n",
      "Epoch [1/1], Step [7591/7635], Loss: 4.1247\n",
      "Epoch [1/1], Step [7592/7635], Loss: 4.1688\n",
      "Epoch [1/1], Step [7593/7635], Loss: 4.1246\n",
      "Epoch [1/1], Step [7594/7635], Loss: 4.0743\n",
      "Epoch [1/1], Step [7595/7635], Loss: 4.0940\n",
      "Epoch [1/1], Step [7596/7635], Loss: 4.0793\n",
      "Epoch [1/1], Step [7597/7635], Loss: 4.1223\n",
      "Epoch [1/1], Step [7598/7635], Loss: 4.0743\n",
      "Epoch [1/1], Step [7599/7635], Loss: 3.9959\n",
      "Epoch [1/1], Step [7600/7635], Loss: 4.0261\n",
      "Epoch [1/1], Step [7601/7635], Loss: 4.0696\n",
      "Epoch [1/1], Step [7602/7635], Loss: 4.0823\n",
      "Epoch [1/1], Step [7603/7635], Loss: 4.1297\n",
      "Epoch [1/1], Step [7604/7635], Loss: 4.0794\n",
      "Epoch [1/1], Step [7605/7635], Loss: 4.0672\n",
      "Epoch [1/1], Step [7606/7635], Loss: 3.9878\n",
      "Epoch [1/1], Step [7607/7635], Loss: 4.0371\n",
      "Epoch [1/1], Step [7608/7635], Loss: 4.0388\n",
      "Epoch [1/1], Step [7609/7635], Loss: 4.0879\n",
      "Epoch [1/1], Step [7610/7635], Loss: 4.1223\n",
      "Epoch [1/1], Step [7611/7635], Loss: 4.0757\n",
      "Epoch [1/1], Step [7612/7635], Loss: 4.0756\n",
      "Epoch [1/1], Step [7613/7635], Loss: 4.0932\n",
      "Epoch [1/1], Step [7614/7635], Loss: 4.0674\n",
      "Epoch [1/1], Step [7615/7635], Loss: 4.0555\n",
      "Epoch [1/1], Step [7616/7635], Loss: 4.0610\n",
      "Epoch [1/1], Step [7617/7635], Loss: 3.9965\n",
      "Epoch [1/1], Step [7618/7635], Loss: 4.0409\n",
      "Epoch [1/1], Step [7619/7635], Loss: 4.1092\n",
      "Epoch [1/1], Step [7620/7635], Loss: 4.0474\n",
      "Epoch [1/1], Step [7621/7635], Loss: 4.0644\n",
      "Epoch [1/1], Step [7622/7635], Loss: 3.9954\n",
      "Epoch [1/1], Step [7623/7635], Loss: 4.1345\n",
      "Epoch [1/1], Step [7624/7635], Loss: 4.0727\n",
      "Epoch [1/1], Step [7625/7635], Loss: 4.0633\n",
      "Epoch [1/1], Step [7626/7635], Loss: 4.1040\n",
      "Epoch [1/1], Step [7627/7635], Loss: 4.0103\n",
      "Epoch [1/1], Step [7628/7635], Loss: 4.1010\n",
      "Epoch [1/1], Step [7629/7635], Loss: 4.0375\n",
      "Epoch [1/1], Step [7630/7635], Loss: 4.0809\n",
      "Epoch [1/1], Step [7631/7635], Loss: 4.0780\n",
      "Epoch [1/1], Step [7632/7635], Loss: 4.0486\n",
      "Epoch [1/1], Step [7633/7635], Loss: 4.0468\n",
      "Epoch [1/1], Step [7634/7635], Loss: 4.0706\n",
      "Epoch [1/1] Average Loss: 4.4651, Perplexity: 86.93\n"
     ]
    }
   ],
   "source": [
    "# train(model,num_epochs,optimizer,criterion,data_loader,path_to_save_folder,train_run_label)\n",
    "#return (all_losses,train_losses,perplexities)\n",
    "from src.train import train\n",
    "num_epochs = 1\n",
    "\n",
    "model = RegularizedLanguageModel(vocab_size, embedding_dim, context_length, dropout=0.2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "data_loader = train_dataloader\n",
    "train_run_label = \"1_droupot_model\"\n",
    "print_every = 1\n",
    "\n",
    "\n",
    "\n",
    "(all_losses,train_losses,perplexities) = train(model=model,num_epochs=num_epochs,optimizer=optimizer,criterion=criterion,data_loader=data_loader,path_to_save_folder=path_to_save_folder,\n",
    "                                               train_run_label=train_run_label,vocab_size=vocab_size,device=device,print_every=print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f996fc0-ce31-42c5-9012-8ccd2cf6ec1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+UAAAHUCAYAAABceomrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAACG90lEQVR4nOzdd3RURRvH8d+mF5IQahJ6j/QmVem9KGJ5pUizoIKAWBAbRSWAoqgICiqoiKCACKJUqdKr9CJVek1CS533D8zKkkLKJrsJ3885e2Rn59595mbde5+duTMWY4wRAAAAAADIci6ODgAAAAAAgLsVSTkAAAAAAA5CUg4AAAAAgIOQlAMAAAAA4CAk5QAAAAAAOAhJOQAAAAAADkJSDgAAAACAg5CUAwAAAADgICTlAAAAAAA4CEk5cJspU6bIYrFo06ZNjg4lRUOHDpXFYknyMW7cOEeHJ0k6cuSILBaLpkyZYi1LOL5Hjhy54/br16/XQw89pKJFi8rT01MFCxZU3bp19dJLL9nUGz9+vM17AEBOkdrvwUaNGqlRo0YOiTGp7/qMKl68uCwWS7Jt+vbbb63nvOXLl9vtfRPOrenRo0cPFS9e3G6xSNLy5cttzu+urq4qWLCgHn30Ue3Zs8eu75UWFotFQ4cOzbT9J3WtMG3aNI0dOzbT3tPZpXTdl9rrqsyU8FmdOXOmQ+PIrtwcHQCAjFmwYIECAgJsykqUKOGgaOxn/vz5euCBB9SoUSONHj1awcHBOnXqlDZt2qTp06drzJgx1rrjx49Xvnz51KNHD8cFDAB2ltbvwZzGz89PK1eu1N9//61SpUrZvPb111/L399fERERDooua40YMUKNGzdWdHS0Nm3apOHDh2vp0qXasWOHChUq5Ojw7K5t27Zau3atgoODrWXTpk3Tzp07NWDAAMcF5gSSuu6TZHOskP2QlAPZXI0aNZQvXz677/fatWvy8fGx+35Ta/To0SpRooQWLlwoN7f/vqoef/xxjR492mFxAUBWScv3YPny5bM6vEx33333aceOHfr666/13nvvWcv//vtvrVy5Uk899ZQmTZrkwAizTpkyZVSnTh1JUoMGDZQ7d249+eSTmjJlit54440M7dvR5/uk5M+fX/nz53d0GFkuNX+LzLrug2MxfB1Ip9WrV6tp06by8/OTj4+P6tWrp/nz59vUuXbtml5++WWVKFFCXl5eypMnj2rWrKkffvjBWufQoUN6/PHHFRISYh2a2LRpU23bts0ucX799deqUqWK9f0feuihREPeevTooVy5cmnHjh1q0aKF/Pz81LRp02T3efDgQfXs2VNlypSRj4+PChUqpPbt22vHjh12iVmSLly4oHz58tlciCZwcfnvq6t48eLatWuXVqxYYR3CdevwwYiICOvfwMPDQ4UKFdKAAQN09epVm31aLBb17dtXX3zxhcqWLStPT0+VL19e06dPt6mXmr8pANhDar8HpcTD1xOGlL///vsaNWqUihcvLm9vbzVq1Ej79+9XTEyMXnvtNYWEhCggIEAPPfSQzp49a7PP4sWLq127dvr5559VuXJleXl5qWTJkvrkk09SFf+BAwfUuXNnFShQQJ6enrrnnnv02Wefpbr9Li4u6tatm7755hvFx8dby7/++msVKVJEzZo1S3K7uXPnqm7duvLx8ZGfn5+aN2+utWvXJqo3f/58Va1aVZ6enipRooQ++OCDJPdnjNH48eNVtWpVeXt7KzAwUI888ogOHTqU6rbYW0KCfvToUWvZjBkzVLduXfn6+ipXrlxq2bKltm7darNdSuf7Ro0aqWLFilq1apXq1Kkjb29vFSpUSG+99Zbi4uLuGNPp06fVu3dvFS5cWB4eHipRooSGDRum2NhYSTePY5s2bZQ3b14dO3bMut21a9dUoUIF3XPPPdZz8+3D1xs1aqT58+fr6NGjNkO2jTEqU6aMWrZsmSieK1euKCAgQH369Ekx7hs3bmjw4ME21wl9+vTR5cuXrXU6dOigYsWK2XwOE9SuXVvVq1e3Pk/t5yXheK9cuVL16tWTj4+PevXqlfJBToWE//dHjx6t9957T0WLFpWXl5dq1qyppUuXJqqfmutZSTpx4oSeeeYZFSlSRB4eHgoJCdEjjzyiM2fO2NSLiYnRG2+8oZCQEPn7+6tZs2bat2+fTZ2tW7eqXbt21u+GkJAQtW3bVv/880+G259dkZQD6bBixQo1adJE4eHh+uqrr/TDDz/Iz89P7du314wZM6z1Bg4cqAkTJqhfv35asGCBvvvuOz366KO6cOGCtU6bNm20efNmjR49WosXL9aECRNUrVo1m5NBSuLi4hQbG2t93HriDAsL05NPPqkKFSpo9uzZ+vjjj/XXX3+pbt26OnDggM1+oqOj9cADD6hJkyb65ZdfNGzYsGTf8+TJk8qbN69GjhypBQsW6LPPPpObm5tq166d6Is3verWrav169erX79+Wr9+vWJiYpKs9/PPP6tkyZKqVq2a1q5dq7Vr1+rnn3+WdPNE37BhQ33zzTfq16+ffv/9dw0aNEhTpkzRAw88IGOMzb7mzp2rTz75RMOHD9fMmTNVrFgxderUyeb+qNT8TQHAHlL7PZiSzz77TH/++ac+++wzffnll9q7d6/at2+vJ598UufOndPXX3+t0aNHa8mSJXrqqacSbb9t2zYNGDBAL774on7++WfVq1dP/fv3TzaBTbB7927de++92rlzp8aMGaNff/1Vbdu2Vb9+/VI8v9yuV69eOnnypBYuXCjp5jnvm2++UY8ePRL9MCHdHOL84IMPyt/fXz/88IO++uorXbp0SY0aNdLq1aut9ZYuXaoHH3xQfn5+mj59ut5//339+OOPmjx5cqJ99u7dWwMGDFCzZs00Z84cjR8/Xrt27VK9evUSJSRZ5eDBg5Jk7U0eMWKEOnXqpPLly+vHH3/Ud999p8jISN1///3avXu3zbYpne9Pnz6txx9/XF26dNEvv/yiRx55RO+++6769++fYjynT59WrVq1tHDhQr399tv6/fff9eSTTyosLExPP/20pJs/fn/33Xfy8fHRY489Zv08P//88zp8+LB+/PFH+fr6Jrn/8ePHq379+goKCrKe69euXSuLxaIXXnhBixcvTnRd8+233yoiIiLFpNwYow4dOuiDDz7QE088ofnz52vgwIH65ptv1KRJE0VFRUm6+Tk8duyY/vjjD5vt9+7dqw0bNqhnz57WsrR8Xk6dOqWuXbuqc+fO+u233/T888+neJylxNd9t1/7JRg3bpwWLFigsWPHaurUqXJxcVHr1q1tfqBK7fXsiRMndO+99+rnn3/WwIED9fvvv2vs2LEKCAjQpUuXbN739ddf19GjR/Xll19q4sSJOnDggNq3b2+N8erVq2revLnOnDmjzz77TIsXL9bYsWNVtGhRRUZG3rH9OZYBYGPy5MlGktm4cWOyderUqWMKFChgIiMjrWWxsbGmYsWKpnDhwiY+Pt4YY0zFihVNhw4dkt3P+fPnjSQzduzYNMc5ZMgQIynRo1ChQsYYYy5dumS8vb1NmzZtbLY7duyY8fT0NJ07d7aWde/e3UgyX3/9dZrjMOZm26Ojo02ZMmXMiy++aC0/fPiwkWQmT55sLUs4vocPH05xn+fPnzf33XeftV3u7u6mXr16JiwszOa4G2NMhQoVTMOGDRPtIywszLi4uCT6W86cOdNIMr/99pu1TJLx9vY2p0+ftmlXaGioKV26tLXsTn9TALCXtHwPNmzY0OZ7MOH7t0qVKiYuLs5aPnbsWCPJPPDAAzbbDxgwwEgy4eHh1rJixYoZi8Vitm3bZlO3efPmxt/f31y9etXmvW79rm/ZsqUpXLiwzf6MMaZv377Gy8vLXLx4McW2FytWzLRt29batkceecQYY8z8+fONxWIxhw8fNj/99JORZJYtW2aMMSYuLs6EhISYSpUq2bQ5MjLSFChQwNSrV89aVrt2bRMSEmKuX79uLYuIiDB58uQxt14er1271kgyY8aMsYnv+PHjxtvb27z66qvWsu7du5tixYql2K60WrZsmZFkZsyYYWJiYsy1a9fMypUrTenSpY2rq6vZvn27OXbsmHFzczMvvPCCzbaRkZEmKCjIPPbYYzYxJne+b9iwoZFkfvnlF5vyp59+2ri4uJijR49ayySZIUOGWJ/37t3b5MqVy6aOMcZ88MEHRpLZtWuXtWz16tXGzc3NDBgwwHz99ddGkvnyyy9ttkvqWqFt27ZJHt+IiAjj5+dn+vfvb1Nevnx507hx40T1b7VgwQIjyYwePdqmfMaMGUaSmThxojHGmJiYGFOwYEGbaydjjHn11VeNh4eHOX/+vDEmbZ+XhOO9dOnSFGNMkNx1nyRTqlQpa72E/x+T+3w3a9bMWpba69levXoZd3d3s3v37mTjS/is3n7d+eOPPxpJZu3atcYYYzZt2mQkmTlz5qSq3XcLesqBNLp69arWr1+vRx55RLly5bKWu7q66oknntA///xj7S2uVauWfv/9d7322mtavny5rl+/brOvPHnyqFSpUnr//ff14YcfauvWrUkOjUrJkiVLtHHjRuvjt99+kyStXbtW169fTzT5WZEiRdSkSZMkhzA9/PDDqXrP2NhYjRgxQuXLl5eHh4fc3Nzk4eGhAwcO2G022Lx582rVqlXauHGjRo4cqQcffFD79+/X4MGDValSJZ0/f/6O+/j1119VsWJFVa1a1eYX5ZYtWyY5Y2/Tpk1VsGBB63NXV1f973//08GDB61Dqu70NwUAe7HH92CbNm1sepTvueceSTcn0rpVQvmtw4olqUKFCqpSpYpNWefOnRUREaEtW7Yk+Z43btzQ0qVL9dBDD8nHx8fm+7dNmza6ceOG1q1bd+cD8K9evXpp7ty5unDhgr766is1btw4yVnO9+3bp5MnT+qJJ56waXOuXLn08MMPa926dbp27ZquXr2qjRs3qmPHjvLy8rLWS+ghvNWvv/4qi8Wirl272rQjKChIVapUSfPM78aYRL2cqfG///1P7u7u8vHxUYMGDRQXF6eZM2eqcuXKWrhwoWJjY9WtWzeb/Xp5ealhw4ZJxpjc+d7Pz08PPPCATVnnzp0VHx+vlStXJhvfr7/+qsaNGyskJMQmhtatW0u62SOboH79+nrvvfc0duxYPffcc+ratauefPLJVB2H5GLu2bOnpkyZYh3+/scff2j37t3q27dvitsm9Hzffq306KOPytfX13qt5Obmpq5du2r27NkKDw+XdLPH+rvvvtODDz6ovHnzWo9DWj4vgYGBatKkSZrae/t138aNGzVnzpxE9ZL7fK9cuVJxcXFpup79/fff1bhxY+v3REpu//xUrlxZ0n+3WpQuXVqBgYEaNGiQPv/880QjOe5WJOVAGl26dEnGmCRnuQwJCZEk61DmTz75RIMGDdKcOXPUuHFj5cmTRx06dLAOsbJYLFq6dKlatmyp0aNHq3r16sqfP7/69euX6iE8VapUUc2aNa2PhC+/hBiSi/P24dY+Pj7y9/dP1XsOHDhQb731ljp06KB58+Zp/fr12rhxo6pUqWL3JLVmzZoaNGiQfvrpJ508eVIvvviijhw5kqrJ3s6cOaO//vpL7u7uNg8/Pz8ZYxJd0AYFBSXaR0JZav+mAGBvGfkezJMnj81zDw+PFMtv3LhhU56a78XbXbhwQbGxsfr0008Tff+2adNGklL1g0KCRx55RF5eXvroo480b968ZBO4O5334uPjdenSJV26dEnx8fEpti3BmTNnZIxRwYIFE7Vl3bp1aWqHJH3zzTeJ9pMao0aN0saNG7VlyxYdO3ZMhw4dUocOHawxStK9996baN8zZsxIFGNK5/tbf5hOcKe/d0IM8+bNS/T+FSpUkJT4792lSxd5eHgoKipKr7zySqqOQUpeeOEFRUZG6vvvv5d0c+h24cKF9eCDD6a43YULF+Tm5pZoUjmLxaKgoCCbNvfq1Us3btywzjWzcOFCnTp1ymboelo/L+mZMf32676aNWuqYsWKieol9/mOjo7WlStX0nQ9e+7cORUuXDhV8SX8QJHA09NTkqzXhwEBAVqxYoWqVq2q119/XRUqVFBISIiGDBmSrlt0cgpmXwfSKDAwUC4uLjp16lSi106ePClJ1lkxfX19NWzYMA0bNkxnzpyx9rC2b99ee/fulSQVK1ZMX331lSRp//79+vHHHzV06FBFR0fr888/T3ecCV+KycV5+8ydaVmXderUqerWrZtGjBhhU37+/Hnlzp077cGmkru7u4YMGaKPPvpIO3fuvGP9fPnyydvbW19//XWyr9/q9OnTieoklCUcz9T8TQEgs6T1ezCjUvO9eLvAwEBrb1ty9/OmZelOHx8fPf744woLC5O/v786duyYZL07nfdcXFwUGBgoY4wsFkuKbUuQL18+WSwWrVq1yppc3CqpspS0b99eGzduTNM2klSyZEnVrFkzydcSzmUJc6HcSUrn+6Tukb/T3zshhsqVK9vMkn+rhCRPutnD3KVLFwUGBsrT01NPPvmk/vzzT+sPQ+lRunRptW7dWp999plat26tuXPnatiwYXJ1dU1xu7x58yo2Nlbnzp2zScyNMTp9+rTuvfdea1n58uVVq1YtTZ48Wb1799bkyZMVEhKiFi1aWOuk9fOSlmuvtEru8+3h4aFcuXLJzc0t1dez+fPnt+skbJUqVdL06dNljNFff/2lKVOmaPjw4fL29tZrr71mt/fJTugpB9LI19dXtWvX1uzZs216hePj4zV16lQVLlxYZcuWTbRdwYIF1aNHD3Xq1En79u3TtWvXEtUpW7as3nzzTVWqVCnZYYGpVbduXXl7e2vq1Kk25f/884/++OOPFGdXvxOLxZLoxDJ//nydOHEi3fu8XVInCUnW4fG3nuA9PT2T7KFv166d/v77b+XNmzfRr8o1a9ZMNPxx6dKlNhckcXFxmjFjhkqVKpXkL8Sp+ZsCQHql5Xsws+zatUvbt2+3KZs2bZr8/PxsZpy+lY+Pjxo3bqytW7eqcuXKSX7/ppTgJeW5555T+/bt9fbbb9sMyb1VuXLlVKhQIU2bNs1mIs+rV69q1qxZ1hnZfX19VatWLc2ePdtmZEBkZKTmzZtns8927drJGKMTJ04k2Y5KlSqlqR1JnY8yqmXLlnJzc9Pff/+dZIxpeY/IyEjNnTvXpmzatGlycXFRgwYNkt2uXbt22rlzp0qVKpXk+9/6WR0yZIhWrVql77//XjNmzND27dtT1Vue3Lk+Qf/+/fXXX3+pe/fucnV1tU4wl5KEa6Hbr5VmzZqlq1evJrpW6tmzp9avX6/Vq1dr3rx51ve69TjY8/OSEcl9vu+//365urqm6Xq2devWWrZsmd0m801gsVhUpUoVffTRR8qdO3eGr32zM3rKgWT88ccf1qU4btWmTRuFhYWpefPmaty4sV5++WV5eHho/Pjx2rlzp3744QfrL5+1a9dWu3btVLlyZQUGBmrPnj367rvvrBcGf/31l/r27atHH31UZcqUkYeHh/744w/99ddfGf6lMHfu3Hrrrbf0+uuvq1u3burUqZMuXLigYcOGycvLS0OGDEn3vtu1a6cpU6YoNDRUlStX1ubNm/X++++nemhTarRs2VKFCxdW+/btFRoaqvj4eG3btk1jxoxRrly5bGaCTfjFdcaMGSpZsqS8vLxUqVIlDRgwQLNmzVKDBg304osvqnLlyoqPj9exY8e0aNEivfTSS6pdu7Z1P/ny5VOTJk301ltvydfXV+PHj9fevXttlkW7098UAOwlLd+DmSUkJEQPPPCAhg4dquDgYE2dOlWLFy/WqFGjUvzO+/jjj3Xffffp/vvv13PPPafixYsrMjJSBw8e1Lx58xLNYn0nVatWTfK+2Vu5uLho9OjR6tKli9q1a6fevXsrKipK77//vi5fvqyRI0da677zzjtq1aqVmjdvrpdeeklxcXEaNWqUfH19dfHiRWu9+vXr65lnnlHPnj21adMmNWjQQL6+vjp16pRWr16tSpUq6bnnnktTW+ytePHiGj58uN544w0dOnRIrVq1UmBgoM6cOaMNGzZYR3ilRt68efXcc8/p2LFjKlu2rH777TdNmjRJzz33nIoWLZrsdsOHD9fixYtVr1499evXT+XKldONGzd05MgR/fbbb/r8889VuHBhLV68WGFhYXrrrbesCW9YWJhefvllNWrUSA899FCy71GpUiXNnj1bEyZMUI0aNeTi4mLzg0Pz5s1Vvnx5LVu2TF27dlWBAgXu2N7mzZurZcuWGjRokCIiIlS/fn399ddfGjJkiKpVq6YnnnjCpn6nTp00cOBAderUSVFRUYnuRc+Kz8vmzZsVEBCQqLx8+fI2tyW4urqqefPmGjhwoOLj4zVq1ChFRETYfBZSez07fPhw/f7772rQoIFef/11VapUSZcvX9aCBQs0cOBAhYaGpjr+X3/9VePHj1eHDh1UsmRJGWM0e/ZsXb58Wc2bN8/AkcnmHDK9HODEEmb8TO6RMBPoqlWrTJMmTYyvr6/x9vY2derUMfPmzbPZ12uvvWZq1qxpAgMDjaenpylZsqR58cUXrbN0njlzxvTo0cOEhoYaX19fkytXLlO5cmXz0UcfmdjY2BTjTJiF89y5cynW+/LLL03lypWNh4eHCQgIMA8++KDNLKjG3JyN1dfXN9XH6NKlS+bJJ580BQoUMD4+Pua+++4zq1atSnb23/TMvj5jxgzTuXNnU6ZMGZMrVy7j7u5uihYtap544olEs38eOXLEtGjRwvj5+RlJNrOzXrlyxbz55pumXLly1mNQqVIl8+KLL9rMtC7J9OnTx4wfP96UKlXKuLu7m9DQUPP999/bvNed/qYAYC9p+R5M7vv3/ffft6mXMEPyTz/9ZFOe1MojCTOgz5w501SoUMF4eHiY4sWLmw8//NBm26S+6xPKe/XqZQoVKmTc3d1N/vz5Tb169cy77757x7bfOvt6cm6ffT3BnDlzTO3atY2Xl5fx9fU1TZs2NX/++Wei7efOnWs9PxYtWtSMHDnSem693ddff21q165tPeeXKlXKdOvWzWzatMlaJzNnX7/975WUOXPmmMaNGxt/f3/j6elpihUrZh555BGzZMkSmxiTO983bNjQVKhQwSxfvtzUrFnTeHp6muDgYPP666+bmJgYm7q6bfZ1Y4w5d+6c6devnylRooRxd3c3efLkMTVq1DBvvPGGuXLlijl58qQpUKCAadKkic3s+PHx8aZ9+/Ymd+7c1muDpK4VLl68aB555BGTO3duY7FYkvw7DR061Egy69atu+PxSnD9+nUzaNAgU6xYMePu7m6Cg4PNc889Zy5dupRk/c6dOxtJpn79+snuMzWfl4TjnVopzb4uySxevNgY89//j6NGjTLDhg0zhQsXNh4eHqZatWpm4cKFifabmutZY27OIN+rVy8TFBRk3N3dTUhIiHnsscfMmTNnjDHJf1Zv/37Yu3ev6dSpkylVqpTx9vY2AQEBplatWmbKlCmpPhY5kcWY2xbqBYC7kMViUZ8+fTRu3DhHhwIATqF48eKqWLGifv31V0eHgizQqFEjnT9/PkvmKsgsNWvWlMViSdd9+znFkSNHVKJECb3//vt6+eWXHR0OUonh6wAAAACypYiICO3cuVO//vqrNm/erJ9//tnRIQFpRlIOAAAAIFvasmWLGjdurLx582rIkCHWpeKA7ITh6wAAAAAAOAhLogEAAAAA4CAk5QAAAAAAOAhJOQAAAAAADpLjJ3qLj4/XyZMn5efnJ4vF4uhwAACQMUaRkZEKCQmRiwu/j2cU53oAgLNJy7k+xyflJ0+eVJEiRRwdBgAAiRw/flyFCxd2dBiZJiwsTK+//rr69++vsWPHJlln+fLlaty4caLyPXv2KDQ0NFXvw7keAOCsUnOuz/FJuZ+fn6SbB8Pf39/B0QAAcHNd3SJFiljPUTnRxo0bNXHiRFWuXDlV9fft22dzns6fP3+q34tzPQDA2aTlXJ/jk/KEYWz+/v6cqAEATiWnDrW+cuWKunTpokmTJundd99N1TYFChRQ7ty50/V+nOsBAM4qNed6bmQDAAB21adPH7Vt21bNmjVL9TbVqlVTcHCwmjZtqmXLlqVYNyoqShERETYPAACyqxzfUw4AALLO9OnTtWXLFm3cuDFV9YODgzVx4kTVqFFDUVFR+u6779S0aVMtX75cDRo0SHKbsLAwDRs2zJ5hAwDgMCTlAADALo4fP67+/ftr0aJF8vLyStU25cqVU7ly5azP69atq+PHj+uDDz5INikfPHiwBg4caH2ecN8eAADZEUk5gGwrLi5OMTExjg4DSMTV1VVubm459p7x5GzevFlnz55VjRo1rGVxcXFauXKlxo0bp6ioKLm6ut5xP3Xq1NHUqVOTfd3T01Oenp52iRkAkDpcd9my57mepBxAtnTlyhX9888/MsY4OhQgST4+PgoODpaHh4ejQ8kyTZs21Y4dO2zKevbsqdDQUA0aNChVCbkkbd26VcHBwZkRIgAgHbjuSpq9zvUk5QCynbi4OP3zzz/y8fFR/vz577reSDg3Y4yio6N17tw5HT58WGXKlJGLy90xr6qfn58qVqxoU+br66u8efNaywcPHqwTJ07o22+/lSSNHTtWxYsXV4UKFRQdHa2pU6dq1qxZmjVrVpbHDwBIjOuuxOx9ricpB5DtxMTEyBij/Pnzy9vb29HhAIl4e3vL3d1dR48eVXR0dKrvr74bnDp1SseOHbM+j46O1ssvv6wTJ07I29tbFSpU0Pz589WmTRsHRgkASMB1V9Lsea4nKQeQbfFLLZzZ3dI7fifLly+3eT5lyhSb56+++qpeffXVrAsIAJAuXHclZq9zPVcMAAAAAAA4CEk5AAAAAAAOQlIOAHCYKVOmKHfu3NbnQ4cOVdWqVR0WDwAAyBxx8UZr/76gX7ad0Nq/LyguPnNncm/UqJEGDBiQqe9hLyTlAJBFevTooQ4dOjg6DEk37wu7/XHfffc5OqxUmTVrlmrXrq2AgAD5+fmpQoUKeumll6yvk9gDAOBcFuw8pftG/aFOk9ap//Rt6jRpne4b9YcW7Dzl6NAkSdu3b1enTp1UpEgReXt765577tHHH3+cZe/PRG8AcJeaPHmyWrVqZX2ekTU2Y2Ji5O7ubo+wUrRkyRI9/vjjGjFihB544AFZLBbt3r1bS5cuzfT3BgAAabdg5yk9N3WLbu8XPx1+Q89N3aIJXaurVcVgh8SWYPPmzcqfP7+mTp2qIkWKaM2aNXrmmWfk6uqqvn37Zvr701OeBn2nbVGLj1Zo89FLjg4FwC2MMboWHeuQhzH2G3q1YsUK1apVS56engoODtZrr72m2NhY6+szZ85UpUqV5O3trbx586pZs2a6evWqpJszXNeqVUu+vr7KnTu36tevr6NHj6b4frlz51ZQUJD1kSdPHklSfHy8hg8frsKFC8vT01NVq1bVggULrNsdOXJEFotFP/74oxo1aiQvLy9NnTo1yff48MMPValSJfn6+qpIkSJ6/vnndeXKlXQfo19//VX33XefXnnlFZUrV05ly5ZVhw4d9Omnn0q6ORx+2LBh2r59u3UEQMJs3+Hh4XrmmWdUoEAB+fv7q0mTJtq+fbt13wk97F988YWKFCkiHx8fPfroo7p8+bK1TnqOMwAAOVFK10c3YuIk3RyyPmze7kQJuSRr2dB5u22Gsie3z4yYOnWqatasKT8/PwUFBalz5846e/as9fVevXrpk08+UcOGDVWyZEl17dpVPXv21OzZszP0vqlFT3kaHLt4TfvPXFHE9RhHhwLgFtdj4lT+7YUOee/dw1vKxyPjX6UnTpxQmzZt1KNHD3377bfau3evnn76aXl5eWno0KE6deqUOnXqpNGjR+uhhx5SZGSkVq1aJWOMYmNj1aFDBz399NP64YcfFB0drQ0bNqR76ZKPP/5YY8aM0RdffKFq1arp66+/1gMPPKBdu3apTJky1nqDBg3SmDFjNHnyZHl6eia5LxcXF33yyScqXry4Dh8+rOeff16vvvqqxo8fn67YgoKCNG3aNO3cuVMVK1ZM9Pr//vc/7dy5UwsWLNCSJUskSQEBATLGqG3btsqTJ49+++03BQQE6IsvvlDTpk21f/9+6w8SBw8e1I8//qh58+YpIiJCTz75pPr06aPvv//e7scZAIDsLKVrr8bl8mtyz1racPiiToXfSLae0c0e8w2HL6puqbySpPtGLdPFq9GJ6h4Z2TbdsUZHR+udd95RuXLldPbsWb344ovq0aOHfvvtt2S3CQ8Pt14fZDaS8jTwdLs5sCAqNs7BkQDIacaPH68iRYpo3LhxslgsCg0N1cmTJzVo0CC9/fbbOnXqlGJjY9WxY0cVK1ZMklSpUiVJ0sWLFxUeHq527dqpVKlSkqR77rnnju/ZqVMnubq6Wp9PnTpVHTp00AcffKBBgwbp8ccflySNGjVKy5Yt09ixY/XZZ59Z6w8YMEAdO3ZM8T1unWClRIkSeuedd/Tcc8+lOyl/4YUXtGrVKlWqVEnFihVTnTp11KJFC3Xp0kWenp7y9vZWrly55ObmpqCgIOt2f/zxh3bs2KGzZ89af0D44IMPNGfOHM2cOVPPPPOMJOnGjRv65ptvVLhwYUnSp59+qrZt22rMmDHy8PBI13EGAOBudTYy+YQ8PfXSq1evXtZ/lyxZUp988olq1aqlK1euKFeuXInqr127Vj/++KPmz5+fqXElIClPA0+3mxevUbHxDo4EwK283V21e3hLh723PezZs0d169a16XWtX7++rly5on/++UdVqlRR06ZNValSJbVs2VItWrTQI488osDAQOXJk0c9evRQy5Yt1bx5czVr1kyPPfaYgoNTvj/ro48+UrNmzazPg4ODFRERoZMnT6p+/fo2devXr28z1FuSatasecd2LVu2TCNGjNDu3bsVERGh2NhY3bhxQ1evXpWvr29qDo0NX19fzZ8/X3///beWLVumdevW6aWXXtLHH3+stWvXysfHJ8ntNm/erCtXrihv3rw25devX9fff/9tfV60aFFrQi5JdevWVXx8vPbt26eGDRum6zgDAJATpXTt5fLv9UwBP69U7evWeqsHNc5YYEnYunWrhg4dqm3btunixYuKj7+Zzx07dkzly5e3qbtr1y49+OCDevvtt9W8eXO7x5IU7ilPA2tPeQxJOeBMLBaLfDzcHPKw19BlY0yifSXcr26xWOTq6qrFixfr999/V/ny5fXpp5+qXLlyOnz4sKSbk7atXbtW9erV04wZM1S2bFmtW7cuxfcMCgpS6dKlrY9bk+SkYrm97E5J9dGjR9WmTRtVrFhRs2bN0ubNm6097TExGbsNqFSpUnrqqaf05ZdfasuWLdq9e7dmzJiRbP34+HgFBwdr27ZtNo99+/bplVdeSXa7hDYn/Dc9xxkAgJwopesjr387LWqVyKPgAC8ld7VkkRQc4KVaJfLccb/pdfXqVbVo0UK5cuXS1KlTtXHjRv3888+Sbg5rv9Xu3bvVpEkTPf3003rzzTfT/Z5pRVKeBp7uDF8HkDnKly+vNWvW2Ewct2bNGvn5+alQoUKSbiaG9evX17Bhw7R161Z5eHhYTyqSVK1aNQ0ePFhr1qxRxYoVNW3atDTH4e/vr5CQEK1evdqmfM2aNWkeqr1p0ybFxsZqzJgxqlOnjsqWLauTJ0+mOaY7KV68uHx8fKyT3nl4eCguzvZ7unr16jp9+rTc3NxsfogoXbq08uXLZ6137NgxmxjXrl0rFxcXlS1b1lpmj+MMAMDdwNXFoiHtb/ZE356YJzwf0r68XF0yb36WvXv36vz58xo5cqTuv/9+hYaG2kzylmDXrl1q3Lixunfvrvfeey/T4kkKw9fTwMM1ISmnpxxA+oSHh2vbtm02ZXny5NHzzz+vsWPH6oUXXlDfvn21b98+DRkyRAMHDpSLi4vWr1+vpUuXqkWLFipQoIDWr1+vc+fO6Z577tHhw4c1ceJEPfDAAwoJCdG+ffu0f/9+devWLV0xvvLKKxoyZIhKlSqlqlWravLkydq2bZu+//77NO2nVKlSio2N1aeffqr27dvrzz//1Oeff56umBIMHTpU165dU5s2bVSsWDFdvnxZn3zyiWJiYqxDzBImldu2bZsKFy4sPz8/NWvWTHXr1lWHDh00atQolStXTidPntRvv/2mDh06WIfie3l5qXv37vrggw8UERGhfv366bHHHlNQUJDdjzMAAHeDVhWDNaFrdQ2bt9tm0regAC8NaV8+05dDK1q0qDw8PPTpp5/q2Wef1c6dO/XOO+/Y1ElIyFu0aKGBAwfq9OnTkiRXV1flz58/U+OTSMrThHvKAWTU8uXLVa1aNZuy7t27a8qUKfrtt9/0yiuvqEqVKsqTJ4+efPJJ69Apf39/rVy5UmPHjlVERISKFSumMWPGqHXr1jpz5oz27t2rb775RhcuXFBwcLD69u2r3r17pyvGfv36KSIiQi+99JLOnj2r8uXLa+7cuTYzr6dG1apV9eGHH2rUqFEaPHiwGjRooLCwsAwlsQ0bNtRnn32mbt266cyZMwoMDFS1atW0aNEilStXTpL08MMPa/bs2WrcuLEuX76syZMnW2dYfeONN9SrVy+dO3dOQUFBatCggQoWLGjdf+nSpdWxY0e1adNGFy9eVJs2bayT0vn4+Nj1OAMAcLdoVTFYzcsHacPhizobeUMF/G4OWc/MHvIE+fPn15QpU/T666/rk08+UfXq1fXBBx/ogQcesNb56aefdO7cOX3//fc2nRDFihXTkSNHMj1Gi7HnIrtOKCIiQgEBAQoPD5e/v3+G9vX2Lzv17dqj6tektAa2KGenCAGk1Y0bN3T48GGVKFFCXl6pm0AEuJOhQ4dqzpw5iUYypFdKn1N7npvA8QSAzMR1V/Lsda536D3lK1euVPv27RUSEiKLxaI5c+bYvD579my1bNlS+fLlk8VisduFUnr9tyQaPeUAAAAAgIxzaFJ+9epVValSRePGjUv29fr162vkyJFZHFnSEoav34hhojcAAAAAQMY59J7y1q1bq3Xr1sm+/sQTT0hSlozjTw2Pf3vKo+Ny9Ih/ALgrDR06VEOHDnV0GAAA4C6T4yZ6i4qKUlRUlPV5RESE3fbt5npzIoLYOIavAwAAAAAyLsetUx4WFqaAgADro0iRInbbt7vLzcMVG09POeAMcvg8lcjm+HwCAHISzmuJ2euY5LikfPDgwQoPD7c+jh8/brd9J/SUx9BTDjiUq+vN+R2io6MdHAmQvGvXrkmS3N3dHRwJAADpx3VX8ux1rs9xw9c9PT3l6emZKft2d735GwZJOeBYbm5u8vHx0blz5+Tu7i4Xlxz3+yKyMWOMrl27prNnzyp37tzWixkAALIjrrsSs/e5Pscl5ZnJ3XpPOUM3AEeyWCwKDg7W4cOHdfToUUeHAyQpd+7cCgoKcnQYAABkCNddybPXud6hSfmVK1d08OBB6/PDhw9r27ZtypMnj4oWLaqLFy/q2LFjOnnypCRp3759kqSgoCCHXOi4/furUAz3lAMO5+HhoTJlyjCUCk7J3d2dHnIAQI7BdVdi9jzXOzQp37Rpkxo3bmx9PnDgQElS9+7dNWXKFM2dO1c9e/a0vv74449LkoYMGeKQZWvc/10SLSaW4euAM3BxcZGXl5ejwwAAAMjxuO7KPA5Nyhs1apTijHU9evRQjx49si6gO3B3+Xf4ejxJOQAAAAAg47hLPw3crBO9MXwdAAAAAJBxJOVpkLAkGj3lAAAAAAB7IClPA4+EnvJYesoBAAAAABlHUp4Gbv/eUx5DTzkAAAAAwA5IytMg4Z5y1ikHAAAAANgDSXkaWIevx9FTDgAAAADIOJLyNHB3+3f4Oj3lAAAAAAA7IClPA3d6ygEAAAAAdkRSngbuLiTlAAAAAAD7ISlPg/+Gr5OUAwAAAAAyjqQ8Df4bvm5kDPeVAwAAAAAyhqQ8DRKSconJ3gAAAAAAGUdSngYetyTl0QxhBwAAAABkEEl5Gni43ZKUx5KUAwAAAAAyhqQ8DVxdLHJzuTnZG0k5AAAAACCjSMrTyPPf3vKo2DgHRwIAAAAAyO5IytMoYQg7PeUAAAAAgIwiKU8jTzdXSVIUSTkAAAAAIINIytPIwzp8naQcAAAAAJAxJOVpxD3lAAAAAAB7ISlPI+4pBwAAAADYC0l5GiUk5TFxxsGRAAAAAACyO5LyNHJ3TUjK6SkHAAAAAGQMSXkaebgyfB0AAAAAYB8k5Wnk7mqRJEXTUw4AAAAAyCCS8jRi+DoAAAAAwF5IytPIOtEbw9cBAAAAABlEUp5GHq7Mvg4AAAAAsA+S8jRKGL7OPeUAAAAAgIwiKU8jd7d/J3pj+DoAAAAAIINIytPIzeXmIYuNJykHAAAAAGQMSXkaWSd6455yAAAAAEAGkZSnkZvLzeHrLIkGAAAAAMgokvI0SpjoLZaecgAAAABABpGUp5G7Kz3lAAAAAAD7IClPI3fWKQcAAAAA2AlJeRq5WZNyesoBAAAAABnj0KR85cqVat++vUJCQmSxWDRnzhyb140xGjp0qEJCQuTt7a1GjRpp165djgn2Xx7/Dl9nSTQAAAAAQEY5NCm/evWqqlSponHjxiX5+ujRo/Xhhx9q3Lhx2rhxo4KCgtS8eXNFRkZmcaT/Segpj45l+DoAAAAAIGPcHPnmrVu3VuvWrZN8zRijsWPH6o033lDHjh0lSd98840KFiyoadOmqXfv3lkZqpV19nV6ygEAAAAAGeS095QfPnxYp0+fVosWLaxlnp6eatiwodasWZPsdlFRUYqIiLB52BOzrwMAAAAA7MVpk/LTp09LkgoWLGhTXrBgQetrSQkLC1NAQID1UaRIEbvGxezrAAAAAAB7cdqkPIHFYrF5boxJVHarwYMHKzw83Po4fvy4XeNxc6GnHAAAAABgHw69pzwlQUFBkm72mAcHB1vLz549m6j3/Faenp7y9PTMtLjc3f69p5yecgAAAABABjltT3mJEiUUFBSkxYsXW8uio6O1YsUK1atXz2FxubuwTjkAAAAAwD4c2lN+5coVHTx40Pr88OHD2rZtm/LkyaOiRYtqwIABGjFihMqUKaMyZcpoxIgR8vHxUefOnR0WsxsTvQEAAAAA7MShPeWbNm1StWrVVK1aNUnSwIEDVa1aNb399tuSpFdffVUDBgzQ888/r5o1a+rEiRNatGiR/Pz8HBYzE70BAJA6YWFhslgsGjBgQKrq//nnn3Jzc1PVqlUzNS4AAJyJQ3vKGzVqJGOST24tFouGDh2qoUOHZl1Qd5CwJNqNmDgHRwIAgPPauHGjJk6cqMqVK6eqfnh4uLp166amTZvqzJkzmRwdAADOw2nvKXd2ZyOjHB0CAABO6cqVK+rSpYsmTZqkwMDAVG3Tu3dvde7cWXXr1s3k6AAAcC4k5Wl09MI167+5rxwAgMT69Omjtm3bqlmzZqmqP3nyZP39998aMmRIqupHRUUpIiLC5gEAQHbltEuiOat7gv+7nz0qNt56jzkAAJCmT5+uLVu2aOPGjamqf+DAAb322mtatWqV3NxSd1kSFhamYcOGZSRMAACcBhllGhXL62v9dxyTvQEAYHX8+HH1799fU6dOlZeX1x3rx8XFqXPnzho2bJjKli2b6vcZPHiwwsPDrY/jx49nJGwAAByKnvI0cnOxWP8dG8/wdQAAEmzevFlnz55VjRo1rGVxcXFauXKlxo0bp6ioKLm6ulpfi4yM1KZNm7R161b17dtXkhQfHy9jjNzc3LRo0SI1adIk0ft4enrK09Mz8xsEAEAWIClPI4vFIlcXi+LijWLj6SkHACBB06ZNtWPHDpuynj17KjQ0VIMGDbJJyCXJ398/Uf3x48frjz/+0MyZM1WiRIlMjxkAAEcjKU8HN5JyAAAS8fPzU8WKFW3KfH19lTdvXmv54MGDdeLECX377bdycXFJVL9AgQLy8vJKVA4AQE7FPeXpkDCEPZbZ1wEASJNTp07p2LFjjg4DAACnQU95Ori5ukiKo6ccAIA7WL58uc3zKVOmpFh/6NChGjp0aKbFAwCAs6GnPB3+6yknKQcAAAAApB9JeTq4uf6blDP7OgAAAAAgA0jK08HN5eZho6ccAAAAAJARJOXp4JowfJ17ygEAAAAAGUBSng7W4evMvg4AAAAAyACS8nRImOgtjp5yAAAAAEAGkJSng/WecpJyAAAAAEAGkJSnA7OvAwAAAADsgaQ8HVinHAAAAABgDyTl6cDwdQAAAACAPZCUp8N/w9dJygEAAAAA6UdSng7WdcpZEg0AAAAAkAEk5emw+eglSdKhc1cdHAkAAAAAIDsjKU+Ha9FxkqQ52044OBIAAAAAQHZGUp4ODcrmlyS1rhjk4EgAAAAAANkZSXk6lCmQS5Lk6sLhAwAAAACkH1llOrgx0RsAAAAAwA5IytOBJdEAAAAAAPZAUp4Obv8OW4+Np6ccAAAAAJB+JOXp4J7QUx5HTzkAAAAAIP1IytPB1dpTTlIOAAAAAEg/kvJ0+K+nnOHrAAAAAID0IylPh4TZ12PoKQcAAAAAZABJeTq4ut48bHHcUw4AAAAAyACS8nRwT1innNnXAQAAAAAZQFKeDm6uTPQGAAAAAMg4kvJ0SLinnCXRAAAAAAAZQVKeDm7/zr4ew+zrAAAAAIAMcPqkPDIyUgMGDFCxYsXk7e2tevXqaePGjQ6Nye3fdcrjGL4OAAAAAMgAp0/Kn3rqKS1evFjfffedduzYoRYtWqhZs2Y6ceKEw2JiSTQAAAAAgD04dVJ+/fp1zZo1S6NHj1aDBg1UunRpDR06VCVKlNCECRMcFpd1+Hosw9cBAAAAAOnn5ugAUhIbG6u4uDh5eXnZlHt7e2v16tVJbhMVFaWoqCjr84iICLvH5WGdfZ2kHAAAAACQfk7dU+7n56e6devqnXfe0cmTJxUXF6epU6dq/fr1OnXqVJLbhIWFKSAgwPooUqSI3eNyd/s3KWf2dQAAAABABjh1Ui5J3333nYwxKlSokDw9PfXJJ5+oc+fOcnV1TbL+4MGDFR4ebn0cP37c7jG5/9tTHs3s6wAAAACADHDq4euSVKpUKa1YsUJXr15VRESEgoOD9b///U8lSpRIsr6np6c8PT0zNSZ3lkQDAAAAANiB0/eUJ/D19VVwcLAuXbqkhQsX6sEHH3RYLAn3lMcwfB0AAAAAkAFO31O+cOFCGWNUrlw5HTx4UK+88orKlSunnj17OiymhOHrzL4OAAAAAMgIp+8pDw8PV58+fRQaGqpu3brpvvvu06JFi+Tu7u6wmBImeuOecgAAAABARjh9T/ljjz2mxx57zNFh2HB34Z5yAAAAAEDGOX1PuTNKGL4eb6S4eO4rBwAAAACkD0l5OiQMX5foLQcAAAAApB9JeTokLIkmcV85AAAAACD9SMrTwd3lv8MWy7JoAAAAAIB0IilPBxcXi9yY7A0AAAAAkEEk5emUMNlbNGuVAwAAAADSiaQ8nRLuK6enHAAAAACQXiTl6eTx7wzsMdxTDgAAAABIJ5LydEoYvk5POQAAAAAgvUjK08nt3+HrLIkGAAAAAEgvkvJ0svaUM9EbAAAAACCdSMrTycOVe8oBAAAAABlDUp5O1p7yeHrKAQAAAADpQ1KeTtYl0Ri+DgAAAABIJ5LydHJn+DoAAAAAIINIytMpISmPjotzcCQAAAAAgOyKpDydrMPX6SkHAAAAAKQTSXk6uf3bUx5LUg4AAAAASCeS8nRKWBItltnXAQAAAADpRFKeTm7/Dl+PZvZ1AAAAAEA6kZSnk5tLQk85w9cBAAAAAOlDUp5OHm6sUw4AAAAAyBiS8nRK6CmPoaccAAAAAJBOJOXplHBPeWwcPeUAAAAAgPQhKU+nhNnXY0jKAQAAAADpRFKeTgk95TGsUw4AAAAASCeS8nSy3lNOTzkAAAAAIJ1IytPJw+3fJdHoKQcAAAAApBNJeTq5ufw7fD2ennIAAAAAQPqQlKeTm3WiN3rKAQAAAADpQ1KeTh4siQYAAAAAyCCS8nSipxwAAAAAkFEk5enk4+EqSboaFevgSAAAcE5hYWGyWCwaMGBAsnVWr16t+vXrK2/evPL29lZoaKg++uijrAsSAAAHc3N0ANmVt/vNpPxGbJyDIwEAwPls3LhREydOVOXKlVOs5+vrq759+6py5cry9fXV6tWr1bt3b/n6+uqZZ57JomgBAHAcesrTyf3fJdGiY7mnHACAW125ckVdunTRpEmTFBgYmGLdatWqqVOnTqpQoYKKFy+url27qmXLllq1alUWRQsAgGORlKeTp/WecpJyAABu1adPH7Vt21bNmjVL87Zbt27VmjVr1LBhw2TrREVFKSIiwuYBAEB2xfD1dEroKWeiNwAA/jN9+nRt2bJFGzduTNN2hQsX1rlz5xQbG6uhQ4fqqaeeSrZuWFiYhg0bltFQAQBwCk7dUx4bG6s333xTJUqUkLe3t0qWLKnhw4crPt7xvdMergxfBwDgVsePH1f//v01depUeXl5pWnbVatWadOmTfr88881duxY/fDDD8nWHTx4sMLDw62P48ePZzR0AAAcxql7ykeNGqXPP/9c33zzjSpUqKBNmzapZ8+eCggIUP/+/R0am3tCUs7wdQAAJEmbN2/W2bNnVaNGDWtZXFycVq5cqXHjxikqKkqurq5JbluiRAlJUqVKlXTmzBkNHTpUnTp1SrKup6enPD097d8AAAAcwKmT8rVr1+rBBx9U27ZtJUnFixfXDz/8oE2bNiW7TVRUlKKioqzPM+s+Mw837ikHAOBWTZs21Y4dO2zKevbsqdDQUA0aNCjZhPx2xhibczkAADmZUyfl9913nz7//HPt379fZcuW1fbt27V69WqNHTs22W2y6j4zhq8DAGDLz89PFStWtCnz9fVV3rx5reWDBw/WiRMn9O2330qSPvvsMxUtWlShoaGSbq5b/sEHH+iFF17I2uABAHAQp07KBw0apPDwcIWGhsrV1VVxcXF67733kh3OJt082Q8cOND6PCIiQkWKFLF7bO5uFkn0lAMAkBanTp3SsWPHrM/j4+M1ePBgHT58WG5ubipVqpRGjhyp3r17OzBKAACyjlMn5TNmzNDUqVM1bdo0VahQQdu2bdOAAQMUEhKi7t27J7lNVt1n5uH63+zr8fFGLi6WTH9PAACym+XLl9s8nzJlis3zF154gV5xAMBdzamT8ldeeUWvvfaaHn/8cUk3J385evSowsLCkk3Ks0rCkmiSFBMfL0+X1N0nBwCAs7p8+bI2bNigs2fPJlrppFu3bg6KCgCAnM2pk/Jr167JxcV21TZXV1enWBLN85ak/HT4DRXL6+vAaAAAyJh58+apS5cuunr1qvz8/GSx/DcCzGKxkJQDAJBJnDopb9++vd577z0VLVpUFSpU0NatW/Xhhx+qV69ejg5Nnm7/9Yy7WBi6DgDI3l566SX16tVLI0aMkI+Pj6PDAQDgruHUSfmnn36qt956S88//7zOnj2rkJAQ9e7dW2+//bajQ5Mk+Xm5KfJGLJO9AQCyvRMnTqhfv34k5AAAZDGnTsr9/Pw0duzYFJdAcyT3fyd7i403Do4EAICMadmypTZt2qSSJUs6OhQAAO4qTp2UOzs3F5ZFAwBkX3PnzrX+u23btnrllVe0e/duVapUSe7u7jZ1H3jggawODwCAu0K6kvLjx4/LYrGocOHCkqQNGzZo2rRpKl++vJ555hm7BujMEpLyOHrKAQDZUIcOHRKVDR8+PFGZxWJRXFxcFkQEAMDdx+XOVRLr3Lmzli1bJkk6ffq0mjdvrg0bNuj1119P8mSeU7ndslY5AADZTXx8fKoeJOQAAGSedCXlO3fuVK1atSRJP/74oypWrKg1a9Zo2rRpmjJlij3jc2purjd7ymMZvg4AAAAASId0JeUxMTHy9PSUJC1ZssR6n1loaKhOnTplv+icnLsLE70BAHKGfv366ZNPPklUPm7cOA0YMCDrAwIA4C6RrqS8QoUK+vzzz7Vq1SotXrxYrVq1kiSdPHlSefPmtWuAziyhp5yJ3gAA2d2sWbNUv379ROX16tXTzJkzHRARAAB3h3Ql5aNGjdIXX3yhRo0aqVOnTqpSpYqkm7O4JgxrvxskTPQWyz3lAIBs7sKFCwoICEhU7u/vr/PnzzsgIgAA7g7pmn29UaNGOn/+vCIiIhQYGGgtf+aZZ+Tj42O34Jydp7urJOlGLBPgAACyt9KlS2vBggXq27evTfnvv//O2uXAXSIu3mjD4Ys6G3lDBfy8VKtEHrn+2wkFIPOkKym/fv26jDHWhPzo0aP6+eefdc8996hly5Z2DdCZ+XjcTMqvR5OUAwCyt4EDB6pv3746d+6cmjRpIklaunSpxowZo7Fjxzo2OACZbsHOUxo2b7dOhd+wlgUHeGlI+/JqVTHYgZEBOV+6kvIHH3xQHTt21LPPPqvLly+rdu3acnd31/nz5/Xhhx/queees3ecTsmDJdEAADlEr169FBUVpffee0/vvPOOJKl48eKaMGGCunXr5uDoAGSmBTtP6bmpW3T7Fe3p8Bt6buoWTehancQcyETpuqd8y5Ytuv/++yVJM2fOVMGCBXX06FF9++23Sc7cmlN5uN08fNEMXwcA5ADPPfec/vnnH505c0YRERE6dOgQCTmQw8XFGw2btztRQi7JWjZs3m7FsdoQkGnSlZRfu3ZNfn5+kqRFixapY8eOcnFxUZ06dXT06FG7BujMEnrKo5l9HQCQQ5w7d0779u3T9u3bmeANuAtsOHzRZsj67YykU+E3tOHwxawLCrjLpCspL126tObMmaPjx49r4cKFatGihSTp7Nmz8vf3t2uAziwq9mYyvvtkhIMjAQAgY65evapevXopODhYDRo00P3336/g4GA9+eSTunbtmqPDA5BJzkYmn5Cnpx6AtEtXUv7222/r5ZdfVvHixVWrVi3VrVtX0s1e82rVqtk1QGc2f8cpSdKcbScdHAkAABkzcOBArVixQvPmzdPly5d1+fJl/fLLL1qxYoVeeuklR4cHIJMU8POyaz0AaZeuid4eeeQR3XfffTp16pR1jXJJatq0qR566CG7BQcAALLGrFmzNHPmTDVq1Mha1qZNG3l7e+uxxx7ThAkTHBccgExTq0QeBQd46XT4jSTvK7dICgq4uTwagMyRrp5ySQoKClK1atV08uRJnThxQpJUq1YthYaG2i04AACQNa5du6aCBQsmKi9QoADD14EczNXFoiHty0u6mYDfKuH5kPblWa8cyETpSsrj4+M1fPhwBQQEqFixYipatKhy586td955R/HxTHoGAEB2U7duXQ0ZMkQ3bvx33+j169c1bNgw621qAHKmVhWDNaFrdQUF2A5RDwrwYjk0IAuka/j6G2+8oa+++kojR45U/fr1ZYzRn3/+qaFDh+rGjRt677337B0nAADIRB9//LFatWqlwoULq0qVKrJYLNq2bZu8vLy0cOFCR4cHIJO1qhis5uWDtOHwRZ2NvKECfjeHrNNDDmS+dCXl33zzjb788ks98MAD1rIqVaqoUKFCev7550nKAQDIZipWrKgDBw5o6tSp2rt3r4wxevzxx9WlSxd5e3s7OjwAWcDVxaK6pfI6OgzgrpOupPzixYtJ3jseGhqqixfvnjUMB7cOVdjve+XGL4gAgBzA29tbTz/9tKPDAADgrpKue8qrVKmicePGJSofN26cKleunOGgsouS+XNJkioWCnBwJAAAZNy+ffvUt29fNW3aVM2aNVPfvn21d+9eR4cFAECOlq6e8tGjR6tt27ZasmSJ6tatK4vFojVr1uj48eP67bff7B2j00roIY+LT2oBCQAAso+ZM2eqU6dOqlmzpnVit3Xr1qlSpUqaNm2aHn30UQdHCABAzpSunvKGDRtq//79euihh3T58mVdvHhRHTt21K5duzR58mR7x+i03FxvJuWxJOUAgGzu1Vdf1eDBg7V27Vp9+OGH+vDDD7VmzRq9/vrrGjRokKPDAwAgx0pXT7kkhYSEJJrQbfv27frmm2/09ddfZziw7MDV2lPOMnAAgOzt9OnT6tatW6Lyrl276v3333dARAAA3B3S1VOOm2LibvaQ7z9zxcGRAACQMY0aNdKqVasSla9evVr333+/AyICAODukO6eckgnLl13dAgAANjFAw88oEGDBmnz5s2qU6eOpJv3lP/0008aNmyY5s6da1MXAADYB0l5BtQvzTqOAICc4fnnn5ckjR8/XuPHj0/yNUmyWCyKi4vL0tgAAMjJ0pSUd+zYMcXXL1++nJFYsh0Pt5uj/z1cuQsAAJC9xTM/CgAADpGmbDIgICDFR7FixZKcJCancv83GY+Oi5cxzMAOAMh+2rRpo/DwcOvz9957z+ZH9gsXLqh8+fIOiAwAgLtDmnrK76blzlLD3eW/3zQuXo1W3lyeDowGAIC0W7hwoaKioqzPR40apU6dOil37tySpNjYWO3bt89B0QEAkPMx7joDbsnJtftUhOMCAQAgnW4f6cXILwAAshZJeQa4WCzWf8fGcREDAAAAAEgbkvIMcHX5LymPiWOCHABA9mOxWGS55UfmhDIAAJA1WBLNTmLoKQcAZEPGGPXo0UOenjfnRblx44aeffZZ+fr6SpLN/eYAAMD+SMozwNPtv4EGBf2Z5A0AkP10797d5nnXrl0T1bmbVlYBACCrkZRngMViUcn8vjp07qpi4+kpBwBkP6ysAgCAYzn9PeXFixe33u9266NPnz6ODk2S5PHvWuXcUw4AAAAASCunT8o3btyoU6dOWR+LFy+WJD366KMOjuymvacjJUlHLlxzcCQAAAAAgOzG6ZPy/PnzKygoyPr49ddfVapUKTVs2NDRodl4a85OR4cAAAAAAMhmstU95dHR0Zo6daoGDhyY7HItUVFRNjPFRkREZFV4AAAAAACkidP3lN9qzpw5unz5snr06JFsnbCwMAUEBFgfRYoUyZLYHq5eOEveBwAAAACQc2SrpPyrr75S69atFRISkmydwYMHKzw83Po4fvx4psbUrW4xSVKh3F6Z+j4AAAAAgJwn2wxfP3r0qJYsWaLZs2enWM/T01Oenlm3Zriry81h9CyJBgAAAABIq2zTUz558mQVKFBAbdu2dXQoNlz+vbf9yIWrDo4EAAAAAJDdZIukPD4+XpMnT1b37t3l5uZcnftztp6QJP2247SDIwEAAAAAZDfZIilfsmSJjh07pl69ejk6lEQuXI12dAgAAAAAgGzKubqdk9GiRQsZwz3bAAAAAICcJVv0lAMAAAAAkBORlAMAAAAA4CAk5RnU7J6C1n8zxB4AAAAAkBYk5RnUu2FJ67//uXTdgZEAAAAAALIbkvIMSlinXJJu+ScAAAAAAHdEUp5BJfP5Wv+980S4AyMBAAAAAGQ3JOUZFOjrYf33DpJyAAAAAEAakJQDAAAAAOAgJOV2dDUqztEhAAAAAACyEZJyO5qy5oijQwAAAAAAZCMk5QAAAAAAOAhJOQAAAAAADkJSDgAAAACAg5CUAwAAAADgICTlAAAAAAA4CEk5AAAAAAAOQlJuB/+rWcTRIQAAAAAAsiGScjvoUK2Qo0MAAAAAAGRDJOV2cOBspKNDAAAAAABkQyTldmC55d97TkU4LA4AAJxJWFiYLBaLBgwYkGyd2bNnq3nz5sqfP7/8/f1Vt25dLVy4MOuCBADAwUjK7cDT3dX679Yfr3JgJAAAOIeNGzdq4sSJqly5cor1Vq5cqebNm+u3337T5s2b1bhxY7Vv315bt27NokgBAHAsN0cHkBM0LlfA0SEAAOA0rly5oi5dumjSpEl69913U6w7duxYm+cjRozQL7/8onnz5qlatWqZGCUAAM6BnnI7yOXJbxsAACTo06eP2rZtq2bNmqV52/j4eEVGRipPnjzJ1omKilJERITNAwCA7Ips0g68PVzvXAkAgLvA9OnTtWXLFm3cuDFd248ZM0ZXr17VY489lmydsLAwDRs2LL0hAgDgVOgpBwAAdnH8+HH1799fU6dOlZeXV5q3/+GHHzR06FDNmDFDBQokf2vY4MGDFR4ebn0cP348I2EDAOBQ9JQDAAC72Lx5s86ePasaNWpYy+Li4rRy5UqNGzdOUVFRcnVNenTZjBkz9OSTT+qnn36647B3T09PeXp62jV2AAAchaQcAADYRdOmTbVjxw6bsp49eyo0NFSDBg1KNiH/4Ycf1KtXL/3www9q27ZtVoQKAIDTICnPBMYYWSyWO1cEACAH8fPzU8WKFW3KfH19lTdvXmv54MGDdeLECX377beSbibk3bp108cff6w6dero9OnTkiRvb28FBARkbQMAAHAA7im3k4eqFbL+e9vxy44LBAAAJ3bq1CkdO3bM+vyLL75QbGys+vTpo+DgYOujf//+DowSAICsQ0+5nRTP62v99+XrMQ6MBAAA57F8+XKb51OmTEnxdQAA7jb0lNuJn9d/v2/ExhkHRgIAAAAAyC5Iyu2kTaVg67/j4knKAQAAAAB3RlJuJwX9/1uaZcX+sw6MBAAAAACQXZCU28mts63/sOG4omLjHBgNAAAAACA7ICnPJN+uOeroEAAAAAAATo6kPJOMXrhX8dxbDgAAAABIAUl5JomJM/p27RFHhwEAAAAAcGJOn5SfOHFCXbt2Vd68eeXj46OqVatq8+bNjg4rSQ3L5rd5PnPLPw6KBAAAAACQHTh1Un7p0iXVr19f7u7u+v3337V7926NGTNGuXPndnRoSepYvZDNc4ssydQEAAAAAEByc3QAKRk1apSKFCmiyZMnW8uKFy/uuIDuwMfDqQ8nAAAAAMDJOHVP+dy5c1WzZk09+uijKlCggKpVq6ZJkyaluE1UVJQiIiJsHlmlZrFAm+cnLl/PsvcGAAAAAGQ/Tp2UHzp0SBMmTFCZMmW0cOFCPfvss+rXr5++/fbbZLcJCwtTQECA9VGkSJEsizfQ18Pm+cWr0Vn23gAAAACA7Mepk/L4+HhVr15dI0aMULVq1dS7d289/fTTmjBhQrLbDB48WOHh4dbH8ePHszDixIxhWTQAAAAAQNKcOikPDg5W+fLlbcruueceHTt2LNltPD095e/vb/NwpIcnrHHo+wMAAAAAnJdTJ+X169fXvn37bMr279+vYsWKOSiitNty7LKjQwAAAAAAOCmnTspffPFFrVu3TiNGjNDBgwc1bdo0TZw4UX369HF0aGkSH88QdgAAAABAYk6dlN977736+eef9cMPP6hixYp65513NHbsWHXp0sXRoSWra52iicqmrDmS9YEAAAAAAJye0y+s3a5dO7Vr187RYaSai8WSqGz4r7vV674SDogGAAAAAODMnLqnPDvKl8szyfLL11geDQAAAABgi6Tczp66P+ke8arDF2dxJAAAAAAAZ0dSbmc+Hm4qUyCXo8MAAAAAAGQDJOWZYN4L9zk6BAAAAABANkBSngm83F0dHQIAAAAAIBsgKc9CYb/vcXQIAAAAAAAnQlKeSUY9XClR2RcrDjkgEgAAAACAsyIpzyQdqhVKsrzXlI1afeC8Pl5yQHHxJoujAgAAAAA4EzdHB5BTebolfV/5H3vP6o+9ZyVJhQK99UiNwlkZFgAAAADAidBT7kBHL1x1dAgAAAAAAAciKc9EE5+okeLrhtHrAAAAAHBXIynPRC0qBDk6BAAAAACAEyMpdyAjusoBAAAA4G5GUp7J/h7RJtnX5m0/lYWRAAAAAACcDUl5JnN1sST72rGL1zR49l9ZGA0AAAAAwJmQlDvYDxuOa9fJcEeHAQAAAABwAJLyLPBJp2opvt72k9VZFAkAAAAAwJmQlGeBQrm971jHsD4aAAAAANx1SMqzQEF/zzvW6TxpfRZEAgAAAABwJiTlWaBwoI8+frxqinXWHrqg8GsxWRMQAAAAAMApkJRnkQerFlLrikEp1qkyfJH+PHg+iyICAAAAADgaSXkWKujvdcc6Xb5cr6tRsVq067RuxMRlQVQAAAAAAEchKc9CLzYvq4qF/O9Yr8a7i/XMd5v11pydkqRT4dc1fvlBXboandkhAgAAAACyEEl5Fgrwdtf0Z+resd6NmHhJ0k+b/5Ek/e+LdRq9YJ9e/ml7psYHAAAAAMhaJOVZLJenm1a80ihN2xy7eE2StOoA95sDAAAAQE5CUu4AxfL6OjoEAAAAAIATICl3cscuXHN0CAAAAACATEJS7iAH3mudqnoN3l9m/beRUXRsfGaFBAAAAADIYiTlDuLu6qJVrzZO0zYxcUbl316gMxE3MikqAAAAAEBWIil3oCJ5fPRj7zvPxn6r2HijaeuPZVJEAAAAAICsRFLuYLVK5NH3T9VO0zYmk2IBAAAAAGQtknInUL90vjRvcy4ySiv2n9Op8OuKiydNBwAAAIDsyM3RAeCm6c/U0eMT16Wq7l//XNa97x2wPm9cLr8m96yVWaEBAAAAADIJPeVOok7JvKmuu3zfOZvny255fvziNY3744DCr8XYLTYAAAAAQOagp9yJdKxeSLO3nEjXtl+vPqwpa47o2MWb65rvORWpz7pUt2d4AAAAAAA7Iyl3IqFBfunedvivu22erz10IaPhAAAAAAAyGcPXnUinWkUdHQIAAAAAIAs5dVI+dOhQWSwWm0dQUJCjw8o0fl7u2vtOK0eHAQAAAADIIk4/fL1ChQpasmSJ9bmrq6sDo8l8Xu45u30AAAAAgP84dU+5JLm5uSkoKMj6yJ8/v6NDynS7hrW06/4ib8TIGNYyBwAAAABn4/RJ+YEDBxQSEqISJUro8ccf16FDh1KsHxUVpYiICJtHduPr6aYyBXJZn7evEpLmfVy8Gq3v1h7RmoPnVWnoIr0y8y+1/niVPli4T+HXSdIBAAAAwBk4dVJeu3Ztffvtt1q4cKEmTZqk06dPq169erpwIfmZxcPCwhQQEGB9FClSJAsjzhwvNCmdru3e+mWXOn+5XpI0c/M/2nMqQuOWHVSVYYv0zq977BkiAAAAACAdnDopb926tR5++GFVqlRJzZo10/z58yVJ33zzTbLbDB48WOHh4dbH8ePHsyrcTFO2oJ8erVHYrvv8+s/Ddt0fAAAAACDtnH6it1v5+vqqUqVKOnDgQLJ1PD095enpmYVRZY3Rj1RWvJFmbfnHbvv89a+TqlEsUMEB3nbbJwAAAAAg9Zy6p/x2UVFR2rNnj4KDgx0dSqZ7rlEpSVKbSjeXgLNYLBrzWBW7LpnWd9pWNR2zwm77AwAAAACkjVMn5S+//LJWrFihw4cPa/369XrkkUcUERGh7t27Ozq0TNexemGteKWRPu1U3abcy91Vqwc1VpPQAnZ5n2vRcfrn0jV9vuJvXY2KZQI4AAAAAMhCTj18/Z9//lGnTp10/vx55c+fX3Xq1NG6detUrFgxR4eWJYrl9U2yvHCgj77uca8ajF6mYxevZfh9Xv5pu9YduqiRv++Vt7ur2lUOVt1SedWxun3vYwcAAAAA2LKYHN41GhERoYCAAIWHh8vf39/R4djV3+euZOrw8z9eaqiS+XPduSIAIE1y8rnJETieAABnk5Zzk1MPX0fKSuXPpSJ5Mm+Stv1nrkiSDp27ovoj/9DUdUfTtP3Jy9f1y7YTio2Lz4zwAAAAACDbc+rh67iz75+soy9XH1KPesXVxM695s9O3Wzz/M05O9W1TjF9tfqw8vt56oEqISlu3+iD5YqOjdexC9fk6mrRg1ULqVBuZnoHAAAAgAQk5dlc0bw+Gv5gxSx7v+Kvzbf++05JeXTszR7yMYv3S5K+XXNU615vmnnBAQAAAEA2w/D1HChfLg+982CFTH+f8csPatwfB1I9PP10xI1MjggAAAAAshd6ynOQpS811O87TqlH/RLK5emmt37ZlanvN3rBPknSB4v267PO1RUU4KkeX2/UwzUKa3Cb0Ex9bwAAAADICegpz0FK5c+lvk3KKJfnzd9avulVK8veu8+0Leo0cb0io2I1Zc0RlXtzwR23WbL7jGZt/icLogMAOEJYWJgsFosGDBiQbJ1Tp06pc+fOKleunFxcXFKsCwBATkRSnoM1LJtfR0a2ladb1vyZo1MxjP1GTJwSVuF76ttNeumn7Tp+y1rr0bHx1nvRAQDZ18aNGzVx4kRVrlw5xXpRUVHKnz+/3njjDVWpUiWLogMAwHmQlN8FnGkh+tC3FujZqZu17fhla9mla9GSpNi4eNUJW6p6I/9QfHzao74SFau9pyPsFSoAIJ2uXLmiLl26aNKkSQoMDEyxbvHixfXxxx+rW7duCggIyKIIAQBwHiTld4HaJfJIkvL7eTo4kpsW7jqjDp/9maj8lZl/6eLVaJ2/EqXIG7GSpKjYOO06GW7tXU9JszEr1GrsKq35+7zdYwYApF6fPn3Utm1bNWvWLFP2HxUVpYiICJsHAADZFRO93QU++l9VTfnziB6rWUSnwq9rzd8XdOj8VYUG+cnb3VU/bDimPL4eWn/4okPi+2HDceX29tDPW09Yy6Li4hR+Xer3w1at2H9O7z1UUV1qF0u0rTFGl67FKI+vh3V29wU7T6teqXxZFj8A4D/Tp0/Xli1btHHjxkx7j7CwMA0bNizT9g8AQFYiKb8L5MvlqZdblpN0c13z2iXz2rze674SkmzXIM9KP2w4ph82HLMpq/XeUpvnb/y8UyXy+WrY3N0K9HXX1Cdry83VRW//skvfrTuqiU/UyMqQAQBJOH78uPr3769FixbJy8sr095n8ODBGjhwoPV5RESEihQpkmnvBwBAZiIpR7bRedJ66787TVqnZxqU0nfrjkqSPli0z/ra4fNXFRdvFBsfL0831yyPEwDuVps3b9bZs2dVo8Z/P5TGxcVp5cqVGjdunKKiouTqmvHvZU9PT3l6OsctWQAAZBRJOaxmP19PHcevcXQYqbLxyCVtPLLJ+nz/mSvWf686cF4tx67U6fAbal0xSL6ebhr6QAVHhAkAd5WmTZtqx44dNmU9e/ZUaGioBg0aZJeEHACAnIakHFbViwbqyMi2Oh1+Q9v/uayvVh/WBgfdZ55RB8/eTNJ/+ncd9OblC6qgv5fORtxQ+RB/HblwTWMW7dObbcurXJDfHff35pwd2n/6iqY9fXPYPAAgMT8/P1WsWNGmzNfXV3nz5rWWDx48WCdOnNC3335rrbNt2zZJN2dtP3funLZt2yYPDw+VL18+y2IHAMBRSMqRSFCAl4ICglSvVF5VGrpIkrRzWEu5uVgU+tYCB0eXPl2+XJ9kec/JGzSodaj2no7Uqy3LyWKxWF+Ljo3XgbOR8vdy19R1N+95X3vogu4vkz9LYgaAnOjUqVM6dsx2HpFq1apZ/71582ZNmzZNxYoV05EjR7I4OgAAsh5JOZLl5+Wu9a83lYeri3J53vyodK5dVEf+vWc7qdnaF7/YQM0/WpnVoabbyfAb6j99mySpTsm8ur90Pr35y07l8fHQ7ztP6e9zV23qx8YbbT56ST9tOq4Tl69rco97bXrOjTE6cuGaiuXxkSTFGSN3etYB3MWWL19u83zKlCmJ6qRm2UsAAHIqknKkqKC/7ey5Ix6qZP33xavRio6NV52w/2ZKL1PQT3l9PXThanSWxWgv3b/ecMc6PSfbLvHT5pNValSugJ5rWEqBvh76+s8jeufX3ZKkYnl9dPTCNS17uZHy+HjI39vNpif+VmcjbmjP6Ug1KJMv2ToAAAAAch6ScqRbHl8Pm+fVi+aWJP35WhMt33dOU9Yc1rpD2fOe9NTaf+aK9p+5ohOXrqt4Ph99tuxv62tHL1yTJDX+YLm1bEj78roWHadVB85pSs9a8nK/OenR/aOXKSo2XhO6VFfrSsFZ2gYAAAAAjkNSjgz7+PGq+mzZQb3/aBVJkpe7q1pVDFKrikHq9vUGrdx/TiXy+erw+at32FP2NX/HqVTVGzZvt/XfU9cd1Z5TkepQLURRsfGSpJUHzt0xKY+8ESM/L/f0BwsAAADAaXCzKzLswaqFtOjFhiqVP1ei16b0uFdb3mqu6kUDrWWP1SwsSZrbt76CA7wSbXO3eHf+Hs3a8o+e+OrWYfMWhV+PkTFG245f1pWoWJttPl16QJWGLtKYRfuSvAfz/YV71XnSOsXExac5HmOM4uO5rxMAAADISiTlyFQuLhbl8fXQc41KSZIev7eIRj1cWbuGtVTlwrn1S9/6Gvu/qtozvJUOvNda9Uvntdl+wYD7HRG2w/yw4ZiqDFuk0Qv3qcNnf6rj+D91PTpOkrRo12mNWbxfkvTpHwc1YcXfNtsev3hNny37W2v+vqAlu8+k+b17TdmoxmOWKzo26YQ+IY47+eufy9p+/HKSrx29cDXZ/QMAAAB3I4vJ4VOeRkREKCAgQOHh4fL393d0OHe1GzFx1nuoUzJp5SG999se/fB0HdUtlVcnLl9X/ZF/ZEGE2c/s5+vp+MVrWrz7jH79678h9K0rBql8sL8W7j6t11rdo7/PXdHSvWc1/IEKKp7PV3O2ntCE5X9rQtfq2n8mUpUL51a9f4/x9GfqqE7J/34c+W7tEb31yy5J0qutyun5RqWTjedGTJx12bw9w1vJ2+O/v/fyfWfVY/JG3Vs8UD89W8+uxwHIbjg32RfHEwDgbNJybuKecmSZ1CTkkvR0g5J6ukHJJF9b9Wpj3T96mT3DytY6jl+TZPnvO0/r952nJUldv/pvjfZGHyzXpjebacCMbZKkJmNWpLj/LccuWRNySRq9YJ81KT94NlJ9vt+qF5qWVrvKIZKka7f0pl+JirVJyr9ff3Nd4o1HLqW2eQAAAECOx/B1OD3XW5YIy5vLQx/8O6HcT8/WTXG7AG8mQ0tKzXeXpPh6vx+2qu+0LSr+2vwkk/72n67WqAV7NWDGNu07E6m+07aq+GvzNX3DMbUa+98a9R0n/KnjF6/pVPh11iAGAAAAksHwdTg9Y4xenLFNXu6uGvlwZZvXwq/HqMqwRdbnwQFeOhV+Q5K0bnBTnbh8TQ9PWCtJ6ly7qFqUL6get601jqw38YkaGr1wn4L8vTSuczXl9vGQMSbRGu0Xr0Zr4spDeqRGYQX6uCuXl5s83Vx1PTpOP20+riahBVQ40Mdmm4W7Tuv79cf0waOVVcDv5kSCe05FyNPNRSWTmIxQkmLi4vXqzL9Ut1RePVazSOY0GrgF5yb74ngCAJxNWs5NJOXI9n7adFw/bfpHox+pLD8vN9X4tyd4/etNVdD/ZlJ26Wq0cvu4y2Kx6J1fd+ur1YcdGTJuM6R9eQ2bt1sB3u56ocnN4fHNyxfUu/P3aPFtk9Ztf7uFPvnjgL5afVgB3u7a+lZz7T0dqbIFc8nN1UXFX5svSepQNURjH6+m8GsxqjL85g83e99ppXhj5O3uqitRsdal5X7ceFyvzvpLkrRkYAOVLuCXZJw7T4TLzdWiYnl8NXbpfrWqEKRqt6wskBaxcfFyc016sNIv207Iz8tNTUILpmvfcH6cm+yL4wkAcDYk5bfgRH13iYmLV5k3fpd0MwFL6j72+Hij6RuP6/Wfd8jTzUXPNiylj5ceyOpQYWde7i66EWM7s/uYR6uoYqEAtbxlWP2tGpXLry+71dR7v+3R5D+PWMv/GtpCC3ae1j1B/nrxx23qWruoht6yxnzvhiX1xYpDkqQjI9umOdbnpm7W0r1ntezlRlq067TWHbqgTztVl4ebi06H31CdsKWSpGb3FNCNmHh992QtWSwWHTl/VX5ebsqbyzPJ/V64EmUdTQDnxrnJvjieAABnQ1J+C07Ud58zETdkjBSUyjXQjTE6cPaKWnx0M3HrVreYvl17VJJUtmAuLXqxoWZsPKZBs3ZIkkrm89WA5mV1PTrWWgbnVbVIbm1LZom25Li5WBSbwprt95XOp9UHz0uSvn+qtvacitCT95VQXLyRq4tF6w9fVJkCuZJMntcduqDHJ66TJHWvW0zf/PtZe6VlOdUpmUdbj13Wu/P32Gyz5rUmslikumE3Z8i//YeAC1ei9MXKQ5q48pBCAry08tXGyfbCJ+XNOTvkarFo2IMVU70NMoZzk31xPAEAzoak/BacqJFaCcOeu9YpKleLRd+sPaovu9VUs/IFFRdvVHX4IkXeiNXWt5or0NfDZptbvfNgBW09flm1S+RR7RJ51eiD5XrqvhJad/iCdp6ISPb9G5XLr+X7zmVO4+AQn3WurkvXotW5VlFZLFJ0XLzKvbkgzftZMrChmn3430z5tyflHcf/qS3HLtuUNSqXX1N61rrjvs9fibJO/rfylcYqmtcn2brbj1/Wgl2n9UKT0vLxSP3iHbFx8dp5MkIVQ/ytPxbsOhkuF4tF9wSn/ns5qXkHbjd49g65u1o0/A4/MJyNuKE8vh5p+vHCnjg32RfHEwDgbEjKb8GJGqn17HebtWDXaS1+sYFKF8ily9dirMl3cj5avN9m6Hu5gn5a+GKDJOtejYrV8Hm7tXTvWY15rIoOnbuiYfN2q2zBXFo4oIE12Ugq0Qdu1a5ysIrm8dH8Hac0/MGK6v71hiTr7Xu3lXUo+77TkSoc6K3VB8/rnV9367PO1VWlSG7tPR2hVmNXWbd5s+09erh6Ya08cE6lC+TSkfPXtO7QBQ1pX16l/701RJJ+63e/iuX10YUr0Tp/NUoLd55Wx+qF9fYvO/Vso1JqXK6Ate7Qubs0Zc0RdaldVO89VEmzt/yjgT9ulyRtH9JCEddjVCSP7Y8BJy9fV/evN6h7veLqWqeYftl2Qu/8ultfPFFTNYr9dx//pavR2nkyXPVL5dO5K1GqPeLm0P9dw1rK1zPxDweRN2L097mr6vDZn3KxSIfC0n77gT1wbrIvjicAwNmQlN+CEzVSyxijq9FxypXEhXxyDp27oiZjVqhy4QBN7nGv/L3d5Z6GnrcTl68rJMDLpvfv1qT8uydr6Ymvkk64gNTwdnfV9Zi4JF+b06e+nvhyvSKjYu+4Hw9XF0XHxd+xXoKE3vy4eKNSr/9mLf+xd1099sXaRPX7NC6lAc3Kau3fF1S1aG4NnrVD83ecsu4r4f+LfLk8tOnN5tbt6oYt1anwGxr1cCVVKxpovQ1lQLMyioqN16BWoda6ZyNuqNa/SfvtcWY1zk32xfEEADgbkvJbcKJGZrt4NVr+Xm52Gwb7y7YTGr1gn15vc4/aVg7WkfNX1eiD5ZIS3x/9w9N1FHkjRiMX7NWhc1d1b/FABQd4y9fTTT9sOGaXeID0er5RKcXGG01ceShD+6lSJLe23/K5H9QqVMEBXhowY9sdt21bKVj9m5VRQX8vm+UTEwxsXlanI26oauHcmvfXSX3etYZND3t8vNGJy9cT9eRnFOcm++J4AgCcDUn5LThRIydImEBMujnD/F//XFaAt4dKF/hv3e3b77d99PM12njkkl5qXlabj13ifnUglSb3uFcr9p/TlDVHrGXvdKioJ+oUs9t7cG6yL44nAMDZpOXclPpxugAcJiEhlyR3VxfVKJYnUZ3bJ8D66dl61n9H3IhR5aGJewmPjGyryX8e1rB/l/vqVKtosj3sdUvmla+nm8Z1rqbQt9I+WRmQXfScsjFR2VtzdiqPj4faVg52QEQAACAnIykH7gL+Xu7a/GYzebq7asvRS+r29QZ1rVNUklSmgJ+1XljHSnqwaoh8PdwU6OuuiSsP6cqNWIU9XMlm7esPHq2il3/aLn8vN01/pq6Gzdul6sUCFRrkpwerFlJcvNGOE+Hq8NmfkqQ/XmqoZfvO6Z1fbyb/IQFeOhl+Q5JUJI+3vu1VWwOmb9X2f8Jt4v7iiRrq/d3mTD02QGr1mbZFTe9pJS931oEHAAD2w/B14C4Ufj1G/l5uslgsMsZo5uZ/VC7IT5UL507V9vHxRqsOnleFEH/lS2Itbkm6ERNn7VFf/nIjFc/nq5i4eO06GaFKhQJ0JSpWfx48r6b3FLAm/LdOcpffz1Mb32iWabPRlymQSwfOXsmUfSPnmtOnvqoWyZ3h/XBusi+OJwDA2aTl3OSYBVoBOFSAt7t1uLvFYtGjNYukOiGXJBcXixqWzZ9sQi5Jnm4ualQuv2oVz6Oi/06S5e7qoqpFcsvVxaIAb3e1qRRs0wOfoGHZ/PrjpYaSpAPvtVblwgF6rGZhbX6zmbXOrmEt9VjNwvq9//0a36W6XF0s+vCxKtbXt7zVXC+3KJtsfP+7t4iqpCK5KpbCut24+ySM/gAAALCXbDV8PSwsTK+//rr69++vsWPHOjocACmwWCya0rNWognoUjLrubr69a9TerlFOesM2O6uLprb9z5rnYlP1JCnu6t8Pd00+pGbSfg9wf5qUb6g3Fxd5OpiUVRsvPL4eqhvkzIKvx6jSasO6+UWZRUa5K+nvt1kja9dpWCbWb0lyc/LTZ1rFVUeXw99tfqwvn+qtmLijAJ93LXl2CX1mrIp2fgndaupjxbv1+5TEXds60/P1tWjnydeGgwAAAB3l2yTlG/cuFETJ05U5cqVHR0KgDRIbUIuSTWK5UlyErtbtagQlGR5wpJ0D1YtZFP+RtvyeqlFOet9wP+rWUR/7DurR6oXlq+nq0rk81W5ID+99ctO3Vc6n566v6R1294NS9nsq2HZAmoSWkAl8/lqYIuymrXlhO4tHqhTl2/oyIWral6+oJqXLyhJavbhCh1MYXj8vcXzaMPrTROtmy3dnICv7Ju/Kzo26XXBu9QuqvceqqS/z11R5I1YrTt0QSN/3ytJeqZBSZUukEsPVAlJ14R8KU32BwAAAPvLFkn5lStX1KVLF02aNEnvvvuuo8MBkM3cOjHXqEcq2ywx1+zfJHpKz1p33I+ri0Vf97jX+jxhiazQoMT3CS0a0EDh12MkSd+uPaqO1Qvp5OXrGjpvt97tUEGSVMDfS20qBem3Hac15tEqCvR1VwE/L0nSLRPuq23lYM3/65Q6ViukVhWD1KhcAUlSqfw3l8TL7+dpTcpfb3OPdbs1rzXR/jORMpIqFwrQlmOXtXDXab3zYEUt2n1a/advSxR3WMdK+nHTccXF33m6kaahBbR079k71stJqhXN7egQAABADpMtJnrr3r278uTJo48++kiNGjVS1apVkx2+HhUVpaioKOvziIgIFSlShMlfADiluHij4xevqXg+X5vysN/36IsVh9QktIC+6l5TUbHxKc76PW/7Sfl7u6th2fypfu+zkTcU6OOhoxeuasuxy6oQ4q8KIQE6fP6qWo1dqe71iqtzrZuz9H+waJ/cXCyas+2kdfv1rzfVCz9slSS9/0hlHTp3NcnlxFxdLNYkf9ZzdfXwBNth+wObl9ULTUpr2/HLmrHxuKZvPC5JKpXfV+cioxRxIzbVbbrdyy3KKsDHQ2/N2Znufdxq+jN1VKdk3gzvh4nJ7IvjCQBwNjlqnfLp06dry5Yt2rgx8YVeUsLCwjRs2LBMjgoA7MPVxZIoIZekl1uUU8My+VWtaKAsFssdl+FqXyUkze+d0CtfuoCfSt+yNF6JfL7aOayl3F3/mwt0XOfqkmSTlBf099KPvetanxfL66tDI9ro73NXVLpALlksFkXciNGNmDjVeu/mMP0axfJo1auNdf/oZZKkbW83V24fD0lStaKBqlY0UNFx8dp05JLm9r1PPh6uKjH4N0nStKdrq0igj3XbtpWD9dm/cUnSlD8P68dN/+iLJ2ro+MVrypvLU+WCbrarfqm8GrNov/o0Lq3yIf76evVhDf91t4L8vVQ40Fux8Ubb/p1foGaxQG06einJY2aPhBwAAOBWTt1Tfvz4cdWsWVOLFi1SlSo3J3SipxwAHOfWJeqOjGyb6u2OXrgqX08364z916PjdCMmToG+HknWv3WCwMPnr+r4xWtq8O8ogANnIjVz8z96tmGpZLe/k/h4ox0nwlUuyM/6g0dMXLzcXCzW942PN6oTtlRnI6M0/MEK6lSrqM0PFRlBz659cTwBAM4mLecmp07K58yZo4ceekiurv/1EMXFxcliscjFxUVRUVE2ryWFEzUA2E96k/LsKj7eyGJJ24SFqcG5yb44ngAAZ5Njhq83bdpUO3bssCnr2bOnQkNDNWjQoDsm5AAA++rXpLQ++eOgetYv7uhQsoSLi32TcQAAgNs5dVLu5+enihUr2pT5+voqb968icoBAJnvxeZl1b5KiHXmdwAAAGSMUyflAADnYrFYVKag350rAgAAIFWyXVK+fPlyR4cAAAAAAIBd2GcaWQAAAAAAkGYk5QAAAAAAOAhJOQAAAAAADkJSDgAAAACAg5CUAwAAAADgICTlAAAAAAA4CEk5AAAAAAAOQlIOAAAAAICDkJQDAAAAAOAgJOUAAAAAADgISTkAAAAAAA5CUg4AAAAAgIOQlAMAAAAA4CBujg4gsxljJEkREREOjgQAgJsSzkkJ5yhkDOd6AICzScu5Pscn5ZGRkZKkIkWKODgSAABsRUZGKiAgwNFhZHuc6wEAzio153qLyeE/08fHx+vkyZPy8/OTxWLJ0L4iIiJUpEgRHT9+XP7+/naK0DFySltoh/PJKW2hHc4np7QloR27d+9WuXLl5OLCnWQZZc9zvTPLKf8PZCWOWdpxzNKOY5Y2d8vxMsYoMjJSISEhdzzX5/iechcXFxUuXNiu+/T3988xH6Cc0hba4XxySltoh/PJKW0pVKgQCbmdZMa53pnllP8HshLHLO04ZmnHMUubu+F4pXY0HFcDAAAAAAA4CEk5AAAAAAAOQlKeBp6enhoyZIg8PT0dHUqG5ZS20A7nk1PaQjucT05pS05pB7Ien52045ilHccs7ThmacPxSizHT/QGAAAAAICzoqccAAAAAAAHISkHAAAAAMBBSMoBAAAAAHAQknIAAAAAAByEpDwNxo8frxIlSsjLy0s1atTQqlWrHBbLypUr1b59e4WEhMhisWjOnDk2rxtjNHToUIWEhMjb21uNGjXSrl27bOpERUXphRdeUL58+eTr66sHHnhA//zzj02dS5cu6YknnlBAQIACAgL0xBNP6PLly3ZrR1hYmO699175+fmpQIEC6tChg/bt25ct2zJhwgRVrlxZ/v7+8vf3V926dfX7779nu3bcLiwsTBaLRQMGDMhWbRk6dKgsFovNIygoKFu14VYnTpxQ165dlTdvXvn4+Khq1aravHlztmpP8eLFE/1NLBaL+vTpk23aIEmxsbF68803VaJECXl7e6tkyZIaPny44uPjrXWyS1vgXNLz907NZ+3Wuq1bt07yuiG7yoxjdvHiRb3wwgsqV66cfHx8VLRoUfXr10/h4eGZ3JrMkdbr1xUrVqhGjRry8vJSyZIl9fnnnyeqM2vWLJUvX16enp4qX768fv7558wK3yHsfcwmTZqk+++/X4GBgQoMDFSzZs20YcOGzGxClsuMz1mC6dOny2KxqEOHDnaO2okYpMr06dONu7u7mTRpktm9e7fp37+/8fX1NUePHnVIPL/99pt54403zKxZs4wk8/PPP9u8PnLkSOPn52dmzZplduzYYf73v/+Z4OBgExERYa3z7LPPmkKFCpnFixebLVu2mMaNG5sqVaqY2NhYa51WrVqZihUrmjVr1pg1a9aYihUrmnbt2tmtHS1btjSTJ082O3fuNNu2bTNt27Y1RYsWNVeuXMl2bZk7d66ZP3++2bdvn9m3b595/fXXjbu7u9m5c2e2asetNmzYYIoXL24qV65s+vfvby3PDm0ZMmSIqVChgjl16pT1cfbs2WzVhgQXL140xYoVMz169DDr1683hw8fNkuWLDEHDx7MVu05e/aszd9j8eLFRpJZtmxZtmmDMca8++67Jm/evObXX381hw8fNj/99JPJlSuXGTt2rLVOdmkLnEt6/t6p+awl+PDDD03r1q2TvG7IrjLjmO3YscN07NjRzJ071xw8eNAsXbrUlClTxjz88MNZ0SS7Suv166FDh4yPj4/p37+/2b17t5k0aZJxd3c3M2fOtNZZs2aNcXV1NSNGjDB79uwxI0aMMG5ubmbdunVZ1axMlRnHrHPnzuazzz4zW7duNXv27DE9e/Y0AQEB5p9//smqZmWqzDhmCY4cOWIKFSpk7r//fvPggw9mcksch6Q8lWrVqmWeffZZm7LQ0FDz2muvOSii/9x+co2PjzdBQUFm5MiR1rIbN26YgIAA8/nnnxtjjLl8+bJxd3c306dPt9Y5ceKEcXFxMQsWLDDGGLN7924jyeZLdu3atUaS2bt3b6a05ezZs0aSWbFiRbZvizHGBAYGmi+//DJbtiMyMtKUKVPGLF682DRs2NCalGeXtgwZMsRUqVIlydeySxsSDBo0yNx3333Jvp7d2pOgf//+plSpUiY+Pj5btaFt27amV69eNmUdO3Y0Xbt2NcZk378HHCs9f+/UfNYSbNu2zRQuXNicOnUqxyTlmX3MbvXjjz8aDw8PExMTY78GZIG0Xr+++uqrJjQ01Kasd+/epk6dOtbnjz32mGnVqpVNnZYtW5rHH3/cTlE7VmYcs9vFxsYaPz8/880332Q8YCeQWccsNjbW1K9f33z55Zeme/fuOTopZ/h6KkRHR2vz5s1q0aKFTXmLFi20Zs0aB0WVvMOHD+v06dM28Xp6eqphw4bWeDdv3qyYmBibOiEhIapYsaK1ztq1axUQEKDatWtb69SpU0cBAQGZ1u6EoWF58uTJ1m2Ji4vT9OnTdfXqVdWtWzdbtqNPnz5q27atmjVrZlOendpy4MABhYSEqESJEnr88cd16NChbNcGSZo7d65q1qypRx99VAUKFFC1atU0adIk6+vZrT3Sze/VqVOnqlevXrJYLNmqDffdd5+WLl2q/fv3S5K2b9+u1atXq02bNpKy598Djpeev3dqPmuSdO3aNXXq1Enjxo2zuY0nu8vMY3a78PBw+fv7y83NzX4NyGTpuX5du3ZtovotW7bUpk2bFBMTk2KdnPC9lFnH7HbXrl1TTEyM9Xo3O8vMYzZ8+HDlz59fTz75pP0DdzLZ55vFgc6fP6+4uDgVLFjQprxgwYI6ffq0g6JKXkJMScV79OhRax0PDw8FBgYmqpOw/enTp1WgQIFE+y9QoECmtNsYo4EDB+q+++5TxYoVs2VbduzYobp16+rGjRvKlSuXfv75Z5UvX976pZRd2jF9+nRt2bJFGzduTPRadvmb1K5dW99++63Kli2rM2fO6N1331W9evW0a9eubNOGBIcOHdKECRM0cOBAvf7669qwYYP69esnT09PdevWLdu1R5LmzJmjy5cvq0ePHtb3zi5tGDRokMLDwxUaGipXV1fFxcXpvffeU6dOnbJdW+A80vP3Ts1nTZJefPFF1atXTw8++KAdI3a8zDxmt7pw4YLeeecd9e7dO4MRZ630XL+ePn06yfqxsbE6f/68goODk62TE76XMuuY3e61115ToUKFEnV8ZEeZdcz+/PNPffXVV9q2bVtmhe5USMrTwGKx2Dw3xiQqcybpiff2OknVz6x29+3bV3/99ZdWr16d6LXs0pZy5cpp27Ztunz5smbNmqXu3btrxYoVycbgjO04fvy4+vfvr0WLFsnLyyvZes7eltatW1v/XalSJdWtW1elSpXSN998ozp16iT5/s7WhgTx8fGqWbOmRowYIUmqVq2adu3apQkTJqhbt27JxuKs7ZGkr776Sq1bt1ZISIhNeXZow4wZMzR16lRNmzZNFSpU0LZt2zRgwACFhISoe/fuycbhjG1B5hs6dKiGDRuWYp2EH0DT+/dO6bM2d+5c/fHHH9q6dWtawnYoRx+zW0VERKht27YqX768hgwZcqfQnVJav4uSqn97eXa7Jk6rzDhmCUaPHq0ffvhBy5cvT/E6K7ux5zGLjIxU165dNWnSJOXLl8/+wTohhq+nQr58+eTq6pro156zZ88m+pXHGSQMTUsp3qCgIEVHR+vSpUsp1jlz5kyi/Z87d87u7X7hhRc0d+5cLVu2TIULF862bfHw8FDp0qVVs2ZNhYWFqUqVKvr444+zVTs2b96ss2fPqkaNGnJzc5Obm5tWrFihTz75RG5ubtb3yQ5tuZWvr68qVaqkAwcOZKu/hyQFBwerfPnyNmX33HOPjh07Zo1Dyj7tOXr0qJYsWaKnnnrKWpad2vDKK6/otdde0+OPP65KlSrpiSee0IsvvqiwsLBs1xZkvr59+2rPnj0pPipWrJiuv3dqPmt//PGH/v77b+XOndv6nS5JDz/8sBo1amTHltqPo49ZgsjISLVq1co68s3d3d1OLcwa6bl+DQoKSrK+m5ub8ubNm2KdnPC9lFnHLMEHH3ygESNGaNGiRapcubJ9g3eQzDhmf//9t44cOaL27dtbv7e+/fZbzZ07V25ubvr7778zrT2OQlKeCh4eHqpRo4YWL15sU7548WLVq1fPQVElr0SJEgoKCrKJNzo6WitWrLDGW6NGDbm7u9vUOXXqlHbu3GmtU7duXYWHh9ss2bB+/XqFh4fbrd3GGPXt21ezZ8/WH3/8oRIlSmTbtiTXvqioqGzVjqZNm2rHjh3atm2b9VGzZk116dJF27ZtU8mSJbNNW24VFRWlPXv2KDg4OFv9PSSpfv36iZYK3L9/v4oVKyYp+/1/MnnyZBUoUEBt27a1lmWnNly7dk0uLranT1dXV+uSaNmpLch8+fLlU2hoaIoPLy+vdP29U/NZe+211/TXX3/ZfKdL0kcffaTJkydnXsMzwNHHTLrZQ96iRQt5eHho7ty52bJHMz3Xr3Xr1k1Uf9GiRapZs6b1R4nk6uSE76XMOmaS9P777+udd97RggULVLNmTfsH7yCZccxCQ0MTXYs+8MADaty4sbZt26YiRYpkWnscJhMnkctREqb6/+qrr8zu3bvNgAEDjK+vrzly5IhD4omMjDRbt241W7duNZLMhx9+aLZu3WpdemDkyJEmICDAzJ492+zYscN06tQpyeV4ChcubJYsWWK2bNlimjRpkuRyPJUrVzZr1641a9euNZUqVbLrcjzPPfecCQgIMMuXL7dZKunatWvWOtmlLYMHDzYrV640hw8fNn/99Zd5/fXXjYuLi1m0aFG2akdSbp19Pbu05aWXXjLLly83hw4dMuvWrTPt2rUzfn5+1v9ns0MbEmzYsMG4ubmZ9957zxw4cMB8//33xsfHx0ydOtVaJ7u0Jy4uzhQtWtQMGjQo0WvZpQ3du3c3hQoVsi6JNnv2bJMvXz7z6quvZru2wLmk5u9drlw5M3v2bOvz1HzWbqccMvu6MZlzzCIiIkzt2rVNpUqVzMGDB22uT279/zM7uNP162uvvWaeeOIJa/2EpapefPFFs3v3bvPVV18lWqrqzz//NK6urmbkyJFmz549ZuTIkTlySTR7HrNRo0YZDw8PM3PmTJvPU2RkZJa3LzNkxjG7XU6ffZ2kPA0+++wzU6xYMePh4WGqV69uXbbLEZYtW2YkJXp0797dGHNzyY8hQ4aYoKAg4+npaRo0aGB27Nhhs4/r16+bvn37mjx58hhvb2/Trl07c+zYMZs6Fy5cMF26dDF+fn7Gz8/PdOnSxVy6dMlu7UiqDZLM5MmTrXWyS1t69epl/Xzkz5/fNG3a1JqQZ6d2JOX2pDw7tCVh3Vl3d3cTEhJiOnbsaHbt2pWt2nCrefPmmYoVKxpPT08TGhpqJk6caPN6dmnPwoULjSSzb9++RK9llzZERESY/v37m6JFixovLy9TsmRJ88Ybb5ioqKhs1xY4l9T8vdNzjrxdTkrKM+OYJXeNJckcPnw4axpmRyldv3bv3t00bNjQpv7y5ctNtWrVjIeHhylevLiZMGFCon3+9NNPply5csbd3d2EhoaaWbNmZXYzspS9j1mxYsWS/DwNGTIkC1qTNTLjc3arnJ6UW4z59656AAAAAACQpbinHAAAAAAAByEpBwAAAADAQUjKAQAAAABwEJJyAAAAAAAchKQcAAAAAAAHISkHAAAAAMBBSMoBAAAAAHAQknIAAAAAAByEpBwAAABAhlgsFs2ZM8fRYQDZEkk5cJc4e/asevfuraJFi8rT01NBQUFq2bKl1q5dK4mTKQAA2VWPHj1ksVgSPVq1auXo0ACkgpujAwCQNR5++GHFxMTom2++UcmSJXXmzBktXbpUFy9edHRoAAAgg1q1aqXJkyfblHl6ejooGgBpQU85cBe4fPmyVq9erVGjRqlx48YqVqyYatWqpcGDB6tt27YqXry4JOmhhx6SxWKxPpekefPmqUaNGvLy8lLJkiU1bNgwxcbGWl+3WCyaMGGCWrduLW9vb5UoUUI//fST9fXo6Gj17dtXwcHB8vLyUvHixRUWFpZVTQcA4K6QMAru1kdgYKCkO5+rJWnHjh1q0qSJvL29lTdvXj3zzDO6cuWKTZ2vv/5aFSpUkKenp4KDg9W3b1+b18+fP6+HHnpIPj4+KlOmjObOnZu5jQZyCJJy4C6QK1cu5cqVS3PmzFFUVFSi1zdu3ChJmjx5sk6dOmV9vnDhQnXt2lX9+vXT7t279cUXX2jKlCl67733bLZ/66239PDDD2v79u3q2rWrOnXqpD179kiSPvnkE82dO1c//vij9u3bp6lTp9ok/QAAIPOldK6+du2aWrVqpcDAQG3cuFE//fSTlixZYpN0T5gwQX369NEzzzyjHTt2aO7cuSpdurTNewwbNkyPPfaY/vrrL7Vp00ZdunRhRB6QGgbAXWHmzJkmMDDQeHl5mXr16pnBgweb7du3W1+XZH7++Webbe6//34zYsQIm7LvvvvOBAcH22z37LPP2tSpXbu2ee6554wxxrzwwgumSZMmJj4+3s4tAgAAxhjTvXt34+rqanx9fW0ew4cPN8bc+Vw9ceJEExgYaK5cuWJ9ff78+cbFxcWcPn3aGGNMSEiIeeONN5KNQZJ58803rc+vXLliLBaL+f333+3WTiCn4p5y4C7x8MMPq23btlq1apXWrl2rBQsWaPTo0fryyy/Vo0ePJLfZvHmzNm7caNMzHhcXpxs3bujatWvy8fGRJNWtW9dmu7p162rbtm2Sbk4+07x5c5UrV06tWrVSu3bt1KJFi0xpIwAAd6vGjRtrwoQJNmV58uSx/julc/WePXtUpUoV+fr6Wl+vX7++4uPjtW/fPlksFp08eVJNmzZNMYbKlStb/+3r6ys/Pz+dPXs2vU0C7hok5cBdxMvLS82bN1fz5s319ttv66mnntKQIUOSTcrj4+M1bNgwdezYMcl9pcRisUiSqlevrsOHD+v333/XkiVL9Nhjj6lZs2aaOXNmhtsDAABu8vX1TTSc/E4SztXGGOu/k6rj7e2dqv25u7sn2jY+Pj5NMQF3I+4pB+5i5cuX19WrVyXdPJHGxcXZvF69enXt27dPpUuXTvRwcfnv62PdunU2261bt06hoaHW5/7+/vrf//6nSZMmacaMGZo1axb3mAEAkIVSOleXL19e27Zts14TSNKff/4pFxcXlS1bVv9v145d4Y/jOI4/75C6XFEXd+nKRgYZXAZ2KUVxk3QlSupSMhk4srji+wfIeKVusEgMRotMLErKqGSykOFsV1e/LL/y+f14PsbP8P68P9/l3av3N5lM0tPTw8XFxbf2LP0WbsqlX+Dl5YV8Ps/8/DwDAwMkk0mur68pl8tMTk4C1IftyMgIra2tdHR0sLGxwcTEBNlslnw+Tzwe5+bmhtvbW3Z2dur1q9UqQ0NDjI6OUqlUuLq64vDwEIAoishkMgwODhKPx6lWq6TTadrb20N8CkmSfqT393eenp4azpqbm0mlUsDXs3p2dpbNzU0KhQKlUonn52eKxSJzc3N0dXUBUCqVWFpaorOzk/HxcV5fX7m8vKRYLH7vQ6UfyFAu/QJtbW0MDw8TRREPDw98fHyQzWZZXFxkfX0dgL29PVZXVzk4OKC7u5vHx0fGxsY4OTlhe3ubcrlMS0sLfX19LCwsNNTf2tri6OiI5eVl0uk0lUqF/v7++t27u7vc39/T1NRELpfj9PS0YdMuSZL+ztnZGZlMpuGst7eXu7s74OtZnUgkOD8/Z2VlhVwuRyKRYHp6mv39/XqtQqHA29sbURSxtrZGKpViZmbm+x4o/WCxWq1WC92EpP9XLBbj+PiYqamp0K1IkqQ/cFZL/zZXVZIkSZIkBWIolyRJkiQpEH9flyRJkiQpEDflkiRJkiQFYiiXJEmSJCkQQ7kkSZIkSYEYyiVJkiRJCsRQLkmSJElSIIZySZIkSZICMZRLkiRJkhSIoVySJEmSpEA+AU6umsan+IufAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#read_list_from_file(label,path_to_save_folder)\n",
    "from src.helper import read_list_from_file\n",
    "all_losses = read_list_from_file(\"0_final/dropout/1_droupot_model_step_losses\",path_to_save_folder)\n",
    "\n",
    "\n",
    "from src.plot import plot_two\n",
    "plot_two(all_losses,\"Loss For all Steps\",train_losses,\"la2\",axLabel1=(\"Steps\",\"Loss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930c125",
   "metadata": {},
   "source": [
    "### 4.3. Evaluating the dropout model\n",
    "\n",
    "Now, we'll compute the perplexity of our modified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39b8da71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized Model Perplexity: 52.23\n"
     ]
    }
   ],
   "source": [
    "from src.train import evaluate\n",
    "path_to_model = path_to_save_folder +\"/\"+\"0_final/dropout/1_droupot_model_model\"\n",
    "model.load_state_dict(torch.load(path_to_model, weights_only=True))\n",
    "\n",
    "perplexity_regularized = evaluate(model, dev_dataloader,criterion,device,vocab_size)\n",
    "print(f\"Regularized Model Perplexity: {perplexity_regularized:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f9fae",
   "metadata": {},
   "source": [
    "## 5. Improving the Model\n",
    "\n",
    "Now, try to further improve the model. For example, you could:\n",
    "- Increase the model depth.\n",
    "- Increase the embedding dimension.\n",
    "- Introduce non-linear activation functions.\n",
    "- Adjust the `context_length`.\n",
    "- Adjust the parameters of the optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac05d28",
   "metadata": {},
   "source": [
    "## 6. Generating text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e3d78c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto es  su papelisia de mayo con lo largo muchas anunci\n",
      "Esto es   PNR. En febre O principal almas positivas que Pner\n",
      "Esto es  Herb Deanroteen ser organizó su últimismo se constit\n",
      "Esto es izó en el momento de Novela decepoca. Adaptó a\n",
      "Esto es  tensos Mark Jenkins era su rojo ubicidad de las actuación\n",
      "Esto es izando las puede soldado no a las acces enl I\n",
      "Esto es  mero quéndiceímite de 10 a cocina en la e\n",
      "Esto es . Energicionales la cualquir Talces es Nuest\n",
      "Esto es  propato sobre toda. Hay ense mismo era un hogar\n",
      "Esto es  en voluntar por ordencias ese encuideradas a rec\n",
      "\n",
      "\n",
      "New Beginning\n",
      "A los artistas se les infra en el compromiso a 000 como un curios del\n",
      "A los artistas se les hablico El material había sido que pá\n",
      "A los artistas se les pastidad ausión que se vida tarde fí\n",
      "A los artistas se lesiones puesta y la Facultades para propiedades finan\n",
      "A los artistas se les tien lomo matrimon sufrir los pasarici\n",
      "A los artistas se les élbumes de mano respalda con los cerr\n",
      "A los artistas se les infancia llamaneses de oficial y dicho que\n",
      "A los artistas se les españos otros esta esta durección\n",
      "A los artistas se les prá Orficiembre la a que k durante to\n",
      "A los artistas se les perros y la libro en conjunto La 2017 pasada\n"
     ]
    }
   ],
   "source": [
    "#def generate_text(model, tokenizer, start_text, context_length=15, temperature=1.0):\n",
    "from src.model import generate_text\n",
    "\n",
    "\n",
    "start_text = \"Esto es \"\n",
    "\n",
    "for x in range(10):\n",
    "    generated_text = generate_text(model, tokenizer, start_text, device=device, context_length=20)\n",
    "    print(generated_text)\n",
    "\n",
    "start_text = \"A los artistas se les\"\n",
    "print(\"\\n\\nNew Beginning\")\n",
    "\n",
    "for x in range(10):\n",
    "    generated_text = generate_text(model, tokenizer, start_text, device=device, context_length=20)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2f373-1fd7-4274-ac33-57bc220c513a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Extra Trainining Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cebfd1b-9d09-462b-9333-fcdad84adaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainComplete import TrainComplete\n",
    "trainclass = TrainComplete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29f4ecda-de52-484c-b074-93a5924e6126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create Dataset 1220000 / 1221617Epoch [1/5], Step [0/30540], Loss: 10.8486\n",
      "Validation perplexity: 47961.72177241704\n",
      "Epoch [1/5], Step [75/30540], Loss: 6.7554\n",
      "Epoch [1/5], Step [150/30540], Loss: 6.2882\n",
      "Epoch [1/5], Step [225/30540], Loss: 5.9785\n",
      "Epoch [1/5], Step [300/30540], Loss: 5.6980\n",
      "Epoch [1/5], Step [375/30540], Loss: 5.4392\n",
      "Epoch [1/5], Step [450/30540], Loss: 5.4715\n",
      "Epoch [1/5], Step [525/30540], Loss: 5.2506\n",
      "Epoch [1/5], Step [600/30540], Loss: 5.0214\n",
      "Epoch [1/5], Step [675/30540], Loss: 5.2598\n",
      "Epoch [1/5], Step [750/30540], Loss: 4.8799\n",
      "Epoch [1/5], Step [825/30540], Loss: 4.9024\n",
      "Epoch [1/5], Step [900/30540], Loss: 5.0711\n",
      "Epoch [1/5], Step [975/30540], Loss: 4.9747\n",
      "Epoch [1/5], Step [1050/30540], Loss: 4.6437\n",
      "Epoch [1/5], Step [1125/30540], Loss: 4.7919\n",
      "Epoch [1/5], Step [1200/30540], Loss: 4.9404\n",
      "Epoch [1/5], Step [1275/30540], Loss: 4.7859\n",
      "Epoch [1/5], Step [1350/30540], Loss: 4.7665\n",
      "Epoch [1/5], Step [1425/30540], Loss: 4.5975\n",
      "Epoch [1/5], Step [1500/30540], Loss: 4.7668\n",
      "Epoch [1/5], Step [1575/30540], Loss: 4.7378\n",
      "Epoch [1/5], Step [1650/30540], Loss: 4.8283\n",
      "Epoch [1/5], Step [1725/30540], Loss: 4.5022\n",
      "Epoch [1/5], Step [1800/30540], Loss: 4.5344\n",
      "Epoch [1/5], Step [1875/30540], Loss: 4.5501\n",
      "Epoch [1/5], Step [1950/30540], Loss: 4.5415\n",
      "Epoch [1/5], Step [2025/30540], Loss: 4.6025\n",
      "Epoch [1/5], Step [2100/30540], Loss: 4.7163\n",
      "Epoch [1/5], Step [2175/30540], Loss: 4.7408\n",
      "Epoch [1/5], Step [2250/30540], Loss: 4.3988\n",
      "Epoch [1/5], Step [2325/30540], Loss: 4.5045\n",
      "Epoch [1/5], Step [2400/30540], Loss: 4.6401\n",
      "Epoch [1/5], Step [2475/30540], Loss: 4.4219\n",
      "Epoch [1/5], Step [2550/30540], Loss: 4.5802\n",
      "Epoch [1/5], Step [2625/30540], Loss: 4.5359\n",
      "Epoch [1/5], Step [2700/30540], Loss: 4.4727\n",
      "Epoch [1/5], Step [2775/30540], Loss: 4.3244\n",
      "Epoch [1/5], Step [2850/30540], Loss: 4.4561\n",
      "Epoch [1/5], Step [2925/30540], Loss: 4.4640\n",
      "Epoch [1/5], Step [3000/30540], Loss: 4.5842\n",
      "Validation perplexity: 78.63625060382132\n",
      "Epoch [1/5], Step [3075/30540], Loss: 4.4830\n",
      "Epoch [1/5], Step [3150/30540], Loss: 4.4169\n",
      "Epoch [1/5], Step [3225/30540], Loss: 4.4719\n",
      "Epoch [1/5], Step [3300/30540], Loss: 4.5184\n",
      "Epoch [1/5], Step [3375/30540], Loss: 4.1311\n",
      "Epoch [1/5], Step [3450/30540], Loss: 4.2358\n",
      "Epoch [1/5], Step [3525/30540], Loss: 4.6961\n",
      "Epoch [1/5], Step [3600/30540], Loss: 4.5231\n",
      "Epoch [1/5], Step [3675/30540], Loss: 4.5118\n",
      "Epoch [1/5], Step [3750/30540], Loss: 4.3829\n",
      "Epoch [1/5], Step [3825/30540], Loss: 4.4081\n",
      "Epoch [1/5], Step [3900/30540], Loss: 4.3828\n",
      "Epoch [1/5], Step [3975/30540], Loss: 4.5145\n",
      "Epoch [1/5], Step [4050/30540], Loss: 4.5135\n",
      "Epoch [1/5], Step [4125/30540], Loss: 4.3207\n",
      "Epoch [1/5], Step [4200/30540], Loss: 4.4895\n",
      "Epoch [1/5], Step [4275/30540], Loss: 4.2824\n",
      "Epoch [1/5], Step [4350/30540], Loss: 4.4083\n",
      "Epoch [1/5], Step [4425/30540], Loss: 4.2590\n",
      "Epoch [1/5], Step [4500/30540], Loss: 4.3214\n",
      "Epoch [1/5], Step [4575/30540], Loss: 4.2779\n",
      "Epoch [1/5], Step [4650/30540], Loss: 4.2129\n",
      "Epoch [1/5], Step [4725/30540], Loss: 4.2664\n",
      "Epoch [1/5], Step [4800/30540], Loss: 4.2774\n",
      "Epoch [1/5], Step [4875/30540], Loss: 4.4158\n",
      "Epoch [1/5], Step [4950/30540], Loss: 4.3938\n",
      "Epoch [1/5], Step [5025/30540], Loss: 4.3517\n",
      "Epoch [1/5], Step [5100/30540], Loss: 4.3179\n",
      "Epoch [1/5], Step [5175/30540], Loss: 4.0885\n",
      "Epoch [1/5], Step [5250/30540], Loss: 4.2104\n",
      "Epoch [1/5], Step [5325/30540], Loss: 4.2168\n",
      "Epoch [1/5], Step [5400/30540], Loss: 4.2808\n",
      "Epoch [1/5], Step [5475/30540], Loss: 4.3159\n",
      "Epoch [1/5], Step [5550/30540], Loss: 4.3561\n",
      "Epoch [1/5], Step [5625/30540], Loss: 4.2964\n",
      "Epoch [1/5], Step [5700/30540], Loss: 4.3487\n",
      "Epoch [1/5], Step [5775/30540], Loss: 4.3863\n",
      "Epoch [1/5], Step [5850/30540], Loss: 4.2851\n",
      "Epoch [1/5], Step [5925/30540], Loss: 4.1326\n",
      "Epoch [1/5], Step [6000/30540], Loss: 4.2122\n",
      "Validation perplexity: 63.61911358327319\n",
      "Epoch [1/5], Step [6075/30540], Loss: 4.1988\n",
      "Epoch [1/5], Step [6150/30540], Loss: 4.2627\n",
      "Epoch [1/5], Step [6225/30540], Loss: 4.1930\n",
      "Epoch [1/5], Step [6300/30540], Loss: 4.2067\n",
      "Epoch [1/5], Step [6375/30540], Loss: 4.1727\n",
      "Epoch [1/5], Step [6450/30540], Loss: 4.2621\n",
      "Epoch [1/5], Step [6525/30540], Loss: 4.3229\n",
      "Epoch [1/5], Step [6600/30540], Loss: 4.1113\n",
      "Epoch [1/5], Step [6675/30540], Loss: 4.0910\n",
      "Epoch [1/5], Step [6750/30540], Loss: 4.2561\n",
      "Epoch [1/5], Step [6825/30540], Loss: 4.3002\n",
      "Epoch [1/5], Step [6900/30540], Loss: 4.2289\n",
      "Epoch [1/5], Step [6975/30540], Loss: 4.1316\n",
      "Epoch [1/5], Step [7050/30540], Loss: 4.1133\n",
      "Epoch [1/5], Step [7125/30540], Loss: 4.2527\n",
      "Epoch [1/5], Step [7200/30540], Loss: 4.2675\n",
      "Epoch [1/5], Step [7275/30540], Loss: 4.2086\n",
      "Epoch [1/5], Step [7350/30540], Loss: 4.2407\n",
      "Epoch [1/5], Step [7425/30540], Loss: 4.1050\n",
      "Epoch [1/5], Step [7500/30540], Loss: 4.0951\n",
      "Epoch [1/5], Step [7575/30540], Loss: 4.0814\n",
      "Epoch [1/5], Step [7650/30540], Loss: 4.0959\n",
      "Epoch [1/5], Step [7725/30540], Loss: 4.1837\n",
      "Epoch [1/5], Step [7800/30540], Loss: 4.2126\n",
      "Epoch [1/5], Step [7875/30540], Loss: 4.2054\n",
      "Epoch [1/5], Step [7950/30540], Loss: 4.1914\n",
      "Epoch [1/5], Step [8025/30540], Loss: 4.1449\n",
      "Epoch [1/5], Step [8100/30540], Loss: 4.1243\n",
      "Epoch [1/5], Step [8175/30540], Loss: 4.2013\n",
      "Epoch [1/5], Step [8250/30540], Loss: 4.1254\n",
      "Epoch [1/5], Step [8325/30540], Loss: 4.0640\n",
      "Epoch [1/5], Step [8400/30540], Loss: 4.1212\n",
      "Epoch [1/5], Step [8475/30540], Loss: 4.0541\n",
      "Epoch [1/5], Step [8550/30540], Loss: 4.2778\n",
      "Epoch [1/5], Step [8625/30540], Loss: 4.0243\n",
      "Epoch [1/5], Step [8700/30540], Loss: 4.1383\n",
      "Epoch [1/5], Step [8775/30540], Loss: 4.2596\n",
      "Epoch [1/5], Step [8850/30540], Loss: 3.9839\n",
      "Epoch [1/5], Step [8925/30540], Loss: 4.1703\n",
      "Epoch [1/5], Step [9000/30540], Loss: 4.1847\n",
      "Validation perplexity: 57.55657710188148\n",
      "Epoch [1/5], Step [9075/30540], Loss: 4.1727\n",
      "Epoch [1/5], Step [9150/30540], Loss: 4.1619\n",
      "Epoch [1/5], Step [9225/30540], Loss: 4.1701\n",
      "Epoch [1/5], Step [9300/30540], Loss: 4.1213\n",
      "Epoch [1/5], Step [9375/30540], Loss: 4.2886\n",
      "Epoch [1/5], Step [9450/30540], Loss: 4.1067\n",
      "Epoch [1/5], Step [9525/30540], Loss: 4.0746\n",
      "Epoch [1/5], Step [9600/30540], Loss: 3.9740\n",
      "Epoch [1/5], Step [9675/30540], Loss: 4.2513\n",
      "Epoch [1/5], Step [9750/30540], Loss: 4.0935\n",
      "Epoch [1/5], Step [9825/30540], Loss: 4.0810\n",
      "Epoch [1/5], Step [9900/30540], Loss: 4.0187\n",
      "Epoch [1/5], Step [9975/30540], Loss: 4.2342\n",
      "Epoch [1/5], Step [10050/30540], Loss: 4.1827\n",
      "Epoch [1/5], Step [10125/30540], Loss: 4.1065\n",
      "Epoch [1/5], Step [10200/30540], Loss: 4.2363\n",
      "Epoch [1/5], Step [10275/30540], Loss: 4.0123\n",
      "Epoch [1/5], Step [10350/30540], Loss: 4.1665\n",
      "Epoch [1/5], Step [10425/30540], Loss: 4.0563\n",
      "Epoch [1/5], Step [10500/30540], Loss: 4.1337\n",
      "Epoch [1/5], Step [10575/30540], Loss: 4.0365\n",
      "Epoch [1/5], Step [10650/30540], Loss: 4.0731\n",
      "Epoch [1/5], Step [10725/30540], Loss: 4.0081\n",
      "Epoch [1/5], Step [10800/30540], Loss: 3.9933\n",
      "Epoch [1/5], Step [10875/30540], Loss: 4.1165\n",
      "Epoch [1/5], Step [10950/30540], Loss: 4.0987\n",
      "Epoch [1/5], Step [11025/30540], Loss: 4.0565\n",
      "Epoch [1/5], Step [11100/30540], Loss: 4.2075\n",
      "Epoch [1/5], Step [11175/30540], Loss: 4.0927\n",
      "Epoch [1/5], Step [11250/30540], Loss: 4.0740\n",
      "Epoch [1/5], Step [11325/30540], Loss: 4.0286\n",
      "Epoch [1/5], Step [11400/30540], Loss: 3.8461\n",
      "Epoch [1/5], Step [11475/30540], Loss: 4.0961\n",
      "Epoch [1/5], Step [11550/30540], Loss: 4.1799\n",
      "Epoch [1/5], Step [11625/30540], Loss: 3.9884\n",
      "Epoch [1/5], Step [11700/30540], Loss: 3.9317\n",
      "Epoch [1/5], Step [11775/30540], Loss: 3.9965\n",
      "Epoch [1/5], Step [11850/30540], Loss: 4.0663\n",
      "Epoch [1/5], Step [11925/30540], Loss: 4.0272\n",
      "Epoch [1/5], Step [12000/30540], Loss: 3.9432\n",
      "Validation perplexity: 54.1164442151755\n",
      "Epoch [1/5], Step [12075/30540], Loss: 4.1144\n",
      "Epoch [1/5], Step [12150/30540], Loss: 4.0948\n",
      "Epoch [1/5], Step [12225/30540], Loss: 3.9974\n",
      "Epoch [1/5], Step [12300/30540], Loss: 4.2321\n",
      "Epoch [1/5], Step [12375/30540], Loss: 4.0506\n",
      "Epoch [1/5], Step [12450/30540], Loss: 4.0702\n",
      "Epoch [1/5], Step [12525/30540], Loss: 4.1426\n",
      "Epoch [1/5], Step [12600/30540], Loss: 3.8787\n",
      "Epoch [1/5], Step [12675/30540], Loss: 3.9175\n",
      "Epoch [1/5], Step [12750/30540], Loss: 3.9540\n",
      "Epoch [1/5], Step [12825/30540], Loss: 4.0337\n",
      "Epoch [1/5], Step [12900/30540], Loss: 4.0317\n",
      "Epoch [1/5], Step [12975/30540], Loss: 3.9981\n",
      "Epoch [1/5], Step [13050/30540], Loss: 3.9666\n",
      "Epoch [1/5], Step [13125/30540], Loss: 3.9776\n",
      "Epoch [1/5], Step [13200/30540], Loss: 4.1428\n",
      "Epoch [1/5], Step [13275/30540], Loss: 3.9714\n",
      "Epoch [1/5], Step [13350/30540], Loss: 4.0965\n",
      "Epoch [1/5], Step [13425/30540], Loss: 3.9390\n",
      "Epoch [1/5], Step [13500/30540], Loss: 4.1033\n",
      "Epoch [1/5], Step [13575/30540], Loss: 4.0214\n",
      "Epoch [1/5], Step [13650/30540], Loss: 4.2105\n",
      "Epoch [1/5], Step [13725/30540], Loss: 4.0457\n",
      "Epoch [1/5], Step [13800/30540], Loss: 3.9541\n",
      "Epoch [1/5], Step [13875/30540], Loss: 3.9586\n",
      "Epoch [1/5], Step [13950/30540], Loss: 4.3209\n",
      "Epoch [1/5], Step [14025/30540], Loss: 4.0801\n",
      "Epoch [1/5], Step [14100/30540], Loss: 3.9872\n",
      "Epoch [1/5], Step [14175/30540], Loss: 3.8877\n",
      "Epoch [1/5], Step [14250/30540], Loss: 3.9745\n",
      "Epoch [1/5], Step [14325/30540], Loss: 3.8966\n",
      "Epoch [1/5], Step [14400/30540], Loss: 3.9941\n",
      "Epoch [1/5], Step [14475/30540], Loss: 3.9653\n",
      "Epoch [1/5], Step [14550/30540], Loss: 4.0164\n",
      "Epoch [1/5], Step [14625/30540], Loss: 3.9604\n",
      "Epoch [1/5], Step [14700/30540], Loss: 3.8891\n",
      "Epoch [1/5], Step [14775/30540], Loss: 4.1143\n",
      "Epoch [1/5], Step [14850/30540], Loss: 4.0130\n",
      "Epoch [1/5], Step [14925/30540], Loss: 4.1177\n",
      "Epoch [1/5], Step [15000/30540], Loss: 4.0443\n",
      "Validation perplexity: 52.00156496724124\n",
      "Epoch [1/5], Step [15075/30540], Loss: 3.9616\n",
      "Epoch [1/5], Step [15150/30540], Loss: 4.0620\n",
      "Epoch [1/5], Step [15225/30540], Loss: 4.2087\n",
      "Epoch [1/5], Step [15300/30540], Loss: 4.0191\n",
      "Epoch [1/5], Step [15375/30540], Loss: 4.0642\n",
      "Epoch [1/5], Step [15450/30540], Loss: 4.0223\n",
      "Epoch [1/5], Step [15525/30540], Loss: 3.9740\n",
      "Epoch [1/5], Step [15600/30540], Loss: 4.1338\n",
      "Epoch [1/5], Step [15675/30540], Loss: 4.0465\n",
      "Epoch [1/5], Step [15750/30540], Loss: 4.0132\n",
      "Epoch [1/5], Step [15825/30540], Loss: 3.8507\n",
      "Epoch [1/5], Step [15900/30540], Loss: 3.9630\n",
      "Epoch [1/5], Step [15975/30540], Loss: 4.0654\n",
      "Epoch [1/5], Step [16050/30540], Loss: 4.1654\n",
      "Epoch [1/5], Step [16125/30540], Loss: 4.0646\n",
      "Epoch [1/5], Step [16200/30540], Loss: 4.0798\n",
      "Epoch [1/5], Step [16275/30540], Loss: 4.0063\n",
      "Epoch [1/5], Step [16350/30540], Loss: 3.8530\n",
      "Epoch [1/5], Step [16425/30540], Loss: 3.9639\n",
      "Epoch [1/5], Step [16500/30540], Loss: 4.0230\n",
      "Epoch [1/5], Step [16575/30540], Loss: 4.0223\n",
      "Epoch [1/5], Step [16650/30540], Loss: 3.9437\n",
      "Epoch [1/5], Step [16725/30540], Loss: 3.9469\n",
      "Epoch [1/5], Step [16800/30540], Loss: 4.0037\n",
      "Epoch [1/5], Step [16875/30540], Loss: 4.1294\n",
      "Epoch [1/5], Step [16950/30540], Loss: 4.0931\n",
      "Epoch [1/5], Step [17025/30540], Loss: 4.0284\n",
      "Epoch [1/5], Step [17100/30540], Loss: 4.0755\n",
      "Epoch [1/5], Step [17175/30540], Loss: 3.9245\n",
      "Epoch [1/5], Step [17250/30540], Loss: 4.1007\n",
      "Epoch [1/5], Step [17325/30540], Loss: 3.7964\n",
      "Epoch [1/5], Step [17400/30540], Loss: 3.9750\n",
      "Epoch [1/5], Step [17475/30540], Loss: 3.9245\n",
      "Epoch [1/5], Step [17550/30540], Loss: 3.9970\n",
      "Epoch [1/5], Step [17625/30540], Loss: 3.9088\n",
      "Epoch [1/5], Step [17700/30540], Loss: 4.0340\n",
      "Epoch [1/5], Step [17775/30540], Loss: 4.0399\n",
      "Epoch [1/5], Step [17850/30540], Loss: 4.0111\n",
      "Epoch [1/5], Step [17925/30540], Loss: 3.6666\n",
      "Epoch [1/5], Step [18000/30540], Loss: 4.0244\n",
      "Validation perplexity: 50.6085315743786\n",
      "Epoch [1/5], Step [18075/30540], Loss: 3.9832\n",
      "Epoch [1/5], Step [18150/30540], Loss: 3.9460\n",
      "Epoch [1/5], Step [18225/30540], Loss: 3.9800\n",
      "Epoch [1/5], Step [18300/30540], Loss: 3.8192\n",
      "Epoch [1/5], Step [18375/30540], Loss: 3.8375\n",
      "Epoch [1/5], Step [18450/30540], Loss: 3.9878\n",
      "Epoch [1/5], Step [18525/30540], Loss: 3.9600\n",
      "Epoch [1/5], Step [18600/30540], Loss: 4.0105\n",
      "Epoch [1/5], Step [18675/30540], Loss: 4.0361\n",
      "Epoch [1/5], Step [18750/30540], Loss: 3.8558\n",
      "Epoch [1/5], Step [18825/30540], Loss: 4.0131\n",
      "Epoch [1/5], Step [18900/30540], Loss: 4.0824\n",
      "Epoch [1/5], Step [18975/30540], Loss: 3.9688\n",
      "Epoch [1/5], Step [19050/30540], Loss: 4.0895\n",
      "Epoch [1/5], Step [19125/30540], Loss: 3.9763\n",
      "Epoch [1/5], Step [19200/30540], Loss: 3.9997\n",
      "Epoch [1/5], Step [19275/30540], Loss: 4.0280\n",
      "Epoch [1/5], Step [19350/30540], Loss: 3.7851\n",
      "Epoch [1/5], Step [19425/30540], Loss: 3.9938\n",
      "Epoch [1/5], Step [19500/30540], Loss: 3.9532\n",
      "Epoch [1/5], Step [19575/30540], Loss: 3.9240\n",
      "Epoch [1/5], Step [19650/30540], Loss: 4.1715\n",
      "Epoch [1/5], Step [19725/30540], Loss: 3.8845\n",
      "Epoch [1/5], Step [19800/30540], Loss: 3.8211\n",
      "Epoch [1/5], Step [19875/30540], Loss: 3.9006\n",
      "Epoch [1/5], Step [19950/30540], Loss: 4.0252\n",
      "Epoch [1/5], Step [20025/30540], Loss: 3.9628\n",
      "Epoch [1/5], Step [20100/30540], Loss: 3.8615\n",
      "Epoch [1/5], Step [20175/30540], Loss: 4.1467\n",
      "Epoch [1/5], Step [20250/30540], Loss: 4.0559\n",
      "Epoch [1/5], Step [20325/30540], Loss: 4.1949\n",
      "Epoch [1/5], Step [20400/30540], Loss: 3.8650\n",
      "Epoch [1/5], Step [20475/30540], Loss: 4.0678\n",
      "Epoch [1/5], Step [20550/30540], Loss: 3.9325\n",
      "Epoch [1/5], Step [20625/30540], Loss: 3.9403\n",
      "Epoch [1/5], Step [20700/30540], Loss: 4.0636\n",
      "Epoch [1/5], Step [20775/30540], Loss: 3.8697\n",
      "Epoch [1/5], Step [20850/30540], Loss: 4.0358\n",
      "Epoch [1/5], Step [20925/30540], Loss: 3.9400\n",
      "Epoch [1/5], Step [21000/30540], Loss: 3.9738\n",
      "Validation perplexity: 49.53720625545601\n",
      "Epoch [1/5], Step [21075/30540], Loss: 4.0205\n",
      "Epoch [1/5], Step [21150/30540], Loss: 3.9361\n",
      "Epoch [1/5], Step [21225/30540], Loss: 4.0333\n",
      "Epoch [1/5], Step [21300/30540], Loss: 3.9097\n",
      "Epoch [1/5], Step [21375/30540], Loss: 3.8396\n",
      "Epoch [1/5], Step [21450/30540], Loss: 3.9900\n",
      "Epoch [1/5], Step [21525/30540], Loss: 3.9582\n",
      "Epoch [1/5], Step [21600/30540], Loss: 4.0411\n",
      "Epoch [1/5], Step [21675/30540], Loss: 3.9057\n",
      "Epoch [1/5], Step [21750/30540], Loss: 3.8968\n",
      "Epoch [1/5], Step [21825/30540], Loss: 4.0060\n",
      "Epoch [1/5], Step [21900/30540], Loss: 4.0149\n",
      "Epoch [1/5], Step [21975/30540], Loss: 3.9235\n",
      "Epoch [1/5], Step [22050/30540], Loss: 3.9734\n",
      "Epoch [1/5], Step [22125/30540], Loss: 3.8937\n",
      "Epoch [1/5], Step [22200/30540], Loss: 3.8366\n",
      "Epoch [1/5], Step [22275/30540], Loss: 3.9746\n",
      "Epoch [1/5], Step [22350/30540], Loss: 3.8983\n",
      "Epoch [1/5], Step [22425/30540], Loss: 3.9618\n",
      "Epoch [1/5], Step [22500/30540], Loss: 3.8916\n",
      "Epoch [1/5], Step [22575/30540], Loss: 4.0008\n",
      "Epoch [1/5], Step [22650/30540], Loss: 3.9536\n",
      "Epoch [1/5], Step [22725/30540], Loss: 4.0760\n",
      "Epoch [1/5], Step [22800/30540], Loss: 4.0139\n",
      "Epoch [1/5], Step [22875/30540], Loss: 4.0346\n",
      "Epoch [1/5], Step [22950/30540], Loss: 3.8319\n",
      "Epoch [1/5], Step [23025/30540], Loss: 4.0320\n",
      "Epoch [1/5], Step [23100/30540], Loss: 3.8920\n",
      "Epoch [1/5], Step [23175/30540], Loss: 3.9052\n",
      "Epoch [1/5], Step [23250/30540], Loss: 4.0354\n",
      "Epoch [1/5], Step [23325/30540], Loss: 4.0864\n",
      "Epoch [1/5], Step [23400/30540], Loss: 4.0163\n",
      "Epoch [1/5], Step [23475/30540], Loss: 3.8987\n",
      "Epoch [1/5], Step [23550/30540], Loss: 4.0647\n",
      "Epoch [1/5], Step [23625/30540], Loss: 3.8139\n",
      "Epoch [1/5], Step [23700/30540], Loss: 3.9386\n",
      "Epoch [1/5], Step [23775/30540], Loss: 3.9965\n",
      "Epoch [1/5], Step [23850/30540], Loss: 3.9028\n",
      "Epoch [1/5], Step [23925/30540], Loss: 4.0485\n",
      "Epoch [1/5], Step [24000/30540], Loss: 4.1215\n",
      "Validation perplexity: 48.822973767657736\n",
      "Epoch [1/5], Step [24075/30540], Loss: 3.8862\n",
      "Epoch [1/5], Step [24150/30540], Loss: 4.0520\n",
      "Epoch [1/5], Step [24225/30540], Loss: 3.8536\n",
      "Epoch [1/5], Step [24300/30540], Loss: 3.8725\n",
      "Epoch [1/5], Step [24375/30540], Loss: 3.9278\n",
      "Epoch [1/5], Step [24450/30540], Loss: 3.9198\n",
      "Epoch [1/5], Step [24525/30540], Loss: 3.9747\n",
      "Epoch [1/5], Step [24600/30540], Loss: 3.9044\n",
      "Epoch [1/5], Step [24675/30540], Loss: 3.9501\n",
      "Epoch [1/5], Step [24750/30540], Loss: 4.0690\n",
      "Epoch [1/5], Step [24825/30540], Loss: 3.9261\n",
      "Epoch [1/5], Step [24900/30540], Loss: 3.9603\n",
      "Epoch [1/5], Step [24975/30540], Loss: 3.8433\n",
      "Epoch [1/5], Step [25050/30540], Loss: 3.9493\n",
      "Epoch [1/5], Step [25125/30540], Loss: 4.0050\n",
      "Epoch [1/5], Step [25200/30540], Loss: 3.8914\n",
      "Epoch [1/5], Step [25275/30540], Loss: 3.7958\n",
      "Epoch [1/5], Step [25350/30540], Loss: 3.8126\n",
      "Epoch [1/5], Step [25425/30540], Loss: 4.0629\n",
      "Epoch [1/5], Step [25500/30540], Loss: 3.9891\n",
      "Epoch [1/5], Step [25575/30540], Loss: 4.0537\n",
      "Epoch [1/5], Step [25650/30540], Loss: 3.9845\n",
      "Epoch [1/5], Step [25725/30540], Loss: 3.8705\n",
      "Epoch [1/5], Step [25800/30540], Loss: 3.9423\n",
      "Epoch [1/5], Step [25875/30540], Loss: 4.0182\n",
      "Epoch [1/5], Step [25950/30540], Loss: 3.9298\n",
      "Epoch [1/5], Step [26025/30540], Loss: 3.9031\n",
      "Epoch [1/5], Step [26100/30540], Loss: 3.9297\n",
      "Epoch [1/5], Step [26175/30540], Loss: 3.9064\n",
      "Epoch [1/5], Step [26250/30540], Loss: 3.8601\n",
      "Epoch [1/5], Step [26325/30540], Loss: 3.9345\n",
      "Epoch [1/5], Step [26400/30540], Loss: 3.9698\n",
      "Epoch [1/5], Step [26475/30540], Loss: 3.9145\n",
      "Epoch [1/5], Step [26550/30540], Loss: 3.9901\n",
      "Epoch [1/5], Step [26625/30540], Loss: 4.0456\n",
      "Epoch [1/5], Step [26700/30540], Loss: 3.9911\n",
      "Epoch [1/5], Step [26775/30540], Loss: 3.8445\n",
      "Epoch [1/5], Step [26850/30540], Loss: 3.9946\n",
      "Epoch [1/5], Step [26925/30540], Loss: 4.0249\n",
      "Epoch [1/5], Step [27000/30540], Loss: 3.9080\n",
      "Validation perplexity: 48.114026626340525\n",
      "Epoch [1/5], Step [27075/30540], Loss: 3.9568\n",
      "Epoch [1/5], Step [27150/30540], Loss: 3.9890\n",
      "Epoch [1/5], Step [27225/30540], Loss: 3.8291\n",
      "Epoch [1/5], Step [27300/30540], Loss: 3.8097\n",
      "Epoch [1/5], Step [27375/30540], Loss: 4.0600\n",
      "Epoch [1/5], Step [27450/30540], Loss: 4.0649\n",
      "Epoch [1/5], Step [27525/30540], Loss: 3.8189\n",
      "Epoch [1/5], Step [27600/30540], Loss: 4.1012\n",
      "Epoch [1/5], Step [27675/30540], Loss: 4.1259\n",
      "Epoch [1/5], Step [27750/30540], Loss: 3.9005\n",
      "Epoch [1/5], Step [27825/30540], Loss: 3.8797\n",
      "Epoch [1/5], Step [27900/30540], Loss: 3.9967\n",
      "Epoch [1/5], Step [27975/30540], Loss: 3.9802\n",
      "Epoch [1/5], Step [28050/30540], Loss: 3.9294\n",
      "Epoch [1/5], Step [28125/30540], Loss: 3.9101\n",
      "Epoch [1/5], Step [28200/30540], Loss: 4.0819\n",
      "Epoch [1/5], Step [28275/30540], Loss: 3.8581\n",
      "Epoch [1/5], Step [28350/30540], Loss: 3.9832\n",
      "Epoch [1/5], Step [28425/30540], Loss: 3.9723\n",
      "Epoch [1/5], Step [28500/30540], Loss: 3.8512\n",
      "Epoch [1/5], Step [28575/30540], Loss: 4.0713\n",
      "Epoch [1/5], Step [28650/30540], Loss: 3.8435\n",
      "Epoch [1/5], Step [28725/30540], Loss: 4.0745\n",
      "Epoch [1/5], Step [28800/30540], Loss: 3.9696\n",
      "Epoch [1/5], Step [28875/30540], Loss: 3.9953\n",
      "Epoch [1/5], Step [28950/30540], Loss: 3.9453\n",
      "Epoch [1/5], Step [29025/30540], Loss: 3.8154\n",
      "Epoch [1/5], Step [29100/30540], Loss: 3.9884\n",
      "Epoch [1/5], Step [29175/30540], Loss: 3.8783\n",
      "Epoch [1/5], Step [29250/30540], Loss: 3.8697\n",
      "Epoch [1/5], Step [29325/30540], Loss: 3.9650\n",
      "Epoch [1/5], Step [29400/30540], Loss: 4.0789\n",
      "Epoch [1/5], Step [29475/30540], Loss: 3.9194\n",
      "Epoch [1/5], Step [29550/30540], Loss: 3.9011\n",
      "Epoch [1/5], Step [29625/30540], Loss: 3.9213\n",
      "Epoch [1/5], Step [29700/30540], Loss: 3.9387\n",
      "Epoch [1/5], Step [29775/30540], Loss: 3.8720\n",
      "Epoch [1/5], Step [29850/30540], Loss: 3.9180\n",
      "Epoch [1/5], Step [29925/30540], Loss: 3.7912\n",
      "Epoch [1/5], Step [30000/30540], Loss: 3.9362\n",
      "Validation perplexity: 47.50929608367896\n",
      "Epoch [1/5], Step [30075/30540], Loss: 4.0229\n",
      "Epoch [1/5], Step [30150/30540], Loss: 3.8665\n",
      "Epoch [1/5], Step [30225/30540], Loss: 3.9554\n",
      "Epoch [1/5], Step [30300/30540], Loss: 3.8991\n",
      "Epoch [1/5], Step [30375/30540], Loss: 3.8407\n",
      "Epoch [1/5], Step [30450/30540], Loss: 3.8933\n",
      "Epoch [1/5], Step [30525/30540], Loss: 3.8821\n",
      "Epoch [1/5] Average Loss: 4.1435, Perplexity: 63.02\n",
      "Epoch [2/5], Step [0/30540], Loss: 4.0750\n",
      "Validation perplexity: 47.448068157347095\n",
      "Epoch [2/5], Step [75/30540], Loss: 3.8565\n",
      "Epoch [2/5], Step [150/30540], Loss: 3.7694\n",
      "Epoch [2/5], Step [225/30540], Loss: 3.7969\n",
      "Epoch [2/5], Step [300/30540], Loss: 3.9632\n",
      "Epoch [2/5], Step [375/30540], Loss: 3.9102\n",
      "Epoch [2/5], Step [450/30540], Loss: 3.7699\n",
      "Epoch [2/5], Step [525/30540], Loss: 3.9695\n",
      "Epoch [2/5], Step [600/30540], Loss: 4.0245\n",
      "Epoch [2/5], Step [675/30540], Loss: 3.8730\n",
      "Epoch [2/5], Step [750/30540], Loss: 3.8682\n",
      "Epoch [2/5], Step [825/30540], Loss: 3.8494\n",
      "Epoch [2/5], Step [900/30540], Loss: 3.9726\n",
      "Epoch [2/5], Step [975/30540], Loss: 3.9057\n",
      "Epoch [2/5], Step [1050/30540], Loss: 3.9112\n",
      "Epoch [2/5], Step [1125/30540], Loss: 4.0148\n",
      "Epoch [2/5], Step [1200/30540], Loss: 4.0026\n",
      "Epoch [2/5], Step [1275/30540], Loss: 3.8992\n",
      "Epoch [2/5], Step [1350/30540], Loss: 3.9389\n",
      "Epoch [2/5], Step [1425/30540], Loss: 3.9253\n",
      "Epoch [2/5], Step [1500/30540], Loss: 3.8521\n",
      "Epoch [2/5], Step [1575/30540], Loss: 3.8365\n",
      "Epoch [2/5], Step [1650/30540], Loss: 3.8825\n",
      "Epoch [2/5], Step [1725/30540], Loss: 4.0884\n",
      "Epoch [2/5], Step [1800/30540], Loss: 3.8112\n",
      "Epoch [2/5], Step [1875/30540], Loss: 3.9419\n",
      "Epoch [2/5], Step [1950/30540], Loss: 3.8199\n",
      "Epoch [2/5], Step [2025/30540], Loss: 3.9046\n",
      "Epoch [2/5], Step [2100/30540], Loss: 3.9257\n",
      "Epoch [2/5], Step [2175/30540], Loss: 3.9502\n",
      "Epoch [2/5], Step [2250/30540], Loss: 3.7773\n",
      "Epoch [2/5], Step [2325/30540], Loss: 3.9521\n",
      "Epoch [2/5], Step [2400/30540], Loss: 3.9807\n",
      "Epoch [2/5], Step [2475/30540], Loss: 3.9106\n",
      "Epoch [2/5], Step [2550/30540], Loss: 3.8145\n",
      "Epoch [2/5], Step [2625/30540], Loss: 3.8822\n",
      "Epoch [2/5], Step [2700/30540], Loss: 3.9546\n",
      "Epoch [2/5], Step [2775/30540], Loss: 3.8132\n",
      "Epoch [2/5], Step [2850/30540], Loss: 3.8838\n",
      "Epoch [2/5], Step [2925/30540], Loss: 3.9220\n",
      "Epoch [2/5], Step [3000/30540], Loss: 3.9775\n",
      "Validation perplexity: 47.18259889659428\n",
      "Epoch [2/5], Step [3075/30540], Loss: 3.9144\n",
      "Epoch [2/5], Step [3150/30540], Loss: 3.8899\n",
      "Epoch [2/5], Step [3225/30540], Loss: 3.8791\n",
      "Epoch [2/5], Step [3300/30540], Loss: 3.8924\n",
      "Epoch [2/5], Step [3375/30540], Loss: 3.9740\n",
      "Epoch [2/5], Step [3450/30540], Loss: 3.8626\n",
      "Epoch [2/5], Step [3525/30540], Loss: 3.8570\n",
      "Epoch [2/5], Step [3600/30540], Loss: 3.8283\n",
      "Epoch [2/5], Step [3675/30540], Loss: 4.0115\n",
      "Epoch [2/5], Step [3750/30540], Loss: 3.8804\n",
      "Epoch [2/5], Step [3825/30540], Loss: 3.9091\n",
      "Epoch [2/5], Step [3900/30540], Loss: 3.9188\n",
      "Epoch [2/5], Step [3975/30540], Loss: 3.8552\n",
      "Epoch [2/5], Step [4050/30540], Loss: 3.9309\n",
      "Epoch [2/5], Step [4125/30540], Loss: 3.8965\n",
      "Epoch [2/5], Step [4200/30540], Loss: 3.9590\n",
      "Epoch [2/5], Step [4275/30540], Loss: 3.9405\n",
      "Epoch [2/5], Step [4350/30540], Loss: 3.8898\n",
      "Epoch [2/5], Step [4425/30540], Loss: 3.8196\n",
      "Epoch [2/5], Step [4500/30540], Loss: 3.9492\n",
      "Epoch [2/5], Step [4575/30540], Loss: 3.9966\n",
      "Epoch [2/5], Step [4650/30540], Loss: 3.7674\n",
      "Epoch [2/5], Step [4725/30540], Loss: 3.9450\n",
      "Epoch [2/5], Step [4800/30540], Loss: 3.8769\n",
      "Epoch [2/5], Step [4875/30540], Loss: 3.9411\n",
      "Epoch [2/5], Step [4950/30540], Loss: 3.8217\n",
      "Epoch [2/5], Step [5025/30540], Loss: 3.7497\n",
      "Epoch [2/5], Step [5100/30540], Loss: 3.9068\n",
      "Epoch [2/5], Step [5175/30540], Loss: 3.8409\n",
      "Epoch [2/5], Step [5250/30540], Loss: 3.9413\n",
      "Epoch [2/5], Step [5325/30540], Loss: 3.9588\n",
      "Epoch [2/5], Step [5400/30540], Loss: 3.9791\n",
      "Epoch [2/5], Step [5475/30540], Loss: 3.9936\n",
      "Epoch [2/5], Step [5550/30540], Loss: 4.0515\n",
      "Epoch [2/5], Step [5625/30540], Loss: 3.8357\n",
      "Epoch [2/5], Step [5700/30540], Loss: 3.8558\n",
      "Epoch [2/5], Step [5775/30540], Loss: 3.9610\n",
      "Epoch [2/5], Step [5850/30540], Loss: 3.8587\n",
      "Epoch [2/5], Step [5925/30540], Loss: 3.8098\n",
      "Epoch [2/5], Step [6000/30540], Loss: 3.8931\n",
      "Validation perplexity: 46.648862184356446\n",
      "Epoch [2/5], Step [6075/30540], Loss: 3.9549\n",
      "Epoch [2/5], Step [6150/30540], Loss: 3.9877\n",
      "Epoch [2/5], Step [6225/30540], Loss: 3.9518\n",
      "Epoch [2/5], Step [6300/30540], Loss: 4.0032\n",
      "Epoch [2/5], Step [6375/30540], Loss: 3.8808\n",
      "Epoch [2/5], Step [6450/30540], Loss: 3.9463\n",
      "Epoch [2/5], Step [6525/30540], Loss: 4.0330\n",
      "Epoch [2/5], Step [6600/30540], Loss: 3.8614\n",
      "Epoch [2/5], Step [6675/30540], Loss: 3.9187\n",
      "Epoch [2/5], Step [6750/30540], Loss: 3.7991\n",
      "Epoch [2/5], Step [6825/30540], Loss: 3.9629\n",
      "Epoch [2/5], Step [6900/30540], Loss: 4.0280\n",
      "Epoch [2/5], Step [6975/30540], Loss: 3.9370\n",
      "Epoch [2/5], Step [7050/30540], Loss: 3.9158\n",
      "Epoch [2/5], Step [7125/30540], Loss: 3.8891\n",
      "Epoch [2/5], Step [7200/30540], Loss: 3.9374\n",
      "Epoch [2/5], Step [7275/30540], Loss: 3.9494\n",
      "Epoch [2/5], Step [7350/30540], Loss: 3.9293\n",
      "Epoch [2/5], Step [7425/30540], Loss: 3.9385\n",
      "Epoch [2/5], Step [7500/30540], Loss: 3.8541\n",
      "Epoch [2/5], Step [7575/30540], Loss: 3.9419\n",
      "Epoch [2/5], Step [7650/30540], Loss: 3.7389\n",
      "Epoch [2/5], Step [7725/30540], Loss: 3.9808\n",
      "Epoch [2/5], Step [7800/30540], Loss: 3.8885\n",
      "Epoch [2/5], Step [7875/30540], Loss: 3.8298\n",
      "Epoch [2/5], Step [7950/30540], Loss: 3.9120\n",
      "Epoch [2/5], Step [8025/30540], Loss: 3.8475\n",
      "Epoch [2/5], Step [8100/30540], Loss: 3.9476\n",
      "Epoch [2/5], Step [8175/30540], Loss: 3.8733\n",
      "Epoch [2/5], Step [8250/30540], Loss: 4.0103\n",
      "Epoch [2/5], Step [8325/30540], Loss: 4.0418\n",
      "Epoch [2/5], Step [8400/30540], Loss: 4.0247\n",
      "Epoch [2/5], Step [8475/30540], Loss: 3.7817\n",
      "Epoch [2/5], Step [8550/30540], Loss: 3.9036\n",
      "Epoch [2/5], Step [8625/30540], Loss: 3.8609\n",
      "Epoch [2/5], Step [8700/30540], Loss: 3.8955\n",
      "Epoch [2/5], Step [8775/30540], Loss: 3.8714\n",
      "Epoch [2/5], Step [8850/30540], Loss: 3.9944\n",
      "Epoch [2/5], Step [8925/30540], Loss: 3.9910\n",
      "Epoch [2/5], Step [9000/30540], Loss: 3.8137\n",
      "Validation perplexity: 46.34148303083371\n",
      "Epoch [2/5], Step [9075/30540], Loss: 3.9593\n",
      "Epoch [2/5], Step [9150/30540], Loss: 3.9157\n",
      "Epoch [2/5], Step [9225/30540], Loss: 3.8206\n",
      "Epoch [2/5], Step [9300/30540], Loss: 3.9337\n",
      "Epoch [2/5], Step [9375/30540], Loss: 3.8087\n",
      "Epoch [2/5], Step [9450/30540], Loss: 3.6380\n",
      "Epoch [2/5], Step [9525/30540], Loss: 3.8692\n",
      "Epoch [2/5], Step [9600/30540], Loss: 3.8201\n",
      "Epoch [2/5], Step [9675/30540], Loss: 3.9950\n",
      "Epoch [2/5], Step [9750/30540], Loss: 3.9652\n",
      "Epoch [2/5], Step [9825/30540], Loss: 3.9208\n",
      "Epoch [2/5], Step [9900/30540], Loss: 3.9560\n",
      "Epoch [2/5], Step [9975/30540], Loss: 3.9669\n",
      "Epoch [2/5], Step [10050/30540], Loss: 3.8542\n",
      "Epoch [2/5], Step [10125/30540], Loss: 3.8836\n",
      "Epoch [2/5], Step [10200/30540], Loss: 3.8216\n",
      "Epoch [2/5], Step [10275/30540], Loss: 3.7790\n",
      "Epoch [2/5], Step [10350/30540], Loss: 3.8515\n",
      "Epoch [2/5], Step [10425/30540], Loss: 3.9151\n",
      "Epoch [2/5], Step [10500/30540], Loss: 3.8769\n",
      "Epoch [2/5], Step [10575/30540], Loss: 3.8424\n",
      "Epoch [2/5], Step [10650/30540], Loss: 4.0077\n",
      "Epoch [2/5], Step [10725/30540], Loss: 3.9900\n",
      "Epoch [2/5], Step [10800/30540], Loss: 3.9055\n",
      "Epoch [2/5], Step [10875/30540], Loss: 3.8491\n",
      "Epoch [2/5], Step [10950/30540], Loss: 3.8536\n",
      "Epoch [2/5], Step [11025/30540], Loss: 3.7703\n",
      "Epoch [2/5], Step [11100/30540], Loss: 3.8929\n",
      "Epoch [2/5], Step [11175/30540], Loss: 3.8951\n",
      "Epoch [2/5], Step [11250/30540], Loss: 3.9097\n",
      "Epoch [2/5], Step [11325/30540], Loss: 3.8624\n",
      "Epoch [2/5], Step [11400/30540], Loss: 3.9306\n",
      "Epoch [2/5], Step [11475/30540], Loss: 3.9613\n",
      "Epoch [2/5], Step [11550/30540], Loss: 3.8524\n",
      "Epoch [2/5], Step [11625/30540], Loss: 3.8611\n",
      "Epoch [2/5], Step [11700/30540], Loss: 3.8334\n",
      "Epoch [2/5], Step [11775/30540], Loss: 3.9236\n",
      "Epoch [2/5], Step [11850/30540], Loss: 3.8839\n",
      "Epoch [2/5], Step [11925/30540], Loss: 3.9250\n",
      "Epoch [2/5], Step [12000/30540], Loss: 3.8700\n",
      "Validation perplexity: 46.041735472519996\n",
      "Epoch [2/5], Step [12075/30540], Loss: 3.8450\n",
      "Epoch [2/5], Step [12150/30540], Loss: 3.8847\n",
      "Epoch [2/5], Step [12225/30540], Loss: 3.9928\n",
      "Epoch [2/5], Step [12300/30540], Loss: 3.8741\n",
      "Epoch [2/5], Step [12375/30540], Loss: 3.9211\n",
      "Epoch [2/5], Step [12450/30540], Loss: 3.9207\n",
      "Epoch [2/5], Step [12525/30540], Loss: 3.9547\n",
      "Epoch [2/5], Step [12600/30540], Loss: 3.8987\n",
      "Epoch [2/5], Step [12675/30540], Loss: 3.9274\n",
      "Epoch [2/5], Step [12750/30540], Loss: 3.8076\n",
      "Epoch [2/5], Step [12825/30540], Loss: 3.8088\n",
      "Epoch [2/5], Step [12900/30540], Loss: 3.8717\n",
      "Epoch [2/5], Step [12975/30540], Loss: 3.9134\n",
      "Epoch [2/5], Step [13050/30540], Loss: 3.8206\n",
      "Epoch [2/5], Step [13125/30540], Loss: 3.9664\n",
      "Epoch [2/5], Step [13200/30540], Loss: 3.9111\n",
      "Epoch [2/5], Step [13275/30540], Loss: 3.9613\n",
      "Epoch [2/5], Step [13350/30540], Loss: 4.0940\n",
      "Epoch [2/5], Step [13425/30540], Loss: 3.9494\n",
      "Epoch [2/5], Step [13500/30540], Loss: 3.8555\n",
      "Epoch [2/5], Step [13575/30540], Loss: 3.8650\n",
      "Epoch [2/5], Step [13650/30540], Loss: 3.9217\n",
      "Epoch [2/5], Step [13725/30540], Loss: 3.9077\n",
      "Epoch [2/5], Step [13800/30540], Loss: 3.8492\n",
      "Epoch [2/5], Step [13875/30540], Loss: 3.9088\n",
      "Epoch [2/5], Step [13950/30540], Loss: 3.9065\n",
      "Epoch [2/5], Step [14025/30540], Loss: 3.8825\n",
      "Epoch [2/5], Step [14100/30540], Loss: 3.9968\n",
      "Epoch [2/5], Step [14175/30540], Loss: 3.9624\n",
      "Epoch [2/5], Step [14250/30540], Loss: 3.8721\n",
      "Epoch [2/5], Step [14325/30540], Loss: 3.9188\n",
      "Epoch [2/5], Step [14400/30540], Loss: 3.8352\n",
      "Epoch [2/5], Step [14475/30540], Loss: 3.8301\n",
      "Epoch [2/5], Step [14550/30540], Loss: 3.9798\n",
      "Epoch [2/5], Step [14625/30540], Loss: 3.7697\n",
      "Epoch [2/5], Step [14700/30540], Loss: 3.7285\n",
      "Epoch [2/5], Step [14775/30540], Loss: 4.0642\n",
      "Epoch [2/5], Step [14850/30540], Loss: 3.9716\n",
      "Epoch [2/5], Step [14925/30540], Loss: 3.9629\n",
      "Epoch [2/5], Step [15000/30540], Loss: 3.8579\n",
      "Validation perplexity: 45.80818745471215\n",
      "Epoch [2/5], Step [15075/30540], Loss: 3.7380\n",
      "Epoch [2/5], Step [15150/30540], Loss: 3.9635\n",
      "Epoch [2/5], Step [15225/30540], Loss: 3.9460\n",
      "Epoch [2/5], Step [15300/30540], Loss: 3.9090\n",
      "Epoch [2/5], Step [15375/30540], Loss: 3.8483\n",
      "Epoch [2/5], Step [15450/30540], Loss: 3.9508\n",
      "Epoch [2/5], Step [15525/30540], Loss: 3.8035\n",
      "Epoch [2/5], Step [15600/30540], Loss: 3.9806\n",
      "Epoch [2/5], Step [15675/30540], Loss: 3.9987\n",
      "Epoch [2/5], Step [15750/30540], Loss: 3.7821\n",
      "Epoch [2/5], Step [15825/30540], Loss: 3.9668\n",
      "Epoch [2/5], Step [15900/30540], Loss: 3.8896\n",
      "Epoch [2/5], Step [15975/30540], Loss: 3.8206\n",
      "Epoch [2/5], Step [16050/30540], Loss: 3.8505\n",
      "Epoch [2/5], Step [16125/30540], Loss: 3.7719\n",
      "Epoch [2/5], Step [16200/30540], Loss: 3.8508\n",
      "Epoch [2/5], Step [16275/30540], Loss: 3.8990\n",
      "Epoch [2/5], Step [16350/30540], Loss: 3.9608\n",
      "Epoch [2/5], Step [16425/30540], Loss: 3.8368\n",
      "Epoch [2/5], Step [16500/30540], Loss: 3.9355\n",
      "Epoch [2/5], Step [16575/30540], Loss: 3.7644\n",
      "Epoch [2/5], Step [16650/30540], Loss: 3.8893\n",
      "Epoch [2/5], Step [16725/30540], Loss: 3.8958\n",
      "Epoch [2/5], Step [16800/30540], Loss: 3.8804\n",
      "Epoch [2/5], Step [16875/30540], Loss: 3.7967\n",
      "Epoch [2/5], Step [16950/30540], Loss: 3.7779\n",
      "Epoch [2/5], Step [17025/30540], Loss: 3.8867\n",
      "Epoch [2/5], Step [17100/30540], Loss: 3.8551\n",
      "Epoch [2/5], Step [17175/30540], Loss: 3.8662\n",
      "Epoch [2/5], Step [17250/30540], Loss: 3.8329\n",
      "Epoch [2/5], Step [17325/30540], Loss: 3.8390\n",
      "Epoch [2/5], Step [17400/30540], Loss: 3.8196\n",
      "Epoch [2/5], Step [17475/30540], Loss: 3.7602\n",
      "Epoch [2/5], Step [17550/30540], Loss: 3.7509\n",
      "Epoch [2/5], Step [17625/30540], Loss: 3.9621\n",
      "Epoch [2/5], Step [17700/30540], Loss: 3.9267\n",
      "Epoch [2/5], Step [17775/30540], Loss: 3.6973\n",
      "Epoch [2/5], Step [17850/30540], Loss: 3.8216\n",
      "Epoch [2/5], Step [17925/30540], Loss: 3.8908\n",
      "Epoch [2/5], Step [18000/30540], Loss: 4.0013\n",
      "Validation perplexity: 45.55117118255163\n",
      "Epoch [2/5], Step [18075/30540], Loss: 3.9692\n",
      "Epoch [2/5], Step [18150/30540], Loss: 3.9677\n",
      "Epoch [2/5], Step [18225/30540], Loss: 3.8963\n",
      "Epoch [2/5], Step [18300/30540], Loss: 3.8587\n",
      "Epoch [2/5], Step [18375/30540], Loss: 3.8761\n",
      "Epoch [2/5], Step [18450/30540], Loss: 3.9154\n",
      "Epoch [2/5], Step [18525/30540], Loss: 3.8264\n",
      "Epoch [2/5], Step [18600/30540], Loss: 3.8107\n",
      "Epoch [2/5], Step [18675/30540], Loss: 3.8507\n",
      "Epoch [2/5], Step [18750/30540], Loss: 3.8419\n",
      "Epoch [2/5], Step [18825/30540], Loss: 3.9206\n",
      "Epoch [2/5], Step [18900/30540], Loss: 3.8186\n",
      "Epoch [2/5], Step [18975/30540], Loss: 3.7634\n",
      "Epoch [2/5], Step [19050/30540], Loss: 3.8439\n",
      "Epoch [2/5], Step [19125/30540], Loss: 3.8965\n",
      "Epoch [2/5], Step [19200/30540], Loss: 3.9787\n",
      "Epoch [2/5], Step [19275/30540], Loss: 3.8786\n",
      "Epoch [2/5], Step [19350/30540], Loss: 3.9136\n",
      "Epoch [2/5], Step [19425/30540], Loss: 3.7588\n",
      "Epoch [2/5], Step [19500/30540], Loss: 3.9612\n",
      "Epoch [2/5], Step [19575/30540], Loss: 3.9731\n",
      "Epoch [2/5], Step [19650/30540], Loss: 3.9341\n",
      "Epoch [2/5], Step [19725/30540], Loss: 3.9022\n",
      "Epoch [2/5], Step [19800/30540], Loss: 3.9643\n",
      "Epoch [2/5], Step [19875/30540], Loss: 3.8085\n",
      "Epoch [2/5], Step [19950/30540], Loss: 3.7342\n",
      "Epoch [2/5], Step [20025/30540], Loss: 3.8110\n",
      "Epoch [2/5], Step [20100/30540], Loss: 3.8436\n",
      "Epoch [2/5], Step [20175/30540], Loss: 3.8847\n",
      "Epoch [2/5], Step [20250/30540], Loss: 3.8907\n",
      "Epoch [2/5], Step [20325/30540], Loss: 3.8596\n",
      "Epoch [2/5], Step [20400/30540], Loss: 3.8523\n",
      "Epoch [2/5], Step [20475/30540], Loss: 3.8497\n",
      "Epoch [2/5], Step [20550/30540], Loss: 3.7573\n",
      "Epoch [2/5], Step [20625/30540], Loss: 3.8933\n",
      "Epoch [2/5], Step [20700/30540], Loss: 3.8037\n",
      "Epoch [2/5], Step [20775/30540], Loss: 3.7840\n",
      "Epoch [2/5], Step [20850/30540], Loss: 3.9231\n",
      "Epoch [2/5], Step [20925/30540], Loss: 3.9173\n",
      "Epoch [2/5], Step [21000/30540], Loss: 3.8233\n",
      "Validation perplexity: 45.36938866775347\n",
      "Epoch [2/5], Step [21075/30540], Loss: 3.8695\n",
      "Epoch [2/5], Step [21150/30540], Loss: 3.8127\n",
      "Epoch [2/5], Step [21225/30540], Loss: 3.7821\n",
      "Epoch [2/5], Step [21300/30540], Loss: 3.8246\n",
      "Epoch [2/5], Step [21375/30540], Loss: 3.8635\n",
      "Epoch [2/5], Step [21450/30540], Loss: 3.8087\n",
      "Epoch [2/5], Step [21525/30540], Loss: 3.8835\n",
      "Epoch [2/5], Step [21600/30540], Loss: 3.7866\n",
      "Epoch [2/5], Step [21675/30540], Loss: 3.8304\n",
      "Epoch [2/5], Step [21750/30540], Loss: 3.8161\n",
      "Epoch [2/5], Step [21825/30540], Loss: 3.8285\n",
      "Epoch [2/5], Step [21900/30540], Loss: 3.7798\n",
      "Epoch [2/5], Step [21975/30540], Loss: 3.8452\n",
      "Epoch [2/5], Step [22050/30540], Loss: 3.7927\n",
      "Epoch [2/5], Step [22125/30540], Loss: 3.9097\n",
      "Epoch [2/5], Step [22200/30540], Loss: 3.7821\n",
      "Epoch [2/5], Step [22275/30540], Loss: 3.8532\n",
      "Epoch [2/5], Step [22350/30540], Loss: 3.7310\n",
      "Epoch [2/5], Step [22425/30540], Loss: 3.8792\n",
      "Epoch [2/5], Step [22500/30540], Loss: 3.7863\n",
      "Epoch [2/5], Step [22575/30540], Loss: 3.8279\n",
      "Epoch [2/5], Step [22650/30540], Loss: 3.8641\n",
      "Epoch [2/5], Step [22725/30540], Loss: 3.8800\n",
      "Epoch [2/5], Step [22800/30540], Loss: 3.8547\n",
      "Epoch [2/5], Step [22875/30540], Loss: 3.9220\n",
      "Epoch [2/5], Step [22950/30540], Loss: 3.8331\n",
      "Epoch [2/5], Step [23025/30540], Loss: 3.7910\n",
      "Epoch [2/5], Step [23100/30540], Loss: 3.7811\n",
      "Epoch [2/5], Step [23175/30540], Loss: 3.8401\n",
      "Epoch [2/5], Step [23250/30540], Loss: 3.8268\n",
      "Epoch [2/5], Step [23325/30540], Loss: 3.8181\n",
      "Epoch [2/5], Step [23400/30540], Loss: 3.7972\n",
      "Epoch [2/5], Step [23475/30540], Loss: 3.8547\n",
      "Epoch [2/5], Step [23550/30540], Loss: 3.9209\n",
      "Epoch [2/5], Step [23625/30540], Loss: 3.8711\n",
      "Epoch [2/5], Step [23700/30540], Loss: 4.0153\n",
      "Epoch [2/5], Step [23775/30540], Loss: 3.8277\n",
      "Epoch [2/5], Step [23850/30540], Loss: 3.9691\n",
      "Epoch [2/5], Step [23925/30540], Loss: 3.9153\n",
      "Epoch [2/5], Step [24000/30540], Loss: 3.9267\n",
      "Validation perplexity: 45.17257544115227\n",
      "Epoch [2/5], Step [24075/30540], Loss: 3.7902\n",
      "Epoch [2/5], Step [24150/30540], Loss: 4.0892\n",
      "Epoch [2/5], Step [24225/30540], Loss: 3.8878\n",
      "Epoch [2/5], Step [24300/30540], Loss: 3.9784\n",
      "Epoch [2/5], Step [24375/30540], Loss: 3.7277\n",
      "Epoch [2/5], Step [24450/30540], Loss: 3.9329\n",
      "Epoch [2/5], Step [24525/30540], Loss: 3.8225\n",
      "Epoch [2/5], Step [24600/30540], Loss: 3.8492\n",
      "Epoch [2/5], Step [24675/30540], Loss: 3.8911\n",
      "Epoch [2/5], Step [24750/30540], Loss: 3.7964\n",
      "Epoch [2/5], Step [24825/30540], Loss: 3.7892\n",
      "Epoch [2/5], Step [24900/30540], Loss: 3.7919\n",
      "Epoch [2/5], Step [24975/30540], Loss: 4.0097\n",
      "Epoch [2/5], Step [25050/30540], Loss: 3.8661\n",
      "Epoch [2/5], Step [25125/30540], Loss: 3.7125\n",
      "Epoch [2/5], Step [25200/30540], Loss: 3.9369\n",
      "Epoch [2/5], Step [25275/30540], Loss: 3.8439\n",
      "Epoch [2/5], Step [25350/30540], Loss: 3.7902\n",
      "Epoch [2/5], Step [25425/30540], Loss: 3.9605\n",
      "Epoch [2/5], Step [25500/30540], Loss: 3.8796\n",
      "Epoch [2/5], Step [25575/30540], Loss: 3.8809\n",
      "Epoch [2/5], Step [25650/30540], Loss: 3.9365\n",
      "Epoch [2/5], Step [25725/30540], Loss: 3.9005\n",
      "Epoch [2/5], Step [25800/30540], Loss: 3.8290\n",
      "Epoch [2/5], Step [25875/30540], Loss: 3.8073\n",
      "Epoch [2/5], Step [25950/30540], Loss: 3.9053\n",
      "Epoch [2/5], Step [26025/30540], Loss: 3.7565\n",
      "Epoch [2/5], Step [26100/30540], Loss: 3.9406\n",
      "Epoch [2/5], Step [26175/30540], Loss: 3.7609\n",
      "Epoch [2/5], Step [26250/30540], Loss: 3.9738\n",
      "Epoch [2/5], Step [26325/30540], Loss: 3.8354\n",
      "Epoch [2/5], Step [26400/30540], Loss: 3.9031\n",
      "Epoch [2/5], Step [26475/30540], Loss: 3.8661\n",
      "Epoch [2/5], Step [26550/30540], Loss: 3.8874\n",
      "Epoch [2/5], Step [26625/30540], Loss: 3.8381\n",
      "Epoch [2/5], Step [26700/30540], Loss: 3.8637\n",
      "Epoch [2/5], Step [26775/30540], Loss: 3.9496\n",
      "Epoch [2/5], Step [26850/30540], Loss: 3.8743\n",
      "Epoch [2/5], Step [26925/30540], Loss: 3.9076\n",
      "Epoch [2/5], Step [27000/30540], Loss: 3.8682\n",
      "Validation perplexity: 44.98247436048023\n",
      "Epoch [2/5], Step [27075/30540], Loss: 3.7713\n",
      "Epoch [2/5], Step [27150/30540], Loss: 3.8796\n",
      "Epoch [2/5], Step [27225/30540], Loss: 3.8307\n",
      "Epoch [2/5], Step [27300/30540], Loss: 3.9139\n",
      "Epoch [2/5], Step [27375/30540], Loss: 3.8867\n",
      "Epoch [2/5], Step [27450/30540], Loss: 3.8147\n",
      "Epoch [2/5], Step [27525/30540], Loss: 3.8619\n",
      "Epoch [2/5], Step [27600/30540], Loss: 3.8546\n",
      "Epoch [2/5], Step [27675/30540], Loss: 3.9119\n",
      "Epoch [2/5], Step [27750/30540], Loss: 3.8661\n",
      "Epoch [2/5], Step [27825/30540], Loss: 3.7705\n",
      "Epoch [2/5], Step [27900/30540], Loss: 3.7573\n",
      "Epoch [2/5], Step [27975/30540], Loss: 3.8486\n",
      "Epoch [2/5], Step [28050/30540], Loss: 3.7962\n",
      "Epoch [2/5], Step [28125/30540], Loss: 3.8686\n",
      "Epoch [2/5], Step [28200/30540], Loss: 3.9322\n",
      "Epoch [2/5], Step [28275/30540], Loss: 3.8990\n",
      "Epoch [2/5], Step [28350/30540], Loss: 3.7917\n",
      "Epoch [2/5], Step [28425/30540], Loss: 3.8386\n",
      "Epoch [2/5], Step [28500/30540], Loss: 3.8266\n",
      "Epoch [2/5], Step [28575/30540], Loss: 3.8356\n",
      "Epoch [2/5], Step [28650/30540], Loss: 3.7658\n",
      "Epoch [2/5], Step [28725/30540], Loss: 3.7881\n",
      "Epoch [2/5], Step [28800/30540], Loss: 3.9428\n",
      "Epoch [2/5], Step [28875/30540], Loss: 3.9125\n",
      "Epoch [2/5], Step [28950/30540], Loss: 3.7475\n",
      "Epoch [2/5], Step [29025/30540], Loss: 3.8191\n",
      "Epoch [2/5], Step [29100/30540], Loss: 3.8217\n",
      "Epoch [2/5], Step [29175/30540], Loss: 3.8180\n",
      "Epoch [2/5], Step [29250/30540], Loss: 3.8436\n",
      "Epoch [2/5], Step [29325/30540], Loss: 3.8654\n",
      "Epoch [2/5], Step [29400/30540], Loss: 3.9285\n",
      "Epoch [2/5], Step [29475/30540], Loss: 3.7811\n",
      "Epoch [2/5], Step [29550/30540], Loss: 3.9051\n",
      "Epoch [2/5], Step [29625/30540], Loss: 3.8743\n",
      "Epoch [2/5], Step [29700/30540], Loss: 3.7251\n",
      "Epoch [2/5], Step [29775/30540], Loss: 3.7984\n",
      "Epoch [2/5], Step [29850/30540], Loss: 3.8358\n",
      "Epoch [2/5], Step [29925/30540], Loss: 3.8092\n",
      "Epoch [2/5], Step [30000/30540], Loss: 4.0517\n",
      "Validation perplexity: 44.83896723760315\n",
      "Epoch [2/5], Step [30075/30540], Loss: 3.9288\n",
      "Epoch [2/5], Step [30150/30540], Loss: 3.8636\n",
      "Epoch [2/5], Step [30225/30540], Loss: 3.7593\n",
      "Epoch [2/5], Step [30300/30540], Loss: 3.9987\n",
      "Epoch [2/5], Step [30375/30540], Loss: 3.9094\n",
      "Epoch [2/5], Step [30450/30540], Loss: 3.8795\n",
      "Epoch [2/5], Step [30525/30540], Loss: 3.8324\n",
      "Epoch [2/5] Average Loss: 3.8805, Perplexity: 48.45\n",
      "Epoch [3/5], Step [0/30540], Loss: 3.9699\n",
      "Validation perplexity: 44.785923118029096\n",
      "Epoch [3/5], Step [75/30540], Loss: 3.8971\n",
      "Epoch [3/5], Step [150/30540], Loss: 3.6554\n",
      "Epoch [3/5], Step [225/30540], Loss: 3.8542\n",
      "Epoch [3/5], Step [300/30540], Loss: 3.8374\n",
      "Epoch [3/5], Step [375/30540], Loss: 3.7748\n",
      "Epoch [3/5], Step [450/30540], Loss: 3.8001\n",
      "Epoch [3/5], Step [525/30540], Loss: 3.8990\n",
      "Epoch [3/5], Step [600/30540], Loss: 3.8361\n",
      "Epoch [3/5], Step [675/30540], Loss: 3.8952\n",
      "Epoch [3/5], Step [750/30540], Loss: 3.8217\n",
      "Epoch [3/5], Step [825/30540], Loss: 3.8693\n",
      "Epoch [3/5], Step [900/30540], Loss: 4.0027\n",
      "Epoch [3/5], Step [975/30540], Loss: 3.8204\n",
      "Epoch [3/5], Step [1050/30540], Loss: 3.9432\n",
      "Epoch [3/5], Step [1125/30540], Loss: 3.7562\n",
      "Epoch [3/5], Step [1200/30540], Loss: 3.8735\n",
      "Epoch [3/5], Step [1275/30540], Loss: 3.8141\n",
      "Epoch [3/5], Step [1350/30540], Loss: 3.9241\n",
      "Epoch [3/5], Step [1425/30540], Loss: 3.8004\n",
      "Epoch [3/5], Step [1500/30540], Loss: 3.8220\n",
      "Epoch [3/5], Step [1575/30540], Loss: 3.8389\n",
      "Epoch [3/5], Step [1650/30540], Loss: 3.9360\n",
      "Epoch [3/5], Step [1725/30540], Loss: 3.8810\n",
      "Epoch [3/5], Step [1800/30540], Loss: 3.6962\n",
      "Epoch [3/5], Step [1875/30540], Loss: 3.8165\n",
      "Epoch [3/5], Step [1950/30540], Loss: 3.8403\n",
      "Epoch [3/5], Step [2025/30540], Loss: 3.9580\n",
      "Epoch [3/5], Step [2100/30540], Loss: 3.7534\n",
      "Epoch [3/5], Step [2175/30540], Loss: 3.9189\n",
      "Epoch [3/5], Step [2250/30540], Loss: 3.9175\n",
      "Epoch [3/5], Step [2325/30540], Loss: 3.8076\n",
      "Epoch [3/5], Step [2400/30540], Loss: 3.7464\n",
      "Epoch [3/5], Step [2475/30540], Loss: 3.9242\n",
      "Epoch [3/5], Step [2550/30540], Loss: 3.9495\n",
      "Epoch [3/5], Step [2625/30540], Loss: 3.7988\n",
      "Epoch [3/5], Step [2700/30540], Loss: 3.9602\n",
      "Epoch [3/5], Step [2775/30540], Loss: 3.8757\n",
      "Epoch [3/5], Step [2850/30540], Loss: 3.8404\n",
      "Epoch [3/5], Step [2925/30540], Loss: 3.7983\n",
      "Epoch [3/5], Step [3000/30540], Loss: 3.8749\n",
      "Validation perplexity: 44.72371629959102\n",
      "Epoch [3/5], Step [3075/30540], Loss: 3.8365\n",
      "Epoch [3/5], Step [3150/30540], Loss: 3.7646\n",
      "Epoch [3/5], Step [3225/30540], Loss: 3.9021\n",
      "Epoch [3/5], Step [3300/30540], Loss: 3.7040\n",
      "Epoch [3/5], Step [3375/30540], Loss: 3.8475\n",
      "Epoch [3/5], Step [3450/30540], Loss: 3.9078\n",
      "Epoch [3/5], Step [3525/30540], Loss: 3.8667\n",
      "Epoch [3/5], Step [3600/30540], Loss: 3.9426\n",
      "Epoch [3/5], Step [3675/30540], Loss: 3.9776\n",
      "Epoch [3/5], Step [3750/30540], Loss: 3.9001\n",
      "Epoch [3/5], Step [3825/30540], Loss: 3.8104\n",
      "Epoch [3/5], Step [3900/30540], Loss: 3.8523\n",
      "Epoch [3/5], Step [3975/30540], Loss: 3.8334\n",
      "Epoch [3/5], Step [4050/30540], Loss: 3.7951\n",
      "Epoch [3/5], Step [4125/30540], Loss: 3.7664\n",
      "Epoch [3/5], Step [4200/30540], Loss: 4.0137\n",
      "Epoch [3/5], Step [4275/30540], Loss: 3.6682\n",
      "Epoch [3/5], Step [4350/30540], Loss: 3.7933\n",
      "Epoch [3/5], Step [4425/30540], Loss: 3.9308\n",
      "Epoch [3/5], Step [4500/30540], Loss: 3.7634\n",
      "Epoch [3/5], Step [4575/30540], Loss: 3.8824\n",
      "Epoch [3/5], Step [4650/30540], Loss: 3.8143\n",
      "Epoch [3/5], Step [4725/30540], Loss: 3.9133\n",
      "Epoch [3/5], Step [4800/30540], Loss: 3.7996\n",
      "Epoch [3/5], Step [4875/30540], Loss: 3.8158\n",
      "Epoch [3/5], Step [4950/30540], Loss: 3.8050\n",
      "Epoch [3/5], Step [5025/30540], Loss: 3.8473\n",
      "Epoch [3/5], Step [5100/30540], Loss: 3.9249\n",
      "Epoch [3/5], Step [5175/30540], Loss: 3.7779\n",
      "Epoch [3/5], Step [5250/30540], Loss: 3.7909\n",
      "Epoch [3/5], Step [5325/30540], Loss: 3.8181\n",
      "Epoch [3/5], Step [5400/30540], Loss: 3.8268\n",
      "Epoch [3/5], Step [5475/30540], Loss: 3.7336\n",
      "Epoch [3/5], Step [5550/30540], Loss: 3.6875\n",
      "Epoch [3/5], Step [5625/30540], Loss: 3.7352\n",
      "Epoch [3/5], Step [5700/30540], Loss: 3.8000\n",
      "Epoch [3/5], Step [5775/30540], Loss: 3.8043\n",
      "Epoch [3/5], Step [5850/30540], Loss: 3.8117\n",
      "Epoch [3/5], Step [5925/30540], Loss: 3.9592\n",
      "Epoch [3/5], Step [6000/30540], Loss: 3.8946\n",
      "Validation perplexity: 44.55224743647507\n",
      "Epoch [3/5], Step [6075/30540], Loss: 3.8327\n",
      "Epoch [3/5], Step [6150/30540], Loss: 3.8429\n",
      "Epoch [3/5], Step [6225/30540], Loss: 3.9091\n",
      "Epoch [3/5], Step [6300/30540], Loss: 3.7699\n",
      "Epoch [3/5], Step [6375/30540], Loss: 3.9516\n",
      "Epoch [3/5], Step [6450/30540], Loss: 3.8877\n",
      "Epoch [3/5], Step [6525/30540], Loss: 3.9815\n",
      "Epoch [3/5], Step [6600/30540], Loss: 3.7701\n",
      "Epoch [3/5], Step [6675/30540], Loss: 3.7032\n",
      "Epoch [3/5], Step [6750/30540], Loss: 3.8596\n",
      "Epoch [3/5], Step [6825/30540], Loss: 4.0140\n",
      "Epoch [3/5], Step [6900/30540], Loss: 3.8026\n",
      "Epoch [3/5], Step [6975/30540], Loss: 3.7621\n",
      "Epoch [3/5], Step [7050/30540], Loss: 3.8043\n",
      "Epoch [3/5], Step [7125/30540], Loss: 3.8469\n",
      "Epoch [3/5], Step [7200/30540], Loss: 3.7726\n",
      "Epoch [3/5], Step [7275/30540], Loss: 3.7936\n",
      "Epoch [3/5], Step [7350/30540], Loss: 3.8543\n",
      "Epoch [3/5], Step [7425/30540], Loss: 3.8276\n",
      "Epoch [3/5], Step [7500/30540], Loss: 3.8586\n",
      "Epoch [3/5], Step [7575/30540], Loss: 3.8652\n",
      "Epoch [3/5], Step [7650/30540], Loss: 3.7938\n",
      "Epoch [3/5], Step [7725/30540], Loss: 3.8838\n",
      "Epoch [3/5], Step [7800/30540], Loss: 3.8906\n",
      "Epoch [3/5], Step [7875/30540], Loss: 3.8125\n",
      "Epoch [3/5], Step [7950/30540], Loss: 3.7818\n",
      "Epoch [3/5], Step [8025/30540], Loss: 3.7549\n",
      "Epoch [3/5], Step [8100/30540], Loss: 3.8772\n",
      "Epoch [3/5], Step [8175/30540], Loss: 3.7895\n",
      "Epoch [3/5], Step [8250/30540], Loss: 3.8653\n",
      "Epoch [3/5], Step [8325/30540], Loss: 3.9043\n",
      "Epoch [3/5], Step [8400/30540], Loss: 3.9942\n",
      "Epoch [3/5], Step [8475/30540], Loss: 3.8188\n",
      "Epoch [3/5], Step [8550/30540], Loss: 3.8166\n",
      "Epoch [3/5], Step [8625/30540], Loss: 3.8471\n",
      "Epoch [3/5], Step [8700/30540], Loss: 3.7777\n",
      "Epoch [3/5], Step [8775/30540], Loss: 3.7703\n",
      "Epoch [3/5], Step [8850/30540], Loss: 3.8053\n",
      "Epoch [3/5], Step [8925/30540], Loss: 3.8496\n",
      "Epoch [3/5], Step [9000/30540], Loss: 3.8590\n",
      "Validation perplexity: 44.43581862169362\n",
      "Epoch [3/5], Step [9075/30540], Loss: 3.7170\n",
      "Epoch [3/5], Step [9150/30540], Loss: 3.9451\n",
      "Epoch [3/5], Step [9225/30540], Loss: 3.7787\n",
      "Epoch [3/5], Step [9300/30540], Loss: 3.7867\n",
      "Epoch [3/5], Step [9375/30540], Loss: 3.8812\n",
      "Epoch [3/5], Step [9450/30540], Loss: 3.8071\n",
      "Epoch [3/5], Step [9525/30540], Loss: 3.9906\n",
      "Epoch [3/5], Step [9600/30540], Loss: 3.8915\n",
      "Epoch [3/5], Step [9675/30540], Loss: 3.7917\n",
      "Epoch [3/5], Step [9750/30540], Loss: 3.8590\n",
      "Epoch [3/5], Step [9825/30540], Loss: 3.8199\n",
      "Epoch [3/5], Step [9900/30540], Loss: 3.7976\n",
      "Epoch [3/5], Step [9975/30540], Loss: 3.8106\n",
      "Epoch [3/5], Step [10050/30540], Loss: 3.9159\n",
      "Epoch [3/5], Step [10125/30540], Loss: 3.7982\n",
      "Epoch [3/5], Step [10200/30540], Loss: 3.6864\n",
      "Epoch [3/5], Step [10275/30540], Loss: 3.9303\n",
      "Epoch [3/5], Step [10350/30540], Loss: 3.8343\n",
      "Epoch [3/5], Step [10425/30540], Loss: 3.6332\n",
      "Epoch [3/5], Step [10500/30540], Loss: 3.7656\n",
      "Epoch [3/5], Step [10575/30540], Loss: 3.7346\n",
      "Epoch [3/5], Step [10650/30540], Loss: 3.8384\n",
      "Epoch [3/5], Step [10725/30540], Loss: 3.8396\n",
      "Epoch [3/5], Step [10800/30540], Loss: 3.9085\n",
      "Epoch [3/5], Step [10875/30540], Loss: 3.8462\n",
      "Epoch [3/5], Step [10950/30540], Loss: 3.8178\n",
      "Epoch [3/5], Step [11025/30540], Loss: 3.9854\n",
      "Epoch [3/5], Step [11100/30540], Loss: 3.7589\n",
      "Epoch [3/5], Step [11175/30540], Loss: 3.7640\n",
      "Epoch [3/5], Step [11250/30540], Loss: 3.8459\n",
      "Epoch [3/5], Step [11325/30540], Loss: 3.9024\n",
      "Epoch [3/5], Step [11400/30540], Loss: 3.8012\n",
      "Epoch [3/5], Step [11475/30540], Loss: 3.7583\n",
      "Epoch [3/5], Step [11550/30540], Loss: 3.7797\n",
      "Epoch [3/5], Step [11625/30540], Loss: 3.7660\n",
      "Epoch [3/5], Step [11700/30540], Loss: 3.8144\n",
      "Epoch [3/5], Step [11775/30540], Loss: 3.7957\n",
      "Epoch [3/5], Step [11850/30540], Loss: 3.8536\n",
      "Epoch [3/5], Step [11925/30540], Loss: 3.8541\n",
      "Epoch [3/5], Step [12000/30540], Loss: 3.8159\n",
      "Validation perplexity: 44.22879065639569\n",
      "Epoch [3/5], Step [12075/30540], Loss: 3.7184\n",
      "Epoch [3/5], Step [12150/30540], Loss: 3.8364\n",
      "Epoch [3/5], Step [12225/30540], Loss: 3.8274\n",
      "Epoch [3/5], Step [12300/30540], Loss: 3.7583\n",
      "Epoch [3/5], Step [12375/30540], Loss: 3.8564\n",
      "Epoch [3/5], Step [12450/30540], Loss: 3.7943\n",
      "Epoch [3/5], Step [12525/30540], Loss: 3.8507\n",
      "Epoch [3/5], Step [12600/30540], Loss: 4.0119\n",
      "Epoch [3/5], Step [12675/30540], Loss: 3.7975\n",
      "Epoch [3/5], Step [12750/30540], Loss: 3.7367\n",
      "Epoch [3/5], Step [12825/30540], Loss: 3.8250\n",
      "Epoch [3/5], Step [12900/30540], Loss: 3.8568\n",
      "Epoch [3/5], Step [12975/30540], Loss: 3.7919\n",
      "Epoch [3/5], Step [13050/30540], Loss: 3.8678\n",
      "Epoch [3/5], Step [13125/30540], Loss: 4.0112\n",
      "Epoch [3/5], Step [13200/30540], Loss: 3.8912\n",
      "Epoch [3/5], Step [13275/30540], Loss: 3.8975\n",
      "Epoch [3/5], Step [13350/30540], Loss: 3.8082\n",
      "Epoch [3/5], Step [13425/30540], Loss: 3.9604\n",
      "Epoch [3/5], Step [13500/30540], Loss: 3.8739\n",
      "Epoch [3/5], Step [13575/30540], Loss: 3.8143\n",
      "Epoch [3/5], Step [13650/30540], Loss: 3.8035\n",
      "Epoch [3/5], Step [13725/30540], Loss: 3.8461\n",
      "Epoch [3/5], Step [13800/30540], Loss: 3.9576\n",
      "Epoch [3/5], Step [13875/30540], Loss: 3.7865\n",
      "Epoch [3/5], Step [13950/30540], Loss: 3.9722\n",
      "Epoch [3/5], Step [14025/30540], Loss: 3.7936\n",
      "Epoch [3/5], Step [14100/30540], Loss: 3.8437\n",
      "Epoch [3/5], Step [14175/30540], Loss: 3.8155\n",
      "Epoch [3/5], Step [14250/30540], Loss: 3.8150\n",
      "Epoch [3/5], Step [14325/30540], Loss: 3.7914\n",
      "Epoch [3/5], Step [14400/30540], Loss: 3.8283\n",
      "Epoch [3/5], Step [14475/30540], Loss: 3.7655\n",
      "Epoch [3/5], Step [14550/30540], Loss: 3.8531\n",
      "Epoch [3/5], Step [14625/30540], Loss: 3.6698\n",
      "Epoch [3/5], Step [14700/30540], Loss: 3.8106\n",
      "Epoch [3/5], Step [14775/30540], Loss: 3.8338\n",
      "Epoch [3/5], Step [14850/30540], Loss: 3.6904\n",
      "Epoch [3/5], Step [14925/30540], Loss: 3.8595\n",
      "Epoch [3/5], Step [15000/30540], Loss: 3.8597\n",
      "Validation perplexity: 44.17409120416211\n",
      "Epoch [3/5], Step [15075/30540], Loss: 3.7228\n",
      "Epoch [3/5], Step [15150/30540], Loss: 3.8329\n",
      "Epoch [3/5], Step [15225/30540], Loss: 3.7307\n",
      "Epoch [3/5], Step [15300/30540], Loss: 3.8434\n",
      "Epoch [3/5], Step [15375/30540], Loss: 3.7731\n",
      "Epoch [3/5], Step [15450/30540], Loss: 3.8213\n",
      "Epoch [3/5], Step [15525/30540], Loss: 3.7946\n",
      "Epoch [3/5], Step [15600/30540], Loss: 3.7490\n",
      "Epoch [3/5], Step [15675/30540], Loss: 3.7518\n",
      "Epoch [3/5], Step [15750/30540], Loss: 3.7091\n",
      "Epoch [3/5], Step [15825/30540], Loss: 3.7503\n",
      "Epoch [3/5], Step [15900/30540], Loss: 3.7346\n",
      "Epoch [3/5], Step [15975/30540], Loss: 3.8329\n",
      "Epoch [3/5], Step [16050/30540], Loss: 3.8176\n",
      "Epoch [3/5], Step [16125/30540], Loss: 3.8206\n",
      "Epoch [3/5], Step [16200/30540], Loss: 3.8095\n",
      "Epoch [3/5], Step [16275/30540], Loss: 3.8665\n",
      "Epoch [3/5], Step [16350/30540], Loss: 3.7902\n",
      "Epoch [3/5], Step [16425/30540], Loss: 3.9318\n",
      "Epoch [3/5], Step [16500/30540], Loss: 3.8458\n",
      "Epoch [3/5], Step [16575/30540], Loss: 3.8225\n",
      "Epoch [3/5], Step [16650/30540], Loss: 3.9296\n",
      "Epoch [3/5], Step [16725/30540], Loss: 3.8583\n",
      "Epoch [3/5], Step [16800/30540], Loss: 3.8746\n",
      "Epoch [3/5], Step [16875/30540], Loss: 3.8515\n",
      "Epoch [3/5], Step [16950/30540], Loss: 3.7853\n",
      "Epoch [3/5], Step [17025/30540], Loss: 3.8381\n",
      "Epoch [3/5], Step [17100/30540], Loss: 3.8205\n",
      "Epoch [3/5], Step [17175/30540], Loss: 3.8959\n",
      "Epoch [3/5], Step [17250/30540], Loss: 3.8387\n",
      "Epoch [3/5], Step [17325/30540], Loss: 3.8809\n",
      "Epoch [3/5], Step [17400/30540], Loss: 3.7998\n",
      "Epoch [3/5], Step [17475/30540], Loss: 3.8487\n",
      "Epoch [3/5], Step [17550/30540], Loss: 3.7811\n",
      "Epoch [3/5], Step [17625/30540], Loss: 3.9611\n",
      "Epoch [3/5], Step [17700/30540], Loss: 3.8494\n",
      "Epoch [3/5], Step [17775/30540], Loss: 3.8388\n",
      "Epoch [3/5], Step [17850/30540], Loss: 3.8749\n",
      "Epoch [3/5], Step [17925/30540], Loss: 3.8612\n",
      "Epoch [3/5], Step [18000/30540], Loss: 3.6799\n",
      "Validation perplexity: 44.08151646191603\n",
      "Epoch [3/5], Step [18075/30540], Loss: 3.8474\n",
      "Epoch [3/5], Step [18150/30540], Loss: 3.7640\n",
      "Epoch [3/5], Step [18225/30540], Loss: 3.8318\n",
      "Epoch [3/5], Step [18300/30540], Loss: 3.7992\n",
      "Epoch [3/5], Step [18375/30540], Loss: 3.8070\n",
      "Epoch [3/5], Step [18450/30540], Loss: 3.7553\n",
      "Epoch [3/5], Step [18525/30540], Loss: 3.7965\n",
      "Epoch [3/5], Step [18600/30540], Loss: 3.8930\n",
      "Epoch [3/5], Step [18675/30540], Loss: 3.8249\n",
      "Epoch [3/5], Step [18750/30540], Loss: 3.7623\n",
      "Epoch [3/5], Step [18825/30540], Loss: 3.8731\n",
      "Epoch [3/5], Step [18900/30540], Loss: 3.8674\n",
      "Epoch [3/5], Step [18975/30540], Loss: 3.8214\n",
      "Epoch [3/5], Step [19050/30540], Loss: 3.8545\n",
      "Epoch [3/5], Step [19125/30540], Loss: 3.7381\n",
      "Epoch [3/5], Step [19200/30540], Loss: 3.7990\n",
      "Epoch [3/5], Step [19275/30540], Loss: 3.8861\n",
      "Epoch [3/5], Step [19350/30540], Loss: 3.8433\n",
      "Epoch [3/5], Step [19425/30540], Loss: 3.8742\n",
      "Epoch [3/5], Step [19500/30540], Loss: 3.8059\n",
      "Epoch [3/5], Step [19575/30540], Loss: 3.7143\n",
      "Epoch [3/5], Step [19650/30540], Loss: 3.8320\n",
      "Epoch [3/5], Step [19725/30540], Loss: 3.8882\n",
      "Epoch [3/5], Step [19800/30540], Loss: 3.8985\n",
      "Epoch [3/5], Step [19875/30540], Loss: 3.8153\n",
      "Epoch [3/5], Step [19950/30540], Loss: 3.8179\n",
      "Epoch [3/5], Step [20025/30540], Loss: 3.7868\n",
      "Epoch [3/5], Step [20100/30540], Loss: 3.8386\n",
      "Epoch [3/5], Step [20175/30540], Loss: 3.7990\n",
      "Epoch [3/5], Step [20250/30540], Loss: 3.6330\n",
      "Epoch [3/5], Step [20325/30540], Loss: 3.8234\n",
      "Epoch [3/5], Step [20400/30540], Loss: 3.8634\n",
      "Epoch [3/5], Step [20475/30540], Loss: 3.8762\n",
      "Epoch [3/5], Step [20550/30540], Loss: 3.8487\n",
      "Epoch [3/5], Step [20625/30540], Loss: 3.8190\n",
      "Epoch [3/5], Step [20700/30540], Loss: 3.7622\n",
      "Epoch [3/5], Step [20775/30540], Loss: 3.8258\n",
      "Epoch [3/5], Step [20850/30540], Loss: 3.8276\n",
      "Epoch [3/5], Step [20925/30540], Loss: 3.7713\n",
      "Epoch [3/5], Step [21000/30540], Loss: 3.8769\n",
      "Validation perplexity: 43.949128304503496\n",
      "Epoch [3/5], Step [21075/30540], Loss: 3.7830\n",
      "Epoch [3/5], Step [21150/30540], Loss: 3.8035\n",
      "Epoch [3/5], Step [21225/30540], Loss: 3.8285\n",
      "Epoch [3/5], Step [21300/30540], Loss: 3.8046\n",
      "Epoch [3/5], Step [21375/30540], Loss: 4.0206\n",
      "Epoch [3/5], Step [21450/30540], Loss: 3.6993\n",
      "Epoch [3/5], Step [21525/30540], Loss: 3.7102\n",
      "Epoch [3/5], Step [21600/30540], Loss: 3.8559\n",
      "Epoch [3/5], Step [21675/30540], Loss: 3.8878\n",
      "Epoch [3/5], Step [21750/30540], Loss: 3.8137\n",
      "Epoch [3/5], Step [21825/30540], Loss: 3.8574\n",
      "Epoch [3/5], Step [21900/30540], Loss: 3.9142\n",
      "Epoch [3/5], Step [21975/30540], Loss: 3.8353\n",
      "Epoch [3/5], Step [22050/30540], Loss: 3.6628\n",
      "Epoch [3/5], Step [22125/30540], Loss: 3.9642\n",
      "Epoch [3/5], Step [22200/30540], Loss: 3.7715\n",
      "Epoch [3/5], Step [22275/30540], Loss: 3.8352\n",
      "Epoch [3/5], Step [22350/30540], Loss: 3.7817\n",
      "Epoch [3/5], Step [22425/30540], Loss: 3.8943\n",
      "Epoch [3/5], Step [22500/30540], Loss: 3.8987\n",
      "Epoch [3/5], Step [22575/30540], Loss: 3.7757\n",
      "Epoch [3/5], Step [22650/30540], Loss: 3.7922\n",
      "Epoch [3/5], Step [22725/30540], Loss: 3.8957\n",
      "Epoch [3/5], Step [22800/30540], Loss: 3.8143\n",
      "Epoch [3/5], Step [22875/30540], Loss: 3.9124\n",
      "Epoch [3/5], Step [22950/30540], Loss: 3.8100\n",
      "Epoch [3/5], Step [23025/30540], Loss: 3.8269\n",
      "Epoch [3/5], Step [23100/30540], Loss: 3.8982\n",
      "Epoch [3/5], Step [23175/30540], Loss: 3.8401\n",
      "Epoch [3/5], Step [23250/30540], Loss: 3.7990\n",
      "Epoch [3/5], Step [23325/30540], Loss: 3.9309\n",
      "Epoch [3/5], Step [23400/30540], Loss: 4.0118\n",
      "Epoch [3/5], Step [23475/30540], Loss: 3.8670\n",
      "Epoch [3/5], Step [23550/30540], Loss: 3.8721\n",
      "Epoch [3/5], Step [23625/30540], Loss: 3.8639\n",
      "Epoch [3/5], Step [23700/30540], Loss: 3.8505\n",
      "Epoch [3/5], Step [23775/30540], Loss: 3.8554\n",
      "Epoch [3/5], Step [23850/30540], Loss: 3.8591\n",
      "Epoch [3/5], Step [23925/30540], Loss: 3.9233\n",
      "Epoch [3/5], Step [24000/30540], Loss: 3.8261\n",
      "Validation perplexity: 43.832433505064124\n",
      "Epoch [3/5], Step [24075/30540], Loss: 3.8359\n",
      "Epoch [3/5], Step [24150/30540], Loss: 3.7305\n",
      "Epoch [3/5], Step [24225/30540], Loss: 3.7389\n",
      "Epoch [3/5], Step [24300/30540], Loss: 3.7927\n",
      "Epoch [3/5], Step [24375/30540], Loss: 3.8581\n",
      "Epoch [3/5], Step [24450/30540], Loss: 3.8713\n",
      "Epoch [3/5], Step [24525/30540], Loss: 3.7556\n",
      "Epoch [3/5], Step [24600/30540], Loss: 3.9111\n",
      "Epoch [3/5], Step [24675/30540], Loss: 3.7967\n",
      "Epoch [3/5], Step [24750/30540], Loss: 3.8320\n",
      "Epoch [3/5], Step [24825/30540], Loss: 3.7364\n",
      "Epoch [3/5], Step [24900/30540], Loss: 3.8454\n",
      "Epoch [3/5], Step [24975/30540], Loss: 3.8624\n",
      "Epoch [3/5], Step [25050/30540], Loss: 3.7581\n",
      "Epoch [3/5], Step [25125/30540], Loss: 3.9594\n",
      "Epoch [3/5], Step [25200/30540], Loss: 3.8530\n",
      "Epoch [3/5], Step [25275/30540], Loss: 3.8499\n",
      "Epoch [3/5], Step [25350/30540], Loss: 3.8133\n",
      "Epoch [3/5], Step [25425/30540], Loss: 3.8185\n",
      "Epoch [3/5], Step [25500/30540], Loss: 3.7836\n",
      "Epoch [3/5], Step [25575/30540], Loss: 3.8423\n",
      "Epoch [3/5], Step [25650/30540], Loss: 3.8799\n",
      "Epoch [3/5], Step [25725/30540], Loss: 3.7575\n",
      "Epoch [3/5], Step [25800/30540], Loss: 3.8655\n",
      "Epoch [3/5], Step [25875/30540], Loss: 3.8242\n",
      "Epoch [3/5], Step [25950/30540], Loss: 3.9207\n",
      "Epoch [3/5], Step [26025/30540], Loss: 3.8287\n",
      "Epoch [3/5], Step [26100/30540], Loss: 3.9619\n",
      "Epoch [3/5], Step [26175/30540], Loss: 3.8734\n",
      "Epoch [3/5], Step [26250/30540], Loss: 3.8827\n",
      "Epoch [3/5], Step [26325/30540], Loss: 3.8094\n",
      "Epoch [3/5], Step [26400/30540], Loss: 3.9255\n",
      "Epoch [3/5], Step [26475/30540], Loss: 3.7435\n",
      "Epoch [3/5], Step [26550/30540], Loss: 3.7633\n",
      "Epoch [3/5], Step [26625/30540], Loss: 3.8644\n",
      "Epoch [3/5], Step [26700/30540], Loss: 3.8372\n",
      "Epoch [3/5], Step [26775/30540], Loss: 3.7856\n",
      "Epoch [3/5], Step [26850/30540], Loss: 3.9087\n",
      "Epoch [3/5], Step [26925/30540], Loss: 3.8740\n",
      "Epoch [3/5], Step [27000/30540], Loss: 3.7930\n",
      "Validation perplexity: 43.715153781999206\n",
      "Epoch [3/5], Step [27075/30540], Loss: 3.8977\n",
      "Epoch [3/5], Step [27150/30540], Loss: 3.7949\n",
      "Epoch [3/5], Step [27225/30540], Loss: 3.8262\n",
      "Epoch [3/5], Step [27300/30540], Loss: 3.8891\n",
      "Epoch [3/5], Step [27375/30540], Loss: 3.8117\n",
      "Epoch [3/5], Step [27450/30540], Loss: 3.8270\n",
      "Epoch [3/5], Step [27525/30540], Loss: 3.6958\n",
      "Epoch [3/5], Step [27600/30540], Loss: 3.8883\n",
      "Epoch [3/5], Step [27675/30540], Loss: 3.9369\n",
      "Epoch [3/5], Step [27750/30540], Loss: 3.8524\n",
      "Epoch [3/5], Step [27825/30540], Loss: 3.7285\n",
      "Epoch [3/5], Step [27900/30540], Loss: 3.7999\n",
      "Epoch [3/5], Step [27975/30540], Loss: 3.8499\n",
      "Epoch [3/5], Step [28050/30540], Loss: 3.6882\n",
      "Epoch [3/5], Step [28125/30540], Loss: 3.7438\n",
      "Epoch [3/5], Step [28200/30540], Loss: 3.7366\n",
      "Epoch [3/5], Step [28275/30540], Loss: 3.7962\n",
      "Epoch [3/5], Step [28350/30540], Loss: 3.7880\n",
      "Epoch [3/5], Step [28425/30540], Loss: 3.7963\n",
      "Epoch [3/5], Step [28500/30540], Loss: 3.8086\n",
      "Epoch [3/5], Step [28575/30540], Loss: 3.8639\n",
      "Epoch [3/5], Step [28650/30540], Loss: 3.7737\n",
      "Epoch [3/5], Step [28725/30540], Loss: 3.8414\n",
      "Epoch [3/5], Step [28800/30540], Loss: 3.7418\n",
      "Epoch [3/5], Step [28875/30540], Loss: 3.8222\n",
      "Epoch [3/5], Step [28950/30540], Loss: 3.8024\n",
      "Epoch [3/5], Step [29025/30540], Loss: 3.7921\n",
      "Epoch [3/5], Step [29100/30540], Loss: 3.8433\n",
      "Epoch [3/5], Step [29175/30540], Loss: 3.8869\n",
      "Epoch [3/5], Step [29250/30540], Loss: 3.8514\n",
      "Epoch [3/5], Step [29325/30540], Loss: 3.8456\n",
      "Epoch [3/5], Step [29400/30540], Loss: 3.8711\n",
      "Epoch [3/5], Step [29475/30540], Loss: 3.8246\n",
      "Epoch [3/5], Step [29550/30540], Loss: 3.9138\n",
      "Epoch [3/5], Step [29625/30540], Loss: 3.7578\n",
      "Epoch [3/5], Step [29700/30540], Loss: 3.7474\n",
      "Epoch [3/5], Step [29775/30540], Loss: 3.8321\n",
      "Epoch [3/5], Step [29850/30540], Loss: 3.7218\n",
      "Epoch [3/5], Step [29925/30540], Loss: 3.8612\n",
      "Epoch [3/5], Step [30000/30540], Loss: 3.7091\n",
      "Validation perplexity: 43.66480192662542\n",
      "Epoch [3/5], Step [30075/30540], Loss: 3.8112\n",
      "Epoch [3/5], Step [30150/30540], Loss: 3.6844\n",
      "Epoch [3/5], Step [30225/30540], Loss: 3.8740\n",
      "Epoch [3/5], Step [30300/30540], Loss: 3.8435\n",
      "Epoch [3/5], Step [30375/30540], Loss: 3.9403\n",
      "Epoch [3/5], Step [30450/30540], Loss: 3.8264\n",
      "Epoch [3/5], Step [30525/30540], Loss: 3.8546\n",
      "Epoch [3/5] Average Loss: 3.8329, Perplexity: 46.20\n",
      "Epoch [4/5], Step [0/30540], Loss: 3.8698\n",
      "Validation perplexity: 43.66578429940575\n",
      "Epoch [4/5], Step [75/30540], Loss: 3.8171\n",
      "Epoch [4/5], Step [150/30540], Loss: 3.8042\n",
      "Epoch [4/5], Step [225/30540], Loss: 3.7291\n",
      "Epoch [4/5], Step [300/30540], Loss: 3.8267\n",
      "Epoch [4/5], Step [375/30540], Loss: 3.6596\n",
      "Epoch [4/5], Step [450/30540], Loss: 3.8131\n",
      "Epoch [4/5], Step [525/30540], Loss: 3.6927\n",
      "Epoch [4/5], Step [600/30540], Loss: 3.7594\n",
      "Epoch [4/5], Step [675/30540], Loss: 3.8678\n",
      "Epoch [4/5], Step [750/30540], Loss: 3.7826\n",
      "Epoch [4/5], Step [825/30540], Loss: 3.8570\n",
      "Epoch [4/5], Step [900/30540], Loss: 3.9243\n",
      "Epoch [4/5], Step [975/30540], Loss: 3.8771\n",
      "Epoch [4/5], Step [1050/30540], Loss: 3.7192\n",
      "Epoch [4/5], Step [1125/30540], Loss: 3.8555\n",
      "Epoch [4/5], Step [1200/30540], Loss: 3.8064\n",
      "Epoch [4/5], Step [1275/30540], Loss: 3.7839\n",
      "Epoch [4/5], Step [1350/30540], Loss: 3.8189\n",
      "Epoch [4/5], Step [1425/30540], Loss: 3.8345\n",
      "Epoch [4/5], Step [1500/30540], Loss: 3.8758\n",
      "Epoch [4/5], Step [1575/30540], Loss: 3.8302\n",
      "Epoch [4/5], Step [1650/30540], Loss: 3.7974\n",
      "Epoch [4/5], Step [1725/30540], Loss: 3.8516\n",
      "Epoch [4/5], Step [1800/30540], Loss: 3.7046\n",
      "Epoch [4/5], Step [1875/30540], Loss: 3.7284\n",
      "Epoch [4/5], Step [1950/30540], Loss: 3.8970\n",
      "Epoch [4/5], Step [2025/30540], Loss: 3.8285\n",
      "Epoch [4/5], Step [2100/30540], Loss: 3.8524\n",
      "Epoch [4/5], Step [2175/30540], Loss: 3.7600\n",
      "Epoch [4/5], Step [2250/30540], Loss: 3.8249\n",
      "Epoch [4/5], Step [2325/30540], Loss: 3.7963\n",
      "Epoch [4/5], Step [2400/30540], Loss: 3.6881\n",
      "Epoch [4/5], Step [2475/30540], Loss: 3.8270\n",
      "Epoch [4/5], Step [2550/30540], Loss: 3.9120\n",
      "Epoch [4/5], Step [2625/30540], Loss: 3.8466\n",
      "Epoch [4/5], Step [2700/30540], Loss: 3.9246\n",
      "Epoch [4/5], Step [2775/30540], Loss: 3.7718\n",
      "Epoch [4/5], Step [2850/30540], Loss: 3.8167\n",
      "Epoch [4/5], Step [2925/30540], Loss: 3.7706\n",
      "Epoch [4/5], Step [3000/30540], Loss: 3.8830\n",
      "Validation perplexity: 43.669962378481685\n",
      "Epoch [4/5], Step [3075/30540], Loss: 3.7001\n",
      "Epoch [4/5], Step [3150/30540], Loss: 3.7851\n",
      "Epoch [4/5], Step [3225/30540], Loss: 3.8647\n",
      "Epoch [4/5], Step [3300/30540], Loss: 3.7950\n",
      "Epoch [4/5], Step [3375/30540], Loss: 3.7968\n",
      "Epoch [4/5], Step [3450/30540], Loss: 3.8141\n",
      "Epoch [4/5], Step [3525/30540], Loss: 3.8856\n",
      "Epoch [4/5], Step [3600/30540], Loss: 3.7783\n",
      "Epoch [4/5], Step [3675/30540], Loss: 3.7595\n",
      "Epoch [4/5], Step [3750/30540], Loss: 3.7655\n",
      "Epoch [4/5], Step [3825/30540], Loss: 3.7311\n",
      "Epoch [4/5], Step [3900/30540], Loss: 3.8368\n",
      "Epoch [4/5], Step [3975/30540], Loss: 3.8813\n",
      "Epoch [4/5], Step [4050/30540], Loss: 3.8106\n",
      "Epoch [4/5], Step [4125/30540], Loss: 3.7132\n",
      "Epoch [4/5], Step [4200/30540], Loss: 3.8707\n",
      "Epoch [4/5], Step [4275/30540], Loss: 3.8271\n",
      "Epoch [4/5], Step [4350/30540], Loss: 3.7203\n",
      "Epoch [4/5], Step [4425/30540], Loss: 3.8953\n",
      "Epoch [4/5], Step [4500/30540], Loss: 3.7323\n",
      "Epoch [4/5], Step [4575/30540], Loss: 3.8745\n",
      "Epoch [4/5], Step [4650/30540], Loss: 3.8332\n",
      "Epoch [4/5], Step [4725/30540], Loss: 3.7823\n",
      "Epoch [4/5], Step [4800/30540], Loss: 3.8201\n",
      "Epoch [4/5], Step [4875/30540], Loss: 3.8183\n",
      "Epoch [4/5], Step [4950/30540], Loss: 3.8083\n",
      "Epoch [4/5], Step [5025/30540], Loss: 3.8546\n",
      "Epoch [4/5], Step [5100/30540], Loss: 3.8318\n",
      "Epoch [4/5], Step [5175/30540], Loss: 3.8038\n",
      "Epoch [4/5], Step [5250/30540], Loss: 3.7423\n",
      "Epoch [4/5], Step [5325/30540], Loss: 3.9260\n",
      "Epoch [4/5], Step [5400/30540], Loss: 3.8267\n",
      "Epoch [4/5], Step [5475/30540], Loss: 3.8927\n",
      "Epoch [4/5], Step [5550/30540], Loss: 3.7198\n",
      "Epoch [4/5], Step [5625/30540], Loss: 3.8571\n",
      "Epoch [4/5], Step [5700/30540], Loss: 3.8851\n",
      "Epoch [4/5], Step [5775/30540], Loss: 3.8809\n",
      "Epoch [4/5], Step [5850/30540], Loss: 3.8749\n",
      "Epoch [4/5], Step [5925/30540], Loss: 3.8941\n",
      "Epoch [4/5], Step [6000/30540], Loss: 3.7508\n",
      "Validation perplexity: 43.57815806824027\n",
      "Epoch [4/5], Step [6075/30540], Loss: 3.9612\n",
      "Epoch [4/5], Step [6150/30540], Loss: 3.8570\n",
      "Epoch [4/5], Step [6225/30540], Loss: 3.7693\n",
      "Epoch [4/5], Step [6300/30540], Loss: 3.8325\n",
      "Epoch [4/5], Step [6375/30540], Loss: 3.7360\n",
      "Epoch [4/5], Step [6450/30540], Loss: 3.7993\n",
      "Epoch [4/5], Step [6525/30540], Loss: 3.8490\n",
      "Epoch [4/5], Step [6600/30540], Loss: 3.8546\n",
      "Epoch [4/5], Step [6675/30540], Loss: 3.8463\n",
      "Epoch [4/5], Step [6750/30540], Loss: 3.8081\n",
      "Epoch [4/5], Step [6825/30540], Loss: 3.6968\n",
      "Epoch [4/5], Step [6900/30540], Loss: 3.8454\n",
      "Epoch [4/5], Step [6975/30540], Loss: 3.9549\n",
      "Epoch [4/5], Step [7050/30540], Loss: 3.9663\n",
      "Epoch [4/5], Step [7125/30540], Loss: 3.8250\n",
      "Epoch [4/5], Step [7200/30540], Loss: 3.8274\n",
      "Epoch [4/5], Step [7275/30540], Loss: 3.8437\n",
      "Epoch [4/5], Step [7350/30540], Loss: 3.7542\n",
      "Epoch [4/5], Step [7425/30540], Loss: 3.7878\n",
      "Epoch [4/5], Step [7500/30540], Loss: 3.8389\n",
      "Epoch [4/5], Step [7575/30540], Loss: 3.6841\n",
      "Epoch [4/5], Step [7650/30540], Loss: 3.9266\n",
      "Epoch [4/5], Step [7725/30540], Loss: 3.8974\n",
      "Epoch [4/5], Step [7800/30540], Loss: 3.8677\n",
      "Epoch [4/5], Step [7875/30540], Loss: 3.8089\n",
      "Epoch [4/5], Step [7950/30540], Loss: 3.8112\n",
      "Epoch [4/5], Step [8025/30540], Loss: 3.8081\n",
      "Epoch [4/5], Step [8100/30540], Loss: 3.8262\n",
      "Epoch [4/5], Step [8175/30540], Loss: 3.8798\n",
      "Epoch [4/5], Step [8250/30540], Loss: 3.9169\n",
      "Epoch [4/5], Step [8325/30540], Loss: 3.7510\n",
      "Epoch [4/5], Step [8400/30540], Loss: 3.8175\n",
      "Epoch [4/5], Step [8475/30540], Loss: 3.8763\n",
      "Epoch [4/5], Step [8550/30540], Loss: 3.8724\n",
      "Epoch [4/5], Step [8625/30540], Loss: 3.6001\n",
      "Epoch [4/5], Step [8700/30540], Loss: 3.6673\n",
      "Epoch [4/5], Step [8775/30540], Loss: 3.8303\n",
      "Epoch [4/5], Step [8850/30540], Loss: 3.8571\n",
      "Epoch [4/5], Step [8925/30540], Loss: 3.8228\n",
      "Epoch [4/5], Step [9000/30540], Loss: 3.9429\n",
      "Validation perplexity: 43.48500423413966\n",
      "Epoch [4/5], Step [9075/30540], Loss: 3.7750\n",
      "Epoch [4/5], Step [9150/30540], Loss: 3.8243\n",
      "Epoch [4/5], Step [9225/30540], Loss: 3.9243\n",
      "Epoch [4/5], Step [9300/30540], Loss: 3.7825\n",
      "Epoch [4/5], Step [9375/30540], Loss: 3.8263\n",
      "Epoch [4/5], Step [9450/30540], Loss: 3.7832\n",
      "Epoch [4/5], Step [9525/30540], Loss: 3.9011\n",
      "Epoch [4/5], Step [9600/30540], Loss: 3.8954\n",
      "Epoch [4/5], Step [9675/30540], Loss: 3.9189\n",
      "Epoch [4/5], Step [9750/30540], Loss: 3.6996\n",
      "Epoch [4/5], Step [9825/30540], Loss: 3.6576\n",
      "Epoch [4/5], Step [9900/30540], Loss: 3.8323\n",
      "Epoch [4/5], Step [9975/30540], Loss: 3.8402\n",
      "Epoch [4/5], Step [10050/30540], Loss: 3.7578\n",
      "Epoch [4/5], Step [10125/30540], Loss: 3.8374\n",
      "Epoch [4/5], Step [10200/30540], Loss: 3.8328\n",
      "Epoch [4/5], Step [10275/30540], Loss: 3.9227\n",
      "Epoch [4/5], Step [10350/30540], Loss: 3.7357\n",
      "Epoch [4/5], Step [10425/30540], Loss: 3.6707\n",
      "Epoch [4/5], Step [10500/30540], Loss: 3.8359\n",
      "Epoch [4/5], Step [10575/30540], Loss: 3.9027\n",
      "Epoch [4/5], Step [10650/30540], Loss: 3.7441\n",
      "Epoch [4/5], Step [10725/30540], Loss: 3.7886\n",
      "Epoch [4/5], Step [10800/30540], Loss: 3.9078\n",
      "Epoch [4/5], Step [10875/30540], Loss: 3.7706\n",
      "Epoch [4/5], Step [10950/30540], Loss: 3.7496\n",
      "Epoch [4/5], Step [11025/30540], Loss: 3.8796\n",
      "Epoch [4/5], Step [11100/30540], Loss: 3.7124\n",
      "Epoch [4/5], Step [11175/30540], Loss: 3.7238\n",
      "Epoch [4/5], Step [11250/30540], Loss: 3.8436\n",
      "Epoch [4/5], Step [11325/30540], Loss: 3.7783\n",
      "Epoch [4/5], Step [11400/30540], Loss: 3.8454\n",
      "Epoch [4/5], Step [11475/30540], Loss: 3.8269\n",
      "Epoch [4/5], Step [11550/30540], Loss: 3.8155\n",
      "Epoch [4/5], Step [11625/30540], Loss: 3.7605\n",
      "Epoch [4/5], Step [11700/30540], Loss: 3.8295\n",
      "Epoch [4/5], Step [11775/30540], Loss: 3.8319\n",
      "Epoch [4/5], Step [11850/30540], Loss: 3.8173\n",
      "Epoch [4/5], Step [11925/30540], Loss: 3.8668\n",
      "Epoch [4/5], Step [12000/30540], Loss: 3.7686\n",
      "Validation perplexity: 43.42563914749143\n",
      "Epoch [4/5], Step [12075/30540], Loss: 3.6933\n",
      "Epoch [4/5], Step [12150/30540], Loss: 3.7237\n",
      "Epoch [4/5], Step [12225/30540], Loss: 3.7417\n",
      "Epoch [4/5], Step [12300/30540], Loss: 3.7681\n",
      "Epoch [4/5], Step [12375/30540], Loss: 3.8891\n",
      "Epoch [4/5], Step [12450/30540], Loss: 3.8767\n",
      "Epoch [4/5], Step [12525/30540], Loss: 3.8108\n",
      "Epoch [4/5], Step [12600/30540], Loss: 3.7161\n",
      "Epoch [4/5], Step [12675/30540], Loss: 3.8229\n",
      "Epoch [4/5], Step [12750/30540], Loss: 3.9142\n",
      "Epoch [4/5], Step [12825/30540], Loss: 3.8385\n",
      "Epoch [4/5], Step [12900/30540], Loss: 3.9001\n",
      "Epoch [4/5], Step [12975/30540], Loss: 3.7269\n",
      "Epoch [4/5], Step [13050/30540], Loss: 3.7804\n",
      "Epoch [4/5], Step [13125/30540], Loss: 3.8723\n",
      "Epoch [4/5], Step [13200/30540], Loss: 3.9045\n",
      "Epoch [4/5], Step [13275/30540], Loss: 3.9053\n",
      "Epoch [4/5], Step [13350/30540], Loss: 3.8194\n",
      "Epoch [4/5], Step [13425/30540], Loss: 3.8528\n",
      "Epoch [4/5], Step [13500/30540], Loss: 3.7997\n",
      "Epoch [4/5], Step [13575/30540], Loss: 3.9544\n",
      "Epoch [4/5], Step [13650/30540], Loss: 3.8567\n",
      "Epoch [4/5], Step [13725/30540], Loss: 3.7952\n",
      "Epoch [4/5], Step [13800/30540], Loss: 3.7970\n",
      "Epoch [4/5], Step [13875/30540], Loss: 3.8110\n",
      "Epoch [4/5], Step [13950/30540], Loss: 3.8399\n",
      "Epoch [4/5], Step [14025/30540], Loss: 3.8185\n",
      "Epoch [4/5], Step [14100/30540], Loss: 3.7947\n",
      "Epoch [4/5], Step [14175/30540], Loss: 3.7966\n",
      "Epoch [4/5], Step [14250/30540], Loss: 3.8136\n",
      "Epoch [4/5], Step [14325/30540], Loss: 3.7871\n",
      "Epoch [4/5], Step [14400/30540], Loss: 3.8225\n",
      "Epoch [4/5], Step [14475/30540], Loss: 3.7601\n",
      "Epoch [4/5], Step [14550/30540], Loss: 3.8768\n",
      "Epoch [4/5], Step [14625/30540], Loss: 3.7312\n",
      "Epoch [4/5], Step [14700/30540], Loss: 3.7992\n",
      "Epoch [4/5], Step [14775/30540], Loss: 3.8263\n",
      "Epoch [4/5], Step [14850/30540], Loss: 3.8942\n",
      "Epoch [4/5], Step [14925/30540], Loss: 3.8341\n",
      "Epoch [4/5], Step [15000/30540], Loss: 3.8028\n",
      "Validation perplexity: 43.39432830464756\n",
      "Epoch [4/5], Step [15075/30540], Loss: 3.8476\n",
      "Epoch [4/5], Step [15150/30540], Loss: 3.9385\n",
      "Epoch [4/5], Step [15225/30540], Loss: 3.7788\n",
      "Epoch [4/5], Step [15300/30540], Loss: 3.8167\n",
      "Epoch [4/5], Step [15375/30540], Loss: 3.8593\n",
      "Epoch [4/5], Step [15450/30540], Loss: 3.7790\n",
      "Epoch [4/5], Step [15525/30540], Loss: 3.7455\n",
      "Epoch [4/5], Step [15600/30540], Loss: 3.8098\n",
      "Epoch [4/5], Step [15675/30540], Loss: 3.9302\n",
      "Epoch [4/5], Step [15750/30540], Loss: 3.7075\n",
      "Epoch [4/5], Step [15825/30540], Loss: 3.8467\n",
      "Epoch [4/5], Step [15900/30540], Loss: 3.7194\n",
      "Epoch [4/5], Step [15975/30540], Loss: 3.7767\n",
      "Epoch [4/5], Step [16050/30540], Loss: 3.9027\n",
      "Epoch [4/5], Step [16125/30540], Loss: 3.8719\n",
      "Epoch [4/5], Step [16200/30540], Loss: 3.7895\n",
      "Epoch [4/5], Step [16275/30540], Loss: 3.8320\n",
      "Epoch [4/5], Step [16350/30540], Loss: 3.8110\n",
      "Epoch [4/5], Step [16425/30540], Loss: 3.7782\n",
      "Epoch [4/5], Step [16500/30540], Loss: 3.8520\n",
      "Epoch [4/5], Step [16575/30540], Loss: 3.8079\n",
      "Epoch [4/5], Step [16650/30540], Loss: 3.8106\n",
      "Epoch [4/5], Step [16725/30540], Loss: 3.7845\n",
      "Epoch [4/5], Step [16800/30540], Loss: 3.8908\n",
      "Epoch [4/5], Step [16875/30540], Loss: 3.7692\n",
      "Epoch [4/5], Step [16950/30540], Loss: 3.8793\n",
      "Epoch [4/5], Step [17025/30540], Loss: 3.8196\n",
      "Epoch [4/5], Step [17100/30540], Loss: 3.8804\n",
      "Epoch [4/5], Step [17175/30540], Loss: 3.8104\n",
      "Epoch [4/5], Step [17250/30540], Loss: 3.7450\n",
      "Epoch [4/5], Step [17325/30540], Loss: 3.7950\n",
      "Epoch [4/5], Step [17400/30540], Loss: 3.7162\n",
      "Epoch [4/5], Step [17475/30540], Loss: 3.8949\n",
      "Epoch [4/5], Step [17550/30540], Loss: 3.9345\n",
      "Epoch [4/5], Step [17625/30540], Loss: 3.7964\n",
      "Epoch [4/5], Step [17700/30540], Loss: 3.9138\n",
      "Epoch [4/5], Step [17775/30540], Loss: 3.8368\n",
      "Epoch [4/5], Step [17850/30540], Loss: 3.7246\n",
      "Epoch [4/5], Step [17925/30540], Loss: 3.8860\n",
      "Epoch [4/5], Step [18000/30540], Loss: 3.7821\n",
      "Validation perplexity: 43.30604929123894\n",
      "Epoch [4/5], Step [18075/30540], Loss: 3.7463\n",
      "Epoch [4/5], Step [18150/30540], Loss: 3.7559\n",
      "Epoch [4/5], Step [18225/30540], Loss: 3.8466\n",
      "Epoch [4/5], Step [18300/30540], Loss: 3.7395\n",
      "Epoch [4/5], Step [18375/30540], Loss: 3.7481\n",
      "Epoch [4/5], Step [18450/30540], Loss: 3.7980\n",
      "Epoch [4/5], Step [18525/30540], Loss: 3.8622\n",
      "Epoch [4/5], Step [18600/30540], Loss: 3.8320\n",
      "Epoch [4/5], Step [18675/30540], Loss: 3.7580\n",
      "Epoch [4/5], Step [18750/30540], Loss: 3.7425\n",
      "Epoch [4/5], Step [18825/30540], Loss: 3.7635\n",
      "Epoch [4/5], Step [18900/30540], Loss: 3.9002\n",
      "Epoch [4/5], Step [18975/30540], Loss: 3.7809\n",
      "Epoch [4/5], Step [19050/30540], Loss: 3.8365\n",
      "Epoch [4/5], Step [19125/30540], Loss: 3.7048\n",
      "Epoch [4/5], Step [19200/30540], Loss: 3.8209\n",
      "Epoch [4/5], Step [19275/30540], Loss: 3.7828\n",
      "Epoch [4/5], Step [19350/30540], Loss: 3.6792\n",
      "Epoch [4/5], Step [19425/30540], Loss: 3.6627\n",
      "Epoch [4/5], Step [19500/30540], Loss: 3.7911\n",
      "Epoch [4/5], Step [19575/30540], Loss: 3.7461\n",
      "Epoch [4/5], Step [19650/30540], Loss: 3.9348\n",
      "Epoch [4/5], Step [19725/30540], Loss: 3.7869\n",
      "Epoch [4/5], Step [19800/30540], Loss: 3.8587\n",
      "Epoch [4/5], Step [19875/30540], Loss: 3.8128\n",
      "Epoch [4/5], Step [19950/30540], Loss: 3.8690\n",
      "Epoch [4/5], Step [20025/30540], Loss: 3.7997\n",
      "Epoch [4/5], Step [20100/30540], Loss: 3.8935\n",
      "Epoch [4/5], Step [20175/30540], Loss: 3.8502\n",
      "Epoch [4/5], Step [20250/30540], Loss: 3.7626\n",
      "Epoch [4/5], Step [20325/30540], Loss: 3.8552\n",
      "Epoch [4/5], Step [20400/30540], Loss: 3.8843\n",
      "Epoch [4/5], Step [20475/30540], Loss: 3.8781\n",
      "Epoch [4/5], Step [20550/30540], Loss: 3.8744\n",
      "Epoch [4/5], Step [20625/30540], Loss: 3.8387\n",
      "Epoch [4/5], Step [20700/30540], Loss: 3.8346\n",
      "Epoch [4/5], Step [20775/30540], Loss: 3.6334\n",
      "Epoch [4/5], Step [20850/30540], Loss: 3.7887\n",
      "Epoch [4/5], Step [20925/30540], Loss: 3.8077\n",
      "Epoch [4/5], Step [21000/30540], Loss: 3.9764\n",
      "Validation perplexity: 43.22651620576457\n",
      "Epoch [4/5], Step [21075/30540], Loss: 3.7853\n",
      "Epoch [4/5], Step [21150/30540], Loss: 3.7419\n",
      "Epoch [4/5], Step [21225/30540], Loss: 3.9100\n",
      "Epoch [4/5], Step [21300/30540], Loss: 3.8420\n",
      "Epoch [4/5], Step [21375/30540], Loss: 3.7613\n",
      "Epoch [4/5], Step [21450/30540], Loss: 3.7824\n",
      "Epoch [4/5], Step [21525/30540], Loss: 3.8411\n",
      "Epoch [4/5], Step [21600/30540], Loss: 3.8641\n",
      "Epoch [4/5], Step [21675/30540], Loss: 3.7848\n",
      "Epoch [4/5], Step [21750/30540], Loss: 3.8468\n",
      "Epoch [4/5], Step [21825/30540], Loss: 3.7189\n",
      "Epoch [4/5], Step [21900/30540], Loss: 3.7069\n",
      "Epoch [4/5], Step [21975/30540], Loss: 3.7684\n",
      "Epoch [4/5], Step [22050/30540], Loss: 3.9174\n",
      "Epoch [4/5], Step [22125/30540], Loss: 3.7280\n",
      "Epoch [4/5], Step [22200/30540], Loss: 3.8568\n",
      "Epoch [4/5], Step [22275/30540], Loss: 3.9549\n",
      "Epoch [4/5], Step [22350/30540], Loss: 3.8486\n",
      "Epoch [4/5], Step [22425/30540], Loss: 3.8768\n",
      "Epoch [4/5], Step [22500/30540], Loss: 3.8674\n",
      "Epoch [4/5], Step [22575/30540], Loss: 3.6640\n",
      "Epoch [4/5], Step [22650/30540], Loss: 3.9369\n",
      "Epoch [4/5], Step [22725/30540], Loss: 3.7695\n",
      "Epoch [4/5], Step [22800/30540], Loss: 3.8915\n",
      "Epoch [4/5], Step [22875/30540], Loss: 3.7634\n",
      "Epoch [4/5], Step [22950/30540], Loss: 3.9570\n",
      "Epoch [4/5], Step [23025/30540], Loss: 3.7087\n",
      "Epoch [4/5], Step [23100/30540], Loss: 3.6839\n",
      "Epoch [4/5], Step [23175/30540], Loss: 3.7960\n",
      "Epoch [4/5], Step [23250/30540], Loss: 3.7224\n",
      "Epoch [4/5], Step [23325/30540], Loss: 3.8334\n",
      "Epoch [4/5], Step [23400/30540], Loss: 3.9089\n",
      "Epoch [4/5], Step [23475/30540], Loss: 3.8171\n",
      "Epoch [4/5], Step [23550/30540], Loss: 3.9237\n",
      "Epoch [4/5], Step [23625/30540], Loss: 3.7901\n",
      "Epoch [4/5], Step [23700/30540], Loss: 3.7666\n",
      "Epoch [4/5], Step [23775/30540], Loss: 3.8381\n",
      "Epoch [4/5], Step [23850/30540], Loss: 3.7870\n",
      "Epoch [4/5], Step [23925/30540], Loss: 3.8361\n",
      "Epoch [4/5], Step [24000/30540], Loss: 3.8694\n",
      "Validation perplexity: 43.15873299216714\n",
      "Epoch [4/5], Step [24075/30540], Loss: 3.8372\n",
      "Epoch [4/5], Step [24150/30540], Loss: 3.8860\n",
      "Epoch [4/5], Step [24225/30540], Loss: 3.6869\n",
      "Epoch [4/5], Step [24300/30540], Loss: 3.7433\n",
      "Epoch [4/5], Step [24375/30540], Loss: 3.7792\n",
      "Epoch [4/5], Step [24450/30540], Loss: 3.6969\n",
      "Epoch [4/5], Step [24525/30540], Loss: 3.7776\n",
      "Epoch [4/5], Step [24600/30540], Loss: 3.7690\n",
      "Epoch [4/5], Step [24675/30540], Loss: 3.6824\n",
      "Epoch [4/5], Step [24750/30540], Loss: 3.8188\n",
      "Epoch [4/5], Step [24825/30540], Loss: 3.8016\n",
      "Epoch [4/5], Step [24900/30540], Loss: 3.9075\n",
      "Epoch [4/5], Step [24975/30540], Loss: 3.8313\n",
      "Epoch [4/5], Step [25050/30540], Loss: 3.8772\n",
      "Epoch [4/5], Step [25125/30540], Loss: 3.8450\n",
      "Epoch [4/5], Step [25200/30540], Loss: 3.8992\n",
      "Epoch [4/5], Step [25275/30540], Loss: 3.7891\n",
      "Epoch [4/5], Step [25350/30540], Loss: 3.8728\n",
      "Epoch [4/5], Step [25425/30540], Loss: 3.7896\n",
      "Epoch [4/5], Step [25500/30540], Loss: 3.9028\n",
      "Epoch [4/5], Step [25575/30540], Loss: 3.8110\n",
      "Epoch [4/5], Step [25650/30540], Loss: 3.7928\n",
      "Epoch [4/5], Step [25725/30540], Loss: 3.6469\n",
      "Epoch [4/5], Step [25800/30540], Loss: 3.9003\n",
      "Epoch [4/5], Step [25875/30540], Loss: 3.7886\n",
      "Epoch [4/5], Step [25950/30540], Loss: 3.7385\n",
      "Epoch [4/5], Step [26025/30540], Loss: 3.9196\n",
      "Epoch [4/5], Step [26100/30540], Loss: 3.8742\n",
      "Epoch [4/5], Step [26175/30540], Loss: 3.8556\n",
      "Epoch [4/5], Step [26250/30540], Loss: 3.8624\n",
      "Epoch [4/5], Step [26325/30540], Loss: 3.7862\n",
      "Epoch [4/5], Step [26400/30540], Loss: 3.7517\n",
      "Epoch [4/5], Step [26475/30540], Loss: 3.6317\n",
      "Epoch [4/5], Step [26550/30540], Loss: 3.7406\n",
      "Epoch [4/5], Step [26625/30540], Loss: 3.8040\n",
      "Epoch [4/5], Step [26700/30540], Loss: 3.6584\n",
      "Epoch [4/5], Step [26775/30540], Loss: 3.7203\n",
      "Epoch [4/5], Step [26850/30540], Loss: 3.7990\n",
      "Epoch [4/5], Step [26925/30540], Loss: 3.9116\n",
      "Epoch [4/5], Step [27000/30540], Loss: 3.7294\n",
      "Validation perplexity: 43.18527809053802\n",
      "Epoch [4/5], Step [27075/30540], Loss: 3.8867\n",
      "Epoch [4/5], Step [27150/30540], Loss: 3.8352\n",
      "Epoch [4/5], Step [27225/30540], Loss: 3.7698\n",
      "Epoch [4/5], Step [27300/30540], Loss: 3.8362\n",
      "Epoch [4/5], Step [27375/30540], Loss: 3.8290\n",
      "Epoch [4/5], Step [27450/30540], Loss: 3.8588\n",
      "Epoch [4/5], Step [27525/30540], Loss: 3.8678\n",
      "Epoch [4/5], Step [27600/30540], Loss: 3.7144\n",
      "Epoch [4/5], Step [27675/30540], Loss: 3.7664\n",
      "Epoch [4/5], Step [27750/30540], Loss: 3.8687\n",
      "Epoch [4/5], Step [27825/30540], Loss: 3.8429\n",
      "Epoch [4/5], Step [27900/30540], Loss: 3.7660\n",
      "Epoch [4/5], Step [27975/30540], Loss: 3.8910\n",
      "Epoch [4/5], Step [28050/30540], Loss: 3.8263\n",
      "Epoch [4/5], Step [28125/30540], Loss: 3.7670\n",
      "Epoch [4/5], Step [28200/30540], Loss: 3.7496\n",
      "Epoch [4/5], Step [28275/30540], Loss: 3.9462\n",
      "Epoch [4/5], Step [28350/30540], Loss: 3.7321\n",
      "Epoch [4/5], Step [28425/30540], Loss: 3.7759\n",
      "Epoch [4/5], Step [28500/30540], Loss: 3.8124\n",
      "Epoch [4/5], Step [28575/30540], Loss: 3.6362\n",
      "Epoch [4/5], Step [28650/30540], Loss: 3.8093\n",
      "Epoch [4/5], Step [28725/30540], Loss: 3.6608\n",
      "Epoch [4/5], Step [28800/30540], Loss: 3.8635\n",
      "Epoch [4/5], Step [28875/30540], Loss: 3.8718\n",
      "Epoch [4/5], Step [28950/30540], Loss: 3.7895\n",
      "Epoch [4/5], Step [29025/30540], Loss: 3.7261\n",
      "Epoch [4/5], Step [29100/30540], Loss: 3.8482\n",
      "Epoch [4/5], Step [29175/30540], Loss: 3.8157\n",
      "Epoch [4/5], Step [29250/30540], Loss: 3.8320\n",
      "Epoch [4/5], Step [29325/30540], Loss: 3.7933\n",
      "Epoch [4/5], Step [29400/30540], Loss: 3.7918\n",
      "Epoch [4/5], Step [29475/30540], Loss: 3.8119\n",
      "Epoch [4/5], Step [29550/30540], Loss: 3.8972\n",
      "Epoch [4/5], Step [29625/30540], Loss: 3.7928\n",
      "Epoch [4/5], Step [29700/30540], Loss: 3.8622\n",
      "Epoch [4/5], Step [29775/30540], Loss: 3.7812\n",
      "Epoch [4/5], Step [29850/30540], Loss: 3.8113\n",
      "Epoch [4/5], Step [29925/30540], Loss: 3.8240\n",
      "Epoch [4/5], Step [30000/30540], Loss: 3.7222\n",
      "Validation perplexity: 43.13290676889861\n",
      "Epoch [4/5], Step [30075/30540], Loss: 3.7732\n",
      "Epoch [4/5], Step [30150/30540], Loss: 3.9230\n",
      "Epoch [4/5], Step [30225/30540], Loss: 3.8559\n",
      "Epoch [4/5], Step [30300/30540], Loss: 3.8125\n",
      "Epoch [4/5], Step [30375/30540], Loss: 3.7648\n",
      "Epoch [4/5], Step [30450/30540], Loss: 3.8468\n",
      "Epoch [4/5], Step [30525/30540], Loss: 3.8300\n",
      "Epoch [4/5] Average Loss: 3.8089, Perplexity: 45.10\n",
      "Epoch [5/5], Step [0/30540], Loss: 3.7159\n",
      "Validation perplexity: 43.098172091773705\n",
      "Epoch [5/5], Step [75/30540], Loss: 3.9053\n",
      "Epoch [5/5], Step [150/30540], Loss: 3.8781\n",
      "Epoch [5/5], Step [225/30540], Loss: 3.7614\n",
      "Epoch [5/5], Step [300/30540], Loss: 3.8070\n",
      "Epoch [5/5], Step [375/30540], Loss: 3.7902\n",
      "Epoch [5/5], Step [450/30540], Loss: 3.8461\n",
      "Epoch [5/5], Step [525/30540], Loss: 3.8817\n",
      "Epoch [5/5], Step [600/30540], Loss: 3.8310\n",
      "Epoch [5/5], Step [675/30540], Loss: 3.7590\n",
      "Epoch [5/5], Step [750/30540], Loss: 3.7615\n",
      "Epoch [5/5], Step [825/30540], Loss: 3.7149\n",
      "Epoch [5/5], Step [900/30540], Loss: 3.8077\n",
      "Epoch [5/5], Step [975/30540], Loss: 3.8297\n",
      "Epoch [5/5], Step [1050/30540], Loss: 3.6760\n",
      "Epoch [5/5], Step [1125/30540], Loss: 3.7995\n",
      "Epoch [5/5], Step [1200/30540], Loss: 3.7923\n",
      "Epoch [5/5], Step [1275/30540], Loss: 3.8478\n",
      "Epoch [5/5], Step [1350/30540], Loss: 3.8054\n",
      "Epoch [5/5], Step [1425/30540], Loss: 3.7463\n",
      "Epoch [5/5], Step [1500/30540], Loss: 3.7385\n",
      "Epoch [5/5], Step [1575/30540], Loss: 3.8050\n",
      "Epoch [5/5], Step [1650/30540], Loss: 3.8420\n",
      "Epoch [5/5], Step [1725/30540], Loss: 3.8329\n",
      "Epoch [5/5], Step [1800/30540], Loss: 3.7984\n",
      "Epoch [5/5], Step [1875/30540], Loss: 3.8668\n",
      "Epoch [5/5], Step [1950/30540], Loss: 3.8327\n",
      "Epoch [5/5], Step [2025/30540], Loss: 3.7443\n",
      "Epoch [5/5], Step [2100/30540], Loss: 3.7671\n",
      "Epoch [5/5], Step [2175/30540], Loss: 3.8438\n",
      "Epoch [5/5], Step [2250/30540], Loss: 3.7538\n",
      "Epoch [5/5], Step [2325/30540], Loss: 3.7759\n",
      "Epoch [5/5], Step [2400/30540], Loss: 3.8151\n",
      "Epoch [5/5], Step [2475/30540], Loss: 3.7367\n",
      "Epoch [5/5], Step [2550/30540], Loss: 3.9132\n",
      "Epoch [5/5], Step [2625/30540], Loss: 3.8252\n",
      "Epoch [5/5], Step [2700/30540], Loss: 3.7015\n",
      "Epoch [5/5], Step [2775/30540], Loss: 3.8240\n",
      "Epoch [5/5], Step [2850/30540], Loss: 3.8164\n",
      "Epoch [5/5], Step [2925/30540], Loss: 3.8643\n",
      "Epoch [5/5], Step [3000/30540], Loss: 3.9112\n",
      "Validation perplexity: 43.11806663030825\n",
      "Epoch [5/5], Step [3075/30540], Loss: 3.7570\n",
      "Epoch [5/5], Step [3150/30540], Loss: 3.7599\n",
      "Epoch [5/5], Step [3225/30540], Loss: 3.8770\n",
      "Epoch [5/5], Step [3300/30540], Loss: 3.8836\n",
      "Epoch [5/5], Step [3375/30540], Loss: 3.7881\n",
      "Epoch [5/5], Step [3450/30540], Loss: 3.8358\n",
      "Epoch [5/5], Step [3525/30540], Loss: 3.7254\n",
      "Epoch [5/5], Step [3600/30540], Loss: 3.8217\n",
      "Epoch [5/5], Step [3675/30540], Loss: 3.7134\n",
      "Epoch [5/5], Step [3750/30540], Loss: 3.7739\n",
      "Epoch [5/5], Step [3825/30540], Loss: 3.8009\n",
      "Epoch [5/5], Step [3900/30540], Loss: 3.8524\n",
      "Epoch [5/5], Step [3975/30540], Loss: 3.7533\n",
      "Epoch [5/5], Step [4050/30540], Loss: 3.7703\n",
      "Epoch [5/5], Step [4125/30540], Loss: 3.8018\n",
      "Epoch [5/5], Step [4200/30540], Loss: 3.8151\n",
      "Epoch [5/5], Step [4275/30540], Loss: 3.7642\n",
      "Epoch [5/5], Step [4350/30540], Loss: 3.7563\n",
      "Epoch [5/5], Step [4425/30540], Loss: 3.8493\n",
      "Epoch [5/5], Step [4500/30540], Loss: 3.7459\n",
      "Epoch [5/5], Step [4575/30540], Loss: 3.7141\n",
      "Epoch [5/5], Step [4650/30540], Loss: 3.7784\n",
      "Epoch [5/5], Step [4725/30540], Loss: 3.7919\n",
      "Epoch [5/5], Step [4800/30540], Loss: 3.8194\n",
      "Epoch [5/5], Step [4875/30540], Loss: 3.7803\n",
      "Epoch [5/5], Step [4950/30540], Loss: 3.8230\n",
      "Epoch [5/5], Step [5025/30540], Loss: 3.6696\n",
      "Epoch [5/5], Step [5100/30540], Loss: 3.6676\n",
      "Epoch [5/5], Step [5175/30540], Loss: 3.9059\n",
      "Epoch [5/5], Step [5250/30540], Loss: 3.7432\n",
      "Epoch [5/5], Step [5325/30540], Loss: 3.7646\n",
      "Epoch [5/5], Step [5400/30540], Loss: 3.8060\n",
      "Epoch [5/5], Step [5475/30540], Loss: 3.7537\n",
      "Epoch [5/5], Step [5550/30540], Loss: 3.9153\n",
      "Epoch [5/5], Step [5625/30540], Loss: 3.7170\n",
      "Epoch [5/5], Step [5700/30540], Loss: 3.7552\n",
      "Epoch [5/5], Step [5775/30540], Loss: 3.8464\n",
      "Epoch [5/5], Step [5850/30540], Loss: 3.8268\n",
      "Epoch [5/5], Step [5925/30540], Loss: 3.7962\n",
      "Epoch [5/5], Step [6000/30540], Loss: 3.8910\n",
      "Validation perplexity: 43.08816063312337\n",
      "Epoch [5/5], Step [6075/30540], Loss: 3.7506\n",
      "Epoch [5/5], Step [6150/30540], Loss: 3.8129\n",
      "Epoch [5/5], Step [6225/30540], Loss: 3.8538\n",
      "Epoch [5/5], Step [6300/30540], Loss: 3.7790\n",
      "Epoch [5/5], Step [6375/30540], Loss: 3.6758\n",
      "Epoch [5/5], Step [6450/30540], Loss: 3.8823\n",
      "Epoch [5/5], Step [6525/30540], Loss: 3.8344\n",
      "Epoch [5/5], Step [6600/30540], Loss: 3.7329\n",
      "Epoch [5/5], Step [6675/30540], Loss: 3.7493\n",
      "Epoch [5/5], Step [6750/30540], Loss: 3.7370\n",
      "Epoch [5/5], Step [6825/30540], Loss: 3.8890\n",
      "Epoch [5/5], Step [6900/30540], Loss: 3.7807\n",
      "Epoch [5/5], Step [6975/30540], Loss: 3.8201\n",
      "Epoch [5/5], Step [7050/30540], Loss: 3.7068\n",
      "Epoch [5/5], Step [7125/30540], Loss: 3.7963\n",
      "Epoch [5/5], Step [7200/30540], Loss: 3.7992\n",
      "Epoch [5/5], Step [7275/30540], Loss: 3.7638\n",
      "Epoch [5/5], Step [7350/30540], Loss: 3.8013\n",
      "Epoch [5/5], Step [7425/30540], Loss: 3.7687\n",
      "Epoch [5/5], Step [7500/30540], Loss: 3.9148\n",
      "Epoch [5/5], Step [7575/30540], Loss: 3.8518\n",
      "Epoch [5/5], Step [7650/30540], Loss: 3.7568\n",
      "Epoch [5/5], Step [7725/30540], Loss: 3.7353\n",
      "Epoch [5/5], Step [7800/30540], Loss: 3.8323\n",
      "Epoch [5/5], Step [7875/30540], Loss: 3.9403\n",
      "Epoch [5/5], Step [7950/30540], Loss: 3.8438\n",
      "Epoch [5/5], Step [8025/30540], Loss: 3.8113\n",
      "Epoch [5/5], Step [8100/30540], Loss: 3.7108\n",
      "Epoch [5/5], Step [8175/30540], Loss: 3.6883\n",
      "Epoch [5/5], Step [8250/30540], Loss: 3.8252\n",
      "Epoch [5/5], Step [8325/30540], Loss: 3.8070\n",
      "Epoch [5/5], Step [8400/30540], Loss: 3.7464\n",
      "Epoch [5/5], Step [8475/30540], Loss: 3.7332\n",
      "Epoch [5/5], Step [8550/30540], Loss: 3.8282\n",
      "Epoch [5/5], Step [8625/30540], Loss: 3.8067\n",
      "Epoch [5/5], Step [8700/30540], Loss: 3.8868\n",
      "Epoch [5/5], Step [8775/30540], Loss: 3.7162\n",
      "Epoch [5/5], Step [8850/30540], Loss: 3.7849\n",
      "Epoch [5/5], Step [8925/30540], Loss: 3.8254\n",
      "Epoch [5/5], Step [9000/30540], Loss: 3.8034\n",
      "Validation perplexity: 43.10288069457818\n",
      "Epoch [5/5], Step [9075/30540], Loss: 3.7568\n",
      "Epoch [5/5], Step [9150/30540], Loss: 3.7206\n",
      "Epoch [5/5], Step [9225/30540], Loss: 3.8100\n",
      "Epoch [5/5], Step [9300/30540], Loss: 3.7099\n",
      "Epoch [5/5], Step [9375/30540], Loss: 3.7486\n",
      "Epoch [5/5], Step [9450/30540], Loss: 3.8174\n",
      "Epoch [5/5], Step [9525/30540], Loss: 3.8414\n",
      "Epoch [5/5], Step [9600/30540], Loss: 3.9183\n",
      "Epoch [5/5], Step [9675/30540], Loss: 3.8445\n",
      "Epoch [5/5], Step [9750/30540], Loss: 3.8412\n",
      "Epoch [5/5], Step [9825/30540], Loss: 3.8032\n",
      "Epoch [5/5], Step [9900/30540], Loss: 3.8423\n",
      "Epoch [5/5], Step [9975/30540], Loss: 3.8537\n",
      "Epoch [5/5], Step [10050/30540], Loss: 3.7865\n",
      "Epoch [5/5], Step [10125/30540], Loss: 3.8379\n",
      "Epoch [5/5], Step [10200/30540], Loss: 3.7038\n",
      "Epoch [5/5], Step [10275/30540], Loss: 3.8209\n",
      "Epoch [5/5], Step [10350/30540], Loss: 3.7640\n",
      "Epoch [5/5], Step [10425/30540], Loss: 3.7225\n",
      "Epoch [5/5], Step [10500/30540], Loss: 3.8903\n",
      "Epoch [5/5], Step [10575/30540], Loss: 3.9511\n",
      "Epoch [5/5], Step [10650/30540], Loss: 3.8433\n",
      "Epoch [5/5], Step [10725/30540], Loss: 3.8774\n",
      "Epoch [5/5], Step [10800/30540], Loss: 3.7436\n",
      "Epoch [5/5], Step [10875/30540], Loss: 3.8505\n",
      "Epoch [5/5], Step [10950/30540], Loss: 3.8313\n",
      "Epoch [5/5], Step [11025/30540], Loss: 3.7006\n",
      "Epoch [5/5], Step [11100/30540], Loss: 3.7676\n",
      "Epoch [5/5], Step [11175/30540], Loss: 3.6693\n",
      "Epoch [5/5], Step [11250/30540], Loss: 3.7889\n",
      "Epoch [5/5], Step [11325/30540], Loss: 3.8571\n",
      "Epoch [5/5], Step [11400/30540], Loss: 3.8240\n",
      "Epoch [5/5], Step [11475/30540], Loss: 3.8479\n",
      "Epoch [5/5], Step [11550/30540], Loss: 3.7984\n",
      "Epoch [5/5], Step [11625/30540], Loss: 3.7330\n",
      "Epoch [5/5], Step [11700/30540], Loss: 3.7426\n",
      "Epoch [5/5], Step [11775/30540], Loss: 3.7349\n",
      "Epoch [5/5], Step [11850/30540], Loss: 3.8867\n",
      "Epoch [5/5], Step [11925/30540], Loss: 3.9029\n",
      "Epoch [5/5], Step [12000/30540], Loss: 3.9107\n",
      "Validation perplexity: 43.01147050571329\n",
      "Epoch [5/5], Step [12075/30540], Loss: 3.7735\n",
      "Epoch [5/5], Step [12150/30540], Loss: 3.8114\n",
      "Epoch [5/5], Step [12225/30540], Loss: 3.6732\n",
      "Epoch [5/5], Step [12300/30540], Loss: 3.8134\n",
      "Epoch [5/5], Step [12375/30540], Loss: 3.8106\n",
      "Epoch [5/5], Step [12450/30540], Loss: 3.8369\n",
      "Epoch [5/5], Step [12525/30540], Loss: 3.8067\n",
      "Epoch [5/5], Step [12600/30540], Loss: 3.9346\n",
      "Epoch [5/5], Step [12675/30540], Loss: 3.7731\n",
      "Epoch [5/5], Step [12750/30540], Loss: 3.7467\n",
      "Epoch [5/5], Step [12825/30540], Loss: 3.8847\n",
      "Epoch [5/5], Step [12900/30540], Loss: 3.8724\n",
      "Epoch [5/5], Step [12975/30540], Loss: 3.8136\n",
      "Epoch [5/5], Step [13050/30540], Loss: 3.7754\n",
      "Epoch [5/5], Step [13125/30540], Loss: 3.8125\n",
      "Epoch [5/5], Step [13200/30540], Loss: 3.8077\n",
      "Epoch [5/5], Step [13275/30540], Loss: 3.8115\n",
      "Epoch [5/5], Step [13350/30540], Loss: 3.8150\n",
      "Epoch [5/5], Step [13425/30540], Loss: 3.8685\n",
      "Epoch [5/5], Step [13500/30540], Loss: 3.8431\n",
      "Epoch [5/5], Step [13575/30540], Loss: 3.8372\n",
      "Epoch [5/5], Step [13650/30540], Loss: 3.8111\n",
      "Epoch [5/5], Step [13725/30540], Loss: 3.8353\n",
      "Epoch [5/5], Step [13800/30540], Loss: 3.8066\n",
      "Epoch [5/5], Step [13875/30540], Loss: 3.7151\n",
      "Epoch [5/5], Step [13950/30540], Loss: 3.7732\n",
      "Epoch [5/5], Step [14025/30540], Loss: 3.7662\n",
      "Epoch [5/5], Step [14100/30540], Loss: 3.6975\n",
      "Epoch [5/5], Step [14175/30540], Loss: 3.8038\n",
      "Epoch [5/5], Step [14250/30540], Loss: 3.6928\n",
      "Epoch [5/5], Step [14325/30540], Loss: 3.7963\n",
      "Epoch [5/5], Step [14400/30540], Loss: 3.8094\n",
      "Epoch [5/5], Step [14475/30540], Loss: 3.7729\n",
      "Epoch [5/5], Step [14550/30540], Loss: 3.8319\n",
      "Epoch [5/5], Step [14625/30540], Loss: 3.8071\n",
      "Epoch [5/5], Step [14700/30540], Loss: 3.8367\n",
      "Epoch [5/5], Step [14775/30540], Loss: 3.7909\n",
      "Epoch [5/5], Step [14850/30540], Loss: 3.8678\n",
      "Epoch [5/5], Step [14925/30540], Loss: 3.8669\n",
      "Epoch [5/5], Step [15000/30540], Loss: 3.8311\n",
      "Validation perplexity: 42.911684350769114\n",
      "Epoch [5/5], Step [15075/30540], Loss: 3.9326\n",
      "Epoch [5/5], Step [15150/30540], Loss: 3.7919\n",
      "Epoch [5/5], Step [15225/30540], Loss: 3.8431\n",
      "Epoch [5/5], Step [15300/30540], Loss: 3.8193\n",
      "Epoch [5/5], Step [15375/30540], Loss: 3.7008\n",
      "Epoch [5/5], Step [15450/30540], Loss: 3.7743\n",
      "Epoch [5/5], Step [15525/30540], Loss: 3.8376\n",
      "Epoch [5/5], Step [15600/30540], Loss: 3.7549\n",
      "Epoch [5/5], Step [15675/30540], Loss: 3.7571\n",
      "Epoch [5/5], Step [15750/30540], Loss: 3.7479\n",
      "Epoch [5/5], Step [15825/30540], Loss: 3.7108\n",
      "Epoch [5/5], Step [15900/30540], Loss: 3.8635\n",
      "Epoch [5/5], Step [15975/30540], Loss: 3.7435\n",
      "Epoch [5/5], Step [16050/30540], Loss: 3.8056\n",
      "Epoch [5/5], Step [16125/30540], Loss: 3.8422\n",
      "Epoch [5/5], Step [16200/30540], Loss: 3.7846\n",
      "Epoch [5/5], Step [16275/30540], Loss: 3.7669\n",
      "Epoch [5/5], Step [16350/30540], Loss: 3.7945\n",
      "Epoch [5/5], Step [16425/30540], Loss: 3.7684\n",
      "Epoch [5/5], Step [16500/30540], Loss: 3.6321\n",
      "Epoch [5/5], Step [16575/30540], Loss: 3.8040\n",
      "Epoch [5/5], Step [16650/30540], Loss: 3.7241\n",
      "Epoch [5/5], Step [16725/30540], Loss: 3.7415\n",
      "Epoch [5/5], Step [16800/30540], Loss: 3.8110\n",
      "Epoch [5/5], Step [16875/30540], Loss: 3.8155\n",
      "Epoch [5/5], Step [16950/30540], Loss: 3.7399\n",
      "Epoch [5/5], Step [17025/30540], Loss: 3.8559\n",
      "Epoch [5/5], Step [17100/30540], Loss: 3.7502\n",
      "Epoch [5/5], Step [17175/30540], Loss: 3.7330\n",
      "Epoch [5/5], Step [17250/30540], Loss: 3.6989\n",
      "Epoch [5/5], Step [17325/30540], Loss: 3.6756\n",
      "Epoch [5/5], Step [17400/30540], Loss: 3.8769\n",
      "Epoch [5/5], Step [17475/30540], Loss: 3.7388\n",
      "Epoch [5/5], Step [17550/30540], Loss: 3.8403\n",
      "Epoch [5/5], Step [17625/30540], Loss: 3.8910\n",
      "Epoch [5/5], Step [17700/30540], Loss: 3.6835\n",
      "Epoch [5/5], Step [17775/30540], Loss: 3.7805\n",
      "Epoch [5/5], Step [17850/30540], Loss: 3.7901\n",
      "Epoch [5/5], Step [17925/30540], Loss: 3.7544\n",
      "Epoch [5/5], Step [18000/30540], Loss: 3.8559\n",
      "Validation perplexity: 42.91320693344841\n",
      "Epoch [5/5], Step [18075/30540], Loss: 3.7743\n",
      "Epoch [5/5], Step [18150/30540], Loss: 3.7914\n",
      "Epoch [5/5], Step [18225/30540], Loss: 3.7817\n",
      "Epoch [5/5], Step [18300/30540], Loss: 3.7494\n",
      "Epoch [5/5], Step [18375/30540], Loss: 3.8777\n",
      "Epoch [5/5], Step [18450/30540], Loss: 3.7099\n",
      "Epoch [5/5], Step [18525/30540], Loss: 3.7608\n",
      "Epoch [5/5], Step [18600/30540], Loss: 3.8950\n",
      "Epoch [5/5], Step [18675/30540], Loss: 3.7285\n",
      "Epoch [5/5], Step [18750/30540], Loss: 3.7533\n",
      "Epoch [5/5], Step [18825/30540], Loss: 3.7699\n",
      "Epoch [5/5], Step [18900/30540], Loss: 3.7119\n",
      "Epoch [5/5], Step [18975/30540], Loss: 3.7084\n",
      "Epoch [5/5], Step [19050/30540], Loss: 3.8943\n",
      "Epoch [5/5], Step [19125/30540], Loss: 3.8211\n",
      "Epoch [5/5], Step [19200/30540], Loss: 3.8779\n",
      "Epoch [5/5], Step [19275/30540], Loss: 3.7686\n",
      "Epoch [5/5], Step [19350/30540], Loss: 3.8146\n",
      "Epoch [5/5], Step [19425/30540], Loss: 3.8649\n",
      "Epoch [5/5], Step [19500/30540], Loss: 3.6459\n",
      "Epoch [5/5], Step [19575/30540], Loss: 3.8428\n",
      "Epoch [5/5], Step [19650/30540], Loss: 3.8189\n",
      "Epoch [5/5], Step [19725/30540], Loss: 3.7144\n",
      "Epoch [5/5], Step [19800/30540], Loss: 3.8859\n",
      "Epoch [5/5], Step [19875/30540], Loss: 3.7958\n",
      "Epoch [5/5], Step [19950/30540], Loss: 3.7974\n",
      "Epoch [5/5], Step [20025/30540], Loss: 3.7768\n",
      "Epoch [5/5], Step [20100/30540], Loss: 3.8392\n",
      "Epoch [5/5], Step [20175/30540], Loss: 3.8659\n",
      "Epoch [5/5], Step [20250/30540], Loss: 3.6794\n",
      "Epoch [5/5], Step [20325/30540], Loss: 3.7645\n",
      "Epoch [5/5], Step [20400/30540], Loss: 3.8591\n",
      "Epoch [5/5], Step [20475/30540], Loss: 3.8008\n",
      "Epoch [5/5], Step [20550/30540], Loss: 3.8101\n",
      "Epoch [5/5], Step [20625/30540], Loss: 3.8729\n",
      "Epoch [5/5], Step [20700/30540], Loss: 3.7849\n",
      "Epoch [5/5], Step [20775/30540], Loss: 3.7368\n",
      "Epoch [5/5], Step [20850/30540], Loss: 3.7602\n",
      "Epoch [5/5], Step [20925/30540], Loss: 3.7282\n",
      "Epoch [5/5], Step [21000/30540], Loss: 3.7349\n",
      "Validation perplexity: 42.86441202224701\n",
      "Epoch [5/5], Step [21075/30540], Loss: 3.7615\n",
      "Epoch [5/5], Step [21150/30540], Loss: 3.6381\n",
      "Epoch [5/5], Step [21225/30540], Loss: 3.7179\n",
      "Epoch [5/5], Step [21300/30540], Loss: 3.9485\n",
      "Epoch [5/5], Step [21375/30540], Loss: 3.8278\n",
      "Epoch [5/5], Step [21450/30540], Loss: 3.7103\n",
      "Epoch [5/5], Step [21525/30540], Loss: 3.7259\n",
      "Epoch [5/5], Step [21600/30540], Loss: 3.7587\n",
      "Epoch [5/5], Step [21675/30540], Loss: 3.9557\n",
      "Epoch [5/5], Step [21750/30540], Loss: 3.7571\n",
      "Epoch [5/5], Step [21825/30540], Loss: 3.7982\n",
      "Epoch [5/5], Step [21900/30540], Loss: 3.7443\n",
      "Epoch [5/5], Step [21975/30540], Loss: 3.8154\n",
      "Epoch [5/5], Step [22050/30540], Loss: 3.6930\n",
      "Epoch [5/5], Step [22125/30540], Loss: 3.7008\n",
      "Epoch [5/5], Step [22200/30540], Loss: 3.8792\n",
      "Epoch [5/5], Step [22275/30540], Loss: 3.8852\n",
      "Epoch [5/5], Step [22350/30540], Loss: 3.7362\n",
      "Epoch [5/5], Step [22425/30540], Loss: 3.8521\n",
      "Epoch [5/5], Step [22500/30540], Loss: 3.8854\n",
      "Epoch [5/5], Step [22575/30540], Loss: 3.8144\n",
      "Epoch [5/5], Step [22650/30540], Loss: 3.7746\n",
      "Epoch [5/5], Step [22725/30540], Loss: 3.7214\n",
      "Epoch [5/5], Step [22800/30540], Loss: 3.7628\n",
      "Epoch [5/5], Step [22875/30540], Loss: 3.7636\n",
      "Epoch [5/5], Step [22950/30540], Loss: 3.8146\n",
      "Epoch [5/5], Step [23025/30540], Loss: 3.8737\n",
      "Epoch [5/5], Step [23100/30540], Loss: 3.6577\n",
      "Epoch [5/5], Step [23175/30540], Loss: 3.8487\n",
      "Epoch [5/5], Step [23250/30540], Loss: 3.7594\n",
      "Epoch [5/5], Step [23325/30540], Loss: 3.8241\n",
      "Epoch [5/5], Step [23400/30540], Loss: 3.7225\n",
      "Epoch [5/5], Step [23475/30540], Loss: 3.7577\n",
      "Epoch [5/5], Step [23550/30540], Loss: 3.6437\n",
      "Epoch [5/5], Step [23625/30540], Loss: 3.7207\n",
      "Epoch [5/5], Step [23700/30540], Loss: 3.8434\n",
      "Epoch [5/5], Step [23775/30540], Loss: 3.7840\n",
      "Epoch [5/5], Step [23850/30540], Loss: 3.7527\n",
      "Epoch [5/5], Step [23925/30540], Loss: 3.7566\n",
      "Epoch [5/5], Step [24000/30540], Loss: 3.8763\n",
      "Validation perplexity: 42.846281754752184\n",
      "Epoch [5/5], Step [24075/30540], Loss: 3.8225\n",
      "Epoch [5/5], Step [24150/30540], Loss: 3.7954\n",
      "Epoch [5/5], Step [24225/30540], Loss: 3.6961\n",
      "Epoch [5/5], Step [24300/30540], Loss: 3.8219\n",
      "Epoch [5/5], Step [24375/30540], Loss: 3.9042\n",
      "Epoch [5/5], Step [24450/30540], Loss: 3.7633\n",
      "Epoch [5/5], Step [24525/30540], Loss: 3.8017\n",
      "Epoch [5/5], Step [24600/30540], Loss: 3.8097\n",
      "Epoch [5/5], Step [24675/30540], Loss: 3.7489\n",
      "Epoch [5/5], Step [24750/30540], Loss: 3.7732\n",
      "Epoch [5/5], Step [24825/30540], Loss: 3.9334\n",
      "Epoch [5/5], Step [24900/30540], Loss: 3.8004\n",
      "Epoch [5/5], Step [24975/30540], Loss: 3.7406\n",
      "Epoch [5/5], Step [25050/30540], Loss: 3.8109\n",
      "Epoch [5/5], Step [25125/30540], Loss: 3.8389\n",
      "Epoch [5/5], Step [25200/30540], Loss: 3.7636\n",
      "Epoch [5/5], Step [25275/30540], Loss: 3.8233\n",
      "Epoch [5/5], Step [25350/30540], Loss: 3.7965\n",
      "Epoch [5/5], Step [25425/30540], Loss: 3.7270\n",
      "Epoch [5/5], Step [25500/30540], Loss: 3.6828\n",
      "Epoch [5/5], Step [25575/30540], Loss: 3.7334\n",
      "Epoch [5/5], Step [25650/30540], Loss: 3.8048\n",
      "Epoch [5/5], Step [25725/30540], Loss: 3.8687\n",
      "Epoch [5/5], Step [25800/30540], Loss: 3.8884\n",
      "Epoch [5/5], Step [25875/30540], Loss: 3.9078\n",
      "Epoch [5/5], Step [25950/30540], Loss: 3.8629\n",
      "Epoch [5/5], Step [26025/30540], Loss: 3.6641\n",
      "Epoch [5/5], Step [26100/30540], Loss: 3.9293\n",
      "Epoch [5/5], Step [26175/30540], Loss: 3.8299\n",
      "Epoch [5/5], Step [26250/30540], Loss: 3.7483\n",
      "Epoch [5/5], Step [26325/30540], Loss: 3.7600\n",
      "Epoch [5/5], Step [26400/30540], Loss: 3.7335\n",
      "Epoch [5/5], Step [26475/30540], Loss: 3.7820\n",
      "Epoch [5/5], Step [26550/30540], Loss: 3.8309\n",
      "Epoch [5/5], Step [26625/30540], Loss: 3.8706\n",
      "Epoch [5/5], Step [26700/30540], Loss: 3.8315\n",
      "Epoch [5/5], Step [26775/30540], Loss: 3.8247\n",
      "Epoch [5/5], Step [26850/30540], Loss: 3.7992\n",
      "Epoch [5/5], Step [26925/30540], Loss: 3.7873\n",
      "Epoch [5/5], Step [27000/30540], Loss: 3.8425\n",
      "Validation perplexity: 42.73596311959791\n",
      "Epoch [5/5], Step [27075/30540], Loss: 3.7712\n",
      "Epoch [5/5], Step [27150/30540], Loss: 3.7854\n",
      "Epoch [5/5], Step [27225/30540], Loss: 3.7065\n",
      "Epoch [5/5], Step [27300/30540], Loss: 3.8103\n",
      "Epoch [5/5], Step [27375/30540], Loss: 3.8030\n",
      "Epoch [5/5], Step [27450/30540], Loss: 3.8785\n",
      "Epoch [5/5], Step [27525/30540], Loss: 3.8373\n",
      "Epoch [5/5], Step [27600/30540], Loss: 3.7375\n",
      "Epoch [5/5], Step [27675/30540], Loss: 3.7612\n",
      "Epoch [5/5], Step [27750/30540], Loss: 3.8117\n",
      "Epoch [5/5], Step [27825/30540], Loss: 3.7320\n",
      "Epoch [5/5], Step [27900/30540], Loss: 3.8422\n",
      "Epoch [5/5], Step [27975/30540], Loss: 3.6986\n",
      "Epoch [5/5], Step [28050/30540], Loss: 3.8543\n",
      "Epoch [5/5], Step [28125/30540], Loss: 3.9817\n",
      "Epoch [5/5], Step [28200/30540], Loss: 3.7884\n",
      "Epoch [5/5], Step [28275/30540], Loss: 3.7149\n",
      "Epoch [5/5], Step [28350/30540], Loss: 3.6921\n",
      "Epoch [5/5], Step [28425/30540], Loss: 3.7320\n",
      "Epoch [5/5], Step [28500/30540], Loss: 3.8728\n",
      "Epoch [5/5], Step [28575/30540], Loss: 3.8285\n",
      "Epoch [5/5], Step [28650/30540], Loss: 3.7911\n",
      "Epoch [5/5], Step [28725/30540], Loss: 3.8010\n",
      "Epoch [5/5], Step [28800/30540], Loss: 3.8372\n",
      "Epoch [5/5], Step [28875/30540], Loss: 3.7172\n",
      "Epoch [5/5], Step [28950/30540], Loss: 3.8397\n",
      "Epoch [5/5], Step [29025/30540], Loss: 3.6966\n",
      "Epoch [5/5], Step [29100/30540], Loss: 3.6785\n",
      "Epoch [5/5], Step [29175/30540], Loss: 3.8318\n",
      "Epoch [5/5], Step [29250/30540], Loss: 3.7510\n",
      "Epoch [5/5], Step [29325/30540], Loss: 3.6751\n",
      "Epoch [5/5], Step [29400/30540], Loss: 3.7852\n",
      "Epoch [5/5], Step [29475/30540], Loss: 3.6542\n",
      "Epoch [5/5], Step [29550/30540], Loss: 3.7678\n",
      "Epoch [5/5], Step [29625/30540], Loss: 3.7250\n",
      "Epoch [5/5], Step [29700/30540], Loss: 3.7233\n",
      "Epoch [5/5], Step [29775/30540], Loss: 3.7497\n",
      "Epoch [5/5], Step [29850/30540], Loss: 3.7490\n",
      "Epoch [5/5], Step [29925/30540], Loss: 3.8254\n",
      "Epoch [5/5], Step [30000/30540], Loss: 3.7384\n",
      "Validation perplexity: 42.80117739204574\n",
      "Epoch [5/5], Step [30075/30540], Loss: 3.8164\n",
      "Epoch [5/5], Step [30150/30540], Loss: 3.8616\n",
      "Epoch [5/5], Step [30225/30540], Loss: 3.7911\n",
      "Epoch [5/5], Step [30300/30540], Loss: 3.7494\n",
      "Epoch [5/5], Step [30375/30540], Loss: 3.6774\n",
      "Epoch [5/5], Step [30450/30540], Loss: 3.8726\n",
      "Epoch [5/5], Step [30525/30540], Loss: 3.8321\n",
      "Epoch [5/5] Average Loss: 3.7945, Perplexity: 44.46\n"
     ]
    }
   ],
   "source": [
    "from src.model import RegularizedLanguageModel\n",
    "from src.model import LanguageModel,SimpleLanguageModel,LanguageModelExtraRelu\n",
    "model = model = LanguageModelExtraRelu(vocab_size, embedding_dim, context_length).to(device)\n",
    "\n",
    "trainclass.train(model,\n",
    "              vocab_size,device,raw_text,\"spanish_LinearRelu_dopout_2Relu_ep5_batchsize32_evaluate_every3000\",\n",
    "                print_every=75,evaluate_every=3000,optimizer=None,criterion=None,\n",
    "              batch_size = 32,\n",
    "              embedding_dim = 128,\n",
    "              context_length = 32,\n",
    "              num_epochs =  5\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd895e-4b6b-49d6-89b8-a93cc1194b93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a7c79-9b66-4fe7-ae13-27c74c98db81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d949f-0e59-4294-92f9-edd9e42dd902",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e76c3-9f1e-4a50-8b6e-eb44600f5080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd9d2b-dab2-4221-8675-bf711bfcf5a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c6716-eb37-4da4-8c9b-38713f708355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe539ba-db28-47ae-877a-999b7f36ad3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493668b0-34c5-47cf-9d4c-1c357bc6695d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
