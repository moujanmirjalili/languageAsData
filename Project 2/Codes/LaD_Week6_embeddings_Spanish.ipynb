{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e904cf-5d82-4fe8-b4e0-a4d953ec7da2",
   "metadata": {},
   "source": [
    "Course: Language as Data, University of Göttingen\n",
    "\n",
    "# Week 6: Embeddings\n",
    "In this lab, we examine how token ids can be mapped into embeddings. The notebook adapts parts from [Sebastian Raschka's notebook](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb) accompanying chapter 2 of his book [\"Build a Large Language Model (from Scratch)\"](https://www.manning.com/books/build-a-large-language-model-from-scratch).\n",
    "\n",
    "As an example, we use the same book as in the previous notebook: [Emma](https://www.gutenberg.org/cache/epub/158/pg158.txt) by Jane Austen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a6d9bc-64c7-4edb-bf12-9686b02edfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a1e6d44-93c2-4fed-8e40-058866b98350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'torch>=2.0.1' 'jupyterlab>=4.0' 'tiktoken>=0.5.1' 'numpy>=1.25,<2.0' --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d361f098-d151-493f-a682-b1c25e675ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "text_path = \"content/spa_wikipedia_2021_30K-sentences.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc84dfef-27a3-452d-8865-5f466f3333a2",
   "metadata": {},
   "source": [
    "# 1. Token Ids\n",
    "\n",
    "You can use any tokenizer. Here, we use the pre-trained tokenizer employed in the gpt2 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a32d48d-f5ba-4e03-88ba-0d16b85a45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e650282-9e6b-43b4-99fb-6a9027f7a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helper  import get_cleaned_spanish_text_as_string\n",
    "raw_text = get_cleaned_spanish_text_as_string(text_path)\n",
    "enc_text = tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f72e8-7452-4a31-8694-56cc9953f30b",
   "metadata": {},
   "source": [
    "The tokenizer maps each token into a token id: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99a45d84-5726-426c-a500-5206ae5361aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12 de abril de 1996 es una actriz de la industria del entretenimiento para adultos y una personalid\n",
      "\n",
      " 12 de abril de 1996 es una actriz de la industria del entretenimiento para adultos y una personalidad de internet. 15 Y el Señor dijo a Noé La hijas de tus\n",
      "\n",
      "[1105, 390, 450, 22379, 390, 8235, 1658, 555, 64, 719, 47847, 390, 8591, 2226, 7496, 1619, 920, 1186, 268, 320, 1153, 78, 31215, 4044, 418, 331, 555, 64, 2614, 32482, 390, 5230, 13, 1315, 575, 1288, 1001, 12654, 273, 2566, 7639, 257, 1400, 2634, 4689, 16836, 292, 390, 256, 385]\n",
      "\n",
      "[' 12', ' de', ' ab', 'ril', ' de', ' 1996', ' es', ' un', 'a', ' act', 'riz', ' de', ' la', ' indust', 'ria', ' del', ' ent', 'ret', 'en', 'im', 'ient', 'o', ' para', ' adult', 'os', ' y', ' un', 'a', ' personal', 'idad', ' de', ' internet', '.', ' 15', ' Y', ' el', ' Se', 'ñ', 'or', ' di', 'jo', ' a', ' No', 'é', ' La', ' hij', 'as', ' de', ' t', 'us']\n"
     ]
    }
   ],
   "source": [
    "print(raw_text[0:100])\n",
    "print()\n",
    "print(tokenizer.decode(enc_text[0:50]))\n",
    "print()\n",
    "tokens = [tokenizer.decode([tok]) for tok in enc_text[0:50]]\n",
    "print(enc_text[0:50])\n",
    "print()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4271e8f-2eb1-4164-bdac-108ee3664e27",
   "metadata": {},
   "source": [
    "Why is the raw text shorter than the decoded text in the print statements above? Make sure you understand what the indices refer to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a2d1685-ff22-4b22-a85c-ecbb5192cc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40313]\n",
      "[1845, 271]\n"
     ]
    }
   ],
   "source": [
    "# Note that the gpt2 tokenizer was trained with cased training data. \n",
    "# Example: \"Paris\" is kept as a single token, \"paris\" is split into two tokens\n",
    "print(tokenizer.encode(\"Paris\"))\n",
    "print(tokenizer.encode(\"paris\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c0944-ebec-45be-b274-5006361d6be6",
   "metadata": {},
   "source": [
    "# 2. Sliding Window\n",
    "Language models generate text one word at a time. During training, we iteratively predict every word of the training data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e9e8ab-4675-4972-bc5f-66500f3a7242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[555] --> 64\n",
      " un --> a\n",
      "[555, 64] --> 719\n",
      " una -->  act\n",
      "[555, 64, 719] --> 47847\n",
      " una act --> riz\n",
      "[555, 64, 719, 47847] --> 390\n",
      " una actriz -->  de\n"
     ]
    }
   ],
   "source": [
    "enc_sample = enc_text[7:99]\n",
    "\n",
    "context_length = 4\n",
    "for i in range(1, context_length+1):\n",
    "    context = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "\n",
    "    print(context, \"-->\", target)\n",
    "    print(tokenizer.decode(context), \"-->\",tokenizer.decode([target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbe5c9-14eb-4c99-a972-396da24eae71",
   "metadata": {},
   "source": [
    "The **context length** (also called context size) indicates the maximum sequence length that the model accepts. It is a hyperparameter that is set when configuring the model architecture. For gpt2, the context size was set to 1024 tokens, for llama3, the context size was set to 8,192 tokens. Note that punctuation symbols (as the comma above) are also tokens (id 11). \n",
    "\n",
    "\n",
    "We implement the **Dataset** class in torch to split the training data into overlapping input sequences with the specified context size. \n",
    "\n",
    "For each input sequence, the **target sequence** is set by shifting the input sequence by one token to the right. Input and output sequences are represented as torch tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7d05392-fdeb-4ba8-b5c6-62ce4b9eb31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, context_length):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - context_length):\n",
    "            input_sequence = token_ids[i:i + context_length]\n",
    "            \n",
    "            #shift to the right\n",
    "            target_sequence = token_ids[i + 1: i + context_length + 1]\n",
    "\n",
    "            # input and output are represented as tensors\n",
    "            self.input_ids.append(torch.tensor(input_sequence))\n",
    "            self.target_ids.append(torch.tensor(target_sequence))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6570ec0f-12b0-479e-aa18-666ea1ce8057",
   "metadata": {},
   "source": [
    "# 3. Batching\n",
    "The data is fed to the model in batches. The **batch size** is also a hyperparameter. It refers to the number of training examples the model sees in one iteration of the training process before updating its weights. Both gpt2 and llama-3 use a batch size of 512. Smaller models use a batch size of 128. We implement the **DataLoader** class in torch to split the training data into sequences with the specified context size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6067efe4-37db-4d1d-9d5c-bea57f21982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataloader(txt, batch_size=8, context_length=4, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDataset(txt, tokenizer, context_length)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98fb4b00-c528-4991-ae88-af0039a7cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a manual seed for reproducibility of shuffling and weight initialization\n",
    "torch.manual_seed(0) \n",
    "dataloader = create_dataloader(raw_text, batch_size=8, context_length=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe5d416-0338-4f17-9b1d-be855d544fbc",
   "metadata": {},
   "source": [
    "The dataloader can be used as an iterator. Each batch consists of input sequences and target sequences.  When working with tensors, it is important to understand the dimensions. Vary the batch size and max_length parameters and inspect how it affects the shape of the tensors. For your project, ensure to split the data into training, development, and test portions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9796ad36-7e37-44d0-a6fc-e5f1fed2693c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152705\n",
      "1: [tensor([[ 1105,   390,   450, 22379],\n",
      "        [  390,   450, 22379,   390],\n",
      "        [  450, 22379,   390,  8235],\n",
      "        [22379,   390,  8235,  1658],\n",
      "        [  390,  8235,  1658,   555],\n",
      "        [ 8235,  1658,   555,    64],\n",
      "        [ 1658,   555,    64,   719],\n",
      "        [  555,    64,   719, 47847]]), tensor([[  390,   450, 22379,   390],\n",
      "        [  450, 22379,   390,  8235],\n",
      "        [22379,   390,  8235,  1658],\n",
      "        [  390,  8235,  1658,   555],\n",
      "        [ 8235,  1658,   555,    64],\n",
      "        [ 1658,   555,    64,   719],\n",
      "        [  555,    64,   719, 47847],\n",
      "        [   64,   719, 47847,   390]])]\n"
     ]
    }
   ],
   "source": [
    "print(len(dataloader))\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "print(\"1:\", first_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6b675f0-a2c8-4fde-8c98-4f96967b2f59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[ 1105,   390,   450, 22379],\n",
      "        [  390,   450, 22379,   390],\n",
      "        [  450, 22379,   390,  8235],\n",
      "        [22379,   390,  8235,  1658],\n",
      "        [  390,  8235,  1658,   555],\n",
      "        [ 8235,  1658,   555,    64],\n",
      "        [ 1658,   555,    64,   719],\n",
      "        [  555,    64,   719, 47847]])\n",
      "Shape: torch.Size([8, 4])\n",
      "\n",
      "Targets:\n",
      " tensor([[  390,   450, 22379,   390],\n",
      "        [  450, 22379,   390,  8235],\n",
      "        [22379,   390,  8235,  1658],\n",
      "        [  390,  8235,  1658,   555],\n",
      "        [ 8235,  1658,   555,    64],\n",
      "        [ 1658,   555,    64,   719],\n",
      "        [  555,    64,   719, 47847],\n",
      "        [   64,   719, 47847,   390]])\n",
      "Shape: torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = first_batch\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"Shape:\", inputs.shape)\n",
    "print(\"\\nTargets:\\n\", targets)\n",
    "print(\"Shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3fcabf-0242-4484-a4f8-a7ed301d7ba9",
   "metadata": {},
   "source": [
    "## 4. Embeddings\n",
    "\n",
    "We can now decide how we want to represent our input data in the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045e7b0-5202-4f4e-8644-f91059e34968",
   "metadata": {},
   "source": [
    "### 4.1 Token Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5461dcc",
   "metadata": {},
   "source": [
    "The token embeddings project the token IDs of the tokenizer into vector space.\n",
    "The dimensions of the embedding matrix are determined by the vocabulary size of the tokenizer and the embedding size. The embedding size is a hyperparameter. \n",
    "\n",
    "GPT-2 uses a vocabulary size of 50,257 and an embedding size of 768. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48a4e701-9402-41a3-80f8-f4c4a60b5c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50256\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.max_token_value)\n",
    "\n",
    "vocab_size = tokenizer.max_token_value+1\n",
    "embedding_dim = 256 \n",
    "\n",
    "# Create the token embedding layer\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2bd83e",
   "metadata": {},
   "source": [
    "Now, we can pass our token IDs through the embedding layer to get the token embeddings. \n",
    "The weights of the embedding layer are randomly initalized and get optimized during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdb5339c-48bc-42c9-941c-a6ebae39d70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of token embeddings: torch.Size([8, 4, 256])\n",
      "First sequence tensor([[ 0.2794,  0.9326,  0.0547,  ..., -1.0102, -0.7279, -0.1012],\n",
      "        [-0.0574, -2.3481, -0.3402,  ...,  0.3958,  0.4084, -1.2151],\n",
      "        [ 1.2543, -0.8234,  1.1952,  ..., -0.1504, -0.2927,  0.8922],\n",
      "        [ 0.0996,  0.3128, -1.6327,  ...,  0.2467, -0.4801, -1.2572]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "\n",
    "print(\"Shape of token embeddings:\", token_embeddings.shape)\n",
    "print(\"First sequence\", token_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2138606b",
   "metadata": {},
   "source": [
    "This outputs a tensor of shape (batch_size, context_length, embedding_dim)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78196259",
   "metadata": {},
   "source": [
    "### 4.2 Positional Embeddings\n",
    "\n",
    "The positional embeddings indicate the order of the input tokens within each sequence. GPT-2 uses absolute position embeddings. \n",
    "We'll create a positional embedding layer according to our context_length with the same embedding size as the token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5d64359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position IDs: tensor([0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "position_embedding_layer = torch.nn.Embedding(context_length, embedding_dim)\n",
    "position_ids = torch.arange(context_length)\n",
    "print(\"Position IDs:\", position_ids)\n",
    "position_embeddings = position_embedding_layer(position_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac0b480-c081-4e92-90f2-1ec8c459a5d0",
   "metadata": {},
   "source": [
    "The position embedding layer multiplies the position ids with randomly initialized weights that are optimized during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04020897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of position embeddings: torch.Size([4, 256])\n",
      "Position embeddings: tensor([[ 0.5785,  0.1814,  0.2622,  ..., -0.5162,  1.1787,  0.4018],\n",
      "        [ 1.6504,  2.3930,  0.0143,  ..., -0.0124,  0.4445, -0.8851],\n",
      "        [ 0.6348,  0.1572, -1.0412,  ...,  2.1842,  1.1838, -0.7935],\n",
      "        [ 0.9566,  0.3479,  0.5343,  ...,  0.3031, -0.8450, -0.0861]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of position embeddings:\", position_embeddings.shape)\n",
    "print(\"Position embeddings:\", position_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c23fed",
   "metadata": {},
   "source": [
    "### 4.3 Combining Token and Positional Embeddings\n",
    "\n",
    "The token embeddings and positional embeddings are combined to get the final input embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "037c95ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings shape: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "print(\"Input embeddings shape:\", input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5248c759-8251-4b61-8a23-74c6657c3f59",
   "metadata": {},
   "source": [
    "## 5 Note\n",
    "Embedding layers are a computationally efficient implementation. We could also use a one-hot encoding and a linear layer that multiplies the one-hot matrix with the embedding weights. If you are interested in more details on this, Sebastian Raschka's [bonus material](see https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd8ed7-8ae7-44ed-a135-f36b0e96b97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ec6f31-dadd-4066-9e9c-43b16cb93660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e214a-f8b1-46a7-b515-e05045b3b043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
