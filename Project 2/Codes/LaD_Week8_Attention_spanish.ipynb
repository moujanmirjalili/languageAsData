{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cc0039",
   "metadata": {},
   "source": [
    "# Week 8 - Training a Language Model with Attention \n",
    "\n",
    "This week, we add an attention mechanism to our model. This notebook is a shortened version by Sebastian Rascha's notebook: https://github.com/rasbt/LLMs-from-scratch/blob/main/ch03/01_main-chapter-code/ch03.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5422cf",
   "metadata": {},
   "source": [
    "## 1. Setting up data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d7338f3-1e32-4070-a246-06aad16e1dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo\n"
     ]
    }
   ],
   "source": [
    "print(\"Hallo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e93791b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.3.1.post300\n",
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from importlib.metadata import version\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85cfb2",
   "metadata": {},
   "source": [
    "### 1.1 Data preparation\n",
    "The steps are the same as last week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a62dd454-bc92-4f13-84a5-987f5738ab14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines replaced 30000\n",
      "Total number of tokens: 1221649\n",
      "First 10 tokens: [1105, 390, 450, 22379, 390, 8235, 1658, 555, 64, 719]\n",
      "Text size:  3701778\n"
     ]
    }
   ],
   "source": [
    "#from src.helper  import\n",
    "from src.helper import get_cleaned_spanish_text_as_string, clean_text_spanish_remove,get_lines_without_number,clean_spanish_text,get_cleaned_text\n",
    "\n",
    "text_path = \"content/spa_wikipedia_2021_30K-sentences.txt\"\n",
    "path_to_save_folder= \"model/attent_data\"\n",
    "\n",
    "#raw_text = get_cleaned_spanish_text_as_string(text_path)\n",
    "#enc_text = tokenizer.encode(raw_text)\n",
    "\n",
    "# Load the text data\n",
    "raw_text = get_cleaned_spanish_text_as_string(text_path) #Standart \n",
    "#raw_text = get_cleaned_text(text_path,clean_text_spanish_remove)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Tokenize the text\n",
    "token_ids = tokenizer.encode(raw_text)\n",
    "\n",
    "print(\"Total number of tokens:\", len(token_ids))\n",
    "print(\"First 10 tokens:\", token_ids[:10])\n",
    "print(\"Text size: \",len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9dbf256-d7b8-401f-b488-aed41acb7a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.dataset import GPTDataset\n",
    "from src.dataset import create_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa541f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4117fb24",
   "metadata": {},
   "source": [
    "### 1.2 Initializing a model\n",
    "\n",
    "We set up our model. We add a second layer and a non-linear activation function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd9376cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import LanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c75557",
   "metadata": {},
   "source": [
    "## 2. Implementing Self-Attention\n",
    "\n",
    "We'll implement self-attention step by step, starting with the basic computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095d505",
   "metadata": {},
   "source": [
    "### 3.1 Computing Attention Scores\n",
    "\n",
    "For each token in the sequence, we compute Query, Key, and Value vectors using learnable weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "820a2d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.attentionModel import SelfAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca4f19",
   "metadata": {},
   "source": [
    "### 3.2 Masking for Causal Attention\n",
    "\n",
    "In language models, we often use causal (or masked) attention to prevent the model from attending to future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c50ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.attentionModel import CausalSelfAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c805eff",
   "metadata": {},
   "source": [
    "## 3. Extending to multi-head attention\n",
    "\n",
    "Multi-head attention allows the model to attend to information from different representation subspaces at different positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "502b8372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.attentionModel import MultiHeadCausalAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17dc895",
   "metadata": {},
   "source": [
    "## 4. Adding attention to the model\n",
    "\n",
    "Now, we'll update our model to include the multi-head causal attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de5d871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.attentionModel import LanguageModelWithAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c27b881",
   "metadata": {},
   "source": [
    "## 5. Training and evaluation\n",
    "\n",
    "We'll train the modified model and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3317c7e",
   "metadata": {},
   "source": [
    "### 5.1 Setting up training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ae6c27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 16 #Standard 32 \n",
    "context_length = 32  # Increased context size\n",
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 128\n",
    "attention_dim = 64\n",
    "hidden_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_dataloader(\n",
    "    raw_text, batch_size=batch_size, \n",
    "    context_length=context_length,     shuffle=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9fecbe66-4c8f-444f-be9a-8afa11c06330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = LanguageModelWithAttention(\n",
    "    vocab_size, embedding_dim, attention_dim, context_length, hidden_dim, num_heads, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters'\n",
    "num_epochs = 4\n",
    "data_loader = train_dataloader\n",
    "\n",
    "print_every = 75\n",
    "evaluate_every = 3000\n",
    "train_run_label = \"basic_att_batchsize16_ep4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bcb99c",
   "metadata": {},
   "source": [
    "### 5.2 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9300e04-6509-4db7-af0b-f369304c53ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n",
      "Epoch [1/4], Step [0/61080], Loss: 10.8473\n",
      "Validation perplexity: 50540.22227627894\n",
      "Epoch [1/4], Step [75/61080], Loss: 6.8057\n",
      "Epoch [1/4], Step [150/61080], Loss: 6.9529\n",
      "Epoch [1/4], Step [225/61080], Loss: 6.7151\n",
      "Epoch [1/4], Step [300/61080], Loss: 6.5713\n",
      "Epoch [1/4], Step [375/61080], Loss: 6.7121\n",
      "Epoch [1/4], Step [450/61080], Loss: 6.5298\n",
      "Epoch [1/4], Step [525/61080], Loss: 6.4536\n",
      "Epoch [1/4], Step [600/61080], Loss: 6.5191\n",
      "Epoch [1/4], Step [675/61080], Loss: 6.8836\n",
      "Epoch [1/4], Step [750/61080], Loss: 6.4027\n",
      "Epoch [1/4], Step [825/61080], Loss: 5.9638\n",
      "Epoch [1/4], Step [900/61080], Loss: 6.1015\n",
      "Epoch [1/4], Step [975/61080], Loss: 6.1614\n",
      "Epoch [1/4], Step [1050/61080], Loss: 6.0849\n",
      "Epoch [1/4], Step [1125/61080], Loss: 5.6714\n",
      "Epoch [1/4], Step [1200/61080], Loss: 5.7513\n",
      "Epoch [1/4], Step [1275/61080], Loss: 5.7572\n",
      "Epoch [1/4], Step [1350/61080], Loss: 5.5538\n",
      "Epoch [1/4], Step [1425/61080], Loss: 5.8570\n",
      "Epoch [1/4], Step [1500/61080], Loss: 5.6207\n",
      "Epoch [1/4], Step [1575/61080], Loss: 5.8033\n",
      "Epoch [1/4], Step [1650/61080], Loss: 5.7255\n",
      "Epoch [1/4], Step [1725/61080], Loss: 5.7545\n",
      "Epoch [1/4], Step [1800/61080], Loss: 5.5748\n",
      "Epoch [1/4], Step [1875/61080], Loss: 5.4921\n",
      "Epoch [1/4], Step [1950/61080], Loss: 5.9596\n",
      "Epoch [1/4], Step [2025/61080], Loss: 5.3417\n",
      "Epoch [1/4], Step [2100/61080], Loss: 5.4633\n",
      "Epoch [1/4], Step [2175/61080], Loss: 5.2736\n",
      "Epoch [1/4], Step [2250/61080], Loss: 5.5584\n",
      "Epoch [1/4], Step [2325/61080], Loss: 5.3531\n",
      "Epoch [1/4], Step [2400/61080], Loss: 5.5778\n",
      "Epoch [1/4], Step [2475/61080], Loss: 5.4336\n",
      "Epoch [1/4], Step [2550/61080], Loss: 5.4558\n",
      "Epoch [1/4], Step [2625/61080], Loss: 5.3611\n",
      "Epoch [1/4], Step [2700/61080], Loss: 5.2809\n",
      "Epoch [1/4], Step [2775/61080], Loss: 4.9723\n",
      "Epoch [1/4], Step [2850/61080], Loss: 5.2356\n",
      "Epoch [1/4], Step [2925/61080], Loss: 5.5222\n",
      "Epoch [1/4], Step [3000/61080], Loss: 5.3134\n",
      "Validation perplexity: 151.92502814782557\n",
      "Epoch [1/4], Step [3075/61080], Loss: 5.5188\n",
      "Epoch [1/4], Step [3150/61080], Loss: 5.0747\n",
      "Epoch [1/4], Step [3225/61080], Loss: 5.4858\n",
      "Epoch [1/4], Step [3300/61080], Loss: 5.2632\n",
      "Epoch [1/4], Step [3375/61080], Loss: 5.3004\n",
      "Epoch [1/4], Step [3450/61080], Loss: 5.2213\n",
      "Epoch [1/4], Step [3525/61080], Loss: 5.1716\n",
      "Epoch [1/4], Step [3600/61080], Loss: 5.2083\n",
      "Epoch [1/4], Step [3675/61080], Loss: 5.0982\n",
      "Epoch [1/4], Step [3750/61080], Loss: 5.2084\n",
      "Epoch [1/4], Step [3825/61080], Loss: 5.2908\n",
      "Epoch [1/4], Step [3900/61080], Loss: 5.2229\n",
      "Epoch [1/4], Step [3975/61080], Loss: 4.9989\n",
      "Epoch [1/4], Step [4050/61080], Loss: 4.9203\n",
      "Epoch [1/4], Step [4125/61080], Loss: 5.0504\n",
      "Epoch [1/4], Step [4200/61080], Loss: 4.8606\n",
      "Epoch [1/4], Step [4275/61080], Loss: 4.9597\n",
      "Epoch [1/4], Step [4350/61080], Loss: 4.8242\n",
      "Epoch [1/4], Step [4425/61080], Loss: 4.9752\n",
      "Epoch [1/4], Step [4500/61080], Loss: 4.9571\n",
      "Epoch [1/4], Step [4575/61080], Loss: 4.9032\n",
      "Epoch [1/4], Step [4650/61080], Loss: 5.0985\n",
      "Epoch [1/4], Step [4725/61080], Loss: 5.1119\n",
      "Epoch [1/4], Step [4800/61080], Loss: 4.8510\n",
      "Epoch [1/4], Step [4875/61080], Loss: 5.3105\n",
      "Epoch [1/4], Step [4950/61080], Loss: 4.4878\n",
      "Epoch [1/4], Step [5025/61080], Loss: 4.9513\n",
      "Epoch [1/4], Step [5100/61080], Loss: 4.9658\n",
      "Epoch [1/4], Step [5175/61080], Loss: 4.9440\n",
      "Epoch [1/4], Step [5250/61080], Loss: 5.2128\n",
      "Epoch [1/4], Step [5325/61080], Loss: 4.8646\n",
      "Epoch [1/4], Step [5400/61080], Loss: 4.6649\n",
      "Epoch [1/4], Step [5475/61080], Loss: 4.4726\n",
      "Epoch [1/4], Step [5550/61080], Loss: 4.8642\n",
      "Epoch [1/4], Step [5625/61080], Loss: 4.7476\n",
      "Epoch [1/4], Step [5700/61080], Loss: 5.0665\n",
      "Epoch [1/4], Step [5775/61080], Loss: 4.8178\n",
      "Epoch [1/4], Step [5850/61080], Loss: 4.6525\n",
      "Epoch [1/4], Step [5925/61080], Loss: 4.6304\n",
      "Epoch [1/4], Step [6000/61080], Loss: 5.0701\n",
      "Validation perplexity: 101.82958031478915\n",
      "Epoch [1/4], Step [6075/61080], Loss: 5.2855\n",
      "Epoch [1/4], Step [6150/61080], Loss: 4.9305\n",
      "Epoch [1/4], Step [6225/61080], Loss: 4.4973\n",
      "Epoch [1/4], Step [6300/61080], Loss: 5.0940\n",
      "Epoch [1/4], Step [6375/61080], Loss: 4.8691\n",
      "Epoch [1/4], Step [6450/61080], Loss: 4.7719\n",
      "Epoch [1/4], Step [6525/61080], Loss: 4.9498\n",
      "Epoch [1/4], Step [6600/61080], Loss: 4.9101\n",
      "Epoch [1/4], Step [6675/61080], Loss: 4.7682\n",
      "Epoch [1/4], Step [6750/61080], Loss: 4.8837\n",
      "Epoch [1/4], Step [6825/61080], Loss: 4.9509\n",
      "Epoch [1/4], Step [6900/61080], Loss: 4.6814\n",
      "Epoch [1/4], Step [6975/61080], Loss: 4.5733\n",
      "Epoch [1/4], Step [7050/61080], Loss: 4.7052\n",
      "Epoch [1/4], Step [7125/61080], Loss: 4.6499\n",
      "Epoch [1/4], Step [7200/61080], Loss: 4.6925\n",
      "Epoch [1/4], Step [7275/61080], Loss: 4.5791\n",
      "Epoch [1/4], Step [7350/61080], Loss: 4.6338\n",
      "Epoch [1/4], Step [7425/61080], Loss: 4.9596\n",
      "Epoch [1/4], Step [7500/61080], Loss: 4.6223\n",
      "Epoch [1/4], Step [7575/61080], Loss: 4.7608\n",
      "Epoch [1/4], Step [7650/61080], Loss: 5.0830\n",
      "Epoch [1/4], Step [7725/61080], Loss: 4.8924\n",
      "Epoch [1/4], Step [7800/61080], Loss: 4.7254\n",
      "Epoch [1/4], Step [7875/61080], Loss: 4.6768\n",
      "Epoch [1/4], Step [7950/61080], Loss: 4.6293\n",
      "Epoch [1/4], Step [8025/61080], Loss: 4.6071\n",
      "Epoch [1/4], Step [8100/61080], Loss: 4.7292\n",
      "Epoch [1/4], Step [8175/61080], Loss: 4.4593\n",
      "Epoch [1/4], Step [8250/61080], Loss: 4.8365\n",
      "Epoch [1/4], Step [8325/61080], Loss: 4.6404\n",
      "Epoch [1/4], Step [8400/61080], Loss: 4.6418\n",
      "Epoch [1/4], Step [8475/61080], Loss: 4.5592\n",
      "Epoch [1/4], Step [8550/61080], Loss: 4.6554\n",
      "Epoch [1/4], Step [8625/61080], Loss: 4.7730\n",
      "Epoch [1/4], Step [8700/61080], Loss: 4.7465\n",
      "Epoch [1/4], Step [8775/61080], Loss: 4.6500\n",
      "Epoch [1/4], Step [8850/61080], Loss: 4.6044\n",
      "Epoch [1/4], Step [8925/61080], Loss: 4.6408\n",
      "Epoch [1/4], Step [9000/61080], Loss: 4.6246\n",
      "Validation perplexity: 85.17247640429161\n",
      "Epoch [1/4], Step [9075/61080], Loss: 4.4255\n",
      "Epoch [1/4], Step [9150/61080], Loss: 4.6413\n",
      "Epoch [1/4], Step [9225/61080], Loss: 4.7500\n",
      "Epoch [1/4], Step [9300/61080], Loss: 4.5090\n",
      "Epoch [1/4], Step [9375/61080], Loss: 4.6807\n",
      "Epoch [1/4], Step [9450/61080], Loss: 4.4830\n",
      "Epoch [1/4], Step [9525/61080], Loss: 4.5220\n",
      "Epoch [1/4], Step [9600/61080], Loss: 4.5290\n",
      "Epoch [1/4], Step [9675/61080], Loss: 4.6739\n",
      "Epoch [1/4], Step [9750/61080], Loss: 4.8210\n",
      "Epoch [1/4], Step [9825/61080], Loss: 4.1478\n",
      "Epoch [1/4], Step [9900/61080], Loss: 4.5604\n",
      "Epoch [1/4], Step [9975/61080], Loss: 4.5737\n",
      "Epoch [1/4], Step [10050/61080], Loss: 5.0650\n",
      "Epoch [1/4], Step [10125/61080], Loss: 4.3611\n",
      "Epoch [1/4], Step [10200/61080], Loss: 4.6715\n",
      "Epoch [1/4], Step [10275/61080], Loss: 4.7608\n",
      "Epoch [1/4], Step [10350/61080], Loss: 4.5439\n",
      "Epoch [1/4], Step [10425/61080], Loss: 4.2300\n",
      "Epoch [1/4], Step [10500/61080], Loss: 4.3100\n",
      "Epoch [1/4], Step [10575/61080], Loss: 4.5386\n",
      "Epoch [1/4], Step [10650/61080], Loss: 4.5127\n",
      "Epoch [1/4], Step [10725/61080], Loss: 4.5680\n",
      "Epoch [1/4], Step [10800/61080], Loss: 5.0685\n",
      "Epoch [1/4], Step [10875/61080], Loss: 4.5787\n",
      "Epoch [1/4], Step [10950/61080], Loss: 4.5790\n",
      "Epoch [1/4], Step [11025/61080], Loss: 4.5398\n",
      "Epoch [1/4], Step [11100/61080], Loss: 4.3143\n",
      "Epoch [1/4], Step [11175/61080], Loss: 4.7553\n",
      "Epoch [1/4], Step [11250/61080], Loss: 4.4763\n",
      "Epoch [1/4], Step [11325/61080], Loss: 4.5363\n",
      "Epoch [1/4], Step [11400/61080], Loss: 4.2847\n",
      "Epoch [1/4], Step [11475/61080], Loss: 4.3817\n",
      "Epoch [1/4], Step [11550/61080], Loss: 4.6224\n",
      "Epoch [1/4], Step [11625/61080], Loss: 4.8660\n",
      "Epoch [1/4], Step [11700/61080], Loss: 4.2989\n",
      "Epoch [1/4], Step [11775/61080], Loss: 4.2726\n",
      "Epoch [1/4], Step [11850/61080], Loss: 4.5063\n",
      "Epoch [1/4], Step [11925/61080], Loss: 4.6842\n",
      "Epoch [1/4], Step [12000/61080], Loss: 4.5863\n",
      "Validation perplexity: 76.16020600416732\n",
      "Epoch [1/4], Step [12075/61080], Loss: 4.2608\n",
      "Epoch [1/4], Step [12150/61080], Loss: 4.3434\n",
      "Epoch [1/4], Step [12225/61080], Loss: 4.6479\n",
      "Epoch [1/4], Step [12300/61080], Loss: 4.5292\n",
      "Epoch [1/4], Step [12375/61080], Loss: 4.5012\n",
      "Epoch [1/4], Step [12450/61080], Loss: 4.3790\n",
      "Epoch [1/4], Step [12525/61080], Loss: 4.5397\n",
      "Epoch [1/4], Step [12600/61080], Loss: 4.3799\n",
      "Epoch [1/4], Step [12675/61080], Loss: 4.5544\n",
      "Epoch [1/4], Step [12750/61080], Loss: 4.3917\n",
      "Epoch [1/4], Step [12825/61080], Loss: 4.4575\n",
      "Epoch [1/4], Step [12900/61080], Loss: 4.6986\n",
      "Epoch [1/4], Step [12975/61080], Loss: 4.3566\n",
      "Epoch [1/4], Step [13050/61080], Loss: 4.5022\n",
      "Epoch [1/4], Step [13125/61080], Loss: 4.3949\n",
      "Epoch [1/4], Step [13200/61080], Loss: 4.6289\n",
      "Epoch [1/4], Step [13275/61080], Loss: 4.6272\n",
      "Epoch [1/4], Step [13350/61080], Loss: 4.4207\n",
      "Epoch [1/4], Step [13425/61080], Loss: 4.4124\n",
      "Epoch [1/4], Step [13500/61080], Loss: 4.4286\n",
      "Epoch [1/4], Step [13575/61080], Loss: 4.4841\n",
      "Epoch [1/4], Step [13650/61080], Loss: 4.5809\n",
      "Epoch [1/4], Step [13725/61080], Loss: 4.5926\n",
      "Epoch [1/4], Step [13800/61080], Loss: 4.5462\n",
      "Epoch [1/4], Step [13875/61080], Loss: 4.3844\n",
      "Epoch [1/4], Step [13950/61080], Loss: 4.3118\n",
      "Epoch [1/4], Step [14025/61080], Loss: 4.4313\n",
      "Epoch [1/4], Step [14100/61080], Loss: 4.4703\n",
      "Epoch [1/4], Step [14175/61080], Loss: 4.4352\n",
      "Epoch [1/4], Step [14250/61080], Loss: 4.3113\n",
      "Epoch [1/4], Step [14325/61080], Loss: 4.3147\n",
      "Epoch [1/4], Step [14400/61080], Loss: 4.5152\n",
      "Epoch [1/4], Step [14475/61080], Loss: 4.5776\n",
      "Epoch [1/4], Step [14550/61080], Loss: 4.4888\n",
      "Epoch [1/4], Step [14625/61080], Loss: 4.0812\n",
      "Epoch [1/4], Step [14700/61080], Loss: 4.5451\n",
      "Epoch [1/4], Step [14775/61080], Loss: 4.4912\n",
      "Epoch [1/4], Step [14850/61080], Loss: 4.3662\n",
      "Epoch [1/4], Step [14925/61080], Loss: 4.6768\n",
      "Epoch [1/4], Step [15000/61080], Loss: 4.6952\n",
      "Validation perplexity: 70.82976285638829\n",
      "Epoch [1/4], Step [15075/61080], Loss: 4.4319\n",
      "Epoch [1/4], Step [15150/61080], Loss: 4.5727\n",
      "Epoch [1/4], Step [15225/61080], Loss: 4.4898\n",
      "Epoch [1/4], Step [15300/61080], Loss: 4.3883\n",
      "Epoch [1/4], Step [15375/61080], Loss: 4.6777\n",
      "Epoch [1/4], Step [15450/61080], Loss: 4.3456\n",
      "Epoch [1/4], Step [15525/61080], Loss: 4.2536\n",
      "Epoch [1/4], Step [15600/61080], Loss: 4.5386\n",
      "Epoch [1/4], Step [15675/61080], Loss: 4.4368\n",
      "Epoch [1/4], Step [15750/61080], Loss: 4.7224\n",
      "Epoch [1/4], Step [15825/61080], Loss: 4.4014\n",
      "Epoch [1/4], Step [15900/61080], Loss: 4.2581\n",
      "Epoch [1/4], Step [15975/61080], Loss: 4.4039\n",
      "Epoch [1/4], Step [16050/61080], Loss: 4.4756\n",
      "Epoch [1/4], Step [16125/61080], Loss: 4.5560\n",
      "Epoch [1/4], Step [16200/61080], Loss: 4.2715\n",
      "Epoch [1/4], Step [16275/61080], Loss: 4.5180\n",
      "Epoch [1/4], Step [16350/61080], Loss: 4.3001\n",
      "Epoch [1/4], Step [16425/61080], Loss: 4.2683\n",
      "Epoch [1/4], Step [16500/61080], Loss: 4.5209\n",
      "Epoch [1/4], Step [16575/61080], Loss: 4.5572\n",
      "Epoch [1/4], Step [16650/61080], Loss: 4.4297\n",
      "Epoch [1/4], Step [16725/61080], Loss: 4.4823\n",
      "Epoch [1/4], Step [16800/61080], Loss: 4.6585\n",
      "Epoch [1/4], Step [16875/61080], Loss: 4.2031\n",
      "Epoch [1/4], Step [16950/61080], Loss: 4.1471\n",
      "Epoch [1/4], Step [17025/61080], Loss: 4.4399\n",
      "Epoch [1/4], Step [17100/61080], Loss: 4.5863\n",
      "Epoch [1/4], Step [17175/61080], Loss: 4.5630\n",
      "Epoch [1/4], Step [17250/61080], Loss: 4.3648\n",
      "Epoch [1/4], Step [17325/61080], Loss: 4.6328\n",
      "Epoch [1/4], Step [17400/61080], Loss: 4.3931\n",
      "Epoch [1/4], Step [17475/61080], Loss: 4.2166\n",
      "Epoch [1/4], Step [17550/61080], Loss: 4.4837\n",
      "Epoch [1/4], Step [17625/61080], Loss: 4.2815\n",
      "Epoch [1/4], Step [17700/61080], Loss: 4.1827\n",
      "Epoch [1/4], Step [17775/61080], Loss: 4.4646\n",
      "Epoch [1/4], Step [17850/61080], Loss: 4.2901\n",
      "Epoch [1/4], Step [17925/61080], Loss: 4.3009\n",
      "Epoch [1/4], Step [18000/61080], Loss: 4.3320\n",
      "Validation perplexity: 66.25233959806486\n",
      "Epoch [1/4], Step [18075/61080], Loss: 4.6042\n",
      "Epoch [1/4], Step [18150/61080], Loss: 4.4328\n",
      "Epoch [1/4], Step [18225/61080], Loss: 4.5607\n",
      "Epoch [1/4], Step [18300/61080], Loss: 4.3733\n",
      "Epoch [1/4], Step [18375/61080], Loss: 4.3818\n",
      "Epoch [1/4], Step [18450/61080], Loss: 4.3456\n",
      "Epoch [1/4], Step [18525/61080], Loss: 4.1232\n",
      "Epoch [1/4], Step [18600/61080], Loss: 4.3162\n",
      "Epoch [1/4], Step [18675/61080], Loss: 4.5855\n",
      "Epoch [1/4], Step [18750/61080], Loss: 4.5114\n",
      "Epoch [1/4], Step [18825/61080], Loss: 4.3683\n",
      "Epoch [1/4], Step [18900/61080], Loss: 4.1538\n",
      "Epoch [1/4], Step [18975/61080], Loss: 4.3145\n",
      "Epoch [1/4], Step [19050/61080], Loss: 4.4030\n",
      "Epoch [1/4], Step [19125/61080], Loss: 4.4175\n",
      "Epoch [1/4], Step [19200/61080], Loss: 4.5900\n",
      "Epoch [1/4], Step [19275/61080], Loss: 4.7024\n",
      "Epoch [1/4], Step [19350/61080], Loss: 4.4764\n",
      "Epoch [1/4], Step [19425/61080], Loss: 4.4316\n",
      "Epoch [1/4], Step [19500/61080], Loss: 4.2216\n",
      "Epoch [1/4], Step [19575/61080], Loss: 4.4277\n",
      "Epoch [1/4], Step [19650/61080], Loss: 4.1549\n",
      "Epoch [1/4], Step [19725/61080], Loss: 4.3796\n",
      "Epoch [1/4], Step [19800/61080], Loss: 4.4381\n",
      "Epoch [1/4], Step [19875/61080], Loss: 4.5389\n",
      "Epoch [1/4], Step [19950/61080], Loss: 4.4423\n",
      "Epoch [1/4], Step [20025/61080], Loss: 4.3154\n",
      "Epoch [1/4], Step [20100/61080], Loss: 4.5302\n",
      "Epoch [1/4], Step [20175/61080], Loss: 4.3250\n",
      "Epoch [1/4], Step [20250/61080], Loss: 4.5033\n",
      "Epoch [1/4], Step [20325/61080], Loss: 4.4184\n",
      "Epoch [1/4], Step [20400/61080], Loss: 4.2909\n",
      "Epoch [1/4], Step [20475/61080], Loss: 4.3770\n",
      "Epoch [1/4], Step [20550/61080], Loss: 4.5472\n",
      "Epoch [1/4], Step [20625/61080], Loss: 4.1596\n",
      "Epoch [1/4], Step [20700/61080], Loss: 4.4286\n",
      "Epoch [1/4], Step [20775/61080], Loss: 4.3147\n",
      "Epoch [1/4], Step [20850/61080], Loss: 4.4661\n",
      "Epoch [1/4], Step [20925/61080], Loss: 4.2479\n",
      "Epoch [1/4], Step [21000/61080], Loss: 4.3600\n",
      "Validation perplexity: 63.35232388162387\n",
      "Epoch [1/4], Step [21075/61080], Loss: 4.5529\n",
      "Epoch [1/4], Step [21150/61080], Loss: 4.1296\n",
      "Epoch [1/4], Step [21225/61080], Loss: 4.2846\n",
      "Epoch [1/4], Step [21300/61080], Loss: 4.4680\n",
      "Epoch [1/4], Step [21375/61080], Loss: 4.2851\n",
      "Epoch [1/4], Step [21450/61080], Loss: 4.5685\n",
      "Epoch [1/4], Step [21525/61080], Loss: 4.1649\n",
      "Epoch [1/4], Step [21600/61080], Loss: 4.3751\n",
      "Epoch [1/4], Step [21675/61080], Loss: 4.2520\n",
      "Epoch [1/4], Step [21750/61080], Loss: 4.4846\n",
      "Epoch [1/4], Step [21825/61080], Loss: 4.2814\n",
      "Epoch [1/4], Step [21900/61080], Loss: 4.3263\n",
      "Epoch [1/4], Step [21975/61080], Loss: 4.3110\n",
      "Epoch [1/4], Step [22050/61080], Loss: 4.4945\n",
      "Epoch [1/4], Step [22125/61080], Loss: 4.5791\n",
      "Epoch [1/4], Step [22200/61080], Loss: 4.2825\n",
      "Epoch [1/4], Step [22275/61080], Loss: 4.3290\n",
      "Epoch [1/4], Step [22350/61080], Loss: 4.4108\n",
      "Epoch [1/4], Step [22425/61080], Loss: 4.2649\n",
      "Epoch [1/4], Step [22500/61080], Loss: 4.6013\n",
      "Epoch [1/4], Step [22575/61080], Loss: 4.0742\n",
      "Epoch [1/4], Step [22650/61080], Loss: 4.4323\n",
      "Epoch [1/4], Step [22725/61080], Loss: 4.3621\n",
      "Epoch [1/4], Step [22800/61080], Loss: 4.2576\n",
      "Epoch [1/4], Step [22875/61080], Loss: 4.3842\n",
      "Epoch [1/4], Step [22950/61080], Loss: 4.1001\n",
      "Epoch [1/4], Step [23025/61080], Loss: 4.3623\n",
      "Epoch [1/4], Step [23100/61080], Loss: 4.3085\n",
      "Epoch [1/4], Step [23175/61080], Loss: 4.5392\n",
      "Epoch [1/4], Step [23250/61080], Loss: 4.4003\n",
      "Epoch [1/4], Step [23325/61080], Loss: 4.3129\n",
      "Epoch [1/4], Step [23400/61080], Loss: 4.5117\n",
      "Epoch [1/4], Step [23475/61080], Loss: 4.5168\n",
      "Epoch [1/4], Step [23550/61080], Loss: 4.4961\n",
      "Epoch [1/4], Step [23625/61080], Loss: 4.1506\n",
      "Epoch [1/4], Step [23700/61080], Loss: 4.3920\n",
      "Epoch [1/4], Step [23775/61080], Loss: 3.9747\n",
      "Epoch [1/4], Step [23850/61080], Loss: 4.6211\n",
      "Epoch [1/4], Step [23925/61080], Loss: 4.5302\n",
      "Epoch [1/4], Step [24000/61080], Loss: 4.1302\n",
      "Validation perplexity: 60.99645264221214\n",
      "Epoch [1/4], Step [24075/61080], Loss: 4.4385\n",
      "Epoch [1/4], Step [24150/61080], Loss: 4.2706\n",
      "Epoch [1/4], Step [24225/61080], Loss: 4.4476\n",
      "Epoch [1/4], Step [24300/61080], Loss: 4.6065\n",
      "Epoch [1/4], Step [24375/61080], Loss: 4.2653\n",
      "Epoch [1/4], Step [24450/61080], Loss: 4.3048\n",
      "Epoch [1/4], Step [24525/61080], Loss: 4.1398\n",
      "Epoch [1/4], Step [24600/61080], Loss: 4.5367\n",
      "Epoch [1/4], Step [24675/61080], Loss: 4.1263\n",
      "Epoch [1/4], Step [24750/61080], Loss: 4.1275\n",
      "Epoch [1/4], Step [24825/61080], Loss: 4.5304\n",
      "Epoch [1/4], Step [24900/61080], Loss: 4.2659\n",
      "Epoch [1/4], Step [24975/61080], Loss: 4.2282\n",
      "Epoch [1/4], Step [25050/61080], Loss: 4.4038\n",
      "Epoch [1/4], Step [25125/61080], Loss: 4.3429\n",
      "Epoch [1/4], Step [25200/61080], Loss: 4.3363\n",
      "Epoch [1/4], Step [25275/61080], Loss: 4.2146\n",
      "Epoch [1/4], Step [25350/61080], Loss: 4.3268\n",
      "Epoch [1/4], Step [25425/61080], Loss: 4.0577\n",
      "Epoch [1/4], Step [25500/61080], Loss: 4.4577\n",
      "Epoch [1/4], Step [25575/61080], Loss: 4.2078\n",
      "Epoch [1/4], Step [25650/61080], Loss: 4.1708\n",
      "Epoch [1/4], Step [25725/61080], Loss: 4.5188\n",
      "Epoch [1/4], Step [25800/61080], Loss: 4.2313\n",
      "Epoch [1/4], Step [25875/61080], Loss: 4.1058\n",
      "Epoch [1/4], Step [25950/61080], Loss: 4.2764\n",
      "Epoch [1/4], Step [26025/61080], Loss: 4.4140\n",
      "Epoch [1/4], Step [26100/61080], Loss: 4.3155\n",
      "Epoch [1/4], Step [26175/61080], Loss: 4.5913\n",
      "Epoch [1/4], Step [26250/61080], Loss: 4.3233\n",
      "Epoch [1/4], Step [26325/61080], Loss: 4.1290\n",
      "Epoch [1/4], Step [26400/61080], Loss: 4.4188\n",
      "Epoch [1/4], Step [26475/61080], Loss: 4.2157\n",
      "Epoch [1/4], Step [26550/61080], Loss: 4.1953\n",
      "Epoch [1/4], Step [26625/61080], Loss: 4.4955\n",
      "Epoch [1/4], Step [26700/61080], Loss: 4.1729\n",
      "Epoch [1/4], Step [26775/61080], Loss: 4.3220\n",
      "Epoch [1/4], Step [26850/61080], Loss: 4.2351\n",
      "Epoch [1/4], Step [26925/61080], Loss: 4.3104\n",
      "Epoch [1/4], Step [27000/61080], Loss: 4.4133\n",
      "Validation perplexity: 58.80045438656942\n",
      "Epoch [1/4], Step [27075/61080], Loss: 4.1760\n",
      "Epoch [1/4], Step [27150/61080], Loss: 4.2913\n",
      "Epoch [1/4], Step [27225/61080], Loss: 4.1307\n",
      "Epoch [1/4], Step [27300/61080], Loss: 4.2252\n",
      "Epoch [1/4], Step [27375/61080], Loss: 4.2047\n",
      "Epoch [1/4], Step [27450/61080], Loss: 4.4244\n",
      "Epoch [1/4], Step [27525/61080], Loss: 4.2978\n",
      "Epoch [1/4], Step [27600/61080], Loss: 4.2517\n",
      "Epoch [1/4], Step [27675/61080], Loss: 3.9923\n",
      "Epoch [1/4], Step [27750/61080], Loss: 4.4833\n",
      "Epoch [1/4], Step [27825/61080], Loss: 4.2195\n",
      "Epoch [1/4], Step [27900/61080], Loss: 4.3539\n",
      "Epoch [1/4], Step [27975/61080], Loss: 4.2326\n",
      "Epoch [1/4], Step [28050/61080], Loss: 4.4630\n",
      "Epoch [1/4], Step [28125/61080], Loss: 4.0908\n",
      "Epoch [1/4], Step [28200/61080], Loss: 4.0640\n",
      "Epoch [1/4], Step [28275/61080], Loss: 4.2174\n",
      "Epoch [1/4], Step [28350/61080], Loss: 4.2083\n",
      "Epoch [1/4], Step [28425/61080], Loss: 4.4630\n",
      "Epoch [1/4], Step [28500/61080], Loss: 4.2986\n",
      "Epoch [1/4], Step [28575/61080], Loss: 4.2261\n",
      "Epoch [1/4], Step [28650/61080], Loss: 4.4268\n",
      "Epoch [1/4], Step [28725/61080], Loss: 4.1908\n",
      "Epoch [1/4], Step [28800/61080], Loss: 4.3000\n",
      "Epoch [1/4], Step [28875/61080], Loss: 3.9602\n",
      "Epoch [1/4], Step [28950/61080], Loss: 4.2671\n",
      "Epoch [1/4], Step [29025/61080], Loss: 4.4778\n",
      "Epoch [1/4], Step [29100/61080], Loss: 4.4305\n",
      "Epoch [1/4], Step [29175/61080], Loss: 4.0130\n",
      "Epoch [1/4], Step [29250/61080], Loss: 4.6817\n",
      "Epoch [1/4], Step [29325/61080], Loss: 4.3150\n",
      "Epoch [1/4], Step [29400/61080], Loss: 4.1451\n",
      "Epoch [1/4], Step [29475/61080], Loss: 4.3658\n",
      "Epoch [1/4], Step [29550/61080], Loss: 4.4455\n",
      "Epoch [1/4], Step [29625/61080], Loss: 4.4504\n",
      "Epoch [1/4], Step [29700/61080], Loss: 4.4168\n",
      "Epoch [1/4], Step [29775/61080], Loss: 4.2486\n",
      "Epoch [1/4], Step [29850/61080], Loss: 4.2912\n",
      "Epoch [1/4], Step [29925/61080], Loss: 4.2282\n",
      "Epoch [1/4], Step [30000/61080], Loss: 4.4670\n",
      "Validation perplexity: 57.081614651161374\n",
      "Epoch [1/4], Step [30075/61080], Loss: 4.1322\n",
      "Epoch [1/4], Step [30150/61080], Loss: 4.2577\n",
      "Epoch [1/4], Step [30225/61080], Loss: 3.8963\n",
      "Epoch [1/4], Step [30300/61080], Loss: 3.9854\n",
      "Epoch [1/4], Step [30375/61080], Loss: 4.2424\n",
      "Epoch [1/4], Step [30450/61080], Loss: 4.2097\n",
      "Epoch [1/4], Step [30525/61080], Loss: 4.6611\n",
      "Epoch [1/4], Step [30600/61080], Loss: 4.3355\n",
      "Epoch [1/4], Step [30675/61080], Loss: 4.5321\n",
      "Epoch [1/4], Step [30750/61080], Loss: 4.2307\n",
      "Epoch [1/4], Step [30825/61080], Loss: 4.4421\n",
      "Epoch [1/4], Step [30900/61080], Loss: 4.4549\n",
      "Epoch [1/4], Step [30975/61080], Loss: 4.0481\n",
      "Epoch [1/4], Step [31050/61080], Loss: 4.2579\n",
      "Epoch [1/4], Step [31125/61080], Loss: 4.2981\n",
      "Epoch [1/4], Step [31200/61080], Loss: 4.3318\n",
      "Epoch [1/4], Step [31275/61080], Loss: 4.0821\n",
      "Epoch [1/4], Step [31350/61080], Loss: 4.3627\n",
      "Epoch [1/4], Step [31425/61080], Loss: 3.9905\n",
      "Epoch [1/4], Step [31500/61080], Loss: 4.3042\n",
      "Epoch [1/4], Step [31575/61080], Loss: 3.9765\n",
      "Epoch [1/4], Step [31650/61080], Loss: 4.2138\n",
      "Epoch [1/4], Step [31725/61080], Loss: 4.2003\n",
      "Epoch [1/4], Step [31800/61080], Loss: 4.4155\n",
      "Epoch [1/4], Step [31875/61080], Loss: 4.2645\n",
      "Epoch [1/4], Step [31950/61080], Loss: 3.9302\n",
      "Epoch [1/4], Step [32025/61080], Loss: 4.4325\n",
      "Epoch [1/4], Step [32100/61080], Loss: 4.1982\n",
      "Epoch [1/4], Step [32175/61080], Loss: 4.0379\n",
      "Epoch [1/4], Step [32250/61080], Loss: 4.4582\n",
      "Epoch [1/4], Step [32325/61080], Loss: 4.3194\n",
      "Epoch [1/4], Step [32400/61080], Loss: 4.1932\n",
      "Epoch [1/4], Step [32475/61080], Loss: 4.1513\n",
      "Epoch [1/4], Step [32550/61080], Loss: 4.3534\n",
      "Epoch [1/4], Step [32625/61080], Loss: 4.2726\n",
      "Epoch [1/4], Step [32700/61080], Loss: 4.2806\n",
      "Epoch [1/4], Step [32775/61080], Loss: 4.1560\n",
      "Epoch [1/4], Step [32850/61080], Loss: 4.2976\n",
      "Epoch [1/4], Step [32925/61080], Loss: 4.1885\n",
      "Epoch [1/4], Step [33000/61080], Loss: 4.5090\n",
      "Validation perplexity: 55.56467122029034\n",
      "Epoch [1/4], Step [33075/61080], Loss: 4.3190\n",
      "Epoch [1/4], Step [33150/61080], Loss: 4.4422\n",
      "Epoch [1/4], Step [33225/61080], Loss: 4.1912\n",
      "Epoch [1/4], Step [33300/61080], Loss: 4.1970\n",
      "Epoch [1/4], Step [33375/61080], Loss: 4.2897\n",
      "Epoch [1/4], Step [33450/61080], Loss: 4.5953\n",
      "Epoch [1/4], Step [33525/61080], Loss: 4.1484\n",
      "Epoch [1/4], Step [33600/61080], Loss: 4.2544\n",
      "Epoch [1/4], Step [33675/61080], Loss: 4.0415\n",
      "Epoch [1/4], Step [33750/61080], Loss: 4.3922\n",
      "Epoch [1/4], Step [33825/61080], Loss: 4.1768\n",
      "Epoch [1/4], Step [33900/61080], Loss: 4.2870\n",
      "Epoch [1/4], Step [33975/61080], Loss: 4.4520\n",
      "Epoch [1/4], Step [34050/61080], Loss: 4.5117\n",
      "Epoch [1/4], Step [34125/61080], Loss: 4.1131\n",
      "Epoch [1/4], Step [34200/61080], Loss: 4.1818\n",
      "Epoch [1/4], Step [34275/61080], Loss: 4.2248\n",
      "Epoch [1/4], Step [34350/61080], Loss: 4.1858\n",
      "Epoch [1/4], Step [34425/61080], Loss: 4.3753\n",
      "Epoch [1/4], Step [34500/61080], Loss: 3.8789\n",
      "Epoch [1/4], Step [34575/61080], Loss: 4.2630\n",
      "Epoch [1/4], Step [34650/61080], Loss: 4.0835\n",
      "Epoch [1/4], Step [34725/61080], Loss: 4.0635\n",
      "Epoch [1/4], Step [34800/61080], Loss: 4.4566\n",
      "Epoch [1/4], Step [34875/61080], Loss: 4.0056\n",
      "Epoch [1/4], Step [34950/61080], Loss: 4.1967\n",
      "Epoch [1/4], Step [35025/61080], Loss: 4.1696\n",
      "Epoch [1/4], Step [35100/61080], Loss: 4.3028\n",
      "Epoch [1/4], Step [35175/61080], Loss: 4.2453\n",
      "Epoch [1/4], Step [35250/61080], Loss: 4.0397\n",
      "Epoch [1/4], Step [35325/61080], Loss: 4.1686\n",
      "Epoch [1/4], Step [35400/61080], Loss: 4.1180\n",
      "Epoch [1/4], Step [35475/61080], Loss: 4.1988\n",
      "Epoch [1/4], Step [35550/61080], Loss: 4.5007\n",
      "Epoch [1/4], Step [35625/61080], Loss: 4.4592\n",
      "Epoch [1/4], Step [35700/61080], Loss: 4.3976\n",
      "Epoch [1/4], Step [35775/61080], Loss: 4.3648\n",
      "Epoch [1/4], Step [35850/61080], Loss: 4.3040\n",
      "Epoch [1/4], Step [35925/61080], Loss: 3.9890\n",
      "Epoch [1/4], Step [36000/61080], Loss: 4.3440\n",
      "Validation perplexity: 54.422209767805924\n",
      "Epoch [1/4], Step [36075/61080], Loss: 4.0885\n",
      "Epoch [1/4], Step [36150/61080], Loss: 3.9340\n",
      "Epoch [1/4], Step [36225/61080], Loss: 4.0761\n",
      "Epoch [1/4], Step [36300/61080], Loss: 4.0847\n",
      "Epoch [1/4], Step [36375/61080], Loss: 3.9514\n",
      "Epoch [1/4], Step [36450/61080], Loss: 4.2327\n",
      "Epoch [1/4], Step [36525/61080], Loss: 4.2893\n",
      "Epoch [1/4], Step [36600/61080], Loss: 4.3810\n",
      "Epoch [1/4], Step [36675/61080], Loss: 4.2282\n",
      "Epoch [1/4], Step [36750/61080], Loss: 4.3034\n",
      "Epoch [1/4], Step [36825/61080], Loss: 4.0300\n",
      "Epoch [1/4], Step [36900/61080], Loss: 4.2060\n",
      "Epoch [1/4], Step [36975/61080], Loss: 4.4341\n",
      "Epoch [1/4], Step [37050/61080], Loss: 4.1437\n",
      "Epoch [1/4], Step [37125/61080], Loss: 4.0278\n",
      "Epoch [1/4], Step [37200/61080], Loss: 4.0745\n",
      "Epoch [1/4], Step [37275/61080], Loss: 4.3817\n",
      "Epoch [1/4], Step [37350/61080], Loss: 4.1465\n",
      "Epoch [1/4], Step [37425/61080], Loss: 4.3025\n",
      "Epoch [1/4], Step [37500/61080], Loss: 4.1304\n",
      "Epoch [1/4], Step [37575/61080], Loss: 4.0382\n",
      "Epoch [1/4], Step [37650/61080], Loss: 4.4731\n",
      "Epoch [1/4], Step [37725/61080], Loss: 4.1397\n",
      "Epoch [1/4], Step [37800/61080], Loss: 3.8793\n",
      "Epoch [1/4], Step [37875/61080], Loss: 4.1420\n",
      "Epoch [1/4], Step [37950/61080], Loss: 4.1513\n",
      "Epoch [1/4], Step [38025/61080], Loss: 4.0888\n",
      "Epoch [1/4], Step [38100/61080], Loss: 4.5093\n",
      "Epoch [1/4], Step [38175/61080], Loss: 4.0740\n",
      "Epoch [1/4], Step [38250/61080], Loss: 4.0983\n",
      "Epoch [1/4], Step [38325/61080], Loss: 4.0555\n",
      "Epoch [1/4], Step [38400/61080], Loss: 4.0015\n",
      "Epoch [1/4], Step [38475/61080], Loss: 4.6246\n",
      "Epoch [1/4], Step [38550/61080], Loss: 4.3537\n",
      "Epoch [1/4], Step [38625/61080], Loss: 4.2775\n",
      "Epoch [1/4], Step [38700/61080], Loss: 4.2675\n",
      "Epoch [1/4], Step [38775/61080], Loss: 4.2055\n",
      "Epoch [1/4], Step [38850/61080], Loss: 4.7273\n",
      "Epoch [1/4], Step [38925/61080], Loss: 4.1730\n",
      "Epoch [1/4], Step [39000/61080], Loss: 4.2875\n",
      "Validation perplexity: 53.67028584660292\n",
      "Epoch [1/4], Step [39075/61080], Loss: 3.9812\n",
      "Epoch [1/4], Step [39150/61080], Loss: 4.1483\n",
      "Epoch [1/4], Step [39225/61080], Loss: 4.1589\n",
      "Epoch [1/4], Step [39300/61080], Loss: 4.3001\n",
      "Epoch [1/4], Step [39375/61080], Loss: 4.0990\n",
      "Epoch [1/4], Step [39450/61080], Loss: 4.2468\n",
      "Epoch [1/4], Step [39525/61080], Loss: 4.4185\n",
      "Epoch [1/4], Step [39600/61080], Loss: 4.3197\n",
      "Epoch [1/4], Step [39675/61080], Loss: 4.1688\n",
      "Epoch [1/4], Step [39750/61080], Loss: 4.5782\n",
      "Epoch [1/4], Step [39825/61080], Loss: 4.0388\n",
      "Epoch [1/4], Step [39900/61080], Loss: 4.2717\n",
      "Epoch [1/4], Step [39975/61080], Loss: 3.9749\n",
      "Epoch [1/4], Step [40050/61080], Loss: 4.3181\n",
      "Epoch [1/4], Step [40125/61080], Loss: 4.0704\n",
      "Epoch [1/4], Step [40200/61080], Loss: 4.1506\n",
      "Epoch [1/4], Step [40275/61080], Loss: 4.2162\n",
      "Epoch [1/4], Step [40350/61080], Loss: 4.2086\n",
      "Epoch [1/4], Step [40425/61080], Loss: 4.2004\n",
      "Epoch [1/4], Step [40500/61080], Loss: 4.3675\n",
      "Epoch [1/4], Step [40575/61080], Loss: 3.9350\n",
      "Epoch [1/4], Step [40650/61080], Loss: 4.4023\n",
      "Epoch [1/4], Step [40725/61080], Loss: 4.1865\n",
      "Epoch [1/4], Step [40800/61080], Loss: 3.8837\n",
      "Epoch [1/4], Step [40875/61080], Loss: 4.0963\n",
      "Epoch [1/4], Step [40950/61080], Loss: 4.1463\n",
      "Epoch [1/4], Step [41025/61080], Loss: 4.0630\n",
      "Epoch [1/4], Step [41100/61080], Loss: 4.1629\n",
      "Epoch [1/4], Step [41175/61080], Loss: 4.5323\n",
      "Epoch [1/4], Step [41250/61080], Loss: 4.2143\n",
      "Epoch [1/4], Step [41325/61080], Loss: 4.4086\n",
      "Epoch [1/4], Step [41400/61080], Loss: 4.2358\n",
      "Epoch [1/4], Step [41475/61080], Loss: 4.1622\n",
      "Epoch [1/4], Step [41550/61080], Loss: 4.2696\n",
      "Epoch [1/4], Step [41625/61080], Loss: 4.0101\n",
      "Epoch [1/4], Step [41700/61080], Loss: 4.3711\n",
      "Epoch [1/4], Step [41775/61080], Loss: 3.9800\n",
      "Epoch [1/4], Step [41850/61080], Loss: 4.1466\n",
      "Epoch [1/4], Step [41925/61080], Loss: 3.9416\n",
      "Epoch [1/4], Step [42000/61080], Loss: 4.2155\n",
      "Validation perplexity: 52.181938493574435\n",
      "Epoch [1/4], Step [42075/61080], Loss: 3.8981\n",
      "Epoch [1/4], Step [42150/61080], Loss: 4.1187\n",
      "Epoch [1/4], Step [42225/61080], Loss: 4.1436\n",
      "Epoch [1/4], Step [42300/61080], Loss: 4.0641\n",
      "Epoch [1/4], Step [42375/61080], Loss: 4.2351\n",
      "Epoch [1/4], Step [42450/61080], Loss: 4.0845\n",
      "Epoch [1/4], Step [42525/61080], Loss: 4.1665\n",
      "Epoch [1/4], Step [42600/61080], Loss: 3.9232\n",
      "Epoch [1/4], Step [42675/61080], Loss: 3.9949\n",
      "Epoch [1/4], Step [42750/61080], Loss: 4.2150\n",
      "Epoch [1/4], Step [42825/61080], Loss: 4.1168\n",
      "Epoch [1/4], Step [42900/61080], Loss: 3.9322\n",
      "Epoch [1/4], Step [42975/61080], Loss: 4.6203\n",
      "Epoch [1/4], Step [43050/61080], Loss: 4.0543\n",
      "Epoch [1/4], Step [43125/61080], Loss: 4.2873\n",
      "Epoch [1/4], Step [43200/61080], Loss: 4.1869\n",
      "Epoch [1/4], Step [43275/61080], Loss: 4.0942\n",
      "Epoch [1/4], Step [43350/61080], Loss: 3.8398\n",
      "Epoch [1/4], Step [43425/61080], Loss: 3.9401\n",
      "Epoch [1/4], Step [43500/61080], Loss: 4.2722\n",
      "Epoch [1/4], Step [43575/61080], Loss: 4.2687\n",
      "Epoch [1/4], Step [43650/61080], Loss: 4.0532\n",
      "Epoch [1/4], Step [43725/61080], Loss: 3.9977\n",
      "Epoch [1/4], Step [43800/61080], Loss: 4.3021\n",
      "Epoch [1/4], Step [43875/61080], Loss: 4.0868\n",
      "Epoch [1/4], Step [43950/61080], Loss: 4.4434\n",
      "Epoch [1/4], Step [44025/61080], Loss: 4.1150\n",
      "Epoch [1/4], Step [44100/61080], Loss: 4.2086\n",
      "Epoch [1/4], Step [44175/61080], Loss: 4.4046\n",
      "Epoch [1/4], Step [44250/61080], Loss: 4.3851\n",
      "Epoch [1/4], Step [44325/61080], Loss: 4.0694\n",
      "Epoch [1/4], Step [44400/61080], Loss: 4.2363\n",
      "Epoch [1/4], Step [44475/61080], Loss: 4.2590\n",
      "Epoch [1/4], Step [44550/61080], Loss: 4.0524\n",
      "Epoch [1/4], Step [44625/61080], Loss: 4.3233\n",
      "Epoch [1/4], Step [44700/61080], Loss: 4.2032\n",
      "Epoch [1/4], Step [44775/61080], Loss: 4.0463\n",
      "Epoch [1/4], Step [44850/61080], Loss: 4.1272\n",
      "Epoch [1/4], Step [44925/61080], Loss: 4.1379\n",
      "Epoch [1/4], Step [45000/61080], Loss: 4.6124\n",
      "Validation perplexity: 51.76302252692531\n",
      "Epoch [1/4], Step [45075/61080], Loss: 3.9866\n",
      "Epoch [1/4], Step [45150/61080], Loss: 4.2600\n",
      "Epoch [1/4], Step [45225/61080], Loss: 4.0494\n",
      "Epoch [1/4], Step [45300/61080], Loss: 4.0941\n",
      "Epoch [1/4], Step [45375/61080], Loss: 4.0863\n",
      "Epoch [1/4], Step [45450/61080], Loss: 4.6062\n",
      "Epoch [1/4], Step [45525/61080], Loss: 4.1075\n",
      "Epoch [1/4], Step [45600/61080], Loss: 4.0734\n",
      "Epoch [1/4], Step [45675/61080], Loss: 4.1661\n",
      "Epoch [1/4], Step [45750/61080], Loss: 4.3244\n",
      "Epoch [1/4], Step [45825/61080], Loss: 4.2955\n",
      "Epoch [1/4], Step [45900/61080], Loss: 4.0863\n",
      "Epoch [1/4], Step [45975/61080], Loss: 3.8601\n",
      "Epoch [1/4], Step [46050/61080], Loss: 4.2417\n",
      "Epoch [1/4], Step [46125/61080], Loss: 4.3021\n",
      "Epoch [1/4], Step [46200/61080], Loss: 4.2578\n",
      "Epoch [1/4], Step [46275/61080], Loss: 4.4277\n",
      "Epoch [1/4], Step [46350/61080], Loss: 4.2170\n",
      "Epoch [1/4], Step [46425/61080], Loss: 4.3330\n",
      "Epoch [1/4], Step [46500/61080], Loss: 4.3666\n",
      "Epoch [1/4], Step [46575/61080], Loss: 4.3358\n",
      "Epoch [1/4], Step [46650/61080], Loss: 4.2052\n",
      "Epoch [1/4], Step [46725/61080], Loss: 4.0417\n",
      "Epoch [1/4], Step [46800/61080], Loss: 4.2556\n",
      "Epoch [1/4], Step [46875/61080], Loss: 4.3329\n",
      "Epoch [1/4], Step [46950/61080], Loss: 4.1924\n",
      "Epoch [1/4], Step [47025/61080], Loss: 4.4647\n",
      "Epoch [1/4], Step [47100/61080], Loss: 4.1339\n",
      "Epoch [1/4], Step [47175/61080], Loss: 4.3834\n",
      "Epoch [1/4], Step [47250/61080], Loss: 4.3419\n",
      "Epoch [1/4], Step [47325/61080], Loss: 4.2092\n",
      "Epoch [1/4], Step [47400/61080], Loss: 4.1554\n",
      "Epoch [1/4], Step [47475/61080], Loss: 3.8997\n",
      "Epoch [1/4], Step [47550/61080], Loss: 4.3344\n",
      "Epoch [1/4], Step [47625/61080], Loss: 4.2392\n",
      "Epoch [1/4], Step [47700/61080], Loss: 4.2100\n",
      "Epoch [1/4], Step [47775/61080], Loss: 4.3024\n",
      "Epoch [1/4], Step [47850/61080], Loss: 4.3543\n",
      "Epoch [1/4], Step [47925/61080], Loss: 4.2361\n",
      "Epoch [1/4], Step [48000/61080], Loss: 4.2019\n",
      "Validation perplexity: 51.094398605196695\n",
      "Epoch [1/4], Step [48075/61080], Loss: 4.0084\n",
      "Epoch [1/4], Step [48150/61080], Loss: 3.9930\n",
      "Epoch [1/4], Step [48225/61080], Loss: 4.1870\n",
      "Epoch [1/4], Step [48300/61080], Loss: 4.0126\n",
      "Epoch [1/4], Step [48375/61080], Loss: 4.2351\n",
      "Epoch [1/4], Step [48450/61080], Loss: 4.1631\n",
      "Epoch [1/4], Step [48525/61080], Loss: 4.5075\n",
      "Epoch [1/4], Step [48600/61080], Loss: 4.1923\n",
      "Epoch [1/4], Step [48675/61080], Loss: 4.1092\n",
      "Epoch [1/4], Step [48750/61080], Loss: 4.2087\n",
      "Epoch [1/4], Step [48825/61080], Loss: 4.1411\n",
      "Epoch [1/4], Step [48900/61080], Loss: 3.8752\n",
      "Epoch [1/4], Step [48975/61080], Loss: 4.3317\n",
      "Epoch [1/4], Step [49050/61080], Loss: 4.1508\n",
      "Epoch [1/4], Step [49125/61080], Loss: 4.3421\n",
      "Epoch [1/4], Step [49200/61080], Loss: 4.0663\n",
      "Epoch [1/4], Step [49275/61080], Loss: 3.9019\n",
      "Epoch [1/4], Step [49350/61080], Loss: 4.2265\n",
      "Epoch [1/4], Step [49425/61080], Loss: 4.1951\n",
      "Epoch [1/4], Step [49500/61080], Loss: 4.2288\n",
      "Epoch [1/4], Step [49575/61080], Loss: 4.2509\n",
      "Epoch [1/4], Step [49650/61080], Loss: 3.8552\n",
      "Epoch [1/4], Step [49725/61080], Loss: 4.2756\n",
      "Epoch [1/4], Step [49800/61080], Loss: 4.1854\n",
      "Epoch [1/4], Step [49875/61080], Loss: 4.2541\n",
      "Epoch [1/4], Step [49950/61080], Loss: 4.1497\n",
      "Epoch [1/4], Step [50025/61080], Loss: 4.1815\n",
      "Epoch [1/4], Step [50100/61080], Loss: 4.1000\n",
      "Epoch [1/4], Step [50175/61080], Loss: 4.2447\n",
      "Epoch [1/4], Step [50250/61080], Loss: 4.1014\n",
      "Epoch [1/4], Step [50325/61080], Loss: 3.9298\n",
      "Epoch [1/4], Step [50400/61080], Loss: 4.0585\n",
      "Epoch [1/4], Step [50475/61080], Loss: 4.3855\n",
      "Epoch [1/4], Step [50550/61080], Loss: 4.1755\n",
      "Epoch [1/4], Step [50625/61080], Loss: 4.1744\n",
      "Epoch [1/4], Step [50700/61080], Loss: 4.0559\n",
      "Epoch [1/4], Step [50775/61080], Loss: 4.3490\n",
      "Epoch [1/4], Step [50850/61080], Loss: 4.0884\n",
      "Epoch [1/4], Step [50925/61080], Loss: 4.1619\n",
      "Epoch [1/4], Step [51000/61080], Loss: 4.1556\n",
      "Validation perplexity: 50.32059824447746\n",
      "Epoch [1/4], Step [51075/61080], Loss: 4.3486\n",
      "Epoch [1/4], Step [51150/61080], Loss: 3.9431\n",
      "Epoch [1/4], Step [51225/61080], Loss: 4.3537\n",
      "Epoch [1/4], Step [51300/61080], Loss: 4.1667\n",
      "Epoch [1/4], Step [51375/61080], Loss: 4.1682\n",
      "Epoch [1/4], Step [51450/61080], Loss: 4.3084\n",
      "Epoch [1/4], Step [51525/61080], Loss: 4.0877\n",
      "Epoch [1/4], Step [51600/61080], Loss: 4.0551\n",
      "Epoch [1/4], Step [51675/61080], Loss: 4.1216\n",
      "Epoch [1/4], Step [51750/61080], Loss: 4.1066\n",
      "Epoch [1/4], Step [51825/61080], Loss: 4.0011\n",
      "Epoch [1/4], Step [51900/61080], Loss: 4.1122\n",
      "Epoch [1/4], Step [51975/61080], Loss: 4.0588\n",
      "Epoch [1/4], Step [52050/61080], Loss: 4.2649\n",
      "Epoch [1/4], Step [52125/61080], Loss: 4.2201\n",
      "Epoch [1/4], Step [52200/61080], Loss: 4.2611\n",
      "Epoch [1/4], Step [52275/61080], Loss: 4.2185\n",
      "Epoch [1/4], Step [52350/61080], Loss: 4.3956\n",
      "Epoch [1/4], Step [52425/61080], Loss: 4.0519\n",
      "Epoch [1/4], Step [52500/61080], Loss: 4.0140\n",
      "Epoch [1/4], Step [52575/61080], Loss: 3.9837\n",
      "Epoch [1/4], Step [52650/61080], Loss: 4.3898\n",
      "Epoch [1/4], Step [52725/61080], Loss: 4.1276\n",
      "Epoch [1/4], Step [52800/61080], Loss: 4.3451\n",
      "Epoch [1/4], Step [52875/61080], Loss: 4.0566\n",
      "Epoch [1/4], Step [52950/61080], Loss: 3.8804\n",
      "Epoch [1/4], Step [53025/61080], Loss: 4.2862\n",
      "Epoch [1/4], Step [53100/61080], Loss: 4.0738\n",
      "Epoch [1/4], Step [53175/61080], Loss: 4.2958\n",
      "Epoch [1/4], Step [53250/61080], Loss: 4.2872\n",
      "Epoch [1/4], Step [53325/61080], Loss: 4.1818\n",
      "Epoch [1/4], Step [53400/61080], Loss: 3.9563\n",
      "Epoch [1/4], Step [53475/61080], Loss: 3.9758\n",
      "Epoch [1/4], Step [53550/61080], Loss: 4.1363\n",
      "Epoch [1/4], Step [53625/61080], Loss: 4.1952\n",
      "Epoch [1/4], Step [53700/61080], Loss: 4.1575\n",
      "Epoch [1/4], Step [53775/61080], Loss: 4.1547\n",
      "Epoch [1/4], Step [53850/61080], Loss: 4.4518\n",
      "Epoch [1/4], Step [53925/61080], Loss: 4.1257\n",
      "Epoch [1/4], Step [54000/61080], Loss: 4.2971\n",
      "Validation perplexity: 49.332692665631434\n",
      "Epoch [1/4], Step [54075/61080], Loss: 4.3680\n",
      "Epoch [1/4], Step [54150/61080], Loss: 4.2483\n",
      "Epoch [1/4], Step [54225/61080], Loss: 4.3443\n",
      "Epoch [1/4], Step [54300/61080], Loss: 3.9576\n",
      "Epoch [1/4], Step [54375/61080], Loss: 4.1393\n",
      "Epoch [1/4], Step [54450/61080], Loss: 4.1903\n",
      "Epoch [1/4], Step [54525/61080], Loss: 3.9293\n",
      "Epoch [1/4], Step [54600/61080], Loss: 4.2514\n",
      "Epoch [1/4], Step [54675/61080], Loss: 4.1833\n",
      "Epoch [1/4], Step [54750/61080], Loss: 4.2233\n",
      "Epoch [1/4], Step [54825/61080], Loss: 4.1353\n",
      "Epoch [1/4], Step [54900/61080], Loss: 4.0689\n",
      "Epoch [1/4], Step [54975/61080], Loss: 4.3129\n",
      "Epoch [1/4], Step [55050/61080], Loss: 4.1762\n",
      "Epoch [1/4], Step [55125/61080], Loss: 4.5494\n",
      "Epoch [1/4], Step [55200/61080], Loss: 4.1094\n",
      "Epoch [1/4], Step [55275/61080], Loss: 3.9215\n",
      "Epoch [1/4], Step [55350/61080], Loss: 3.7761\n",
      "Epoch [1/4], Step [55425/61080], Loss: 3.8614\n",
      "Epoch [1/4], Step [55500/61080], Loss: 4.2366\n",
      "Epoch [1/4], Step [55575/61080], Loss: 4.5383\n",
      "Epoch [1/4], Step [55650/61080], Loss: 4.1915\n",
      "Epoch [1/4], Step [55725/61080], Loss: 4.1924\n",
      "Epoch [1/4], Step [55800/61080], Loss: 4.1260\n",
      "Epoch [1/4], Step [55875/61080], Loss: 4.0253\n",
      "Epoch [1/4], Step [55950/61080], Loss: 4.1225\n",
      "Epoch [1/4], Step [56025/61080], Loss: 4.3204\n",
      "Epoch [1/4], Step [56100/61080], Loss: 4.1136\n",
      "Epoch [1/4], Step [56175/61080], Loss: 4.2758\n",
      "Epoch [1/4], Step [56250/61080], Loss: 4.2789\n",
      "Epoch [1/4], Step [56325/61080], Loss: 4.3839\n",
      "Epoch [1/4], Step [56400/61080], Loss: 4.3655\n",
      "Epoch [1/4], Step [56475/61080], Loss: 3.9874\n",
      "Epoch [1/4], Step [56550/61080], Loss: 4.2494\n",
      "Epoch [1/4], Step [56625/61080], Loss: 4.1135\n",
      "Epoch [1/4], Step [56700/61080], Loss: 4.2551\n",
      "Epoch [1/4], Step [56775/61080], Loss: 4.2778\n",
      "Epoch [1/4], Step [56850/61080], Loss: 4.1797\n",
      "Epoch [1/4], Step [56925/61080], Loss: 4.6029\n",
      "Epoch [1/4], Step [57000/61080], Loss: 4.2963\n",
      "Validation perplexity: 48.93842756917287\n",
      "Epoch [1/4], Step [57075/61080], Loss: 4.1732\n",
      "Epoch [1/4], Step [57150/61080], Loss: 4.0392\n",
      "Epoch [1/4], Step [57225/61080], Loss: 4.1927\n",
      "Epoch [1/4], Step [57300/61080], Loss: 4.3078\n",
      "Epoch [1/4], Step [57375/61080], Loss: 4.2735\n",
      "Epoch [1/4], Step [57450/61080], Loss: 4.2291\n",
      "Epoch [1/4], Step [57525/61080], Loss: 4.2595\n",
      "Epoch [1/4], Step [57600/61080], Loss: 4.0884\n",
      "Epoch [1/4], Step [57675/61080], Loss: 4.2452\n",
      "Epoch [1/4], Step [57750/61080], Loss: 4.1539\n",
      "Epoch [1/4], Step [57825/61080], Loss: 4.2861\n",
      "Epoch [1/4], Step [57900/61080], Loss: 4.0458\n",
      "Epoch [1/4], Step [57975/61080], Loss: 4.1830\n",
      "Epoch [1/4], Step [58050/61080], Loss: 4.0948\n",
      "Epoch [1/4], Step [58125/61080], Loss: 4.0978\n",
      "Epoch [1/4], Step [58200/61080], Loss: 4.1677\n",
      "Epoch [1/4], Step [58275/61080], Loss: 3.9131\n",
      "Epoch [1/4], Step [58350/61080], Loss: 4.2321\n",
      "Epoch [1/4], Step [58425/61080], Loss: 4.1886\n",
      "Epoch [1/4], Step [58500/61080], Loss: 4.0848\n",
      "Epoch [1/4], Step [58575/61080], Loss: 4.0608\n",
      "Epoch [1/4], Step [58650/61080], Loss: 3.9272\n",
      "Epoch [1/4], Step [58725/61080], Loss: 4.4076\n",
      "Epoch [1/4], Step [58800/61080], Loss: 4.1230\n",
      "Epoch [1/4], Step [58875/61080], Loss: 3.9979\n",
      "Epoch [1/4], Step [58950/61080], Loss: 4.2477\n",
      "Epoch [1/4], Step [59025/61080], Loss: 4.2944\n",
      "Epoch [1/4], Step [59100/61080], Loss: 3.7662\n",
      "Epoch [1/4], Step [59175/61080], Loss: 4.2362\n",
      "Epoch [1/4], Step [59250/61080], Loss: 4.0555\n",
      "Epoch [1/4], Step [59325/61080], Loss: 4.2876\n",
      "Epoch [1/4], Step [59400/61080], Loss: 4.0255\n",
      "Epoch [1/4], Step [59475/61080], Loss: 3.8843\n",
      "Epoch [1/4], Step [59550/61080], Loss: 4.2405\n",
      "Epoch [1/4], Step [59625/61080], Loss: 4.2231\n",
      "Epoch [1/4], Step [59700/61080], Loss: 4.1520\n",
      "Epoch [1/4], Step [59775/61080], Loss: 4.1627\n",
      "Epoch [1/4], Step [59850/61080], Loss: 4.0573\n",
      "Epoch [1/4], Step [59925/61080], Loss: 4.0096\n",
      "Epoch [1/4], Step [60000/61080], Loss: 3.9161\n",
      "Validation perplexity: 48.66187643535616\n",
      "Epoch [1/4], Step [60075/61080], Loss: 3.9727\n",
      "Epoch [1/4], Step [60150/61080], Loss: 4.1191\n",
      "Epoch [1/4], Step [60225/61080], Loss: 4.1738\n",
      "Epoch [1/4], Step [60300/61080], Loss: 3.9206\n",
      "Epoch [1/4], Step [60375/61080], Loss: 4.0521\n",
      "Epoch [1/4], Step [60450/61080], Loss: 4.1025\n",
      "Epoch [1/4], Step [60525/61080], Loss: 4.2271\n",
      "Epoch [1/4], Step [60600/61080], Loss: 4.1830\n",
      "Epoch [1/4], Step [60675/61080], Loss: 4.0853\n",
      "Epoch [1/4], Step [60750/61080], Loss: 3.7926\n",
      "Epoch [1/4], Step [60825/61080], Loss: 4.1190\n",
      "Epoch [1/4], Step [60900/61080], Loss: 4.2901\n",
      "Epoch [1/4], Step [60975/61080], Loss: 4.2206\n",
      "Epoch [1/4], Step [61050/61080], Loss: 4.1957\n",
      "Epoch [1/4] Average Loss: 4.4003, Perplexity: 81.48\n",
      "Epoch [2/4], Step [0/61080], Loss: 4.1965\n",
      "Validation perplexity: 48.38749422591237\n",
      "Epoch [2/4], Step [75/61080], Loss: 4.0165\n",
      "Epoch [2/4], Step [150/61080], Loss: 4.0485\n",
      "Epoch [2/4], Step [225/61080], Loss: 4.1036\n",
      "Epoch [2/4], Step [300/61080], Loss: 4.1302\n",
      "Epoch [2/4], Step [375/61080], Loss: 3.9595\n",
      "Epoch [2/4], Step [450/61080], Loss: 4.0175\n",
      "Epoch [2/4], Step [525/61080], Loss: 4.0926\n",
      "Epoch [2/4], Step [600/61080], Loss: 3.8743\n",
      "Epoch [2/4], Step [675/61080], Loss: 4.0658\n",
      "Epoch [2/4], Step [750/61080], Loss: 4.0717\n",
      "Epoch [2/4], Step [825/61080], Loss: 4.1970\n",
      "Epoch [2/4], Step [900/61080], Loss: 4.1270\n",
      "Epoch [2/4], Step [975/61080], Loss: 3.9947\n",
      "Epoch [2/4], Step [1050/61080], Loss: 3.9677\n",
      "Epoch [2/4], Step [1125/61080], Loss: 4.1360\n",
      "Epoch [2/4], Step [1200/61080], Loss: 4.0530\n",
      "Epoch [2/4], Step [1275/61080], Loss: 4.1325\n",
      "Epoch [2/4], Step [1350/61080], Loss: 4.4204\n",
      "Epoch [2/4], Step [1425/61080], Loss: 4.0374\n",
      "Epoch [2/4], Step [1500/61080], Loss: 4.1250\n",
      "Epoch [2/4], Step [1575/61080], Loss: 4.1403\n",
      "Epoch [2/4], Step [1650/61080], Loss: 4.0884\n",
      "Epoch [2/4], Step [1725/61080], Loss: 4.0122\n",
      "Epoch [2/4], Step [1800/61080], Loss: 3.9624\n",
      "Epoch [2/4], Step [1875/61080], Loss: 4.1936\n",
      "Epoch [2/4], Step [1950/61080], Loss: 4.0484\n",
      "Epoch [2/4], Step [2025/61080], Loss: 4.0814\n",
      "Epoch [2/4], Step [2100/61080], Loss: 4.2923\n",
      "Epoch [2/4], Step [2175/61080], Loss: 4.2397\n",
      "Epoch [2/4], Step [2250/61080], Loss: 4.0561\n",
      "Epoch [2/4], Step [2325/61080], Loss: 3.9068\n",
      "Epoch [2/4], Step [2400/61080], Loss: 4.1362\n",
      "Epoch [2/4], Step [2475/61080], Loss: 4.1252\n",
      "Epoch [2/4], Step [2550/61080], Loss: 4.2263\n",
      "Epoch [2/4], Step [2625/61080], Loss: 4.2345\n",
      "Epoch [2/4], Step [2700/61080], Loss: 3.9726\n",
      "Epoch [2/4], Step [2775/61080], Loss: 4.1177\n",
      "Epoch [2/4], Step [2850/61080], Loss: 4.0698\n",
      "Epoch [2/4], Step [2925/61080], Loss: 3.9398\n",
      "Epoch [2/4], Step [3000/61080], Loss: 4.0732\n",
      "Validation perplexity: 47.627506908446435\n",
      "Epoch [2/4], Step [3075/61080], Loss: 3.9686\n",
      "Epoch [2/4], Step [3150/61080], Loss: 3.9396\n",
      "Epoch [2/4], Step [3225/61080], Loss: 3.9491\n",
      "Epoch [2/4], Step [3300/61080], Loss: 4.0143\n",
      "Epoch [2/4], Step [3375/61080], Loss: 3.9780\n",
      "Epoch [2/4], Step [3450/61080], Loss: 3.8319\n",
      "Epoch [2/4], Step [3525/61080], Loss: 4.1579\n",
      "Epoch [2/4], Step [3600/61080], Loss: 4.5024\n",
      "Epoch [2/4], Step [3675/61080], Loss: 4.1283\n",
      "Epoch [2/4], Step [3750/61080], Loss: 4.2539\n",
      "Epoch [2/4], Step [3825/61080], Loss: 4.1244\n",
      "Epoch [2/4], Step [3900/61080], Loss: 4.2936\n",
      "Epoch [2/4], Step [3975/61080], Loss: 4.1048\n",
      "Epoch [2/4], Step [4050/61080], Loss: 3.9461\n",
      "Epoch [2/4], Step [4125/61080], Loss: 4.0915\n",
      "Epoch [2/4], Step [4200/61080], Loss: 4.0053\n",
      "Epoch [2/4], Step [4275/61080], Loss: 4.1729\n",
      "Epoch [2/4], Step [4350/61080], Loss: 4.1218\n",
      "Epoch [2/4], Step [4425/61080], Loss: 4.0064\n",
      "Epoch [2/4], Step [4500/61080], Loss: 4.1892\n",
      "Epoch [2/4], Step [4575/61080], Loss: 4.1964\n",
      "Epoch [2/4], Step [4650/61080], Loss: 3.9918\n",
      "Epoch [2/4], Step [4725/61080], Loss: 4.1275\n",
      "Epoch [2/4], Step [4800/61080], Loss: 4.1016\n",
      "Epoch [2/4], Step [4875/61080], Loss: 4.2371\n",
      "Epoch [2/4], Step [4950/61080], Loss: 4.2360\n",
      "Epoch [2/4], Step [5025/61080], Loss: 4.1890\n",
      "Epoch [2/4], Step [5100/61080], Loss: 4.2839\n",
      "Epoch [2/4], Step [5175/61080], Loss: 4.5539\n",
      "Epoch [2/4], Step [5250/61080], Loss: 4.1420\n",
      "Epoch [2/4], Step [5325/61080], Loss: 3.9664\n",
      "Epoch [2/4], Step [5400/61080], Loss: 4.0109\n",
      "Epoch [2/4], Step [5475/61080], Loss: 4.1773\n",
      "Epoch [2/4], Step [5550/61080], Loss: 4.0513\n",
      "Epoch [2/4], Step [5625/61080], Loss: 3.9444\n",
      "Epoch [2/4], Step [5700/61080], Loss: 4.0730\n",
      "Epoch [2/4], Step [5775/61080], Loss: 4.1591\n",
      "Epoch [2/4], Step [5850/61080], Loss: 4.0642\n",
      "Epoch [2/4], Step [5925/61080], Loss: 4.2539\n",
      "Epoch [2/4], Step [6000/61080], Loss: 4.0916\n",
      "Validation perplexity: 47.502173329581055\n",
      "Epoch [2/4], Step [6075/61080], Loss: 4.0840\n",
      "Epoch [2/4], Step [6150/61080], Loss: 4.1817\n",
      "Epoch [2/4], Step [6225/61080], Loss: 4.0834\n",
      "Epoch [2/4], Step [6300/61080], Loss: 4.3399\n",
      "Epoch [2/4], Step [6375/61080], Loss: 4.0108\n",
      "Epoch [2/4], Step [6450/61080], Loss: 4.1761\n",
      "Epoch [2/4], Step [6525/61080], Loss: 3.9021\n",
      "Epoch [2/4], Step [6600/61080], Loss: 4.1190\n",
      "Epoch [2/4], Step [6675/61080], Loss: 4.3357\n",
      "Epoch [2/4], Step [6750/61080], Loss: 3.8846\n",
      "Epoch [2/4], Step [6825/61080], Loss: 4.0933\n",
      "Epoch [2/4], Step [6900/61080], Loss: 3.8927\n",
      "Epoch [2/4], Step [6975/61080], Loss: 4.2176\n",
      "Epoch [2/4], Step [7050/61080], Loss: 4.2204\n",
      "Epoch [2/4], Step [7125/61080], Loss: 4.0913\n",
      "Epoch [2/4], Step [7200/61080], Loss: 4.0154\n",
      "Epoch [2/4], Step [7275/61080], Loss: 4.2429\n",
      "Epoch [2/4], Step [7350/61080], Loss: 3.9097\n",
      "Epoch [2/4], Step [7425/61080], Loss: 4.1873\n",
      "Epoch [2/4], Step [7500/61080], Loss: 3.8746\n",
      "Epoch [2/4], Step [7575/61080], Loss: 4.3414\n",
      "Epoch [2/4], Step [7650/61080], Loss: 4.1073\n",
      "Epoch [2/4], Step [7725/61080], Loss: 4.2238\n",
      "Epoch [2/4], Step [7800/61080], Loss: 4.0895\n",
      "Epoch [2/4], Step [7875/61080], Loss: 4.0783\n",
      "Epoch [2/4], Step [7950/61080], Loss: 4.0888\n",
      "Epoch [2/4], Step [8025/61080], Loss: 3.8634\n",
      "Epoch [2/4], Step [8100/61080], Loss: 4.1176\n",
      "Epoch [2/4], Step [8175/61080], Loss: 4.3945\n",
      "Epoch [2/4], Step [8250/61080], Loss: 3.9713\n",
      "Epoch [2/4], Step [8325/61080], Loss: 4.0927\n",
      "Epoch [2/4], Step [8400/61080], Loss: 3.9768\n",
      "Epoch [2/4], Step [8475/61080], Loss: 4.2375\n",
      "Epoch [2/4], Step [8550/61080], Loss: 4.1053\n",
      "Epoch [2/4], Step [8625/61080], Loss: 4.3012\n",
      "Epoch [2/4], Step [8700/61080], Loss: 4.1460\n",
      "Epoch [2/4], Step [8775/61080], Loss: 4.1777\n",
      "Epoch [2/4], Step [8850/61080], Loss: 4.1730\n",
      "Epoch [2/4], Step [8925/61080], Loss: 4.1762\n",
      "Epoch [2/4], Step [9000/61080], Loss: 4.3039\n",
      "Validation perplexity: 47.25207172538758\n",
      "Epoch [2/4], Step [9075/61080], Loss: 4.2388\n",
      "Epoch [2/4], Step [9150/61080], Loss: 3.9375\n",
      "Epoch [2/4], Step [9225/61080], Loss: 4.1913\n",
      "Epoch [2/4], Step [9300/61080], Loss: 4.1890\n",
      "Epoch [2/4], Step [9375/61080], Loss: 4.0167\n",
      "Epoch [2/4], Step [9450/61080], Loss: 4.0010\n",
      "Epoch [2/4], Step [9525/61080], Loss: 3.9251\n",
      "Epoch [2/4], Step [9600/61080], Loss: 4.1970\n",
      "Epoch [2/4], Step [9675/61080], Loss: 3.8207\n",
      "Epoch [2/4], Step [9750/61080], Loss: 3.8836\n",
      "Epoch [2/4], Step [9825/61080], Loss: 4.0668\n",
      "Epoch [2/4], Step [9900/61080], Loss: 3.9581\n",
      "Epoch [2/4], Step [9975/61080], Loss: 4.3330\n",
      "Epoch [2/4], Step [10050/61080], Loss: 4.1931\n",
      "Epoch [2/4], Step [10125/61080], Loss: 4.0887\n",
      "Epoch [2/4], Step [10200/61080], Loss: 3.9111\n",
      "Epoch [2/4], Step [10275/61080], Loss: 4.0178\n",
      "Epoch [2/4], Step [10350/61080], Loss: 3.9572\n",
      "Epoch [2/4], Step [10425/61080], Loss: 4.2661\n",
      "Epoch [2/4], Step [10500/61080], Loss: 4.0452\n",
      "Epoch [2/4], Step [10575/61080], Loss: 4.0735\n",
      "Epoch [2/4], Step [10650/61080], Loss: 4.0269\n",
      "Epoch [2/4], Step [10725/61080], Loss: 4.2606\n",
      "Epoch [2/4], Step [10800/61080], Loss: 4.1869\n",
      "Epoch [2/4], Step [10875/61080], Loss: 3.9611\n",
      "Epoch [2/4], Step [10950/61080], Loss: 4.2366\n",
      "Epoch [2/4], Step [11025/61080], Loss: 3.8637\n",
      "Epoch [2/4], Step [11100/61080], Loss: 3.9426\n",
      "Epoch [2/4], Step [11175/61080], Loss: 3.7908\n",
      "Epoch [2/4], Step [11250/61080], Loss: 3.9388\n",
      "Epoch [2/4], Step [11325/61080], Loss: 4.3012\n",
      "Epoch [2/4], Step [11400/61080], Loss: 4.3222\n",
      "Epoch [2/4], Step [11475/61080], Loss: 4.2851\n",
      "Epoch [2/4], Step [11550/61080], Loss: 4.2801\n",
      "Epoch [2/4], Step [11625/61080], Loss: 3.9438\n",
      "Epoch [2/4], Step [11700/61080], Loss: 4.1605\n",
      "Epoch [2/4], Step [11775/61080], Loss: 4.1822\n",
      "Epoch [2/4], Step [11850/61080], Loss: 4.1277\n",
      "Epoch [2/4], Step [11925/61080], Loss: 4.2445\n",
      "Epoch [2/4], Step [12000/61080], Loss: 3.9126\n",
      "Validation perplexity: 46.308685603464454\n",
      "Epoch [2/4], Step [12075/61080], Loss: 4.0531\n",
      "Epoch [2/4], Step [12150/61080], Loss: 4.0635\n",
      "Epoch [2/4], Step [12225/61080], Loss: 3.8491\n",
      "Epoch [2/4], Step [12300/61080], Loss: 4.2014\n",
      "Epoch [2/4], Step [12375/61080], Loss: 3.8870\n",
      "Epoch [2/4], Step [12450/61080], Loss: 4.2633\n",
      "Epoch [2/4], Step [12525/61080], Loss: 3.8551\n",
      "Epoch [2/4], Step [12600/61080], Loss: 4.1089\n",
      "Epoch [2/4], Step [12675/61080], Loss: 4.2411\n",
      "Epoch [2/4], Step [12750/61080], Loss: 4.0883\n",
      "Epoch [2/4], Step [12825/61080], Loss: 4.0348\n",
      "Epoch [2/4], Step [12900/61080], Loss: 4.1498\n",
      "Epoch [2/4], Step [12975/61080], Loss: 4.0585\n",
      "Epoch [2/4], Step [13050/61080], Loss: 3.9210\n",
      "Epoch [2/4], Step [13125/61080], Loss: 3.8663\n",
      "Epoch [2/4], Step [13200/61080], Loss: 4.3023\n",
      "Epoch [2/4], Step [13275/61080], Loss: 4.0692\n",
      "Epoch [2/4], Step [13350/61080], Loss: 4.2148\n",
      "Epoch [2/4], Step [13425/61080], Loss: 4.0134\n",
      "Epoch [2/4], Step [13500/61080], Loss: 4.1826\n",
      "Epoch [2/4], Step [13575/61080], Loss: 4.3257\n",
      "Epoch [2/4], Step [13650/61080], Loss: 3.8072\n",
      "Epoch [2/4], Step [13725/61080], Loss: 3.9647\n",
      "Epoch [2/4], Step [13800/61080], Loss: 4.0832\n",
      "Epoch [2/4], Step [13875/61080], Loss: 3.9214\n",
      "Epoch [2/4], Step [13950/61080], Loss: 3.8208\n",
      "Epoch [2/4], Step [14025/61080], Loss: 4.0200\n",
      "Epoch [2/4], Step [14100/61080], Loss: 4.0380\n",
      "Epoch [2/4], Step [14175/61080], Loss: 4.0830\n",
      "Epoch [2/4], Step [14250/61080], Loss: 4.2760\n",
      "Epoch [2/4], Step [14325/61080], Loss: 3.9033\n",
      "Epoch [2/4], Step [14400/61080], Loss: 4.1818\n",
      "Epoch [2/4], Step [14475/61080], Loss: 4.0582\n",
      "Epoch [2/4], Step [14550/61080], Loss: 4.0609\n",
      "Epoch [2/4], Step [14625/61080], Loss: 4.2639\n",
      "Epoch [2/4], Step [14700/61080], Loss: 3.8328\n",
      "Epoch [2/4], Step [14775/61080], Loss: 4.0396\n",
      "Epoch [2/4], Step [14850/61080], Loss: 4.1885\n",
      "Epoch [2/4], Step [14925/61080], Loss: 3.8890\n",
      "Epoch [2/4], Step [15000/61080], Loss: 4.0349\n",
      "Validation perplexity: 46.320237366333224\n",
      "Epoch [2/4], Step [15075/61080], Loss: 3.9117\n",
      "Epoch [2/4], Step [15150/61080], Loss: 4.1771\n",
      "Epoch [2/4], Step [15225/61080], Loss: 4.0548\n",
      "Epoch [2/4], Step [15300/61080], Loss: 4.2248\n",
      "Epoch [2/4], Step [15375/61080], Loss: 4.1226\n",
      "Epoch [2/4], Step [15450/61080], Loss: 4.2553\n",
      "Epoch [2/4], Step [15525/61080], Loss: 3.9988\n",
      "Epoch [2/4], Step [15600/61080], Loss: 4.1057\n",
      "Epoch [2/4], Step [15675/61080], Loss: 4.0259\n",
      "Epoch [2/4], Step [15750/61080], Loss: 3.8936\n",
      "Epoch [2/4], Step [15825/61080], Loss: 4.1158\n",
      "Epoch [2/4], Step [15900/61080], Loss: 4.1003\n",
      "Epoch [2/4], Step [15975/61080], Loss: 4.1020\n",
      "Epoch [2/4], Step [16050/61080], Loss: 4.0225\n",
      "Epoch [2/4], Step [16125/61080], Loss: 4.0481\n",
      "Epoch [2/4], Step [16200/61080], Loss: 3.8551\n",
      "Epoch [2/4], Step [16275/61080], Loss: 3.9271\n",
      "Epoch [2/4], Step [16350/61080], Loss: 4.0548\n",
      "Epoch [2/4], Step [16425/61080], Loss: 4.1008\n",
      "Epoch [2/4], Step [16500/61080], Loss: 4.2099\n",
      "Epoch [2/4], Step [16575/61080], Loss: 3.9593\n",
      "Epoch [2/4], Step [16650/61080], Loss: 3.7683\n",
      "Epoch [2/4], Step [16725/61080], Loss: 4.1016\n",
      "Epoch [2/4], Step [16800/61080], Loss: 4.1434\n",
      "Epoch [2/4], Step [16875/61080], Loss: 3.8595\n",
      "Epoch [2/4], Step [16950/61080], Loss: 4.2067\n",
      "Epoch [2/4], Step [17025/61080], Loss: 4.2375\n",
      "Epoch [2/4], Step [17100/61080], Loss: 4.1058\n",
      "Epoch [2/4], Step [17175/61080], Loss: 4.0311\n",
      "Epoch [2/4], Step [17250/61080], Loss: 3.9869\n",
      "Epoch [2/4], Step [17325/61080], Loss: 3.8735\n",
      "Epoch [2/4], Step [17400/61080], Loss: 4.1773\n",
      "Epoch [2/4], Step [17475/61080], Loss: 4.0463\n",
      "Epoch [2/4], Step [17550/61080], Loss: 4.0530\n",
      "Epoch [2/4], Step [17625/61080], Loss: 4.1726\n",
      "Epoch [2/4], Step [17700/61080], Loss: 3.8577\n",
      "Epoch [2/4], Step [17775/61080], Loss: 4.0843\n",
      "Epoch [2/4], Step [17850/61080], Loss: 3.7800\n",
      "Epoch [2/4], Step [17925/61080], Loss: 3.8939\n",
      "Epoch [2/4], Step [18000/61080], Loss: 4.0408\n",
      "Validation perplexity: 45.80378869420229\n",
      "Epoch [2/4], Step [18075/61080], Loss: 3.9901\n",
      "Epoch [2/4], Step [18150/61080], Loss: 3.9247\n",
      "Epoch [2/4], Step [18225/61080], Loss: 4.0278\n",
      "Epoch [2/4], Step [18300/61080], Loss: 4.0654\n",
      "Epoch [2/4], Step [18375/61080], Loss: 3.9509\n",
      "Epoch [2/4], Step [18450/61080], Loss: 4.1491\n",
      "Epoch [2/4], Step [18525/61080], Loss: 4.0088\n",
      "Epoch [2/4], Step [18600/61080], Loss: 4.1999\n",
      "Epoch [2/4], Step [18675/61080], Loss: 3.8031\n",
      "Epoch [2/4], Step [18750/61080], Loss: 4.1721\n",
      "Epoch [2/4], Step [18825/61080], Loss: 4.3473\n",
      "Epoch [2/4], Step [18900/61080], Loss: 3.8023\n",
      "Epoch [2/4], Step [18975/61080], Loss: 4.1806\n",
      "Epoch [2/4], Step [19050/61080], Loss: 3.8998\n",
      "Epoch [2/4], Step [19125/61080], Loss: 4.0917\n",
      "Epoch [2/4], Step [19200/61080], Loss: 4.0195\n",
      "Epoch [2/4], Step [19275/61080], Loss: 4.0600\n",
      "Epoch [2/4], Step [19350/61080], Loss: 3.7701\n",
      "Epoch [2/4], Step [19425/61080], Loss: 3.9574\n",
      "Epoch [2/4], Step [19500/61080], Loss: 4.0319\n",
      "Epoch [2/4], Step [19575/61080], Loss: 4.2654\n",
      "Epoch [2/4], Step [19650/61080], Loss: 4.4920\n",
      "Epoch [2/4], Step [19725/61080], Loss: 3.9084\n",
      "Epoch [2/4], Step [19800/61080], Loss: 3.9707\n",
      "Epoch [2/4], Step [19875/61080], Loss: 4.0944\n",
      "Epoch [2/4], Step [19950/61080], Loss: 4.0801\n",
      "Epoch [2/4], Step [20025/61080], Loss: 4.2198\n",
      "Epoch [2/4], Step [20100/61080], Loss: 3.8564\n",
      "Epoch [2/4], Step [20175/61080], Loss: 3.9846\n",
      "Epoch [2/4], Step [20250/61080], Loss: 4.1081\n",
      "Epoch [2/4], Step [20325/61080], Loss: 4.2330\n",
      "Epoch [2/4], Step [20400/61080], Loss: 4.0975\n",
      "Epoch [2/4], Step [20475/61080], Loss: 4.1321\n",
      "Epoch [2/4], Step [20550/61080], Loss: 4.0274\n",
      "Epoch [2/4], Step [20625/61080], Loss: 3.9824\n",
      "Epoch [2/4], Step [20700/61080], Loss: 4.1209\n",
      "Epoch [2/4], Step [20775/61080], Loss: 3.9881\n",
      "Epoch [2/4], Step [20850/61080], Loss: 3.8075\n",
      "Epoch [2/4], Step [20925/61080], Loss: 4.2290\n",
      "Epoch [2/4], Step [21000/61080], Loss: 4.1237\n",
      "Validation perplexity: 45.41340508833448\n",
      "Epoch [2/4], Step [21075/61080], Loss: 4.2665\n",
      "Epoch [2/4], Step [21150/61080], Loss: 4.0716\n",
      "Epoch [2/4], Step [21225/61080], Loss: 4.0727\n",
      "Epoch [2/4], Step [21300/61080], Loss: 4.1607\n",
      "Epoch [2/4], Step [21375/61080], Loss: 4.1174\n",
      "Epoch [2/4], Step [21450/61080], Loss: 3.9745\n",
      "Epoch [2/4], Step [21525/61080], Loss: 4.0673\n",
      "Epoch [2/4], Step [21600/61080], Loss: 4.3396\n",
      "Epoch [2/4], Step [21675/61080], Loss: 3.9634\n",
      "Epoch [2/4], Step [21750/61080], Loss: 3.7974\n",
      "Epoch [2/4], Step [21825/61080], Loss: 3.9986\n",
      "Epoch [2/4], Step [21900/61080], Loss: 3.9844\n",
      "Epoch [2/4], Step [21975/61080], Loss: 3.9077\n",
      "Epoch [2/4], Step [22050/61080], Loss: 4.1629\n",
      "Epoch [2/4], Step [22125/61080], Loss: 4.0302\n",
      "Epoch [2/4], Step [22200/61080], Loss: 3.7510\n",
      "Epoch [2/4], Step [22275/61080], Loss: 4.0271\n",
      "Epoch [2/4], Step [22350/61080], Loss: 3.9943\n",
      "Epoch [2/4], Step [22425/61080], Loss: 4.1782\n",
      "Epoch [2/4], Step [22500/61080], Loss: 4.0988\n",
      "Epoch [2/4], Step [22575/61080], Loss: 3.8593\n",
      "Epoch [2/4], Step [22650/61080], Loss: 3.7600\n",
      "Epoch [2/4], Step [22725/61080], Loss: 4.0910\n",
      "Epoch [2/4], Step [22800/61080], Loss: 3.6847\n",
      "Epoch [2/4], Step [22875/61080], Loss: 4.0569\n",
      "Epoch [2/4], Step [22950/61080], Loss: 3.9214\n",
      "Epoch [2/4], Step [23025/61080], Loss: 4.2417\n",
      "Epoch [2/4], Step [23100/61080], Loss: 3.8720\n",
      "Epoch [2/4], Step [23175/61080], Loss: 3.9932\n",
      "Epoch [2/4], Step [23250/61080], Loss: 3.7678\n",
      "Epoch [2/4], Step [23325/61080], Loss: 4.1035\n",
      "Epoch [2/4], Step [23400/61080], Loss: 4.2002\n",
      "Epoch [2/4], Step [23475/61080], Loss: 4.0600\n",
      "Epoch [2/4], Step [23550/61080], Loss: 4.0010\n",
      "Epoch [2/4], Step [23625/61080], Loss: 3.9876\n",
      "Epoch [2/4], Step [23700/61080], Loss: 3.9259\n",
      "Epoch [2/4], Step [23775/61080], Loss: 4.2435\n",
      "Epoch [2/4], Step [23850/61080], Loss: 4.0681\n",
      "Epoch [2/4], Step [23925/61080], Loss: 4.3069\n",
      "Epoch [2/4], Step [24000/61080], Loss: 4.0763\n",
      "Validation perplexity: 45.062940764044704\n",
      "Epoch [2/4], Step [24075/61080], Loss: 4.3663\n",
      "Epoch [2/4], Step [24150/61080], Loss: 4.0734\n",
      "Epoch [2/4], Step [24225/61080], Loss: 3.7120\n",
      "Epoch [2/4], Step [24300/61080], Loss: 4.0512\n",
      "Epoch [2/4], Step [24375/61080], Loss: 3.9156\n",
      "Epoch [2/4], Step [24450/61080], Loss: 3.9690\n",
      "Epoch [2/4], Step [24525/61080], Loss: 4.0436\n",
      "Epoch [2/4], Step [24600/61080], Loss: 4.1611\n",
      "Epoch [2/4], Step [24675/61080], Loss: 3.9570\n",
      "Epoch [2/4], Step [24750/61080], Loss: 4.2293\n",
      "Epoch [2/4], Step [24825/61080], Loss: 3.8945\n",
      "Epoch [2/4], Step [24900/61080], Loss: 3.8017\n",
      "Epoch [2/4], Step [24975/61080], Loss: 3.9674\n",
      "Epoch [2/4], Step [25050/61080], Loss: 4.0767\n",
      "Epoch [2/4], Step [25125/61080], Loss: 3.9274\n",
      "Epoch [2/4], Step [25200/61080], Loss: 4.0705\n",
      "Epoch [2/4], Step [25275/61080], Loss: 4.1421\n",
      "Epoch [2/4], Step [25350/61080], Loss: 3.9467\n",
      "Epoch [2/4], Step [25425/61080], Loss: 4.0695\n",
      "Epoch [2/4], Step [25500/61080], Loss: 4.1271\n",
      "Epoch [2/4], Step [25575/61080], Loss: 3.7727\n",
      "Epoch [2/4], Step [25650/61080], Loss: 4.0147\n",
      "Epoch [2/4], Step [25725/61080], Loss: 3.7511\n",
      "Epoch [2/4], Step [25800/61080], Loss: 4.1336\n",
      "Epoch [2/4], Step [25875/61080], Loss: 4.0594\n",
      "Epoch [2/4], Step [25950/61080], Loss: 4.0675\n",
      "Epoch [2/4], Step [26025/61080], Loss: 4.2464\n",
      "Epoch [2/4], Step [26100/61080], Loss: 4.0582\n",
      "Epoch [2/4], Step [26175/61080], Loss: 3.7915\n",
      "Epoch [2/4], Step [26250/61080], Loss: 3.9434\n",
      "Epoch [2/4], Step [26325/61080], Loss: 4.2141\n",
      "Epoch [2/4], Step [26400/61080], Loss: 4.0607\n",
      "Epoch [2/4], Step [26475/61080], Loss: 4.0093\n",
      "Epoch [2/4], Step [26550/61080], Loss: 3.9409\n",
      "Epoch [2/4], Step [26625/61080], Loss: 4.0181\n",
      "Epoch [2/4], Step [26700/61080], Loss: 3.7539\n",
      "Epoch [2/4], Step [26775/61080], Loss: 4.0325\n",
      "Epoch [2/4], Step [26850/61080], Loss: 3.9124\n",
      "Epoch [2/4], Step [26925/61080], Loss: 4.1571\n",
      "Epoch [2/4], Step [27000/61080], Loss: 3.9178\n",
      "Validation perplexity: 45.11649756103294\n",
      "Epoch [2/4], Step [27075/61080], Loss: 4.0066\n",
      "Epoch [2/4], Step [27150/61080], Loss: 4.0226\n",
      "Epoch [2/4], Step [27225/61080], Loss: 4.1397\n",
      "Epoch [2/4], Step [27300/61080], Loss: 4.0460\n",
      "Epoch [2/4], Step [27375/61080], Loss: 4.2483\n",
      "Epoch [2/4], Step [27450/61080], Loss: 4.1197\n",
      "Epoch [2/4], Step [27525/61080], Loss: 4.0268\n",
      "Epoch [2/4], Step [27600/61080], Loss: 3.9570\n",
      "Epoch [2/4], Step [27675/61080], Loss: 4.0389\n",
      "Epoch [2/4], Step [27750/61080], Loss: 4.0798\n",
      "Epoch [2/4], Step [27825/61080], Loss: 3.7737\n",
      "Epoch [2/4], Step [27900/61080], Loss: 4.2673\n",
      "Epoch [2/4], Step [27975/61080], Loss: 4.1413\n",
      "Epoch [2/4], Step [28050/61080], Loss: 4.0188\n",
      "Epoch [2/4], Step [28125/61080], Loss: 4.0361\n",
      "Epoch [2/4], Step [28200/61080], Loss: 4.3526\n",
      "Epoch [2/4], Step [28275/61080], Loss: 4.2308\n",
      "Epoch [2/4], Step [28350/61080], Loss: 4.0177\n",
      "Epoch [2/4], Step [28425/61080], Loss: 3.9942\n",
      "Epoch [2/4], Step [28500/61080], Loss: 4.3611\n",
      "Epoch [2/4], Step [28575/61080], Loss: 4.2083\n",
      "Epoch [2/4], Step [28650/61080], Loss: 3.8505\n",
      "Epoch [2/4], Step [28725/61080], Loss: 3.8954\n",
      "Epoch [2/4], Step [28800/61080], Loss: 3.8719\n",
      "Epoch [2/4], Step [28875/61080], Loss: 4.1874\n",
      "Epoch [2/4], Step [28950/61080], Loss: 4.0734\n",
      "Epoch [2/4], Step [29025/61080], Loss: 4.2351\n",
      "Epoch [2/4], Step [29100/61080], Loss: 4.1672\n",
      "Epoch [2/4], Step [29175/61080], Loss: 4.0458\n",
      "Epoch [2/4], Step [29250/61080], Loss: 4.0607\n",
      "Epoch [2/4], Step [29325/61080], Loss: 4.0120\n",
      "Epoch [2/4], Step [29400/61080], Loss: 4.0813\n",
      "Epoch [2/4], Step [29475/61080], Loss: 4.0294\n",
      "Epoch [2/4], Step [29550/61080], Loss: 3.9597\n",
      "Epoch [2/4], Step [29625/61080], Loss: 4.0910\n",
      "Epoch [2/4], Step [29700/61080], Loss: 4.0536\n",
      "Epoch [2/4], Step [29775/61080], Loss: 4.0102\n",
      "Epoch [2/4], Step [29850/61080], Loss: 4.2748\n",
      "Epoch [2/4], Step [29925/61080], Loss: 3.9982\n",
      "Epoch [2/4], Step [30000/61080], Loss: 4.0398\n",
      "Validation perplexity: 44.61919179180287\n",
      "Epoch [2/4], Step [30075/61080], Loss: 4.3144\n",
      "Epoch [2/4], Step [30150/61080], Loss: 3.7890\n",
      "Epoch [2/4], Step [30225/61080], Loss: 4.2734\n",
      "Epoch [2/4], Step [30300/61080], Loss: 4.1475\n",
      "Epoch [2/4], Step [30375/61080], Loss: 4.2693\n",
      "Epoch [2/4], Step [30450/61080], Loss: 3.9931\n",
      "Epoch [2/4], Step [30525/61080], Loss: 3.9063\n",
      "Epoch [2/4], Step [30600/61080], Loss: 3.9965\n",
      "Epoch [2/4], Step [30675/61080], Loss: 3.9309\n",
      "Epoch [2/4], Step [30750/61080], Loss: 4.2559\n",
      "Epoch [2/4], Step [30825/61080], Loss: 3.8438\n",
      "Epoch [2/4], Step [30900/61080], Loss: 4.1525\n",
      "Epoch [2/4], Step [30975/61080], Loss: 4.0357\n",
      "Epoch [2/4], Step [31050/61080], Loss: 4.0742\n",
      "Epoch [2/4], Step [31125/61080], Loss: 4.0866\n",
      "Epoch [2/4], Step [31200/61080], Loss: 4.1423\n",
      "Epoch [2/4], Step [31275/61080], Loss: 4.3624\n",
      "Epoch [2/4], Step [31350/61080], Loss: 4.1764\n",
      "Epoch [2/4], Step [31425/61080], Loss: 4.2309\n",
      "Epoch [2/4], Step [31500/61080], Loss: 4.2589\n",
      "Epoch [2/4], Step [31575/61080], Loss: 3.8504\n",
      "Epoch [2/4], Step [31650/61080], Loss: 3.9519\n",
      "Epoch [2/4], Step [31725/61080], Loss: 4.0512\n",
      "Epoch [2/4], Step [31800/61080], Loss: 4.1960\n",
      "Epoch [2/4], Step [31875/61080], Loss: 3.9350\n",
      "Epoch [2/4], Step [31950/61080], Loss: 4.0384\n",
      "Epoch [2/4], Step [32025/61080], Loss: 4.1972\n",
      "Epoch [2/4], Step [32100/61080], Loss: 4.0717\n",
      "Epoch [2/4], Step [32175/61080], Loss: 4.3871\n",
      "Epoch [2/4], Step [32250/61080], Loss: 4.3124\n",
      "Epoch [2/4], Step [32325/61080], Loss: 3.7959\n",
      "Epoch [2/4], Step [32400/61080], Loss: 4.0182\n",
      "Epoch [2/4], Step [32475/61080], Loss: 4.1407\n",
      "Epoch [2/4], Step [32550/61080], Loss: 4.0437\n",
      "Epoch [2/4], Step [32625/61080], Loss: 4.1354\n",
      "Epoch [2/4], Step [32700/61080], Loss: 4.0582\n",
      "Epoch [2/4], Step [32775/61080], Loss: 3.8453\n",
      "Epoch [2/4], Step [32850/61080], Loss: 4.1997\n",
      "Epoch [2/4], Step [32925/61080], Loss: 3.8793\n",
      "Epoch [2/4], Step [33000/61080], Loss: 4.2889\n",
      "Validation perplexity: 44.222064864205755\n",
      "Epoch [2/4], Step [33075/61080], Loss: 4.0210\n",
      "Epoch [2/4], Step [33150/61080], Loss: 4.1366\n",
      "Epoch [2/4], Step [33225/61080], Loss: 4.1008\n",
      "Epoch [2/4], Step [33300/61080], Loss: 4.1769\n",
      "Epoch [2/4], Step [33375/61080], Loss: 4.2334\n",
      "Epoch [2/4], Step [33450/61080], Loss: 4.0107\n",
      "Epoch [2/4], Step [33525/61080], Loss: 3.8416\n",
      "Epoch [2/4], Step [33600/61080], Loss: 4.0121\n",
      "Epoch [2/4], Step [33675/61080], Loss: 3.9987\n",
      "Epoch [2/4], Step [33750/61080], Loss: 4.0808\n",
      "Epoch [2/4], Step [33825/61080], Loss: 4.1083\n",
      "Epoch [2/4], Step [33900/61080], Loss: 4.2059\n",
      "Epoch [2/4], Step [33975/61080], Loss: 4.2523\n",
      "Epoch [2/4], Step [34050/61080], Loss: 4.0820\n",
      "Epoch [2/4], Step [34125/61080], Loss: 4.0440\n",
      "Epoch [2/4], Step [34200/61080], Loss: 3.9958\n",
      "Epoch [2/4], Step [34275/61080], Loss: 4.0991\n",
      "Epoch [2/4], Step [34350/61080], Loss: 4.0018\n",
      "Epoch [2/4], Step [34425/61080], Loss: 3.9731\n",
      "Epoch [2/4], Step [34500/61080], Loss: 3.7854\n",
      "Epoch [2/4], Step [34575/61080], Loss: 4.2339\n",
      "Epoch [2/4], Step [34650/61080], Loss: 4.1531\n",
      "Epoch [2/4], Step [34725/61080], Loss: 4.0450\n",
      "Epoch [2/4], Step [34800/61080], Loss: 3.8551\n",
      "Epoch [2/4], Step [34875/61080], Loss: 3.9504\n",
      "Epoch [2/4], Step [34950/61080], Loss: 4.1128\n",
      "Epoch [2/4], Step [35025/61080], Loss: 3.8633\n",
      "Epoch [2/4], Step [35100/61080], Loss: 4.2921\n",
      "Epoch [2/4], Step [35175/61080], Loss: 4.0533\n",
      "Epoch [2/4], Step [35250/61080], Loss: 4.3505\n",
      "Epoch [2/4], Step [35325/61080], Loss: 4.0916\n",
      "Epoch [2/4], Step [35400/61080], Loss: 3.9007\n",
      "Epoch [2/4], Step [35475/61080], Loss: 3.5885\n",
      "Epoch [2/4], Step [35550/61080], Loss: 4.0112\n",
      "Epoch [2/4], Step [35625/61080], Loss: 4.2541\n",
      "Epoch [2/4], Step [35700/61080], Loss: 4.1524\n",
      "Epoch [2/4], Step [35775/61080], Loss: 4.0021\n",
      "Epoch [2/4], Step [35850/61080], Loss: 4.2305\n",
      "Epoch [2/4], Step [35925/61080], Loss: 3.8838\n",
      "Epoch [2/4], Step [36000/61080], Loss: 3.8247\n",
      "Validation perplexity: 44.36933744006235\n",
      "Epoch [2/4], Step [36075/61080], Loss: 3.8358\n",
      "Epoch [2/4], Step [36150/61080], Loss: 3.8836\n",
      "Epoch [2/4], Step [36225/61080], Loss: 3.9605\n",
      "Epoch [2/4], Step [36300/61080], Loss: 3.8291\n",
      "Epoch [2/4], Step [36375/61080], Loss: 4.0928\n",
      "Epoch [2/4], Step [36450/61080], Loss: 3.9254\n",
      "Epoch [2/4], Step [36525/61080], Loss: 3.9697\n",
      "Epoch [2/4], Step [36600/61080], Loss: 4.6242\n",
      "Epoch [2/4], Step [36675/61080], Loss: 4.0168\n",
      "Epoch [2/4], Step [36750/61080], Loss: 3.9140\n",
      "Epoch [2/4], Step [36825/61080], Loss: 4.1771\n",
      "Epoch [2/4], Step [36900/61080], Loss: 3.8831\n",
      "Epoch [2/4], Step [36975/61080], Loss: 4.1680\n",
      "Epoch [2/4], Step [37050/61080], Loss: 4.0351\n",
      "Epoch [2/4], Step [37125/61080], Loss: 4.1834\n",
      "Epoch [2/4], Step [37200/61080], Loss: 4.3199\n",
      "Epoch [2/4], Step [37275/61080], Loss: 4.1082\n",
      "Epoch [2/4], Step [37350/61080], Loss: 3.8930\n",
      "Epoch [2/4], Step [37425/61080], Loss: 3.7949\n",
      "Epoch [2/4], Step [37500/61080], Loss: 3.9828\n",
      "Epoch [2/4], Step [37575/61080], Loss: 4.2337\n",
      "Epoch [2/4], Step [37650/61080], Loss: 3.8197\n",
      "Epoch [2/4], Step [37725/61080], Loss: 4.0421\n",
      "Epoch [2/4], Step [37800/61080], Loss: 4.1225\n",
      "Epoch [2/4], Step [37875/61080], Loss: 3.8534\n",
      "Epoch [2/4], Step [37950/61080], Loss: 4.1074\n",
      "Epoch [2/4], Step [38025/61080], Loss: 4.1379\n",
      "Epoch [2/4], Step [38100/61080], Loss: 4.0305\n",
      "Epoch [2/4], Step [38175/61080], Loss: 3.8093\n",
      "Epoch [2/4], Step [38250/61080], Loss: 4.2217\n",
      "Epoch [2/4], Step [38325/61080], Loss: 4.0526\n",
      "Epoch [2/4], Step [38400/61080], Loss: 4.4499\n",
      "Epoch [2/4], Step [38475/61080], Loss: 3.9026\n",
      "Epoch [2/4], Step [38550/61080], Loss: 3.9490\n",
      "Epoch [2/4], Step [38625/61080], Loss: 3.9326\n",
      "Epoch [2/4], Step [38700/61080], Loss: 4.2208\n",
      "Epoch [2/4], Step [38775/61080], Loss: 3.9230\n",
      "Epoch [2/4], Step [38850/61080], Loss: 4.2020\n",
      "Epoch [2/4], Step [38925/61080], Loss: 4.2599\n",
      "Epoch [2/4], Step [39000/61080], Loss: 4.1699\n",
      "Validation perplexity: 44.135378282455385\n",
      "Epoch [2/4], Step [39075/61080], Loss: 4.2655\n",
      "Epoch [2/4], Step [39150/61080], Loss: 4.1664\n",
      "Epoch [2/4], Step [39225/61080], Loss: 4.1400\n",
      "Epoch [2/4], Step [39300/61080], Loss: 4.0112\n",
      "Epoch [2/4], Step [39375/61080], Loss: 4.1384\n",
      "Epoch [2/4], Step [39450/61080], Loss: 4.2025\n",
      "Epoch [2/4], Step [39525/61080], Loss: 3.9938\n",
      "Epoch [2/4], Step [39600/61080], Loss: 4.0769\n",
      "Epoch [2/4], Step [39675/61080], Loss: 4.1328\n",
      "Epoch [2/4], Step [39750/61080], Loss: 3.8396\n",
      "Epoch [2/4], Step [39825/61080], Loss: 3.9361\n",
      "Epoch [2/4], Step [39900/61080], Loss: 4.0223\n",
      "Epoch [2/4], Step [39975/61080], Loss: 3.9588\n",
      "Epoch [2/4], Step [40050/61080], Loss: 4.2304\n",
      "Epoch [2/4], Step [40125/61080], Loss: 4.0086\n",
      "Epoch [2/4], Step [40200/61080], Loss: 3.9919\n",
      "Epoch [2/4], Step [40275/61080], Loss: 3.9139\n",
      "Epoch [2/4], Step [40350/61080], Loss: 3.9266\n",
      "Epoch [2/4], Step [40425/61080], Loss: 3.9494\n",
      "Epoch [2/4], Step [40500/61080], Loss: 3.9011\n",
      "Epoch [2/4], Step [40575/61080], Loss: 4.0380\n",
      "Epoch [2/4], Step [40650/61080], Loss: 3.9476\n",
      "Epoch [2/4], Step [40725/61080], Loss: 3.9884\n",
      "Epoch [2/4], Step [40800/61080], Loss: 4.0524\n",
      "Epoch [2/4], Step [40875/61080], Loss: 4.1266\n",
      "Epoch [2/4], Step [40950/61080], Loss: 3.9993\n",
      "Epoch [2/4], Step [41025/61080], Loss: 4.0403\n",
      "Epoch [2/4], Step [41100/61080], Loss: 3.9315\n",
      "Epoch [2/4], Step [41175/61080], Loss: 3.8774\n",
      "Epoch [2/4], Step [41250/61080], Loss: 3.9773\n",
      "Epoch [2/4], Step [41325/61080], Loss: 4.3687\n",
      "Epoch [2/4], Step [41400/61080], Loss: 3.9623\n",
      "Epoch [2/4], Step [41475/61080], Loss: 4.1417\n",
      "Epoch [2/4], Step [41550/61080], Loss: 4.0796\n",
      "Epoch [2/4], Step [41625/61080], Loss: 3.9606\n",
      "Epoch [2/4], Step [41700/61080], Loss: 4.2011\n",
      "Epoch [2/4], Step [41775/61080], Loss: 3.8246\n",
      "Epoch [2/4], Step [41850/61080], Loss: 4.1754\n",
      "Epoch [2/4], Step [41925/61080], Loss: 4.0646\n",
      "Epoch [2/4], Step [42000/61080], Loss: 4.1543\n",
      "Validation perplexity: 43.849236136551895\n",
      "Epoch [2/4], Step [42075/61080], Loss: 3.8160\n",
      "Epoch [2/4], Step [42150/61080], Loss: 4.2292\n",
      "Epoch [2/4], Step [42225/61080], Loss: 3.9960\n",
      "Epoch [2/4], Step [42300/61080], Loss: 4.0970\n",
      "Epoch [2/4], Step [42375/61080], Loss: 3.9766\n",
      "Epoch [2/4], Step [42450/61080], Loss: 4.3164\n",
      "Epoch [2/4], Step [42525/61080], Loss: 4.0450\n",
      "Epoch [2/4], Step [42600/61080], Loss: 4.1842\n",
      "Epoch [2/4], Step [42675/61080], Loss: 4.1156\n",
      "Epoch [2/4], Step [42750/61080], Loss: 4.0636\n",
      "Epoch [2/4], Step [42825/61080], Loss: 3.9685\n",
      "Epoch [2/4], Step [42900/61080], Loss: 4.2012\n",
      "Epoch [2/4], Step [42975/61080], Loss: 4.2989\n",
      "Epoch [2/4], Step [43050/61080], Loss: 4.0183\n",
      "Epoch [2/4], Step [43125/61080], Loss: 3.9542\n",
      "Epoch [2/4], Step [43200/61080], Loss: 3.7815\n",
      "Epoch [2/4], Step [43275/61080], Loss: 4.0343\n",
      "Epoch [2/4], Step [43350/61080], Loss: 4.1007\n",
      "Epoch [2/4], Step [43425/61080], Loss: 4.0379\n",
      "Epoch [2/4], Step [43500/61080], Loss: 4.0600\n",
      "Epoch [2/4], Step [43575/61080], Loss: 3.8234\n",
      "Epoch [2/4], Step [43650/61080], Loss: 4.1570\n",
      "Epoch [2/4], Step [43725/61080], Loss: 3.7678\n",
      "Epoch [2/4], Step [43800/61080], Loss: 4.1326\n",
      "Epoch [2/4], Step [43875/61080], Loss: 3.8639\n",
      "Epoch [2/4], Step [43950/61080], Loss: 3.9821\n",
      "Epoch [2/4], Step [44025/61080], Loss: 4.2628\n",
      "Epoch [2/4], Step [44100/61080], Loss: 4.0878\n",
      "Epoch [2/4], Step [44175/61080], Loss: 4.0032\n",
      "Epoch [2/4], Step [44250/61080], Loss: 4.0195\n",
      "Epoch [2/4], Step [44325/61080], Loss: 4.1659\n",
      "Epoch [2/4], Step [44400/61080], Loss: 3.8835\n",
      "Epoch [2/4], Step [44475/61080], Loss: 3.9466\n",
      "Epoch [2/4], Step [44550/61080], Loss: 3.9761\n",
      "Epoch [2/4], Step [44625/61080], Loss: 4.3583\n",
      "Epoch [2/4], Step [44700/61080], Loss: 4.1409\n",
      "Epoch [2/4], Step [44775/61080], Loss: 3.9996\n",
      "Epoch [2/4], Step [44850/61080], Loss: 4.1690\n",
      "Epoch [2/4], Step [44925/61080], Loss: 4.0278\n",
      "Epoch [2/4], Step [45000/61080], Loss: 4.0963\n",
      "Validation perplexity: 43.58622123583852\n",
      "Epoch [2/4], Step [45075/61080], Loss: 3.8712\n",
      "Epoch [2/4], Step [45150/61080], Loss: 3.9272\n",
      "Epoch [2/4], Step [45225/61080], Loss: 3.8122\n",
      "Epoch [2/4], Step [45300/61080], Loss: 3.8351\n",
      "Epoch [2/4], Step [45375/61080], Loss: 4.2279\n",
      "Epoch [2/4], Step [45450/61080], Loss: 4.1291\n",
      "Epoch [2/4], Step [45525/61080], Loss: 4.0549\n",
      "Epoch [2/4], Step [45600/61080], Loss: 4.0118\n",
      "Epoch [2/4], Step [45675/61080], Loss: 3.8980\n",
      "Epoch [2/4], Step [45750/61080], Loss: 4.0104\n",
      "Epoch [2/4], Step [45825/61080], Loss: 3.9432\n",
      "Epoch [2/4], Step [45900/61080], Loss: 4.1287\n",
      "Epoch [2/4], Step [45975/61080], Loss: 3.9370\n",
      "Epoch [2/4], Step [46050/61080], Loss: 4.2719\n",
      "Epoch [2/4], Step [46125/61080], Loss: 4.0113\n",
      "Epoch [2/4], Step [46200/61080], Loss: 4.2336\n",
      "Epoch [2/4], Step [46275/61080], Loss: 3.8976\n",
      "Epoch [2/4], Step [46350/61080], Loss: 4.2417\n",
      "Epoch [2/4], Step [46425/61080], Loss: 4.1688\n",
      "Epoch [2/4], Step [46500/61080], Loss: 4.0951\n",
      "Epoch [2/4], Step [46575/61080], Loss: 3.9531\n",
      "Epoch [2/4], Step [46650/61080], Loss: 3.7881\n",
      "Epoch [2/4], Step [46725/61080], Loss: 4.1548\n",
      "Epoch [2/4], Step [46800/61080], Loss: 4.3384\n",
      "Epoch [2/4], Step [46875/61080], Loss: 3.8618\n",
      "Epoch [2/4], Step [46950/61080], Loss: 4.2061\n",
      "Epoch [2/4], Step [47025/61080], Loss: 4.0034\n",
      "Epoch [2/4], Step [47100/61080], Loss: 4.0295\n",
      "Epoch [2/4], Step [47175/61080], Loss: 4.0125\n",
      "Epoch [2/4], Step [47250/61080], Loss: 4.0654\n",
      "Epoch [2/4], Step [47325/61080], Loss: 3.9293\n",
      "Epoch [2/4], Step [47400/61080], Loss: 3.7883\n",
      "Epoch [2/4], Step [47475/61080], Loss: 4.4018\n",
      "Epoch [2/4], Step [47550/61080], Loss: 4.1962\n",
      "Epoch [2/4], Step [47625/61080], Loss: 3.9114\n",
      "Epoch [2/4], Step [47700/61080], Loss: 4.0245\n",
      "Epoch [2/4], Step [47775/61080], Loss: 4.0343\n",
      "Epoch [2/4], Step [47850/61080], Loss: 4.1148\n",
      "Epoch [2/4], Step [47925/61080], Loss: 4.2260\n",
      "Epoch [2/4], Step [48000/61080], Loss: 4.2717\n",
      "Validation perplexity: 43.56800356951172\n",
      "Epoch [2/4], Step [48075/61080], Loss: 4.3210\n",
      "Epoch [2/4], Step [48150/61080], Loss: 4.0683\n",
      "Epoch [2/4], Step [48225/61080], Loss: 3.9735\n",
      "Epoch [2/4], Step [48300/61080], Loss: 4.1153\n",
      "Epoch [2/4], Step [48375/61080], Loss: 3.9939\n",
      "Epoch [2/4], Step [48450/61080], Loss: 4.1347\n",
      "Epoch [2/4], Step [48525/61080], Loss: 3.9552\n",
      "Epoch [2/4], Step [48600/61080], Loss: 4.2254\n",
      "Epoch [2/4], Step [48675/61080], Loss: 4.0706\n",
      "Epoch [2/4], Step [48750/61080], Loss: 4.0045\n",
      "Epoch [2/4], Step [48825/61080], Loss: 4.0769\n",
      "Epoch [2/4], Step [48900/61080], Loss: 3.7539\n",
      "Epoch [2/4], Step [48975/61080], Loss: 4.1243\n",
      "Epoch [2/4], Step [49050/61080], Loss: 3.9942\n",
      "Epoch [2/4], Step [49125/61080], Loss: 3.8563\n",
      "Epoch [2/4], Step [49200/61080], Loss: 4.1464\n",
      "Epoch [2/4], Step [49275/61080], Loss: 3.7851\n",
      "Epoch [2/4], Step [49350/61080], Loss: 4.0235\n",
      "Epoch [2/4], Step [49425/61080], Loss: 3.9513\n",
      "Epoch [2/4], Step [49500/61080], Loss: 4.1222\n",
      "Epoch [2/4], Step [49575/61080], Loss: 4.0720\n",
      "Epoch [2/4], Step [49650/61080], Loss: 4.0546\n",
      "Epoch [2/4], Step [49725/61080], Loss: 3.9942\n",
      "Epoch [2/4], Step [49800/61080], Loss: 4.0292\n",
      "Epoch [2/4], Step [49875/61080], Loss: 4.1122\n",
      "Epoch [2/4], Step [49950/61080], Loss: 3.8863\n",
      "Epoch [2/4], Step [50025/61080], Loss: 3.8968\n",
      "Epoch [2/4], Step [50100/61080], Loss: 3.9217\n",
      "Epoch [2/4], Step [50175/61080], Loss: 3.9617\n",
      "Epoch [2/4], Step [50250/61080], Loss: 3.9810\n",
      "Epoch [2/4], Step [50325/61080], Loss: 3.9104\n",
      "Epoch [2/4], Step [50400/61080], Loss: 4.1466\n",
      "Epoch [2/4], Step [50475/61080], Loss: 4.2688\n",
      "Epoch [2/4], Step [50550/61080], Loss: 3.8781\n",
      "Epoch [2/4], Step [50625/61080], Loss: 4.0455\n",
      "Epoch [2/4], Step [50700/61080], Loss: 4.1660\n",
      "Epoch [2/4], Step [50775/61080], Loss: 4.2475\n",
      "Epoch [2/4], Step [50850/61080], Loss: 4.1499\n",
      "Epoch [2/4], Step [50925/61080], Loss: 3.9725\n",
      "Epoch [2/4], Step [51000/61080], Loss: 3.9504\n",
      "Validation perplexity: 43.52888431495926\n",
      "Epoch [2/4], Step [51075/61080], Loss: 3.8785\n",
      "Epoch [2/4], Step [51150/61080], Loss: 3.7947\n",
      "Epoch [2/4], Step [51225/61080], Loss: 3.9996\n",
      "Epoch [2/4], Step [51300/61080], Loss: 4.0156\n",
      "Epoch [2/4], Step [51375/61080], Loss: 3.8931\n",
      "Epoch [2/4], Step [51450/61080], Loss: 3.8913\n",
      "Epoch [2/4], Step [51525/61080], Loss: 3.9729\n",
      "Epoch [2/4], Step [51600/61080], Loss: 4.0230\n",
      "Epoch [2/4], Step [51675/61080], Loss: 3.9575\n",
      "Epoch [2/4], Step [51750/61080], Loss: 3.9195\n",
      "Epoch [2/4], Step [51825/61080], Loss: 4.3019\n",
      "Epoch [2/4], Step [51900/61080], Loss: 3.9403\n",
      "Epoch [2/4], Step [51975/61080], Loss: 4.1704\n",
      "Epoch [2/4], Step [52050/61080], Loss: 3.9718\n",
      "Epoch [2/4], Step [52125/61080], Loss: 4.0438\n",
      "Epoch [2/4], Step [52200/61080], Loss: 3.9856\n",
      "Epoch [2/4], Step [52275/61080], Loss: 3.8620\n",
      "Epoch [2/4], Step [52350/61080], Loss: 4.1508\n",
      "Epoch [2/4], Step [52425/61080], Loss: 4.2316\n",
      "Epoch [2/4], Step [52500/61080], Loss: 3.8814\n",
      "Epoch [2/4], Step [52575/61080], Loss: 4.0076\n",
      "Epoch [2/4], Step [52650/61080], Loss: 4.0138\n",
      "Epoch [2/4], Step [52725/61080], Loss: 3.9970\n",
      "Epoch [2/4], Step [52800/61080], Loss: 4.1827\n",
      "Epoch [2/4], Step [52875/61080], Loss: 4.0271\n",
      "Epoch [2/4], Step [52950/61080], Loss: 4.0296\n",
      "Epoch [2/4], Step [53025/61080], Loss: 3.7535\n",
      "Epoch [2/4], Step [53100/61080], Loss: 3.9456\n",
      "Epoch [2/4], Step [53175/61080], Loss: 3.9152\n",
      "Epoch [2/4], Step [53250/61080], Loss: 4.0711\n",
      "Epoch [2/4], Step [53325/61080], Loss: 4.2614\n",
      "Epoch [2/4], Step [53400/61080], Loss: 3.9632\n",
      "Epoch [2/4], Step [53475/61080], Loss: 3.9641\n",
      "Epoch [2/4], Step [53550/61080], Loss: 4.1510\n",
      "Epoch [2/4], Step [53625/61080], Loss: 4.1131\n",
      "Epoch [2/4], Step [53700/61080], Loss: 3.9726\n",
      "Epoch [2/4], Step [53775/61080], Loss: 3.9323\n",
      "Epoch [2/4], Step [53850/61080], Loss: 4.1225\n",
      "Epoch [2/4], Step [53925/61080], Loss: 3.8774\n",
      "Epoch [2/4], Step [54000/61080], Loss: 3.8927\n",
      "Validation perplexity: 42.88387352320981\n",
      "Epoch [2/4], Step [54075/61080], Loss: 4.3608\n",
      "Epoch [2/4], Step [54150/61080], Loss: 4.1188\n",
      "Epoch [2/4], Step [54225/61080], Loss: 4.2633\n",
      "Epoch [2/4], Step [54300/61080], Loss: 4.1838\n",
      "Epoch [2/4], Step [54375/61080], Loss: 4.1120\n",
      "Epoch [2/4], Step [54450/61080], Loss: 4.0495\n",
      "Epoch [2/4], Step [54525/61080], Loss: 3.9501\n",
      "Epoch [2/4], Step [54600/61080], Loss: 3.9485\n",
      "Epoch [2/4], Step [54675/61080], Loss: 3.9269\n",
      "Epoch [2/4], Step [54750/61080], Loss: 4.1139\n",
      "Epoch [2/4], Step [54825/61080], Loss: 3.7245\n",
      "Epoch [2/4], Step [54900/61080], Loss: 3.8616\n",
      "Epoch [2/4], Step [54975/61080], Loss: 4.2264\n",
      "Epoch [2/4], Step [55050/61080], Loss: 3.8273\n",
      "Epoch [2/4], Step [55125/61080], Loss: 4.1220\n",
      "Epoch [2/4], Step [55200/61080], Loss: 3.9973\n",
      "Epoch [2/4], Step [55275/61080], Loss: 4.1134\n",
      "Epoch [2/4], Step [55350/61080], Loss: 4.2148\n",
      "Epoch [2/4], Step [55425/61080], Loss: 4.0266\n",
      "Epoch [2/4], Step [55500/61080], Loss: 3.8701\n",
      "Epoch [2/4], Step [55575/61080], Loss: 4.0030\n",
      "Epoch [2/4], Step [55650/61080], Loss: 3.9433\n",
      "Epoch [2/4], Step [55725/61080], Loss: 4.0031\n",
      "Epoch [2/4], Step [55800/61080], Loss: 4.0448\n",
      "Epoch [2/4], Step [55875/61080], Loss: 3.9079\n",
      "Epoch [2/4], Step [55950/61080], Loss: 3.9923\n",
      "Epoch [2/4], Step [56025/61080], Loss: 4.1174\n",
      "Epoch [2/4], Step [56100/61080], Loss: 3.8628\n",
      "Epoch [2/4], Step [56175/61080], Loss: 4.0501\n",
      "Epoch [2/4], Step [56250/61080], Loss: 3.8719\n",
      "Epoch [2/4], Step [56325/61080], Loss: 4.0145\n",
      "Epoch [2/4], Step [56400/61080], Loss: 3.8211\n",
      "Epoch [2/4], Step [56475/61080], Loss: 4.0105\n",
      "Epoch [2/4], Step [56550/61080], Loss: 3.7800\n",
      "Epoch [2/4], Step [56625/61080], Loss: 4.1509\n",
      "Epoch [2/4], Step [56700/61080], Loss: 3.7500\n",
      "Epoch [2/4], Step [56775/61080], Loss: 3.9150\n",
      "Epoch [2/4], Step [56850/61080], Loss: 3.9362\n",
      "Epoch [2/4], Step [56925/61080], Loss: 4.0199\n",
      "Epoch [2/4], Step [57000/61080], Loss: 4.1508\n",
      "Validation perplexity: 42.87119263936011\n",
      "Epoch [2/4], Step [57075/61080], Loss: 3.7714\n",
      "Epoch [2/4], Step [57150/61080], Loss: 3.8107\n",
      "Epoch [2/4], Step [57225/61080], Loss: 4.0942\n",
      "Epoch [2/4], Step [57300/61080], Loss: 3.8109\n",
      "Epoch [2/4], Step [57375/61080], Loss: 4.0659\n",
      "Epoch [2/4], Step [57450/61080], Loss: 4.3236\n",
      "Epoch [2/4], Step [57525/61080], Loss: 3.8433\n",
      "Epoch [2/4], Step [57600/61080], Loss: 4.0076\n",
      "Epoch [2/4], Step [57675/61080], Loss: 3.9098\n",
      "Epoch [2/4], Step [57750/61080], Loss: 3.9374\n",
      "Epoch [2/4], Step [57825/61080], Loss: 4.2604\n",
      "Epoch [2/4], Step [57900/61080], Loss: 3.8316\n",
      "Epoch [2/4], Step [57975/61080], Loss: 3.9259\n",
      "Epoch [2/4], Step [58050/61080], Loss: 4.0862\n",
      "Epoch [2/4], Step [58125/61080], Loss: 4.0057\n",
      "Epoch [2/4], Step [58200/61080], Loss: 4.0239\n",
      "Epoch [2/4], Step [58275/61080], Loss: 4.1206\n",
      "Epoch [2/4], Step [58350/61080], Loss: 4.1417\n",
      "Epoch [2/4], Step [58425/61080], Loss: 3.8435\n",
      "Epoch [2/4], Step [58500/61080], Loss: 4.1123\n",
      "Epoch [2/4], Step [58575/61080], Loss: 3.8528\n",
      "Epoch [2/4], Step [58650/61080], Loss: 3.8131\n",
      "Epoch [2/4], Step [58725/61080], Loss: 4.0237\n",
      "Epoch [2/4], Step [58800/61080], Loss: 4.1327\n",
      "Epoch [2/4], Step [58875/61080], Loss: 4.0561\n",
      "Epoch [2/4], Step [58950/61080], Loss: 4.1698\n",
      "Epoch [2/4], Step [59025/61080], Loss: 3.9807\n",
      "Epoch [2/4], Step [59100/61080], Loss: 3.9789\n",
      "Epoch [2/4], Step [59175/61080], Loss: 3.9864\n",
      "Epoch [2/4], Step [59250/61080], Loss: 3.9225\n",
      "Epoch [2/4], Step [59325/61080], Loss: 3.8501\n",
      "Epoch [2/4], Step [59400/61080], Loss: 4.1849\n",
      "Epoch [2/4], Step [59475/61080], Loss: 3.8188\n",
      "Epoch [2/4], Step [59550/61080], Loss: 3.9508\n",
      "Epoch [2/4], Step [59625/61080], Loss: 4.2453\n",
      "Epoch [2/4], Step [59700/61080], Loss: 3.9236\n",
      "Epoch [2/4], Step [59775/61080], Loss: 4.1936\n",
      "Epoch [2/4], Step [59850/61080], Loss: 3.8615\n",
      "Epoch [2/4], Step [59925/61080], Loss: 4.1949\n",
      "Epoch [2/4], Step [60000/61080], Loss: 3.9381\n",
      "Validation perplexity: 42.630289399877604\n",
      "Epoch [2/4], Step [60075/61080], Loss: 3.9868\n",
      "Epoch [2/4], Step [60150/61080], Loss: 4.0666\n",
      "Epoch [2/4], Step [60225/61080], Loss: 3.9780\n",
      "Epoch [2/4], Step [60300/61080], Loss: 4.0097\n",
      "Epoch [2/4], Step [60375/61080], Loss: 3.8731\n",
      "Epoch [2/4], Step [60450/61080], Loss: 3.9955\n",
      "Epoch [2/4], Step [60525/61080], Loss: 3.9884\n",
      "Epoch [2/4], Step [60600/61080], Loss: 3.8779\n",
      "Epoch [2/4], Step [60675/61080], Loss: 4.1106\n",
      "Epoch [2/4], Step [60750/61080], Loss: 3.8755\n",
      "Epoch [2/4], Step [60825/61080], Loss: 4.1846\n",
      "Epoch [2/4], Step [60900/61080], Loss: 4.0506\n",
      "Epoch [2/4], Step [60975/61080], Loss: 3.9757\n",
      "Epoch [2/4], Step [61050/61080], Loss: 3.6292\n",
      "Epoch [2/4] Average Loss: 4.0508, Perplexity: 57.44\n",
      "Epoch [3/4], Step [0/61080], Loss: 4.0608\n",
      "Validation perplexity: 42.900471577912626\n",
      "Epoch [3/4], Step [75/61080], Loss: 4.3034\n",
      "Epoch [3/4], Step [150/61080], Loss: 4.0224\n",
      "Epoch [3/4], Step [225/61080], Loss: 3.9714\n",
      "Epoch [3/4], Step [300/61080], Loss: 3.9367\n",
      "Epoch [3/4], Step [375/61080], Loss: 3.9116\n",
      "Epoch [3/4], Step [450/61080], Loss: 3.9117\n",
      "Epoch [3/4], Step [525/61080], Loss: 4.1330\n",
      "Epoch [3/4], Step [600/61080], Loss: 3.9581\n",
      "Epoch [3/4], Step [675/61080], Loss: 3.8331\n",
      "Epoch [3/4], Step [750/61080], Loss: 3.7812\n",
      "Epoch [3/4], Step [825/61080], Loss: 3.9293\n",
      "Epoch [3/4], Step [900/61080], Loss: 3.9696\n",
      "Epoch [3/4], Step [975/61080], Loss: 3.9327\n",
      "Epoch [3/4], Step [1050/61080], Loss: 3.9590\n",
      "Epoch [3/4], Step [1125/61080], Loss: 4.0675\n",
      "Epoch [3/4], Step [1200/61080], Loss: 4.1823\n",
      "Epoch [3/4], Step [1275/61080], Loss: 4.1305\n",
      "Epoch [3/4], Step [1350/61080], Loss: 3.9903\n",
      "Epoch [3/4], Step [1425/61080], Loss: 3.9418\n",
      "Epoch [3/4], Step [1500/61080], Loss: 4.0496\n",
      "Epoch [3/4], Step [1575/61080], Loss: 4.0905\n",
      "Epoch [3/4], Step [1650/61080], Loss: 4.0548\n",
      "Epoch [3/4], Step [1725/61080], Loss: 3.9080\n",
      "Epoch [3/4], Step [1800/61080], Loss: 3.9361\n",
      "Epoch [3/4], Step [1875/61080], Loss: 3.8199\n",
      "Epoch [3/4], Step [1950/61080], Loss: 4.0015\n",
      "Epoch [3/4], Step [2025/61080], Loss: 3.9952\n",
      "Epoch [3/4], Step [2100/61080], Loss: 3.9825\n",
      "Epoch [3/4], Step [2175/61080], Loss: 3.8495\n",
      "Epoch [3/4], Step [2250/61080], Loss: 4.1141\n",
      "Epoch [3/4], Step [2325/61080], Loss: 4.0119\n",
      "Epoch [3/4], Step [2400/61080], Loss: 3.9717\n",
      "Epoch [3/4], Step [2475/61080], Loss: 3.9305\n",
      "Epoch [3/4], Step [2550/61080], Loss: 3.9927\n",
      "Epoch [3/4], Step [2625/61080], Loss: 4.0233\n",
      "Epoch [3/4], Step [2700/61080], Loss: 3.9893\n",
      "Epoch [3/4], Step [2775/61080], Loss: 3.9958\n",
      "Epoch [3/4], Step [2850/61080], Loss: 3.7449\n",
      "Epoch [3/4], Step [2925/61080], Loss: 4.1441\n",
      "Epoch [3/4], Step [3000/61080], Loss: 3.9506\n",
      "Validation perplexity: 42.46260708129703\n",
      "Epoch [3/4], Step [3075/61080], Loss: 3.8588\n",
      "Epoch [3/4], Step [3150/61080], Loss: 3.9436\n",
      "Epoch [3/4], Step [3225/61080], Loss: 3.8932\n",
      "Epoch [3/4], Step [3300/61080], Loss: 4.0121\n",
      "Epoch [3/4], Step [3375/61080], Loss: 3.8976\n",
      "Epoch [3/4], Step [3450/61080], Loss: 4.0187\n",
      "Epoch [3/4], Step [3525/61080], Loss: 3.8807\n",
      "Epoch [3/4], Step [3600/61080], Loss: 4.0111\n",
      "Epoch [3/4], Step [3675/61080], Loss: 4.2854\n",
      "Epoch [3/4], Step [3750/61080], Loss: 4.1384\n",
      "Epoch [3/4], Step [3825/61080], Loss: 3.9921\n",
      "Epoch [3/4], Step [3900/61080], Loss: 3.9670\n",
      "Epoch [3/4], Step [3975/61080], Loss: 4.0722\n",
      "Epoch [3/4], Step [4050/61080], Loss: 3.8575\n",
      "Epoch [3/4], Step [4125/61080], Loss: 4.0380\n",
      "Epoch [3/4], Step [4200/61080], Loss: 4.1632\n",
      "Epoch [3/4], Step [4275/61080], Loss: 4.3042\n",
      "Epoch [3/4], Step [4350/61080], Loss: 3.8957\n",
      "Epoch [3/4], Step [4425/61080], Loss: 3.9973\n",
      "Epoch [3/4], Step [4500/61080], Loss: 4.2127\n",
      "Epoch [3/4], Step [4575/61080], Loss: 3.6780\n",
      "Epoch [3/4], Step [4650/61080], Loss: 3.8450\n",
      "Epoch [3/4], Step [4725/61080], Loss: 4.1518\n",
      "Epoch [3/4], Step [4800/61080], Loss: 3.8729\n",
      "Epoch [3/4], Step [4875/61080], Loss: 4.0488\n",
      "Epoch [3/4], Step [4950/61080], Loss: 4.0043\n",
      "Epoch [3/4], Step [5025/61080], Loss: 3.9989\n",
      "Epoch [3/4], Step [5100/61080], Loss: 3.9627\n",
      "Epoch [3/4], Step [5175/61080], Loss: 3.9897\n",
      "Epoch [3/4], Step [5250/61080], Loss: 4.0147\n",
      "Epoch [3/4], Step [5325/61080], Loss: 4.0082\n",
      "Epoch [3/4], Step [5400/61080], Loss: 3.9637\n",
      "Epoch [3/4], Step [5475/61080], Loss: 3.9235\n",
      "Epoch [3/4], Step [5550/61080], Loss: 3.9800\n",
      "Epoch [3/4], Step [5625/61080], Loss: 4.0340\n",
      "Epoch [3/4], Step [5700/61080], Loss: 3.5958\n",
      "Epoch [3/4], Step [5775/61080], Loss: 3.8171\n",
      "Epoch [3/4], Step [5850/61080], Loss: 3.9853\n",
      "Epoch [3/4], Step [5925/61080], Loss: 3.9446\n",
      "Epoch [3/4], Step [6000/61080], Loss: 3.9348\n",
      "Validation perplexity: 42.37046158323512\n",
      "Epoch [3/4], Step [6075/61080], Loss: 3.7116\n",
      "Epoch [3/4], Step [6150/61080], Loss: 3.9933\n",
      "Epoch [3/4], Step [6225/61080], Loss: 3.8550\n",
      "Epoch [3/4], Step [6300/61080], Loss: 4.0554\n",
      "Epoch [3/4], Step [6375/61080], Loss: 4.2113\n",
      "Epoch [3/4], Step [6450/61080], Loss: 3.9157\n",
      "Epoch [3/4], Step [6525/61080], Loss: 3.9466\n",
      "Epoch [3/4], Step [6600/61080], Loss: 3.8499\n",
      "Epoch [3/4], Step [6675/61080], Loss: 3.9985\n",
      "Epoch [3/4], Step [6750/61080], Loss: 4.0405\n",
      "Epoch [3/4], Step [6825/61080], Loss: 4.1924\n",
      "Epoch [3/4], Step [6900/61080], Loss: 3.8272\n",
      "Epoch [3/4], Step [6975/61080], Loss: 3.8573\n",
      "Epoch [3/4], Step [7050/61080], Loss: 4.0227\n",
      "Epoch [3/4], Step [7125/61080], Loss: 4.2464\n",
      "Epoch [3/4], Step [7200/61080], Loss: 3.9071\n",
      "Epoch [3/4], Step [7275/61080], Loss: 4.2337\n",
      "Epoch [3/4], Step [7350/61080], Loss: 4.0666\n",
      "Epoch [3/4], Step [7425/61080], Loss: 4.1177\n",
      "Epoch [3/4], Step [7500/61080], Loss: 4.1839\n",
      "Epoch [3/4], Step [7575/61080], Loss: 3.8785\n",
      "Epoch [3/4], Step [7650/61080], Loss: 4.2193\n",
      "Epoch [3/4], Step [7725/61080], Loss: 3.8314\n",
      "Epoch [3/4], Step [7800/61080], Loss: 3.6902\n",
      "Epoch [3/4], Step [7875/61080], Loss: 3.8921\n",
      "Epoch [3/4], Step [7950/61080], Loss: 4.4033\n",
      "Epoch [3/4], Step [8025/61080], Loss: 3.8898\n",
      "Epoch [3/4], Step [8100/61080], Loss: 3.8178\n",
      "Epoch [3/4], Step [8175/61080], Loss: 4.1745\n",
      "Epoch [3/4], Step [8250/61080], Loss: 3.8666\n",
      "Epoch [3/4], Step [8325/61080], Loss: 4.1905\n",
      "Epoch [3/4], Step [8400/61080], Loss: 3.8843\n",
      "Epoch [3/4], Step [8475/61080], Loss: 4.0301\n",
      "Epoch [3/4], Step [8550/61080], Loss: 4.2168\n",
      "Epoch [3/4], Step [8625/61080], Loss: 4.0311\n",
      "Epoch [3/4], Step [8700/61080], Loss: 3.7344\n",
      "Epoch [3/4], Step [8775/61080], Loss: 4.1020\n",
      "Epoch [3/4], Step [8850/61080], Loss: 3.9359\n",
      "Epoch [3/4], Step [8925/61080], Loss: 4.2727\n",
      "Epoch [3/4], Step [9000/61080], Loss: 3.9565\n",
      "Validation perplexity: 42.12273957233251\n",
      "Epoch [3/4], Step [9075/61080], Loss: 4.3080\n",
      "Epoch [3/4], Step [9150/61080], Loss: 4.0712\n",
      "Epoch [3/4], Step [9225/61080], Loss: 4.2598\n",
      "Epoch [3/4], Step [9300/61080], Loss: 3.9479\n",
      "Epoch [3/4], Step [9375/61080], Loss: 3.9751\n",
      "Epoch [3/4], Step [9450/61080], Loss: 4.0293\n",
      "Epoch [3/4], Step [9525/61080], Loss: 3.9056\n",
      "Epoch [3/4], Step [9600/61080], Loss: 4.1081\n",
      "Epoch [3/4], Step [9675/61080], Loss: 4.1095\n",
      "Epoch [3/4], Step [9750/61080], Loss: 4.0181\n",
      "Epoch [3/4], Step [9825/61080], Loss: 3.9801\n",
      "Epoch [3/4], Step [9900/61080], Loss: 4.0590\n",
      "Epoch [3/4], Step [9975/61080], Loss: 4.0429\n",
      "Epoch [3/4], Step [10050/61080], Loss: 3.9835\n",
      "Epoch [3/4], Step [10125/61080], Loss: 4.0480\n",
      "Epoch [3/4], Step [10200/61080], Loss: 3.8521\n",
      "Epoch [3/4], Step [10275/61080], Loss: 4.1066\n",
      "Epoch [3/4], Step [10350/61080], Loss: 3.8703\n",
      "Epoch [3/4], Step [10425/61080], Loss: 3.9199\n",
      "Epoch [3/4], Step [10500/61080], Loss: 3.9362\n",
      "Epoch [3/4], Step [10575/61080], Loss: 3.8321\n",
      "Epoch [3/4], Step [10650/61080], Loss: 3.9242\n",
      "Epoch [3/4], Step [10725/61080], Loss: 4.0187\n",
      "Epoch [3/4], Step [10800/61080], Loss: 3.8816\n",
      "Epoch [3/4], Step [10875/61080], Loss: 4.0810\n",
      "Epoch [3/4], Step [10950/61080], Loss: 3.9763\n",
      "Epoch [3/4], Step [11025/61080], Loss: 3.9727\n",
      "Epoch [3/4], Step [11100/61080], Loss: 4.0683\n",
      "Epoch [3/4], Step [11175/61080], Loss: 3.8802\n",
      "Epoch [3/4], Step [11250/61080], Loss: 4.0796\n",
      "Epoch [3/4], Step [11325/61080], Loss: 4.2294\n",
      "Epoch [3/4], Step [11400/61080], Loss: 4.1865\n",
      "Epoch [3/4], Step [11475/61080], Loss: 4.0205\n",
      "Epoch [3/4], Step [11550/61080], Loss: 3.9369\n",
      "Epoch [3/4], Step [11625/61080], Loss: 3.8385\n",
      "Epoch [3/4], Step [11700/61080], Loss: 3.7504\n",
      "Epoch [3/4], Step [11775/61080], Loss: 3.9813\n",
      "Epoch [3/4], Step [11850/61080], Loss: 4.0570\n",
      "Epoch [3/4], Step [11925/61080], Loss: 3.7895\n",
      "Epoch [3/4], Step [12000/61080], Loss: 4.0160\n",
      "Validation perplexity: 42.031089889182404\n",
      "Epoch [3/4], Step [12075/61080], Loss: 4.0876\n",
      "Epoch [3/4], Step [12150/61080], Loss: 4.2059\n",
      "Epoch [3/4], Step [12225/61080], Loss: 3.9725\n",
      "Epoch [3/4], Step [12300/61080], Loss: 3.8920\n",
      "Epoch [3/4], Step [12375/61080], Loss: 4.0893\n",
      "Epoch [3/4], Step [12450/61080], Loss: 3.8601\n",
      "Epoch [3/4], Step [12525/61080], Loss: 3.9655\n",
      "Epoch [3/4], Step [12600/61080], Loss: 3.9772\n",
      "Epoch [3/4], Step [12675/61080], Loss: 4.0039\n",
      "Epoch [3/4], Step [12750/61080], Loss: 4.1330\n",
      "Epoch [3/4], Step [12825/61080], Loss: 4.0139\n",
      "Epoch [3/4], Step [12900/61080], Loss: 3.8507\n",
      "Epoch [3/4], Step [12975/61080], Loss: 4.0475\n",
      "Epoch [3/4], Step [13050/61080], Loss: 3.9471\n",
      "Epoch [3/4], Step [13125/61080], Loss: 3.9436\n",
      "Epoch [3/4], Step [13200/61080], Loss: 4.0176\n",
      "Epoch [3/4], Step [13275/61080], Loss: 4.0163\n",
      "Epoch [3/4], Step [13350/61080], Loss: 3.8433\n",
      "Epoch [3/4], Step [13425/61080], Loss: 4.2378\n",
      "Epoch [3/4], Step [13500/61080], Loss: 4.2176\n",
      "Epoch [3/4], Step [13575/61080], Loss: 4.0249\n",
      "Epoch [3/4], Step [13650/61080], Loss: 3.9120\n",
      "Epoch [3/4], Step [13725/61080], Loss: 3.9817\n",
      "Epoch [3/4], Step [13800/61080], Loss: 3.9268\n",
      "Epoch [3/4], Step [13875/61080], Loss: 4.0625\n",
      "Epoch [3/4], Step [13950/61080], Loss: 3.9502\n",
      "Epoch [3/4], Step [14025/61080], Loss: 4.2648\n",
      "Epoch [3/4], Step [14100/61080], Loss: 3.7021\n",
      "Epoch [3/4], Step [14175/61080], Loss: 3.9774\n",
      "Epoch [3/4], Step [14250/61080], Loss: 4.0199\n",
      "Epoch [3/4], Step [14325/61080], Loss: 4.0063\n",
      "Epoch [3/4], Step [14400/61080], Loss: 3.7880\n",
      "Epoch [3/4], Step [14475/61080], Loss: 4.1220\n",
      "Epoch [3/4], Step [14550/61080], Loss: 4.2373\n",
      "Epoch [3/4], Step [14625/61080], Loss: 4.1332\n",
      "Epoch [3/4], Step [14700/61080], Loss: 3.8631\n",
      "Epoch [3/4], Step [14775/61080], Loss: 4.4501\n",
      "Epoch [3/4], Step [14850/61080], Loss: 4.0519\n",
      "Epoch [3/4], Step [14925/61080], Loss: 4.1570\n",
      "Epoch [3/4], Step [15000/61080], Loss: 3.8303\n",
      "Validation perplexity: 41.98993800190221\n",
      "Epoch [3/4], Step [15075/61080], Loss: 4.0415\n",
      "Epoch [3/4], Step [15150/61080], Loss: 4.1384\n",
      "Epoch [3/4], Step [15225/61080], Loss: 3.9355\n",
      "Epoch [3/4], Step [15300/61080], Loss: 3.9156\n",
      "Epoch [3/4], Step [15375/61080], Loss: 3.9897\n",
      "Epoch [3/4], Step [15450/61080], Loss: 3.8544\n",
      "Epoch [3/4], Step [15525/61080], Loss: 4.1003\n",
      "Epoch [3/4], Step [15600/61080], Loss: 4.2439\n",
      "Epoch [3/4], Step [15675/61080], Loss: 3.8893\n",
      "Epoch [3/4], Step [15750/61080], Loss: 3.8607\n",
      "Epoch [3/4], Step [15825/61080], Loss: 4.0970\n",
      "Epoch [3/4], Step [15900/61080], Loss: 4.0496\n",
      "Epoch [3/4], Step [15975/61080], Loss: 4.2651\n",
      "Epoch [3/4], Step [16050/61080], Loss: 3.8735\n",
      "Epoch [3/4], Step [16125/61080], Loss: 4.1486\n",
      "Epoch [3/4], Step [16200/61080], Loss: 3.8879\n",
      "Epoch [3/4], Step [16275/61080], Loss: 4.0747\n",
      "Epoch [3/4], Step [16350/61080], Loss: 3.9084\n",
      "Epoch [3/4], Step [16425/61080], Loss: 3.8659\n",
      "Epoch [3/4], Step [16500/61080], Loss: 4.0012\n",
      "Epoch [3/4], Step [16575/61080], Loss: 3.9297\n",
      "Epoch [3/4], Step [16650/61080], Loss: 3.9435\n",
      "Epoch [3/4], Step [16725/61080], Loss: 3.9933\n",
      "Epoch [3/4], Step [16800/61080], Loss: 3.8748\n",
      "Epoch [3/4], Step [16875/61080], Loss: 3.8511\n",
      "Epoch [3/4], Step [16950/61080], Loss: 4.1472\n",
      "Epoch [3/4], Step [17025/61080], Loss: 4.1375\n",
      "Epoch [3/4], Step [17100/61080], Loss: 4.0461\n",
      "Epoch [3/4], Step [17175/61080], Loss: 4.0854\n",
      "Epoch [3/4], Step [17250/61080], Loss: 3.9083\n",
      "Epoch [3/4], Step [17325/61080], Loss: 4.0586\n",
      "Epoch [3/4], Step [17400/61080], Loss: 4.0830\n",
      "Epoch [3/4], Step [17475/61080], Loss: 3.7433\n",
      "Epoch [3/4], Step [17550/61080], Loss: 4.0650\n",
      "Epoch [3/4], Step [17625/61080], Loss: 4.0420\n",
      "Epoch [3/4], Step [17700/61080], Loss: 3.9887\n",
      "Epoch [3/4], Step [17775/61080], Loss: 3.8824\n",
      "Epoch [3/4], Step [17850/61080], Loss: 4.1295\n",
      "Epoch [3/4], Step [17925/61080], Loss: 4.1095\n",
      "Epoch [3/4], Step [18000/61080], Loss: 3.9706\n",
      "Validation perplexity: 41.63611115900079\n",
      "Epoch [3/4], Step [18075/61080], Loss: 4.0368\n",
      "Epoch [3/4], Step [18150/61080], Loss: 3.8815\n",
      "Epoch [3/4], Step [18225/61080], Loss: 3.9758\n",
      "Epoch [3/4], Step [18300/61080], Loss: 4.1240\n",
      "Epoch [3/4], Step [18375/61080], Loss: 3.9247\n",
      "Epoch [3/4], Step [18450/61080], Loss: 4.1654\n",
      "Epoch [3/4], Step [18525/61080], Loss: 3.9948\n",
      "Epoch [3/4], Step [18600/61080], Loss: 3.8861\n",
      "Epoch [3/4], Step [18675/61080], Loss: 3.8115\n",
      "Epoch [3/4], Step [18750/61080], Loss: 4.0004\n",
      "Epoch [3/4], Step [18825/61080], Loss: 3.8644\n",
      "Epoch [3/4], Step [18900/61080], Loss: 4.0263\n",
      "Epoch [3/4], Step [18975/61080], Loss: 4.1657\n",
      "Epoch [3/4], Step [19050/61080], Loss: 3.8180\n",
      "Epoch [3/4], Step [19125/61080], Loss: 3.8617\n",
      "Epoch [3/4], Step [19200/61080], Loss: 4.2183\n",
      "Epoch [3/4], Step [19275/61080], Loss: 4.0690\n",
      "Epoch [3/4], Step [19350/61080], Loss: 3.9549\n",
      "Epoch [3/4], Step [19425/61080], Loss: 4.1207\n",
      "Epoch [3/4], Step [19500/61080], Loss: 4.1771\n",
      "Epoch [3/4], Step [19575/61080], Loss: 3.7555\n",
      "Epoch [3/4], Step [19650/61080], Loss: 3.9412\n",
      "Epoch [3/4], Step [19725/61080], Loss: 3.8896\n",
      "Epoch [3/4], Step [19800/61080], Loss: 4.0012\n",
      "Epoch [3/4], Step [19875/61080], Loss: 4.1886\n",
      "Epoch [3/4], Step [19950/61080], Loss: 3.9930\n",
      "Epoch [3/4], Step [20025/61080], Loss: 3.9744\n",
      "Epoch [3/4], Step [20100/61080], Loss: 4.0651\n",
      "Epoch [3/4], Step [20175/61080], Loss: 4.0117\n",
      "Epoch [3/4], Step [20250/61080], Loss: 4.1447\n",
      "Epoch [3/4], Step [20325/61080], Loss: 4.1395\n",
      "Epoch [3/4], Step [20400/61080], Loss: 3.9129\n",
      "Epoch [3/4], Step [20475/61080], Loss: 3.8564\n",
      "Epoch [3/4], Step [20550/61080], Loss: 4.0833\n",
      "Epoch [3/4], Step [20625/61080], Loss: 4.0301\n",
      "Epoch [3/4], Step [20700/61080], Loss: 4.1842\n",
      "Epoch [3/4], Step [20775/61080], Loss: 4.0471\n",
      "Epoch [3/4], Step [20850/61080], Loss: 3.8983\n",
      "Epoch [3/4], Step [20925/61080], Loss: 4.0586\n",
      "Epoch [3/4], Step [21000/61080], Loss: 3.9316\n",
      "Validation perplexity: 41.735058114861666\n",
      "Epoch [3/4], Step [21075/61080], Loss: 4.1248\n",
      "Epoch [3/4], Step [21150/61080], Loss: 3.9131\n",
      "Epoch [3/4], Step [21225/61080], Loss: 4.0913\n",
      "Epoch [3/4], Step [21300/61080], Loss: 4.0229\n",
      "Epoch [3/4], Step [21375/61080], Loss: 3.9188\n",
      "Epoch [3/4], Step [21450/61080], Loss: 3.9965\n",
      "Epoch [3/4], Step [21525/61080], Loss: 3.9489\n",
      "Epoch [3/4], Step [21600/61080], Loss: 4.0440\n",
      "Epoch [3/4], Step [21675/61080], Loss: 4.0542\n",
      "Epoch [3/4], Step [21750/61080], Loss: 3.8285\n",
      "Epoch [3/4], Step [21825/61080], Loss: 3.9010\n",
      "Epoch [3/4], Step [21900/61080], Loss: 4.1669\n",
      "Epoch [3/4], Step [21975/61080], Loss: 4.1495\n",
      "Epoch [3/4], Step [22050/61080], Loss: 4.0637\n",
      "Epoch [3/4], Step [22125/61080], Loss: 4.1893\n",
      "Epoch [3/4], Step [22200/61080], Loss: 4.0749\n",
      "Epoch [3/4], Step [22275/61080], Loss: 4.0248\n",
      "Epoch [3/4], Step [22350/61080], Loss: 3.8161\n",
      "Epoch [3/4], Step [22425/61080], Loss: 3.7545\n",
      "Epoch [3/4], Step [22500/61080], Loss: 4.0133\n",
      "Epoch [3/4], Step [22575/61080], Loss: 4.0701\n",
      "Epoch [3/4], Step [22650/61080], Loss: 3.8550\n",
      "Epoch [3/4], Step [22725/61080], Loss: 3.8901\n",
      "Epoch [3/4], Step [22800/61080], Loss: 4.0763\n",
      "Epoch [3/4], Step [22875/61080], Loss: 4.0915\n",
      "Epoch [3/4], Step [22950/61080], Loss: 4.2278\n",
      "Epoch [3/4], Step [23025/61080], Loss: 4.1275\n",
      "Epoch [3/4], Step [23100/61080], Loss: 4.1080\n",
      "Epoch [3/4], Step [23175/61080], Loss: 4.0389\n",
      "Epoch [3/4], Step [23250/61080], Loss: 4.0777\n",
      "Epoch [3/4], Step [23325/61080], Loss: 4.0177\n",
      "Epoch [3/4], Step [23400/61080], Loss: 3.9070\n",
      "Epoch [3/4], Step [23475/61080], Loss: 4.0138\n",
      "Epoch [3/4], Step [23550/61080], Loss: 4.0275\n",
      "Epoch [3/4], Step [23625/61080], Loss: 4.1133\n",
      "Epoch [3/4], Step [23700/61080], Loss: 4.1509\n",
      "Epoch [3/4], Step [23775/61080], Loss: 3.9089\n",
      "Epoch [3/4], Step [23850/61080], Loss: 4.3057\n",
      "Epoch [3/4], Step [23925/61080], Loss: 3.8457\n",
      "Epoch [3/4], Step [24000/61080], Loss: 3.9513\n",
      "Validation perplexity: 41.49795839921857\n",
      "Epoch [3/4], Step [24075/61080], Loss: 3.9380\n",
      "Epoch [3/4], Step [24150/61080], Loss: 3.7828\n",
      "Epoch [3/4], Step [24225/61080], Loss: 3.9335\n",
      "Epoch [3/4], Step [24300/61080], Loss: 4.0329\n",
      "Epoch [3/4], Step [24375/61080], Loss: 4.2403\n",
      "Epoch [3/4], Step [24450/61080], Loss: 3.9894\n",
      "Epoch [3/4], Step [24525/61080], Loss: 4.2271\n",
      "Epoch [3/4], Step [24600/61080], Loss: 4.0010\n",
      "Epoch [3/4], Step [24675/61080], Loss: 3.9085\n",
      "Epoch [3/4], Step [24750/61080], Loss: 3.7983\n",
      "Epoch [3/4], Step [24825/61080], Loss: 3.8469\n",
      "Epoch [3/4], Step [24900/61080], Loss: 4.0335\n",
      "Epoch [3/4], Step [24975/61080], Loss: 3.9749\n",
      "Epoch [3/4], Step [25050/61080], Loss: 4.2500\n",
      "Epoch [3/4], Step [25125/61080], Loss: 4.2010\n",
      "Epoch [3/4], Step [25200/61080], Loss: 3.9170\n",
      "Epoch [3/4], Step [25275/61080], Loss: 3.9897\n",
      "Epoch [3/4], Step [25350/61080], Loss: 3.9579\n",
      "Epoch [3/4], Step [25425/61080], Loss: 3.8847\n",
      "Epoch [3/4], Step [25500/61080], Loss: 3.9404\n",
      "Epoch [3/4], Step [25575/61080], Loss: 3.9950\n",
      "Epoch [3/4], Step [25650/61080], Loss: 4.0481\n",
      "Epoch [3/4], Step [25725/61080], Loss: 3.8702\n",
      "Epoch [3/4], Step [25800/61080], Loss: 4.3122\n",
      "Epoch [3/4], Step [25875/61080], Loss: 4.0160\n",
      "Epoch [3/4], Step [25950/61080], Loss: 4.1429\n",
      "Epoch [3/4], Step [26025/61080], Loss: 4.0461\n",
      "Epoch [3/4], Step [26100/61080], Loss: 4.0388\n",
      "Epoch [3/4], Step [26175/61080], Loss: 4.1530\n",
      "Epoch [3/4], Step [26250/61080], Loss: 3.9467\n",
      "Epoch [3/4], Step [26325/61080], Loss: 3.7348\n",
      "Epoch [3/4], Step [26400/61080], Loss: 3.9210\n",
      "Epoch [3/4], Step [26475/61080], Loss: 4.0671\n",
      "Epoch [3/4], Step [26550/61080], Loss: 4.0208\n",
      "Epoch [3/4], Step [26625/61080], Loss: 4.0035\n",
      "Epoch [3/4], Step [26700/61080], Loss: 4.1355\n",
      "Epoch [3/4], Step [26775/61080], Loss: 4.1823\n",
      "Epoch [3/4], Step [26850/61080], Loss: 4.0237\n",
      "Epoch [3/4], Step [26925/61080], Loss: 3.8848\n",
      "Epoch [3/4], Step [27000/61080], Loss: 4.0982\n",
      "Validation perplexity: 41.5935650082201\n",
      "Epoch [3/4], Step [27075/61080], Loss: 4.0627\n",
      "Epoch [3/4], Step [27150/61080], Loss: 3.9443\n",
      "Epoch [3/4], Step [27225/61080], Loss: 3.9453\n",
      "Epoch [3/4], Step [27300/61080], Loss: 4.0361\n",
      "Epoch [3/4], Step [27375/61080], Loss: 3.9368\n",
      "Epoch [3/4], Step [27450/61080], Loss: 4.1704\n",
      "Epoch [3/4], Step [27525/61080], Loss: 3.8597\n",
      "Epoch [3/4], Step [27600/61080], Loss: 3.9340\n",
      "Epoch [3/4], Step [27675/61080], Loss: 3.8963\n",
      "Epoch [3/4], Step [27750/61080], Loss: 3.9629\n",
      "Epoch [3/4], Step [27825/61080], Loss: 4.0303\n",
      "Epoch [3/4], Step [27900/61080], Loss: 3.8005\n",
      "Epoch [3/4], Step [27975/61080], Loss: 4.0502\n",
      "Epoch [3/4], Step [28050/61080], Loss: 4.1262\n",
      "Epoch [3/4], Step [28125/61080], Loss: 3.8961\n",
      "Epoch [3/4], Step [28200/61080], Loss: 4.0746\n",
      "Epoch [3/4], Step [28275/61080], Loss: 4.0003\n",
      "Epoch [3/4], Step [28350/61080], Loss: 3.9792\n",
      "Epoch [3/4], Step [28425/61080], Loss: 3.9889\n",
      "Epoch [3/4], Step [28500/61080], Loss: 3.9443\n",
      "Epoch [3/4], Step [28575/61080], Loss: 4.0134\n",
      "Epoch [3/4], Step [28650/61080], Loss: 4.0043\n",
      "Epoch [3/4], Step [28725/61080], Loss: 4.1756\n",
      "Epoch [3/4], Step [28800/61080], Loss: 4.1148\n",
      "Epoch [3/4], Step [28875/61080], Loss: 3.7776\n",
      "Epoch [3/4], Step [28950/61080], Loss: 3.9593\n",
      "Epoch [3/4], Step [29025/61080], Loss: 4.1398\n",
      "Epoch [3/4], Step [29100/61080], Loss: 4.0963\n",
      "Epoch [3/4], Step [29175/61080], Loss: 3.9572\n",
      "Epoch [3/4], Step [29250/61080], Loss: 4.0588\n",
      "Epoch [3/4], Step [29325/61080], Loss: 3.9932\n",
      "Epoch [3/4], Step [29400/61080], Loss: 4.2666\n",
      "Epoch [3/4], Step [29475/61080], Loss: 3.9347\n",
      "Epoch [3/4], Step [29550/61080], Loss: 4.1527\n",
      "Epoch [3/4], Step [29625/61080], Loss: 3.9895\n",
      "Epoch [3/4], Step [29700/61080], Loss: 4.0906\n",
      "Epoch [3/4], Step [29775/61080], Loss: 4.0424\n",
      "Epoch [3/4], Step [29850/61080], Loss: 3.7716\n",
      "Epoch [3/4], Step [29925/61080], Loss: 4.0001\n",
      "Epoch [3/4], Step [30000/61080], Loss: 3.7297\n",
      "Validation perplexity: 41.29438279466684\n",
      "Epoch [3/4], Step [30075/61080], Loss: 3.8163\n",
      "Epoch [3/4], Step [30150/61080], Loss: 3.9120\n",
      "Epoch [3/4], Step [30225/61080], Loss: 3.9345\n",
      "Epoch [3/4], Step [30300/61080], Loss: 4.1577\n",
      "Epoch [3/4], Step [30375/61080], Loss: 4.0380\n",
      "Epoch [3/4], Step [30450/61080], Loss: 4.0095\n",
      "Epoch [3/4], Step [30525/61080], Loss: 4.2207\n",
      "Epoch [3/4], Step [30600/61080], Loss: 4.0984\n",
      "Epoch [3/4], Step [30675/61080], Loss: 3.8879\n",
      "Epoch [3/4], Step [30750/61080], Loss: 3.9947\n",
      "Epoch [3/4], Step [30825/61080], Loss: 3.9968\n",
      "Epoch [3/4], Step [30900/61080], Loss: 3.6941\n",
      "Epoch [3/4], Step [30975/61080], Loss: 4.0676\n",
      "Epoch [3/4], Step [31050/61080], Loss: 4.0937\n",
      "Epoch [3/4], Step [31125/61080], Loss: 4.0525\n",
      "Epoch [3/4], Step [31200/61080], Loss: 3.7691\n",
      "Epoch [3/4], Step [31275/61080], Loss: 3.9435\n",
      "Epoch [3/4], Step [31350/61080], Loss: 3.7611\n",
      "Epoch [3/4], Step [31425/61080], Loss: 4.0414\n",
      "Epoch [3/4], Step [31500/61080], Loss: 3.9089\n",
      "Epoch [3/4], Step [31575/61080], Loss: 4.2207\n",
      "Epoch [3/4], Step [31650/61080], Loss: 3.9807\n",
      "Epoch [3/4], Step [31725/61080], Loss: 3.9610\n",
      "Epoch [3/4], Step [31800/61080], Loss: 3.9379\n",
      "Epoch [3/4], Step [31875/61080], Loss: 4.0862\n",
      "Epoch [3/4], Step [31950/61080], Loss: 3.9320\n",
      "Epoch [3/4], Step [32025/61080], Loss: 4.2775\n",
      "Epoch [3/4], Step [32100/61080], Loss: 3.7875\n",
      "Epoch [3/4], Step [32175/61080], Loss: 3.8256\n",
      "Epoch [3/4], Step [32250/61080], Loss: 3.7772\n",
      "Epoch [3/4], Step [32325/61080], Loss: 4.0101\n",
      "Epoch [3/4], Step [32400/61080], Loss: 4.0941\n",
      "Epoch [3/4], Step [32475/61080], Loss: 4.0988\n",
      "Epoch [3/4], Step [32550/61080], Loss: 4.2889\n",
      "Epoch [3/4], Step [32625/61080], Loss: 3.9271\n",
      "Epoch [3/4], Step [32700/61080], Loss: 3.9651\n",
      "Epoch [3/4], Step [32775/61080], Loss: 4.0704\n",
      "Epoch [3/4], Step [32850/61080], Loss: 3.9786\n",
      "Epoch [3/4], Step [32925/61080], Loss: 3.8977\n",
      "Epoch [3/4], Step [33000/61080], Loss: 3.9503\n",
      "Validation perplexity: 41.158282317176926\n",
      "Epoch [3/4], Step [33075/61080], Loss: 3.7393\n",
      "Epoch [3/4], Step [33150/61080], Loss: 3.9068\n",
      "Epoch [3/4], Step [33225/61080], Loss: 3.7247\n",
      "Epoch [3/4], Step [33300/61080], Loss: 3.9080\n",
      "Epoch [3/4], Step [33375/61080], Loss: 3.9965\n",
      "Epoch [3/4], Step [33450/61080], Loss: 3.8673\n",
      "Epoch [3/4], Step [33525/61080], Loss: 4.0019\n",
      "Epoch [3/4], Step [33600/61080], Loss: 3.8943\n",
      "Epoch [3/4], Step [33675/61080], Loss: 4.0643\n",
      "Epoch [3/4], Step [33750/61080], Loss: 3.9475\n",
      "Epoch [3/4], Step [33825/61080], Loss: 3.9756\n",
      "Epoch [3/4], Step [33900/61080], Loss: 3.9524\n",
      "Epoch [3/4], Step [33975/61080], Loss: 4.0036\n",
      "Epoch [3/4], Step [34050/61080], Loss: 4.0557\n",
      "Epoch [3/4], Step [34125/61080], Loss: 3.9778\n",
      "Epoch [3/4], Step [34200/61080], Loss: 3.8790\n",
      "Epoch [3/4], Step [34275/61080], Loss: 4.0837\n",
      "Epoch [3/4], Step [34350/61080], Loss: 3.7746\n",
      "Epoch [3/4], Step [34425/61080], Loss: 3.9799\n",
      "Epoch [3/4], Step [34500/61080], Loss: 4.2018\n",
      "Epoch [3/4], Step [34575/61080], Loss: 4.1159\n",
      "Epoch [3/4], Step [34650/61080], Loss: 3.6850\n",
      "Epoch [3/4], Step [34725/61080], Loss: 3.7970\n",
      "Epoch [3/4], Step [34800/61080], Loss: 4.0752\n",
      "Epoch [3/4], Step [34875/61080], Loss: 4.2966\n",
      "Epoch [3/4], Step [34950/61080], Loss: 3.9264\n",
      "Epoch [3/4], Step [35025/61080], Loss: 3.7777\n",
      "Epoch [3/4], Step [35100/61080], Loss: 4.0085\n",
      "Epoch [3/4], Step [35175/61080], Loss: 3.8716\n",
      "Epoch [3/4], Step [35250/61080], Loss: 3.6836\n",
      "Epoch [3/4], Step [35325/61080], Loss: 3.9332\n",
      "Epoch [3/4], Step [35400/61080], Loss: 4.0326\n",
      "Epoch [3/4], Step [35475/61080], Loss: 3.8776\n",
      "Epoch [3/4], Step [35550/61080], Loss: 4.1706\n",
      "Epoch [3/4], Step [35625/61080], Loss: 4.0091\n",
      "Epoch [3/4], Step [35700/61080], Loss: 3.9092\n",
      "Epoch [3/4], Step [35775/61080], Loss: 3.9884\n",
      "Epoch [3/4], Step [35850/61080], Loss: 3.9816\n",
      "Epoch [3/4], Step [35925/61080], Loss: 3.8300\n",
      "Epoch [3/4], Step [36000/61080], Loss: 3.9865\n",
      "Validation perplexity: 41.125932646822235\n",
      "Epoch [3/4], Step [36075/61080], Loss: 4.0673\n",
      "Epoch [3/4], Step [36150/61080], Loss: 3.9611\n",
      "Epoch [3/4], Step [36225/61080], Loss: 3.8176\n",
      "Epoch [3/4], Step [36300/61080], Loss: 4.0311\n",
      "Epoch [3/4], Step [36375/61080], Loss: 3.7157\n",
      "Epoch [3/4], Step [36450/61080], Loss: 3.8660\n",
      "Epoch [3/4], Step [36525/61080], Loss: 4.2934\n",
      "Epoch [3/4], Step [36600/61080], Loss: 3.7684\n",
      "Epoch [3/4], Step [36675/61080], Loss: 3.9673\n",
      "Epoch [3/4], Step [36750/61080], Loss: 3.9242\n",
      "Epoch [3/4], Step [36825/61080], Loss: 3.8608\n",
      "Epoch [3/4], Step [36900/61080], Loss: 3.8203\n",
      "Epoch [3/4], Step [36975/61080], Loss: 3.9691\n",
      "Epoch [3/4], Step [37050/61080], Loss: 3.8713\n",
      "Epoch [3/4], Step [37125/61080], Loss: 4.1047\n",
      "Epoch [3/4], Step [37200/61080], Loss: 3.6791\n",
      "Epoch [3/4], Step [37275/61080], Loss: 3.9221\n",
      "Epoch [3/4], Step [37350/61080], Loss: 4.0363\n",
      "Epoch [3/4], Step [37425/61080], Loss: 3.9576\n",
      "Epoch [3/4], Step [37500/61080], Loss: 3.8852\n",
      "Epoch [3/4], Step [37575/61080], Loss: 3.9288\n",
      "Epoch [3/4], Step [37650/61080], Loss: 4.0087\n",
      "Epoch [3/4], Step [37725/61080], Loss: 3.9940\n",
      "Epoch [3/4], Step [37800/61080], Loss: 3.9435\n",
      "Epoch [3/4], Step [37875/61080], Loss: 3.8303\n",
      "Epoch [3/4], Step [37950/61080], Loss: 4.1511\n",
      "Epoch [3/4], Step [38025/61080], Loss: 3.8037\n",
      "Epoch [3/4], Step [38100/61080], Loss: 4.2650\n",
      "Epoch [3/4], Step [38175/61080], Loss: 3.7048\n",
      "Epoch [3/4], Step [38250/61080], Loss: 4.2032\n",
      "Epoch [3/4], Step [38325/61080], Loss: 4.0000\n",
      "Epoch [3/4], Step [38400/61080], Loss: 3.6580\n",
      "Epoch [3/4], Step [38475/61080], Loss: 4.0253\n",
      "Epoch [3/4], Step [38550/61080], Loss: 4.1138\n",
      "Epoch [3/4], Step [38625/61080], Loss: 4.1514\n",
      "Epoch [3/4], Step [38700/61080], Loss: 4.0028\n",
      "Epoch [3/4], Step [38775/61080], Loss: 3.9198\n",
      "Epoch [3/4], Step [38850/61080], Loss: 4.0612\n",
      "Epoch [3/4], Step [38925/61080], Loss: 3.9893\n",
      "Epoch [3/4], Step [39000/61080], Loss: 4.1231\n",
      "Validation perplexity: 41.06500766628904\n",
      "Epoch [3/4], Step [39075/61080], Loss: 3.8986\n",
      "Epoch [3/4], Step [39150/61080], Loss: 3.9439\n",
      "Epoch [3/4], Step [39225/61080], Loss: 3.8334\n",
      "Epoch [3/4], Step [39300/61080], Loss: 3.9569\n",
      "Epoch [3/4], Step [39375/61080], Loss: 3.9104\n",
      "Epoch [3/4], Step [39450/61080], Loss: 3.8509\n",
      "Epoch [3/4], Step [39525/61080], Loss: 4.0143\n",
      "Epoch [3/4], Step [39600/61080], Loss: 4.2732\n",
      "Epoch [3/4], Step [39675/61080], Loss: 3.9363\n",
      "Epoch [3/4], Step [39750/61080], Loss: 3.9189\n",
      "Epoch [3/4], Step [39825/61080], Loss: 3.9524\n",
      "Epoch [3/4], Step [39900/61080], Loss: 3.8355\n",
      "Epoch [3/4], Step [39975/61080], Loss: 3.9242\n",
      "Epoch [3/4], Step [40050/61080], Loss: 4.0516\n",
      "Epoch [3/4], Step [40125/61080], Loss: 4.0957\n",
      "Epoch [3/4], Step [40200/61080], Loss: 3.7697\n",
      "Epoch [3/4], Step [40275/61080], Loss: 3.9790\n",
      "Epoch [3/4], Step [40350/61080], Loss: 4.0597\n",
      "Epoch [3/4], Step [40425/61080], Loss: 3.8905\n",
      "Epoch [3/4], Step [40500/61080], Loss: 3.8102\n",
      "Epoch [3/4], Step [40575/61080], Loss: 3.7229\n",
      "Epoch [3/4], Step [40650/61080], Loss: 3.9927\n",
      "Epoch [3/4], Step [40725/61080], Loss: 4.1940\n",
      "Epoch [3/4], Step [40800/61080], Loss: 3.8361\n",
      "Epoch [3/4], Step [40875/61080], Loss: 3.8500\n",
      "Epoch [3/4], Step [40950/61080], Loss: 3.8741\n",
      "Epoch [3/4], Step [41025/61080], Loss: 3.8086\n",
      "Epoch [3/4], Step [41100/61080], Loss: 4.1408\n",
      "Epoch [3/4], Step [41175/61080], Loss: 3.6673\n",
      "Epoch [3/4], Step [41250/61080], Loss: 4.0048\n",
      "Epoch [3/4], Step [41325/61080], Loss: 3.8739\n",
      "Epoch [3/4], Step [41400/61080], Loss: 4.1565\n",
      "Epoch [3/4], Step [41475/61080], Loss: 3.9740\n",
      "Epoch [3/4], Step [41550/61080], Loss: 4.0660\n",
      "Epoch [3/4], Step [41625/61080], Loss: 3.7154\n",
      "Epoch [3/4], Step [41700/61080], Loss: 4.1613\n",
      "Epoch [3/4], Step [41775/61080], Loss: 4.1151\n",
      "Epoch [3/4], Step [41850/61080], Loss: 4.1434\n",
      "Epoch [3/4], Step [41925/61080], Loss: 3.8428\n",
      "Epoch [3/4], Step [42000/61080], Loss: 3.9222\n",
      "Validation perplexity: 41.005710272590754\n",
      "Epoch [3/4], Step [42075/61080], Loss: 4.1260\n",
      "Epoch [3/4], Step [42150/61080], Loss: 4.1703\n",
      "Epoch [3/4], Step [42225/61080], Loss: 3.9301\n",
      "Epoch [3/4], Step [42300/61080], Loss: 3.8952\n",
      "Epoch [3/4], Step [42375/61080], Loss: 3.8319\n",
      "Epoch [3/4], Step [42450/61080], Loss: 4.0172\n",
      "Epoch [3/4], Step [42525/61080], Loss: 4.0261\n",
      "Epoch [3/4], Step [42600/61080], Loss: 4.0175\n",
      "Epoch [3/4], Step [42675/61080], Loss: 3.8278\n",
      "Epoch [3/4], Step [42750/61080], Loss: 3.8671\n",
      "Epoch [3/4], Step [42825/61080], Loss: 3.8529\n",
      "Epoch [3/4], Step [42900/61080], Loss: 4.2045\n",
      "Epoch [3/4], Step [42975/61080], Loss: 3.9251\n",
      "Epoch [3/4], Step [43050/61080], Loss: 4.0634\n",
      "Epoch [3/4], Step [43125/61080], Loss: 4.0971\n",
      "Epoch [3/4], Step [43200/61080], Loss: 3.9851\n",
      "Epoch [3/4], Step [43275/61080], Loss: 4.0571\n",
      "Epoch [3/4], Step [43350/61080], Loss: 3.8517\n",
      "Epoch [3/4], Step [43425/61080], Loss: 3.8951\n",
      "Epoch [3/4], Step [43500/61080], Loss: 3.9267\n",
      "Epoch [3/4], Step [43575/61080], Loss: 4.0184\n",
      "Epoch [3/4], Step [43650/61080], Loss: 4.2111\n",
      "Epoch [3/4], Step [43725/61080], Loss: 4.1512\n",
      "Epoch [3/4], Step [43800/61080], Loss: 3.9756\n",
      "Epoch [3/4], Step [43875/61080], Loss: 3.8863\n",
      "Epoch [3/4], Step [43950/61080], Loss: 3.9234\n",
      "Epoch [3/4], Step [44025/61080], Loss: 4.0406\n",
      "Epoch [3/4], Step [44100/61080], Loss: 3.9229\n",
      "Epoch [3/4], Step [44175/61080], Loss: 3.9259\n",
      "Epoch [3/4], Step [44250/61080], Loss: 3.9206\n",
      "Epoch [3/4], Step [44325/61080], Loss: 3.9994\n",
      "Epoch [3/4], Step [44400/61080], Loss: 4.0561\n",
      "Epoch [3/4], Step [44475/61080], Loss: 3.9832\n",
      "Epoch [3/4], Step [44550/61080], Loss: 3.7270\n",
      "Epoch [3/4], Step [44625/61080], Loss: 3.9483\n",
      "Epoch [3/4], Step [44700/61080], Loss: 4.0315\n",
      "Epoch [3/4], Step [44775/61080], Loss: 3.9212\n",
      "Epoch [3/4], Step [44850/61080], Loss: 3.9771\n",
      "Epoch [3/4], Step [44925/61080], Loss: 4.0089\n",
      "Epoch [3/4], Step [45000/61080], Loss: 3.9572\n",
      "Validation perplexity: 40.958942510930726\n",
      "Epoch [3/4], Step [45075/61080], Loss: 3.9853\n",
      "Epoch [3/4], Step [45150/61080], Loss: 3.9819\n",
      "Epoch [3/4], Step [45225/61080], Loss: 4.1843\n",
      "Epoch [3/4], Step [45300/61080], Loss: 3.6542\n",
      "Epoch [3/4], Step [45375/61080], Loss: 3.8671\n",
      "Epoch [3/4], Step [45450/61080], Loss: 3.7636\n",
      "Epoch [3/4], Step [45525/61080], Loss: 3.8891\n",
      "Epoch [3/4], Step [45600/61080], Loss: 3.9247\n",
      "Epoch [3/4], Step [45675/61080], Loss: 3.8633\n",
      "Epoch [3/4], Step [45750/61080], Loss: 3.8609\n",
      "Epoch [3/4], Step [45825/61080], Loss: 3.8732\n",
      "Epoch [3/4], Step [45900/61080], Loss: 3.8505\n",
      "Epoch [3/4], Step [45975/61080], Loss: 3.9965\n",
      "Epoch [3/4], Step [46050/61080], Loss: 3.8298\n",
      "Epoch [3/4], Step [46125/61080], Loss: 3.8581\n",
      "Epoch [3/4], Step [46200/61080], Loss: 3.8115\n",
      "Epoch [3/4], Step [46275/61080], Loss: 3.9609\n",
      "Epoch [3/4], Step [46350/61080], Loss: 3.6768\n",
      "Epoch [3/4], Step [46425/61080], Loss: 4.0273\n",
      "Epoch [3/4], Step [46500/61080], Loss: 4.0304\n",
      "Epoch [3/4], Step [46575/61080], Loss: 4.1276\n",
      "Epoch [3/4], Step [46650/61080], Loss: 4.0273\n",
      "Epoch [3/4], Step [46725/61080], Loss: 3.8476\n",
      "Epoch [3/4], Step [46800/61080], Loss: 3.8504\n",
      "Epoch [3/4], Step [46875/61080], Loss: 3.8126\n",
      "Epoch [3/4], Step [46950/61080], Loss: 3.8292\n",
      "Epoch [3/4], Step [47025/61080], Loss: 3.9732\n",
      "Epoch [3/4], Step [47100/61080], Loss: 3.9801\n",
      "Epoch [3/4], Step [47175/61080], Loss: 3.9367\n",
      "Epoch [3/4], Step [47250/61080], Loss: 3.8223\n",
      "Epoch [3/4], Step [47325/61080], Loss: 4.1302\n",
      "Epoch [3/4], Step [47400/61080], Loss: 3.9901\n",
      "Epoch [3/4], Step [47475/61080], Loss: 4.0250\n",
      "Epoch [3/4], Step [47550/61080], Loss: 4.0018\n",
      "Epoch [3/4], Step [47625/61080], Loss: 3.9345\n",
      "Epoch [3/4], Step [47700/61080], Loss: 3.9162\n",
      "Epoch [3/4], Step [47775/61080], Loss: 3.8115\n",
      "Epoch [3/4], Step [47850/61080], Loss: 3.8105\n",
      "Epoch [3/4], Step [47925/61080], Loss: 4.0226\n",
      "Epoch [3/4], Step [48000/61080], Loss: 3.9012\n",
      "Validation perplexity: 40.55674163125278\n",
      "Epoch [3/4], Step [48075/61080], Loss: 4.0908\n",
      "Epoch [3/4], Step [48150/61080], Loss: 4.2320\n",
      "Epoch [3/4], Step [48225/61080], Loss: 4.1660\n",
      "Epoch [3/4], Step [48300/61080], Loss: 3.8718\n",
      "Epoch [3/4], Step [48375/61080], Loss: 4.1224\n",
      "Epoch [3/4], Step [48450/61080], Loss: 3.9310\n",
      "Epoch [3/4], Step [48525/61080], Loss: 4.0132\n",
      "Epoch [3/4], Step [48600/61080], Loss: 4.0935\n",
      "Epoch [3/4], Step [48675/61080], Loss: 3.9189\n",
      "Epoch [3/4], Step [48750/61080], Loss: 3.8528\n",
      "Epoch [3/4], Step [48825/61080], Loss: 4.0846\n",
      "Epoch [3/4], Step [48900/61080], Loss: 4.0065\n",
      "Epoch [3/4], Step [48975/61080], Loss: 3.8001\n",
      "Epoch [3/4], Step [49050/61080], Loss: 4.2574\n",
      "Epoch [3/4], Step [49125/61080], Loss: 3.9920\n",
      "Epoch [3/4], Step [49200/61080], Loss: 3.7357\n",
      "Epoch [3/4], Step [49275/61080], Loss: 3.9609\n",
      "Epoch [3/4], Step [49350/61080], Loss: 3.9225\n",
      "Epoch [3/4], Step [49425/61080], Loss: 4.1906\n",
      "Epoch [3/4], Step [49500/61080], Loss: 4.0552\n",
      "Epoch [3/4], Step [49575/61080], Loss: 3.9952\n",
      "Epoch [3/4], Step [49650/61080], Loss: 3.7026\n",
      "Epoch [3/4], Step [49725/61080], Loss: 4.1854\n",
      "Epoch [3/4], Step [49800/61080], Loss: 3.8670\n",
      "Epoch [3/4], Step [49875/61080], Loss: 3.8883\n",
      "Epoch [3/4], Step [49950/61080], Loss: 4.1266\n",
      "Epoch [3/4], Step [50025/61080], Loss: 3.9581\n",
      "Epoch [3/4], Step [50100/61080], Loss: 3.9600\n",
      "Epoch [3/4], Step [50175/61080], Loss: 3.9461\n",
      "Epoch [3/4], Step [50250/61080], Loss: 3.7947\n",
      "Epoch [3/4], Step [50325/61080], Loss: 3.9233\n",
      "Epoch [3/4], Step [50400/61080], Loss: 4.1906\n",
      "Epoch [3/4], Step [50475/61080], Loss: 3.9325\n",
      "Epoch [3/4], Step [50550/61080], Loss: 4.1090\n",
      "Epoch [3/4], Step [50625/61080], Loss: 3.7088\n",
      "Epoch [3/4], Step [50700/61080], Loss: 4.1152\n",
      "Epoch [3/4], Step [50775/61080], Loss: 4.1347\n",
      "Epoch [3/4], Step [50850/61080], Loss: 4.1332\n",
      "Epoch [3/4], Step [50925/61080], Loss: 3.9760\n",
      "Epoch [3/4], Step [51000/61080], Loss: 3.9331\n",
      "Validation perplexity: 40.673367763179336\n",
      "Epoch [3/4], Step [51075/61080], Loss: 3.8396\n",
      "Epoch [3/4], Step [51150/61080], Loss: 4.1037\n",
      "Epoch [3/4], Step [51225/61080], Loss: 3.8399\n",
      "Epoch [3/4], Step [51300/61080], Loss: 3.9131\n",
      "Epoch [3/4], Step [51375/61080], Loss: 3.8392\n",
      "Epoch [3/4], Step [51450/61080], Loss: 3.6823\n",
      "Epoch [3/4], Step [51525/61080], Loss: 3.8154\n",
      "Epoch [3/4], Step [51600/61080], Loss: 3.9027\n",
      "Epoch [3/4], Step [51675/61080], Loss: 4.1120\n",
      "Epoch [3/4], Step [51750/61080], Loss: 3.7521\n",
      "Epoch [3/4], Step [51825/61080], Loss: 3.8620\n",
      "Epoch [3/4], Step [51900/61080], Loss: 3.9650\n",
      "Epoch [3/4], Step [51975/61080], Loss: 4.1831\n",
      "Epoch [3/4], Step [52050/61080], Loss: 4.0376\n",
      "Epoch [3/4], Step [52125/61080], Loss: 3.9641\n",
      "Epoch [3/4], Step [52200/61080], Loss: 3.7461\n",
      "Epoch [3/4], Step [52275/61080], Loss: 4.0180\n",
      "Epoch [3/4], Step [52350/61080], Loss: 4.0636\n",
      "Epoch [3/4], Step [52425/61080], Loss: 3.9354\n",
      "Epoch [3/4], Step [52500/61080], Loss: 3.9752\n",
      "Epoch [3/4], Step [52575/61080], Loss: 3.9966\n",
      "Epoch [3/4], Step [52650/61080], Loss: 3.9631\n",
      "Epoch [3/4], Step [52725/61080], Loss: 3.8589\n",
      "Epoch [3/4], Step [52800/61080], Loss: 3.9676\n",
      "Epoch [3/4], Step [52875/61080], Loss: 4.2529\n",
      "Epoch [3/4], Step [52950/61080], Loss: 3.9982\n",
      "Epoch [3/4], Step [53025/61080], Loss: 4.1042\n",
      "Epoch [3/4], Step [53100/61080], Loss: 3.8491\n",
      "Epoch [3/4], Step [53175/61080], Loss: 3.8954\n",
      "Epoch [3/4], Step [53250/61080], Loss: 3.8599\n",
      "Epoch [3/4], Step [53325/61080], Loss: 3.9718\n",
      "Epoch [3/4], Step [53400/61080], Loss: 4.0262\n",
      "Epoch [3/4], Step [53475/61080], Loss: 3.9576\n",
      "Epoch [3/4], Step [53550/61080], Loss: 3.9089\n",
      "Epoch [3/4], Step [53625/61080], Loss: 3.7013\n",
      "Epoch [3/4], Step [53700/61080], Loss: 4.1699\n",
      "Epoch [3/4], Step [53775/61080], Loss: 4.3044\n",
      "Epoch [3/4], Step [53850/61080], Loss: 3.8418\n",
      "Epoch [3/4], Step [53925/61080], Loss: 3.6226\n",
      "Epoch [3/4], Step [54000/61080], Loss: 3.8494\n",
      "Validation perplexity: 40.52323201332093\n",
      "Epoch [3/4], Step [54075/61080], Loss: 4.0177\n",
      "Epoch [3/4], Step [54150/61080], Loss: 4.2177\n",
      "Epoch [3/4], Step [54225/61080], Loss: 3.7308\n",
      "Epoch [3/4], Step [54300/61080], Loss: 4.0661\n",
      "Epoch [3/4], Step [54375/61080], Loss: 4.2035\n",
      "Epoch [3/4], Step [54450/61080], Loss: 3.8378\n",
      "Epoch [3/4], Step [54525/61080], Loss: 3.8301\n",
      "Epoch [3/4], Step [54600/61080], Loss: 3.9305\n",
      "Epoch [3/4], Step [54675/61080], Loss: 3.9285\n",
      "Epoch [3/4], Step [54750/61080], Loss: 3.8455\n",
      "Epoch [3/4], Step [54825/61080], Loss: 3.9893\n",
      "Epoch [3/4], Step [54900/61080], Loss: 3.9790\n",
      "Epoch [3/4], Step [54975/61080], Loss: 3.7725\n",
      "Epoch [3/4], Step [55050/61080], Loss: 3.9914\n",
      "Epoch [3/4], Step [55125/61080], Loss: 3.9251\n",
      "Epoch [3/4], Step [55200/61080], Loss: 3.9366\n",
      "Epoch [3/4], Step [55275/61080], Loss: 3.9917\n",
      "Epoch [3/4], Step [55350/61080], Loss: 3.8567\n",
      "Epoch [3/4], Step [55425/61080], Loss: 4.0051\n",
      "Epoch [3/4], Step [55500/61080], Loss: 3.9386\n",
      "Epoch [3/4], Step [55575/61080], Loss: 3.9240\n",
      "Epoch [3/4], Step [55650/61080], Loss: 3.9611\n",
      "Epoch [3/4], Step [55725/61080], Loss: 4.0457\n",
      "Epoch [3/4], Step [55800/61080], Loss: 3.9301\n",
      "Epoch [3/4], Step [55875/61080], Loss: 3.9273\n",
      "Epoch [3/4], Step [55950/61080], Loss: 3.9554\n",
      "Epoch [3/4], Step [56025/61080], Loss: 3.9336\n",
      "Epoch [3/4], Step [56100/61080], Loss: 3.8970\n",
      "Epoch [3/4], Step [56175/61080], Loss: 3.9971\n",
      "Epoch [3/4], Step [56250/61080], Loss: 3.9361\n",
      "Epoch [3/4], Step [56325/61080], Loss: 3.7335\n",
      "Epoch [3/4], Step [56400/61080], Loss: 4.0957\n",
      "Epoch [3/4], Step [56475/61080], Loss: 3.9486\n",
      "Epoch [3/4], Step [56550/61080], Loss: 4.0799\n",
      "Epoch [3/4], Step [56625/61080], Loss: 3.8111\n",
      "Epoch [3/4], Step [56700/61080], Loss: 4.0243\n",
      "Epoch [3/4], Step [56775/61080], Loss: 3.7460\n",
      "Epoch [3/4], Step [56850/61080], Loss: 3.7890\n",
      "Epoch [3/4], Step [56925/61080], Loss: 3.8348\n",
      "Epoch [3/4], Step [57000/61080], Loss: 3.8443\n",
      "Validation perplexity: 40.483427019692954\n",
      "Epoch [3/4], Step [57075/61080], Loss: 3.9624\n",
      "Epoch [3/4], Step [57150/61080], Loss: 3.7663\n",
      "Epoch [3/4], Step [57225/61080], Loss: 3.9004\n",
      "Epoch [3/4], Step [57300/61080], Loss: 3.7478\n",
      "Epoch [3/4], Step [57375/61080], Loss: 4.0040\n",
      "Epoch [3/4], Step [57450/61080], Loss: 4.0394\n",
      "Epoch [3/4], Step [57525/61080], Loss: 4.0524\n",
      "Epoch [3/4], Step [57600/61080], Loss: 4.0295\n",
      "Epoch [3/4], Step [57675/61080], Loss: 4.0497\n",
      "Epoch [3/4], Step [57750/61080], Loss: 4.0278\n",
      "Epoch [3/4], Step [57825/61080], Loss: 4.1187\n",
      "Epoch [3/4], Step [57900/61080], Loss: 4.0193\n",
      "Epoch [3/4], Step [57975/61080], Loss: 3.5747\n",
      "Epoch [3/4], Step [58050/61080], Loss: 3.7827\n",
      "Epoch [3/4], Step [58125/61080], Loss: 3.9873\n",
      "Epoch [3/4], Step [58200/61080], Loss: 3.7531\n",
      "Epoch [3/4], Step [58275/61080], Loss: 3.6910\n",
      "Epoch [3/4], Step [58350/61080], Loss: 3.8922\n",
      "Epoch [3/4], Step [58425/61080], Loss: 3.7606\n",
      "Epoch [3/4], Step [58500/61080], Loss: 4.0243\n",
      "Epoch [3/4], Step [58575/61080], Loss: 4.1868\n",
      "Epoch [3/4], Step [58650/61080], Loss: 3.9841\n",
      "Epoch [3/4], Step [58725/61080], Loss: 3.9012\n",
      "Epoch [3/4], Step [58800/61080], Loss: 3.9364\n",
      "Epoch [3/4], Step [58875/61080], Loss: 4.0156\n",
      "Epoch [3/4], Step [58950/61080], Loss: 3.7724\n",
      "Epoch [3/4], Step [59025/61080], Loss: 3.8307\n",
      "Epoch [3/4], Step [59100/61080], Loss: 4.1410\n",
      "Epoch [3/4], Step [59175/61080], Loss: 3.8533\n",
      "Epoch [3/4], Step [59250/61080], Loss: 4.0223\n",
      "Epoch [3/4], Step [59325/61080], Loss: 3.8408\n",
      "Epoch [3/4], Step [59400/61080], Loss: 3.9852\n",
      "Epoch [3/4], Step [59475/61080], Loss: 4.0552\n",
      "Epoch [3/4], Step [59550/61080], Loss: 3.9162\n",
      "Epoch [3/4], Step [59625/61080], Loss: 4.1624\n",
      "Epoch [3/4], Step [59700/61080], Loss: 4.0613\n",
      "Epoch [3/4], Step [59775/61080], Loss: 4.0051\n",
      "Epoch [3/4], Step [59850/61080], Loss: 3.9650\n",
      "Epoch [3/4], Step [59925/61080], Loss: 4.1852\n",
      "Epoch [3/4], Step [60000/61080], Loss: 3.8253\n",
      "Validation perplexity: 40.46362008692035\n",
      "Epoch [3/4], Step [60075/61080], Loss: 3.8296\n",
      "Epoch [3/4], Step [60150/61080], Loss: 4.2122\n",
      "Epoch [3/4], Step [60225/61080], Loss: 4.2628\n",
      "Epoch [3/4], Step [60300/61080], Loss: 4.1563\n",
      "Epoch [3/4], Step [60375/61080], Loss: 4.1845\n",
      "Epoch [3/4], Step [60450/61080], Loss: 3.8404\n",
      "Epoch [3/4], Step [60525/61080], Loss: 3.9332\n",
      "Epoch [3/4], Step [60600/61080], Loss: 3.8259\n",
      "Epoch [3/4], Step [60675/61080], Loss: 4.2203\n",
      "Epoch [3/4], Step [60750/61080], Loss: 3.7403\n",
      "Epoch [3/4], Step [60825/61080], Loss: 3.8375\n",
      "Epoch [3/4], Step [60900/61080], Loss: 3.9186\n",
      "Epoch [3/4], Step [60975/61080], Loss: 3.9280\n",
      "Epoch [3/4], Step [61050/61080], Loss: 4.0121\n",
      "Epoch [3/4] Average Loss: 3.9827, Perplexity: 53.66\n",
      "Epoch [4/4], Step [0/61080], Loss: 3.8429\n",
      "Validation perplexity: 40.28918544681805\n",
      "Epoch [4/4], Step [75/61080], Loss: 3.7183\n",
      "Epoch [4/4], Step [150/61080], Loss: 3.8074\n",
      "Epoch [4/4], Step [225/61080], Loss: 3.7814\n",
      "Epoch [4/4], Step [300/61080], Loss: 3.8154\n",
      "Epoch [4/4], Step [375/61080], Loss: 4.1177\n",
      "Epoch [4/4], Step [450/61080], Loss: 4.1597\n",
      "Epoch [4/4], Step [525/61080], Loss: 3.8492\n",
      "Epoch [4/4], Step [600/61080], Loss: 4.2444\n",
      "Epoch [4/4], Step [675/61080], Loss: 4.0059\n",
      "Epoch [4/4], Step [750/61080], Loss: 3.8004\n",
      "Epoch [4/4], Step [825/61080], Loss: 4.0075\n",
      "Epoch [4/4], Step [900/61080], Loss: 3.9001\n",
      "Epoch [4/4], Step [975/61080], Loss: 3.7288\n",
      "Epoch [4/4], Step [1050/61080], Loss: 4.1484\n",
      "Epoch [4/4], Step [1125/61080], Loss: 4.0813\n",
      "Epoch [4/4], Step [1200/61080], Loss: 3.8572\n",
      "Epoch [4/4], Step [1275/61080], Loss: 3.8895\n",
      "Epoch [4/4], Step [1350/61080], Loss: 3.8088\n",
      "Epoch [4/4], Step [1425/61080], Loss: 3.9307\n",
      "Epoch [4/4], Step [1500/61080], Loss: 4.0215\n",
      "Epoch [4/4], Step [1575/61080], Loss: 3.9610\n",
      "Epoch [4/4], Step [1650/61080], Loss: 3.8830\n",
      "Epoch [4/4], Step [1725/61080], Loss: 3.8633\n",
      "Epoch [4/4], Step [1800/61080], Loss: 3.7450\n",
      "Epoch [4/4], Step [1875/61080], Loss: 3.7358\n",
      "Epoch [4/4], Step [1950/61080], Loss: 4.1766\n",
      "Epoch [4/4], Step [2025/61080], Loss: 4.1219\n",
      "Epoch [4/4], Step [2100/61080], Loss: 4.0063\n",
      "Epoch [4/4], Step [2175/61080], Loss: 3.8775\n",
      "Epoch [4/4], Step [2250/61080], Loss: 3.8803\n",
      "Epoch [4/4], Step [2325/61080], Loss: 4.0287\n",
      "Epoch [4/4], Step [2400/61080], Loss: 3.6238\n",
      "Epoch [4/4], Step [2475/61080], Loss: 4.0932\n",
      "Epoch [4/4], Step [2550/61080], Loss: 3.8745\n",
      "Epoch [4/4], Step [2625/61080], Loss: 3.7702\n",
      "Epoch [4/4], Step [2700/61080], Loss: 3.7519\n",
      "Epoch [4/4], Step [2775/61080], Loss: 4.1828\n",
      "Epoch [4/4], Step [2850/61080], Loss: 3.8719\n",
      "Epoch [4/4], Step [2925/61080], Loss: 4.0059\n",
      "Epoch [4/4], Step [3000/61080], Loss: 4.0779\n",
      "Validation perplexity: 40.13893705475468\n",
      "Epoch [4/4], Step [3075/61080], Loss: 4.2142\n",
      "Epoch [4/4], Step [3150/61080], Loss: 3.9374\n",
      "Epoch [4/4], Step [3225/61080], Loss: 3.7648\n",
      "Epoch [4/4], Step [3300/61080], Loss: 3.8107\n",
      "Epoch [4/4], Step [3375/61080], Loss: 3.9602\n",
      "Epoch [4/4], Step [3450/61080], Loss: 4.1227\n",
      "Epoch [4/4], Step [3525/61080], Loss: 4.1563\n",
      "Epoch [4/4], Step [3600/61080], Loss: 3.8369\n",
      "Epoch [4/4], Step [3675/61080], Loss: 3.7727\n",
      "Epoch [4/4], Step [3750/61080], Loss: 3.9776\n",
      "Epoch [4/4], Step [3825/61080], Loss: 3.7851\n",
      "Epoch [4/4], Step [3900/61080], Loss: 3.9134\n",
      "Epoch [4/4], Step [3975/61080], Loss: 3.8961\n",
      "Epoch [4/4], Step [4050/61080], Loss: 4.1900\n",
      "Epoch [4/4], Step [4125/61080], Loss: 4.0708\n",
      "Epoch [4/4], Step [4200/61080], Loss: 3.8069\n",
      "Epoch [4/4], Step [4275/61080], Loss: 4.1479\n",
      "Epoch [4/4], Step [4350/61080], Loss: 4.0921\n",
      "Epoch [4/4], Step [4425/61080], Loss: 3.9140\n",
      "Epoch [4/4], Step [4500/61080], Loss: 3.8908\n",
      "Epoch [4/4], Step [4575/61080], Loss: 3.7175\n",
      "Epoch [4/4], Step [4650/61080], Loss: 3.9390\n",
      "Epoch [4/4], Step [4725/61080], Loss: 3.9832\n",
      "Epoch [4/4], Step [4800/61080], Loss: 4.0104\n",
      "Epoch [4/4], Step [4875/61080], Loss: 4.0286\n",
      "Epoch [4/4], Step [4950/61080], Loss: 3.8488\n",
      "Epoch [4/4], Step [5025/61080], Loss: 3.8960\n",
      "Epoch [4/4], Step [5100/61080], Loss: 3.9297\n",
      "Epoch [4/4], Step [5175/61080], Loss: 3.7746\n",
      "Epoch [4/4], Step [5250/61080], Loss: 3.8507\n",
      "Epoch [4/4], Step [5325/61080], Loss: 3.7979\n",
      "Epoch [4/4], Step [5400/61080], Loss: 3.8389\n",
      "Epoch [4/4], Step [5475/61080], Loss: 3.7903\n",
      "Epoch [4/4], Step [5550/61080], Loss: 3.9550\n",
      "Epoch [4/4], Step [5625/61080], Loss: 3.9471\n",
      "Epoch [4/4], Step [5700/61080], Loss: 4.2055\n",
      "Epoch [4/4], Step [5775/61080], Loss: 3.8254\n",
      "Epoch [4/4], Step [5850/61080], Loss: 3.8003\n",
      "Epoch [4/4], Step [5925/61080], Loss: 4.0267\n",
      "Epoch [4/4], Step [6000/61080], Loss: 3.9224\n",
      "Validation perplexity: 40.17785609936025\n",
      "Epoch [4/4], Step [6075/61080], Loss: 4.0323\n",
      "Epoch [4/4], Step [6150/61080], Loss: 4.2404\n",
      "Epoch [4/4], Step [6225/61080], Loss: 4.1657\n",
      "Epoch [4/4], Step [6300/61080], Loss: 4.0106\n",
      "Epoch [4/4], Step [6375/61080], Loss: 3.9892\n",
      "Epoch [4/4], Step [6450/61080], Loss: 3.8522\n",
      "Epoch [4/4], Step [6525/61080], Loss: 4.0606\n",
      "Epoch [4/4], Step [6600/61080], Loss: 4.1205\n",
      "Epoch [4/4], Step [6675/61080], Loss: 3.9517\n",
      "Epoch [4/4], Step [6750/61080], Loss: 3.8083\n",
      "Epoch [4/4], Step [6825/61080], Loss: 3.8612\n",
      "Epoch [4/4], Step [6900/61080], Loss: 4.0439\n",
      "Epoch [4/4], Step [6975/61080], Loss: 3.8913\n",
      "Epoch [4/4], Step [7050/61080], Loss: 4.0434\n",
      "Epoch [4/4], Step [7125/61080], Loss: 3.8000\n",
      "Epoch [4/4], Step [7200/61080], Loss: 3.7162\n",
      "Epoch [4/4], Step [7275/61080], Loss: 4.0582\n",
      "Epoch [4/4], Step [7350/61080], Loss: 4.0810\n",
      "Epoch [4/4], Step [7425/61080], Loss: 4.0258\n",
      "Epoch [4/4], Step [7500/61080], Loss: 3.9614\n",
      "Epoch [4/4], Step [7575/61080], Loss: 3.9649\n",
      "Epoch [4/4], Step [7650/61080], Loss: 4.0549\n",
      "Epoch [4/4], Step [7725/61080], Loss: 3.9745\n",
      "Epoch [4/4], Step [7800/61080], Loss: 3.9353\n",
      "Epoch [4/4], Step [7875/61080], Loss: 3.8539\n",
      "Epoch [4/4], Step [7950/61080], Loss: 3.9150\n",
      "Epoch [4/4], Step [8025/61080], Loss: 3.8641\n",
      "Epoch [4/4], Step [8100/61080], Loss: 3.9596\n",
      "Epoch [4/4], Step [8175/61080], Loss: 4.0504\n",
      "Epoch [4/4], Step [8250/61080], Loss: 4.1030\n",
      "Epoch [4/4], Step [8325/61080], Loss: 4.0267\n",
      "Epoch [4/4], Step [8400/61080], Loss: 3.9488\n",
      "Epoch [4/4], Step [8475/61080], Loss: 3.7476\n",
      "Epoch [4/4], Step [8550/61080], Loss: 3.8786\n",
      "Epoch [4/4], Step [8625/61080], Loss: 3.9271\n",
      "Epoch [4/4], Step [8700/61080], Loss: 4.0136\n",
      "Epoch [4/4], Step [8775/61080], Loss: 4.0846\n",
      "Epoch [4/4], Step [8850/61080], Loss: 3.7672\n",
      "Epoch [4/4], Step [8925/61080], Loss: 3.8206\n",
      "Epoch [4/4], Step [9000/61080], Loss: 4.1156\n",
      "Validation perplexity: 40.174387259071636\n",
      "Epoch [4/4], Step [9075/61080], Loss: 3.8809\n",
      "Epoch [4/4], Step [9150/61080], Loss: 3.8626\n",
      "Epoch [4/4], Step [9225/61080], Loss: 3.9681\n",
      "Epoch [4/4], Step [9300/61080], Loss: 3.8446\n",
      "Epoch [4/4], Step [9375/61080], Loss: 3.7978\n",
      "Epoch [4/4], Step [9450/61080], Loss: 3.9076\n",
      "Epoch [4/4], Step [9525/61080], Loss: 4.0203\n",
      "Epoch [4/4], Step [9600/61080], Loss: 4.0211\n",
      "Epoch [4/4], Step [9675/61080], Loss: 3.9644\n",
      "Epoch [4/4], Step [9750/61080], Loss: 4.1050\n",
      "Epoch [4/4], Step [9825/61080], Loss: 4.0959\n",
      "Epoch [4/4], Step [9900/61080], Loss: 4.0693\n",
      "Epoch [4/4], Step [9975/61080], Loss: 3.9996\n",
      "Epoch [4/4], Step [10050/61080], Loss: 3.9037\n",
      "Epoch [4/4], Step [10125/61080], Loss: 3.9167\n",
      "Epoch [4/4], Step [10200/61080], Loss: 4.1317\n",
      "Epoch [4/4], Step [10275/61080], Loss: 3.8308\n",
      "Epoch [4/4], Step [10350/61080], Loss: 4.0331\n",
      "Epoch [4/4], Step [10425/61080], Loss: 3.7148\n",
      "Epoch [4/4], Step [10500/61080], Loss: 3.8553\n",
      "Epoch [4/4], Step [10575/61080], Loss: 4.1656\n",
      "Epoch [4/4], Step [10650/61080], Loss: 4.0183\n",
      "Epoch [4/4], Step [10725/61080], Loss: 4.1007\n",
      "Epoch [4/4], Step [10800/61080], Loss: 4.0297\n",
      "Epoch [4/4], Step [10875/61080], Loss: 3.9620\n",
      "Epoch [4/4], Step [10950/61080], Loss: 3.8017\n",
      "Epoch [4/4], Step [11025/61080], Loss: 3.8433\n",
      "Epoch [4/4], Step [11100/61080], Loss: 3.8602\n",
      "Epoch [4/4], Step [11175/61080], Loss: 3.9050\n",
      "Epoch [4/4], Step [11250/61080], Loss: 3.9782\n",
      "Epoch [4/4], Step [11325/61080], Loss: 3.9778\n",
      "Epoch [4/4], Step [11400/61080], Loss: 3.9616\n",
      "Epoch [4/4], Step [11475/61080], Loss: 3.9581\n",
      "Epoch [4/4], Step [11550/61080], Loss: 4.0260\n",
      "Epoch [4/4], Step [11625/61080], Loss: 4.1269\n",
      "Epoch [4/4], Step [11700/61080], Loss: 4.0511\n",
      "Epoch [4/4], Step [11775/61080], Loss: 3.8162\n",
      "Epoch [4/4], Step [11850/61080], Loss: 3.8448\n",
      "Epoch [4/4], Step [11925/61080], Loss: 3.7422\n",
      "Epoch [4/4], Step [12000/61080], Loss: 4.0887\n",
      "Validation perplexity: 39.95727732626335\n",
      "Epoch [4/4], Step [12075/61080], Loss: 4.0333\n",
      "Epoch [4/4], Step [12150/61080], Loss: 3.9540\n",
      "Epoch [4/4], Step [12225/61080], Loss: 4.0036\n",
      "Epoch [4/4], Step [12300/61080], Loss: 3.8164\n",
      "Epoch [4/4], Step [12375/61080], Loss: 3.8196\n",
      "Epoch [4/4], Step [12450/61080], Loss: 3.9306\n",
      "Epoch [4/4], Step [12525/61080], Loss: 4.0452\n",
      "Epoch [4/4], Step [12600/61080], Loss: 3.9506\n",
      "Epoch [4/4], Step [12675/61080], Loss: 4.1567\n",
      "Epoch [4/4], Step [12750/61080], Loss: 3.6638\n",
      "Epoch [4/4], Step [12825/61080], Loss: 3.9098\n",
      "Epoch [4/4], Step [12900/61080], Loss: 3.8835\n",
      "Epoch [4/4], Step [12975/61080], Loss: 4.2539\n",
      "Epoch [4/4], Step [13050/61080], Loss: 3.8261\n",
      "Epoch [4/4], Step [13125/61080], Loss: 3.8803\n",
      "Epoch [4/4], Step [13200/61080], Loss: 3.8239\n",
      "Epoch [4/4], Step [13275/61080], Loss: 3.8941\n",
      "Epoch [4/4], Step [13350/61080], Loss: 3.9804\n",
      "Epoch [4/4], Step [13425/61080], Loss: 4.5224\n",
      "Epoch [4/4], Step [13500/61080], Loss: 3.9528\n",
      "Epoch [4/4], Step [13575/61080], Loss: 4.0618\n",
      "Epoch [4/4], Step [13650/61080], Loss: 4.0021\n",
      "Epoch [4/4], Step [13725/61080], Loss: 4.1212\n",
      "Epoch [4/4], Step [13800/61080], Loss: 3.9987\n",
      "Epoch [4/4], Step [13875/61080], Loss: 3.9326\n",
      "Epoch [4/4], Step [13950/61080], Loss: 3.7767\n",
      "Epoch [4/4], Step [14025/61080], Loss: 3.8142\n",
      "Epoch [4/4], Step [14100/61080], Loss: 3.7726\n",
      "Epoch [4/4], Step [14175/61080], Loss: 3.9540\n",
      "Epoch [4/4], Step [14250/61080], Loss: 3.9539\n",
      "Epoch [4/4], Step [14325/61080], Loss: 3.7527\n",
      "Epoch [4/4], Step [14400/61080], Loss: 3.9596\n",
      "Epoch [4/4], Step [14475/61080], Loss: 3.9718\n",
      "Epoch [4/4], Step [14550/61080], Loss: 3.9366\n",
      "Epoch [4/4], Step [14625/61080], Loss: 3.7029\n",
      "Epoch [4/4], Step [14700/61080], Loss: 3.9916\n",
      "Epoch [4/4], Step [14775/61080], Loss: 4.0218\n",
      "Epoch [4/4], Step [14850/61080], Loss: 3.9151\n",
      "Epoch [4/4], Step [14925/61080], Loss: 3.8319\n",
      "Epoch [4/4], Step [15000/61080], Loss: 3.6624\n",
      "Validation perplexity: 40.307100260468516\n",
      "Epoch [4/4], Step [15075/61080], Loss: 4.1111\n",
      "Epoch [4/4], Step [15150/61080], Loss: 3.9548\n",
      "Epoch [4/4], Step [15225/61080], Loss: 3.9081\n",
      "Epoch [4/4], Step [15300/61080], Loss: 3.8020\n",
      "Epoch [4/4], Step [15375/61080], Loss: 4.2009\n",
      "Epoch [4/4], Step [15450/61080], Loss: 3.8071\n",
      "Epoch [4/4], Step [15525/61080], Loss: 3.8969\n",
      "Epoch [4/4], Step [15600/61080], Loss: 4.0878\n",
      "Epoch [4/4], Step [15675/61080], Loss: 3.8961\n",
      "Epoch [4/4], Step [15750/61080], Loss: 3.8284\n",
      "Epoch [4/4], Step [15825/61080], Loss: 3.8228\n",
      "Epoch [4/4], Step [15900/61080], Loss: 3.9230\n",
      "Epoch [4/4], Step [15975/61080], Loss: 3.9148\n",
      "Epoch [4/4], Step [16050/61080], Loss: 4.1464\n",
      "Epoch [4/4], Step [16125/61080], Loss: 3.8810\n",
      "Epoch [4/4], Step [16200/61080], Loss: 4.0397\n",
      "Epoch [4/4], Step [16275/61080], Loss: 4.0204\n",
      "Epoch [4/4], Step [16350/61080], Loss: 3.7865\n",
      "Epoch [4/4], Step [16425/61080], Loss: 4.0507\n",
      "Epoch [4/4], Step [16500/61080], Loss: 3.7853\n",
      "Epoch [4/4], Step [16575/61080], Loss: 4.0763\n",
      "Epoch [4/4], Step [16650/61080], Loss: 3.7978\n",
      "Epoch [4/4], Step [16725/61080], Loss: 4.2056\n",
      "Epoch [4/4], Step [16800/61080], Loss: 3.8813\n",
      "Epoch [4/4], Step [16875/61080], Loss: 3.9306\n",
      "Epoch [4/4], Step [16950/61080], Loss: 3.9070\n",
      "Epoch [4/4], Step [17025/61080], Loss: 3.9408\n",
      "Epoch [4/4], Step [17100/61080], Loss: 3.9598\n",
      "Epoch [4/4], Step [17175/61080], Loss: 4.0958\n",
      "Epoch [4/4], Step [17250/61080], Loss: 3.7630\n",
      "Epoch [4/4], Step [17325/61080], Loss: 3.8686\n",
      "Epoch [4/4], Step [17400/61080], Loss: 3.9845\n",
      "Epoch [4/4], Step [17475/61080], Loss: 3.8481\n",
      "Epoch [4/4], Step [17550/61080], Loss: 3.8203\n",
      "Epoch [4/4], Step [17625/61080], Loss: 3.8381\n",
      "Epoch [4/4], Step [17700/61080], Loss: 3.6824\n",
      "Epoch [4/4], Step [17775/61080], Loss: 3.7062\n",
      "Epoch [4/4], Step [17850/61080], Loss: 3.8952\n",
      "Epoch [4/4], Step [17925/61080], Loss: 4.0828\n",
      "Epoch [4/4], Step [18000/61080], Loss: 3.8196\n",
      "Validation perplexity: 40.055201407758226\n",
      "Epoch [4/4], Step [18075/61080], Loss: 3.7823\n",
      "Epoch [4/4], Step [18150/61080], Loss: 3.8967\n",
      "Epoch [4/4], Step [18225/61080], Loss: 3.8464\n",
      "Epoch [4/4], Step [18300/61080], Loss: 3.8828\n",
      "Epoch [4/4], Step [18375/61080], Loss: 4.0397\n",
      "Epoch [4/4], Step [18450/61080], Loss: 3.8129\n",
      "Epoch [4/4], Step [18525/61080], Loss: 3.8914\n",
      "Epoch [4/4], Step [18600/61080], Loss: 3.8490\n",
      "Epoch [4/4], Step [18675/61080], Loss: 4.0158\n",
      "Epoch [4/4], Step [18750/61080], Loss: 3.9116\n",
      "Epoch [4/4], Step [18825/61080], Loss: 3.9990\n",
      "Epoch [4/4], Step [18900/61080], Loss: 3.8850\n",
      "Epoch [4/4], Step [18975/61080], Loss: 3.9331\n",
      "Epoch [4/4], Step [19050/61080], Loss: 3.9414\n",
      "Epoch [4/4], Step [19125/61080], Loss: 4.1403\n",
      "Epoch [4/4], Step [19200/61080], Loss: 4.3325\n",
      "Epoch [4/4], Step [19275/61080], Loss: 3.8880\n",
      "Epoch [4/4], Step [19350/61080], Loss: 4.1177\n",
      "Epoch [4/4], Step [19425/61080], Loss: 4.0700\n",
      "Epoch [4/4], Step [19500/61080], Loss: 3.9591\n",
      "Epoch [4/4], Step [19575/61080], Loss: 3.7393\n",
      "Epoch [4/4], Step [19650/61080], Loss: 3.8435\n",
      "Epoch [4/4], Step [19725/61080], Loss: 4.0581\n",
      "Epoch [4/4], Step [19800/61080], Loss: 3.9770\n",
      "Epoch [4/4], Step [19875/61080], Loss: 3.8208\n",
      "Epoch [4/4], Step [19950/61080], Loss: 3.9212\n",
      "Epoch [4/4], Step [20025/61080], Loss: 3.9841\n",
      "Epoch [4/4], Step [20100/61080], Loss: 3.8896\n",
      "Epoch [4/4], Step [20175/61080], Loss: 3.7882\n",
      "Epoch [4/4], Step [20250/61080], Loss: 4.1246\n",
      "Epoch [4/4], Step [20325/61080], Loss: 4.0210\n",
      "Epoch [4/4], Step [20400/61080], Loss: 4.2481\n",
      "Epoch [4/4], Step [20475/61080], Loss: 3.8590\n",
      "Epoch [4/4], Step [20550/61080], Loss: 3.9012\n",
      "Epoch [4/4], Step [20625/61080], Loss: 4.1384\n",
      "Epoch [4/4], Step [20700/61080], Loss: 4.0668\n",
      "Epoch [4/4], Step [20775/61080], Loss: 3.9482\n",
      "Epoch [4/4], Step [20850/61080], Loss: 4.3328\n",
      "Epoch [4/4], Step [20925/61080], Loss: 4.0820\n",
      "Epoch [4/4], Step [21000/61080], Loss: 3.8071\n",
      "Validation perplexity: 39.929671756573235\n",
      "Epoch [4/4], Step [21075/61080], Loss: 3.8042\n",
      "Epoch [4/4], Step [21150/61080], Loss: 3.9694\n",
      "Epoch [4/4], Step [21225/61080], Loss: 4.1444\n",
      "Epoch [4/4], Step [21300/61080], Loss: 3.8313\n",
      "Epoch [4/4], Step [21375/61080], Loss: 3.9559\n",
      "Epoch [4/4], Step [21450/61080], Loss: 3.8725\n",
      "Epoch [4/4], Step [21525/61080], Loss: 3.9502\n",
      "Epoch [4/4], Step [21600/61080], Loss: 4.0423\n",
      "Epoch [4/4], Step [21675/61080], Loss: 3.5378\n",
      "Epoch [4/4], Step [21750/61080], Loss: 3.8548\n",
      "Epoch [4/4], Step [21825/61080], Loss: 3.8786\n",
      "Epoch [4/4], Step [21900/61080], Loss: 3.9756\n",
      "Epoch [4/4], Step [21975/61080], Loss: 3.8708\n",
      "Epoch [4/4], Step [22050/61080], Loss: 3.7589\n",
      "Epoch [4/4], Step [22125/61080], Loss: 3.7586\n",
      "Epoch [4/4], Step [22200/61080], Loss: 3.8926\n",
      "Epoch [4/4], Step [22275/61080], Loss: 4.1362\n",
      "Epoch [4/4], Step [22350/61080], Loss: 3.9630\n",
      "Epoch [4/4], Step [22425/61080], Loss: 3.7898\n",
      "Epoch [4/4], Step [22500/61080], Loss: 3.7799\n",
      "Epoch [4/4], Step [22575/61080], Loss: 3.8796\n",
      "Epoch [4/4], Step [22650/61080], Loss: 3.8501\n",
      "Epoch [4/4], Step [22725/61080], Loss: 3.8066\n",
      "Epoch [4/4], Step [22800/61080], Loss: 3.9215\n",
      "Epoch [4/4], Step [22875/61080], Loss: 3.8731\n",
      "Epoch [4/4], Step [22950/61080], Loss: 3.9746\n",
      "Epoch [4/4], Step [23025/61080], Loss: 3.9773\n",
      "Epoch [4/4], Step [23100/61080], Loss: 3.8496\n",
      "Epoch [4/4], Step [23175/61080], Loss: 3.9360\n",
      "Epoch [4/4], Step [23250/61080], Loss: 3.8751\n",
      "Epoch [4/4], Step [23325/61080], Loss: 3.9440\n",
      "Epoch [4/4], Step [23400/61080], Loss: 3.7596\n",
      "Epoch [4/4], Step [23475/61080], Loss: 3.7646\n",
      "Epoch [4/4], Step [23550/61080], Loss: 3.8387\n",
      "Epoch [4/4], Step [23625/61080], Loss: 3.8467\n",
      "Epoch [4/4], Step [23700/61080], Loss: 4.0838\n",
      "Epoch [4/4], Step [23775/61080], Loss: 3.6319\n",
      "Epoch [4/4], Step [23850/61080], Loss: 3.8187\n",
      "Epoch [4/4], Step [23925/61080], Loss: 4.0205\n",
      "Epoch [4/4], Step [24000/61080], Loss: 3.7233\n",
      "Validation perplexity: 39.88145853283631\n",
      "Epoch [4/4], Step [24075/61080], Loss: 4.0424\n",
      "Epoch [4/4], Step [24150/61080], Loss: 3.9532\n",
      "Epoch [4/4], Step [24225/61080], Loss: 3.6839\n",
      "Epoch [4/4], Step [24300/61080], Loss: 3.9239\n",
      "Epoch [4/4], Step [24375/61080], Loss: 4.0514\n",
      "Epoch [4/4], Step [24450/61080], Loss: 3.8655\n",
      "Epoch [4/4], Step [24525/61080], Loss: 3.8167\n",
      "Epoch [4/4], Step [24600/61080], Loss: 3.7494\n",
      "Epoch [4/4], Step [24675/61080], Loss: 3.8808\n",
      "Epoch [4/4], Step [24750/61080], Loss: 3.9229\n",
      "Epoch [4/4], Step [24825/61080], Loss: 4.0161\n",
      "Epoch [4/4], Step [24900/61080], Loss: 4.2007\n",
      "Epoch [4/4], Step [24975/61080], Loss: 3.7565\n",
      "Epoch [4/4], Step [25050/61080], Loss: 4.1282\n",
      "Epoch [4/4], Step [25125/61080], Loss: 3.7973\n",
      "Epoch [4/4], Step [25200/61080], Loss: 4.0409\n",
      "Epoch [4/4], Step [25275/61080], Loss: 3.8188\n",
      "Epoch [4/4], Step [25350/61080], Loss: 3.8727\n",
      "Epoch [4/4], Step [25425/61080], Loss: 3.8911\n",
      "Epoch [4/4], Step [25500/61080], Loss: 3.9500\n",
      "Epoch [4/4], Step [25575/61080], Loss: 4.1494\n",
      "Epoch [4/4], Step [25650/61080], Loss: 4.0322\n",
      "Epoch [4/4], Step [25725/61080], Loss: 3.7954\n",
      "Epoch [4/4], Step [25800/61080], Loss: 4.2970\n",
      "Epoch [4/4], Step [25875/61080], Loss: 3.9382\n",
      "Epoch [4/4], Step [25950/61080], Loss: 3.8976\n",
      "Epoch [4/4], Step [26025/61080], Loss: 4.0240\n",
      "Epoch [4/4], Step [26100/61080], Loss: 3.8131\n",
      "Epoch [4/4], Step [26175/61080], Loss: 4.2190\n",
      "Epoch [4/4], Step [26250/61080], Loss: 4.0773\n",
      "Epoch [4/4], Step [26325/61080], Loss: 4.0741\n",
      "Epoch [4/4], Step [26400/61080], Loss: 3.9593\n",
      "Epoch [4/4], Step [26475/61080], Loss: 3.7538\n",
      "Epoch [4/4], Step [26550/61080], Loss: 3.8087\n",
      "Epoch [4/4], Step [26625/61080], Loss: 3.8314\n",
      "Epoch [4/4], Step [26700/61080], Loss: 3.7531\n",
      "Epoch [4/4], Step [26775/61080], Loss: 3.6803\n",
      "Epoch [4/4], Step [26850/61080], Loss: 3.9286\n",
      "Epoch [4/4], Step [26925/61080], Loss: 3.6959\n",
      "Epoch [4/4], Step [27000/61080], Loss: 4.0938\n",
      "Validation perplexity: 39.614882181463884\n",
      "Epoch [4/4], Step [27075/61080], Loss: 4.2420\n",
      "Epoch [4/4], Step [27150/61080], Loss: 4.2630\n",
      "Epoch [4/4], Step [38100/61080], Loss: 4.0194\n",
      "Epoch [4/4], Step [38175/61080], Loss: 4.1089\n",
      "Epoch [4/4], Step [38250/61080], Loss: 3.7794\n",
      "Epoch [4/4], Step [38325/61080], Loss: 4.0249\n",
      "Epoch [4/4], Step [38400/61080], Loss: 3.8233\n",
      "Epoch [4/4], Step [38475/61080], Loss: 3.9405\n",
      "Epoch [4/4], Step [38550/61080], Loss: 3.7958\n",
      "Epoch [4/4], Step [38625/61080], Loss: 3.8877\n",
      "Epoch [4/4], Step [38700/61080], Loss: 3.8385\n",
      "Epoch [4/4], Step [38775/61080], Loss: 3.7494\n",
      "Epoch [4/4], Step [38850/61080], Loss: 4.1648\n",
      "Epoch [4/4], Step [38925/61080], Loss: 4.3694\n",
      "Epoch [4/4], Step [39000/61080], Loss: 4.0442\n",
      "Validation perplexity: 39.448018018182495\n",
      "Epoch [4/4], Step [39075/61080], Loss: 3.9132\n",
      "Epoch [4/4], Step [39150/61080], Loss: 3.8522\n",
      "Epoch [4/4], Step [39225/61080], Loss: 3.7907\n",
      "Epoch [4/4], Step [39300/61080], Loss: 3.8532\n",
      "Epoch [4/4], Step [39375/61080], Loss: 4.1363\n",
      "Epoch [4/4], Step [39450/61080], Loss: 4.1968\n",
      "Epoch [4/4], Step [39525/61080], Loss: 3.9632\n",
      "Epoch [4/4], Step [39600/61080], Loss: 3.9819\n",
      "Epoch [4/4], Step [39675/61080], Loss: 4.0753\n",
      "Epoch [4/4], Step [39750/61080], Loss: 4.0171\n",
      "Epoch [4/4], Step [39825/61080], Loss: 3.8879\n",
      "Epoch [4/4], Step [39900/61080], Loss: 3.9002\n",
      "Epoch [4/4], Step [39975/61080], Loss: 4.0531\n",
      "Epoch [4/4], Step [40050/61080], Loss: 3.9326\n",
      "Epoch [4/4], Step [40125/61080], Loss: 3.7670\n",
      "Epoch [4/4], Step [40200/61080], Loss: 4.1635\n",
      "Epoch [4/4], Step [40275/61080], Loss: 4.0200\n",
      "Epoch [4/4], Step [40350/61080], Loss: 3.7015\n",
      "Epoch [4/4], Step [40425/61080], Loss: 4.1138\n",
      "Epoch [4/4], Step [40500/61080], Loss: 3.8785\n",
      "Epoch [4/4], Step [40575/61080], Loss: 3.9724\n",
      "Epoch [4/4], Step [40650/61080], Loss: 4.0203\n",
      "Epoch [4/4], Step [40725/61080], Loss: 3.6935\n",
      "Epoch [4/4], Step [40800/61080], Loss: 3.9143\n",
      "Epoch [4/4], Step [40875/61080], Loss: 4.0491\n",
      "Epoch [4/4], Step [40950/61080], Loss: 4.1648\n",
      "Epoch [4/4], Step [41025/61080], Loss: 3.9635\n",
      "Epoch [4/4], Step [41100/61080], Loss: 3.9003\n",
      "Epoch [4/4], Step [41175/61080], Loss: 3.9428\n",
      "Epoch [4/4], Step [41250/61080], Loss: 4.0081\n",
      "Epoch [4/4], Step [41325/61080], Loss: 4.1526\n",
      "Epoch [4/4], Step [41400/61080], Loss: 4.0307\n",
      "Epoch [4/4], Step [41475/61080], Loss: 4.0546\n",
      "Epoch [4/4], Step [41550/61080], Loss: 3.8804\n",
      "Epoch [4/4], Step [41625/61080], Loss: 4.0775\n",
      "Epoch [4/4], Step [41700/61080], Loss: 3.8621\n",
      "Epoch [4/4], Step [41775/61080], Loss: 3.9839\n",
      "Epoch [4/4], Step [41850/61080], Loss: 3.7740\n",
      "Epoch [4/4], Step [41925/61080], Loss: 3.7820\n",
      "Epoch [4/4], Step [42000/61080], Loss: 3.8879\n",
      "Validation perplexity: 39.17939810604826\n",
      "Epoch [4/4], Step [42075/61080], Loss: 3.9167\n",
      "Epoch [4/4], Step [42150/61080], Loss: 4.0110\n",
      "Epoch [4/4], Step [42225/61080], Loss: 3.8648\n",
      "Epoch [4/4], Step [42300/61080], Loss: 3.8987\n",
      "Epoch [4/4], Step [42375/61080], Loss: 3.9606\n",
      "Epoch [4/4], Step [42450/61080], Loss: 3.9424\n",
      "Epoch [4/4], Step [42525/61080], Loss: 3.7679\n",
      "Epoch [4/4], Step [42600/61080], Loss: 4.0759\n",
      "Epoch [4/4], Step [42675/61080], Loss: 4.0656\n",
      "Epoch [4/4], Step [42750/61080], Loss: 4.0864\n",
      "Epoch [4/4], Step [42825/61080], Loss: 4.0046\n",
      "Epoch [4/4], Step [42900/61080], Loss: 3.8322\n",
      "Epoch [4/4], Step [42975/61080], Loss: 3.9585\n",
      "Epoch [4/4], Step [43050/61080], Loss: 3.8399\n",
      "Epoch [4/4], Step [43125/61080], Loss: 4.0660\n",
      "Epoch [4/4], Step [43200/61080], Loss: 3.8186\n",
      "Epoch [4/4], Step [43275/61080], Loss: 4.1236\n",
      "Epoch [4/4], Step [43350/61080], Loss: 4.0275\n",
      "Epoch [4/4], Step [43425/61080], Loss: 3.9621\n",
      "Epoch [4/4], Step [43500/61080], Loss: 3.7903\n",
      "Epoch [4/4], Step [43575/61080], Loss: 3.8676\n",
      "Epoch [4/4], Step [43650/61080], Loss: 4.2017\n",
      "Epoch [4/4], Step [43725/61080], Loss: 3.8821\n",
      "Epoch [4/4], Step [43800/61080], Loss: 4.0718\n",
      "Epoch [4/4], Step [43875/61080], Loss: 3.7732\n",
      "Epoch [4/4], Step [43950/61080], Loss: 3.8899\n",
      "Epoch [4/4], Step [44025/61080], Loss: 3.8753\n",
      "Epoch [4/4], Step [44100/61080], Loss: 3.9083\n",
      "Epoch [4/4], Step [44175/61080], Loss: 3.9807\n",
      "Epoch [4/4], Step [44250/61080], Loss: 3.7120\n",
      "Epoch [4/4], Step [44325/61080], Loss: 3.8938\n",
      "Epoch [4/4], Step [44400/61080], Loss: 4.0307\n",
      "Epoch [4/4], Step [44475/61080], Loss: 3.8627\n",
      "Epoch [4/4], Step [44550/61080], Loss: 3.8024\n",
      "Epoch [4/4], Step [44625/61080], Loss: 3.8273\n",
      "Epoch [4/4], Step [44700/61080], Loss: 3.8564\n",
      "Epoch [4/4], Step [44775/61080], Loss: 3.7131\n",
      "Epoch [4/4], Step [44850/61080], Loss: 4.1603\n",
      "Epoch [4/4], Step [44925/61080], Loss: 3.8938\n",
      "Epoch [4/4], Step [45000/61080], Loss: 3.8919\n",
      "Validation perplexity: 39.230586867041715\n",
      "Epoch [4/4], Step [45075/61080], Loss: 3.6237\n",
      "Epoch [4/4], Step [45150/61080], Loss: 3.9731\n",
      "Epoch [4/4], Step [45225/61080], Loss: 3.8102\n",
      "Epoch [4/4], Step [45300/61080], Loss: 3.9067\n",
      "Epoch [4/4], Step [45375/61080], Loss: 4.0621\n",
      "Epoch [4/4], Step [45450/61080], Loss: 3.8651\n",
      "Epoch [4/4], Step [45525/61080], Loss: 4.0620\n",
      "Epoch [4/4], Step [45600/61080], Loss: 3.8696\n",
      "Epoch [4/4], Step [45675/61080], Loss: 3.8667\n",
      "Epoch [4/4], Step [45750/61080], Loss: 4.0878\n",
      "Epoch [4/4], Step [45825/61080], Loss: 3.7766\n",
      "Epoch [4/4], Step [45900/61080], Loss: 3.8773\n",
      "Epoch [4/4], Step [45975/61080], Loss: 4.0191\n",
      "Epoch [4/4], Step [46050/61080], Loss: 4.2944\n",
      "Epoch [4/4], Step [46125/61080], Loss: 4.1050\n",
      "Epoch [4/4], Step [46200/61080], Loss: 4.1138\n",
      "Epoch [4/4], Step [46275/61080], Loss: 3.7924\n",
      "Epoch [4/4], Step [46350/61080], Loss: 4.2955\n",
      "Epoch [4/4], Step [46425/61080], Loss: 3.7043\n",
      "Epoch [4/4], Step [46500/61080], Loss: 3.9561\n",
      "Epoch [4/4], Step [46575/61080], Loss: 3.8491\n",
      "Epoch [4/4], Step [46650/61080], Loss: 3.7563\n",
      "Epoch [4/4], Step [46725/61080], Loss: 4.0708\n",
      "Epoch [4/4], Step [46800/61080], Loss: 4.1392\n",
      "Epoch [4/4], Step [46875/61080], Loss: 3.9704\n",
      "Epoch [4/4], Step [46950/61080], Loss: 3.9147\n",
      "Epoch [4/4], Step [47025/61080], Loss: 3.8127\n",
      "Epoch [4/4], Step [47100/61080], Loss: 3.9925\n",
      "Epoch [4/4], Step [47175/61080], Loss: 4.0079\n",
      "Epoch [4/4], Step [47250/61080], Loss: 4.0159\n",
      "Epoch [4/4], Step [47325/61080], Loss: 3.8502\n",
      "Epoch [4/4], Step [47400/61080], Loss: 3.7811\n",
      "Epoch [4/4], Step [47475/61080], Loss: 3.9704\n",
      "Epoch [4/4], Step [47550/61080], Loss: 3.8604\n",
      "Epoch [4/4], Step [47625/61080], Loss: 4.0761\n",
      "Epoch [4/4], Step [47700/61080], Loss: 3.7862\n",
      "Epoch [4/4], Step [47775/61080], Loss: 3.8275\n",
      "Epoch [4/4], Step [47850/61080], Loss: 4.1764\n",
      "Epoch [4/4], Step [47925/61080], Loss: 3.8348\n",
      "Epoch [4/4], Step [48000/61080], Loss: 4.2538\n",
      "Validation perplexity: 39.5518944617738\n",
      "Epoch [4/4], Step [48075/61080], Loss: 3.9638\n",
      "Epoch [4/4], Step [48150/61080], Loss: 3.8335\n",
      "Epoch [4/4], Step [48225/61080], Loss: 3.8270\n",
      "Epoch [4/4], Step [48300/61080], Loss: 3.8607\n",
      "Epoch [4/4], Step [48375/61080], Loss: 3.8625\n",
      "Epoch [4/4], Step [48450/61080], Loss: 3.7423\n",
      "Epoch [4/4], Step [48525/61080], Loss: 3.8920\n",
      "Epoch [4/4], Step [48600/61080], Loss: 3.8029\n",
      "Epoch [4/4], Step [48675/61080], Loss: 3.9514\n",
      "Epoch [4/4], Step [48750/61080], Loss: 4.2567\n",
      "Epoch [4/4], Step [48825/61080], Loss: 3.9480\n",
      "Epoch [4/4], Step [48900/61080], Loss: 4.1397\n",
      "Epoch [4/4], Step [48975/61080], Loss: 3.9220\n",
      "Epoch [4/4], Step [49050/61080], Loss: 3.7316\n",
      "Epoch [4/4], Step [49125/61080], Loss: 3.8905\n",
      "Epoch [4/4], Step [49200/61080], Loss: 3.7919\n",
      "Epoch [4/4], Step [49275/61080], Loss: 4.1190\n",
      "Epoch [4/4], Step [49350/61080], Loss: 3.9805\n",
      "Epoch [4/4], Step [49425/61080], Loss: 4.0603\n",
      "Epoch [4/4], Step [49500/61080], Loss: 3.7948\n",
      "Epoch [4/4], Step [49575/61080], Loss: 4.1961\n",
      "Epoch [4/4], Step [49650/61080], Loss: 3.8269\n",
      "Epoch [4/4], Step [49725/61080], Loss: 3.9279\n",
      "Epoch [4/4], Step [49800/61080], Loss: 3.8154\n",
      "Epoch [4/4], Step [49875/61080], Loss: 4.1046\n",
      "Epoch [4/4], Step [49950/61080], Loss: 4.0251\n",
      "Epoch [4/4], Step [50025/61080], Loss: 3.9945\n",
      "Epoch [4/4], Step [50100/61080], Loss: 3.7851\n",
      "Epoch [4/4], Step [50175/61080], Loss: 4.0892\n",
      "Epoch [4/4], Step [50250/61080], Loss: 3.9437\n",
      "Epoch [4/4], Step [50325/61080], Loss: 4.0039\n",
      "Epoch [4/4], Step [50400/61080], Loss: 4.0714\n",
      "Epoch [4/4], Step [50475/61080], Loss: 3.7667\n",
      "Epoch [4/4], Step [50550/61080], Loss: 4.0324\n",
      "Epoch [4/4], Step [50625/61080], Loss: 3.9032\n",
      "Epoch [4/4], Step [50700/61080], Loss: 3.7964\n",
      "Epoch [4/4], Step [50775/61080], Loss: 3.8261\n",
      "Epoch [4/4], Step [50850/61080], Loss: 3.8993\n",
      "Epoch [4/4], Step [50925/61080], Loss: 3.9493\n",
      "Epoch [4/4], Step [51000/61080], Loss: 4.0459\n",
      "Validation perplexity: 39.4961886036689\n",
      "Epoch [4/4], Step [51075/61080], Loss: 4.2108\n",
      "Epoch [4/4], Step [51150/61080], Loss: 4.2172\n",
      "Epoch [4/4], Step [51225/61080], Loss: 3.7765\n",
      "Epoch [4/4], Step [51300/61080], Loss: 3.9754\n",
      "Epoch [4/4], Step [51375/61080], Loss: 3.8334\n",
      "Epoch [4/4], Step [51450/61080], Loss: 3.8442\n",
      "Epoch [4/4], Step [51525/61080], Loss: 3.8912\n",
      "Epoch [4/4], Step [51600/61080], Loss: 4.0189\n",
      "Epoch [4/4], Step [51675/61080], Loss: 4.0284\n",
      "Epoch [4/4], Step [51750/61080], Loss: 4.0491\n",
      "Epoch [4/4], Step [51825/61080], Loss: 3.9767\n",
      "Epoch [4/4], Step [51900/61080], Loss: 3.8590\n",
      "Epoch [4/4], Step [51975/61080], Loss: 3.8080\n",
      "Epoch [4/4], Step [52050/61080], Loss: 3.8250\n",
      "Epoch [4/4], Step [52125/61080], Loss: 4.0652\n",
      "Epoch [4/4], Step [52200/61080], Loss: 4.0137\n",
      "Epoch [4/4], Step [52275/61080], Loss: 3.9947\n",
      "Epoch [4/4], Step [52350/61080], Loss: 3.9851\n",
      "Epoch [4/4], Step [52425/61080], Loss: 3.8577\n",
      "Epoch [4/4], Step [52500/61080], Loss: 3.8565\n",
      "Epoch [4/4], Step [52575/61080], Loss: 3.9173\n",
      "Epoch [4/4], Step [52650/61080], Loss: 3.9177\n",
      "Epoch [4/4], Step [52725/61080], Loss: 3.9996\n",
      "Epoch [4/4], Step [52800/61080], Loss: 3.6580\n",
      "Epoch [4/4], Step [52875/61080], Loss: 3.8322\n",
      "Epoch [4/4], Step [52950/61080], Loss: 4.1664\n",
      "Epoch [4/4], Step [53025/61080], Loss: 3.9903\n",
      "Epoch [4/4], Step [53100/61080], Loss: 3.9398\n",
      "Epoch [4/4], Step [53175/61080], Loss: 3.9012\n",
      "Epoch [4/4], Step [53250/61080], Loss: 4.1047\n",
      "Epoch [4/4], Step [53325/61080], Loss: 3.9048\n",
      "Epoch [4/4], Step [53400/61080], Loss: 3.9630\n",
      "Epoch [4/4], Step [53475/61080], Loss: 3.7761\n",
      "Epoch [4/4], Step [53550/61080], Loss: 3.6560\n",
      "Epoch [4/4], Step [53625/61080], Loss: 3.8232\n",
      "Epoch [4/4], Step [53700/61080], Loss: 4.0981\n",
      "Epoch [4/4], Step [53775/61080], Loss: 3.8857\n",
      "Epoch [4/4], Step [53850/61080], Loss: 3.9381\n",
      "Epoch [4/4], Step [53925/61080], Loss: 3.9066\n",
      "Epoch [4/4], Step [54000/61080], Loss: 3.8987\n",
      "Validation perplexity: 39.04612752668756\n",
      "Epoch [4/4], Step [54075/61080], Loss: 3.7635\n",
      "Epoch [4/4], Step [54150/61080], Loss: 4.0452\n",
      "Epoch [4/4], Step [54225/61080], Loss: 3.8457\n",
      "Epoch [4/4], Step [54300/61080], Loss: 3.8125\n",
      "Epoch [4/4], Step [54375/61080], Loss: 3.7707\n",
      "Epoch [4/4], Step [54450/61080], Loss: 3.9247\n",
      "Epoch [4/4], Step [54525/61080], Loss: 3.6384\n",
      "Epoch [4/4], Step [54600/61080], Loss: 3.9514\n",
      "Epoch [4/4], Step [54675/61080], Loss: 3.9492\n",
      "Epoch [4/4], Step [54750/61080], Loss: 3.9549\n",
      "Epoch [4/4], Step [54825/61080], Loss: 3.7987\n",
      "Epoch [4/4], Step [54900/61080], Loss: 3.8741\n",
      "Epoch [4/4], Step [54975/61080], Loss: 4.0120\n",
      "Epoch [4/4], Step [55050/61080], Loss: 3.7319\n",
      "Epoch [4/4], Step [55125/61080], Loss: 3.9546\n",
      "Epoch [4/4], Step [55200/61080], Loss: 4.1192\n",
      "Epoch [4/4], Step [55275/61080], Loss: 4.2050\n",
      "Epoch [4/4], Step [55350/61080], Loss: 3.9333\n",
      "Epoch [4/4], Step [55425/61080], Loss: 3.9529\n",
      "Epoch [4/4], Step [55500/61080], Loss: 4.0917\n",
      "Epoch [4/4], Step [55575/61080], Loss: 3.8944\n",
      "Epoch [4/4], Step [55650/61080], Loss: 4.0444\n",
      "Epoch [4/4], Step [55725/61080], Loss: 3.9666\n",
      "Epoch [4/4], Step [55800/61080], Loss: 3.9233\n",
      "Epoch [4/4], Step [55875/61080], Loss: 3.7498\n",
      "Epoch [4/4], Step [55950/61080], Loss: 4.0136\n",
      "Epoch [4/4], Step [56025/61080], Loss: 3.9792\n",
      "Epoch [4/4], Step [56100/61080], Loss: 3.8040\n",
      "Epoch [4/4], Step [56175/61080], Loss: 3.9919\n",
      "Epoch [4/4], Step [56250/61080], Loss: 3.8484\n",
      "Epoch [4/4], Step [56325/61080], Loss: 3.9533\n",
      "Epoch [4/4], Step [56400/61080], Loss: 3.7468\n",
      "Epoch [4/4], Step [56475/61080], Loss: 4.1726\n",
      "Epoch [4/4], Step [56550/61080], Loss: 4.0421\n",
      "Epoch [4/4], Step [56625/61080], Loss: 3.9335\n",
      "Epoch [4/4], Step [56700/61080], Loss: 3.7683\n",
      "Epoch [4/4], Step [56775/61080], Loss: 3.8788\n",
      "Epoch [4/4], Step [56850/61080], Loss: 3.7802\n",
      "Epoch [4/4], Step [56925/61080], Loss: 3.8719\n",
      "Epoch [4/4], Step [57000/61080], Loss: 3.7891\n",
      "Validation perplexity: 39.07826447103407\n",
      "Epoch [4/4], Step [57075/61080], Loss: 3.8558\n",
      "Epoch [4/4], Step [57150/61080], Loss: 3.9924\n",
      "Epoch [4/4], Step [57225/61080], Loss: 3.5991\n",
      "Epoch [4/4], Step [57300/61080], Loss: 3.8354\n",
      "Epoch [4/4], Step [57375/61080], Loss: 3.9836\n",
      "Epoch [4/4], Step [57450/61080], Loss: 4.1464\n",
      "Epoch [4/4], Step [57525/61080], Loss: 3.8749\n",
      "Epoch [4/4], Step [57600/61080], Loss: 3.7247\n",
      "Epoch [4/4], Step [57675/61080], Loss: 3.9591\n",
      "Epoch [4/4], Step [57750/61080], Loss: 3.9195\n",
      "Epoch [4/4], Step [57825/61080], Loss: 3.9703\n",
      "Epoch [4/4], Step [57900/61080], Loss: 3.7452\n",
      "Epoch [4/4], Step [57975/61080], Loss: 3.7984\n",
      "Epoch [4/4], Step [58050/61080], Loss: 3.7700\n",
      "Epoch [4/4], Step [58125/61080], Loss: 4.0006\n",
      "Epoch [4/4], Step [58200/61080], Loss: 4.0934\n",
      "Epoch [4/4], Step [58275/61080], Loss: 3.7730\n",
      "Epoch [4/4], Step [58350/61080], Loss: 4.0532\n",
      "Epoch [4/4], Step [58425/61080], Loss: 4.0186\n",
      "Epoch [4/4], Step [58500/61080], Loss: 3.9597\n",
      "Epoch [4/4], Step [58575/61080], Loss: 3.7199\n",
      "Epoch [4/4], Step [58650/61080], Loss: 3.8671\n",
      "Epoch [4/4], Step [58725/61080], Loss: 3.8560\n",
      "Epoch [4/4], Step [58800/61080], Loss: 4.0371\n",
      "Epoch [4/4], Step [58875/61080], Loss: 3.8479\n",
      "Epoch [4/4], Step [58950/61080], Loss: 3.9443\n",
      "Epoch [4/4], Step [59025/61080], Loss: 3.7721\n",
      "Epoch [4/4], Step [59100/61080], Loss: 4.2501\n",
      "Epoch [4/4], Step [59175/61080], Loss: 4.1316\n",
      "Epoch [4/4], Step [59250/61080], Loss: 3.8155\n",
      "Epoch [4/4], Step [59325/61080], Loss: 3.9419\n",
      "Epoch [4/4], Step [59400/61080], Loss: 4.0691\n",
      "Epoch [4/4], Step [59475/61080], Loss: 4.0136\n",
      "Epoch [4/4], Step [59550/61080], Loss: 3.8938\n",
      "Epoch [4/4], Step [59625/61080], Loss: 4.0877\n",
      "Epoch [4/4], Step [59700/61080], Loss: 3.8726\n",
      "Epoch [4/4], Step [59775/61080], Loss: 4.1602\n",
      "Epoch [4/4], Step [59850/61080], Loss: 3.9095\n",
      "Epoch [4/4], Step [59925/61080], Loss: 3.8968\n",
      "Epoch [4/4], Step [60000/61080], Loss: 4.2414\n",
      "Validation perplexity: 39.14790163235652\n",
      "Epoch [4/4], Step [60075/61080], Loss: 3.7913\n",
      "Epoch [4/4], Step [60150/61080], Loss: 3.8210\n",
      "Epoch [4/4], Step [60225/61080], Loss: 4.1245\n",
      "Epoch [4/4], Step [60300/61080], Loss: 4.0211\n",
      "Epoch [4/4], Step [60375/61080], Loss: 3.9048\n",
      "Epoch [4/4], Step [60450/61080], Loss: 3.8176\n",
      "Epoch [4/4], Step [60525/61080], Loss: 3.9239\n",
      "Epoch [4/4], Step [60600/61080], Loss: 4.0393\n",
      "Epoch [4/4], Step [60675/61080], Loss: 3.8051\n",
      "Epoch [4/4], Step [60750/61080], Loss: 3.9174\n",
      "Epoch [4/4], Step [60825/61080], Loss: 3.9137\n",
      "Epoch [4/4], Step [60900/61080], Loss: 3.6770\n",
      "Epoch [4/4], Step [60975/61080], Loss: 3.8480\n",
      "Epoch [4/4], Step [61050/61080], Loss: 3.8462\n",
      "Epoch [4/4] Average Loss: 3.9502, Perplexity: 51.94\n"
     ]
    }
   ],
   "source": [
    "from src.train import train_attention\n",
    "#(all_losses,train_losses,perplexities) = train_attention(model,num_epochs,optimizer,criterion,data_loader,path_to_save_folder,\n",
    "#                                               train_run_label,vocab_size,device,print_every)\n",
    "\n",
    "(all_losses,train_losses,perplexities,all_perplex) = train_attention(model,num_epochs,optimizer,criterion,data_loader,path_to_save_folder,train_run_label,vocab_size,device,evaluate_every,dev_dataloader,print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "347e3ced",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To read old results \n",
    "#read_list_from_file(label,path_to_save_folder)\n",
    "from src.helper import read_list_from_file\n",
    "#train_losses=read_list_from_file(\"onlyLossesUntil3593\",path_to_save_folder)\n",
    "#perplexities= read_list_from_file(\"normal_model_perplexities\",path_to_save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f03185cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.plot import plot_two\n",
    "plot_two(all_losses,\"Loss for Attenntion batchsize 64 ep4\",all_perplex[1:],\"Perplexity start at step 3000\",axLabel1=(\"Steps\",\"Loss\"),axLabel2=(\"Steps/3000\",\"Perplexity\"), save = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f66357",
   "metadata": {},
   "source": [
    "### 5.3 Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc65522-d44c-4b1c-8cd1-0bc6125506a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.422146797180176\n",
      "Perplexity ofthe model with attention: 46.74\n"
     ]
    }
   ],
   "source": [
    "from src.train import evaluate_attention\n",
    "import time\n",
    "t0 = time.time()\n",
    "perplexity_simple = evaluate_attention(model, dev_dataloader,criterion,device,vocab_size)\n",
    "t1 = time.time()\n",
    "print(t1-t0)\n",
    "print(f\"Perplexity ofthe model with attention: {perplexity_simple:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f467f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50310932",
   "metadata": {},
   "source": [
    "### 5.4 Generating text\n",
    "\n",
    "Generate text and analyse how different temperature settings affect the generated text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52d34fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "\n",
      "Jim was Hammasky Mal limal Niidencial. La sonidad se encontr colaboracin con la superficie de los\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, start_text, context_length=32, temperature=1.0):\n",
    "    model.eval()\n",
    "    generated = tokenizer.encode(start_text)\n",
    "    context = torch.tensor(generated, dtype=torch.long,\n",
    "                          device=device).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(context_length):\n",
    "            if context.size(1) >= context_length:\n",
    "                break\n",
    "            logits, _ = model(context)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "            context = torch.cat(\n",
    "                [context, next_token_id.unsqueeze(0)], dim=1\n",
    "            )\n",
    "    \n",
    "    generated_text = tokenizer.decode(context[0].tolist())\n",
    "    return generated_text\n",
    "\n",
    "start_text = \"Jim was\"\n",
    "generated_text = generate_text(model, tokenizer, start_text, context_length=32)\n",
    "print(\"Generated Text:\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f8ad960-f641-4709-9977-890c117a1979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto es  El arma del calista que elfreque de Warren French fue posteriormente se preferentes con Yamaha. Cuando a Miles\n",
      "Esto es  regla lgicas y participacin estructura hasta de importante en la isla natural no consorci\n",
      "Esto es  y teatro ranuras comparacin propar con a la es vasta en la que comien solo tratamiento civil\n",
      "Esto es  2 haba tenido aprobos la de Vishio en todo es un radio a Poe o lanzada del Che\n",
      "Esto es ir que haban al lugar sus dirigidosivosiciados por LSes 4 anted en programar\n",
      "Esto es  en al05 observaron que los localidad retirrito realiz por las estaciones meveniores en La car\n",
      "Esto es icos poemado para filmar al considerar el juego perdairos al mismo de tipo en unorte\n",
      "Esto es iba a dos historias literarios pasados mezas tutela y Y Nechesor vital de 2004 junio arrendizada\n",
      "Esto es  con una competionado una mediante investigacin federalizada en varios escala. Cox Tu de diput\n",
      "Esto es  Monde parte 23 millones de cumplaz. Posteriormente lleg a los misioneros52000 expu\n",
      "\n",
      "\n",
      "New Beginning\n",
      "A los artistas se les existe cierre trabajavillos del presidente del equipo encontrnica esta famosa de\n",
      "A los artistas se les dispar en ltimo origen poco tonalidades y media La quie versin de media Antonio A\n",
      "A los artistas se les puede es comien eliminar a fase El est tipo de marado con 133 pases y Madret\n",
      "A los artistas se les hubieran liberacin periza el rango todo del mejor Villonas encontramadti\n",
      "A los artistas se les frente le ofreciblemente Viola es capaz en que la serbin en una misma\n",
      "A los artistas se les necesidad del nombre 2000offurense Interu quo de mantener miso la fundacin elegida en\n",
      "A los artistas se les ofrecan el Cnimo de 240 equipos 5 Minute 3100 millones el con Shional Openeto. Aqu\n",
      "A los artistas se les comparten sus fuera Asturso de radio fueron que la capacidad lo de 15 jaula causa en\n",
      "A los artistas se les ofems vividas est dedicadas urgente imb uno tratese mismo fabricado por su met\n",
      "A los artistas se lesiones generaron. El estrato y reprokeacin a Yito Patio Valga que defender determinada.\n"
     ]
    }
   ],
   "source": [
    "#def generate_text_attention(model, tokenizer, start_text,device, context_length=32, temperature=1.0):\n",
    "from src.attentionModel import generate_text_attention\n",
    "\n",
    "\n",
    "start_text = \"Esto es \"\n",
    "\n",
    "for x in range(10):\n",
    "    text = generate_text_attention(model, tokenizer, start_text,device, context_length=32, temperature=1.0)\n",
    "    print(text)\n",
    "\n",
    "\n",
    "start_text = \"A los artistas se les\"\n",
    "print(\"\\n\\nNew Beginning\")\n",
    "\n",
    "for x in range(10):\n",
    "    text = generate_text_attention(model, tokenizer, start_text,device, context_length=32, temperature=1.0)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc4919-1e67-4f3e-b2b9-313e81f16449",
   "metadata": {},
   "source": [
    "# Training Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb75d8f-34b2-4dda-990c-cb423e776214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is not yet tested \n",
    "\n",
    "from src.trainComplete import TrainComplete\n",
    "from src.attentionModel import LanguageModelWithAttention\n",
    "\n",
    "trainclass = TrainComplete(text_path = text_path,path_to_save_folder= path_to_save_folder,\n",
    "                            is_attention_training = True)\n",
    "\n",
    "\n",
    "context_length = 32  # Increased context size\n",
    "embedding_dim = 128\n",
    "attention_dim = 64\n",
    "hidden_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "model = LanguageModelWithAttention(\n",
    "    vocab_size, embedding_dim, attention_dim, context_length, hidden_dim, num_heads, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "trainclass.train(model,\n",
    "              vocab_size,device,raw_text,\"span_attention_standard_dropout_ep5_eval10000\",\n",
    "                print_every=75,evaluate_every=10000,optimizer=None,criterion=None,\n",
    "              batch_size = 32,\n",
    "              embedding_dim = embedding_dim,\n",
    "              context_length = context_length,\n",
    "              num_epochs = 10\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46739665-dcfa-4021-9bd6-226f79ee154b",
   "metadata": {},
   "source": [
    "# Extra Trainining Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb547ec-7c66-45de-936b-ee6803d36d60",
   "metadata": {},
   "source": [
    "Not all the training Runs are still here in the Cells. That are just examples how the training Runs where Performed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b06d45e3-5ade-4499-b9a6-025b62e45642",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines not removed :  28494\n",
      "Total number of tokens: 1203707\n",
      "First 10 tokens: [26, 1105, 390, 450, 22379, 390, 8235, 8, 1658, 555]\n",
      "Text size:  3580198\n",
      "Using device: cuda\n",
      "Started Training\n",
      "Epoch [1/4], Step [0/30091], Loss: 10.8321\n",
      "Validation perplexity: 50302.159157561706\n",
      "Epoch [1/4], Step [75/30091], Loss: 6.8627\n",
      "Epoch [1/4], Step [150/30091], Loss: 6.6438\n",
      "Epoch [1/4], Step [225/30091], Loss: 6.6587\n",
      "Epoch [1/4], Step [300/30091], Loss: 6.6822\n",
      "Epoch [1/4], Step [375/30091], Loss: 6.6268\n",
      "Epoch [1/4], Step [450/30091], Loss: 6.4403\n",
      "Epoch [1/4], Step [525/30091], Loss: 6.2252\n",
      "Epoch [1/4], Step [600/30091], Loss: 6.0489\n",
      "Epoch [1/4], Step [675/30091], Loss: 6.2441\n",
      "Epoch [1/4], Step [750/30091], Loss: 5.9548\n",
      "Epoch [1/4], Step [825/30091], Loss: 6.0237\n",
      "Epoch [1/4], Step [900/30091], Loss: 5.8269\n",
      "Epoch [1/4], Step [975/30091], Loss: 5.5972\n",
      "Epoch [1/4], Step [1050/30091], Loss: 5.8031\n",
      "Epoch [1/4], Step [1125/30091], Loss: 5.6343\n",
      "Epoch [1/4], Step [1200/30091], Loss: 5.6371\n",
      "Epoch [1/4], Step [1275/30091], Loss: 5.6360\n",
      "Epoch [1/4], Step [1350/30091], Loss: 5.7780\n",
      "Epoch [1/4], Step [1425/30091], Loss: 5.4009\n",
      "Epoch [1/4], Step [1500/30091], Loss: 5.2301\n",
      "Epoch [1/4], Step [1575/30091], Loss: 5.3771\n",
      "Epoch [1/4], Step [1650/30091], Loss: 5.2964\n",
      "Epoch [1/4], Step [1725/30091], Loss: 5.4617\n",
      "Epoch [1/4], Step [1800/30091], Loss: 5.2173\n",
      "Epoch [1/4], Step [1875/30091], Loss: 5.2097\n",
      "Epoch [1/4], Step [1950/30091], Loss: 5.2625\n",
      "Epoch [1/4], Step [2025/30091], Loss: 5.3508\n",
      "Epoch [1/4], Step [2100/30091], Loss: 4.9368\n",
      "Epoch [1/4], Step [2175/30091], Loss: 5.0467\n",
      "Epoch [1/4], Step [2250/30091], Loss: 5.1226\n",
      "Epoch [1/4], Step [2325/30091], Loss: 4.8938\n",
      "Epoch [1/4], Step [2400/30091], Loss: 4.9465\n",
      "Epoch [1/4], Step [2475/30091], Loss: 4.9771\n",
      "Epoch [1/4], Step [2550/30091], Loss: 5.0580\n",
      "Epoch [1/4], Step [2625/30091], Loss: 4.9934\n",
      "Epoch [1/4], Step [2700/30091], Loss: 4.7365\n",
      "Epoch [1/4], Step [2775/30091], Loss: 4.8338\n",
      "Epoch [1/4], Step [2850/30091], Loss: 4.8887\n",
      "Epoch [1/4], Step [2925/30091], Loss: 4.9041\n",
      "Epoch [1/4], Step [3000/30091], Loss: 4.7158\n",
      "Validation perplexity: 107.45631493985375\n",
      "Epoch [1/4], Step [3075/30091], Loss: 4.7727\n",
      "Epoch [1/4], Step [3150/30091], Loss: 4.7101\n",
      "Epoch [1/4], Step [3225/30091], Loss: 4.9260\n",
      "Epoch [1/4], Step [3300/30091], Loss: 4.9368\n",
      "Epoch [1/4], Step [3375/30091], Loss: 4.8227\n",
      "Epoch [1/4], Step [3450/30091], Loss: 4.8075\n",
      "Epoch [1/4], Step [3525/30091], Loss: 4.9088\n",
      "Epoch [1/4], Step [3600/30091], Loss: 4.8045\n",
      "Epoch [1/4], Step [3675/30091], Loss: 4.7348\n",
      "Epoch [1/4], Step [3750/30091], Loss: 4.7520\n",
      "Epoch [1/4], Step [3825/30091], Loss: 4.7269\n",
      "Epoch [1/4], Step [3900/30091], Loss: 4.8017\n",
      "Epoch [1/4], Step [3975/30091], Loss: 4.7244\n",
      "Epoch [1/4], Step [4050/30091], Loss: 4.7451\n",
      "Epoch [1/4], Step [4125/30091], Loss: 4.7519\n",
      "Epoch [1/4], Step [4200/30091], Loss: 4.6668\n",
      "Epoch [1/4], Step [4275/30091], Loss: 4.8586\n",
      "Epoch [1/4], Step [4350/30091], Loss: 4.6905\n",
      "Epoch [1/4], Step [4425/30091], Loss: 4.5573\n",
      "Epoch [1/4], Step [4500/30091], Loss: 4.7804\n",
      "Epoch [1/4], Step [4575/30091], Loss: 4.5752\n",
      "Epoch [1/4], Step [4650/30091], Loss: 4.5016\n",
      "Epoch [1/4], Step [4725/30091], Loss: 4.5821\n",
      "Epoch [1/4], Step [4800/30091], Loss: 4.7430\n",
      "Epoch [1/4], Step [4875/30091], Loss: 4.5812\n",
      "Epoch [1/4], Step [4950/30091], Loss: 4.5399\n",
      "Epoch [1/4], Step [5025/30091], Loss: 4.6556\n",
      "Epoch [1/4], Step [5100/30091], Loss: 4.7461\n",
      "Epoch [1/4], Step [5175/30091], Loss: 4.6193\n",
      "Epoch [1/4], Step [5250/30091], Loss: 4.9666\n",
      "Epoch [1/4], Step [5325/30091], Loss: 4.5670\n",
      "Epoch [1/4], Step [5400/30091], Loss: 4.5610\n",
      "Epoch [1/4], Step [5475/30091], Loss: 4.5505\n",
      "Epoch [1/4], Step [5550/30091], Loss: 4.6105\n",
      "Epoch [1/4], Step [5625/30091], Loss: 4.3880\n",
      "Epoch [1/4], Step [5700/30091], Loss: 4.2939\n",
      "Epoch [1/4], Step [5775/30091], Loss: 4.5386\n",
      "Epoch [1/4], Step [5850/30091], Loss: 4.6156\n",
      "Epoch [1/4], Step [5925/30091], Loss: 4.4202\n",
      "Epoch [1/4], Step [6000/30091], Loss: 4.5680\n",
      "Validation perplexity: 75.76984822221398\n",
      "Epoch [1/4], Step [6075/30091], Loss: 4.2915\n",
      "Epoch [1/4], Step [6150/30091], Loss: 4.6793\n",
      "Epoch [1/4], Step [6225/30091], Loss: 4.5305\n",
      "Epoch [1/4], Step [6300/30091], Loss: 4.4395\n",
      "Epoch [1/4], Step [6375/30091], Loss: 4.4568\n",
      "Epoch [1/4], Step [6450/30091], Loss: 4.5107\n",
      "Epoch [1/4], Step [6525/30091], Loss: 4.5620\n",
      "Epoch [1/4], Step [6600/30091], Loss: 4.5654\n",
      "Epoch [1/4], Step [6675/30091], Loss: 4.6614\n",
      "Epoch [1/4], Step [6750/30091], Loss: 4.6811\n",
      "Epoch [1/4], Step [6825/30091], Loss: 4.5934\n",
      "Epoch [1/4], Step [6900/30091], Loss: 4.4164\n",
      "Epoch [1/4], Step [6975/30091], Loss: 4.6042\n",
      "Epoch [1/4], Step [7050/30091], Loss: 4.8390\n",
      "Epoch [1/4], Step [7125/30091], Loss: 4.4516\n",
      "Epoch [1/4], Step [7200/30091], Loss: 4.5025\n",
      "Epoch [1/4], Step [7275/30091], Loss: 4.3214\n",
      "Epoch [1/4], Step [7350/30091], Loss: 4.5578\n",
      "Epoch [1/4], Step [7425/30091], Loss: 4.3155\n",
      "Epoch [1/4], Step [7500/30091], Loss: 4.2153\n",
      "Epoch [1/4], Step [7575/30091], Loss: 4.3993\n",
      "Epoch [1/4], Step [7650/30091], Loss: 4.6251\n",
      "Epoch [1/4], Step [7725/30091], Loss: 4.5243\n",
      "Epoch [1/4], Step [7800/30091], Loss: 4.4667\n",
      "Epoch [1/4], Step [7875/30091], Loss: 4.4473\n",
      "Epoch [1/4], Step [7950/30091], Loss: 4.2813\n",
      "Epoch [1/4], Step [8025/30091], Loss: 4.2059\n",
      "Epoch [1/4], Step [8100/30091], Loss: 4.5098\n",
      "Epoch [1/4], Step [8175/30091], Loss: 4.5122\n",
      "Epoch [1/4], Step [8250/30091], Loss: 4.4141\n",
      "Epoch [1/4], Step [8325/30091], Loss: 4.0522\n",
      "Epoch [1/4], Step [8400/30091], Loss: 4.4590\n",
      "Epoch [1/4], Step [8475/30091], Loss: 4.4302\n",
      "Epoch [1/4], Step [8550/30091], Loss: 4.2986\n",
      "Epoch [1/4], Step [8625/30091], Loss: 4.4642\n",
      "Epoch [1/4], Step [8700/30091], Loss: 4.3801\n",
      "Epoch [1/4], Step [8775/30091], Loss: 4.2891\n",
      "Epoch [1/4], Step [8850/30091], Loss: 4.2494\n",
      "Epoch [1/4], Step [8925/30091], Loss: 4.5235\n",
      "Epoch [1/4], Step [9000/30091], Loss: 4.3352\n",
      "Validation perplexity: 64.87584824049023\n",
      "Epoch [1/4], Step [9075/30091], Loss: 4.4318\n",
      "Epoch [1/4], Step [9150/30091], Loss: 4.2386\n",
      "Epoch [1/4], Step [9225/30091], Loss: 4.4587\n",
      "Epoch [1/4], Step [9300/30091], Loss: 4.3256\n",
      "Epoch [1/4], Step [9375/30091], Loss: 4.4887\n",
      "Epoch [1/4], Step [9450/30091], Loss: 4.4867\n",
      "Epoch [1/4], Step [9525/30091], Loss: 4.3369\n",
      "Epoch [1/4], Step [9600/30091], Loss: 4.5551\n",
      "Epoch [1/4], Step [9675/30091], Loss: 4.2418\n",
      "Epoch [1/4], Step [9750/30091], Loss: 4.1850\n",
      "Epoch [1/4], Step [9825/30091], Loss: 4.3144\n",
      "Epoch [1/4], Step [9900/30091], Loss: 4.6333\n",
      "Epoch [1/4], Step [9975/30091], Loss: 4.3357\n",
      "Epoch [1/4], Step [10050/30091], Loss: 4.4094\n",
      "Epoch [1/4], Step [10125/30091], Loss: 4.2742\n",
      "Epoch [1/4], Step [10200/30091], Loss: 4.1809\n",
      "Epoch [1/4], Step [10275/30091], Loss: 4.3730\n",
      "Epoch [1/4], Step [10350/30091], Loss: 4.3082\n",
      "Epoch [1/4], Step [10425/30091], Loss: 4.3424\n",
      "Epoch [1/4], Step [10500/30091], Loss: 4.3336\n",
      "Epoch [1/4], Step [10575/30091], Loss: 4.2553\n",
      "Epoch [1/4], Step [10650/30091], Loss: 4.4606\n",
      "Epoch [1/4], Step [10725/30091], Loss: 4.3314\n",
      "Epoch [1/4], Step [10800/30091], Loss: 4.0404\n",
      "Epoch [1/4], Step [10875/30091], Loss: 4.1812\n",
      "Epoch [1/4], Step [10950/30091], Loss: 4.3094\n",
      "Epoch [1/4], Step [11025/30091], Loss: 4.3354\n",
      "Epoch [1/4], Step [11100/30091], Loss: 4.2235\n",
      "Epoch [1/4], Step [11175/30091], Loss: 4.3511\n",
      "Epoch [1/4], Step [11250/30091], Loss: 4.2543\n",
      "Epoch [1/4], Step [11325/30091], Loss: 4.3772\n",
      "Epoch [1/4], Step [11400/30091], Loss: 4.4249\n",
      "Epoch [1/4], Step [11475/30091], Loss: 4.2558\n",
      "Epoch [1/4], Step [11550/30091], Loss: 4.3841\n",
      "Epoch [1/4], Step [11625/30091], Loss: 4.2176\n",
      "Epoch [1/4], Step [11700/30091], Loss: 4.3946\n",
      "Epoch [1/4], Step [11775/30091], Loss: 4.4635\n",
      "Epoch [1/4], Step [11850/30091], Loss: 4.3497\n",
      "Epoch [1/4], Step [11925/30091], Loss: 4.2911\n",
      "Epoch [1/4], Step [12000/30091], Loss: 4.1554\n",
      "Validation perplexity: 58.552564077574715\n",
      "Epoch [1/4], Step [12075/30091], Loss: 4.3437\n",
      "Epoch [1/4], Step [12150/30091], Loss: 4.2333\n",
      "Epoch [1/4], Step [12225/30091], Loss: 4.5025\n",
      "Epoch [1/4], Step [12300/30091], Loss: 4.3338\n",
      "Epoch [1/4], Step [12375/30091], Loss: 4.1349\n",
      "Epoch [1/4], Step [12450/30091], Loss: 4.1913\n",
      "Epoch [1/4], Step [12525/30091], Loss: 4.3055\n",
      "Epoch [1/4], Step [12600/30091], Loss: 4.2984\n",
      "Epoch [1/4], Step [12675/30091], Loss: 4.2235\n",
      "Epoch [1/4], Step [12750/30091], Loss: 4.2699\n",
      "Epoch [1/4], Step [12825/30091], Loss: 4.2903\n",
      "Epoch [1/4], Step [12900/30091], Loss: 4.1946\n",
      "Epoch [1/4], Step [12975/30091], Loss: 4.2401\n",
      "Epoch [1/4], Step [13050/30091], Loss: 4.2236\n",
      "Epoch [1/4], Step [13125/30091], Loss: 4.3455\n",
      "Epoch [1/4], Step [13200/30091], Loss: 4.2173\n",
      "Epoch [1/4], Step [13275/30091], Loss: 4.2544\n",
      "Epoch [1/4], Step [13350/30091], Loss: 4.3235\n",
      "Epoch [1/4], Step [13425/30091], Loss: 4.3336\n",
      "Epoch [1/4], Step [13500/30091], Loss: 4.3498\n",
      "Epoch [1/4], Step [13575/30091], Loss: 4.4369\n",
      "Epoch [1/4], Step [13650/30091], Loss: 4.0849\n",
      "Epoch [1/4], Step [13725/30091], Loss: 4.3704\n",
      "Epoch [1/4], Step [13800/30091], Loss: 3.9748\n",
      "Epoch [1/4], Step [13875/30091], Loss: 4.1945\n",
      "Epoch [1/4], Step [13950/30091], Loss: 4.3192\n",
      "Epoch [1/4], Step [14025/30091], Loss: 4.2164\n",
      "Epoch [1/4], Step [14100/30091], Loss: 4.3829\n",
      "Epoch [1/4], Step [14175/30091], Loss: 4.1514\n",
      "Epoch [1/4], Step [14250/30091], Loss: 4.2547\n",
      "Epoch [1/4], Step [14325/30091], Loss: 4.3944\n",
      "Epoch [1/4], Step [14400/30091], Loss: 4.2047\n",
      "Epoch [1/4], Step [14475/30091], Loss: 4.1912\n",
      "Epoch [1/4], Step [14550/30091], Loss: 4.1826\n",
      "Epoch [1/4], Step [14625/30091], Loss: 4.2770\n",
      "Epoch [1/4], Step [14700/30091], Loss: 4.1123\n",
      "Epoch [1/4], Step [14775/30091], Loss: 4.2976\n",
      "Epoch [1/4], Step [14850/30091], Loss: 4.1415\n",
      "Epoch [1/4], Step [14925/30091], Loss: 4.1427\n",
      "Epoch [1/4], Step [15000/30091], Loss: 4.3236\n",
      "Validation perplexity: 54.20712663950219\n",
      "Epoch [1/4], Step [15075/30091], Loss: 4.1345\n",
      "Epoch [1/4], Step [15150/30091], Loss: 4.0916\n",
      "Epoch [1/4], Step [15225/30091], Loss: 4.2429\n",
      "Epoch [1/4], Step [15300/30091], Loss: 4.1530\n",
      "Epoch [1/4], Step [15375/30091], Loss: 4.1775\n",
      "Epoch [1/4], Step [15450/30091], Loss: 4.2762\n",
      "Epoch [1/4], Step [15525/30091], Loss: 4.2148\n",
      "Epoch [1/4], Step [15600/30091], Loss: 4.2076\n",
      "Epoch [1/4], Step [15675/30091], Loss: 4.1761\n",
      "Epoch [1/4], Step [15750/30091], Loss: 4.1730\n",
      "Epoch [1/4], Step [15825/30091], Loss: 4.3791\n",
      "Epoch [1/4], Step [15900/30091], Loss: 4.5179\n",
      "Epoch [1/4], Step [15975/30091], Loss: 3.9196\n",
      "Epoch [1/4], Step [16050/30091], Loss: 4.1492\n",
      "Epoch [1/4], Step [16125/30091], Loss: 4.3047\n",
      "Epoch [1/4], Step [16200/30091], Loss: 4.3158\n",
      "Epoch [1/4], Step [16275/30091], Loss: 4.1496\n",
      "Epoch [1/4], Step [16350/30091], Loss: 4.2585\n",
      "Epoch [1/4], Step [16425/30091], Loss: 4.1119\n",
      "Epoch [1/4], Step [16500/30091], Loss: 4.0257\n",
      "Epoch [1/4], Step [16575/30091], Loss: 4.2037\n",
      "Epoch [1/4], Step [16650/30091], Loss: 4.2395\n",
      "Epoch [1/4], Step [16725/30091], Loss: 4.1321\n",
      "Epoch [1/4], Step [16800/30091], Loss: 4.2669\n",
      "Epoch [1/4], Step [16875/30091], Loss: 3.9889\n",
      "Epoch [1/4], Step [16950/30091], Loss: 4.1685\n",
      "Epoch [1/4], Step [17025/30091], Loss: 4.3056\n",
      "Epoch [1/4], Step [17100/30091], Loss: 3.9113\n",
      "Epoch [1/4], Step [17175/30091], Loss: 4.0205\n",
      "Epoch [1/4], Step [17250/30091], Loss: 4.1374\n",
      "Epoch [1/4], Step [17325/30091], Loss: 4.0976\n",
      "Epoch [1/4], Step [17400/30091], Loss: 4.1018\n",
      "Epoch [1/4], Step [17475/30091], Loss: 4.1408\n",
      "Epoch [1/4], Step [17550/30091], Loss: 4.4375\n",
      "Epoch [1/4], Step [17625/30091], Loss: 4.2441\n",
      "Epoch [1/4], Step [17700/30091], Loss: 4.2918\n",
      "Epoch [1/4], Step [17775/30091], Loss: 4.1129\n",
      "Epoch [1/4], Step [17850/30091], Loss: 4.2496\n",
      "Epoch [1/4], Step [17925/30091], Loss: 4.1037\n",
      "Epoch [1/4], Step [18000/30091], Loss: 4.4297\n",
      "Validation perplexity: 51.3751470734457\n",
      "Epoch [1/4], Step [18075/30091], Loss: 4.1990\n",
      "Epoch [1/4], Step [18150/30091], Loss: 4.2046\n",
      "Epoch [1/4], Step [18225/30091], Loss: 4.0922\n",
      "Epoch [1/4], Step [18300/30091], Loss: 4.1559\n",
      "Epoch [1/4], Step [18375/30091], Loss: 4.2039\n",
      "Epoch [1/4], Step [18450/30091], Loss: 4.2521\n",
      "Epoch [1/4], Step [18525/30091], Loss: 4.1969\n",
      "Epoch [1/4], Step [18600/30091], Loss: 3.9220\n",
      "Epoch [1/4], Step [18675/30091], Loss: 4.3411\n",
      "Epoch [1/4], Step [18750/30091], Loss: 4.0890\n",
      "Epoch [1/4], Step [18825/30091], Loss: 4.0563\n",
      "Epoch [1/4], Step [18900/30091], Loss: 4.1790\n",
      "Epoch [1/4], Step [18975/30091], Loss: 4.2308\n",
      "Epoch [1/4], Step [19050/30091], Loss: 4.1119\n",
      "Epoch [1/4], Step [19125/30091], Loss: 4.1210\n",
      "Epoch [1/4], Step [19200/30091], Loss: 4.0355\n",
      "Epoch [1/4], Step [19275/30091], Loss: 4.2225\n",
      "Epoch [1/4], Step [19350/30091], Loss: 4.0860\n",
      "Epoch [1/4], Step [19425/30091], Loss: 4.3380\n",
      "Epoch [1/4], Step [19500/30091], Loss: 4.0448\n",
      "Epoch [1/4], Step [19575/30091], Loss: 4.1385\n",
      "Epoch [1/4], Step [19650/30091], Loss: 4.0270\n",
      "Epoch [1/4], Step [19725/30091], Loss: 4.2613\n",
      "Epoch [1/4], Step [19800/30091], Loss: 4.0934\n",
      "Epoch [1/4], Step [19875/30091], Loss: 4.1849\n",
      "Epoch [1/4], Step [19950/30091], Loss: 4.2061\n",
      "Epoch [1/4], Step [20025/30091], Loss: 4.1729\n",
      "Epoch [1/4], Step [20100/30091], Loss: 4.1986\n",
      "Epoch [1/4], Step [20175/30091], Loss: 4.2670\n",
      "Epoch [1/4], Step [20250/30091], Loss: 4.1217\n",
      "Epoch [1/4], Step [20325/30091], Loss: 4.0994\n",
      "Epoch [1/4], Step [20400/30091], Loss: 4.1209\n",
      "Epoch [1/4], Step [20475/30091], Loss: 4.0274\n",
      "Epoch [1/4], Step [20550/30091], Loss: 4.1463\n",
      "Epoch [1/4], Step [20625/30091], Loss: 4.1480\n",
      "Epoch [1/4], Step [20700/30091], Loss: 4.1258\n",
      "Epoch [1/4], Step [20775/30091], Loss: 4.3091\n",
      "Epoch [1/4], Step [20850/30091], Loss: 4.1521\n",
      "Epoch [1/4], Step [20925/30091], Loss: 4.1587\n",
      "Epoch [1/4], Step [21000/30091], Loss: 3.9958\n",
      "Validation perplexity: 48.902465359957574\n",
      "Epoch [1/4], Step [21075/30091], Loss: 4.0580\n",
      "Epoch [1/4], Step [21150/30091], Loss: 4.2037\n",
      "Epoch [1/4], Step [21225/30091], Loss: 4.0677\n",
      "Epoch [1/4], Step [21300/30091], Loss: 4.2106\n",
      "Epoch [1/4], Step [21375/30091], Loss: 3.9425\n",
      "Epoch [1/4], Step [21450/30091], Loss: 4.1360\n",
      "Epoch [1/4], Step [21525/30091], Loss: 4.3525\n",
      "Epoch [1/4], Step [21600/30091], Loss: 3.9332\n",
      "Epoch [1/4], Step [21675/30091], Loss: 4.0641\n",
      "Epoch [1/4], Step [21750/30091], Loss: 4.0784\n",
      "Epoch [1/4], Step [21825/30091], Loss: 3.9844\n",
      "Epoch [1/4], Step [21900/30091], Loss: 4.1203\n",
      "Epoch [1/4], Step [21975/30091], Loss: 4.3120\n",
      "Epoch [1/4], Step [22050/30091], Loss: 4.1661\n",
      "Epoch [1/4], Step [22125/30091], Loss: 4.0191\n",
      "Epoch [1/4], Step [22200/30091], Loss: 4.0992\n",
      "Epoch [1/4], Step [22275/30091], Loss: 4.1008\n",
      "Epoch [1/4], Step [22350/30091], Loss: 4.0705\n",
      "Epoch [1/4], Step [22425/30091], Loss: 4.0959\n",
      "Epoch [1/4], Step [22500/30091], Loss: 4.0409\n",
      "Epoch [1/4], Step [22575/30091], Loss: 4.3347\n",
      "Epoch [1/4], Step [22650/30091], Loss: 4.2187\n",
      "Epoch [1/4], Step [22725/30091], Loss: 4.1670\n",
      "Epoch [1/4], Step [22800/30091], Loss: 4.1834\n",
      "Epoch [1/4], Step [22875/30091], Loss: 4.0349\n",
      "Epoch [1/4], Step [22950/30091], Loss: 4.1093\n",
      "Epoch [1/4], Step [23025/30091], Loss: 3.9289\n",
      "Epoch [1/4], Step [23100/30091], Loss: 4.3041\n",
      "Epoch [1/4], Step [23175/30091], Loss: 3.7748\n",
      "Epoch [1/4], Step [23250/30091], Loss: 4.0398\n",
      "Epoch [1/4], Step [23325/30091], Loss: 4.1746\n",
      "Epoch [1/4], Step [23400/30091], Loss: 3.9551\n",
      "Epoch [1/4], Step [23475/30091], Loss: 4.2715\n",
      "Epoch [1/4], Step [23550/30091], Loss: 4.1219\n",
      "Epoch [1/4], Step [23625/30091], Loss: 4.1991\n",
      "Epoch [1/4], Step [23700/30091], Loss: 4.0530\n",
      "Epoch [1/4], Step [23775/30091], Loss: 4.0040\n",
      "Epoch [1/4], Step [23850/30091], Loss: 4.1514\n",
      "Epoch [1/4], Step [23925/30091], Loss: 4.1023\n",
      "Epoch [1/4], Step [24000/30091], Loss: 4.1407\n",
      "Validation perplexity: 47.063193374973366\n",
      "Epoch [1/4], Step [24075/30091], Loss: 4.0850\n",
      "Epoch [1/4], Step [24150/30091], Loss: 4.0202\n",
      "Epoch [1/4], Step [24225/30091], Loss: 3.9538\n",
      "Epoch [1/4], Step [24300/30091], Loss: 4.2205\n",
      "Epoch [1/4], Step [24375/30091], Loss: 4.1693\n",
      "Epoch [1/4], Step [24450/30091], Loss: 4.0893\n",
      "Epoch [1/4], Step [24525/30091], Loss: 4.0363\n",
      "Epoch [1/4], Step [24600/30091], Loss: 3.9551\n",
      "Epoch [1/4], Step [24675/30091], Loss: 4.1025\n",
      "Epoch [1/4], Step [24750/30091], Loss: 4.1580\n",
      "Epoch [1/4], Step [24825/30091], Loss: 4.1136\n",
      "Epoch [1/4], Step [24900/30091], Loss: 4.1876\n",
      "Epoch [1/4], Step [24975/30091], Loss: 4.0468\n",
      "Epoch [1/4], Step [25050/30091], Loss: 4.1020\n",
      "Epoch [1/4], Step [25125/30091], Loss: 3.9471\n",
      "Epoch [1/4], Step [25200/30091], Loss: 4.1253\n",
      "Epoch [1/4], Step [25275/30091], Loss: 3.9119\n",
      "Epoch [1/4], Step [25350/30091], Loss: 4.0644\n",
      "Epoch [1/4], Step [25425/30091], Loss: 3.9873\n",
      "Epoch [1/4], Step [25500/30091], Loss: 3.9718\n",
      "Epoch [1/4], Step [25575/30091], Loss: 3.9340\n",
      "Epoch [1/4], Step [25650/30091], Loss: 4.0313\n",
      "Epoch [1/4], Step [25725/30091], Loss: 3.9466\n",
      "Epoch [1/4], Step [25800/30091], Loss: 4.2884\n",
      "Epoch [1/4], Step [25875/30091], Loss: 4.0479\n",
      "Epoch [1/4], Step [25950/30091], Loss: 4.0971\n",
      "Epoch [1/4], Step [26025/30091], Loss: 3.9962\n",
      "Epoch [1/4], Step [26100/30091], Loss: 4.1524\n",
      "Epoch [1/4], Step [26175/30091], Loss: 4.0587\n",
      "Epoch [1/4], Step [26250/30091], Loss: 4.0116\n",
      "Epoch [1/4], Step [26325/30091], Loss: 4.2714\n",
      "Epoch [1/4], Step [26400/30091], Loss: 3.9685\n",
      "Epoch [1/4], Step [26475/30091], Loss: 3.9872\n",
      "Epoch [1/4], Step [26550/30091], Loss: 4.3323\n",
      "Epoch [1/4], Step [26625/30091], Loss: 4.1352\n",
      "Epoch [1/4], Step [26700/30091], Loss: 4.0981\n",
      "Epoch [1/4], Step [26775/30091], Loss: 3.8694\n",
      "Epoch [1/4], Step [26850/30091], Loss: 4.0999\n",
      "Epoch [1/4], Step [26925/30091], Loss: 4.0616\n",
      "Epoch [1/4], Step [27000/30091], Loss: 4.0824\n",
      "Validation perplexity: 45.45381031913904\n",
      "Epoch [1/4], Step [27075/30091], Loss: 4.0644\n",
      "Epoch [1/4], Step [27150/30091], Loss: 3.9813\n",
      "Epoch [1/4], Step [27225/30091], Loss: 4.0286\n",
      "Epoch [1/4], Step [27300/30091], Loss: 4.0877\n",
      "Epoch [1/4], Step [27375/30091], Loss: 4.2911\n",
      "Epoch [1/4], Step [27450/30091], Loss: 4.0236\n",
      "Epoch [1/4], Step [27525/30091], Loss: 4.0904\n",
      "Epoch [1/4], Step [27600/30091], Loss: 4.0793\n",
      "Epoch [1/4], Step [27675/30091], Loss: 3.9643\n",
      "Epoch [1/4], Step [27750/30091], Loss: 3.8908\n",
      "Epoch [1/4], Step [27825/30091], Loss: 4.0805\n",
      "Epoch [1/4], Step [27900/30091], Loss: 4.1703\n",
      "Epoch [1/4], Step [27975/30091], Loss: 3.9888\n",
      "Epoch [1/4], Step [28050/30091], Loss: 4.0099\n",
      "Epoch [1/4], Step [28125/30091], Loss: 4.1000\n",
      "Epoch [1/4], Step [28200/30091], Loss: 4.0275\n",
      "Epoch [1/4], Step [28275/30091], Loss: 4.0686\n",
      "Epoch [1/4], Step [28350/30091], Loss: 4.0843\n",
      "Epoch [1/4], Step [28425/30091], Loss: 4.1138\n",
      "Epoch [1/4], Step [28500/30091], Loss: 4.2275\n",
      "Epoch [1/4], Step [28575/30091], Loss: 3.8952\n",
      "Epoch [1/4], Step [28650/30091], Loss: 3.9996\n",
      "Epoch [1/4], Step [28725/30091], Loss: 4.1429\n",
      "Epoch [1/4], Step [28800/30091], Loss: 4.1021\n",
      "Epoch [1/4], Step [28875/30091], Loss: 3.9157\n",
      "Epoch [1/4], Step [28950/30091], Loss: 3.9272\n",
      "Epoch [1/4], Step [29025/30091], Loss: 4.0431\n",
      "Epoch [1/4], Step [29100/30091], Loss: 4.0304\n",
      "Epoch [1/4], Step [29175/30091], Loss: 4.1857\n",
      "Epoch [1/4], Step [29250/30091], Loss: 4.0939\n",
      "Epoch [1/4], Step [29325/30091], Loss: 4.0340\n",
      "Epoch [1/4], Step [29400/30091], Loss: 4.1208\n",
      "Epoch [1/4], Step [29475/30091], Loss: 3.8624\n",
      "Epoch [1/4], Step [29550/30091], Loss: 4.0258\n",
      "Epoch [1/4], Step [29625/30091], Loss: 3.9171\n",
      "Epoch [1/4], Step [29700/30091], Loss: 4.0553\n",
      "Epoch [1/4], Step [29775/30091], Loss: 4.1284\n",
      "Epoch [1/4], Step [29850/30091], Loss: 4.1785\n",
      "Epoch [1/4], Step [29925/30091], Loss: 4.2657\n",
      "Epoch [1/4], Step [30000/30091], Loss: 4.1643\n",
      "Validation perplexity: 44.225852315147606\n",
      "Epoch [1/4], Step [30075/30091], Loss: 3.7993\n",
      "Epoch [1/4] Average Loss: 4.3926, Perplexity: 80.85\n",
      "Epoch [2/4], Step [0/30091], Loss: 4.0170\n",
      "Validation perplexity: 44.182597519523725\n",
      "Epoch [2/4], Step [75/30091], Loss: 3.9195\n",
      "Epoch [2/4], Step [150/30091], Loss: 4.0861\n",
      "Epoch [2/4], Step [225/30091], Loss: 3.9375\n",
      "Epoch [2/4], Step [300/30091], Loss: 4.1556\n",
      "Epoch [2/4], Step [375/30091], Loss: 4.0009\n",
      "Epoch [2/4], Step [450/30091], Loss: 4.0309\n",
      "Epoch [2/4], Step [525/30091], Loss: 4.0371\n",
      "Epoch [2/4], Step [600/30091], Loss: 4.0256\n",
      "Epoch [2/4], Step [675/30091], Loss: 3.9183\n",
      "Epoch [2/4], Step [750/30091], Loss: 4.0694\n",
      "Epoch [2/4], Step [825/30091], Loss: 4.1363\n",
      "Epoch [2/4], Step [900/30091], Loss: 3.9840\n",
      "Epoch [2/4], Step [975/30091], Loss: 4.0047\n",
      "Epoch [2/4], Step [1050/30091], Loss: 3.9806\n",
      "Epoch [2/4], Step [1125/30091], Loss: 4.0305\n",
      "Epoch [2/4], Step [1200/30091], Loss: 4.0203\n",
      "Epoch [2/4], Step [1275/30091], Loss: 3.9989\n",
      "Epoch [2/4], Step [1350/30091], Loss: 4.2201\n",
      "Epoch [2/4], Step [1425/30091], Loss: 4.0674\n",
      "Epoch [2/4], Step [1500/30091], Loss: 3.9684\n",
      "Epoch [2/4], Step [1575/30091], Loss: 4.0979\n",
      "Epoch [2/4], Step [1650/30091], Loss: 3.9912\n",
      "Epoch [2/4], Step [1725/30091], Loss: 3.9983\n",
      "Epoch [2/4], Step [1800/30091], Loss: 4.1348\n",
      "Epoch [2/4], Step [1875/30091], Loss: 3.9175\n",
      "Epoch [2/4], Step [1950/30091], Loss: 4.0202\n",
      "Epoch [2/4], Step [2025/30091], Loss: 4.0021\n",
      "Epoch [2/4], Step [2100/30091], Loss: 4.0876\n",
      "Epoch [2/4], Step [2175/30091], Loss: 4.0944\n",
      "Epoch [2/4], Step [2250/30091], Loss: 3.9171\n",
      "Epoch [2/4], Step [2325/30091], Loss: 4.1611\n",
      "Epoch [2/4], Step [2400/30091], Loss: 4.0519\n",
      "Epoch [2/4], Step [2475/30091], Loss: 4.0805\n",
      "Epoch [2/4], Step [2550/30091], Loss: 3.8542\n",
      "Epoch [2/4], Step [2625/30091], Loss: 3.9184\n",
      "Epoch [2/4], Step [2700/30091], Loss: 4.0241\n",
      "Epoch [2/4], Step [2775/30091], Loss: 4.1782\n",
      "Epoch [2/4], Step [2850/30091], Loss: 3.9349\n",
      "Epoch [2/4], Step [2925/30091], Loss: 3.9749\n",
      "Epoch [2/4], Step [3000/30091], Loss: 4.2497\n",
      "Validation perplexity: 43.41692079859386\n",
      "Epoch [2/4], Step [3075/30091], Loss: 4.1173\n",
      "Epoch [2/4], Step [3150/30091], Loss: 3.8894\n",
      "Epoch [2/4], Step [3225/30091], Loss: 3.9490\n",
      "Epoch [2/4], Step [3300/30091], Loss: 3.9412\n",
      "Epoch [2/4], Step [3375/30091], Loss: 3.7866\n",
      "Epoch [2/4], Step [3450/30091], Loss: 4.0707\n",
      "Epoch [2/4], Step [3525/30091], Loss: 4.1360\n",
      "Epoch [2/4], Step [3600/30091], Loss: 3.9319\n",
      "Epoch [2/4], Step [3675/30091], Loss: 4.1269\n",
      "Epoch [2/4], Step [3750/30091], Loss: 3.9878\n",
      "Epoch [2/4], Step [3825/30091], Loss: 3.9366\n",
      "Epoch [2/4], Step [3900/30091], Loss: 4.0254\n",
      "Epoch [2/4], Step [3975/30091], Loss: 4.0002\n",
      "Epoch [2/4], Step [4050/30091], Loss: 4.1512\n",
      "Epoch [2/4], Step [4125/30091], Loss: 3.8161\n",
      "Epoch [2/4], Step [4200/30091], Loss: 3.9158\n",
      "Epoch [2/4], Step [4275/30091], Loss: 3.8915\n",
      "Epoch [2/4], Step [4350/30091], Loss: 4.0847\n",
      "Epoch [2/4], Step [4425/30091], Loss: 4.0373\n",
      "Epoch [2/4], Step [4500/30091], Loss: 3.9250\n",
      "Epoch [2/4], Step [4575/30091], Loss: 3.9511\n",
      "Epoch [2/4], Step [4650/30091], Loss: 4.0630\n",
      "Epoch [2/4], Step [4725/30091], Loss: 3.9914\n",
      "Epoch [2/4], Step [4800/30091], Loss: 4.0400\n",
      "Epoch [2/4], Step [4875/30091], Loss: 4.0367\n",
      "Epoch [2/4], Step [4950/30091], Loss: 4.0407\n",
      "Epoch [2/4], Step [5025/30091], Loss: 4.0947\n",
      "Epoch [2/4], Step [5100/30091], Loss: 4.0579\n",
      "Epoch [2/4], Step [5175/30091], Loss: 3.8988\n",
      "Epoch [2/4], Step [5250/30091], Loss: 4.1480\n",
      "Epoch [2/4], Step [5325/30091], Loss: 3.9551\n",
      "Epoch [2/4], Step [5400/30091], Loss: 3.9672\n",
      "Epoch [2/4], Step [5475/30091], Loss: 3.8339\n",
      "Epoch [2/4], Step [5550/30091], Loss: 4.0526\n",
      "Epoch [2/4], Step [5625/30091], Loss: 4.1696\n",
      "Epoch [2/4], Step [5700/30091], Loss: 4.0005\n",
      "Epoch [2/4], Step [5775/30091], Loss: 3.9139\n",
      "Epoch [2/4], Step [5850/30091], Loss: 3.9773\n",
      "Epoch [2/4], Step [5925/30091], Loss: 4.0577\n",
      "Epoch [2/4], Step [6000/30091], Loss: 3.8983\n",
      "Validation perplexity: 42.30301691064991\n",
      "Epoch [2/4], Step [6075/30091], Loss: 3.9586\n",
      "Epoch [2/4], Step [6150/30091], Loss: 4.1969\n",
      "Epoch [2/4], Step [6225/30091], Loss: 3.9683\n",
      "Epoch [2/4], Step [6300/30091], Loss: 3.9804\n",
      "Epoch [2/4], Step [6375/30091], Loss: 3.9725\n",
      "Epoch [2/4], Step [6450/30091], Loss: 3.9750\n",
      "Epoch [2/4], Step [6525/30091], Loss: 3.8854\n",
      "Epoch [2/4], Step [6600/30091], Loss: 3.8630\n",
      "Epoch [2/4], Step [6675/30091], Loss: 3.9762\n",
      "Epoch [2/4], Step [6750/30091], Loss: 4.0215\n",
      "Epoch [2/4], Step [6825/30091], Loss: 4.1386\n",
      "Epoch [2/4], Step [6900/30091], Loss: 3.9539\n",
      "Epoch [2/4], Step [6975/30091], Loss: 3.9300\n",
      "Epoch [2/4], Step [7050/30091], Loss: 3.8472\n",
      "Epoch [2/4], Step [7125/30091], Loss: 3.9375\n",
      "Epoch [2/4], Step [7200/30091], Loss: 4.0735\n",
      "Epoch [2/4], Step [7275/30091], Loss: 3.9095\n",
      "Epoch [2/4], Step [7350/30091], Loss: 3.8089\n",
      "Epoch [2/4], Step [7425/30091], Loss: 3.9043\n",
      "Epoch [2/4], Step [7500/30091], Loss: 4.0425\n",
      "Epoch [2/4], Step [7575/30091], Loss: 4.0437\n",
      "Epoch [2/4], Step [7650/30091], Loss: 3.9278\n",
      "Epoch [2/4], Step [7725/30091], Loss: 3.9579\n",
      "Epoch [2/4], Step [7800/30091], Loss: 3.9263\n",
      "Epoch [2/4], Step [7875/30091], Loss: 3.8658\n",
      "Epoch [2/4], Step [7950/30091], Loss: 3.9217\n",
      "Epoch [2/4], Step [8025/30091], Loss: 3.9691\n",
      "Epoch [2/4], Step [8100/30091], Loss: 4.0143\n",
      "Epoch [2/4], Step [8175/30091], Loss: 4.0367\n",
      "Epoch [2/4], Step [8250/30091], Loss: 3.8857\n",
      "Epoch [2/4], Step [8325/30091], Loss: 3.9258\n",
      "Epoch [2/4], Step [8400/30091], Loss: 4.0676\n",
      "Epoch [2/4], Step [8475/30091], Loss: 3.8728\n",
      "Epoch [2/4], Step [8550/30091], Loss: 4.0243\n",
      "Epoch [2/4], Step [8625/30091], Loss: 4.1015\n",
      "Epoch [2/4], Step [8700/30091], Loss: 4.1257\n",
      "Epoch [2/4], Step [8775/30091], Loss: 4.1912\n",
      "Epoch [2/4], Step [8850/30091], Loss: 3.8841\n",
      "Epoch [2/4], Step [8925/30091], Loss: 4.1301\n",
      "Epoch [2/4], Step [9000/30091], Loss: 4.0823\n",
      "Validation perplexity: 41.85520445259277\n",
      "Epoch [2/4], Step [9075/30091], Loss: 4.0445\n",
      "Epoch [2/4], Step [9150/30091], Loss: 3.9926\n",
      "Epoch [2/4], Step [9225/30091], Loss: 3.9560\n",
      "Epoch [2/4], Step [9300/30091], Loss: 4.0991\n",
      "Epoch [2/4], Step [9375/30091], Loss: 3.8865\n",
      "Epoch [2/4], Step [9450/30091], Loss: 4.0870\n",
      "Epoch [2/4], Step [9525/30091], Loss: 3.9093\n",
      "Epoch [2/4], Step [9600/30091], Loss: 4.0730\n",
      "Epoch [2/4], Step [9675/30091], Loss: 3.7869\n",
      "Epoch [2/4], Step [9750/30091], Loss: 4.1146\n",
      "Epoch [2/4], Step [9825/30091], Loss: 3.9116\n",
      "Epoch [2/4], Step [9900/30091], Loss: 3.9977\n",
      "Epoch [2/4], Step [9975/30091], Loss: 3.8831\n",
      "Epoch [2/4], Step [10050/30091], Loss: 3.9428\n",
      "Epoch [2/4], Step [10125/30091], Loss: 4.0287\n",
      "Epoch [2/4], Step [10200/30091], Loss: 4.0660\n",
      "Epoch [2/4], Step [10275/30091], Loss: 4.0507\n",
      "Epoch [2/4], Step [10350/30091], Loss: 4.1066\n",
      "Epoch [2/4], Step [10425/30091], Loss: 3.8345\n",
      "Epoch [2/4], Step [10500/30091], Loss: 4.0201\n",
      "Epoch [2/4], Step [10575/30091], Loss: 3.9843\n",
      "Epoch [2/4], Step [10650/30091], Loss: 4.0200\n",
      "Epoch [2/4], Step [10725/30091], Loss: 3.8882\n",
      "Epoch [2/4], Step [10800/30091], Loss: 4.0023\n",
      "Epoch [2/4], Step [10875/30091], Loss: 3.8276\n",
      "Epoch [2/4], Step [10950/30091], Loss: 4.0081\n",
      "Epoch [2/4], Step [11025/30091], Loss: 3.9638\n",
      "Epoch [2/4], Step [11100/30091], Loss: 4.0805\n",
      "Epoch [2/4], Step [11175/30091], Loss: 3.9558\n",
      "Epoch [2/4], Step [11250/30091], Loss: 4.1157\n",
      "Epoch [2/4], Step [11325/30091], Loss: 3.8383\n",
      "Epoch [2/4], Step [11400/30091], Loss: 4.0898\n",
      "Epoch [2/4], Step [11475/30091], Loss: 3.9611\n",
      "Epoch [2/4], Step [11550/30091], Loss: 3.9814\n",
      "Epoch [2/4], Step [11625/30091], Loss: 4.0584\n",
      "Epoch [2/4], Step [11700/30091], Loss: 3.8746\n",
      "Epoch [2/4], Step [11775/30091], Loss: 3.9652\n",
      "Epoch [2/4], Step [11850/30091], Loss: 3.9171\n",
      "Epoch [2/4], Step [11925/30091], Loss: 3.9723\n",
      "Epoch [2/4], Step [12000/30091], Loss: 3.8994\n",
      "Validation perplexity: 40.73082992274641\n",
      "Epoch [2/4], Step [12075/30091], Loss: 3.8725\n",
      "Epoch [2/4], Step [12150/30091], Loss: 3.8819\n",
      "Epoch [2/4], Step [12225/30091], Loss: 3.9475\n",
      "Epoch [2/4], Step [12300/30091], Loss: 3.9974\n",
      "Epoch [2/4], Step [12375/30091], Loss: 3.9414\n",
      "Epoch [2/4], Step [12450/30091], Loss: 3.8832\n",
      "Epoch [2/4], Step [12525/30091], Loss: 4.0067\n",
      "Epoch [2/4], Step [12600/30091], Loss: 4.1184\n",
      "Epoch [2/4], Step [12675/30091], Loss: 4.0834\n",
      "Epoch [2/4], Step [12750/30091], Loss: 3.9469\n",
      "Epoch [2/4], Step [12825/30091], Loss: 3.9603\n",
      "Epoch [2/4], Step [12900/30091], Loss: 4.0080\n",
      "Epoch [2/4], Step [12975/30091], Loss: 3.9319\n",
      "Epoch [2/4], Step [13050/30091], Loss: 4.0315\n",
      "Epoch [2/4], Step [13125/30091], Loss: 3.9679\n",
      "Epoch [2/4], Step [13200/30091], Loss: 3.9891\n",
      "Epoch [2/4], Step [13275/30091], Loss: 3.9756\n",
      "Epoch [2/4], Step [13350/30091], Loss: 3.8646\n",
      "Epoch [2/4], Step [13425/30091], Loss: 3.8453\n",
      "Epoch [2/4], Step [13500/30091], Loss: 3.8645\n",
      "Epoch [2/4], Step [13575/30091], Loss: 3.9208\n",
      "Epoch [2/4], Step [13650/30091], Loss: 3.9129\n",
      "Epoch [2/4], Step [13725/30091], Loss: 3.9377\n",
      "Epoch [2/4], Step [13800/30091], Loss: 4.0755\n",
      "Epoch [2/4], Step [13875/30091], Loss: 4.0599\n",
      "Epoch [2/4], Step [13950/30091], Loss: 3.8707\n",
      "Epoch [2/4], Step [14025/30091], Loss: 3.9035\n",
      "Epoch [2/4], Step [14100/30091], Loss: 3.9575\n",
      "Epoch [2/4], Step [14175/30091], Loss: 4.0097\n",
      "Epoch [2/4], Step [14250/30091], Loss: 4.0507\n",
      "Epoch [2/4], Step [14325/30091], Loss: 3.8380\n",
      "Epoch [2/4], Step [14400/30091], Loss: 4.1374\n",
      "Epoch [2/4], Step [14475/30091], Loss: 3.9154\n",
      "Epoch [2/4], Step [14550/30091], Loss: 3.9752\n",
      "Epoch [2/4], Step [14625/30091], Loss: 3.8568\n",
      "Epoch [2/4], Step [14700/30091], Loss: 3.9046\n",
      "Epoch [2/4], Step [14775/30091], Loss: 4.0055\n",
      "Epoch [2/4], Step [14850/30091], Loss: 3.8696\n",
      "Epoch [2/4], Step [14925/30091], Loss: 3.9316\n",
      "Epoch [2/4], Step [15000/30091], Loss: 4.1233\n",
      "Validation perplexity: 40.12206755648866\n",
      "Epoch [2/4], Step [15075/30091], Loss: 3.7870\n",
      "Epoch [2/4], Step [15150/30091], Loss: 3.8595\n",
      "Epoch [2/4], Step [15225/30091], Loss: 3.9255\n",
      "Epoch [2/4], Step [15300/30091], Loss: 3.9946\n",
      "Epoch [2/4], Step [15375/30091], Loss: 3.8847\n",
      "Epoch [2/4], Step [15450/30091], Loss: 4.0219\n",
      "Epoch [2/4], Step [15525/30091], Loss: 4.1163\n",
      "Epoch [2/4], Step [15600/30091], Loss: 3.9329\n",
      "Epoch [2/4], Step [15675/30091], Loss: 3.9812\n",
      "Epoch [2/4], Step [15750/30091], Loss: 3.9765\n",
      "Epoch [2/4], Step [15825/30091], Loss: 4.0023\n",
      "Epoch [2/4], Step [15900/30091], Loss: 3.8773\n",
      "Epoch [2/4], Step [15975/30091], Loss: 3.8588\n",
      "Epoch [2/4], Step [16050/30091], Loss: 3.9202\n",
      "Epoch [2/4], Step [16125/30091], Loss: 3.9535\n",
      "Epoch [2/4], Step [16200/30091], Loss: 3.9006\n",
      "Epoch [2/4], Step [16275/30091], Loss: 3.9058\n",
      "Epoch [2/4], Step [16350/30091], Loss: 4.0348\n",
      "Epoch [2/4], Step [16425/30091], Loss: 3.8629\n",
      "Epoch [2/4], Step [16500/30091], Loss: 4.0312\n",
      "Epoch [2/4], Step [16575/30091], Loss: 3.7705\n",
      "Epoch [2/4], Step [16650/30091], Loss: 3.8911\n",
      "Epoch [2/4], Step [16725/30091], Loss: 3.9911\n",
      "Epoch [2/4], Step [16800/30091], Loss: 4.1914\n",
      "Epoch [2/4], Step [16875/30091], Loss: 4.0796\n",
      "Epoch [2/4], Step [16950/30091], Loss: 3.8604\n",
      "Epoch [2/4], Step [17025/30091], Loss: 4.1319\n",
      "Epoch [2/4], Step [17100/30091], Loss: 3.7822\n",
      "Epoch [2/4], Step [17175/30091], Loss: 3.9602\n",
      "Epoch [2/4], Step [17250/30091], Loss: 3.8775\n",
      "Epoch [2/4], Step [17325/30091], Loss: 3.8694\n",
      "Epoch [2/4], Step [17400/30091], Loss: 3.9581\n",
      "Epoch [2/4], Step [17475/30091], Loss: 3.9453\n",
      "Epoch [2/4], Step [17550/30091], Loss: 3.8167\n",
      "Epoch [2/4], Step [17625/30091], Loss: 3.9608\n",
      "Epoch [2/4], Step [17700/30091], Loss: 3.9214\n",
      "Epoch [2/4], Step [17775/30091], Loss: 3.7913\n",
      "Epoch [2/4], Step [17850/30091], Loss: 3.7305\n",
      "Epoch [2/4], Step [17925/30091], Loss: 3.9195\n",
      "Epoch [2/4], Step [18000/30091], Loss: 4.0054\n",
      "Validation perplexity: 39.45247757114243\n",
      "Epoch [2/4], Step [18075/30091], Loss: 3.9933\n",
      "Epoch [2/4], Step [18150/30091], Loss: 3.7238\n",
      "Epoch [2/4], Step [18225/30091], Loss: 4.0045\n",
      "Epoch [2/4], Step [18300/30091], Loss: 3.9757\n",
      "Epoch [2/4], Step [18375/30091], Loss: 3.8000\n",
      "Epoch [2/4], Step [18450/30091], Loss: 3.9319\n",
      "Epoch [2/4], Step [18525/30091], Loss: 3.9476\n",
      "Epoch [2/4], Step [18600/30091], Loss: 4.0071\n",
      "Epoch [2/4], Step [18675/30091], Loss: 4.1704\n",
      "Epoch [2/4], Step [18750/30091], Loss: 3.9724\n",
      "Epoch [2/4], Step [18825/30091], Loss: 3.8776\n",
      "Epoch [2/4], Step [18900/30091], Loss: 3.8189\n",
      "Epoch [2/4], Step [18975/30091], Loss: 3.9877\n",
      "Epoch [2/4], Step [19050/30091], Loss: 3.9518\n",
      "Epoch [2/4], Step [19125/30091], Loss: 3.7931\n",
      "Epoch [2/4], Step [19200/30091], Loss: 3.9722\n",
      "Epoch [2/4], Step [19275/30091], Loss: 3.9647\n",
      "Epoch [2/4], Step [19350/30091], Loss: 4.0124\n",
      "Epoch [2/4], Step [19425/30091], Loss: 3.9581\n",
      "Epoch [2/4], Step [19500/30091], Loss: 3.8627\n",
      "Epoch [2/4], Step [19575/30091], Loss: 3.6996\n",
      "Epoch [2/4], Step [19650/30091], Loss: 4.0404\n",
      "Epoch [2/4], Step [19725/30091], Loss: 3.8870\n",
      "Epoch [2/4], Step [19800/30091], Loss: 3.8714\n",
      "Epoch [2/4], Step [19875/30091], Loss: 3.7257\n",
      "Epoch [2/4], Step [19950/30091], Loss: 4.0287\n",
      "Epoch [2/4], Step [20025/30091], Loss: 3.9711\n",
      "Epoch [2/4], Step [20100/30091], Loss: 3.8184\n",
      "Epoch [2/4], Step [20175/30091], Loss: 3.9621\n",
      "Epoch [2/4], Step [20250/30091], Loss: 3.8851\n",
      "Epoch [2/4], Step [20325/30091], Loss: 4.0549\n",
      "Epoch [2/4], Step [20400/30091], Loss: 3.9559\n",
      "Epoch [2/4], Step [20475/30091], Loss: 3.9107\n",
      "Epoch [2/4], Step [20550/30091], Loss: 4.1087\n",
      "Epoch [2/4], Step [20625/30091], Loss: 3.8113\n",
      "Epoch [2/4], Step [20700/30091], Loss: 3.8846\n",
      "Epoch [2/4], Step [20775/30091], Loss: 3.9208\n",
      "Epoch [2/4], Step [20850/30091], Loss: 4.0511\n",
      "Epoch [2/4], Step [20925/30091], Loss: 3.9645\n",
      "Epoch [2/4], Step [21000/30091], Loss: 3.8544\n",
      "Validation perplexity: 38.826158921418354\n",
      "Epoch [2/4], Step [21075/30091], Loss: 3.8409\n",
      "Epoch [2/4], Step [21150/30091], Loss: 4.0157\n",
      "Epoch [2/4], Step [21225/30091], Loss: 3.9953\n",
      "Epoch [2/4], Step [21300/30091], Loss: 3.8984\n",
      "Epoch [2/4], Step [21375/30091], Loss: 3.9080\n",
      "Epoch [2/4], Step [21450/30091], Loss: 3.7239\n",
      "Epoch [2/4], Step [21525/30091], Loss: 4.0419\n",
      "Epoch [2/4], Step [21600/30091], Loss: 3.9584\n",
      "Epoch [2/4], Step [21675/30091], Loss: 3.9401\n",
      "Epoch [2/4], Step [21750/30091], Loss: 3.9211\n",
      "Epoch [2/4], Step [21825/30091], Loss: 3.9600\n",
      "Epoch [2/4], Step [21900/30091], Loss: 3.9218\n",
      "Epoch [2/4], Step [21975/30091], Loss: 4.0512\n",
      "Epoch [2/4], Step [22050/30091], Loss: 3.9478\n",
      "Epoch [2/4], Step [22125/30091], Loss: 3.9842\n",
      "Epoch [2/4], Step [22200/30091], Loss: 3.9891\n",
      "Epoch [2/4], Step [22275/30091], Loss: 4.0001\n",
      "Epoch [2/4], Step [22350/30091], Loss: 3.9175\n",
      "Epoch [2/4], Step [22425/30091], Loss: 3.9831\n",
      "Epoch [2/4], Step [22500/30091], Loss: 4.0680\n",
      "Epoch [2/4], Step [22575/30091], Loss: 3.9706\n",
      "Epoch [2/4], Step [22650/30091], Loss: 3.8860\n",
      "Epoch [2/4], Step [22725/30091], Loss: 3.9291\n",
      "Epoch [2/4], Step [22800/30091], Loss: 3.7756\n",
      "Epoch [2/4], Step [22875/30091], Loss: 4.0110\n",
      "Epoch [2/4], Step [22950/30091], Loss: 3.8441\n",
      "Epoch [2/4], Step [23025/30091], Loss: 3.7587\n",
      "Epoch [2/4], Step [23100/30091], Loss: 3.8704\n",
      "Epoch [2/4], Step [23175/30091], Loss: 3.8244\n",
      "Epoch [2/4], Step [23250/30091], Loss: 3.8909\n",
      "Epoch [2/4], Step [23325/30091], Loss: 3.7374\n",
      "Epoch [2/4], Step [23400/30091], Loss: 3.9575\n",
      "Epoch [2/4], Step [23475/30091], Loss: 3.9305\n",
      "Epoch [2/4], Step [23550/30091], Loss: 3.8856\n",
      "Epoch [2/4], Step [23625/30091], Loss: 4.1291\n",
      "Epoch [2/4], Step [23700/30091], Loss: 3.9023\n",
      "Epoch [2/4], Step [23775/30091], Loss: 3.9300\n",
      "Epoch [2/4], Step [23850/30091], Loss: 4.0421\n",
      "Epoch [2/4], Step [23925/30091], Loss: 3.8710\n",
      "Epoch [2/4], Step [24000/30091], Loss: 3.9715\n",
      "Validation perplexity: 38.23696825776829\n",
      "Epoch [2/4], Step [24075/30091], Loss: 3.8166\n",
      "Epoch [2/4], Step [24150/30091], Loss: 3.9586\n",
      "Epoch [2/4], Step [24225/30091], Loss: 3.9349\n",
      "Epoch [2/4], Step [24300/30091], Loss: 3.7991\n",
      "Epoch [2/4], Step [24375/30091], Loss: 4.0028\n",
      "Epoch [2/4], Step [24450/30091], Loss: 3.9148\n",
      "Epoch [2/4], Step [24525/30091], Loss: 3.9514\n",
      "Epoch [2/4], Step [24600/30091], Loss: 3.8111\n",
      "Epoch [2/4], Step [24675/30091], Loss: 3.9577\n",
      "Epoch [2/4], Step [24750/30091], Loss: 3.8781\n",
      "Epoch [2/4], Step [24825/30091], Loss: 3.8625\n",
      "Epoch [2/4], Step [24900/30091], Loss: 3.9046\n",
      "Epoch [2/4], Step [24975/30091], Loss: 3.9651\n",
      "Epoch [2/4], Step [25050/30091], Loss: 3.9014\n",
      "Epoch [2/4], Step [25125/30091], Loss: 3.9205\n",
      "Epoch [2/4], Step [25200/30091], Loss: 3.8852\n",
      "Epoch [2/4], Step [25275/30091], Loss: 3.9439\n",
      "Epoch [2/4], Step [25350/30091], Loss: 4.0905\n",
      "Epoch [2/4], Step [25425/30091], Loss: 3.7093\n",
      "Epoch [2/4], Step [25500/30091], Loss: 3.8023\n",
      "Epoch [2/4], Step [25575/30091], Loss: 3.6986\n",
      "Epoch [2/4], Step [25650/30091], Loss: 3.9277\n",
      "Epoch [2/4], Step [25725/30091], Loss: 4.0423\n",
      "Epoch [2/4], Step [25800/30091], Loss: 3.9862\n",
      "Epoch [2/4], Step [25875/30091], Loss: 3.7369\n",
      "Epoch [2/4], Step [25950/30091], Loss: 4.0426\n",
      "Epoch [2/4], Step [26025/30091], Loss: 3.7381\n",
      "Epoch [2/4], Step [26100/30091], Loss: 3.9455\n",
      "Epoch [2/4], Step [26175/30091], Loss: 3.9142\n",
      "Epoch [2/4], Step [26250/30091], Loss: 3.8988\n",
      "Epoch [2/4], Step [26325/30091], Loss: 3.9039\n",
      "Epoch [2/4], Step [26400/30091], Loss: 3.6875\n",
      "Epoch [2/4], Step [26475/30091], Loss: 3.8952\n",
      "Epoch [2/4], Step [26550/30091], Loss: 3.8663\n",
      "Epoch [2/4], Step [26625/30091], Loss: 3.8800\n",
      "Epoch [2/4], Step [26700/30091], Loss: 3.8208\n",
      "Epoch [2/4], Step [26775/30091], Loss: 3.7509\n",
      "Epoch [2/4], Step [26850/30091], Loss: 3.8966\n",
      "Epoch [2/4], Step [26925/30091], Loss: 3.9222\n",
      "Epoch [2/4], Step [27000/30091], Loss: 3.8355\n",
      "Validation perplexity: 38.09794912778717\n",
      "Epoch [2/4], Step [27075/30091], Loss: 3.8898\n",
      "Epoch [2/4], Step [27150/30091], Loss: 3.8914\n",
      "Epoch [2/4], Step [27225/30091], Loss: 3.9991\n",
      "Epoch [2/4], Step [27300/30091], Loss: 3.9052\n",
      "Epoch [2/4], Step [27375/30091], Loss: 3.7889\n",
      "Epoch [2/4], Step [27450/30091], Loss: 3.9584\n",
      "Epoch [2/4], Step [27525/30091], Loss: 3.9511\n",
      "Epoch [2/4], Step [27600/30091], Loss: 3.9465\n",
      "Epoch [2/4], Step [27675/30091], Loss: 3.8137\n",
      "Epoch [2/4], Step [27750/30091], Loss: 3.8654\n",
      "Epoch [2/4], Step [27825/30091], Loss: 3.8676\n",
      "Epoch [2/4], Step [27900/30091], Loss: 3.8862\n",
      "Epoch [2/4], Step [27975/30091], Loss: 3.9063\n",
      "Epoch [2/4], Step [28050/30091], Loss: 3.7970\n",
      "Epoch [2/4], Step [28125/30091], Loss: 3.9677\n",
      "Epoch [2/4], Step [28200/30091], Loss: 3.8463\n",
      "Epoch [2/4], Step [28275/30091], Loss: 3.9280\n",
      "Epoch [2/4], Step [28350/30091], Loss: 3.8921\n",
      "Epoch [2/4], Step [28425/30091], Loss: 3.9410\n",
      "Epoch [2/4], Step [28500/30091], Loss: 3.8546\n",
      "Epoch [2/4], Step [28575/30091], Loss: 4.1037\n",
      "Epoch [2/4], Step [28650/30091], Loss: 3.7989\n",
      "Epoch [2/4], Step [28725/30091], Loss: 3.8937\n",
      "Epoch [2/4], Step [28800/30091], Loss: 4.0115\n",
      "Epoch [2/4], Step [28875/30091], Loss: 3.8341\n",
      "Epoch [2/4], Step [28950/30091], Loss: 4.0202\n",
      "Epoch [2/4], Step [29025/30091], Loss: 3.9662\n",
      "Epoch [2/4], Step [29100/30091], Loss: 3.8811\n",
      "Epoch [2/4], Step [29175/30091], Loss: 3.9380\n",
      "Epoch [2/4], Step [29250/30091], Loss: 3.7961\n",
      "Epoch [2/4], Step [29325/30091], Loss: 3.9956\n",
      "Epoch [2/4], Step [29400/30091], Loss: 3.8334\n",
      "Epoch [2/4], Step [29475/30091], Loss: 4.0847\n",
      "Epoch [2/4], Step [29550/30091], Loss: 3.9703\n",
      "Epoch [2/4], Step [29625/30091], Loss: 3.9875\n",
      "Epoch [2/4], Step [29700/30091], Loss: 4.0256\n",
      "Epoch [2/4], Step [29775/30091], Loss: 4.0476\n",
      "Epoch [2/4], Step [29850/30091], Loss: 3.8575\n",
      "Epoch [2/4], Step [29925/30091], Loss: 4.1195\n",
      "Epoch [2/4], Step [30000/30091], Loss: 3.9202\n",
      "Validation perplexity: 37.59741985363573\n",
      "Epoch [2/4], Step [30075/30091], Loss: 3.9052\n",
      "Epoch [2/4] Average Loss: 3.9575, Perplexity: 52.32\n",
      "Epoch [3/4], Step [0/30091], Loss: 3.9785\n",
      "Validation perplexity: 37.686809001876284\n",
      "Epoch [3/4], Step [75/30091], Loss: 3.8649\n",
      "Epoch [3/4], Step [150/30091], Loss: 3.9632\n",
      "Epoch [3/4], Step [225/30091], Loss: 3.7377\n",
      "Epoch [3/4], Step [300/30091], Loss: 4.0363\n",
      "Epoch [3/4], Step [375/30091], Loss: 3.9608\n",
      "Epoch [3/4], Step [450/30091], Loss: 3.8123\n",
      "Epoch [3/4], Step [525/30091], Loss: 3.8744\n",
      "Epoch [3/4], Step [600/30091], Loss: 3.8685\n",
      "Epoch [3/4], Step [675/30091], Loss: 3.9917\n",
      "Epoch [3/4], Step [750/30091], Loss: 3.7837\n",
      "Epoch [3/4], Step [825/30091], Loss: 4.0098\n",
      "Epoch [3/4], Step [900/30091], Loss: 3.8470\n",
      "Epoch [3/4], Step [975/30091], Loss: 3.9691\n",
      "Epoch [3/4], Step [1050/30091], Loss: 4.0365\n",
      "Epoch [3/4], Step [1125/30091], Loss: 3.9406\n",
      "Epoch [3/4], Step [1200/30091], Loss: 3.9677\n",
      "Epoch [3/4], Step [1275/30091], Loss: 3.9888\n",
      "Epoch [3/4], Step [1350/30091], Loss: 3.9272\n",
      "Epoch [3/4], Step [1425/30091], Loss: 3.8851\n",
      "Epoch [3/4], Step [1500/30091], Loss: 3.8496\n",
      "Epoch [3/4], Step [1575/30091], Loss: 3.7129\n",
      "Epoch [3/4], Step [1650/30091], Loss: 3.8901\n",
      "Epoch [3/4], Step [1725/30091], Loss: 4.0798\n",
      "Epoch [3/4], Step [1800/30091], Loss: 3.8126\n",
      "Epoch [3/4], Step [1875/30091], Loss: 4.0425\n",
      "Epoch [3/4], Step [1950/30091], Loss: 3.8524\n",
      "Epoch [3/4], Step [2025/30091], Loss: 3.9969\n",
      "Epoch [3/4], Step [2100/30091], Loss: 3.9986\n",
      "Epoch [3/4], Step [2175/30091], Loss: 3.8590\n",
      "Epoch [3/4], Step [2250/30091], Loss: 3.7072\n",
      "Epoch [3/4], Step [2325/30091], Loss: 3.7584\n",
      "Epoch [3/4], Step [2400/30091], Loss: 4.0412\n",
      "Epoch [3/4], Step [2475/30091], Loss: 3.7417\n",
      "Epoch [3/4], Step [2550/30091], Loss: 3.7419\n",
      "Epoch [3/4], Step [2625/30091], Loss: 4.0111\n",
      "Epoch [3/4], Step [2700/30091], Loss: 3.7708\n",
      "Epoch [3/4], Step [2775/30091], Loss: 3.8456\n",
      "Epoch [3/4], Step [2850/30091], Loss: 3.8986\n",
      "Epoch [3/4], Step [2925/30091], Loss: 3.8754\n",
      "Epoch [3/4], Step [3000/30091], Loss: 3.8985\n",
      "Validation perplexity: 37.13200517727112\n",
      "Epoch [3/4], Step [3075/30091], Loss: 3.6911\n",
      "Epoch [3/4], Step [3150/30091], Loss: 3.9170\n",
      "Epoch [3/4], Step [3225/30091], Loss: 3.8061\n",
      "Epoch [3/4], Step [3300/30091], Loss: 3.6475\n",
      "Epoch [3/4], Step [3375/30091], Loss: 3.8570\n",
      "Epoch [3/4], Step [3450/30091], Loss: 4.0100\n",
      "Epoch [3/4], Step [3525/30091], Loss: 3.8336\n",
      "Epoch [3/4], Step [3600/30091], Loss: 3.9674\n",
      "Epoch [3/4], Step [3675/30091], Loss: 3.9318\n",
      "Epoch [3/4], Step [3750/30091], Loss: 3.8654\n",
      "Epoch [3/4], Step [3825/30091], Loss: 3.8167\n",
      "Epoch [3/4], Step [3900/30091], Loss: 3.8932\n",
      "Epoch [3/4], Step [3975/30091], Loss: 3.6667\n",
      "Epoch [3/4], Step [4050/30091], Loss: 3.7797\n",
      "Epoch [3/4], Step [4125/30091], Loss: 3.8012\n",
      "Epoch [3/4], Step [4200/30091], Loss: 3.9884\n",
      "Epoch [3/4], Step [4275/30091], Loss: 3.8486\n",
      "Epoch [3/4], Step [4350/30091], Loss: 3.8197\n",
      "Epoch [3/4], Step [4425/30091], Loss: 3.8951\n",
      "Epoch [3/4], Step [4500/30091], Loss: 3.8644\n",
      "Epoch [3/4], Step [4575/30091], Loss: 3.8381\n",
      "Epoch [3/4], Step [4650/30091], Loss: 3.7459\n",
      "Epoch [3/4], Step [4725/30091], Loss: 3.9219\n",
      "Epoch [3/4], Step [4800/30091], Loss: 3.8630\n",
      "Epoch [3/4], Step [4875/30091], Loss: 3.9075\n",
      "Epoch [3/4], Step [4950/30091], Loss: 3.9856\n",
      "Epoch [3/4], Step [5025/30091], Loss: 3.7766\n",
      "Epoch [3/4], Step [5100/30091], Loss: 3.9132\n",
      "Epoch [3/4], Step [5175/30091], Loss: 3.9856\n",
      "Epoch [3/4], Step [5250/30091], Loss: 3.8445\n",
      "Epoch [3/4], Step [5325/30091], Loss: 3.7314\n",
      "Epoch [3/4], Step [5400/30091], Loss: 3.8740\n",
      "Epoch [3/4], Step [5475/30091], Loss: 3.8069\n",
      "Epoch [3/4], Step [5550/30091], Loss: 3.7932\n",
      "Epoch [3/4], Step [5625/30091], Loss: 3.9332\n",
      "Epoch [3/4], Step [5700/30091], Loss: 3.9103\n",
      "Epoch [3/4], Step [5775/30091], Loss: 3.8158\n",
      "Epoch [3/4], Step [5850/30091], Loss: 3.9460\n",
      "Epoch [3/4], Step [5925/30091], Loss: 3.8730\n",
      "Epoch [3/4], Step [6000/30091], Loss: 3.8961\n",
      "Validation perplexity: 36.71295674538585\n",
      "Epoch [3/4], Step [6075/30091], Loss: 4.0436\n",
      "Epoch [3/4], Step [6150/30091], Loss: 3.8723\n",
      "Epoch [3/4], Step [6225/30091], Loss: 3.8864\n",
      "Epoch [3/4], Step [6300/30091], Loss: 3.8931\n",
      "Epoch [3/4], Step [6375/30091], Loss: 3.6477\n",
      "Epoch [3/4], Step [6450/30091], Loss: 3.8286\n",
      "Epoch [3/4], Step [6525/30091], Loss: 3.9499\n",
      "Epoch [3/4], Step [6600/30091], Loss: 3.8784\n",
      "Epoch [3/4], Step [6675/30091], Loss: 3.7825\n",
      "Epoch [3/4], Step [6750/30091], Loss: 3.9682\n",
      "Epoch [3/4], Step [6825/30091], Loss: 3.9163\n",
      "Epoch [3/4], Step [6900/30091], Loss: 3.7173\n",
      "Epoch [3/4], Step [6975/30091], Loss: 3.9633\n",
      "Epoch [3/4], Step [7050/30091], Loss: 3.8640\n",
      "Epoch [3/4], Step [7125/30091], Loss: 3.9596\n",
      "Epoch [3/4], Step [7200/30091], Loss: 3.7356\n",
      "Epoch [3/4], Step [7275/30091], Loss: 4.0186\n",
      "Epoch [3/4], Step [7350/30091], Loss: 3.8455\n",
      "Epoch [3/4], Step [7425/30091], Loss: 3.9854\n",
      "Epoch [3/4], Step [7500/30091], Loss: 3.8359\n",
      "Epoch [3/4], Step [7575/30091], Loss: 3.9699\n",
      "Epoch [3/4], Step [7650/30091], Loss: 3.9016\n",
      "Epoch [3/4], Step [7725/30091], Loss: 3.7940\n",
      "Epoch [3/4], Step [7800/30091], Loss: 3.8000\n",
      "Epoch [3/4], Step [7875/30091], Loss: 3.9708\n",
      "Epoch [3/4], Step [7950/30091], Loss: 3.8545\n",
      "Epoch [3/4], Step [8025/30091], Loss: 3.9648\n",
      "Epoch [3/4], Step [8100/30091], Loss: 3.8906\n",
      "Epoch [3/4], Step [8175/30091], Loss: 3.7921\n",
      "Epoch [3/4], Step [8250/30091], Loss: 3.9345\n",
      "Epoch [3/4], Step [8325/30091], Loss: 3.7010\n",
      "Epoch [3/4], Step [8400/30091], Loss: 3.8805\n",
      "Epoch [3/4], Step [8475/30091], Loss: 3.9660\n",
      "Epoch [3/4], Step [8550/30091], Loss: 3.8796\n",
      "Epoch [3/4], Step [8625/30091], Loss: 3.7925\n",
      "Epoch [3/4], Step [8700/30091], Loss: 3.9301\n",
      "Epoch [3/4], Step [8775/30091], Loss: 3.7798\n",
      "Epoch [3/4], Step [8850/30091], Loss: 3.6381\n",
      "Epoch [3/4], Step [8925/30091], Loss: 3.8244\n",
      "Epoch [3/4], Step [9000/30091], Loss: 3.9344\n",
      "Validation perplexity: 36.49741698725382\n",
      "Epoch [3/4], Step [9075/30091], Loss: 3.7411\n",
      "Epoch [3/4], Step [9150/30091], Loss: 3.9379\n",
      "Epoch [3/4], Step [9225/30091], Loss: 3.7330\n",
      "Epoch [3/4], Step [9300/30091], Loss: 4.0334\n",
      "Epoch [3/4], Step [9375/30091], Loss: 3.8662\n",
      "Epoch [3/4], Step [9450/30091], Loss: 4.0304\n",
      "Epoch [3/4], Step [9525/30091], Loss: 3.9251\n",
      "Epoch [3/4], Step [9600/30091], Loss: 3.7985\n",
      "Epoch [3/4], Step [9675/30091], Loss: 3.6913\n",
      "Epoch [3/4], Step [9750/30091], Loss: 3.9403\n",
      "Epoch [3/4], Step [9825/30091], Loss: 3.9222\n",
      "Epoch [3/4], Step [9900/30091], Loss: 3.8344\n",
      "Epoch [3/4], Step [9975/30091], Loss: 3.7224\n",
      "Epoch [3/4], Step [10050/30091], Loss: 3.7711\n",
      "Epoch [3/4], Step [10125/30091], Loss: 3.8547\n",
      "Epoch [3/4], Step [10200/30091], Loss: 3.8420\n",
      "Epoch [3/4], Step [10275/30091], Loss: 3.7833\n",
      "Epoch [3/4], Step [10350/30091], Loss: 3.6749\n",
      "Epoch [3/4], Step [10425/30091], Loss: 3.8212\n",
      "Epoch [3/4], Step [10500/30091], Loss: 3.8403\n",
      "Epoch [3/4], Step [10575/30091], Loss: 3.8908\n",
      "Epoch [3/4], Step [10650/30091], Loss: 3.8761\n",
      "Epoch [3/4], Step [10725/30091], Loss: 4.0464\n",
      "Epoch [3/4], Step [10800/30091], Loss: 3.9422\n",
      "Epoch [3/4], Step [10875/30091], Loss: 3.9570\n",
      "Epoch [3/4], Step [10950/30091], Loss: 3.8141\n",
      "Epoch [3/4], Step [11025/30091], Loss: 3.9099\n",
      "Epoch [3/4], Step [11100/30091], Loss: 3.9252\n",
      "Epoch [3/4], Step [11175/30091], Loss: 3.8782\n",
      "Epoch [3/4], Step [11250/30091], Loss: 3.8733\n",
      "Epoch [3/4], Step [11325/30091], Loss: 3.8828\n",
      "Epoch [3/4], Step [11400/30091], Loss: 3.8031\n",
      "Epoch [3/4], Step [11475/30091], Loss: 3.8676\n",
      "Epoch [3/4], Step [11550/30091], Loss: 3.9844\n",
      "Epoch [3/4], Step [11625/30091], Loss: 3.8959\n",
      "Epoch [3/4], Step [11700/30091], Loss: 3.7531\n",
      "Epoch [3/4], Step [11775/30091], Loss: 3.7449\n",
      "Epoch [3/4], Step [11850/30091], Loss: 3.7500\n",
      "Epoch [3/4], Step [11925/30091], Loss: 3.6844\n",
      "Epoch [3/4], Step [12000/30091], Loss: 3.8956\n",
      "Validation perplexity: 36.19601497791451\n",
      "Epoch [3/4], Step [12075/30091], Loss: 3.8921\n",
      "Epoch [3/4], Step [12150/30091], Loss: 3.7289\n",
      "Epoch [3/4], Step [12225/30091], Loss: 3.7756\n",
      "Epoch [3/4], Step [12300/30091], Loss: 3.7660\n",
      "Epoch [3/4], Step [12375/30091], Loss: 3.7677\n",
      "Epoch [3/4], Step [12450/30091], Loss: 3.8936\n",
      "Epoch [3/4], Step [12525/30091], Loss: 3.8081\n",
      "Epoch [3/4], Step [12600/30091], Loss: 4.0012\n",
      "Epoch [3/4], Step [12675/30091], Loss: 3.7138\n",
      "Epoch [3/4], Step [12750/30091], Loss: 3.9274\n",
      "Epoch [3/4], Step [12825/30091], Loss: 3.9136\n",
      "Epoch [3/4], Step [12900/30091], Loss: 3.7104\n",
      "Epoch [3/4], Step [12975/30091], Loss: 3.6315\n",
      "Epoch [3/4], Step [13050/30091], Loss: 3.6909\n",
      "Epoch [3/4], Step [13125/30091], Loss: 3.9346\n",
      "Epoch [3/4], Step [13200/30091], Loss: 3.7453\n",
      "Epoch [3/4], Step [13275/30091], Loss: 3.8058\n",
      "Epoch [3/4], Step [13350/30091], Loss: 3.8899\n",
      "Epoch [3/4], Step [13425/30091], Loss: 3.7790\n",
      "Epoch [3/4], Step [13500/30091], Loss: 4.0264\n",
      "Epoch [3/4], Step [13575/30091], Loss: 3.7189\n",
      "Epoch [3/4], Step [13650/30091], Loss: 3.8108\n",
      "Epoch [3/4], Step [13725/30091], Loss: 3.8108\n",
      "Epoch [3/4], Step [13800/30091], Loss: 3.8157\n",
      "Epoch [3/4], Step [13875/30091], Loss: 3.9530\n",
      "Epoch [3/4], Step [13950/30091], Loss: 4.0739\n",
      "Epoch [3/4], Step [14025/30091], Loss: 3.8179\n",
      "Epoch [3/4], Step [14100/30091], Loss: 3.8686\n",
      "Epoch [3/4], Step [14175/30091], Loss: 3.9425\n",
      "Epoch [3/4], Step [14250/30091], Loss: 3.8869\n",
      "Epoch [3/4], Step [14325/30091], Loss: 3.9113\n",
      "Epoch [3/4], Step [14400/30091], Loss: 3.8974\n",
      "Epoch [3/4], Step [14475/30091], Loss: 3.8493\n",
      "Epoch [3/4], Step [14550/30091], Loss: 3.9212\n",
      "Epoch [3/4], Step [14625/30091], Loss: 3.8013\n",
      "Epoch [3/4], Step [14700/30091], Loss: 3.9289\n",
      "Epoch [3/4], Step [14775/30091], Loss: 3.8906\n",
      "Epoch [3/4], Step [14850/30091], Loss: 3.9318\n",
      "Epoch [3/4], Step [14925/30091], Loss: 3.9879\n",
      "Epoch [3/4], Step [15000/30091], Loss: 3.7173\n",
      "Validation perplexity: 36.03112701395737\n",
      "Epoch [3/4], Step [15075/30091], Loss: 3.8309\n",
      "Epoch [3/4], Step [15150/30091], Loss: 3.8148\n",
      "Epoch [3/4], Step [15225/30091], Loss: 3.9253\n",
      "Epoch [3/4], Step [15300/30091], Loss: 3.9731\n",
      "Epoch [3/4], Step [15375/30091], Loss: 3.9646\n",
      "Epoch [3/4], Step [15450/30091], Loss: 4.0263\n",
      "Epoch [3/4], Step [15525/30091], Loss: 3.9141\n",
      "Epoch [3/4], Step [15600/30091], Loss: 3.8498\n",
      "Epoch [3/4], Step [15675/30091], Loss: 3.7865\n",
      "Epoch [3/4], Step [15750/30091], Loss: 3.9617\n",
      "Epoch [3/4], Step [15825/30091], Loss: 3.8854\n",
      "Epoch [3/4], Step [15900/30091], Loss: 3.7980\n",
      "Epoch [3/4], Step [15975/30091], Loss: 3.6845\n",
      "Epoch [3/4], Step [16050/30091], Loss: 3.9286\n",
      "Epoch [3/4], Step [16125/30091], Loss: 3.8560\n",
      "Epoch [3/4], Step [16200/30091], Loss: 3.8583\n",
      "Epoch [3/4], Step [16275/30091], Loss: 3.6576\n",
      "Epoch [3/4], Step [16350/30091], Loss: 3.8464\n",
      "Epoch [3/4], Step [16425/30091], Loss: 4.0385\n",
      "Epoch [3/4], Step [16500/30091], Loss: 3.9432\n",
      "Epoch [3/4], Step [16575/30091], Loss: 3.9333\n",
      "Epoch [3/4], Step [16650/30091], Loss: 3.8928\n",
      "Epoch [3/4], Step [16725/30091], Loss: 3.9794\n",
      "Epoch [3/4], Step [16800/30091], Loss: 3.8564\n",
      "Epoch [3/4], Step [16875/30091], Loss: 3.9287\n",
      "Epoch [3/4], Step [16950/30091], Loss: 3.8271\n",
      "Epoch [3/4], Step [17025/30091], Loss: 3.8499\n",
      "Epoch [3/4], Step [17100/30091], Loss: 3.7644\n",
      "Epoch [3/4], Step [17175/30091], Loss: 3.8001\n",
      "Epoch [3/4], Step [17250/30091], Loss: 4.0977\n",
      "Epoch [3/4], Step [17325/30091], Loss: 3.7480\n",
      "Epoch [3/4], Step [17400/30091], Loss: 4.0421\n",
      "Epoch [3/4], Step [17475/30091], Loss: 3.7647\n",
      "Epoch [3/4], Step [17550/30091], Loss: 3.7950\n",
      "Epoch [3/4], Step [17625/30091], Loss: 3.9189\n",
      "Epoch [3/4], Step [17700/30091], Loss: 3.8267\n",
      "Epoch [3/4], Step [17775/30091], Loss: 3.9702\n",
      "Epoch [3/4], Step [17850/30091], Loss: 3.9400\n",
      "Epoch [3/4], Step [17925/30091], Loss: 3.8869\n",
      "Epoch [3/4], Step [18000/30091], Loss: 3.8691\n",
      "Validation perplexity: 35.53921936139923\n",
      "Epoch [3/4], Step [18075/30091], Loss: 4.0586\n",
      "Epoch [3/4], Step [18150/30091], Loss: 3.9361\n",
      "Epoch [3/4], Step [18225/30091], Loss: 3.9253\n",
      "Epoch [3/4], Step [18300/30091], Loss: 3.8938\n",
      "Epoch [3/4], Step [18375/30091], Loss: 3.8730\n",
      "Epoch [3/4], Step [18450/30091], Loss: 3.8465\n",
      "Epoch [3/4], Step [18525/30091], Loss: 3.7591\n",
      "Epoch [3/4], Step [18600/30091], Loss: 3.9484\n",
      "Epoch [3/4], Step [18675/30091], Loss: 3.8945\n",
      "Epoch [3/4], Step [18750/30091], Loss: 3.9134\n",
      "Epoch [3/4], Step [18825/30091], Loss: 4.0017\n",
      "Epoch [3/4], Step [18900/30091], Loss: 3.8161\n",
      "Epoch [3/4], Step [18975/30091], Loss: 3.8493\n",
      "Epoch [3/4], Step [19050/30091], Loss: 3.6235\n",
      "Epoch [3/4], Step [19125/30091], Loss: 3.9096\n",
      "Epoch [3/4], Step [19200/30091], Loss: 3.6874\n",
      "Epoch [3/4], Step [19275/30091], Loss: 3.7942\n",
      "Epoch [3/4], Step [19350/30091], Loss: 3.9026\n",
      "Epoch [3/4], Step [19425/30091], Loss: 3.7776\n",
      "Epoch [3/4], Step [19500/30091], Loss: 3.7977\n",
      "Epoch [3/4], Step [19575/30091], Loss: 3.7232\n",
      "Epoch [3/4], Step [19650/30091], Loss: 3.8809\n",
      "Epoch [3/4], Step [19725/30091], Loss: 3.8595\n",
      "Epoch [3/4], Step [19800/30091], Loss: 3.8668\n",
      "Epoch [3/4], Step [19875/30091], Loss: 3.7955\n",
      "Epoch [3/4], Step [19950/30091], Loss: 3.9517\n",
      "Epoch [3/4], Step [20025/30091], Loss: 3.9350\n",
      "Epoch [3/4], Step [20100/30091], Loss: 3.9425\n",
      "Epoch [3/4], Step [20175/30091], Loss: 3.6691\n",
      "Epoch [3/4], Step [20250/30091], Loss: 3.9720\n",
      "Epoch [3/4], Step [20325/30091], Loss: 3.9344\n",
      "Epoch [3/4], Step [20400/30091], Loss: 3.9512\n",
      "Epoch [3/4], Step [20475/30091], Loss: 3.8492\n",
      "Epoch [3/4], Step [20550/30091], Loss: 3.8992\n",
      "Epoch [3/4], Step [20625/30091], Loss: 3.8594\n",
      "Epoch [3/4], Step [20700/30091], Loss: 3.7905\n",
      "Epoch [3/4], Step [20775/30091], Loss: 3.9152\n",
      "Epoch [3/4], Step [20850/30091], Loss: 3.8620\n",
      "Epoch [3/4], Step [20925/30091], Loss: 3.7688\n",
      "Epoch [3/4], Step [21000/30091], Loss: 3.9096\n",
      "Validation perplexity: 35.47685960694133\n",
      "Epoch [3/4], Step [21075/30091], Loss: 4.0647\n",
      "Epoch [3/4], Step [21150/30091], Loss: 3.8028\n",
      "Epoch [3/4], Step [21225/30091], Loss: 3.9407\n",
      "Epoch [3/4], Step [21300/30091], Loss: 3.6735\n",
      "Epoch [3/4], Step [21375/30091], Loss: 4.0327\n",
      "Epoch [3/4], Step [21450/30091], Loss: 3.9752\n",
      "Epoch [3/4], Step [21525/30091], Loss: 3.7579\n",
      "Epoch [3/4], Step [21600/30091], Loss: 4.0602\n",
      "Epoch [3/4], Step [21675/30091], Loss: 4.0410\n",
      "Epoch [3/4], Step [21750/30091], Loss: 3.8153\n",
      "Epoch [3/4], Step [21825/30091], Loss: 3.9156\n",
      "Epoch [3/4], Step [21900/30091], Loss: 3.8630\n",
      "Epoch [3/4], Step [21975/30091], Loss: 3.9342\n",
      "Epoch [3/4], Step [22050/30091], Loss: 3.8599\n",
      "Epoch [3/4], Step [22125/30091], Loss: 4.0108\n",
      "Epoch [3/4], Step [22200/30091], Loss: 3.7879\n",
      "Epoch [3/4], Step [22275/30091], Loss: 3.7818\n",
      "Epoch [3/4], Step [22350/30091], Loss: 3.7826\n",
      "Epoch [3/4], Step [22425/30091], Loss: 3.6660\n",
      "Epoch [3/4], Step [22500/30091], Loss: 3.9050\n",
      "Epoch [3/4], Step [22575/30091], Loss: 3.9110\n",
      "Epoch [3/4], Step [22650/30091], Loss: 3.7611\n",
      "Epoch [3/4], Step [22725/30091], Loss: 3.9055\n",
      "Epoch [3/4], Step [22800/30091], Loss: 3.7520\n",
      "Epoch [3/4], Step [22875/30091], Loss: 3.7080\n",
      "Epoch [3/4], Step [22950/30091], Loss: 3.8391\n",
      "Epoch [3/4], Step [23025/30091], Loss: 3.8026\n",
      "Epoch [3/4], Step [23100/30091], Loss: 3.8211\n",
      "Epoch [3/4], Step [23175/30091], Loss: 3.9103\n",
      "Epoch [3/4], Step [23250/30091], Loss: 3.8486\n",
      "Epoch [3/4], Step [23325/30091], Loss: 3.6676\n",
      "Epoch [3/4], Step [23400/30091], Loss: 3.7617\n",
      "Epoch [3/4], Step [23475/30091], Loss: 3.8134\n",
      "Epoch [3/4], Step [23550/30091], Loss: 3.9328\n",
      "Epoch [3/4], Step [23625/30091], Loss: 3.9093\n",
      "Epoch [3/4], Step [23700/30091], Loss: 3.6668\n",
      "Epoch [3/4], Step [23775/30091], Loss: 3.7671\n",
      "Epoch [3/4], Step [23850/30091], Loss: 3.9043\n",
      "Epoch [3/4], Step [23925/30091], Loss: 3.8127\n",
      "Epoch [3/4], Step [24000/30091], Loss: 3.7714\n",
      "Validation perplexity: 35.078468901672395\n",
      "Epoch [3/4], Step [24075/30091], Loss: 3.8302\n",
      "Epoch [3/4], Step [24150/30091], Loss: 3.8347\n",
      "Epoch [3/4], Step [24225/30091], Loss: 3.9924\n",
      "Epoch [3/4], Step [24300/30091], Loss: 3.8969\n",
      "Epoch [3/4], Step [24375/30091], Loss: 3.8397\n",
      "Epoch [3/4], Step [24450/30091], Loss: 3.9671\n",
      "Epoch [3/4], Step [24525/30091], Loss: 3.9023\n",
      "Epoch [3/4], Step [24600/30091], Loss: 3.8700\n",
      "Epoch [3/4], Step [24675/30091], Loss: 3.8406\n",
      "Epoch [3/4], Step [24750/30091], Loss: 3.6836\n",
      "Epoch [3/4], Step [24825/30091], Loss: 3.9155\n",
      "Epoch [3/4], Step [24900/30091], Loss: 3.9468\n",
      "Epoch [3/4], Step [24975/30091], Loss: 3.8769\n",
      "Epoch [3/4], Step [25050/30091], Loss: 3.9101\n",
      "Epoch [3/4], Step [25125/30091], Loss: 3.9786\n",
      "Epoch [3/4], Step [25200/30091], Loss: 3.6710\n",
      "Epoch [3/4], Step [25275/30091], Loss: 3.7556\n",
      "Epoch [3/4], Step [25350/30091], Loss: 3.6165\n",
      "Epoch [3/4], Step [25425/30091], Loss: 3.8464\n",
      "Epoch [3/4], Step [25500/30091], Loss: 3.7963\n",
      "Epoch [3/4], Step [25575/30091], Loss: 3.8203\n",
      "Epoch [3/4], Step [25650/30091], Loss: 3.8585\n",
      "Epoch [3/4], Step [25725/30091], Loss: 3.9017\n",
      "Epoch [3/4], Step [25800/30091], Loss: 4.0035\n",
      "Epoch [3/4], Step [25875/30091], Loss: 3.8192\n",
      "Epoch [3/4], Step [25950/30091], Loss: 3.8206\n",
      "Epoch [3/4], Step [26025/30091], Loss: 3.7998\n",
      "Epoch [3/4], Step [26100/30091], Loss: 3.8302\n",
      "Epoch [3/4], Step [26175/30091], Loss: 3.8870\n",
      "Epoch [3/4], Step [26250/30091], Loss: 3.8742\n",
      "Epoch [3/4], Step [26325/30091], Loss: 3.8463\n",
      "Epoch [3/4], Step [26400/30091], Loss: 3.8621\n",
      "Epoch [3/4], Step [26475/30091], Loss: 3.7813\n",
      "Epoch [3/4], Step [26550/30091], Loss: 3.8692\n",
      "Epoch [3/4], Step [26625/30091], Loss: 3.6434\n",
      "Epoch [3/4], Step [26700/30091], Loss: 3.7362\n",
      "Epoch [3/4], Step [26775/30091], Loss: 4.0234\n",
      "Epoch [3/4], Step [26850/30091], Loss: 3.8403\n",
      "Epoch [3/4], Step [26925/30091], Loss: 3.7760\n",
      "Epoch [3/4], Step [27000/30091], Loss: 3.7244\n",
      "Validation perplexity: 35.02212116642881\n",
      "Epoch [3/4], Step [27075/30091], Loss: 3.8453\n",
      "Epoch [3/4], Step [27150/30091], Loss: 3.8348\n",
      "Epoch [3/4], Step [27225/30091], Loss: 3.8769\n",
      "Epoch [3/4], Step [27300/30091], Loss: 4.0411\n",
      "Epoch [3/4], Step [27375/30091], Loss: 3.9080\n",
      "Epoch [3/4], Step [27450/30091], Loss: 3.9296\n",
      "Epoch [3/4], Step [27525/30091], Loss: 3.9054\n",
      "Epoch [3/4], Step [27600/30091], Loss: 3.7428\n",
      "Epoch [3/4], Step [27675/30091], Loss: 3.9148\n",
      "Epoch [3/4], Step [27750/30091], Loss: 3.8771\n",
      "Epoch [3/4], Step [27825/30091], Loss: 3.8670\n",
      "Epoch [3/4], Step [27900/30091], Loss: 3.9102\n",
      "Epoch [3/4], Step [27975/30091], Loss: 3.8685\n",
      "Epoch [3/4], Step [28050/30091], Loss: 3.7354\n",
      "Epoch [3/4], Step [28125/30091], Loss: 3.8235\n",
      "Epoch [3/4], Step [28200/30091], Loss: 3.8163\n",
      "Epoch [3/4], Step [28275/30091], Loss: 3.8404\n",
      "Epoch [3/4], Step [28350/30091], Loss: 3.7986\n",
      "Epoch [3/4], Step [28425/30091], Loss: 3.7126\n",
      "Epoch [3/4], Step [28500/30091], Loss: 3.9253\n",
      "Epoch [3/4], Step [28575/30091], Loss: 3.7941\n",
      "Epoch [3/4], Step [28650/30091], Loss: 3.7621\n",
      "Epoch [3/4], Step [28725/30091], Loss: 3.9188\n",
      "Epoch [3/4], Step [28800/30091], Loss: 3.6763\n",
      "Epoch [3/4], Step [28875/30091], Loss: 3.8142\n",
      "Epoch [3/4], Step [28950/30091], Loss: 3.8533\n",
      "Epoch [3/4], Step [29025/30091], Loss: 3.8177\n",
      "Epoch [3/4], Step [29100/30091], Loss: 3.7214\n",
      "Epoch [3/4], Step [29175/30091], Loss: 3.7365\n",
      "Epoch [3/4], Step [29250/30091], Loss: 3.6025\n",
      "Epoch [3/4], Step [29325/30091], Loss: 3.8679\n",
      "Epoch [3/4], Step [29400/30091], Loss: 3.7913\n",
      "Epoch [3/4], Step [29475/30091], Loss: 3.9820\n",
      "Epoch [3/4], Step [29550/30091], Loss: 3.8481\n",
      "Epoch [3/4], Step [29625/30091], Loss: 3.8629\n",
      "Epoch [3/4], Step [29700/30091], Loss: 3.8750\n",
      "Epoch [3/4], Step [29775/30091], Loss: 3.8463\n",
      "Epoch [3/4], Step [29850/30091], Loss: 3.9568\n",
      "Epoch [3/4], Step [29925/30091], Loss: 3.7897\n",
      "Epoch [3/4], Step [30000/30091], Loss: 3.8291\n",
      "Validation perplexity: 34.71703257081315\n",
      "Epoch [3/4], Step [30075/30091], Loss: 3.7227\n",
      "Epoch [3/4] Average Loss: 3.8632, Perplexity: 47.62\n",
      "Epoch [4/4], Step [0/30091], Loss: 3.8767\n",
      "Validation perplexity: 34.71344509471395\n",
      "Epoch [4/4], Step [75/30091], Loss: 3.8422\n",
      "Epoch [4/4], Step [150/30091], Loss: 3.9594\n",
      "Epoch [4/4], Step [225/30091], Loss: 3.7661\n",
      "Epoch [4/4], Step [300/30091], Loss: 3.8459\n",
      "Epoch [4/4], Step [375/30091], Loss: 3.7414\n",
      "Epoch [4/4], Step [450/30091], Loss: 3.7930\n",
      "Epoch [4/4], Step [525/30091], Loss: 3.8389\n",
      "Epoch [4/4], Step [600/30091], Loss: 3.7826\n",
      "Epoch [4/4], Step [675/30091], Loss: 3.6648\n",
      "Epoch [4/4], Step [750/30091], Loss: 4.0225\n",
      "Epoch [4/4], Step [825/30091], Loss: 3.5764\n",
      "Epoch [4/4], Step [900/30091], Loss: 3.6571\n",
      "Epoch [4/4], Step [975/30091], Loss: 3.8327\n",
      "Epoch [4/4], Step [1050/30091], Loss: 3.8187\n",
      "Epoch [4/4], Step [1125/30091], Loss: 3.8555\n",
      "Epoch [4/4], Step [1200/30091], Loss: 3.8028\n",
      "Epoch [4/4], Step [1275/30091], Loss: 3.9541\n",
      "Epoch [4/4], Step [1350/30091], Loss: 3.7630\n",
      "Epoch [4/4], Step [1425/30091], Loss: 3.9379\n",
      "Epoch [4/4], Step [1500/30091], Loss: 3.8427\n",
      "Epoch [4/4], Step [1575/30091], Loss: 3.7809\n",
      "Epoch [4/4], Step [1650/30091], Loss: 3.7375\n",
      "Epoch [4/4], Step [1725/30091], Loss: 3.7376\n",
      "Epoch [4/4], Step [1800/30091], Loss: 3.8875\n",
      "Epoch [4/4], Step [1875/30091], Loss: 3.6870\n",
      "Epoch [4/4], Step [1950/30091], Loss: 3.8840\n",
      "Epoch [4/4], Step [2025/30091], Loss: 3.8990\n",
      "Epoch [4/4], Step [2100/30091], Loss: 4.0412\n",
      "Epoch [4/4], Step [2175/30091], Loss: 3.6740\n",
      "Epoch [4/4], Step [2250/30091], Loss: 3.7084\n",
      "Epoch [4/4], Step [2325/30091], Loss: 3.7375\n",
      "Epoch [4/4], Step [2400/30091], Loss: 3.8206\n",
      "Epoch [4/4], Step [2475/30091], Loss: 3.7549\n",
      "Epoch [4/4], Step [2550/30091], Loss: 3.7412\n",
      "Epoch [4/4], Step [2625/30091], Loss: 3.8159\n",
      "Epoch [4/4], Step [2700/30091], Loss: 3.9065\n",
      "Epoch [4/4], Step [2775/30091], Loss: 3.9115\n",
      "Epoch [4/4], Step [2850/30091], Loss: 3.9401\n",
      "Epoch [4/4], Step [2925/30091], Loss: 3.8748\n",
      "Epoch [4/4], Step [3000/30091], Loss: 3.8499\n",
      "Validation perplexity: 34.543267447542505\n",
      "Epoch [4/4], Step [3075/30091], Loss: 3.7550\n",
      "Epoch [4/4], Step [3150/30091], Loss: 3.8094\n",
      "Epoch [4/4], Step [3225/30091], Loss: 3.7399\n",
      "Epoch [4/4], Step [3300/30091], Loss: 3.7330\n",
      "Epoch [4/4], Step [3375/30091], Loss: 3.7912\n",
      "Epoch [4/4], Step [3450/30091], Loss: 3.8111\n",
      "Epoch [4/4], Step [3525/30091], Loss: 3.8140\n",
      "Epoch [4/4], Step [3600/30091], Loss: 3.8986\n",
      "Epoch [4/4], Step [3675/30091], Loss: 3.9331\n",
      "Epoch [4/4], Step [3750/30091], Loss: 3.6490\n",
      "Epoch [4/4], Step [3825/30091], Loss: 3.8445\n",
      "Epoch [4/4], Step [3900/30091], Loss: 3.7584\n",
      "Epoch [4/4], Step [3975/30091], Loss: 3.7510\n",
      "Epoch [4/4], Step [4050/30091], Loss: 3.7315\n",
      "Epoch [4/4], Step [4125/30091], Loss: 3.8608\n",
      "Epoch [4/4], Step [4200/30091], Loss: 3.9066\n",
      "Epoch [4/4], Step [4275/30091], Loss: 3.9010\n",
      "Epoch [4/4], Step [4350/30091], Loss: 3.7061\n",
      "Epoch [4/4], Step [4425/30091], Loss: 4.0280\n",
      "Epoch [4/4], Step [4500/30091], Loss: 3.6983\n",
      "Epoch [4/4], Step [4575/30091], Loss: 3.8802\n",
      "Epoch [4/4], Step [4650/30091], Loss: 3.5904\n",
      "Epoch [4/4], Step [4725/30091], Loss: 3.6124\n",
      "Epoch [4/4], Step [4800/30091], Loss: 3.8846\n",
      "Epoch [4/4], Step [4875/30091], Loss: 3.7936\n",
      "Epoch [4/4], Step [4950/30091], Loss: 3.8373\n",
      "Epoch [4/4], Step [5025/30091], Loss: 3.8125\n",
      "Epoch [4/4], Step [5100/30091], Loss: 3.8774\n",
      "Epoch [4/4], Step [5175/30091], Loss: 3.8974\n",
      "Epoch [4/4], Step [5250/30091], Loss: 3.7994\n",
      "Epoch [4/4], Step [5325/30091], Loss: 3.9825\n",
      "Epoch [4/4], Step [5400/30091], Loss: 3.7558\n",
      "Epoch [4/4], Step [5475/30091], Loss: 3.9370\n",
      "Epoch [4/4], Step [5550/30091], Loss: 3.8080\n",
      "Epoch [4/4], Step [5625/30091], Loss: 3.8489\n",
      "Epoch [4/4], Step [5700/30091], Loss: 3.7399\n",
      "Epoch [4/4], Step [5775/30091], Loss: 3.7728\n",
      "Epoch [4/4], Step [5850/30091], Loss: 3.8293\n",
      "Epoch [4/4], Step [5925/30091], Loss: 3.7212\n",
      "Epoch [4/4], Step [6000/30091], Loss: 3.8037\n",
      "Validation perplexity: 34.286588872468066\n",
      "Epoch [4/4], Step [6075/30091], Loss: 3.8117\n",
      "Epoch [4/4], Step [6150/30091], Loss: 3.8091\n",
      "Epoch [4/4], Step [6225/30091], Loss: 4.0545\n",
      "Epoch [4/4], Step [6300/30091], Loss: 3.7578\n",
      "Epoch [4/4], Step [6375/30091], Loss: 3.7467\n",
      "Epoch [4/4], Step [6450/30091], Loss: 3.8788\n",
      "Epoch [4/4], Step [6525/30091], Loss: 3.9511\n",
      "Epoch [4/4], Step [6600/30091], Loss: 3.9001\n",
      "Epoch [4/4], Step [6675/30091], Loss: 3.7904\n",
      "Epoch [4/4], Step [6750/30091], Loss: 3.7964\n",
      "Epoch [4/4], Step [6825/30091], Loss: 3.6912\n",
      "Epoch [4/4], Step [6900/30091], Loss: 3.8185\n",
      "Epoch [4/4], Step [6975/30091], Loss: 3.9735\n",
      "Epoch [4/4], Step [7050/30091], Loss: 3.6565\n",
      "Epoch [4/4], Step [7125/30091], Loss: 3.9209\n",
      "Epoch [4/4], Step [7200/30091], Loss: 3.7798\n",
      "Epoch [4/4], Step [7275/30091], Loss: 3.6664\n",
      "Epoch [4/4], Step [7350/30091], Loss: 3.9389\n",
      "Epoch [4/4], Step [7425/30091], Loss: 3.7225\n",
      "Epoch [4/4], Step [7500/30091], Loss: 3.8195\n",
      "Epoch [4/4], Step [7575/30091], Loss: 3.8563\n",
      "Epoch [4/4], Step [7650/30091], Loss: 3.8084\n",
      "Epoch [4/4], Step [7725/30091], Loss: 3.8473\n",
      "Epoch [4/4], Step [7800/30091], Loss: 3.9320\n",
      "Epoch [4/4], Step [7875/30091], Loss: 3.8104\n",
      "Epoch [4/4], Step [7950/30091], Loss: 3.5341\n",
      "Epoch [4/4], Step [8025/30091], Loss: 3.9745\n",
      "Epoch [4/4], Step [8100/30091], Loss: 3.8190\n",
      "Epoch [4/4], Step [8175/30091], Loss: 3.7432\n",
      "Epoch [4/4], Step [8250/30091], Loss: 3.8194\n",
      "Epoch [4/4], Step [8325/30091], Loss: 3.8188\n",
      "Epoch [4/4], Step [8400/30091], Loss: 3.7560\n",
      "Epoch [4/4], Step [8475/30091], Loss: 3.9220\n",
      "Epoch [4/4], Step [8550/30091], Loss: 3.8604\n",
      "Epoch [4/4], Step [8625/30091], Loss: 3.8392\n",
      "Epoch [4/4], Step [8700/30091], Loss: 3.8866\n",
      "Epoch [4/4], Step [8775/30091], Loss: 3.8682\n",
      "Epoch [4/4], Step [8850/30091], Loss: 3.7563\n",
      "Epoch [4/4], Step [8925/30091], Loss: 3.8693\n",
      "Epoch [4/4], Step [9000/30091], Loss: 3.8936\n",
      "Validation perplexity: 34.186724135488475\n",
      "Epoch [4/4], Step [9075/30091], Loss: 3.8236\n",
      "Epoch [4/4], Step [9150/30091], Loss: 3.7994\n",
      "Epoch [4/4], Step [9225/30091], Loss: 3.7639\n",
      "Epoch [4/4], Step [9300/30091], Loss: 3.8719\n",
      "Epoch [4/4], Step [9375/30091], Loss: 3.8640\n",
      "Epoch [4/4], Step [9450/30091], Loss: 3.8462\n",
      "Epoch [4/4], Step [9525/30091], Loss: 3.8626\n",
      "Epoch [4/4], Step [9600/30091], Loss: 3.6416\n",
      "Epoch [4/4], Step [9675/30091], Loss: 3.7605\n",
      "Epoch [4/4], Step [9750/30091], Loss: 3.8802\n",
      "Epoch [4/4], Step [9825/30091], Loss: 3.8838\n",
      "Epoch [4/4], Step [9900/30091], Loss: 3.5608\n",
      "Epoch [4/4], Step [9975/30091], Loss: 3.9924\n",
      "Epoch [4/4], Step [10050/30091], Loss: 3.8479\n",
      "Epoch [4/4], Step [10125/30091], Loss: 3.6569\n",
      "Epoch [4/4], Step [10200/30091], Loss: 3.8318\n",
      "Epoch [4/4], Step [10275/30091], Loss: 3.7704\n",
      "Epoch [4/4], Step [10350/30091], Loss: 3.8443\n",
      "Epoch [4/4], Step [10425/30091], Loss: 3.7416\n",
      "Epoch [4/4], Step [10500/30091], Loss: 3.8033\n",
      "Epoch [4/4], Step [10575/30091], Loss: 4.0111\n",
      "Epoch [4/4], Step [10650/30091], Loss: 3.7319\n",
      "Epoch [4/4], Step [10725/30091], Loss: 3.7914\n",
      "Epoch [4/4], Step [10800/30091], Loss: 3.8067\n",
      "Epoch [4/4], Step [10875/30091], Loss: 4.0006\n",
      "Epoch [4/4], Step [10950/30091], Loss: 3.6981\n",
      "Epoch [4/4], Step [11025/30091], Loss: 3.8197\n",
      "Epoch [4/4], Step [11100/30091], Loss: 3.9105\n",
      "Epoch [4/4], Step [11175/30091], Loss: 3.6699\n",
      "Epoch [4/4], Step [11250/30091], Loss: 3.8455\n",
      "Epoch [4/4], Step [11325/30091], Loss: 3.7383\n",
      "Epoch [4/4], Step [11400/30091], Loss: 3.7683\n",
      "Epoch [4/4], Step [11475/30091], Loss: 3.9317\n",
      "Epoch [4/4], Step [11550/30091], Loss: 3.8664\n",
      "Epoch [4/4], Step [11625/30091], Loss: 3.7666\n",
      "Epoch [4/4], Step [11700/30091], Loss: 3.8347\n",
      "Epoch [4/4], Step [11775/30091], Loss: 3.9452\n",
      "Epoch [4/4], Step [11850/30091], Loss: 3.8115\n",
      "Epoch [4/4], Step [11925/30091], Loss: 3.8752\n",
      "Epoch [4/4], Step [12000/30091], Loss: 4.0318\n",
      "Validation perplexity: 34.13742792336578\n",
      "Epoch [4/4], Step [12075/30091], Loss: 3.8317\n",
      "Epoch [4/4], Step [12150/30091], Loss: 3.8208\n",
      "Epoch [4/4], Step [12225/30091], Loss: 3.7778\n",
      "Epoch [4/4], Step [12300/30091], Loss: 3.5201\n",
      "Epoch [4/4], Step [12375/30091], Loss: 3.7801\n",
      "Epoch [4/4], Step [12450/30091], Loss: 3.7308\n",
      "Epoch [4/4], Step [12525/30091], Loss: 3.8424\n",
      "Epoch [4/4], Step [12600/30091], Loss: 3.7676\n",
      "Epoch [4/4], Step [12675/30091], Loss: 3.8767\n",
      "Epoch [4/4], Step [12750/30091], Loss: 3.8333\n",
      "Epoch [4/4], Step [12825/30091], Loss: 3.7851\n",
      "Epoch [4/4], Step [12900/30091], Loss: 3.9007\n",
      "Epoch [4/4], Step [12975/30091], Loss: 3.8567\n",
      "Epoch [4/4], Step [13050/30091], Loss: 3.7693\n",
      "Epoch [4/4], Step [13125/30091], Loss: 3.8741\n",
      "Epoch [4/4], Step [13200/30091], Loss: 3.8169\n",
      "Epoch [4/4], Step [13275/30091], Loss: 3.8640\n",
      "Epoch [4/4], Step [13350/30091], Loss: 3.7034\n",
      "Epoch [4/4], Step [13425/30091], Loss: 3.8267\n",
      "Epoch [4/4], Step [13500/30091], Loss: 3.8295\n",
      "Epoch [4/4], Step [13575/30091], Loss: 3.7978\n",
      "Epoch [4/4], Step [13650/30091], Loss: 4.0134\n",
      "Epoch [4/4], Step [13725/30091], Loss: 3.9912\n",
      "Epoch [4/4], Step [13800/30091], Loss: 3.8151\n",
      "Epoch [4/4], Step [13875/30091], Loss: 3.7733\n",
      "Epoch [4/4], Step [13950/30091], Loss: 3.7186\n",
      "Epoch [4/4], Step [14025/30091], Loss: 3.8895\n",
      "Epoch [4/4], Step [14100/30091], Loss: 3.8093\n",
      "Epoch [4/4], Step [14175/30091], Loss: 3.9130\n",
      "Epoch [4/4], Step [14250/30091], Loss: 3.6459\n",
      "Epoch [4/4], Step [14325/30091], Loss: 3.7327\n",
      "Epoch [4/4], Step [14400/30091], Loss: 3.8660\n",
      "Epoch [4/4], Step [14475/30091], Loss: 3.9510\n",
      "Epoch [4/4], Step [14550/30091], Loss: 3.8123\n",
      "Epoch [4/4], Step [14625/30091], Loss: 3.8441\n",
      "Epoch [4/4], Step [14700/30091], Loss: 3.8288\n",
      "Epoch [4/4], Step [14775/30091], Loss: 3.8059\n",
      "Epoch [4/4], Step [14850/30091], Loss: 3.7948\n",
      "Epoch [4/4], Step [14925/30091], Loss: 3.8620\n",
      "Epoch [4/4], Step [15000/30091], Loss: 3.7337\n",
      "Validation perplexity: 33.853782475884536\n",
      "Epoch [4/4], Step [15075/30091], Loss: 3.9500\n",
      "Epoch [4/4], Step [15150/30091], Loss: 3.7170\n",
      "Epoch [4/4], Step [15225/30091], Loss: 3.9634\n",
      "Epoch [4/4], Step [15300/30091], Loss: 3.6109\n",
      "Epoch [4/4], Step [15375/30091], Loss: 3.7138\n",
      "Epoch [4/4], Step [15450/30091], Loss: 3.8437\n",
      "Epoch [4/4], Step [15525/30091], Loss: 3.6730\n",
      "Epoch [4/4], Step [15600/30091], Loss: 3.7015\n",
      "Epoch [4/4], Step [15675/30091], Loss: 3.6480\n",
      "Epoch [4/4], Step [15750/30091], Loss: 3.8768\n",
      "Epoch [4/4], Step [15825/30091], Loss: 3.8281\n",
      "Epoch [4/4], Step [15900/30091], Loss: 3.5928\n",
      "Epoch [4/4], Step [15975/30091], Loss: 3.8420\n",
      "Epoch [4/4], Step [16050/30091], Loss: 3.9083\n",
      "Epoch [4/4], Step [16125/30091], Loss: 3.9155\n",
      "Epoch [4/4], Step [16200/30091], Loss: 3.8068\n",
      "Epoch [4/4], Step [16275/30091], Loss: 3.8115\n",
      "Epoch [4/4], Step [16350/30091], Loss: 3.9254\n",
      "Epoch [4/4], Step [16425/30091], Loss: 3.8069\n",
      "Epoch [4/4], Step [16500/30091], Loss: 3.8409\n",
      "Epoch [4/4], Step [16575/30091], Loss: 3.8271\n",
      "Epoch [4/4], Step [16650/30091], Loss: 3.8150\n",
      "Epoch [4/4], Step [16725/30091], Loss: 3.7876\n",
      "Epoch [4/4], Step [16800/30091], Loss: 3.7229\n",
      "Epoch [4/4], Step [16875/30091], Loss: 3.7811\n",
      "Epoch [4/4], Step [16950/30091], Loss: 3.7054\n",
      "Epoch [4/4], Step [17025/30091], Loss: 3.7519\n",
      "Epoch [4/4], Step [17100/30091], Loss: 3.7030\n",
      "Epoch [4/4], Step [17175/30091], Loss: 3.7484\n",
      "Epoch [4/4], Step [17250/30091], Loss: 3.8508\n",
      "Epoch [4/4], Step [17325/30091], Loss: 3.8775\n",
      "Epoch [4/4], Step [17400/30091], Loss: 3.8121\n",
      "Epoch [4/4], Step [17475/30091], Loss: 3.7142\n",
      "Epoch [4/4], Step [17550/30091], Loss: 3.7164\n",
      "Epoch [4/4], Step [17625/30091], Loss: 3.7587\n",
      "Epoch [4/4], Step [17700/30091], Loss: 3.8565\n",
      "Epoch [4/4], Step [17775/30091], Loss: 3.7989\n",
      "Epoch [4/4], Step [17850/30091], Loss: 3.9649\n",
      "Epoch [4/4], Step [17925/30091], Loss: 3.6485\n",
      "Epoch [4/4], Step [18000/30091], Loss: 3.8625\n",
      "Validation perplexity: 33.573101569911664\n",
      "Epoch [4/4], Step [18075/30091], Loss: 3.7979\n",
      "Epoch [4/4], Step [18150/30091], Loss: 3.7576\n",
      "Epoch [4/4], Step [18225/30091], Loss: 3.7482\n",
      "Epoch [4/4], Step [18300/30091], Loss: 3.9159\n",
      "Epoch [4/4], Step [18375/30091], Loss: 3.6641\n",
      "Epoch [4/4], Step [18450/30091], Loss: 3.7963\n",
      "Epoch [4/4], Step [18525/30091], Loss: 3.7167\n",
      "Epoch [4/4], Step [18600/30091], Loss: 3.5677\n",
      "Epoch [4/4], Step [18675/30091], Loss: 3.8290\n",
      "Epoch [4/4], Step [18750/30091], Loss: 3.8961\n",
      "Epoch [4/4], Step [18825/30091], Loss: 3.9369\n",
      "Epoch [4/4], Step [18900/30091], Loss: 3.8465\n",
      "Epoch [4/4], Step [18975/30091], Loss: 3.7585\n",
      "Epoch [4/4], Step [19050/30091], Loss: 3.8069\n",
      "Epoch [4/4], Step [19125/30091], Loss: 3.8629\n",
      "Epoch [4/4], Step [19200/30091], Loss: 3.7842\n",
      "Epoch [4/4], Step [19275/30091], Loss: 3.7731\n",
      "Epoch [4/4], Step [19350/30091], Loss: 3.8998\n",
      "Epoch [4/4], Step [19425/30091], Loss: 3.8234\n",
      "Epoch [4/4], Step [19500/30091], Loss: 3.8760\n",
      "Epoch [4/4], Step [19575/30091], Loss: 3.8094\n",
      "Epoch [4/4], Step [19650/30091], Loss: 3.7309\n",
      "Epoch [4/4], Step [19725/30091], Loss: 3.7291\n",
      "Epoch [4/4], Step [19800/30091], Loss: 3.8332\n",
      "Epoch [4/4], Step [19875/30091], Loss: 3.6578\n",
      "Epoch [4/4], Step [19950/30091], Loss: 3.8602\n",
      "Epoch [4/4], Step [20025/30091], Loss: 3.7151\n",
      "Epoch [4/4], Step [20100/30091], Loss: 3.7108\n",
      "Epoch [4/4], Step [20175/30091], Loss: 3.8801\n",
      "Epoch [4/4], Step [20250/30091], Loss: 3.8474\n",
      "Epoch [4/4], Step [20325/30091], Loss: 3.8619\n",
      "Epoch [4/4], Step [20400/30091], Loss: 3.6801\n",
      "Epoch [4/4], Step [20475/30091], Loss: 3.7658\n",
      "Epoch [4/4], Step [20550/30091], Loss: 3.9429\n",
      "Epoch [4/4], Step [20625/30091], Loss: 3.6634\n",
      "Epoch [4/4], Step [20700/30091], Loss: 3.7494\n",
      "Epoch [4/4], Step [20775/30091], Loss: 3.8883\n",
      "Epoch [4/4], Step [20850/30091], Loss: 3.9331\n",
      "Epoch [4/4], Step [20925/30091], Loss: 3.6353\n",
      "Epoch [4/4], Step [21000/30091], Loss: 3.8718\n",
      "Validation perplexity: 33.486428265057384\n",
      "Epoch [4/4], Step [21075/30091], Loss: 3.7743\n",
      "Epoch [4/4], Step [21150/30091], Loss: 3.8104\n",
      "Epoch [4/4], Step [21225/30091], Loss: 3.8649\n",
      "Epoch [4/4], Step [21300/30091], Loss: 3.7383\n",
      "Epoch [4/4], Step [21375/30091], Loss: 3.6589\n",
      "Epoch [4/4], Step [21450/30091], Loss: 3.8159\n",
      "Epoch [4/4], Step [21525/30091], Loss: 3.9921\n",
      "Epoch [4/4], Step [21600/30091], Loss: 3.8750\n",
      "Epoch [4/4], Step [21675/30091], Loss: 3.7684\n",
      "Epoch [4/4], Step [21750/30091], Loss: 3.8584\n",
      "Epoch [4/4], Step [21825/30091], Loss: 3.9335\n",
      "Epoch [4/4], Step [21900/30091], Loss: 3.9035\n",
      "Epoch [4/4], Step [21975/30091], Loss: 3.8695\n",
      "Epoch [4/4], Step [22050/30091], Loss: 3.6836\n",
      "Epoch [4/4], Step [22125/30091], Loss: 3.7428\n",
      "Epoch [4/4], Step [22200/30091], Loss: 3.6832\n",
      "Epoch [4/4], Step [22275/30091], Loss: 3.8666\n",
      "Epoch [4/4], Step [22350/30091], Loss: 4.0043\n",
      "Epoch [4/4], Step [22425/30091], Loss: 3.8334\n",
      "Epoch [4/4], Step [22500/30091], Loss: 3.8000\n",
      "Epoch [4/4], Step [22575/30091], Loss: 3.8365\n",
      "Epoch [4/4], Step [22650/30091], Loss: 3.8574\n",
      "Epoch [4/4], Step [22725/30091], Loss: 3.6186\n",
      "Epoch [4/4], Step [22800/30091], Loss: 3.8269\n",
      "Epoch [4/4], Step [22875/30091], Loss: 3.8785\n",
      "Epoch [4/4], Step [22950/30091], Loss: 3.8865\n",
      "Epoch [4/4], Step [23025/30091], Loss: 3.6908\n",
      "Epoch [4/4], Step [23100/30091], Loss: 3.7272\n",
      "Epoch [4/4], Step [23175/30091], Loss: 3.5950\n",
      "Epoch [4/4], Step [23250/30091], Loss: 3.8253\n",
      "Epoch [4/4], Step [23325/30091], Loss: 3.8008\n",
      "Epoch [4/4], Step [23400/30091], Loss: 3.7469\n",
      "Epoch [4/4], Step [23475/30091], Loss: 3.8333\n",
      "Epoch [4/4], Step [23550/30091], Loss: 3.7446\n",
      "Epoch [4/4], Step [23625/30091], Loss: 3.8087\n",
      "Epoch [4/4], Step [23700/30091], Loss: 3.7680\n",
      "Epoch [4/4], Step [23775/30091], Loss: 3.9486\n",
      "Epoch [4/4], Step [23850/30091], Loss: 3.8424\n",
      "Epoch [4/4], Step [23925/30091], Loss: 3.7095\n",
      "Epoch [4/4], Step [24000/30091], Loss: 3.8404\n",
      "Validation perplexity: 33.48253496571783\n",
      "Epoch [4/4], Step [24075/30091], Loss: 3.9055\n",
      "Epoch [4/4], Step [24150/30091], Loss: 3.9442\n",
      "Epoch [4/4], Step [24225/30091], Loss: 3.8538\n",
      "Epoch [4/4], Step [24300/30091], Loss: 3.7844\n",
      "Epoch [4/4], Step [24375/30091], Loss: 3.7735\n",
      "Epoch [4/4], Step [24450/30091], Loss: 3.7448\n",
      "Epoch [4/4], Step [24525/30091], Loss: 3.8271\n",
      "Epoch [4/4], Step [24600/30091], Loss: 3.7320\n",
      "Epoch [4/4], Step [24675/30091], Loss: 3.8596\n",
      "Epoch [4/4], Step [24750/30091], Loss: 3.7832\n",
      "Epoch [4/4], Step [24825/30091], Loss: 3.9421\n",
      "Epoch [4/4], Step [24900/30091], Loss: 3.7031\n",
      "Epoch [4/4], Step [24975/30091], Loss: 3.6474\n",
      "Epoch [4/4], Step [25050/30091], Loss: 3.9181\n",
      "Epoch [4/4], Step [25125/30091], Loss: 3.6797\n",
      "Epoch [4/4], Step [25200/30091], Loss: 3.8376\n",
      "Epoch [4/4], Step [25275/30091], Loss: 3.7099\n",
      "Epoch [4/4], Step [25350/30091], Loss: 3.7632\n",
      "Epoch [4/4], Step [25425/30091], Loss: 3.8138\n",
      "Epoch [4/4], Step [25500/30091], Loss: 3.7763\n",
      "Epoch [4/4], Step [25575/30091], Loss: 3.8015\n",
      "Epoch [4/4], Step [25650/30091], Loss: 3.7102\n",
      "Epoch [4/4], Step [25725/30091], Loss: 3.8224\n",
      "Epoch [4/4], Step [25800/30091], Loss: 3.7711\n",
      "Epoch [4/4], Step [25875/30091], Loss: 3.8573\n",
      "Epoch [4/4], Step [25950/30091], Loss: 3.9147\n",
      "Epoch [4/4], Step [26025/30091], Loss: 3.8740\n",
      "Epoch [4/4], Step [26100/30091], Loss: 3.7165\n",
      "Epoch [4/4], Step [26175/30091], Loss: 3.7012\n",
      "Epoch [4/4], Step [26250/30091], Loss: 3.7793\n",
      "Epoch [4/4], Step [26325/30091], Loss: 3.8557\n",
      "Epoch [4/4], Step [26400/30091], Loss: 3.7425\n",
      "Epoch [4/4], Step [26475/30091], Loss: 3.8050\n",
      "Epoch [4/4], Step [26550/30091], Loss: 3.8702\n",
      "Epoch [4/4], Step [26625/30091], Loss: 3.9198\n",
      "Epoch [4/4], Step [26700/30091], Loss: 3.7599\n",
      "Epoch [4/4], Step [26775/30091], Loss: 3.8673\n",
      "Epoch [4/4], Step [26850/30091], Loss: 3.8508\n",
      "Epoch [4/4], Step [26925/30091], Loss: 3.8590\n",
      "Epoch [4/4], Step [27000/30091], Loss: 4.0192\n",
      "Validation perplexity: 33.160990928124534\n",
      "Epoch [4/4], Step [27075/30091], Loss: 3.7802\n",
      "Epoch [4/4], Step [27150/30091], Loss: 3.7711\n",
      "Epoch [4/4], Step [27225/30091], Loss: 3.9289\n",
      "Epoch [4/4], Step [27300/30091], Loss: 3.9021\n",
      "Epoch [4/4], Step [27375/30091], Loss: 3.8512\n",
      "Epoch [4/4], Step [27450/30091], Loss: 3.7349\n",
      "Epoch [4/4], Step [27525/30091], Loss: 3.8258\n",
      "Epoch [4/4], Step [27600/30091], Loss: 3.6751\n",
      "Epoch [4/4], Step [27675/30091], Loss: 3.7872\n",
      "Epoch [4/4], Step [27750/30091], Loss: 3.7302\n",
      "Epoch [4/4], Step [27825/30091], Loss: 3.6638\n",
      "Epoch [4/4], Step [27900/30091], Loss: 3.6785\n",
      "Epoch [4/4], Step [27975/30091], Loss: 3.7665\n",
      "Epoch [4/4], Step [28050/30091], Loss: 3.6462\n",
      "Epoch [4/4], Step [28125/30091], Loss: 3.8633\n",
      "Epoch [4/4], Step [28200/30091], Loss: 3.8986\n",
      "Epoch [4/4], Step [28275/30091], Loss: 3.8483\n",
      "Epoch [4/4], Step [28350/30091], Loss: 3.8533\n",
      "Epoch [4/4], Step [28425/30091], Loss: 3.8543\n",
      "Epoch [4/4], Step [28500/30091], Loss: 3.7730\n",
      "Epoch [4/4], Step [28575/30091], Loss: 3.8429\n",
      "Epoch [4/4], Step [28650/30091], Loss: 3.8062\n",
      "Epoch [4/4], Step [28725/30091], Loss: 3.8225\n",
      "Epoch [4/4], Step [28800/30091], Loss: 3.7724\n",
      "Epoch [4/4], Step [28875/30091], Loss: 3.7911\n",
      "Epoch [4/4], Step [28950/30091], Loss: 3.7366\n",
      "Epoch [4/4], Step [29025/30091], Loss: 3.8028\n",
      "Epoch [4/4], Step [29100/30091], Loss: 3.7211\n",
      "Epoch [4/4], Step [29175/30091], Loss: 3.6878\n",
      "Epoch [4/4], Step [29250/30091], Loss: 3.8098\n",
      "Epoch [4/4], Step [29325/30091], Loss: 3.9123\n",
      "Epoch [4/4], Step [29400/30091], Loss: 3.7719\n",
      "Epoch [4/4], Step [29475/30091], Loss: 3.7956\n",
      "Epoch [4/4], Step [29550/30091], Loss: 3.9451\n",
      "Epoch [4/4], Step [29625/30091], Loss: 3.7295\n",
      "Epoch [4/4], Step [29700/30091], Loss: 3.8276\n",
      "Epoch [4/4], Step [29775/30091], Loss: 3.6428\n",
      "Epoch [4/4], Step [29850/30091], Loss: 3.8970\n",
      "Epoch [4/4], Step [29925/30091], Loss: 3.7771\n",
      "Epoch [4/4], Step [30000/30091], Loss: 3.8646\n",
      "Validation perplexity: 33.06810642956479\n",
      "Epoch [4/4], Step [30075/30091], Loss: 3.8759\n",
      "Epoch [4/4] Average Loss: 3.8131, Perplexity: 45.29\n"
     ]
    }
   ],
   "source": [
    "#from src.helper  import\n",
    "from src.helper import get_cleaned_spanish_text_as_string, clean_text_spanish_remove,get_lines_without_number,clean_spanish_text,get_cleaned_text\n",
    "from src.helper import clean_text_spanish_both,clean_text_both,get_cleaned_spanish_text_as_string,clean_text_spanish_remove,get_lines_without_number,clean_spanish_text,get_cleaned_text\n",
    "\n",
    "text_path = \"content/spa_wikipedia_2021_30K-sentences.txt\"\n",
    "path_to_save_folder= \"model/attent_data\"\n",
    "\n",
    "#raw_text = get_cleaned_spanish_text_as_string(text_path)\n",
    "#enc_text = tokenizer.encode(raw_text)\n",
    "\n",
    "# Load the text data\n",
    "#raw_text = get_cleaned_spanish_text_as_string(text_path) #Standart \n",
    "raw_text = get_cleaned_text(text_path,clean_text_spanish_remove)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Tokenize the text\n",
    "token_ids = tokenizer.encode(raw_text)\n",
    "\n",
    "print(\"Total number of tokens:\", len(token_ids))\n",
    "print(\"First 10 tokens:\", token_ids[:10])\n",
    "print(\"Text size: \",len(raw_text))\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32 #Standard 32 \n",
    "context_length = 32  # Increased context size\n",
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 128\n",
    "attention_dim = 64\n",
    "hidden_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_dataloader(\n",
    "    raw_text, batch_size=batch_size, \n",
    "    context_length=context_length,     shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = LanguageModelWithAttention(\n",
    "    vocab_size, embedding_dim, attention_dim, context_length, hidden_dim, num_heads, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters'\n",
    "num_epochs = 4\n",
    "data_loader = train_dataloader\n",
    "\n",
    "print_every = 75\n",
    "evaluate_every = 3000\n",
    "train_run_label = \"basic_att_preprocessingRemove\"\n",
    "\n",
    "from src.train import train_attention\n",
    "#(all_losses,train_losses,perplexities) = train_attention(model,num_epochs,optimizer,criterion,data_loader,path_to_save_folder,\n",
    "#                                               train_run_label,vocab_size,device,print_every)\n",
    "\n",
    "(all_losses,train_losses,perplexities,all_perplex) = train_attention(model,num_epochs,optimizer,criterion,data_loader,path_to_save_folder,train_run_label,vocab_size,device,evaluate_every,dev_dataloader,print_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2bae02-0175-49b7-afd8-3a343184c58a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Another one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c55eec-b0e3-41c4-b456-6b7fc3e02a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6804b9d-530e-4b8c-a9e6-e5d7902aec8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines not removed :  28494\n",
      "Total lines replaced 28494\n",
      "Total number of tokens: 1127637\n",
      "First 10 tokens: [1105, 390, 450, 22379, 390, 8235, 1658, 555, 64, 719]\n",
      "Text size:  3475552\n",
      "Using device: cuda\n",
      "Started Training\n",
      "Epoch [1/4], Step [0/28190], Loss: 10.8310\n",
      "Validation perplexity: 49869.324770656036\n",
      "Epoch [1/4], Step [75/28190], Loss: 6.9254\n",
      "Epoch [1/4], Step [150/28190], Loss: 6.7594\n",
      "Epoch [1/4], Step [225/28190], Loss: 6.6811\n",
      "Epoch [1/4], Step [300/28190], Loss: 6.8016\n",
      "Epoch [1/4], Step [375/28190], Loss: 6.6541\n",
      "Epoch [1/4], Step [450/28190], Loss: 6.5188\n",
      "Epoch [1/4], Step [525/28190], Loss: 6.4779\n",
      "Epoch [1/4], Step [600/28190], Loss: 6.3179\n",
      "Epoch [1/4], Step [675/28190], Loss: 6.2427\n",
      "Epoch [1/4], Step [750/28190], Loss: 6.0403\n",
      "Epoch [1/4], Step [825/28190], Loss: 5.9730\n",
      "Epoch [1/4], Step [900/28190], Loss: 5.8638\n",
      "Epoch [1/4], Step [975/28190], Loss: 5.9628\n",
      "Epoch [1/4], Step [1050/28190], Loss: 5.8493\n",
      "Epoch [1/4], Step [1125/28190], Loss: 5.6870\n",
      "Epoch [1/4], Step [1200/28190], Loss: 5.6583\n",
      "Epoch [1/4], Step [1275/28190], Loss: 5.7517\n",
      "Epoch [1/4], Step [1350/28190], Loss: 5.5793\n",
      "Epoch [1/4], Step [1425/28190], Loss: 5.6638\n",
      "Epoch [1/4], Step [1500/28190], Loss: 5.4908\n",
      "Epoch [1/4], Step [1575/28190], Loss: 5.3325\n",
      "Epoch [1/4], Step [1650/28190], Loss: 5.0710\n",
      "Epoch [1/4], Step [1725/28190], Loss: 5.3167\n",
      "Epoch [1/4], Step [1800/28190], Loss: 5.1571\n",
      "Epoch [1/4], Step [1875/28190], Loss: 5.4640\n",
      "Epoch [1/4], Step [1950/28190], Loss: 5.1855\n",
      "Epoch [1/4], Step [2025/28190], Loss: 5.2470\n",
      "Epoch [1/4], Step [2100/28190], Loss: 5.0978\n",
      "Epoch [1/4], Step [2175/28190], Loss: 5.3173\n",
      "Epoch [1/4], Step [2250/28190], Loss: 5.3570\n",
      "Epoch [1/4], Step [2325/28190], Loss: 5.4231\n",
      "Epoch [1/4], Step [2400/28190], Loss: 5.1409\n",
      "Epoch [1/4], Step [2475/28190], Loss: 5.0804\n",
      "Epoch [1/4], Step [2550/28190], Loss: 5.0832\n",
      "Epoch [1/4], Step [2625/28190], Loss: 4.8908\n",
      "Epoch [1/4], Step [2700/28190], Loss: 5.0968\n",
      "Epoch [1/4], Step [2775/28190], Loss: 4.9648\n",
      "Epoch [1/4], Step [2850/28190], Loss: 5.0234\n",
      "Epoch [1/4], Step [2925/28190], Loss: 4.9109\n",
      "Epoch [1/4], Step [3000/28190], Loss: 5.1065\n",
      "Validation perplexity: 117.9481992646122\n",
      "Epoch [1/4], Step [3075/28190], Loss: 5.2513\n",
      "Epoch [1/4], Step [3150/28190], Loss: 4.9298\n",
      "Epoch [1/4], Step [3225/28190], Loss: 4.9477\n",
      "Epoch [1/4], Step [3300/28190], Loss: 5.0661\n",
      "Epoch [1/4], Step [3375/28190], Loss: 4.6711\n",
      "Epoch [1/4], Step [3450/28190], Loss: 4.8684\n",
      "Epoch [1/4], Step [3525/28190], Loss: 4.9350\n",
      "Epoch [1/4], Step [3600/28190], Loss: 4.9135\n",
      "Epoch [1/4], Step [3675/28190], Loss: 4.9447\n",
      "Epoch [1/4], Step [3750/28190], Loss: 4.9622\n",
      "Epoch [1/4], Step [3825/28190], Loss: 4.5685\n",
      "Epoch [1/4], Step [3900/28190], Loss: 4.8611\n",
      "Epoch [1/4], Step [3975/28190], Loss: 4.6944\n",
      "Epoch [1/4], Step [4050/28190], Loss: 4.8406\n",
      "Epoch [1/4], Step [4125/28190], Loss: 4.7509\n",
      "Epoch [1/4], Step [4200/28190], Loss: 4.6956\n",
      "Epoch [1/4], Step [4275/28190], Loss: 4.9347\n",
      "Epoch [1/4], Step [4350/28190], Loss: 4.6095\n",
      "Epoch [1/4], Step [4425/28190], Loss: 4.8856\n",
      "Epoch [1/4], Step [4500/28190], Loss: 4.7501\n",
      "Epoch [1/4], Step [4575/28190], Loss: 4.6648\n",
      "Epoch [1/4], Step [4650/28190], Loss: 4.8650\n",
      "Epoch [1/4], Step [4725/28190], Loss: 4.8418\n",
      "Epoch [1/4], Step [4800/28190], Loss: 4.6128\n",
      "Epoch [1/4], Step [4875/28190], Loss: 4.6045\n",
      "Epoch [1/4], Step [4950/28190], Loss: 4.6108\n",
      "Epoch [1/4], Step [5025/28190], Loss: 4.8943\n",
      "Epoch [1/4], Step [5100/28190], Loss: 4.6712\n",
      "Epoch [1/4], Step [5175/28190], Loss: 4.9590\n",
      "Epoch [1/4], Step [5250/28190], Loss: 4.7275\n",
      "Epoch [1/4], Step [5325/28190], Loss: 4.6805\n",
      "Epoch [1/4], Step [5400/28190], Loss: 4.8091\n",
      "Epoch [1/4], Step [5475/28190], Loss: 4.6186\n",
      "Epoch [1/4], Step [5550/28190], Loss: 4.6588\n",
      "Epoch [1/4], Step [5625/28190], Loss: 4.5791\n",
      "Epoch [1/4], Step [5700/28190], Loss: 4.8788\n",
      "Epoch [1/4], Step [5775/28190], Loss: 4.7876\n",
      "Epoch [1/4], Step [5850/28190], Loss: 4.6090\n",
      "Epoch [1/4], Step [5925/28190], Loss: 5.0276\n",
      "Epoch [1/4], Step [6000/28190], Loss: 4.6422\n",
      "Validation perplexity: 83.88659933675281\n",
      "Epoch [1/4], Step [6075/28190], Loss: 4.7016\n",
      "Epoch [1/4], Step [6150/28190], Loss: 4.7503\n",
      "Epoch [1/4], Step [6225/28190], Loss: 4.4206\n",
      "Epoch [1/4], Step [6300/28190], Loss: 4.6417\n",
      "Epoch [1/4], Step [6375/28190], Loss: 4.4660\n",
      "Epoch [1/4], Step [6450/28190], Loss: 4.5760\n",
      "Epoch [1/4], Step [6525/28190], Loss: 4.5632\n",
      "Epoch [1/4], Step [6600/28190], Loss: 4.7440\n",
      "Epoch [1/4], Step [6675/28190], Loss: 4.4153\n",
      "Epoch [1/4], Step [6750/28190], Loss: 4.7275\n",
      "Epoch [1/4], Step [6825/28190], Loss: 4.6291\n",
      "Epoch [1/4], Step [6900/28190], Loss: 4.4490\n",
      "Epoch [1/4], Step [6975/28190], Loss: 4.4900\n",
      "Epoch [1/4], Step [7050/28190], Loss: 4.3288\n",
      "Epoch [1/4], Step [7125/28190], Loss: 4.3486\n",
      "Epoch [1/4], Step [7200/28190], Loss: 4.9174\n",
      "Epoch [1/4], Step [7275/28190], Loss: 4.7559\n",
      "Epoch [1/4], Step [7350/28190], Loss: 4.5964\n",
      "Epoch [1/4], Step [7425/28190], Loss: 4.7316\n",
      "Epoch [1/4], Step [7500/28190], Loss: 4.4055\n",
      "Epoch [1/4], Step [7575/28190], Loss: 4.5011\n",
      "Epoch [1/4], Step [7650/28190], Loss: 4.6991\n",
      "Epoch [1/4], Step [7725/28190], Loss: 4.6653\n",
      "Epoch [1/4], Step [7800/28190], Loss: 4.6151\n",
      "Epoch [1/4], Step [7875/28190], Loss: 4.5132\n",
      "Epoch [1/4], Step [7950/28190], Loss: 4.3414\n",
      "Epoch [1/4], Step [8025/28190], Loss: 4.5107\n",
      "Epoch [1/4], Step [8100/28190], Loss: 4.5580\n",
      "Epoch [1/4], Step [8175/28190], Loss: 4.6804\n",
      "Epoch [1/4], Step [8250/28190], Loss: 4.5348\n",
      "Epoch [1/4], Step [8325/28190], Loss: 4.6789\n",
      "Epoch [1/4], Step [8400/28190], Loss: 4.6721\n",
      "Epoch [1/4], Step [8475/28190], Loss: 4.2748\n",
      "Epoch [1/4], Step [8550/28190], Loss: 4.4691\n",
      "Epoch [1/4], Step [8625/28190], Loss: 4.5455\n",
      "Epoch [1/4], Step [8700/28190], Loss: 4.5087\n",
      "Epoch [1/4], Step [8775/28190], Loss: 4.5720\n",
      "Epoch [1/4], Step [8850/28190], Loss: 4.3618\n",
      "Epoch [1/4], Step [8925/28190], Loss: 4.3966\n",
      "Epoch [1/4], Step [9000/28190], Loss: 4.6342\n",
      "Validation perplexity: 71.01437838215854\n",
      "Epoch [1/4], Step [9075/28190], Loss: 4.1828\n",
      "Epoch [1/4], Step [9150/28190], Loss: 4.5014\n",
      "Epoch [1/4], Step [9225/28190], Loss: 4.4251\n",
      "Epoch [1/4], Step [9300/28190], Loss: 4.4597\n",
      "Epoch [1/4], Step [9375/28190], Loss: 4.4379\n",
      "Epoch [1/4], Step [9450/28190], Loss: 4.3137\n",
      "Epoch [1/4], Step [9525/28190], Loss: 4.3731\n",
      "Epoch [1/4], Step [9600/28190], Loss: 4.6489\n",
      "Epoch [1/4], Step [9675/28190], Loss: 4.4678\n",
      "Epoch [1/4], Step [9750/28190], Loss: 4.7816\n",
      "Epoch [1/4], Step [9825/28190], Loss: 4.3879\n",
      "Epoch [1/4], Step [9900/28190], Loss: 4.4484\n",
      "Epoch [1/4], Step [9975/28190], Loss: 4.3271\n",
      "Epoch [1/4], Step [10050/28190], Loss: 4.3265\n",
      "Epoch [1/4], Step [10125/28190], Loss: 4.3096\n",
      "Epoch [1/4], Step [10200/28190], Loss: 4.3348\n",
      "Epoch [1/4], Step [10275/28190], Loss: 4.2851\n",
      "Epoch [1/4], Step [10350/28190], Loss: 4.5930\n",
      "Epoch [1/4], Step [10425/28190], Loss: 4.3901\n",
      "Epoch [1/4], Step [10500/28190], Loss: 4.4257\n",
      "Epoch [1/4], Step [10575/28190], Loss: 4.3361\n",
      "Epoch [1/4], Step [10650/28190], Loss: 4.4401\n",
      "Epoch [1/4], Step [10725/28190], Loss: 4.4143\n",
      "Epoch [1/4], Step [10800/28190], Loss: 4.3178\n",
      "Epoch [1/4], Step [10875/28190], Loss: 4.5085\n",
      "Epoch [1/4], Step [10950/28190], Loss: 4.7330\n",
      "Epoch [1/4], Step [11025/28190], Loss: 4.3809\n",
      "Epoch [1/4], Step [11100/28190], Loss: 4.2224\n",
      "Epoch [1/4], Step [11175/28190], Loss: 4.5013\n",
      "Epoch [1/4], Step [11250/28190], Loss: 4.4431\n",
      "Epoch [1/4], Step [11325/28190], Loss: 4.4108\n",
      "Epoch [1/4], Step [11400/28190], Loss: 4.3623\n",
      "Epoch [1/4], Step [11475/28190], Loss: 4.3343\n",
      "Epoch [1/4], Step [11550/28190], Loss: 4.3526\n",
      "Epoch [1/4], Step [11625/28190], Loss: 4.4491\n",
      "Epoch [1/4], Step [11700/28190], Loss: 4.3343\n",
      "Epoch [1/4], Step [11775/28190], Loss: 4.3614\n",
      "Epoch [1/4], Step [11850/28190], Loss: 4.4054\n",
      "Epoch [1/4], Step [11925/28190], Loss: 4.5274\n",
      "Epoch [1/4], Step [12000/28190], Loss: 4.4364\n",
      "Validation perplexity: 63.240457098714955\n",
      "Epoch [1/4], Step [12075/28190], Loss: 4.3949\n",
      "Epoch [1/4], Step [12150/28190], Loss: 4.3632\n",
      "Epoch [1/4], Step [12225/28190], Loss: 4.3449\n",
      "Epoch [1/4], Step [12300/28190], Loss: 4.5758\n",
      "Epoch [1/4], Step [12375/28190], Loss: 4.4970\n",
      "Epoch [1/4], Step [12450/28190], Loss: 4.4440\n",
      "Epoch [1/4], Step [12525/28190], Loss: 4.3939\n",
      "Epoch [1/4], Step [12600/28190], Loss: 4.4053\n",
      "Epoch [1/4], Step [12675/28190], Loss: 4.4705\n",
      "Epoch [1/4], Step [12750/28190], Loss: 4.3541\n",
      "Epoch [1/4], Step [12825/28190], Loss: 4.4594\n",
      "Epoch [1/4], Step [12900/28190], Loss: 4.1264\n",
      "Epoch [1/4], Step [12975/28190], Loss: 4.4053\n",
      "Epoch [1/4], Step [13050/28190], Loss: 4.4202\n",
      "Epoch [1/4], Step [13125/28190], Loss: 4.4011\n",
      "Epoch [1/4], Step [13200/28190], Loss: 4.0557\n",
      "Epoch [1/4], Step [13275/28190], Loss: 4.4102\n",
      "Epoch [1/4], Step [13350/28190], Loss: 4.2632\n",
      "Epoch [1/4], Step [13425/28190], Loss: 4.4563\n",
      "Epoch [1/4], Step [13500/28190], Loss: 4.3185\n",
      "Epoch [1/4], Step [13575/28190], Loss: 4.3422\n",
      "Epoch [1/4], Step [13650/28190], Loss: 4.2136\n",
      "Epoch [1/4], Step [13725/28190], Loss: 4.3030\n",
      "Epoch [1/4], Step [13800/28190], Loss: 4.4239\n",
      "Epoch [1/4], Step [13875/28190], Loss: 4.4295\n",
      "Epoch [1/4], Step [13950/28190], Loss: 4.4853\n",
      "Epoch [1/4], Step [14025/28190], Loss: 4.3295\n",
      "Epoch [1/4], Step [14100/28190], Loss: 4.2252\n",
      "Epoch [1/4], Step [14175/28190], Loss: 4.4385\n",
      "Epoch [1/4], Step [14250/28190], Loss: 4.4631\n",
      "Epoch [1/4], Step [14325/28190], Loss: 4.1698\n",
      "Epoch [1/4], Step [14400/28190], Loss: 4.2654\n",
      "Epoch [1/4], Step [14475/28190], Loss: 4.4047\n",
      "Epoch [1/4], Step [14550/28190], Loss: 4.2023\n",
      "Epoch [1/4], Step [14625/28190], Loss: 4.4048\n",
      "Epoch [1/4], Step [14700/28190], Loss: 4.1657\n",
      "Epoch [1/4], Step [14775/28190], Loss: 4.3836\n",
      "Epoch [1/4], Step [14850/28190], Loss: 4.3303\n",
      "Epoch [1/4], Step [14925/28190], Loss: 4.3213\n",
      "Epoch [1/4], Step [15000/28190], Loss: 4.4199\n",
      "Validation perplexity: 58.19845585804785\n",
      "Epoch [1/4], Step [15075/28190], Loss: 4.3461\n",
      "Epoch [1/4], Step [15150/28190], Loss: 4.6149\n",
      "Epoch [1/4], Step [15225/28190], Loss: 4.1196\n",
      "Epoch [1/4], Step [15300/28190], Loss: 4.3289\n",
      "Epoch [1/4], Step [15375/28190], Loss: 4.5261\n",
      "Epoch [1/4], Step [15450/28190], Loss: 4.2825\n",
      "Epoch [1/4], Step [15525/28190], Loss: 4.1571\n",
      "Epoch [1/4], Step [15600/28190], Loss: 4.2618\n",
      "Epoch [1/4], Step [15675/28190], Loss: 4.3546\n",
      "Epoch [1/4], Step [15750/28190], Loss: 4.2143\n",
      "Epoch [1/4], Step [15825/28190], Loss: 4.1420\n",
      "Epoch [1/4], Step [15900/28190], Loss: 4.2441\n",
      "Epoch [1/4], Step [15975/28190], Loss: 4.1647\n",
      "Epoch [1/4], Step [16050/28190], Loss: 4.3268\n",
      "Epoch [1/4], Step [16125/28190], Loss: 4.3937\n",
      "Epoch [1/4], Step [16200/28190], Loss: 4.3789\n",
      "Epoch [1/4], Step [16275/28190], Loss: 4.4151\n",
      "Epoch [1/4], Step [16350/28190], Loss: 4.3356\n",
      "Epoch [1/4], Step [16425/28190], Loss: 4.4390\n",
      "Epoch [1/4], Step [16500/28190], Loss: 4.2693\n",
      "Epoch [1/4], Step [16575/28190], Loss: 4.1385\n",
      "Epoch [1/4], Step [16650/28190], Loss: 4.3335\n",
      "Epoch [1/4], Step [16725/28190], Loss: 4.2550\n",
      "Epoch [1/4], Step [16800/28190], Loss: 4.4022\n",
      "Epoch [1/4], Step [16875/28190], Loss: 4.0879\n",
      "Epoch [1/4], Step [16950/28190], Loss: 4.5035\n",
      "Epoch [1/4], Step [17025/28190], Loss: 4.4260\n",
      "Epoch [1/4], Step [17100/28190], Loss: 4.1708\n",
      "Epoch [1/4], Step [17175/28190], Loss: 4.2981\n",
      "Epoch [1/4], Step [17250/28190], Loss: 4.3062\n",
      "Epoch [1/4], Step [17325/28190], Loss: 4.1144\n",
      "Epoch [1/4], Step [17400/28190], Loss: 4.1721\n",
      "Epoch [1/4], Step [17475/28190], Loss: 4.2853\n",
      "Epoch [1/4], Step [17550/28190], Loss: 4.1566\n",
      "Epoch [1/4], Step [17625/28190], Loss: 4.3210\n",
      "Epoch [1/4], Step [17700/28190], Loss: 4.2180\n",
      "Epoch [1/4], Step [17775/28190], Loss: 4.2936\n",
      "Epoch [1/4], Step [17850/28190], Loss: 4.2606\n",
      "Epoch [1/4], Step [17925/28190], Loss: 4.3019\n",
      "Epoch [1/4], Step [18000/28190], Loss: 4.1759\n",
      "Validation perplexity: 55.175629976675694\n",
      "Epoch [1/4], Step [18075/28190], Loss: 4.2460\n",
      "Epoch [1/4], Step [18150/28190], Loss: 4.0935\n",
      "Epoch [1/4], Step [18225/28190], Loss: 4.2694\n",
      "Epoch [1/4], Step [18300/28190], Loss: 4.4433\n",
      "Epoch [1/4], Step [18375/28190], Loss: 4.3220\n",
      "Epoch [1/4], Step [18450/28190], Loss: 4.3252\n",
      "Epoch [1/4], Step [18525/28190], Loss: 4.1374\n",
      "Epoch [1/4], Step [18600/28190], Loss: 4.3384\n",
      "Epoch [1/4], Step [18675/28190], Loss: 4.1164\n",
      "Epoch [1/4], Step [18750/28190], Loss: 4.2579\n",
      "Epoch [1/4], Step [18825/28190], Loss: 4.2173\n",
      "Epoch [1/4], Step [18900/28190], Loss: 4.3356\n",
      "Epoch [1/4], Step [18975/28190], Loss: 4.0044\n",
      "Epoch [1/4], Step [19050/28190], Loss: 4.2337\n",
      "Epoch [1/4], Step [19125/28190], Loss: 4.2185\n",
      "Epoch [1/4], Step [19200/28190], Loss: 4.5426\n",
      "Epoch [1/4], Step [19275/28190], Loss: 4.2501\n",
      "Epoch [1/4], Step [19350/28190], Loss: 4.0884\n",
      "Epoch [1/4], Step [19425/28190], Loss: 4.2147\n",
      "Epoch [1/4], Step [19500/28190], Loss: 4.2799\n",
      "Epoch [1/4], Step [19575/28190], Loss: 4.2641\n",
      "Epoch [1/4], Step [19650/28190], Loss: 4.3514\n",
      "Epoch [1/4], Step [19725/28190], Loss: 4.0253\n",
      "Epoch [1/4], Step [19800/28190], Loss: 4.1891\n",
      "Epoch [1/4], Step [19875/28190], Loss: 4.2679\n",
      "Epoch [1/4], Step [19950/28190], Loss: 4.2423\n",
      "Epoch [1/4], Step [20025/28190], Loss: 4.2132\n",
      "Epoch [1/4], Step [20100/28190], Loss: 4.3450\n",
      "Epoch [1/4], Step [20175/28190], Loss: 4.2409\n",
      "Epoch [1/4], Step [20250/28190], Loss: 4.2190\n",
      "Epoch [1/4], Step [20325/28190], Loss: 4.2719\n",
      "Epoch [1/4], Step [20400/28190], Loss: 4.2571\n",
      "Epoch [1/4], Step [20475/28190], Loss: 4.0484\n",
      "Epoch [1/4], Step [20550/28190], Loss: 4.1871\n",
      "Epoch [1/4], Step [20625/28190], Loss: 4.1561\n",
      "Epoch [1/4], Step [20700/28190], Loss: 3.9983\n",
      "Epoch [1/4], Step [20775/28190], Loss: 4.2355\n",
      "Epoch [1/4], Step [20850/28190], Loss: 4.0819\n",
      "Epoch [1/4], Step [20925/28190], Loss: 4.1466\n",
      "Epoch [1/4], Step [21000/28190], Loss: 4.2663\n",
      "Validation perplexity: 52.65644244777446\n",
      "Epoch [1/4], Step [21075/28190], Loss: 4.1023\n",
      "Epoch [1/4], Step [21150/28190], Loss: 4.3142\n",
      "Epoch [1/4], Step [21225/28190], Loss: 4.2563\n",
      "Epoch [1/4], Step [21300/28190], Loss: 4.1962\n",
      "Epoch [1/4], Step [21375/28190], Loss: 4.1027\n",
      "Epoch [1/4], Step [21450/28190], Loss: 4.3213\n",
      "Epoch [1/4], Step [21525/28190], Loss: 4.2239\n",
      "Epoch [1/4], Step [21600/28190], Loss: 4.0831\n",
      "Epoch [1/4], Step [21675/28190], Loss: 4.1966\n",
      "Epoch [1/4], Step [21750/28190], Loss: 3.9797\n",
      "Epoch [1/4], Step [21825/28190], Loss: 4.1189\n",
      "Epoch [1/4], Step [21900/28190], Loss: 4.2978\n",
      "Epoch [1/4], Step [21975/28190], Loss: 4.3344\n",
      "Epoch [1/4], Step [22050/28190], Loss: 4.2250\n",
      "Epoch [1/4], Step [22125/28190], Loss: 4.1981\n",
      "Epoch [1/4], Step [22200/28190], Loss: 4.2532\n",
      "Epoch [1/4], Step [22275/28190], Loss: 4.2612\n",
      "Epoch [1/4], Step [22350/28190], Loss: 4.2176\n",
      "Epoch [1/4], Step [22425/28190], Loss: 4.3186\n",
      "Epoch [1/4], Step [22500/28190], Loss: 4.0766\n",
      "Epoch [1/4], Step [22575/28190], Loss: 4.2302\n",
      "Epoch [1/4], Step [22650/28190], Loss: 4.2622\n",
      "Epoch [1/4], Step [22725/28190], Loss: 4.2253\n",
      "Epoch [1/4], Step [22800/28190], Loss: 4.2534\n",
      "Epoch [1/4], Step [22875/28190], Loss: 4.3417\n",
      "Epoch [1/4], Step [22950/28190], Loss: 4.3449\n",
      "Epoch [1/4], Step [23025/28190], Loss: 4.4790\n",
      "Epoch [1/4], Step [23100/28190], Loss: 4.1443\n",
      "Epoch [1/4], Step [23175/28190], Loss: 4.1948\n",
      "Epoch [1/4], Step [23250/28190], Loss: 4.0874\n",
      "Epoch [1/4], Step [23325/28190], Loss: 3.9063\n",
      "Epoch [1/4], Step [23400/28190], Loss: 4.0654\n",
      "Epoch [1/4], Step [23475/28190], Loss: 4.1065\n",
      "Epoch [1/4], Step [23550/28190], Loss: 4.4890\n",
      "Epoch [1/4], Step [23625/28190], Loss: 4.2562\n",
      "Epoch [1/4], Step [23700/28190], Loss: 4.3797\n",
      "Epoch [1/4], Step [23775/28190], Loss: 4.1963\n",
      "Epoch [1/4], Step [23850/28190], Loss: 3.9842\n",
      "Epoch [1/4], Step [23925/28190], Loss: 4.0385\n",
      "Epoch [1/4], Step [24000/28190], Loss: 4.0504\n",
      "Validation perplexity: 50.93333223491253\n",
      "Epoch [1/4], Step [24075/28190], Loss: 4.1582\n",
      "Epoch [1/4], Step [24150/28190], Loss: 4.1364\n",
      "Epoch [1/4], Step [24225/28190], Loss: 4.3321\n",
      "Epoch [1/4], Step [24300/28190], Loss: 4.2999\n",
      "Epoch [1/4], Step [24375/28190], Loss: 4.2847\n",
      "Epoch [1/4], Step [24450/28190], Loss: 4.3272\n",
      "Epoch [1/4], Step [24525/28190], Loss: 4.2610\n",
      "Epoch [1/4], Step [24600/28190], Loss: 4.0367\n",
      "Epoch [1/4], Step [24675/28190], Loss: 4.2983\n",
      "Epoch [1/4], Step [24750/28190], Loss: 4.1446\n",
      "Epoch [1/4], Step [24825/28190], Loss: 4.2935\n",
      "Epoch [1/4], Step [24900/28190], Loss: 4.2055\n",
      "Epoch [1/4], Step [24975/28190], Loss: 4.0677\n",
      "Epoch [1/4], Step [25050/28190], Loss: 4.0260\n",
      "Epoch [1/4], Step [25125/28190], Loss: 3.9654\n",
      "Epoch [1/4], Step [25200/28190], Loss: 4.0853\n",
      "Epoch [1/4], Step [25275/28190], Loss: 4.1182\n",
      "Epoch [1/4], Step [25350/28190], Loss: 4.4677\n",
      "Epoch [1/4], Step [25425/28190], Loss: 4.2553\n",
      "Epoch [1/4], Step [25500/28190], Loss: 4.2527\n",
      "Epoch [1/4], Step [25575/28190], Loss: 4.2500\n",
      "Epoch [1/4], Step [25650/28190], Loss: 3.9902\n",
      "Epoch [1/4], Step [25725/28190], Loss: 4.1459\n",
      "Epoch [1/4], Step [25800/28190], Loss: 4.3662\n",
      "Epoch [1/4], Step [25875/28190], Loss: 4.2889\n",
      "Epoch [1/4], Step [25950/28190], Loss: 4.1706\n",
      "Epoch [1/4], Step [26025/28190], Loss: 4.2403\n",
      "Epoch [1/4], Step [26100/28190], Loss: 4.1138\n",
      "Epoch [1/4], Step [26175/28190], Loss: 4.1260\n",
      "Epoch [1/4], Step [26250/28190], Loss: 4.2966\n",
      "Epoch [1/4], Step [26325/28190], Loss: 4.2407\n",
      "Epoch [1/4], Step [26400/28190], Loss: 4.2261\n",
      "Epoch [1/4], Step [26475/28190], Loss: 4.2612\n",
      "Epoch [1/4], Step [26550/28190], Loss: 4.2790\n",
      "Epoch [1/4], Step [26625/28190], Loss: 4.2520\n",
      "Epoch [1/4], Step [26700/28190], Loss: 4.1951\n",
      "Epoch [1/4], Step [26775/28190], Loss: 4.1003\n",
      "Epoch [1/4], Step [26850/28190], Loss: 4.2101\n",
      "Epoch [1/4], Step [26925/28190], Loss: 3.9982\n",
      "Epoch [1/4], Step [27000/28190], Loss: 4.2060\n",
      "Validation perplexity: 49.05779629347394\n",
      "Epoch [1/4], Step [27075/28190], Loss: 4.1489\n",
      "Epoch [1/4], Step [27150/28190], Loss: 4.0707\n",
      "Epoch [1/4], Step [27225/28190], Loss: 4.3046\n",
      "Epoch [1/4], Step [27300/28190], Loss: 4.3293\n",
      "Epoch [1/4], Step [27375/28190], Loss: 4.2317\n",
      "Epoch [1/4], Step [27450/28190], Loss: 4.0646\n",
      "Epoch [1/4], Step [27525/28190], Loss: 4.1854\n",
      "Epoch [1/4], Step [27600/28190], Loss: 4.1969\n",
      "Epoch [1/4], Step [27675/28190], Loss: 3.9997\n",
      "Epoch [1/4], Step [27750/28190], Loss: 4.0685\n",
      "Epoch [1/4], Step [27825/28190], Loss: 4.2569\n",
      "Epoch [1/4], Step [27900/28190], Loss: 4.1617\n",
      "Epoch [1/4], Step [27975/28190], Loss: 4.2385\n",
      "Epoch [1/4], Step [28050/28190], Loss: 4.2381\n",
      "Epoch [1/4], Step [28125/28190], Loss: 4.3439\n",
      "Epoch [1/4] Average Loss: 4.5062, Perplexity: 90.58\n",
      "Epoch [2/4], Step [0/28190], Loss: 4.2068\n",
      "Validation perplexity: 48.561656356108855\n",
      "Epoch [2/4], Step [75/28190], Loss: 4.2693\n",
      "Epoch [2/4], Step [150/28190], Loss: 4.1209\n",
      "Epoch [2/4], Step [225/28190], Loss: 4.2024\n",
      "Epoch [2/4], Step [300/28190], Loss: 4.1930\n",
      "Epoch [2/4], Step [375/28190], Loss: 4.1046\n",
      "Epoch [2/4], Step [450/28190], Loss: 3.9438\n",
      "Epoch [2/4], Step [525/28190], Loss: 4.1954\n",
      "Epoch [2/4], Step [600/28190], Loss: 4.1414\n",
      "Epoch [2/4], Step [675/28190], Loss: 3.9658\n",
      "Epoch [2/4], Step [750/28190], Loss: 4.1847\n",
      "Epoch [2/4], Step [825/28190], Loss: 3.9925\n",
      "Epoch [2/4], Step [900/28190], Loss: 4.1892\n",
      "Epoch [2/4], Step [975/28190], Loss: 3.9319\n",
      "Epoch [2/4], Step [1050/28190], Loss: 4.1250\n",
      "Epoch [2/4], Step [1125/28190], Loss: 4.1371\n",
      "Epoch [2/4], Step [1200/28190], Loss: 4.2523\n",
      "Epoch [2/4], Step [1275/28190], Loss: 4.0777\n",
      "Epoch [2/4], Step [1350/28190], Loss: 4.3192\n",
      "Epoch [2/4], Step [1425/28190], Loss: 4.2779\n",
      "Epoch [2/4], Step [1500/28190], Loss: 4.1441\n",
      "Epoch [2/4], Step [1575/28190], Loss: 4.1785\n",
      "Epoch [2/4], Step [1650/28190], Loss: 4.3166\n",
      "Epoch [2/4], Step [1725/28190], Loss: 4.1402\n",
      "Epoch [2/4], Step [1800/28190], Loss: 4.1244\n",
      "Epoch [2/4], Step [1875/28190], Loss: 4.0840\n",
      "Epoch [2/4], Step [1950/28190], Loss: 3.9529\n",
      "Epoch [2/4], Step [2025/28190], Loss: 4.1414\n",
      "Epoch [2/4], Step [2100/28190], Loss: 3.8558\n",
      "Epoch [2/4], Step [2175/28190], Loss: 4.0237\n",
      "Epoch [2/4], Step [2250/28190], Loss: 4.2443\n",
      "Epoch [2/4], Step [2325/28190], Loss: 4.1333\n",
      "Epoch [2/4], Step [2400/28190], Loss: 3.9444\n",
      "Epoch [2/4], Step [2475/28190], Loss: 4.1786\n",
      "Epoch [2/4], Step [2550/28190], Loss: 4.0491\n",
      "Epoch [2/4], Step [2625/28190], Loss: 4.1315\n",
      "Epoch [2/4], Step [2700/28190], Loss: 4.0829\n",
      "Epoch [2/4], Step [2775/28190], Loss: 4.0480\n",
      "Epoch [2/4], Step [2850/28190], Loss: 3.9361\n",
      "Epoch [2/4], Step [2925/28190], Loss: 4.0882\n",
      "Epoch [2/4], Step [3000/28190], Loss: 4.0464\n",
      "Validation perplexity: 47.301441078904645\n",
      "Epoch [2/4], Step [3075/28190], Loss: 4.0899\n",
      "Epoch [2/4], Step [3150/28190], Loss: 4.0798\n",
      "Epoch [2/4], Step [3225/28190], Loss: 4.1812\n",
      "Epoch [2/4], Step [3300/28190], Loss: 4.1411\n",
      "Epoch [2/4], Step [3375/28190], Loss: 4.1072\n",
      "Epoch [2/4], Step [3450/28190], Loss: 4.1512\n",
      "Epoch [2/4], Step [3525/28190], Loss: 4.2635\n",
      "Epoch [2/4], Step [3600/28190], Loss: 4.0443\n",
      "Epoch [2/4], Step [3675/28190], Loss: 4.2205\n",
      "Epoch [2/4], Step [3750/28190], Loss: 4.1247\n",
      "Epoch [2/4], Step [3825/28190], Loss: 4.1674\n",
      "Epoch [2/4], Step [3900/28190], Loss: 4.0241\n",
      "Epoch [2/4], Step [3975/28190], Loss: 4.1530\n",
      "Epoch [2/4], Step [4050/28190], Loss: 4.0652\n",
      "Epoch [2/4], Step [4125/28190], Loss: 3.9837\n",
      "Epoch [2/4], Step [4200/28190], Loss: 3.9863\n",
      "Epoch [2/4], Step [4275/28190], Loss: 4.1172\n",
      "Epoch [2/4], Step [4350/28190], Loss: 4.2387\n",
      "Epoch [2/4], Step [4425/28190], Loss: 4.1376\n",
      "Epoch [2/4], Step [4500/28190], Loss: 4.0937\n",
      "Epoch [2/4], Step [4575/28190], Loss: 3.9855\n",
      "Epoch [2/4], Step [4650/28190], Loss: 4.0736\n",
      "Epoch [2/4], Step [4725/28190], Loss: 4.1721\n",
      "Epoch [2/4], Step [4800/28190], Loss: 4.0210\n",
      "Epoch [2/4], Step [4875/28190], Loss: 4.1871\n",
      "Epoch [2/4], Step [4950/28190], Loss: 4.2837\n",
      "Epoch [2/4], Step [5025/28190], Loss: 4.1617\n",
      "Epoch [2/4], Step [5100/28190], Loss: 4.0016\n",
      "Epoch [2/4], Step [5175/28190], Loss: 4.2146\n",
      "Epoch [2/4], Step [5250/28190], Loss: 4.0692\n",
      "Epoch [2/4], Step [5325/28190], Loss: 3.9505\n",
      "Epoch [2/4], Step [5400/28190], Loss: 4.0739\n",
      "Epoch [2/4], Step [5475/28190], Loss: 3.9985\n",
      "Epoch [2/4], Step [5550/28190], Loss: 4.1187\n",
      "Epoch [2/4], Step [5625/28190], Loss: 4.1182\n",
      "Epoch [2/4], Step [5700/28190], Loss: 4.1106\n",
      "Epoch [2/4], Step [5775/28190], Loss: 4.0831\n",
      "Epoch [2/4], Step [5850/28190], Loss: 4.0305\n",
      "Epoch [2/4], Step [5925/28190], Loss: 4.1270\n",
      "Epoch [2/4], Step [6000/28190], Loss: 3.9732\n",
      "Validation perplexity: 46.03253346626691\n",
      "Epoch [2/4], Step [6075/28190], Loss: 4.2674\n",
      "Epoch [2/4], Step [6150/28190], Loss: 4.1701\n",
      "Epoch [2/4], Step [6225/28190], Loss: 4.0471\n",
      "Epoch [2/4], Step [6300/28190], Loss: 3.9614\n",
      "Epoch [2/4], Step [6375/28190], Loss: 4.2269\n",
      "Epoch [2/4], Step [6450/28190], Loss: 4.1505\n",
      "Epoch [2/4], Step [6525/28190], Loss: 3.9992\n",
      "Epoch [2/4], Step [6600/28190], Loss: 4.1432\n",
      "Epoch [2/4], Step [6675/28190], Loss: 3.9214\n",
      "Epoch [2/4], Step [6750/28190], Loss: 4.0773\n",
      "Epoch [2/4], Step [6825/28190], Loss: 3.9638\n",
      "Epoch [2/4], Step [6900/28190], Loss: 4.0933\n",
      "Epoch [2/4], Step [6975/28190], Loss: 3.9644\n",
      "Epoch [2/4], Step [7050/28190], Loss: 4.1137\n",
      "Epoch [2/4], Step [7125/28190], Loss: 3.9287\n",
      "Epoch [2/4], Step [7200/28190], Loss: 4.0792\n",
      "Epoch [2/4], Step [7275/28190], Loss: 4.1870\n",
      "Epoch [2/4], Step [7350/28190], Loss: 4.0672\n",
      "Epoch [2/4], Step [7425/28190], Loss: 3.9279\n",
      "Epoch [2/4], Step [7500/28190], Loss: 4.2841\n",
      "Epoch [2/4], Step [7575/28190], Loss: 4.2688\n",
      "Epoch [2/4], Step [7650/28190], Loss: 4.0925\n",
      "Epoch [2/4], Step [7725/28190], Loss: 4.0631\n",
      "Epoch [2/4], Step [7800/28190], Loss: 4.1451\n",
      "Epoch [2/4], Step [7875/28190], Loss: 4.1342\n",
      "Epoch [2/4], Step [7950/28190], Loss: 3.9947\n",
      "Epoch [2/4], Step [8025/28190], Loss: 3.8811\n",
      "Epoch [2/4], Step [8100/28190], Loss: 4.2772\n",
      "Epoch [2/4], Step [8175/28190], Loss: 4.0881\n",
      "Epoch [2/4], Step [8250/28190], Loss: 3.9241\n",
      "Epoch [2/4], Step [8325/28190], Loss: 4.0662\n",
      "Epoch [2/4], Step [8400/28190], Loss: 4.0119\n",
      "Epoch [2/4], Step [8475/28190], Loss: 4.0837\n",
      "Epoch [2/4], Step [8550/28190], Loss: 4.1608\n",
      "Epoch [2/4], Step [8625/28190], Loss: 4.1362\n",
      "Epoch [2/4], Step [8700/28190], Loss: 4.1578\n",
      "Epoch [2/4], Step [8775/28190], Loss: 4.2102\n",
      "Epoch [2/4], Step [8850/28190], Loss: 4.2050\n",
      "Epoch [2/4], Step [8925/28190], Loss: 4.1241\n",
      "Epoch [2/4], Step [9000/28190], Loss: 3.9786\n",
      "Validation perplexity: 45.17146118540088\n",
      "Epoch [2/4], Step [9075/28190], Loss: 3.8869\n",
      "Epoch [2/4], Step [9150/28190], Loss: 4.2486\n",
      "Epoch [2/4], Step [9225/28190], Loss: 4.0346\n",
      "Epoch [2/4], Step [9300/28190], Loss: 3.9519\n",
      "Epoch [2/4], Step [9375/28190], Loss: 4.0903\n",
      "Epoch [2/4], Step [9450/28190], Loss: 4.0518\n",
      "Epoch [2/4], Step [9525/28190], Loss: 3.9457\n",
      "Epoch [2/4], Step [9600/28190], Loss: 4.1153\n",
      "Epoch [2/4], Step [9675/28190], Loss: 4.2531\n",
      "Epoch [2/4], Step [9750/28190], Loss: 4.0612\n",
      "Epoch [2/4], Step [9825/28190], Loss: 3.9463\n",
      "Epoch [2/4], Step [9900/28190], Loss: 3.9022\n",
      "Epoch [2/4], Step [9975/28190], Loss: 4.0913\n",
      "Epoch [2/4], Step [10050/28190], Loss: 4.1106\n",
      "Epoch [2/4], Step [10125/28190], Loss: 4.0101\n",
      "Epoch [2/4], Step [10200/28190], Loss: 3.9612\n",
      "Epoch [2/4], Step [10275/28190], Loss: 4.1584\n",
      "Epoch [2/4], Step [10350/28190], Loss: 4.1052\n",
      "Epoch [2/4], Step [10425/28190], Loss: 3.9298\n",
      "Epoch [2/4], Step [10500/28190], Loss: 4.1140\n",
      "Epoch [2/4], Step [10575/28190], Loss: 4.1060\n",
      "Epoch [2/4], Step [10650/28190], Loss: 4.1347\n",
      "Epoch [2/4], Step [10725/28190], Loss: 4.0433\n",
      "Epoch [2/4], Step [10800/28190], Loss: 4.1602\n",
      "Epoch [2/4], Step [10875/28190], Loss: 4.0741\n",
      "Epoch [2/4], Step [10950/28190], Loss: 4.0819\n",
      "Epoch [2/4], Step [11025/28190], Loss: 3.9684\n",
      "Epoch [2/4], Step [11100/28190], Loss: 4.0564\n",
      "Epoch [2/4], Step [11175/28190], Loss: 4.0597\n",
      "Epoch [2/4], Step [11250/28190], Loss: 3.9194\n",
      "Epoch [2/4], Step [11325/28190], Loss: 4.1675\n",
      "Epoch [2/4], Step [11400/28190], Loss: 4.2483\n",
      "Epoch [2/4], Step [11475/28190], Loss: 4.0191\n",
      "Epoch [2/4], Step [11550/28190], Loss: 3.9683\n",
      "Epoch [2/4], Step [11625/28190], Loss: 4.1540\n",
      "Epoch [2/4], Step [11700/28190], Loss: 4.1376\n",
      "Epoch [2/4], Step [11775/28190], Loss: 4.1303\n",
      "Epoch [2/4], Step [11850/28190], Loss: 3.9674\n",
      "Epoch [2/4], Step [11925/28190], Loss: 4.1366\n",
      "Epoch [2/4], Step [12000/28190], Loss: 4.0473\n",
      "Validation perplexity: 44.13764149708964\n",
      "Epoch [2/4], Step [12075/28190], Loss: 4.0070\n",
      "Epoch [2/4], Step [12150/28190], Loss: 3.9857\n",
      "Epoch [2/4], Step [12225/28190], Loss: 4.1317\n",
      "Epoch [2/4], Step [12300/28190], Loss: 4.0599\n",
      "Epoch [2/4], Step [12375/28190], Loss: 4.2211\n",
      "Epoch [2/4], Step [12450/28190], Loss: 4.1093\n",
      "Epoch [2/4], Step [12525/28190], Loss: 3.9896\n",
      "Epoch [2/4], Step [12600/28190], Loss: 4.0248\n",
      "Epoch [2/4], Step [12675/28190], Loss: 4.0627\n",
      "Epoch [2/4], Step [12750/28190], Loss: 4.0881\n",
      "Epoch [2/4], Step [12825/28190], Loss: 4.0886\n",
      "Epoch [2/4], Step [12900/28190], Loss: 4.0925\n",
      "Epoch [2/4], Step [12975/28190], Loss: 4.0163\n",
      "Epoch [2/4], Step [13050/28190], Loss: 4.1429\n",
      "Epoch [2/4], Step [13125/28190], Loss: 4.0410\n",
      "Epoch [2/4], Step [13200/28190], Loss: 4.0593\n",
      "Epoch [2/4], Step [13275/28190], Loss: 4.0616\n",
      "Epoch [2/4], Step [13350/28190], Loss: 4.1214\n",
      "Epoch [2/4], Step [13425/28190], Loss: 4.0361\n",
      "Epoch [2/4], Step [13500/28190], Loss: 4.0655\n",
      "Epoch [2/4], Step [13575/28190], Loss: 4.0996\n",
      "Epoch [2/4], Step [13650/28190], Loss: 3.9567\n",
      "Epoch [2/4], Step [13725/28190], Loss: 4.0755\n",
      "Epoch [2/4], Step [13800/28190], Loss: 4.0502\n",
      "Epoch [2/4], Step [13875/28190], Loss: 4.0430\n",
      "Epoch [2/4], Step [13950/28190], Loss: 3.9695\n",
      "Epoch [2/4], Step [14025/28190], Loss: 4.0334\n",
      "Epoch [2/4], Step [14100/28190], Loss: 4.0675\n",
      "Epoch [2/4], Step [14175/28190], Loss: 4.0332\n",
      "Epoch [2/4], Step [14250/28190], Loss: 4.0674\n",
      "Epoch [2/4], Step [14325/28190], Loss: 3.9613\n",
      "Epoch [2/4], Step [14400/28190], Loss: 3.9881\n",
      "Epoch [2/4], Step [14475/28190], Loss: 3.8670\n",
      "Epoch [2/4], Step [14550/28190], Loss: 4.1536\n",
      "Epoch [2/4], Step [14625/28190], Loss: 3.9799\n",
      "Epoch [2/4], Step [14700/28190], Loss: 4.0032\n",
      "Epoch [2/4], Step [14775/28190], Loss: 3.9022\n",
      "Epoch [2/4], Step [14850/28190], Loss: 3.9685\n",
      "Epoch [2/4], Step [14925/28190], Loss: 3.8338\n",
      "Epoch [2/4], Step [15000/28190], Loss: 3.9982\n",
      "Validation perplexity: 43.61893303865911\n",
      "Epoch [2/4], Step [15075/28190], Loss: 4.1666\n",
      "Epoch [2/4], Step [15150/28190], Loss: 4.0057\n",
      "Epoch [2/4], Step [15225/28190], Loss: 3.9274\n",
      "Epoch [2/4], Step [15300/28190], Loss: 4.1879\n",
      "Epoch [2/4], Step [15375/28190], Loss: 3.9255\n",
      "Epoch [2/4], Step [15450/28190], Loss: 4.0504\n",
      "Epoch [2/4], Step [15525/28190], Loss: 3.9494\n",
      "Epoch [2/4], Step [15600/28190], Loss: 4.1183\n",
      "Epoch [2/4], Step [15675/28190], Loss: 3.9621\n",
      "Epoch [2/4], Step [15750/28190], Loss: 4.0489\n",
      "Epoch [2/4], Step [15825/28190], Loss: 4.0831\n",
      "Epoch [2/4], Step [15900/28190], Loss: 3.9547\n",
      "Epoch [2/4], Step [15975/28190], Loss: 4.1652\n",
      "Epoch [2/4], Step [16050/28190], Loss: 4.0714\n",
      "Epoch [2/4], Step [16125/28190], Loss: 4.1189\n",
      "Epoch [2/4], Step [16200/28190], Loss: 4.0152\n",
      "Epoch [2/4], Step [16275/28190], Loss: 4.1196\n",
      "Epoch [2/4], Step [16350/28190], Loss: 4.0646\n",
      "Epoch [2/4], Step [16425/28190], Loss: 4.0076\n",
      "Epoch [2/4], Step [16500/28190], Loss: 3.9718\n",
      "Epoch [2/4], Step [16575/28190], Loss: 4.0062\n",
      "Epoch [2/4], Step [16650/28190], Loss: 4.0421\n",
      "Epoch [2/4], Step [16725/28190], Loss: 3.7933\n",
      "Epoch [2/4], Step [16800/28190], Loss: 4.1115\n",
      "Epoch [2/4], Step [16875/28190], Loss: 3.9780\n",
      "Epoch [2/4], Step [16950/28190], Loss: 4.2821\n",
      "Epoch [2/4], Step [17025/28190], Loss: 3.9636\n",
      "Epoch [2/4], Step [17100/28190], Loss: 4.0168\n",
      "Epoch [2/4], Step [17175/28190], Loss: 4.0358\n",
      "Epoch [2/4], Step [17250/28190], Loss: 4.0708\n",
      "Epoch [2/4], Step [17325/28190], Loss: 4.1107\n",
      "Epoch [2/4], Step [17400/28190], Loss: 3.9944\n",
      "Epoch [2/4], Step [17475/28190], Loss: 3.9877\n",
      "Epoch [2/4], Step [17550/28190], Loss: 4.1044\n",
      "Epoch [2/4], Step [17625/28190], Loss: 4.1429\n",
      "Epoch [2/4], Step [17700/28190], Loss: 4.1321\n",
      "Epoch [2/4], Step [17775/28190], Loss: 4.0769\n",
      "Epoch [2/4], Step [17850/28190], Loss: 3.8482\n",
      "Epoch [2/4], Step [17925/28190], Loss: 4.0048\n",
      "Epoch [2/4], Step [18000/28190], Loss: 4.0338\n",
      "Validation perplexity: 42.91286016460998\n",
      "Epoch [2/4], Step [18075/28190], Loss: 3.8550\n",
      "Epoch [2/4], Step [18150/28190], Loss: 3.8891\n",
      "Epoch [2/4], Step [18225/28190], Loss: 4.0399\n",
      "Epoch [2/4], Step [18300/28190], Loss: 3.9322\n",
      "Epoch [2/4], Step [18375/28190], Loss: 4.0655\n",
      "Epoch [2/4], Step [18450/28190], Loss: 4.2465\n",
      "Epoch [2/4], Step [18525/28190], Loss: 4.1779\n",
      "Epoch [2/4], Step [18600/28190], Loss: 3.9988\n",
      "Epoch [2/4], Step [18675/28190], Loss: 4.0494\n",
      "Epoch [2/4], Step [18750/28190], Loss: 3.9491\n",
      "Epoch [2/4], Step [18825/28190], Loss: 3.9740\n",
      "Epoch [2/4], Step [18900/28190], Loss: 4.0135\n",
      "Epoch [2/4], Step [18975/28190], Loss: 4.0835\n",
      "Epoch [2/4], Step [19050/28190], Loss: 3.9319\n",
      "Epoch [2/4], Step [19125/28190], Loss: 3.9902\n",
      "Epoch [2/4], Step [19200/28190], Loss: 3.9118\n",
      "Epoch [2/4], Step [19275/28190], Loss: 4.0439\n",
      "Epoch [2/4], Step [19350/28190], Loss: 3.9771\n",
      "Epoch [2/4], Step [19425/28190], Loss: 4.0823\n",
      "Epoch [2/4], Step [19500/28190], Loss: 4.0571\n",
      "Epoch [2/4], Step [19575/28190], Loss: 4.0126\n",
      "Epoch [2/4], Step [19650/28190], Loss: 3.8369\n",
      "Epoch [2/4], Step [19725/28190], Loss: 3.9338\n",
      "Epoch [2/4], Step [19800/28190], Loss: 3.9970\n",
      "Epoch [2/4], Step [19875/28190], Loss: 4.0586\n",
      "Epoch [2/4], Step [19950/28190], Loss: 4.1287\n",
      "Epoch [2/4], Step [20025/28190], Loss: 4.0341\n",
      "Epoch [2/4], Step [20100/28190], Loss: 3.8872\n",
      "Epoch [2/4], Step [20175/28190], Loss: 4.0371\n",
      "Epoch [2/4], Step [20250/28190], Loss: 4.0835\n",
      "Epoch [2/4], Step [20325/28190], Loss: 4.0410\n",
      "Epoch [2/4], Step [20400/28190], Loss: 3.8205\n",
      "Epoch [2/4], Step [20475/28190], Loss: 3.9407\n",
      "Epoch [2/4], Step [20550/28190], Loss: 4.0753\n",
      "Epoch [2/4], Step [20625/28190], Loss: 3.9453\n",
      "Epoch [2/4], Step [20700/28190], Loss: 3.9944\n",
      "Epoch [2/4], Step [20775/28190], Loss: 4.0255\n",
      "Epoch [2/4], Step [20850/28190], Loss: 3.9640\n",
      "Epoch [2/4], Step [20925/28190], Loss: 4.0890\n",
      "Epoch [2/4], Step [21000/28190], Loss: 3.8511\n",
      "Validation perplexity: 42.239060650999896\n",
      "Epoch [2/4], Step [21075/28190], Loss: 4.1807\n",
      "Epoch [2/4], Step [21150/28190], Loss: 4.2305\n",
      "Epoch [2/4], Step [21225/28190], Loss: 4.2725\n",
      "Epoch [2/4], Step [21300/28190], Loss: 4.1301\n",
      "Epoch [2/4], Step [21375/28190], Loss: 4.1930\n",
      "Epoch [2/4], Step [21450/28190], Loss: 3.9229\n",
      "Epoch [2/4], Step [21525/28190], Loss: 4.1883\n",
      "Epoch [2/4], Step [21600/28190], Loss: 4.0612\n",
      "Epoch [2/4], Step [21675/28190], Loss: 4.1186\n",
      "Epoch [2/4], Step [21750/28190], Loss: 3.9716\n",
      "Epoch [2/4], Step [21825/28190], Loss: 4.1873\n",
      "Epoch [2/4], Step [21900/28190], Loss: 3.9472\n",
      "Epoch [2/4], Step [21975/28190], Loss: 4.0339\n",
      "Epoch [2/4], Step [22050/28190], Loss: 3.9059\n",
      "Epoch [2/4], Step [22125/28190], Loss: 3.9185\n",
      "Epoch [2/4], Step [22200/28190], Loss: 4.0529\n",
      "Epoch [2/4], Step [22275/28190], Loss: 4.1821\n",
      "Epoch [2/4], Step [22350/28190], Loss: 3.9302\n",
      "Epoch [2/4], Step [22425/28190], Loss: 4.0829\n",
      "Epoch [2/4], Step [22500/28190], Loss: 3.9786\n",
      "Epoch [2/4], Step [22575/28190], Loss: 3.9087\n",
      "Epoch [2/4], Step [22650/28190], Loss: 4.0516\n",
      "Epoch [2/4], Step [22725/28190], Loss: 4.0130\n",
      "Epoch [2/4], Step [22800/28190], Loss: 3.8891\n",
      "Epoch [2/4], Step [22875/28190], Loss: 3.8723\n",
      "Epoch [2/4], Step [22950/28190], Loss: 3.9085\n",
      "Epoch [2/4], Step [23025/28190], Loss: 4.0720\n",
      "Epoch [2/4], Step [23100/28190], Loss: 4.0428\n",
      "Epoch [2/4], Step [23175/28190], Loss: 3.9143\n",
      "Epoch [2/4], Step [23250/28190], Loss: 3.9662\n",
      "Epoch [2/4], Step [23325/28190], Loss: 3.8300\n",
      "Epoch [2/4], Step [23400/28190], Loss: 4.0238\n",
      "Epoch [2/4], Step [23475/28190], Loss: 3.9690\n",
      "Epoch [2/4], Step [23550/28190], Loss: 4.1851\n",
      "Epoch [2/4], Step [23625/28190], Loss: 3.9936\n",
      "Epoch [2/4], Step [23700/28190], Loss: 4.2045\n",
      "Epoch [2/4], Step [23775/28190], Loss: 4.0023\n",
      "Epoch [2/4], Step [23850/28190], Loss: 3.9245\n",
      "Epoch [2/4], Step [23925/28190], Loss: 3.9592\n",
      "Epoch [2/4], Step [24000/28190], Loss: 3.9687\n",
      "Validation perplexity: 41.79473897194408\n",
      "Epoch [2/4], Step [24075/28190], Loss: 3.9962\n",
      "Epoch [2/4], Step [24150/28190], Loss: 4.1927\n",
      "Epoch [2/4], Step [24225/28190], Loss: 3.9636\n",
      "Epoch [2/4], Step [24300/28190], Loss: 4.1217\n",
      "Epoch [2/4], Step [24375/28190], Loss: 3.9796\n",
      "Epoch [2/4], Step [24450/28190], Loss: 3.8344\n",
      "Epoch [2/4], Step [24525/28190], Loss: 3.9654\n",
      "Epoch [2/4], Step [24600/28190], Loss: 4.0702\n",
      "Epoch [2/4], Step [24675/28190], Loss: 4.0162\n",
      "Epoch [2/4], Step [24750/28190], Loss: 4.1433\n",
      "Epoch [2/4], Step [24825/28190], Loss: 3.9536\n",
      "Epoch [2/4], Step [24900/28190], Loss: 3.8842\n",
      "Epoch [2/4], Step [24975/28190], Loss: 3.8635\n",
      "Epoch [2/4], Step [25050/28190], Loss: 4.0240\n",
      "Epoch [2/4], Step [25125/28190], Loss: 3.9913\n",
      "Epoch [2/4], Step [25200/28190], Loss: 3.9465\n",
      "Epoch [2/4], Step [25275/28190], Loss: 3.8864\n",
      "Epoch [2/4], Step [25350/28190], Loss: 3.8788\n",
      "Epoch [2/4], Step [25425/28190], Loss: 4.0062\n",
      "Epoch [2/4], Step [25500/28190], Loss: 4.1107\n",
      "Epoch [2/4], Step [25575/28190], Loss: 4.1596\n",
      "Epoch [2/4], Step [25650/28190], Loss: 3.9808\n",
      "Epoch [2/4], Step [25725/28190], Loss: 3.9638\n",
      "Epoch [2/4], Step [25800/28190], Loss: 4.0902\n",
      "Epoch [2/4], Step [25875/28190], Loss: 3.8615\n",
      "Epoch [2/4], Step [25950/28190], Loss: 3.8814\n",
      "Epoch [2/4], Step [26025/28190], Loss: 4.0080\n",
      "Epoch [2/4], Step [26100/28190], Loss: 3.9809\n",
      "Epoch [2/4], Step [26175/28190], Loss: 3.9746\n",
      "Epoch [2/4], Step [26250/28190], Loss: 3.9417\n",
      "Epoch [2/4], Step [26325/28190], Loss: 3.9029\n",
      "Epoch [2/4], Step [26400/28190], Loss: 4.1224\n",
      "Epoch [2/4], Step [26475/28190], Loss: 3.9638\n",
      "Epoch [2/4], Step [26550/28190], Loss: 4.0236\n",
      "Epoch [2/4], Step [26625/28190], Loss: 3.9790\n",
      "Epoch [2/4], Step [26700/28190], Loss: 3.8958\n",
      "Epoch [2/4], Step [26775/28190], Loss: 3.9454\n",
      "Epoch [2/4], Step [26850/28190], Loss: 3.9566\n",
      "Epoch [2/4], Step [26925/28190], Loss: 3.8911\n",
      "Epoch [2/4], Step [27000/28190], Loss: 4.0967\n",
      "Validation perplexity: 41.19945600043628\n",
      "Epoch [2/4], Step [27075/28190], Loss: 4.0707\n",
      "Epoch [2/4], Step [27150/28190], Loss: 3.8919\n",
      "Epoch [2/4], Step [27225/28190], Loss: 3.9202\n",
      "Epoch [2/4], Step [27300/28190], Loss: 3.8674\n",
      "Epoch [2/4], Step [27375/28190], Loss: 4.1834\n",
      "Epoch [2/4], Step [27450/28190], Loss: 4.0721\n",
      "Epoch [2/4], Step [27525/28190], Loss: 3.8093\n",
      "Epoch [2/4], Step [27600/28190], Loss: 4.1027\n",
      "Epoch [2/4], Step [27675/28190], Loss: 4.1844\n",
      "Epoch [2/4], Step [27750/28190], Loss: 4.0059\n",
      "Epoch [2/4], Step [27825/28190], Loss: 3.9700\n",
      "Epoch [2/4], Step [27900/28190], Loss: 3.9952\n",
      "Epoch [2/4], Step [27975/28190], Loss: 3.8380\n",
      "Epoch [2/4], Step [28050/28190], Loss: 4.0481\n",
      "Epoch [2/4], Step [28125/28190], Loss: 4.1079\n",
      "Epoch [2/4] Average Loss: 4.0521, Perplexity: 57.52\n",
      "Epoch [3/4], Step [0/28190], Loss: 4.2242\n",
      "Validation perplexity: 40.93802730950345\n",
      "Epoch [3/4], Step [75/28190], Loss: 3.9033\n",
      "Epoch [3/4], Step [150/28190], Loss: 4.1361\n",
      "Epoch [3/4], Step [225/28190], Loss: 3.8702\n",
      "Epoch [3/4], Step [300/28190], Loss: 3.9209\n",
      "Epoch [3/4], Step [375/28190], Loss: 3.9476\n",
      "Epoch [3/4], Step [450/28190], Loss: 3.8430\n",
      "Epoch [3/4], Step [525/28190], Loss: 4.0106\n",
      "Epoch [3/4], Step [600/28190], Loss: 3.9960\n",
      "Epoch [3/4], Step [675/28190], Loss: 4.0490\n",
      "Epoch [3/4], Step [750/28190], Loss: 3.9075\n",
      "Epoch [3/4], Step [825/28190], Loss: 3.9071\n",
      "Epoch [3/4], Step [900/28190], Loss: 3.9866\n",
      "Epoch [3/4], Step [975/28190], Loss: 3.9368\n",
      "Epoch [3/4], Step [1050/28190], Loss: 3.9513\n",
      "Epoch [3/4], Step [1125/28190], Loss: 3.9341\n",
      "Epoch [3/4], Step [1200/28190], Loss: 4.0686\n",
      "Epoch [3/4], Step [1275/28190], Loss: 4.0312\n",
      "Epoch [3/4], Step [1350/28190], Loss: 3.8710\n",
      "Epoch [3/4], Step [1425/28190], Loss: 4.0570\n",
      "Epoch [3/4], Step [1500/28190], Loss: 3.9867\n",
      "Epoch [3/4], Step [1575/28190], Loss: 4.0391\n",
      "Epoch [3/4], Step [1650/28190], Loss: 3.9183\n",
      "Epoch [3/4], Step [1725/28190], Loss: 4.1300\n",
      "Epoch [3/4], Step [1800/28190], Loss: 4.0506\n",
      "Epoch [3/4], Step [1875/28190], Loss: 3.9860\n",
      "Epoch [3/4], Step [1950/28190], Loss: 3.7358\n",
      "Epoch [3/4], Step [2025/28190], Loss: 3.8908\n",
      "Epoch [3/4], Step [2100/28190], Loss: 3.8081\n",
      "Epoch [3/4], Step [2175/28190], Loss: 4.0781\n",
      "Epoch [3/4], Step [2250/28190], Loss: 4.1068\n",
      "Epoch [3/4], Step [2325/28190], Loss: 4.0369\n",
      "Epoch [3/4], Step [2400/28190], Loss: 4.0890\n",
      "Epoch [3/4], Step [2475/28190], Loss: 3.8668\n",
      "Epoch [3/4], Step [2550/28190], Loss: 3.9736\n",
      "Epoch [3/4], Step [2625/28190], Loss: 3.9657\n",
      "Epoch [3/4], Step [2700/28190], Loss: 4.0357\n",
      "Epoch [3/4], Step [2775/28190], Loss: 3.8701\n",
      "Epoch [3/4], Step [2850/28190], Loss: 3.9770\n",
      "Epoch [3/4], Step [2925/28190], Loss: 3.9697\n",
      "Epoch [3/4], Step [3000/28190], Loss: 3.9352\n",
      "Validation perplexity: 40.54770672199595\n",
      "Epoch [3/4], Step [3075/28190], Loss: 3.8426\n",
      "Epoch [3/4], Step [3150/28190], Loss: 4.1343\n",
      "Epoch [3/4], Step [3225/28190], Loss: 3.8741\n",
      "Epoch [3/4], Step [3300/28190], Loss: 3.9536\n",
      "Epoch [3/4], Step [3375/28190], Loss: 3.8771\n",
      "Epoch [3/4], Step [3450/28190], Loss: 4.0506\n",
      "Epoch [3/4], Step [3525/28190], Loss: 3.9306\n",
      "Epoch [3/4], Step [3600/28190], Loss: 3.8899\n",
      "Epoch [3/4], Step [3675/28190], Loss: 4.0784\n",
      "Epoch [3/4], Step [3750/28190], Loss: 4.0787\n",
      "Epoch [3/4], Step [3825/28190], Loss: 3.8514\n",
      "Epoch [3/4], Step [3900/28190], Loss: 4.1640\n",
      "Epoch [3/4], Step [3975/28190], Loss: 3.8649\n",
      "Epoch [3/4], Step [4050/28190], Loss: 3.8951\n",
      "Epoch [3/4], Step [4125/28190], Loss: 4.1044\n",
      "Epoch [3/4], Step [4200/28190], Loss: 4.0453\n",
      "Epoch [3/4], Step [4275/28190], Loss: 3.8517\n",
      "Epoch [3/4], Step [4350/28190], Loss: 3.9293\n",
      "Epoch [3/4], Step [4425/28190], Loss: 3.8551\n",
      "Epoch [3/4], Step [4500/28190], Loss: 4.1645\n",
      "Epoch [3/4], Step [4575/28190], Loss: 4.2014\n",
      "Epoch [3/4], Step [4650/28190], Loss: 3.8257\n",
      "Epoch [3/4], Step [4725/28190], Loss: 4.0331\n",
      "Epoch [3/4], Step [4800/28190], Loss: 4.1362\n",
      "Epoch [3/4], Step [4875/28190], Loss: 4.0115\n",
      "Epoch [3/4], Step [4950/28190], Loss: 3.8847\n",
      "Epoch [3/4], Step [5025/28190], Loss: 4.0268\n",
      "Epoch [3/4], Step [5100/28190], Loss: 3.8388\n",
      "Epoch [3/4], Step [5175/28190], Loss: 3.7834\n",
      "Epoch [3/4], Step [5250/28190], Loss: 4.1118\n",
      "Epoch [3/4], Step [5325/28190], Loss: 3.9514\n",
      "Epoch [3/4], Step [5400/28190], Loss: 3.8290\n",
      "Epoch [3/4], Step [5475/28190], Loss: 3.9322\n",
      "Epoch [3/4], Step [5550/28190], Loss: 4.0630\n",
      "Epoch [3/4], Step [5625/28190], Loss: 4.0208\n",
      "Epoch [3/4], Step [5700/28190], Loss: 4.0273\n",
      "Epoch [3/4], Step [5775/28190], Loss: 3.9794\n",
      "Epoch [3/4], Step [5850/28190], Loss: 4.0511\n",
      "Epoch [3/4], Step [5925/28190], Loss: 4.0800\n",
      "Epoch [3/4], Step [6000/28190], Loss: 3.9453\n",
      "Validation perplexity: 40.17393200528875\n",
      "Epoch [3/4], Step [6075/28190], Loss: 3.9515\n",
      "Epoch [3/4], Step [6150/28190], Loss: 3.8918\n",
      "Epoch [3/4], Step [6225/28190], Loss: 3.8993\n",
      "Epoch [3/4], Step [6300/28190], Loss: 4.1385\n",
      "Epoch [3/4], Step [6375/28190], Loss: 3.9190\n",
      "Epoch [3/4], Step [6450/28190], Loss: 4.1966\n",
      "Epoch [3/4], Step [6525/28190], Loss: 3.8651\n",
      "Epoch [3/4], Step [6600/28190], Loss: 4.0023\n",
      "Epoch [3/4], Step [6675/28190], Loss: 3.8342\n",
      "Epoch [3/4], Step [6750/28190], Loss: 4.1108\n",
      "Epoch [3/4], Step [6825/28190], Loss: 4.1332\n",
      "Epoch [3/4], Step [6900/28190], Loss: 3.9507\n",
      "Epoch [3/4], Step [6975/28190], Loss: 4.0420\n",
      "Epoch [3/4], Step [7050/28190], Loss: 4.0030\n",
      "Epoch [3/4], Step [7125/28190], Loss: 3.9813\n",
      "Epoch [3/4], Step [7200/28190], Loss: 4.0042\n",
      "Epoch [3/4], Step [7275/28190], Loss: 3.7905\n",
      "Epoch [3/4], Step [7350/28190], Loss: 3.8881\n",
      "Epoch [3/4], Step [7425/28190], Loss: 3.8480\n",
      "Epoch [3/4], Step [7500/28190], Loss: 3.6716\n",
      "Epoch [3/4], Step [7575/28190], Loss: 3.8312\n",
      "Epoch [3/4], Step [7650/28190], Loss: 3.9178\n",
      "Epoch [3/4], Step [7725/28190], Loss: 3.9726\n",
      "Epoch [3/4], Step [7800/28190], Loss: 4.0049\n",
      "Epoch [3/4], Step [7875/28190], Loss: 4.0544\n",
      "Epoch [3/4], Step [7950/28190], Loss: 3.9018\n",
      "Epoch [3/4], Step [8025/28190], Loss: 3.9860\n",
      "Epoch [3/4], Step [8100/28190], Loss: 3.9484\n",
      "Epoch [3/4], Step [8175/28190], Loss: 3.9272\n",
      "Epoch [3/4], Step [8250/28190], Loss: 4.0139\n",
      "Epoch [3/4], Step [8325/28190], Loss: 3.9443\n",
      "Epoch [3/4], Step [8400/28190], Loss: 4.0705\n",
      "Epoch [3/4], Step [8475/28190], Loss: 4.0084\n",
      "Epoch [3/4], Step [8550/28190], Loss: 3.8650\n",
      "Epoch [3/4], Step [8625/28190], Loss: 4.0034\n",
      "Epoch [3/4], Step [8700/28190], Loss: 3.9203\n",
      "Epoch [3/4], Step [8775/28190], Loss: 3.8479\n",
      "Epoch [3/4], Step [8850/28190], Loss: 4.0277\n",
      "Epoch [3/4], Step [8925/28190], Loss: 3.9236\n",
      "Epoch [3/4], Step [9000/28190], Loss: 3.9054\n",
      "Validation perplexity: 39.81694283260816\n",
      "Epoch [3/4], Step [9075/28190], Loss: 4.0904\n",
      "Epoch [3/4], Step [9150/28190], Loss: 3.9169\n",
      "Epoch [3/4], Step [9225/28190], Loss: 3.9057\n",
      "Epoch [3/4], Step [9300/28190], Loss: 3.9328\n",
      "Epoch [3/4], Step [9375/28190], Loss: 4.0728\n",
      "Epoch [3/4], Step [9450/28190], Loss: 4.1921\n",
      "Epoch [3/4], Step [9525/28190], Loss: 3.9793\n",
      "Epoch [3/4], Step [9600/28190], Loss: 3.9980\n",
      "Epoch [3/4], Step [9675/28190], Loss: 3.9577\n",
      "Epoch [3/4], Step [9750/28190], Loss: 4.1727\n",
      "Epoch [3/4], Step [9825/28190], Loss: 3.9850\n",
      "Epoch [3/4], Step [9900/28190], Loss: 3.9615\n",
      "Epoch [3/4], Step [9975/28190], Loss: 3.9832\n",
      "Epoch [3/4], Step [10050/28190], Loss: 3.9075\n",
      "Epoch [3/4], Step [10125/28190], Loss: 3.9126\n",
      "Epoch [3/4], Step [10200/28190], Loss: 3.8618\n",
      "Epoch [3/4], Step [10275/28190], Loss: 4.0302\n",
      "Epoch [3/4], Step [10350/28190], Loss: 3.9192\n",
      "Epoch [3/4], Step [10425/28190], Loss: 3.9269\n",
      "Epoch [3/4], Step [10500/28190], Loss: 4.0300\n",
      "Epoch [3/4], Step [10575/28190], Loss: 3.9703\n",
      "Epoch [3/4], Step [10650/28190], Loss: 3.9487\n",
      "Epoch [3/4], Step [10725/28190], Loss: 3.8857\n",
      "Epoch [3/4], Step [10800/28190], Loss: 3.8967\n",
      "Epoch [3/4], Step [10875/28190], Loss: 3.9408\n",
      "Epoch [3/4], Step [10950/28190], Loss: 3.9818\n",
      "Epoch [3/4], Step [11025/28190], Loss: 4.0265\n",
      "Epoch [3/4], Step [11100/28190], Loss: 3.9954\n",
      "Epoch [3/4], Step [11175/28190], Loss: 3.8454\n",
      "Epoch [3/4], Step [11250/28190], Loss: 3.8689\n",
      "Epoch [3/4], Step [11325/28190], Loss: 4.0579\n",
      "Epoch [3/4], Step [11400/28190], Loss: 3.9491\n",
      "Epoch [3/4], Step [11475/28190], Loss: 3.8856\n",
      "Epoch [3/4], Step [11550/28190], Loss: 3.7786\n",
      "Epoch [3/4], Step [11625/28190], Loss: 3.8743\n",
      "Epoch [3/4], Step [11700/28190], Loss: 3.8869\n",
      "Epoch [3/4], Step [11775/28190], Loss: 3.9080\n",
      "Epoch [3/4], Step [11850/28190], Loss: 3.8934\n",
      "Epoch [3/4], Step [11925/28190], Loss: 3.9518\n",
      "Epoch [3/4], Step [12000/28190], Loss: 3.9545\n",
      "Validation perplexity: 39.46912759884822\n",
      "Epoch [3/4], Step [12075/28190], Loss: 3.9548\n",
      "Epoch [3/4], Step [12150/28190], Loss: 4.0046\n",
      "Epoch [3/4], Step [12225/28190], Loss: 4.2638\n",
      "Epoch [3/4], Step [12300/28190], Loss: 3.8517\n",
      "Epoch [3/4], Step [12375/28190], Loss: 4.0210\n",
      "Epoch [3/4], Step [12450/28190], Loss: 3.8896\n",
      "Epoch [3/4], Step [12525/28190], Loss: 3.9790\n",
      "Epoch [3/4], Step [12600/28190], Loss: 3.9212\n",
      "Epoch [3/4], Step [12675/28190], Loss: 4.0018\n",
      "Epoch [3/4], Step [12750/28190], Loss: 3.9406\n",
      "Epoch [3/4], Step [12825/28190], Loss: 4.0944\n",
      "Epoch [3/4], Step [12900/28190], Loss: 3.8469\n",
      "Epoch [3/4], Step [12975/28190], Loss: 4.0381\n",
      "Epoch [3/4], Step [13050/28190], Loss: 3.8215\n",
      "Epoch [3/4], Step [13125/28190], Loss: 3.8975\n",
      "Epoch [3/4], Step [13200/28190], Loss: 4.1404\n",
      "Epoch [3/4], Step [13275/28190], Loss: 3.8414\n",
      "Epoch [3/4], Step [13350/28190], Loss: 3.8177\n",
      "Epoch [3/4], Step [13425/28190], Loss: 3.9219\n",
      "Epoch [3/4], Step [13500/28190], Loss: 3.9502\n",
      "Epoch [3/4], Step [13575/28190], Loss: 3.7960\n",
      "Epoch [3/4], Step [13650/28190], Loss: 4.0318\n",
      "Epoch [3/4], Step [13725/28190], Loss: 3.9625\n",
      "Epoch [3/4], Step [13800/28190], Loss: 4.0511\n",
      "Epoch [3/4], Step [13875/28190], Loss: 4.0598\n",
      "Epoch [3/4], Step [13950/28190], Loss: 3.9569\n",
      "Epoch [3/4], Step [14025/28190], Loss: 3.9861\n",
      "Epoch [3/4], Step [14100/28190], Loss: 4.0124\n",
      "Epoch [3/4], Step [14175/28190], Loss: 3.9447\n",
      "Epoch [3/4], Step [14250/28190], Loss: 4.0979\n",
      "Epoch [3/4], Step [14325/28190], Loss: 3.9878\n",
      "Epoch [3/4], Step [14400/28190], Loss: 3.9216\n",
      "Epoch [3/4], Step [14475/28190], Loss: 3.8986\n",
      "Epoch [3/4], Step [14550/28190], Loss: 3.8534\n",
      "Epoch [3/4], Step [14625/28190], Loss: 3.9827\n",
      "Epoch [3/4], Step [14700/28190], Loss: 3.8953\n",
      "Epoch [3/4], Step [14775/28190], Loss: 3.9323\n",
      "Epoch [3/4], Step [14850/28190], Loss: 4.0002\n",
      "Epoch [3/4], Step [14925/28190], Loss: 3.9851\n",
      "Epoch [3/4], Step [15000/28190], Loss: 3.9091\n",
      "Validation perplexity: 38.84507393844976\n",
      "Epoch [3/4], Step [15075/28190], Loss: 3.8583\n",
      "Epoch [3/4], Step [15150/28190], Loss: 3.9064\n",
      "Epoch [3/4], Step [15225/28190], Loss: 3.9480\n",
      "Epoch [3/4], Step [15300/28190], Loss: 4.0812\n",
      "Epoch [3/4], Step [15375/28190], Loss: 4.0825\n",
      "Epoch [3/4], Step [15450/28190], Loss: 4.0398\n",
      "Epoch [3/4], Step [15525/28190], Loss: 3.9616\n",
      "Epoch [3/4], Step [15600/28190], Loss: 4.0092\n",
      "Epoch [3/4], Step [15675/28190], Loss: 3.8574\n",
      "Epoch [3/4], Step [15750/28190], Loss: 4.0465\n",
      "Epoch [3/4], Step [15825/28190], Loss: 3.7986\n",
      "Epoch [3/4], Step [15900/28190], Loss: 3.9509\n",
      "Epoch [3/4], Step [15975/28190], Loss: 3.8551\n",
      "Epoch [3/4], Step [16050/28190], Loss: 3.9890\n",
      "Epoch [3/4], Step [16125/28190], Loss: 4.0313\n",
      "Epoch [3/4], Step [16200/28190], Loss: 3.9370\n",
      "Epoch [3/4], Step [16275/28190], Loss: 3.9223\n",
      "Epoch [3/4], Step [16350/28190], Loss: 3.9427\n",
      "Epoch [3/4], Step [16425/28190], Loss: 3.7246\n",
      "Epoch [3/4], Step [16500/28190], Loss: 4.0121\n",
      "Epoch [3/4], Step [16575/28190], Loss: 4.1047\n",
      "Epoch [3/4], Step [16650/28190], Loss: 3.9840\n",
      "Epoch [3/4], Step [16725/28190], Loss: 4.0658\n",
      "Epoch [3/4], Step [16800/28190], Loss: 3.9209\n",
      "Epoch [3/4], Step [16875/28190], Loss: 3.9451\n",
      "Epoch [3/4], Step [16950/28190], Loss: 3.9801\n",
      "Epoch [3/4], Step [17025/28190], Loss: 4.0664\n",
      "Epoch [3/4], Step [17100/28190], Loss: 4.0010\n",
      "Epoch [3/4], Step [17175/28190], Loss: 3.8773\n",
      "Epoch [3/4], Step [17250/28190], Loss: 3.8087\n",
      "Epoch [3/4], Step [17325/28190], Loss: 4.1719\n",
      "Epoch [3/4], Step [17400/28190], Loss: 4.0205\n",
      "Epoch [3/4], Step [17475/28190], Loss: 3.9848\n",
      "Epoch [3/4], Step [17550/28190], Loss: 3.9346\n",
      "Epoch [3/4], Step [17625/28190], Loss: 3.9384\n",
      "Epoch [3/4], Step [17700/28190], Loss: 4.1241\n",
      "Epoch [3/4], Step [17775/28190], Loss: 3.8933\n",
      "Epoch [3/4], Step [17850/28190], Loss: 3.8999\n",
      "Epoch [3/4], Step [17925/28190], Loss: 4.0724\n",
      "Epoch [3/4], Step [18000/28190], Loss: 3.8858\n",
      "Validation perplexity: 38.857952206098524\n",
      "Epoch [3/4], Step [18075/28190], Loss: 3.8701\n",
      "Epoch [3/4], Step [18150/28190], Loss: 4.0431\n",
      "Epoch [3/4], Step [18225/28190], Loss: 4.0796\n",
      "Epoch [3/4], Step [18300/28190], Loss: 3.8423\n",
      "Epoch [3/4], Step [18375/28190], Loss: 3.9399\n",
      "Epoch [3/4], Step [18450/28190], Loss: 3.9341\n",
      "Epoch [3/4], Step [18525/28190], Loss: 4.1280\n",
      "Epoch [3/4], Step [18600/28190], Loss: 3.9096\n",
      "Epoch [3/4], Step [18675/28190], Loss: 3.8147\n",
      "Epoch [3/4], Step [18750/28190], Loss: 3.9994\n",
      "Epoch [3/4], Step [18825/28190], Loss: 3.9028\n",
      "Epoch [3/4], Step [18900/28190], Loss: 3.8401\n",
      "Epoch [3/4], Step [18975/28190], Loss: 3.9522\n",
      "Epoch [3/4], Step [19050/28190], Loss: 3.9777\n",
      "Epoch [3/4], Step [19125/28190], Loss: 4.0451\n",
      "Epoch [3/4], Step [19200/28190], Loss: 3.8683\n",
      "Epoch [3/4], Step [19275/28190], Loss: 3.9874\n",
      "Epoch [3/4], Step [19350/28190], Loss: 4.0130\n",
      "Epoch [3/4], Step [19425/28190], Loss: 3.6844\n",
      "Epoch [3/4], Step [19500/28190], Loss: 3.8872\n",
      "Epoch [3/4], Step [19575/28190], Loss: 4.0108\n",
      "Epoch [3/4], Step [19650/28190], Loss: 3.8364\n",
      "Epoch [3/4], Step [19725/28190], Loss: 3.9885\n",
      "Epoch [3/4], Step [19800/28190], Loss: 3.7951\n",
      "Epoch [3/4], Step [19875/28190], Loss: 3.7772\n",
      "Epoch [3/4], Step [19950/28190], Loss: 4.0357\n",
      "Epoch [3/4], Step [20025/28190], Loss: 3.9709\n",
      "Epoch [3/4], Step [20100/28190], Loss: 3.9682\n",
      "Epoch [3/4], Step [20175/28190], Loss: 4.0091\n",
      "Epoch [3/4], Step [20250/28190], Loss: 3.8181\n",
      "Epoch [3/4], Step [20325/28190], Loss: 3.8386\n",
      "Epoch [3/4], Step [20400/28190], Loss: 4.0180\n",
      "Epoch [3/4], Step [20475/28190], Loss: 3.9567\n",
      "Epoch [3/4], Step [20550/28190], Loss: 3.8760\n",
      "Epoch [3/4], Step [20625/28190], Loss: 3.9813\n",
      "Epoch [3/4], Step [20700/28190], Loss: 3.9393\n",
      "Epoch [3/4], Step [20775/28190], Loss: 3.9914\n",
      "Epoch [3/4], Step [20850/28190], Loss: 4.0753\n",
      "Epoch [3/4], Step [20925/28190], Loss: 3.9587\n",
      "Epoch [3/4], Step [21000/28190], Loss: 3.8868\n",
      "Validation perplexity: 38.38769415223145\n",
      "Epoch [3/4], Step [21075/28190], Loss: 3.7386\n",
      "Epoch [3/4], Step [21150/28190], Loss: 3.9589\n",
      "Epoch [3/4], Step [21225/28190], Loss: 3.9533\n",
      "Epoch [3/4], Step [21300/28190], Loss: 3.8122\n",
      "Epoch [3/4], Step [21375/28190], Loss: 4.0331\n",
      "Epoch [3/4], Step [21450/28190], Loss: 4.1065\n",
      "Epoch [3/4], Step [21525/28190], Loss: 4.0633\n",
      "Epoch [3/4], Step [21600/28190], Loss: 3.9597\n",
      "Epoch [3/4], Step [21675/28190], Loss: 3.8418\n",
      "Epoch [3/4], Step [21750/28190], Loss: 3.9635\n",
      "Epoch [3/4], Step [21825/28190], Loss: 3.9864\n",
      "Epoch [3/4], Step [21900/28190], Loss: 3.9461\n",
      "Epoch [3/4], Step [21975/28190], Loss: 3.9032\n",
      "Epoch [3/4], Step [22050/28190], Loss: 3.9223\n",
      "Epoch [3/4], Step [22125/28190], Loss: 4.0859\n",
      "Epoch [3/4], Step [22200/28190], Loss: 3.8944\n",
      "Epoch [3/4], Step [22275/28190], Loss: 3.9248\n",
      "Epoch [3/4], Step [22350/28190], Loss: 3.9294\n",
      "Epoch [3/4], Step [22425/28190], Loss: 4.0617\n",
      "Epoch [3/4], Step [22500/28190], Loss: 3.9177\n",
      "Epoch [3/4], Step [22575/28190], Loss: 3.8961\n",
      "Epoch [3/4], Step [22650/28190], Loss: 4.0104\n",
      "Epoch [3/4], Step [22725/28190], Loss: 3.8314\n",
      "Epoch [3/4], Step [22800/28190], Loss: 3.8248\n",
      "Epoch [3/4], Step [22875/28190], Loss: 3.8625\n",
      "Epoch [3/4], Step [22950/28190], Loss: 3.9058\n",
      "Epoch [3/4], Step [23025/28190], Loss: 3.9120\n",
      "Epoch [3/4], Step [23100/28190], Loss: 3.8840\n",
      "Epoch [3/4], Step [23175/28190], Loss: 4.0392\n",
      "Epoch [3/4], Step [23250/28190], Loss: 3.8239\n",
      "Epoch [3/4], Step [23325/28190], Loss: 3.9032\n",
      "Epoch [3/4], Step [23400/28190], Loss: 3.8543\n",
      "Epoch [3/4], Step [23475/28190], Loss: 4.1278\n",
      "Epoch [3/4], Step [23550/28190], Loss: 3.7960\n",
      "Epoch [3/4], Step [23625/28190], Loss: 4.0591\n",
      "Epoch [3/4], Step [23700/28190], Loss: 3.9753\n",
      "Epoch [3/4], Step [23775/28190], Loss: 3.9606\n",
      "Epoch [3/4], Step [23850/28190], Loss: 3.6750\n",
      "Epoch [3/4], Step [23925/28190], Loss: 3.7897\n",
      "Epoch [3/4], Step [24000/28190], Loss: 4.1400\n",
      "Validation perplexity: 38.055607677999745\n",
      "Epoch [3/4], Step [24075/28190], Loss: 3.9271\n",
      "Epoch [3/4], Step [24150/28190], Loss: 3.9841\n",
      "Epoch [3/4], Step [24225/28190], Loss: 3.9354\n",
      "Epoch [3/4], Step [24300/28190], Loss: 3.7710\n",
      "Epoch [3/4], Step [24375/28190], Loss: 3.8696\n",
      "Epoch [3/4], Step [24450/28190], Loss: 3.8212\n",
      "Epoch [3/4], Step [24525/28190], Loss: 4.0423\n",
      "Epoch [3/4], Step [24600/28190], Loss: 4.0775\n",
      "Epoch [3/4], Step [24675/28190], Loss: 3.8081\n",
      "Epoch [3/4], Step [24750/28190], Loss: 4.0186\n",
      "Epoch [3/4], Step [24825/28190], Loss: 3.9881\n",
      "Epoch [3/4], Step [24900/28190], Loss: 3.9740\n",
      "Epoch [3/4], Step [24975/28190], Loss: 3.9073\n",
      "Epoch [3/4], Step [25050/28190], Loss: 3.7139\n",
      "Epoch [3/4], Step [25125/28190], Loss: 3.8814\n",
      "Epoch [3/4], Step [25200/28190], Loss: 3.9867\n",
      "Epoch [3/4], Step [25275/28190], Loss: 3.9692\n",
      "Epoch [3/4], Step [25350/28190], Loss: 4.1080\n",
      "Epoch [3/4], Step [25425/28190], Loss: 3.9058\n",
      "Epoch [3/4], Step [25500/28190], Loss: 4.1061\n",
      "Epoch [3/4], Step [25575/28190], Loss: 4.1676\n",
      "Epoch [3/4], Step [25650/28190], Loss: 3.7878\n",
      "Epoch [3/4], Step [25725/28190], Loss: 3.8984\n",
      "Epoch [3/4], Step [25800/28190], Loss: 4.2493\n",
      "Epoch [3/4], Step [25875/28190], Loss: 4.0311\n",
      "Epoch [3/4], Step [25950/28190], Loss: 3.9476\n",
      "Epoch [3/4], Step [26025/28190], Loss: 4.0314\n",
      "Epoch [3/4], Step [26100/28190], Loss: 3.9096\n",
      "Epoch [3/4], Step [26175/28190], Loss: 3.9191\n",
      "Epoch [3/4], Step [26250/28190], Loss: 3.7723\n",
      "Epoch [3/4], Step [26325/28190], Loss: 4.0197\n",
      "Epoch [3/4], Step [26400/28190], Loss: 3.8734\n",
      "Epoch [3/4], Step [26475/28190], Loss: 3.9685\n",
      "Epoch [3/4], Step [26550/28190], Loss: 3.9683\n",
      "Epoch [3/4], Step [26625/28190], Loss: 3.8382\n",
      "Epoch [3/4], Step [26700/28190], Loss: 3.8035\n",
      "Epoch [3/4], Step [26775/28190], Loss: 3.8448\n",
      "Epoch [3/4], Step [26850/28190], Loss: 3.8661\n",
      "Epoch [3/4], Step [26925/28190], Loss: 3.8377\n",
      "Epoch [3/4], Step [27000/28190], Loss: 3.9315\n",
      "Validation perplexity: 37.989964925879384\n",
      "Epoch [3/4], Step [27075/28190], Loss: 4.0113\n",
      "Epoch [3/4], Step [27150/28190], Loss: 3.9047\n",
      "Epoch [3/4], Step [27225/28190], Loss: 4.0524\n",
      "Epoch [3/4], Step [27300/28190], Loss: 3.7825\n",
      "Epoch [3/4], Step [27375/28190], Loss: 3.9437\n",
      "Epoch [3/4], Step [27450/28190], Loss: 3.9673\n",
      "Epoch [3/4], Step [27525/28190], Loss: 3.9682\n",
      "Epoch [3/4], Step [27600/28190], Loss: 3.8766\n",
      "Epoch [3/4], Step [27675/28190], Loss: 3.9663\n",
      "Epoch [3/4], Step [27750/28190], Loss: 3.9586\n",
      "Epoch [3/4], Step [27825/28190], Loss: 3.9968\n",
      "Epoch [3/4], Step [27900/28190], Loss: 3.7326\n",
      "Epoch [3/4], Step [27975/28190], Loss: 4.0257\n",
      "Epoch [3/4], Step [28050/28190], Loss: 3.9866\n",
      "Epoch [3/4], Step [28125/28190], Loss: 3.9822\n",
      "Epoch [3/4] Average Loss: 3.9531, Perplexity: 52.10\n",
      "Epoch [4/4], Step [0/28190], Loss: 4.1013\n",
      "Validation perplexity: 37.82617856139421\n",
      "Epoch [4/4], Step [75/28190], Loss: 3.8845\n",
      "Epoch [4/4], Step [150/28190], Loss: 3.8801\n",
      "Epoch [4/4], Step [225/28190], Loss: 3.9337\n",
      "Epoch [4/4], Step [300/28190], Loss: 3.9588\n",
      "Epoch [4/4], Step [375/28190], Loss: 4.0553\n",
      "Epoch [4/4], Step [450/28190], Loss: 3.7937\n",
      "Epoch [4/4], Step [525/28190], Loss: 3.8343\n",
      "Epoch [4/4], Step [600/28190], Loss: 3.9022\n",
      "Epoch [4/4], Step [675/28190], Loss: 4.0092\n",
      "Epoch [4/4], Step [750/28190], Loss: 3.7662\n",
      "Epoch [4/4], Step [825/28190], Loss: 3.9263\n",
      "Epoch [4/4], Step [900/28190], Loss: 3.8976\n",
      "Epoch [4/4], Step [975/28190], Loss: 3.9383\n",
      "Epoch [4/4], Step [1050/28190], Loss: 4.0117\n",
      "Epoch [4/4], Step [1125/28190], Loss: 3.8627\n",
      "Epoch [4/4], Step [1200/28190], Loss: 4.0376\n",
      "Epoch [4/4], Step [1275/28190], Loss: 3.8336\n",
      "Epoch [4/4], Step [1350/28190], Loss: 4.1452\n",
      "Epoch [4/4], Step [1425/28190], Loss: 3.9091\n",
      "Epoch [4/4], Step [1500/28190], Loss: 3.9166\n",
      "Epoch [4/4], Step [1575/28190], Loss: 3.8903\n",
      "Epoch [4/4], Step [1650/28190], Loss: 3.9721\n",
      "Epoch [4/4], Step [1725/28190], Loss: 3.9238\n",
      "Epoch [4/4], Step [1800/28190], Loss: 4.0100\n",
      "Epoch [4/4], Step [1875/28190], Loss: 3.8660\n",
      "Epoch [4/4], Step [1950/28190], Loss: 3.8736\n",
      "Epoch [4/4], Step [2025/28190], Loss: 3.8086\n",
      "Epoch [4/4], Step [2100/28190], Loss: 3.9716\n",
      "Epoch [4/4], Step [2175/28190], Loss: 4.1095\n",
      "Epoch [4/4], Step [2250/28190], Loss: 3.8855\n",
      "Epoch [4/4], Step [2325/28190], Loss: 3.9555\n",
      "Epoch [4/4], Step [2400/28190], Loss: 3.8355\n",
      "Epoch [4/4], Step [2475/28190], Loss: 4.0110\n",
      "Epoch [4/4], Step [2550/28190], Loss: 3.7843\n",
      "Epoch [4/4], Step [2625/28190], Loss: 3.9034\n",
      "Epoch [4/4], Step [2700/28190], Loss: 3.9263\n",
      "Epoch [4/4], Step [2775/28190], Loss: 4.0696\n",
      "Epoch [4/4], Step [2850/28190], Loss: 4.0385\n",
      "Epoch [4/4], Step [2925/28190], Loss: 3.9713\n",
      "Epoch [4/4], Step [3000/28190], Loss: 3.8808\n",
      "Validation perplexity: 37.59305305372164\n",
      "Epoch [4/4], Step [3075/28190], Loss: 3.8847\n",
      "Epoch [4/4], Step [3150/28190], Loss: 3.7793\n",
      "Epoch [4/4], Step [3225/28190], Loss: 3.9779\n",
      "Epoch [4/4], Step [3300/28190], Loss: 3.8424\n",
      "Epoch [4/4], Step [3375/28190], Loss: 3.8498\n",
      "Epoch [4/4], Step [3450/28190], Loss: 3.9047\n",
      "Epoch [4/4], Step [3525/28190], Loss: 4.0943\n",
      "Epoch [4/4], Step [3600/28190], Loss: 3.8481\n",
      "Epoch [4/4], Step [3675/28190], Loss: 3.9248\n",
      "Epoch [4/4], Step [3750/28190], Loss: 3.9210\n",
      "Epoch [4/4], Step [3825/28190], Loss: 3.8440\n",
      "Epoch [4/4], Step [3900/28190], Loss: 3.8182\n",
      "Epoch [4/4], Step [3975/28190], Loss: 3.8310\n",
      "Epoch [4/4], Step [4050/28190], Loss: 3.8060\n",
      "Epoch [4/4], Step [4125/28190], Loss: 3.7944\n",
      "Epoch [4/4], Step [4200/28190], Loss: 3.8947\n",
      "Epoch [4/4], Step [4275/28190], Loss: 3.8174\n",
      "Epoch [4/4], Step [4350/28190], Loss: 3.8415\n",
      "Epoch [4/4], Step [4425/28190], Loss: 3.9369\n",
      "Epoch [4/4], Step [4500/28190], Loss: 3.8246\n",
      "Epoch [4/4], Step [4575/28190], Loss: 4.0117\n",
      "Epoch [4/4], Step [4650/28190], Loss: 3.8241\n",
      "Epoch [4/4], Step [4725/28190], Loss: 3.8470\n",
      "Epoch [4/4], Step [4800/28190], Loss: 3.9060\n",
      "Epoch [4/4], Step [4875/28190], Loss: 3.8942\n",
      "Epoch [4/4], Step [4950/28190], Loss: 3.8031\n",
      "Epoch [4/4], Step [5025/28190], Loss: 3.8479\n",
      "Epoch [4/4], Step [5100/28190], Loss: 3.8811\n",
      "Epoch [4/4], Step [5175/28190], Loss: 4.0231\n",
      "Epoch [4/4], Step [5250/28190], Loss: 3.9552\n",
      "Epoch [4/4], Step [5325/28190], Loss: 4.0285\n",
      "Epoch [4/4], Step [5400/28190], Loss: 3.8692\n",
      "Epoch [4/4], Step [5475/28190], Loss: 3.8453\n",
      "Epoch [4/4], Step [5550/28190], Loss: 4.0452\n",
      "Epoch [4/4], Step [5625/28190], Loss: 3.8442\n",
      "Epoch [4/4], Step [5700/28190], Loss: 3.9285\n",
      "Epoch [4/4], Step [5775/28190], Loss: 3.8144\n",
      "Epoch [4/4], Step [5850/28190], Loss: 3.9079\n",
      "Epoch [4/4], Step [5925/28190], Loss: 4.0203\n",
      "Epoch [4/4], Step [6000/28190], Loss: 3.9177\n",
      "Validation perplexity: 37.21356982827098\n",
      "Epoch [4/4], Step [6075/28190], Loss: 3.9527\n",
      "Epoch [4/4], Step [6150/28190], Loss: 3.8556\n",
      "Epoch [4/4], Step [6225/28190], Loss: 3.7853\n",
      "Epoch [4/4], Step [6300/28190], Loss: 3.8636\n",
      "Epoch [4/4], Step [6375/28190], Loss: 3.9723\n",
      "Epoch [4/4], Step [6450/28190], Loss: 3.9986\n",
      "Epoch [4/4], Step [6525/28190], Loss: 3.8972\n",
      "Epoch [4/4], Step [6600/28190], Loss: 3.9341\n",
      "Epoch [4/4], Step [6675/28190], Loss: 3.7933\n",
      "Epoch [4/4], Step [6750/28190], Loss: 3.9815\n",
      "Epoch [4/4], Step [6825/28190], Loss: 3.9826\n",
      "Epoch [4/4], Step [6900/28190], Loss: 3.9218\n",
      "Epoch [4/4], Step [6975/28190], Loss: 3.9368\n",
      "Epoch [4/4], Step [7050/28190], Loss: 3.8236\n",
      "Epoch [4/4], Step [7125/28190], Loss: 3.8523\n",
      "Epoch [4/4], Step [7200/28190], Loss: 3.9106\n",
      "Epoch [4/4], Step [7275/28190], Loss: 3.9700\n",
      "Epoch [4/4], Step [7350/28190], Loss: 3.7171\n",
      "Epoch [4/4], Step [7425/28190], Loss: 3.8481\n",
      "Epoch [4/4], Step [7500/28190], Loss: 3.8625\n",
      "Epoch [4/4], Step [7575/28190], Loss: 3.7648\n",
      "Epoch [4/4], Step [7650/28190], Loss: 3.9682\n",
      "Epoch [4/4], Step [7725/28190], Loss: 4.0083\n",
      "Epoch [4/4], Step [7800/28190], Loss: 3.9291\n",
      "Epoch [4/4], Step [7875/28190], Loss: 3.9268\n",
      "Epoch [4/4], Step [7950/28190], Loss: 4.0241\n",
      "Epoch [4/4], Step [8025/28190], Loss: 3.8508\n",
      "Epoch [4/4], Step [8100/28190], Loss: 3.9272\n",
      "Epoch [4/4], Step [8175/28190], Loss: 3.8800\n",
      "Epoch [4/4], Step [8250/28190], Loss: 3.9988\n",
      "Epoch [4/4], Step [8325/28190], Loss: 4.1272\n",
      "Epoch [4/4], Step [8400/28190], Loss: 3.9837\n",
      "Epoch [4/4], Step [8475/28190], Loss: 3.8754\n",
      "Epoch [4/4], Step [8550/28190], Loss: 3.7583\n",
      "Epoch [4/4], Step [8625/28190], Loss: 3.9960\n",
      "Epoch [4/4], Step [8700/28190], Loss: 3.9297\n",
      "Epoch [4/4], Step [8775/28190], Loss: 4.0411\n",
      "Epoch [4/4], Step [8850/28190], Loss: 3.9108\n",
      "Epoch [4/4], Step [8925/28190], Loss: 3.8174\n",
      "Epoch [4/4], Step [9000/28190], Loss: 3.7910\n",
      "Validation perplexity: 37.0982572475795\n",
      "Epoch [4/4], Step [9075/28190], Loss: 3.8235\n",
      "Epoch [4/4], Step [9150/28190], Loss: 3.9279\n",
      "Epoch [4/4], Step [9225/28190], Loss: 3.9396\n",
      "Epoch [4/4], Step [9300/28190], Loss: 3.6934\n",
      "Epoch [4/4], Step [9375/28190], Loss: 3.7415\n",
      "Epoch [4/4], Step [9450/28190], Loss: 3.8432\n",
      "Epoch [4/4], Step [9525/28190], Loss: 3.7497\n",
      "Epoch [4/4], Step [9600/28190], Loss: 3.8356\n",
      "Epoch [4/4], Step [9675/28190], Loss: 3.9938\n",
      "Epoch [4/4], Step [9750/28190], Loss: 3.9126\n",
      "Epoch [4/4], Step [9825/28190], Loss: 3.8781\n",
      "Epoch [4/4], Step [9900/28190], Loss: 3.7920\n",
      "Epoch [4/4], Step [9975/28190], Loss: 3.7570\n",
      "Epoch [4/4], Step [10050/28190], Loss: 3.8499\n",
      "Epoch [4/4], Step [10125/28190], Loss: 4.0097\n",
      "Epoch [4/4], Step [10200/28190], Loss: 3.9020\n",
      "Epoch [4/4], Step [10275/28190], Loss: 3.9142\n",
      "Epoch [4/4], Step [10350/28190], Loss: 3.8505\n",
      "Epoch [4/4], Step [10425/28190], Loss: 3.7602\n",
      "Epoch [4/4], Step [10500/28190], Loss: 3.7576\n",
      "Epoch [4/4], Step [10575/28190], Loss: 3.9959\n",
      "Epoch [4/4], Step [10650/28190], Loss: 3.8477\n",
      "Epoch [4/4], Step [10725/28190], Loss: 4.0034\n",
      "Epoch [4/4], Step [10800/28190], Loss: 3.8161\n",
      "Epoch [4/4], Step [10875/28190], Loss: 3.9680\n",
      "Epoch [4/4], Step [10950/28190], Loss: 3.7589\n",
      "Epoch [4/4], Step [11025/28190], Loss: 3.8608\n",
      "Epoch [4/4], Step [11100/28190], Loss: 3.7798\n",
      "Epoch [4/4], Step [11175/28190], Loss: 3.8081\n",
      "Epoch [4/4], Step [11250/28190], Loss: 3.8335\n",
      "Epoch [4/4], Step [11325/28190], Loss: 3.9588\n",
      "Epoch [4/4], Step [11400/28190], Loss: 3.7744\n",
      "Epoch [4/4], Step [11475/28190], Loss: 3.9358\n",
      "Epoch [4/4], Step [11550/28190], Loss: 4.0509\n",
      "Epoch [4/4], Step [11625/28190], Loss: 4.0240\n",
      "Epoch [4/4], Step [11700/28190], Loss: 4.0252\n",
      "Epoch [4/4], Step [11775/28190], Loss: 3.8808\n",
      "Epoch [4/4], Step [11850/28190], Loss: 3.9511\n",
      "Epoch [4/4], Step [11925/28190], Loss: 3.9432\n",
      "Epoch [4/4], Step [12000/28190], Loss: 3.9438\n",
      "Validation perplexity: 36.91719475387255\n",
      "Epoch [4/4], Step [12075/28190], Loss: 3.9223\n",
      "Epoch [4/4], Step [12150/28190], Loss: 4.0419\n",
      "Epoch [4/4], Step [12225/28190], Loss: 3.9226\n",
      "Epoch [4/4], Step [12300/28190], Loss: 3.9540\n",
      "Epoch [4/4], Step [12375/28190], Loss: 3.9725\n",
      "Epoch [4/4], Step [12450/28190], Loss: 4.1082\n",
      "Epoch [4/4], Step [12525/28190], Loss: 3.9179\n",
      "Epoch [4/4], Step [12600/28190], Loss: 3.9884\n",
      "Epoch [4/4], Step [12675/28190], Loss: 4.0412\n",
      "Epoch [4/4], Step [12750/28190], Loss: 3.8750\n",
      "Epoch [4/4], Step [12825/28190], Loss: 3.9563\n",
      "Epoch [4/4], Step [12900/28190], Loss: 3.9727\n",
      "Epoch [4/4], Step [12975/28190], Loss: 3.8199\n",
      "Epoch [4/4], Step [13050/28190], Loss: 3.9381\n",
      "Epoch [4/4], Step [13125/28190], Loss: 4.0935\n",
      "Epoch [4/4], Step [13200/28190], Loss: 3.9620\n",
      "Epoch [4/4], Step [13275/28190], Loss: 3.8554\n",
      "Epoch [4/4], Step [13350/28190], Loss: 3.8153\n",
      "Epoch [4/4], Step [13425/28190], Loss: 4.0704\n",
      "Epoch [4/4], Step [13500/28190], Loss: 3.8160\n",
      "Epoch [4/4], Step [13575/28190], Loss: 3.8949\n",
      "Epoch [4/4], Step [13650/28190], Loss: 3.9979\n",
      "Epoch [4/4], Step [13725/28190], Loss: 3.9001\n",
      "Epoch [4/4], Step [13800/28190], Loss: 4.0796\n",
      "Epoch [4/4], Step [13875/28190], Loss: 3.8439\n",
      "Epoch [4/4], Step [13950/28190], Loss: 3.9828\n",
      "Epoch [4/4], Step [14025/28190], Loss: 3.8946\n",
      "Epoch [4/4], Step [14100/28190], Loss: 4.1747\n",
      "Epoch [4/4], Step [14175/28190], Loss: 3.9561\n",
      "Epoch [4/4], Step [14250/28190], Loss: 3.9968\n",
      "Epoch [4/4], Step [14325/28190], Loss: 3.8506\n",
      "Epoch [4/4], Step [14400/28190], Loss: 4.0089\n",
      "Epoch [4/4], Step [14475/28190], Loss: 3.6645\n",
      "Epoch [4/4], Step [14550/28190], Loss: 3.9261\n",
      "Epoch [4/4], Step [14625/28190], Loss: 3.8574\n",
      "Epoch [4/4], Step [14700/28190], Loss: 4.0895\n",
      "Epoch [4/4], Step [14775/28190], Loss: 3.8550\n",
      "Epoch [4/4], Step [14850/28190], Loss: 3.7632\n",
      "Epoch [4/4], Step [14925/28190], Loss: 3.9828\n",
      "Epoch [4/4], Step [15000/28190], Loss: 3.8235\n",
      "Validation perplexity: 36.69168324573566\n",
      "Epoch [4/4], Step [15075/28190], Loss: 3.9385\n",
      "Epoch [4/4], Step [15150/28190], Loss: 3.8294\n",
      "Epoch [4/4], Step [15225/28190], Loss: 3.9034\n",
      "Epoch [4/4], Step [15300/28190], Loss: 3.9085\n",
      "Epoch [4/4], Step [15375/28190], Loss: 3.9896\n",
      "Epoch [4/4], Step [15450/28190], Loss: 3.8156\n",
      "Epoch [4/4], Step [15525/28190], Loss: 3.9537\n",
      "Epoch [4/4], Step [15600/28190], Loss: 3.9961\n",
      "Epoch [4/4], Step [15675/28190], Loss: 4.0204\n",
      "Epoch [4/4], Step [15750/28190], Loss: 3.8342\n",
      "Epoch [4/4], Step [15825/28190], Loss: 4.0449\n",
      "Epoch [4/4], Step [15900/28190], Loss: 3.8408\n",
      "Epoch [4/4], Step [15975/28190], Loss: 3.8401\n",
      "Epoch [4/4], Step [16050/28190], Loss: 3.9423\n",
      "Epoch [4/4], Step [16125/28190], Loss: 3.7955\n",
      "Epoch [4/4], Step [16200/28190], Loss: 3.8944\n",
      "Epoch [4/4], Step [16275/28190], Loss: 3.9439\n",
      "Epoch [4/4], Step [16350/28190], Loss: 3.7673\n",
      "Epoch [4/4], Step [16425/28190], Loss: 3.9342\n",
      "Epoch [4/4], Step [16500/28190], Loss: 4.2313\n",
      "Epoch [4/4], Step [16575/28190], Loss: 3.6986\n",
      "Epoch [4/4], Step [16650/28190], Loss: 3.8619\n",
      "Epoch [4/4], Step [16725/28190], Loss: 3.8636\n",
      "Epoch [4/4], Step [16800/28190], Loss: 3.8786\n",
      "Epoch [4/4], Step [16875/28190], Loss: 3.9473\n",
      "Epoch [4/4], Step [16950/28190], Loss: 3.8436\n",
      "Epoch [4/4], Step [17025/28190], Loss: 3.8539\n",
      "Epoch [4/4], Step [17100/28190], Loss: 3.9648\n",
      "Epoch [4/4], Step [17175/28190], Loss: 3.9093\n",
      "Epoch [4/4], Step [17250/28190], Loss: 3.9212\n",
      "Epoch [4/4], Step [17325/28190], Loss: 3.9375\n",
      "Epoch [4/4], Step [17400/28190], Loss: 3.9263\n",
      "Epoch [4/4], Step [17475/28190], Loss: 3.7469\n",
      "Epoch [4/4], Step [17550/28190], Loss: 3.9415\n",
      "Epoch [4/4], Step [17625/28190], Loss: 3.9889\n",
      "Epoch [4/4], Step [17700/28190], Loss: 3.7860\n",
      "Epoch [4/4], Step [17775/28190], Loss: 3.9150\n",
      "Epoch [4/4], Step [17850/28190], Loss: 3.9171\n",
      "Epoch [4/4], Step [17925/28190], Loss: 4.0515\n",
      "Epoch [4/4], Step [18000/28190], Loss: 3.9131\n",
      "Validation perplexity: 36.47189332418415\n",
      "Epoch [4/4], Step [18075/28190], Loss: 3.7748\n",
      "Epoch [4/4], Step [18150/28190], Loss: 3.8525\n",
      "Epoch [4/4], Step [18225/28190], Loss: 3.7527\n",
      "Epoch [4/4], Step [18300/28190], Loss: 3.8986\n",
      "Epoch [4/4], Step [18375/28190], Loss: 3.8208\n",
      "Epoch [4/4], Step [18450/28190], Loss: 4.0709\n",
      "Epoch [4/4], Step [18525/28190], Loss: 3.8029\n",
      "Epoch [4/4], Step [18600/28190], Loss: 3.8723\n",
      "Epoch [4/4], Step [18675/28190], Loss: 3.8090\n",
      "Epoch [4/4], Step [18750/28190], Loss: 3.9631\n",
      "Epoch [4/4], Step [18825/28190], Loss: 3.8953\n",
      "Epoch [4/4], Step [18900/28190], Loss: 3.9020\n",
      "Epoch [4/4], Step [18975/28190], Loss: 3.9358\n",
      "Epoch [4/4], Step [19050/28190], Loss: 3.8814\n",
      "Epoch [4/4], Step [19125/28190], Loss: 3.8383\n",
      "Epoch [4/4], Step [19200/28190], Loss: 3.8775\n",
      "Epoch [4/4], Step [19275/28190], Loss: 3.9578\n",
      "Epoch [4/4], Step [19350/28190], Loss: 3.8193\n",
      "Epoch [4/4], Step [19425/28190], Loss: 3.8329\n",
      "Epoch [4/4], Step [19500/28190], Loss: 3.9398\n",
      "Epoch [4/4], Step [19575/28190], Loss: 3.9816\n",
      "Epoch [4/4], Step [19650/28190], Loss: 3.9463\n",
      "Epoch [4/4], Step [19725/28190], Loss: 4.0099\n",
      "Epoch [4/4], Step [19800/28190], Loss: 3.9103\n",
      "Epoch [4/4], Step [19875/28190], Loss: 3.8993\n",
      "Epoch [4/4], Step [19950/28190], Loss: 3.7524\n",
      "Epoch [4/4], Step [20025/28190], Loss: 3.8319\n",
      "Epoch [4/4], Step [20100/28190], Loss: 3.9415\n",
      "Epoch [4/4], Step [20175/28190], Loss: 3.9063\n",
      "Epoch [4/4], Step [20250/28190], Loss: 3.8631\n",
      "Epoch [4/4], Step [20325/28190], Loss: 3.7709\n",
      "Epoch [4/4], Step [20400/28190], Loss: 4.0499\n",
      "Epoch [4/4], Step [20475/28190], Loss: 3.9519\n",
      "Epoch [4/4], Step [20550/28190], Loss: 4.0356\n",
      "Epoch [4/4], Step [20625/28190], Loss: 3.8666\n",
      "Epoch [4/4], Step [20700/28190], Loss: 3.9444\n",
      "Epoch [4/4], Step [20775/28190], Loss: 4.0135\n",
      "Epoch [4/4], Step [20850/28190], Loss: 4.0259\n",
      "Epoch [4/4], Step [20925/28190], Loss: 3.8302\n",
      "Epoch [4/4], Step [21000/28190], Loss: 3.9090\n",
      "Validation perplexity: 36.4281975792011\n",
      "Epoch [4/4], Step [21075/28190], Loss: 4.0311\n",
      "Epoch [4/4], Step [21150/28190], Loss: 4.0105\n",
      "Epoch [4/4], Step [21225/28190], Loss: 3.9134\n",
      "Epoch [4/4], Step [21300/28190], Loss: 4.0009\n",
      "Epoch [4/4], Step [21375/28190], Loss: 3.9715\n",
      "Epoch [4/4], Step [21450/28190], Loss: 3.9565\n",
      "Epoch [4/4], Step [21525/28190], Loss: 3.9830\n",
      "Epoch [4/4], Step [21600/28190], Loss: 3.6875\n",
      "Epoch [4/4], Step [21675/28190], Loss: 4.0087\n",
      "Epoch [4/4], Step [21750/28190], Loss: 3.7148\n",
      "Epoch [4/4], Step [21825/28190], Loss: 3.8358\n",
      "Epoch [4/4], Step [21900/28190], Loss: 3.8371\n",
      "Epoch [4/4], Step [21975/28190], Loss: 3.8698\n",
      "Epoch [4/4], Step [22050/28190], Loss: 3.7967\n",
      "Epoch [4/4], Step [22125/28190], Loss: 3.8143\n",
      "Epoch [4/4], Step [22200/28190], Loss: 3.8338\n",
      "Epoch [4/4], Step [22275/28190], Loss: 3.7686\n",
      "Epoch [4/4], Step [22350/28190], Loss: 3.9514\n",
      "Epoch [4/4], Step [22425/28190], Loss: 3.7213\n",
      "Epoch [4/4], Step [22500/28190], Loss: 3.9697\n",
      "Epoch [4/4], Step [22575/28190], Loss: 3.8683\n",
      "Epoch [4/4], Step [22650/28190], Loss: 3.8829\n",
      "Epoch [4/4], Step [22725/28190], Loss: 3.6969\n",
      "Epoch [4/4], Step [22800/28190], Loss: 3.9517\n",
      "Epoch [4/4], Step [22875/28190], Loss: 3.7692\n",
      "Epoch [4/4], Step [22950/28190], Loss: 3.8929\n",
      "Epoch [4/4], Step [23025/28190], Loss: 3.8351\n",
      "Epoch [4/4], Step [23100/28190], Loss: 4.0046\n",
      "Epoch [4/4], Step [23175/28190], Loss: 3.8254\n",
      "Epoch [4/4], Step [23250/28190], Loss: 3.9174\n",
      "Epoch [4/4], Step [23325/28190], Loss: 3.8225\n",
      "Epoch [4/4], Step [23400/28190], Loss: 3.8987\n",
      "Epoch [4/4], Step [23475/28190], Loss: 3.8655\n",
      "Epoch [4/4], Step [23550/28190], Loss: 3.8924\n",
      "Epoch [4/4], Step [23625/28190], Loss: 3.7071\n",
      "Epoch [4/4], Step [23700/28190], Loss: 3.9435\n",
      "Epoch [4/4], Step [23775/28190], Loss: 4.0290\n",
      "Epoch [4/4], Step [23850/28190], Loss: 3.9144\n",
      "Epoch [4/4], Step [23925/28190], Loss: 3.8098\n",
      "Epoch [4/4], Step [24000/28190], Loss: 3.8385\n",
      "Validation perplexity: 36.183424042994105\n",
      "Epoch [4/4], Step [24075/28190], Loss: 3.8856\n",
      "Epoch [4/4], Step [24150/28190], Loss: 3.9931\n",
      "Epoch [4/4], Step [24225/28190], Loss: 3.7770\n",
      "Epoch [4/4], Step [24300/28190], Loss: 3.7820\n",
      "Epoch [4/4], Step [24375/28190], Loss: 3.8635\n",
      "Epoch [4/4], Step [24450/28190], Loss: 3.8988\n",
      "Epoch [4/4], Step [24525/28190], Loss: 3.9855\n",
      "Epoch [4/4], Step [24600/28190], Loss: 3.8629\n",
      "Epoch [4/4], Step [24675/28190], Loss: 3.8448\n",
      "Epoch [4/4], Step [24750/28190], Loss: 3.8824\n",
      "Epoch [4/4], Step [24825/28190], Loss: 3.7876\n",
      "Epoch [4/4], Step [24900/28190], Loss: 3.8476\n",
      "Epoch [4/4], Step [24975/28190], Loss: 4.0546\n",
      "Epoch [4/4], Step [25050/28190], Loss: 3.8603\n",
      "Epoch [4/4], Step [25125/28190], Loss: 4.0284\n",
      "Epoch [4/4], Step [25200/28190], Loss: 3.8024\n",
      "Epoch [4/4], Step [25275/28190], Loss: 3.7434\n",
      "Epoch [4/4], Step [25350/28190], Loss: 4.0050\n",
      "Epoch [4/4], Step [25425/28190], Loss: 3.8714\n",
      "Epoch [4/4], Step [25500/28190], Loss: 3.9932\n",
      "Epoch [4/4], Step [25575/28190], Loss: 3.7421\n",
      "Epoch [4/4], Step [25650/28190], Loss: 3.8519\n",
      "Epoch [4/4], Step [25725/28190], Loss: 3.9060\n",
      "Epoch [4/4], Step [25800/28190], Loss: 3.9019\n",
      "Epoch [4/4], Step [25875/28190], Loss: 3.8999\n",
      "Epoch [4/4], Step [25950/28190], Loss: 4.0576\n",
      "Epoch [4/4], Step [26025/28190], Loss: 3.8328\n",
      "Epoch [4/4], Step [26100/28190], Loss: 3.9135\n",
      "Epoch [4/4], Step [26175/28190], Loss: 3.9628\n",
      "Epoch [4/4], Step [26250/28190], Loss: 3.9813\n",
      "Epoch [4/4], Step [26325/28190], Loss: 4.0501\n",
      "Epoch [4/4], Step [26400/28190], Loss: 3.9281\n",
      "Epoch [4/4], Step [26475/28190], Loss: 3.9054\n",
      "Epoch [4/4], Step [26550/28190], Loss: 3.8220\n",
      "Epoch [4/4], Step [26625/28190], Loss: 3.7463\n",
      "Epoch [4/4], Step [26700/28190], Loss: 3.8380\n",
      "Epoch [4/4], Step [26775/28190], Loss: 3.9375\n",
      "Epoch [4/4], Step [26850/28190], Loss: 3.7436\n",
      "Epoch [4/4], Step [26925/28190], Loss: 3.8321\n",
      "Epoch [4/4], Step [27000/28190], Loss: 3.8498\n",
      "Validation perplexity: 35.881140196943875\n",
      "Epoch [4/4], Step [27075/28190], Loss: 3.9208\n",
      "Epoch [4/4], Step [27150/28190], Loss: 3.9214\n",
      "Epoch [4/4], Step [27225/28190], Loss: 3.7035\n",
      "Epoch [4/4], Step [27300/28190], Loss: 3.7748\n",
      "Epoch [4/4], Step [27375/28190], Loss: 4.0233\n",
      "Epoch [4/4], Step [27450/28190], Loss: 3.6675\n",
      "Epoch [4/4], Step [27525/28190], Loss: 3.9158\n",
      "Epoch [4/4], Step [27600/28190], Loss: 3.8153\n",
      "Epoch [4/4], Step [27675/28190], Loss: 3.9181\n",
      "Epoch [4/4], Step [27750/28190], Loss: 3.8750\n",
      "Epoch [4/4], Step [27825/28190], Loss: 3.9118\n",
      "Epoch [4/4], Step [27900/28190], Loss: 3.8546\n",
      "Epoch [4/4], Step [27975/28190], Loss: 4.0098\n",
      "Epoch [4/4], Step [28050/28190], Loss: 3.7628\n",
      "Epoch [4/4], Step [28125/28190], Loss: 3.9394\n",
      "Epoch [4/4] Average Loss: 3.9006, Perplexity: 49.43\n"
     ]
    }
   ],
   "source": [
    "#from src.helper  import\n",
    "from src.helper import get_cleaned_spanish_text_as_string, clean_text_spanish_remove,get_lines_without_number,clean_spanish_text,get_cleaned_text\n",
    "from src.helper import clean_text_spanish_both,clean_text_both,get_cleaned_spanish_text_as_string,clean_text_spanish_remove,get_lines_without_number,clean_spanish_text,get_cleaned_text\n",
    "\n",
    "text_path = \"content/spa_wikipedia_2021_30K-sentences.txt\"\n",
    "path_to_save_folder= \"model/attent_data\"\n",
    "\n",
    "#raw_text = get_cleaned_spanish_text_as_string(text_path)\n",
    "#enc_text = tokenizer.encode(raw_text)\n",
    "\n",
    "# Load the text data\n",
    "#raw_text = get_cleaned_spanish_text_as_string(text_path) #Standart \n",
    "raw_text =  get_cleaned_text(text_path,clean_text_spanish_both)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Tokenize the text\n",
    "token_ids = tokenizer.encode(raw_text)\n",
    "\n",
    "print(\"Total number of tokens:\", len(token_ids))\n",
    "print(\"First 10 tokens:\", token_ids[:10])\n",
    "print(\"Text size: \",len(raw_text))\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32 #Standard 32 \n",
    "context_length = 32  # Increased context size\n",
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 128\n",
    "attention_dim = 64\n",
    "hidden_dim = 64\n",
    "num_heads = 4\n",
    "\n",
    "# Create the DataLoader\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_dataloader(\n",
    "    raw_text, batch_size=batch_size, \n",
    "    context_length=context_length,     shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = LanguageModelWithAttention(\n",
    "    vocab_size, embedding_dim, attention_dim, context_length, hidden_dim, num_heads, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop parameters'\n",
    "num_epochs = 4\n",
    "data_loader = train_dataloader\n",
    "\n",
    "print_every = 75\n",
    "evaluate_every = 3000\n",
    "train_run_label = \"basic_att_preprocessingBoth\"\n",
    "\n",
    "from src.train import train_attention\n",
    "#(all_losses,train_losses,perplexities) = train_attention(model,num_epochs,optimizer,criterion,data_loader,path_to_save_folder,\n",
    "#                                               train_run_label,vocab_size,device,print_every)\n",
    "\n",
    "(all_losses,train_losses,perplexities,all_perplex) = train_attention(model,num_epochs,optimizer,criterion,data_loader,path_to_save_folder,train_run_label,vocab_size,device,evaluate_every,dev_dataloader,print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c34811-1d86-43a0-8ed3-cbe1d4a93520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a150d-09ff-4f9b-a821-bc121148b261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "display_name": "Python 3",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
